Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 638?646,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Quantitative modeling of the neural representation of adjective-noun 
phrases to account for fMRI activation 
 
 
Kai-min K. Chang1   Vladimir L. Cherkassky2   Tom M. Mitchell3   Marcel Adam Just2
Language Technologies Institute1
Center for Cognitive Brain Imaging2
Machine Learning Department3
Carnegie Mellon University 
Pittsburgh, PA 15213, U.S.A. 
{kkchang,cherkassky,tom.mitchell,just}@cmu.edu 
 
  
 
Abstract 
 
Recent advances in functional Magnetic 
Resonance Imaging (fMRI) offer a significant 
new approach to studying semantic represen-
tations in humans by making it possible to di-
rectly observe brain activity while people 
comprehend words and sentences. In this 
study, we investigate how humans compre-
hend adjective-noun phrases (e.g. strong dog) 
while their neural activity is recorded. Classi-
fication analysis shows that the distributed 
pattern of neural activity contains sufficient 
signal to decode differences among phrases. 
Furthermore, vector-based semantic models 
can explain a significant portion of system-
atic variance in the observed neural activity. 
Multiplicative composition models of the 
two-word phrase outperform additive models, 
consistent with the assumption that people 
use adjectives to modify the meaning of the 
noun, rather than conjoining the meaning of 
the adjective and noun. 
1 Introduction 
How humans represent meanings of individual 
words and how lexical semantic knowledge is 
combined to form complex concepts are issues 
fundamental to the study of human knowledge. 
There have been a variety of approaches from 
different scientific communities trying to charac-
terize semantic representations. Linguists have 
tried to characterize the meaning of a word with 
feature-based approaches, such as semantic roles 
(Kipper et al, 2006), as well as word-relation 
approaches, such as WordNet (Miller, 1995). 
Computational linguists have demonstrated that a 
word?s meaning is captured to some extent by 
the distribution of words and phrases with which 
it commonly co-occurs (Church & Hanks, 1990). 
Psychologists have studied word meaning 
through feature-norming studies (Cree & McRae, 
2003) in which human participants are asked to 
list the features they associate with various 
words. There are also efforts to recover the latent 
semantic structure from text corpora using tech-
niques such as LSA (Landauer & Dumais, 1997) 
and topic models (Blei et al, 2003). 
Recent advances in functional Magnetic 
Resonance Imaging (fMRI) provide a significant 
new approach to studying semantic 
representations in humans by making it possible 
to directly observe brain activity while people 
comprehend words and sentences. fMRI 
measures the hemodynamic response (changes in 
blood flow and blood oxygenation) related to 
neural activity in the human brain. Images can be 
acquired at good spatial resolution and reason-
able temporal resolution ? the activity level of 
15,000 - 20,000 brain volume elements (voxels) 
of about 50 mm3 each can be measured every 1 
second. Recent multivariate analyses of fMRI 
activity have shown that classifiers can be 
trained to decode which of several visually pre-
sented objects or object categories a person is 
contemplating, given the person?s fMRI-
measured neural activity (Cox and Savoy, 2003; 
O'Toole et al, 2005; Haynes and Rees, 2006; 
Mitchell et al, 2004). Furthermore, Mitchell et 
al. (2008) showed that word features computed 
from the occurrences of stimulus words (within a 
trillion-token Google text corpus that captures 
the typical use of words in English text) can 
predict the brain activity associated with the 
638
meaning of these words. They developed a 
generative model that is capable of predicting 
fMRI neural activity well enough that it can 
successfully match words it has not yet 
encountered to their previously unseen fMRI 
images with accuracies far above chance level. 
The distributed pattern of neural activity encodes 
the meanings of words, and the model?s success 
indicates some initial access to the encoding. 
Given these early succesess in using fMRI to 
discriminate categorial information and to model 
lexical semantic representations of individual 
words, it is interesting to ask whether a similar 
approach can be used to study the representation 
of adjective-noun phrases. In this study, we 
applied the vector-based models of semantic 
composition used in computational linguistics to 
model neural activation patterns obtained while 
subjects comprehended adjective-noun phrases. 
In an object-contemplation task, human partici-
pants were presented with 12 text labels of ob-
jects (e.g. dog) and were instructed to think of 
the same properties of the stimulus object consis-
tently during multiple presentations of each item. 
The participants were also shown adjective-noun 
phrases, where adjectives were used to modify 
the meaning of nouns (e.g. strong dog). 
Mitchell and Lapata (2008) presented a 
framework for representing the meaning of 
phrases and sentences in vector space. They 
discussed how an additive model, a 
multiplicative model, a weighted additive model, 
a Kintsch (2001) model, and a model which 
combines multiplicative and additive models can 
be used to model human behavior in similiarity 
judgements when human participants were 
presented with a reference containing a subject-
verb phrase (e.g., horse ran) and two landmarks 
(e.g., galloped and dissolved) and asked to 
choose which landmark was most similiar to the 
reference (in this case, galloped). They compared 
the composition models to human similarity 
ratings and found that all models were 
statistically significantly correlated with human 
judgements. Moreover, the multiplicative and 
combined model performed signficantlly better 
than the non-compositional models. Our 
approach is similar to that of Mitchell and Lapata 
(2008) in that we compared additive and 
multiplicative models to non-compositional 
models in terms of their ability to model human 
data. Our work differs from these efforts because 
we focus on modeling neural activity while 
people comprehend adjective-noun phrases. 
In section 2, we describe the experiment and 
how functional brain images were acquired. In 
section 3, we apply classifier analysis to see if 
the distributed pattern of neural activity contains 
sufficient signal to discriminate among phrases. 
In section 4, we discuss a vector-based approach 
to modeling the lexical semantic knowledge 
using word occurrence measures in a text corpus. 
Two composition models, namely the additive 
and the multiplicative models, along with two 
non-composition models, namely the adjective 
and the noun models, are used to explain the 
systematic variance in neural activation. Section 
5 distinguishes between two types of adjectives 
that are used in our stimuli: attribute-specifying 
adjectives and object-modifying adjectives. 
Classifier analysis suggests people interpret the 
two types of adjectives differently. Finally, we 
discuss some of the implications of our work and 
suggest some future studies. 
2 Brain Imaging Experiments on Adjec-
tive-Noun Comprehension 
2.1 Experimental Paradigm 
Nineteen right-handed adults (aged between 18 
and 32) from the Carnegie Mellon community 
participated and gave informed consent approved 
by the University of Pittsburgh and Carnegie 
Mellon Institutional Review Boards. Four addi-
tional participants were excluded from the analy-
sis due to head motion greater than 2.5 mm. 
The stimuli were text labels of 12 concrete 
nouns from 4 semantic categories with 3 
exemplars per category. The 12 nouns were bear, 
cat, dog (animal); bottle, cup, knife (utensil); 
carrot, corn, tomato (vegetable); airplane, train, 
and truck (vehicle; see Table 1). The fMRI 
neural signatures of these objects have been 
found in previous studies to elicit different neural 
activity. The participants were also shown each 
of the 12 nouns paired with an adjective, where 
the adjectives are expected to emphasize certain 
semantic properties of the nouns. For instance, in 
the case of strong dog, the adjective is used to 
emphasize the visual or physical aspect (e.g. 
muscular) of a dog, as opposed to the behavioral 
aspects (e.g. play, eat, petted) that people more 
often associate with the term. Notice that the last 
three adjectives in Table 1 are marked by aster-
isks to denote they are object-modifying adjec-
tives. These adjectives appear to behave differ-
ently from the ordinary attribute-specifying ad-
jectives. Section 5 is devoted to discussing the 
different adjective types in more detail. 
639
 
Adjective Noun Category 
Soft Bear Animal 
Large Cat Animal 
Strong Dog Animal 
Plastic Bottle Utensil 
Small Cup Utensil 
Sharp Knife Utensil 
Hard Carrot Vegetable 
Cut Corn Vegetable 
Firm Tomato Vegetable 
Paper* Airplane Vehicle 
Model* Train Vehicle 
Toy* Truck Vehicle 
Table 1. Word stimuli. Asterisks mark the ob-
ject-modifying adjectives, as opposed to the or-
dinary attribute-specifying adjectives. 
 
To ensure that participants had a consistent set 
of properties to think about, they were each 
asked to generate and write a set of properties for 
each exemplar in a session prior to the scanning 
session (such as ?4 legs, house pet, fed by me? 
for dog). However, nothing was done to elicit 
consistency across participants. The entire set of 
24 stimuli was presented 6 times during the 
scanning session, in a different random order 
each time. Participants silently viewed the 
stimuli and were asked to think of the same item 
properties consistently across the 6 presentations 
of the items. Each stimulus was presented for 3s, 
followed by a 7s rest period, during which the 
participants were instructed to fixate on an X 
displayed in the center of the screen. There were 
two additional presentations of fixation, 31s 
each, at the beginning and end of each session, to 
provide a baseline measure of activity. 
2.2 Data Acquisition and Processing 
Functional images were acquired on a Siemens 
Allegra 3.0T scanner (Siemens, Erlangen, 
Germany) at the Brain Imaging Research Center 
of Carnegie Mellon University and the 
University of Pittsburgh using a gradient echo 
EPI pulse sequence with TR = 1000 ms, TE = 30 
ms, and a 60? flip angle. Seventeen 5-mm thick 
oblique-axial slices were imaged with a gap of 1-
mm between slices. The acquisition matrix was 
64 x 64 with 3.125 x 3.125 x 5-mm voxels. Data 
processing were performed with Statistical 
Parametric Mapping software (SPM2, Wellcome 
Department of Cognitive Neurology, London, 
UK; Friston, 2005). The data were corrected for 
slice timing, motion, and linear trend, and were 
temporally smoothed with a high-pass filter 
using a 190s cutoff. The data were normalized to 
the MNI template brain image using a 12-
parameter affine transformation and resampled to 
3 x 3 x 6-mm3 voxels. 
The percent signal change (PSC) relative to 
the fixation condition was computed for each 
item presentation at each voxel. The mean of the 
four images (mean PSC) acquired within a 4s 
window, offset 4s from the stimulus onset (to 
account for the delay in hemodynamic response), 
provided the main input measure for subsequent 
analysis. The mean PSC data for each word 
presentation were further normalized to have 
mean zero and variance one to equate the 
variation between participants over exemplars. 
Due to the inherent limitations in the temporal 
properties of fMRI data, we consider here only 
the spatial distribution of the neural activity after 
the stimuli are comprehended and do not attempt 
to model the cogntive process of comprehension.  
3 Does the distribution of neural activ-
ity encode sufficient signal to classify 
adjective-noun phrases? 
3.1 Classifier Analysis 
We are interested in whether the distribution of 
neural activity encodes sufficient signal to de-
code both nouns and adjective-noun phrases. 
Given the observed neural activity when partici-
pants comprehended the adjective-noun phrases, 
Gaussian Na?ve Bayes classifiers were trained to 
identify cognitive states associated with viewing 
stimuli from the evoked patterns of functional 
activity (mean PSC). For instance, the classifier 
would predict which of the 24 exemplars the par-
ticipant was viewing and thinking about. Sepa-
rate classifiers were also trained for classifying 
the isolated nouns, the phrases, and the 4 seman-
tic categories. 
Since fMRI acquires the neural activity at 
15,000 ? 20,000 distinct voxel locations, many of 
which might not exhibit neural activity that en-
codes word or phrase meaning, the classifier 
analysis selected the voxels whose responses to 
the 24 different items were most stable across 
presentations. Voxel stability was computed as 
the average pairwise correlation between 24 item 
vectors across presentations. The focus on the 
most stable voxels effectively increased the 
signal-to-noise ratio in the data and facilitated 
further analysis by classifiers. Many of our 
previous analyses have indicated that 120 voxels 
is a set size suitable for our purposes. 
640
Classification results were evaluated using 6-
fold cross validation, where one of the 6 repeti-
tions was left out for each fold. The voxel selec-
tion procedure was performed separately inside 
each fold, using only the training data. Since 
multiple classes were involved, rank accuracy 
was used (Mitchell et al, 2004) to evaluate the 
classifier. Given a new fMRI image to classify, 
the classifier outputs a rank-ordered list of possi-
ble class labels from most to least likely. The 
rank accuracy is defined as the percentile rank of 
the correct class in this ordered output list. Rank 
accuracy ranges from 0 to 1. Classification 
analysis was performed separately for each par-
ticipant, and the mean rank accuracy was com-
puted over the participants. 
3.2 Results and Discussion 
Table 2 shows the results of the exemplar-level 
classification analysis. All classification accura-
cies were significantly higher than chance (p < 
0.05), where the chance level for each classifica-
tion is determined based on the empirical distri-
bution of rank accuracies for randomly generated 
null models. One hundred null models were gen-
erated by permuting the class labels. The classi-
fier was able to distinguish among the 24 exem-
plars with mean rank accuracies close to 70%. 
We also determined the classification accuracies 
separately for nouns only and phrases only. Dis-
tinct classifiers were trained. Classification accu-
racies were significantly higher (p < 0.05) for the 
nouns, calculated with a paired t-test. For 3 par-
ticipants, the classifier did not achieve reliable 
classification accuracies for the phrase stimuli. 
Moreover, we determined the classification accu-
racies separately for each semantic category of 
stimuli. There were no significant differences in 
accuracy across categories, except for the differ-
ence between vegetables and vehicles.  
 
Classifier Racc 
All 24 exemplars 0.69 
Nouns 0.71 
Phrases 0.64 
Animals 0.67 
Tools 0.66 
Vegetables 0.65 
Vehicles 0.69 
Table 2. Rank accuracies for classifiers. Distinct 
classifiers were trained to distinguish all 24 ex-
amples, nouns only, phrases only, and only 
words within each of the 4 semantic categories.  
 
High classification accuracies indicate that the 
distributed pattern of neural activity does encode 
sufficient signal to discriminate differences 
among stimuli. The classification accuracy for 
the nouns was on par with previous research, 
providing a replication of previous findings 
(Mitchell et al 2004). The classifiers performed 
better on the nouns than the phrases, consistent 
with our expectation that characterizing phrases 
is more difficult than characterizing nouns in 
isolation. It is easier for participants to recall 
properties associated with a familiar object than 
to comprehend a noun whose meaning is further 
modified by an adjective. The classification 
analysis also helps us to identify participants 
whose mental representations for phrases are 
consistent across phrase presentations. Subse-
quent regression analysis on phrase activation 
will be based on subjects who perform the phrase 
task well. 
4 Using vector-based models of seman-
tic representation to account for the 
systematic variances in neural activity 
4.1 Lexical Semantic Representation 
Computational linguists have demonstrated that a 
word?s meaning is captured to some extent by 
the distribution of words and phrases with which 
it commonly co-occurs (Church and Hanks, 
1990). Consequently, Mitchell et al (2008) en-
coded the meaning of a word as a vector of in-
termediate semantic features computed from the 
co-occurrences with stimulus words within the 
Google trillion-token text corpus that captures 
the typical use of words in English text. Moti-
vated by existing conjectures regarding the cen-
trality of sensory-motor features in neural repre-
sentations of objects (Caramazza and Shelton, 
1998), they selected a set of 25 semantic features 
defined by 25 verbs: see, hear, listen, taste, 
smell, eat, touch, rub, lift, manipulate, run, push, 
fill, move, ride, say, fear, open, approach, near, 
enter, drive, wear, break, and clean. These verbs 
generally correspond to basic sensory and motor 
activities, actions performed on objects, and ac-
tions involving changes in spatial relationships. 
Because there are only 12 stimuli in our ex-
periment, we consider only 5 sensory verbs (see 
hear, smell, eat and touch) to avoid overfitting 
with the full set of 25 verbs. Following the work 
of Bullinaria and Levy (2007), we consider the 
?basic semantic vector? which normalizes n(c,t), 
the count of times context word c occurs within a 
window of 5 words around the target word t. The 
641
basic semantic vector is thus the vector of condi-
tional probabilities, 
 
( ) ( )( )
( )
( )?==
c
tcn
tcn
tp
tcp
tcp
,
,,
|  
 
where all components are positive and sum to 
one. Table 3 shows the semantic representation 
for strong and dog. Notice that strong is heavily 
loaded on see and smell, whereas dog is heavily 
loaded on eat and see, consistent with the intui-
tive interpretation of these two words. 
 
 See Hear Smell Eat Touch 
Strong 0.63 0.06 0.26 0.03 0.03 
Dog 0.34 0.06 0.05 0.54 0.02 
Table 3. The lexical semantic representation for 
strong and dog. 
4.2 Semantic Composition 
We adopt the vector-based semantic composition 
models discussed in Mitchell and Lapata (2008). 
Let u and v denote the meaning of the adjective 
and noun, respectively, and let p denote the com-
position of the two words in vector space. We 
consider two non-composition models, the 
adjective model and the noun model, as well as 
two composition models, the additive model and 
the multplicative model. 
The adjective model assumes that the meaning 
of the composition is the same as the adjective: 
 
up =  
 
The noun model assumes that the meaning of 
the composition is the same as the noun: 
 
vp =  
 
The adjective model and the noun model cor-
respond to the assumption that when people 
comprehend phrases, they focus exclusively on 
one of the two words. This serves as a baseline 
for comparison to other models. 
The additive model assumes the meaning of 
the composition is a linear combination of the 
adjective and noun vector: 
 
vBuAp ?+?=  
 
where A and B are vectors of weighting coeffi-
cients. 
The multiplicative model assumes the mean-
ing of the composition is the element-wise prod-
uct of the two vectors: 
 
vuCp ??=  
 
Mitchell and Lapata (2008) fitted the parame-
ters of the weighting vectors A, B, and C, though 
we assume A = B = C = 1, since we are interested 
in the model comparison. Also, there are no 
model complexity issues, since the number of 
parameters in the four models is the same. 
More critically, the additive model and multi-
plicative model correspond to different cognitive 
processes. On the one hand, the additive model 
assumes that people concatenate the meanings of 
the two words when comprehending phrases. On 
the other hand, the multiplicative model assumes 
that the contribution of u is scaled to its rele-
vance to v, or vice versa. Notice that the former 
assumption of the multiplicative model corre-
sponds to the modifier-head interpretation where 
adjectives are used to modify the meaning of 
nouns. To foreshadow our results, we found the 
modifier-head interpretation of the multiplicative 
model to best account for the neural activity ob-
served in adjective-noun phrase data.
Table 4 shows the semantic representation for 
strong dog under each of the four models. Al-
though the multiplicative model appears to have 
small loadings on all features, the relative distri-
bution of loadings still encodes sufficient infor-
mation, as our later analysis will show. Notice 
how the additive model concatenates the mean-
ing of two words and is heavily loaded on see, 
eat, and smell, whereas the multiplicative model 
zeros out unshared features like eat and smell. As 
a result, the multiplicative model predicts that the 
visual aspects will be emphasized when a par-
ticipant is thinking about strong dog, while the 
additive model predicts that, in addition, the be-
havioral aspects (e.g., eat, smell, and hear) of 
dog will be emphasized. 
 
 See Hear Smell Eat Touch
Adj 0.63 0.06 0.26 0.03 0.03 
Noun 0.34 0.06 0.05 0.54 0.02 
Add 0.96 0.12 0.31 0.57 0.04
Multi 0.21 0.00 0.01 0.01 0.00
Table 4. The semantic representation for strong 
dog under the adjective, noun, additive, and 
multiplicative models. 
 
642
Notice that these 4 vector-based semantic 
composition models ignore word order. This cor-
responds to the bag-of-words assumption, such 
that the representation for strong dog will be the 
same as that of dog strong. The bag-of-words 
model is used as a simplifying assumption in 
several semantic models, including LSA (Lan-
dauer & Dumais, 1997) and topic models (Blei et 
al., 2003). 
There were two main hypotheses that we 
tested. First, people usually regard the noun in 
the adjective-noun pair as the linguistic head. 
Therefore, meaning associated with the noun 
should be more evoked. Thus, we predicted that 
the noun model would outperform the adjective 
model. Second, people make more interpreta-
tions that use adjectives to modify the meaning 
of the noun, rather than disjunctive interpreta-
tions that add together or take the union of the 
semantic features of the two words. Thus, we 
predicted that the multiplicative model would 
outperform the additive model. 
4.3 Regression Fit 
In this analysis, we train a regression model to fit 
the activation profile for the 12 phrase stimuli. 
We focused on subjects for whom the classifier 
established reliable classification accuracies for 
the phrase stimuli. The regression model exam-
ined to what extent the semantic feature vectors 
(explanatory variables) can account for the varia-
tion in neural activity (response variable) across 
the 12 stimuli. All explanatory variables were 
entered into the regression model simultane-
ously. More precisely, the predicted activity av at 
voxel v in the brain for word w is given by 
 
( )?
=
+=
n
i
viviv wfa
1
??  
 
where fi(w) is the value of the ith intermediate 
semantic feature for word w, ?vi is the regression 
coefficient that specifies the degree to which the 
ith intermediate semantic feature activates voxel 
v, and ?v is the model?s error term that represents 
the unexplained variation in the response vari-
able. Least squares estimates of ?vi were obtained 
to minimize the sum of squared errors in recon-
structing the training fMRI images. An L2 regu-
larization with lambda = 1.0 was added to pre-
vent overfitting given the high parameter-to-
data-points ratios. A regression model was 
trained for each of the 120 voxels and the re-
ported R2 is the average across the 120 voxels. 
R2 measures the amount of systematic variance 
explained by the model. Regression results were 
evaluated using 6-fold cross validation, where 
one of the 6 repetitions was left out for each fold.  
Linear regression assumes a linear dependency 
among the variables and compares the variance 
due to the independent variables against the vari-
ance due to the residual errors. While the linear-
ity assumption may be overly simplistic, it re-
flects the assumption that fMRI activity often 
reflects a superimposition of contributions from 
different sources, and has provided a useful first 
order approximation in the field (Mitchell et al, 
2008). 
4.4 Results and Discussion 
The second column of Table 5 shows the R2 re-
gression fit (averaged across 120 voxels) of the 
adjective, noun, additive, and multiplicative 
model to the neural activity observed in adjec-
tive-noun phrase data. The noun model signifi-
cantly (p < 0.05) outperformed the adjective 
model, estimated with a paired t-test. Moreover, 
the difference between the additive and adjective 
models was not significant, whereas the differ-
ence between the additive and noun models was 
significant (p < 0.05). The multiplicative model 
significantly (p < 0.05) outperformed both of the 
non-compositional models, as well as the addi-
tive model. 
More importantly, the two hypotheses that we 
were testing were both verified. Notice Table 5 
supports our hypothesis that the noun model 
should outperform the adjective model based on 
the assumption that the noun is generally more 
central to the phrase meaning than is the adjec-
tive. Table 5 also supports our hypothesis that 
the multiplicative model should outperform the 
additive model, based on the assumption that 
adjectives are used to emphasize particular se-
mantic features that will already be represented 
in the semantic feature vector of the noun. Our 
findings here are largely consistent with Mitchell 
and Lapata (2008). 
 
 R2 Racc 
Adjective 0.34 0.57 
Noun 0.36 0.61 
Additive 0.35 0.60 
Multiplicative 0.42 0.62 
Table 5. Regression fit and regression-based 
classification rank accuracy of the adjective, 
noun, additive, and multiplicative models for 
phrase stimuli. 
 
643
Following Mitchell et al (2008), the regres-
sion model can be used to decode mental states. 
Specifically, for each regression model, the esti-
mated regression weights can be used to generate 
the predicted activity for each word. Then, a pre-
viously unseen neural activation vector is identi-
fied with the class of the predicted activation that 
had the highest correlation with the given ob-
served neural activation vector. Notice that, 
unlike Mitchell et al (2008), where the regres-
sion model was used to make predictions for 
items outside the training set, here we are just 
showing that the regression model can be used 
for classification purposes. 
The third column of Table 5 shows the rank 
accuracies classifying mental concepts using the 
predicted activation from the adjective, noun, 
additive, and multiplicative models. All rank ac-
curacies were significantly higher (p < 0.05) than 
chance, where the chance level for each classifi-
cation is again determined by permutation test-
ing. More importantly, here we observe a rank-
ing of these four models similar to that observed 
for the regression analysis. Namely, the noun 
model performs significantly better (p < 0.05) 
than the adjective model, and the multiplicative 
model performs significantly better (p < 0.05) 
than the additive model. However, the difference 
between the multiplicative model and the noun 
model is not statistically significant in this case. 
5 Comparing the attribute-specifying 
adjectives with the object-modifying 
adjectives 
Some of the phrases contained adjectives that 
changed the meaning of the noun. In the case of 
vehicle nouns, adjectives were chosen to modify 
the manipulability of the nouns (e.g., to make an 
airplane more manipulable, paper was chosen as 
the modifier). This type of modifier raises two 
issues. First, these modifiers (e.g. paper, model, 
toy) more typically assume the part of speech 
(POS) tag of nouns, unlike our other modifiers 
(e.g., soft, large, strong) whose typical POS tag 
is adjective. Second, these modifiers combine 
with the noun to denote a very different object 
from the noun in isolation (paper airplane, 
model train, toy truck), in comparison to other 
cases where the adjective simply specifies an 
attribute of the noun (soft bear, large cat, strong 
dog, etc.). In order to study this difference, we 
performed classification analysis separately for 
the attribute-specifying adjectives and the object-
modifying adjectives. 
Our hypothesis is that the phrases with attrib-
ute-specifying adjectives will be much more dif-
ficult to distinguish from the original nouns than 
the adjectives that change the referent. For in-
stance, we hypothesize that it is much more dif-
ficult to distinguish the neural representation for 
strong dog versus dog than it is to distinguish the 
neural representation for paper airplane versus 
airplane. To verify this, Gaussian Na?ve Bayes 
classifiers were trained to discriminate between 
each of the 12 pairs of nouns and adjective-noun 
phrases. The average classification for phrases 
with object-modifying adjectives is 0.76, 
whereas classification accuracies for phrases 
with attribute-specifying adjectives are 0.68. The 
difference is statistically significant at p < 0.05. 
This result supports our hypothesis. 
Furthermore, we performed regression-based 
classification separately for the two types of ad-
jectives. Notice that the number of phrases with 
object-modifying adjectives is much less than the 
number of phrases with attribute-specifying ad-
jectives (3 vs. 9). This affects the parameter-to-
data-points ratio in our regression model. Conse-
quently, an L2 regularization with lambda = 10.0 
was used to prevent overfitting. Table 6 shows a 
pattern similar to that seen in section 4 is ob-
served for the attribute-specifying adjectives. 
That is, the noun model outperformed the adjec-
tive model and the multiplicative model outper-
formed the additive model when using attribute-
specifying adjectives. However, for the object-
modifying adjectives, the noun model no longer 
outperformed the adjective model. Moreover, the 
additive model performed better than the noun 
model. Although neither difference is statistically 
significant, this clearly shows a pattern different 
from the attribute-specifying adjectives. This 
result suggests that when interpreting phrases 
like paper airplane, it is more important to con-
sider contributions from the adjectives, compared 
to when interpreting phrases like strong dog, 
where the contribution from the adjective is sim-
ply to specify a property of the item typically 
referred to by the noun in isolation. 
  
 Attribute-
specifying 
Object-
modifying 
Adjective 0.57 0.65 
Noun 0.62 0.64 
Additive 0.61 0.65 
Multiplicative 0.63 0.67 
Table 6. Separate regression-based classification 
rank accuracy for phrases with attribute-
specifying or object-modifying adjectives. 
644
In light of this observation, we plan to extend 
our analysis of adjective-nouns phrases to noun-
noun phrases, where participants will be shown 
noun phrases (e.g. carrot knife) and instructed to 
think of a likely meaning for the phrases. Unlike 
adjective-noun phrases, where a single interpre-
tation often dominates, noun-noun combinations 
allow multiple interpretations (e.g., carrot knife 
can be interpreted as a knife that is specifically 
used to cut carrots or a knife carved out of car-
rots). There exists an extensive literature on the 
conceptual combination of noun-noun phrases. 
Costello and Keane (1997) provide extensive 
studies on the polysemy of conceptual combina-
tion. More importantly, they outline different 
rules of combination, including property map-
ping, relational mapping, hybrid mapping, etc. It 
will be interesting to see if different composition 
models better account for neural activation when 
different kinds of combination rules are used. 
6 Contribution and Conclusion 
Experimental results have shown that the distrib-
uted pattern of neural activity while people are 
comprehending adjective-noun phrases does con-
tain sufficient information to decode the stimuli 
with accuracies significantly above chance. Fur-
thermore, vector-based semantic models can ex-
plain a significant portion of systematic variance 
in observed neural activity. Multiplicative com-
position models outperform additive models, a 
trend that is consistent with the assumption that 
people use adjectives to modify the meaning of 
the noun, rather than conjoining the meaning of 
the adjective and noun. 
In this study, we represented the meaning of 
both adjectives and nouns in terms of their co-
occurrences with 5 sensory verbs. While this 
type of representation might be justified for con-
crete nouns (hypothesizing that their neural rep-
resentations are largely grounded in sensory-
motor features), it might be that a different repre-
sentation is needed for adjectives. Further re-
search is needed to investigate alternative repre-
sentations for both nouns and adjectives. More-
over, the composition models that we presented 
here are overly simplistic in a number of ways. 
We look forward to future research to extend the 
intermediate representation and to experiment 
with different modeling methodologies. An al-
ternative approach is to model the semantic rep-
resentation as a hidden variable using a genera-
tive probabilistic model that describes how neu-
ral activity is generated from some latent seman-
tic representation. We are currently exploring the 
infinite latent semantic feature model (ILFM; 
Griffiths & Ghahramani, 2005), which assumes a 
non-parametric Indian Buffet prior to the binary 
feature vector and models neural activation with 
a linear Gaussian model. The basic proposition 
of the model is that the human semantic knowl-
edge system is capable of storing an infinite list 
of features (or semantic components) associated 
with a concept; however, only a subset is ac-
tively recalled during any given task (context-
dependent). Thus, a set of latent indicator vari-
ables is introduced to indicate whether a feature 
is actively recalled at any given task. We are in-
vestigating if the compositional models also op-
erate in the learned latent semantic space.  
The premise of our research relies on ad-
vancements in the fields of computational lin-
guistics and cognitive neuroimaging. Indeed, we 
are at an especially opportune time in the history 
of the study of language, when linguistic corpora 
allow word meanings to be computed from the 
distribution of word co-occurrence in a trillion-
token text corpus, and brain imaging technology 
allows us to directly observe and model neural 
activity associated with the conceptual combina-
tion of lexical items. An improved understanding 
of language processing in the brain could yield a 
more biologically-informed model of semantic 
representation of lexical knowledge. We there-
fore look forward to further brain imaging stud-
ies shedding new light on the nature of human 
representation of semantic knowledge. 
Acknowledgements 
This research was supported by the National Sci-
ence Foundation, Grant No. IIS-0835797, and by 
the W. M. Keck Foundation. We would like to 
thank Jennifer Moore for help in preparation of 
the manuscript. 
References  
Blei, D. M., Ng, A. Y., Jordan, and M. I.. 2003. La-
tent dirichlet alocation. Journal of Machine Learn-
ing Research 3, 993-1022. 
Bullinaria, J., and Levy, J. 2007. Extracting semantic 
representations from word co-occurrence statistics: 
A computational study. Behavioral Research 
Methods, 39:510-526. 
Caramazza, A., and Shelton, J. R. 1998. Domain-
specific knowledge systems in the brain the ani-
mate inanimate distinction. Journal of Cognitive 
Neuroscience 10(1), 1-34. 
645
Church, K. W., and Hanks, P. 1990. Word association 
norms, mutual information, and lexicography. 
Computational Linguistics, 16, 22-29. 
Cree, G. S., and McRae, K. 2003. Analyzing the fac-
tors underlying the structure and computation of 
the meaning of chipmunk, cherry, chisel, cheese, 
and cello (and many other such concrete nouns). 
Journal of Experimental Psychology: General 
132(2), 163-201. 
Costello, F., and Keane, M. 2001. Testing two theo-
ries of conceptual combination: Alignment versus 
diagnosticity in the comprehension and production 
of combined concepts. Journal of Experimental 
Psychology: Learning, Memory & Cognition, 
27(1): 255-271. 
Cox, D. D., and Savoy, R. L. 2003. Functioning mag-
netic resonance imaging (fMRI) "brain reading": 
Detecting and classifying distributed patterns of 
fMRI activity in human visual cortex. NeuroImage 
19, 261-270. 
Friston, K. J. 2005. Models of brain function in neuro-
imaging. Annual Review of Psychology 56, 57-87. 
Griffiths, T. L., and Ghahramani, Z. 2005. Infinite 
latent feature models and the Indian buffet process. 
Gatsby Unit Technical Report GCNU-TR-2005-
001. 
Haynes, J. D., and Rees, G. 2006. Decoding mental 
states from brain activity in humans. Nature Re-
views Neuroscience 7(7), 523-534. 
Kintsch, W. 2001. Prediction. Cognitive Science, 
25(2):173-202. 
Landauer, T.K., and Dumais, S. T. 1997. A solution to 
Plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of 
knowledge. Psychological Review, 104(2), 211-
240. 
Miller, G. A. 1995. WordNet: A lexical database for 
English. Communications of the ACM 38, 39-41. 
Mitchell, J., and Lapata, M. 2008. Vector-based mod-
els of semantic composition. Proceedings of ACL-
08: HLT, 236-244. 
Mitchell, T., Hutchinson, R., Niculescu, R. S., 
Pereira, F., Wang, X., Just, M. A., and Newman, S. 
D. 2004. Learning to decode cognitive states from 
brain images. Machine Learning 57, 145-175. 
Mitchell, T., Shinkareva, S.V., Carlson, A., Chang, 
K.M., Malave, V.L., Mason, R.A., and Just, M.A. 
2008. Predicting human brain activity associated 
with the meanings of nouns. Science 320, 1191-
1195. 
O'Toole, A. J., Jiang, F., Abdi, H., and Haxby, J. V. 
2005. Partially distributed representations of ob-
jects and faces in ventral temporal cortex. Journal 
of Cognitive Neuroscience, 17, 580-590. 
 
646
Learning to Classify Email into "Speech Acts"  
 
William W. Cohen1 Vitor R. Carvalho2 Tom M. Mitchell1,2 
1Center for Automated Learning & Discovery 
 Carnegie Mellon University 
  Pittsburgh, PA 15213 
2Language Technology Institute  
Carnegie Mellon University  
   Pittsburgh, PA 15213 
Abstract 
It is often useful to classify email accord-
ing to the intent of the sender (e.g., "pro-
pose a meeting", "deliver information"). 
We present experimental results in learn-
ing to classify email in this fashion, 
where each class corresponds to a verb-
noun pair taken from a predefined ontol-
ogy describing typical ?email speech 
acts?.   We demonstrate that, although 
this categorization problem is quite dif-
ferent from ?topical? text classification, 
certain categories of messages can none-
theless be detected with high precision 
(above 80%) and reasonable recall (above 
50%) using existing text-classification 
learning methods. This result suggests 
that useful task-tracking tools could be 
constructed based on automatic classifi-
cation into this taxonomy.  
1 Introduction 
In this paper we discuss using machine learn-
ing methods to classify email according to the 
intent of the sender.  In particular, we classify 
emails according to an ontology of verbs (e.g., 
propose, commit, deliver) and nouns (e.g., infor-
mation, meeting, task), which jointly describe the 
?email speech act? intended by the email sender.   
A method for accurate classification of email 
into such categories would have many potential 
benefits. For instance, it could be used to help an 
email user track the status of ongoing joint activi-
ties.  Delegation and coordination of joint tasks is 
a time-consuming and error-prone activity, and 
the cost of errors is high: it is not uncommon that 
commitments are forgotten, deadlines are missed, 
and opportunities are wasted because of a failure 
to properly track, delegate, and prioritize sub-
tasks. The classification methods we consider  
 
methods which could be used to partially auto-
mate this sort of activity tracking. A hypothetical 
example of an email assistant that works along 
these lines is shown in Figure 1. 
Bill,
Do you have any sample 
scheduling-related email we 
could use as data?  -Steve
Assistant announces:  ?new 
email request, priority 
unknown.?
Sure, I?ll put some together 
shortly. -Bill
Assistant:  ?should I add this 
new commitment to your to-
do list??
Fred, can you collect the msgs
from the CSPACE corpora 
tagged w/ the  ?meeting?
noun, ASAP? -Bill
Assistant:  notices outgoing
request, may take action if no 
answer is received promptly.
Yes, I can get to that in the 
next few days.  Is next 
Monday ok? -Fred
Assistant:  notices incoming 
commitment. ?Should I send 
Fred a reminder on Monday??
 
Figure 1 - Dialog with a hypothetical email assistant 
that automatically detects email speech acts.  Dashed 
boxes indicate outgoing messages.  (Messages have 
been edited for space and anonymity.) 
2 Related Work 
Our research builds on earlier work defining il-
locutionary points of speech acts (Searle, 1975), 
and relating such speech acts to email and work-
flow tracking (Winograd, 1987, Flores & Lud-
low, 1980, Weigant et al 2003). Winograd 
suggested that research explicating the speech-act 
based ?language-action perspective? on human 
communication could be used to build more use-
ful tools for coordinating joint activities.  The 
Coordinator (Winograd, 1987) was one such sys-
tem, in which users augmented email messages 
with additional annotations indicating intent. 
While such systems have been useful in lim-
ited contexts, they have also been criticized as 
cumbersome: by forcing users to conform to a 
particular formal system, they constrain commu-
nication and make it less natural (Schoop, 2001); 
in short, users often prefer unstructured email 
interactions (Camino et al 1998). We note that 
these difficulties are avoided if messages can be 
automatically annotated by intent, rather than 
soliciting a statement of intent from the user. 
Murakoshi et al (1999) proposed an email an-
notation scheme broadly similar to ours, called a 
?deliberation tree?, and an algorithm for con-
structing deliberation trees automatically, but 
their approach was not quantitatively evaluated. 
The approach is based on recognizing a set of 
hand-coded linguistic ?clues?.  A limitation of 
their approach is that these hand-coded linguistic 
?clues? are language-specific (and in fact limited 
to Japanese text.) 
Prior research on machine learning for text 
classification has primarily considered classifica-
tion of documents by topic (Lewis, 1992; Yang, 
1999), but also has addressed sentiment detection 
(Pang et al, 2002;  Weibe et al, 2001) and au-
thorship attribution (e.g., Argamon et al 2003).   
There has been some previous use of machine 
learning to classify email messages (Cohen 1996; 
Sahami et al, 1998; Rennie, 2000; Segal & 
Kephart, 2000).  However, to our knowledge, 
none of these systems has investigated learning 
methods for assigning email speech acts. Instead, 
email is generally classified into folders (i.e., ac-
cording to topic) or according to whether or not it 
is ?spam?. Learning systems have been previ-
ously used to automatically detect acts in 
conversational speech (e.g. Finke et al, 1998). 
3 An Ontology of Email Acts 
Our ontology of nouns and verbs covering some 
of the possible speech acts associated with emails 
is summarized in Figure 2.  We assume that a 
single email message may contain multiple acts, 
and that each act is described by a verb-noun pair 
drawn from this ontology (e.g., "deliver data").   
The underlined nodes in the figure indicate the 
nouns and verbs for which we have trained clas-
sifiers (as discussed in subsequent sections). 
To define the noun and verb ontology of 
Figure 2, we first examined email from several 
corpora (including our own inboxes) to find regu-
larities, and then performed a more detailed 
analysis of one corpus. The ontology was further 
refined in the process of labeling the corpora de-
scribed below. 
In refining this ontology, we adopted several 
principles. First, we believe that it is more impor-
tant for the ontology to reflect observed linguistic 
behavior than to reflect any abstract view of the 
space of possible speech acts. As a consequence, 
the taxonomy of verbs contains concepts that are 
atomic linguistically, but combine several illocu-
tionary points. (For example, the linguistic unit 
"let's do lunch" is both directive, as it requests the 
receiver, and commissive, as it implicitly com-
mits the sender. In our taxonomy this is a single 
'propose' act.) Also, acts which are abstractly 
possible but not observed in our data are not rep-
resented (for instance, declarations). 
 
Noun 
Activity Information 
Meeting 
Logistics 
Data 
Opinion Ongoing 
Activity 
Data Single 
Event 
Meeting Other   
Short Term 
Task 
Other 
Data Committee 
<Verb><Noun> 
Verb 
Remind 
Propose 
Deliver 
Commit 
Request 
Amend 
Refuse 
Greet 
Other Negotiate 
Initiate Conclude 
 
Figure 2 ? Taxonomy  
Second, we believe that the taxonomy must re-
flect common non-linguistic uses of email, such 
as the use of email as a mechanism to deliver 
files. We have grouped this with the linguistically 
similar speech act of delivering information. 
The verbs in Figure 1 are defined as follows.  
A request asks (or orders) the recipient to per-
form some activity. A question is also considered 
a request (for delivery of information).  
A propose message proposes a joint activity, 
i.e., asks the recipient to perform some activity 
and commits the sender as well, provided the re-
cipient agrees to the request.  A typical example 
is an email suggesting a joint meeting.  
An amend message amends an earlier proposal. 
Like a proposal, the message involves both a 
commitment and a request.  However, while a 
proposal is associated with a new task, an 
amendment is a suggested modification of an 
already-proposed task. 
A commit message commits the sender to 
some future course of action, or confirms the 
senders' intent to comply with some previously 
described course of action.   
A deliver message delivers something, e.g., 
some information, a PowerPoint presentation,  
the URL of a website, the answer to a question, a 
message sent "FYI?, or an opinion. 
The refuse, greet, and remind verbs occurred 
very infrequently in our data, and hence we did 
not attempt to learn classifiers for them (in this 
initial study). The primary reason for restricting 
ourselves in this way was our expectation that 
human annotators would be slower and less reli-
able if given a more complex taxonomy.  
The nouns in Figure 2 constitute possible ob-
jects for the email speech act verbs. The nouns 
fall into two broad categories. 
Information nouns are associated with email 
speech acts described by the verbs Deliver, Re-
mind and Amend, in which the email explicitly 
contains information. We also associate informa-
tion nouns with the verb Request, where the 
email contains instead a description of the needed 
information (e.g., "Please send your birthdate." 
versus "My birthdate is ?".  The request act is 
actually for a 'deliver information' activity). In-
formation includes data believed to be fact as 
well as opinions, and also attached data files. 
Activity nouns are generally associated with 
email speech acts described by the verbs Pro-
pose, Request, Commit, and Refuse.  Activities 
include meetings, as well as longer term activities 
such as committee memberships.   
Notice every email speech act is itself an ac-
tivity.  The <verb><noun> node in Figure 1 indi-
cates that any email speech act can also serve as 
the noun associated with some other email 
speech act.  For example, just as (deliver infor-
mation) is a legitimate speech act, so is (commit 
(deliver information)). Automatically construct-
ing such nested speech acts is an interesting and 
difficult topic; however, in the current paper we 
consider only the problem of determining top-
level the verb for such compositional speech acts. 
For instance, for a message containing a (commit 
(deliver information)) our goal would be to 
automatically detect the commit verb but not the 
inner (deliver information) compound noun. 
4 Categorization Results 
4.1 Corpora 
Although email is ubiquitous, large and realis-
tic email corpora are rarely available for research 
purposes.  The limited availability is largely due 
to privacy issues: for instance, in most US aca-
demic institutions, a users? email can only be dis-
tributed to researchers if all senders of the email 
also provided explicit written consent. 
The email corpora used in our experiments 
consist of four different email datasets collected 
from working groups who signed agreements to 
make their email accessible to researchers. The 
first three datasets, N01F3, N02F2, and N03F2 
are annotated subsets of a larger corpus, the 
CSpace email corpus, which contains approxi-
mately 15,000 email messages collected from a 
management course at Carnegie Mellon Univer-
sity. In this course, 277 MBA students, organized 
in approximately 50 teams of four to six mem-
bers, ran simulated companies in different market 
scenarios over a 14-week period (Kraut et al). 
N02F2, N01F3 and N03F2 are collections of all 
email messages written by participants from three 
different teams, and contain 351, 341 and 443 
different email messages respectively.  
The fourth dataset, the PW CALO corpus, was 
generated during a four-day exercise conducted 
at SRI specifically to generate an email corpus. 
During this time a group of six people assumed 
different work roles (e.g. project leader, finance 
manager, researcher, administrative assistant, etc) 
and performed a number of group activities.  
There are 222 email messages in this corpus. 
These email corpora are all task-related, and 
associated with a small working group, so it is 
not surprising that they contain many instances of 
the email acts described above?for instance, the 
CSpace corpora contain an average of about 1.3 
email verbs per message. Informal analysis of 
other personal inboxes suggests that this sort of 
email is common for many university users. We 
believe that negotiation of shared tasks is a cen-
tral use of email in many work environments.  
All messages were preprocessed by removing 
quoted material, attachments, and non-subject 
header information.  This preprocessing was per-
formed manually, but was limited to operations 
which can be reliably automated. The most diffi-
cult step is removal of quoted material, which we 
address elsewhere (Carvalho & Cohen, 2004). 
4.2 Inter-Annotator Agreement  
Each message may be annotated with several 
labels, as it may contain several speech acts.   To 
evaluate inter-annotator agreement, we double-
labeled N03F2 for the verbs Deliver, Commit, 
Request, Amend, and Propose, and the noun, 
Meeting, and computed the kappa statistic (Car-
letta, 1996) for each of these, defined as 
R
RA
?
?
=
1
?
 
where A is the empirical probability of agreement 
on a category, and R is the probability of agree-
ment for two annotators that label documents at 
random (with the empirically observed frequency 
of each label). Hence kappa ranges from -1 to +1. 
The results in Table 1 show that agreement is 
good, but not perfect. 
 
Email Act Kappa 
Meeting 0.82 
Deliver 0.75 
Commit 0.72 
Request 0.81 
Amend 0.83 
Propose 0.72 
Table 1 - Inter-Annotator Agreement on N03F2. 
We also took doubly-annotated messages 
which had only a single verb label and con-
structed the 5-class confusion matrix for the two 
annotators shown in Table 2. Note kappa values 
are somewhat higher for the shorter one-act mes-
sages. 
 
            Req Prop Amd Cmt Dlv kappa 
Req 55 0 0 0 0 0.97 
Prop 1 11 0 0 1 0.77 
Amd 0 1 15 0 0 0.87 
Cmt 1 3 1 24 4 0.78 
Dlv 1 0 2 3 135 0.91 
Table 2 - Inter-annotator agreement on documents 
with only one category. 
4.3 Learnability of Categories 
Representation of documents. To assess the 
types of message features that are most important 
for prediction, we adopted Support Vector Ma-
chines (Joachims, 2001) as our baseline learning 
method, and a TFIDF-weighted bag-of-words as 
a baseline representation for messages.  We then 
conducted a series of experiments with the 
N03F2 corpus only to explore the effect of dif-
ferent representations.   
NF032 Cmt Dlv Directive 
Baseline SVM 25.0 49.8 75.2 
no tfidf  47.3 58.4 74.6 
+bigrams 46.1 66.1 76.0 
+times 43.6 60.1 73.2 
+POSTags 48.6 61.8 75.4 
+personPhrases 41.2 61.1 73.4 
 
NF02F2 and NF01F3 Cmt Dlv Directive 
Baseline SVM 10.1 56.3 66.1 
All ?useful? features 42.0 64.0 73.3 
Table 3 ? F1 for different feature sets. 
 
We noted that the most discriminating words 
for most of these categories were common words, 
not the low-to-intermediate frequency words that 
are most discriminative in topical classification. 
This suggested that the TFIDF weighting was 
inappropriate, but that a bigram representation 
might be more informative. Experiments showed 
that adding bigrams to an unweighted bag of 
words representation slightly improved perform-
ance, especially on Deliver. These results are 
shown in Table 4 on the rows marked ?no tfidf? 
and ?bigrams?. (The TFIDF-weighted SVM is 
shown in the row marked ?baseline?, and the ma-
jority classifier in the row marked ?default?; all 
numbers are F1 measures on 10-fold cross-
validation.) Examination of messages suggested 
other possible improvements. Since much nego-
tiation involves timing, we ran a hand-coded ex-
tractor for time and date expressions on the data, 
and extracted as features the number of time ex-
pressions in a message, and the words that oc-
curred near a time (for instance, one such feature 
is ?the word ?before? appears near a time?). 
These results appear in the row marked ?times?.  
Similarly, we ran a part of speech (POS) tagger 
and added features for words appearing near a 
pronoun or proper noun (?personPhrases? in the 
table), and also added POS counts. 
To derive a final representation for each cate-
gory, we pooled all features that improved per-
formance over ?no tfidf? for that category.  We 
then compared performance of these document 
representations to the original TFIDF bag of 
words baseline on the (unexamined) N02F2 and 
N01F3 corpora.  As Table 3 shows, substantial 
improvement with respect to F1 and kappa was 
obtained by adding these additional features over 
the baseline representation. This result contrasts 
with previous experiments with bigrams for topi-
cal text classification (Scott & Matwin, 1999)  
and sentiment detection (Pang et al, 2002).  The 
difference is probably that in this task, more in-
formative words are potentially ambiguous: for 
instance, ?will you? and ?I will? are correlated 
with requests and commitments, respectively, but 
the individual words in these bigrams are less 
predictive. 
Learning methods.  In another experiment, 
we fixed the document representation to be un-
weighted word frequency counts and varied the 
learning algorithm. In these experiments, we 
pooled all the data from the four corpora, a total 
of 9602 features in the 1357 messages, and since 
the nouns and verbs are not mutually exclusive, 
we formulated the task as a set of several binary 
classification problems, one for each verb. 
The following learners were used from the 
Based on the MinorThird toolkit (Cohen, 2004). 
VP is an implementation of the voted perceptron 
algorithm (Freund & Schapire, 1999). DT is a 
simple decision tree learning system, which 
learns trees of depth at most five, and chooses 
splits to maximize the function ( )00112
?+?+ + WWWW  suggested by Schapire and 
Singer (1999) as an appropriate objective for 
?weak learners?. AB is an implementation of the 
confidence-rated boosting method described by 
Singer and Schapire (1999), used to boost the DT 
algorithm 10 times.  SVM is a support vector ma-
chine with a linear kernel (as used above). 
 
Act 
 VP AB SVM  DT 
Request 
(450/907) 
Error 
F1 
0.25 
0.58 
0.22 
0.65 
0.23 
0.64 
0.20 
0.69 
Proposal 
(140/1217) 
Error 
F1 
0.11 
0.19 
0.12 
0.26 
0.12 
0.44 
0.10 
0.13 
Delivery 
(873/484) 
Error 
F1 
0.26 
0.80 
0.28 
0.78 
0.27 
0.78 
0.30 
0.76 
Commit-
ment 
(208/1149) 
Error 
F1 
0.15 
0.21 
0.14 
0.44 
0.17 
0.47 
0.15 
0.11 
Directive 
(605/752) 
Error 
F1 
0.25 
0.72 
0.23 
0.73 
0.23 
0.73 
0.19 
0.78 
Commis-
sive 
(993/364) 
Error 
F1 
0.23 
0.84 
0.23 
0.84 
0.24 
0.83 
0.22 
0.85 
Meet 
(345/1012) 
Error 
F1 
0.187 
0.573 
0.17 
0.62 
0.14 
0.72 
0.18
0.60 
Table 4 ? Learning on the entire corpus. 
Table 4 reports the results on the most common 
verbs, using 5-fold cross-validation to assess ac-
curacy. One surprise was that DT (which we had 
intended merely as a base learner for AB) works 
surprisingly well for several verbs, while AB sel-
dom improves much over DT.  We conjecture 
that the bias towards large-margin classifiers that 
is followed by SVM, AB, and VP (and which has 
been so successful in topic-oriented text classifi-
cation) may be less appropriate for this task, per-
haps because positive and negative classes are 
not clearly separated (as suggested by substantial 
inter-annotator disagreement). 
Class:
 Commisive
 (Total: 1357 msgs)
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
c
is
io
n Voted Perceptron
AdaBoost
SVM
Decision Tree
 
Figure 3 - Precision/Recall for Commissive act 
Further results are shown in Figure 3-5, which 
provide precision-recall curves for many of these 
classes. The lowest recall level in these graphs 
corresponds to the precision of random guessing. 
These graphs indicate that high-precision predic-
tions can be made for the top-level of the verb 
hierarchy, as well as verbs Request and Deliver, 
if one is willing to slightly reduce recall.  
Class:  Directive
(Total: 1357 msgs)
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
ci
s
io
n VotedPerceptron
AdaBoost
SVM
DecisionTree
 
Figure 4 - Precision/Recall for Directive act 
 
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
c
is
io
n
Meet
Dlv
Req
AdaBoost Learner
(Total: 1357
 
messages)
 
Figure 5 - Precision/Recall of 3 different classes 
using AdaBoost 
 
 
Transferability. One important question in-
volves the generality of these classifiers: to what 
range of corpora can they be accurately applied?  
Is it possible to train a single set of email-act 
classifiers that work for many users, or is it nec-
essary to train individual classifiers for each 
user? To explore this issue we trained a DT clas-
sifier for Directive emails on the NF01F3 corpus, 
and tested it on the NF02F2 corpus; trained the 
same classifier on NF02F2 and tested it on 
NF01F3; and also performed a 5-fold cross-
validation experiment within each corpus.   
(NF02F2 and NF01F3 are for disjoint sets of us-
ers, but are approximately the same size.)  We 
then performed the same experiment with VP for 
Deliver verbs and SVM for Commit verbs (in 
each case picking the top-performing learner with 
respect to F1).  The results are shown in Table 5. 
  
 Test Data 
DT/Directive 1f3 2f2 
Train Data Error F1 Error F1 
1f3 25.1 71.6 16.4 72.8 
2f2 20.1 68.8 18.8 71.2 
VP/Deliver  
1f3 30.1 55.1 21.1 56.1 
2f2 35.0 25.4 21.1 35.7 
SVM/Commit  
1f3 23.4 39.7 15.2 31.6 
2f2 31.9 27.3 16.4 15.1 
Table 5 - Transferability of classifiers 
 
If learned classifiers were highly specific to a 
particular set of users, one would expect that the 
diagonal entries of these tables (the ones based 
on cross-validation within a corpus) would ex-
hibit much better performance than the off-
diagonal entries.  In fact, no such pattern is 
shown. For Directive verbs, performance is simi-
lar across all table entries, and for Deliver and 
Commit, it seems to be somewhat better to train 
on NF01F3 regardless of the test set. 
4.4 Future Directions 
None of the algorithms or representations dis-
cussed above take into account the context of an 
email message, which intuitively is important in 
detecting implicit speech acts.  A plausible notion 
of context is simply the preceding message in an 
email thread. 
Exploiting this context is non-trivial for sev-
eral reasons.  Detecting threads is difficult; al-
though email headers contain a ?reply-to? field, 
users often use the ?reply? mechanism to start 
what is intuitively a new thread.  Also, since 
email is asynchronous, two or more users may 
reply simultaneously to a message, leading to a 
thread structure which is a tree, rather than a se-
quence.  Finally, most sequential learning models 
assume a single category is assigned to each in-
stance?e.g., (Ratnaparkhi, 1999)?whereas our 
scheme allows multiple categories. 
Classification of emails according to our verb-
noun ontology constitutes a special case of a gen-
eral family of learning problems we might call 
factored classification problems, as the classes 
(email speech acts) are factored into two features 
(verbs and nouns) which jointly determine this 
class. A variety of real-world text classification 
problems can be naturally expressed as factored 
problems, and from a theoretical viewpoint, the 
additional structure may allow construction of 
new, more effective algorithms.   
For example, the factored classes provide a 
more elaborate structure for generative probabil-
istic models, such as those assumed by Na?ve 
Bayes. For instance, in learning email acts, one 
might assume words were drawn from a mixture 
distribution with one mixture component pro-
duces words conditioned on the verb class factor, 
and a second mixture component generates words 
conditioned on the noun (see Blei et al(2003) for 
a related mixture model).  Alternatively, models 
of the dependencies between the different factors 
(nouns and verbs) might also be used to improve 
classification accuracy, for instance by building 
into a classifier the knowledge that some nouns 
and verbs are incompatible.  
The fact that an email can contain multiple 
email speech acts almost certainly makes learn-
ing more difficult: in fact, disagreement between 
human annotators is generally higher for longer 
messages.  This problem could be addressed by 
more detailed annotation: rather than annotating 
each message with all the acts it contains, human 
annotators could label smaller message segments 
(say, sentences or paragraphs). An alternative to 
more detailed (and expensive) annotation would 
be to use learning algorithms that implicitly seg-
ment a message. As an example, another mixture 
model formulation might be used, in which each 
mixture component corresponds to a single verb 
category.    
5 Concluding Remarks 
We have presented an ontology of ?email 
speech acts? that is designed to capture some im-
portant properties of a central use of email: nego-
tiating and coordinating joint activities. Unlike 
previous attempts to combine speech act theory 
with email (Winograd, 1987; Flores and Ludlow, 
1980), we propose a system which passively ob-
serves email and automatically classifies it by 
intention. This reduces the burden on the users of 
the system, and avoids sacrificing the flexibility 
and socially desirable aspects of informal, natural 
language communication. 
This problem also raises a number of interest-
ing research issues. We showed that entity ex-
traction and part of speech tagging improves 
classifier performance, but leave open the ques-
tion of whether other types of linguistic analysis 
would be useful. Predicting speech acts requires 
context, which makes classification an inherently 
sequential task, and the labels assigned to mes-
sages have non-trivial structure; we also leave 
open the question of whether these properties can 
be effectively exploited. 
  Our experiments show that many categories 
of messages can be detected, with high precision 
and moderate recall, using existing text-
classification learning methods. This suggests 
that useful task-tracking tools could be con-
structed based on automatic classifiers?a poten-
tially important practical application. 
References 
S. Argamon, M. ?ari? and S. S. Stein. (2003). Style 
mining of electronic messages for multiple authorship 
discrimination: first results. Proceedings of the 9th 
ACM SIGKDD, Washington, D.C. 
 
V. Bellotti, N. Ducheneaut, M. Howard and I. Smith. 
(2003). Taking email to task: the design and evalua-
tion of a task management centered email tool. Pro-
ceedings of the Conference on Human Factors in 
Computing Systems, Ft. Lauderdale, Florida.  
 
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. 
(2003).  Hierarchical topic models and the nested Chi-
nese restaurant process.  Advances in Neural Informa-
tion Processing Systems, 16, MIT Press. 
 
B. M. Camino, A. E. Millewski, D. R. Millen and T. 
M. Smith. (1998). Replying to email with structured 
responses. International Journal of Human-Computer 
Studies. Vol. 48, Issue 6, pp 763 ? 776. 
 
J. Carletta. (1996). Assessing Agreement on Classifi-
cation Tasks: The Kappa Statistic. Computational 
Linguistics, Vol. 22, No. 2, pp 249-254. 
 
V. R. Carvalho & W. W. Cohen (2004). Learning to 
Extract Signature and Reply Lines from Email.  To 
appear in Proc. of the 2004 Conference on Email and 
Anti-Spam. Mountain View, California. 
 
W. W. Cohen. (1996). Learning Rules that Classify E-
Mail. Proceedings of the 1996 AAAI Spring Sympo-
sium on Machine Learning and Information Access, 
Palo Alto, California. 
 
W. W. Cohen. (2004). Minorthird: Methods for Identi-
fying Names and Ontological Relations in Text using 
Heuristics for Inducing Regularities from Data, 
http://minorthird.sourceforge.net. 
 
M. Finke, M. Lapata, A. Lavie, L. Levin, L. May-
fieldTomokiyo, T. Polzin, K. Ries, A. Waibel and K. 
Zechner. (1998). CLARITY: Inferring Discourse 
Structure from Speech. In Applying Machine Learn-
ing to Discourse Processing, AAAI'98. 
 
F. Flores, and J.J. Ludlow. (1980). Doing and Speak-
ing in the Office. In: G. Fick, H. Spraque Jr. (Eds.). 
Decision Support Systems: Issues and Challenges, 
Pergamon Press, New York, pp. 95-118. 
 
Y. Freund and R. Schapire. (1999). Large Margin 
Classification using the Perceptron Algorithm.  Ma-
chine Learning 37(3), 277?296. 
 
T. Joachims. (2001). A Statistical Learning Model of 
Text Classification with Support Vector Machines. 
Proc. of the Conference on Research and Develop-
ment in Information Retrieval (SIGIR), ACM, 2001. 
 
R. E. Kraut, S. R. Fussell, F. J. Lerch, and J. A. 
Espinosa. (under review). Coordination in teams: evi-
dence from a simulated management game. To appear 
in the Journal of Organizational Behavior. 
 
D. D. Lewis. (1992). Representation and Learning in 
Information Retrieval. PhD Thesis, No. 91-93, Com-
puter Science Dept., Univ of Mass at Amherst 
 
A. McCallum, D. Freitag and F. Pereira. (2000). 
Maximum Entropy Markov Models for Information 
Extraction and Segmentation. Proc. of the 17th Int?l 
Conf. on Machine Learning, Nashville, TN. 
 
B. Pang, L. Lee and S. Vaithyanathan. (2002). 
Thumbs up? Sentiment Classification using Machine 
Learning Techniques. Proc. of the 2002 Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP), pp 79-86. 
 
A. E. Milewski and T. M. Smith. (1997). An Experi-
mental System For Transactional Messaging. Proc. of 
the international ACM  SIGGROUP conference on 
Supporting group work: the integration challenge, pp. 
325-330. 
 
H. Murakoshi, A. Shimazu and K. Ochimizu. (1999). 
Construction of Deliberation Structure in Email 
Communication,. Pacific Association for Computa-
tional Linguistics, pp. 16-28, Waterloo, Canada. 
 
A. Ratnaparkhi. (1999). Learning to Parse Natural 
Language with Maximum Entropy Models. Machine 
Learning, Vol. 34, pp. 151-175. 
  
J. D. M. Rennie. (2000). Ifile: An Application of Ma-
chine Learning to Mail Filtering. Proc. of the KDD-
2000 Workshop on Text Mining, Boston, MA. 
 
M. Sahami, S. Dumais, D. Heckerman and E. Horvitz. 
(1998). A Bayesian Approach to Filtering Junk E-
Mail. AAAI'98 Workshop on Learning for Text Cate-
gorization. Madison, WI. 
 
M. Schoop. (2001). An introduction to the language-
action perspective. SIGGROUP Bulletin, Vol. 22, No. 
2, pp 3-8. 
 
S. Scott and S. Matwin. (1999). Feature engineering 
for text classification. Proc. of 16th International Con-
ference on Machine Learning, Bled, Slovenia. 
 
J. R. Searle. (1975). A taxonomy of illocutionary acts.  
In K. Gunderson (Ed.), Language, Mind and Knowl-
edge, pp. 344-369.  Minneapolis: University of Min-
nesota Press. 
 
R. B. Segal and J. O. Kephart. (2000). Swiftfile: An 
intelligent assistant for organizing e-mail. In AAAI 
2000 Spring Symposium on Adaptive User Interfaces, 
Stanford, CA. 
 
Y. Yang. (1999). An Evaluation of Statistical Ap-
proaches to Text Categorization. Information Re-
trieval, Vol. 1, Numbers 1-2, pp 69-90. 
 
S. Wermter and M. L?chel. (1996). Learning dialog 
act processing. Proc. of the International Conference 
on Computational Linguistics, Kopenhagen, Denmark. 
 
R. E. Schapire and Y. Singer. (1998). Improved boost-
ing algorithms using confidence-rated predictions. The 
11th Annual Conference on Computational Learning 
Theory, Madison, WI. 
 
H. Wiegend, G. Goldkuhl, and A. de Moor. (2003). 
Proc. of the Eighth Annual Working Conference on 
Language-Action Perspective on Communication 
Modelling (LAP 2003), Tilburg, The Netherlands. 
 
J. Wiebe, R. Bruce, M. Bell, M. Martin and T. Wilson. 
(2001). A Corpus Study of Evaluative and Speculative 
Language.  Proceedings of the 2nd ACL SIGdial 
Workshop on Discourse and Dialogue. Aalborg, 
Denmark. 
 
T. Winograd. 1987. A Language/Action Perspective 
on the Design of Cooperative Work. Human-
Computer Interactions, 3:1, pp. 3-30. 
 
S. Whittaker, Q. Jones, B. Nardi, M. Creech, L. 
Terveen, E. Isaacs and J. Hainsworth. (in press). Us-
ing Personal Social Networks to Organize Communi-
cation in a Social desktop. To appear in Transactions 
on Human Computer Interaction. 
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Coupling Semi-Supervised Learning of Categories and Relations
Andrew Carlson1, Justin Betteridge1, Estevam R. Hruschka Jr.1,2 and Tom M. Mitchell1
1School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{acarlson,jbetter,tom.mitchell}@cs.cmu.edu
2Federal University of Sao Carlos
Sao Carlos, SP - Brazil
estevam@dc.ufscar.br
Abstract
We consider semi-supervised learning of
information extraction methods, especially
for extracting instances of noun categories
(e.g., ?athlete,? ?team?) and relations (e.g.,
?playsForTeam(athlete,team)?). Semi-
supervised approaches using a small number
of labeled examples together with many un-
labeled examples are often unreliable as they
frequently produce an internally consistent,
but nevertheless incorrect set of extractions.
We propose that this problem can be over-
come by simultaneously learning classifiers
for many different categories and relations
in the presence of an ontology defining
constraints that couple the training of these
classifiers. Experimental results show that
simultaneously learning a coupled collection
of classifiers for 30 categories and relations
results in much more accurate extractions
than training classifiers individually.
1 Introduction
A great wealth of knowledge is expressed on the web
in natural language. Translating this into a struc-
tured knowledge base containing facts about enti-
ties (e.g., ?Disney?) and relations between those en-
tities (e.g. CompanyIndustry(?Disney?, ?entertain-
ment?)) would be of great use to many applications.
Although fully supervised methods for learning to
extract such facts from text work well, the cost
of collecting many labeled examples of each type
of knowledge to be extracted is impractical. Re-
searchers have also explored semi-supervised learn-
ing methods that rely primarily on unlabeled data,
Figure 1: We show that significant improvements in ac-
curacy result from coupling the training of information
extractors for many inter-related categories and relations
(B), compared with the simpler but much more difficult
task of learning a single information extractor (A).
but these approaches tend to suffer from the fact that
they face an under-constrained learning task, result-
ing in extractions that are often inaccurate.
We present an approach to semi-supervised learn-
ing that yields more accurate results by coupling the
training of many information extractors. The intu-
ition behind our approach (summarized in Figure 1)
is that semi-supervised training of a single type of
extractor such as ?coach? is much more difficult than
simultaneously training many extractors that cover
a variety of inter-related entity and relation types.
In particular, prior knowledge about the relation-
ships between these different entities and relations
(e.g., that ?coach(x)? implies ?person(x)? and ?not
sport(x)?) allows unlabeled data to become a much
more useful constraint during training.
Although previous work has coupled the learning
of multiple categories, or used static category rec-
ognizers to check arguments for learned relation ex-
1
tractors, our work is the first we know of to couple
the simultaneous semi-supervised training of multi-
ple categories and relations. Our experiments show
that this coupling results in more accurate extrac-
tions. Based on our results reported here, we hy-
pothesize that significant accuracy improvements in
information extraction will be possible by coupling
the training of hundreds or thousands of extractors.
2 Problem Statement
It will be helpful to first explain our use of common
terms. An ontology is a collection of unary and bi-
nary predicates, also called categories and relations,
respectively.1 An instance of a category, or a cate-
gory instance, is a noun phrase; an instance of a rela-
tion, or a relation instance, is a pair of noun phrases.
Instances can be positive or negative with respect
to a specific predicate, meaning that the predicate
holds or does not hold for that particular instance.
A promoted instance is an instance which our algo-
rithm believes to be a positive instance of some pred-
icate. Also associated with both categories and rela-
tions are patterns: strings of tokens with placehold-
ers (e.g., ?game against arg1? and ?arg1 , head coach
of arg2?). A promoted pattern is a pattern believed
to be a high-probability indicator for some predicate.
The challenge addressed by this work is to learn
extractors to automatically populate the categories
and relations of a specified ontology with high-
confidence instances, starting from a few seed pos-
itive instances and patterns for each predicate and
a large corpus of sentences annotated with part-of-
speech (POS) tags. We focus on extracting facts that
are stated multiple times in the corpus, which we
can assess probabilistically using corpus statistics.
We do not resolve strings to real-world entities? the
problems of synonym resolution and disambiguation
of strings that can refer to multiple entities are left
for future work.
3 Related Work
Work on multitask learning has demonstrated that
supervised learning of multiple ?related? functions
together can yield higher accuracy than learning the
functions separately (Thrun, 1996; Caruana, 1997).
Semi-supervised multitask learning has been shown
1We do not consider predicates of higher arity in this work.
to increase accuracy when tasks are related, allow-
ing one to use a prior that encourages similar pa-
rameters (Liu et al, 2008). Our work also involves
semi-supervised training of multiple coupled func-
tions, but differs in that we assume explicit prior
knowledge of the precise way in which our multi-
ple functions are related (e.g., that the values of the
functions applied to the same input are mutually ex-
clusive, or that one implies the other).
In this paper, we focus on a ?bootstrapping?
method for semi-supervised learning. Bootstrap-
ping approaches start with a small number of la-
beled ?seed? examples, use those seed examples to
train an initial model, then use this model to la-
bel some of the unlabeled data. The model is
then retrained, using the original seed examples plus
the self-labeled examples. This process iterates,
gradually expanding the amount of labeled data.
Such approaches have shown promise in applica-
tions such as web page classification (Blum and
Mitchell, 1998), named entity classification (Collins
and Singer, 1999), parsing (McClosky et al, 2006),
and machine translation (Ueffing, 2006).
Bootstrapping approaches to information extrac-
tion can yield impressive results with little initial
human effort (Brin, 1998; Agichtein and Gravano,
2000; Ravichandran and Hovy, 2002; Pasca et al,
2006). However, after many iterations, they usu-
ally suffer from semantic drift, where errors in label-
ing accumulate and the learned concept ?drifts? from
what was intended (Curran et al, 2007). Coupling
the learning of predicates by using positive exam-
ples of one predicate as negative examples for oth-
ers has been shown to help limit this drift (Riloff and
Jones, 1999; Yangarber, 2003). Additionally, ensur-
ing that relation arguments are of certain, expected
types can help mitigate the promotion of incorrect
instances (Pas?ca et al, 2006; Rosenfeld and Feld-
man, 2007). Our work builds on these ideas to cou-
ple the simultaneous bootstrapped training of multi-
ple categories and multiple relations.
Our approach to information extraction is based
on using high precision contextual patterns (e.g., ?is
mayor of arg1? suggests that arg1 is a city). An early
pattern-based approach to information extraction ac-
quired ?is a? relations from text using generic con-
textual patterns (Hearst, 1992). This approach was
later scaled up to the web by Etzioni et al (2005).
2
Other research explores the task of ?open informa-
tion extraction?, where the predicates to be learned
are not specified in advance (Shinyama and Sekine,
2006; Banko et al, 2007), but emerge instead from
analysis of the data. In contrast, our approach re-
lies strongly on knowledge in the ontology about the
predicates to be learned, and relationships among
them, in order to achieve high accuracy.
Chang et al (2007) present a framework for
learning that optimizes the data likelihood plus
constraint-based penalty terms than capture prior
knowledge, and demonstrate it with semi-supervised
learning of segmentation models. Constraints that
capture domain knowledge guide bootstrap learn-
ing of a structured model by penalizing or disallow-
ing violations of those constraints. While similar in
spirit, our work differs in that we consider learning
many models, rather than one structured model, and
that we are consider a much larger scale application
in a different domain.
4 Approach
4.1 Coupling of Predicates
As mentioned above, our approach hinges on the no-
tion of coupling the learning of multiple functions
in order to constrain the semi-supervised learning
problem we face. Our system learns four different
types of functions. For each category c:
1. fc,inst : NP (C)? [0, 1]
2. fc,patt : PattC(C)? [0, 1]
and for each relation r:
1. fr,inst : NP (C)?NP (C)? [0, 1]
2. fr,patt : PattR(C)? [0, 1]
where C is the input corpus, NP (C) is the set of
valid noun phrases in C, PattC(C) is the set of valid
category patterns in C, and PattR(C) is the set of
valid relation patterns in C. ?Valid? noun phrases,
category patterns, and relation patterns are defined
in Section 4.2.2.
The learning of these functions is coupled in two
ways:
1. Sharing among same-arity predicates according
to logical relations
2. Relation argument type-checking
These methods of coupling are made possible by
prior knowledge in the input ontology, beyond the
lists of categories and relations mentioned above.
We provide general descriptions of these methods
of coupling in the next sections, while the details are
given in section 4.2.
4.1.1 Sharing among same-arity predicates
Each predicate P in the ontology has a list of other
same-arity predicates with which P is mutually
exclusive, where mutuallyExclusive(P, P ?) ?
(P (arg1) ? ?P ?(arg1)) ? (P ?(arg1) ?
?P (arg1)), and similarly for relations. These mu-
tually exclusive relationships are used to carry out
the following simple but crucial coupling: if predi-
cate A is mutually exclusive with predicate B, A?s
positive instances and patterns become negative in-
stances and negative patterns for B. For example,
if ?city?, having an instance ?Boston? and a pattern
?mayor of arg1?, is mutually exclusive with ?scien-
tist?, then ?Boston? and ?mayor of arg1? will become
a negative instance and a negative pattern respec-
tively for ?scientist.? Such negative instances and
patterns provide negative evidence to constrain the
bootstrapping process and forestall divergence.
Some categories are declared to be a subset of
one of the other categories being populated, where
subset(P, P ?) ? P (arg1) ? P ?(arg1), (e.g., ?ath-
lete? is a subset of ?person?). This prior knowledge
is used to share instances and patterns of the subcat-
egory (e.g., ?athlete?) as positive instances and pat-
terns for the super-category (e.g., ?person?).
4.1.2 Relation argument type-checking
The last type of prior knowledge we use to couple
the learning of functions is type checking informa-
tion which couples the learning of relations with cat-
egories. For example, the arguments of the ?ceoOf?
relation are declared to be of the categories ?person?
and ?company?. Our approach does not promote a
pair of noun phrases as an instance of a relation un-
less the two noun phrases are classified as belonging
to the correct argument types. Additionally, when a
relation instance is promoted, the arguments become
promoted instances of their respective categories.
4.2 Algorithm Description
In this section, we describe our algorithm, CBL
(Coupled Bootstrap Learner), in detail.
The inputs to CBL are a large corpus of POS-
tagged sentences and an initial ontology with pre-
3
Algorithm 1: CBL Algorithm
Input: An ontology O, and text corpus C
Output: Trusted instances/patterns for each
predicate
SHARE initial instances/patterns among
predicates;
for i = 1, 2, . . . ,? do
foreach predicate p ? O do
EXTRACT candidate instances/patterns;
FILTER candidates;
TRAIN instance/pattern classifiers;
ASSESS candidates using classifiers;
PROMOTE highest-confidence candidates;
end
SHARE promoted items among predicates;
end
defined categories, relations, mutually exclusive re-
lationships between same-arity predicates, subset re-
lationships between some categories, seed instances
for all predicates, and seed patterns for the cate-
gories. Categories in the input ontology also have
a flag indicating whether instances must be proper
nouns, common nouns, or whether they can be ei-
ther (e.g., instances of ?city? are proper nouns).
Algorithm 1 gives a summary of the CBL algo-
rithm. First, seed instances and patterns are shared
among predicates using the available mutual exclu-
sion, subset, and type-checking relations. Then,
for an indefinite number of iterations, CBL expands
the sets of promoted instances and patterns for each
predicate, as detailed below.
CBL was designed to allow learning many pred-
icates simultaneously from a large sample of text
from the web. In each iteration of the algorithm, the
information needed from the text corpus is gathered
in two passes through the corpus using the MapRe-
duce framework (Dean and Ghemawat, 2008). This
allows us to complete an iteration of the system in
1 hour using a corpus containing millions of web
pages (see Section 5.3 for details on the corpus).
4.2.1 Sharing
At the start of execution, seed instances and pat-
terns are shared among predicates according to the
mutual exclusion, subset, and type-checking con-
straints. Newly promoted instances and patterns are
shared at the end of each iteration.
4.2.2 Candidate Extraction
CBL finds new candidate instances by using
newly promoted patterns to extract the noun phrases
that co-occur with those patterns in the text corpus.
To keep the size of this set manageable, CBL lim-
its the number of new candidate instances for each
predicate to 1000 by selecting the ones that occur
with the most newly promoted patterns. An analo-
gous procedure is used to extract candidate patterns.
Candidate extraction is performed for all predicates
in a single pass through the corpus using the MapRe-
duce framework.
The candidate extraction procedure has defini-
tions for valid instances and patterns that limit ex-
traction to instances that look like noun phrases and
patterns that are likely to be informative. Here we
provide brief descriptions of those definitions.
Category Instances In the placeholder of a cate-
gory pattern, CBL looks for a noun phrase. It uses
part-of-speech tags to segment noun phrases, ignor-
ing determiners. Proper nouns containing prepo-
sitions are segmented using a reimplementation of
the Lex algorithm (Downey et al, 2007). Cate-
gory instances are only extracted if they obey the
proper/common noun specification of the category.
Category Patterns If a promoted category in-
stance is found in a sentence, CBL extracts the pre-
ceding words as a candidate pattern if they are verbs
followed by a sequence of adjectives, prepositions,
or determiners (e.g., ?being acquired by arg1?) or
nouns and adjectives followed by a sequence of ad-
jectives, prepositions, or determiners (e.g., ?former
CEO of arg1?).
CBL extracts the words following the instance as
a candidate pattern if they are verbs followed option-
ally by a noun phrase (e.g., ?arg1 broke the home run
record?), or verbs followed by a preposition (e.g.,
?arg1 said that?).
Relation Instances If a promoted relation pattern
(e.g., ?arg1 is mayor of arg2?) is found, a candi-
date relation instance is extracted if both placehold-
ers are valid noun phrases, and if they obey the
proper/common specifications for their categories.
Relation Patterns If both arguments from a pro-
moted relation instance are found in a sentence then
4
the intervening sequence of words is extracted as a
candidate relation pattern if it contains no more than
5 tokens, has a content word, has an uncapitalized
word, and has at least one non-noun.
4.2.3 Candidate Filtering
Candidate instances and patterns are filtered to
maintain high precision, and to avoid extremely spe-
cific patterns. An instance is only considered for as-
sessment if it co-occurs with at least two promoted
patterns in the text corpus, and if its co-occurrence
count with all promoted patterns is at least three
times greater than its co-occurrence count with neg-
ative patterns. Candidate patterns are filtered in the
same manner using instances.
All co-occurrence counts needed by the filtering
step are obtained with an additional pass through
the corpus using MapReduce. This implementa-
tion is much more efficient than one that relies on
web search queries. CBL typically requires co-
occurrence counts of at least 10,000 instances with
any of at least 10,000 patterns, which would require
100 million hit count queries.
4.2.4 Candidate Assessment
Next, for each predicate CBL trains a discretized
Na??ve Bayes classifier to classify the candidate in-
stances. Its features include pointwise mutual infor-
mation (PMI) scores (Turney, 2001) of the candidate
instance with each of the positive and negative pat-
terns associated with the class. The current sets of
promoted and negative instances are used as training
examples for the classifier. Attributes are discretized
based on information gain (Fayyad and Irani, 1993).
Patterns are assessed using an estimate of the pre-
cision of each pattern p:
Precision(p) =
?
i?I count(i, p)
count(p)
where I is the set of promoted instances for the
predicate currently being considered, count(i, p) is
the co-occurrence count of instance i with pattern p,
and count(p) is the hit count of the pattern p. This
is a pessimistic estimate because it assumes that the
rest of the occurrences of pattern p are not with pos-
itive examples of the predicate. We also penalize
extremely rare patterns by thresholding the denomi-
nator using the 25th percentile candidate pattern hit
count (McDowell and Cafarella, 2006).
All of the co-occurrence counts needed for the as-
sessment step are collected in the same MapReduce
pass as those required for filtering candidates.
4.2.5 Candidate Promotion
CBL then ranks the candidates according to their
assessment scores and promotes at most 100 in-
stances and 5 patterns for each predicate.
5 Experimental Evaluation
We designed our experimental evaluation to try to
answer the following questions: Can CBL iterate
many times and still achieve high precision? How
helpful are the types of coupling that we employ?
Can we extend existing semantic resources?
5.1 Configurations of the Algorithm
We ran our algorithm in three configurations:
? Full: The algorithm as described in Section 4.2.
? No Sharing Among Same-Arity Predicates (NS):
This configuration couples predicates only us-
ing type-checking constraints. It uses the full
algorithm, except that predicates of the same ar-
ity do not share promoted instances and patterns
with each other. Seed instances and patterns are
shared, though, so each predicate has a small,
fixed pool of negative evidence.
? No Category/Relation coupling (NCR): This
configuration couples predicates using mutual
exclusion and subset constraints, but not type-
checking. It uses the full algorithm, except
that relation instance arguments are not fil-
tered or assessed using their specified categories,
and arguments of promoted relations are not
shared as promoted instances of categories. The
only type-checking information used is the com-
mon/proper noun specifications of arguments for
filtering out implausible instances.
5.2 Initial ontology
Our ontology contained categories and relations re-
lated to two domains: companies and sports. Ex-
tra categories were added to provide negative evi-
dence to the domain-related categories: ?hobby? for
?economic sector?; ?actor,? ?politician,? and ?scien-
tist? for ?athlete? and ?coach?; and ?board game? for
?sport?. Table 1 lists each predicate in the leftmost
column. Categories were started with 10?20 seed
5
5 iterations 10 iterations 15 iterations
Predicate Full NS NCR Full NS NCR Full NS NCR
Actor 93 100 100 93 97 100 100 97 100
Athlete 100 100 100 100 93 100 100 73 100
Board Game 93 76 93 89 27 93 89 30 93
City 100 100 100 100 97 100 100 100 100
Coach 100 63 73 97 53 43 97 47 47
Company 100 100 100 97 90 97 100 90 100
Country 60 40 60 30 43 27 40 23 40
Economic Sector 77 63 73 57 67 67 50 63 40
Hobby 67 63 67 40 40 57 20 23 30
Person 97 97 90 97 93 97 93 97 93
Politician 93 93 97 73 53 90 90 53 87
Product 97 87 90 90 87 100 97 90 77
Product Type 93 93 90 70 73 97 77 80 67
Scientist 100 90 97 97 63 97 93 60 100
Sport 100 90 100 93 67 83 97 27 90
Sports Team 100 97 100 97 70 100 90 50 100
Category Average 92 84 89 82 70 84 83 63 79
Acquired(Company, Company) 77 77 80 67 80 47 70 63 47
CeoOf(Person, Company) 97 87 100 90 87 97 90 80 83
CoachesTeam(Coach, Sports Team) 100 100 100 100 100 97 100 100 90
CompetesIn(Company, Econ. Sector) 97 97 80 100 93 67 97 63 60
CompetesWith(Company, Company) 93 80 60 77 70 37 70 60 43
HasOfficesIn(Company, City) 97 93 40 93 90 27 93 57 30
HasOperationsIn(Company, Country) 100 95 50 100 97 40 90 83 13
HeadquarteredIn(Company, City) 77 90 20 70 77 27 70 60 7
LocatedIn(City, Country) 90 67 57 63 50 43 73 50 30
PlaysFor(Athlete, Sports Team) 100 100 0 100 97 7 100 43 0
PlaysSport(Athlete, Sport) 100 100 27 93 80 10 100 40 30
TeamPlaysSport(Sports Team, Sport) 100 100 77 100 97 80 93 83 67
Produces(Company, Product) 91 83 90 83 93 67 93 80 57
HasType(Product, Product Type) 73 63 17 33 67 33 40 57 27
Relation Average 92 88 57 84 84 48 84 66 42
All 92 86 74 83 76 68 84 64 62
Table 1: Precision (%) for each predicate. Results are presented after 5, 10, and 15 iterations, for the Full, No Sharing
(NS), and No Category/Relation Coupling (NCR) configurations of CBL . Note that we expect Full and NCR to
perform similarly for categories, but for Full to outperform NCR on relations and for Full to outperform NS on both
categories and relations.
6
instances and 5 seed patterns. The seed instances
were specified by a human, and the seed patterns
were derived from the generic patterns of Hearst
for each predicate (Hearst, 1992). Relations were
started with similar numbers of seed instances, and
no seed patterns (it is less obvious how to gener-
ate good seed patterns from relation names). Most
predicates were declared as mutually exclusive with
most others, except for special cases (e.g., ?hobby?
and ?sport?; ?university? and ?sports team?; and ?has
offices in? and ?headquartered in?).
5.3 Corpus
Our text corpus was from a 200-million page web
crawl. We parsed the HTML, filtered out non-
English pages using a stop word ratio threshold, then
filtered out web spam and adult content using a ?bad
word? list. The pages were then segmented into sen-
tences, tokenized, and tagged with parts-of-speech
using the OpenNLP package. Finally, we filtered
the sentences to eliminate those that were likely to
be noisy and not useful for learning (e.g., sentences
without a verb, without any lowercase words, with
too many words that were all capital letters). This
yielded a corpus of roughly 514-million sentences.
5.4 Experimental Procedure
We ran each configuration for 15 iterations. To eval-
uate the precision of promoted instances, we sam-
pled 30 instances from the promoted set for each
predicate in each configuration after 5, 10, and 15 it-
erations, pooled together the samples for each pred-
icate, and then judged their correctness. The judge
did not know which run an instance was sampled
from. We estimated the precision of the promoted
instances from each run after 5, 10, and 15 itera-
tions as the number of correct promoted instances
divided by the number sampled. While samples of
30 instances do not produce tight confidence inter-
vals around individual estimates, they are sufficient
for testing for the effects in which we are interested.
5.5 Results
Table 1 shows the precision of each of the three al-
gorithm configurations for each category and rela-
tion after 5, 10, and 15 iterations. As is apparent
in this table, fully coupled training (Full) outper-
forms training when coupling is removed between
categories and relations (NCR), and also when cou-
pling is removed among predicates of the same ar-
ity (NS). The net effect is substantial, as is appar-
ent from the bottom row of Table 1, which shows
that the precision of Full outperforms NS by 6% and
NCR by 18% after the first 5 iterations, and by an
even larger 20% and 22% after 15 iterations. This
increasing gap in precision as iterations increase re-
flects the ability of coupled learning to constrain the
system to reduce the otherwise common drift asso-
ciated with self-trained classifiers.
Using Student?s paired t-test, we found that for
categories, the difference in performance between
Full and NS is statistically significant after 5, 10,
and 15 iterations (p-value < 0.05).2 No significant
difference was found between Full and NCR for cat-
egories, but this is not a surprise, because NCR still
uses mutually exclusive and subset constraints. The
same test finds that the differences between Full and
NS are significant for relations after 15 iterations,
and the differences between Full and NCR are sig-
nificant after 5, 10, and 15 iterations for relations.
The worst-performing categories after 15 itera-
tions of Full are ?country,? ?economic sector,? and
?hobby.? The Full configuration of CBL promoted
1637 instances for ?country,? far more than the num-
ber of correct answers. Many of these are general
geographic regions like ?Bayfield Peninsula? and
?Baltic Republics.? In the ?hobby? case, promoting
patterns like ?the types of arg1? led to the category
drifting into a general list of plural common nouns.
?Economic sector? drifted into academic fields like
?Behavioral Science? and ?Political Sciences.? We
expect that the learning of these categories would
be significantly better if there were even more cat-
egories being learned to provide additional negative
evidence during the filtering and assessment steps of
the algorithm.
At this stage of development, obtaining high re-
call is not a priority because our intent is to create
a continuously running and continuously improving
system; it is our hope that high recall will come with
time. However, to very roughly convey the com-
pleteness of the current results we show in Table 2
the average number of instances promoted for cate-
2Our selection of the paired t-test was motivated by the work
of Smucker et al (2007), but the Wilcoxon signed rank test
gives the same results.
7
Categories Relations
Configuration Instances Prec. Instances Prec.
Full 970 83 191 84
NS 1337 63 307 66
NCR 916 79 458 42
Table 2: Average numbers of promoted category and re-
lation instances and estimates of their precision for each
configuration of CBL after 15 iterations.
Figure 2: Extracted facts for two companies discovered
by CBL Full. These two companies were extracted by
the learned ?company? extractor, and the relations shown
were extracted by learned relation extractors.
gories and relations for each of the three configura-
tions of CBL after 15 iterations. For categories, not
sharing examples results in fewer negative examples
during the filtering and assessment steps. This yields
more promoted instances on average. For relations,
not using type checking yields higher relative recall,
but at a much lower level of precision.
Figure 2 gives one view of the type of information
extracted by the collection of learned category and
relation classifiers. Note the initial seed examples
provided to CBL did not include information about
either company or any of these relation instances.3
5.6 Comparison to an Existing Database
To estimate the capacity of our algorithm to con-
tribute additional facts to publicly available seman-
tic resources, we compared the complete lists of in-
stances promoted during the Full 15 iteration run
for certain categories to corresponding lists in the
Freebase database (Metaweb Technologies, 2009).
Excluding the categories that did not have a di-
rectly corresponding Freebase list, we computed for
each category: Precision ? |CBLInstances| ?
|Matches|, where Precision is the estimated pre-
cision from our random sample of 30 instances,
|CBLInstances| is the total number of instances
promoted for that category, and |Matches| is the
3See http://rtw.ml.cmu.edu/sslnlp09 for re-
sults from a full run of the system.
Est. CBL Freebase Est. New
Category Prec. Instances Matches Instances
Actor 100 522 465 57
Athlete 100 117 54 63
Board Game 89 18 6 10
City 100 1799 1665 134
Company 100 1937 995 942
Econ. Sector 50 1541 137 634
Politician 90 962 74 792
Product 97 1259 0 1221
Sports Team 90 414 139 234
Sport 97 613 134 461
Table 3: Estimated numbers of ?new instances? (correct
instances promoted by CBL in the Full 15 iteration run
which do not have a match in Freebase) and the values
used in calculating them.
number of promoted instances that had an exact
match in Freebase. While exact matches may under-
estimate the number of matches, it should be noted
that rather than make definitive claims, our intent
here is simply to give rough estimates, which are
shown in Table 3. These approximate numbers in-
dicate a potential to use CBL to extend existing se-
mantic resources like Freebase.
6 Conclusion
We have presented a method of coupling the semi-
supervised learning of categories and relations and
demonstrated empirically that the coupling forestalls
the problem of semantic drift associated with boot-
strap learning methods. We suspect that learning
additional predicates simultaneously will yield even
more accurate learning. An approximate compari-
son with an existing repository of semantic knowl-
edge, Freebase, suggests that our methods can con-
tribute new facts to existing resources.
Acknowledgments
This work is supported in part by DARPA, Google,
a Yahoo! Fellowship to Andrew Carlson, and the
Brazilian research agency CNPq. We also gratefully
acknowledge Jamie Callan for making available his
collection of web pages, Yahoo! for use of their M45
computing cluster, and the anonymous reviewers for
their comments.
8
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In JCDL.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In WebDB Workshop at
6th International Conference on Extending Database
Technology.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41?75.
Ming-Wei Chang, Lev-Arie Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In ACL.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In EMNLP.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In PACLING.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating complex named entities in web text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-
interval discretization of continuous-valued attributes
for classification learning. In UAI.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING.
Qiuhua Liu, Xuejun Liao, and Lawrence Carin. 2008.
Semi-supervised multitask learning. In NIPS.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In NAACL.
Luke K. McDowell and Michael Cafarella. 2006.
Ontology-driven information extraction with on-
tosyphon. In ISWC.
Metaweb Technologies. 2009. Freebase data dumps.
http://download.freebase.com/datadumps/.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and similarities on
the web: fact extraction in the fast lane. In ACL.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and search-
ing the world wide web of facts - step one: The one-
million fact extraction challenge. In AAAI.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In ACL.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In AAAI.
Benjamin Rosenfeld and Ronen Feldman. 2007. Us-
ing corpus statistics on entities to improve semi-
supervised relation extraction from the web. In ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In HLT-NAACL.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests for
information retrieval evaluation. In CIKM.
Sebastian Thrun. 1996. Is learning the n-th thing any
easier than learning the first? In NIPS.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In EMCL.
Nicola Ueffing. 2006. Self-training for machine trans-
lation. In NIPS workshop on Machine Learning for
Multilingual Information Access.
Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In ACL.
9
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 529?539,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Random Walk Inference and Learning in A Large Scale Knowledge Base
Ni Lao
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
nlao@cs.cmu.edu
Tom Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cs.cmu.edu
William W. Cohen
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
wcohen@cs.cmu.edu
Abstract
We consider the problem of performing learn-
ing and inference in a large scale knowledge
base containing imperfect knowledge with
incomplete coverage. We show that a soft
inference procedure based on a combination
of constrained, weighted, random walks
through the knowledge base graph can be
used to reliably infer new beliefs for the
knowledge base. More specifically, we
show that the system can learn to infer
different target relations by tuning the weights
associated with random walks that follow
different paths through the graph, using a
version of the Path Ranking Algorithm (Lao
and Cohen, 2010b). We apply this approach to
a knowledge base of approximately 500,000
beliefs extracted imperfectly from the web
by NELL, a never-ending language learner
(Carlson et al, 2010). This new system
improves significantly over NELL?s earlier
Horn-clause learning and inference method:
it obtains nearly double the precision at rank
100, and the new learning method is also
applicable to many more inference tasks.
1 Introduction
Although there is a great deal of recent research
on extracting knowledge from text (Agichtein and
Gravano, 2000; Etzioni et al, 2005; Snow et
al., 2006; Pantel and Pennacchiotti, 2006; Banko
et al, 2007; Yates et al, 2007), much less
progress has been made on the problem of drawing
reliable inferences from this imperfectly extracted
knowledge. In particular, traditional logical
inference methods are too brittle to be used to make
complex inferences from automatically-extracted
knowledge, and probabilistic inference methods
(Richardson and Domingos, 2006) suffer from
scalability problems. This paper considers the
problem of constructing inference methods that can
scale to large knowledge bases (KB?s), and that are
robust to imperfect knowledge. The KB we consider
is a large triple store, which can be represented as a
labeled, directed graph in which each entity a is a
node, each binary relation R(a, b) is an edge labeled
R between a and b, and unary concepts C(a) are
represented as an edge labeled ?isa? between the
node for the entity a and a node for the concept
C. We present a trainable inference method that
learns to infer relations by combining the results of
different random walks through this graph, and show
that the method achieves good scaling properties and
robust inference in a KB containing over 500,000
triples extracted from the web by the NELL system
(Carlson et al, 2010).
1.1 The NELL Case Study
To evaluate our approach experimentally, we study
it in the context of the NELL (Never Ending
Language Learning) research project, which is an
effort to develop a never-ending learning system that
operates 24 hours per day, for years, to continuously
improve its ability to read (extract structured facts
from) the web (Carlson et al, 2010). NELL began
operation in January 2010. As of March 2011,
NELL had built a knowledge base containing several
million candidate beliefs which it had extracted from
the web with varying confidence. Among these,
529
NELL had fairly high confidence in approximately
half a million, which we refer to as NELL?s
(confident) beliefs. NELL had lower confidence in a
few million others, which we refer to as its candidate
beliefs.
NELL is given as input an ontology that defines
hundreds of categories (e.g., person, beverage,
athlete, sport) and two-place typed relations among
these categories (e.g., atheletePlaysSport(?athlete?,
?sport?)), which it must learn to extract from the
web. It is also provided a set of 10 to 20 positive
seed examples of each such category and relation,
along with a downloaded collection of 500 million
web pages from the ClueWeb2009 corpus (Callan
and Hoy, 2009) as unlabeled data, and access to
100,000 queries each day to Google?s search engine.
Each day, NELL has two tasks: (1) to extract
additional beliefs from the web to populate its
growing knowledge base (KB) with instances of the
categories and relations in its ontology, and (2) to
learn to perform task 1 better today than it could
yesterday. We can measure its learning competence
by allowing it to consider the same text documents
today as it did yesterday, and recording whether it
extracts more beliefs, more accurately today.1
NELL uses a large-scale semi-supervised multi-
task learning algorithm that couples the training
of over 1500 different classifiers and extraction
methods (see (Carlson et al, 2010)). Although
many of the details of NELL?s learning method
are not central to this paper, two points should
be noted. First, NELL is a multistrategy learning
system, with components that learn from different
?views? of the data (Blum and Mitchell, 1998): for
instance, one view uses orthographic features of
a potential entity name (like ?contains capitalized
words?), and another uses free-text contexts in
which the noun phrase is found (e.g., ?X frequently
follows the bigram ?mayor of? ?). Second, NELL
is a bootstrapping system, which self-trains on its
growing collection of confident beliefs.
1.2 Knowledge Base Inference: Horn Clauses
Although NELL has now grown a sizable knowl-
edge base, its ability to perform inference over this
1NELL?s current KB is available online at
http://rtw.ml.cmu.edu.
Eli Manning Giants
AthletePlays
ForTeam
HinesWard Steelers
AthletePlays
ForTeam NFL
TeamPlays
InLeague
MLBTeamPlays
InLeague
TeamPlays
InLeague
Figure 1: An example subgraph.
knowledge base is currently very limited. At present
its only inference method beyond simple inheritance
involves applying first order Horn clause rules to
infer new beliefs from current beliefs. For example,
it may use a Horn clause such as
AthletePlaysForTeam(a, b) (1)
? TeamPlaysInLeague(b, c)
? AthletePlaysInLeague(a,c)
to infer that AthletePlaysInLeague(HinesWard,NFL),
if it has already extracted the beliefs in the
preconditions of the rule, with variables a, b and c
bound to HinesWard, PittsburghSteelers and NFL
respectively as shown in Figure 1. NELL currently
has a set of approximately 600 such rules, which
it has learned by data mining its knowledge base
of beliefs. Each learned rule carries a conditional
probability that its conclusion will hold, given that
its preconditions are satisfied.
NELL learns these Horn clause rules using
a variant of the FOIL algorithm (Quinlan and
Cameron-Jones, 1993), henceforth N-FOIL.
N-FOIL takes as input a set of positive and
negative examples of a rule?s consequent
(e.g., +AthletePlaysInLeague(HinesWard,NFL),
?AthletePlaysInLeague(HinesWard,NBA)), and
uses a ?separate-and-conquer? strategy to learn a
set of Horn clauses that fit the data well. Each
Horn clause is learned by starting with a general
rule and progressively specializing it, so that it
still covers many positive examples but covers few
negative examples. After a clause is learned, the
examples covered by that clause are removed from
the training set, and the process repeats until no
positive examples remain.
Learning first-order Horn clauses is computation-
ally expensive?not only is the search space large,
but some Horn clauses can be costly to evaluate
(Cohen and Page, 1995). N-FOIL uses two tricks
to improve its scalability. First, it assumes that
the consequent predicate is functional?e.g., that
530
each Athlete plays in at most one League. This
means that explicit negative examples need not
be provided (Zelle et al, 1995): e.g., if Ath-
letePlaysInLeague(HinesWard,NFL) is a positive
example, then AthletePlaysInLeague(HinesWard,c?)
for any other value of c? is negative. In general,
this constraint guides the search algorithm toward
Horn clauses that have fewer possible instantiations,
and hence are less expensive to match. Second,
N-FOIL uses ?relational pathfinding? (Richards
and Mooney, 1992) to produce general rules?i.e.,
the starting point for a predicate R is found
by looking at positive instances R(a, b) of the
consequent, and finding a clause that corresponds
to a bounded-length path of binary relations that
link a to b. In the example above, a start clause
might be the clause (1). As in FOIL, the clause
is then (potentially) specialized by greedily adding
additional conditions (like ProfessionalAthlete(a))
or by replacing variables with constants (eg,
replacing c with NFL).
For each N-FOIL rule, an estimated conditional
probability P? (conclusion|preconditions) is calcu-
lated using a Dirichlet prior according to
P? = (N+ +m ? prior)/(N+ +N? +m) (2)
where N+ is the number of positive instances
matched by this rule in the FOIL training data,
N? is the number of negative instances matched,
m = 5 and prior = 0.5. As the results below
show, N-FOIL generally learns a small number of
high-precision inference rules. One important role
of these inference rules is that they contribute to
the bootstrapping procedure, as inferences made by
N-FOIL increase either the number of candidate
beliefs, or (if the inference is already a candidate)
improve NELL?s confidence in candidate beliefs.
1.3 Knowledge Base Inference: Graph
Random Walks
In this paper, we consider an alternative approach,
based on the Path Ranking Algorithm (PRA) of Lao
and Cohen (2010b), described in detail below. PRA
learns to rank graph nodes b relative to a query
node a. PRA begins by enumerating a large set of
bounded-length edge-labeled path types, similar to
the initial clauses used in NELL?s variant of FOIL.
These path types are treated as ranking ?experts?,
each performing a random walk through the graph,
constrained to follow that sequence of edge types,
and ranking nodes b by their weights in the resulting
distribution. Finally, PRA combines the weights
contributed by different ?experts? using logistic
regression to predict the probability that the relation
R(a, b) is satisfied.
As an example, consider a path from a to b via
the sequence of edge types isa, isa?1 (the inverse of
isa), and AthletePlaysInLeague, which corresponds
to the Horn clause
isa(a, c) ? isa?1(c, a?) (3)
? AthletePlaysInLeague(a?, b)
? AthletePlaysInLeague(a, b)
Suppose a random walk starts at a query node a
(say a=HinesWard). If HinesWard is linked to the
single concept node ProfessionalAthlete via isa, the
walk will reach that node with probability 1 after
one step. If A is the set of ProfessionalAthlete?s
in the KB, then after two steps, the walk will have
probability 1/|A| of being at any a? ? A. If L is
the set of athletic leagues and ` ? L, let A` be the
set of athletes in league `: after three steps, the walk
will have probability |A`|/|A| of being at any point
b ? L. In short, the ranking associated with this
path gives the prior probability of a value b being an
athletic league for a?which is useful as a feature in
a combined ranking method, although not by itself a
high-precision inference rule.
Note that the rankings produced by this ?expert?
will change as the knowledge base evolves?for
instance, if the system learns about proportionally
more soccer players than hockey players over time,
then the league rankings for the path of clause (3)
will change. Also, the ranking is specific to the
query node a. For instance, suppose the KB contains
facts which reflect the ambiguity of the team name
?Giants?2 as in Figure 1. Then the path for clause (1)
above will give lower weight to b = NFL for a =
EliManning than to b = NFL for a = HinesWard.
The main contribution of this paper is to introduce
and evaluate PRA as an algorithm for making
probabilistic inference in large KBs. Compared to
Horn clause inference, the key characteristics of this
new inference method are as follows:
2San Francisco?s Major-League Baseball and New York?s
National Football League teams are both called the ?Giants?.
531
? The evidence in support of inferring a relation
instance R(a, b) is based on many existing
paths between a and b in the current KB,
combined using a learned logistic function.
? The confidence in an inference is sensitive to
the current state of the knowledge base, and the
specific entities being queried (since the paths
used in the inference have these properties).
? Experimentally, the inference method yields
many more moderately-confident inferences
than the Horn clauses learned by N-FOIL.
? The learning and inference are more efficient
than N-FOIL, in part because we can exploit
efficient approximation schemes for random
walks (Lao and Cohen, 2010a). The resulting
inference is as fast as 10 milliseconds per query
on average.
The Path Ranking Algorithm (PRA) we use is
similar to that described elsewhere (Lao and Cohen,
2010b), except that to achieve efficient model
learning, the paths between a and b are determined
by the statistics from a population of training
queries rather than enumerated completely. PRA
uses random walks to generate relational features
on graph data, and combine them with a logistic
regression model. Compared to other relational
models (e.g. FOIL, Markov Logic Networks), PRA
is extremely efficient at link prediction or retrieval
tasks, in which we are interested in identifying top
links from a large number of candidates, instead of
focusing on a particular node pair or joint inferences.
1.4 Related Work
The TextRunner system (Cafarella et al, 2006)
answers list queries on a large knowledge base
produced by open domain information extrac-
tion. Spreading activation is used to measure
the closeness of any node to the query term
nodes. This approach is similar to the random
walk with restart approach which is used as a
baseline in our experiment. The FactRank system
(Jain and Pantel, 2010) compares different ways of
constructing random walks, and combining them
with extraction scores. However, the shortcoming
of both approaches is that they ignore edge type
information, which is important for achieving high
accuracy predictions.
The HOLMES system (Schoenmackers et al,
2008) derives new assertions using a few manually
written inference rules. A Markov network
corresponding to the grounding of these rules to
the knowledge base is constructed for each query,
and then belief propagation is used for inference.
In comparison, our proposed approach discovers
inference rules automatically from training data.
Similarly, the Markov Logic Networks (Richard-
son and Domingos, 2006) are Markov networks
constructed corresponding to the grounding of rules
to knowledge bases. In comparison, our proposed
approach is much more efficient by avoiding the
harder problem of joint inferences and by leveraging
efficient random walk schemes (Lao and Cohen,
2010a).
Below we describe our approach in greater detail,
provide experimental evidence of its value for
performing inference in NELL?s knowledge base,
and discuss implications of this work and directions
for future research.
2 Approach
In this section, we first describe how we formulate
link (relation) prediction on a knowledge base as
a ranking task. Then we review the Path Ranking
Algorithm (PRA) introduced by Lao and Cohen
(2010b; 2010a). After that, we describe two
improvements to the PRA method to make it more
suitable for the task of link prediction in knowledge
bases. The first improvement helps PRA deal
with the large number of relations typical of large
knowledge bases. The second improvement aims at
improving the quality of inference by applying low
variance sampling.
2.1 Learning with NELL?s Knowledge Base
For each relationR in the knowledge base we train a
model for the link prediction task: given a concept a,
find all other concepts b which potentially have the
relationR(a, b). This prediction is made based on an
existing knowledge base extracted imperfectly from
the web. Although a model can potentially benefit
from predicting multiple relations jointly, such joint
inference is beyond the scope of this work.
532
To ensure a reasonable number of training
instances, we generate labeled training example
queries from 48 relations which have more than
100 instances in the knowledge base. We create
two tasks for each relation?i.e., predicting b given
a and predicting a given b? yielding 96 tasks in
all. Each node a which has relation R in the
knowledge base with any other node is treated as a
training query, the actual nodes b in the knowledge
base known to satisfy R(a, b) are treated as labeled
positive examples, and any other nodes are treated
as negative examples.
2.2 Path Ranking Algorithm Review
We now review the Path Ranking Algorithm
introduced by Lao and Cohen (2010b). A relation
path P is defined as a sequence of relations
R1 . . . R`, and in order to emphasize the types
associated with each step, P can also be written as
T0 R1??? . . . R`?? T`, where Ti = range(Ri) =
domain(Ri+1), and we also define domain(P ) ?
T0, range(P ) ? T`. In the experiments in this
paper, there is only one type of node which we call
a concept, which can be connected through different
types of relations. In this notation, relations like ?the
team a certain player plays for?, and ?the league a
certain player?s team is in? can be expressed by the
paths below (respectively):
P1 : concept
AtheletePlayesForTeam??????????????? concept
P2 : concept
AtheletePlayesForTeam??????????????? concept
TeamPlaysInLeagure?????????????? concept
For any relation path P = R1 . . . R` and a
seed node s ? domain(P ), a path constrained
random walk defines a distribution hs,P recursively
as follows. If P is the empty path, then define
hs,P (e) =
{
1, if e = s
0, otherwise (4)
If P = R1 . . . R` is nonempty, then let P ? =
R1 . . . R`?1, and define
hs,P (e) =
?
e??range(P ?)
hs,P ?(e?) ? P (e|e?;R`), (5)
where P (e|e?;R`) = R`(e?,e)|R`(e?,?)| is the probability ofreaching node e from node e? with a one step random
walk with edge type R`. R(e?, e) indicates whether
there exists an edge with type R that connect e? to e.
More generally, given a set of paths P1, . . . , Pn,
one could treat each hs,Pi(e) as a path feature for
the node e, and rank nodes by a linear model
?1hs,P1(e) + ?2hs,P2(e) + . . . ?nhs,Pn(e)
where ?i are appropriate weights for the paths. This
gives a ranking of nodes e related to the query node
s by the following scoring function
score(e; s) =
?
P?P`
hs,P (e)?P , (6)
where P` is the set of relation paths with length? `.
Given a relation R and a set of node pairs
{(si, ti)} for which we know whether R(si, ti) is
true or not, we can construct a training dataset
D = {(xi, yi)}, where xi is a vector of all the
path features for the pair (si, ti)?i.e., the j-th
component of xi is hsi,Pj (ti), and where yi is a
boolean variable indicating whetherR(si, ti) is true.
We then train a logistic function to predict the
conditional probability P (y|x; ?). The parameter
vector ? is estimated by maximizing a regularized
form of the conditional likelihood of y given x. In
particular, we maximize the objective function
O(?) =
?
i
oi(?)? ?1|?|1 ? ?2|?|2, (7)
where ?1 controls L1-regularization to help struc-
ture selection, and ?2 controls L2-regularization
to prevent overfitting. oi(?) is the per-instance
weighted log conditional likelihood given by
oi(?) = wi[yi ln pi + (1? yi) ln(1? pi)], (8)
where pi is the predicted probability p(yi =
1|xi; ?) = exp(?
Txi)
1+exp(?Txi) , and wi is an importanceweight to each example. A biased sampling
procedure selects only a small subset of negative
samples to be included in the objective (see (Lao and
Cohen, 2010b) for detail).
2.3 Data-Driven Path Finding
In prior work with PRA, P` was defined as all
relation paths of length at most `. When the number
of edge types is small, one can generate P` by
533
Table 1: Number of paths in PRA models of maximum
path length 3 and 4. Averaged over 96 tasks.
`=3 `=4
all paths up to length L 15, 376 1, 906, 624
+query support? ? = 0.01 522 5016
+ever reach a target entity 136 792
+L1 regularization 63 271
enumeration; however, for domains with a large
number of edge types (e.g., a knowledge base), it is
impractical to enumerate all possible relation paths
even for small `. For instance, if the number of
edge types related to each node type is 100, even
the number of length three paths types easily reaches
millions. For other domains like parsed natural
language sentences, useful relation paths can be as
long as ten relations (Minkov and Cohen, 2008). In
this case, even with smaller number of possible edge
types, the total number of relation paths is still too
large for systematic enumeration.
In order to apply PRA to these domains, we
modify the path generation procedure in PRA to
produce only relation paths which are potentially
useful for the task. Define a query s to be supporting
a path P if hs,P (e) 6= 0 for any entity e. We require
that any path node created during path finding needs
to be supported by at least a fraction ? of the training
queries si, as well as being of length no more than
` (In the experiments, we set ? = 0.01) We also
require that in order for a relation path to be included
in the PRA model, it must retrieve at least one target
entity ti in the training set. As we can see from
Table 1, together these two constraints dramatically
reduce the number of relation paths that need to be
considered, relative to systematically enumerating
all possible relation paths. L1 regularization reduces
the size of the model even more.
The idea of finding paths that connects nodes in a
graph is not new. It has been embodied previously in
first-order learning systems (Richards and Mooney,
1992) as well as N-FOIL, and relational database
searching systems (Bhalotia et al, 2002). These
approaches consider a single query during path
finding. In comparison, the data-driven path finding
method we described here uses statistics from a
population of queries, and therefore can potentially
determine the importance of a path more reliably.
Table 2: Comparing PRA with RWR models. MRRs and
training times are averaged over 96 tasks.
`=2 `=3
MRR Time MRR Time
RWR(no train) 0.271 0.456
RWR 0.280 3.7s 0.471 9.2s
PRA 0.307 5.7s 0.516 15.4s
2.4 Low-Variance Sampling
Lao and Cohen (2010a) previously showed that
sampling techniques like finger printing and particle
filtering can significantly speedup random walk
without sacrificing retrieval quality. However, the
sampling procedures can induce a loss of diversity
in the particle population. For example, consider a
node in the graph with just two out links with equal
weights, and suppose we are required to generate
two walkers starting from this node. A disappointing
result is that with 50 percent chance both walkers
will follow the same branch, and leave the other
branch with no probability mass.
To overcome this problem, we apply a technique
called Low-Variance Sampling (LVS) (Thrun et
al., 2005), which is commonly used in robotics
to improve the quality of sampling. Instead of
generating independent samples from a distribution,
LVS uses a single random number to generate all
samples, which are evenly distributed across the
whole distribution. Note that given a distribution
P (x), any number r in [0, 1] points to exactly one
x value, namely x = argminj
?
m=1..j P (m) ?
r. Suppose we want to generate M samples from
P (x). LVS first generates a random number r in
the interval [0,M?1]. Then LVS repeatedly adds
the fixed amount M?1 to r and chooses x values
corresponding to the resulting numbers.
3 Results
This section reports empirical results of applying
random walk inference to NELL?s knowledge base
after the 165th iteration of its learning process. We
first investigate PRA?s behavior by cross validation
on the training queries. Then we compare PRA and
N-FOIL?s ability to reliably infer new beliefs, by
leveraging the Amazon Mechanical Turk service.
534
3.1 Cross Validation on the Training Queries
Random Walk with Restart (RWR) (also called
personalized PageRank (Haveliwala, 2002)) is a
general-purpose graph proximity measure which
has been shown to be fairly successful for many
types of tasks. We compare PRA to two versions
of RWR on the 96 tasks of link prediction with
NELL?s knowledge base. The two baseline methods
are an untrained RWR model and a trained RWR
model as described by Lao and Cohen (2010b). (In
brief, in the trained RWR model, the walker will
probabilistically prefer to follow edges associated
with different labels, where the weight for each edge
label is chosen to minimize a loss function, such as
Equation 7. In the untrained model, edge weights
are uniform.) We explored a range of values for
the regularization parameters L1 and L2 using cross
validation on the training data, and we fix both
L1 and L2 parameters to 0.001 for all tasks. The
maximum path length is fixed to 3.3
Table 2 compares the three methods using
5-fold cross validation and the Mean Reciprocal
Rank (MRR)4 measure, which is defined as the
inverse rank of the highest ranked relevant result
in a set of results. If the the first returned
result is relevant, then MRR is 1.0, otherwise,
it is smaller than 1.0. Supervised training can
significantly improve retrieval quality (p-value=9 ?
10?8 comparing untrained and trained RWR), and
leveraging path information can produce further
improvement (p-value=4? 10?4 comparing trained
RWR with PRA). The average training time for a
predicate is only a few seconds.
We also investigate the effect of low-variance
sampling on the quality of prediction. Figure 2 com-
pares independent and low variance sampling when
applied to finger printing and particle filtering (Lao
and Cohen, 2010a). The horizontal axis corresponds
to the speedup of random walk compared with
exact inference, and the vertical axis measures the
quality of prediction by MRR with three fold cross
validation on the training query set. Low-variance
3Results with maximum length 4 are not reported here.
Generally models with length 4 paths produce slightly better
results, but are 4-5 times slower to train
4For a set of queries Q,
MRR = 1|Q|
?
q?Q
1rank of the first correct answer for q
 
0.4
0.5
0 1 2 3 4 5
M
R
R
Random Walk Speedup
Exact
Independent Fingerprinting
Low Variance Fingerprinting
Independent Filtering
Low Variance Filtering
10k
1k
100
10k
1k
100k
Figure 2: Compare inference speed and quality over 96
tasks. The speedup is relative to exact inference, which is
on average 23ms per query.
sampling can improve prediction for both finger
printing and particle filtering. The numbers on the
curves indicate the number of particles (or walkers).
When using a large number of particles, the particle
filtering methods converge to the exact inference.
Interestingly, when using a large number of walkers,
the finger printing methods produce even better
prediction quality than exact inference. Lao and
Cohen noticed a similar improvement on retrieval
tasks, and conjectured that it is because the sampling
inference imposes a regularization penalty on longer
relation paths (2010a).
3.2 Evaluation by Mechanical Turk
The cross-validation result above assumes that the
knowledge base is complete and correct, which
we know to be untrue. To accurately compare
PRA and N-FOIL?s ability to reliably infer new
beliefs from an imperfect knowledge base, we
use human assessments obtained from Amazon
Mechanical Turk. To limit labeling costs, and
since our goal is to improve the performance of
NELL, we do not include RWR-based approaches
in this comparison. Among all the 24 functional
predicates, N-FOIL discovers confident rules for
8 of them (it produces no result for the other 16
predicates). Therefore, we compare the quality
of PRA to N-FOIL on these 8 predicates only.
Among all the 72 non-functional predicates?which
535
Table 3: The top two weighted PRA paths for tasks on which N-FOIL discovers confident rules. c stands for concept.
ID PRA Path (Comment)
athletePlaysForTeam
1 c athletePlaysInLeague???????????????? c leaguePlayers?????????? c athletePlaysForTeam???????????????? c (teams with many players in the athlete?s league)
2 c athletePlaysInLeague???????????????? c leagueTeams?????????? c teamAgainstTeam????????????? c (teams that play against many teams in the athlete?s league)
athletePlaysInLeague
3 c athletePlaysSport????????????? c players?????? c athletePlaysInLeague???????????????? c (the league that players of a certain sport belong to)
4 c isa??? c isa?1????? c athletePlaysInLeague???????????????? c (popular leagues with many players)
athletePlaysSport
5 c isa??? c isa?1????? c athletePlaysSport????????????? c (popular sports of all the athletes)
6 c athletePlaysInLeague???????????????? c superpartOfOrganization?????????????????? c teamPlaysSport???????????? c (popular sports of a certain league)
stadiumLocatedInCity
7 c stadiumHomeTeam?????????????? c teamHomeStadium?????????????? c stadiumLocatedInCity???????????????? c (city of the stadium with the same team)
8 c latitudeLongitude????????????? c latitudeLongitudeOf??????????????? c stadiumLocatedInCity???????????????? c (city of the stadium with the same location)
teamHomeStadium
9 c teamPlaysInCity????????????? c cityStadiums?????????? c (stadiums located in the same city with the query team)
10 c teamMember?????????? c athletePlaysForTeam???????????????? c teamHomeStadium?????????????? c (home stadium of teams which share players with the query)
teamPlaysInCity
11 c teamHomeStadium?????????????? c stadiumLocatedInCity???????????????? c (city of the team?s home stadium)
12 c teamHomeStadium?????????????? c stadiumHomeTeam?????????????? c teamPlaysInCity????????????? c (city of teams with the same home stadium as the query)
teamPlaysInLeague
13 c teamPlaysSport???????????? c players?????? c athletePlaysInLeague???????????????? c (the league that the query team?s members belong to)
14 c teamPlaysAgainstTeam????????????????? c teamPlaysInLeague?????????????? c (the league that the query team?s competing team belongs to)
teamPlaysSport
15 c isa??? c isa?1????? c teamPlaysSport???????????? c (sports played by many teams)
16 c teamPlaysInLeague?????????????? c leagueTeams?????????? c teamPlaysSport???????????? c (the sport played by other teams in the league)
Table 4: Amazon Mechanical Turk evaluation for the promoted knowledge. Using paired t-test at task level, PRA is
not statistically different from N-FOIL for p@10 (p-value=0.3), but is significantly better for p@100 (p-value=0.003)
PRA N-FOIL
Task Pmajority #Paths p@10 p@100 p@1000 #Rules #Query p@10 p@100 p@1000
athletePlaysForTeam 0.07 125 0.4 0.46 0.66 1(+1) 7 0.6 0.08 0.01
athletePlaysInLeague 0.60 15 1.0 0.84 0.80 3(+30) 332 0.9 0.80 0.24
athletePlaysSport 0.73 34 1.0 0.78 0.70 2(+30) 224 1.0 0.82 0.18
stadiumLocatedInCity 0.05 18 0.9 0.62 0.54 1(+0) 25 0.7 0.16 0.00
teamHomeStadium 0.02 66 0.3 0.48 0.34 1(+0) 2 0.2 0.02 0.00
teamPlaysInCity 0.10 29 1.0 0.86 0.62 1(+0) 60 0.9 0.56 0.06
teamPlaysInLeague 0.26 36 1.0 0.70 0.64 4(+151) 30 0.9 0.18 0.02
teamPlaysSport 0.42 21 0.7 0.60 0.62 4(+86) 48 0.9 0.42 0.02
average 0.28 43 0.79 0.668 0.615 91 0.76 0.38 0.07
teamMember 0.01 203 0.8 0.64 0.48
companiesHeadquarteredIn 0.05 42 0.6 0.54 0.60
publicationJournalist 0.02 25 0.7 0.70 0.64
producedBy 0.19 13 0.5 0.58 0.68 N-FOIL does not produce results
competesWith 0.19 74 0.6 0.56 0.72 for non-functional predicates
hasOfficeInCity 0.03 262 0.9 0.84 0.60
teamWonTrophy 0.24 56 0.5 0.50 0.46
worksFor 0.13 62 0.6 0.60 0.74
average 0.11 92 0.650 0.620 0.615
536
N-FOIL cannot be applied to?PRA exhibits a wide
range of performance in cross-validation. The are 43
tasks for which PRA obtains MRR higher than 0.4
and builds a model with more than 10 path features.
We randomly sampled 8 of these predicates to be
evaluated by Amazon Mechanical Turk.
Table 3 shows the top two weighted PRA features
for each task on which N-FOIL can successfully
learn rules. These PRA rules can be categorized into
broad coverage rules which behave like priors over
correct answers (e.g. 1-2, 4-6, 15), accurate rules
which leverage specific relation sequences (e.g. 9,
11, 14), rules which leverage information about the
synonyms of the query node (e.g. 7-8, 10, 12),
and rules which leverage information from a local
neighborhood of the query node (e.g. 3, 12-13, 16).
The synonym paths are useful, because an entity
may have multiple names on the web. We find
that all 17 general rules (no specialization) learned
by N-FOIL can be expressed as length two relation
paths such as path 11. In comparison, PRA explores
a feature space with many length three paths.
For each relation R to be evaluated, we generate
test queries s which belong to domain(R). Queries
which appear in the training set are excluded. For
each query node s, we applied a trained model
(either PRA or N-FOIL) to generate a ranked list
of candidate t nodes. For PRA, the candidates
are sorted by their scores as in Eq. (6). For
N-FOIL, the candidates are sorted by the estimated
accuracies of the rules as in Eq. (2) (which generate
the candidates). Since there are about 7 thousand
(and 13 thousand) test queries s for each functional
(and non-functional) predicate R, and there are
(potentially) thousands of candidates t returned for
each query s, we cannot evaluate all candidates of
all queries. Therefore, we first sort the queries s for
each predicate R by the scores of their top ranked
candidate t in descending order, and then calculate
precisions at top 10, 100 and 1000 positions for the
list of result R(sR,1, tR,11 ), R(sR,2, tR,21 ), ..., where
sR,1 is the first query for predicate R, tR,11 is its first
candidate, sR,2 is the second query for predicate R,
tR,21 is its first candidate, so on and so forth. To
reduce the labeling load, we judge all top 10 queries
for each predicate, but randomly sample 50 out of
the top 100, and randomly sample 50 out of the
Table 5: Comparing Mechanical Turk workers? voted
assessments with our gold standard labels based on 100
samples.
AMT=F AMT=T
Gold=F 25% 15%
Gold=T 11% 49%
top 1000. Each belief is evaluated by 5 workers
at Mechanical Turk, who are given assertions like
?Hines Ward plays for the team Steelers?, as well
as Google search links for each entity, and the
combination of both entities. Statistics shows
that the workers spend on average 25 seconds to
judge each belief. We also remove some workers?
judgments which are obviously incorrect5. We
sampled 100 beliefs, and compared their voted result
to gold-standard labels produced by one author of
this paper. Table 5 shows that 74% of the time the
workers? voted result agrees with our judgement.
Table 4 shows the evaluation result. The
Pmajority column shows for each predicate the
accuracy achieved by the majority prediction: given
a query R(a, ?), predict the b that most often
satisfies R over all possible a in the knowledge
base. Thus, the higher Pmajority is, the simpler
the task. Predicting the functional predicates
is generally easier predicting the non-functional
predicates. The #Query column shows the number
of queries on which N-FOIL is able to match any
of its rules, and hence produce a candidate belief.
For most predicates, N-FOIL is only able to produce
results for at most a few hundred queries. In
comparison, PRA is able to produce results for 6,599
queries on average for each functional predicate, and
12,519 queries on average for each non-functional
predicate. Although the precision at 10 (p@10) of
N-FOIL is comparable to that of PRA, precision
at 10 and at 1000 (p@100 and p@1000) are much
lower6.
The #Path column shows the number of paths
learned by PRA, and the #Rule column shows the
number of rules learned by N-FOIL, with the num-
bers before brackets correspond to unspecialized
rules, and the numbers in brackets correspond to
5Certain workers label all the questions with the same
answer
6If a method makes k predictions, and k < n, then p@n is
the number correct out of the k predictions, divided by n
537
specialized rules. Generally, specialized rules have
much smaller recall than unspecialized rules. There-
fore, the PRA approach achieves high recall partially
by combining a large number of unspecialized paths,
which correspond to unspecialized rules. However,
learning more accurate specialized paths is part of
our future work.
A significant advantage of PRA over N-FOIL is
that it can be applied to non-functional predicates.
The last eight rows of Table 4 show PRA?s
performance on eight of these predicates. Compared
to the result on functional predicates, precisions
at 10 and at 100 of non-functional predicates
are slightly lower, but precisions at 1000 are
comparable. We note that for some predicates
precision at 1000 is better than at 100. After
some investigation we found that for many relations,
the top portion of the result list is more diverse:
i.e. showing products produced by different com-
panies, journalist working at different publications.
While the lower half of the result list is more
homogeneous: i.e. showing relations concentrated
on one or two companies/publications. On the
other hand, through the process of labeling the
Mechanical Turk workers seem to build up a prior
about which company/publication is likely to have
correct beliefs, and their judgments are positively
biased towards these companies/publications. These
two factors combined together result in positive bias
towards the lower portion of the result list. In future
work we hope to design a labeling strategy which
avoids this bias.
4 Conclusions and Future Work
We have shown that a soft inference procedure based
on a combination of constrained, weighted, random
walks through the knowledge base graph can be
used to reliably infer new beliefs for the knowledge
base. We applied this approach to a knowledge
base of approximately 500,000 beliefs extracted
imperfectly from the web by NELL. This new
system improves significantly over NELL?s earlier
Horn-clause learning and inference method: it
obtains nearly double the precision at rank 100. The
inference and learning are both very efficient?our
experiment shows that the inference time is as fast
as 10 milliseconds per query on average, and the
training for a predicate takes only a few seconds.
There are several prominent directions for future
work. First, inference starting from both the query
nodes and target nodes (Richards and Mooney,
1992) can be much more efficient in discovering
long paths than just inference from the query nodes.
Second, inference starting from the target nodes
of training queries is a potential way to discover
specialized paths (with grounded nodes). Third,
generalizing inference paths to inference trees or
graphs can produce more expressive random walk
inference models. Overall, we believe that random
walk is a promising way to scale up relational
learning to domains with very large data sets.
Acknowledgments
This work was supported by NIH under grant
R01GM081293, by NSF under grant IIS0811562,
by DARPA under awards FA8750-08-1-0009 and
AF8750-09-C-0179, and by a gift from Google.
We thank Geoffrey J. Gordon for the suggestion
of applying low variance sampling to random walk
inference. We also thank Bryan Kisiel for help with
the NELL system.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries - DL ?00, pages 85?94, New York, New York,
USA. ACM Press.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
Information Extraction from the Web. In IJCAI, pages
2670?2676.
Gaurav Bhalotia, Arvind Hulgeri, Charuta Nakhe,
Soumen Chakrabarti, and S. Sudarshan. 2002.
Keyword searching and browsing in databases using
banks. ICDE, pages 431?440.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the eleventh annual conference on
Computational learning theory - COLT? 98, pages
92?100, New York, New York, USA. ACM Press.
MJ Cafarella, M Banko, and O Etzioni. 2006. Relational
Web Search. In WWW.
Jamie Callan and Mark Hoy. 2009. Clueweb09 data set.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
538
Mitchell. 2010. Toward an Architecture for
Never-Ending Language Learning. In AAAI.
William W. Cohen and David Page. 1995. Polyno-
mial learnability and inductive logic programming:
Methods and results. New Generation Comput.,
13(3&4):369?409.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.
Weld, and Er Yates. 2005. Unsupervised named-
entity extraction from the web: An experimental study.
Artificial Intelligence, 165:91?134.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW, pages 517?526.
Alpa Jain and Patrick Pantel. 2010. Factrank: Random
walks on a web of facts. In COLING, pages 501?509.
Ni Lao and William W. Cohen. 2010a. Fast query exe-
cution for retrieval models based on path-constrained
random walks. KDD.
Ni Lao and William W. Cohen. 2010b. Relational
retrieval using a combination of path-constrained
random walks. Machine Learning.
Einat Minkov and William W. Cohen. 2008. Learning
graph walk based similarity measures for parsed text.
EMNLP, pages 907?916.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In ACL.
J. Ross Quinlan and R. Mike Cameron-Jones. 1993.
FOIL: A Midterm Report. In ECML, pages 3?20.
Bradley L. Richards and Raymond J. Mooney. 1992.
Learning relations by pathfinding. In Proceedings
of the Tenth National Conference on Artificial Intel-
ligence (AAAI-92), pages 50?55, San Jose, CA, July.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling Textual Inference to the Web.
In EMNLP, pages 79?88.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic Taxonomy Induction from Heterogenous
Evidence. In ACL.
Sebastian Thrun, Wolfram Burgard, and Dieter Fox.
2005. Probabilistic Robotics (Intelligent Robotics and
Autonomous Agents). The MIT Press.
Alexander Yates, Michele Banko, Matthew Broadhead,
Michael J. Cafarella, Oren Etzioni, and Stephen
Soderland. 2007. TextRunner: Open Information
Extraction on the Web. In HLT-NAACL (Demonstra-
tions), pages 25?26.
John M. Zelle, Cynthia A. Thompson, Mary Elaine
Califf, and Raymond J. Mooney. 1995. Inducing
logic programs without explicit negative examples.
In Proceedings of the Fifth International Workshop
on Inductive Logic Programming (ILP-95), pages
403?416, Leuven, Belgium.
539
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1447?1455,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Discovering Relations between Noun Categories 
 
Thahir P Mohamed * Estevam R Hruschka Jr. Tom M Mitchell 
University Of Pittsburgh Federal University of Sao Carlos Carnegie Mellon University 
pmthahir@gmail.com estevam@cs.cmu.edu tom.mitchell@cs.cmu.edu 
   
   
Abstract 
Traditional approaches to Relation Extraction 
from text require manually defining the rela-
tions to be extracted.  We propose here an ap-
proach to automatically discovering relevant 
relations, given a large text corpus plus an ini-
tial ontology defining hundreds of noun cate-
gories (e.g., Athlete, Musician, Instrument).  
Our approach discovers frequently stated rela-
tions between pairs of these categories, using a 
two step process. For each pair of categories 
(e.g., Musician and Instrument) it first co-
clusters the text contexts that connect known 
instances of the two categories, generating a 
candidate relation for each resulting cluster.  It 
then applies a trained classifier to determine 
which of these candidate relations is semanti-
cally valid. Our experiments apply this to a text 
corpus containing approximately 200 million 
web pages and an ontology containing 122 cat-
egories from the NELL system [Carlson et al, 
2010b], producing a set of 781 proposed can-
didate relations, approximately half of which 
are semantically valid.  We conclude this is a 
useful approach to semi-automatic extension of 
the ontology for large-scale information extrac-
tion systems such as NELL. 
1 Introduction 
The Never-Ending Language Learner (NELL) 
(Carlson et al, 2010b)) is a computer system that 
learns continuously to extract facts from the web.  
NELL is given as input an initial ontology that 
specifies the semantic categories (e.g. city, compa-
ny, sportsTeam) and semantic relations (e.g. hasOf-
ficesIn(company,city), teamPlay-
sInCity(sportsTeam,city)) it must extract from the 
web.  In addition, it is provided 10-20 seed positive 
training examples for each of these categories and 
relations, along with hundreds of millions of unla-
beled web page.  Given this input, NELL applies a 
large-scale multitask, semisupervised learning 
method to learn to extract new instances of these 
categories (e.g., city(?London?)) and relations 
(e.g., teamPlaysInCity(?Steelers?,?Pittsburgh?)) 
from the web.  During the past 17 months NELL 
has been running nearly continuously, learning to 
extract over 600 categories and relations, and pop-
ulating a knowledge base containing over 700,000 
instances of these categories and relations with a 
precision of approximately 0.851. 
This paper considers the problem of automati-
cally discovering new relations to extend the on-
tology of systems such as NELL, enabling them to 
increase over time their learning and extraction 
capabilities.  More precisely, we consider the fol-
lowing problem: 
 
Input: 
? An ontology specifying a set of categories 
? A knowledge base containing instances of these 
categories (perhaps including errors) 
? A large text corpus 
 
Output: 
? A set of two-argument relations that are fre-
quently mentioned in the text corpus, and 
whose argument types correspond to categories 
in the input ontology (e.g., RiverFlows-
ThroughCity(<River>,<City>). 
? For each proposed relation, a set of instances 
(i.e. RiverFlowsThroughCity(?Nile?,?Cairo?)). 
? For each proposed relation, a set of text extrac-
tion patterns that can be used to extract addi-
tional instances of the relation (e.g., the text ?X 
in the heart of Y?, where X is a known river, 
and Y a known City, suggests extracting 
RiverFlowsThroughCity(X,Y)). 
 
Note the above inputs are easily available from 
NELL in the form of its existing ontology and ex-
tracted knowledge base.  Note also that the outputs 
                                                          
*  Thahir P. Mohamed is currently at Amazon Inc. 
1  NELL?s extracted knowledge can be viewed and 
downloaded at http://rtw.ml.cmu.edu. 
1447
  
of our system are sufficient to initiate NELL?s 
learning of additional extraction methods to further 
populate each proposed relation.  One goal of this 
research is to create a system that can provide 
NELL with an ongoing set of new learning and 
extraction tasks. The system is called OntExt (On-
tology Extension System) 
 
Table 1 shows a sample of successful relations 
and corresponding relation contexts and sample 
seed instances generated by OntExt. 
 
Table 1. Examples of valid relations (generated 
by OntExt), their text extraction patterns and 
extracted instances. 
name(category1- 
main context- 
category2) 
Extraction pat-
terns 
Seed 
Instances 
 
River 
-in heart of- 
City 
 
?in heart of? 
?in the center 
of? 
?which flows 
through? 
?Seine, Paris? 
?Nile, Cairo? 
?Tiber river, Rome? 
?River arno, Florence? 
Food 
-to produce-
Chemical 
?to produce? 
?to make? 
?to form? 
?Salt, Chlorine? 
?Sugar, Carbon diox-
ide? 
?Protein, Serotonin? 
 
StadiumOrVenue 
-in downtown- 
City 
 
?in downtown? 
 
?Ford field, Detroit? 
?Superdome, New Or-
leans? 
?Turner field, Atlanta? 
 
Disease 
-caused by- 
Bacteria 
 
?caused by? 
?is the causa-
tive agent of? 
?is the cause 
of? 
?pneumonia, legionel-
la? 
?mastitis, staphylococ-
cus aureus? 
?gonorrhea, neisseria 
gonorrhoeae? 
Disease 
-destroys-
CellType 
?destroys? 
?attacks? 
"alzheimer, brain cells" 
?vitiligo", melano-
cytes" 
"aids, lymphocytes" 
County 
-county-
StateOrProvince 
?county? 
?county of? 
?county in? 
"sufolk, massachusetts" 
"marin, california" 
"sussex, delaware" 
"osceola, michigan" 
2 Background 
Traditional Relation Extraction 
We define Traditional RE systems as those that 
require the user to specify information about the 
relations to be learned. For instance, SnowBall 
(Agichtein and Gravano 2000) & CPL (Carlson et 
al. 2009) are bootstrapped learning systems that 
require manual input of relation predicates. In the-
se systems, for each relation predicate, the relation 
name (e.g. City ?Capital of? Country), the seed in-
stances and the category type (e.g. City, Country, 
Celebrity etc) are provided (for domain and range). 
In CPL (Carlson et al 2009), learning of rela-
tion/category instances is coupled by using con-
straints such as mutual exclusion relationships 
among the predicates. The authors show that this 
coupling reduces semantic drift, which commonly 
occurs with bootstrapping systems, thus leading to 
improved precision. CPL achieved 89% precision 
for the relation instances extracted (Carlson, Bet-
teridge et al 2009).  KNOWITALL (Etzioni, Ca-
farella et al 2005) is a web-scale relation extrac-
tion system, which requires as input the relation 
names. Hence, in these ?traditional relation extrac-
tion? methods, the need to manually define the re-
lations to be extracted makes it difficult to work in 
applications having thousands of possible relation 
predicates.  
2.1 Open Relation Extraction 
Open RE methods do not require a user to manual-
ly specify the information about the relations to be 
learned, such as their names, seed examples, etc. 
TextRunner (Banko, Cararella et al 2007) is such 
an Open Information Extraction system that re-
trieves from the web millions of relational tuples 
between noun phrase entities. TextRunner uses a 
deep linguistic parser to perform self-supervised 
learning and extracts a positive set (i.e. valid rela-
tion between entities) and a negative set (i.e. inva-
lid relationships) of relational tuples based on cer-
tain heuristics. Then, a Naive Bayes classifier is 
built having features such as part-of-speech tags of 
the words in the relation tuples, number of tokens, 
stopwords etc., and uses the labeled instances as 
the training set. This classifier runs on sentences 
from a web corpus to extract millions of relational 
tuples. However, of the 11 million high confident 
relational tuples extracted by this system only 1 
million were concrete facts (Banko, Cararella et al 
2007). Of these concrete facts 88% were estimated 
to be correct. For instance, (Mountain View, head-
quarters of, Google) is a tuple representing a valid 
concrete fact. The remaining 90% of the tuples are 
abstract or do not have well-formed arguments or 
well-formed relations. For instance, (Einstein, de-
rived, theory) is an abstract tuple as it does not 
1448
  
have enough information to indicate a concrete fact 
(Banko, Cararella et al 2007) because the specific 
theory which Einstein derived is missing in that 
tuple. In the tuple (45, ?went to?, ?Boston?), one of 
the arguments (i.e. 45) is not well formed.  
In (Banko and Etzioni, 2008) a Conditional 
Random Field (CRF) classifier is used to perform 
Open Relation Extraction which improves by more 
than 60% the F-score achieved by the Naive Bayes 
model in the TextRunner system. However the 
CRF approach does not solve the problem associ-
ated with extraction of abstract/non-well formed 
tuples. Further, in the same work, it is shown that 
Open RE has a much lower recall in comparison to 
Traditional RE systems. On four common relations 
(Acquisition, Birthplace, InvetorOf, WonAward), 
Open RE attained a recall of 18.4% in comparison 
to 58.4% achieved by Traditional RE (Banko and 
Etzioni 2008). Both Open RE systems discussed 
(Banko, Cararella et al 2007; Banko and Etzioni 
2008) do not perform learning of the category type 
of the entities involved in the relations. They are 
single-pass and do not perform continuous learning 
to improve/extend on what has been learnt. 
2.2 Unsupervised Methods to Extract Rela-
tions between Named Entities 
In general, traditional RE methods extract concrete 
facts and have much higher recall for a given rela-
tion, than Open RE methods. This is due to the 
knowledge fed into Traditional RE methods such 
as the category type of the entities in the relation 
and seed instances for the relation. Traditional RE 
methods require the relations to be manually de-
fined and extract instances only for them. Open RE 
methods, on the other hand, do not require any 
such domain specific knowledge to be manually 
input.  They extract instances for a wide spectrum 
of relations that are not manually pre-defined.  
To overcome the drawbacks of using Traditional 
and Open RE methods, some researchers have used 
unsupervised learning methods to automatically 
generate new relations (with seeds and contexts) 
between specific categories. These automatically 
generated relations can then be used as input to 
Traditional RE systems.  
Hasegawa et.al (Hasegawa, Sekine et al 2004), 
propose an unsupervised clustering based ap-
proach. One feature vector for each co-occurring 
NE pair is formed based on the context words in 
which the NE pair co-occurs. Then, a cosine-
similarity metric is applied to each pair of feature 
vectors to generate a ?NE-pair x NE-pair? matrix. 
Clustering is done on this matrix and each cluster 
of NE-pairs corresponds to a relation predicate.  
The work by Zhang et.al (Zhang, Su et al 2005) 
generates a shallow parse tree for each sentence 
containing a NE pair to generate relation instances. 
A tree similarity metric is used to cluster the rela-
tion instances. This method gives improved F-
score over Hasegawa et.al (Hasegawa, Sekine et al 
2004). Further they use a specialized NE tagger 
built to recognize entities that belong to specific 
predefined categories. The aforementioned meth-
ods (Hasegawa, Sekine et al 2004) (Zhang, Su et 
al. 2005) were tested on a news corpus to identify 
relations between only a couple of pairs of entity 
types (Person-GeoPoliticalEntity and Company-
Company).  
Both of these methods cluster NE-pairs primari-
ly based on lexical similarity of the context words 
connecting the entities. Hence NE-pairs connected 
by lexically different but semantically similar con-
text patterns (e.g. river ?in heart of? city and river 
?flows through? city) would probably not get clus-
tered together. The web data is, however, much 
noisier and has a larger number of entity types (i.e. 
category predicates), thus, another issue is that for 
web scale data NE pairs X NE pairs similarity ma-
trix would not be scalable for many thousands of 
NE-pairs. 
3 Ontology Extension System - OntExt 
The OntExt system for ontology extension, pro-
posed in this paper, combines characteristics from 
both ?Traditional RE? and ?Open RE,? to discover 
new relations among categories that are already 
present in the ontology, and for which many in-
stances have already been extracted.  
Our proposed method for automatic relation ex-
traction offers the following advantages over the 
methods discussed above.  
? The key idea in our approach is to make use 
of redundancy of information in web data - the 
same relational fact is often stated multiple 
times in large text corpora, using different con-
text patterns.  We use this redundancy to clus-
ter together context patterns which are seman-
tically similar although they may be lexically 
dissimilar. 
1449
  
? Instead of clustering on the ?NE-pairs X 
NE-pairs? matrix, clustering is done on a ?Con-
text-pattern X Context-pattern? matrix. This is 
much more scalable as the context patterns are 
fewer in number and since our method applies 
several criteria to prune out irrelevant patterns. 
? To accommodate errors in the input catego-
ry instances and ambiguity in web data, we 
build a classifier which learns to distinguish 
valid relations from semantically invalid rela-
tions. 
 
OntExt has 3 components. 1) It starts exploring 
a large web corpus and 2) category instances ex-
tracted by CPL to generate new relations. After the 
relations are generated, 3) a classifier is developed 
to classify semantically valid relations. 
3.1 Pre-processing 
Following along the same strategy used in [Carlson 
et al, 2010], OntExt uses as input a corpus of 2 
billion sentences, which was generated by using 
the OpenNLP2 package to extract, tokenize, and 
POS-tag sentences from the 500 million web page 
English portion of the ClueWeb09 data [Callan and 
Hoy, 2009]. Before performing relation extraction, 
this corpus is preprocessed. First, sentences which 
contain a pair of known category instances are re-
trieved (e.g. the sentence ?Ottawa is the capital of 
Canada.?, where ?Ottawa? is a known instance of 
the ?City? category and ?Canada? is a known in-
stance of ?Country?).  For every category pair (e.g. 
<City, Country>) the sentences containing known 
instances of both categories are grouped into a set 
S. The text between the two instances is called the 
?context pattern? (e.g. ?is the capital of? is a context 
pattern). Three types of pruning are done on this 
set S. 
1. If the context pattern is a rare one (i.e. if the 
context pattern occurs in less than a threshold 
number of sentences), all sentences with that 
context pattern are removed. Thus we retain 
only frequently occurring contexts.  We use a 
threshold requiring at least 5 sentences in the 
experiments presented in Section 4. 
2. Context patterns which co-occur with very 
few instances of either category type are re-
moved. For example, the category pair <Vehi-
cle,SportsTeam> has several sentences such as 
                                                          
2  http://opennlp.sourceforge.net. 
?Car was engulfed in flames?,  ?Truck was en-
gulfed in flames? etc. Note that Flames (Calga-
ry Flames) is a SportsTeam. But here flames 
clearly does not refer to a Sportsteam. This 
context ?was engulfed in? connects several in-
stance of a ?Vehicle? category to a single in-
stance of SportsTeam instance. Hence all sen-
tences with this context are removed. Note this 
context would not have been removed in step 1 
as that is just a threshold on the number of sen-
tences in which any pair occurs. We use a 
threshold requiring at least 3 distinct instances 
of both the domain and the range, for each 
context. 
3. Banko et.al, 2008 show that most binary re-
lational contexts fall under certain types of lex-
ico-synctatic patterns. They include context 
patterns like ?C1 Verb C2?, ?C1 NP Prep C2?, 
?C1 Verb Prep C2? and ?C1 to Verb C2? (C1 
and C2 are category instances). Hence context 
patterns which do not fall under the above 
types are removed from the set S as they are 
not likely to produce relation instances.  
3.2 Relation Generation 
From the previous pre-processing step OntExt re-
trieves for each category pair a pruned set S? of 
sentences. Each sentence has a pair of category 
instances and the context connecting them. 
 
Algorithm 1: Relation Generator 
 
Input: One pair of Categories (C1, C2) and set of 
sentences, each containing a pair of instances 
known to belong to C1 and C2. The phrase con-
necting the instances in the sentence is the context. 
Output: Relations and their seed instances  
 
Steps: 
1. From the input sentences, build a Context by 
Context co-occurrence matrix (Shown in figure 
1). The matrix is then normalized. 
2.  Apply K-means clustering on the matrix to 
cluster the related contexts together. Each clus-
ter corresponds to a possible new relation be-
tween the two input categories. (Weka Ma-
chine Learning package [Hall et al, 2009] was 
used to perform K-means clustering. The value 
of K was set to 5 based on trial and error ex-
periments.) 
1450
  
3. Rank the known instance pairs (belonging to 
C1,C2) for each cluster and take the top 50 as 
seed instances for the relation 
 
The key data structure used by OntExt is a co-
occurrence matrix of the contexts for each category 
pair, as shown in Figure 1. In this matrix, each cell 
corresponds to the number of pairs of category in-
stances that both contexts co-occur with (e.g. the 
sentences ?Vioxx can cure Arthritis? and ?Vioxx is 
a treatment for Arthritis? provide a case where the 
2 contexts ?can cure? and ?is a treatment for? co-
occur with an instance pair [Vioxx, Arthritis]). Ini-
tially, the value of Matrix(I,j) is the number of cat-
egory instance pairs that occur with both context i 
and context j.  We then normalize each cell in the 
matrix, dividing it by by the total count for its row. 
?
?
? N
j
jiMatrix
jiMatrixjiMatrix
0
),(
),(),(
 
We also give higher weight to contexts which co-
occur with only a few contexts over ones which are 
generic and co-occur with most contexts. 
|}0),(:)({|*),(),( ?? jiMatrixjContext
NjiMatrixjiMatrix
 
 
Where N is the total number of contexts, and 
|{Context(j) : Matrix(i,j) > 0}| refers to the number 
of cells in the row Matrix(i) which are greater than 
zero. 
For example, for the <drug, disease> category 
pair after 122 contexts were obtained after prepro-
cessing. Contexts such as ?to treat?, ?for treatment 
of?, ?medication? which all indicate the same rela-
tion (drug-to treat-disease) have high co-
occurrence values (see Figure 1). Similarly con-
texts such as ?can cause?, ?may cause?, ?can lead to? 
(indicating the relation drug-can cause-disease) 
have high co-occurrence values (see Figure 1). 
When OntExt performs clustering on this co-
occurrence matrix the contexts with large co-
occurrences get clustered together. Each cluster is 
then used to propose a possible new relation. The 
centroid of each cluster is used to build the relation 
name. If the centroid of a cluster is the context ?for 
treatment of?, then the relation name is ?drug-for-
treatment-of-disease?. 
OntExt next generates seed instances for the 
proposed relation. The seed instances which co-
occur with contexts corresponding to the cluster 
centroid or close to centroid will be best repre-
sentative of the relation. So the strength of the seed 
instance is inversely proportional to the standard 
deviation of the context from the centroid of the 
relation contexts cluster.  Also the strength of the 
seed instance is directly proportional to the number 
of times it co-occurs with the context. 
 
 
 
Figure 1: This figure shows the Context by Context 
sub-matrix (with 6 contexts) for the category pair 
(Drug, Disease) and the seed instances for each 
relation.  As described in the text, each entry gives 
the normalized count of the number of known 
<drug, disease> pairs that occur with both the row 
context and the column context. 
 
To summarize, each seed instance s (pair of cat-
egory instances) is weighted as follows  
   
Where, 
Pattern_cluster is the cluster of pattern contexts for 
this given relation 
Occ(c,s) is the number of times instance ?s? co-
occurs with the pattern context ?c? 
sd(c) is the standard deviation of the context from 
the centroid of the pattern cluster.   
Using this metric the instances are ranked and the 
top 50 are output as initial seed instances for the 
proposed relation.   
1451
  
3.3 Classifying semantically valid relations 
More than half of the relations generated in the 
previous step are invalid due to the following rea-
sons 
1. Error in category instances: The category 
instances input to OntExt come from NELL. In 
the version of the knowledge base used in the-
se experiments, the accuracy of these instances 
was 78%. Due to the erroneous category in-
stances some invalid relations are generated by 
OntExt. For instance the generated relation, 
?condiment-wearing-clothing? with seeds 
(pig,dress), (rabbit,pants) etc. Here ?pig?  and 
?rabbit? were incorrectly identified by NELL as 
instances of ?condiment?. 
2. Semantic Ambiguity: Consider the generated 
relation ?bakedgood-baking-magazine? with in-
stances (cookies,time), (cupcakes, people), etc. 
Here the instances ?time? and ?people? do not 
refer to magazines, although they can in gen-
eral.  Due to the semantic ambiguity of these 
instances this invalid relation got generated 
3. Semantically Incomplete relations: Some of 
the generated relations require a third entity or 
some more contextual information, in order to 
be considered semantically valid. For instance, 
?personUs-said-company? or ?newspaper-is-
reporting-that-company?. These don?t stand by 
themselves as two-argument relational facts 
and need more information to be complete 
4. Illogical relations: Some generated relations 
simply have no real semantic meaning. These 
relations are generated due to the category in-
stances appearing together in some unrelated 
contexts. E.g. the generated relation ?date-
starting-date? with seeds such as (Wednesday, 
June), (friday, July) and the relation ?country-
minister-of- economicsector? with seeds (ja-
pan,agriculture), (india, industry).  
The introduction of these invalid relations can 
adversely affect the performance of NELL. How-
ever, it is a challenging problem to develop auto-
mated ways to distinguish between valid and inva-
lid relations without any domain specific 
knowledge. To approach this problem, we identi-
fied a set of features which can help characterizing 
valid and invalid relations, and which can be gen-
erated automatically. Below is a description of the 
features and the intuition behind their use for this 
classification task. 
Each generated relation has a pair of category 
types (C1, C2), a corresponding set of seed in-
stances (which are pairs of instances belonging to 
C1 and C2) and pattern contexts connecting C1 
and C2.  Let N be the number of seed instance 
pairs and N1 and N2 be number of unique instanc-
es (out of these N instance pairs) belonging to cat-
egories C1 and C2 respectively. 
1. Normalized frequency count: The frequency 
count of each category instance is obtained 
from the corpus and normalized by the catego-
ry instance with maximum count. For a given 
relation, a feature is generated by averaging 
the normalized frequency counts of the in-
stances belonging to C1. Another similar fea-
ture is generated for C2 following the same 
strategy. For example the relation <Profession 
?believe that? Movie> was generated due to 
common words like ?predator?, ?earthquake? 
being identified as movie names out of con-
text. These features can help identify such in-
valid relations.   
2. Distribution of extraction patterns: NELL 
learns instances as well as extraction patterns 
for each category (e.g. the category Actor has 
extraction patterns such as ?_ got an Oscar 
award?, ?_ is the movie?s lead actor?). If a cat-
egory instance co-occurs in the web corpus 
with several extraction patterns belonging to 
other categories, then that instance has large 
ambiguity. We measure ambiguity of an in-
stance (i) belonging to category ?C? with re-
spect to another category ?M? (where M is not 
a sub type or super type of ?C?) as   
Ambiguity(i,M) =  
 withoccurs-co i''  that C''in  patterns extraction of #
 ithoccurs-co i'' that M''in  patterns extraction of # 
We measure the average ambiguity for the set 
of instances (of size N) belonging to category 
C in the generated seeds as follows, 
  
??CiM NMiAmbiguityMax /),((
) 
   
Two features are generated for categories C1  
and C2 in the relation. 
3. Relationship characteristics: We identified a 
few characteristics of the relation which help 
in identifying valid relations.  If in the generat-
ed relation, most instances of C1 co-occur only 
1452
  
with very few instances of C2 (or vice versa) 
then the relation could be weak. For example, 
<Organization ?Provides? EconomicSector> - 
the instance ?Information? (of category Eco-
nomicSector) connects to a large percentage of 
items in the category ?Organization? but does 
not express a meaningful relation. So we con-
sider the instance (in this example ?Infor-
mation?, let us call it ?maxconnect_instance?) 
co-occurring with maximum number of in-
stances of the other category. The percentage 
of instances it co-occurs with from among the 
total number of instances of the other category 
which are part of the seed instances is taken as 
a feature. Also if that instance is a very com-
mon word (like ?information? which in several 
contexts does not refer to ?EconomicSector?) 
then this could indicate the presence of an in-
valid relation. So the normalized frequency 
count of this instance (maxconnect_instance) is 
taken as another feature.  
4. Pattern Contexts: The number of pattern con-
texts attained through pattern clustering for the 
relation is taken as another feature. The pres-
ence of several pattern contexts connecting the 
instances between the two categories could in-
dicate that the relation is a valid one.  The 
presence of Hearst patterns (Hearst M, 1992) 
referring to a hyponym (?is-a?) relation in pat-
tern contexts indicates the possibility of a valid 
relation, and is taken as another feature 
Another feature is regarding how specific is 
the context pattern to this relation. If the same 
context connects say C1 instances to instances 
of several other categories apart from C2, then 
this context is not unique to this relation and 
might not indicate a meaningful valid relation-
ship. So the ratio of the number of instances in 
C2 connected to C1 versus the number of in-
stances from all categories connected to C1 by 
the most significant pattern context (i.e. cen-
troid in pattern cluster) is taken as a feature. A 
similar feature is generated for C2 as well. 
4 Experimental Setup and Results 
4.1 CPL System 
CPL (Carlson et al, 2010) is a semi-supervised 
learning system which takes in an input ontology 
(containing category and relation predicates and 
corresponding seed instances) and constraints 
(such as Mutual exclusion rules between predi-
cates). The system iteratively extracts patterns and 
instances for the category/relation predicates from 
a web corpus of around 500 million web pages.  
CPL is one learning component in NELL (the Nev-
er Ending Language Learner) (Carlson et al, 
2010b). 
4.2 Relation Generation: 
We use approximately 22,000 category instances 
belonging to 122 categories extracted by CPL at 
the end of its 20th iteration and the web corpus as 
input to perform the co-clustering described in 
Section 3.2 and generate the new relations. The 
process generated 781 relations. For each relation, 
the relation name, types of the categories involved 
in the relation and the seed instances and patterns 
for each relation were generated. Table 1 in section 
1 shows a sample of valid relations generated by 
this method. 
Tables 2, 3, 4 and 5 show invalid relations for 
each type of invalidity, ?Error in the Category In-
stances?, ?Semantic Ambiguity?, ?Semantically 
Incomplete Relations? and ?Illogical Relations? 
respectively. More specifically, Table 2 shows a 
sample of relations generated due to an entity be-
ing labeled incorrectly as to belong to a category. 
The incorrect category instances are in italics. 
Table 3 presents a sample of relations which 
were generated because of semantic ambiguity. 
Instances with ambiguity are in italics. 
Table 4 shows some of the generated relations 
which are semantically incomplete. 
Table 5 presents samples of illogical relations 
which do not establish any concrete fact. 
Table 2. Examples of Incorrect category in-
stances. 
name(category1 
-main context- 
category2) 
Relation 
Contexts 
Seed 
Instances 
SportsGame 
-Beating- 
Country 
?beating? 
 
"tournament,Sri Lanka" 
"champions, France" 
"match, canada" 
Animal 
-will eat-
Condiment 
?will eat? 
?eating? 
 
"wolf, sheep" 
"fox, rabbit" 
"lion, lamb" 
 
 
 
 
 
1453
  
 
Table 3. Examples of Semantically Ambiguous 
relations.  
Name Relation 
Contexts 
Seed 
Instances 
Bird 
-play- 
City 
?play? 
 
"Cardinals, Atlanta" 
"Ravens, Miami" 
"Eagles, Chicago" 
BakedGood 
-baking- 
Magazine 
?baking? 
"time, cakes" 
"people, cookies" 
 
Table 4. Examples of semantically incomplete 
relations. 
Name Relation 
Contexts 
Seed 
Instances 
Personus 
acknowledged 
Date 
 
?acknowledged? 
?warned? 
?met? 
 
"mr obama, tues-
day" 
"george w . bush, 
tuesday" 
"al gore, thursday" 
NewsPaper 
-is reporting 
that- 
Company 
?is reporting 
that? 
?writes that? 
?reported that? 
"financial times, 
apple" 
"wall street jour-
nal, gm" 
"wall street jour-
nal, yahoo" 
 
Table 5. Examples of relations representing 
facts that are not concrete.  
Name Relation 
Contexts 
Seed 
Instances 
Emotion 
-of living in-
StateOrPro 
vince 
?of living in? 
"joy, california" 
"excitement, colora-
do" 
"fear, iowa" 
BodyPart 
-to keep- 
BodyPart 
 
?to keep? 
?guard? 
?hand, eye? 
?nose, throat? 
?eye, brain? 
?elbow, hand? 
4.3 Relation Classification: 
To determine the feasibility of automatically classi-
fying OntExt?s proposed relations as valid or inva-
lid, we trained and tested a classifier using the fea-
tures described above, using manually assigned 
class label for some of the generated relations (252 
relations) as valid or invalid (the criteria for which 
was explained before). 115 of these 252 relations 
were found to be valid by manual evaluation. This 
shows the need for a machine learning classifier to 
identify valid/invalid relations. The various fea-
tures described earlier (such as normalized fre-
quency count, relationship characteristics, pattern 
context features, distribution of extraction patterns) 
were generated for each relation. Ten-fold cross 
validation experiments were carried out with vari-
ous classifiers.  A Random Forest classifier per-
formed the best.  Precision, recall and ROC-area is 
shown in the table below (ROC area is the area 
under the ROC curve which plots the classifier 
performance by having the True Positive Rate on 
the Y-axis and False Positive Rate on the X-axis). 
 
Table 6. Classifier performance. 
RelationType Precision Recall ROC Area 
Valid 71.6 72.2 0.804 
Invalid 76.5 75.9 0.804 
Weighted 
Avg. 
74.2 74.2 0.804 
 
These results indicate that the system is able to 
learn to identify semantically valid relations with-
out using any manually input information. The val-
id relations generated can be input to NELL, al-
lowing it to iteratively learn additional instances 
for each proposed relation. 
5 Conclusion and Future work: 
Open Relation Extraction and Traditional Relation 
Extraction have their respective strengths and 
weaknesses. The OntExt system proposed in this 
work combines the strengths of both of those 
methods. The relation predicates automatically 
generated by our approach are typed, have a mean-
ingful name identifying the relation, and are ac-
companied by suggested context patterns and seed 
instances.  These relations can be input to NELL to 
learn more instances for the relation. We propose 
in the future to integrate this relation generation 
system into NELL, to iteratively extend NELL?s 
initial ontology, providing an ongoing stream of 
new learning tasks.  After every fixed set of 
NELL?s iterations, its growing knowledge base 
would be input to the relation generation system 
which will in turn feed NELL with new relation 
predicates.  One additional area for future research 
is to extend OntExt to discover new categories in 
addition to new relations. 
1454
  
Acknowledgements  
We gratefully acknowledge support for this re-
search from Darpa, Google, Yahoo! and the Brazil-
ian research agency CNPq. We also gratefully 
acknowledge Dr. Madhavi Ganapathiraju (at Uni-
versity of Pittsburgh) for her support and encour-
agement. 
References  
Agichtein, E. and L. Gravano (2000). "Snowball: Ex-
tracting relations from large plain-text collections." 
Procs. of the Fifth ACM International Conference on 
Digital Libraries. 
Banko, M., M. Cararella, et al (2007). "Open infor-
mation extraction from the web." In Procs. of IJCAI. 
Banko, M. and O. Etzioni (2008). "The Tradeoffs Be-
tween Open and Traditional Relation Extraction." In 
Proceedings of ACL-08. 
Callan, J., and Hoy, M. (2009). Clueweb09 data set. 
http://boston.lti.cs.cmu.edu/Data/clueweb09/. 
Carlson, A., J. Betteridge, et al (2009). "Coupling 
Semi-Supervised Learning of Categories and Rela-
tions." Proceedings of the NAACL HLT 2009 Work-
shop on Semi-supervised Learning for Natural Lan-
guage Processing. 
A. Carlson, J. Betteridge, et al (2010). ?Coupled Semi-
Supervised Learning for Information Extraction,? 
Proceedings of the ACM International Conference on 
Web Search and Data Mining (WSDM), 2010. 
A. Carlson, J. Betteridge, et al, (2010b). ?Toward an 
Architecture for Never-Ending Language Learning,? 
Proceedings of the Conference on Artificial Intelli-
gence (AAAI), 2010. 
Etzioni, O., M. Cafarella, et al (2005). "Unsupervised 
named-entity extraction from the web: An experi-
mental study." Artificial Intelligence. 
Hasegawa, T., S. Sekine, et al (2004). "Discovering 
relations among named entities from large corpora." 
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics  
Zhang, M., J. Su, et al (2005). "Discovering Relations 
between Named Entities from a Large Raw Corpus 
Using Tree Similarity-based Clustering." IJCNLP 05. 
Hasegawa, T., S. Sekine, et al (2004). "Discovering 
relations among named entities from large corpora." 
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics  
Zhang, M., J. Su, et al (2005). "Discovering Relations 
between Named Entities from a Large Raw Corpus 
Using Tree Similarity-based Clustering." IJCNL 
Hearst, M. (1992) Automatic Acquisition of Hyponyms 
from Large Text Corpora. Proc. of the Fourteenth In-
ternational Conference on Computational Linguistics, 
Nantes, F 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, Ian H. Witten (2009); 
The WEKA Data Mining Software: An Update; 
SIGKDD Explorations, Volume 11, Issue 1.
 
1455
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 754?765, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Weakly Supervised Training of Semantic Parsers
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
We present a method for training a semantic
parser using only a knowledge base and an un-
labeled text corpus, without any individually
annotated sentences. Our key observation is
that multiple forms of weak supervision can be
combined to train an accurate semantic parser:
semantic supervision from a knowledge base,
and syntactic supervision from dependency-
parsed sentences. We apply our approach
to train a semantic parser that uses 77 rela-
tions from Freebase in its knowledge repre-
sentation. This semantic parser extracts in-
stances of binary relations with state-of-the-
art accuracy, while simultaneously recovering
much richer semantic structures, such as con-
junctions of multiple relations with partially
shared arguments. We demonstrate recovery
of this richer structure by extracting logical
forms from natural language queries against
Freebase. On this task, the trained semantic
parser achieves 80% precision and 56% recall,
despite never having seen an annotated logical
form.
1 Introduction
Semantic parsing converts natural language state-
ments into logical forms in a meaning repre-
sentation language. For example, the phrase
?town in California? might be represented as
?x.CITY(x) ? LOCATEDIN(x,CALIFORNIA), where
CITY, LOCATEDIN and CALIFORNIA are predicates
and entities from a knowledge base. The expressiv-
ity and utility of semantic parsing is derived from
this meaning representation, which is essentially a
program that is directly executable by a computer.
In this sense, broad coverage semantic parsing is the
goal of natural language understanding.
Unfortunately, due to data annotation constraints,
modern semantic parsers only operate in narrow do-
mains. The best performing semantic parsers are
trained using extensive manual annotation: typi-
cally, a number of sentences must be annotated with
their desired logical form. Although other forms of
supervision exist (Clarke et al2010; Liang et al
2011), these methods similarly require annotations
for individual sentences. More automated training
methods are required to produce semantic parsers
with richer meaning representations.
This paper presents an algorithm for training a se-
mantic parser without per-sentence annotations. In-
stead, our approach exploits two easily-obtainable
sources of supervision: a large knowledge base and
(automatically) dependency-parsed sentences. The
semantic parser is trained to identify relation in-
stances from the knowledge base while simulta-
neously producing parses that syntactically agree
with the dependency parses. Combining these two
sources of supervision allows us to train an accurate
semantic parser for any knowledge base without an-
notated training data.
We demonstrate our approach by training a Com-
binatory Categorial Grammar (CCG) (Steedman,
1996) that parses sentences into logical forms con-
taining any of 77 relations from Freebase. Our
training data consists of relation instances from
Freebase and automatically dependency-parsed sen-
tences from a web corpus. The trained semantic
parser extracts binary relations with state-of-the-art
performance, while recovering considerably richer
semantic structure. We demonstrate recovery of this
semantic structure using natural language queries
754
town
N : ?x.CITY(x)
Lex
in
(N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Lex California
N : ?x.x = CALIFORNIA
Lex
N\N : ?g.?x.?y.y = CALIFORNIA ? g(x) ? LOCATEDIN(x, y)
>
N : ?x.?y.y = CALIFORNIA ? CITY(x) ? LOCATEDIN(x, y)
<
Figure 1: An example parse of ?town in California? using the example CCG lexicon. The first stage in parsing
retrieves a category from each word from the lexicon, represented by the ?Lex? entries. The second stage applies CCG
combination rules, in this case both forms of function application, to combine these categories into a semantic parse.
against Freebase. Our weakly-supervised semantic
parser predicts the correct logical form for 56% of
queries, despite never seeing a labeled logical form.
This paper is structured as follows. We first pro-
vide some background information on CCG and the
structure of a knowledge base in Section 2. Section
3 formulates the weakly supervised training prob-
lem for semantic parsers and presents our algorithm.
Section 4 describes how we applied our algorithm to
construct a semantic parser for Freebase, and Sec-
tion 5 presents our results. We conclude with related
work and discussion.
2 Background
2.1 Combinatory Categorial Grammar
Combinatory Categorial grammar (CCG) is a lin-
guistic formalism that represents both the syntax and
semantics of language (Steedman, 1996). CCG is a
lexicalized formalism that encodes all grammatical
information in a lexicon ?. This lexicon contains
syntactic and semantic categories for each word. A
lexicon may include entries such as:
town := N : ?x.CITY(x)
California := N : ?x.x = CALIFORNIA
in := (N\N)/N : ?f.?g.?x.
?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Each entry of the lexicon w := s : l maps a word or
short phrase w to a syntactic category s and a logical
form l. Syntactic categories s may be atomic (N ) or
complex (N\N ). Logical forms l are lambda calcu-
lus expressions constructed using predicates from a
knowledge base. These logical forms combine dur-
ing parsing to form a complete logical form for the
parsed text.
Parses are constructed by combining adjacent cat-
egories using several combination rules, such as for-
ward (>) and backward (<) application:
X/Y : f Y : g =? X : f(g) (>)
Y : g X\Y : f =? X : f(g) (<)
These rules mean that the complex categoryX/Y
(X\Y ) behaves like a function which accepts an ar-
gument of type Y on its right (left) and returns a
value of type X . Parsing amounts to sequentially
applying these two rules, as shown in Figure 1. The
result of parsing is an ordered pair, containing both
a syntactic parse tree and an associated logical form.
We refer to such an ordered pair as a semantic parse,
or by using the letter `.
Given a lexicon, there may be multiple seman-
tic parses ` for a given phrase w. Like context-free
grammars (CFGs), CCGs can be extended to repre-
sent a probability distribution over parses P (`|w; ?)
where ? is a parameter vector.
2.2 Knowledge Base
The main input to our system is a propositional
knowledge base K = (E,R,C,?), containing
entities E, categories C, relations R and relation
instances ?. Categories and relations are pred-
icates which operate on entities and return truth
values; categories c ? C are one-place predi-
cates (CITY(e)) and relations r ? R are two-
place predicates (LOCATEDIN(e1, e2)). Entities e ?
E represent real-world entities and have a set of
known text names. For example, CALIFORNIA
is an entity whose text names include ?Califor-
nia? and ?CA.? Relation instances r(e1, e2) ? ?
are facts asserted by the knowledge base, such
as LOCATEDIN(SACRAMENTO,CALIFORNIA). Ex-
amples of such knowledge bases include Freebase
(Bollacker et al2008), NELL (Carlson et al
2010), and YAGO (Suchanek et al2007).
The knowledge base influences the semantic
parser in two ways. First, CCG logical forms are
constructed by combining categories, relations and
entities from the knowledge base with logical con-
nectives; hence, the predicates in the knowledge
base determine the expressivity of the parser?s se-
mantic representation. Second, the known relation
755
instances r(e1, e2) ? ? are used as weak supervi-
sion to train the semantic parser.
3 Weakly Supervised Semantic Parsing
We define weakly supervised semantic parsing as
the following learning problem.
Input:
1. A knowledge base K = (E,R,C,?), as de-
fined above.
2. A corpus of dependency-parsed sentences S.
3. A CCG lexicon ? that produces logical forms
containing predicates from K. Section 4.1 de-
scribes an approach to generate this lexicon.
4. A procedure for identifying mentions of enti-
ties from K in sentences from S. (e.g., simple
string matching).
Output:
1. Parameters ? for the CCG that produce correct
semantic parses ` for sentences s ? S.
This problem is ill-posed without additional as-
sumptions: since the correct logical form for a sen-
tence is never observed, there is no a priori reason
to prefer one semantic parse to another. Our train-
ing algorithm makes two assumptions about correct
semantic parses, which are encoded as weak super-
vision constraints. These constraints make learning
possible by adding an inductive bias:
1. Every relation instance r(e1, e2) ? ? is ex-
pressed by at least one sentence in S (Riedel
et al2010; Hoffmann et al2011).
2. The correct semantic parse of a sentence s con-
tains a subset of the syntactic dependencies
contained in a dependency parse of s.
Our weakly supervised training uses these con-
straints as a proxy for labeled semantic parses. The
training algorithm has two steps. First, the algo-
rithm constructs a graphical model that contains
both the semantic parser and constant factors en-
coding the above two constraints. This graphical
model is then used to estimate parameters ? for the
semantic parser, essentially optimizing ? to produce
parses that satisfy the weak supervision constraints.
If our assumptions are correct and sufficiently con-
strain the parameter space, then this procedure will
identify parameters for an accurate semantic parser.
3.1 Encoding the Weak Supervision
Constraints
The first step of training constructs a graphical
model containing the semantic parser and two weak
supervision constraints. However, the first weak su-
pervision constraint couples the semantic parses for
every sentence s ? S. Such coupling would result in
an undesirably large graphical model. We therefore
modify this constraint to enforce that every relation
r(e1, e2) is expressed at least once in S(e1,e2) ? S,
the subset of sentences which mention both e1 and
e2. These mentions are detected using the provided
mention-identification procedure.
Figure 2 depicts the graphical model constructed
for training. The semantic constraint couples the ex-
tractions for all sentences S(e1,e2), so the graphical
model is instantiated once per (e1, e2) tuple. The
model has 4 types of random variables and values:
Si = si represents a sentence, Li = `i represents
a semantic parse, Zi = zi represents the satisfac-
tion of the syntactic constraint and Yr = yr repre-
sents the truth value of relation r. Si, Li and Zi are
replicated once for each sentence s ? S(e1,e2), while
Yr is replicated once for each relation type r in the
knowledge base (all r ? R).
For each entity pair (e1, e2), this graphical model
defines a conditional distribution over L,Y,Z given
S. This distribution factorizes as:
p(Y = y,Z = z,L = `|S = s; ?) =
1
Zs
?
r
?(yr, `)
?
i
?(zi, `i, si)?(si, `i; ?)
The factorization contains three replicated fac-
tors. ? represents the semantic parser, which is
parametrized by ? and produces a semantic parse
`i for each sentence si. ? and ? are deterministic
factors representing the two weak supervision con-
straints. We now describe each factor in more detail.
Semantic Parser
The factor ? represents the semantic parser, which
is a log-linear probabilistic CCG using the input lex-
icon ?. Given a sentence s and parameters ?, the
parser defines an unnormalized probability distribu-
tion over semantic parses `, each of which includes
both a syntactic CCG parse tree and logical form.
756
YlocatedIn
?
Y
acquired
?
Y
capitalOf
?
L
1
S
1
?
Z
1
?
L
2
S
2
?
Z
2
?
Figure 2: Factor graph containing the semantic parser
? and weak supervision constraints ? and ?, instanti-
ated for an (e1, e2) tuple occurring in 2 sentences S1 and
S2, with corresponding semantic parses L1 and L2. The
knowledge base contains 3 relations, represented by the
Y variables.
Let f(`, s) represent a feature function mapping se-
mantic parses to vectors of feature values1. The fac-
tor ? is then defined as:
?(s, `; ?) = exp{?T f(`, s)}
If the features f(`, s) factorize according to the
structure of the CCG parse tree, it is possible to
perform exact inference using a CKY-style dynamic
programming algorithm. However, other aspects of
the graphical model preclude exact inference, so we
perform approximate inference using beam search.
Inference is explained in more detail in Section 3.2.
Semantic Constraint
The semantic constraint states that, given an entity
tuple (e1, e2), every relation instance r(e1, e2) ? ?
must be expressed somewhere in S(e1,e2). Further-
more, no semantic parse can express a relation in-
stance which is not in the knowledge base. This con-
straint is identical to the multiple deterministic-OR
constraint used by Hoffmann et al2011) to train a
sentential relation extractor.
The graphical model contains a semantic con-
straint factor ? and one binary variable Yr for each
relation r in the knowledge base. Yr represents
whether r(e1, e2) is expressed by any sentence in
S(e1,e2). The ? factor determines whether each se-
mantic parse in ` extracts a relation between e1 and
e2. It then aggregates these sentence-level extrac-
tions using a deterministic OR: if any sentence ex-
tracts r(e1, e2) then Yr = 1. Otherwise, Yr = 0.
1Section 4.3 describes the features used by our semantic
parser for Freebase.
?(Yr, `) =
1 if Yr = 1 ? ?i.EXTRACTS(`i, r, e1, e2)
1 if Yr = 0 ? 6 ?i.EXTRACTS(`i, r, e1, e2)
0 otherwise
The EXTRACTS function determines the relation
instances that are asserted by a semantic parse `.
EXTRACTS(`, r, e1, e2) is true if ` asserts the rela-
tion r(e1, e2) and false otherwise. This function es-
sentially converts the semantic parser into a senten-
tial relation extractor, and its implementation may
depend on the types of logical connectives included
in the lexicon ?. Logical forms in our Freebase se-
mantic parser consist of conjunctions of predicates
from the knowledge base; we therefore define EX-
TRACTS(`, r, e1, e2) as true if `?s logical form con-
tains the clauses r(x, y), x = e1 and y = e2.
Syntactic Constraint
A problem with the semantic constraint is that it
admits a large number of ungrammatical parses. The
syntactic constraint penalizes ungrammatical parses
by encouraging the semantic parser to produce parse
trees that agree with a dependency parse of the same
sentence. Specifically, the syntactic constraint re-
quires the predicate-argument structure of the CCG
parse to agree with the predicate-argument structure
of the dependency parse.
Agreement is defined as a function of each CCG
rule application in `. In the parse tree `, each rule
application combines two subtrees, `h and `c, into a
single tree spanning a larger portion of the sentence.
A rule application is consistent with a dependency
parse t if the head words of `h and `c have a depen-
dency edge between them in t. AGREE(`, t) is true
if and only if every rule application in ` is consistent
with t. This syntactic constraint is encoded in the
graphical model by the ? factors and Z variables:
?(z, `, s) = 1 if z = AGREE(`,DEPPARSE(s))
0 otherwise
3.2 Parameter Estimation
To train the model, a single training example is con-
structed for every tuple of entities (e1, e2). The in-
put to the model is s = S(e1,e2), the set of sentences
757
containing e1 and e2. The weak supervision vari-
ables, y, z, are the output of the model. y is con-
structed by setting yr = 1 if r(e1, e2) ? ?, and 0
otherwise. This setting trains the semantic parser to
extract every true relation instance between (e1, e2)
from some sentence in S(e1,e2), while simultane-
ously avoiding incorrect instances. Finally, z = 1,
to encourage agreement between the semantic and
dependency parses. The training data for the model
is therefore a collection, {(sj , yj , zj)}nj=1, where j
indexes entity tuples (e1, e2).
Training optimizes the semantic parser parame-
ters ? to predict Y = yj ,Z = zj given S = sj . The
parameters ? are estimated by running the structured
perceptron algorithm (Collins, 2002) on the training
data defined above. The structured perceptron al-
gorithm iteratively applies a simple update rule for
each example (sj , yj , zj) in the training data:
`predicted ? arg max
`
max
y,z
p(`, y, z|sj ; ?t)
`actual ? arg max
`
p(`|yj , zj , sj ; ?t)
?
t+1 ? ?t +
?
i
f(`
actual
i , si)
?
?
i
f(`
predicted
i , si)
Each iteration of training requires solving two
maximization problems. The first maximization,
max`,y,z p(`, y, z|s; ?t), is straightforward because y
and z are deterministic functions of `. Therefore,
it is solved by finding the maximum probability as-
signment `, then choosing values for y and z that
satisfy the weak supervision constraints.
The second maximization, max` p(`|y, z, s; ?t), is
more challenging. When y and z are given, the infer-
ence procedure must restrict its search to the parses
` which satisfy these weak supervision constraints.
The original formulation of the ? factors permitted
tractable inference (Hoffmann et al2011), but the
EXTRACTS function and the ? factors preclude ef-
ficient inference. We approximate this maximiza-
tion using beam search over CCG parses `. For each
sentence s, we perform a beam search to produce
k = 300 possible semantic parses. We then check
the value of ? for each generated parse and elimi-
nate parses which do not satisfy this syntactic con-
straint. Finally, we apply EXTRACTS to each parse,
then use the greedy approximate inference proce-
dure from Hoffmann et al2011) for the ? factors.
4 Building a Grammar for Freebase
We apply the training algorithm from the previous
section to produce a semantic parser for a subset of
Freebase. This section describes details of the gram-
mar we construct for this task, including the con-
struction of the lexicon ?, some extensions to the
CCG parser, and the features used during training.
In this section, we assume access to a knowledge
base K = (E,C,R,?), a corpus of dependency-
parsed sentences S and a procedure for identifying
mentions of entities in sentences.
4.1 Constructing the Lexicon ?
The first step in constructing the semantic parser
is defining a lexicon ?. We construct ? by ap-
plying simple dependency-parse-based heuristics to
sentences in the training corpus. The resulting lex-
icon ? captures a variety of linguistic phenomena,
including verbs, common nouns (?city?), noun com-
pounds (?California city?) and prepositional modi-
fiers (?city in California?).
The first step in lexicon construction is to use the
mention identification procedure to identify all men-
tions of entities in the sentences S. This process
results in (e1, e2, s) triples, consisting of sentences
with two entity mentions. The dependency path be-
tween e1 and e2 in s is then matched against the de-
pendency parse patterns in Table 1. Each matched
pattern adds one or more lexical entries to ?
Each pattern in Table 1 has a corresponding lexi-
cal category template, which is a CCG lexical cate-
gory containing parameters e, c and r that are chosen
at initialization time. Given the triple (e1, e2, s), re-
lations r are chosen such that r(e1, e2) ? ?, and
categories c are chosen such that c(e1) ? ? or
c(e2) ? ?. The template is then instantiated with
every combination of these e, c and r values.
After instantiating lexical categories for each sen-
tence in S, we prune infrequent lexical categories to
improve parser efficiency. This pruning step is re-
quired because the common noun pattern generates
a large number of lexical categories, the majority
of which are incorrect. Therefore, we eliminate all
common noun categories instantiated by fewer than
758
Part of
Dependency Parse Pattern Lexical Category Template
Speech
Proper (name of entity e) w :=N : ?x.x = e
Noun Sacramento Sacramento :=N : ?x.x = SACRAMENTO
Common e1
SBJ
===? [is, are, was, ...]
OBJ
?=== w w :=N : ?x.c(x)
Noun Sacramento is the capital capital :=N : ?x.CITY(x)
Noun e1
NMOD
?===== e2 Type change N : ?x.c(x) to N |N : ?f.?x.?y.c(x) ? f(y) ? r(x, y)
Modifier Sacramento, California N : ?x.CITY(x) to N |N : ?f.?x.?y.CITY(x) ? f(y) ? LOCATEDIN(x, y)
Preposition
e1
NMOD
?===== w
PMOD
?===== e2 w := (N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? r(x, y)
Sacramento in California in := (N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
SBJ
===? VB*
ADV
?=== w
PMOD
?===== e2 w := PP/N : ?f.?x.f(x)
Sacramento is located in California in := PP/N : ?f.?x.f(x)
Verb
e1
SBJ
===? w*
OBJ
?=== e2 w* := (S\N)/N : ?f.?g.?x, y.f(y) ? g(x) ? r(x, y)
Sacramento governs California governs := (S\N)/N : ?f.?g.?x, y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
SBJ
===? w*
ADV
?=== [IN,TO]
PMOD
?===== e2 w* := (S\N)/PP : ?f.?g.?x, y.f(y) ? g(x) ? r(x, y)
Sacramento is located in California is located := (S\N)/PP : ?f.?g.?x, y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
NMOD
?===== w*
ADV
?=== [IN,TO]
PMOD
?===== e2 w* := (N\N)/PP : ?f.?g.?y.f(y) ? g(x) ? r(x, y)
Sacramento located in California located := (N\N)/PP : ?f.?g.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Forms of
(none) w* := (S\N)/N : ?f.?g.?x.g(x) ? f(x)
?to be?
Table 1: Dependency parse patterns used to instantiate lexical categories for the semantic parser lexicon ?. Each
pattern is followed by an example phrase that instantiates it. An * indicates a position that may be filled by multiple
consecutive words in the sentence. e1 and e2 are the entities identified in the sentence, r represents a relation where
r(e1, e2), and c represents a category where c(e1). Each template may be instantiated with multiple values for the
variables e, c, r.
5 sentences in S. The other rules are less fertile, so
we do not need to prune their output.
In addition to these categories, the grammar in-
cludes type-changing rules from N to N |N . These
rules capture noun compounds by allowing nouns to
become functions from nouns to nouns. There are
several such type-changing rules since the resulting
category includes a hidden relation r between the
noun and its modifier (see Table 1). As with lexical
categories, the set of type changing rules included
in the grammar is determined by matching depen-
dency parse patterns to the training data. Similar
rules for noun compounds are used in other CCG
parsers (Clark and Curran, 2007).
The instantiated lexicon represents the semantics
of words and phrases as conjunctions of predicates
from the knowledge base, possibly including exis-
tentially quantified variables and ? expressions. The
syntactic types N and PP are semantically rep-
resented as functions from entities to truth values
(e.g., ?x.CITY(x)), while sentences S are statements
with no ? terms, such as ?x, y.x = CALIFORNIA ?
CITY(y) ? LOCATEDIN(x, y). Variables in the seman-
tic representation (x, y) range over entities from the
knowledge base. Intuitively, the N and PP cate-
gories represent sets of entities, while sentences rep-
resent assertions about the world.
4.2 Extensions to CCG
The semantic parser is trained using sentences from
a web corpus, which contains many out-of-domain
words. As a consequence, many of the words en-
countered during training cannot be represented us-
ing the vocabulary of predicates from the knowl-
edge base. To handle these extraneous words, we
allow the CCG parser to skip words while parsing
a sentence. During parsing, the parser first decides
whether to retrieve a lexical category for each word
in the sentence. The sentence is then parsed as if
only the retrieved lexical categories existed.
4.3 Features
The features f(`, s) for our probabilistic CCG con-
tain two sets of features. The first set contains lexi-
cal features, which count the number of times each
lexical entry is used in `. The second set contains
rule application features, which count the number
of times each combination rule is applied to each
possible set of arguments. An argument is defined
by its syntactic and semantic category, and in some
cases by the lexical entry which created it. We lex-
759
icalize arguments for prepositional phrases PP and
common nouns (initialized by the second rule in Ta-
ble 1). This lexicalization allows the parser to dis-
tinguish between prepositional phrases headed by
different prepositions, as well as between different
common nouns. All other types are distinguished
solely by syntactic and semantic category.
5 Evaluation
In this section, we evaluate the performance of
a semantic parser for Freebase, trained using our
weakly-supervised algorithm. Empirical compari-
son is somewhat difficult because the most compara-
ble previous work ? weakly-supervised relation ex-
traction ? uses a shallower semantic representation.
Our evaluation therefore has two components: (1) a
binary relation extraction task, to demonstrate that
the trained semantic parser extracts instances of bi-
nary relations with performance comparable to other
state-of-the-art systems, and (2) a natural language
database query task, to demonstrate the parser?s abil-
ity to extract more complex logical forms than bi-
nary relation instances, such as logical expressions
involving conjunctions of multiple categories and re-
lations with partially shared arguments.
5.1 Corpus Construction
Our experiments use a subset of 77 relations2 from
Freebase3 as the knowledge base and a corpus of
web sentences. We constructed the sentence corpus
by first sampling sentences from a web crawl and
parsing them with MaltParser (Nivre et al2006).
Long sentences tended to have noisy parses while
also rarely expressing relations, so we discarded
sentences longer than 10 words. Entities were iden-
tified by performing a simple string match between
canonical entity names in Freebase and proper noun
phrases identified by the parser. In cases where a
single noun phrase matched multiple entities, we se-
lected the entity participating in the most relations.
The resulting corpus contains 2.5 million (e1, e2, s)
triples, from which we reserved 10% for validation
and 10% for testing. The validation set was used
to estimate performance during algorithm develop-
2These relations are defined by a set of MQL queries and
potentially traverse multiple relation links.
3http://www.freebase.com
Relation Name
Relation
Sentences
Instances
CITYLOCATEDINSTATE 2951 13422
CITYLOCATEDINCOUNTRY 1696 7904
CITYOFPERSONBIRTH 397 440
COMPANIESHEADQUARTEREDHERE 326 432
MUSICARTISTMUSICIAN 251 291
CITYUNIVERSITIES 239 338
CITYCAPITALOFCOUNTRY 123 2529
HASHUSBAND 103 367
PARENTOFPERSON 85 356
HASSPOUSE 81 461
Table 2: Occurrence statistics for the 10 most frequent
relations in the training data. ?Relation Instances? shows
the number of entity tuples (e1, e2) that appear as positive
examples for each relation, and ?Sentences? shows the
total number of sentences in which these tuples appear.
ment, while the test set was used to generate the fi-
nal experimental results. All triples for each (e1, e2)
tuple were placed in the same set.
Approximately 1% of the resulting (e1, e2, s)
triples are positive examples, meaning there exists
some relation r where r(e1, e2) ? ?4. To improve
training efficiency and prediction performance, we
subsample 5% of the negative examples for training,
producing a training set of 125k sentences with 27k
positive examples. The validation and test sets retain
the original positive/negative ratio. Table 2 shows
some statistics of the most frequent relations in the
test set.
5.2 Relation Extraction
The first experiment measures the semantic parser?s
ability to extract relations from sentences in our web
corpus. We compare our semantic parser to MUL-
TIR (Hoffmann et al2011), which is a state-of-
the-art weakly supervised relation extractor. This
method uses the same weak supervision constraint
and parameter estimation procedure, but replaces the
semantic parser by a linear classifier. The features
for this classifier include the dependency path be-
tween the entity mentions, the type of each mention,
and the intervening context (Mintz et al2009).
Both the semantic parser and MULTIR were
trained by running 5 iterations of the structured per-
4Note that the positive/negative ratio was much lower with-
out the length filter or entity disambiguation, which is partly
why filtering was performed.
760
MULTIR
PARSE+DEP
PARSE
PARSE-DEP
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
Figure 3: Aggregate precision as a function of recall, for
MULTIR (Hoffman et al2011) and our three semantic
parser variants.
MULTIR
PARSE+DEP
PARSE
PARSE-DEP
0 600 1200 1800 2400 3000
0
0.2
0.4
0.6
0.8
1.0
Figure 4: Sentential precision as a function of the ex-
pected number of correct extractions for MULTIR (Hoff-
man et al2011) and our three semantic parser variants.
ceptron algorithm5. At test time, both models pre-
dicted a relation r ? R or NONE for each (e1, e2, s)
triple in the test set. The parser parses the sen-
tence without considering the entities marked in the
sentence, then applies the EXTRACTS function de-
fined in Section 3.1 to identify a relation between e1
and e2. We compare three versions of the semantic
parser: PARSE, which is the basic semantic parser,
PARSE+DEP which additionally observes the cor-
rect dependency parse at test time, and PARSE-DEP
which is trained without the syntactic constraint.
Note that MULTIR uses the sentence?s dependency
parse to construct its feature vector.
Our evaluation considers two performance mea-
sures: aggregate and sentential precision/recall. Ag-
gregate precision takes the union of all extracted re-
lation instances r(e1, e2) from the test corpus and
compares these instances to Freebase. To pro-
5The structured perceptron algorithm does not converge to a
parameter estimate, and we empirically found that performance
did not improve beyond 5 iterations.
MULTIR
PARSE+DEP
PARSE
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
Figure 5: Aggregate precision as a function of recall,
ignoring the two most frequent relations, CITYLOCATE-
DINSTATE and CITYLOCATEDINCOUNTRY.
duce a precision/recall curve, each extracted in-
stance r(e1, e2) is assigned the maximum score over
all sentences which extracted it. This metric is easy
to compute, but may be inaccurate due to inaccura-
cies and missing relations in Freebase.
Sentential precision computes the precision of ex-
tractions on individual (e1, e2, s) tuples. This met-
ric is evaluated by manually sampling and evaluat-
ing 100 test sentences from which a relation was ex-
tracted per model. Unfortunately, it is difficult to
compute recall for this metric, since the true number
of sentences expressing relations is unknown. We
instead report precision as a function of the expected
number of correct extractions, which is directly pro-
portional to recall.
Figure 3 displays aggregate precision/recall and
Figure 4 displays sentential precision/recall for all
4 models. Generally, PARSE behaves like MUL-
TIR with somewhat lower recall. In the sentential
evaluation, PARSE+DEP outperforms both PARSE
and MULTIR. The difference between PARSE+DEP?s
aggregate and sentential precision stems from the
fact that PARSE+DEP extracts each relation instance
from more sentences than either MULTIR or PARSE.
PARSE-DEP has the worst performance in both eval-
uations, suggesting the importance of syntactic su-
pervision. Precision in the aggregate experiment is
low partially due to examples with incorrect entity
disambiguation.
We found that the skewed distribution of relation
types hides interesting differences between the mod-
els. Therefore, we include Figure 5 comparing our
syntactically-supervised parsers to MULTIR, ignor-
ing the two most frequent relations (which together
761
make up over half of all relation instances). Both
PARSE and PARSE+DEP are considerably more pre-
cise than MULTIR on these less frequent relations
because their compositional meaning representation
shares parameter strength between relations. For
example, the semantic parsers learn that ?in? often
combines with a city to form a prepositional phrase;
the parsers can apply this knowledge to identify city
arguments of any relation. However, MULTIR is ca-
pable of higher recall, since its dependency parse
features can represent syntactic dependencies that
cannot be represented by our semantic parsers. This
limitation is a consequence of our heuristic lexicon
initialization procedure, and could be rectified by a
more flexible initialization procedure.
5.3 Natural Language Database Queries
The second experiment measures our trained
parser?s ability to correctly translate natural lan-
guage queries into logical queries against Freebase.
To avoid biasing the evaluation, we constructed
a test corpus of natural language queries in a data-
driven fashion. We searched the test data for sen-
tences with two related entities separated by an ?is
a? expression. The portion of the sentence before the
?is a? expression was discarded and the remainder
retained as a candidate query. For example ?Jesse is
an author from Austin, Texas,? was converted into
the candidate query ?author from Austin, Texas.?
Each candidate query was then annotated with a log-
ical form using categories and relations from the
knowledge base; candidate queries without satisfac-
tory logical forms were discarded. We annotated 50
validation and 50 test queries in this fashion. The
validation set was used to estimate performance dur-
ing algorithm development and the test set was used
to generate the final results. Example queries with
their annotated logical forms are shown in Table 3.
Table 4 displays the results of the query evalua-
tion. For this evaluation, we forced the parser to in-
clude every word of the query in the parse. Precision
is the percentage of successfully parsed queries for
which the correct logical form was predicted. Re-
call is the percentage of all queries for which the
correct logical form was predicted. This evalua-
tion demonstrates that the semantic parser success-
fully interprets common nouns and identifies mul-
tiple relations with shared arguments. The perfor-
Example Query Logical Form
capital of Russia ?x.CITYCAPITALOFCOUNTRY(x, RUSSIA)
wife of Abraham ?x.HASHUSBAND(x,ABRAHAM)
vocalist from ?x.MUSICIAN(x)?
London, England PERSONBORNIN(x, LONDON)?
CITYINCOUNTRY(LONDON, ENGLAND)
home of ?x.HEADQUARTERS(CONOCOPHILLIPS, x)
ConocoPhillips ?CITYINCOUNTRY(x, CANADA)
in Canada
Table 3: Example natural language queries and their cor-
rect annotated logical form.
Precision Recall
PARSE 0.80 0.56
PARSE-DEP 0.45 0.32
Table 4: Precision and recall for predicting logical forms
of natural language queries against Freebase. The table
compares PARSE, trained with syntactic supervision to
PARSE-DEP, trained without syntactic supervision.
mance difference between PARSE and PARSE-DEP
also demonstrates the benefit of including syntactic
supervision.
Examining the system output, we find two ma-
jor sources of error. The first is missing lexical cat-
egories for uncommon words (e.g., ?ex-guitarist?),
which negatively impact recall by making some
queries unparsable. The second is difficulty distin-
guishing between relations with similar type signa-
tures, such as CITYLOCATEDINCOUNTRY and CITY-
CAPITALOFCOUNTRY.
6 Related Work
There are many approaches to supervised seman-
tic parsing, including inductive logic programming
(Zelle and Mooney, 1996), probabilistic and syn-
chronous grammars (Ge and Mooney, 2005; Wong
and Mooney, 2006; Wong and Mooney, 2007; Lu et
al., 2008), and automatically learned transformation
rules (Kate et al2005). This work most closely
follows the work on semantic parsing using CCG
(Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007; Kwiatkowski et al2010). These su-
pervised systems are all trained with annotated sen-
tence/logical form pairs; hence these approaches are
labor intensive and do not scale to broad domains
with large numbers of predicates.
Several recent papers have attempted to reduce
the amount of human supervision required to train
762
a semantic parser. One line of work eliminates the
need for an annotated logical form, instead using
only the correct answer for a database query (Liang
et al2011) or even a binary correct/incorrect sig-
nal (Clarke et al2010). This type of feedback may
be easier to obtain than full logical forms, but still
requires individually annotated sentences. Other ap-
proaches are completely unsupervised, but do not tie
the language to an existing meaning representation
(Poon and Domingos, 2009). It is also possible to
self-train a semantic parser without any labeled data
(Goldwasser et al2011). However, this approach
does not perform as well as more supervised ap-
proaches, since the parser?s self-training predictions
are not constrained by the correct logical form.
Recent research has produced several weakly su-
pervised relation extractors (Craven and Kumlien,
1999; Mintz et al2009; Wu and Weld, 2010; Riedel
et al2010; Hoffmann et al2011). These sys-
tems scale up to hundreds of predicates, but have
much shallower semantic representations than se-
mantic parsers. For example, these systems can-
not be directly used to respond to natural language
queries. This work extends weakly supervised rela-
tion extraction to produce richer semantic structure,
using only slightly more supervision in the form of
dependency parses.
7 Discussion
This paper presents a method for training a seman-
tic parser using only a knowledge base and a cor-
pus of unlabeled sentences. Our key observation is
that multiple forms of weak supervision can be com-
bined to train an accurate semantic parser: semantic
supervision from a knowledge base of facts, and syn-
tactic supervision in the form of a standard depen-
dency parser. We presented an algorithm for train-
ing a semantic parser in the form of a probabilistic
Combinatory Categorial Grammar, using these two
types of weak supervision. We used this algorithm
to train a semantic parser for an ontology of 77 Free-
base predicates, using Freebase itself as the weak se-
mantic supervision.
Experimental results show that our trained se-
mantic parser extracts binary relations as well as
a state-of-the-art weakly supervised relation extrac-
tor (Hoffmann et al2011). Further experiments
tested our trained parser?s ability to extract more
complex meanings from sentences, including logi-
cal forms involving conjunctions of multiple relation
and category predicates with shared arguments (e.g.,
?x.MUSICIAN(x) ? PERSONBORNIN(x, LONDON) ?
CITYINCOUNTRY(LONDON, ENGLAND)). To test this
capability, we applied the trained parser to natural
language queries against Freebase. The semantic
parser correctly interpreted 56% of these queries,
despite the broad domain and never having seen an
annotated logical form. Together, these two experi-
mental analyses suggest that the combination of syn-
tactic and semantic weak supervision is indeed a suf-
ficient basis for training semantic parsers for a di-
verse range of corpora and predicate ontologies.
One limitation of our method is the reliance on
hand-built dependency parse patterns for lexicon ini-
tialization. Although these patterns capture a va-
riety of linguistic phenomena, they require manual
engineering and may miss important relations. An
area for future work is developing an automated
way to produce this lexicon, perhaps by extend-
ing the recent work on automatic lexicon generation
(Kwiatkowski et al2010) to the weakly supervised
setting. Such an algorithm seems especially impor-
tant if one wishes to model phenomena such as ad-
jectives, which are difficult to initialize heuristically
without generating large numbers of lexical entries.
An elegant aspect of semantic parsing is that it is
easily extensible to include more complex linguis-
tic phenomena, such as quantification and events
(multi-argument relations). In the future, we plan
to increase the expressivity of our parser?s mean-
ing representation to capture more linguistic and se-
mantic phenomena. In this fashion, we can make
progress toward broad coverage semantic parsing,
and thus natural language understanding.
Acknowledgments
This research has been supported in part by DARPA
under contract number FA8750-09-C-0179, and by a
grant from Google. Additionally, we thank Yahoo!
for use of their M45 cluster. We also gratefully ac-
knowledge the contributions of our colleagues on the
NELL project, Justin Betteridge for collecting the
Freebase relations, Jamie Callan and colleagues for
the web crawl, and Thomas Kollar and Matt Gardner
for helpful comments on earlier drafts of this paper.
763
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management of
Data, pages 1247?1250.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intelli-
gence.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Ruifang Ge and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics.
In Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In The 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proceedings, The Twentieth National Con-
ference on Artificial Intelligence and the Seventeenth
Innovative Applications of Artificial Intelligence Con-
ference.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Proceedings of the Association for Computational
Linguistics, Portland, Oregon. Association for Com-
putational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the 2010 European
conference on Machine learning and Knowledge Dis-
covery in Databases.
Mark Steedman. 1996. Surface Structure and Interpre-
tation. The MIT Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international confer-
ence on World Wide Web, WWW ?07, pages 697?706,
New York, NY, USA. ACM.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of the 48th
764
Annual Meeting of the Association for Computational
Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the thirteenth national
conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: structured clas-
sification with probabilistic categorial grammars. In
UAI ?05, Proceedings of the 21st Conference in Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
765
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 833?838,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Learning and Inference in a Large Knowledge-base
using Latent Syntactic Cues
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{mg1,ppt,bkisiel,tom.mitchell}@cs.cmu.edu
Abstract
Automatically constructed Knowledge Bases
(KBs) are often incomplete and there is a gen-
uine need to improve their coverage. Path
Ranking Algorithm (PRA) is a recently pro-
posed method which aims to improve KB cov-
erage by performing inference directly over
the KB graph. For the first time, we demon-
strate that addition of edges labeled with la-
tent features mined from a large dependency
parsed corpus of 500 million Web documents
can significantly outperform previous PRA-
based approaches on the KB inference task.
We present extensive experimental results val-
idating this finding. The resources presented
in this paper are publicly available.
1 Introduction
Over the last few years, several large scale Knowl-
edge Bases (KBs) such as Freebase (Bollacker et
al., 2008), NELL (Carlson et al, 2010), and YAGO
(Suchanek et al, 2007) have been developed. Each
such KB consists of millions of facts (e.g., (Tiger
Woods, playsSport, Golf )) spanning over multiple
relations. Unfortunately, these KBs are often incom-
plete and there is a need to increase their coverage of
facts to make them useful in practical applications.
A strategy to increase coverage might be to per-
form inference directly over the KB represented as a
graph. For example, if the KB contained the follow-
ing facts, (Tiger Woods, participatesIn, PGA Tour))
and (Golf, sportOfTournament, PGA Tour), then by
putting these two facts together, we could potentially
infer that (Tiger Woods, playsSport, Golf ). The
Figure 1: Example demonstrating how lexicalized syn-
tactic edges can improve connectivity in the KB enabling
PRA (Lao and Cohen, 2010) to discover relationships be-
tween Alex Rodriguez and World Series. Edges with la-
tent labels can improve inference performance by reduc-
ing data sparsity. See Section 1.1 for details.
recently proposed Path Ranking Algorithm (PRA)
(Lao and Cohen, 2010) performs such inference by
automatically learning semantic inference rules over
the KB (Lao et al, 2011). PRA uses features based
off of sequences of edge types, e.g., ?playsSport,
sportOfTournament?, to predict missing facts in the
KB.
PRA was extended by (Lao et al, 2012) to per-
form inference over a KB augmented with depen-
dency parsed sentences. While this opens up the
possibility of learning syntactic-semantic inference
rules, the set of syntactic edge labels used are
just the unlexicalized dependency role labels (e.g.,
nobj, dobj, etc., without the corresponding words),
thereby limiting overall expressitivity of the learned
inference rules. To overcome this limitation, in this
paper we augment the KB graph by adding edges
with more expressive lexicalized syntactic labels
(where the labels are words instead of dependen-
833
cies). These additional edges, e.g., (Alex Rodriguez,
?plays for?, NY Yankees), are mined by extracting
600 million Subject-Verb-Object (SVO) triples from
a large corpus of 500m dependency parsed docu-
ments, which would have been prohibitively expen-
sive to add directly as in (Lao et al, 2012). In order
to overcome the explosion of path features and data
sparsity, we derive edge labels by learning latent em-
beddings of the lexicalized edges. Through exten-
sive experiments on real world datasets, we demon-
strate effectiveness of the proposed approach.
1.1 Motivating Example
In Figure 1, the KB graph (only solid edges) is dis-
connected, thereby making it impossible for PRA to
discover any relationship between Alex Rodriguez
and World Series. However, addition of the two
edges with SVO-based lexicalized syntactic edges
(e.g., (Alex Rodriguez, plays for, NY Yankees)) re-
stores this inference possibility. For example, PRA
might use the edge sequence ??plays for?, team-
PlaysIn? as evidence for predicting the relation in-
stance (Alex Rodriguez, athleteWonChampionship,
World Series). Unfortunately, such na??ve addition
of lexicalized edges may result in significant data
sparsity, which can be overcome by mapping lexi-
calized edge labels to some latent embedding (e.g.,
(Alex Rodriguez, LatentFeat#5, NY Yankees) and
running PRA over this augmented graph. Using la-
tent embeddings, PRA could then use the following
edge sequence as a feature in its prediction models:
?LatentFeat#5, teamPlaysIn?. We find this strategy
to be very effective as described in Section 4.
2 Related Work
There is a long history of methods using suface-level
lexical patterns for extracting relational facts from
text corpora (Hearst, 1992; Brin, 1999; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002;
Etzioni et al, 2004). Syntactic information in the
form of dependency paths have been explored in
(Snow et al, 2006; Suchanek et al, 2006). A
method of latent embedding of relation instances
for sentence-level relation extraction was shown in
(Wang et al, 2011). However, none of this prior
work makes explicit use of the background KBs as
we explore in this paper.
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010) has been used previously to perform inference
over graph-structured KBs (Lao et al, 2011), and to
learn formation of online communities (Settles and
Dow, 2013). In (Lao et al, 2012), PRA is extended
to perform inference over a KB using syntactic in-
formation from parsed text. In contrast to these pre-
vious PRA-based approaches where all edge labels
are either KB labels or at surface-level, in this pa-
per we explore using latent edge labels in addition
to surface-level labels in the graph over which PRA
is applied. In particular, we focus on the problem of
performing inference over a large KB and learn la-
tent edge labels by mining dependency syntax statis-
tics from a large text corpus.
Though we use Principal Components Analysis
(PCA) for dimensionality reduction for the experi-
ments in this paper, this is by no means the only
choice. Various other dimensionality reduction tech-
niques, and in particular, other verb clustering tech-
niques (Korhonen et al, 2003), may also be used.
OpenIE systems such as Reverb (Etzioni et al,
2011) also extract verb-anchored dependency triples
from large text corpus. In contrast to such ap-
proaches, we focus on how latent embedding of
verbs in such triples can be combined with explicit
background knowledge to improve coverage of ex-
isting KBs. This has the added capability of infer-
ring facts which are not explicitly mentioned in text.
The recently proposed Universal Schema (Riedel
et al, 2013) also demonstrates the benefit of us-
ing latent features for increasing coverage of KBs.
Key differences between that approach and ours in-
clude our use of syntactic information as opposed to
surface-level patterns in theirs, and also the ability
of the proposed PRA-based method to generate use-
ful inference rules which is beyond the capability of
the matrix factorization approach in (Riedel et al,
2013).
3 Method
3.1 Path Ranking Algorithm (PRA)
In this section, we present a brief overview of the
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010), building on the notations in (Lao et al, 2012).
Let G = (V,E, T ) be the graph, where V is the set
of vertices, E is the set of edges, and T is the set of
edge types. For each edge (v1, t, v2) ? E, we have
834
v1, v2 ? V and t ? T . LetR ? T be the set of types
predicted by PRA. R could in principal equal T , but
in this paper we restrict prediction to KB relations,
while T also includes types derived from surface text
and latent embeddings. Let pi = ?t1, t2, . . . , tw? be
a path type of length w over graph G, where ti ? T
is the type of the ith edge in the path. Each such
path type is also a feature in the PRA model. For
a given source and target node pair s, t ? V , let
P (s ? t;pi) be the value of the feature pi specify-
ing the probability of reaching node t starting from
node s and following a path constrained by path type
pi. We approximate these probabilities using random
walks. A value of 0 indicates unreachability from s
to t using path type pi.
Let B = {pi1, . . . , pim} be the set of all features
(path types). The score that relation r holds between
node s and node t is given by the following function:
ScorePRA(s, t, r) =
?
pi?B
P (s? t;pi) ?rpi
where ?rpi is the weight of feature pi in class r ? R.
Feature Selection: The set B of possible path
types grows exponentially in the length of the paths
that are considered. In order to have a manageable
set of features to compute, we first perform a feature
selection step. The goal of this step is to select for
computation only those path types that commonly
connect sources and targets of relation r. We per-
form this feature selection by doing length-bounded
random walks from a given list of source and tar-
get nodes, keeping track of how frequently each path
type leads from a source node to a target node. The
most common m path types are selected for the set
B.
Training: We perform standard logistic regres-
sion with L2 regularization to learn the weights ?rpi.
We follow the strategy in (Lao and Cohen, 2010) to
generate positive and negative training instances.
3.2 PRAsyntactic
In this section, we shall extend the knowledge graph
G = (V,E, T ) from the previous section with an
augmented graph G
?
= (V,E
?
, T
?
), where E ? E
?
and T ? T
?
, with the set of vertices unchanged.
In order to get the edges in E
?
? E, we first
collect a set of Subject-Verb-Object (SVO) triples
D = {(s, v, o, c)} from a large dependency parsed
text corpus, with c ? R+ denoting the frequency
of this triple in the corpus. The additional edge
set is then defined as Esyntactic = E
?
? E =
{(s, v, o) | ?(s, v, o, c) ? D, s, o ? V }. We de-
fine S = {v | ?(s, v, o) ? Esyntactic} and set
T
?
= T ? S. In other words, for each pair of
directly connected nodes in the KB graph G, we add
an additional edge between those two nodes for each
verb which takes the NPs represented by two nodes
as subjects and objects (or vice versa) as observed in
a text corpus. In Figure 1, (Alex Rodriguez, ?plays
for?, NY Yankees) is an example of such an edge.
PRA is then applied over this augmented graph
G
?
, over the same set of prediction types R as be-
fore. We shall refer to this version of PRA as
PRAsyntactic. For the experiments in this paper, we
collected |D| = 600 million SVO triples1 from the
entire ClueWeb corpus (Callan et al, 2009), parsed
using the Malt parser (Nivre et al, 2007) by the
Hazy project (Kumar et al, 2013).
3.3 PRAlatent
In this section we construct G
??
= (V,E??, T
??
),
another syntactic-information-induced extension of
the knowledge graph G, but instead of using the sur-
face forms of verbs in S (see previous section) as
edge types, we derive those edges types T
??
based
on latent embeddings of those verbs. We note that
E ? E
??
, and T ? T
??
.
In order to learn the latent or low dimensional em-
beddings of the verbs in S, we first define QS =
{(s, o) | ?(s, v, o, c) ? D, v ? S}, the set of
subject-object tuples in D which are connected by
at least one verb in S. We now construct a matrix
X|S|?|QS | whose entry Xv,q = c, where v ? S, q =
(s, o) ? QS , and (s, v, o, c) ? D. After row normal-
izing and centering matrix X , we apply PCA on this
matrix. Let A|S|?d with d << |QS | be the low di-
mensional embeddings of the verbs in S as induced
by PCA. We use two strategies to derive mappings
for verbs from matrix A.
? PRAlatentc : The verb is mapped to concatena-
tion of the k2 most positive columns in the row
in A that corresponds to the verb. Similarly, for
the most negative k2 columns.
1This data and other resources from the paper are publicly
available at http://rtw.ml.cmu.edu/emnlp2013 pra/.
835
Precision Recall F1
PRA 0.800 0.331 0.468
PRAsyntactic 0.804 0.271 0.405
PRAlatentc 0.885 0.334 0.485
PRAlatentd 0.868 0.424 0.570
Table 1: Comparison of performance of different variants
of PRA micro averaged across 15 NELL relations. We
find that use of latent edge labels, in particular the pro-
posed approach PRAlatentd , significantly outperforms
other approaches. This is our main result. (See Section 4)
? PRAlatentd : The verb is mapped to disjunction
of top-k most positive and negative columns in
the row in A that corresponds to the verb.
4 Experiments
We compared the various methods using 15 NELL
relations. For each relation, we split NELL?s known
relation instances into 90% training and 10% testing.
For each method, we then selected 750 path features
and trained the model, as described in Section 3, us-
ing GraphChi (Kyrola et al, 2012) to perform the
random walk graph computations. To evaluate the
model, we took all source nodes in the testing data
and used the model to predict target nodes. We re-
port the precision and recall (on the set of known tar-
get nodes) of the set of predictions for each model
that are above a certain confidence threshold. Be-
cause we used strong regularization, we picked for
our threshold a model score of 0.405, correspond-
ing to 60% probability of the relation instance being
true; values higher than this left many relations with-
out any predictions. Table 1 contains the results.
As can be seen in the table, PRAsyntactic on av-
erage performs slightly worse than PRA. While
the extra syntactic features are very informative for
some relations, they also introduce a lot of spar-
sity, which makes the model perform worse on other
relations. When using latent factorization meth-
ods to reduce the sparsity of the syntactic features,
we see a significant improvement in performance.
PRAlatentc has a 45% reduction in precision er-
rors vs. PRA while maintaining the same recall,
and PRAlatentd reduces precision errors by 35%
while improving recall by 27%. Section 4.1 con-
tains some qualitative analysis of how sparsity is re-
duced with the latent methods. As a piece quanti-
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
PRAPRAsyntacticPRAlatentcPRAlatentd
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
PRAPRAsyntacticPRAlatentcPRAlatentd
Figure 2: Precision (y axis) - Recall (x axis) plots for the
relations cityLiesOnRiver (top) and athletePlaysForTeam
(bottom). PRAlatentd (rightmost plot), the proposed ap-
proach which exploits latent edge labels, outperforms
other alternatives.
tative analysis, there were 908 possible path types
found in the feature selection step with PRA on the
relation cityLiesOnRiver (of which we then selected
750). For PRAsyntactic, there were 73,820, while
PRAlatentc had 47,554 and PRAlatentd had 58,414.
Table 2 shows F1 scores for each model on
each relation, and Figure 2 shows representative
Precision-Recall plots for two NELL relations. In
both cases, we find that PRAlatentd significantly
outperforms other baselines.
4.1 Discussion
While examining the model weights for each of
the methods, we saw a few occasions where sur-
face relations and NELL relations combined to form
interpretable path types. For example, in ath-
letePlaysForTeam, some highly weighted features
took the form of ?athletePlaysSport, ?(sport) played
by (team)??. A high weight on this feature would
bias the prediction towards teams that are known to
play the same sport as the athlete.
For PRA, the top features for the best performing
relations are path types that contain a single edge
836
PRA PRAsyntactic PRAlatentc PRAlatentd
animalIsTypeOfAnimal 0.52 0.50 0.47 0.53
athletePlaysForTeam 0.22 0.21 0.56 0.64
athletePlaysInLeague 0.81 0.75 0.73 0.74
cityLiesOnRiver 0.05 0 0.07 0.31
cityLocatedInCountry 0.15 0.20 0.45 0.55
companyCeo 0.29 0.18 0.25 0.35
countryHasCompanyOffice 0 0 0 0
drugHasSideEffect 0.96 0.95 0.94 0.94
headquarteredIn 0.31 0.11 0.41 0.64
locationLocatedWithinLocation 0.40 0.38 0.38 0.41
publicationJournalist 0.10 0.06 0.10 0.16
roomCanContainFurniture 0.72 0.70 0.71 0.73
stadiumLocatedInCity 0.53 0 0.13 0.67
teamPlaysAgainstTeam 0.47 0.24 0.26 0.21
writerWroteBook 0.59 0.62 0.73 0.80
Table 2: F1 performance of different variants of PRA for all 15 relations tested.
which is a supertype or subtype of the relation be-
ing predicted. For instance, for the relation ath-
letePlaysForTeam (shown in Figure 2), the highest-
weighted features in PRA are athleteLedSport-
sTeam (more specific than athletePlaysForTeam)
and personBelongsToOrganization (more general
than athletePlaysForTeam). For the same rela-
tion, PRAsyntactic has features like ?scored for?,
?signed?, ?have?, and ?led?. When using a latent
embedding of these verb phrases, ?signed?, ?have?,
and ?led? all have the same representation in the la-
tent space, and so it seems clear that PRAlatent gains
a lot by reducing the sparsity inherent in using sur-
face verb forms.
For cityLiesOnRiver, where PRA does not per-
form as well, there is no NELL relation that is an im-
mediate supertype or subtype, and so PRA does not
have as much evidence to use. It finds features that,
e.g., are analogous to the statement ?cities in the
same state probably lie on the same river?. Adding
lexical labels gives the model edges to use like ?lies
on?, ?runs through?, ?flows through?, ?starts in?
and ?reaches?, and these features give a significant
boost in performance to PRAsyntactic. Once again,
almost all of those verb phrases share the same latent
embedding, and so PRAlatent gains another signifi-
cant boost in performance by combining them into a
single feature.
5 Conclusion
In this paper, we introduced the use of latent lexi-
cal edge labels for PRA-based inference over knowl-
edge bases. We obtained such latent edge labels
by mining a large dependency parsed corpus of
500 million web documents and performing PCA
on the result. Through extensive experiments on
real datasets, we demonstrated that the proposed ap-
proach significantly outperforms previous state-of-
the-art baselines.
Acknowledgments
We thank William Cohen (CMU) for enlightening
conversations on topics discussed in this paper. We
thank the ClueWeb project (CMU) and the Hazy
Research Group (http://hazy.cs.wisc.edu/hazy/) for
their generous help with data sets; and to the anony-
mous reviewers for their constructive comments.
This research has been supported in part by DARPA
(under contract number FA8750-13-2-0005), and
Google. Any opinions, findings, conclusions and
recommendations expressed in this paper are the au-
thors? and do not necessarily reflect those of the
sponsors.
837
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM conference on Digital
libraries.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of SIGMOD.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web.
J. Callan, M. Hoy, C. Yoo, and L. Zhao. 2009.
Clueweb09 data set. boston.lti.cs.cmu.edu.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka Jr, and Tom M Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In AAAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
itall:(preliminary results). In Proceedings of WWW.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second generation.
In Proceedings of IJCAI.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational Linguistics.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of ACL.
Arun Kumar, Feng Niu, and Christopher Re?. 2013.
Hazy: making it easier to build and maintain big-data
analytics. Communications of the ACM, 56(3):40?49.
Aapo Kyrola, Guy Blelloch, and Carlos Guestrin. 2012.
Graphchi: Large-scale graph computation on just a pc.
In Proceedings of the 10th USENIX Symposium on Op-
erating Systems Design and Implementation (OSDI),
pages 31?46.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53?67.
Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of EMNLP. Associa-
tion for Computational Linguistics.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT.
Burr Settles and Steven Dow. 2013. Let?s get together:
the formation and success of online creative collabora-
tions. In Proceedings of CHI.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL.
Fabian M Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of KDD.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of WWW.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation top-
ics. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1426?1436. Association for Computational Linguis-
tics.
838
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 233?243,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Aligning context-based statistical models of language
with brain activity during reading
Leila Wehbe
1,2
, Ashish Vaswani
3
, Kevin Knight
3
and Tom Mitchell
1,2
1
Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA
2
Center for the Neural Basis of Computation, Carnegie Mellon University, Pittsburgh, PA
3
Information Sciences Institute, University of Southern California, Los Angeles, CA
lwehbe@cs.cmu.edu, vaswani@usc.edu, knight@isi.edu, tom.mitchell@cs.cmu.edu
Abstract
Many statistical models for natural language pro-
cessing exist, including context-based neural net-
works that (1) model the previously seen context
as a latent feature vector, (2) integrate successive
words into the context using some learned represen-
tation (embedding), and (3) compute output proba-
bilities for incoming words given the context. On
the other hand, brain imaging studies have sug-
gested that during reading, the brain (a) continu-
ously builds a context from the successive words
and every time it encounters a word it (b) fetches its
properties from memory and (c) integrates it with
the previous context with a degree of effort that is
inversely proportional to how probable the word is.
This hints to a parallelism between the neural net-
works and the brain in modeling context (1 and a),
representing the incoming words (2 and b) and in-
tegrating it (3 and c). We explore this parallelism to
better understand the brain processes and the neu-
ral networks representations. We study the align-
ment between the latent vectors used by neural net-
works and brain activity observed via Magnetoen-
cephalography (MEG) when subjects read a story.
For that purpose we apply the neural network to the
same text the subjects are reading, and explore the
ability of these three vector representations to pre-
dict the observed word-by-word brain activity.
Our novel results show that: before a new word i
is read, brain activity is well predicted by the neural
network latent representation of context and the pre-
dictability decreases as the brain integrates the word
and changes its own representation of context. Sec-
ondly, the neural network embedding of word i can
predict the MEG activity when word i is presented
to the subject, revealing that it is correlated with the
brain?s own representation of word i. Moreover, we
obtain that the activity is predicted in different re-
gions of the brain with varying delay. The delay is
consistent with the placement of each region on the
processing pathway that starts in the visual cortex
and moves to higher level regions. Finally, we show
that the output probability computed by the neural
networks agrees with the brain?s own assessment of
the probability of word i, as it can be used to predict
the brain activity after the word i?s properties have
been fetched from memory and the brain is in the
process of integrating it into the context.
1 Introduction
Natural language processing has recently seen a
surge in increasingly complex models that achieve
impressive goals. Models like deep neural net-
works and vector space models have become pop-
ular to solve diverse tasks like sentiment analy-
sis and machine translation. Because of the com-
plexity of these models, it is not always clear how
to assess and compare their performances as they
might be useful for one task and not the other.
It is also not easy to interpret their very high-
dimensional and mostly unsupervised representa-
tions. The brain is another computational system
that processes language. Since we can record brain
activity using neuroimaging, we propose a new di-
rection that promises to improve our understand-
ing of both how the brain is processing language
and of what the neural networks are modeling by
aligning the brain data with the neural networks
representations.
In this paper we study the representations of two
kinds of neural networks that are built to predict
the incoming word: recurrent and finite context
models. The first model is the Recurrent Neural
Network Language Model (Mikolov et al., 2011)
which uses the entire history of words to model
context. The second is the Neural Probabilistic
Language Model (NPLM) which uses limited con-
text constrained to the recent words (3 grams or 5
grams). We trained these models on a large Harry
Potter fan fiction corpus and we then used them to
predict the words of chapter 9 of Harry Potter and
the Sorcerer?s Stone (Rowling, 2012). In paral-
lel, we ran an MEG experiment in which 3 subject
read the words of chapter 9 one by one while their
brain activity was recorded. We then looked for
the alignment between the word-by-word vectors
produced by the neural networks and the word-by-
word neural activity recorded by MEG.
Our neural networks have 3 key constituents:
a hidden layer that summarizes the history of the
previous words ; an embeddings vector that sum-
marizes the (constant) properties of a given word
and finally the output probability of a word given
233
Reading comprehension is reflected in the subsequent acti-
vation of the left superior temporal cortex at 200?600 ms
(Halgren et al., 2002; Helenius et al., 1998; Pylkka?nen
et al., 2002, 2006; Pylkka?nen and Marantz, 2003; Simos
et al., 1997). This sustained activation differentiates
between words and nonwords (Salmelin et al., 1996; Wil-
son et al., 2005; Wydell et al., 2003). Apart from lexical-se-
mantic aspects it also seems to be sensitive to phonological
manipulation (Wydell et al., 2003).
As discussed above, in speech perception activation is
concentrated to a rather small area in the brain and we
have to rely on time information to dissociate between dif-
ferent processes. Here, the different processes are separable
both in timing and location. Because of that, one might
think that it is easier to characterize language-related pro-
cesses in the visual than auditory modality. However, here
the difficulties appear at another level. In reading, activa-
tion is detected bilaterally in the occipital cortex, along
the temporal lobes, in the parietal cortex and, in vocalized
reading, also in the frontal lobes, at various times with
respect to stimulus onset. Interindividual variability further
complicates the picture, resulting in practically excessive
amounts of temporal and spatial information. The areas
and time windows depicted in Fig. 5, with specific roles
in reading, form a limited subset of all active areas
observed during reading. In order to perform proper func-
tional localization one needs to vary the stimuli and tasks
systematically, in a parametric fashion. Let us now consid-
er how one may extract activation reflecting pre-lexical let-
ter-string analysis and lexical-semantic processing.
3.2. Pre-lexical analysis
In order to tease apart early pre-lexical processes in
reading, Tarkiainen and colleagues (Tarkiainen et al.,
1999) used words, syllables, and single letters, imbedded
in a noisy background, at four different noise levels
(Fig. 6). For control, the sequences also contained symbol
strings. One sequence was composed of plain noise stimuli.
The stimuli were thus varied along two major dimensions:
the amount of features to process increased with noise and
with the number of items, letters or symbols. On the other
hand, word-likeness was highest for clearly visible complete
words and lowest for symbols and noise.
At the level of the brain, as illustrated in Fig. 7, the data
showed a clear dissociation between two processes within
the first 200 ms: visual feature analysis occurred at about
100 ms after stimulus presentation, with the active areas
around the occipital midline, along the ventral stream. In
these areas, the signal increased with increasing noise and
with the number of items in the string, similarly for letters
and symbols. Only 50 ms later, at about 150 ms, the left
inferior occipitotemporal cortex showed letter-string spe-
cific activation. This signal increased with the visibility of
the letter strings. It was strongest for words, weaker for syl-
lables, and still weaker for single letters. Crucially, the acti-
vation was significantly stronger for letter than symbol
strings of equal length.
Bilateral occipitotemporal activation at about 200 ms
post-stimulus is consistently reported in MEG studies of
reading (Cornelissen et al., 2003b; Pammer et al., 2004; Sal-
melin et al., 1996, 2000b) but, interestingly, functional
specificity for letter-strings is found most systematically
in the left hemisphere. The MEG data on letter-string spe-
cific activation are in good agreement with intracranial
recordings, both with respect to timing and location and
the pre-lexical nature of the activation (Nobre et al., 1994).
3.3. Lexical-semantic analysis
To identify cortical dynamics of reading comprehension,
Helenius and colleagues (Helenius et al., 1998) employed a
Visual feature
analysis
Non-specific Words =Nonwords Nonwords
Letter-string
analysis
Time (ms)
0 400 800 0 400 800 0 400 800
Lexical-semantic
analysis
Fig. 5. Cortical dynamics of silent reading. Dots represent centres of active cortical patches collected from individual subjects. The curves display the
mean time course of activation in the depicted source areas. Visual feature analysis in the occipital cortex (!100 ms) is stimulus non-specific. The stimulus
content starts to matter by !150 ms when activation reflecting letter-string analysis is observed in the left occipitotemporal cortex. Subsequent activation
of the left superior temporal cortex at !200?600 ms reflects lexical-semantic analysis and, probably, also phonological analysis. Modified from Salmelin
et al. (2000a).
!L
ipp
inc
ott
W
illi
am
s&
W
ilk
ins
20
00
R. Salmelin / Clinical Neurophysiology xxx (2006) xxx?xxx 5
ARTICLE IN PRESS
Please cite this article as: Riitta Salmelin, Clinical neurophysiology of language: The MEG approach, Clinical Neurophysiology
(2006), doi:10.1016/j.clinph.2006.07.316
Figure 1: Cortical dynamics of silent reading. This figure
is adapted from (Salmelin, 2007). Dots represent projected
sources of activity in the visual cortex (left brain sketch) and
the temporal cortex (right brain sketch). The curves display
the m an time course of activation in the depicted ource ar-
eas for different conditions. The initial visual feature anal-
ysis in the visual cortex at ?100 ms is non-specific to lan-
guage. Comparing responses to letter strings and other vi-
sual stimuli rev als that letter string nalysis ccurs around
150 ms. Finally comparing the responses to words and non-
words (made-up words) reveals lexical-se antic analysis in
the temporal cortex at ?200-500ms.
the context. We set out to find the brain analogs
of these model constituents using an MEG decod-
ing task. We compare the different models and
h ir represen ations n term of how well they
can be used to decode the word being read from
MEG data. We obtain correspondences between
the models and the brain data that are consistent
with model of language processing in which
brain activity encodes story context, and where
each new word generates additional brain activity,
flowing generally from visual processing areas to
more high level areas, culminating in an updated
story cont xt, and reflecting a overall m gnitude
of neural effort influenced by the probability of
that new word given the previous context.
1.1 Neural processes involved in reading
Humans read with an average speed of 3 words
per second. Reading requires us to perceive in-
coming words and gradually integrate them into
a representation of the meaning. As words are
read, it takes 100ms for the visual input to reach
the visual cortex. 50ms later, the visual input is
processed as letter strings in a specialized region
of the left visual cortex (Salmelin, 2007). Be-
tween 200-500ms, the word?s semantic properties
are processed (see Fig. 1). Less is understood
about the cortical dynamics of word integration, as
multiple theories exist (Friederici, 2002; Hagoort,
2003).
Magnetoencephalography (MEG) is a brain-
imaging tool that is well suited for studying lan-
guage. MEG records the change in the magnetic
field on the surface of the head that is caused by
a large set of aligned neurons that are changing
their firing patterns in synchrony in response to
a stimulus. Because of the nature of the signal,
MEG recordings are directly related to neural ac-
tivity and have no latency. They are sampled at
a high frequency (typically 1kHz) that is ideal for
tracking the fast dynamics of language processing.
In this work, we are interested in the mecha-
nism of human text understanding as the meaning
of ncoming words is fetched from memory and
integrated with the context. Interestingly, this is
analogous to neural network models of language
that are used to predict the incoming word. The
mental representation of the previous context is
analogous to the latent layer of the neural network
which summarizes the relevant context before see-
ing the word. The representation of the meaning
of a word is analogous to the embedding that the
neural network learns in training and then uses.
Finally, one common hypotheses is that the brain
integrates the word with inversely proportional ef-
fort to how predictable the word is (Frank et al.,
2013). There is a well studied response known as
the N400 that is an increase of the activity in the
mporal cortex that has been recently shown to be
graded by the amount of surprisal of the incoming
word given the context (Frank et al., 2013). This is
analogous to the output probability of the incom-
ing word from the neural network.
Fig. 2 shows a hypothetical activity in an MEG
sensor as a subject reads a story in our experi-
ment, in which words are presented one at a time
for 500ms each. We conjecture that the activity in
time window a, i.e. before word i is understood, is
mostly related to the previous context before see-
ing word i. We also conjecture that the activity in
time window b is related to understanding word i
and integrating it into the context, leading to a new
representation of context in window c.
Using three types of features from neural net-
works (hidden layer context representation, output
probabilities and word embeddings) from three
different models of language (one recurrent model
and two finite context models), we therefore set to
predict the activity in the brain in different time
windows. We want to align the brain data with the
various model constituents to understand where
and when different types of processes are com-
puted in the brain, and simultaneously, we want to
234
Harry'
Harry'
had'
had'
never'
never'
embedding(iC1)' embedding(i)' embedding(i+1)'
context(iC1)'context(iC2)' context(i)'
out.'prob.(iC1)' out.'prob.(i)' out.'prob.(i+1)'
Studying'the'construc<on'of'meaning'
3'
word i+1 word i-1 word i 
0.5's'0.5's' 0.5's'
a b
c
Leila'Wehbe'
'?''Harry'''''''had''''''''never'?''
Figure 2: [Top] Sketch of the updates of a neural network
reading chapter 9 after it has been trained. Every word cor-
responds to a fixed embedding vector (magenta). A context
vector (blue) is computed before the word is seen given the
previous words. Given the context vector, the probability of
every word can be computed (symbolized by the histogram
in green). We only use the output probability of the actual
word (red circle). [Bottom] Hypothetical activity in an MEG
sensor when the subject reads the corresponding words. The
time periods approximated as a, b and c can be tested for in-
formation content relating to: the context of the story before
seeing word i (modeled by the context vector at i), the repre-
sentation of the properties of word i (the embedding of word
i) and the integration of word i into the context (the output
probability of word i). The periods drawn here are only a
conjecture on the timings of such cognitive events.
use the brain data to shed light on what the neural
network vectors are representing.
Related work
Decoding cognitive states from brain data is a
recent field that has been growing in popularity.
Most decoding studies that study language use
functional Magnetic Resonance Imaging (fMRI),
while some studies use MEG. MEG?s high tempo-
ral resolution makes it invaluable for looking at the
dynamics of language understanding. (Sudre et
al., 2012) decode from MEG the word a subject is
reading. The authors estimate from the MEG data
the semantic features of the word and use these as
an intermediate step to decode what the word is.
This is in principle similar to the classification ap-
proach we follow, as we will also use the feature
vectors as an intermediate step for word classifica-
tion. However the experimental paradigm in (Su-
dre et al., 2012) is to present to the subjects sin-
gle isolated words and to find how the brain rep-
resents their semantic features; whereas we have a
much more complex and ?naturalistic? experiment
in which the subjects read a non-artificial passage
of text, and we look at processes that exceed in-
dividual word processing: the construction of the
meanings of the successive words and the predic-
tion/integration of incoming words.
In (Frank et al., 2013), the amount of surprisal
that a word has given its context is used to pre-
dict the intensity of the N400 response described
previously. This is the closest study we could find
to our approach. This study was concerned with
analyzing the brain processes related only to sur-
prisal while we propose a more integral account
of the processes in the brain. The study also didn?t
address the major contribution we propose here,
which is to shed light on the inner constituents of
language models using brain imaging.
1.2 Recurrent and finite context neural
networks
Similar to standard language models, neural lan-
guage models also learn probability distributions
over words given their previous context. However,
unlike standard language models, words are rep-
resented as real-valued vectors in a high dimen-
sional space. These word vectors, referred to as
word embeddings, can be different for input and
output words, and are learned from training data.
Thus, although at training and test time, the in-
put and output to the neural language models are
one-hot representation of words, it is their em-
beddings that are used to compute word proba-
bility distributions. After training the embedding
vectors are fixed and it is these vectors that we
will use later on to predict MEG data. To predict
MEG data, we will also use the latent vector rep-
resentations of context that these neural networks
produce, as well as the probability of the current
word given the context. In this section, we will
describe how recurrent neural network language
models and feedforward neural probabilistic lan-
guage models compute word probabilities. In the
interest of space, we keep this description brief,
and for details, the reader is requested to refer to
the original papers describing these models.
235
w(t) s(t? 1)
y(t) c(t)
hidden
s(t)
output
P (w
t+1
| s(t))
D W
D
?
X
Figure 3: Recurrent neural network language model.
Recurrent Neural Network Language Model
Unlike standard feedforward neural language
models that only look at a fixed number of past
words, recurrent neural network language models
use all the previous history from position 1 to t?1
to predict the next word. This is typically achieved
by feedback connections, where the hidden layer
activations used for predicting the word in posi-
tion t ? 1 are fed back into the network to com-
pute the hidden layer activations for predicting the
next word. The hidden layer thus stores the history
of all previous words. We use the RNNLM archi-
tecture as described in Mikolov (2012), shown in
Figure 3. The input to the RNNLM at position t
are the one-hot representation of the current word,
w(t), and the activations from the hidden layer at
position t ? 1, s(t ? 1). The output of the hidden
layer at position t? 1 is
s(t) = ? (Dw(t) + Ws(t? 1)) ,
where D is the matrix of input word embeddings,
W is a matrix that transforms the activations from
the hidden layer in position t ? 1, and ? is a
sigmoid function, defined as ?(x) =
1
1+exp(?x)
,
that is applied elementwise. We need to compute
the probability of the next word w(t + 1) given
the hidden state s(t). For fast estimation of out-
put word probabilities, Mikolov (2012) divides the
computation into two stages: First, the probability
distribution over word classes is computed, after
which the probability distribution over the subset
of words belonging to the class are computed. The
class probability of a particular class with indexm
at position t is computed as:
P (c
m
(t) | s(t)) =
exp (s(t)Xv
m
)
?
C
c=1
(exp (s(t)Xv
c
))
,
where X is a matrix of class embeddings and v
m
is a one-hot vector representing the class with in-
dex m. The normalization constant is computed
u
1
u
2
input
words
input
embeddings
hidden
h
1
hidden
h
2
output
P (w | u)
D
?
M
C
1
C
2
D
Figure 4: Neural probabilistic language model
over all classes C. Each class specifies a subset
V
?
of words, potentially smaller than the entire vo-
cabulary V . The probability of an output word l at
position t + 1 given that its class is m is defined
as:
P (y
l
(t+ 1) | c
m
(t), s(t)) =
exp (s(t)D
?
v
l
)
?
V
?
k=1
(exp (s(t)D
?
v
k
))
,
where D
?
is a matrix of output word embeddings
and v
l
is a one hot vector representing the word
with index l. The probability of the word w(t+1)
given its class c
i
can now be computed as:
P (w(t+ 1) | s(t)) =P (w(t+ 1) | c
i
, s(t))
P (c
i
| s(t)).
Neural Probabilistic Language Model
We use the feedforward neural probabilistic lan-
guage model architecture of Vaswani et al. (2013),
as shown in Figure 4. Each context u comprises
a sequence of words u
j
(1 ? j ? n ? 1) repre-
sented as one-hot vectors, which are fed as input
to the neural network. At the output layer, the neu-
ral network computes the probability P (w | u) for
each word w, as follows.
The output of the first hidden layer h
1
is
h
1
= ?
?
?
n?1
?
j=1
C
j
Du
j
+ b
1
?
?
,
where D is a matrix of input word embeddings
which is shared across all positions, the C
j
are the
context matrices for each word in u, b
1
is a vec-
tor of biases with the same dimension as h
1
, and ?
is applied elementwise. Vaswani et al. (2013) use
rectified linear units (Nair and Hinton, 2010) for
236
the hidden layers h
1
and h
2
, which use the activa-
tion function ?(x) = max(0, x).
The output of the second layer h
2
is
h
2
= ? (Mh
1
+ b
2
) ,
where M is a weight matrix between h
1
and h
2
and b
2
is a vector of biases for h
2
. The probabil-
ity of the output word is computed at the output
softmax layer as:
P (w | u) =
exp
(
v
w
D
?
h
2
+ b
T
v
w
)
?
V
w
?
=1
exp (v
w
?
D
?
h
2
+ b
T
v
w
?
)
,
where D
?
is the matrix of output word embed-
dings, b is a vector of biases for every output word
and v
w
its the one hot representation of the word
w in the vocabulary.
2 Methods
We describe in this section our approach. In sum-
mary, we trained the neural network models on
a Harry Potter fan fiction database. We then ran
these models on chapter 9 of Harry Potter and the
Sorcerer?s Stone (Rowling, 2012) and computed
the context and embedding vectors and the output
probability for each word. In parallel, 3 subjects
read the same chapter in an MEG scanner. We
build models that predict the MEG data for each
word as a function of the different neural network
constituents. We then test these models with a
classification task that we explain below. We de-
tect correspondences between the neural network
components and the brain processes that under-
lie reading in the following fashion. If using a
neural network vector (e.g. the RNNLM embed-
ding vector) allows us to classify significantly bet-
ter than chance in a given region of the brain at
a given time (e.g. the visual cortex at time 100-
200ms), then we can hypothesize a relationship
between that neural network constituent and the
time/location of the analogous brain process.
2.1 Training the Neural Networks
We used the freely available training tools pro-
vided by Mikolov (2012)
1
and Vaswani et al.
(2013)
2
to train our RNNLM and NPLM models
used in our brain data classification experiments.
Our training data comprised around 67.5 million
1
http://rnnlm.org/
2
http://nlg.isi.edu/software/nplm
words for training and 100 thousand words for val-
idation from the Harry Potter fan fiction database
(http://harrypotterfanfiction.com). We restricted
the vocabulary to the top 100 thousand words
which covered all but 4 words from Chapter 9 of
Harry Potter and the Sorcerer?s Stone.
For the RNNLM, we trained models with differ-
ent hidden layers and learning rates and found the
RNNLM with 250 hidden units to perform best on
the validation set. We extracted our word embed-
dings from the input matrix D (Figure 3). We used
the default settings for all other hyper parameters.
We trained 3-gram and 5-gram NPLMs with
150 dimensional word embeddings and experi-
mented with different number of units for the first
hidden layer (h
1
in Figure 4), and different learn-
ing rates. For both the 3-gram and 5-gram mod-
els, we found 750 hidden units to perform the best
on the validation set and chose those models for
our final experiments. We used the output word
embeddings D
?
in our experiments. We visually
inspected the nearest neighbors in the 150 dimen-
sional word embedding space for some words and
didn?t find the neighbors from D
?
or D to be dis-
tinctly better than each other. We leave the com-
parison of input and output embeddings on brain
activity prediction for future work.
2.2 MEG paradigm
We recorded MEG data for three subjects (2 fe-
males and one male) while they read chapter 9
of Harry Potter and the Sorcerer?s Stone (Rowl-
ing, 2012). The participants were native English
speakers and right handed. They were chosen to
be familiar with the material: we made sure they
had read the Harry Potter books or seen the movies
series and were familiar with the characters and
the story. All the participants signed the consent
form, which was approved by the University of
Pittsburgh Institutional Review Board, and were
compensated for their participation.
The words of the story were presented in rapid
serial visual format (Buchweitz et al., 2009):
words were presented one by one at the center
of the screen for 0.5 seconds each. The text was
shown in 4 experimental blocks of ?11 minutes.
In total, 5176 words were presented. Chapter 9
was presented in its entirety without modifications
and each subject read the chapter only once.
One can think of an MEG machine as a large
helmet, with sensors located on the helmet that
237
record the magnetic activity. Our MEG recordings
were acquired on an Elekta Neuromag device at
the University of Pittsburgh Medical Center Pres-
byterian Hospital. This machine has 306 sensors
distributed into 102 locations on the surface of the
subject?s head. Each location groups 3 sensors or
two types: one magnometer that records the in-
tensity of the magnetic field and two planar gra-
diometers that record the change in the magnetic
field along two orthogonal planes
3
.
Our sampling frequency was 1kHz. For prepro-
cessing, we used Signal Space Separation method
(SSS, (Taulu et al., 2004)), followed by its tempo-
ral extension (tSSS, (Taulu and Simola, 2006)).
For each subject, the experiment data consists
therefore of a 306 dimensional time series of
length?45 minutes. We averaged the signal in ev-
ery sensor into 100ms non-overlapping time bins.
Since words were presented for 500ms each, we
therefore obtain for every word p = 306 ? 5 val-
ues corresponding to 306 vectors of 5 points.
2.3 Decoding experiment
To find which parts of brain activity are related to
the neural network constituents (e.g. the RNNLM
context vector), we run a prediction and classifica-
tion experiment in a 10-fold cross validated fash-
ion. At every fold, we train a linear model to pre-
dict MEG data as a function of one of the feature
sets, using 90% of the data. On the remaining 10%
of the data, we run a classification experiment.
MEG data is very noisy. Therefore, classify-
ing single word waveforms yields a low accuracy,
peaking at 60%, which might lead to false nega-
tives when looking for correspondences between
neural network features and brain data. To reveal
informative features, one can boost signal by ei-
ther having several repetitions of the stimuli in the
experiment and then averaging (Sudre et al., 2012)
or by combining the words into larger chunks (We-
hbe et al., 2014). We chose the latter because the
former sacrifices word and feature diversity.
At testing, we therefore repeat the following
300 times. Two sets of words are chosen ran-
domly from the test fold. To form the first set, 20
words are sampled without replacement from the
test sample (unseen by the classifier). To form the
second set, the k
th
word is chosen randomly from
all words in the test fold having the same length as
3
In this paper, we treat these three different sensors as
three different dimensions without further exploiting their
physical properties.
the k
th
word of the first set. Since every fold of
the data was used 9 times in the training phase and
once in the testing phase, and since we use a high
number of randomized comparisons, this averages
out biases in the accuracy estimation. Classifying
sets of 20 words improves the classification accu-
racy greatly while lowering its variance and makes
it dissociable from chance performance. We com-
pare only between words of equal length, to mini-
mize the effect of the low level visual features on
the classification accuracy.
After averaging out the results of multiple folds,
we end up with average accuracies that reveal how
related one of the models? constituents (e.g. the
RNNLM context vector) is to brain data.
2.3.1 Annotation of the stimulus text
We have 9 sets of annotations for the words of the
experiment. Each set j can be described as a ma-
trix F
j
in which each row i corresponds to the vec-
tor of annotations of word i. Our annotations cor-
respond to the 3 model constituents for each of the
3 models: the hidden layer representation before
word i, the output probability of word i and the
learned embeddings for word i.
2.3.2 Classification
In order to align the brain processes and the differ-
ent constituents of the different models, we use a
classification task. The task is to classify the word
a subject is reading out of two possible choices
from its MEG recording. The classifier uses one
type of feature in an intermediate classification
step. For example, the classifier learns to predict
the MEG activity for any setting of the RNNLM
hidden layer. Given an unseen MEG recording for
an unknown word i and two possible story words
i
?
and i
??
(one of which being the true word i), the
classifier predicts the MEG activity when reading
i
?
and i
??
from their hidden layer vectors. It then
assigns the label i
?
or i
??
to the word recording i
depending on which prediction is the closest to the
recording. The following are the detailed steps of
this complex classification task. However, for the
rest of the paper the most useful point to keep in
mind is that the main purpose of the classification
is to find a correspondence between the brain data
and a given feature set j.
1. Normalize the columns of M (zero mean,
standard deviation = 1). Pick feature set F
j
and normalize its columns to a minimum of 0
and a maximum of 1.
238
2. Divide the data into 10 folds, for each fold b:
(a) Isolate M
b
and F
b
j
as test data. The re-
mainder M
?b
and F
?b
j
will be used for
training
4
.
(b) Subtract the mean of the columns of
M
?b
from M
b
and M
?b
and the mean
of the columns of F
?b
j
from F
b
j
and F
?b
j
(c) Use ridge regression to solve
M
?b
= F
?b
j
? ?
t
j
by tuning the ? parameter to every one
of the p output dimensions indepen-
dently. ? is chosen via generalized cross
validation (Golub et al., 1979).
(d) Perform a binary classification. Sample
from the set of words in b a set c of 20
words. Then sample from b another set
of 20 words such that the k
th
word in c
and d have the same number of letters.
For every sample (c,d):
i. predict the MEG data for c and d as:
P
c
= F
c
j
? ?
b
j
and P
d
= F
d
j
? ?
b
j
ii. assign to M
c
the label c or d depend-
ing on which of P
c
or P
d
is closest
(Euclidean distance).
iii. assign to M
d
the label c or d de-
pending on which of P
c
or P
d
is
closest (Euclidean distance).
3. Compute the average accuracy.
2.3.3 Restricting the analysis spatially: a
searchlight equivalent
We adapt the searchlight method (Kriegeskorte et
al., 2006) to MEG. The searchlight is a discovery
procedure used in fMRI in which a cube is slid
over the brain and an analysis is performed in each
location separately. It allows to find regions in the
brain where a specific phenomenon is occurring.
In the MEG sensor space, for every one of the 102
sensor locations `, we assign a group of sensors g
`
.
For every location `, we identify the locations that
immediately surround it in any direction (Anterior,
Right Anterior, Right etc...) when looking at the
2D flat representation of the location of the sensors
in the MEG helmet (see Fig. 9 for an illustration of
the 2D helmet). g
`
therefore contains the 3 sensors
at location ` and at the neighboring locations. The
maximum number of sensors in a group is 3 ? 9.
4
The rows from M
?b
and F
?b
j
that correspond to the five
words before or after the test set are ignored in order to make
the test set independent.
The locations at the edge of the helmet have fewer
sensors because of the missing neighbor locations.
2.3.4 Restricting the analysis temporally
Instead of using the entire time course of the word,
we can use only one of the corresponding 100ms
time windows. Obtaining a high classification ac-
curacy using one of the time windows and feature
set j means that the analogous type of information
is encoded at that time.
2.3.5 Classification accuracy by time and
region
The above steps compute whole brain accuracy us-
ing all the time series. In order to perform a more
precise spatio-temporal analysis, one can use only
one time windowm and one location ` for the clas-
sification. This can answer the question of when
and where different information is represented by
brain activity. For every location, we will use only
the columns corresponding to the time pointm for
the sensors belonging to the group g
`
. Step (d) of
the classification procedure is changed as such:
(d) Perform a binary classification. Sample from
the set of words in b a set c of 20 words. Then
sample from b another set of 20 words such
that the k
th
word in c and d have the same
number of letters. For every sample (c,d), and
for every setting of {m, `}:
i. predict the MEG data for c and d as:
P
c
{m,`}
= F
c
j
? ?
b
j,{m,`}
and
P
d
{m,`}
= F
d
j
? ?
b
j,{m,`}
ii. assign to M
c
{m,`}
the label c or d depend-
ing on which of P
c
{m,`}
or P
d
{m,`}
is clos-
est (Euclidean distance).
iii. assign to M
d
{m,`}
the label c or d depend-
ing on which of P
c
{m,`}
or P
d
{m,`}
is clos-
est (Euclidean distance).
2.3.6 Statistical significance testing
We determine the distribution for chance perfor-
mance empirically. Because the successive word
samples in our MEG and feature matrices are not
independent and identically distributed, we break
the relationship between the MEG and feature ma-
trices by shifting the feature matrices by large de-
lays (e.g. 2000 to 2500 words) and we repeat
the classification using the delayed matrices. This
simulates chance performance more fairly than a
permutation test because it keeps the time struc-
ture of the matrices. It was used in (Wehbe et al.,
239
2014) and inspired by (Chwialkowski and Gret-
ton, 2014). For every {m, `} setting we can there-
fore compute a standardized z-value by subtract-
ing the mean of the shifted classifications and di-
viding by the standard deviation. We then com-
pute the p-value for the true classification accu-
racy being due to chance. Since the three p-values
for the three subjects for a given {m, `} are inde-
pendent, we combine them using Fisher?s method
for independent test statistics (Fisher, 1925). The
statistics we obtain for every {m, `} are depen-
dent because they comprise nearby time and space
windows. We control the false discovery rate us-
ing (Benjamini and Yekutieli, 2001) to adjust for
the testing at multiple locations and time windows.
This method doesn?t assume any kind of indepen-
dence or positive dependence.
3 Results
We present in Fig. 5 the accuracy using all the time
windows and sensors. In Fig. 6 we present the
classification accuracy when running the classifi-
cation at every time window exclusively. In Fig. 9
we present the accuracy when running the classifi-
cation using different time windows and groups of
sensors centered at every one of the 102 locations.
It is important to lay down some conventions
to understand the complex results in these plots.
To recap, we are trying to find parallels between
model constituents and brain processes. We use:
? a subset of the data (for example the time
window 0-100ms and all the sensors)
? one type of feature (for example the hidden
context layer from the NPLM 3g model)
and we obtain a classification accuracy A. If A
is low, there is probably no relationship between
the feature set and the subset of data. If A is high,
it hints to an association between the subset of data
and the mental process that is analogous to the fea-
ture set. For example, when using all the sensors
and time window 0-100ms, along with the NPLM
3g hidden layer, we obtain an accuracy of 0.70
(higher than chance with p < 10
?14
, see Fig. 6).
Since the NPLM 3g hidden layer summarizes the
context of the story before seeing word i, this sug-
gests that the brain is still processing the context
of the story before word i between 0-100ms.
Fig. 6 shows the accuracy for different types
of features when using all of the time points and
all the sensors to classify a word. We can see
NPLM 3g NPLM 5g RNNLM0.6
0.8
1
class
ificati
on ac
curac
y
 
 hidden layer output probability embeddings
hidden layer output probability embeddings0.6
0.8
1
class
ificati
on ac
curac
y
 
 NPLM 3g NPLM 5g RNNLM
Figure 5: Average accuracy using all time windows and
sensors, grouped by model (top) and type of feature (bot-
tom). All accuracies are significantly higher than chance
(p < 10
?8
).
0 200 4000.5
0.6
0.7
NPLM 3g
0 200 4000.5
0.6
0.7
NPLM 5g
0 200 4000.5
0.6
0.7
RNNLM
 
 hidden layeroutput probabilityembeddings
Figure 6: Average accuracy in different time windows
when using different types of features as input to the clas-
sifier, for different models. Accuracy is plotted in the center
of the respective time window. Points marked with a circle
are significantly higher than chance accuracy for the given
feature set and time window after correction.
similar classification accuracies for the three types
of models, with RNNLM ahead for the hidden
layer and embeddings and behind for the output
probability features. The hidden layer features
are the most powerful for classification. Between
the three types of features, the hidden layer fea-
tures are the best at capturing the information con-
tained in the brain data, suggesting that most of
the brain activity is encoding the previous context.
The embedding features are the second best. Fi-
nally the output probability have the smallest ac-
curacies. This makes sense considering that they
capture much less information than the other two
high dimensional descriptive vectors, as they do
not represent the complex properties of the words,
only a numerical assessment of their likelihood.
Fig. 6 shows the accuracy when using different
windows of time exclusively, for the 100ms time
240
windows starting at 0, 100 . . . 400ms after word
presentation. We can see that using the embed-
ding vector becomes increasingly more useful for
classification until 300-400ms, and then its perfor-
mance starts decreasing. This results aligns with
the following hypothesis: the word is being per-
ceived and understood by the brain gradually after
its presentation, and therefore the brain represen-
tation of the word becomes gradually similar to the
neural network representation of the word (i.e. the
embedding vector). The output probability feature
accuracy peaks at a later time than the embeddings
accuracy. Obtaining a higher than chance accu-
racy at time window m using the output probabil-
ity as input to the classifier suggests strongly that
the brain is integrating the word at time window
m, because it is responding differently for pre-
dictable and unpredictable words
5
. The integra-
tion step happens after the perception step, which
is probably why the output probability curves peak
later than the embeddings curves.
?500 0 500 10000.5
0.6
0.7
0.8
Hidden Layer
 
 NPLM 3GNPLM 5GRNNLM
Figure 7: Average accuracy in time for the different hidden
layers. The analysis is extended to the time windows before
and after the word is presented, the input feature is restricted
to be the hidden layer before the central word is seen. The
first vertical bar indicates the onset of the word, the second
one indicates the end of its presentation.
?1000 0 10000.6
0.8 Subject 1
?1000 0 10000.6
0.8 Subject 2
?1000 0 10000.6
0.8 Subject 3
 
 
hidden layer output probability embeddings
Figure 8: Accuracy in time when using the RNNLM fea-
tures for each of the three subjects.
To understand the time dynamics of the hidden
layer accuracy we need to see a larger time scale
than the word itself. The hidden layer captures the
5
the fact that we can classify accurately during windows
300-400ms indicates that the classifier is taking advantage of
the N400 response discussed in the introduction
context before word i is seen. Therefore it seems
reasonable that the hidden layer is not only related
to the activity when the word is on the screen, but
also related to the activity before the word is pre-
sented, which is the time when the brain is inte-
grating the previous words to build that context.
On the other hand, as the word i and subsequent
words are integrated, the context starts diverging
from the context of word i (computed before see-
ing word i). We therefore ran the same analysis
as before, but this time we also included the time
windows before and after word i in the analysis,
while maintaining the hidden layer vector to be the
context before word i is seen. We see the behav-
ior we predicted in the results: the context before
seeing word i becomes gradually more useful for
classification until word i is seen, and then it grad-
ually decreases until it is no longer useful since
the context has changed. We observe the RNNLM
hidden layer has a higher classification accuracy
than the finite context NPLMs. This might be due
to the fact that the RNNLM has a more complete
representation of context that captures more of the
properties of the previous words.
To show the consistency of the results, we plot
as illustration the three curves we obtain for each
subject for the RNNLM (Fig. 8). The patterns
seem very consistent indicating the phenomena we
described can be detected at the subject level.
We now move on to the spatial decomposition
of the analysis. When the visual input enters the
brain, it first reaches the visual cortex at the back
of the head, and then moves anteriorly towards the
left and right temporal cortices and eventually the
frontal cortex. As it flows through these areas, it
is processed to higher levels of interpretations. In
Fig. 9, we plot the accuracy for different regions
of the brain and different time windows for the
RNNLM features. To make the plots simpler we
multiplied by zero the accuracies which were not
significantly higher than chance. We expand a few
characteristic plots. We see that in the back of the
head the embedding features have an accuracy that
seems to peak very early on. As we move forward
in the brain towards the left and right temporal cor-
tices, we see the embeddings accuracy peaking at
a later time, reflecting the delay it takes for the in-
formation to reach this part of the brain. The out-
put probability start being useful for classification
after the embeddings, and specifically in the left
temporal cortex which is the cite where the N400
241
Back%
 
 
hidden layer
output probability
embeddings 0 5000.50.6
0.7
time (s)ac
cur
acy
Le(% Right%
 
 
hidden layer
output probability
embeddings 0 5000.5
0.60.7
time (s)a
ccu
rac
y
 
 
hidden layer
output probability
embeddings 0 5000.5
0.60.7
time (s)a
ccu
rac
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
Figure 9: Average accuracy in time and space on the MEG helmet when using the RNNLM features. For each of the 102
locations the average accuracy for the group of sensors centered at that location is plotted versus time. The axes are defined
in the rightmost, empty plot. Three plots have been magnified to show the increasing delay in high accuracy when using the
embeddings feature, reflecting the delay in processing the incoming word as information travels through the brain. A sensor
map is provided in the lower right corner: visual cortex = cyan, temporal = red, frontal = dark green.
is reported in the literature. Finally, as we reach
the frontal cortex, we see that the embeddings fea-
tures have an even later accuracy peak.
4 Conclusion and contributions
Novel brain data exploration We present here
a novel and revealing approach to shed light on
the brain processes involved in reading. This is a
departure from the classical approach of control-
ling for a few variables in the text (e.g. showing
a sentence with an expected target word versus an
unexpected one). While we cannot make clear cut
causal claims because we did not control for our
variables, we are able to explore the data much
more and offer a much richer interpretation than
is possible with artificially constrained stimuli.
Comparing two models of language Adding
brain data into the equation allowed us to com-
pare the performance of the models and to identify
a slight advantage for the RNNLM in capturing
the text contents. Numerical comparison is how-
ever a secondary contribution of our approach. We
showed that it might be possible to use brain data
to understand, interpret and illustrate what exactly
is being encoded by the obscure vectors that neural
networks compute, by drawing parallels between
the models constituents and brain processes.
Anecdotally, in the process of running the ex-
periments, we noticed that the accuracy for the
hidden layer of the RNNLM was peaking in the
time window corresponding to word i?2, and that
it was decreasing during word i ? 1. Since this
was against our expectations, we went back and
looked at the code and found that it was indeed
returning a delayed value and corrected the fea-
tures. We therefore used the brain data in order to
correct a mis-specification in our neural network
model. This hints if not proves the potential of our
approach for assessing language models.
Future Work The work described here is our
first attempt along the promising endeavor of
matching complex computational models of lan-
guage with brain processes using brain recordings.
We plan to extend our efforts by (1) collecting data
from more subjects and using various types of text
and (2) make the brain data help us with training
better statistical language models by using it to de-
termine whether the models are expressive enough
or have reached a sufficient degree of convergence.
Acknowledgements
This research was supported in part by NICHD
grant 5R01HD07328-02. We thank Nicole Rafidi
for help with data acquisition.
242
References
Yoav Benjamini and Daniel Yekutieli. 2001. The con-
trol of the false discovery rate in multiple testing un-
der dependency. Annals of statistics, pages 1165?
1188.
Augusto Buchweitz, Robert A Mason, L?eda Tomitch,
and Marcel Adam Just. 2009. Brain activation
for reading and listening comprehension: An fMRI
study of modality effects and individual differences
in language comprehension. Psychology & neuro-
science, 2(2):111?123.
Kacper Chwialkowski and Arthur Gretton. 2014.
A kernel independence test for random processes.
arXiv preprint arXiv:1402.4501.
Ronald Aylmer Fisher. 1925. Statistical methods for
research workers. Genesis Publishing Pvt Ltd.
Stefan L Frank, Leun J Otten, Giulia Galli, and
Gabriella Vigliocco. 2013. Word surprisal predicts
N400 amplitude during reading. In Proceedings of
the 51st annual meeting of the Association for Com-
putational Linguistics, pages 878?883.
Angela D Friederici. 2002. Towards a neural basis
of auditory sentence processing. Trends in cognitive
sciences, 6(2):78?84.
Gene H Golub, Michael Heath, and Grace Wahba.
1979. Generalized cross-validation as a method for
choosing a good ridge parameter. Technometrics,
21(2):215?223.
Peter Hagoort. 2003. How the brain solves the binding
problem for language: a neurocomputational model
of syntactic processing. Neuroimage, 20:S18?S29.
Nikolaus Kriegeskorte, Rainer Goebel, and Peter Ban-
dettini. 2006. Information-based functional brain
mapping. Proceedings of the National Academy
of Sciences of the United States of America,
103(10):3863?3868.
Tomas Mikolov, Stefan Kombrink, Anoop Deoras,
Lukar Burget, and J Cernocky. 2011. RNNLM-
recurrent neural network language modeling toolkit.
In Proc. of the 2011 ASRU Workshop, pages 196?
201.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted Boltzmann machines.
In Proceedings of ICML, pages 807?814.
Joanne K. Rowling. 2012. Harry Potter and the Sor-
cerer?s Stone. Harry Potter US. Pottermore Limited.
Riitta Salmelin. 2007. Clinical neurophysiology of
language: the MEG approach. Clinical Neurophysi-
ology, 118(2):237?254.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking neural coding of percep-
tual and semantic features of concrete nouns. Neu-
roImage, 62(1):451?463.
Samu Taulu and Juha Simola. 2006. Spatiotem-
poral signal space separation method for rejecting
nearby interference in MEG measurements. Physics
in medicine and biology, 51(7):1759.
Samu Taulu, Matti Kajola, and Juha Simola. 2004.
Suppression of interference and artifacts by the sig-
nal space separation method. Brain topography,
16(4):269?275.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation.
Leila Wehbe, Brian Murphy, Partha Talukdar, Alona
Fyshe, Aaditya Ramdas, and Tom Mitchell. 2014.
Simultaneously uncovering the patterns of brain re-
gions involved in different story reading subpro-
cesses. in press.
243
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 397?406,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Incorporating Vector Space Similarity in Random Walk Inference over
Knowledge Bases
Matt Gardner
Carnegie Mellon University
mg1@cs.cmu.edu
Partha Talukdar
?
Indian Institute of Science
ppt@serc.iisc.in
Jayant Krishnamurthy
Carnegie Mellon University
jayantk@cs.cmu.edu
Tom Mitchell
Carnegie Mellon University
tom@cs.cmu.edu
Abstract
Much work in recent years has gone into
the construction of large knowledge bases
(KBs), such as Freebase, DBPedia, NELL,
and YAGO. While these KBs are very
large, they are still very incomplete, ne-
cessitating the use of inference to fill in
gaps. Prior work has shown how to make
use of a large text corpus to augment ran-
dom walk inference over KBs. We present
two improvements to the use of such large
corpora to augment KB inference. First,
we present a new technique for combin-
ing KB relations and surface text into a
single graph representation that is much
more compact than graphs used in prior
work. Second, we describe how to incor-
porate vector space similarity into random
walk inference over KBs, reducing the fea-
ture sparsity inherent in using surface text.
This allows us to combine distributional
similarity with symbolic logical inference
in novel and effective ways. With exper-
iments on many relations from two sepa-
rate KBs, we show that our methods sig-
nificantly outperform prior work on KB
inference, both in the size of problem our
methods can handle and in the quality of
predictions made.
1 Introduction
Much work in recent years has gone into the
construction of large knowledge bases, either
by collecting contributions from many users,
as with Freebase (Bollacker et al., 2008) and
?
Research carried out while at the Machine Learning
Department, Carnegie Mellon University.
DBPedia (Mendes et al., 2012), or automat-
ically from web text or other resources, as
done by NELL (Carlson et al., 2010) and
YAGO (Suchanek et al., 2007). These knowl-
edge bases contain millions of real-world enti-
ties and relationships between them. However,
even though they are very large, they are still
very incomplete, missing large fractions of possi-
ble relationships between common entities (West
et al., 2014). Thus the task of inference over
these knowledge bases, predicting new relation-
ships simply by examining the knowledge base it-
self, has become increasingly important.
A promising technique for inferring new re-
lation instances in a knowledge base is random
walk inference, first proposed by Lao and Cohen
(2010). In this method, called the Path Ranking
Algorithm (PRA), the knowledge base is encoded
as a graph, and random walks are used to find
paths that connect the source and target nodes of
relation instances. These paths are used as features
in a logistic regression classifier that predicts new
instances of the given relation. Each path can be
viewed as a horn clause using knowledge base re-
lations as predicates, and so PRA can be thought
of as a kind of discriminatively trained logical in-
ference.
One major deficiency of random walk inference
is the connectivity of the knowledge base graph?
if there is no path connecting two nodes in the
graph, PRA cannot predict any relation instance
between them. Thus prior work has introduced the
use of a text corpus to increase the connectivity of
the graph used as input to PRA (Lao et al., 2012;
Gardner et al., 2013). This approach is not without
its own problems, however. Whereas knowledge
base relations are semantically coherent and dif-
ferent relations have distinct meanings, this is not
397
true of surface text. For example, ?The Nile flows
through Cairo? and ?The Nile runs through Cairo?
have very similar if not identical meaning. Adding
a text corpus to the inference graph increases con-
nectivity, but it also dramatically increases feature
sparsity.
We introduce two new techniques for making
better use of a text corpus for knowledge base in-
ference. First, we describe a new way of incor-
porating the text corpus into the knowledge base
graph that enables much more efficient process-
ing than prior techniques, allowing us to approach
problems that prior work could not feasibly solve.
Second, we introduce the use of vector space sim-
ilarity in random walk inference in order to reduce
the sparsity of surface forms. That is, when fol-
lowing a sequence of edge types in a random walk
on a graph, we allow the walk to follow edges that
are semantically similar to the given edge types,
as defined by some vector space embedding of the
edge types. If a path calls for an edge of type
?flows through?, for example, we accept other
edge types (such as ?runs through?) with probabil-
ity proportional to the vector space similarity be-
tween the two edge types. This lets us combine
notions of distributional similarity with symbolic
logical inference, with the result of decreasing the
sparsity of the feature space considered by PRA.
We show with experiments using both the NELL
and Freebase knowledge bases that this method
gives significantly better performance than prior
approaches to incorporating text data into random
walk inference.
2 Graph Construction
Our method for knowledge base inference, de-
scribed in Section 3, performs random walks over
a graph to obtain features for a logistic regression
classifier. Prior to detailing that technique, we first
describe how we produce a graph G = (N , E ,R)
from a set of knowledge base (KB) relation in-
stances and a set of surface relation instances ex-
tracted from a corpus. Producing a graph from
a knowledge base is straightforward: the set of
nodes N is made up of the entities in the KB; the
set of edge types R is the set of relation types in
the KB, and the typed edges E correspond to re-
lation instances from the KB, with one edge of
type r connecting entity nodes for each (n
1
, r, n
2
)
triple in the KB. Less straightforward is how to
construct a graph from a corpus, and how to con-
nect that graph to the KB graph. We describe our
methods for each of those below.
To create a graph from a corpus, we first prepro-
cess the corpus to obtain a collection of surface
relations, such as those extracted by open infor-
mation extraction systems like OLLIE (Mausam et
al., 2012). These surface relations consist of a pair
of noun phrases in the corpus, and the verb-like
connection between them (either an actual verb,
as done by Talukdar et al. (2012), a dependency
path, as done by Riedel et al. (2013), or OpenIE
relations (Mausam et al., 2012)). The verb-like
connections are naturally represented as edges in
the graph, as they have a similar semantics to the
knowledge base relations that are already repre-
sented as edges. We thus create a graph from these
triples exactly as we do from a KB, with nodes cor-
responding to noun phrase types and edges corre-
sponding to surface relation triples.
So far these two subgraphs we have created
are entirely disconnected, with the KB graph con-
taining nodes representing entities, and the sur-
face relation graph containing nodes representing
noun phrases, with no edges between these noun
phrases and entities. We connect these two graphs
by making use of the ALIAS relation in the KB,
which links entities to potential noun phrase ref-
erents. Each noun phrase in the surface relation
graph is connected to those entity nodes which the
noun phrase can possibly refer to according to the
KB. These edges are not the output of an entity
linking system, as done by Lao et al. (2012), but
express instead the notion that the noun phrase can
refer to the KB entity. The use of an entity linking
system would certainly allow a stronger connec-
tion between noun phrase nodes and entity nodes,
but it would require much more preprocessing and
a much larger graph representation, as each men-
tion of each noun phrase would need its own node,
as opposed to letting every mention of the same
noun phrase share the same node. This graph rep-
resentation allows us to add tens of millions of sur-
face relations to a graph of tens of millions of KB
relations, and perform all of the processing on a
single machine.
As will be discussed in more detail in Section 4,
we also allow edge types to optionally have an as-
sociated vector that ideally captures something of
the semantics of the edge type.
Figure 1 shows the graph constructions used in
our experiments on a subset of KB and surface re-
398
KB Relations:
(Monongahela, RIVERFLOWSTHROUGHCITY, Pittsburgh)
(Pittsburgh, ALIAS, ?Pittsburgh?)
(Pittsburgh, ALIAS, ?Steel City?)
(Monongahela, ALIAS, ?Monongahela River?)
(Monongahela, ALIAS, ?The Mon?)
Surface Relations:
(?The Mon?, ?flows through?, ?Steel City?)
(?Monongahela River?, ?runs through?, ?Pittsburgh?)
Embeddings:
?flows through?: [.2, -.1, .9]
?runs through?: [.1, -.3, .8]
(a) An example data set.
(c) An example graph that replaces surface relations with a
cluster label, as done by Gardner et al. (2013). Note, how-
ever, that the graph structure differs from that prior work;
see Section 5.
(b) An example graph that combines a KB and surface rela-
tions.
(d) An example graph that uses vector space representations
of surface edges, as introduced in this paper.
Figure 1: Example graph construction as used in the experiments in this paper. A graph using only KB
edges is simply a subset of these graphs containing only the RIVERFLOWSTHROUGHCITY edge, and is
not shown.
lations. Note that Figures 1b and 1c are shown as
rough analogues of graphs used in prior work (de-
scribed in more detail in Section 5), and we use
them for comparison in our experiments.
3 The Path Ranking Algorithm
We perform knowledge base inference using the
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010). We begin this section with a brief overview
of PRA, then we present our modification to the
PRA algorithm that allows us to incorporate vector
space similarity into random walk inference.
PRA can be thought of as a method for exploit-
ing local graph structure to generate non-linear
feature combinations for a prediction model. PRA
generates a feature matrix over pairs of nodes in
a graph, then uses logistic regression to classify
those node pairs as belonging to a particular rela-
tion.
More formally, given a graph G with nodes N ,
edges E , and edge labelsR, and a set of node pairs
(s
i
, t
i
) ? D, one can create a connectivity matrix
where rows correspond to node pairs and columns
correspond to edge lables. PRA augments this
matrix with additional columns corresponding to
sequences of edge labels, called path types, and
changes the cell values from representing the pres-
ence of an edge to representing the specificity of
the connection that the path type makes between
the node pair.
Because the feature space considered by PRA
is so large (the set of all possible edge label se-
quences, with cardinality
?
l
i=1
|R|
i
, assuming a
bound l on the maximum path length), the first
step PRA must perform is feature selection, which
is done using random walks over the graph. The
second step of PRA is feature computation, where
each cell in the feature matrix is computed using
a constrained random walk that follows the path
type corresponding to the feature. We now explain
each of these steps in more detail.
Feature selection finds path types pi that are
likely to be useful in predicting new instances of
the relation represented by the input node pairs .
These path types are found by performing random
walks on the graph G starting at the source and
target nodes in D, recording which paths connect
some source node with its target. The edge se-
quences are ranked by frequency of connecting a
source node to a corresponding target node, and
the top k are kept.
Feature computation. Once a set of path types
399
is selected as features, the next step of the PRA
algorithm is to compute a value for each cell in the
feature matrix, corresponding to a node pair and a
path type. The value computed is the probability
of arriving at the target node of a node pair, given
that a random walk began at the source node and
was constrained to follow the path type: p(t|s, pi).
Once these steps have been completed, the re-
sulting feature matrix can be used with whatever
model or learning algorithm is desired; in this and
prior work, simple logistic regression has been
used as the prediction algorithm.
4 Vector space random walks
Our modifications to PRA are confined entirely to
the feature computation step described above; fea-
ture selection (finding potentially useful sequences
of edge types) proceeds as normal, using the sym-
bolic edge types. When computing feature val-
ues, however, we allow a walk to follow an edge
that is semantically similar to the edge type in the
path, as defined by Euclidean distance in the vec-
tor space.
More formally, consider a path type pi. Re-
call that pi is a sequence of edge types <
e
1
, e
2
, . . . , e
l
>, where l is the length of the path;
we will use pi
i
to denote the i
th
edge type in the
sequence. To compute feature values, PRA begins
at some node and follows edges of type pi
i
until
the sequence is finished and a target node has been
reached. Specifically, if a random walk is at a node
n with m outgoing edge types {e
1
, e
2
, . . . , e
m
},
PRA selects the edge type from that set which
matches pi
i
, then selects uniformally at random
from all outgoing edges of that type. If there is
no match in the set, the random walk restarts from
the original start node.
We modify the selection of which edge type to
follow. When a random walk is at a node n with
m outgoing edge types {e
1
, e
2
, . . . , e
m
}, instead
of selecting only the edge type that matches pi
i
,
we allow the walk to select instead an edge that
is close to pi
i
in vector space. For each edge type
at node n, we select the edge with the following
probability:
p(e
j
|pi
i
) ? exp(??v(e
j
) ?v(pi
i
)), ?j, 1 ? j ? m
where v(?) is a function that returns the vector
representation of an edge type, and ? is a spiki-
ness parameter that determines how much weight
to give to the vector space similarity. As ? ap-
proaches infinity, the normalized exponential ap-
proximates a delta function on the closest edge
type to pi
i
, in {e
1
, e
2
, . . . , e
m
}. If pi
i
is in the set
of outgoing edges, this algorithm converges to the
original PRA.
However, if pi
i
is not in the set of outgoing edge
types at a node and all of the edge types are very
dissimilar to pi
i
, this algorithm (with ? not close to
infinity) will lead to a largely uniform distribution
over edge types at that node, and no way for the
random walk to restart. To recover the restart be-
havior of the original PRA, we introduce an addi-
tional restart parameter?, and add another value to
the categorical distribution before normalization:
p(restart|pi
i
) ? exp(? ? ?)
When this restart type is selected, the random
walk begins again, following pi
1
starting at the
source node. With ? set to a value greater than the
maximum similarity between (non-identical) edge
type vectors, and ? set to infinity, this algorithm
exactly replicates the original PRA.
Not all edge types have vector space representa-
tions: the ALIAS relation cannot have a meaning-
ful vector representation, and we do not use vec-
tors to represent KB relations, finding that doing
so was not useful in practice (which makes intu-
itive sense: KB relations are already latent repre-
sentations themselves). While performing random
walks, if pi
i
has no vector representation, we fall
back to the original PRA algorithm for selecting
the next edge.
We note here that when working with vector
spaces it is natural to try clustering the vectors to
reduce the parameter space. Each path type pi is
a feature in our model, and if two path types dif-
fer only in one edge type, and the differing edge
types have very similar vectors, the resultant fea-
ture values will be essentially identical for both
path types. It seems reasonable that running a
simple clustering algorithm over these path types,
to reduce the number of near-duplicate features,
would improve performance. We did not find this
to be the case, however; all attempts we made to
use clustering over these vectors gave performance
indistinguishable from not using clustering. From
this we conclude that the main issue hindering per-
formance when using PRA over these kinds of
graphs is one of limited connectivity, not one of
too many parameters in the model. Though the
400
feature space considered by PRA is very large, the
number of attested features in a real graph is much
smaller, and it is this sparsity which our vector
space methods address.
5 Related Work
Knowledge base inference. Random walk infer-
ence over knowledge bases was first introduced by
Lao and Cohen (2010). This work was improved
upon shortly afterward to also make use of a large
corpus, by representing the corpus as a graph and
connecting it to the knowledge base (Lao et al.,
2012). Gardner et al. (2013) further showed that
replacing surface relation labels with a represen-
tation of a latent embedding of the relation led
to improved prediction performance. This result
is intuitive: the feature space considered by PRA
is exponentially large, and surface relations are
sparse. The relations ?[river] flows through [city]?
and ?[river] runs through [city]? have near iden-
tical meaning, and both should be very predic-
tive for the knowledge base relation RIVERFLOW-
STHROUGHCITY. However, if one of these rela-
tions only appears in the training data and the other
only appears in the test data, neither will be useful
for prediction. Gardner et al. (2013) attempted to
solve this issue by finding a latent symbolic repre-
sentation of the surface relations (such as a cluster-
ing) and replacing the edge labels in the graph with
these latent representations. This makes it more
likely for surface relations seen in training data to
also be seen at test time, and naturally improved
performance.
This representation, however, is still brittle, as
it is still a symbolic representation that is prone to
mismatches between training and test data. If the
clustering algorithm used is too coarse, the fea-
tures will not be useful, and if it is too fine, there
will be more mismatches. Also, verbs that are on
the boundaries of several clusters are problematic
to represent in this manner. We solve these prob-
lems by modifying the PRA algorithm to directly
use vector representations of edge types during the
random walk inference.
These two prior techniques are the most directly
related work to what we present in this paper, and
we compare our work to theirs.
Graph construction. In addition to the incor-
poration of vector space similarity into the PRA
algorithm, the major difference between our work
and the prior approaches mentioned above is in the
construction of the graph used by PRA. We con-
trast our method of graph construction with these
prior approaches in more detail below.
Lao et al. (2012) represent every word of ev-
ery sentence in the corpus as a node in the graph,
with edges between the nodes representing depen-
dency relationships between the words. They then
connect this graph to the KB graph using a simple
entity linking system (combined with coreference
resolution). The resultant graph is enormous, such
that they needed to do complex indexing on the
graph and use a cluster of 500 machines to per-
form the PRA computations. Also, as the edges
represent dependency labels, not words, with this
graph representation the PRA algorithm does not
have access to the verbs or other predicative words
that appear in the corpus, which frequently express
relations. PRA only uses edge types as feature
components, not node types, and so the rich infor-
mation contained in the words is lost. This graph
construction also would not allow the incorpora-
tion of vector space similarity that we introduced,
as dependency labels do not lend themselves well
to vector space representations.
Gardner et al. (2013) take an approach very sim-
ilar to the one presented in Section 2, preprocess-
ing the corpus to obtain surface relations. How-
ever, instead of creating a graph with nodes rep-
resenting noun phrases, they added edges from
the surface relations directly to the entity nodes
in the graph. Using the ALIAS relation, as we do,
they added an edge between every possible con-
cept pair that could be represented by the noun
phrases in a surface relation instance. This leads
to some nonsensical edges added to the graph,
and if the ALIAS relation has high degree (as it
does for many common noun phrases in Freebase),
it quickly becomes unscalable?this method of
graph construction runs out of disk space when
attempting to run on the Freebase experiments in
Section 6. Also, in conflating entity nodes in the
graph with noun phrases, they lose an important
distinction that turns out to be useful for predic-
tion, as we discuss in Section 6.4.
1
1
Recent notions of ?universal schema? (Riedel et al.,
2013) also put KB entities and noun phrases into the same
conceptual space, though they opt for using noun phrases in-
stead of the KB entities used by Gardner et al. In general
this is problematic, as it relies on some kind of entity linking
system as preprocessing, and cannot handle common noun
references of proper entities without losing information. Our
method, and that of Lao et al., skirts this issue entirely by not
trying to merge KB entities with noun phrases.
401
Other related work. Also related to the present
work is recent research on programming lan-
guages for probabilistic logic (Wang et al., 2013).
This work, called ProPPR, uses random walks to
locally ground a query in a small graph before per-
forming propositional inference over the grounded
representation. In some sense this technique is
like a recursive version of PRA, allowing for more
complex inferences than a single iteration of PRA
can make. However, this technique has not yet
been extended to work with large text corpora, and
it does not yet appear to be scalable enough to han-
dle the large graphs that we use in this work. How
best to incorporate the work presented in this pa-
per with ProPPR is an open, and very interesting,
question.
Examples of other systems aimed at reason-
ing over common-sense knowledge are the CYC
project (Lenat, 1995) and ConceptNet (Liu and
Singh, 2004). These common-sense resources
could easily be incorporated into the graphs we
use for performing random walk inference.
Lines of research that seek to incorporate dis-
tributional semantics into traditional natural lan-
guage processing tasks, such as parsing (Socher
et al., 2013a), named entity recognition (Passos et
al., 2014), and sentiment analysis (Socher et al.,
2013b), are also related to what we present in this
paper. While our task is quite different from these
prior works, we also aim to combine distributional
semantics with more traditional methods (in our
case, symbolic logical inference), and we take in-
spiration from these methods.
6 Experiments
We perform both the feature selection step and the
feature computation step of PRA using GraphChi,
an efficient single-machine graph processing li-
brary (Kyrola et al., 2012). We use MAL-
LET?s implementation of logistic regression, with
both L1 and L2 regularization (McCallum, 2002).
To obtain negative evidence, we used a closed
world assumption, treating any (source, target)
pair found during the feature computation step as
a negative example if it was not given as a positive
example. We tuned the parameters to our methods
using a coarse, manual grid search with cross vali-
dation on the training data described below. The
parameters we tuned were the L1 and L2 regu-
larization parameters, how many random walks to
perform in the feature selection and computation
NELL Freebase
Entities 1.2M 20M
Relation instances 3.4M 67M
Total relation types 520 4215
Relation types tested 10 24
Avg. instances/relation 810 200
SVO triples used 404k 28M
Table 1: Statistics of the data used in our experi-
ments.
steps of PRA, and spikiness and restart parameters
for vector space walks. The results presented were
not very sensitive to changes in these parameters.
6.1 Data
We ran experiments on both the NELL and Free-
base knowledge bases. The characteristics of these
knowledge bases are shown in Table 1. The Free-
base KB is very large; to make it slightly more
manageable we filtered out relations that did not
seem applicable to relation extraction, as well as a
few of the largest relations.
2
This still left a very
large, mostly intact KB, as can be seen in the ta-
ble. For our text corpus, we make use of a set of
subject-verb-object triples extracted from depen-
dency parses of ClueWeb documents (Talukdar et
al., 2012). There are 670M such triples in the
data set, most of which are completely irrelevant to
the knowledge base relations we are trying to pre-
dict. For each KB, we filter the SVO triples, keep-
ing only those which can possibly connect training
and test instances of the relations we used in our
experiments. The number of SVO triples kept for
each KB is also shown in Table 1. We obtained
vector space representations of these surface rela-
tions by running PCA on the SVO matrix.
We selected 10 NELL relations and 24 Free-
base relations for testing our methods. The NELL
relations were hand-selected as the relations with
the largest number of known instances that had a
reasonable precision (the NELL KB is automati-
cally created, and some relations have low preci-
sion). We split the known instances of these rela-
tions into 75% training and 25% testing, giving on
average about 650 training instances and 160 test
2
We removed anything under /user, /common, /type (ex-
cept for the relation /type/object/type), /base, and /freebase,
as not applicable to our task. We also removed relations deal-
ing with individual music tracks, book editions, and TV epid-
sodes, as they are very large, very specific, and unlikely to be
useful for predicting the relations in our test set.
402
instances for each relation.
The 24 Freebase relations were semi-randomly
selected. We first filtered the 4215 relations based
on two criteria: the number of relation instances
must be between 1000 and 10000, and there must
be no mediator in the relation.
3
Once we selected
the relations, we kept all instances of each rela-
tion that had some possible connection in the SVO
data.
4
This left on average 200 instances per rela-
tion, which we again split 75%-25% into training
and test sets.
6.2 Methods
The methods we compare correspond to the graphs
shown in Figure 1. The KB method uses the orig-
inal PRA algorithm on just the KB relations, as
presented by Lao and Cohen (2010). KB + SVO
adds surface relations to the graph (Figure 1b). We
present this as roughly analogous to the methods
introduced by Lao et al. (2012), though with some
significant differences in graph representation, as
described in Section 5. KB + Clustered SVO fol-
lows the methods of Gardner et al. (2013), but us-
ing the graph construction introduced in this pa-
per (Figure 1c; their graph construction techniques
would have made graphs too large to be feasible
for the Freebase experiments). KB + Vector SVO
is our method (Figure 1d).
6.3 Evaluation
As evaluation metrics, we use mean average pre-
cision (MAP) and mean reciprocal rank (MRR),
following recent work evaluating relation extrac-
tion performance (West et al., 2014). We test sig-
nificance using a paired permutation test.
The results of these experiments are shown in
Table 2 and Table 3. In Table 4 we show average
precision for every relation tested on the NELL
KB, and we show the same for Freebase in Table 5.
6.4 Discussion
We can see from the tables that KB + Vector SVO
(the method presented in this paper) significantly
outperforms prior approaches in both MAP and
3
A mediator in Freebase is a reified relation in-
stance meant to handle n-ary relations, for instance
/film/performance. PRA in general, and our implementation
of it in particular, needs some modification to be well-suited
to predicting relations with mediators.
4
We first tried randomly selecting instances from these re-
lations, but found that the probability of selecting an instance
that benefited from an SVO connection was negligible. In or-
der to make use of the methods we present, we thus restricted
ourselves to only those that had a possible SVO connection.
Method MAP MRR
KB 0.193 0.635
KB + SVO 0.218 0.763
KB + Clustered SVO 0.276 0.900
KB + Vector SVO 0.301 0.900
Table 2: Results on the NELL knowledge base.
The bolded line is significantly better than all other
results with p < 0.025.
Method MAP MRR
KB 0.278 0.614
KB + SVO 0.294 0.639
KB + Clustered SVO 0.326 0.651
KB + Vector SVO 0.350 0.670
Table 3: Results on the Freebase knowledge base.
The bolded line is significantly better than all other
results with p < 0.0002.
MRR. We believe that this is due to the reduction
in feature sparsity enabled by using vector space
instead of symbolic representations (as that is the
only real difference between KB + Clustered SVO
and KB + Vector SVO), allowing PRA to make
better use of path types found in the training data.
When looking at the results for individual relations
in Table 4 and Table 5, we see that KB + Vector
SVO outperforms other methods on the majority
of relations, and it is a close second when it does
not.
We can also see from the results that mean av-
erage precision seems a little low for all meth-
ods tested. This is because MAP is computed as
the precision of all possible correct predictions in
a ranked list, where precision is counted as 0 if
the correct prediction is not included in the list.
In other words, there are many relation instances
in our randomly selected test set that are not in-
ferrable from the knowledge base, and the low re-
call hurts the MAP metric. MRR, which judges the
precision of the top prediction for each relation,
gives us some confidence that the main issue here
is one of recall, as MRR is reasonably high, es-
pecially on the NELL KB. As further evidence, if
we compute average precision for each query node
(instead of for each relation), excluding queries for
which the system did not return any predictions,
MAP ranges from .29 (KB) to .45 (KB + Vector
SVO) on NELL (with around 30% of queries hav-
ing no prediction), and from .40 (KB) to .49 (KB +
403
Relation KB KB + SVO KB + Clustered SVO KB + Vector SVO
ActorStarredInMovie 0.000 0.032 0.032 0.037
AthletePlaysForTeam 0.200 0.239 0.531 0.589
CityLocatedInCountry 0.126 0.169 0.255 0.347
JournalistWritesForPublication 0.218 0.254 0.291 0.319
RiverFlowsThroughCity 0.000 0.001 0.052 0.076
SportsTeamPositionForSport 0.217 0.217 0.178 0.180
StadiumLocatedInCity 0.090 0.156 0.275 0.321
StateHasLake 0.000 0.000 0.000 0.000
TeamPlaysInLeague 0.934 0.936 0.947 0.939
WriterWroteBook 0.144 0.179 0.195 0.202
Table 4: Average precision for each relation tested on the NELL KB. The best performing method on
each relation is bolded.
Relation KB KB + SVO KB + C-SVO KB + V-SVO
/amusement parks/park/rides 0.000 0.009 0.004 0.013
/architecture/architect/structures designed 0.072 0.199 0.257 0.376
/astronomy/constellation/contains 0.004 0.017 0.000 0.008
/automotive/automotive class/examples 0.003 0.001 0.002 0.006
/automotive/model/automotive class 0.737 0.727 0.742 0.768
/aviation/airline/hubs 0.322 0.286 0.298 0.336
/book/literary series/author s 0.798 0.812 0.818 0.830
/computer/software genre/software in genre 0.000 0.001 0.001 0.001
/education/field of study/journals in this discipline 0.001 0.003 0.003 0.001
/film/film/rating 0.914 0.905 0.914 0.905
/geography/island/body of water 0.569 0.556 0.580 0.602
/geography/lake/basin countries 0.420 0.361 0.409 0.437
/geography/lake/cities 0.111 0.134 0.177 0.175
/geography/river/cities 0.030 0.038 0.045 0.066
/ice hockey/hockey player/hockey position 0.307 0.243 0.222 0.364
/location/administrative division/country 0.989 0.988 0.991 0.989
/medicine/disease/symptoms 0.061 0.078 0.068 0.067
/medicine/drug/drug class 0.169 0.164 0.135 0.157
/people/ethnicity/languages spoken 0.134 0.226 0.188 0.223
/spaceflight/astronaut/missions 0.010 0.186 0.796 0.848
/transportation/bridge/body of water spanned 0.534 0.615 0.681 0.727
/tv/tv program creator/programs created 0.164 0.179 0.163 0.181
/visual art/art period movement/associated artists 0.044 0.040 0.046 0.037
/visual art/visual artist/associated periods or movements 0.276 0.295 0.282 0.290
Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on
each relation is bolded. For space considerations, ?Clustered SVO? is shortened to ?C-SVO? and ?Vector
SVO? is shortened to ?V-SVO? in the table header.
404
Vector SVO) on Freebase, (where 21% of queries
gave no prediction). Our methods thus also im-
prove MAP when calculated in this manner, but it
is not an entirely fair metric,
5
so we use standard
MAP to present our main results.
One interesting phenomenon to note is
a novel use of the ALIAS relation in some
of the relation models. The best exam-
ple of this was found with the relation
/people/ethnicity/languages spoken. A
high-weighted feature when adding surface
relations was the edge sequence <ALIAS, ALIAS
INVERSE>. This edge sequence reflects the
fact that languages frequently share a name
with the group of people that speaks them (e.g.,
Maori, French). And because PRA can gen-
erate compositional features, we also find the
following edge sequence for the same relation:
</people/ethnicity/included in group,
ALIAS, ALIAS INVERSE>. This feature captures
the same notion that languages get their names
from groups of people, but applies it to subgroups
within an ethnicity. These features would be
very difficult, perhaps impossible, to include in
systems that do not distinguish between noun
phrases and knowledge base entities, such as
the graphs constructed by Gardner et al. (2013),
or typical relation extraction systems, which
generally only work with noun phrases after
performing a heuristic entity linking.
7 Conclusion
We have offered two main contributions to the task
of knowledge base inference. First, we have pre-
sented a new technique for combining knowledge
base relations and surface text into a single graph
representation that is much more compact than
graphs used in prior work. This allowed us to ap-
ply methods introduced previously to much larger
problems, running inference on a single machine
over the entire Freebase KB combined with tens of
millions of surface relations. Second, we have de-
scribed how to incorporate vector space similarity
into random walk inference over knowledge bases,
reducing the feature sparsity inherent in using sur-
face text. This allows us to combine distributional
similarity with symbolic logical inference in novel
and effective ways. With experiments on many
5
MAP is intended to include some sense of recall, but ex-
cluding queries with no predictions removes that and opens
the metric to opportunistic behavior.
relations from two separate knowledge bases, we
have shown that our methods significantly outper-
form prior work on knowledge base inference.
The code and data used in the ex-
periments in this paper are available at
http://rtw.ml.cmu.edu/emnlp2014 vector space pra/.
Acknowledgments
This research has been supported in part by
DARPA under contract number FA8750-13-2-
0005, by NSF under grant 31043,18,1121946, and
by generous support from Yahoo! and Google.
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of SIGMOD.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and
inference in a large knowledge-base using latent
syntactic cues. In Proceedings of EMNLP. Associa-
tion for Computational Linguistics.
Aapo Kyrola, Guy Blelloch, and Carlos Guestrin.
2012. Graphchi: Large-scale graph computation
on just a pc. In Proceedings of the 10th USENIX
Symposium on Operating Systems Design and Im-
plementation (OSDI), pages 31?46.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53?67.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL.
Douglas B Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
Hugo Liu and Push Singh. 2004. Conceptnet: a practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4):211?226.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523?534. Asso-
ciation for Computational Linguistics.
405
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Pablo N. Mendes, Max Jakob, and Christian Bizer.
2012. Dbpedia for nlp: A multilingual cross-domain
knowledge base. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC?12).
Alexandre Passos, Vineet Kumar, and Andrew Mc-
Callum. 2014. Lexicon infused phrase embed-
dings for named entity resolution. arXiv preprint
arXiv:1404.5367.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Acquiring temporal constraints be-
tween relations. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 992?1001. ACM.
William Yang Wang, Kathryn Mazaitis, and
William W. Cohen. 2013. Programming with
personalized pagerank: A locally groundable
first-order probabilistic logic. In Proceedings of the
22Nd ACM International Conference on Conference
on Information &#38; Knowledge Management,
CIKM ?13, pages 2129?2138, New York, NY, USA.
ACM.
Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In WWW.
406
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1930?1936,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
CTPs: Contextual Temporal Profiles for Time Scoping Facts using
State Change Detection
Derry Tanti Wijaya
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
dwijaya@cs.cmu.edu
Ndapandula Nakashole
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
ndapa@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
tom.mitchell@cs.cmu.edu
Abstract
Temporal scope adds a time dimension to
facts in Knowledge Bases (KBs). These
time scopes specify the time periods when
a given fact was valid in real life. With-
out temporal scope, many facts are under-
specified, reducing the usefulness of the
data for upper level applications such as
Question Answering. Existing methods
for temporal scope inference and extrac-
tion still suffer from low accuracy. In this
paper, we present a new method that lever-
ages temporal profiles augmented with
context? Contextual Temporal Profiles
(CTPs) of entities. Through change pat-
terns in an entity?s CTP, we model the en-
tity?s state change brought about by real
world events that happen to the entity (e.g,
hired, fired, divorced, etc.). This leads to
a new formulation of the temporal scoping
problem as a state change detection prob-
lem. Our experiments show that this for-
mulation of the problem, and the resulting
solution are highly effective for inferring
temporal scope of facts.
1 Introduction
Recent years have seen the emergence of large
Knowledge Bases (KBs) of facts (Carlson 2010;
Auer 2007; Bollacker 2008; Suchanek 2007).
While the wealth of accumulated facts is huge,
most KBs are still sparsely populated in terms of
temporal scope. Time information is an important
dimension in KBs because knowledge is not static,
it changes over time: people get divorced; coun-
tries elect new leaders; and athletes change teams.
This means that facts are not always indefinitely
true. Therefore, temporal scope has crucial impli-
cations for KB accuracy.
Figure 1: Behavior patterns of context uni-grams
for the US presidency state change as seen in the
Google Books N-grams corpus: the rise of ?elect?,
immediately followed by the rise of ?administra-
tion? and ?president?.
Towards bridging the time gap in KBs, we
propose a new method for temporal scope infer-
ence. Our method is based on leveraging aggre-
gate statistics from a time-stamped corpus. First
we generate Contextual Temporal Profiles (CTPs)
of entities from contexts surrounding mentions of
these entities in the corpus. We then detect change
patterns in the CTPs. We then use these changes
to determine when a given entity undergoes a spe-
cific state change caused by real world events. Our
main insight is as follows: events that happen to
an entity change the entity?s state and therefore its
facts. Thus by learning when a given entity under-
goes a specific state change, we can directly infer
the time scopes of its facts. For example, in the di-
vorce event, the person?s state changes from ?mar-
ried? to ?divorced? hence the hasSpouse relation
no longer applies to it, signaling the end time of
its current hasSpouse value. In a country election
event, the country?s state changes and it obtains a
new value for its hasPresident relation.
1930
Our method involves learning context units
(uni-grams and bi-grams surrounding mentions of
an entity) that are relevant to a given state change.
For this we use a few seed examples of entities that
have gone through the state change. For example,
for the US presidency state change denoting the
beginning of a US presidency, given seed exam-
ples such as (Richard Nixon, 1969) and (Jimmy
Carter, 1977), relevant context units include uni-
grams such as ?administration? and ?elect?, which
are common to both CTPs in 1969 and 1977 re-
spectively. Secondly, we learn the mention behav-
ior of these context units for an entity undergoing
a given state change (section 3 has more details).
Figure 1 shows a motivating example, we see the
behavior patterns of context uni-grams for the US
presidency state change: the rise of ?elect? at the
beginning of presidencies, immediately followed
by the rise of ?administration? and ?president? in
the context of the entities, Nixon and Carter.
2 Related work
Prior work mainly falls into two categories: i)
methods that extract temporal scope from text,
at the time of fact extraction; ii) methods that
infer temporal scope from aggregate statistics in
large Web corpora. Early methods mostly fall
under category i); Timely YAGO (Wang 2010),
TIE (Ling 2010), and PRAVDA (Wang 2011) are
three such methods. Timely YAGO applies regu-
lar expressions to Wikipedia infoboxes to extract
time scopes. It is therefore not applicable to any
other corpora but Wikipedia. The TIE (Ling 2010)
system produces a maximal set of events and their
temporal relations based on the text of a given sen-
tence. PRAVDA uses textual patterns along with
a graph-based re-ranking method. Methods falling
under category i) have the downside that it is un-
clear how they can be applied to facts that are al-
ready in the knowledge base. Only one other ap-
proach learned time scopes from aggregate cor-
pus statistics, a recent system called CoTS (Taluk-
dar 2012b). CoTS uses temporal profiles of facts
and how the mentions of such facts rise and fall
over time. However, CoTS is based on frequency
counts of fact mentions and does not take into ac-
count state change inducing context. For exam-
ple, to find the time scope of Nixon presidency,
CoTS uses the rise and fall of the mention ?nixon?
and ?president? over time. To improve accuracy,
CoTS combined this frequency signal with manu-
ally supplied constraints such as the functionality
of the US presidency relation to scope the begin-
ning and end of Nixon presidency. In contrast, the
proposed system does not require constraints as in-
put.
There have also been tools and competitions
developed to facilitate temporal scope extraction.
TARSQI (Verhagen 2005) is a tool for automat-
ically annotating time expressions in text. The
TempEval (Verhagen 2007) challenge has led to
a number of works on temporal relation extrac-
tion (Puscasu 2007; Yoshikawa 2009; Bethard
2007).
3 Method
Given an entity and its Contextual Temporal Pro-
file (CTP), we can learn when such an entity un-
dergoes a specific state change. We can then di-
rectly infer the begin or end time of the fact asso-
ciated with the state change.
The CTP of an entity at a given time point t con-
tains the context within which the entity is men-
tioned at that time. Our method is based on two
related insights: i) the context of the entity at time
t reflects the events happening to the entity and
the state of the entity at time t. ii) the differ-
ence in context before, at time t ? 1, and after, at
time t, reflect the associated state change at time
t. However an entity can undergo a multiplicity of
changes at the same time. Thus both the contexts
and the differences in contexts can contain infor-
mation pertaining to several state changes. We
therefore need a way of determining which part
of the context is relevant to a given state change
sc
i
. To this end, we generate what we refer to as
an aggregate state vector, V s(e, sc
i
) for a hypo-
thetical average entity e undergoing state change
sc
i
. We generate V s(e, sc
i
) from the CTPs of a
seed set of entities at the time they undergo state
change sc
i
.
3.1 Learning State and State Change Vectors
To build CTPs for entities, we use two time-
stamped corpora: the Google Books Ngram cor-
pus (Michel 2011); and the English Gigaword
(Graff 2003) corpus. The Google Books Ngram
corpus contains n-grams for n = 1?5; along with
occurrence statistics from over about 5 million
digitized books. The English Gigaword (Graff
1931
2003) corpus contains newswire text from 1994-
2008. From these corpora, we use the time granu-
larity of a year as it is the finest granularity com-
mon to both corpora.
Definition 1 (Contextual Temporal Profile)
The Contextual Temporal Profile (CTP) of an
entity e at time t, C
e
(t), consists of the context
within which e is mentioned. Specifically C
e
(t)
consists of uni-grams and bi-grams generated
from the 5-grams(Google Books Ngram) or
sentences (Gigaword) that mention e at time t.
Notice that the CTPs can contain context units
(bi-grams or uni-grams) that are simply noise. To
filter the noise, we compute tf-idf statistics for
each contextual unit and only retain the top k rank-
ing units in C
e
(t). In our experiments, we used
k = 100. We compute tf-idf by treating each time
unit t as a document containing words that occur
in the context of e (Wijaya 2011).
Furthermore, CTPs may contain context units
attributed to several state changes. We therefore
tease apart the CTPs to isolate contexts specific
to a given state change. For this, our method
takes as input a small set of seed entities, S(sc
i
),
for each type of state change. Thus for the US
presidency state change that denotes the begin-
ning of a US presidency, we would have seeds as
follows: (Richard Nixon, 1969), (Jimmy Carter,
1977). From the CTPs of the seeds for state
change sc
i
, we generate an aggregate state vector,
V s(e, sc
i
). To obtain the few dozen seeds required
by our method, one can leverage semi-structured
sources such as Wikipedia infoboxes, where rela-
tions e.g., spouse often have time information.
Definition 2 ( Aggregate State Vector for e)
The aggregate state vector of a mean entity
e for state change sc
i
, V s(e, sc
i
), is made
up of the contextual units from the CTPs of
entities in the seed set S(sc
i
) that undergo
state change sc
i
. Thus, we have: V s(e, sc
i
) =
1
|S(sc
i
)|
?
e,t:(e,t)?S(sc
i
)
C
e
(t).
Thus, the state vector V s(e, sc
i
) reflects events
happening to e and the state of e at the time it
undergoes the state change sc
i
. Additionally, we
compute another type of aggregate vector, aggre-
gate change vector 4V s(e, sc
i
) to capture the
change patterns in the context units of e. Recall
that context units rise or fall due to state change,
as seen earlier in Figure 1.
Definition 3 ( Aggregate Change Vector for e)
The aggregate change vector of a mean entity e
for state change sc
i
, 4V s(e, sc
i
), is made up of
the change in the contextual units of the CTPs
of entities in the seed set S(sc
i
) that undergo
state change sc
i
. Thus, we have: 4V s(e, sc
i
) =
1
|S(sc
i
)|
?
e,t:(e,t)?S(sc
i
)
C
e
(t)? C
e
(t? 1).
The aggregate state vector V s(e, sc
i
) and the
aggregate change vector 4V s(e, sc
i
) are then
used to detect state changes.
3.2 Detecting State Changes
To detect state changes in a previously unseen en-
tity e
new
, we generate its state vector, C
e
new
(t),
and its change vector, 4C
e
new
(t) = C
e
new
(t) -
C
e
new
(t ? 1), for every time point t. We consider
every time point t in the CTP of the new entity to
be a candidate for a given state change sc
i
, which
we seek to determine whether e
new
goes through
and at which time point. We then compare the
state vector and change vector of every candidate
time point t to the aggregate state and aggregate
change vector of state change sc
i
. We use cosine
similarity to measure similarities between the state
vector and the aggregate state vector and between
the change vector and the aggregate change vector.
To combine these two vector similarities, we sum
the state vector and change vector similarities. In
future we can explore cross validation and a sepa-
rate development set to define a weighted sum for
combining these two similarities.
The highest ranking candidate time point (most
similar to the aggregate state and aggregate change
vector) is then considered to be the start of state
change sc
i
for the new entity e
new
.
4 Experiments
We carried out experiments to answer the fol-
lowing questions: Is treating temporal scoping
as state change detection in Contextual Temporal
Profiles(CTPs) effective? Do CTPs help improve
temporal scope extraction over context-unaware
temporal profiles?
4.1 Methods under Comparison
We answer these questions by comparing to the
following methods.
1. CoTS a state-of-the-art temporal scoping
system (Talukdar 2012b)
1932
2. MaxEnt a baseline to which CoTS was com-
pared. It is a Maximum Entropy classifier
trained separately for each relation using nor-
malized counts and gradients of facts as fea-
tures. An Integer Linear Program (ILP) is
used to predict which facts are active at which
times. This is done based on the output of
the MAXENT classifier together with tem-
poral intra-relation constraints that regulate
the temporal scoping of one or more fac-
sts from a single relation (e.g., FUNCTIONAL
constraints on US President relation that reg-
ulate that at most one fact from the relation
can be true at any given time i.e., there is only
one US President at any given time).
3. MaxEnt + Intra Relation Constraints
MaxEnt with cross relation constraints
added: constraints that couple facts from
multiple relations e.g., a constraint that Al
Gore?s vice presidency is aligned exactly
with Bill Clinton?s presidency.
We evaluate on the same set of facts as CoTS
and its baselines: facts from the US Administra-
tion domain ( US President, US Vice President,
and US Secretary of State); and facts from the
Academy Awards domain (Best Director and Best
Picture). The number of facts per relation are as
follows: US President, 9; US Vice President, 12;
US Secretary of State, 13; Best Director, 14; and
Best Picture, 14. Our method however is not spe-
cific to these relations from these two domains.
Since our method does not depend on temporal
constraints, the method can work a very different
domain, for example one where many facts can ex-
ist for any time span without being superseded by
another, as long as the entities involved experience
a change of state. Thus, it can be applied to re-
lations like spouse, even though many people are
married in a year as these people change state from
single or engaged to married.
Similar to CoTS, the datasets from which the
CTPs were generated are as follows: The Google
Books Ngram (1960-2008) dataset (Michel 2011)
for the US Administration domain and the En-
glish Gigaword (1994-2008) dataset (Graff 2003)
for Academy Award domain.
Figure 2: Precision @ k using Contextual Tempo-
ral Profiles.
Figure 3: Comparison of F1 scores with CoTS and
other baselines.
4.2 CTPs Begin time precision
To compute precision we used cross validation,
in particular, leave-one-out cross validation due to
the small number of facts per relation.We predict
the begin time of each fact, the time the fact starts
to be valid. True begin times were determined by
a human annotator. This human annotated data
formed the gold-standard which we used to deter-
mine Precision (P), Recall (R), and the F1 mea-
sure. All evaluations were performed at the year
level, the finest granularity common to the two
time-stamped datasets.
For our first experiment, we report the aver-
age precision@k, where k=1 to n, where n=47 is
the number of years between 1960 to 2008 to se-
lect from. As can be seen in Figure 2, precision
quickly reaches 1 for most relations. The true be-
gin time is usually found within top k=5 results.
1933
4.3 Comparison to baselines
For our second experiment, we compared to the F1
scores of CoTS and other baselines in (Talukdar
2012b). As can be seen in Figure 3, our CTPs ap-
proach gives comparable or better F1 (@k=1) than
systems that use only plain temporal profiles, even
when these systems are supplemented with many
carefully crafted, hand-specified constraints.
We note that the performance on the US Secre-
tary of State relation is low in both CoTS (Taluk-
dar 2012b) and in our approach. We found that this
was due to few documents mentioning the ?sec-
retary of state? in Google Books Ngram dataset.
This leads to weak signals for predicting the tem-
poral scope of secretary of state appointments.
We also observe that the uni-grams and bi-
grams in the train CTPs and change vectors reflect
meaningful events and state changes happening to
the entities (Table 1). For example, after ?becom-
ing president? and ?taking office?, US presidents
often see a drop in mentions of their previous (job
title state) such as ?senator?, ?governor? or ?vice
president? as they gain the?president? state.
4.4 Discussion
Overall, our results show that our method is
promising for detecting begin time of facts. In its
current state, our method performs poorly on in-
ferring end times as contexts relevant to a fact of-
ten still mentioned with the entity even after the
fact ceases to be valid. For example, the entity
Al Gore is still mentioned a lot with the bi-gram
?vice president? even after he is no longer a vice
president. Prior work, CoTS, inferred end times
by leveraging manually specified constraints, e.g.,
that there can only be one vice president at a time:
the beginning of one signals the end of another
(Talukdar 2012b). However such methods do not
scale due to the amount of constraints that must be
hand-specified. In future, we would like to inves-
tigate how to better detect the end times of facts.
5 Conclusion
This paper presented a new approach for inferring
temporal scopes of facts. Our approach is to re-
formulate temporal scoping as a state change de-
tection problem. To this end, we introduced Con-
textual Temporal Profiles (CTPs) which are entity
temporal profiles enriched with relevant context.
Relation CTP State
Context
Unigrams and Bigrams
in CTP Change Vectors
US President was
elected,
took office,
became
president
vice president (-), by
president (+), adminis-
tration (+), senator (-),
governor (-), candidate
(-)
Best Picture nominated
for, to
win, won
the, was
nominated
best picture (+), hour
minute (-), academy
award (+), oscar (+),
nominated (+), won (+),
star (-), best actress (+),
best actor (+), best sup-
porting (+)
Table 1: Example behavior of various contex-
tual units (unigrams and bigrams) automatically
learned in the train CTPs and change vector. The
(+) and (-) signs indicate rise and fall in mention
frequency, respectively.
From the CTPs, we learned change vectors that re-
flect change patterns in context units of CTPs. Our
experiments showed that the change patterns are
highly relevant for detecting state change, which
is an effective way of identifying begin times of
facts. For future work, we would like to investi-
gate how our method can be improved to dp better
at detecting fact end times. We also would like to
investigate time-stamped corpora of finer-grained
granularity such as day. This information can be
obtained by subscribing to daily newsfeeds of spe-
cific entities.
Acknowledgments
We thank members of the NELL team at CMU
for their helpful comments. This research was
supported by DARPA under contract number
FA8750-13-2-0005 and in part by Fulbright and
Google Anita Borg Memorial Scholarship.
References
A. Angel, N. Koudas, N. Sarkas, D. Srivastava:
Dense Subgraph Maintenance under Streaming
Edge Weight Updates for Real-time Story Identi-
fication. In Proceedings of the VLDB Endowment,
PVLDB 5(10):574?585, 2012.
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-
niak, Z.G. Ives: DBpedia: A Nucleus for a Web of
Open Data. In Proceedings of the 6th International
Semantic Web Conference (ISWC), pages 722?735,
Busan, Korea, 2007.
M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, O. Etzioni: Open Information Extraction from
1934
the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2670?2676, Hyderabad, India, 2007.
S. Bethard and J.H. Martin. Cu-tmp: Temporal relation
classification using syntactic and semantic features.
In In SemEval-2007, 2007.
K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.
Taylor: Freebase: a Collaboratively Created Graph
Database for Structuring Human Knowledge. In
Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD),
pages, 1247-1250, Vancouver, BC, Canada, 2008.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka,
T.M. Mitchell: Coupled Semi-supervised Learning
for Information Extraction. In Proceedings of the
Third International Conference on Web Search and
Web Data Mining (WSDM), pages 101?110, New
York, NY, USA, 2010.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R.
Hruschka Jr., T. M. Mitchell: Toward an Architec-
ture for Never-Ending Language Learning. In Pro-
ceedings of the Twenty-Fourth AAAI Conference on
Artificial Intelligence (AAAI) 2010.
L. Del Corro, R. Gemulla: ClausIE: clause-based
open information extraction. In Proceedings of the
22nd International Conference on World Wide Web
(WWW), pages 355-366. 2013.
A. Das Sarma, A. Jain, C. Yu: Dynamic Relationship
and Event Discovery. In Proceedings of the Forth
International Conference on Web Search and Web
Data Mining (WSDM), pages 207?216, Hong Kong,
China, 2011.
A. Fader, S. Soderland, O. Etzioni: Identifying Rela-
tions for Open Information Extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1535?1545, Edinburgh, UK, 2011.
D. Graff, J. Kong, K. Chen, and K. Maeda. English gi-
gaword. Linguistic Data Consortium, Philadelphia,
2003.
C. Havasi, R. Speer, J. Alonso. ConceptNet 3: a Flex-
ible, Multilingual Semantic Network for Common
Sense Knowledge. In Proceedings of the Recent Ad-
vances in Natural Language Processing (RANLP),
Borovets, Bulgaria, 2007.
J. Hoffart, F. Suchanek, K. Berberich, E. Lewis-
Kelham, G. de Melo, G. Weikum: YAGO2: Ex-
ploring and Querying World Knowledge in Time,
Space, Context, and Many Languages. In Proceed-
ings of the 20th International Conference on World
Wide Web (WWW), pages 229?232, Hyderabad, In-
dia. 2011.
X. Ling and D.S. Weld. Temporal information extrac-
tion. In Proceedings of AAAI, 2010.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The Google
Books Team, Joseph P. Pickett, Dale Holberg, Dan
Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
Martin A. Nowak, Erez Lieberman Aiden: Quantita-
tive Analysis of Culture Using Millions of Digitized
Books. Science, 331(6014):176182.
N. Nakashole, M. Theobald, G. Weikum: Scalable
Knowledge Harvesting with High Precision and
High Recall. In Proceedings of the 4th International
Conference on Web Search and Web Data Mining
(WSDM), pages 227?326, Hong Kong, China, 2011.
N. Nakashole, T. Tylenda, G. Weikum: Fine-grained
Semantic Typing of Emerging Entities. In Proceed-
ings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp. 1488-
1497, 2013.
N.Nakahsole, T. M. Mitchell: Language-Aware Truth
Assessment of Fact Candidates In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 1009-1019,
2014.
G. Puscasu. Wvali: Temporal relation identification by
syntactico-semantic analysis. In Proceedings of the
4th International Workshop on SemEval, 2007.
J. Pustejovsky, J. Castano, R. Ingria, R. Sauri, R
Gaizauskas, A. Setzer, G. Katz, and D. Radev.
Timeml: Robust specification of event and temporal
expressions in text. In Fifth International Workshop
on Computational Semantics, 2003.
P. P. Talukdar, D. T. Wijaya, Tom M. Mitchell: Acquir-
ing temporal constraints between relations. In Pro-
ceeding of the 21st ACM International Conference
on Information and Knowledge Management, pages
992-1001, CIKM 2012.
P. P. Talukdar, D. T. Wijaya, T. Mitchell: Coupled
temporal scoping of relational facts. In Proceedings
of the fifth ACM international conference on Web
search and data mining. ACM, 2012.
M. Verhagen, I. Mani, R. Sauri, R. Knippen, S.B. Jang,
J. Littman, A. Rumshisky, J. Phillips, and J. Puste-
jovsky. Automating temporal annotation with tarsqi.
In Proceedings of the ACL Session on Interactive
poster and demonstration sessions, 2005.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. Semeval-2007 task 15:
Tempeval temporal relation identi
cation. In Proceedings of the 4th International Work-
shop on Semantic Evaluations, 2007.
D. T. Wijaya, and R. Yeniterzi: Understanding seman-
tic change of words over centuries. In Proceedings
of the 2011 international workshop on DETecting
and Exploiting Cultural diversiTy on the social web.
ACM, 2011.
1935
F. M. Suchanek, G. Kasneci, G. Weikum: Yago: a
Core of Semantic Knowledge. In Proceedings of the
16th International Conference on World Wide Web
(WWW) pages, 697-706, Banff, Alberta, Canada,
2007.
Y. Wang, M. Zhu, L. Qu, M. Spaniol, and G. Weikum:
Timely yago: harvesting, querying, and visualizing
temporal knowledge from wikipedia. In Proceedings
of the 13th International Conference on Extending-
Database Technology, 2010.
W. Wu, H. Li, H. Wang, K. Zhu: Probase: A
Probabilistic Taxonomy for Text Understanding. In
Proceedings of the International Conference on
Management of Data (SIGMOD), pages 481?492,
Scottsdale, AZ, USA, 2012.
Y. Wang, B. Yang, L. Qu, M. Spaniol, and G. Weikum:
Harvesting facts from textual web sources by con-
strained label propagation. In Proceedings of CIKM,
2011.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. Jointly identifying temporal relations with
markov logic. In Proceedings of ACL, 2009.
1936
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 570?580,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Which Noun Phrases Denote Which Concepts?
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
Resolving polysemy and synonymy is re-
quired for high-quality information extraction.
We present ConceptResolver, a component for
the Never-Ending Language Learner (NELL)
(Carlson et al, 2010) that handles both phe-
nomena by identifying the latent concepts that
noun phrases refer to. ConceptResolver per-
forms both word sense induction and synonym
resolution on relations extracted from text us-
ing an ontology and a small amount of la-
beled data. Domain knowledge (the ontology)
guides concept creation by defining a set of
possible semantic types for concepts. Word
sense induction is performed by inferring a set
of semantic types for each noun phrase. Syn-
onym detection exploits redundant informa-
tion to train several domain-specific synonym
classifiers in a semi-supervised fashion. When
ConceptResolver is run on NELL?s knowledge
base, 87% of the word senses it creates cor-
respond to real-world concepts, and 85% of
noun phrases that it suggests refer to the same
concept are indeed synonyms.
1 Introduction
Many information extraction systems construct
knowledge bases by extracting structured assertions
from free text (e.g., NELL (Carlson et al, 2010),
TextRunner (Banko et al, 2007)). A major limi-
tation of many of these systems is that they fail to
distinguish between noun phrases and the underly-
ing concepts they refer to. As a result, a polysemous
phrase like ?apple? will refer sometimes to the con-
cept Apple Computer (the company), and other times
to the concept apple (the fruit). Furthermore, two
synonymous noun phrases like ?apple? and ?Apple
?apple?
?apple computer?
apple (the fruit)
Apple Computer
Figure 1: An example mapping from noun phrases (left)
to a set of underlying concepts (right). Arrows indicate
which noun phrases can refer to which concepts.
[eli lilly, lilly]
[kaspersky labs, kaspersky lab, kaspersky]
[careerbuilder, careerbuilder.com]
[l 3 communications, level 3 communications]
[cellular, u.s. cellular]
[jc penney, jc penny]
[nielsen media research, nielsen company]
[universal studios, universal music group, universal]
[amr corporation, amr]
[intel corp, intel corp., intel corporation, intel]
[emmitt smith, chris canty]
[albert pujols, pujols]
[carlos boozer, dennis martinez]
[jason hirsh, taylor buchholz]
[chris snyder, ryan roberts]
[j.p. losman, losman, jp losman]
[san francisco giants, francisco rodriguez]
[andruw jones, andruw]
[aaron heilman, bret boone]
[roberto clemente, clemente]
Figure 2: A random sample of concepts created by Con-
ceptResolver. The first 10 concepts are from company,
while the second 10 are from athlete.
Computer? can refer to the same underlying con-
cept. The result of ignoring this many-to-many map-
ping between noun phrases and underlying concepts
(see Figure 1) is confusion about the meaning of ex-
tracted information. To minimize such confusion, a
system must separately represent noun phrases, the
underlying concepts to which they can refer, and the
many-to-many ?can refer to? relation between them.
The relations extracted by systems like NELL ac-
tually apply to concepts, not to noun phrases. Say
570
the system extracts the relation ceoOf(x1, x2) be-
tween the noun phrases x1 and x2. The correct in-
terpretation of this extracted relation is that there ex-
ist concepts c1 and c2 such that x1 can refer to c1,
x2 can refer to c2, and ceoOf(c1, c2). If the orig-
inal relation were ceoOf(?steve?, ?apple?), then c1
would be Steve Jobs, and c2 would be Apple Com-
puter. A similar interpretation holds for one-place
category predicates like person(x1). We define con-
cept discovery as the problem of (1) identifying con-
cepts like c1 and c2 from extracted predicates like
ceoOf(x1, x2) and (2) mapping noun phrases like
x1, x2 to the concepts they can refer to.
The main input to ConceptResolver is a set of
extracted category and relation instances over noun
phrases, like person(x1) and ceoOf(x1, x2), pro-
duced by running NELL. Here, any individual noun
phrase xi can be labeled with multiple categories
and relations. The output of ConceptResolver is
a set of concepts, {c1, c2, ..., cn}, and a mapping
from each noun phrase in the input to the set of
concepts it can refer to. Like many other systems
(Miller, 1995; Yates and Etzioni, 2007; Lin and Pan-
tel, 2002), ConceptResolver represents each output
concept ci as a set of synonymous noun phrases,
i.e., ci = {xi1, xi2, ..., xim}. For example, Figure 2
shows several concepts output by ConceptResolver;
each concept clearly reveals which noun phrases can
refer to it. Each concept also has a semantic type that
corresponds to a category in ConceptResolver?s on-
tology; for instance, the first 10 concepts in Figure 2
belong to the category company.
Previous approaches to concept discovery use lit-
tle prior knowledge, clustering noun phrases based
on co-occurrence statistics (Pantel and Lin, 2002).
In comparison, ConceptResolver uses a knowledge-
rich approach. In addition to the extracted relations,
ConceptResolver takes as input two other sources of
information: an ontology, and a small number of la-
beled synonyms. The ontology contains a schema
for the relation and category predicates found in
the input instances, including properties of predi-
cates like type restrictions on its domain and range.
The category predicates are used to assign semantic
types to each concept, and the properties of relation
predicates are used to create evidence for synonym
resolution. The labeled synonyms are used as train-
ing data during synonym resolution, where they are
1. Induce Word Senses
i. Use extracted category instances to create
one or more senses per noun phrase.
ii. Use argument type constraints to produce re-
lation evidence for synonym resolution.
2. Cluster Synonymous Senses
For each category C defined in the ontology:
i. Train a semi-supervised classifier to predict
synonymy.
ii. Cluster word senses with semantic type C
using classifier?s predictions.
iii. Output sense clusters as concepts with se-
mantic type C.
Figure 3: High-level outline of ConceptResolver?s algo-
rithm.
used to train a semi-supervised classifier.
ConceptResolver discovers concepts using the
process outlined in Figure 3. It first performs word
sense induction, using the extracted category in-
stances to create one or more unambiguous word
senses for each noun phrase in the knowledge base.
Each word sense is a copy of the original noun
phrase paired with a semantic type (a category) that
restricts the concepts it can refer to. ConceptRe-
solver then performs synonym resolution on these
word senses. This step treats the senses of each se-
mantic type independently, first training a synonym
classifier then clustering the senses based on the
classifier?s decisions. The result of this process is
clusters of synonymous word senses, which are out-
put as concepts. Concepts inherit the semantic type
of the word senses they contain.
We evaluate ConceptResolver using a subset of
NELL?s knowledge base, presenting separate results
for the concepts of each semantic type. The eval-
uation shows that, on average, 87% of the word
senses created by ConceptResolver correspond to
real-world concepts. We additionally find that, on
average, 85% of the noun phrases in each concept
refer to the same real-world entity.
2 Prior Work
Previous work on concept discovery has focused
on the subproblems of word sense induction and
synonym resolution. Word sense induction is typ-
ically performed using unsupervised clustering. In
the SemEval word sense induction and disambigua-
571
tion task (Agirre and Soroa, 2007; Manandhar et al,
2010), all of the submissions in 2007 created senses
by clustering the contexts each word occurs in, and
the 2010 event explicitly disallowed the use of exter-
nal resources like ontologies. Other systems cluster
words to find both word senses and concepts (Pantel
and Lin, 2002; Lin and Pantel, 2002). ConceptRe-
solver?s category-based approach is quite different
from these clustering approaches. Snow et al (2006)
describe a system which adds new word senses to
WordNet. However, Snow et al assume the exis-
tence of an oracle which provides the senses of each
word. In contrast, ConceptResolver automatically
determines the number of senses for each word.
Synonym resolution on relations extracted from
web text has been previously studied by Resolver
(Yates and Etzioni, 2007), which finds synonyms in
relation triples extracted by TextRunner (Banko et
al., 2007). In contrast to our system, Resolver is un-
supervised and does not have a schema for the re-
lations. Due to different inputs, ConceptResolver
and Resolver are not precisely comparable. How-
ever, our evaluation shows that ConceptResolver has
higher synonym resolution precision than Resolver,
which we attribute to our semi-supervised approach
and the known relation schema.
Synonym resolution also arises in record link-
age (Winkler, 1999; Ravikumar and Cohen, 2004)
and citation matching (Bhattacharya and Getoor,
2007; Bhattacharya and Getoor, 2006; Poon and
Domingos, 2007). As with word sense induction,
many approaches to these problems are unsuper-
vised. A problem with these algorithms is that they
require the authors to define domain-specific simi-
larity heuristics to achieve good performance. Other
synonym resolution work is fully supervised (Singla
and Domingos, 2006; McCallum and Wellner, 2004;
Snow et al, 2007), training models using manually
constructed sets of synonyms. These approaches use
large amounts of labeled data, which can be difficult
to create. ConceptResolver?s approach lies between
these two extremes: we label a small number of syn-
onyms (10 pairs), then use semi-supervised training
to learn a similarity function. We think our tech-
nique is a good compromise, as it avoids much of
the manual effort of the other approaches: tuning the
similarity function in one case, and labeling a large
amount of data in the other
ConceptResolver uses a novel algorithm for semi-
supervised clustering which is conceptually similar
to other work in the area. Like other approaches
(Basu et al, 2004; Xing et al, 2003; Klein et al,
2002), we learn a similarity measure for clustering
based on a set of must-link and cannot-link con-
straints. Unlike prior work, our algorithm exploits
multiple views of the data to improve the similar-
ity measure. As far as we know, ConceptResolver
is the first application of semi-supervised cluster-
ing to relational data ? where the items being clus-
tered are connected by relations (Getoor and Diehl,
2005). Interestingly, the relational setting also pro-
vides us with the independent views that are benefi-
cial to semi-supervised training.
Concept discovery is also related to coreference
resolution (Ng, 2008; Poon and Domingos, 2008).
The difference between the two problems is that
coreference resolution finds noun phrases that refer
to the same concept within a specific document. We
think the concepts produced by a system like Con-
ceptResolver could be used to improve coreference
resolution by providing prior knowledge about noun
phrases that can refer to the same concept. This
knowledge could be especially helpful for cross-
document coreference resolution systems (Haghighi
and Klein, 2010), which actually represent concepts
and track mentions of them across documents.
3 Background: Never-Ending Language
Learner
ConceptResolver is designed as a component for the
Never-Ending Language Learner (NELL) (Carlson
et al, 2010). In this section, we provide some per-
tinent background information about NELL that in-
fluenced the design of ConceptResolver 1.
NELL is an information extraction system that
has been running 24x7 for over a year, using coupled
semi-supervised learning to populate an ontology
from unstructured text found on the web. The ontol-
ogy defines two types of predicates: categories (e.g.,
company and CEO) and relations (e.g., ceoOf-
Company). Categories are single-argument pred-
icates, and relations are two-argument predicates.
1More information about NELL, including browsable and
downloadable versions of its knowledge base, is available from
http://rtw.ml.cmu.edu.
572
NELL?s knowledge base contains both definitions
for predicates and extracted instances of each pred-
icate. At present, NELL?s knowledge base defines
approximately 500 predicates and contains over half
a million extracted instances of these predicates with
an accuracy of approximately 0.85.
Relations between predicates are an important
component of NELL?s ontology. For ConceptRe-
solver, the most important relations are domain and
range, which define argument types for each rela-
tion predicate. For example, the first argument of
ceoOfCompany must be a CEO and the second ar-
gument must be a company. Argument type restric-
tions inform ConceptResolver?s word sense induc-
tion process (Section 4.1).
Multiple sources of information are used to popu-
late each predicate with high precision. The system
runs four independent extractors for each predicate:
the first uses web co-occurrence statistics, the sec-
ond uses HTML structures on webpages, the third
uses the morphological structure of the noun phrase
itself, and the fourth exploits empirical regularities
within the knowledge base. These subcomponents
are described in more detail by Carlson et al (2010)
and Wang and Cohen (2007). NELL learns using
a bootstrapping process, iteratively re-training these
extractors using instances in the knowledge base,
then adding some predictions of the learners to the
knowledge base. This iterative learning process can
be viewed as a discrete approximation to EM which
does not explicitly instantiate every latent variable.
As in other information extraction systems, the
category and relation instances extracted by NELL
contain polysemous and synonymous noun phrases.
ConceptResolver was developed to reduce the im-
pact of these phenomena.
4 ConceptResolver
This section describes ConceptResolver, our new
component which creates concepts from NELL?s ex-
tractions. It uses a two-step procedure, first creating
one or more senses for each noun phrase, then clus-
tering synonymous senses to create concepts.
4.1 Word Sense Induction
ConceptResolver induces word senses using a sim-
ple assumption about noun phrases and concepts. If
a noun phrase has multiple senses, the senses should
be distinguishable from context. People can deter-
mine the sense of an ambiguous word given just a
few surrounding words (Kaplan, 1955). We hypoth-
esize that local context enables sense disambigua-
tion by defining the semantic type of the ambiguous
word. ConceptResolver makes the simplifying as-
sumption that all word senses can be distinguished
on the basis of semantic type. As the category pred-
icates in NELL?s ontology define a set of possible
semantic types, this assumption is equivalent to the
one-sense-per-category assumption: a noun phrase
refers to at most one concept in each category of
NELL?s ontology. For example, this means that a
noun phrase can refer to a company and a fruit, but
not multiple companies.
ConceptResolver uses the extracted category as-
sertions to define word senses. Each word sense is
represented as a tuple containing a noun phrase and
a category. In synonym resolution, the category acts
like a type constraint, and only senses with the same
category type can be synonymous. To create senses,
the system interprets each extracted category predi-
cate c(x) as evidence that category c contains a con-
cept denoted by noun phrase x. Because it assumes
that there is at most one such concept, Concept-
Resolver creates one sense of x for each extracted
category predicate. As a concrete example, say
the input assertions contain company(?apple?) and
fruit(?apple?). Sense induction creates two senses
for ?apple?: (?apple?, company) and (?apple?, fruit).
The second step of sense induction produces ev-
idence for synonym resolution by creating relations
between word senses. These relations are created
from input relations and the ontology?s argument
type constraints. Each extracted relation is mapped
to all possible sense relations that satisfy the ar-
gument type constraints. For example, the noun
phrase relation ceoOfCompany(?steve jobs?, ?ap-
ple?) would map to ceoOfCompany((?steve jobs?,
ceo), (?apple?, company)). It would not map to a
similar relation with (?apple?, fruit), however, as
(?apple?, fruit) is not in the range of ceoOfCom-
pany. This process is effective because the relations
in the ontology have restrictive domains and ranges,
so only a small fraction of sense pairs satisfy the ar-
gument type restrictions. It is also not vital that this
mapping be perfect, as the sense relations are only
573
used as evidence for synonym resolution. The final
output of sense induction is a sense-disambiguated
knowledge base, where each noun phrase has been
converted into one or more word senses, and rela-
tions hold between pairs of senses.
4.2 Synonym Resolution
After mapping each noun phrase to one or more
senses (each with a distinct category type), Con-
ceptResolver performs semi-supervised clustering
to find synonymous senses. As only senses with
the same category type can be synonymous, our
synonym resolution algorithm treats senses of each
type independently. For each category, ConceptRe-
solver trains a semi-supervised synonym classifier
then uses its predictions to cluster word senses.
Our key insight is that semantic relations and
string attributes provide independent views of the
data: we can predict that two noun phrases are syn-
onymous either based on the similarity of their text
strings, or based on similarity in the relations NELL
has extracted about them. As a concrete example,
we can decide that (?apple computer?, company)
and (?apple?, company) are synonymous because
the text string ?apple? is similar to ?apple computer,?
or because we have learned that (?steve jobs?, ceo)
is the CEO of both companies. ConceptResolver ex-
ploits these two independent views using co-training
(Blum and Mitchell, 1998) to produce an accurate
synonym classifier using only a handful of labels.
4.2.1 Co-Training the Synonym Classifier
For each category, ConceptResolver co-trains a pair
of synonym classifiers using a handful of labeled
synonymous senses and a large number of automat-
ically created unlabeled sense pairs. Co-training is
a semi-supervised learning algorithm for data sets
where each instance can be classified from two (or
more) independent sets of features. That is, the fea-
tures of each instance xi can be partitioned into two
views, xi = (x1i , x
2
i ), and there exist functions in
each view, f1, f2, such that f1(x1i ) = f
2(x2i ) = yi.
The co-training algorithm uses a bootstrapping pro-
cedure to train f1, f2 using a small set of labeled ex-
amples L and a large pool of unlabeled examples U .
The training process repeatedly trains each classifier
on the labeled examples, then allows each classifier
to label some examples in the unlabeled data pool.
Co-training also has PAC-style theoretical guaran-
tees which show that it can learn classifiers with ar-
bitrarily high accuracy under appropriate conditions
(Blum and Mitchell, 1998).
Figure 4 provides high-level pseudocode for co-
training in the context of ConceptResolver. In Con-
ceptResolver, an instance xi is a pair of senses (e.g.,
<(?apple?, company), (?microsoft?, company)>),
the two views x1i and x
2
i are derived from string
attributes and semantic relations, and the output yi
is whether the senses are synonyms. (The features
of each view are described later in this section.) L
is initialized with a small number of labeled sense
pairs. Ideally, U would contain all pairs of senses
in the category, but this set grows quadratically in
category size. Therefore, ConceptResolver uses the
canopies algorithm (McCallum et al, 2000) to ini-
tialize U with a subset of the sense pairs that are
more likely to be synonymous.
Both the string similarity classifier and the rela-
tion classifier are trained using L2-regularized lo-
gistic regression. The regularization parameter ? is
automatically selected on each iteration by search-
ing for a value which maximizes the loglikelihood
of a validation set, which is constructed by ran-
domly sampling 25% of L on each iteration. ? is
re-selected on each iteration because the initial la-
beled data set is extremely small, so the initial vali-
dation set is not necessarily representative of the ac-
tual data. In our experiments, the initial validation
set contains only 15 instances.
The string similarity classifier bases its decision
on the original noun phrase which mapped to each
sense. We use several string similarity measures as
features, including SoftTFIDF (Cohen et al, 2003),
Level 2 JaroWinkler (Cohen et al, 2003), Fellegi-
Sunter (Fellegi and Sunter, 1969), and Monge-Elkan
(Monge and Elkan, 1996). The first three algorithms
produce similarity scores by matching words in the
two phrases and the fourth is an edit distance. We
also use a heuristic abbreviation detection algorithm
(Schwartz and Hearst, 2003) and convert its output
into a score by dividing the length of the detected
abbreviation by the total length of the string.
The relation classifier?s features capture several
intuitive ways to determine that two items are syn-
onyms from the items they are related to. The re-
lation view contains three features for each relation
574
For each category C:
1. Initialize labeled data L with 10 positive and 50
negative examples (pairs of senses)
2. Initialize unlabeled data U by running canopies
(McCallum et al, 2000) on all senses in C.
3. Repeat 50 times:
i. Train the string similarity classifier on L
ii. Train the relation classifier on L
iii. Label U with each classifier
iv. Add the most confident 5 positive and 25
negative predictions of both classifiers to L
Figure 4: The co-training algorithm for learning synonym
classifiers.
r whose domain is compatible with the current cat-
egory. Consider the sense pair (s, t), and let r(s)
denote s?s values for relation r (i.e., r(s) = {v :
r(s, v)}). For each relation r, we instantiate the fol-
lowing features:
? (Senses which share values are synonyms)
The percent of values of r shared by both s and t,
that is |r(s)?r(t)||r(s)?r(t)| .
? (Senses with different values are not synonyms)
The percent of values of r not shared by s and t, or
1 ? |r(s)?r(t)||r(s)?r(t)| . The feature is set to 0 if either r(s)
or r(t) is empty. This feature is only instantiated if
the ontology specifies that r has at most one value
per sense.
? (Some relations indicate synonymy) A boolean
feature which is true if t ? r(s) or s ? r(t).
The output of co-training is a pair of classifiers for
each category. We combine their predictions using
the assumption that the two views X1, X2 are con-
ditionally independent given Y . As we trained both
classifiers using logistic regression, we have models
for the probabilities P (Y |X1) and P (Y |X2). The
conditional independence assumption implies that
we can combine their predictions using the formula:
P (Y = 1|X1, X2) =
P (Y = 1|X1)P (Y = 1|X2)P (Y = 0)
?
y=0,1 P (Y = y|X
1)P (Y = y|X2)(1? P (Y = y))
The above formula involves a prior term, P (Y ),
because the underlying classifiers are discrimina-
tive. We set P (Y = 1) = .5 in our experi-
ments as this setting reduces our dependence on the
(typically poorly calibrated) probability estimates of
logistic regression. We also limited the probabil-
ity predictions of each classifier to lie in [.01, .99]
to avoid divide-by-zero errors. The probability
P (Y |X1, X2) is the final synonym classifier which
is used for agglomerative clustering.
4.2.2 Agglomerative Clustering
The second step of our algorithm runs agglomera-
tive clustering to enforce transitivity constraints on
the predictions of the co-trained synonym classifier.
As noted in previous works (Snow et al, 2006), syn-
onymy is a transitive relation. If a and b are syn-
onyms, and b and c are synonyms, then a and c must
also be synonyms. Unfortunately, co-training is not
guaranteed to learn a function that satisfies these
transitivity constraints. We enforce the constraints
by running agglomerative clustering, as clusterings
of instances trivially satisfy the transitivity property.
ConceptResolver uses the clustering algorithm
described by Snow et al (2006), which defines a
probabilistic model for clustering and a procedure to
(locally) maximize the likelihood of the final cluster-
ing. The algorithm is essentially bottom-up agglom-
erative clustering of word senses using a similarity
score derived from P (Y |X1, X2). The similarity
score for two senses is defined as:
log
P (Y = 0)P (Y = 1|X1, X2)
P (Y = 1)P (Y = 0|X1, X2)
The similarity score for two clusters is the sum of
the similarity scores for all pairs of senses. The ag-
glomerative clustering algorithm iteratively merges
the two most similar clusters, stopping when the
score of the best possible pair is below 0. The clus-
ters of word senses produced by this process are the
concepts for each category.
5 Evaluation
We perform several experiments to measure Con-
ceptResolver?s performance at each of its respective
tasks. The first experiment evaluates word sense in-
duction using Freebase as a canonical set of con-
cepts. The second experiment evaluates synonym
resolution by comparing ConceptResolver?s sense
clusters to a gold standard clustering.
For both experiments, we used a knowledge base
created by running 140 iterations of NELL. We pre-
processed this knowledge base by removing all noun
575
phrases with zero extracted relations. As Concept-
Resolver treats the instances of each category pred-
icate independently, we chose 7 categories from
NELL?s ontology to use in the evaluation. The cat-
egories were selected on the basis of the number of
extracted relations that ConceptResolver could use
to detect synonyms. The number of noun phrases
in each category is shown in Table 2. We manually
labeled 10 pairs of synonymous senses for each of
these categories. The system automatically synthe-
sized 50 negative examples from the positive exam-
ples by assuming each pair represents a distinct con-
cept, so senses in different pairs are not synonyms.
5.1 Word Sense Induction Evaluation
Our first experiment evaluates the performance of
ConceptResolver?s category-based word sense in-
duction. We estimate two quantities: (1) sense pre-
cision, the fraction of senses created by our system
that correspond to real-world entities, and (2) sense
recall, the fraction of real-world entities that Con-
ceptResolver creates senses for. Sense recall is only
measured over entities which are represented by a
noun phrase in ConceptResolver?s input assertions ?
it is a measure of ConceptResolver?s ability to cre-
ate senses for the noun phrases it is given. Sense
precision is directly determined by how frequently
NELL?s extractors propose correct senses for noun
phrases, while sense recall is related to the correct-
ness of the one-sense-per-category assumption.
Precision and recall were evaluated by comparing
the senses created by ConceptResolver to concepts
in Freebase (Bollacker et al, 2008). We sampled
100 noun phrases from each category and matched
each noun phrase to a set of Freebase concepts. We
interpret each matching Freebase concept as a sense
of the noun phrase. We chose Freebase because it
had good coverage for our evaluation categories.
To align ConceptResolver?s senses with Freebase,
we first matched each of our categories with a set of
similar Freebase categories2. We then used a com-
bination of Freebase?s search API and Mechanical
Turk to align noun phrases with Freebase concepts:
we searched for the noun phrase in Freebase, then
had Mechanical Turk workers label which of the
2In Freebase, concepts are called Topics and categories are
called Types. For clarity, we use our terminology throughout.
Freebase
Category Precision Recall concepts
per Phrase
athlete 0.95 0.56 1.76
city 0.97 0.25 3.86
coach 0.86 0.94 1.06
company 0.85 0.41 2.41
country 0.74 0.56 1.77
sportsteam 0.89 0.30 3.28
stadiumoreventvenue 0.83 0.61 1.63
Table 1: ConceptResolver?s word sense induction perfor-
mance
Figure 5: Empirical distribution of the number of Free-
base concepts per noun phrase in each category
top 10 resulting Freebase concepts the noun phrase
could refer to. After obtaining the list of matching
Freebase concepts for each noun phrase, we com-
puted sense precision as the number of noun phrases
matching ? 1 Freebase concept divided by 100, the
total number of noun phrases. Sense recall is the re-
ciprocal of the average number of Freebase concepts
per noun phrase. Noun phrases matching 0 Freebase
concepts were not included in this computation.
The results of the evaluation in Table 1 show
that ConceptResolver?s word sense induction works
quite well for many categories. Most categories have
high precision, while recall varies by category. Cat-
egories like coach are relatively unambiguous, with
almost exactly 1 sense per noun phrase. Other cate-
gories have almost 4 senses per noun phrase. How-
ever, this average is somewhat misleading. Figure
5 shows the distribution of the number of concepts
per noun phrase in each category. The distribution
shows that most noun phrases are unambiguous, but
a small number of noun phrases have a large num-
ber of senses. In many cases, these noun phrases
576
are generic terms for many items in the category; for
example, ?palace? in stadiumoreventvenue refers
to 10 Freebase concepts. Freebase?s category def-
initions are also overly technical in some cases ?
for example, Freebase?s version of company has a
concept for each registered corporation. This defi-
nition means that some companies like Volkswagen
have more than one concept (in this case, 9 con-
cepts). These results suggest that the one-sense-per-
category assumption holds for most noun phrases.
An important footnote to this evaluation is that the
categories in NELL?s ontology are somewhat arbi-
trary, and that creating subcategories would improve
sense recall. For example, we could define subcat-
egories of sportsteam for various sports (e.g., foot-
ball team); these new categories would allow Con-
ceptResolver to distinguish between teams with the
same name that play different sports. Creating sub-
categories could improve performance in categories
with a high level of polysemy.
5.2 Synonym Resolution Evaluation
Our second experiment evaluates synonym resolu-
tion by comparing the concepts created by Concept-
Resolver to a gold standard set of concepts. Al-
though this experiment is mainly designed to eval-
uate ConceptResolver?s ability to detect synonyms,
it is somewhat affected by the word sense induc-
tion process. Specifically, the gold standard cluster-
ing contains noun phrases that refer to multiple con-
cepts within the same category. (It is unclear how
to create a gold standard clustering without allowing
such mappings.) The word sense induction process
produces only one of these mappings, which limits
maximum possible recall in this experiment.
For this experiment, we report two different mea-
sures of clustering performance. The first measure
is the precision and recall of pairwise synonym de-
cisions, typically known as cluster precision and re-
call. We dub this the clustering metric. We also
adopt the precision/recall measure from Resolver
(Yates and Etzioni, 2007), which we dub the Re-
solver metric. The Resolver metric aligns each pro-
posed cluster containing ? 2 senses with a gold
standard cluster (i.e., a real-world concept) by se-
lecting the cluster that a plurality of the senses in the
proposed cluster refer to. Precision is then the frac-
tion of senses in the proposed cluster which are also
in the gold standard cluster; recall is computed anal-
ogously by swapping the roles of the proposed and
gold standard clusters. Resolver precision can be in-
terpreted as the probability that a randomly sampled
sense (in a cluster with at least 2 senses) is in a clus-
ter representing its true meaning. Incorrect senses
were removed from the data set before evaluating
precision; however, these senses may still affect per-
formance by influencing the clustering process.
Precision was evaluated by sampling 100 random
concepts proposed by ConceptResolver, then manu-
ally scoring each concept using both of the metrics
above. This process mimics aligning each sampled
concept with its best possible match in a gold stan-
dard clustering, then measuring precision with re-
spect to the gold standard.
Recall was evaluated by comparing the system?s
output to a manually constructed set of concepts for
each category. To create this set, we randomly sam-
pled noun phrases from each category and manually
matched each noun phrase to one or more real-world
entities. We then found other noun phrases which re-
ferred to each entity and created a concept for each
entity with at least one unambiguous reference. This
process can create multiple senses for a noun phrase,
depending on the real-world entities represented in
the input assertions. We only included concepts con-
taining at least 2 senses in the test set, as singleton
concepts do not contribute to either recall metric.
The size of each recall test set is listed in Table 2;
we created smaller test sets for categories where syn-
onyms were harder to find. Incorrectly categorized
noun phrases were not included in the gold standard
as they do not correspond to any real-world entities.
Table 2 shows the performance of ConceptRe-
solver on each evaluation category. For each cat-
egory, we also report the baseline recall achieved
by placing each sense in its own cluster. Concept-
Resolver has high precision for several of the cate-
gories. Other categories like athlete and city have
somewhat lower precision. To make this difference
concrete, Figure 2 (first page) shows a random sam-
ple of 10 concepts from both company and athlete.
Recall varies even more widely across categories,
partly because the categories have varying levels of
polysemy, and partly due to differences in average
concept size. The differences in average concept
size are reflected in the baseline recall numbers.
577
Resolver Metric Clustering Metric
Category
# of Recall
Precision Recall F1
Baseline
Precision Recall F1
Baseline
Phrases Set Size Recall Recall
athlete 3886 80 0.69 0.69 0.69 0.46 0.41 0.45 0.43 0.00
city 5710 50 0.66 0.52 0.58 0.42 0.30 0.10 0.15 0.00
coach 889 60 0.90 0.93 0.91 0.43 0.83 0.88 0.85 0.00
company 3553 60 0.93 0.71 0.81 0.39 0.79 0.44 0.57 0.00
country 693 60 0.98 0.50 0.66 0.30 0.94 0.15 0.26 0.00
sportsteam 2085 100 0.95 0.48 0.64 0.29 0.87 0.15 0.26 0.00
stadiumoreventvenue 1662 100 0.84 0.73 0.78 0.39 0.65 0.49 0.56 0.00
Table 2: Synonym resolution performance of ConceptResolver
We attribute the differences in precision across
categories to the different relations available for
each category. For example, none of the relations for
athlete uniquely identify a single athlete, and there-
fore synonymy cannot be accurately represented in
the relation view. Adding more relations to NELL?s
ontology may improve performance in these cases.
We note that the synonym resolution portion of
ConceptResolver is tuned for precision, and that per-
fect recall is not necessarily attainable. Many word
senses participate in only one relation, which may
not provide enough evidence to detect synonymy.
As NELL continually extracts more knowledge, it
is reasonable for ConceptResolver to abstain from
these decisions until more evidence is available.
6 Discussion
In order for information extraction systems to ac-
curately represent knowledge, they must represent
noun phrases, concepts, and the many-to-many map-
ping from noun phrases to concepts they denote. We
present ConceptResolver, a system which takes ex-
tracted relations between noun phrases and identifies
latent concepts that the noun phrases refer to. Two
lessons from ConceptResolver are that (1) ontolo-
gies aid word sense induction, as the senses of pol-
ysemous words tend to have distinct semantic types,
and (2) redundant information, in the form of string
similarity and extracted relations, helps train accu-
rate synonym classifiers.
An interesting aspect of ConceptResolver is that
its performance should improve as NELL?s ontol-
ogy and knowledge base grow in size. Defining
finer-grained categories will improve performance
at word sense induction, as more precise categories
will contain fewer ambiguous noun phrases. Both
extracting more relation instances and adding new
relations to the ontology will improve synonym res-
olution. These scaling properties allow manual ef-
fort to be spent on high-level ontology operations,
not on labeling individual instances. We are inter-
ested in observing ConceptResolver?s performance
as NELL?s ontology and knowledge base grow.
For simplicity of exposition, we have implicitly
assumed thus far that the categories in NELL?s on-
tology are mutually exclusive. However, the ontol-
ogy contains compatible categories like male and
politician, where a single concept can belong to
both categories. In these situations, the one-sense-
per-category assumption may create too many word
senses. We currently address this problem with a
heuristic post-processing step: we merge all pairs of
concepts that belong to compatible categories and
share at least one referring noun phrase. This heuris-
tic typically works well, however there are prob-
lems. An example of a problematic case is ?obama,?
which NELL believes is a male, female, and politi-
cian. In this case, the heuristic cannot decide which
?obama? (the male or female) is the politician. As
such cases are fairly rare, we have not developed a
more sophisticated solution to this problem.
ConceptResolver has been integrated into NELL?s
continual learning process. NELL?s current set of
concepts can be viewed through the knowledge base
browser on NELL?s website, http://rtw.ml.
cmu.edu.
Acknowledgments
This work is supported in part by DARPA (under
contract numbers FA8750-08-1-0009 and AF8750-
09-C-0179) and by Google. We also gratefully ac-
knowledge the contributions of our colleagues on the
NELL project, Jamie Callan for the ClueWeb09 web
crawl and Yahoo! for use of their M45 computing
cluster. Finally, we thank the anonymous reviewers
for their helpful comments.
578
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 7?12.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In Proceedings of the
Twentieth International Joint Conference on Artificial
Intelligence, pages 2670?2676.
Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.
2004. A probabilistic framework for semi-supervised
clustering. In Proceedings of the Tenth ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 59?68.
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
dirichlet model for unsupervised entity resolution. In
Proceedings of the 2006 SIAM International Confer-
ence on Data Mining, pages 47?58.
Indrajit Bhattacharya and Lise Getoor. 2007. Collective
entity resolution in relational data. ACM Transactions
on Knowledge Discovery from Data, 1(1).
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the Eleventh Annual Conference on Computa-
tional Learning Theory, pages 92?100.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management of
Data, pages 1247?1250.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intelli-
gence.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A Comparison of String Distance
Metrics for Name-Matching Tasks. In Proceedings
of the IJCAI-03 Workshop on Information Integration,
pages 73?78, August.
Ivan P. Fellegi and Alan B. Sunter. 1969. A theory for
record linkage. Journal of the American Statistical As-
sociation, 64:1183?1210.
Lise Getoor and Christopher P. Diehl. 2005. Link min-
ing: a survey. SIGKDD Explorations Newsletter, 7:3?
12.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Pro-
ceedings of the 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 385?393, June.
Abraham Kaplan. 1955. An experimental study of ambi-
guity and context. Mechanical Translation, 2:39?46.
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints
to space-level constraints: Making the most of prior
knowledge in data clustering. In Proceedings of
the Nineteenth International Conference on Machine
Learning, pages 307?314.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proceedings of the 19th International
Conference on Computational linguistics - Volume 1,
pages 1?7.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems 18.
Andrew McCallum, Kamal Nigam, and Lyle H. Un-
gar. 2000. Efficient clustering of high-dimensional
data sets with application to reference matching. In
Proceedings of the Sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 169?178.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Alvaro Monge and Charles Elkan. 1996. The field
matching problem: Algorithms and applications. In
Proceedings of the Second International Conference
on Knowledge Discovery and Data Mining, pages
267?270.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 640?649.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 613?619.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
22nd AAAI Conference on Artificial Intelligence - Vol-
ume 1, pages 913?918.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 650?659.
579
Pradeep Ravikumar and William W. Cohen. 2004. A hi-
erarchical graphical model for record linkage. In Pro-
ceedings of the 20th Conference on Uncertainty in Ar-
tificial Intelligence, pages 454?461.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proceedings of the Pacific Sym-
posium on BIOCOMPUTING 2003, pages 451?462.
Parag Singla and Pedro Domingos. 2006. Entity reso-
lution with markov logic. In Proceedings of the Sixth
International Conference on Data Mining, pages 572?
582.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 801?808, Morristown, NJ, USA.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to merge word senses.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1005?1014, June.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In Proceedings of the Seventh IEEE
International Conference on Data Mining, pages 342?
350.
William E. Winkler. 1999. The state of record linkage
and current research problems. Technical report, Sta-
tistical Research Division, U.S. Census Bureau.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart
Russell. 2003. Distance metric learning, with applica-
tion to clustering with side-information. In Advances
in Neural Information Processing Systems 17, pages
505?512.
Alexander Yates and Oren Etzioni. 2007. Unsupervised
resolution of objects and relations on the web. In Pro-
ceedings of the 2007 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
580
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 489?499,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Interpretable Semantic Vectors from a Joint Model of Brain- and Text-
Based Meaning
Alona Fyshe
1
, Partha P. Talukdar
1
, Brian Murphy
2
, Tom M. Mitchell
1
1
Machine Learning Department, Carnegie Mellon University
2
School of Electronics, Electrical Engineering and Computer Science
Queen?s University Belfast
[afyshe,partha.talukdar,tom.mitchell]@cs.cmu.edu
brian.murphy@qub.ac.uk
Abstract
Vector space models (VSMs) represent
word meanings as points in a high dimen-
sional space. VSMs are typically created
using a large text corpora, and so repre-
sent word semantics as observed in text.
We present a new algorithm (JNNSE) that
can incorporate a measure of semantics
not previously used to create VSMs: brain
activation data recorded while people read
words. The resulting model takes advan-
tage of the complementary strengths and
weaknesses of corpus and brain activation
data to give a more complete representa-
tion of semantics. Evaluations show that
the model 1) matches a behavioral mea-
sure of semantics more closely, 2) can
be used to predict corpus data for unseen
words and 3) has predictive power that
generalizes across brain imaging technolo-
gies and across subjects. We believe that
the model is thus a more faithful represen-
tation of mental vocabularies.
1 Introduction
Vector Space Models (VSMs) represent lexical
meaning by assigning each word a point in high di-
mensional space. Beyond their use in NLP appli-
cations, they are of interest to cognitive scientists
as an objective and data-driven method to discover
word meanings (Landauer and Dumais, 1997).
Typically, VSMs are created by collecting word
usage statistics from large amounts of text data and
applying some dimensionality reduction technique
like Singular Value Decomposition (SVD). The
basic assumption is that semantics drives a per-
son?s language production behavior, and as a result
co-occurrence patterns in written text indirectly
encode word meaning. The raw co-occurrence
statistics are unwieldy, but in the compressed
VSM the distance between any two words is con-
ceived to represent their mutual semantic similar-
ity (Sahlgren, 2006; Turney and Pantel, 2010), as
perceived and judged by speakers. This space then
reflects the ?semantic ground truth? of shared lex-
ical meanings in a language community?s vocab-
ulary. However corpus-based VSMs have been
criticized as being noisy or incomplete representa-
tions of meaning (Glenberg and Robertson, 2000).
For example, multiple word senses collide in the
same vector, and noise from mis-parsed sentences
or spam documents can interfere with the final se-
mantic representation.
When a person is reading or writing, the se-
mantic content of each word will be necessarily
activated in the mind, and so in patterns of ac-
tivity over individual neurons. In principle then,
brain activity could replace corpus data as input
to a VSM, and contemporary imaging techniques
allow us to attempt this. Functional Magnetic Res-
onance Imaging (fMRI) and Magnetoencephalog-
raphy (MEG) are two brain activation recording
technologies that measure neuronal activation in
aggregate, and have been shown to have a pre-
dictive relationship with models of word mean-
ing (Mitchell et al, 2008; Palatucci et al, 2009;
Sudre et al, 2012; Murphy et al, 2012b).
1
If brain activation data encodes semantics, we
theorized that including brain data in a model of
semantics could result in a model more consistent
with semantic ground truth. However, the inclu-
sion of brain data will only improve a text-based
model if brain data contains semantic information
not readily available in the corpus. In addition,
if a semantic test involves another subject?s brain
activation data, performance can improve only if
the additional semantic information is consistent
across brains. Of course, brains differ in shape,
size and in connectivity, so additional information
encoded in one brain might not translate to an-
1
For more details on fMRI and MEG, see Section 4.2
489
other. Furthermore, different brain imaging tech-
nologies measure very different correlates of neu-
ronal activity. Due to these differences, it is possi-
ble that one subject?s brain activation data cannot
improve a model?s performance on another sub-
ject?s brain data, or for brain data collected using
a different recording technology. Indeed, inter-
subject models of brain activation is an open re-
search area (Conroy et al, 2013), as is learning the
relationship between recording technologies (En-
gell et al, 2012; Hall et al, 2013). Brain data
can also be corrupted by many types of noise (e.g.
recording room interference, movement artifacts),
another possible hindrance to the use of brain data
in VSMs.
VSMs are interesting from both engineering
and scientific standpoints. In this work we fo-
cus on the scientific question: Can the inclusion
of brain data improve semantic representations
learned from corpus data? What can we learn from
such a model? From an engineering perspective,
brain activation data will likely never replace text
data. Brain activation recordings are both expen-
sive and time consuming to collect, whereas tex-
tual data is vast and much of it is free to download.
However, from a scientific perspective, combining
text and brain data could lead to more consistent
semantic models, in turn leading to a better un-
derstanding of semantics and semantic modeling
generally.
In this paper, we leverage both kinds of data to
build a hybrid VSM using a new matrix factor-
ization method (JNNSE). Our hypothesis is that
the noise of brain and corpus derived statistics
will be largely orthogonal, and so the two data
sources will have complementary strengths as in-
put to VSMs. If this hypothesis is correct, we
should find that the resulting VSM is more suc-
cessful in modeling word semantics as encoded in
human judgements, as well as separate corpus and
brain data that was not used in the derivation of the
model. We will show that our method:
1. creates a VSM that is more correlated to an
independent measure of word semantics.
2. produces word vectors that are more pre-
dictable from the brain activity of different
people, even when brain data is collected
with a different recording technology.
3. predicts corpus representations of withheld
words more accurately than a model that does
not combine data sources.
4. directly maps semantic concepts onto the
brain by jointly learning neural representa-
tions.
Together, these results suggest that corpus and
brain activation data measure semantics in com-
patible and complimentary ways. Our results
are evidence that a joint model of brain- and
text-based semantics may be closer to seman-
tic ground truth than text-only models. Our
findings also indicate that there is additional se-
mantic information available in brain activation
data that is not present in corpus data, and that
there are elements of semantics currently lack-
ing in text-based VSMs. We have made avail-
able the top performing VSMs created with brain
and text data (http://www.cs.cmu.edu/
?
afyshe/papers/acl2014/).
In the following sections we will review NNSE,
and our extension, JNNSE. We will describe the
data used and the experiments to support our posi-
tion that brain data is a valuable source of semantic
information that compliments text data.
2 Non-Negative Sparse Embedding
Non-Negative Sparse Embedding (NNSE) (Mur-
phy et al, 2012a) is an algorithm that produces
a latent representation using matrix factorization.
Standard NNSE begins with a matrix X ? R
w?c
made of c corpus statistics for w words. NNSE
solves the following objective function:
argmin
A,D
w
?
i=1
?
?
X
i,:
?A
i,:
?D
?
?
2
+ ?
?
?
A
?
?
1
(1)
subject to: D
i,:
D
T
i,:
? 1,? 1 ? i ? ` (2)
A
i,j
? 0, 1 ? i ? w, 1 ? j ? ` (3)
The solution will find a matrix A ? R
w?`
that is
sparse, non-negative, and represents word seman-
tics in an `-dimensional latent space. D ? R
`?c
gives the encoding of corpus statistics in the la-
tent space. Together, they factor the original cor-
pus statistics matrix X in a way that minimizes
the reconstruction error. TheL
1
constraint encour-
ages sparsity in A; ? is a hyperparameter. Equa-
tion 2 constrains D to eliminate solutions where
A is made arbitrarily small by making D arbi-
trarily large. Equation 3 ensures that A is non-
negative. We may increase ` to give more dimen-
sional space to represent word semantics, or de-
crease ` for more compact representations.
490
The sparse and non-negative representation in
A produces a more interpretable semantic space,
where interpretability is quantified with a behav-
ioral task (Chang et al, 2009; Murphy et al,
2012a). To illustrate the interpretability of NNSE,
we describe a word by selecting the word?s top
scoring dimensions, and selecting the top scoring
words in those dimensions. For example, the word
chair has the following top scoring dimensions:
1. chairs, seating, couches;
2. mattress, futon, mattresses;
3. supervisor, coordinator, advisor.
These dimensions cover two of the distinct mean-
ings of the word chair (furniture and person of
power).
NNSE?s sparsity constraint dictates that each
word can have a non-zero score in only a few di-
mensions, which aligns well to previous feature
elicitation experiments in psychology. In feature
elicitation, participants are asked to name the char-
acteristics (features) of an object. The number of
characteristics named is usually small (McRae et
al., 2005), which supports the requirement of spar-
sity in the learned latent space.
3 Joint Non-Negative Sparse Embedding
We extend NNSEs to incorporate an additional
source of data for a subset of the words in X ,
and call the approach Joint Non-Negative Sparse
Embeddings (JNNSEs). The JNNSE algorithm
is general enough to incorporate any new infor-
mation about the a word w, but for this study
we will focus on brain activation recordings of
a human subject reading single words. We
will incorporate either fMRI or MEG data, and
call the resulting models JNNSE(fMRI+Text) and
JNNSE(MEG+Text) and refer to them generally
as JNNSE(Brain+Text). For clarity, from here
on, we will refer to NNSE as NNSE(Text), or
NNSE(Brain) depending on the single source of
input data used.
Let us order the rows of the corpus data X so
that the first 1 . . . w
?
rows have both corpus statis-
tics and brain activation recordings. Each brain
activation recording is a row in the brain data ma-
trix Y ? R
w
?
?v
where v is the number of features
derived from the recording. For MEG recordings,
v =sensors ? time points= 306? 150. For fMRI
v = grey-matter voxels =' 20, 000 depending on
the brain anatomy of each individual subject. The
new objective function is:
argmin
A,D
(c)
,D
(b)
w
?
i=1
?
?
X
i,:
?A
i,:
?D
(c)
?
?
2
+
w
?
?
i=1
?
?
Y
i,:
?A
i,:
?D
(b)
?
?
2
+ ?
?
?
A
?
?
1
(4)
subject to: D
(c)
i,:
D
(c)
i,:
T
? 1, ? 1 ? i ? ` (5)
D
(b)
i,:
D
(b)
i,:
T
? 1,? 1 ? i ? ` (6)
A
i,j
? 0, 1 ? i ? w, 1 ? j ? `
(7)
We have introduced an additional constraint on the
rows 1 . . . w
?
, requiring that some of the learned
representations in A also reconstruct the brain ac-
tivation recordings (Y ) through representations in
D
(b)
? R
`?v
. Let us use A
?
to refer to the brain-
constrained rows of A. Words that are close in
?brain space? must have similar representations in
A
?
, which can further percolate to affect the rep-
resentations of other words in A via closeness in
?corpus space?.
With A or D fixed, the objective function for
NNSE(Text) and JNNSE(Brain+Text) is convex.
However, we are solving forA andD, so the prob-
lem is non-convex. To solve for this objective, we
use the online algorithm of Section 3 from Mairal
et al (Mairal et al, 2010). This algorithm is
guaranteed to converge, and in practice we found
that JNNSE(Brain+Text) converged as quickly as
NNSE(Text) for the same `. We used the SPAMS
package
2
to solve, and set ? = 0.025. This al-
gorithm was a very easy extension to NNSE(Text)
and required very little additional tuning.
We also consider learning shared representa-
tions in the case where data X and Y contain the
effects of known disjoint features. For example,
when a person reads a word, the recorded brain
activation data Y will contain the physiological
response to viewing the stimulus, which is unre-
lated to the semantics of the word. These sig-
nals can be attributed to, for example, the num-
ber of letters in the word and the number of white
pixels on the screen (Sudre et al, 2012). To ac-
count for such effects in the data, we augment
A
?
with a set of n fixed, manually defined fea-
tures (e.g. word length) to create A
?
percept
?
R
w?(`+n)
. D
(b)
? R
(`+n)?v
is used withA
?
percept
,
2
SPAMS Package: http://spams-devel.gforge.inria.fr/
491
to reconstruct the brain data Y . More gener-
ally, one could instead allocate a certain num-
ber of latent features specific to X or Y, both of
which could be learned, as explored in some re-
lated work (Gupta et al, 2013). We use 11 per-
ceptual features that characterize the non-semantic
features of the word stimulus (for a list, see sup-
plementary material at http://www.cs.cmu.
edu/
?
afyshe/papers/acl2014/).
The JNNSE algorithm is advantageous in that
it can handle partially paired data. That is, the
algorithm does not require that every row in X
also have a row in Y . Fully paired data is a re-
quirement of many other approaches (White et al,
2012; Jia and Darrell, 2010). Our approach al-
lows us to leverage the semantic information in
corpus data even for words without brain activa-
tion recordings.
JNNSE(Brain+Text) does not require brain data
to be mapped to a common average brain, which
is often the case when one wants to generalize be-
tween human subjects. Such mappings can blur
and distort data, making it less useful for subse-
quent prediction steps. We avoid these mappings,
and instead use the fact that similar words elicit
similar brain activation within a subject. In the
JNNSE algorithm, it is this closeness in ?brain
space? that guides the creation of the latent space
A. Leveraging intra-subject distance measures
to study inter-subject encodings has been studied
previously (Kriegeskorte et al, 2008a; Raizada
and Connolly, 2012), and has even been used
across species (humans and primates) (Kriegesko-
rte et al, 2008b).
Though we restrict ourselves to using one sub-
ject per JNNSE(Brain+Text) model, the JNNSE
algorithm could easily be extended to include
data from multiple brain imaging experiments by
adding a new squared loss term for additional
brain data.
3.1 Related Work
Perhaps the most well known related approach
to joining data sources is Canonical Correlation
Analysis (CCA) (Hotelling, 1936), which has been
applied to brain activation data in the past (Rus-
tandi et al, 2009). CCA seeks two linear trans-
formations that maximally correlate two data sets
in the transformed form. CCA requires that the
data sources be paired (all rows in the corpus data
must have a corresponding brain data), as corre-
lation between points is integral to the objective.
To apply CCA to our data we would need to dis-
card the vast majority of our corpus data, and use
only the 60 rows of X with corresponding rows
in Y. While CCA holds the input data fixed and
maximally correlates the transformed form, we
hold the transformed form fixed and seek a solu-
tion that maximally correlates the reconstruction
(AD
(c)
or A
?
D
(b)
) with the data (X and Y respec-
tively). This shift in error compensation is what
allows our data to be only partially paired. While
a Bayesian formulation of CCA can handle miss-
ing data, our model has missing data for> 97% of
the full w ? (v + c) brain and corpus data matrix.
To our knowledge, this extreme amount of missing
data has not been explored with Bayesian CCA.
One could also use a topic model style formula-
tion to represent this semantic representation task.
Supervised topic models (Blei and McAuliffe,
2007) use a latent topic to generate two observed
outputs: words in a document and a categorical la-
bel for the document. The same idea could be ap-
plied here: the latent semantic representation gen-
erates the observed brain activity and corpus statis-
tics. Generative and discriminative models both
have their own strengths and weaknesses, gener-
ative models being particularly strong when data
sources are limited (Ng and Jordan, 2002). Our
task is an interesting blend of data-limited and
data-rich problem scenarios.
In the past, various pieces of additional informa-
tion have been incorporated into semantic models.
For example, models with behavioral data (Sil-
berer and Lapata, 2012) and models with visual
information (Bruni et al, 2011; Silberer et al,
2013) have both shown to improve semantic rep-
resentations. Other works have correlated VSMs
built with text or images with brain activation
data (Murphy et al, 2012b; Anderson et al, 2013).
To our knowledge, this work is the first to integrate
brain activation data into the construction of the
VSM.
4 Data
4.1 Corpus Data
The corpus statistics used here are the download-
able vectors from Fyshe et al (2013)
3
. They
are compiled from a 16 billion word subset of
ClueWeb09 (Callan and Hoy, 2009) and contain
two types of corpus features: dependency and doc-
ument features, found to be complimentary for
3
http://www.cs.cmu.edu/
?
afyshe/papers/
conll2013/
492
most tasks. Dependency statistics were derived
by dependency parsing the corpus and compil-
ing counts for all dependencies incident on the
word. Document statistics are word-document
co-occurrence counts. Count thresholding was
applied to reduce noise, and positive pointwise-
mutual-information (PPMI) (Church and Hanks,
1990) was applied to the counts. SVD was ap-
plied to the document and dependency statistics
and the top 1000 dimensions of each type were
retained. We selected the rows corresponding to
noun-tagged words (approx. 17000 words).
4.2 Brain Activation Data
We have MEG and fMRI data at our disposal.
MEG measures the magnetic field caused by many
thousands of neurons firing together, and has good
time resolution (1000 Hz) but poor spatial reso-
lution. fMRI measures the change in blood oxy-
genation that results from differential neural ac-
tivity, and has good spatial resolution but poor
time resolution (0.5-1 Hz). We have fMRI data
and MEG data for 18 subjects (9 in each imaging
modality) viewing 60 concrete nouns (Mitchell et
al., 2008; Sudre et al, 2012). The 60 words span
12 word categories (animals, buildings, tools, in-
sects, body parts, furniture, building parts, uten-
sils, vehicles, objects, clothing, food). Each of the
60 words was presented with a line drawing, so
word ambiguity is not an issue. For both record-
ing modalities, all trials for a particular word were
averaged together to create one training instance
per word, with 60 training instances in all for each
subject and imaging modality. More preprocess-
ing details appear in the supplementary material.
5 Experimental Results
Here we explore several variations of JNNSE and
NNSE formulations. For a comparison of the
models used, see Table 1.
5.1 Correlation to Behavioral Data
To test if our joint model of Brain+Text is closer
to semantic ground truth we compared the latent
representation A learned via JNNSE(Brain+Text)
or NNSE(Text) to an independent behavioral mea-
sure of semantics. We collected behavioral data
for the 60 nouns in the form of answers to 218
semantic questions. Answers were gathered with
Mechanical Turk. The full list of questions ap-
pear in the supplementary material. Some exam-
ple questions are:?Is it alive??, and ?Can it bend??.
Mechanical Turk users were asked to respond to
each question for each word on a scale of 1-5. At
least 3 respondents answered each question and
the median score was used. This gives us a se-
mantic representation of each of the 60 words in
a 218-dimensional behavioral space. Because we
required answers to each of the questions for all
words, we do not have the problems of sparsity
that exist for feature production norms from other
studies (McRae et al, 2005). In addition, our an-
swers are ratings, rather than binary yes/no an-
swers.
For a given value of ` we solve the NNSE(Text)
and JNNSE(Brain+Text) objective function as de-
tailed in Equation 1 and 4 respectively. We com-
pared JNNSE(Brain+Text) and NNSE(Text) mod-
els by measuring the correlation of all pairwise
distances in JNNSE(Brain+Text) and NNSE(Text)
space to the pairwise distances in the 218-
dimensional semantic space. Distances were
calculated using normalized Euclidean distance
(equivalent in rank-ordering to cosine distance,
but more suitable for sparse vectors). Figure 1
shows the results of this correlation test. The er-
ror bars for the JNNSE(Brain+Text) models rep-
resent a 95% confidence interval calculated using
the standard error of the mean (SEM) over the 9
person-specific JNNSE(Brain+Text) models. Be-
cause there is only one NNSE(Text) model for
each dimension setting, no SEM can be calculated,
but it suffices to show that the NNSE(Text) corre-
lation does not fall into the 95% confidence inter-
val of the JNNSE(Brain+Text) models. The SVD
matrix for the original corpus data has correlation
0.4279 to the behavioral data, also below the 95%
confidence interval for all JNNSE models. The re-
sults show that a model that incorporates brain ac-
tivation data is more faithful to a behavioral mea-
sure of semantics.
5.2 Word Prediction from Brain Activation
We now show that the JNNSE(Brain+Text) vec-
tors are more consistent with independent sam-
ples of brain activity collected from different sub-
jects, even when recorded using different record-
ing technologies. As previously mentioned, be-
cause there is a large degree of variation between
brains and because MEG and fMRI measure very
different correlates of neuronal activity, this type
of generalization has proven to be very challeng-
ing and is an open research question in the neuro-
science community.
The output A of the JNNSE(Brain+Text) or
493
Table 1: A Comparison of the models explored in this paper, and the data upon which they operate.
Model Name Section(s) Text Data Brain Data Withheld Data
NNSE(Text) 2, 5 X x -
NNSE(Brain) 2, 5.2.1, 5.3 x X -
JNNSE(Brain+Text) 3, 5 X X -
JNNSE(Brain+Text): Dropout task 5.2.2 X X subset of brain data
JNNSE(Brain+Text): Predict corpus 5.3 X X subset of text data
250 500 10000.4
0.42
0.44
0.46
0.48
0.5
Correlation of Semantic Question Distances to JNNSE(fMRI)
Number of Latent Dimensions
Cor
rela
tion
 
 JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 1: Correlation of JNNSE(Brain+Text) and
NNSE(Text) models with the distances in a se-
mantic space constructed from behavioral data.
Error bars indicate SEM.
NNSE(Text) algorithm can be used as a VSM,
which we use for the task of word prediction from
fMRI or MEG recordings. A JNNSE(Brain+Text)
created with a particular human subject?s data is
never used in the prediction framework with that
same subject. For example, if we use fMRI data
from subject 1 to create a JNNSE(fMRI+Text), we
will test it with the remaining 8 fMRI subjects, but
all 9 MEG subjects (fMRI and MEG subjects are
disjoint).
Let us call the VSM learned with
JNNSE(Brain+Text) or NNSE(Text) the se-
mantic vectors. We can train a weight matrix W
that predicts the semantic vector a of a word from
that word?s brain activation vector x: a = Wx.
W can be learned with a variety of methods, we
will use L
2
regularized regression. One can also
train regressors that predict the brain activation
data from the semantic vector: x = Wa, but we
have found this to give lower predictive accuracy.
Note that we must re-train our weight matrix W
for each subject (instead of re-using D
(b)
from
Equation 4) because testing always occurs on a
different subject, and the brain activation data is
not inter-subject aligned.
We train ` independent L
2
regularized regres-
sors to predict the `-dimensional vectors a =
{a
1
. . . a
`
}. The predictions are concatenated
to produce a predicted semantic vector: a? =
{a?
1
, . . . , a?
`
}. We assess word prediction perfor-
mance by testing if the model can differentiate be-
tween two unseen words, a task named 2 vs. 2 pre-
diction (Mitchell et al, 2008; Sudre et al, 2012).
We choose the assignment of the two held out se-
mantic vectors (a
(1)
,a
(2)
) to predicted semantic
vectors (a?
(1)
, a?
(2)
) that minimizes the sum of the
two normalized Euclidean distances. 2 vs. 2 ac-
curacy is the percentage of tests where the correct
assignment is chosen.
The 60 nouns fall into 12 word categories.
Words in the same word category (e.g. screw-
driver and hammer) are closer in semantic space
than words in different word categories, which
makes some 2 vs. 2 tests more difficult than oth-
ers. We choose 150 random pairs of words (with
each word represented equally) to estimate the dif-
ficulty of a typical word pair, without having to
test all
(
60
2
)
word pairs. The same 150 random
pairs are used for all subjects and all VSMs. Ex-
pected chance performance on the 2 vs. 2 test is
50%.
Results for testing on fMRI data in the
2 vs. 2 framework appear in Figure 2.
JNNSE(fMRI+Text) data performed on aver-
age 6% better than the best NNSE(Text), and
exceeding even the original SVD corpus represen-
tations while maintaining interpretability. These
results generalize across brain activity recording
types; JNNSE(MEG+Text) performs as well as
JNNSE(fMRI+Text) when tested on fMRI data.
The results are consistent when testing on MEG
data: JNNSE(MEG+Text) or JNNSE(fMRI+Text)
outperforms NNSE(Text) (see Figure 3).
494
250 500 1000
64
66
68
70
72
74
Number of Latent Dimensions
2 vs
. 2 A
ccu
racy
2 vs. 2 Acc. for JNNSE and NNSE, tested on fMRI data
 
 
JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 2: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
fMRI data. Models created with one subject?s
fMRI data were not used to compute 2 vs. 2 ac-
curacy for that same subject.
250 500 1000
66
68
70
72
74
76
78
80
82
Number of Latent Dimensions
2 vs
. 2 A
ccu
racy
2 vs. 2 Acc. for JNNSE and NNSE, tested on MEG data
 
 
JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 3: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
MEG data. Models created with one subject?s
MEG data were not used to compute 2 vs. 2 ac-
curacy for that same subject.
NNSE(Text) performance decreases as the
number of latent dimension increases. This im-
plies that without the regularizing effect of brain
activation data, the extra NNSE(Text) dimensions
are being used to overfit to the corpus data, or
possibly to fit semantic properties not detectable
with current brain imaging technologies. How-
ever, when brain activation data is included, in-
creasing the number of latent dimensions strictly
increases performance for JNNSE(fMRI+Text).
JNNSE(MEG+Text) has peak performance with
500 latent dimensions, with ? 1% decrease in
performance at 1000 latent dimensions. In previ-
ous work, the ability to decode words from brain
activation data was found to improve with added
latent dimensions (Murphy et al, 2012a). Our
results may differ because our words are POS
tagged, and we included only nouns for the final
NNSE(Text) model. We found that with the orig-
inal ? = 0.05 setting from Murphy et al (Mur-
phy et al, 2012a) produced vectors that were too
sparse; four of the 60 test words had all-zero vec-
tors (JNNSE(Brain+Text) models did have any all-
zero vectors). To improve the NNSE(Text) vectors
for a fair comparison, we reduced ? = 0.025, un-
der which NNSE(Text) did not produce any all-
zero vectors for the 60 words.
Our results show that brain activation data con-
tributes additional information, which leads to an
increase in performance for the task of word pre-
diction from brain activation data. This suggests
that corpus-only models may not capture all rel-
evant semantic information. This conflicts with
previous studies which found that semantic vec-
tors culled from corpus statistics contain all of the
semantic information required to predict brain ac-
tivation (Bullinaria and Levy, 2013).
5.2.1 Prediction from a Brain-only Model
How much predictive power does the corpus data
provide to this word prediction task? To test
this, we calculated the 2 vs. 2 accuracy for a
NNSE(Brain) model trained on brain activation
data only. We train NNSE(Brain) with one sub-
ject?s data and use the resulting vectors to calculate
2 vs. 2 accuracy for the remaining subjects. We
have brain data for only 60 words, so using ` ? 60
latent dimensions leads to an under-constrained
system and a degenerate solution wherein only one
latent dimension is active for any word (and where
the brain data can be perfectly reconstructed). The
degenerate solution makes it impossible to gen-
eralize across words and leads to performance at
chance levels. An NNSE(MEG) trained on MEG
data gave maximum 2 vs. 2 accuracy of 67% when
` = 20. The reduced performance may be due to
the limited training data and the low SNR of the
data, but could also be attributed to the lack of cor-
pus information, which provides another piece of
semantic information.
495
5.2.2 Effect on Rows Without Brain Data
It is possible that some JNNSE(Brain+Text) di-
mensions are being used exclusively to fit brain
activation data, and not the semantics represented
in both brain and corpus data. If a particular
dimension j is solely used for brain data, the
sparsity constraint will favor solutions that sets
A
(i,j)
= 0 for i > w
?
(no brain data constraint),
and A
(i,j)
> 0 for some 0 ? i ? w
?
(brain data
constrained). We found that there were no such
dimensions in the JNNSE(Brain+Text). In fact for
the ` = 1000 JNNSE(Brain+Text), all latent di-
mensions had greater than ? 25% non-zero en-
tries, which implies that all dimensions are being
shared between the two data inputs (corpus and
brain activation), and are used to reconstruct both.
To test that the brain activation data is truly in-
fluencing rows of A not constrained by brain acti-
vation data, we performed a dropout test. We split
the original 60 words into two 30 word groups (as
evenly as possible across word categories). We
trained JNNSE(fMRI+Text) with 30 words, and
tested word prediction with the remaining 8 sub-
jects and the other 30 words. Thus, the training
and testing word sets are disjoint. Because of the
reduced size of the training data, we did see a drop
in performance, but JNNSE(fMRI+Text) vectors
still gave word prediction performance 7% higher
than NNSE(Text) vectors. Full results appear in
the supplementary material.
5.3 Predicting Corpus Data
Here we ask: can an accurate latent representa-
tion of a word be constructed using only brain
activation data? This task simulates the scenario
where there is no reliable corpus representation of
a word, but brain data is available. This scenario
may occur for seldom-used words that fall below
the thresholds used for the compilation of corpus
statistics. It could also be useful for acronym to-
kens (lol, omg) found in social media contexts
where the meaning of the token is actually a full
sentence.
We trained a JNNSE(fMRI+Text) with brain
data for all 60 words, but withhold the corpus data
for 30 of the 60 words (as evenly distributed as
possible amongst the 12 word categories). The
brain activation data for the 30 withheld words
will allow us to create latent representations in
A for withheld words. Simultaneously, we will
learn a mapping from the latent representation to
the corpus data (D
(c)
). This task cannot be per-
Table 2: Mean rank accuracy over 30 words
using corpus representations predicted by a
JNNSE(MEG+Text) model trained with some
rows of the corpus data withheld. Significance
is calculated using Fisher?s method to combine p-
values for each of the subject-dependent models.
Latent Dim size Rank Accuracy p-value
250 65.30 < 10
?19
500 67.37 < 10
?24
1000 63.47 < 10
?15
formed with a NNSE(Text) model because one
cannot learn a latent representation of a word with-
out data of some kind. This further emphasizes the
impact of brain imaging data, which will allow us
to generalize to previously unseen words in corpus
space.
We use the latent representations in A for each
of the words without corpus data and the mapping
to corpus space D
(c)
to predict the withheld cor-
pus data in X . We then rank the withheld rows of
X by their distance to the predicted row of X and
calculate the mean rank accuracy of the held out
words. Results in Table 2 show that we can recre-
ate the withheld corpus data using brain activation
data. Peak mean rank accuracy (67.37) is attained
at ` = 500 latent dimensions. This result shows
that neural semantic representations can create a
latent representation that is faithful to unseen cor-
pus statistics, providing further evidence that the
two data sources share a strong common element.
How much power is the remaining corpus data
supplying in scenarios where we withhold cor-
pus data? To answer this question, we trained an
NNSE(Brain) model on 30 words of brain activa-
tion, and then trained a regressor to predict cor-
pus data from those latent brain-only representa-
tions. We use the trained regressor to predict the
corpus data for the remaining 30 words. Peak per-
formance is attained at ` = 10 latent dimensions,
giving mean rank accuracy of 62.37, significantly
worse than the model that includes both corpus
and brain activation data (67.37).
5.4 Mapping Semantics onto the Brain
Because our method incorporates brain data into
an interpretable semantic model, we can directly
map semantic concepts onto the brain. To do
this, we examined the mappings from the latent
space to the brain space via D
(b)
. We found that
the most interpretable mappings come from mod-
496
!"#$%&'()
(a) D
(b)
matrix, subject P3, dimension with top words bath-
room, balcony, kitchen. MNI coordinates z=-12 (left) and z=-18
(right). Fusiform is associated with shelter words.
!"#$%&'$()*+
!(&%&'$()*+
(b) D
(b)
matrix; subject P1; dimension with top words ankle,
elbow, knee. MNI coordinates z=60 (left) and z=54 (right). Pre-
and post-central areas are activated for body part words.
!"#$%&'(#)*+"#,$%
(c) D
(b)
matrix; subject P1; dimension with top scoring words
buffet, brunch, lunch. MNI coordinates z=30 (left) and z=24
(right). Pars opercularis is believed to be part of the gustatory
cortex, which responds to food related words.
Figure 4: The mappings (D
(b)
) from latent se-
mantic space (A) to brain space (Y ) for fMRI and
words from three semantic categories. Shown are
representations of the fMRI slices such that the
back of the head is at the top of the image, the
front of the head is at the bottom.
els where the perceptual features had been scaled
down (divided by a constant factor), which en-
courages more of the data to be explained by
the semantic features in A. Figure 4 shows the
mappings (D
(b)
) for dimensions related to shel-
ter, food and body parts. The red areas align
with areas of the brain previously known to be
activated by the corresponding concepts (Mitchell
et al, 2008; Just et al, 2010). Our model
has learned these mappings in an unsupervised
setting by relating semantic knowledge gleaned
from word usage to patterns of activation in the
brain. This illustrates how the interpretability of
JNNSE can allow one to explore semantics in
the human brain. The mappings for one subject
are available for download (http://www.cs.
cmu.edu/
?
afyshe/papers/acl2014/).
6 Future Work and Conclusion
We are interested in pursuing many future projects
inspired by the success of this model. We would
like to extend the JNNSE algorithm to incorporate
data from multiple subjects, multiple modalities
and multiple experiments with non-overlapping
words. Including behavioral data and image data
is another possibility.
We have explored a model of semantics that in-
corporates text and brain activation data. Though
the number of words for which we have brain acti-
vation data is comparatively small, we have shown
that including even this small amount of data has
a positive impact on the learned latent representa-
tions, including for words without brain data. We
have provided evidence that the latent representa-
tions are closer to the neural representation of se-
mantics, and possibly, closer to semantic ground
truth. Our results reveal that there are aspects of
semantics not currently represented in text-based
VSMs, indicating that there may be room for im-
provement in either the data or algorithms used to
create VSMs. Our findings also indicate that using
the brain as a semantic test can separate models
that capture this additional semantic information
from those that do not. Thus, the brain is an im-
portant source of both training and testing data.
Acknowledgments
This work was supported in part by NIH un-
der award 5R01HD075328-02, by DARPA under
award FA8750-13-2-0005, and by a fellowship to
Alona Fyshe from the Multimodal Neuroimag-
ing Training Program funded by NIH awards
T90DA022761 and R90DA023420.
References
Andrew J Anderson, Elia Bruni, Ulisse Bordignon,
Massimo Poesio, and Marco Baroni. 2013. Of
words , eyes and brains : Correlating image-based
distributional semantic models with neural represen-
tations of concepts. In Proceedings of the Confer-
ence on Empirical Methods on Natural Language
Processing.
David M Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In Advances in Neural Information
Processing Systems, pages 1?22.
497
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proceedings of the EMNLP 2011 Geometrical Mod-
els for Natural Language Semantics (GEMS).
John A Bullinaria and Joseph P Levy. 2013. Limiting
factors for mapping corpus-based semantic repre-
sentations to brain activity. PloS one, 8(3):e57191,
January.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09
Dataset.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2009. Reading
Tea Leaves : How Humans Interpret Topic Models.
In Advances in Neural Information Processing Sys-
tems, pages 1?9.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Bryan R Conroy, Benjamin D Singer, J Swaroop Gun-
tupalli, Peter J Ramadge, and James V Haxby. 2013.
Inter-subject alignment of human cortical anatomy
using functional connectivity. NeuroImage, 81:400?
11, November.
Andrew D Engell, Scott Huettel, and Gregory Mc-
Carthy. 2012. The fMRI BOLD signal tracks elec-
trophysiological spectral perturbations, not event-
related potentials. NeuroImage, 59(3):2600?6,
February.
Alona Fyshe, Partha Talukdar, Brian Murphy, and Tom
Mitchell. 2013. Documents and Dependencies : an
Exploration of Vector Space Models for Semantic
Composition. In Computational Natural Language
Learning, Sofia, Bulgaria.
Arthur M Glenberg and David a Robertson. 2000.
Symbol Grounding and Meaning: A Compari-
son of High-Dimensional and Embodied Theories
of Meaning. Journal of Memory and Language,
43(3):379?401, October.
Sunil Kumar Gupta, Dinh Phung, Brett Adams, and
Svetha Venkatesh. 2013. Regularized nonnegative
shared subspace learning. Data Mining and Knowl-
edge Discovery, 26(1):57?97.
Emma L Hall, Si?an E Robson, Peter G Morris, and
Matthew J Brookes. 2013. The relationship be-
tween MEG and fMRI. NeuroImage, November.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
Yangqing Jia and Trevor Darrell. 2010. Factorized La-
tent Spaces with Structured Sparsity. In Advances in
Neural Information Processing Systems, volume 23.
Marcel Adam Just, Vladimir L Cherkassky, Sandesh
Aryal, and Tom M Mitchell. 2010. A neuroseman-
tic theory of concrete noun representation based on
the underlying brain codes. PloS one, 5(1):e8622,
January.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008a. Representational similarity analysis
- connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2(November):4,
January.
Nikolaus Kriegeskorte, Marieke Mur, Douglas A Ruff,
Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky,
Keiji Tanaka, and Peter A Bandettin. 2008b. Match-
ing Categorical Object Representations in Inferior
Temporal Cortex of Man and Monkey. Neuron,
60(6):1126?1141.
TK Landauer and ST Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological review, 1(2):211?240.
Julien Mairal, Francis Bach, J Ponce, and Guillermo
Sapiro. 2010. Online learning for matrix factor-
ization and sparse coding. The Journal of Machine
Learning Research, 11:19?60.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547?59,
November.
Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L Malave, Robert A
Mason, and Marcel Adam Just. 2008. Pre-
dicting human brain activity associated with the
meanings of nouns. Science (New York, N.Y.),
320(5880):1191?5, May.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012a. Learning Effective and Interpretable Se-
mantic Models using Non-Negative Sparse Embed-
ding. In Proceedings of Conference on Computa-
tional Linguistics (COLING).
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012b. Selecting Corpus-Semantic Models for Neu-
rolinguistic Decoding. In First Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 114?123, Montreal, Quebec, Canada.
Andrew Y. Ng and Michael I. Jordan. 2002. On dis-
criminative vs. generative classifiers: A compari-
son of logistic regression and naive bayes. In Ad-
vances in neural information processing systems,
volume 14.
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau,
and Tom M Mitchell. 2009. Zero-Shot Learning
with Semantic Output Codes. Advances in Neural
Information Processing Systems, 22:1410?1418.
Rajeev D S Raizada and Andrew C Connolly. 2012.
What Makes Different People?s Representations
Alike : Neural Similarity Space Solves the Problem
of Across-subject fMRI Decoding. Journal of Cog-
nitive Neuroscience, 24(4):868?877.
498
Indrayana Rustandi, Marcel Adam Just, and Tom M
Mitchell. 2009. Integrating Multiple-Study
Multiple-Subject fMRI Datasets Using Canonical
Correlation Analysis. In MICCAI 2009 Workshop:
Statistical modeling and detection issues in intra-
and inter-subject functional MRI data analysis.
Magnus Sahlgren. 2006. The Word-Space Model Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words. Doctor
of philosophy, Stockholm University.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of Semantic Representation with Vi-
sual Attributes. In Association for Computational
Linguistics 2013, Sofia, Bulgaria.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking Neural Coding of Per-
ceptual and Semantic Features of Concrete Nouns.
NeuroImage, 62(1):463?451, May.
Peter D Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning : Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Martha White, Yaoliang Yu, Xinhua Zhang, and Dale
Schuurmans. 2012. Convex multi-view subspace
learning. In Advances in Neural Information Pro-
cessing Systems, pages 1?14.
499
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1009?1019,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Language-Aware Truth Assessment of Fact Candidates
Ndapandula Nakashole
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
ndapa@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
tom.mitchell@cs.cmu.edu
Abstract
This paper introduces FactChecker,
language-aware approach to truth-finding.
FactChecker differs from prior approaches
in that it does not rely on iterative peer
voting, instead it leverages language to
infer believability of fact candidates. In
particular, FactChecker makes use of lin-
guistic features to detect if a given source
objectively states facts or is speculative
and opinionated. To ensure that fact
candidates mentioned in similar sources
have similar believability, FactChecker
augments objectivity with a co-mention
score to compute the overall believability
score of a fact candidate. Our experiments
on various datasets show that FactChecker
yields higher accuracy than existing
approaches.
1 Introduction
Truth-finding algorithms aim to separate true
statements (facts) from false information. More
specifically, given a set of statements whose truth-
fulness is unknown (fact candidates), the key goal
of truth-finding algorithms is to generate a ranking
such that true statements are ranked ahead of false
ones. Truth-finders have the potential to address a
major obstacle on the Web: the problem of sources
spreading inaccurate and conflicting information.
This problem continues to grow with the develop-
ment of tools for easy Web authorship. Blogs, fo-
rums and social networking websites are not sub-
ject to traditional journalistic standards. Conse-
quently, the accuracy of information reported by
these sources is often unclear. Even more estab-
lished newspapers and websites may sometimes
report false information as they race to break sto-
ries. Therefore, truth-finding is becoming an in-
creasingly important problem. Information extrac-
tion projects aim to distill relational facts from nat-
ural language text (Auer et al, 2007; Bollacker et
al., 2008; Carlson et al, 2010; Fader et al, 2011;
Nakashole et al, 2011; Del Corro and Gemulla,
2013). These projects have produced knowledge
bases containing many millions of relational facts
between entities. However, despite these impres-
sive advances, there are still major limitations re-
garding precision. Within the context of informa-
tion extraction, fact extractors assign confidence
scores to extracted facts. However, such scores
are often tied to the extractor?s ability to read and
understand natural language text. This is differ-
ent from a score that indicates the degree to which
a given fact candidate is believable. Such a be-
lievability score is sometimes also referred to as
a credibility score or truthfulness score. The be-
lievability score reflects the likelihood that a given
statement is true. Truth-finding algorithms aim to
compute this score for each fact candidate.
Prior truth-finding methods are mostly based on
iterative voting, where votes are propagated from
sources to fact candidates and then back to sources
(Yin et al, 2007; Galland et al, 2010; Paster-
nack and Roth, 2010; Li et al, 2011; Yin and
Tan, 2011). At the core of iterative voting is the
assumption that candidates mentioned by many
sources are more likely to be true. However, ad-
ditional aspects of a source influence its trustwor-
thiness, besides external votes.
Our goal is to accurately assess truthfulness of
fact candidates by taking into account the lan-
guage of sources that mention them. A Mechan-
ical Turk study we carried out revealed that there
is a significant correlation between objectivity of
language and trustworthiness of sources. Objec-
tivity of language refers to the use of neutral,
impartial language, which is not personal, judg-
mental, or emotional. Trustworthiness refers to
1009
a source of information being reliable and truth-
ful. We use linguistics features to detect if a given
source objectively states facts or is speculative
and opinionated. Additionally, in order to ensure
that fact candidates mentioned in similar sources
have similar believability scores, our believability
computation model incorporates influence of co-
mentions. However, we must avoid falsely boost-
ing co-mentioned fact candidates. Our model ad-
dresses potential false boosts in two ways: first, by
ensuring that co-mention influence is only propa-
gated to related fact candidates; second, by ensur-
ing that the degree of co-mention influence is de-
termined by the trustworthiness of the sources in
which co-mentions occur.
The contribution of this paper is a language-
aware truth-finding approach. More precisely,
we make the following contributions: (1) Al-
ternative Fact Candidates: Truth-finders rank a
given fact candidate with respect to its alter-
natives. For example, alternative places where
Barack Obama could have been born. Virtually
all existing truth-finders assume that the alterna-
tives are provided. In contrast, we developed a
method for generating alternative fact candidates.
(2) Objectivity-Trustworthiness Correlation: We
hypothesize that objectivity of language and trust-
worthiness of sources are positively correlated. To
test this hypothesis, we designed a Mechanical
Turk study. The study showed that this correlation
does in fact hold. (3) Objectivity Classifier: Us-
ing labeled data from the Mechanical Turk study,
we developed and trained an objectivity classifier
which performed better than prior proposed lexi-
cons from literature. (4) Believability Computa-
tion: We developed FactChecker, a truth-finding
method that linearly combines objectivity and co-
mention influence. Our experiments showed that
FactChecker outperforms prior methods.
2 Fact Candidates
In this section, we formally define what constitutes
a fact candidate and describe how we go about
understanding semantics of fact candidates. We
then present our approach for generating alterna-
tive fact candidates.
2.1 Representation
The triple format is the most common representa-
tion of facts in knowledge bases. A formal specifi-
cation of the triple format is presented in the RDF
primer
1
. In RDF, data is represented as subject-
predicate-object (SPO) triples. In this work, we
restrict predicates to verbs (or verbal phrases such
as ?plays for?, ?graduated from?, etc.). Litera-
ture on automatic relation discovery (Fader et al,
2011) has shown that verbal phrases uncover a
large fraction of binary predicates while reducing
the amount of noisy phrases that do not denote any
relations. Therefore, we define a fact candidate as
follows:
Definition 1 (Fact Candidate) A fact candidate
f
i
is an ?S? V ?O? triple; where S is the subject,
V is a verbal phrase, and O is the object. We aim
to compute the truthfulness of f
i
, ?(f
i
) ? {T, F},
where T and F stand for true and false, respec-
tively.
Note that in this paper we are interested in cases
where ?(f
i
) is either T or F . That is, we assess
truthfulness of factual statements and not opinions
whose truthfulness is often both T and F to some
degree. For example, the triples: ?Obama? born in
?Kenya? and ?Obama? graduated from ?Harvard?
are valid fact candidates. However, the triple:
?Obama? deserves ?Nobel Peace Prize? is not.
2.2 Semantics
Based on the SVO triple, the meaning of a fact
candidate can be unclear and ambiguous. There-
fore, we first determine the semantics of a fact can-
didate before computing its truthfulness.
Entity Types. We first determine the expected
types of the subject and object in the SVO. For ex-
ample, for the SVO ?Einstein? died in ?Princeton?,
the expected types are person ? location. We de-
termine this by first computing the types of en-
tities that are valid for each verb (verbal phrase)
in a large SVO collection of 114m SVO triples
(Talukdar et al, 2012). Typing verbal phrases
is a once-off computation. Our phrase typing
method is similar to prior work on typing rela-
tional phrases (Nakashole et al, 2012). Exam-
ples of typed phrases are: ?person? died in ?year?,
?person? died in ?location?, and ?athlete? plays for
?team?. Given a triple, we look up the types for the
subject and the object and then determine which
of the typed phrases are compatible with the cur-
rent triple. We look up entity types in a knowledge
1
http://www.w3.org/TR/rdf-primer/
1010
base containing entities and their types. In partic-
ular, we use the NELL entity typing API (Carlson
et al, 2010). NELL?s entity typing method has
high recall because when entities are not in the
knowledge base, it performs on-the-fly type infer-
ence using the Web. This is not the case for other
options such as (Auer et al, 2007; Bollacker et al,
2008; Hoffart et al, 2011).
Relation Cardinality. Next, we learn cardinali-
ties of verbal phrases. Cardinality refers to how
arguments of a given relation relate to one another
numerically. We define the relation cardinality of a
verb Card(V ), as the average number of expected
arguments per given subject. For example, for the
relation ?died in?, 1 location is expected for each
subject. For other relations, the expected number
of arguments can be greater than 1 but less than
n : n ? R, n > 1. We approximate n using
statistics from the 114m SVO corpus based on the
average number of arguments per given first argu-
ment. In a once-off computation, we generate car-
dinality approximations per typed verbal phrase V
and its inverse V
?1
. For example, we generate
the cardinality estimates for both: ?person? died in
?location? and for ?location? INVERSE-OF(died
in) ?person?.
Synonymous Relations. Natural language is di-
verse. Semantically similar phrases can be syntac-
tically different. Therefore, we learn other verbs
that can be used to substitute V in SVO. We
pre-compute synonymous phrases from the 114m
SVO corpus using distributional semantics in the
same spirit as (Lin and Pantel, 2001; Nakashole et
al., 2012).
Synonymous verbs, relation cardinalities, and
entity types enable us to generate alternative fact
candidates.
2.3 Alternative Fact Candidates
Truth-finding methods rank f
i
relative to alter-
native candidates. While prior methods assume
the alternatives are known apriori, we developed
a method for generating alternative fact candi-
dates. For a given f
i
, we first identify the fixed
argument. The fixed argument is the argument of
the SVO which when fixed, requires finding the
fewest number of alternative candidates. For ex-
ample, for ?Einstein? died in ?Princeton?, the so-
lution is to fix the subject. This is because the car-
dinality of ?person? died in ?location? is one (1).
On the other hand, the cardinality of ?INVERSE-
OF(died in)? is many(n). In other words, the num-
ber of places where a person can be born (one)
is much fewer than the number of people that
can die in a place (many). In our example, al-
ternatives are possible places, other than Prince-
ton, where Einstein could have died. For example:
?Einstein? died in ?Germany? or ?Einstein? died in
?Switzerland?. More generally, the fixed argument
of fact candidate f
i
, is defined as follows:
Definition 2 (Fixed Argument) Let Card(V) be
the cardinality of V and Card(V
?1
) be the car-
dinality of the inverse of V , if Card(V ) <
Card(V
?1
), then the fixed argument is the sub-
ject, Arg
fixed
(f
i
) = S, else it is the object, O. If
Card(V ) == Card(V
?1
), then both arguments
are fixed, one at a time.
We use the fixed argument to define a topic as the
fixed argument plus the verb. Therefore, for the
SVO ?X? died in ?Y?, the topic ?places where X
died?, (Arg
fixed
= S), is not the same as the topic
?people who died in Y? (Arg
fixed
= O).
To locate alternatives, we use the topic
(Arg
fixed
+ V ) as a query. We search three
sources to either locate relevant documents or rele-
vant triples: the Google Web search API, the 114m
SVO collection, and the NELL KB. The SVO col-
lection and the KB return triples, however, the
Web search API returns documents. Therefore,
we apply a triple extractor to the retrieved docu-
ments. For all potential alternative triples, we per-
form type checking to ensure that the arguments
of the triples are type-compatible with f
i
. Further-
more, we generate an additional query for every
synonymous verb sV
i
, replacing V with sV
i
. Ex-
ample queries are: ?Einstein died in?, ?Einstein
passed in?, etc.
3 Objectivity and Trustworthiness
The principle of objective journalism, which is
a significant part of journalistic ethics, aims to
promote factual and fair reporting, undistorted by
emotion or personal bias (Schudson, 1978; Ka-
plan, 2002). Objectivity is also required in refer-
ence sources such as encyclopedias, scientific pub-
lications, and textbooks. For example, Wikipedia
enforces a neutral point-of-view policy (NPOV)
2
.
Articles violating the NPOV policy are marked
2
http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view
1011
to indicate potential bias. While opinions, emo-
tions, and speculations can also be expressed us-
ing objective language, they are often stated using
subjective language (Turney et al, 2002; Riloff
and Wiebe, 2003; Yu and Hatzivassiloglou, 2003;
Wiebe et al, 2004; Liu et al, 2005; Recasens et
al., 2013). For example, consider the following
pieces of text:
(S) Well, I think Obama was born in Kenya
because his grandma who lives in Kenya said
he was born there.
(O) Theories allege that Obama?s published
birth certificate is a forgery, that his actual
birthplace is not Hawaii but Kenya.
Text S is a snippet from Yahoo Answers and
text O is a snippet from the Wikipedia page ti-
tled: ?Barack Obama Citizenship Conspiracy The-
ories?. S is subjective, expressing the opinion of
the author. On the other hand, O is objective, stat-
ing only what has been alleged. Literature on sen-
timent analysis (Turney et al, 2002; Liu et al,
2005), subjectivity detection (Riloff and Wiebe,
2003; Wiebe et al, 2004), and bias detection (Yu
and Hatzivassiloglou, 2003; Recasens et al, 2013)
has developed lexicons for identifying subjective
language. Due to the principle of objective jour-
nalism and the requirement of objectivity placed
on reference sources, we hypothesize a link be-
tween objectivity and trustworthiness as follows.
Hypothesis 1 Objective sources are more trust-
worthy than subjective sources. Therefore, we
can assume that fact candidates stated in objec-
tive sources are more likely to be true than those
stated in subjective sources.
To test the validity of the hypothesis, we carried
out a study where we solicited human input.
3.1 Mechanical Turk Study
We deployed an annotation study on Amazon Me-
chanical Turk (MTurk)
3
, a crowd-sourcing plat-
form for tasks requiring human input. Tasks on
MTurk are small questionnaires consisting of a de-
scription and a set of questions. Our study con-
sisted of two independent tasks. The first task was
titled ?Trustworthiness of News Articles?, where
annotators were given a link to a news article and
3
http://www.mturk.com
Figure 1: Summary of the results of the annotation
study on objectivity and trustworthiness.
asked to judge if they thought it was trustworthy
or not. The second task was titled ?Objectivity
of News Articles?. For this task, annotators were
asked to judge if a given article is objective or sub-
jective. For both tasks a third option of ?not sure?
was provided. We randomly selected 500 news ar-
ticles from a corpus of about 300,000 news articles
obtained from Google News from the topics of
Top News, Business, Entertainment, and SciTech.
For each task, every article was judged by three
annotators. This produced a total of 3000 annota-
tions. When we analyzed the output, we accepted
a label as valid for a given article if the label was
selected by the majority of the judges. Based on
this criteria, we obtained a set of 420 articles that
were both labeled for trustworthiness and objec-
tivity.
A summary of the outcome of the study is
shown in Figure 1; 74% of the untrustworthy
articles were independently labeled as subjec-
tive. On the other hand, 64% of trustworthy
articles were independently labeled as objective.
These results indicate a non-trivial positive cor-
relation between objectivity and trustworthiness.
We leverage this correlation in our believability
computation model. To incorporate objectivity in
FactChecker, we require for a given source docu-
ment, an objectivity score ? [0, 1], where 0 means
the source is subjective and 1 means it is objec-
tive. Next, describe our method for automatically
determining objectivity of sources.
3.2 Automatic Objectivity Detection
We trained a logistic regression classifier to pre-
dict the objectivity of a document. For training
and testing data, we used the labeled data from
the Mechanical Turk study. We additionally used
labeled text from prior work on subjectivity de-
tection (Pang and Lee, 2004). This resulted in a
total of 4, 600 documents, half subjective and the
other half objective. We used 4000 documents for
1012
# Feature
1 Subjectivity lexicon of strong and weak
subjective words (Riloff and Wiebe,
2003).
2 Sentiment lexicon of positive and negative
words (Liu et al, 2005).
3 Wikipedia-derived bias lexicon (Recasens
et al, 2013).
4 Part-of-speech (POS) tags
5 Frequent bi-grams
Table 1: Features used for the objectivity detector.
training, 2000 per label. The rest of the documents
were split into a development set (380) and a test
set (220).
A summary of the features we used is shown
in Table 1. Features 1-3 refer to lexicons devel-
oped by prior methods on subjectivity (Wiebe et
al., 2004), sentiment analysis (Liu et al, 2005) and
bias detection (Recasens et al, 2013). Feature 4
refers to part-of-speech tags of the terms found in
the document that are also in the lexicons. Feature
5 refers to bi-grams that frequently occur (men-
tion frequency of > 10) in the 4, 600 documents.
The most contributing features were the lexicons,
features (1-3) and the frequent bi-grams, feature
5. We discovered that using frequent bi-gram fea-
tures instead of uni-grams or bi-grams resulted in
higher precision. The classifier was able to de-
termine that for example bi-grams such as ?think
that?, ?so funny? and ?you thought? are negative
features for objectivity. Evaluation results of our
objectivity detector vs. baselines are shown in Ta-
ble 2. FactChecker?s objectivity detector has pre-
cision of 0.7814 ? 0.0539, with a 0.9-confidence
Wilson score interval (Brown et al, 2001) and this
outperforms the baselines. Next, we describe how
we leverage objectivity into FactChecker?s truth-
fulness model.
4 Believability Computation Model
FactChecker computes the believability score of a
fact candidate from its: i) objectivity score and
(ii) co-mention score. In this section we define
each of these scores.
The objectivity score reflects the trustworthi-
ness of sources where a fact candidate is men-
tioned. Given a fact candidate f
i
, mentioned in
a set of documents D
i
, where each document d ?
Approach Accuracy
Sentiment Lexicon 0.65?0.06
Wikipedia bias Lexicon 0.69?0.06
Subjectivity Lexicon 0.70?0.06
FC-Objectivity Detector 0.78?0.05
Table 2: Accuracy of the objectivity detector.
D
i
has objectivity O(d), f
i
?s objectivity score is
defined as follows:
Definition 3 (Objectivity Score)
O(f
i
) = log|D
i
|.
?
d
k
?D
i
O(d
k
)
|D
i
|
(1)
We do not use the sum of objectivity of sources
as the objectivity score because this enables fact
candidates mentioned in many low objectivity
sources to have high aggregate objectivity. Sim-
ilarly, we avoid using average objectivity of the
sources as it overestimates objectivity of candi-
dates stated in few sources. A candidate men-
tioned in 10 sources with 0.9 objectivity should
have higher objectivity than a candidate stated in
1 source of 0.9 objectivity. In Equation 1, log|D
i
|
addresses this issue.
The co-mention score aims to ensure that fact
candidates mentioned in similar sources have sim-
ilar believability scores. Suppose candidate f
i
is
mentioned in many highly objective sources, an-
other candidate f
j
is stated in only one highly ob-
jective source d
k
where f
i
is also mentioned. Then
the believability of f
j
should be boosted by it be-
ing co-mentioned with f
i
. If on the other hand f
i
and f
j
were co-mentioned in a subjective source,
f
j
should receive less boost from f
i
. This leads us
to the co-mention score ?(f
i
) of a candidate.
Definition 4 (Co-Mention Score)
?(f
i
) = ?(f
i
) +
?
f
j
?F
w
ij
?(f
j
) (2)
Where ?(f
i
) is the normalized mention fre-
quency of f
i
. The propagation weight w
ij
controls
how much boost is obtained from a co-mentioned
candidate. We define propagation weight, w
ij
, as
the average of the objectivity of the sources that
mention both candidates.
w
ij
= average O(d
k
) : d
k
? (D
i
?D
j
) (3)
1013
where O(d
k
) is the objectivity of document d
k
,
D
i
and D
j
are the sets of documents that mention
f
i
and f
j
, respectively. Notice that we could boost
co-mentioned but not related candidates, thereby
causing false boosts. To remedy this, we only al-
low w
ij
to be greater than zero if the fact can-
didates f
i
and f
j
are on the same topic. Recall
that the topic is determined by the fixed argument
(Definition 2) and the verb. Allowing only fact
candidates on the same topic to influence each
other is important considering that many trivial
facts are often repeated in sources of diverse qual-
ity.
To leverage the inter-dependencies among re-
lated co-mentioned fact candidates, we model the
solution with a graph ranking method. Each fact
candidate is a node and there is an edge between
each pair of related fact candidate nodes f
i
and
f
j
, with w
ij
as the edge weight. Thus, equation 2
can be reformulated as ? = M?, where ? is the
co-mention score vector and M is a Markov ma-
trix which is stochastic, irreducible and aperiodic.
Thus, a power method will converge to a solution
in a similar manner to PageRank. Implementation
consists of iteratively applying Equation 2 until
the change in the score is less than a threshold .
The solution is the final co-mention scores of fact
candidates.
Finally, to compute the believability score of a
fact candidate, we linearly combine its objectivity
score with its co-mention as follows:
Definition 5 (Believability Score)
?(f
i
) = ?O(f
i
) + (1? ?)?(f
i
) (4)
Where ? is a weighting parameter ? [0, 1]
which controls the relative importance of the two
aspects of FactChecker. As we show in our exper-
iments, ? can be robustly chosen within the range
of 0.2 to 0.6. In our experiments we used ? = 0.6.
The entire procedure of FactChecker is summa-
rized in Algorithm 1.
5 Evaluation
We evaluated FactChecker for accuracy. We de-
fine accuracy as the probability of a true fact can-
didate having a higher believability score than a
false candidate. Let ?(f
i
) ? {T, F} be the truth-
fulness of a fact candidate f
i
, accuracy is defined
as:
Algorithm 1 FactChecker
Input: A set F of fact candidates
Input: KB K, SVO corpus C, WebW
Output: A set L of rankings ?f
i
? F
L = ?
while F 6= ? do
pick f
i
from F
A= getAlternatives(f
i
,K,C,W)
PriorityQueue L
i
= ?
for all alternative fact candidates f
?
j
? A do
?(f
?
j
) = getBelievabilityScore(f
?
j
)
L
i
.insert(f
?
j
, ?(f
?
j
))
end for
?(f
i
) = getBelievabilityScore(f
i
)
L
i
.insert(f
i
, ?(f
i
))
L ? L
i
Remove f
i
from F
end while
return L
Acc =
?
(?(f
i
)=T :?(f
j
)=F )
(?(f
i
) > ?(f
j
))
|{?(f
i
, f
j
) : ?(f
i
) = T ? ?(f
j
) = F}|
Datasets. We evaluated FactChecker on three
datasets: i) KB Fact Candidates: The first dataset
consists of fact candidates taken from the fact ex-
traction pipeline of a state-of-the-art knowledge
base, NELL (Carlson et al, 2010). The fact candi-
dates span four different relation types: company
acquisitions, book authors, movie directors and
athlete teams. For each fact candidate, we applied
our alternative candidate generation method. We
only considered fact candidates with non-trivial
alternative candidate sets; where the alternative
candidate set is greater than zero. Since all of
the baselines we compared against assume alter-
natives are provided, we apply all methods to the
same set of alternative fact candidates discovered
by our method. Details of this dataset are shown
as rows starting with ?KB-? in Table 3.
ii) Wikipedia Fact Candidates: For the sec-
ond dataset, we did not restrict the fact candidates
to specific topics from a knowledge base, instead
we aimed to evaluate all fact candidates about a
given entity. We selected entities from Wikipe-
dia. For this, we chose US politicians: all current
state senators, all current state governors, and all
44 presidents. First, we extracted fact candidates
1014
#Candidates #Alternatives
KB-Acquisitions 50 241
KB-Authors 50 295
KB-Directors 50 228
KB-Teams 40 162
WKP Politicians 54 219
GK Quiz 18 72
Table 3: Fact candidate datasets.
from the infoboxes of the Wikipedia pages of the
entities. Second, we applied our alternative can-
didate generation method to discover alternatives
from the Web, SVO corpus, and NELL. Details of
the resulting dataset are shown in the row ?WKP
Politicians? in Table 3.
iii) General Knowledge Quiz: The third
dataset consists of questions from a general
knowledge quiz
4
. We selected questions from
the inventions category. Questions are multiple
choice, with 4 options per question. Thus, from
each question, we created one fact candidate and
3 alternative candidates. Details of the resulting
dataset are shown in the row ?KWP Quiz? in Ta-
ble 3.
Baselines. We compared FactChecker against five
baselines: i) Vote counts the number of sources
that mention the fact candidate. ii) TruthFinder is
an iterative voting approach where votes are prop-
agated from sources to fact candidates and then
back to sources. Implemented as described in (Yin
et al, 2007). iii) Investment is also based on tran-
sitive voting, however scores are updated differ-
ently. A source gets a vote of trust from each
candidate it ?invests? in, but the vote is weighted
by the proportion of trust the source previously
?invested? in the candidate relative to other in-
vestors. Implemented as described in (Pasternack
and Roth, 2010). iv) PooledInvest is a variation
of investment, we report both because in their pa-
per, there was no clear winner among the two vari-
ations. v) 2-Estimates is a probabilistic model
which approximates error rates of sources and fact
candidates (Galland et al, 2010).
5.1 Accuracy on KB Fact Candidates
Figure 2 shows accuracy on KB fact candidates.
FactChecker achieves accuracy between 70% and
88% and is significantly more accurate than the
4
http://www.indiabix.com/general-knowledge/
Figure 2: Accuracy of KB fact candidates.
Figure 3: FactChecker variations.
other approaches on all relations except com-
pany acquisitions. On book authors, movie di-
rectors, and athlete teams, FactChecker outper-
forms all other approaches by at least 10%, 9%,
and 8% respectively. On company acquisitions,
the different methods achieve similar accuracy,
with TruthFinder being the most accurate and
FactChecker is 4% behind. Company acquisitions
also yield the lowest difference between Vote and
the highest performing method, of 6%. For book
authors, movie directors, and athlete teams, the
difference between majority Vote and the highest
performing method (FactChecker in this case) is
13%, 12%, and 13% respectively.
5.2 Accuracy of FactChecker Variations
To quantify how various aspects of our approach
affect overall performance, we studied two varia-
tions. The first variation is FC-Objectivity which
only uses objectivity to compute believability.
Thus, ? = 1 in Definition 5. The second varia-
tion is FC-CoMention which only uses co-mention
scores to compute believability, ? = 0. The
1015
Approach WKP Politicians GK Quiz
Vote 0.85?0.09 0.82?0.15
TruthFinder 0.85?0.09 0.82?0.15
2-Estimates 0.85?0.09 0.82?0.15
Investment 0.86?0.08 0.82?0.15
PooledInvest 0.85?0.09 0.82?0.15
FC-Objectivity 0.88?0.08 0.87?0.12
FC-CoMention 0.85?0.09 0.72?0.18
FactChecker 0.90?0.07 0.87?0.12
Table 4: Accuracy on politicians and quiz data sets
last variation is the full FactChecker method us-
ing both objectivity and co-mentions with ? = 0.6
From Figure 3, it is clear that both the objectiv-
ity of sources and the influence of co-mentions
contribute to the overall accuracy of FactChecker.
Full-fledged FactChecker performs better than
both variations. In most cases, FC-Objectivity per-
forms better than FC-CoMention.
5.3 Accuracy on Wikipedia Fact Candidates
Table 4, column ?WKP Politicians?, shows ac-
curacy on Wikipedia fact candidates, with a 0.9-
confidence Wilson score interval (Brown et al,
2001). For this dataset we again see FactChecker
outperforming the other methods under compari-
son. On this dataset, FactChecker has a accuracy
of 0.9 ? 0.07 and a 5% accuracy advantage over
the other methods. The second best performance
comes from the FC-Objectivity variation, with ac-
curacy of 0.88? 0.08.
5.4 Accuracy on General Knowledge Quiz
Table 4, column ?GK Quiz ?, shows accuracy on
the general knowledge quiz fact candidates. On
this dataset, FactChecker and its objectivity-only
variation (FC-objectivity) have the highest accu-
racy of 87%. Notice that this dataset was the only
one where we did not generate the alternative fact
candidates. Instead, we took the options of the
multiple choice questions as alternatives. Since
the quiz is meant to be taken by humans, the alter-
natives are often very close, plausible answers. Yet
even in this difficult setting, we see FactChecker
outperforming the baselines.
Sample fact candidates, with ranked alternatives
from all three datasets are shown in Table 5.
Figure 4: Effect of ? of FactChecker.
5.5 Parameter Sensitivity
We analyzed the effect of the selection of lambda
? (see Definition 5) on FactChecker?s perfor-
mance. The result of this analysis is shown in Fig-
ure 4. FactChecker is insensitive to this parame-
ter when ? is varied from 0.2 to 0.6. Therefore,
lambda can be robustly chosen within this range.
5.6 Discussion
Overall, from these results we make the follow-
ing observations: i) Majority vote is a competitive
baseline; ii) Iterative voting-based methods pro-
vide slight improvements on majority vote. This
is due to the fact that at the core of iterative vot-
ing is still the assumption that fact candidates
mentioned in many sources are more likely to be
true. Therefore, for both majority vote and it-
erative voting, when mention frequencies of var-
ious alternatives are the same, accuracy suffers.
Based on these observations, it is clear that truth-
finding solutions need to incorporate fine-grained
content-aware features outside of external votes.
FactChecker takes a step in this direction by incor-
porating the document-level feature of objectivity.
6 Related Work
There is a fairly small body of work on truth-
finding (Yin et al, 2007; Galland et al, 2010;
Pasternack and Roth, 2010; Li et al, 2011; Yin and
Tan, 2011; Zhao et al, 2012; Pasternack and Roth,
2013). The method underlying most truth-finding
algorithms is iterative transitive voting (Yin et al,
2007; Galland et al, 2010; Pasternack and Roth,
2010; Li et al, 2011). Fact candidates are ini-
tialized with a score. Trustworthiness of sources
is then computed from the believability of the fact
candidates they mention. In return, believability of
candidates is recomputed based on the trustworthi-
1016
Dataset Fact Candidate Alternatives & Ranking
WKP ?George W. Bush? lived in ?Midland,TX? 1.Midland,TX
2.Compton,CA
3.Washington D.C.
4.Venezuela*
KB ?Dirk Kuyt? plays for ?Liverpool? 1. Liverpool
2.Cardiff City*
3.Netherlands
4.Hungary*
Quiz ?Bifocals? invented by ?Benjamin Franklin? 1. Benjamin Franklin
2. Rudolf Diesel*
3.Thomas Alva Edison*
4.Alfred B. Nobel*
Table 5: Sample rankings by FactChecker, alternatives marked (*) are false. The ranking of the candidate
from the ?KB? dataset is not completely accurate.
ness of their sources. This process is repeated over
several iterations until convergence. (Yin et al,
2007) was the first to implement this idea, subse-
quent work improved upon iterative voting in sev-
eral directions. (Dong et al, 2009) incorporates
copying-detection; giving high trust to sources
that are independently authored. (Galland et al,
2010) approximates error rates of sources and fact
candidates. (Pasternack and Roth, 2010) intro-
duces prior knowledge in the form of linear pro-
gramming constraints in order to ensure that the
truth discovered is consistent with what is already
known. (Yin and Tan, 2011) introduces supervi-
sion by using ground truth facts so that sources
that disagree with the ground truth are penalized.
(Li et al, 2011) uses search engine APIs to gather
additional evidence for believability of fact can-
didates. WikiTrust (Adler and Alfaro, 2007) is
a content-aware but domain-specific method. It
computes trustworthiness of wiki authors based
on the revision history of the articles they have
authored. Motivated by interpretability of prob-
abilistic scores, two recent papers addressed the
truth-finding problem as a probabilistic inference
problem over the sources and the fact candidates
(Zhao et al, 2012; Pasternack and Roth, 2013).
Truth-finders based on textual entailment such as
TruthTeller (Lotan et al, 2013) determine if a sen-
tence states something or not. The focus is on un-
derstanding natural language, including the use of
negation. This is similar to the goal of fact ex-
traction (Banko et al, 2007; Carlson et al, 2010;
Fader et al, 2011; Nakashole et al, 2011; Del
Corro and Gemulla, 2013).
In a departure from prior work, our method
leverages language of sources in its believability
computation model. Furthermore, we introduced
a co-mention score which is designed to avoid po-
tential false boots among fact candidates. Addi-
tionally, we developed a method for generating al-
ternative fact candidates. Prior methods assume
these are readily available. Only (Li et al, 2011)
uses the Web to identify alternatives, however, this
is only done after manually specifying the fixed ar-
gument. In contrast, we introduced a method for
identifying the fixed argument based on relation
cardinalities learned from SVO statistics.
7 Conclusion
In this paper, we presented FactChecker, a
language-aware approach to truth-finding. In con-
trast to prior approaches, which rely on external
votes, FactChecker includes objectivity of sources
in its believability computation model.
FactChecker can be seen as a first step to-
wards language-aware truth-finding. Future di-
rections include using more sentence-level fea-
tures such the use of hedges, assertive verbs, and
factive verbs. These types of words fall into a
class of words used to express certainties, spec-
ulations or doubts ? these are important cues that
FactChecker can leverage.
Acknowledgments
We thank members of the NELL team at CMU
for their helpful comments. This research was
supported by DARPA under contract number
FA8750-13-2-0005.
1017
References
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-
niak, Z.G. Ives: DBpedia: A Nucleus for a Web of
Open Data. In Proceedings of the 6th International
Semantic Web Conference (ISWC), pages 722?735,
Busan, Korea, 2007.
B. T. Adler, L. de Alfaro: A content-driven reputa-
tion system for the wikipedia. In Proceedings of the
16th International Conference on World Wide Web
(WWW), pages 261-270, 2007.
M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, O. Etzioni: Open Information Extraction from
the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2670?2676, Hyderabad, India, 2007.
K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.
Taylor: Freebase: a Collaboratively Created Graph
Database for Structuring Human Knowledge. In
Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD),
pages, 1247-1250, Vancouver, BC, Canada, 2008.
L. D. Brown, T.T. Cai, A. Dasgupta: Interval Estima-
tion for a Binomial Proportion. Statistical Science
16: pages 101?133, 2001.
E. Cabrio, S. Villata: Combining Textual Entailment
and Argumentation Theory for Supporting Online
Debates Interaction. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pp. 208-212, 2012.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka,
T.M. Mitchell: Coupled Semi-supervised Learning
for Information Extraction. In Proceedings of the
Third International Conference on Web Search and
Web Data Mining (WSDM), pages 101?110, New
York, NY, USA, 2010.
L. Del Corro, R. Gemulla: ClausIE: clause-based
open information extraction. In Proceedings of the
22nd International Conference on World Wide Web
(WWW), pages 355-366. 2013.
X. Dong, L. Berti-Equille, D. Srivastava: Truth discov-
ery and copying detection in a dynamic world. In
Proceedings of the VLDB Endowment PVLDB, 2(1),
pp. 562-573, 2009.
A. Fader, S. Soderland, O. Etzioni: Identifying Rela-
tions for Open Information Extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1535?1545, Edinburgh, UK, 2011.
A. Galland, S. Abiteboul, A. Marian, P. Senellart: Cor-
roborating information from disagreeing views. In
Proceedings of the 3rd International Conference on
Web Search and Web Data Mining (WSDM), pages
131-140, 2010.
C. Havasi, R. Speer, J. Alonso: ConceptNet 3: a Flex-
ible, Multilingual Semantic Network for Common
Sense Knowledge. In Proceedings of the Recent Ad-
vances in Natural Language Processing (RANLP),
Borovets, Bulgaria, 2007.
J. Hoffart, F. Suchanek, K. Berberich, E. Lewis-
Kelham, G. de Melo, G. Weikum: YAGO2: Ex-
ploring and Querying World Knowledge in Time,
Space, Context, and Many Languages. In Proceed-
ings of the 20th International Conference on World
Wide Web (WWW), pages 229?232, Hyderabad, In-
dia. 2011.
R. Kaplan: Politics and the American Press: The Rise
of Objectivity, pages 1865-1920, New York, Cam-
bridge University Press, 2002.
X. Li and W. Meng, C. T. Yu: T-verifier: Verifying
truthfulness of fact statements. In Proceedings of
the International Conference on Data Engineering
(ICDE), pp. 63-74, 2011.
D. Lin, P. Pantel: DIRT: discovery of inference rules
from text. KDD 2001
B. Liu, M. Hu, J. Cheng: Opinion Observer: analyzing
and comparing opinions on the Web. InProceedings
of the 14th International Conference on World Wide
Web (WWW), pages 342351, 2005.
A. Lotan, A. Stern, I. Dagan TruthTeller: Annotating
Predicate Truth. In Proceedings of Human Language
Technologies: Conference of the North American
Chapter of the Association of Computational Lin-
guistics (HLT-NAACL), pp. 752-757, 2013.
N. Nakashole, M. Theobald, G. Weikum: Scalable
Knowledge Harvesting with High Precision and
High Recall. In Proceedings of the 4th International
Conference on Web Search and Web Data Mining
(WSDM), pages 227?326, Hong Kong, China, 2011.
N. Nakashole, T. Tylenda, G. Weikum: Fine-grained
Semantic Typing of Emerging Entities. In Proceed-
ings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp. 1488-
1497, 2013.
N. Nakashole, G. Weikum, F. Suchanek: PATTY:
A Taxonomy of Relational Patterns with Seman-
tic Types. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1135 -
1145, Jeju, South Korea, 2012.
V. Nastase, M. Strube, B. Boerschinger, C. Zirn, A.
Elghafari: WikiNet: A Very Large Scale Multi-
Lingual Concept Network. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation(LREC), Malta, 2010.
B. Pang, L. Lee: A Sentimental Education: Sentiment
Analysis Using Subjectivity Summarization Based
1018
on Minimum Cuts. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), 271-278, 2004.
J. Pasternack, D. Roth: Knowing What to Believe. In
Proceedings the International Conference on Com-
putational Linguistics (COLING), pp. 877-885, Bei-
jing, China. 2010.
J. Pasternack, D. Roth: Latent credibility analysis. In
Proceedings of the 22nd International Conference
on World Wide Web (WWW), pp. 1009-1020, 2013.
E. Riloff, J. Wiebe: Learning Learning extraction pat-
terns for subjective expressions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 105112,
2013.
M. Recasens, C. Danescu-Niculescu-Mizil, D. Juraf-
sky: Linguistic Models for Analyzing and Detecting
Biased Language. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pp. 1650-1659, 2013.
F. Niu, C. Zhang, C. Re, J. W. Shavlik: DeepDive:
Web-scale Knowledge-base Construction using Sta-
tistical Learning and Inference. In the VLDS Work-
shop, pages 25-28, 2012.
M. Schudson: Discovering the News: A Social History
of American Newspapers. New York: Basic Books.
1978.
F. M. Suchanek, M. Sozio, G. Weikum: SOFIE: A
Self-organizing Framework for Information Extrac-
tion. InProceedings of the 18th International Con-
ference on World Wide Web (WWW), pages 631?640,
Madrid, Spain, 2009.
P. P. Talukdar, D. T. Wijaya, T.M. Mitchell: Acquir-
ing temporal constraints between relations. In Pro-
ceeding of the 21st ACM International Conference
on Information and Knowledge Management, pages
992-1001, CIKM 2012.
P. D. Turney: Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 417424. 2002.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, M. Martin:
Learning subjective language. Computational Lin-
guistics, 30(3):277308. 2004.
X. Yin, J. Han, P. S. Yu: Truth Discovery with
Multiple Conflicting Information Providers on the
Web. In Proceedings of the International Confer-
ence on Knowledge Discovery in Databases (KDD)
, pages1048-1052. 2007.
X. Yin, W. Tan: Semi-supervised truth discover. In
Proceedings of the 19th International Conference on
World Wide Web (WWW), pp. 217-226, 2011.
H. Yu, V. Hatzivassiloglou: Towards Answering Opin-
ion Questions: Separating Facts from Opinions and
Identifying the Polarity of Opinion Sentences. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages. 129-
136, 2003.
B. Zhao, B. I. P. Rubinstein, J. Gemmell, J. Han: A
Bayesian approach to discovering truth from con-
flicting sources for data integration. In Proceedings
of the VLDB Endowment (PVLDB), 5(6):550-561,
2012.
1019
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1188?1198,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Joint Syntactic and Semantic Parsing with
Combinatory Categorial Grammar
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
We present an approach to training a joint
syntactic and semantic parser that com-
bines syntactic training information from
CCGbank with semantic training informa-
tion from a knowledge base via distant su-
pervision. The trained parser produces a
full syntactic parse of any sentence, while
simultaneously producing logical forms
for portions of the sentence that have a se-
mantic representation within the parser?s
predicate vocabulary. We demonstrate our
approach by training a parser whose se-
mantic representation contains 130 pred-
icates from the NELL ontology. A seman-
tic evaluation demonstrates that this parser
produces logical forms better than both
comparable prior work and a pipelined
syntax-then-semantics approach. A syn-
tactic evaluation on CCGbank demon-
strates that the parser?s dependency F-
score is within 2.5% of state-of-the-art.
1 Introduction
Integrating syntactic parsing with semantics has
long been a goal of natural language processing
and is expected to improve both syntactic and se-
mantic processing. For example, semantics could
help predict the differing prepositional phrase at-
tachments in ?I caught the butterfly with the net?
and ?I caught the butterfly with the spots.? A joint
analysis could also avoid propagating syntactic
parsing errors into semantic processing, thereby
improving performance.
We suggest that a large populated knowledge
base should play a key role in syntactic and se-
mantic parsing: in training the parser, in resolv-
ing syntactic ambiguities when the trained parser
is applied to new text, and in its output semantic
representation. Using semantic information from
the knowledge base at training and test time will
ideally improve the parser?s ability to solve diffi-
cult syntactic parsing problems, as in the exam-
ples above. A semantic representation tied to a
knowledge base allows for powerful inference op-
erations ? such as identifying the possible entity
referents of a noun phrase ? that cannot be per-
formed with shallower representations (e.g., frame
semantics (Baker et al, 1998) or a direct conver-
sion of syntax to logic (Bos, 2005)).
This paper presents an approach to training a
joint syntactic and semantic parser using a large
background knowledge base. Our parser produces
a full syntactic parse of every sentence, and fur-
thermore produces logical forms for portions of
the sentence that have a semantic representation
within the parser?s predicate vocabulary. For ex-
ample, given a phrase like ?my favorite town in
California,? our parser will assign a logical form
like ?x.CITY(x) ? LOCATEDIN(x,CALIFORNIA)
to the ?town in California? portion. Additionally,
the parser uses predicate and entity type informa-
tion during parsing to select a syntactic parse.
Our parser is trained by combining a syntactic
parsing task with a distantly-supervised relation
extraction task. Syntactic information is provided
by CCGbank, a conversion of the Penn Treebank
into the CCG formalism (Hockenmaier and Steed-
man, 2002a). Semantics are learned by training
the parser to extract knowledge base relation in-
stances from a corpus of unlabeled sentences, in
a distantly-supervised training regime. This ap-
proach uses the knowledge base to avoid expen-
sive manual labeling of individual sentence se-
mantics. By optimizing the parser to perform both
tasks simultaneously, we train a parser that pro-
duces accurate syntactic and semantic analyses.
We demonstrate our approach by training a joint
syntactic and semantic parser, which we call ASP.
ASP produces a full syntactic analysis of every
sentence while simultaneously producing logical
forms containing any of 61 category and 69 re-
1188
lation predicates from NELL. Experiments with
ASP demonstrate that jointly analyzing syntax
and semantics improves semantic parsing perfor-
mance over comparable prior work and a pipelined
syntax-then-semantics approach. ASP?s syntactic
parsing performance is within 2.5% of state-of-
the-art; however, we also find that incorporating
semantic information reduces syntactic parsing ac-
curacy by ? 0.5%.
2 Prior Work
This paper combines two lines of prior work:
broad coverage syntactic parsing with CCG and
semantic parsing.
Broad coverage syntactic parsing with CCG has
produced both resources and successful parsers.
These parsers are trained and evaluated using
CCGbank (Hockenmaier and Steedman, 2002a),
an automatic conversion of the Penn Treebank
into the CCG formalism. Several broad cover-
age parsers have been trained using this resource
(Hockenmaier and Steedman, 2002b; Hocken-
maier, 2003b). The parsing model in this paper is
loosely based on C&C (Clark and Curran, 2007b;
Clark and Curran, 2007a), a discriminative log-
linear model for statistical parsing. Some work
has also attempted to automatically derive logi-
cal meaning representations directly from syntac-
tic CCG parses (Bos, 2005; Lewis and Steedman,
2013). However, these approaches to semantics do
not ground the text to beliefs in a knowledge base.
Meanwhile, work on semantic parsing has fo-
cused on producing semantic parsers for answer-
ing simple natural language questions (Zelle and
Mooney, 1996; Ge and Mooney, 2005; Wong and
Mooney, 2006; Wong and Mooney, 2007; Lu et
al., 2008; Kate and Mooney, 2006; Zettlemoyer
and Collins, 2005; Kwiatkowski et al, 2011). This
line of work has typically used a corpus of sen-
tences with annotated logical forms to train the
parser. Recent work has relaxed the requisite su-
pervision conditions (Clarke et al, 2010; Liang et
al., 2011), but has still focused on simple ques-
tions. Finally, some work has looked at applying
semantic parsing to answer queries against large
knowledge bases, such as YAGO (Yahya et al,
2012) and Freebase (Cai and Yates, 2013b; Cai
and Yates, 2013a; Kwiatkowski et al, 2013; Be-
rant et al, 2013). Although this work considers
a larger number (thousands) of predicates than we
do, none of these systems are capable of parsing
open-domain text. Our approach is most closely
related to the distantly-supervised approach of Kr-
ishnamurthy and Mitchell (2012).
The parser presented in this paper can be viewed
as a combination of both a broad coverage syn-
tactic parser and a semantic parser trained using
distant supervision. Combining these two lines
of work has synergistic effects ? for example, our
parser is capable of semantically analyzing con-
junctions and relative clauses based on the syn-
tactic annotation of these categories in CCGbank.
This synergy gives our parser a richer semantic
representation than previous work, while simulta-
neously enabling broad coverage.
3 Parser Design
This section describes the Combinatory Categorial
Grammar (CCG) parsing model used by ASP. The
input to the parser is a part-of-speech tagged sen-
tence, and the output is a syntactic CCG parse tree,
along with zero or more logical forms representing
the semantics of subspans of the sentence. These
logical forms are constructed using category and
relation predicates from a broad coverage knowl-
edge base. The parser also outputs a collection of
dependency structures summarizing the sentence?s
predicate-argument structure. Figure 1 illustrates
ASP?s input/output specification.
3.1 Knowledge Base
The parser uses category and relation predicates
from a broad coverage knowledge base both to
construct logical forms and to parametrize the
parsing model. The knowledge base is assumed
to have two kinds of ontological structure: a gen-
eralization/subsumption hierarchy and argument
type constraints. This paper uses NELL?s ontology
(Carlson et al, 2010), which, for example, speci-
fies that the category ORGANIZATION is a general-
ization of SPORTSTEAM, and that both arguments
to the LOCATEDIN relation must have type LOCA-
TION. These type constraints are enforced during
parsing. Throughout this paper, predicate names
are shown in SMALLCAPS.
3.2 Syntax
ASP uses a lexicalized and semantically-
typed Combinatory Categorial Grammar
(CCG) (Steedman, 1996). Most gram-
matical information in CCG is encoded in
a lexicon ?, containing entries such as:
1189
area / NN
N
?x.LOCATION(x)
that / WDT
(N
1
\N
1
)/(S[dcl]\NP
1
)
2
?f.?g.?z.g(z) ? f(?y.y = z)
includes / VBZ
(S[dcl]\NP
1
)/NP
2
?f.?g.?x, y.g(x) ? f(y)
? LOCATEDIN(y, x)
beautiful / JJ
N
1
/N
1
?f.f
London / NNP
N
?x.M(x, ?london?, CITY)
N : ?x.M(x, ?london?, CITY)
(S[dcl]\NP
1
) :
?g.?x, y.g(x) ? M(y, ?london?, CITY) ? LOCATEDIN(y, x)
N
1
\N
1
: ?g.?z.?x, y.g(z) ? x = z ? M(y, ?london?, CITY) ? LOCATEDIN(y, x)
N : ?z.?x, y.LOCATION(z) ? x = z ? M(y, ?london?, CITY) ? LOCATEDIN(y, x)
Head Argument
word POS semantic type index syntactic category arg. num. word POS semantic type index
that WDT ? 1 (N
1
\N
1
)/(S\NP
1
)
2
1 area NN LOCATION 0
that WDT ? 1 (N
1
\N
1
)/(S\NP
1
)
2
2 includes VBZ LOCATEDIN
?1
2
includes VBZ LOCATEDIN
?1
2 (S[dcl]\NP
1
)/NP
2
1 area NN LOCATION 0
includes VBZ LOCATEDIN
?1
2 (S[dcl]\NP
1
)/NP
2
2 ENTITY:CITY NNP CITY 4
beautiful JJ ? 3 N
1
/N
1
1 ENTITY:CITY NNP CITY 4
Figure 1: Example input and output for ASP. Given a POS-tagged sentence, the parser produces a CCG
syntactic tree and logical form (top), and a collection of dependency structures (bottom).
person := N : PERSON : ?x.PERSON(x)
London := N : CITY : ?x.M(x, ?london?, CITY)
great := N
1
/N
1
: ? : ?f.?x.f(x)
bought :=
(S[dcl]\NP
1
)/NP
2
: ACQUIRED :
?f.?g.?x, y.f(y) ? g(x) ? ACQUIRED(x, y)
Each lexicon entry maps a word to a syntactic
category, semantic type, and logical form. CCG
has two kinds of syntactic categories: atomic and
functional. Atomic categories include N for noun
and S for sentence. Functional categories are
functions constructed recursively from atomic cat-
egories; these categories are denoted using slashes
to separate the category?s argument type from its
return type. The argument type appears on the
right side of the slash, and the return type on the
left. The direction of slash determines where the
argument must appear ? / means an argument on
the right, and \ means an argument on the left.
Syntactic categories in ASP are annotated with
two additional kinds of information. First, atomic
categories may have associated syntactic features
given in square brackets. These features are used
in CCGbank to distinguish variants of atomic syn-
tactic categories, e.g., S[dcl] denotes a declara-
tive sentence. Second, each category is anno-
tated with head and dependency information us-
ing subscripts. These subscripts are used to pop-
ulate predicate-argument dependencies (described
below), and to pass head information using unifi-
cation. For example, the head of the parse in Fig-
ure 1 is ?area,? due to the coindexing of the argu-
ment and return categories in the categoryN
1
\N
1
.
In addition to the syntactic category, each lexi-
con entry has a semantic type and a logical form.
The semantic type is a category or relation pred-
icate that concisely represents the word?s seman-
tics. The semantic type is used to enforce type
constraints during parsing and to include seman-
tics in the parser?s parametrization. The logi-
cal form gives the full semantics of the word in
lambda calculus. The parser also allows lexicon
entries with the semantic type ???, representing
words whose semantics cannot be expressed using
predicates from the ontology.
Parsing in CCG combines adjacent categories
using a small number of combinators, such as
function application:
X/Y : f Y : g =? X : f(g)
Y : g X\Y : f =? X : f(g)
The first rule states that the category X/Y can
be applied to the category Y , returning category
X , and that the logical form f is applied to g to
produce the logical form for the returned category.
Head words and semantic types are also propa-
gated to the returned category based on the anno-
tated head-passing markup.
3.3 Dependency Structures
Parsing a sentence produces a collection of depen-
dency structures which summarize the predicate-
argument structure of the sentence. Dependency
structures are 10-tuples, of the form:
< head word, head POS, head semantic type, head word
index, head word syntactic category, argument number, ar-
gument word, argument POS, argument semantic type, argu-
ment word index >
A dependency structure captures a relationship
between a head word and its argument. During
parsing, whenever a subscripted argument of a
syntactic category is filled, a dependency structure
1190
is created between the head of the applied func-
tion and its argument. For example, in Figure 1,
the first application fills argument 1 of ?beautiful?
with ?London,? creating a dependency structure.
3.4 Logical Forms
ASP performs a best-effort semantic analysis of
every parsed sentence, producing logical forms for
subspans of the sentence when possible. Logical
forms are designed so that the meaning of a sen-
tence is a universally- and existentially-quantified
conjunction of predicates with partially shared ar-
guments. This representation allows the parser to
produce semantic analyses for a reasonable subset
of language, including prepositions, verbs, nouns,
relative clauses, and conjunctions.
Figure 1 shows a representative sample of a log-
ical form produced by ASP. Generally, the parser
produces a lambda calculus statement with sev-
eral existentially-quantified variables ranging over
entities in the knowledge base. The only excep-
tion to this rule is conjunctions, which are rep-
resented using a scoped universal quantifier over
the conjoined predicates. Entity mentions appear
in logical forms via a special mention predicate,
M, instead of as database constants. For exam-
ple, ?London? appears as M(x, ?london?, CITY),
instead of as a constant like LONDON. The mean-
ing of this mention predicate is that x is an en-
tity which can be called ?london? and belongs to
the CITY category. This representation propagates
uncertainty about entity references into the logical
form where background knowledge can be used
for disambiguation. For example, ?London, Eng-
land? is assigned a logical form that disambiguates
?London? to a ?London? located in ?England.?
1
Lexicon entries without a semantic type are au-
tomatically assigned logical forms based on their
head passing markup. For example, in Figure 1,
the adjective ?beautiful? is assigned ?f.f . This
approach allows a logical form to be derived for
most sentences, but (somewhat counterintuitively)
can lose interesting logical forms from constituent
subspans. For example, the preposition ?in? has
syntactic category (N
1
\N
1
)/N
2
, which results in
the logical form ?f.?g.g. This logical form dis-
cards any information present in the argument f .
We avoid this problem by extracting a logical form
from every subtree of the CCG parse.
1
Specifically, ?x.?y.CITYLOCATEDINCOUNTRY(x, y) ?
M(x, ?london?, CITY) ? M(y, ?england?, COUNTRY)
3.5 Parametrization
The parser ? is trained as a discriminative linear
model of the following form:
?(`, d, t|s; ?) = ?
T
?(d, t, s)
Given a parameter vector ? and a sentence s, the
parser produces a score for a syntactic parse tree
t, a collection of dependency structures d and a
logical form `. The score depends on features of
the parse produced by the feature function ?.
? contains four classes of features: lexicon
features, combinator features, dependency fea-
tures and dependency distance features (Table 1).
These features are based on those of C&C (Clark
and Curran, 2007b), modified to include seman-
tic types. The features are designed to share syn-
tactic information about a word across its distinct
semantic realizations in order to transfer syntactic
information from CCGbank to semantic parsing.
The parser also includes a hard type-checking
constraint to ensure that logical forms are well-
typed. This constraint states that dependency
structures with a head semantic type only accept
arguments that (1) have a semantic type, and (2)
are within the domain/range of the head type.
4 Parameter Estimation
This section describes the training procedure for
ASP. Training is performed by minimizing a joint
objective function combining a syntactic parsing
task and a distantly-supervised relation extraction
task. The input training data includes:
1. A collection L of sentences s
i
with annotated
syntactic trees t
i
(e.g., CCGbank).
2. A corpus of sentences S (e.g., Wikipedia).
3. A knowledge base K (e.g., NELL), contain-
ing relation instances r(e
1
, e
2
) ? K.
4. A CCG lexicon ? (see Section 5.2).
Given these resources, the algorithm described
in this section produces parameters ? for a se-
mantic parser. Our parameter estimation proce-
dure constructs a joint objective functionO(?) that
decomposes into syntactic and semantic compo-
nents: O(?) = O
syn
(?) + O
sem
(?). The syntac-
tic component O
syn
is a standard syntactic pars-
ing objective constructed using the syntactic re-
source L. The semantic component O
sem
is a
distantly-supervised relation extraction task based
on the semantic constraint from Krishnamurthy
and Mitchell (2012). These components are de-
scribed in more detail in the following sections.
1191
Lexicon features: word, POS := X : t : `
Word/syntactic category word, X
POS/syntactic category POS, X
Word semantics word, X, t
Combinator features: X Y ? Z or X ? Z
Binary combinator indicator X Y ? Z
Unary combinator indicator X ? Z
Root syntactic category Z
Dependency Features: < h
w
, h
p
, h
t
, h
i
, s, n, a
w
, a
p
, a
t
, a
i
>
Predicate-Argument Indicator < h
w
,?, h
t
,?, s, n, a
w
,?, a
t
,? >
Word-Word Indicator < h
w
,?,?,?, s, n, a
w
,?,?,? >
Predicate-POS Indicator < h
w
,?, h
t
,?, s, n,?, a
p
,?,? >
Word-POS Indicator < h
w
,?,?,?, s, n,?, a
p
,?,? >
POS-Argument Indicator < ?, h
p
,?,?, s, n, a
w
,?, a
t
,? >
POS-Word Indicator < ?, h
p
,?,?, s, n, a
w
,?,?,? >
POS-POS Indicator < ?, h
p
,?,?, s, n,?, a
p
,?,? >
Dependency Distance Features:
Token distance h
w
, h
t
,?, s, n, d d = Number of tokens between h
i
and a
i
: 0, 1, 2 or more.
Token distance word backoff h
w
,?, s, n, d d = Number of tokens between h
i
and a
i
: 0, 1, 2 or more.
Token distance POS backoff ?,?, h
p
, s, n, d d = Number of tokens between h
i
and a
i
: 0, 1, 2 or more.
(The above distance features are repeated using the number of intervening verbs and punctuation marks.)
Table 1: Listing of parser feature templates used in the feature function ?. Each feature template repre-
sents a class of indicator features that fire during parsing when lexicon entries are used, combinators are
applied, or dependency structures are instantiated.
4.1 Syntactic Objective
The syntactic objective is the structured percep-
tron objective instantiated for a syntactic parsing
task. This objective encourages the parser to accu-
rately reproduce the syntactic parses in the anno-
tated corpus L = {(s
i
, t
i
)}
n
i=1
:
O
syn
(?) =
n
?
i=1
|max
?
`,
?
d,
?
t
?(
?
`,
?
d,
?
t|s
i
; ?)?
max
`
?
,d
?
?(`
?
, d
?
, t
i
|s
i
; ?)|
+
The first term in the above expression represents
the best CCG parse of the sentence s
i
according to
the current model. The second term is the best
parse of s
i
whose syntactic tree equals the true
syntactic tree t
i
. In the above equation | ? |
+
de-
notes the positive part of the expression. Minimiz-
ing this objective therefore finds parameters ? that
reproduce the annotated syntactic trees.
4.2 Semantic Objective
The semantic objective corresponds to a distantly-
supervised relation extraction task that constrains
the logical forms produced by the semantic parser.
Distant supervision is provided by the following
constraint: every relation instance r(e
1
, e
2
) ? K
must be expressed by at least one sentence in
S
(e
1
,e
2
)
, the set of sentences that mention both e
1
and e
2
(Hoffmann et al, 2011). If this constraint
is empirically true and sufficiently constrains the
parser?s logical forms, then optimizing the seman-
tic objective produces an accurate semantic parser.
A training example in the semantic objective
consists of the set of sentences mentioning a pair
of entities, S
(e
1
,e
2
)
= {s
1
, s
2
, ...}, paired with a
binary vector representing the set of relations that
the two entities participate in, y
(e
1
,e
2
)
. The distant
supervision constraint ? forces the logical forms
predicted for the sentences to entail the relations
in y
(e
1
,e
2
)
. ? is a deterministic OR constraint that
checks whether each logical form entails the re-
lation instance r(e
1
, e
2
), deterministically setting
y
r
= 1 if any logical form entails the instance and
y
r
= 0 otherwise.
Let (`,d, t) represent a collection of seman-
tic parses for the sentences S = S
(e
1
,e
2
)
. Let
?(`,d, t|S; ?) =
?
|S|
i=1
?(`
i
, d
i
, t
i
|s
i
; ?) represent
the total weight assigned by the parser to a collec-
tion of parses for the sentences S. For the pair of
entities (e
1
, e
2
), the semantic objective is:
O
sem
(?) = |max
?
`,
?
d,
?
t
?(
?
`,
?
d,
?
t|S; ?)? max
`
?
,d
?
,t
?
(
?(y
(e
1
,e
2
)
, `
?
,d
?
, t
?
) + ?(`
?
,d
?
, t
?
|S; ?)
)
|
+
4.3 Optimization
Training minimizes the joint objective using the
structured perceptron algorithm, which can be
viewed as the stochastic subgradient method
(Ratliff et al, 2006) applied to the objective
O(?). We initialize the parameters to zero, i.e.,
?
0
= 0. On each iteration, we sample either a
syntactic example (s
i
, t
i
) or a semantic example
(S
(e
1
,e
2
)
, y
(e
1
,e
2
)
). If a syntactic example is sam-
pled, we apply the following parameter update:
?
`,
?
d,
?
t ? arg max
`,d,t
?(`, d, t|s
i
; ?
t
)
`
?
, d
?
? arg max
`,d
?(`, d, t
i
|s
i
; ?
t
)
?
t+1
? ?
t
+ ?(d
?
, t
i
, s
i
)? ?(
?
d,
?
t, s
i
)
This update moves the parameters toward the fea-
tures of the best parse with the correct syntactic
derivation, ?(d
?
, t
i
, s
i
). If a semantic example is
1192
Labeled Dependencies Unlabeled Dependencies
P R F P R F Coverage
ASP 85.58 85.31 85.44 91.75 91.46 91.60 99.63
ASP-SYN 86.06 85.84 85.95 92.13 91.89 92.01 99.63
C&C (Clark and Curran, 2007b) 88.34 86.96 87.64 93.74 92.28 93.00 99.63
(Hockenmaier, 2003a) 84.3 84.6 84.4 91.8 92.2 92.0 99.83
Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using
precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies.
sampled, we instead apply the following update:
?
`,
?
d,
?
t? arg max
`,d,t
?(`,d, t|S
(e
1
,e
2
)
; ?
t
)
`
?
,d
?
, t
?
? arg max
`,d,t
?(`,d, t|S
(e
1
,e
2
)
; ?
t
)
+ ?(y
(e
1
,e
2
)
, `,d, t)
?
t+1
? ?
t
+ ?(d
?
, t
?
,S
(e
1
,e
2
)
)
? ?(
?
d,
?
t,S
(e
1
,e
2
)
)
This update moves the parameters toward the fea-
tures of the best set of parses that satisfy the distant
supervision constraint. Training outputs the aver-
age of each iteration?s parameters,
?
? =
1
n
?
n
t=1
?
t
.
In practice, we train the parser by performing a
single pass over the examples in the data set.
All of the maximizations above can be per-
formed exactly using a CKY-style chart parsing
algorithm, except for the last one. This maxi-
mization is intractable due to the coupling between
logical forms in ` caused by enforcing the dis-
tant supervision constraint. We approximate this
maximization in two steps. First, we perform a
beam search to produce a list of candidate parses
for each sentence s ? S
(e
1
,e
2
)
. We then extract
relation instances from each parse and apply the
greedy inference algorithm from Hoffmann et al,
(2011) to identify the best set of parses that satisfy
the distant supervision constraint. The procedure
skips any examples with sentences that cannot be
parsed (due to beam search failures) or where the
distant supervision constraint cannot be satisfied.
5 Experiments
The experiments below evaluate ASP?s syntactic
and semantic parsing ability. The parser is trained
on CCGbank and a corpus of Wikipedia sentences,
using NELL?s predicate vocabulary. The syntactic
analyses of the trained parser are evaluated against
CCGbank, and its logical forms are evaluated on
an information extraction task and against an an-
notated test set of Wikipedia sentences.
5.1 Data Sets
The data sets for the evaluation consist of CCG-
bank, a corpus of dependency-parsed Wikipedia
sentences, and a logical knowledge base derived
from NELL and Freebase. Sections 02-21 of
CCGbank were used for training, Section 00 for
validation, and Section 23 for the final results. The
knowledge base?s predicate vocabulary is taken
from NELL, and its instances are taken from Free-
base using a manually-constructed mapping be-
tween Freebase and NELL. Using Freebase rela-
tion instances produces cleaner training data than
NELL?s automatically-extracted instances.
Using the relation instances and Wikipedia sen-
tences, we constructed a data set for distantly-
supervised relation extraction. We identified men-
tions of entities in each sentence using simple
string matching, then aggregated these sentences
by entity pair. 20% of the entity pairs were set
aside for validation. In the remaining training
data, we downsampled entity pairs that did not
participate in at least one relation. We further
eliminated sentences containing more than 30 to-
kens. The resulting training corpus contains 25k
entity pairs (half of which participate in a relation),
41k sentences, and 71 distinct relation predicates.
5.2 Grammar Construction
The grammar for ASP contains the annotated lex-
icon entries and grammar rules in Sections 02-21
of CCGbank, and additional semantic entries pro-
duced using a set of dependency parse heuristics.
The lexicon ? contains all words that occur at
least 20 times in CCGbank. Rare words are re-
placed by their part of speech. The head pass-
ing and dependency markup was generated using
the rules of the C&C parser (Clark and Curran,
2007b). These lexicon entries are also annotated
with logical forms capturing their head passing re-
lationship. For example, the adjective category
N
1
/N
1
is annotated with the logical form ?f.f .
These entries are all assigned semantic type ?.
We augment this lexicon with additional entries
1193
Sentence Extracted Logical Form
St. John, a Mexican-American born in San Francisco, Califor-
nia, her family comes from Zacatecas, Mexico.
?x.?y, z.M(x, ?st. john?) ? M(y, ?san francisco?) ?
PERSONBORNINLOCATION(x, y) ?
CITYLOCATEDINSTATE(y, z) ? M(z, ?california?)
The capital and largest city of Laos is Vientiane and other major
cities include Luang Prabang, Savannakhet and Pakse.
?x, y.M(x, ?vientiane?) ? CITY(x) ?
CITYCAPITALOFCOUNTRY(x, y) ? M(y, ?laos?)
Gellar next played a lead role in James Toback ?s critically
unsuccessful independent ?Harvard Man? (2001), where she
played the daughter of a mobster.
?x.?y.M(y, ?james toback?) ?
DIRECTORDIRECTEDMOVIE(y, x) ?
M(x, ?harvard man?)
Figure 2: Logical forms produced by ASP for sentences in the information extraction corpus. Each
logical form is extracted from the underlined sentence portion.
ASP
PIPELINE
K&M-2012
0 300 600 900
0
0.2
0.4
0.6
0.8
1.0
Figure 3: Logical form precision as a function of
the expected number of correct extracted logical
forms. ASP extracts more correct logical forms
because it jointly analyzes syntax and semantics.
mapping words to logical forms with NELL pred-
icates. These entries are instantiated using a set
of dependency parse patterns, listed in an online
appendix.
2
These patterns are applied to the train-
ing corpus, heuristically identifying verbs, prepo-
sitions, and possessives that express relations, and
nouns that express categories. The patterns also
include special cases for forms of ?to be.? This
process generates ?4000 entries (not counting en-
tity names), representing 69 relations and 61 cate-
gories from NELL. Section 3.2 shows several lex-
icon entries generated by this process.
The parser?s combinators include function ap-
plication, composition, and crossed composition,
as well as several binary and unary type-changing
rules that occur in CCGbank. All combinators
were restricted to only apply to categories that
combine in Sections 02-21. Finally, the grammar
includes a number of heuristically-instantiated bi-
nary rules of the form , N ? N\N that instanti-
ate a relation between adjacent nouns. These rules
capture appositives and some other constructions.
5.3 Supertagging
Parsing in practice can be slow because the
parser?s lexicalized grammar permits a large num-
ber of parses for a sentence. We improve parser
performance by performing supertagging (Banga-
2
http://rtw.ml.cmu.edu/acl2014_asp/
lore and Joshi, 1999; Clark and Curran, 2004).
We trained a logistic regression classifier to pre-
dict the syntactic category of each token in a sen-
tence from features of the surrounding tokens and
POS tags. Subsequent parsing is restricted to only
consider categories whose probability is within a
factor of ? of the highest-scoring category. The
parser uses a backoff strategy, first attempting to
parse with the supertags from ? = 0.01, backing
off to ? = 0.001 if the initial parsing attempt fails.
5.4 Syntactic Evaluation
The syntactic evaluation measures ASP?s ability
to reproduce the predicate-argument dependencies
in CCGbank. As in previous work, our evalu-
ation uses labeled and unlabeled dependencies.
Labeled dependencies are dependency structures
with both words and semantic types removed,
leaving two word indexes, a syntactic category,
and an argument number. Unlabeled dependen-
cies further eliminate the syntactic category and
argument number, leaving a pair of word indexes.
Performance is measured using precision, recall,
and F-measure against the annotated dependency
structures in CCGbank. Precision is the fraction
of predicted dependencies which are in CCGbank,
recall is the fraction of CCGbank dependencies
produced by the parser, and F-measure is the har-
monic mean of precision and recall.
For comparison, we also trained a syntactic ver-
sion of our parser, ASP-SYN, using only the CCG-
bank lexicon and grammar. Comparing against
this parser lets us measure the effect of the rela-
tion extraction task on syntactic parsing.
Table 2 shows the results of our evaluation.
For comparison, we include results for two ex-
isting syntactic CCG parsers: C&C, the current
state-of-the-art CCG parser (Clark and Curran,
2007b), and the next best system (Hockenmaier,
2003a). Both ASP and ASP-SYN perform rea-
sonably well, within 2.5% of the performance of
C&C at the same coverage level. However, ASP-
1194
Logical Form Extraction Extraction
Accuracy Precision Recall
ASP 0.28 0.90 0.32
K&M-2012 0.14 1.00 0.06
PIPELINE 0.2 0.63 0.17
Table 3: Logical form accuracy and extraction pre-
cision/recall on the annotated test set. The high
extraction recall for ASP shows that it produces
more complete logical forms than either baseline.
SYN outperforms ASP by around 0.5%, suggesting
that ASP?s additional semantic knowledge slightly
hurts syntactic parsing performance. This perfor-
mance loss appears to be largely due to poor en-
tity mention detection, as we found that not us-
ing entity mention lexicon entries at test time im-
proves ASP?s labeled and unlabeled F-scores by
0.3% on Section 00. The knowledge base contains
many infrequently-mentioned entities with com-
mon names; these entities contribute incorrect se-
mantic type information that confuses the parser.
5.5 Semantic Evaluation
We performed two semantic evaluations to bet-
ter understand ASP?s ability to construct logical
forms. The first evaluation emphasizes precision
over recall, and the second evaluation accurately
measures recall using a manually labeled test set.
5.5.1 Baselines
For comparison, we also trained two base-
line models. The first baseline, PIPELINE, is
a pipelined syntax-then-semantics approach de-
signed to mimic Boxer (Bos, 2005). This base-
line first syntactically parses each sentence using
ASP-SYN, then produces a semantic analysis by
assigning a logical form to each word. We train
this baseline using the semantic objective (Section
4.2) while holding fixed the syntactic parse of each
sentence. Note that, unlike Boxer, this baseline
learns which logical form to assign each word, and
its logical forms contain NELL predicates.
The second baseline, K&M-2012, is the ap-
proach of Krishnamurthy and Mitchell (2012),
representing the state-of-the-art in distantly-
supervised semantic parsing. This approach trains
a semantic parser by combining distant seman-
tic supervision with syntactic supervision from
dependency parses. The best performing vari-
ant of this system also uses dependency parses
at test time to constrain the interpretation of
test sentences ? hence, this system also uses a
pipelined syntax-then-semantics approach. To im-
prove comparability, we reimplemented this ap-
proach using our parsing model, which has richer
features than were used in their paper.
5.5.2 Information Extraction Evaluation
The information extraction evaluation uses each
system to extract logical forms from a large cor-
pus of sentences, then measures the fraction of
extracted logical forms that are correct. The test
set consists of 8.5k sentences sampled from the
held-out Wikipedia sentences. Each system was
run on this data set, extracting all logical forms
from each sentence that entailed at least one cat-
egory or relation instance. We ranked these ex-
tractions using the parser?s inside chart score, then
manually annotated a sample of 250 logical forms
from each system for correctness. Logical forms
were marked correct if all category and relation
instances entailed by the logical form were ex-
pressed by the sentence. Note that a correct logical
form need not entail all of the relations expressed
by the sentence, reflecting an emphasis on preci-
sion over recall. Figure 2 shows some example
logical forms produced by ASP in the evaluation.
The annotated sample of logical forms allows
us to estimate precision for each system as a func-
tion of the number of correct extractions (Figure
3). The number of correct extractions is directly
proportional to recall, and was estimated from the
total number of extractions and precision at each
rank in the sample. All three systems initially
have high precision, implying that their extracted
logical forms express facts found in the sentence.
However, ASP produces 3 times more correct log-
ical forms than either baseline because it jointly
analyzes syntax and semantics. The baselines suf-
fer from reduced recall because they depend on re-
ceiving an accurate syntactic parse as input; syn-
tactic parsing errors cause these systems to fail.
Examining the incorrect logical forms produced
by ASP reveals that incorrect mention detection is
by far the most common source of mistakes. Ap-
proximately 50% of errors are caused by marking
common nouns as entity mentions (e.g., marking
?coin? as a COMPANY). These errors occur be-
cause the knowledge base contains many infre-
quently mentioned entities with relatively com-
mon names. Another 30% of errors are caused by
assigning an incorrect type to a common proper
noun (e.g, marking ?Bolivia? as a CITY). This
analysis suggests that performing entity linking
before parsing could significantly reduce errors.
1195
Sentence: De Niro and Joe Pesci in ?Goodfellas? offered a virtuoso display of the director?s bravura cinematic
technique and reestablished, enhanced, and consolidated his reputation.
Annotation:
LF: ?x.?p ? {?d.M(d, ?de niro?), ?j.M(j, ?joe pesci?)}?y.p(x) ? STARREDINMOVIE(x, y) ? M(y, ?goodfellas?)
Instances: STARREDINMOVIE(de niro, goodfellas), STARREDINMOVIE(joe pesci, goodfellas)
Prediction:
LF: ?x.?p ? {?d.M(d, ?de niro?), ?j.M(j, ?joe pesci?)}?y.p(x) ? STARREDINMOVIE(x, y) ? M(y, ?goodfellas?)
Instances: STARREDINMOVIE(de niro, goodfellas), STARREDINMOVIE(joe pesci, goodfellas)
Logical form accuracy: 1 / 1 Extraction Precision: 2 / 2 Extraction Recall: 2 / 2
Sentence: In addition to the University of Illinois, Champaign is also home to Parkland College.
Annotation:
LF: ?c, p.M(c, ?champaign?) ? CITY(c) ? M(p, ?parkland college?) ? UNIVERSITYINCITY(p, c)
Instances: CITY(champaign), UNIVERSITYINCITY(parkland college, champaign)
Prediction:
LF 1: ?x.?yM(y, ?illinois?) ? M(x, ?university?) ? CITYLOCATEDINSTATE(x, y)
LF 2: ?c, p.M(c, ?champaign?) ? CITY(c) ? M(p, ?parkland college?) ? UNIVERSITYINCITY(p, c)
Instances: CITY(champaign), UNIVERSITYINCITY(parkland college, champaign),
CITYLOCATEDINSTATE(university, illinois)
Logical form accuracy: 1 / 1 Extraction Precision: 2 / 3 Extraction Recall: 2 / 2
Figure 4: Two test examples with ASP?s predictions and error calculations. The annotated logical forms
are for the italicized sentence spans, while the extracted logical forms are for the underlined spans.
5.5.3 Annotated Sentence Evaluation
A limitation of the previous evaluation is that it
does not measure the completeness of predicted
logical forms, nor estimate what portion of sen-
tences are left unanalyzed. We conducted a second
evaluation to measure these quantities.
The data for this evaluation consists of sen-
tences annotated with logical forms for subspans.
We manually annotated Wikipedia sentences from
the held-out set with logical forms for the largest
subspans for which a logical form existed. To
avoid trivial cases, we only annotated logical
forms containing at least one category or relation
predicate and at least one mention. We also chose
not to annotate mentions of entities that are not in
the knowledge base, as no system would be able
to correctly identify them. The corpus contains 97
sentences with 100 annotated logical forms.
We measured performance using two met-
rics: logical form accuracy, and extraction preci-
sion/recall. Logical form accuracy examines the
predicted logical form for the smallest subspan of
the sentence containing the annotated span, and
marks this prediction correct if it exactly matches
the annotation. A limitation of this metric is that it
does not assign partial credit to logical forms that
are close to, but do not exactly match, the anno-
tation. The extraction metric assigns partial credit
by computing the precision and recall of the cat-
egory and relation instances entailed by the pre-
dicted logical form, using those entailed by the an-
notated logical form as the gold standard. Figure
4 shows the computation of both error metrics on
two examples from the test corpus.
Table 3 shows the results of the annotated sen-
tence evaluation. ASP outperforms both baselines
in logical form accuracy and extraction recall, sug-
gesting that it produces more complete analyses
than either baseline. The extraction precision of
90% suggests that ASP rarely extracts incorrect in-
formation. Precision is higher in this evaluation
because every sentence in the data set has at least
one correct extraction.
6 Discussion
We present an approach to training a joint syntac-
tic and semantic parser. Our parser ASP produces
a full syntactic parse of any sentence, while simul-
taneously producing logical forms for sentence
spans that have a semantic representation within
its predicate vocabulary. The parser is trained
by jointly optimizing performance on a syntac-
tic parsing task and a distantly-supervised rela-
tion extraction task. Experimental results demon-
strate that jointly analyzing syntax and semantics
triples the number of extracted logical forms over
approaches that first analyze syntax, then seman-
tics. However, we also find that incorporating se-
mantics slightly reduces syntactic parsing perfor-
mance. Poor entity mention detection is a major
source of error in both cases, suggesting that fu-
ture work should consider integrating entity link-
ing with joint syntactic and semantic parsing.
Acknowledgments
This work was supported in part by DARPA under
award FA8750-13-2-0005. We additionally thank
Jamie Callan and Chris R?e?s Hazy group for col-
lecting and processing the Wikipedia corpus.
1196
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th International Conference on Com-
putational Linguistics - Volume 1.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing.
Johan Bos. 2005. Towards wide-coverage seman-
tic interpretation. In In Proceedings of Sixth In-
ternational Workshop on Computational Semantics
IWCS-6.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale Semantic Parsing via Schema Matching and
Lexicon Extension. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Qingqing Cai and Alexander Yates. 2013b. Semantic
Parsing Freebase: Towards Open-domain Semantic
Parsing. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intel-
ligence.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Con-
ference on Computational Linguistics.
Stephen Clark and James R. Curran. 2007a. Per-
ceptron training for a wide-coverage lexicalized-
grammar parser. In Proceedings of the Workshop on
Deep Linguistic Processing.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the Ninth Conference on
Computational Natural Language Learning.
Julia Hockenmaier and Mark Steedman. 2002a.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proceedings of Third Interna-
tional Conference on Language Resources and Eval-
uation.
Julia Hockenmaier and Mark Steedman. 2002b. Gen-
erative models for statistical parsing with combina-
tory categorial grammar. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics.
Julia Hockenmaier. 2003a. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Julia Hockenmaier. 2003b. Parsing with generative
models of predicate-argument structure. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In The 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceedings
of the Conference.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179?192.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Association for Compu-
tational Linguistics, Portland, Oregon. Association
for Computational Linguistics.
1197
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. (online) subgradient methods
for structured prediction. Artificial Intelligence and
Statistics.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press, Cambridge, MA, USA.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for
the web of data. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the thirteenth na-
tional conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: struc-
tured classification with probabilistic categorial
grammars. In UAI ?05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence.
1198
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 114?123,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Selecting Corpus-Semantic Models for Neurolinguistic Decoding
Brian Murphy
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
brianmurphy@cmu.edu
Partha Talukdar
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
ppt@cs.cmu.edu
Tom Mitchell
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, USA
tom.mitchell@cs.cmu.edu
Abstract
Neurosemantics aims to learn the mapping
between concepts and the neural activity
which they elicit during neuroimaging ex-
periments. Different approaches have been
used to represent individual concepts, but
current state-of-the-art techniques require
extensive manual intervention to scale to
arbitrary words and domains. To over-
come this challenge, we initiate a system-
atic comparison of automatically-derived
corpus representations, based on various
types of textual co-occurrence. We find
that dependency parse-based features are
the most effective, achieving accuracies
similar to the leading semi-manual ap-
proaches and higher than any published
for a corpus-based model. We also find
that simple word features enriched with
directional information provide a close-to-
optimal solution at much lower computa-
tional cost.
1 Introduction
The cognitive plausibility of computational
models of word meaning has typically been
tested using behavioural benchmarks, such as
identification of synonyms among close asso-
ciates (the TOEFL task for language learners,
see e.g. Landauer and Dumais, 1997); emulating
elicited judgments of pairwise similarity (such as
Rubenstein and Goodenough, 1965); judgments
of category membership (e.g. Battig and Mon-
tague, 1969); and word priming effects (Lund
and Burgess, 1996). Mitchell et al (2008) in-
troduced a new task in neurosemantic decoding
? using models of semantics to learn the map-
ping between concepts and the neural activity
which they elicit during neuroimaging experi-
ments. This was achieved with a linear model
which used training data to find neural basis im-
ages that correspond to the assumed semantic
dimensions (for instance, one such basis image
might be the activity of the brain for words rep-
resenting animate concepts), and subsequently
used these general patterns and known seman-
tic dimensions to infer the fMRI activity that
should be elicited by an unseen stimulus con-
cept. Follow-on work has experimented with
other neuroimaging modalities (Murphy et al,
2009), and with a range of semantic models in-
cluding elicited property norms (Chang et al,
2011), corpus derived models (Devereux and
Kelly, 2010; Pereira et al, 2011) and structured
ontologies (Jelodar et al, 2010).
The current state-of-the-art performance on
this task is achieved using models that are hand-
tailored in some respect, whether using manual
annotation tasks (Palatucci et al, 2009), use of
a domain-appropriate curated corpus (Pereira
et al, 2011), or selection of particular collocates
to suit the concepts to be described (Mitchell
et al, 2008). While these approaches are clearly
very successful, it is questionable whether they
are a general solution to describe the vari-
ous parts-of-speech and semantic domains that
make up a speaker?s vocabulary. The Mitchell
et al (2008) 25-verb model would probably have
to be extended to describe the lexicon at large,
and it is unclear whether such a compact model
could be maintained. While Wikipedia (Pereira
et al, 2011) has very broad and increasing cov-
114
erage, it is possible that it will remain inad-
equate for specialist vocabularies, or for less-
studied languages. And while the method used
by Palatucci et al (2009) distributes the anno-
tation task efficiently by crowd-sourcing, it still
requires that appropriate questions are compiled
by researchers, a task that is both difficult to
perform in a systematic way, and which may not
generalize to more abstract concepts.
In this paper we examine a representative set
of corpus-derived models of meaning, that re-
quire no manual intervention, and are applicable
to any syntactic and semantic domain. We con-
centrate on which types of basic corpus pattern
perform well on the neurosemantic decoding
task: LSA-style word-region co-occurrences,
and various HAL-style word-collocate features
including raw tokens, POS tags, and a full de-
pendency parse. Otherwise a common feature
extraction and preprocessing pipeline is used: a
co-occurrence frequency cutoff, application of a
frequency normalization weighting, and dimen-
sionality reduction with SVD.
The following section describes how the brain
activity data was gathered and processed; the
construction of several corpus-derived models
of meaning; and the regression-based meth-
ods used to predict one from the other, evalu-
ated with a brain-image matching task (Mitchell
et al, 2008). In section 3 we report the re-
sults, and in the Conclusion we discuss both the
practical implications, and what this works sug-
gests for the cognitive plausibility of distribu-
tional models of meaning.
2 Methods
2.1 Brain activity features
The dataset used here is that described in detail
in (Mitchell et al, 2008) and released publicly1
in conjunction with the NAACL 2010 Work-
shop on Computational Neurolinguistics (Mur-
phy et al, 2010). Functional MRI (fMRI) data
was collected from 9 participants while they per-
formed a property generation task. The stimuli
were line-drawings, accompanied by their text
1http://www.cs.cmu.edu/afs/cs/project/theo-
73/www/science2008/data.html
label, of everyday concrete concepts, with 5 ex-
emplars of each of 12 semantic classes (mam-
mals, body parts, buildings, building parts,
clothes, furniture, insects, kitchen utensils, mis-
cellaneous functional artifacts, work tools, veg-
etables, and vehicles). Stimuli remained on
screen for three seconds, and each was each pre-
sented six times, in random order, to give a total
of 360 image presentations in the session.
The fMRI images were recorded with 3.0T
scanner at 1 second intervals, with a spatial reso-
lution of 3x3x6mm. The resulting data was pre-
processed with the SPM package (Friston et al,
2007); the blood-oxygen-level response was ap-
proximated by taking a boxcar average over a
sequence of brain images in each trial; percent
signal change was calculated relative to rest pe-
riods, and the data from each of the six repeti-
tions of each stimulus were averaged to yield a
single brain image for each concept. Finally, a
grey-matter anatomical mask was used to select
only those voxels (three-dimensional pixels) that
overlap with cortex, yielding approximately 20
thousand features per participant.
2.2 Models of semantics
Our objective is to compare current semantic
representations that get state-of-the-art perfor-
mance on the neuro-semantics task with repre-
sentative distributional models of semantics that
can be derived from arbitrary corpora, using
varying degrees of linguistic preprocessing. A
series of candidate models were selected to rep-
resent the variety of ways in which basic textual
features can be extracted and represented, in-
cluding token co-occurrence in a small local win-
dow, dependency parses of whole sentences, and
document co-occurrence, among others. Other
parameters were kept fixed in a way that the
literature suggests would be neutral to the var-
ious models, and so allow a fair comparison
among them (Sahlgren, 2006; Bullinaria and
Levy, 2007; Turney and Pantel, 2010).
All textual statistics were gathered from a set
of 50m English-language web-page documents
consisting of 16 billion words. Where a fixed
text window was used, we chose an extent of
?4 lower-case tokens either side of the target
115
word of interest, which is in the mid-range of
optimal values found by various authors (Lund
and Burgess, 1996; Rapp, 2003; Sahlgren, 2006).
Positive pointwise-mutual-information (1,2) was
used as an association measure to normalize
the observed co-occurrence frequency p(w, f) for
the varying frequency of the target word p(w)
and its features p(f). PPMI up-weights co-
occurrences between rare words, yielding posi-
tive values for collocations that are more com-
mon than would be expected by chance (i.e. if
word distributions were independent), and dis-
cards negative values that represent patterns of
co-occurrences that are rarer than one would ex-
pect by chance. It has been shown to perform
well generally, with both word- and document-
level statistics, in raw and dimensionality re-
duced forms (Bullinaria and Levy, 2007; Turney
and Pantel, 2010).2
PPMIwf =
{
PMIwf if PMIwf > 0
0 otherwise
(1)
PMIwf = log
(
p(w, f)
p(w)p(f)
)
(2)
A frequency threshold is commonly applied
for three reasons: low-frequency co-occurrence
counts are more noisy; PMI is positively bi-
ased towards hapax co-occurrences; and due
to Zipfian distributions a cut-off dramatically
reduces the amount of data to be processed.
Many authors use a threshold of approximately
50-100 occurrences for word-collocate models
(Lund and Burgess, 1996; Lin, 1998; Rapp,
2003). Since Bullinaria and Levy (2007) find
improving performance with models using pro-
gressively lower cutoffs we explored two cut-offs
of 20 and 50 which equate to low co-occurrences
thresholds of 0.00125 or 0.003125 per million re-
spectively; for the word-region model we chose
a threshold of 2 occurrences of a target term in
a document, to keep the input features to a rea-
sonable dimensionality (Bradford, 2008).
After applying these operations to the input
data from each model, the resulting dimension-
2Preliminary analyses confirmed that PPMI per-
formed as well or better than alternatives including log-
likelihood, TF-IDF, and log-entropy.
ality ranged widely, from about 500 thousand,
to tens of millions. A singular value decompo-
sition (SVD) was applied to identify the 1000
dimensions within each model with the great-
est explanatory power, which also has the ef-
fect of combining similar dimensions (such as
synonyms, inflectional variants, topically simi-
lar documents) into common components, and
discarding more noisy dimensions in the data.
Again there is variation in the number of di-
mension that authors use: here we experiment
with 300 and 1000. For decomposition we used
a sparse SVD method, the Implicitly Restarted
Arnoldi Method (Lehoucq et al, 1998; Jones
et al, 2001), which was coherent with the PPMI
normalization used, since a zero value repre-
sented both negative target-feature associations,
and those that were not observed or fell below
the frequency cut-off. We also streamlined the
task by reducing the input data C (of n target
words by m co-occurrence features) to a square
matrix CCT of size n ? n, taking advantage of
the equality of their left singular vectors U. For
SVD to generalize well over the many input fea-
tures, it is also important to have more training
cases that the small set of 60 concrete nouns
used in our evaluation task. Consequently we
gathered all statistics over a set of the 40,000
most frequent word-forms found in the Ameri-
can National Corpus (Nancy Ide and Keith Su-
derman, 2006), which should approximate the
scale and composition of the vocabulary of a
university-educated speaker of English (Nation
and Waring, 1997), and over 95% of tokens typ-
ically encountered in English.
2.2.1 Hand-tailored benchmarks
The state-of-the-art models on this brain ac-
tivity prediction task are both hand-tailored.
Mitchell et al (2008) used a model of seman-
tics based on co-occurrence in the Google 1T 5-
gram corpus of English (Brants and Franz, 2006)
with a small set of 25 Verbs chosen to rep-
resent everyday sensory-motor interaction with
concrete objects, such as see, move, listen. We
recreated this using our current parameters (web
document corpus, co-occurrence frequency cut-
off, PPMI normalization). The second hand-
116
tailored dataset we used was a set of Elicited
Properties inspired by the 20 Questions game,
and gathered using Mechanical Turk (Palatucci
et al, 2009; Palatucci, 2011). Multiple infor-
mants were asked to answer one or more of 218
questions ?related to size, shape, surface prop-
erties, and typical usage? such as Do you see
it daily?, Is it wild?, Is it man-made? with a
scalar response ranging from 1 to 5. The re-
sulting responses were then averaged over infor-
mants, and then the values of each question were
grouped into 5 bins, giving all dimensions simi-
lar mean and variance.
2.2.2 Word-Region Model
Latent Semantic Analysis (Deerwester et al,
1990; Landauer and Dumais, 1997), and its
probabilistic cousins (Blei et al, 2003; Grif-
fiths et al, 2007), express the meaning of a
word as a distribution of co-occurrence across
a set of documents, or other text-regions such
as paragraphs. This word-region matrix in-
stantiates the assumption that words that share
a topical domain (such as medicine, entertain-
ment, philosophy) would be expected to appear
in similar sub-sets of text-regions. In such a
model, the nearest neighbors of a target word
are syntagmatically related (i.e. appear along-
side each other), and for judge might include
lawyer, court, crime, or prison.
The Document model used here is loosely
based on LSA, taking the frequency of occur-
rence of each of our 40,000 vocabulary words
in each of 50 million documents as its input
data, and it follows Bullinaria and Levy (2007);
Turney and Pantel (2010) in using PPMI as a
normalization function. We have not investi-
gated variations on the decomposition algorithm
in any detail, such as those using non-negative
matrix factorization, probabilistic LSA or LDA
topic models, as the objective in this paper is
to provide a direct comparison between the dif-
ferent types of basic collocation information en-
coded in corpora, rather than evaluate the best
algorithmic means for discovering latent dimen-
sions in those co-occurrences. Nor have we eval-
uated performance on a more structured corpus
input (Pereira et al, 2011). However prelimi-
nary tests with the Wikipedia corpus, and with
LDA, using the Gensim package (Rehurek and
Sojka, 2010) yielded similar performances.
2.2.3 Word-Collocate Models
Word-collocate models make a complemen-
tary assumption to that of the document model:
that words with closely-related categorical or
taxonomic properties should appear in the same
position of similar sentences. In a basic word-
collocate model, based on a word-word co-
occurrence matrix, the nearest neighbors of
judge might be athlete, singer, or fire-fighter,
reflecting paradigmatic relatedness (i.e. substi-
tutability). Word-collocate models are further
differentiated by the amount of linguistic anno-
tation attached to word features, ranging from
simple word-form features in a fixed-width win-
dow around the concept word, to more elaborate
word sequence patterns and parses including
parts of speech and dependency relation tags.
Among these alternatives, we might expect a
dependency model to be the most powerful. In-
tuitively, the information that John is sentient
is similarly encoded in the text John likes cake
and John seems to really really like cake, and a
suitably effective parser should be able to gen-
eralize over this variation, to extract the same
dependency relationship of John-subject-like. In
contrast a narrow window-based model might
exclude informative features (such as like in the
second example), while including presumably
uninformative ones (such as really). However
parsers have the disadvantage of being computa-
tionally expensive (meaning that they typically
are applied to smaller corpora) and usually in-
troduce some noise through their errors. Conse-
quently, simpler window-based models have of-
ten been found to be as effective.
The most basic model considered is the
Word-Form model, in which all lower-case to-
kens (word forms and punctuation) found within
four positions left and right of the target word
are recorded, yielding simple features such as
{john, likes}. It may also be termed a ?flat?
model in contrast to those which assign a vari-
able weight to collocates, progressively lower as
one moves further than the target position (e.g.
117
Lund et al, 1995). We did not use a stop-list, as
Bullinaria and Levy (2007) found co-occurrence
with very high frequency words also to be infor-
mative for semantic tasks. We also expect that
the subsequent steps of normalizing with PPMI,
reduction with SVD, and use of regularised re-
gression should be able to recognize when such
high-frequency words are not informative and
then discount these, without the need for such
assumptions upfront.
The Stemmed model is a slight variation on
the Word-Form model, where the same statistics
are aggregated after applying Lancaster stem-
ming (Paice, 1990; Loper and Bird, 2002).
The Directional model, inspired by Schu?tze
and Pedersen (1993), is also derived from the
word-form model, but differentiates between co-
occurrence to the left or to the right of the target
word, with features such as {john L, cake R}.
The Part-of-Speech model (Kanejiya et al,
2003; Widdows, 2003) replaces each lower-
case word-token with its part-of-speech disam-
biguated form (e.g. likes VBZ, cake NN ). These
annotations were extracted from the full depen-
dency parse described below.
The Sequence model draws on a range of
work that uses word sequence patterns (Lin and
Pantel, 2001; Almuhareb and Poesio, 2004; Ba-
roni et al, 2010), and may also be considered an
approximation of models that use shallow syn-
tactic analysis (Grefenstette, 1994; Curran and
Moens, 2002). All distinct token sequences up
to length 4 either side of the target word were
counted.
Finally the Dependency model uses a full
dependency parse, which might be considered
the most informed representation of the word-
collocate relationships instantiated in corpus
sentences, and this approach has been used by
several authors (Lin, 1998; Pado? and Lapata,
2007; Baroni and Lenci, 2010). The features
used are pairs of dependency relation and lex-
eme corresponding to each edge linked to a tar-
get word of interest (e.g. likes subj ). The parser
used here was Malt, which achieves accuracies of
85% when deriving labelled dependencies on En-
glish text (Hall et al, 2007). The features pro-
duced by this module are much more limited,
to those words that have a direct dependency
relation with the word of interest.
2.3 Linear Learning Model
A linear regression model will allow us to eval-
uate how well a given model of word semantics
can be used to predict brain activity. We fol-
low the analysis in Mitchell et al (2008) and
subsequently adopted by several other research
groups (see Murphy et al, 2010). For each par-
ticipant and selected fMRI feature (i.e. each
voxel, which records the time-course of neural
activity at a fixed location in the brain), we train
a model where the level of activation of the latter
(the blood oxygenation level) in response to dif-
ferent concepts is approximated by a regularised
linear combination of their semantic features:
f = C? + ?||?||2 (3)
where f is the vector of activations of a spe-
cific fMRI feature for different concepts, the ma-
trix C contains the values of the semantic fea-
tures for the same concepts, ? is the vector of
weights we must learn for each of those (corpus-
derived) features, and ? tunes the degree of reg-
ularisation. We can illustrate this with a toy
example, containing several stimulus concepts
and their attributes on three semantic dimen-
sions: cat (+animate, -big, +moving); phone
(-animate, -big, -moving); elephant (+animate,
+big, +moving); skate-board (-animate, -big,
+moving). After training over all the voxels in
our fMRI data with this simple semantic model,
we can derive whole brain images that are typ-
ical of each of the semantic dimensions. The
power of the model is its ability to predict ac-
tivity for concepts that were not in the training
set ? for instance the brain activation elicited by
the word car might be approximated by combin-
ing the images see for -animate, +big, +moving,
even though this combination of properties was
not observed during training.
The linear model was estimated with a
least squared errors method and L2 regularisa-
tion, selecting the lambda parameter from the
range 0.0001 to 5000 using Generalized Cross-
Validation (see Hastie et al, 2011, p.244). The
118
activation of each fMRI voxel in response to a
new concept that was not in the training data
was predicted by a ?-weighted sum of the val-
ues on each semantic dimension, building a pic-
ture of expected the global neural activity re-
sponse for an arbitrary concept. Again follow-
ing Mitchell et al (2008) we use a leave-2-out
paradigm in which a linear model for each neu-
ral feature is trained in turn on all concepts mi-
nus 2, having selected the 500 most stable voxels
in the training set using the same correlational
measure across stimulus presentations. For each
of the 2 left-out concepts, we predict the global
neural activation pattern, as just described. We
then try to correctly match the predicted and
observed activations, by measuring the cosine
distance between the model-generated estimate
of fMRI activity and the that observed in the ex-
periment. If the sum of the matched cosine dis-
tances is lower than the sum of the mismatched
distances, we consider the prediction successful
? otherwise as failed. At chance levels, expected
matching accuracy is 50%, and significant per-
formance above chance can be estimated using
the binomial test, once variance had been veri-
fied over independent trials (i.e. where no single
stimulus concept is shared between pairs).
3 Results
Table 1 shows the main results of the leave-
two-out brain-image matching task. They show
the mean classification performance over 1770
word pairs (60 select 2) by 9 participants. All of
these classification accuracies are highly signif-
icant at p  0.001 over test trials (binomial,
chance 50%, n=1770*9) and p < 0.001 over
words (binomial, chance 50%, n=60). There
were some significant differences between mod-
els when making inferences over trials, but for
the small set of words used here it is not possible
to make firm conclusions about the superiority
of one model over the other, that could be confi-
dently expected to generalize to other stimuli or
experiments. However, we do achieve classifica-
tion accuracies that are as high, or higher than
any previously published (Palatucci et al, 2009;
Pereira et al, 2011), while models based on very
Semantic Models Features Accuracy
25 Verbs 25 78.5
Elicited Properties 218 83.5
Document (f2) 1000 76.2
Word Form 1000 80.0
Stemmed 1000 76.2
Direction 1000 80.2
Part-of-Speech 1000 80.0
Sequence 1000 78.5
Dependency 1000 83.1
Table 1: Brain activity prediction accuracy on leave-
2-out pair-matching task. A frequency cutoff of 20
was used for all 1000 dimensional models.
Semantic Models 300 Feats. 1000 Feats.
Document (f2) 79.9 76.2
Word Form 78.1 80.0
Stemmed 77.9 76.2
Direction 80.0 80.2
Part-of-Speech 77.9 80.0
Sequence 72.9 78.5
Dependency 81.6 83.1
Table 2: Effect of SVD dimensionality in the leave-
2-out pair-matching setting; frequency cutoff of 20.
different basic features (directional word-forms;
dependency relations; document co-occurrence)
yield very similar performance.
3.1 Effect of Number of Dimensions
Here we evaluate what effect the number of SVD
dimensions used has on the final performance
of various semantic models. Experimental re-
sults comparing 300 and 1000 dimensions are
presented in Table 2, all based on a frequency
cutoff of 20. We observe that performance im-
proves in 5 out of 7 semantic models compared,
with the highest performance achieved by the
Dependency model when 1000 SVD dimensions
were used.
3.2 Effect of Frequency Cutoff
In this section, we evaluate what effect frequency
cutoff has on the brain prediction accuracy of
various semantic models. From the results in
Table 3, we observe only marginal changes as
the frequency cutoff varied from 20 to 50. This
suggests that the semantic models of this set of
119
Semantic Models Cutoff = 50 Cutoff = 20
Document (f2) 79.9 79.9
Word Form 78.5 78.1
Stemmed 78.2 77.9
Direction 80.8 80.0
Part-of-Speech 77.5 77.9
Sequence 74.4 72.9
Dependency 81.3 81.6
Table 3: Effect of frequency cutoff in the leave-2-out
pair-matching setting; 300 SVD dimensions.
words are not very sensitive to variations in the
frequency cutoff under current experimental set-
tings, and do not benefit clearly from the de-
crease in sparsity and increase in noise that a
lower threshold produces.
3.3 Information Overlap Analysis
To verify that the models are in fact substan-
tially different, we performed a follow-on analy-
sis that measured the informational overlap be-
tween the corpus-derived models. Given two
models A and B, both with dimensionality 40
thousand words by 300 SVD dimensions, we can
evaluate the extent to which A (used as the
predictor semantic representation) contains the
information encoded in B (the explained rep-
resentation). As shown in (4), for each SVD
component c, we take the left singular vector
bc as a dependent variable and fit it with a lin-
ear model, using the matrix A (all left singu-
lar vectors) as independent variables. The ex-
plained variance for this column is weighted by
its squared singular value s2c in B, and the sum of
these component-wise variances gives the total
variance explained R2A?B.
R2A?B =
300?
c=1
s2c?
s2c
RA?bc (4)
Figure 1 indicates that the first three models,
which are all derived from token occurrences in a
?4 window, are close to identical. The sequence
and document models are relatively dissimilar,
and the dependency model occupies a middle
ground, with some similarity to all the models.
It is also interesting to note that the among the
first cluster of word-form derived models, the
Figure 1: Informational Overlap between Corpus-
Derived Datasets, in R2
directional one has the highest similarity to the
dependency model.
4 Conclusion
The main result of this study was that we
achieved classification accuracies as high as
any published, and within a fraction of a per-
centage point of the human benchmark 20
Questions data, using completely unsupervised,
data-driven models of semantics based on a large
random sample of web-text. The most linguisti-
cally informed among the models (and so, per-
haps the most psychologically plausible), based
on dependency parses, is the most successful.
Still the performance of sometimes radically dif-
ferent models, from Document-based (syntag-
matic) and Word-Form-based (paradigmatic), is
surprisingly similar. One reason for this may be
that we have reached a ceiling in performance
on the fMRI data, due to its inherent noise ? in
this regard it is interesting to note that an at-
tempt to classify individual concepts using this
data directly, without an intervening model of
semantics, also achieves about 80% (though on a
different task, Shinkareva et al, 2008). Another
possible explanation is that both methods reveal
equivalent sets of underlying semantic dimen-
sions, but figure 1 suggests not. Alternatively,
it may be that the small set of 60 words exam-
ined here may be as well-distinguished by means
120
of their taxonomic differences, as by their top-
ical differences, a suggestion supported by the
results in Pereira et al (2011, see Figure 2A).
From the perspective of computational effi-
ciency however, some of the models have clearer
advantages. The Dependency and Part-of-
Speech models are processing-intensive, since
the broad vocabulary considered requires that
the very large quantities of text pass through
a parsing or tagging pipeline (though these
tasks can be parallelized). The Sequence and
Document models conversely require very large
amounts of memory to store all their features
during SVD. In comparison, the Direction model
is impressive, as it achieves close to optimal per-
formance, despite being very cheap to produce
in terms of processor time and memory foot-
print. Its relatively superior performance may
be due to the relatively fixed word-order of En-
glish, making it a good approximation of a De-
pendency model. For instance, given the nar-
row ?4 token windows used here, the Direction
features shaky Left and donate Right (relative
to a target noun) are probably nearly identical
to the Dependency features shaky Adj and do-
nate Subj. The Sequence model might also be
seen as an approximate Dependency model, but
one with the addition of more superficial colloca-
tions such as ?fish and chips? or ?Judge Judy?,
which are less relevant to our semantic task.
The evidence for the influence of the scal-
ing parameters (number of SVD dimensions,
frequency cutoff) is mixed: cut-off appears to
have little effect either way, and increasing the
number of dimensions can help or hinder (com-
pare the Sequence and Document models). We
can speculate that the Document model is al-
ready ?saturated? with 300 dimensions/topics,
but that the other models based on properties
have a higher inherent dimensionality. It may
also be a lower cut-off and higher dimensional-
ity would show clearer benefits over a larger set
of semantic/syntactic domains, including lower-
frequency words (the lowest frequency work in
the set of 60 used here was igloo, which has an
incidence of 0.3 per million words in the ANC).
PPMI appears to be both effective, and par-
simonious with assumptions one might make
about conceptual representations, where it
would be cognitively onerous and unnecessary
to encode all negative features (such as the facts
that dogs do not have wheels, are not commu-
nication events, and do not belong in the avi-
ation domain). But while SVD is certainly ef-
fective in dealing with the pervasive synonymy
and polysemy seen in corpus-feature sets, it is
less clear that it reveals psychologically plausi-
ble dimensions of meaning. Alternatives such as
non-negative matrix factorization (Lee and Se-
ung, 1999) or Latent Dirichlet Allocation (Blei
et al, 2003) might extract more readily inter-
pretable dimensions; or alternative regularisa-
tion methods such as Elastic Nets, Lasso (Hastie
et al, 2011), or Network Regularisation (Sandler
et al, 2009) might even be capable of identifying
meaningful clusters of features when learning di-
rectly on co-occurrence data. Finally, we should
consider whether more derived datasets could be
used as input data in place of the basic corpus
features used here, such as the full facts learned
by the NELL system (Carlson et al, 2010), or
crowd-sourced data which can be easily gathered
for any word (e.g. association norms, Kiss et al,
1973), though different algorithmic means would
be needed to deal with their extreme degree of
sparsity.
The results also suggest a series of follow-on
analyses. A priority should be to test these
models against a wider range of neuroimaging
data modalities (e.g. MEG, EEG) and stim-
ulus sets, including abstract kinds (see Mur-
phy et al 2012, for a preliminary study), and
parts-of-speech beyond nouns. It may be that a
putative complementarity between word-region
and word-collocate models is only revealed when
we look at a broader sample of the human
lexicon. And beyond establishing what infor-
mational content is required to make semantic
distinctions, other factorisation methods (e.g.
sparse or non-negative decompositions) could be
applied to yield more interpretable dimensions.
Other classification tasks might also be more
sensitive for detecting differences between mod-
els, such as the test of word identification among
a set by rank accuracy, as used in (Shinkareva
et al, 2008).
121
References
Almuhareb, A. and Poesio, M. (2004). Attribute-
based and value-based clustering: An evaluation.
In Proceedings of EMNLP, pages 158?165.
Baroni, M. and Lenci, A. (2010). Distributional
Memory : A General Framework for Corpus-Based
Semantics. Computational Linguistics, 36(4):673?
721.
Baroni, M., Murphy, B., Barbu, E., and Poesio, M.
(2010). Strudel: A corpus-based semantic model
based on properties and types. Cognitive Science,
34(2):222?254.
Battig, W. F. and Montague, W. E. (1969). Cate-
gory Norms for Verbal Items in 56 Categories: A
Replication and Extension of the Connecticut Cat-
egory Norms. Journal of Experimental Psychology
Monographs, 80(3):1?46.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003).
Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3(4-5):993?1022.
Bradford, R. B. (2008). An empirical study of re-
quired dimensionality for large-scale latent seman-
tic indexing applications. Proceeding of the 17th
ACM conference on Information and knowledge
mining CIKM 08, pages 153?162.
Brants, T. and Franz, A. (2006). Web 1T 5-gram
Version 1.
Bullinaria, J. A. and Levy, J. P. (2007). Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510?526.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B.,
Jr., E. R. H., and Mitchell, T. M. (2010). To-
ward an Architecture for Never-Ending Language
Learning. Artificial Intelligence, 2(4):3?3.
Chang, K.-m. K., Mitchell, T., and Just, M. A.
(2011). Quantitative modeling of the neural repre-
sentation of objects: how semantic feature norms
can account for fMRI activation. NeuroImage,
56(2):716?727.
Curran, J. R. and Moens, M. (2002). Improvements
in automatic thesaurus extraction. In SIGLEX,
pages 59?66.
Deerwester, S., Dumais, S., Landauer, T., Furnas,
G., and Harshman, R. (1990). Indexing by La-
tent Semantic Analysis. Journal of the American
Society of Information Science, 41(6):391 ? 407.
Devereux, B. and Kelly, C. (2010). Using fMRI ac-
tivation to conceptual stimuli to evaluate meth-
ods for extracting conceptual representations from
corpora. In Murphy, B., Korhonen, A., and
Chang, K. K.-M., editors, 1st Workshop on Com-
putational Neurolinguistics.
Friston, K. J., Ashburner, J. T., Kiebel, S. J.,
Nichols, T. E., and Penny, W. D. (2007). Statis-
tical Parametric Mapping: The Analysis of Func-
tional Brain Images, volume 8. Academic Press.
Grefenstette, G. (1994). Explorations in Automatic
Thesaurus Discovery. Kluwer, Dordrecht.
Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B.
(2007). Topics in semantic representation. Psy-
chological Review, 114(2):211?244.
Hall, J., Nilsson, J., Nivre, J., Eryigit, G., Megyesi,
B., Nilsson, M., and Saers, M. (2007). Single Malt
or Blended? A Study in Multilingual Parser Op-
timization. CoNLL Shared Task Session, pages
933?939.
Hastie, T., Tibshirani, R., and Friedman, J. (2011).
The Elements of Statistical Learning, volume 18 of
Springer Series in Statistics. Springer, 5th edition.
Jelodar, A. B., Alizadeh, M., and Khadivi, S. (2010).
WordNet Based Features for Predicting Brain Ac-
tivity associated with meanings of nouns. In Mur-
phy, B., Korhonen, A., and Chang, K. K.-M., ed-
itors, 1st Workshop on Computational Neurolin-
guistics, pages 18?26.
Jones, E., Oliphant, T., Peterson, P., and Et Al.
(2001). SciPy: Open source scientific tools for
Python.
Kanejiya, D., Kumar, A., and Prasad, S. (2003).
Automatic evaluation of students? answers using
syntactically enhanced LSA. Building educational
applications, NAACL, 2:53?60.
Kiss, G. R., Armstrong, C., Milroy, R., and Piper, J.
(1973). An associative thesaurus of English and its
computer analysis. In Aitken, A. J., Bailey, R. W.,
and Hamilton-Smith, N., editors, The Computer
and Literary Studies. Edinburgh University Press.
Landauer, T. and Dumais, S. (1997). A solution to
Plato?s problem: the latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Lee, D. D. and Seung, H. S. (1999). Learning the
parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788?91.
Lehoucq, R. B., Sorensen, D. C., and Yang, C.
(1998). Arpack users? guide: Solution of large
scale eigenvalue problems with implicitly restarted
Arnoldi methods. SIAM.
122
Lin, D. (1998). Automatic Retrieval and Clustering
of Similar Words. In COLING-ACL, pages 768?
774.
Lin, D. and Pantel, P. (2001). DIRT ? discovery
of inference rules from text. Proceedings of the
seventh ACM SIGKDD international conference
on Knowledge discovery and data mining KDD 01,
datamining:323?328.
Loper, E. and Bird, S. (2002). {NLTK}: The natu-
ral language toolkit. In ACL Workshop, volume 1,
pages 63?70. Association for Computational Lin-
guistics.
Lund, K. and Burgess, C. (1996). Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28:203?208.
Lund, K., Burgess, C., and Atchley, R. (1995). Se-
mantic and associative priming in high dimen-
sional semantic space. In Proceedings of the 17th
Cognitive Science Society Meeting, pages 660?665.
Mitchell, T. M., Shinkareva, S. V., Carlson, A.,
Chang, K.-M., Malave, V. L., Mason, R. A., and
Just, M. A. (2008). Predicting Human Brain Ac-
tivity Associated with the Meanings of Nouns. Sci-
ence, 320:1191?1195.
Murphy, B., Baroni, M., and Poesio, M. (2009). EEG
responds to conceptual stimuli and corpus seman-
tics. In Proceedings of EMNLP, pages 619?627.
ACL.
Murphy, B., Korhonen, A., and Chang, K. K.-
M., editors (2010). Proceedings of the 1st Work-
shop on Computational Neurolinguistics, NAACL-
HLT, Los Angeles. ACL.
Murphy, B., Talukdar, P., and Mitchell, T. (2012).
Comparing Abstract and Concrete Conceptual
Representations using Neurosemantic Decoding.
In NAACL Workshop on Cognitive Modelling and
Computational Linguistics.
Nancy Ide and Keith Suderman (2006). The Amer-
ican National Corpus First Release. Proceedings
of the Fifth Language Resources and Evaluation
Conference (LREC).
Nation, P. and Waring, R. (1997). Vocabulary size,
text coverage and word lists. In Schmitt, N. and
McCarthy, M., editors, Vocabulary Description ac-
quisition and pedagogy, pages 6?19. Cambridge
University Press.
Pado?, S. and Lapata, M. (2007). Dependency-based
construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199.
Paice, C. D. (1990). Another stemmer. SIGIR Fo-
rum, 24(3):56?61.
Palatucci, M., Hinton, G., Pomerleau, D., and
Mitchell, T. M. (2009). Zero-Shot Learning with
Semantic Output Codes. Advances in Neural In-
formation Processing Systems, 22:1?9.
Palatucci, M. M. (2011). Thought Recognition: Pre-
dicting and Decoding Brain Activity Using the
Zero-Shot Learning Model. PhD thesis, Carnegie
Mellon University.
Pereira, F., Detre, G., and Botvinick, M. (2011).
Generating Text from Functional Brain Images.
Frontiers in Human Neuroscience, 5:1?11.
Rapp, R. (2003). Word Sense Discovery Based on
Sense Descriptor Dissimilarity. Proceedings of the
Ninth Machine Translation Summit, pp:315?322.
Rehurek, R. and Sojka, P. (2010). Software Frame-
work for Topic Modelling with Large Corpora. In
New Challenges, LREC 2010, pages 45?50. ELRA.
Rubenstein, H. and Goodenough, J. B. (1965). Con-
textual correlates of synonymy. Communications
of the ACM, 8(10):627?633.
Sahlgren, M. (2006). The Word-Space Model: Using
distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Dissertation, Stock-
holm University.
Sandler, T., Talukdar, P. P., Ungar, L. H., and
Blitzer, J. (2009). Regularized Learning with Net-
works of Features. Advances in Neural Informa-
tion Processing Systems 21, 4:1401?1408.
Schu?tze, H. and Pedersen, J. (1993). A Vector Model
for syntagmatic and paradigmatic relatedness. In
Making Sense of Words Proceedings of the 9th
Annual Conference of the University of Waterloo
Centre for the New OED and Text Research, pages
104?113.
Shinkareva, S. V., Mason, R. A., Malave, V. L.,
Wang, W., Mitchell, T. M., and Just, M. A.
(2008). Using fMRI Brain Activation to Iden-
tify Cognitive States Associated with Perception
of Tools and Dwellings. PloS ONE, 3(1).
Turney, P. D. and Pantel, P. (2010). From Frequency
to Meaning: Vector Space Models of Semantics.
Artificial Intelligence, 37(1):141?188.
Widdows, D. (2003). Unsupervised methods for de-
veloping taxonomies by combining syntactic and
statistical information. In NAACL, pages 197?
204. Association for Computational Linguistics.
123
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 1?10,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Vector Space Semantic Parsing: A Framework for
Compositional Vector Space Models
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
We present vector space semantic parsing
(VSSP), a framework for learning compo-
sitional models of vector space semantics.
Our framework uses Combinatory Cate-
gorial Grammar (CCG) to define a cor-
respondence between syntactic categories
and semantic representations, which are
vectors and functions on vectors. The
complete correspondence is a direct con-
sequence of minimal assumptions about
the semantic representations of basic syn-
tactic categories (e.g., nouns are vectors),
and CCG?s tight coupling of syntax and
semantics. Furthermore, this correspon-
dence permits nonuniform semantic repre-
sentations and more expressive composi-
tion operations than previous work. VSSP
builds a CCG semantic parser respecting
this correspondence; this semantic parser
parses text into lambda calculus formulas
that evaluate to vector space representa-
tions. In these formulas, the meanings of
words are represented by parameters that
can be trained in a task-specific fashion.
We present experiments using noun-verb-
noun and adverb-adjective-noun phrases
which demonstrate that VSSP can learn
composition operations that RNN (Socher
et al, 2011) and MV-RNN (Socher et al,
2012) cannot.
1 Introduction
Vector space models represent the semantics of
natural language using vectors and operations on
vectors (Turney and Pantel, 2010). These models
are most commonly used for individual words and
short phrases, where vectors are created using dis-
tributional information from a corpus. Such mod-
els achieve impressive performance on standard-
ized tests (Turney, 2006; Rapp, 2003), correlate
well with human similarity judgments (Griffiths et
al., 2007), and have been successfully applied to a
number of natural language tasks (Collobert et al,
2011).
While vector space representations for indi-
vidual words are well-understood, there remains
much uncertainty about how to compose vector
space representations for phrases out of their com-
ponent words. Recent work in this area raises
many important theoretical questions. For exam-
ple, should all syntactic categories of words be
represented as vectors, or are some categories,
such as adjectives, different? Using distinct se-
mantic representations for distinct syntactic cate-
gories has the advantage of representing the opera-
tional nature of modifier words, but the disadvan-
tage of more complex parameter estimation (Ba-
roni and Zamparelli, 2010). Also, does semantic
composition factorize according to a constituency
parse tree (Socher et al, 2011; Socher et al,
2012)? A binarized constituency parse cannot di-
rectly represent many intuitive intra-sentence de-
pendencies, such as the dependence between a
verb?s subject and its object. What is needed to
resolve these questions is a comprehensive theo-
retical framework for compositional vector space
models.
In this paper, we observe that we already have
such a framework: Combinatory Categorial Gram-
mar (CCG) (Steedman, 1996). CCG provides a
tight mapping between syntactic categories and
semantic types. If we assume that nouns, sen-
tences, and other basic syntactic categories are
represented by vectors, this mapping prescribes
semantic types for all other syntactic categories.1
For example, we get that adjectives are functions
from noun vectors to noun vectors, and that prepo-
1It is not necessary to assume that sentences are vectors.
However, this assumption simplifies presentation and seems
like a reasonable first step. CCG can be used similarly to
explore alternative representations.
1
Input: Log. Form:
?red ball??
semantic
parsing ?Aredvball? evaluation ?
?
?
?
?
? ?
Lexicon: red:= ?x.Aredx
ball:= vball
Params.:Ared =
?
? ?
? ?
?
vball =
?
?
?
?
Figure 1: Overview of vector space semantic pars-
ing (VSSP). A semantic parser first translates nat-
ural language into a logical form, which is then
evaluated to produce a vector.
sitions are functions from a pair of noun vectors
to a noun vector. These semantic type specifica-
tions permit a variety of different composition op-
erations, many of which cannot be represented in
previously-proposed frameworks. Parsing in CCG
applies these functions to each other, naturally de-
riving a vector space representation for an entire
phrase.
The CCG framework provides function type
specifications for each word?s semantics, given its
syntactic category. Instantiating this framework
amounts to selecting particular functions for each
word. Vector space semantic parsing (VSSP) pro-
duces these per-word functions in a two-step pro-
cess. The first step chooses a parametric func-
tional form for each syntactic category, which con-
tains as-yet unknown per-word and global param-
eters. The second step estimates these parameters
using a concrete task of interest, such as predicting
the corpus statistics of adjective-noun compounds.
We present a stochastic gradient algorithm for this
step which resembles training a neural network
with backpropagation. These parameters may also
be estimated in an unsupervised fashion, for ex-
ample, using distributional statistics.
Figure 1 presents an overview of VSSP. The
input to VSSP is a natural language phrase and
a lexicon, which contains the parametrized func-
tional forms for each word. These per-word repre-
sentations are combined by CCG semantic pars-
ing to produce a logical form, which is a sym-
bolic mathematical formula for producing the vec-
tor for a phrase ? for example, Aredvball is a for-
mula that performs matrix-vector multiplication.
This formula is evaluated using learned per-word
and global parameters (values for Ared and vball)
to produce the language?s vector space representa-
tion.
The contributions of this paper are threefold.
First, we demonstrate how CCG provides a the-
oretical basis for vector space models. Second,
we describe VSSP, which is a method for con-
cretely instantiating this theoretical framework.
Finally, we perform experiments comparing VSSP
against other compositional vector space mod-
els. We perform two case studies of composition
using noun-verb-noun and adverb-adjective-noun
phrases, finding that VSSP can learn composition
operations that existing models cannot. We also
find that VSSP produces intuitively reasonable pa-
rameters.
2 Combinatory Categorial Grammar for
Vector Space Models
Combinatory Categorial Grammar (CCG) (Steed-
man, 1996) is a lexicalized grammar formalism
that has been used for both broad coverage syntac-
tic parsing and semantic parsing. Like other lexi-
calized formalisms, CCG has a rich set of syntac-
tic categories, which are combined using a small
set of parsing operations. These syntactic cate-
gories are tightly coupled to semantic represen-
tations, and parsing in CCG simultaneously de-
rives both a syntactic parse tree and a seman-
tic representation for each node in the parse tree.
This coupling between syntax and semantics moti-
vates CCG?s use in semantic parsing (Zettlemoyer
and Collins, 2005), and provides a framework for
building compositional vector space models.
2.1 Syntax
The intuition embodied in CCG is that, syntac-
tically, words and phrases behave like functions.
For example, an adjective like ?red? can com-
bine with a noun like ?ball? to produce another
noun, ?red ball.? Therefore, adjectives are natu-
rally viewed as functions that apply to nouns and
return nouns. CCG generalizes this idea by defin-
ing most parts of speech in terms of such func-
tions.
Parts of speech in CCG are called syntactic cat-
egories. CCG has two kinds of syntactic cat-
egories: atomic categories and functional cate-
gories. Atomic categories are used to represent
phrases that do not accept arguments. These cate-
gories includeN for noun,NP for noun phrase, S
for sentence, and PP for prepositional phrase. All
other parts of speech are represented using func-
tional categories. Functional categories are written
as X/Y or X\Y , where both X and Y are syn-
2
Part of speech Syntactic category Example usage Semantic type Example log. form
Noun N person : N Rd vperson
Adjective N/Nx good person : N ?Rd,Rd? ?x.Agoodx
Determiner NP/Nx the person : NP ?Rd,Rd? ?x.x
Intrans. Verb S\NPx the person ran : S ?Rd,Rd? ?x.Aranx+ bran
Trans. Verb S\NPy/NPx the person ran home : S ?Rd, ?Rd,Rd?? ?x.?y.(Tranx)y
Adverb (S\NP )\(S\NP ) ran lazily : S\NP ??Rd,Rd?, ?Rd,Rd?? [?y.Ay ? ?y.(TlazyA)y]
(S\NP )/(S\NP ) lazily ran : S\NP ??Rd,Rd?, ?Rd,Rd?? [?y.Ay ? ?y.(TlazyA)y]
(N/N)/(N/N) very good person : N ??Rd,Rd?, ?Rd,Rd?? [?y.Ay ? ?y.(TveryA)y]
Preposition (N\Ny)/Nx person in France : N ?Rd, ?Rd,Rd?? ?x.?y.(Tinx)y
(S\NPy)\(S\NP )f/NPx ran in France : S\NP ?Rd, ??Rd,Rd?, ?Rd,Rd??? ?x.?f.?y.(Tinx)(f(y))
Table 1: Common syntactic categories in CCG, paired with their semantic types and example logical
forms. The example usage column shows phrases paired with the syntactic category that results from
using the exemplified syntactic category for the bolded word. For ease of reference, each argument to
a syntactic category on the left is subscripted with its corresponding semantic variable in the example
logical form on the right. The variables x, y, b, v denote vectors, f denotes a function, A denotes a
matrix, and T denotes a tensor. Subscripted variables (Ared) denote parameters. Functions in logical
forms are specified using lambda calculus; for example ?x.Ax is the function that accepts a (vector)
argument x and returns the vector Ax. The notation [f ? g] denotes the higher-order function that,
given input function f , outputs function g.
tactic categories. These categories represent func-
tions that accept an argument of category Y and
return a phrase of category X . The direction of
the slash defines the expected location of the argu-
ment: X/Y expects an argument on the right, and
X\Y expects an argument on the left.2 For ex-
ample, adjectives are represented by the category
N/N ? a function that accepts a noun on the right
and returns a noun.
The left part of Table 1 shows examples of
common syntactic categories, along with exam-
ple uses. Note that some intuitive parts of speech,
such as prepositions, are represented by multiple
syntactic categories. Each of these categories cap-
tures a different use of a preposition, in this case
the noun-modifying and verb-modifying uses.
2.2 Semantics
Semantics in CCG are given by first associating a
semantic type with each syntactic category. Each
word in a syntactic category is then assigned a
semantic representation of the corresponding se-
mantic type. These semantic representations are
known as logical forms. In our case, a logical form
is a fragment of a formula for computing a vector
space representation, containing word-specific pa-
rameters and specifying composition operations.
In order to construct a vector space model, we
associate all of the atomic syntactic categories,
2As a memory aid, note that the top of the slash points in
the direction of the expected argument.
N , NP , S, and PP , with the type Rd. Then,
the logical form for a noun like ?ball? is a vec-
tor vball ? Rd. The functional categories X/Y
and X\Y are associated with functions from the
semantic type of X to the semantic type of Y . For
example, the semantic type of N/N is ?Rd,Rd?,
representing the set of functions from Rd to Rd.3
This semantic type captures the same intuition as
adjective-noun composition models: semantically,
adjectives are functions from noun vectors to noun
vectors.
The right portion of Table 1 shows semantic
types for several syntactic categories, along with
example logical forms. All of these mappings
are a direct consequence of the assumption that
all atomic categories are semantically represented
by vectors. Interestingly, many of these semantic
types contain functions that cannot be represented
in other frameworks. For example, adverbs have
type ??Rd,Rd?, ?Rd,Rd??, representing functions
that accept an adjective argument and return an
adjective. In Table 1, the example logical form
applies a 4-mode tensor to the adjective?s matrix.
Another powerful semantic type is ?Rd, ?Rd,Rd??,
which corresponds to transitive verbs and prepo-
3The notation ?A,B? represents the set of functions
whose domain isA and whose range isB. Somewhat confus-
ingly, the bracketing in this notation is backward relative to
the syntactic categories ? the syntactic category (N\N)/N
has semantic type ?Rd, ?Rd,Rd??, where the inner ?Rd,Rd?
corresponds to the left (N\N).
3
the
NP/N
?x.x
red
N/N
?x.Aredx
ball
N
vball
N : Aredvball
NP : Aredvball
on
(NP\NP )/NP
?x.?y.Aonx+Bony
the
NP/N
?x.x
table
N
vtable
NP : vtable
NP\NP : ?y.Aonvtable +Bony
NP : Aonvtable +BonAredvball
Figure 2: Syntactic CCG parse and corresponding vector space semantic derivation.
sitions. This type represents functions from two
argument vectors to an output vector, which have
been curried to accept one argument vector at a
time. The example logical form for this type uses
a 3-mode tensor to capture interactions between
the two arguments.
Note that this semantic correspondence permits
a wide range of logical forms for each syntactic
category. Each logical form can have an arbitrary
functional form, as long as it has the correct se-
mantic type. This flexibility permits experimenta-
tion with different composition operations. For ex-
ample, adjectives can be represented nonlinearly
by using a logical form such as ?x. tanh(Ax). Or,
adjectives can be represented nonparametrically
by using kernel regression to learn the appropriate
function from vectors to vectors. We can also in-
troduce simplifying assumptions, as demonstrated
by the last entry in Table 1. CCG treats preposi-
tions as modifying intransitive verbs (the category
S\N ). In the example logical form, the verb?s
semantics are represented by the function f , the
verb?s subject noun is represented by y, and f(y)
represents the sentence vector created by compos-
ing the verb with its argument. By only operating
on f(y), this logical form assumes that the action
of a preposition is conditionally independent of the
verb f and noun y, given the sentence f(y).
2.3 Lexicon
The main input to a CCG parser is a lexicon, which
is a mapping from words to syntactic categories
and logical forms. A lexicon contains entries such
as:
ball := N : vball
red := N/N : ?x.Aredx
red := N : vred
flies := ((S\NP )/NP ) : ?x.?y.(Tfliesx)y
Each entry of the lexicon associates a word
(ball) with a syntactic category (N ) and a logical
form (vball) giving its vector space representation.
Note that a word may appear multiple times in the
lexicon with distinct syntactic categories and log-
ical forms. Such repeated entries capture words
with multiple possible uses; parsing must deter-
mine the correct use in the context of a sentence.
2.4 Parsing
Parsing in CCG has two stages. First, a category
for each word in the input is retrieved from the lex-
icon. Second, adjacent categories are iteratively
combined by applying one of a small number of
combinators. The most common combinator is
function application:
X/Y : f Y : g =? X : f(g)
Y : g X\Y : f =? X : f(g)
The function application rule states that a cate-
gory of the form X/Y behaves like a function that
accepts an input category Y and returns category
X . The rule also derives a logical form for the re-
sult by applying the function f (the logical form
for X/Y ) to g (the logical form for Y ). Figure 2
shows how repeatedly applying this rule produces
a syntactic parse tree and logical form for a phrase.
The top row of the parse represents retrieving a
lexicon entry for each word in the input. Each
following line represents a use of the function ap-
plication combinator to syntactically and semanti-
cally combine a pair of adjacent categories. The
order of these operations is ambiguous, and dif-
ferent orderings may result in different parses ? a
CCG parser?s job is to find a correct ordering. The
result of parsing is a syntactic category for the en-
tire phrase, coupled with a logical form giving the
phrase?s vector space representation.
3 Vector Space Semantic Parsing
Vector space semantic parsing (VSSP) is an
approach for constructing compositional vector
space models based on the theoretical framework
of the previous section. VSSP concretely instanti-
ates CCG?s syntactic/semantic correspondence by
adding appropriately-typed logical forms to a syn-
tactic CCG parser?s lexicon. Parsing a sentence
with this lexicon and evaluating the resulting logi-
4
Semantic type Example syntactic categories Logical form template
Rd N,NP, PP, S vw
?Rd,Rd? N/N , NP/N , S/S, S\NP ?x.?(Awx)
?Rd, ?Rd,Rd?? (S\NP )/NP , (NP\NP )/NP ?x.?y.?((Twx)y)
??Rd,Rd?, ?Rd,Rd?? (N/N)/(N/N) [?y.?(Ay)? ?y.?((TwA)y)]
Table 2: Lexicon templates used in this paper to produce a CCG semantic parser. ? represents the
sigmoid function, ?(x) = e
x
1+ex .
cal form produces the sentence?s vector space rep-
resentation.
While it is relatively easy to devise vector space
representations for individual nouns, it is more
challenging to do so for the fairly complex func-
tion types licensed by CCG. VSSP defines these
functions in two phases. First, we create a lexi-
con mapping words to parametrized logical forms.
This lexicon specifies a functional form for each
word, but leaves free some per-word parame-
ters. Parsing with this lexicon produces logical
forms that are essentially functions from these
per-word parameters to vector space representa-
tions. Next, we train these parameters to pro-
duce good vector space representations in a task-
specific fashion. Training performs stochastic gra-
dient descent, backpropagating gradient informa-
tion through the logical forms.
3.1 Producing the Parametrized Lexicon
We create a lexicon using a set of manually-
constructed templates that associate each syntactic
category with a parametrized logical form. Each
template contains variables that are instantiated to
define per-word parameters. The output of this
step is a CCG lexicon which can be used in a
broad coverage syntactic CCG parser (Clark and
Curran, 2007) to produce logical forms for input
language.4
Table 2 shows some templates used to create
logical forms for syntactic categories. To reduce
annotation effort, we define one template per se-
mantic type, covering all syntactic categories with
that type. These templates are instantiated by re-
placing the variable w in each logical form with
the current word. For example, instantiating the
second template for ?red? produces the logical
form ?x.?(Aredx), where Ared is a matrix of pa-
rameters.
4In order to use the lexicon in an existing parser, the gen-
erated syntactic categories must match the parser?s syntac-
tic categories. Then, to produce a logical form for a sen-
tence, simply syntactically parse the sentence, generate log-
ical forms for each input word, and retrace the syntactic
derivation while applying the corresponding semantic oper-
ations to the logical forms.
Note that Table 2 is a only starting point ? devis-
ing appropriate functional forms for each syntactic
category is an empirical question that requires fur-
ther research. We use these templates in our ex-
periments (Section 4), suggesting that they are a
reasonable first step. More complex data sets will
require more complex logical forms. For example,
to use high-dimensional vectors, all matrices and
tensors will have to be made low rank. Another
possible improvement is to tie the parameters for
a single word across related syntactic categories
(such as the transitive and intransitive forms of a
verb).
3.2 Training the Logical Form Parameters
The training problem in VSSP is to optimize the
logical form parameters to best perform a given
task. Our task formulation subsumes both clas-
sification and regression: we assume the input is
a logical form, and the output is a vector. Given a
data set of this form, training can be performed us-
ing stochastic gradient descent in a fashion similar
to backpropagation in a neural network.
The data set for training consists of tuples,
{(`i, yi)}ni=1, where ` is a logical form and y is a
label vector representing the expected task output.
Each logical form ` is treated as a function from
parameter vectors ? to vectors in Rd. For example,
the logical form Aredvball is a function from Ared
and vball to a vector. We use ? to denote the set
of all parameters; for example, ? = {Ared, vball}.
We further assume a loss function L defined over
pairs of label vectors. The training problem is
therefore to minimize the objective:
O(?) =
n?
i=1
L(yi, g(`i(?)) +
?
2
||?||2
Above, g represents a global postprocessing func-
tion which is applied to the output of VSSP to
make a task-specific prediction. This function may
also be parametrized, but we suppress these pa-
rameters for simplicity. As a concrete example,
consider a classification task (as in our evaluation).
In this case, y represents a target distribution over
labels, L is the KL divergence between the pre-
5
dicted and target distributions, and g represents a
softmax classifier.
We optimize the objective O by running
stochastic gradient descent. The gradients of the
parameters ? can be computed by iteratively ap-
plying the chain rule to `, which procedurally
resembles performing backpropagation in a neu-
ral network (Rumelhart et al, 1988; Goller and
Ku?chler, 1996).
4 Comparing Models of Semantic
Composition
This section compares the expressive power of
VSSP to previous work. An advantage of VSSP
is its ability to assign complex logical forms to
categories like adverbs and transitive verbs. This
section examines cases where such complex logi-
cal forms are necessary, using synthetic data sets.
Specifically, we create simple data sets mimick-
ing expected forms of composition in noun-verb-
noun and adverb-adjective-noun phrases. VSSP
is able to learn the correct composition operations
for these data sets, but previously proposed mod-
els cannot.
We compare VSSP against RNN (Socher et al,
2011) and MV-RNN (Socher et al, 2012), two
recursive neural network models which factorize
composition according to a binarized constituency
parse tree. The RNN model represents the seman-
tics of each parse tree node using a single vector,
while the MV-RNN represents each node using
both a matrix and a vector. These representations
seem sufficient for adjectives and nouns, but it is
unclear how they generalize to other natural lan-
guage constructions.
In these experiments, each model is used to map
an input phrase to a vector, which is used to train a
softmax classifier that predicts the task output. For
VSSP, we use the lexicon templates from Table 2.
All nouns are represented as two-dimensional vec-
tors, and all matrices and tensors are full rank. The
parameters of each model (i.e., the per-word vec-
tors, matrices and tensors) and the softmax classi-
fier are trained as described in Section 3.2.
4.1 Propositional Logic
The propositional logic experiment examines the
impact of VSSP?s representation of transitive
verbs. VSSP directly represents these verbs as
two-argument functions, allowing it to learn op-
erations with complex interactions between both
false and false 0,1 false or false 0,1 false xor false 0,1
true and false 0,1 true or false 1,0 true xor false 1,0
false and true 0,1 false or true 1,0 false xor true 1,0
true and true 1,0 true or true 1,0 true xor true 0,1
Table 3: Data for propositional logic experiment.
Composition Formula KL divergence
RNN 0.44
MV-RNN 0.12
VSSP 0.01
Table 4: Training error on the propositional logic
data set. VSSP achieves zero error because its
verb representation can learn arbitrary logical op-
erations.
arguments. In contrast, the RNN and MV-RNN
models learn a set of global weights which are
used to combine the verb with its arguments. The
functional forms of these models limit the kinds of
interactions that can be captured by verbs.
We evaluated the learnability of argument in-
teractions using the simple data set shown in Ta-
ble 3. In this data set, the words ?and,? ?or,? and
?xor? are treated as transitive verbs, while ?true?
and ?false? are nouns. The goal is to predict the
listed truth values, which are represented as two-
dimensional distributions over true and false.
Table 4 shows the training error of each model
on this data set, measured in terms of KL diver-
gence between the model?s predictions and the
true values. VSSP achieves essentially zero train-
ing error because its 3-mode tensor representa-
tion of transitive verbs is trivially able to learn
arbitrary logical operations. RNN and MV-RNN
can learn each logical operation independently, but
cannot learn all three at the same time ? this phe-
nomenon occurs because XOR requires different
global weight matrices than AND/OR. As a re-
sult, these models learn both AND and OR, but fail
to learn XOR. This result suggests that much of
the learning in these models occurs in the global
weight matrices, while the verb representations
can have only limited influence.
Although this data set is synthetic, the interac-
tion given by XOR seems necessary to represent
real verbs. To learn AND and OR, the arguments
need not interact ? it is sufficient to detect a set
of appropriate subject and object arguments, then
threshold the number of such arguments. This
information is essentially type constraints for the
subject and object of a verb. However, type con-
straints are insufficient for real verbs. For exam-
ple, consider the verb ?eats.? All animals eat and
6
very big elephant 1,0 very big mouse 0.3,0.7
pretty big elephant 0.9,0.1 pretty big mouse 0.2,0.8
pretty small elephant 0.8,0.2 pretty small mouse 0.1,0.9
very small elephant 0.7,0.3 very small mouse 0,1
Table 5: Data for adverb-adjective-noun compo-
sition experiment. Higher first dimension values
represent larger objects.
Composition Model KL divergence
RNN 0.10
MV-RNN 0.10
VSSP 0.00
Table 6: Training error of each composition model
on the adverb-adjective-noun experiment.
can be eaten, but not all animals eat all other an-
imals; whether or not ?X eats Y ? is true depends
on an interaction between X and Y .
4.2 Adverb-Adjective-Noun Composition
Adverbs can enhance or attenuate the properties
of adjectives, which in turn can enhance or attenu-
ate the properties of nouns. The adverb-adjective-
noun experiment compares each model?s ability
to learn these effects using a synthetic object size
data set, shown in Table 5. The task is to predict
the size of each described object, which is repre-
sented as a two-dimensional distribution over big
and small. The challenge of this data set is that
an adverb?s impact on size depends on the adjec-
tive being modified ? a very big elephant is big-
ger than a big elephant, but a very small elephant
is smaller than a small elephant. Note that this
task is more difficult than adverb-adjective com-
position (Socher et al, 2012), since in this task
the adverb has to enhance/attenuate the enhanc-
ing/attenuating properties of an adjective.
Table 6 shows the training error of each model
on this data set. VSSP achieves zero training error
because its higher-order treatment of adverbs al-
lows it to accurately represent their enhancing and
attenuating effects. However, none of the other
models are capable of representing these effects.
This result is unsurprising, considering that the
RNN and MV-RNN models essentially add the
adverb and adjective parameters using a learned
linear operator (followed by a nonlinearity). Such
additive combination forces adverbs to have a con-
sistent direction of effect on the size of the noun,
which is incompatible with the desired enhancing
and attenuating behavior.
Examining VSSP?s learned parameters clearly
demonstrates its ability to learn enhancing and
?elephant?
?
1.6
?0.1
?
?mouse?
?
?0.1
1.6
?
?small? ? 0.22 0
0 1.7
? ?big? ? 1.7 ?1.1
0 0.22
?
?very small? ? 0.25 ?.12
?1.34 2.3
? ?very big?? 2.3 ?1.34
?0.12 0.25
?
Figure 3: Parameters for nouns, adjectives and ad-
jective phrases learned by VSSP. When the adverb
?very? is applied to ?small? and ?big,? it enhances
their effect on a modified noun.
attenuating phenomena. Figure 3 demonstrates
VSSP?s learned treatment of ?very.? In the fig-
ure, a high first dimension value represents a large
object, while a high second dimension value rep-
resents a small object; hence the vectors for ele-
phant and mouse show that, by default, elephants
are larger than mice. Similarly, the matrices for
big and small scale up the appropriate dimension
while shrinking the other dimension. Finally, we
show the computed matrices for ?very big? and
?very small? ? this operation is possible because
these phrases have an adjective?s syntactic cate-
gory, N/N . These matrices have the same di-
rection of effect as their unenhanced versions, but
produce a larger scaling in that direction.
5 Related Work
Several models for compositionality in vector
spaces have been proposed in recent years. Much
work has focused on evaluating composition oper-
ations for word pairs (Mitchell and Lapata, 2010;
Widdows, 2008). Many operations have been pro-
posed, including various combinations of addition,
multiplication, and linear operations (Mitchell and
Lapata, 2008), holographic reduced representa-
tions (Plate, 1991) and others (Kintsch, 2001).
Other work has used regression to train models
for adjectives in adjective-noun phrases (Baroni
and Zamparelli, 2010; Guevara, 2010). All of this
work is complementary to ours, as these composi-
tion operations can be used within VSSP by appro-
priately choosing the logical forms in the lexicon.
A few comprehensive frameworks for compo-
sition have also been proposed. One approach
is to take tensor outer products of word vec-
tors, following syntactic structure (Clark and Pul-
man, 2007). However, this approach results in
differently-shaped tensors for different grammati-
cal structures. An improvement of this framework
uses a categorial grammar to ensure that similarly-
7
typed objects lie in the same vector space (Clark
et al, 2008; Coecke et al, 2010; Grefenstette
and Sadrzadeh, 2011). VSSP generalizes this
work by allowing nonlinear composition opera-
tions and considering supervised parameter esti-
mation. Several recent neural network models im-
plicitly use a framework which assumes that com-
position factorizes according to a binarized con-
stituency parse, and that words and phrases have
uniform semantic representations (Socher et al,
2011; Socher et al, 2012). Notably, Hermann
and Blunsom (2013) instantiate such a framework
using CCG. VSSP generalizes these approaches,
as they can be implemented within VSSP by
choosing appropriate logical forms. Furthermore,
our experiments demonstrate that VSSP can learn
composition operations that cannot be learned by
these approaches.
The VSSP framework uses semantic parsing to
define a compositional vector space model. Se-
mantic parsers typically map sentences to logi-
cal semantic representations (Zelle and Mooney,
1996; Kate and Mooney, 2006), with many sys-
tems using CCG as the parsing formalism (Zettle-
moyer and Collins, 2005; Kwiatkowski et al,
2011; Krishnamurthy and Mitchell, 2012). Al-
though previous work has focused on logical se-
mantics, it has demonstrated that semantic parsing
is an elegant technique for specifying models of
compositional semantics. In this paper, we show
how to use semantic parsing to produce composi-
tional models of vector space semantics.
6 Discussion and Future Work
We present vector space semantic parsing (VSSP),
a general framework for building compositional
models of vector space semantics. Our frame-
work is based on Combinatory Categorial Gram-
mar (CCG), which defines a correspondence be-
tween syntactic categories and semantic types rep-
resenting vectors and functions on vectors. A
model in VSSP instantiates this mapping in a CCG
semantic parser. This semantic parser parses nat-
ural language into logical forms, which are in turn
evaluated to produce vector space representations.
We further propose a method for constructing such
a semantic parser using a small number of logi-
cal form templates and task-driven estimation of
per-word parameters. Synthetic data experiments
show that VSSP?s treatment of adverbs and tran-
sitive verbs can learn more functions than prior
work.
An interesting aspect of VSSP is that it high-
lights cases where propositional semantics seem
superior to vector space semantics. For example,
compare ?the ball that I threw? and ?I threw the
ball.? We expect the semantics of these phrases
to be closely related, differing only in that one
phrase refers to the ball, while the other refers to
the throwing event. Therefore, our goal is to de-
fine a logical form for ?that? which appropriately
relates the semantics of the above expressions. It is
easy to devise such a logical form in propositional
semantics, but difficult in vector space semantics.
Producing vector space solutions to such problems
is an area for future work.
Another direction for future work is joint train-
ing of both the semantic parser and vector space
representations. Our proposed approach of adding
logical forms to a broad CCG coverage parser has
the advantage of allowing VSSP to be applied to
general natural language. However, using the syn-
tactic parses from this parser may not result in
the best possible factorization of semantic com-
position. Jointly training the semantic parser and
the vector space representations may lead to better
models of semantic composition.
We also plan to apply VSSP to real data sets.
We have made some progress applying VSSP to
SemEval Task 8, learning to extract relations be-
tween nominals (Hendrickx et al, 2010). Al-
though our work thus far is preliminary, we have
found that the generality of VSSP makes it easy
to experiment with different models of composi-
tion. To swap between models, we simply mod-
ify the CCG lexicon templates ? all of the remain-
ing infrastructure is unchanged. Such preliminary
results suggest the power of VSSP as a general
framework for learning vector space models.
Acknowledgments
This research has been supported in part by
DARPA under award FA8750-13-2-0005, and in
part by a gift from Google. We thank Matt
Gardner, Justin Betteridge, Brian Murphy, Partha
Talukdar, Alona Fyshe and the anonymous review-
ers for their helpful comments.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
8
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of AAAI Spring Symposium on Quan-
tum Interaction.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. Proceedings of the
Second Symposium on Quantum Interaction.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical Foundations for a Com-
positional Distributed Model of Meaning. Lambek
Festschirft, Linguistic Analysis, 36.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537, November.
Christoph Goller and Andreas Ku?chler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Proceedings
of the International Conference on Neural Networks
(ICNN-96), pages 347?352. IEEE.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing.
Thomas L. Griffiths, Joshua B. Tenenbaum, and Mark
Steyvers. 2007. Topics in semantic representation.
Psychological Review 114.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
Geometrical Models of Natural Language Seman-
tics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O?. Se?aghdha, Sebastian
Pado?, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceedings
of the Conference.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2).
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1429.
Tony Plate. 1991. Holographic reduced represen-
tations: convolution algebra for compositional dis-
tributed representations. In Proceedings of the 12th
International Joint Conference on Artificial Intelli-
gence - Volume 1.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
Ninth Machine Translation Summit.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: foundations of
research. chapter Learning internal representations
by error propagation.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1), January.
9
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3), September.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
Second AAAI Symposium on Quantum Interaction.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the thirteenth na-
tional conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: struc-
tured classification with probabilistic categorial
grammars. In UAI ?05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence.
10
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 84?93,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Documents and Dependencies: an Exploration of
Vector Space Models for Semantic Composition
Alona Fyshe, Partha Talukdar, Brian Murphy and Tom Mitchell
Machine Learning Department &
Center for the Neural Basis of Cognition
Carnegie Mellon University, Pittsburgh
{afyshe|ppt|bmurphy|tom.mitchell}@cs.cmu.edu
Abstract
In most previous research on distribu-
tional semantics, Vector Space Models
(VSMs) of words are built either from
topical information (e.g., documents in
which a word is present), or from syntac-
tic/semantic types of words (e.g., depen-
dency parse links of a word in sentences),
but not both. In this paper, we explore the
utility of combining these two representa-
tions to build VSM for the task of seman-
tic composition of adjective-noun phrases.
Through extensive experiments on bench-
mark datasets, we find that even though
a type-based VSM is effective for seman-
tic composition, it is often outperformed
by a VSM built using a combination of
topic- and type-based statistics. We also
introduce a new evaluation task wherein
we predict the composed vector represen-
tation of a phrase from the brain activity of
a human subject reading that phrase. We
exploit a large syntactically parsed corpus
of 16 billion tokens to build our VSMs,
with vectors for both phrases and words,
and make them publicly available.
1 Introduction
Vector space models (VSMs) of word semantics
use large collections of text to represent word
meanings. Each word vector is composed of fea-
tures, where features can be derived from global
corpus co-occurrence patterns (e.g. how often a
word appears in each document), or local corpus
co-occurrence patterns patterns (e.g. how often
two words appear together in the same sentence,
or are linked together in dependency parsed sen-
tences). These two feature types represent dif-
ferent aspects of word meaning (Murphy et al,
2012c), and can be compared with the paradig-
matic/syntagmatic distinction (Sahlgren, 2006).
Global patterns give a more topic-based mean-
ing (e.g. judge might appear in documents also
containing court and verdict). Certain local pat-
terns give a more type-based meaning (e.g. the
noun judge might be modified by the adjective
harsh, or be the subject of decide, as would related
and substitutable words such as referee or con-
ductor). Global patterns have been used in Latent
Semantic Analysis (Landauer and Dumais, 1997)
and LDA Topic models (Blei et al, 2003). Local
patterns based on word co-occurrence in a fixed
width window were used in Hyperspace Analogue
to Language (Lund and Burgess, 1996). Subse-
quent models added increasing linguistic sophisti-
cation, up to full syntactic and dependency parses
(Lin, 1998; Pado? and Lapata, 2007; Baroni and
Lenci, 2010).
In this paper we systematically explore the util-
ity of a global, topic-based VSM built from what
we call Document features, and a local, type-based
VSM built from Dependency features. Our Doc-
ument VSM represents each word w by a vector
where each feature is a specific document, and the
feature value is the number of mentions of word
w in that document. Our Dependency VSM rep-
resents word w with a vector where each feature
is a dependency parse link (e.g., the word w is the
subject of the verb ?eat?), and the feature value is
the number of instances of this dependency fea-
ture for word w across a large text corpus. We
also consider a third Combined VSM in which
the word vector is the concatenation of its Doc-
ument and Dependency features. All three mod-
els subsequently normalize frequencies using pos-
itive pointwise mutual-information (PPMI), and
84
are dimensionality reduced using singular value
decomposition (SVD). This is the first systematic
study of the utility of Document and Dependency
features for semantic composition. We construct
all three VSMs (Dependencies, Documents, Com-
bined) using the same text corpus and preprocess-
ing pipeline, and make the resulting VSMs avail-
able for download (http://www.cs.cmu.
edu/?afyshe/papers/conll2013/). To
our knowledge, this is the first freely available
VSM that includes entries for both words and
adjective-noun phrases, and it is built from a much
larger corpus than previously shared resources (16
billion words, 50 million documents). Our main
contributions include:
? We systematically study complementarity of
topical (Document) and type (Dependency)
features in Vector Space Model (VSM)
for semantic composition of adjective-noun
phrases. To the best of our knowledge, this is
one of the first studies of this kind.
? Through extensive experiments on standard
benchmark datasets, we find that a VSM built
from a combination of topical and type fea-
tures is more effective for semantic compo-
sition, compared to a VSM built from Docu-
ment and Dependency features alone.
? We introduce a novel task: to predict the vec-
tor representation of a composed phrase from
the brain activity of human subjects reading
that phrase.
? We explore two composition methods, addi-
tion and dilation, and find that while addition
performs well on corpus-only tasks, dilation
performs best on the brain activity task.
? We build our VSMs, for both phrases and
words, from a large syntactically parsed text
corpus of 16 billion tokens. We also make
the resulting VSM publicly available.
2 Related Work
Mitchell and Lapata (2010) explored several
methods of combining adjective and noun vec-
tors to estimate phrase vectors, and compared
the similarity judgements of humans to the sim-
ilarity of their predicted phrase vectors. They
found that for adjective-noun phrases, type-based
models outperformed Latent Dirichlet Allocation
(LDA) topic models. For the type-based mod-
els, multiplication performed the best, followed
by weighted addition and a dilation model (for de-
tails on composition functions see Section 4.2).
However, Mitchell and Lapata did not combine
the topic- and type-based models, an idea we ex-
plore in detail in this paper.
Baroni and Zamparelli (2010) extended the typ-
ical vector representation of words. Their model
used matrices to represent adjectives, while nouns
were represented with column vectors. The vec-
tors for nouns and adjective-noun phrases were
derived from local word co-occurrence statistics.
The matrix to represent the adjective was esti-
mated with partial least squares regression where
the product of the learned adjective matrix and
the observed noun vector should equal the ob-
served adjective-noun vector. Socher et al (2012)
also extended word representations beyond sim-
ple vectors. Their model assigns each word a vec-
tor and a matrix, which are composed via an non-
linear function (e.g. tanh) to create phrase rep-
resentations consisting of another vector/matrix
pair. This process can proceed recursively, follow-
ing a parse tree to produce a composite sentence
meaning. Other general semantic composition
frameworks have been suggested, e.g. (Sadrzadeh
and Grefenstette, 2011) who focus on the opera-
tional nature of composition, rather than the rep-
resentations that are supplied to the framework.
Here we focus on creating word representations
that are useful for semantic composition.
Turney (2012) published an exploration of the
impact of domain- and function-specific vector
space models, analogous to the topic and type
meanings encoded by our Document and Depen-
dency models respectively. In Turney?s work,
domain-specific information was represented by
noun token co-occurrence statistics within a lo-
cal window, and functional roles were repre-
sented by generalized token/part-of-speech co-
occurrence patterns with verbs - both of which
are relatively local and shallow when compared
with this work. Similar local context-based fea-
tures were used to cluster phrases in (Lin and Wu,
2009). Though the models discussed here are
not entirely comparable to it, a recent comparison
suggested that broader, deeper features such as
ours may result in representations that are superior
for tasks involving neural activation data (Murphy
et al, 2012b).
85
In contrast to the composite model in (Griffiths
et al, 2005), in this paper we explore the com-
plementarity of semantics captured by topical in-
formation and syntactic/semantic types. We fo-
cus on learning VSMs (involving both words and
phrases) for semantic composition, and use more
expressive dependency-based features in our type-
based VSM. A comparison of vector-space repre-
sentations was recently published (Blacoe and La-
pata, 2012), in which the authors compared sev-
eral methods of combining single words vectors
to create phrase vectors. They found that the best
performance for adjective-noun composition used
point-wise multiplication and a model based on
type-based word co-occurrence patterns.
3 Creating a Vector-Space
To create the Dependency vectors, a 16 billion
word subset of ClueWeb09 (Callan and Hoy,
2009) was dependency parsed using the Malt
parser (Hall et al, 2007). Dependency statistics
were then collected for a predetermined list of
target words and adjective-noun phrases, and for
arbitrary adjective-noun phrases observed in the
corpus. The list was composed of the 40 thou-
sand most frequent single tokens in the Ameri-
can National Corpus (Ide and Suderman, 2006),
and a small number of words and phrases used
as stimuli in our brain imaging experiments. Ad-
ditionally, we included any phrase found in the
corpus whose maximal token span matched the
PoS pattern J+N+, where J and N denote adjec-
tive and noun PoS tags respectively. For each
unit (i.e., word or phrase) in this augmented list,
counts of all unit-external dependencies incident
on the head word were aggregated across the cor-
pus, while unit-internal dependencies were ig-
nored. Each token was appended with its PoS tag,
and the dependency edge label was also included.
This resulted in the extraction of 498 million de-
pendency tuples. For example, the dependency tu-
ple (a/DT, NMOD, 27-inch/JJ television/NN,14),
indicates that a/DT was found as a child of 27-
inch/JJ television/NN with a frequency of 14 in
the corpus.
To create Document vectors, word-document
co-occurrence counts were taken from the same
subset of Clueweb, which covered 50 million doc-
uments. We applied feature-selection for compu-
tational efficiency reasons, ranking documents by
the number of target word/phrase types they con-
tained and choosing the top 10 million.
A series of three additional filtering steps
selected target words/phrases, and Docu-
ment/Dependency features for which there was
adequate data.1 First, a co-occurrence frequency
cut-off was used to reduce the dimensionality
of the matrices, and to discard noisy estimates.
A cutoff of 20 was applied to the dependency
counts, and of 2 to document counts. Positive
pointwise-mutual-information (PPMI) was used
as an association measure to normalize the
observed co-occurrence frequency for the varying
frequency of the target word and its features,
and to discard negative associations. Second, the
target list was filtered to the 57 thousand words
and phrases which had at least 20 non-?stop
word? Dependency co-occurrence types, where
a ?stop word? was one of the 100 most frequent
Dependency features observed (so named be-
cause the dependencies were largely incident on
function words). Third, features observed for
no more than one target were removed, as were
empty target entries. The result was a Document
co-occurrence matrix of 55 thousand targets by
5.2 million features (total 172 million non-zero
entries), and a Dependency matrix of 57 thousand
targets by 1.25 million features (total 35 million
non-zero entries).
A singular value decomposition (SVD) matrix
factorization was computed separately on the De-
pendency and Document statistics matrices, with
1000 latent dimensions retained. For this step
we used Python/Scipy implementation of the Im-
plicitly Restarted Arnoldi method (Lehoucq et al,
1998; Jones et al, 2001). This method is com-
patible with PPMI normalization, since a zero
value represents both negative target-feature asso-
ciations, and those that were not observed or fell
below the frequency cut-off. To combine Docu-
ment and Dependency information, we concate-
nate vectors.
4 Experiments
To evaluate how Document and Dependency di-
mensions can interact and compliment each other,
1In earlier experiments with more than 500 thousand
phrasal entries, we found that the majority of targets were
dominated by non-distinctive stop word co-occurrences, re-
sulting in semantically vacuous representations.
86
Table 1: The nearest neighbors of three queries under three VSMs: all 2000 dimensions (Deps & Docs);
1000 Document dimensions (Docs); 1000 Dependency dimensions (Deps).
Query Deps & Docs Docs Deps
beautiful/JJ wonderful/JJ wonderful/JJ lovely/JJ
lovely/JJ fantastic/JJ gorgeous/JJ
excellent/JJ unspoiled/JJ wonderful/JJ
dog/NN cat/NN dogs/NNS cat/NN
dogs/NNS vet/NN the/DT dog/NN
pet/NN leash/NN dogs/NNS
bad/JJ publicity/NN negative/JJ publicity/NN fast/JJ cash/NN loan/NN negative/JJ publicity/NN
bad/JJ press/NN small/JJ business/NN loan/NN bad/JJ press/NN
unpleasantness/NN important/JJ cities/NNS unpleasantness/NN
Concrete Cats Mixed Cats Concrete Sim Mixed Sim Mixed Related0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1Performance of Documents and Dependency Dimensions for Single Word Tasks
Task
Per
form
anc
e
 
 Docs onlyDeps onlyDocs & Deps
Figure 1: Performance of VSMs for single word
behavioral tasks as we vary Document and Depen-
dency inclusion.
we can perform a qualitative comparison between
the nearest neighbors (NNs) of words and phrases
in the three VSMs ? Dependency, Document, and
Combined (Dependency & Document). Results
appear in Table 1. Note that single words and
phrases can be neighbors of each other, demon-
strating that our VSMs can generalize across syn-
tactic types. In the Document VSM, we get more
topically related words as NNs (e.g., vet and leash
for dog); and in the Dependency VSM, we see
words that might substitute for one another in a
sentence (e.g., gorgeous for beautiful). The two
feature sets can work together to up-weight the
most suitable NNs (as in beautiful), or help to
drown out noise (as in the NNs for bad publicity
in the Document VSM).
4.1 Judgements of Word Similarity
As an initial test of the informativeness of Doc-
ument and Dependency features, we evaluate
the representation of single words. Behavioral
judgement benchmarks have been widely used to
evaluate vector space representations (Lund and
Burgess, 1996; Rapp, 2003; Sahlgren, 2006).
Here we used five such tests. Two tests are catego-
rization tests, where we evaluate how well an au-
tomatic clustering of our word vectors correspond
to pre-defined word categories. The first ?Con-
crete Categories? test-set consists of 82 nouns,
each assigned to one of 10 concrete classes (Battig
and Montague, 1969). The second ?Mixed Cat-
egories? test-set contains 402 nouns in a range
of 21 concrete and abstract classes from Word-
Net (Almuhareb and Poesio, 2004; Miller et al,
1990). Both categorization tests were performed
with the Cluto clustering package (Karypis, 2003)
using cosine distances. Success was measured as
percentage purity over clusters based on their plu-
rality class, with chance performance at 10% and
5% respectively for the ?Concrete Categories? and
?Mixed Categories? tests.
The remaining three tests use group judgements
of similarity: the ?Concrete Similarity? set of
65 concrete word pairs (Rubenstein and Goode-
nough, 1965); and two variations on the Word-
Sim353 test-set (Finkelstein et al, 2002), par-
titioned into subsets corresponding to strict at-
tributional similarity (?Mixed Similarity?, 203
noun pairs), and broader topical ?relatedness?
(?Mixed Relatedness?, 252 noun pairs) (Agirre et
al., 2009). Performance on these benchmarks is
Spearman correlation between the aggregate hu-
man judgements and pairwise cosine distances of
word vectors in a VSM.
The results in Figure 1 show that the Depen-
dency VSM substantially outperforms the Docu-
ment VSM when predicting human judgements of
strict attributional (categorial) similarity (?Simi-
larity? as opposed to ?Relatedness?) for concrete
nouns. Conversely the Document VSM is compet-
87
Figure 2: The performance of three phrase representations for predicting the behavioral phrasal similar-
ity scores from Mitchell and Lapata (2010). The highest correlation is 0.5033 and uses 25 Document
dimensions, 600 Dependency dimensions and the addition composition function.
itive for less concrete word types, and for judge-
ments of broader topical relatedness.
4.2 Judgements of Phrase Similarity
We also evaluated our system on behavioral data
of phrase similarity judgements gathered from 18
human informants. The adjective-noun phrase
pairs are divided into 3 groups: high, medium
and low similarity (Mitchell and Lapata, 2010).
For each pair of phrases, informants rated phrase
similarity on a Likert scale of 1-7. There are 36
phrase pairs in each of the three groups for a to-
tal of 108 phrase pairs. Not all of the phrases oc-
curred frequently enough in our corpus to pass our
thresholds, and so were omitted from our analy-
sis. In several cases we also used pluralizations
of the test phrases (e.g.?dark eyes?) where the
singular form was not found in our VSM. After
these changes we were left with 28, 24 and 28
in the high, medium and low groups respectively.
In total we have 80 observed vectors for the 108
phrase pairs. These adjective-noun phrases were
included in the list of targets, so their statistics
were gathered in the same way as for single words.
This does not impact results for composed vectors,
as all of the single words in the phrases do appear
in our VSMs. A full list of the phrase pairs can be
found in Mitchell and Lapata (2010).
To evaluate, we used three different representa-
tions of phrases. For phrase pairs that passed our
thresholds, we can test the similarity of observed
representations by comparing the VSM represen-
tation of the phrase (no composition function).
For all 108 phrase pairs we can test the composed
phrase representations, derived by applying addi-
tion and dilation operations to word vectors. Mul-
tiplication is not used as SVD representations in-
clude negative values, and so the product of two
negative values would be positive.
Addition is the element-wise sum of two se-
mantic feature vectors saddi = sadji +snouni , where
snouni , sadji , and saddi are the ith element of the
noun, adjective, and predicted phrase vectors, re-
spectively. Dilation of two semantic feature vec-
tors sadj and snoun is calculated by first decom-
posing the noun into a component parallel to the
adjective (x) and a component perpendicular to
the adjective (y) so that snoun = x + y. Dilation
then enhances the adjective component by multi-
plying it by a scalar (?): sdilate = ?x+y. This can
be viewed as taking the representation of the noun,
and up-weighting the elements it shares with the
adjective, which is coherent with the notion of co-
composition (Pustejovsky, 1995). Previous work
(Mitchell and Lapata, 2010) tuned the ? parame-
ter (? = 16.7). We use that value here, though
further optimization might increase performance.
For our evaluation we calculated the cosine dis-
tance between pairs of phrases in the three dif-
ferent representation spaces: observed, addition
and dilation. Results for a range of dimension-
ality settings appear in Figure 2. In the observed
space, we maximized performance when we in-
88
cluded all 1000 of the Document and 350 Depen-
dency dimensions. For consistency the y axis in
Figure 2 extends only to 100 Document dimen-
sions: changes beyond 100 dimensions for ob-
served vectors were minimal. By design, SVD
will tend to use lower dimensions to represent the
strongest signals in the input statistics, which typ-
ically originate in the types of targets that are most
frequent ? in this case single words. We have ob-
served that less frequent and noisier counts, as
might be found for many phrases, are displaced
to the higher dimensions. Consistent with this ob-
servation, maximum performance occurs using a
high number of dimensions (correlation of 0.37 to
human judgements of phrase similarity).
Interestingly, using the single word vectors to
predict the phrase vectors via the addition function
gives the best correlation of any of the represen-
tations, outperforming even the observed phrase
representations. When using 25 Document di-
mensions and 600 Dependency dimensions the
correlation is 0.52, compared to the best per-
formance of 0.51 using Dependency dimensions
only. We speculate that the advantage of com-
posed vectors over observed vectors is due to
sparseness and resulting noise/variance in the ob-
served phrase vectors, as phrases are necessarily
less frequent than their constituent words.
The dilation composition function performs
slightly worse than addition, but shows best per-
formance at the same point as addition. Here, the
highest correlation (0.46) is substantially lower
than that attained by addition, and uses 25 dimen-
sions of the Document, and 600 dimensions of the
Dependency VSM.
To summarize, without documents, {observed,
addition and dilation} phrase vectors have maxi-
mal correlations {0.37, 0.51 and 0.46}. With doc-
uments, {observed, addition and dilation} phrase
vectors have maximal correlations {0.37, 0.52 and
0.50}. Our results using the addition function
(0.52) outperform the results in two previous stud-
ies (Mitchell and Lapata, 2010; Blacoe and Lap-
ata, 2012): (0.46 and 0.48 respectively). This is
evidence that a VSM built from a larger corpus,
and with both Document and Dependency infor-
mation can yield superior results.
4.3 Composed vs Observed Phrase Vectors
Next we tested how well our representations and
semantic composition functions could predict the
observed vector statistics for phrases from the
vectors of their component words. Again, we
explored addition and dilation composition func-
tions. For testing we have 13, 575 vectors for
which both the adjective and noun passed our
thresholds. We predicted a composed phrase vec-
tor using the statistics of the single words and
one of the two composition functions (addition
or dilation). We then sorted the list of observed
phrase vectors by their distance to the composed
phrase vector and recorded the position of the
corresponding observed vector in the list. From
this we calculated percentile rank, the percent of
phrases that are further from the predicted vec-
tor than the observed vector. Percentile rank is:
100 ? (1 ? ?rank/N) where ?rank is the aver-
age position of the correct observed vector in the
sorted list and N = 13, 575 is the size of the list.
Figure 3 shows the changes in percentile rank
in response to varying dimensions of Documents
and Dependencies for the addition function. Di-
lation results are not shown, but the pattern of
performance is very similar. In general, when
one includes more Document dimensions, the per-
centile rank increases. For both the dilation and
addition composition functions the peak perfor-
mance is with 750 Dependency dimensions and
1000 Document dimensions. Dilation?s peak per-
formance is 97.87; addition peaks at 98.03 per-
centile rank. As in Section 4.2, we see that the
accurate representation of phrases requires higher
SVD dimensions.
To evaluate when composition fails, we ex-
amined the cases where the percentile rank was
< 25%. Amongst these words we found an over-
representation of operational adjectives like ?bet-
ter? and ?more?. As observed previously, it is
possible that such adjectives could be better rep-
resented with a matrix or function (Socher et al,
2012; Baroni and Zamparelli, 2010). Composi-
tion may also be failing when the adjective-noun
phrase is non-compositional (e.g. lazy susan); fil-
tering such phrases could improve performance.
4.4 Brain Activity Data
Here we explore the relationship between the neu-
ral activity observed when a person reads a phrase,
89
100 250 500 750 100093
93.5
94
94.5
95
95.5
96
96.5
97
97.5
98
Number of Dependency Dimensions
Per
cen
tile 
Ran
k
Percentile Rank for Varing Doc. and Dep. Dimensions (Addition)
 
 
0 Doc Dims25501005007501000
Figure 3: The percentile rank of observed phrase
vectors compared to vectors created using the ad-
dition composition function.
and our predicted composed VSM for that phrase.
We collected brain activity data using Magnetoen-
cephalography (MEG). MEG is a brain imaging
method with much higher temporal resolution (1
ms) than fMRI (?2 sec). Since words are natu-
rally read at a rate of about 2 per second, MEG is a
better candidate for capturing the fast dynamics of
semantic composition in the brain. Some previous
work has explored adjective-noun composition in
the brain (Chang et al, 2009), but used fMRI and
corpus statistics based only on co-occurrence with
5 hand-selected verbs.
Our MEG data was collected while 9 partici-
pants viewed 38 phrases, each repeated 20 times
(randomly interleaved). The stimulus nouns were
chosen because previous research had shown them
to be decodable from MEG recordings, and the ad-
jectives were selected to modulate their most de-
codable semantic properties (e.g. edibility, ma-
nipulability) (Sudre et al, 2012). The 8 adjec-
tives selected are (?big?, ?small?, ?ferocious?,
?gentle?, ?light?, ?heavy?, ?rotten?, ?tasty?), and
the 6 nouns are (?dog?, ?bear?, ?tomato?, ?car-
rot?, ?hammer?, ?shovel?). The words ?big? and
?small? are paired with every noun, ?ferocious?
and ?gentle? with animals, ?light? and ?heavy?
with tools and ?rotten? and ?tasty? with foods.
We also included the words ?the? and the word
?thing? as semantically neutral fillers, to present
each of the words in a condition without seman-
tic modulation. In total there are 38 phrases (e.g.
?rotten carrot?, ?big hammer?).
In the MEG experiment, the adjective and
paired noun were each shown for 500ms, with a
300ms interval between them, and there were 3
Figure 4: Results for predicting composed phrase
vectors (addition [4a] and dilation [4b]) from
MEG recordings. Results shown are the aver-
age over 9 subjects viewing 38 adjective-noun
phrases. This is the one task on which dilation
outperforms addition.
(a) Addition composition function results.
(b) Dilation composition function results.
seconds in total time between the onset of subse-
quent phrases. Data was preprocessed to maxi-
mize the signal/noise ratio as is common practice
? see Gross et al, (2012). The 20 repeated trials
for each phrase were averaged together to create
one average brain image per phrase.
To determine if the recorded MEG data can be
used to predict our composed vector space rep-
resentations, we devised the following classifica-
tion framework.2 The training data is comprised
of the averaged MEG signal for each of the 38
phrases for one subject, and the labels are the 38
phrases. We use our VSMs and composition func-
tions to form a mapping of the 38 phrases to com-
2Predicting brain activity from VSM representations is
also possible, but provides additional challenges, as parts of
the observed brain activity are not driven by semantics.
90
posed semantic feature vectors w ? {s1 . . . sm}.
The mapping allows us to use Zero Shot Learn-
ing (Palatucci et al, 2009) to predict novel phrases
(not seen during training) from a MEG record-
ing. This is a particularly attractive characteris-
tic for the task of predicting words, as there are
many words and many more phrases in the En-
glish language, and one cannot hope to collect
MEG recordings for all of them.
Formally, let us define the semantic represen-
tation of a phrase w as semantic feature vector
~sw = {s1...sm}, where the semantic space has
dimensionm that varies depending on the number
of Document and/or Dependency dimensions we
include. We utilize the mapping w ? {s1 . . . sm}
to train m independent functions f1(X) ?
s?1, . . . , fm(X) ? s?m where s? represents the
value of a predicted composed semantic feature.
We combine the output of f1 . . . fm to create the
final predicted semantic vector ~s? = {s?1 . . . s?m}.
We use cosine distance to quantify the distance be-
tween true and predicted semantic vectors.
To measure performance we use the 2 vs. 2 test.
For each test we withhold two phrases and train
regressors on the remaining 36. We use the re-
gressors f and MEG data from the two held out
phrases to create two predicted semantic vectors.
We then choose the assignment of predicted se-
mantic vectors (~s?i and ~s?j) to true semantic vec-
tors (~si and ~sj) that minimizes the sum of cosine
distances. If we choose the correct assignment
(~s?i 7? ~si and ~s?j 7? ~sj) we mark the test as cor-
rect. 2 vs. 2 accuracy is the number of 2 vs. 2
tests with correct assignments divided by the total
number of tests. There are (38 choose 2) = 703
distinct 2 vs. 2 tests, and we evaluate on the subset
for which neither the adjective nor noun are shared
(540 pairs). Chance performance is 0.50.
For each f we trained a regressor with L2
penalty. We tune the regularization parame-
ter with leave-one-out-cross-validation on training
data. We train regressors using the first 800 ms of
MEG signal after the noun stimulus appears, when
we assume semantic composition is taking place.
Results appear in Figure 4. The best perfor-
mance (2 vs. 2 accuracy of 0.9440) is achieved
with dilation, 800 dimensions of Dependencies
and zero Document dimensions. When we use
the addition composition function, optimal per-
formance is 0.9212, at 600 Dependency and zero
Document dimensions. Note, however, that the
parameter search here was much coarser that in
Sections 4.2 and 4.3, due to the computation re-
quired. We used a finer grid around the peaks in
performance for addition and dilation and found
minimal improvement (?0.5%) with the addition
of a small number of Document dimensions.
It is intriguing that this neurosemantic task is
the only task for which dilation outperforms addi-
tion. All other composition tasks explored in this
study were concerned with matching composed
word vectors to observed or composed word vec-
tors, whereas here we are interested in matching
composed word vectors to observed brain activity.
Perhaps the brain works in a manner more akin to
the emphasis of elements as modeled by dilation,
rather than a summing of features. Further work
is required to fully understand this phenomenon,
but this is surely a thought-provoking result.3
5 Conclusion
We have performed a systematic study of comple-
mentarity of topical (Document) and type (Depen-
dency) features in Vector Space Model (VSM) for
semantic composition of adjective-noun phrases.
To the best of our knowledge, this is one of the
first such studies of this kind. Through experi-
ments on multiple real world benchmark datasets,
we demonstrated the benefit of combining topic-
and type-based features in a VSM. Additionally,
we introduced a novel task of predicting vec-
tor representations of composed phrases from the
brain activity of human subjects reading those
phrases. We exploited a large syntactically parsed
corpus to build our VSM models, and make them
publicly available. We hope that the findings and
resources from this paper will serve to inform fu-
ture work on VSMs and semantic composition.
Acknowledgment
We are thankful to the anonymous reviewers for their con-
structive comments. We thank CMUs Parallel Data Labo-
ratory (PDL) for making the OpenCloud cluster available,
Justin Betteridge (CMU) for his help with parsing the corpus,
and Yahoo! for providing the M45 cluster. This research has
been supported in part by DARPA (under contract number
FA8750-13-2-0005), NIH (NICHD award 1R01HD075328-
01), Keck Foundation (DT123107), NSF (IIS0835797), and
Google. Any opinions, findings, conclusions and recommen-
dations expressed in this paper are the authors and do not
necessarily reflect those of the sponsors.
3No pun intended.
91
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-
ova, and Marius Pas. 2009. A study on similarity and
relatedness using distributional and WordNet-based ap-
proaches. Proceedings of NAACL-HLT 2009.
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: An evalua-
tion. In Proceedings of EMNLP, pages 158?165.
Marco Baroni and Alessandro Lenci. 2010. Distributional
memory: A general framework for corpus-based seman-
tics. Computational Linguistics, 36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1183?1193. Association
for Computational Linguistics.
W F Battig and W E Montague. 1969. Category Norms
for Verbal Items in 56 Categories: A Replication and Ex-
tension of the Connecticut Category Norms. Journal of
Experimental Psychology Monographs, 80(3):1?46.
William Blacoe and Mirella Lapata. 2012. A Comparison of
Vector-based Representations for Semantic Composition.
In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 546?556, Jeju
Island, Korea.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine Learning
Research, 3(4-5):993?1022.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09 Dataset.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Kai-min Chang, Vladimir L. Cherkassky, Tom M Mitchell,
and Marcel Adam Just. 2009. Quantitative modeling of
the neural representation of adjective-noun phrases to ac-
count for fMRI activation. In Proceedings of the Annual
Meeting of the ACL and the 4th IJCNLP of the AFNLP,
pages 638?646.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: the concept revisited.
ACM Transactions on Information Systems, 20(1):116?
131.
Thomas L Griffiths, Mark Steyvers, David M Blei, and
Joshua B Tenenbaum. 2005. Integrating topics and syn-
tax. Advances in neural information processing systems,
17.
Joachim Gross, Sylvain Baillet, Gareth R. Barnes,
Richard N. Henson, Arjan Hillebrand, Ole Jensen, Karim
Jerbi, Vladimir Litvak, Burkhard Maess, Robert Oost-
enveld, Lauri Parkkonen, Jason R. Taylor, Virginie van
Wassenhove, Michael Wibral, and Jan-Mathijs Schoffe-
len. 2012. Good-practice for conducting and reporting
MEG research. NeuroImage, October.
J Hall, J Nilsson, J Nivre, G Eryigit, B Megyesi, M Nilsson,
and M Saers. 2007. Single Malt or Blended? A Study
in Multilingual Parser Optimization. In Proceedings of
the CoNLL Shared Task Session of EMNLPCoNLL 2007,
volume s. 19-33, pages 933?939. Association for Compu-
tational Linguistics.
Nancy Ide and Keith Suderman. 2006. The American Na-
tional Corpus First Release. Proceedings of the Fifth Lan-
guage Resources and Evaluation Conference (LREC).
Eric Jones, Travis Oliphant, Pearu Peterson, and others.
2001. SciPy: Open source scientific tools for Python.
George Karypis. 2003. CLUTO: A Clustering Toolkit.
Technical Report 02-017, Department of Computer Sci-
ence, University of Minnesota.
T Landauer and S Dumais. 1997. A solution to Plato?s prob-
lem: the latent semantic analysis theory of acquisition, in-
duction, and representation of knowledge. Psychological
Review, 104(2):211?240.
R B Lehoucq, D C Sorensen, and C Yang. 1998. Arpack
users? guide: Solution of large scale eigenvalue problems
with implicitly restarted Arnoldi methods. SIAM.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for
discriminative learning. In Proceedings of the ACL.
Dekang Lin. 1998. Automatic Retrieval and Clustering of
Similar Words. In COLING-ACL, pages 768?774.
K Lund and C Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Behavior
Research Methods, Instruments, and Computers, 28:203?
208.
George A Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Introduction
to WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4):235?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive science,
34(8):1388?429, November.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012a.
Comparing Abstract and Concrete Conceptual Represen-
tations using Neurosemantic Decoding. In NAACL Work-
shop on Cognitive Modelling and Computational Linguis-
tics.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012b.
Selecting Corpus-Semantic Models for Neurolinguistic
Decoding. In First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pages 114?123, Montreal,
Quebec, Canada.
Brian Murphy, Partha Pratim Talukdar, and Tom Mitchell.
2012c. Learning Effective and Interpretable Semantic
Models using Non-Negative Sparse Embedding. In Inter-
national Conference on Computational Linguistics (COL-
ING 2012), Mumbai, India.
S Pado? and M Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Linguis-
tics, 33(2):161?199.
92
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau, and
Tom M Mitchell. 2009. Zero-Shot Learning with Se-
mantic Output Codes. Advances in Neural Information
Processing Systems, 22:1410?1418.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge.
Reinhard Rapp. 2003. Word Sense Discovery Based on
Sense Descriptor Dissimilarity. Proceedings of the Ninth
Machine Translation Summit, pp:315?322.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633, October.
Mehrnoosh Sadrzadeh and Edward Grefenstette. 2011. A
Compositional Distributional Semantics Two Concrete
Constructions and some Experimental Evaluations. Lec-
ture Notes in Computer Science, 7052:35?47.
Magnus Sahlgren. 2006. The Word-Space Model: Using dis-
tributional analysis to represent syntagmatic and paradig-
matic relations between words in high-dimensional vector
spaces. Dissertation, Stockholm University.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic Compositionality
through Recursive Matrix-Vector Spaces. In Conference
on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila We-
hbe, Alona Fyshe, Riitta Salmelin, and Tom Mitchell.
2012. Tracking Neural Coding of Perceptual and Seman-
tic Features of Concrete Nouns. NeuroImage, 62(1):463?
451, May.
Peter D Turney. 2012. Domain and Function : A Dual-Space
Model of Semantic Relations and Compositions. Journal
of Artificial Intelligence Research, 44:533?585.
93
