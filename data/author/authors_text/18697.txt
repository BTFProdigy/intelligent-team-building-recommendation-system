Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1933?1942,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Identifying Manipulated Offerings on Review Portals
Jiwei Li
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
jiweil@cs.cmu.edu
Myle Ott Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853, USA
myleott,cardie@cs.cornell.edu
Abstract
Recent work has developed supervised meth-
ods for detecting deceptive opinion spam?
fake reviews written to sound authentic and
deliberately mislead readers. And whereas
past work has focused on identifying individ-
ual fake reviews, this paper aims to identify
offerings (e.g., hotels) that contain fake re-
views. We introduce a semi-supervised man-
ifold ranking algorithm for this task, which
relies on a small set of labeled individual re-
views for training. Then, in the absence of
gold standard labels (at an offering level),
we introduce a novel evaluation procedure
that ranks artificial instances of real offer-
ings, where each artificial offering contains a
known number of injected deceptive reviews.
Experiments on a novel dataset of hotel re-
views show that the proposed method outper-
forms state-of-art learning baselines.
1 Introduction
Consumers increasingly rely on user-generated
online reviews when making purchase deci-
sions (Cone, 2011; Ipsos, 2012). Unfortunately,
the ease of posting content to the Web, potentially
anonymously, combined with the public?s trust and
growing reliance on opinions and other information
found online, create opportunities and incentives for
unscrupulous businesses to post deceptive opinion
spam?fraudulent or fictitious reviews that are
deliberately written to sound authentic, in order to
deceive the reader (Ott et al 2011).
Unlike other kinds of spam, such as
Web (Martinez-Romo and Araujo, 2009; Castillo
et al 2006) and e-mail spam (Chirita et al 2005),
recent work has found that deceptive opinion spam
is neither easily ignored nor easily identified by
human readers (Ott et al 2011). Accordingly, there
is growing interest in developing automatic, usually
learning-based, methods to help users identify
deceptive opinion spam (see Section 2). Even
in fully-supervised settings, however, automatic
methods are imperfect at identifying individual
deceptive reviews, and erroneously labeling genuine
reviews as deceptive may frustrate and alienate
honest reviewers.
An alternative approach, not yet considered in
previous work, is to instead identify those prod-
uct or service offerings where fake reviews appear
with high probability. For example, a hotel manager
may post fake positive reviews to promote their own
hotel, or fake negative reviews to demote a com-
petitor?s hotel. In both cases, rather than identify-
ing these deceptive reviews individually, it may be
preferable to identify the manipulated offering (i.e.,
the hotel) so that review portal operators, such as
TripAdvisor or Yelp, can further investigate the sit-
uation without alienating users.1
Accordingly, this paper addresses the novel task
of identifying manipulated offerings, which we
frame as a ranking problem, where the goal is to rank
offerings by the proportion of their reviews that are
believed to be deceptive. We propose a novel three-
layer graph model, based on manifold ranking (Zhou
et al 2003a; 2003b), to jointly model deceptive lan-
guage at the offering-, review- and term-level. In
particular, rather than treating reviews within the
same offering as independent units, there is a rein-
forcing relationship between offerings and reviews.
1Manipulating online reviews may also have legal conse-
quences. For example, the Federal Trade Commission (FTC)
has updated their guidelines on the use of endorsements and
testimonials in advertising to suggest that posting deceptive re-
views may be unlawful in the United States (FTC, 2009).
1933
Figure 1: Mutual Reinforcement Graph Model for Hotel
Ranking using the Manifold-Ranking Method
Our manifold ranking approach is semi-
supervised in that it requires no supervisory
information at the offering level; rather, it requires
only a small amount of labeled data at a review
level. Intuitively, and as depicted in Figure 1 for
hotel offerings, we represent hotels, reviews and
terms as nodes in a graph, where each hotel is
connected to its reviews, and each review, in turn, is
connected to the terms used within it. The influence
of labeled data is propagated along the graph to
unlabeled data, such that a hotel is considered more
deceptive if it is heavily linked with other deceptive
reviews, and a review, in turn, is more deceptive if it
is generated by a deceptive hotel.
The success of our semi-supervised approach fur-
ther depends on the ability to learn patterns of truth-
ful and deceptive reviews that generalize across re-
views of different offerings. This is challenging, be-
cause reviews often contain offering-specific vocab-
ulary. For example, reviews of hotels in Los Angeles
are more likely to include keywords such as ?beach?,
?sea?, ?sunshine? or ?LA?, while reviews of Juneau
hotels may contain ?glacier?, ?Juneau?, ?bear? or
?aurora borealis.? A hotel review might also men-
tion the hotel?s restaurant or bar by name.
Unfortunately, it is unclear how important (or
detrimental) offering-specific features are when de-
ciding whether a review is fake. Accordingly, we
propose a dimensionality-reduction approach, based
on Latent Dirichlet Allocation (LDA) (Blei et al
2003), to obtain a vector representation of reviews
for the ranking algorithm that generalizes across re-
views of different offerings. Specifically, we train
an LDA-based topic model to view each review as a
mixture of aspect-, city-, hotel- and review-specific
topics (see Section 6). We then reduce the dimen-
sionality of our data (i.e., labeled and unlabeled re-
views) by replacing each review term vector with a
vector that corresponds to its term distribution over
just its aspect-specific topics, i.e., excluding city-,
hotel- and review-specific topics. We find that, com-
pared to models trained either on the full vocabulary,
or trained on standard LDA document-topic vectors,
this representation allows our models to generalize
better across reviews of different offerings.
We evaluate our approach on the task of identi-
fying (ranking) manipulated hotels. In particular, in
the absence of gold standard offering-level labels,
we introduce a novel evaluation procedure for this
task, in which we rank numerous versions of each
hotel, where each hotel version contains a differ-
ent number of injected, known deceptive reviews.
Thus, we expect hotel versions with larger propor-
tions of deceptive reviews to be ranked higher than
those with smaller proportions.
For labeled training data, we use the Ott et
al. (2011) dataset of 800 positive (5-star) reviews of
20 Chicago hotels (400 deceptive and 400 truthful).
For evaluation, we construct a new FOUR-CITIES
dataset, containing 40 deceptive and 40 truthful re-
views for each of eight hotels in four different cities
(640 reviews total), following the procedure out-
lined in Ott et al (2011). We find that our manifold
ranking approach outperforms several state-of-the-
art learning baselines on this task, including trans-
ductive Support Vector Regression. We addition-
ally apply our approach to a large-scale collection
of real-world reviews from TripAdvisor and explore
the resulting ranking.
In the sections below, we discuss related work
(Section 2) and describe the datasets used in this
work (Section 3), the dimensionality-reduction ap-
proach for representing reviews (Section 4), and the
semi-supervised manifold ranking approach (Sec-
tion 5). We then evaluate the methods quantitatively
(Sections 6 and 7) and qualitatively (Section 8).
2 Related Work
A number of recent approaches have focused on
identifying individual fake reviews or users who post
1934
fake reviews. For example, Jindal and Liu (2008)
train machine learning classifiers to identify dupli-
cate (or near duplicate) reviews. Yoo and Gretzel
(2009) gathered 40 truthful and 42 deceptive hotel
reviews and manually compare the psychologically
relevant linguistic differences between them. Lim et
al. (2010) propose an approach based on abnormal
user behavior to predict spam users, without using
any textual features. Ott et al (2011) solicit decep-
tive reviews from workers on Amazon Mechanical
Turk, and built a dataset containing 400 deceptive
and 400 truthful reviews, which they use to train
and evaluate supervised SVM classifiers. Ott et al
(2012) expand upon this work to estimate preva-
lences of deception in a review community. Mukher-
jee et al (2012) study spam produced by groups of
fake reviewers. Li et al (2013) use topic models
to detect differences between deceptive and truthful
topic-word distributions. In contrast, in this work we
aim to identify fake reviews at an offering level.2
LDA Topic Models. LDA topic models (Blei et
al, 2003) have been employed for many NLP tasks
in recent years. Here, we build on earlier work
that uses topic models to (a) separate background
information from information discussing the vari-
ous ?aspects? of products (e.g., Chemudugunta et
al. (2007)) and (b) identify different levels of infor-
mation (e.g., user-specific, location-specific, time-
specific) (Ramage et al, 2009).
Manifold Ranking Algorithm. The manifold-
ranking method (Zhou et al 2003a; Zhou et al
2003b) is a mutual reinforcement ranking approach
initially proposed to rank data points along their un-
derlying manifold structure. It has been widely used
in many different ranking applications, such as sum-
marization (Wan et al 2007; Wan and Yang, 2007).
3 Dataset
In this paper, we train all of our models using the
CHICAGO dataset of Ott et al(2011), which contains
20 deceptive and 20 truthful reviews from each of 20
Chicago hotels (800 reviews total). This dataset is
2Approaches for identifying individual fake reviews may be
applied to our task, for example, by averaging the review-level
predictions for an offering. This averaging approach is one of
our baselines in Section 7.
City Hotels
Chicago W Chicago, Palomar Chicago
New York Hotel Pennsylvania, Waldorf Astoria
Los Angeles
Sheraton Gateway,
The Westin Los Angeles Airport
Houston
Magnolia Hotel,
Crowne Plaza Houston River Oaks
Table 1: Details of our FOUR-CITIES evaluation data.
unique in that it contains known (gold standard) de-
ceptive reviews, solicited through Amazon Mechan-
ical Turk, and is publicly-available.3
Unfortunately, the CHICAGO dataset is limited,
both in size (800 reviews) and scope, in that it only
contains reviews of hotels in one city: Chicago.
Accordingly, in order to perform a more realistic
evaluation for our task, we construct a new dataset,
FOUR-CITIES, that contains 40 deceptive and 40
truthful reviews from each of eight hotels in four dif-
ferent cities (640 reviews total).
We build the FOUR-CITIES dataset using the same
procedure as Ott et al(2011), by creating and di-
viding 320 Mechanical Turk jobs, called Human-
Intelligence Tasks (HITs), evenly across eight of the
most popular hotels in our four chosen cities (see Ta-
ble 1). Each HIT presents a worker with the name of
a hotel and a link to the hotel?s website. Workers are
asked to imagine that they work for the marketing
department of the hotel and that their boss has asked
them to write a fake positive review, as if they were
a customer, to be posted on a travel review website.
Each worker is allowed to submit a single review,
and is paid $1 for an acceptable submission.
Finally, we augment our deceptive FOUR-CITIES
reviews with a matching set of truthful reviews from
TripAdvisor by randomly sampling 40 positive (5-
star) reviews for each of the eight chosen hotels.
While we cannot know for sure that the sampled re-
views are truthful, previous work has suggested that
rates of deception among popular hotels is likely to
be low (Jindal and Liu, 2008; Lim et al 2010).
4 Topic Models for Dimensionality
Reduction
As mentioned in the introduction, we want to learn
patterns of truthful and deceptive reviews that apply
3We use the dataset available at: http://www.cs.
cornell.edu/?myleott/op_spam.
1935
Figure 2: Graphical illustration of the RLDA topic model.
across hotels in different locations. This is challeng-
ing, however, because hotel reviews often contain
specific information about the hotel or city, and it
is unclear whether these features will generalize to
reviews of other hotels.
We therefore investigate an LDA-based
dimensionality-reduction approach (RLDA) to
derive effective vector representations of reviews.
Specifically, we model each document as a bag of
words, generated from a mixture of: (a) ?aspect?
topics (that discuss various dimensions of the
offering); (b) city-specific topics; (c) hotel-specific
topics; (d) review-specific topics;4 and (e) a back-
ground topic. We use this model to reduce the
dimensionality of the review representation in our
training and test sets, by replacing each review?s
term vector with a vector corresponding to the
distribution over only the aspect-based topics, i.e.,
we exclude city, hotel and review-specific topics, as
well as the background topic.
Below we present specific details of our model
(Sections 4.1 and 4.2). The effectiveness of our
dimensionality-reduction approach will be directly
evaluated in Section 6, by comparing the perfor-
mance of various classifiers trained either on the full
vocabulary, or on our reduced feature representation.
4.1 RLDA Model Details
The plate diagram and generative story for our
model are given in Figures 2 and 3, respectively.
Our model has a similar general structure to stan-
dard LDA, but with additional machinery to handle
different levels of information. In particular, in or-
der to model K aspects in a collection of R reviews,
4These will be terms used in just a small number of reviews.
? Draw ?B ? Dir(?)
? For each aspect z = 1, 2, ...,K: draw ?z ? Dir(?)
? For each city c = 1, 2, ..., C: draw ?c ? Dir(?)
? For each hotel h = 1, 2, ..., H: draw ?h ? Dir(?)
? For each review r:
? Draw pir ? Dir(?)
? Draw ?r ? Dir(?)
? Draw ?r ? Dir(?)
? For each word w in d:
? Draw yw ?Multi(pir)
? If yw = 0:
? Draw zw ?Multi(?)
? Draw w ?Multi(?zw )
? If yw = 1: draw w ?Multi(?B)
? If yw = 2: draw w ?Multi(?d)
? If yw = 3: draw w ?Multi(?h)
? If yw = 4: draw w ?Multi(?c)
Figure 3: Generative story for the RLDA topic model.
of H hotels, in C cities, we first draw multinomial
word distributions corresponding to: the background
topic, ?B; aspect topics, ?k for k ? [1,K]; review-
specific topics, ?r for r ? [1, R]; hotel-specific top-
ics, ?h for h ? [1, H]; and city-specific topics, ?c
for c ? [1, C]. Then, for each word w in review
R, we sample a switch variable, y ? [0, 4], indicat-
ing whether w comes from one of the aspect topics
(y = 0), or the background topic (y = 1), review-
specific topic (y = 2), hotel-specific topic (y = 3)
or city-specific topic (y = 4). If the word comes
from one of the aspect topics, then we further sam-
ple the specific aspect topic, zw ? [1,K]. Finally,
we generate the word, w, from the corresponding ?.
4.2 Inference for RLDA
Given the review collection, our goal is to find the
most likely assignment yw (and zw if yw = 0) for
each word, w, in each review. We perform infer-
ence using Gibbs sampling. It is relatively straight-
forward to derive Gibbs sampling equations that al-
low joint sampling of the zw and yw latent variables
for each word token w:
P (yw = 0, Zw = k) =
Nar,?w + ?
Nr,?w + 5?
?
Ckr,?w + ?
?
k C
k
r,?w +K?
?
Ewk + ??
w E
w
k + V ?
,
P (yw = m,m = 1, 2, 3, 4) =
Nmr,?w + ?
Nr,?w + 5?
?
Ewm + ??
w E
w
m + V ?
,
Note that the subscript ?w indicates that the
count for word token w is excluded. Also, Nr
1936
denotes the number of words in review r and
Nar,?w, N
1
r,?w, N
2
r,?w, N
3
r,?w, N
4
r,?w are the number of
words in review r assigned to the aspect, background,
review-specific, hotel-specific and city-specific topics, re-
spectively, excluding the current word. Ckr,?w denotes
the number of words in review r assigned to aspect topic
k. Ewk , E
w
1 , E
w
2 , E
w
3 , E
w
4 denote the number of times that
the word w is assigned to aspect k, the background topic,
review-specific topic r, hotel-specific topic h, and city-
specific topic c, respectively. We set hyperparameter ?
to 1, ? to 0.5, ? to 0.01. We run 200 iterations of Gibbs
sampling until the topic distribution stabilizes. After each
iteration in Gibbs sampling, we obtain:
piir =
N ir + ??
iN
i
r + 5?
?kr =
Ckr + ??
k C
k
r +K?
?wz =
Ewz + ??
w E
w
z + V ?
?wm =
Ewm + ??
w E
w
m + V ?
(1)
Finally, at the end of Gibbs sampling, we filter out
background, document-specific, hotel-specific and
city-specific information, by replacing each docu-
ment?s term vector with a 1?K aspect-topic vector,
~Gr = ??1r , ?
2
r , ? ? ? , ?
K
r ?.
5 Manifold Ranking for Hotels
In this section, we describe our ranking algorithm ?
based on manifold ranking (Zhou et al 2003a; Zhou
et al 2003b) ? that tries to jointly model deceptive
language at the hotel-, review- and term-level.
5.1 Graph Construction
We use a three-layer (hotel layer, review layer and
term layer) mutual reinforcement model (see Fig-
ure 1). Formally, we represent our three-layer graph
as G = ?VH , VR, VT , EHR, ERR, ERT , ETT ?,
where VH = {Hi}
i=NH
i=1 , VR = {R}
i=HR
i=1 and
VT = {Ti}i=Vi=1 correspond to the set of hotels, re-
views and terms respectively. EHR, ERR and ERT
respectively denote the edges between hotels and re-
views, reviews and reviews and reviews and terms.
Each edge is associated with a weight that denotes
the similarity between two nodes.
Let sim(Hi, Rj), where Hi ? VH and Rj ? VR,
denote the edge weight between hotelHi and review
Rj , calculated as follows:
sim(Hi, Rj) =
{
1 if Ri ? Hj
0 if Ri 6? Hj
(2)
Then we get row normalized matrices DHR ?
RNH?NR and DRH ? RNR?NH as follows:
DHR(i, j) =
sim(Hi, Rj)
?
i? sim(Hi? , Rj)
DRH(i, j) =
sim(Hi, Rj)
?
j? sim(Hi, Rj?)
(3)
As described in Section 4.2, each review is rep-
resented with a 1 ? K aspect vector Gr after fil-
tering undesired information. The edge weight be-
tween two reviews is then the cosine similarity,
sim(Ri, Rj), between two reviews and can be cal-
culated as follows:
sim(Ri, Rj) =
?t=K
t=1 G
t
i ?G
t
j
?
?t=K
t=1 G
t2
i ?
?
?t=K
t=1 G
t2
j
(4)
Since the normalization process will make the
review-to-review relation matrix asymmetric, we
adopt the following strategy: let P denote the sim-
ilarity matrix between reviews, where P (i, j) =
sim(Ri, Rj) and M denotes the diagonal matrix
with (i,i)-element equal to the sum of the ith row
of SIM . The normalized matrix between reviews
DRR ? RNR?NR is calculated as follows:
DRR =M
? 12 ? P ?M?
1
2 (5)
sim(Ri, wj) denotes the similarity between re-
view Ri and term wj and is the conditional prob-
ability of word wj given review Ri. If wj ? Rj ,
sim(Ri, wj) is calculated according to Eq. (6) by
integrating out latent parameters ? and pi. Else if
wj 6? Rj , sim(Ri, wj) = 0.
sim(Ri, wj) =
k=K?
k=1
p(z = k|ri)? p(wj |z = k)
+
?
t?{B,h,c,d}
p(wj |yi = t)p(yi = t|ri)
= pi(a)d
k=K?
k=1
?zd ? ?
(wj)
z +
?
t?{B,h,c,d}
pi(t)d ?
(wj)
t
(6)
Similar to Eq. (3), we further get the normalized ma-
trix DRT ? RHR?V and DTR ? RV?HR .
Similarity between terms sim(wi, wj) is given by
the WordNet path-similarity,5 normalized to create
the matrix DV V .
5Path-similarity is based on the shortest path that con-
nects the senses in the ?is-a? (hypernym/hyponym) tax-
onomy. See http://nltk.googlecode.com/svn/
trunk/doc/howto/wordnet.html.
1937
Input: The hotel set VD, review set VR, term
set VT , normalized transition probability matrix
DHR, DRR, DRH , DRT , DTT , DTR.
Output: the ranking vectors SR, SH , ST .
Begin:
1. Initialization: set the score labeled reviews to
+1 or ?1 and other unlabeled reviews 0: S0R =
[+1, ...,+1,?1, ...,?1, 0, ..., 0]. Set S0H and
S0T to 0. Normalize the score vector.
2. update SkR, S
k
H and S
k
T according to Eq. (7).
3. normalize SkR, S
k
H and S
k
T .
4. fix the score of labeled reviews to +1 and ?1.
Go to step (2) until convergence.
Figure 4: Semi-Supervised Reinforcement Ranking.
5.2 Reinforcement Ranking Based on the
Manifold Method
Based on the set of labeled reviews, nodes for truth-
ful reviews (positive) are initialized with a high
score (1) and nodes for deceptive reviews, a low
score (-1). Given the weighted graph, our task is
to assign a score to the each hotel, each term, and
the remaining unlabeled reviews. Let SH , SR and
ST denote the ranking scores of hotels, reviews and
terms, which are updated during each iteration as
follows until convergence6:
?
??
??
Sk+1H = DHR ? S
k
R
Sk+1R = 1DRR ? S
k
R + 2DRH ? S
k
H + 3DRT ? S
k
t
Sk+1T = 4DTT ? S
k
T + 5DTR ? S
k
R
(7)
where 1 + 2 + 3 = 1 and 4 + 5 = 1. (The score
of labeled reviews will be fixed to +1 or ?1.)
6 Learning Generalizable Classifiers
In Section 4, we introduced RLDA to filter out
review-, hotel- and city-specific information from
our vector-based review representation. Here, we
will directly evaluate the effectiveness of RLDA
by comparing the performance of binary deceptive
vs. truthful classifiers trained on three feature sets:
(a) the full vocabulary, encoded as unigrams and
bigrams (N-GRAMS); (b) a reduced-dimensionality
feature space, based on standard LDA (Blei et
al, 2003); and (c) a reduced-dimensionality feature
6Convergence is achieved if the difference between ranking
scores in two consecutive iterations is less than 0.00001.
space, based on our proposed revised LDA approach
(RLDA).
We compare two kinds of classifiers, which are
trained on only the labeled CHICAGO dataset and
tested on the FOUR-CITIES dataset. First, we use
SVMlight (Joachims, 1999) to train linear SVM clas-
sifiers, which have been shown to perform well in
related work (Ott et al 2011). Second, we train a
two-layer manifold classifier, which is a simplified
version of the model presented in Section 5. In this
model, the graph consists of only review and term
layers, and the score of a labeled review is fixed to
1 or -1 in each iteration. After convergence, reviews
with scores greater than 0 are classified as truthful,
and less than 0 as deceptive.
Results and Discussion The results are shown in
Table 2 and show the average accuracy and preci-
sion/recall w.r.t. the truthful (positive) class. We find
that SVM and MANIFOLD are comparable in all six
conditions, and not surprisingly, perform best when
evaluated on reviews from the two Chicago hotels in
our FOUR-CITIES data. However, the N-GRAM and
LDA feature sets perform much worse than RDLA
when evaluation is performed on reviews from the
other three (non-Chicago) cities. This confirms that
classifiers trained on n-gram features overfit to the
training data (CHICAGO) and do not generalize well
to reviews from other cities. In addition, the stan-
dard LDA-based method for dimensionality reduc-
tion is not sufficient for our specific task.
7 Identifying Manipulated Hotels
In this section, we evaluate the performance of our
manifold ranking approach (see Section 5) on the
task of identifying manipulated hotels.
Baselines. We consider several baseline ranking
approaches to compare to our manifold ranking ap-
proach. Like the manifold ranking approach, the
baselines also employ both the CHICAGO dataset (la-
beled) and FOUR-CITIES dataset (without labels).7
For fair comparison, we use identical processing
techniques for each approach. Topic number is set
7While we have not investigated the effects of unlabeled data
in detail, providing additional unlabeled data (beyond the test
set) boosts the manifold ranking performances reported below
by 1-2%.
1938
city feature set
SVM Manifold
Accuracy Precision Recall Accuracy Precision Recall
Chicago
N-GRAMS 0.831 0.844 0.818 0.835 0.844 0.825
LDA 0.833 0.846 0.819 0.817 0.832 0.802
RLDA 0.830 0.838 0.822 0.841 0.819 0.863
Non-Chicago
N-GRAMS 0.728 0.744 0.714 0.733 0.738 0.727
LDA 0.714 0.696 0.732 0.728 0.715 0.741
RLDA 0.791 0.799 0.780 0.801 0.787 0.815
Table 2: Binary classification results showing that n-gram features overfit to the CHICAGO training data. Results
correspond to evaluation on reviews for the two Chicago hotels from FOUR-CITIES and non-Chicago FOUR-CITIES
reviews (six hotels).
to five for all topic-model-based approaches. Each
baseline makes review-level predictions and then
ranks each hotel by the average of those predictions.
? Review-SVR: Uses linear Tranductive Support
Vector Regression with unigram and bigram fea-
tures, similar to Ott et al (2011).
? Review-SVR+LDA (R): Similar to REVIEW-
SVR but uses our revised LDA (RLDA) topic
model for dimensionality reduction (R).
? Two-Layer Manifold (S): A simplified version of
our model where the hotel-level is removed from
the graph. Dimensionality reduction is performed
using standard LDA (S).
? Two-Layer Manifold (R): Similar to TWO-
LAYER MANIFOLD (S) but uses the revised LDA
(RLDA) model for dimensionality reduction.
? Three-layer Manifold (tf-idf): Our three-layer
manifold ranking model, except with each review
represented as a TF-IDF term vector. Review sim-
ilarity is calculated based on the cosine similarity
between these vectors.
Evaluation Method. To evaluate ranking perfor-
mance in the absence of a gold standard set of ma-
nipulated hotels, we rearrange the FOUR-CITIES test
set of 40 truthful and 40 deceptive reviews for each
of eight hotels: we create 41 versions of each hotel,
where each hotel version contains a different num-
ber of injected deceptive reviews, ranging from 0 to
40. For example, the first version of a hotel will have
40 truthful and 0 deceptive reviews, the second ver-
sion 39 truthful and 1 deceptive, and the 41st ver-
sion 0 truthful and 40 deceptive. In total, we gen-
erate 41 ? 8 = 328 versions of hotel reviews. We
expect versions with larger proportions of deceptive
reviews to receive lower scores by the ranking mod-
els (i.e., they are ranked higher/more deceptive).
Metrics. To qualitatively evaluate the ranking re-
sults, we use the Normalized Discounted Cumula-
tive Gain (NDCG), which is commonly used to eval-
uate retrieval algorithms with respect to an ideal
relevance-based ranking. In particular, NDCG re-
wards rankings with the most relevant results at the
top positions (Liu, 2009), which is also our objec-
tive, namely, to rank versions that have higher pro-
portions of deceptive reviews nearer to the top.
Let R(m) denote the relevance score of mth
ranked hotel version. Then, NDCGN is defined as:
NDCGN =
1
IDCGN
m=N?
m=1
2R(m) ? 1
log2(1 +m)
(8)
where IDCGN refers to discounted cumulative gain
(DCG) of the ideal ranking of the top N results. We
define the ideal ranking according to the proportion
of deceptive reviews in different versions, and re-
port NDCG scores for theNth ranked hotel versions
(N = 8 to 321), at intervals of 8 (to account for ties
among the eight hotels).
Results and Discussion. NDCG results are shown
in Figure 5. We observe that our approach (using
2, 5 or 10 topics) generally outperforms the other
approaches. In particular, approaches that use our
RLDA text representation (OUR APPROACH, TWO-
LAYER MANIFOLD (R), and REVIEW-SVR+LDA
(R)), which tries to remove city- and hotel-specific
information, perform better than those that use
the full vocabulary (REVIEW-SVR, TWO-LAYER
MANIFOLD (S), and THREE-LAYER MANIFOLD
(TF-IDF)). This further confirms that our RLDA
dimensionality reduction technique allows models,
1939
Figure 5: NDCGN results for different approaches. K
indicates the number of topics.
trained on limited data, to generalize to reviews of
different hotels and in different locations. We also
find that approaches that model a reinforcing rela-
tionship between hotels and their reviews are bet-
ter than approaches that model reviews as inde-
pendent units, e.g., TWO-LAYER MANIFOLD (R)
vs. REVIEW-SVR+LDA and TWO-LAYER MANI-
FOLD (S) vs. REVIEW-SVR. This confirms our in-
tuition that a hotel is more deceptive if it is con-
nected with many deceptive reviews, and, in turn,
a review is more deceptive if from a deceptive hotel.
8 Qualitative Evaluation
We now present qualitative evaluations for the
RLDA topic model and the manifold ranking model.
Topic Quality. Table 3 gives the top words for
four aspect topics and four city-specific topics in the
RLDA topic model; Table 4 gives the highest and
lowest ranking term weights in our three-layer man-
ifold model. By comparing the first row of topics in
Table 3, corresponding to aspect topics, to the top
words in Table 4, we observe that the learned top-
ics relate to truthful and deceptive classes. For ex-
ample, Topics 1 and 4 share many terms with the
top truthful terms in the manifold model, e.g., spa-
tial terms, such as location, floor and block,
and punctuation, such as (, ), and $. Similarly,
Topics 2 and 7 share many terms with the top de-
ceptive terms in the manifold model, e.g., hotel,
husband, wife, amazing, experience and
recommend. This makes sense, since topic models
have been shown to produce discriminative topics on
Topic1 Topic2 Topic4 Topic7
location hotel ( hotel
$ stay room service
walk staff ) husband
night restaurant park amazing
block friendly bed will
floor room night weekend
quiet recommend shower friendly
nice love view travel
lobby excellent minute experience
breakfast wife pillow friend
NYC Chicago LA Houston
York Chicago los Houston
ny Michigan Angeles downtown
time mile la Texas
square tower lax cab
nyc Illinois shuttling Westside
street avenue hollywood center
empire Rogers plane Northwest
Chinatown river morning st
station Burnham California museum
Wall Goodman downtown mission
Table 3: Top words in topics extracted from RLDA topic
model (see Section 4). The top row presents topic words
from four aspect topics (K = 10) and the bottom row
presents top words from four city-specific topics.
Deceptive Truthful
term score term score
my -1.063 $ 0.964
visit -0.944 location 0.922
we -0.882 ( 0.884
hotel -0.863 ) 0.884
husband -0.828 bathroom 0.842
family -0.824 floor 0.810
amazing -0.782 breakfast 0.784
experience -0.740 bar 0.762
recommend -0.732 block 0.747
wife -0.680 small 0.721
relax -0.678 but 0.720
vacation -0.651 walk 0.707
will -0.651 lobby 0.707
friendly -0.646 quiet 0.684
Table 4: Term scores from our ranking algorithm.
this data in previous work (Li et al, 2013).
With respect to the second row in Table 4, con-
taining top words from city-specific topics, we ob-
serve that each topic does contain primarily city-
specific information. This helps to explain why re-
moving terms associated with these topics resulted
in a better vector representation for reviews.
1940
Figure 6: Hotel Ranking Distribution on TripAdvisor
Figure 7: Proportion of Singletons vs. Hotel Ranking.
Real-world Evaluation. Finally, we apply our
ranking model to a large-scale collection of real-
world reviews from TripAdvisor. We crawl 878,561
reviews from 3,945 hotels in 25 US cities from Tri-
pAdvisor excluding all non-5-star reviews and re-
moving hotels with fewer than 100 reviews. In the
end, we collect 244,810 reviews from 838 hotels.
We apply our manifold ranking model and rank
all 838 hotels. First, we present a histogram of the
resulting manifold ranking scores in Figure 6. We
observe that the distribution reaches a peak around
0.04, which in our quantitative evaluation (Sec-
tion 7) corresponded to a hotel with 34 truthful and
6 deceptive reviews. These results suggest that the
majority of reviews in TripAdvisor are truthful, in
line with previous findings by Ott et al (2011).
Next, we note that previous work has hypothe-
sized that deceptive reviews are more likely to be
posted by first-time review writers, or singleton re-
viewers (Ott et al 2011; Wu et al 2011). Accord-
ingly, if this hypothesis were valid, then manipu-
lated hotels would have an above-average proportion
of singleton reviews. Figure 7 shows a histogram
of the average proportion of singleton reviews, as
a function of the ranking scores produced by our
model. Noting that lower scores correspond to a
higher predicted proportion of deceptive reviews, we
observe that hotels that are ranked as being more de-
ceptive by our model have much higher proportions
of singleton reviews, on average, compared to hotels
ranked as less deceptive.
9 Conclusion
We study the problem of identifying manipulated of-
ferings on review portals and propose a novel three-
layer graph model, based on manifold ranking for
ranking offerings based on the proportion of reviews
expected to be instances of deceptive opinion spam.
Experimental results illustrate the effectiveness of
our model over several learning-based baselines.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant BCS-0904822, a DARPA
Deft grant, as well as a gift from Google. We also
thank the EMNLP reviewers for their helpful com-
ments and advice.
References
David Blei, Ng Andrew and Michael Jordan. Latent
Dirichlet alocation. 2003. In Journal of Machine
Learning Research.
Carlos Castillo, Debora Donato, Luca Becchetti, Paolo
Boldi, Stefano Leonardi, Massimo Santini and Sebas-
tiano Vigna. A reference collection for web spam. In
ACM Sigir Forum. 2006.
Paul-Alexandru Chirita, Jrg Diederich and Wolfgang Ne-
jdl. MailRank: using ranking for spam detection. In
Proceedings of the 14th ACM international conference
on Information and knowledge management. 2005.
Cone. 2011 Online Influence Trend Tracker.
http://www.coneinc.com/negative-reviews-online-
reverse-purchase-decisions. August.
Yajuan Duan, Zhumin Chen, Furu Wei, Ming Zhou and
Heung-Yeung Shum. Twitter Topic Summarization by
Ranking Tweets Using Social Influence and Content
Quality. In Proceedings of 24th International Confer-
ence on Computational Linguistics 2012.
Federal Trade Commission. Guides Concerning Use of
Endorsements and Testimonials in Advertising. In
FTC 16 CFR Part 255. 2009.
1941
Socialogue: Five Stars? Thumbs Up? A+ or
Just Average? URL:http://www.ipsos-na.com/news-
polls/pressrelease.aspx?id=5929g
Nitin Jindal, and Bing Liu. Opinion spam and analysis. In
Proceedings of the 2008 International Conference on
Web Search and Data Mining. 2008.
Nitin Jindal, Bing Liu and Ee-Peng Lim. Finding Unusual
Review Patterns Using Unexpected Rules. In Proceed-
ings of the 19th ACM international conference on In-
formation and knowledge management.2010.
Thorsten Joachims. Making large-scale support vector
machine learning practical. In Advances in kernel
methods.1999.
Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan Zhu.
Learning to identify review Spam. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence. 2011.
Jiwei Li, Claire Cardie and Sujian Li. TopicSpam: a
Topic-Model-Based Approach for Spam Detection. In
Proceedings of the 51th Annual Meeting of the Associ-
ation for Computational Linguis- tics. 2013.
Peng Li, Jing Jiang and Yinglin Wang. Generating tem-
plates of entity summaries with an entity-aspect model
and pattern mining. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics. 2010.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. Detecting Product Review
Spammers Using Rating Behavior. In Proceedings of
the 19th ACM international conference on Information
and knowledge management. 2010.
Tieyan Liu. Learning to Rank for Information Retrieval.
In Foundations and Trends in Information Retrieval
2009.
Arjun Mukherjee, Bing Liu and Natalie Glance. Spotting
Fake Reviewer Groups in Consumer Reviews . In Pro-
ceedings of the 21st international conference on World
Wide Web. 2012.
Juan Martinez-Romo and Lourdes Araujo. Web spam
identification through language model analysis. In
Proceedings of the 5th international workshop on ad-
versarial information retrieval on the web. 2009.
Myle Ott, Claire Cardie and Jeffrey Hancock. Estimating
the Prevalence of Deception in Online Review Com-
munities. In Proceedings of the 21st international con-
ference on World Wide Web. 2012.
Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Hancock.
Finding Deceptive Opinion Spam by Any Stretch of
the Imagination. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics. 2011.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher Manning. Labeled LDA: A supervised
topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
2009.
Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and
Padhraic Smyth. The author-topic model for authors
and documents. In Proceedings of the 20th conference
on Uncertainty in artificial intelligence.2004.
Xiaojun Wan and Jianwu Yang. Multi-Document Sum-
marization Using Cluster-Based Link Analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval. 2008.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao Manifold-
Ranking Based Topic-Focused Multi-Document Sum-
marization. In Proceedings of International Joint Con-
ferences on Artificial Intelligence,2007.
Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Re-
view Graph based Online Store Review Spammer De-
tection. In Proceedings of International Conference of
Data Mining. 2011.
Guangyu Wu, Derek Greene and , Padraig Cunningham.
Merging multiple criteria to identify suspicious re-
views. In Proceedings of the fourth ACM conference
on Recommender systems. 2011.
Kyung-Hyan Yoo and Ulrike Gretzel. Comparison of De-
ceptive and Truthful Travel Reviews. In Information
and Communication Technologies in Tourism. 2009.
Dengyong Zhou, Olivier Bousquet, Thomas Navin and
Jason Weston. Learning with local and global consis-
tency. In Proceedings of Advances in neural informa-
tion processing systems.2003.
Dengyong Zhou, Jason Weston, Arthur Gretton and
Olivier Bousquet. Ranking on data manifolds. In Pro-
ceedings of Advances in neural information processing
systems.2003.
1942
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 467?476,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Sentiment Analysis on the People?s Daily
Jiwei Li
1
and Eduard Hovy
2
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
2
Language Technology Institute, Carnegie Mellon University, PA 15213, USA
jiweil@stanford.edu ehovy@andrew.cmu.edu
Abstract
We propose a semi-supervised bootstrap-
ping algorithm for analyzing China?s for-
eign relations from the People?s Daily.
Our approach addresses sentiment tar-
get clustering, subjective lexicons extrac-
tion and sentiment prediction in a unified
framework. Different from existing algo-
rithms in the literature, time information
is considered in our algorithm through a
hierarchical bayesian model to guide the
bootstrapping approach. We are hopeful
that our approach can facilitate quantita-
tive political analysis conducted by social
scientists and politicians.
1 Introduction
?We have no permanent allies, no permanent friends, but only
permanent interests.?
-Lord Palmerston
Newspapers, especially those owned by official
governments, e.g., Pravda from Soviet Union,
or People?s Daily from P.R. China, usually pro-
vide direct information about policies and view-
points of government. As national policies change
over time, the tone that newspapers adopt, es-
pecially sentiment, changes along with the poli-
cies. For example, there is a stark contrast be-
tween the American newspapers? attitudes towards
Afghanistan before and after 911. Similarly, con-
sider the following examples extracted from the
People?s Daily
1
:
? People? Daily, Aug 29th, 1963
All those who are being oppressed and exploited, Unite
!! Beat US Imperialism and its lackeys.
? People?s Daily, Oct, 20th, 2002
A healthy, steady and developmental relationship be-
tween China and US, conforms to the fundamental in-
terests of people in both countries, and the trend of his-
torical development.
1
Due to the space constraints, we only show the translated
version in most of this paper.
Automatic opinion extraction from newspapers
such as people?s daily can facilitate sociologists
?or political scientists? research or help political
pundits in their decision making process. While
our approach applies to any newspaper in princi-
ple, we focus here on the People?s Daily
2
(Renmin
Ribao), a daily official newspaper in the People?s
Republic of China.
While massive number of works have been in-
troduced in sentiment analysis or opinion target
extraction literature (for details, see Section 2), a
few challenges limit previous efforts in this spe-
cific task: First, the heavy use of linguistic phe-
nomenon in the People?s Daily including rhetoric,
metaphor, proverb, or even nicknames, makes ex-
isting approaches less effective for sentiment in-
ference as identifying these expressions is a hard
NLP problem in nature.
Second, as we are more interested in the degree
of sentiment rather than binary classification (i.e.,
positive versus negative) towards an entity (e.g.
country or individual) in the news article, straight-
forward algorithms to apply would be document-
level sentiment analysis approaches such as vector
machine/regression (Pang et al., 2002) or super-
vised LDA (Blei and McAuliffe, 2010). A single
news article, usually contains different attitudes
towards multiple countries or individuals simul-
taneously (say praising ?friends? and criticizing
?enemies?), as shown in the following example
from the People?s Daily of Mar. 17th, 1966:
US imperialism set up a puppet regime in Vietnam and
sent expeditionary force. . . People of Vietnam prevailed over
the modern-equipped US troops with a vengeance. . . The re-
sult of Johnson Government?s intensifying invasion is that. . . .
There will be the day, when people from all over the world ex-
ecute the heinous US imperialism by hanging on a gibbet. . . .
The heroic people of Vietnam, obtained great victory in the
struggle against the USA imperialism. . .
The switching of praising of Vietnam and
criticizing of the USA would make aforemen-
2
paper.people.com.cn/rmrb/
467
tioned document-level machine learning algo-
rithms based on bags of words significantly less
effective if not separating attitudes towards Viet-
nam from toward the USA in the first place. Mean-
while, the separating task is by no means trivial in
news articles. While US imperialism, US troops,
Johnson Government, invaders, Ngo Dinh Diem
3
all point to the USA or its allies, People of Viet-
nam, the Workers? party
4
, Ho Chi Minh
5
, Viet-
nam People?s Army point to North Vietnam side.
Clustering entities according to sentiment, espe-
cially in Chinese, is fundamentally a difficult task.
And our goal, trying to identify entities towards
whom an article holds the same attitudes, is dif-
ferent from standard coreference resolution, since
for us the co-referent group may include several
distinct entities.
To address the aforementioned problems, in this
paper, we propose a sentiment analysis approach
based on the following assumptions:
1. In a single news article, sentiment towards an
entity is consistent.
2. Over a certain period of time, sentiments to-
wards an entity are inter-related.
The assumptions will facilitate opinion analy-
sis: (1) if we can identify the attitude towards an
entity (e.g., Vietnam) in a news article as posi-
tive, then negative attitudes expressed in the arti-
cle are about other entities. (2) The assumption
enables sentiment inference for unseen words in a
bootstrapping way without having to employ so-
phisticated NLP algorithms. For example, from
1950s to 1960s, USA is usually referred to as ?a
tiger made of paper? in translated version. It is
a metaphor indicating things that appear powerful
(tiger) but weak in nature (made of paper). If it is
first identified that during the designated time pe-
riod, China held a pretty negative attitude towards
the USA based on clues such as common nega-
tive expressions (e.g., ?evil? or ?reactionary?), we
can easily induce that ?a tiger made of paper?, is a
negative word.
Based on aforementioned two assumptions,
we formulate our approach as a semi-supervised
model, which simultaneously bootstrap sentiment
target lists, extracts subjective vocabularies and
3
Leader of South Vietnam
4
Ruling political party of Vietnam.
5
One of Founders of Democratic Republic of Vietnam
(North Vietnam) and Vietnam Workers? party.
performs sentiment analysis. Time information is
considered through a hierarchical bayesian model
to guide time-, document-, sentence- and term-
level sentiment inference. A small seed set of sub-
jective words constitutes our only source of super-
vision.
The main contributions of this paper can be
summarized as follows:
1. We propose a semi-supervised bootstrapping algorithm
tailored for sentiment analysis in the People?s daily
where time information is incorporated. We are hope-
ful that sentiment cues can shed insights on other NLP
tasks such as coreference or metaphor recognition.
2. In Analytical Political Science, the quantitative evalu-
ation of diplomatic relations is usually a manual task
(Robinson and Shambaugh, 1995). We are hopeful that
our algorithm can enable automated political analysis
and facilitate political scientists? and historians? work.
2 Related Works
Significant research efforts have been invested into
sentiment analysis and opinion extraction. In one
direction, researchers look into predicting over-
all sentiment polarity at document-level (Pang and
Lee, 2008), aspect-level (Wang et al., 2010; Jo
and Oh, 2011), sentence-level (Yang and Cardie,
2014) or tweet-level (Agarwal et al., 2011; Go
et al., 2009), which can be treated as a clas-
sification/regression problem by employing stan-
dard machine-learning techniques, such as Naive
Bayesian, SVM (Pang et al., 2002) or supervised-
LDA (Blei and McAuliffe, 2010) with different
types of features (i.e., unigram, bigram, POS).
Other efforts are focused on targeted sentiment
extraction (Choi et al., 2006; Kim and Hovy, 2006;
Jin et al., 2009; Kim and Hovy, 2006). Usu-
ally, sequence labeling models such as CRF (Laf-
ferty et al., 2001) or HMM (LIU et al., 2004) are
employed for identifying opinion holders (Choi
et al., 2005), topics of opinions (Stoyanov and
Cardie, 2008) or opinion expressions (e.g. (Breck
et al., 2007; Johansson and Moschitti, 2010; Yang
and Cardie, 2012)). Kim and Hovy (2004; 2006)
identified opinion holders and targets by exploring
their semantics rules related to the opinion words.
Choi et al. (2006) jointly extracted opinion expres-
sions, holders and their is-from relations using an
ILP approach. Yang and Cardie (2013) introduced
a sequence tagging model based on CRF to jointly
identify opinion holders, opinion targets, and ex-
pressions.
Methods that relate to our approach include
semi-supervised approaches such as pipeline or
468
propagation algorithms (Qiu et al., 2011; Qiu et
al., 2009; Zhang et al., 2010; Duyu et al., 2013).
Concretely, Qiu et al. (2011) proposed a rule-
based semi-supervised framework called double
propagation for jointly extracting opinion words
and targets. Compared to existing bootstrapping
approaches, our framework is more general one
with less restrictions
6
. In addition, our approach
harness global information (e.g. document-level,
time-level) to guide the bootstrapping algorithm.
Another related work is the approach introduced
by O?Connor et al. (O?Connor et al., 2013) that
extracts international relations from political con-
texts.
3 the People?s Daily
The People?s Daily
7
(Renmin Ribao), established
on 15 June 1946, is a daily official newspaper in
the People?s Republic of China, with a approxi-
mate circulation of 2.5 million worldwide. It is
widely recognized as the mouthpiece of the Cen-
tral Committee of the Communist Party of China
(CPC) (Wu, 1994). Editorials and commentaries
are usually regarded both by foreign observers and
Chinese readers as authoritative statements of gov-
ernment policy
8
. According to incomplete statis-
tics, there have benn at least 13 major redesigns
(face-liftings) for the People?s Daily in history, the
most recent in 2013.
4 Model
In this section, we present our model in detail.
4.1 Target and Expression extraction
We first extract expressions (attitude or sentiment
related terms or phrases) and target (entities to-
ward whom the opinion holder (e.g., the People?s
Daily) holds an attitude). See the following exam-
ples:
1. [Albania Workers? party][T] is the [glorious][E]
[party][T] of [Marxism and Leninism][E].
2. The [heroic][E] [people of Vietnam][T] obtained
[great][E] [victory][E] against [the U.S. imperial-
ism][T,E].
3. We strongly [warn][E] Soviet Revisionism][E,T].
6
Qiu et al.?s rule base approach makes strong assumptions
that consider opinion word to adjectives and targets to be
nouns/noun, thus only capable of capturing sentences with
simple patterns.
7
paper.people.com.cn/rmrb/
8
http://en.wikipedia.org/wiki/People?
s_Daily
While the majority of subjective sentences omit
the opinion holder, as in Examples 1 and 2, there
are still a few circumstances where opinion hold-
ers (e.g., ?we?, ?Chinese people?, ?Chinese gov-
ernment?) are retained (Example 3). Some words
(i.e. U.S. imperialism) can be both target and ex-
pression, and there can be multiple targets (Exam-
ple 2) within one sentence.
We use a semi-Markov Conditional Random
Fields (semi-CRFs) (Sarawagi and Cohen, 2004;
Okanohara et al., 2006) algorithm for target and
expression extraction. Semi-CRF are CRFs that
relax the Markovian assumptions and allow for se-
quence labeling at the segment level. It has been
demonstrated more powerful that CRFs in multi-
ple sequence labeling applications including NER
(Okanohara et al., 2006), Chinese word segmenta-
tion (Andrew, 2006) and opinion expression iden-
tification (Yang and Cardie, 2012). Our approach
is an extension of Yang and Cardie (2012)?s sys-
tem
9
. Features we adopted included:
? word, part of speech tag, word length.
? left and right context words within a window
of 2 and the correspondent POS tags.
? NER feature.
? subjectivity lexicon features from dictio-
nary
10
. The lexicon consists of a set of Chi-
nese words that can act as strong or weak
cues to subjectivity.
? segment-level syntactic features defined in
(Yang and Cardie, 2012).
Most existing NER systems can barely recog-
nize entities such as [ Vietnamese People?s Army ]
as a unified name entity in that Chinese parser usu-
ally divides them into a series of separate words,
namely [ Vietnamese/People?s Army ]. To han-
dle this problem, we first employ the Stanford
NER engine
11
and then iteratively ?chunk? con-
secutive words, at least one of which is labeled as
a name entity by the NER engine, before check-
ing whether the chunked entity matches a bag of
words contained in Chinese encyclopedia, e.g.,
Baidu Encyclopedia
12
and Chinese Wikipedia
13
.
9
Yang and Cardie?s system focuses on expression extrac-
tion (not target) and identifies direct subjective expression
(DSE) and expressive subjective expression (ESE).
10
http://ir.dlut.edu.cn/NewsShow.aspx?
ID=215
11
http://nlp.stanford.edu/downloads/
CRF-NER.shtml
12
http://baike.baidu.com/
13
http://zh.wikipedia.org/wiki/
Wikipedia
469
4.2 Notation
Here we describe the key variables in our model.
Let C
i
denote the name entity of country i, G
i
denote its corresponding collection of news ar-
ticles. G
i
is divided into 60*4=240 time spans
(one for each quarter of the year, 60 years in to-
tal), G
i
= {G
i,t
}. G
i,t
is composed of a series
of documents {d}, and d is composed of a series
of sentences {S}, which is represented as a tuple
S = {E
S
, T
S
}, where E
S
is the expression and
T
S
is the target of current sentence.
Sentiment Score m: As we are interested in the
degree of positiveness or negativeness, we divided
international relations into 7 categories: Antag-
onism (score 1), Tension (score 2), Disharmony
(score 3), Neutrality (score 4), Goodness (score
5), Friendliness (score 6), Brotherhood (Comrade-
ship) (score 7) based on researches in political sci-
ence literature
14
. Each of G
i,t
, document d, sen-
tence S and expression term w is associated with
a sentiment score m
i,t
, m
d
, m
S
and m
w
, respec-
tively. M denotes the list of subjective terms,
M = {w,m
w
}
Document Target List T
d
i
: We use T
d
i
to denote
the collection of entity targets in document d ? G
i
which the People?s daily holds similar attribute to-
wards. For example, suppose document d belongs
to Vietnam article collection (C
i
= V ietnam), T
d
i
can be {Vietnam, Workers? party, People?s Army,
Ho Chi Minh}. While U.S., U.S. troops and Lyn-
don Johnson are also entity targets found in d, they
are not supposed to be included in T
d
i
since the au-
thor holds opposite attributes.
Sentence List d
i
: We further use d
i
denotes the
subset of sentences in d talking about entities from
target list T
d
i
. Similarly, in a Vietnam related arti-
cle, sentences talking about the U.S. are not sup-
posed to be included in d
i
.
4.3 Hierarchical Bayesian Markov Model
In our approach, time information is incorporated
through a hierarchical Bayesian Markov frame-
work where m
i,t
is modeled as a first-order Pois-
son Process given the coherence assumption in
time-dependent political news streams.
m
i,t
? Poisson(m
i,t
,m
i,t?1
) (1)
14
http://www.imir.tsinghua.edu.cn/
publish/iis/7522/20120522140122561915769
Figure 1: Hierarchical Bayesian Model for Infer-
ence
For each document d ? G
i,t
, m
d
is sampled from
a Poisson distribution with mean value of m
i,t
.
m
d
? Poisson(m
d
,m
i,t
) (2)
For sentence S ? d
i
,m
S
is sampled fromm
d
from
a Poisson distribution based on m
d
.
m
S
? Poisson(m
S
,m
d
) (3)
4.4 Intialization
Given a labeled subjective list M , for article d ?
G
i
, we initialize T
d
i
with the name of entity C
i
, d
i
with sentences satisfying T
S
= C
i
and E
S
? M .
m
S
for S ? d
i
, is initialized as the average score
of its containing expression E
s
based on M . Then
the MCMC algorithm is applied by iteratively up-
dating m
d
and m
i,t
according to the posterior dis-
tribution. Let P (m|?) denotes the probability of
parameter m given all other parameters and the
posterior distributions are given by:
P (m
d
= ?|?) ? Poisson(?,m
i,t
)
?
S?d
i
Poisson(?,m
S
)
P (m
i,t
= ?|?) ? Poisson(?,m
i,t?1
)
? Poisson(m
i,t+1
, ?) ? ?
?
d?G
i,t
Poisson(m
d
, ?)
(4)
4.5 Semi-supervised Bootstrapping
Our semi-supervised learning algorithm updates
M , T
d
i
, d
i
, S
d
and S
d
i
iteratively. A brief inter-
pretation is shown in Figure 2 and the details are
shown in Figure 4. Concretely, for each sentence
S ? d ? d
i
, step 1 means, if its expression E
S
exists in subjective list M , we added its target T
S
to T
d
i
and S to d
i
. step 2 means if the target T
S
ex-
ists in T
d
i
, its expression,E
s
, is added to subjective
list M with score m
d
. As M and T
d
i
change in the
iteration, in step 3, we again go over all unconsid-
ered sentences with new M and T
d
i
. m
d
and m
i,t
are then updated based on new m
S
using MCMC
in Equ. 4. Note that sentences with pronoun target
are not involved in the bootstrapping procedure.
470
Figure 2: A brief demonstration of the adopted semi-supervised algorithm. (a)?(b): Sentence (2) is
added to d
i
due to the presence of already known subjective term ?great? . Target B is added to target list
T
d
i
. (b)?(c): term ?heroic? is added to subjective word list M with score 7 since it modifies target B.
Input: Entity C
i
, G
i
, subjective term list M
? for each entity i, each document d
T
d
i
= {C
i
}, d
i
= {S|S ? d,C
i
= T
S
, E
s
?M}
for each sentence S ? d
i
:
. m
s
=
1
|E
S
?M|
?
m
E
s
? Iteratively update m
i,t
, m
d
using MCMC based on
posterior probability shown in Equ.4.
Output: {d
i
}, {T
d
i
}, {m
i,t
} and {m
d
}
Figure 3: Initialization Algorithm.
4.6 Error Prevention in Bootstrapping
Error propagation is highly influential and damag-
ing in bootstrapping algorithms, especially when
extending very limited data to huge corpora. To
avoid the collapse of the algorithm, we select can-
didates for opinion analysis in a extremely strict
manner, at the sacrifice of many subjective sen-
tences
15
. Concretely, we only consider sentences
with exactly one target and at least one expression.
Sentences with multiple targets (e.g., Example 2
in Section 4.1) or no expressions, or no targets are
discarded.
In addition to the strict sentence selection ap-
proach, we adopt the following methods for self-
correction in the boot-strapping procedure:
1. For T
1
, T
2
? T
d
i
, (E
1
, T
1
) ? S
1
, (E
2
, T
2
) ?
S
2
, E
1
, E
2
?M , if |m
E
1
?m
E
2
| > 1: Expel
E
1
andE
2
fromM , expel T
1
and T
2
from T
d
i
,
with the exception of original labeled data.
Explanation: If sentiment scores for two ex-
pressions, whose correspondent targets both
15
Negative effect of strict sentence selection can be partly
compensated by the consideration of time-level information
Input: Entity {C
i
}, Articles Collections {G
i
}, subjective
term list M, sentiment score {m
d
}, {m
i,t
}, target list for
each document {T
d
i
}
Algorithm:
while not convergence:
? for each entity C
i
, document d:
for each sentence S ? d? d
i
1. if E
S
?M , T
s
6? T
d
i
T
d
i
= T
d
i
?
T
s
, d
i
= d
i
?
S, m
S
= m
d
2. if T
s
? T
t
i
, E
s
6?M
M = M
?
(E
s
, S
d
), d
i
= d
i
?
S, m
s
= m
d
3. if E
S
?M,T
S
? T
d
i
d
i
= d
i
?
S, m
S
= m
E
s
?Iteratively update m
i,t
, m
d
using MCMC based on
posterior probability shown in Equ.4 .
end while:
Output: subjective term list M, score {m
i,t
}
Figure 4: Semi-supervised learning algorithm.
belong to the target list T
d
i
, diverge enough,
we discard both expressions and targets based
according to Assumption 1: sentiments to-
wards one entity (or its allies) in an article
should be consistent.
2. ?S ? d, T
S
? T
d
i
, |m
E
S
? m
d
| > 1, T
S
is
expelled from T
d
i
.
Explanation: If target T
S
for sentence S be-
longs to T
d
i
, but its corresponding expression
E
s
is not consistent with article-level senti-
ment m
d
, T
S
is expelled from T
d
i
.
5 Experiment
5.1 Data and Preprocessing
Our data set is composed of the People?s daily
from 1950 to 2010, across a 60-year time span.
471
antagonism (m=1) ??(extremely cruel),??(enemy)
tension (m=2) ??(indignation),??(offend)
disharmony (m=3) ??(disappointed),??(regret)
neutrality (m=4) ??,??(concern)
goodness (m=5) ???(developmental),??(respect)
friendship (m=6) ??(friendship),??(friend)
brotherhood (m=7) ??(firmly),??(brother)
Table 1: Illustration of subjective list M
News articles are first segmented using ICTCLAS
Chinese segmentation word system
16
(Zhang et
al., 2003). Articles with fewer than 200 Chi-
nese words are discarded. News articles are clus-
tered by the presence of a country?s name more
than 2 times based on a country name list from
Wikipedia
17
. Articles mentioning more than 5 dif-
ferent countries are discarded since they usually
talk about international conferences. Note that one
article can appear in different collections (example
in Section 1 will appear in both Vietnam and the
U.S. collection).
Compound sentences are segmented into
clauses based on dependency parse tree. Then
those containing more than 50 characters or
less than 4 characters are discarded. To avoid
complicated inference, sentences with negation
indicators are discarded.
5.2 Obtaining Subjectivity Word List
Since there are few Chinese subjectivity lexicons
(with degrees) available and those exist may not
serve our specific purpose, we manually label a
small number of Chinese subjective terms as seed
corpus. We divided the labeling process into 2
steps rather than directly labeling vocabularies
18
.
We first selected 100 news articles and assigned
each of them (as well as the appropriate coun-
try entity C
i
) to 2 students majoring in Interna-
tional Studies, asking them to give a label sen-
timent score (1 to 7) according to the rules de-
scribed in Section 4.2. 20 students participated
in the procedure. Since annotators have plenty of
background knowledge, they agreed on 98 out of
100. Second, we selected out subjectivity lexicons
by matching to a comprehensive subjectivity lex-
icons list
19
. and ask 2 students select the candi-
dates that signal the document-level label from the
16
http://ictclas.org/
17
http://zh.wikipedia.org/wiki/????-(????)
18
We tried direct vocabulary labeling in the first place, but
got low score for inter agreement, where value of Cohen
?
s ?
is only 0.43.
19
http://ir.dlut.edu.cn/NewsShow.aspx?
ID=215
P R F
Total
semi-CRF 0.74 0.78 0.76
CRF 0.73 0.66 0.68
Single
semi-CRF 0.87 0.92 0.90
CRF 0.80 0.87 0.83
Table 2: Results for Expressions/Targets extrac-
tion.
first step. According to whether a word a selected
or not, the value of Cohen
?
s ? is 0.78, showing
substantial agreement. For the small amount of la-
bels on which the judges disagree, we recruited an
extra judge and to serve as a tie breaker. Table 1
shows some labeled examples.
5.3 Targets and Expressions Extraction
As the good performance of semi-CRF in opinion
extraction has been demonstrated in previous work
(Yang and Cardie, 2012), we briefly go over model
evaluation in this subsection for brevity. We man-
ually labeled 600 sentences and performed 5-fold
cross validation for evaluation. We compare semi-
CRF to Standard CRF. We report performances on
two settings in Table 2. The first setting, Total,
corresponds to performance on the whole dataset,
while second one Single, denotes the performance
on the set of sentences with only one target, which
we are more interested in because multiple-target
sentences are discarded in our algorithm. It turned
out that semi-CRF significantly outputs standard
CRF, approaching 0.90 F-1 score on Single setting.
5.4 Foreign Relation Evaluation
Gold-standard foreign relations are taken from Po-
litical Science research at the Institute of Modern
International Relations, Tsinghua University, ex-
tracted from monthly quantitative China foreign
relations reports with 7 countries (U.S., Japan,
Russia/Soviet, England, France, India, and Ger-
many) from 1950 to 2012
20
.
We consider several baselines. For fair compar-
ison, we use identical processing techniques for
each approach. Some baselines make article-level
predictions, for which we obtain time-period level
relation prediction by averaging the documents.
Coreference+Bootstrap (CB): We first imple-
mented Ngai and Wang?s Chinese coreference sys-
20
Details found here http://www.imir.tsinghua.
edu.cn/publish/iisen/7523/index.html.
472
Model Ours CB No-time
Pearson 0.895 0.753 0.808
Model SVR-d SLDA SVR-S
Pearson 0.482 0.427 0.688
Table 3: Pearson Correlation with Gold Standard.
tem (2007). We then bootstrap sentiment terms
and score based on entity coreference.
No-time: A simplified version of our approach
where each article is considered as an independent
unit and no time-level information is considered.
m
d
is obtained by averaging its containing sen-
tences and used for later bootstrapping.
SVR-d: Uses SVM
light
(Joachims, 1999) to
train a linear SVR (Pang and Lee, 2008) for
document-level sentiment prediction using the un-
igram feature. The 100 labeled documents are
used as training data.
SLDA: supervised-LDA (Blei and McAuliffe,
2010) for document-level label prediction. Topic
number is set to 10, 20, 50, 100 respectively and
we report the best result.
SVR-S: Sentence-level SVR to sentences with
presence of entityC
i
21
. We obtain document-level
prediction by averaging its containing sentences
and then time-period level prediction by averaging
its containing documents.
We report the Pearson Correlation with gold
standards in table 3. As we can observe, simple
document-level regression models, i.e., SVR and
SLDA do not fit this task. The reason is sim-
ple: one article d can appear in different collec-
tions. Recall the Vietnam example in Section 1,
it appears in both G
V ietnam
and G
the U.S.
. Sen-
timent prediction for d should be totally opposite
in the two document collections: very positive in
G
V ietnam
and very negative in G
USA
. But doc-
ument level prediction would treat them equally.
Our approach outperforms No-Time, illustrating
the meaningfulness of exploiting time-level infor-
mation in our task. Our system approaches around
0.9 correlation with the gold standards. The reason
why No-Time is better than CB is also simple: CB
includes only coreferent entities in the target list
(e.g., America for the USA article collection), and
therefore overlooks rich information provided by
non-coreferent entities (e.g., President Nixon or
21
Features we explore include word entities in current sen-
tence, POS, a window of k ? {1, 2} words from the target
and the expression and corresponding POS, and the depen-
dency path between target and expression.
Nixon Government). No-Time instead groups en-
tities according to attitude, thereby enabling more
information to be harnessed. For SVR-S, as the
regression model trained from limited labeled data
can hardly cover unseen terms during testing, the
performance is just OK. SVR-S also suffers from
overlooking rich sources of information since it
only considers sentences with exact mention of the
name entity of the corresponding country.
Figure 5: Examples of China?s Foreign Relations.
6 Diplomatic Relations
?The enemy of my enemy is my friend?
?Arabic proverb
A central characteristic of post-World War Second
international system with which China had to deal
would be overwhelming preeminence of the USA
and USSR as each of the superpowers stood at
the center of a broad alliance system who was en-
gaged in an intense and protracted global conflict
with the other. We choose 6 countries and report
results in Figure 5. One of interesting things we
can observe from Figure 5 is that foreign attitudes
are usually divergent towards two opposing forces:
Sino-American relation (see Figure 5(a)) began to
improve when the Sino-Soviet relation (see Figure
5(b)) reached its bottom at the beginning of 1970s.
Similar patterns appear for Sino-Pakistan (see Fig-
ure 5(c)), Sino-India relations (see Figure 5(d))
473
Figure 6: Top coreference terms Towards USA and Soviet Union/Russia versus time. Blue denotes words
that are both Target and positive words in M . Red denotes words that are both Target and negative words
in M
in early 1960s
22
, and Sino-Vietnam 5(f)), Sino-
American relations in late 1970s. On the con-
trast, attitudes are usually consistent toward allied
forces: Sino-Japan relations with Sino-USA re-
lations before 1990s, and Sino-Vietnam relations
with Sino-Soviet relations in late 1970s and 1980s.
Figure 6 presents top clustering target (T
d
i
) in
the USA and Soviet Union/Russia article collec-
tion. As some of vocabulary terms can be both
target and expression, we use blue to label terms
with positive sentiment, red to label negative ones.
As we can see from Figure 6, targets(T ) extracted
by our model show a very clear pattern where al-
lies and co-referent entities are grouped. Another
interesting thing is, the subjectivity of target words
from different times is generally in accord with the
relation curves shown in Figure 5.
7 Conclusion and Discussion
In this paper, we propose a sentiment analy-
sis algorithm to track China?s foreign relations
from the People?s Daily. Our semi-supervised al-
gorithm harnesses higher level information (i.e.,
document-level, time-level) by incorporating a hi-
erarchical Bayesian approach into the framework,
to resolve sentiment target clustering, create sub-
jective lexicons, and perform sentiment prediction
simultaneously. While we focus here on the Peo-
ple?s Daily for diplomatic relation extraction, the
idea of our approach is general and can be ex-
tended broadly. Another contribution of this work
is the creation a comprehensive Chinese subjec-
22
A fan of history can trace the crucial influence of the
USSR in Sino-India relation in 1960s
tive lexicon list. We are hopeful that our approach
can not only facilitate quantitative research by po-
litical scientists, but also shed light on NLP appli-
cations such as coreference and metaphor, where
sentiment clues can be helpful.
It is worth noting that, while harnessing time-
level information can indeed facilitate opinion
analysis, especially when labeled data is limited in
our specific task, it is not a permanent-perfect as-
sumption, especially considering the diversity and
treacherous currents at the international political
stage.
At algorithm-level, to avoid error propagation
due to limitations of current sentiment analysis
tools (even though semi-CRF produces state-of-art
performance in target and expression extraction
task, a performance of 0.8 F-value, when applied
to the whole corpus, can by no means satisfy
our requirements), we discard a great number of
sentences, among which is contained much useful
information. How to resolve these problems
and improve opinion extraction performance is
our long-term goal in sentiment analysis/opinion
extraction literature.
Acknowledgements
The authors want to thank Bishan Yang and Claire
Cardie for useful comments and discussions. The
authors are thankful for suggestions offered by
EMNLP reviewers.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
474
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media.
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In EMNLP.
David M Blei and Jon D McAuliffe. 2010. Supervised
topic models. arXiv preprint arXiv:1003.0783.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In IJCAI.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In EMNLP.
Tang Duyu, Qin Bing, Zhou LanJun, Wong KamFai,
Zhao Yanyan, and Liu Ting. 2013. Domain-specific
sentiment word extraction by seed expansion and
pattern generation. arXiv preprint arXiv:1309.6722.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford.
Wei Jin, Hung Hay Ho, and Rohini K Srihari. 2009.
A novel lexicalized hmm-based learning framework
for web opinion mining. In ICML.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In ICWSM.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1367. Association for Computa-
tional Linguistics.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Yun-zhong LIU, Ya-ping LIN, and Zhi-ping CHEN.
2004. Text information extraction based on hid-
den markov model [j]. Acta Simulata Systematica
Sinica.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval.
Grace Ngai and Chi-Shing Wang. 2007. A knowledge-
based approach for unsupervised chinese corefer-
ence resolution. Computational Linguistics and
Chinese Language Processing, 12(4):459?484.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving
the scalability of semi-markov conditional random
fields for named entity recognition. In ACL.
Brendan O?Connor, Brandon M Stewart, and Noah A
Smith. 2013. Learning to extract international rela-
tions from political context. In ACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In EMNLP.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In IJCAI.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Computational
linguistics.
Thomas W Robinson and David L Shambaugh. 1995.
Chinese foreign policy: theory and practice. Oxford
University Press.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In NIPS.
Veselin Stoyanov and Claire Cardie. 2008. Topic iden-
tification for fine-grained opinion analysis. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a
rating regression approach. In SIGKDD.
Guoguang Wu. 1994. Command communication: The
politics of editorial formulation in the people?s daily.
China Quarterly, 137:194?211.
Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional ran-
dom fields. In EMNLP.
Bishan Yang and Claire Cardie. 2014. Context-aware
learning for sentence-level sentiment analysis with
posterior regularization. ACL.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of the second SIGHAN work-
shop on Chinese language processing-Volume 17.
475
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters.
476
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1997?2007,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Major Life Event Extraction from Twitter based on
Congratulations/Condolences Speech Acts
Jiwei Li
1
, Alan Ritter
2
, Claire Cardie
3
and Eduard Hovy
4
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
2
Department of Computer Science and Engineering, the Ohio State University, OH 43210, USA
3
Computer Science Department, Cornell University, Ithaca, NY 14853, USA
4
Language Technology Institute, Carnegie Mellon University, PA 15213, USA
jiweil@stanford.edu ritter.1492@osu.edu
cardie@cs.cornell.edu ehovy@andrew.cmu.edu
Abstract
Social media websites provide a platform
for anyone to describe significant events
taking place in their lives in realtime.
Currently, the majority of personal news
and life events are published in a tex-
tual format, motivating information ex-
traction systems that can provide a struc-
tured representations of major life events
(weddings, graduation, etc. . . ). This pa-
per demonstrates the feasibility of accu-
rately extracting major life events. Our
system extracts a fine-grained description
of users? life events based on their pub-
lished tweets. We are optimistic that our
system can help Twitter users more easily
grasp information from users they take in-
terest in following and also facilitate many
downstream applications, for example re-
altime friend recommendation.
1 Introduction
Social networking websites such as Facebook and
Twitter have recently challenged mainstream me-
dia as the freshest source of information on im-
portant news events. In addition to an important
source for breaking news, social media presents a
unique source of information on private events, for
example a friend?s engagement or college gradua-
tion (examples are presented in Figure 1). While
a significant amount of previous work has inves-
tigated event extraction from Twitter (e.g., (Rit-
ter et al., 2012; Diao et al., 2012)), existing ap-
proaches mostly focus on public bursty event ex-
traction, and little progress has been made towards
the problem of automatically extracting the major
life events of ordinary users.
A system which can automatically extract ma-
jor life events and generate fine-grained descrip-
tions as in Figure 1 will not only help Twitter
users with the problem of information overload by
summarizing important events taking place in their
friends lives, but could also facilitate downstream
applications such as friend recommendation (e.g.,
friend recommendation in realtime to people who
were just admitted into the same university, get
the same jobs or internships), targeted online ad-
vertising (e.g., recommend baby care products to
newly expecting mothers, or wedding services to
new couples), information extraction, etc.
Before getting started, we first identify a num-
ber of key challenges in extracting significant life
events from user-generated text, which account the
reason for the lack of previous work in this area:
Challenge 1: Ambiguous Definition for Ma-
jor Life Events Major life event identification
is an open-domain problem. While many types of
events (e.g., marriage, engagement, finding a new
job, giving birth) are universally agreed to be im-
portant, it is difficult to robustly predefine a list of
characteristics for important life events on which
algorithms can rely for extraction or classification.
Challenge 2: Noisiness of Twitter Data: The
user-generated text found in social media websites
such as Twitter is extremely noisy. The language
used to describe life events is highly varied and
ambiguous and social media users frequently dis-
cuss public news and mundane events from their
daily lives, for instance what they ate for lunch.
Even for a predefined life event category, such
as marriage, it is still difficult to accurately iden-
tify mentions. For instance, a search for the
keyphrase ?get married? using Twitter Search
1
re-
sults in a large number of returned results that do
not correspond to a personal event:
? I want to get married once. No divorce & no
cheating, just us two till the end.
(error: wishes)
1
https://twitter.com/search?q=
get
?
married
1997
Figure 1: Examples of users mentioning personal life events on Twitter.
? Can Adam Sandler and Drew Barrymore just
drop the pretense and get married already?
(error: somebody else)
? I got married and had kids on purpose
(error: past)
Challenge 3: the Lack of Training Data Col-
lecting sufficient training data in this task for ma-
chine learning models is difficult for a number of
reasons: (1) A traditional, supervised learning ap-
proach, requires explicit annotation guidelines for
labeling, though it is difficult to know which cat-
egories are most representative in the data apriori.
(2) Unlike public events which are easily identi-
fied based on message volume, significant private
events are only mentioned by one or several users
directly involved in the event. Many important cat-
egories are relatively infrequent, so even a large
annotated dataset may contain just a few or no ex-
amples of these categories, making classification
difficult.
In this paper, we present a pipelined system that
addresses these challenges and extracts a struc-
tured representation of individual life events based
on users? Twitter feeds. We exploit the insight to
automatically gather large volumes of major life
events which can be used as training examples for
machine learning models. Although personal life
events are difficult to identify using traditional
approaches due to their highly diverse nature, we
noticed that users? followers often directly reply
to such messages with CONGRATULATIONS or
CONDOLENCES speech acts, for example:
User1: I got accepted into Harvard !
User2: Congratulations !
These speech acts are easy to identify with high
precision because the possible ways to express
them are relatively constrained. Instead of directly
inspecting tweets to determine whether they corre-
spond to major life events, we start by identifying
replies corresponding to CONGRATULATIONS or
CONDOLENCES, and then retrieve the message
they are in response to, which we assume refer to
important life events.
The proposed system automatically identifies
major life events and then extracts correspondent
event properties. Through the proposed system,
we demonstrate that it is feasible to automatically
reconstruct a detailed list of individual life events
based on users? Twitter streams. We hope that
work presented in this paper will facilitate down-
stream applications and encourage follow-up work
on this task.
2 System Overview
An overview of the components of the system is
presented in Figure 2. Pipeline1 first identifies
the major life event category the input tweet talks
about and filters out the irrelevant tweets and will
be described in Section 4. Next, Pipeline2, as,
demonstrated in Section 5, identifies whether the
speaker is directly involved in the life event. Fi-
nally, Pipeline3 extracts the property of event and
will be illustrated in Section 6.
Section 3 serves as the preparing step for the
pipelined system, describing how we collect train-
ing data in large-scale. The experimental evalua-
tion regarding each pipeline of the system is pre-
sented in the corresponding section (i.e., Section
4,5,6) and the end-to-end evaluation will be pre-
1998
Figure 2: System Overview. Blue: original input tweets. Red: filtered out tweets. Magenta: life event
category. Green: life event property. Pipeline 1 identifies the life category the input tweet talks about
(e.g., marriage, graduation) and filter out irrelevant tweets (e.g., I had beef stick for lunch). Pipeline 2
identifies whether the speaker is directly involved in the event. It will preserve self-reported information
(i.e. ?I got married?) and filtered out unrelated tweets (e.g., ?my friend Chris got married?). Pipeline
3 extracts the property of event (e.g. to whom the speaker married or the speaker admitted by which
university).
sented in Section 7.
3 Personal Life Event Clustering
In this section, we describe how we identify com-
mon categories of major life events by leverag-
ing large quantities of unlabeled data and obtain
a collection of tweets corresponding to each type
of identified event.
3.1 Response based Life Event Detection
While not all major life events will elicit CON-
GRATULATIONS or CONDOLENCES from a user?s
followers, this technique allows us to collect large
volumes of high-precision personal life events
which can be used to train models to recognize the
diverse categories of major life events discussed
by social media users.
3.2 Life Event Clustering
Based on the above intuition, we develop an ap-
proach to obtain a list of individual life event clus-
ters. We first define a small set of seed responses
which capture common CONGRATULATIONS and
CONDOLENCES, including the phrases: ?Congrat-
ulations?, ?Congrats?, ?Sorry to hear that?, ?Awe-
some?, and gather tweets that were observed with
seed responses. Next, an LDA (Blei et al., 2003)
2
based topic model is used to cluster the gathered
2
Topic Number is set to 120.
tweets to automatically identify important cate-
gories of major life events in an unsupervised way.
In our approach, we model the whole conversation
dialogue as a document
3
with the response seeds
(e.g., congratulation) masked out. We furthermore
associate each sentence with a single topic, fol-
lowing strategies adopted by (Ritter et al., 2010;
Gruber et al., 2007). We limit the words in our
document collection to verbs and nouns which
we found to lead to clearer topic representations,
and used collapsed Gibbs Sampling for inference
(Griffiths and Steyvers, 2004).
Next one of the authors manually inspected the
resulting major life event types inferred by the
model, and manually assigned them labels such
as ?getting a job?, ?graduation? or ?marriage?
and discarded incoherent topics
4
. Our methodol-
ogy is inspired by (Ritter et al., 2012) that uses
a LDA-CLUSTERING+HUMAN-IDENTIFICATION
strategy to identify public events from Twitter.
Similar strategies have been widely used in un-
supervised information extraction (Bejan et al.,
2009; Yao et al., 2011) and selectional preference
3
Each whole conversation usually contains multiple
tweets and users.
4
While we applied manual labeling and coherence eval-
uation in this work, an interesting direction for future work
is automatically labeling major life event categories follow-
ing previous work on labeling topics in traditional document-
based topic models (Mimno et al., 2011; Newman et al.,
2010).
1999
Figure 3: Illustration of bootstrapping process.
Input: Reply seed list E = {e}, Tweet conversation col-
lection T = {t}, Retrieved Tweets Collection D = ?.
Identified topic list L=?
Begin
While not stopping:
1. For unprocessed conversation t ? T
if t contains reply e ? E,
? add t to D: D = D + t.
? remove t from T : T = T ? t
2. Run streaming LDA (Yao et al., 2009) on newly added
tweets in D.
3. Manually Identify meaningful/trash topics, giving label
to meaningful topics.
4. Add newly detected meaningful topic l to L.
5. For conversation t belonging to trash topics
? remove t from D: D = D ? t
6. Harvest more tweets based on topic distribution.
7. Manually identify top 20 responses to tweets harvested
from Step 6.
8. Add meaningful responses to E.
End
Output: Identified topic list L. Tweet collection D.
Figure 4: Bootstrapping Algorithm for Response-
based Life event identification.
modeling (Kozareva and Hovy, 2010a; Roberts
and Harabagiu, 2011).
Conversation data was extracted from the CMU
Twitter Warehouse of 2011 which contains a total
number of 10% of all published tweets in that year.
3.3 Expanding dataset using Bootstrapping
While our seed patterns for identifying mes-
sages expressing CONGRATULATIONS and CON-
DOLENCES are very high precision, they don?t
cover all the possible ways these speech acts
can be expressed. We therefore adopt a semi-
supervised bootstrapping approach to expand our
reply seeds and event-related tweets. Our boot-
strapping approach is related to previous work
on semi-supervised information harvesting (e.g.,
(Kozareva and Hovy, 2010b; Davidov et al.,
2007)). To preserve the labeled topics from the
first iteration, we apply a streaming approach to
inference (Yao et al., 2009) over unlabeled tweets
(those which did not match one of the response
Figure 5: Illustration of data retrieved in each step
of bootstrapping.
congratulations (cong, congrats); (that?s) fantastic; (so) cool;
(I?m) (very) sorry to hear that; (that?s) great (good) new;
awesome; what a pity; have fun; great; that sucks; too
bad; (that?s) unfortunate; how sad; fabulous; (that?s)
terrific; (that?s) (so) wonderful; my deepest condolences;
Table 1: Responses retrieved from Bootstrapping.
seeds). We collect responses to the newly added
tweets, then select the top 20 frequent replies
5
.
Next we manually inspect and filter the top ranked
replies, and use them to harvest more tweets. This
process is then repeated with another round of
inference in LDA including manual labeling of
newly inferred topics, etc... An illustration of our
approach is presented in Figure 3 and the details
are presented in Figure 4. The algorithm outputs
a collection of personal life topics L, and a collec-
tion of retrieved tweets D. Each tweet d ? D is
associated with a life event topic l, l ? L.
We repeat the bootstrapping process for 4 iter-
ations and end up with 30 different CONGRATU-
LATIONS and CONDOLENCES patterns (shown in
Table 1) and 42 coherent event types which refer to
significant life events (statistics for harvested data
from each step is shown in Figure 5). We show
examples of the mined topics with correspondent
human labels in Table 3, grouped according to a
specific kind of resemblance.
3.4 Summary and Discussion
The objective of this section is (1) identifying a
category of life events (2) identifying tweets asso-
ciated with each event type which can be used as
candidates for latter self reported personal infor-
mation and life event category identification.
We understand that the event list retrieved from
our approach based on replies in the conversation
is far from covering all types of personal events
(especially the less frequent life events). But our
5
We only treat the first sentence that responds to the be-
ginning of the conversation as replies.
2000
Life Event Proportion
Birthday 9.78
Job 8.39
Wedding
Engagement
7.24
Award 6.20
Sports 6.08
Anniversary 5.44
Give Birth 4.28
Graduate 3.86
Death 3.80
Admission 3.54
Interview
Internship
3.44
Moving 3.26
Travel 3.24
Illness 2.45
Life Event Proportion
Vacation 2.24
Relationship 2.16
Exams 2.02
Election 1.85
New Car 1.65
Running 1.42
Surgery 1.20
Lawsuit 0.64
Acting 0.50
Research 0.48
Essay 0.35
Lost Weight 0.35
Publishing 0.28
Song 0.22
OTHER 15.31
Table 2: List of automatically discovered life event
types with percentage (%) of data covered.
list is still able to cover a large proportion of IM-
PORTANT and COMMON life events. Our latter
work is focused on given a random tweet, identi-
fying whether it corresponds to one of the 42 types
of life events in our list.
Another thing worth noting here is that, while
current section is not focused on self-reported in-
formation identification, we have already obtained
a relatively clean set of data with a large pro-
portion of non self-reported information related
tweets being screened: people do not usually re-
spond to non self-reported information with com-
monly used replies, or in other words, with replies
that will pass our next step human test
6
. These non
self-reported tweets would therefore be excluded
from training data.
4 Life Event Identification
In this section, we focused on deciding whether a
given tweet corresponds to one of the 42 prede-
fined life events.
Our training dataset consists of approximately
72,000 tweets from 42 different categories of life
events inferred by our topic model as described
in Section 3. We used the top 25% of tweets for
which our model assigned highest probability to
each topic. For sparsely populated topics we used
the top 50% of tweets to ensure sufficient cover-
age.
We further collected a random sample of about
10 million tweets from Twitter API
7
as non-life
6
For example, people don?t normally respond to ?I want
to get married once? (example in Challenge 2, Section 1)
with ?Congratulations?.
7
https://dev.twitter.com/
Human Label Top words
Wedding
&engagement
wedding, love, ring, engagement,
engaged, bride, video, marrying
Relationship
Begin
boyfriend, girlfriend, date, check,
relationship, see, look
Anniversary anniversary, years, year, married,
celebrating, wife, celebrate, love
Relation End/
Devoice
relationship, ended, hurt, hate, de-
voice, blessings, single
Graduation graduation, school, college, gradu-
ate, graduating, year, grad
Admission admitted, university, admission, ac-
cepted, college, offer, school
Exam passed, exam, test, school,
semester, finished, exams,
midterms
Research research, presentation, journalism,
paper, conference, go, writing
Essay & Thesis essay, thesis, reading, statement,
dissertation, complete, project
Job job, accepted, announce, join, join-
ing, offer, starting, announced,
work
Interview& In-
ternship
interview, position, accepted, in-
ternship, offered, start, work
Moving house, moving, move, city, home,
car, place, apartment, town, leaving
Travel leave, leaving, flight, home, miss,
house, airport, packing, morning
Vacation vocation, family, trip, country, go,
flying, visited, holiday, Hawaii
Winning Award won, award, support, awards, win-
ning, honor, scholarship, prize
Election/
Promotion/
Nomination
president, elected, run, nominated,
named, promotion, cel, selected,
business, vote
Publishing book, sold, writing, finished, read,
copy, review, release, books, cover
Contract signed, contract, deal, agreements,
agreed, produce, dollar, meeting
song/ video/ al-
bum release
video, song, album, check, show,
see, making, radio, love
Acting play, role, acting, drama, played,
series, movie, actor, theater
Death dies, passed, cancer, family, hospi-
tal, dad, grandma, mom, grandpa
Give Birth baby, born, boy, pregnant, girl, lbs,
name, son, world, daughter, birth
Illness ill, hospital, feeling, sick, cold, flu,
getting, fever, doctors, cough
Surgery surgery, got, test, emergency, blood,
tumor, stomachs, hospital, pain,
brain
Sports win, game, team, season, fans,
played, winning, football, luck
Running run, race, finished, race, marathon,
ran, miles, running, finish, goal
New Car car, buy, bought, cars, get, drive,
pick, seat, color, dollar, meet
Lost Weight weight, lost, week, pounds, loss,
weeks, gym, exercise, running
Birthday birthday, come, celebrate, party,
friends, dinner, tonight, friend
Lawsuit sue, sued, file, lawsuit, lawyer, dol-
lars, illegal, court, jury.
Table 3: Example event types with top words dis-
covered by our model.
2001
event examples and trained a 43-class maximum
entropy classifier based on the following features:
? Word: The sequence of words in the tweet.
? NER: Named entity Tag.
? Dictionary: Word matching a dictionaries of
the top 40 words for each life event category
(automatically inferred by the topic model).
The feature value is the term?s probability
generated by correspondent event.
? Window: If a dictionary term exists, left and
right context words within a window of 3
words and their part-of-speech tags.
Name entity tag is assigned from Ritter et al?s
Twitter NER system (Ritter et al., 2011). Part-of-
Speech tags are assigned based on Twitter POS
package (Owoputi et al., 2013) developed by
CMU ARK Lab. Dictionary and Window are
constructed based on the topic-term distribution
obtained from the previous section.
The average precision and recall are shown in
Table 4. And as we can observe, the dictionary
(with probability) contributes a lot to the perfor-
mance and by taking into account a more compre-
hensive set of information around the key word,
classifier on All feature setting generate signifi-
cantly better performance, with 0.382 prevision
and 0.48 recall, which is acceptable considering
(1) This is is a 43-way classification with much
more negative data than positive (2) Some types of
events are very close to each other (e.g., Leaving
and Vocation). Note that recall is valued more than
precision here as false-positive examples will be
further screened in self-reported information iden-
tification process in the following section.
Feature Setting Precision Recall
Word+NER 0.204 0.326
Word+NER+Dictionary 0.362 0.433
All 0.382 0.487
Table 4: Average Performance of Multi-Class
Classifier on Different Feature Settings. Negative
examples (non important event type) are not con-
sidered.
5 Self-Reported Information
Identification
Although a message might refer to a topic cor-
responding to a life event such as marriage, the
event still might be one in which the speaker is
not directly involved. In this section we describe
the self reported event identification portion of our
pipeline, which takes output from Section 4 and
further identifies whether each tweet refers to an
event directly involving the user who publishes it.
Direct labeling of randomly sampled Twitter
messages is infeasible for the following reasons:
(1) Class imbalance: self-reported events are rela-
tively rare in randomly sampled Twitter messages.
(2) A large proportion of self-reported information
refers to mundane, everyday topics (e.g., ?I just
finished dinner!?). Fortunately, many of the tweets
retrieved from Section 3 consist of self-reported
information and describe major life events. The
candidates for annotation are therefore largely nar-
rowed down.
We manually annotated 800 positive examples
of self-reported events distributed across the event
categories identified in Section 3. We ensured
good coverage by first randomly sampling 10 ex-
amples from each category, the remainder were
sampled from the class distribution in the data.
Negative examples of self-reported information
consisted of a combination of examples from the
original dataset
8
and randomly sampled messages
gathered by searching for the top terms in each of
the pre-identified topics using the Twitter Search
interface
9
. Due to great varieties of negative sce-
narios, the negative dataset constitutes about 2500
tweets.
5.1 Features
Identifying self-reported tweet requires sophisti-
cated feature engineering. Let u denote the term
within the tweet that gets the highest possibility
generated by the correspondent topic. We experi-
mented with combinations of the following types
of features (results are presented in Table ??):
? Bigram: Bigrams within each tweet (punctu-
ation included).
? Window: A window of k ? {0, 1, 2} words
adjacent to u and their part-of-speech tags.
? Tense: A binary feature indicating past tense
identified in by the presence of past tense
verb (VBD).
? Factuality: Factuality denotes whether one
expression is presented as corresponding to
real situations in the world (Saur?? and Puste-
jovsky, 2007). We use Stanford PragBank
10
,
8
Most tweets in the bootstrapping output are positive.
9
The majority of results returned by Twitter Search are
negative examples.
10
http://compprag.christopherpotts.net/
factbank.html
2002
an extension of FactBank (Saur?? and Puste-
jovsky, 2009) which contains a list of modal
words such as ?might?, ?will?, ?want to?
etc
11
.
? I: Whether the subject of the tweet is first per-
son singular.
? Dependency: If the subject is first person
singular and the u is a verb, the dependency
path between the subject and u (or non-
dependency).
Tweet dependency paths were obtained from
(Kong et al., 2014). As the tweet parser we use
only supports one-to-one dependency path iden-
tification but no dependency properties, Depen-
dency is a binary feature. The subject of each
tweet is determined by the dependency link to the
root of the tweet from the parser.
Among the features we explore, Word encodes
the general information within the tweet. Win-
dow addresses the information around topic key
word. The rest of the features specifically address
each of the negative situations described in Chal-
lenge 2, Section 1: Tense captures past event de-
scription, Factuality filters out wishes or imagi-
nation, I and Dependency correspond to whether
the described event involves the speaker. We built
a linear SVM classifier using SVM
light
package
(Joachims, 1999).
5.2 Evaluation
Feature Setting Acc Pre Rec
Bigram+Window 0.76 0.47 0.44
Bigram+Window
+Tense+Factuality
0.77 0.47 0.46
all 0.82 0.51 0.48
Table 5: Performance for self-report information
identification regarding different feature settings.
We report performance on the task of identi-
fying self-reported information in this subsection.
We employ 5-fold cross validation and report Ac-
curacy (Accu), Prevision (Prec) and Recall (Rec)
regarding different feature settings. The Tense,
Factuality, I and Dependency features positively
contribute to performance respectively and the
best performance is obtained when all types of fea-
tures are included.
11
Due to the colloquial property of tweets, we also intro-
duced terms such as ?gonna?, ?wanna?, ?bona?.
precision recall F1
0.82 0.86 0.84
Table 7: Performance for identifying properties.
6 Event Property Extraction
Thus far we have described how to automatically
identify tweets referring to major life events. In
addition, it is desirable to extract important prop-
erties of the event, for example the name of the
university the speaker was admitted to (See Figure
1). In this section we take a supervised approach to
event property extraction, based on manually an-
notated data for a handfull of the major life event
categories automatically identified by our system.
While this approach is unlikely to scale to the di-
versity of important personal events Twitter users
are discussing, our experiments demonstrate that
event property extraction is indeed feasible.
We cast the problem of event property extrac-
tion as a sequence labeling task, using Conditional
Random Fields (Lafferty et al., 2001) for learning
and inference. To make best use of the labeled
data, we trained a unified CRF model for closely
related event categories which often share proper-
ties; the full list is presented in Table 6 and we
labeled 300 tweets in total. Features we used in-
clude:
? word token, capitalization, POS
? left and right context words within a window
of 3 and the correspondent part-of-speech
tags
? word shape, NER
? a gazetteer of universities and employers bor-
rowed from NELL
12
.
We use 5-fold cross-validation and report results
in Table 7.
7 End-to-End Experiment
The evaluation for each part of our system has
been demonstrated in the corresponding section.
We now present a real-world evaluation: to what
degree can our trained system automatically iden-
tify life events in real world.
7.1 Dataset
We constructed a gold-standard life event dataset
using annotators from Amazon?s Mechanical Turk
(Snow et al., 2008) using 2 approaches:
12
http://rtw.ml.cmu.edu/rtw/kbbrowser/
2003
Life Event Property
(a) Acceptance, Graduation Name of University/College
(b) Wedding, Engagement, Falling love Name of Spouse/ partner/ bf/ gf
(c) Getting a job, interview, internship Name of Enterprise
(d) Moving to New Places, Trip, Vocation, Leaving Place, Origin, Destination
(e) Winning Award Name of Award, Prize
Table 6: Labeling Event Property.
? Ask Twitter users to label their own tweets
(Participants include friends, colleagues of
the authors and Turkers from Amazon Me-
chanical Turk
13
).
? Ask Turkers to label other people?s tweets.
For option 1, we asked participants to directly la-
bel their own published tweets. For option 2, for
each tweet, we employed 2 Turkers. Due to the
ambiguity in defining life events, the value co-
hen?s kappa
14
as a measure of inter-rater agree-
ment is 0.54; this does not show significant inter-
annotator agreement. The authors examined dis-
agreements and also verified all positively labeled
tweets. The resulting dataset contains around 900
positive tweets and about 60,000 negative tweets.
To demonstrate the advantage of leveraging
large quantities of unlabeled data, the first base-
line we investigate is a Supervised model which is
trained on the manually annotated labeled dataset,
and evaluated using 5 fold cross validation. Our
Supervised baseline consists of a linear SVM
classifier using bag of words, NER and POS fea-
tures. We also tested a second baseline that
combines Supervised algorithm with an our self-
reported information classifier, denoted as Super-
vised+Self.
Results are reported in Table 8; as we can ob-
serve, the fully supervised approach is not suitable
for this task with only one digit F1 score. The
explanations are as follows: (1) the labeled data
can only cover a small proportion of life events
(2) supervised learning does not separate impor-
tant event categories and will therefore classify
any tweet with highly weighted features (e.g., the
mention of ?I? or ?marriage?) as positive. By us-
ing an additional self-reported information classi-
fier in Supervised+Self, we get a significant boost
in precision with a minor recall loss.
13
https://www.mturk.com/mturk/welcome
14
http://en.wikipedia.org/wiki/Cohen?s_
kappa
Approach Precision Recall
Our approach 0.62 0.48
Supervised 0.13 0.20
Supervised+Self 0.25 0.18
Table 8: Performance for different approaches for
identifying life events in real world.
Approach Precision Recall
Step 1 0.65 0.36
Step 2 0.64 0.43
Step 3 0.62 0.48
Table 9: Performance for different steps of boot-
strapping for identifying life events in real world.
Another interesting question is to what degree
the bootstrapping contributes to the final results.
We keep the self-reported information classifier
fixed (though it?s based the ultimate identified
data source), and train the personal event classifier
based on topic distributions identified from each
of the three steps of bootstrapping
15
. Precision
and recall at various stages of bootstrapping are
presented in Table 9. As bootstrapping continues,
the precision remains roughly constant, but recall
increases as more life events and CONGRATULA-
TIONS and CONDOLENCES are discovered.
8 Related Work
Our work is related to three lines of NLP re-
searches. (1) user-level information extraction on
social media (2) public event extraction on social
media. (3) Data harvesting in Information Extrac-
tion, each of which contains large amount of re-
lated work, to which we can not do fully justice.
User Information Extraction from Twitter
Some early approaches towards understanding
user level information on social media is focused
on user profile/attribute prediction (e.g.,(Ciot et
al., 2013)) user-specific content extraction (Diao
15
which are 24, 38, 42-class classifiers, where 24, 38, 42
denoted the number of topics discovered in each step of boot-
strapping (see Figure 5).
2004
et al., 2012; Diao and Jiang, 2013; Li et al., 2014)
or user personalization (Low et al., 2011) identifi-
cation.
The problem of user life event extraction was
first studied by Li and Cardie?s (2014). They at-
tempted to construct a chronological timeline for
Twitter users from their published tweets based on
two criterion: a personal event should be personal
and time-specific. Their system does not explic-
itly identify a global category of life events (and
tweets discussing correspondent event) but identi-
fies the topics/events that are personal and time-
specific to a given user using an unsupervised ap-
proach, which helps them avoids the nuisance of
explicit definition for life event characteristics and
acquisition of labeled data. However, their sys-
tem has the short-coming that each personal topic
needs to be adequately discussed by the user and
their followers in order to be detected
16
.
Public Event Extraction from Twitter Twitter
serves as a good source for event detection owing
to its real time nature and large number of users.
These approaches include identifying bursty pub-
lic topics (e.g.,(Diao et al., 2012)), topic evolution
(Becker et al., 2011) or disaster outbreak (Sakaki
et al., 2010; Li and Cardie, 2013) by spotting the
increase/decrease of word frequency. Some other
approaches are focused on generating a structured
representation of events (Ritter et al., 2012; Ben-
son et al., 2011).
Data Acquisition in Information Extraction
Our work is also related with semi-supervised data
harvesting approaches, the key idea of which is
that some patterns are learned based on seeds.
They are then used to find additional terms, which
are subsequently used as new seeds in the patterns
to search for additional new patterns (Kozareva
and Hovy, 2010b; Davidov et al., 2007; Riloff
et al., 1999; Igo and Riloff, 2009; Kozareva et
al., 2008). Also related approaches are distant or
weakly supervision (Mintz et al., 2009; Craven et
al., 1999; Hoffmann et al., 2011) that rely on avail-
able structured data sources as a weak source of
supervision for pattern extraction from related text
corpora.
16
The reason is that topic models use word frequency for
topic modeling.
9 Conclusion and Discussion
In this paper, we propose a pipelined system for
major life event extraction from Twitter. Experi-
mental results show that our model is able to ex-
tract a wide variety of major life events.
The key strategy adopted in this work is to ob-
tain a relatively clean training dataset from large
quantity of Twitter data by relying on minimum
efforts of human supervision, and sometimes is at
the sacrifice of recall. To achieve this goal, we rely
on a couple of restrictions and manual screenings,
such as relying on replies, LDA topic identifica-
tion and seed screening. Each part of system de-
pends on the early steps. For example, topic clus-
tering in Section 3 not only offers training data for
event identification in Section 4, but prepares the
training data for self-information identification in
Section 5. .
We acknowledge that our approach is not
perfect due to the following ways: (1) The system
is only capable of discovering a few categories
of life events with many others left unidentified.
(2) Each step of the system will induce errors and
negatively affected the following parts. (3) Some
parts of evaluations are not comprehensive due
to the lack of gold-standard data. (4) Among all
pipelines, event property identification in Section
6 still requires full supervision in CRF model,
making it hard to scale to every event type
17
.
How to address these aspects and generate a more
accurate, comprehensive and fine-grained life
event list for Twitter users constitute our further
work.
Acknowledgements
A special thanks is owned to Myle Ott for sug-
gestions on bootstrapping procedure in data har-
vesting. The authors want to thank Noah Smith,
Chris Dyer and Alok Kothari for useful com-
ments, discussions and suggestions regarding dif-
ferent steps of the system and evaluations. We
thank Lingpeng Kong and members of Noah?s
ARK group at CMU for providing the tweet de-
pendency parser. All data used in this work is ex-
tracted from CMU Twitter Warehouse maintained
by Brendan O?Connor, to whom we want to ex-
press our gratitude.
17
We view weakly supervised life event property extrac-
tion as an interesting direction for future work.
2005
References
Hila Becker, Mor Naaman, and Luis Gravano. 2011.
Beyond trending topics: Real-world event identifi-
cation on twitter. ICWSM, 11:438?441.
Cosmin Adrian Bejan, Matthew Titsworth, Andrew
Hickl, and Sanda M Harabagiu. 2009. Nonparamet-
ric bayesian models for unsupervised event corefer-
ence resolution. In NIPS, pages 73?81.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Annual
Meeting-Association For Computational Linguis-
tics, volume 45, page 232.
Qiming Diao and Jing Jiang. 2013. A unified model
for topics, events and users on twitter. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1869?1879.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng
Lim. 2012. Finding bursty topics from microblogs.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 536?544. Association for
Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
2007. Hidden topic markov models. In Inter-
national Conference on Artificial Intelligence and
Statistics, pages 163?170.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 541?550. Association for Compu-
tational Linguistics.
Sean P Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with web-based corrobora-
tion. In Proceedings of the Workshop on Unsuper-
vised and Minimally Supervised Learning of Lexical
Semantics, pages 18?26. Association for Computa-
tional Linguistics.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah Smith. 2014. A dependency parser for tweets.
In EMNLP.
Zornitsa Kozareva and Eduard Hovy. 2010a. Learn-
ing arguments and supertypes of semantic relations
using recursive patterns. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1482?1491. Association
for Computational Linguistics.
Zornitsa Kozareva and Eduard Hovy. 2010b. Not
all seeds are equal: Measuring the quality of text
mining seeds. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618?626. Association for Computa-
tional Linguistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048?1056.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Jiwei Li and Claire Cardie. 2013. Early stage
influenza detection from twitter. arXiv preprint
arXiv:1309.7340.
Jiwei Li and Claire Cardie. 2014. Timeline generation:
Tracking individuals on twitter. WWW, 2014.
Jiwei Li, Alan Ritter, and Eduard Hovy. 2014.
Weakly supervised user profile extraction from twit-
ter. ACL.
Yucheng Low, Deepak Agarwal, and Alexander J
Smola. 2011. Multiple domain user personaliza-
tion. In Proceedings of the 17th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 123?131. ACM.
David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
2006
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262?
272. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100?108. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI/IAAI, pages 474?479.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012.
Open domain event extraction from twitter. In Pro-
ceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 1104?1112. ACM.
Kirk Roberts and Sanda M Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detec-
tion of argument coercions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 980?990. Association for
Computational Linguistics.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World wide
web, pages 851?860. ACM.
Roser Saur?? and James Pustejovsky. 2007. Deter-
mining modality and factuality for text entailment.
In Semantic Computing, 2007. ICSC 2007. Interna-
tional Conference on, pages 509?516. IEEE.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. Language
resources and evaluation, 43(3):227?268.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254?263. Association for Computational
Linguistics.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1456?1466. Association
for Computational Linguistics.
2007
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039?2048,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Model of Coherence Based on Distributed Sentence Representation
Jiwei Li
1
and Eduard Hovy
3
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
3
Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
jiweil@stanford.edu ehovy@andrew.cmu.edu
Abstract
Coherence is what makes a multi-sentence
text meaningful, both logically and syn-
tactically. To solve the challenge of or-
dering a set of sentences into coherent or-
der, existing approaches focus mostly on
defining and using sophisticated features
to capture the cross-sentence argumenta-
tion logic and syntactic relationships. But
both argumentation semantics and cross-
sentence syntax (such as coreference and
tense rules) are very hard to formalize. In
this paper, we introduce a neural network
model for the coherence task based on
distributed sentence representation. The
proposed approach learns a syntactico-
semantic representation for sentences au-
tomatically, using either recurrent or re-
cursive neural networks. The architecture
obviated the need for feature engineering,
and learns sentence representations, which
are to some extent able to capture the
?rules? governing coherent sentence struc-
ture. The proposed approach outperforms
existing baselines and generates the state-
of-art performance in standard coherence
evaluation tasks
1
.
1 Introduction
Coherence is a central aspect in natural language
processing of multi-sentence texts. It is essen-
tial in generating readable text that the text plan-
ner compute which ordering of clauses (or sen-
tences; we use them interchangeably in this paper)
is likely to support understanding and avoid con-
fusion. As Mann and Thompson (1988) define it,
A text is coherent when it can be ex-
plained what role each clause plays with
regard to the whole.
1
Code available at stanford.edu/
?
jiweil/ or by
request from the first author.
Several researchers in the 1980s and 1990s ad-
dressed the problem, the most influential of
which include: Rhetorical Structure Theory (RST;
(Mann and Thompson, 1988)), which defined
about 25 relations that govern clause interde-
pendencies and ordering and give rise to text
tree structures; the stepwise assembly of seman-
tic graphs to support adductive inference toward
the best explanation (Hobbs et al., 1988); Dis-
course Representation Theory (DRT; (Lascarides
and Asher, 1991)), a formal semantic model of
discourse contexts that constrain coreference and
quantification scoping; the model of intention-
oriented conversation blocks and their stack-based
queueing to model attention flow (Grosz and Sid-
ner, 1986), and more recently an inventory of a
hundred or so binary inter-clause relations and as-
sociated annotated corpus (Penn Discourse Tree-
bank. Work in text planning implemented some
of these models, especially operationalized RST
(Hovy, 1988) and explanation relations (Moore
and Paris, 1989) to govern the planning of coher-
ent paragraphs. Other computational work defined
so called schemas (McKeown, 1985), frames with
fixed sequences of clause types to achieve stereo-
typical communicative intentions.
Little of this work survives. Modern research
tries simply to order a collection of clauses or sen-
tences without giving an account of which order(s)
is/are coherent or what the overall text structure
is. The research focuses on identifying and defin-
ing a set of increasingly sophisticated features by
which algorithms can be trained to propose order-
ings. Features being explored include the clause
entities, organized into a grid (Lapata and Barzi-
lay, 2005; Barzilay and Lapata, 2008), coreference
clues to ordering (Elsner and Charniak, 2008),
named-entity categories (Eisner and Charniak,
2011), syntactic features (Louis and Nenkova,
2012), and others. Besides being time-intensive
(feature engineering usually requites considerable
2039
Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples.
effort and can depend greatly on upstream feature
extraction algorithms), it is not immediately ap-
parent which aspects of a clause or a coherent text
to consider when deciding on ordering. More im-
portantly, the features developed to date are still
incapable of fully specifying the acceptable order-
ing(s) within a context, let alone describe why they
are coherent.
Recently, deep architectures, have been applied
to various natural language processing tasks (see
Section 2). Such deep connectionist architectures
learn a dense, low-dimensional representation of
their problem in a hierarchical way that is capa-
ble of capturing both semantic and syntactic as-
pects of tokens (e.g., (Bengio et al., 2006)), en-
tities, N-grams (Wang and Manning, 2012), or
phrases (Socher et al., 2013). More recent re-
searches have begun looking at higher level dis-
tributed representations that transcend the token
level, such as sentence-level (Le and Mikolov,
2014) or even discourse-level (Kalchbrenner and
Blunsom, 2013) aspects. Just as words combine
to form meaningful sentences, can we take advan-
tage of distributional semantic representations to
explore the composition of sentences to form co-
herent meanings in paragraphs?
In this paper, we demonstrate that it is feasi-
ble to discover the coherent structure of a text
using distributed sentence representations learned
in a deep learning framework. Specifically, we
consider a WINDOW approach for sentences, as
shown in Figure 1, where positive examples are
windows of sentences selected from original arti-
cles generated by humans, and negatives examples
are generated by random replacements
2
. The se-
mantic representations for terms and sentences are
obtained through optimizing the neural network
framework based on these positive vs negative ex-
2
Our approach is inspired by Collobert et al.?s idea (2011)
that a word and its context form a positive training sample
while a random word in that same context gives a negative
training sample, when training word embeddings in the deep
learning framework.
amples and the proposed model produces state-of-
art performance in multiple standard evaluations
for coherence models (Barzilay and Lee, 2004).
The rest of this paper is organized as follows:
We describe related work in Section 2, then de-
scribe how to obtain a distributed representation
for sentences in Section 3, and the window compo-
sition in Section 4. Experimental results are shown
in Section 5, followed by a conclusion.
2 Related Work
Coherence In addition to the early computa-
tional work discussed above, local coherence was
extensively studied within the modeling frame-
work of Centering Theory (Grosz et al., 1995;
Walker et al., 1998; Strube and Hahn, 1999; Poe-
sio et al., 2004), which provides principles to form
a coherence metric (Miltsakaki and Kukich, 2000;
Hasler, 2004). Centering approaches suffer from a
severe dependence on manually annotated input.
A recent popular approach is the entity grid
model introduced by Barzilay and Lapata (2008)
, in which sentences are represented by a vec-
tor of discourse entities along with their gram-
matical roles (e.g., subject or object). Proba-
bilities of transitions between adjacent sentences
are derived from entity features and then concate-
nated to a document vector representation, which
is used as input to machine learning classifiers
such as SVM. Many frameworks have extended
the entity approach, for example, by pre-grouping
entities based on semantic relatedness (Filippova
and Strube, 2007) or adding more useful types
of features such as coreference (Elsner and Char-
niak, 2008), named entities (Eisner and Charniak,
2011), and discourse relations (Lin et al., 2011).
Other systems include the global graph model
(Guinaudeau and Strube, 2013) which projects en-
tities into a global graph. Louis and Nenkova
(2012) introduced an HMM system in which the
coherence between adjacent sentences is modeled
by a hidden Markov framework captured by the
2040
Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The
bottom layer represents word vectors in the sentence. The top layer h
s
denotes the resulting sentence
vector.
transition rules of different topics.
Recurrent and Recursive Neural Networks In
the context of NLP, recurrent neural networks
view a sentence as a sequence of tokens and in-
corporate information from the past (i.e., preced-
ing tokens) (Schuster and Paliwal, 1997; Sutskever
et al., 2011) for acquisition of the current output.
At each step, the recurrent network takes as input
both the output of previous steps and the current
token, convolutes the inputs, and forwards the re-
sult to the next step. It has been successfully ap-
plied to tasks such as language modeling (Mikolov
et al., 2010) and spoken language understanding
(Mesnil et al., 2013). The advantage of recur-
rent network is that it does not depend on exter-
nal deeper structure (e.g., parse tree) and is easy to
implement. However, in the recurrent framework,
long-distance dependencies are difficult to capture
due to the vanishing gradient problem (Bengio et
al., 1994); two tokens may be structurally close to
each other, even though they are far away in word
sequence
3
.
Recursive neural networks comprise another
class of architecture, one that relies and operates
on structured inputs (e.g., parse trees). It com-
putes the representation for each parent based on
its children iteratively in a bottom-up fashion. A
series of variations have been proposed, each tai-
lored to different task-specific requirements, such
as Matrix-Vector RNN (Socher et al., 2012) that
represents every word as both a vector and a ma-
trix, or Recursive Neural Tensor Networks (Socher
et al., 2013) that allow the model to have greater
3
For example, a verb and its corresponding direct object
can be far away in terms of tokens if many adjectives lies in
between, but they are adjacent in the parse tree (Irsoy and
Cardie, 2013).
interactions between the input vectors. Many tasks
have benefited from this recursive framework, in-
cluding parsing (Socher et al., 2011b), sentiment
analysis (Socher et al., 2013), and paraphrase de-
tection (Socher et al., 2011a).
2.1 Distributed Representations
Both recurrent and recursive networks require a
vector representation of each input token. Dis-
tributed representations for words were first pro-
posed in (Rumelhart et al., 1988) and have been
successful for statistical language modeling (El-
man, 1990). Various deep learning architectures
have been explored to learn these embeddings in
an unsupervised manner from a large corpus (Ben-
gio et al., 2006; Collobert and Weston, 2008;
Mnih and Hinton, 2007; Mikolov et al., 2013),
which might have different generalization capabil-
ities and are able to capture the semantic mean-
ings depending on the specific task at hand. These
vector representations can to some extent cap-
ture interesting semantic relationships, such as
King?man ? Queue?woman (Mikolov et al.,
2010), and recently have been successfully used
in various NLP applications, including named en-
tity recognition, tagging, segmentation (Wang et
al., 2013), and machine translation (e.g.,(Collobert
and Weston, 2008; Zou et al., 2013)).
3 Sentence Model
In this section, we demonstrate the strategy
adopted to compute a vector for a sentence given
the sequence of its words and their embeddings.
We implemented two approaches, Recurrent and
Recursive neural networks, following the de-
scriptions in for example (Mikolov et al., 2010;
Sutskever et al., 2011; Socher et al., 2013). As
2041
the details of both approaches can be readily found
there, we make this section brief and omit the de-
tails for brevity.
Let s denote a sentence, comprised of a se-
quence of words s = {w
1
, w
2
, ..., w
n
s
}, where n
s
denotes the number of words within sentence s.
Each word w is associated with a specific vector
embedding e
w
= {e
1
w
, e
2
w
, ..., e
K
w
}, where K de-
notes the dimension of the word embedding. We
wish to compute the vector representation for cur-
rent sentence h
s
= {h
1
s
, h
2
s
, ..., h
K
s
}.
Recurrent Sentence Representation (Recur-
rent) The recurrent network captures certain
general considerations regarding sentential com-
positionality. As shown in Figure 2 (a), for sen-
tence s, recurrent network successively takes word
w
i
at step i, combines its vector representation e
t
w
with former input h
i?1
from step i? 1, calculates
the resulting current embedding h
t
, and passes it
to the next step. The standard recurrent network
calculates h
t
as follows:
h
t
= f(V
Recurrent
?h
t?1
+W
Recurrent
?e
t
w
+b
Recurrent
)
(1)
where W
Recurrent
and V
Recurrent
are K ?K ma-
trixes. b
Recurrent
denotes K ? 1 bias vector and
f = tanh is a standard element-wise nonlinearity.
Note that calculation for representation at time
t = 1 is given by:
h
1
= f(V
Recurrent
?h
0
+W
Recurrent
?e
1
w
+b
Recurrent
)
(2)
where h
0
denotes the global sentence starting vec-
tor.
Recursive Sentence Representation (Recursive)
Recursive sentence representation relies on the
structure of parse trees, where each leaf node of
the tree corresponds to a word from the original
sentence. It computes a representation for each
parent node based on its immediate children re-
cursively in a bottom-up fashion until reaching the
root of the tree. Concretely, for a given parent p
in the tree and its two children c
1
(associated with
vector representation h
c
1
) and c
2
(associated with
vector representation h
c
2
), standard recursive net-
works calculates h
p
for p as follows:
h
p
= f(W
Recursive
? [h
c
1
, h
c
2
] + b
Recursive
) (3)
where [h
c
1
, h
c
2
] denotes the concatenating vec-
tor for children vector representation h
c
1
and h
c
2
.
W
Recursive
is a K ? 2K matrix and b
Recursive
is
the 1?K bias vector. f(?) is tanh function.
Recursive neural models compute parent vec-
tors iteratively until the root node?s representation
is obtained, and use the root embedding to repre-
sent the whole sentence, as shown in Figure 2 (b).
4 Coherence Model
The proposed coherence model adopts a window
approach (Collobert et al., 2011), in which we
train a three-layer neural network based on a slid-
ing windows of L sentences.
4.1 Sentence Convolution
We treat a window of sentences as a clique C and
associate each clique with a tag y
C
that takes the
value 1 if coherent, and 0 otherwise
4
. As shown in
Figure 1, cliques taken from original articles are
treated as coherent and those with sentences ran-
domly replaced are used as negative examples. .
The sentence convolution algorithm adopted in
this paper is defined by a three-layer neural net-
work, i.e., sentence-level input layer, hidden layer,
and overall output layer as shown in Figure 3. For-
mally, each clique C takes as input a (L?K)? 1
vector h
C
by concatenating the embeddings of
all its contained sentences, denoted as h
C
=
[h
s
1
, h
s
2
, ..., h
s
L
]. (Note that if we wish to clas-
sify the first and last sentences and include their
context, we require special beginning and ending
sentence vectors, which are defined as h
<S>
for
s
start
and h
</S>
for s
end
respectively.)
Let H denote the number of neurons in the hid-
den (second) layer. Then each of the hidden lay-
ers takes as input h
C
and performs the convolution
using a non-linear tanh function, parametrized by
W
sen
and b
sen
. The concatenating output vector
for hidden layers, defined as q
C
, can therefore be
rewritten as:
q
C
= f(W
sen
? h
C
+ b
sen
) (4)
where W
sen
is a H? (L?K) dimensional matrix
and b
sen
is a H ? 1 dimensional bias vector.
4
instead of a binary classification (correct/incorrect), an-
other commonly used approach is the contrastive approach
that minimizes the score function max(0, 1 ? s + s
c
) (Col-
lobert et al., 2011; Smith and Eisner, 2005). s denotes the
score of a true (coherent) window and s
c
the score of a cor-
rupt (containing incoherence) one) in an attempt to make the
score of true windows larger and corrupt windows smaller.
We tried the contrastive one for both recurrent and recursive
networks but the binary approach constantly outperformed
the contrastive one in this task.
2042
Figure 3: An example of coherence model based on a window of sentences (clique).
The output layer takes as input q
C
and generates
a scalar using linear function U
T
q
C
+b. A sigmod
function is then adopted to project the value to a
[0,1] probability space, which can be interpreted
as the probability of whether one clique is coher-
ent or not. The execution at the output layer can
be summarized as:
p(y
C
= 1) = sigmod(U
T
q
C
+ b) (5)
where U is anH?1 vector and b denotes the bias.
4.2 Training
In the proposed framework, suppose we have M
training samples, the cost function for recurrent
neural network with regularization on the training
set is given by:
J(?) =
1
M
?
C?trainset
{?y
C
log[p(y
C
= 1)]
? (1? y
C
) log[1? p(y
C
= 1)]}+
Q
2M
?
???
?
2
(6)
where
? = [W
Recurrent
,W
sen
, U
sen
]
The regularization part is paralyzed by Q to avoid
overfitting. A similar loss function is applied to
the recursive network with only minor parameter
altering that is excluded for brevity.
To minimize the objective J(?), we use the di-
agonal variant of AdaGrad (Duchi et al., 2011)
with minibatches, which is widely applied in deep
learning literature (e.g.,(Socher et al., 2011a; Pei
et al., 2014)). The learning rate in AdaGrad is
adapting differently for different parameters at dif-
ferent steps. Concretely, for parameter updates, let
g
i
?
denote the subgradient at time step for param-
eter ?
i
, which is obtained from backpropagation
5
,
the parameter update at time step t is given by:
?
?
= ?
??1
?
?
?
?
t=0
?
g
i2
?
g
i
?
(7)
where ? denotes the learning rate and is set to 0.01
in our approach. Optimal performance is achieved
when batch size is set between 20 and 30.
4.3 Initialization
Elements in W
sen
are initialized by randomly
drawing from the uniform distribution [?, ],
where  =
?
6
?
H+K?L
as suggested in (Collobert
et al., 2011). W
recurrent
, V
recurrent
, W
recursive
and h
0
are initialized by randomly sampling from
a uniform distribution U(?0.2, 0.2). All bias vec-
tors are initialized with 0. Hidden layer numberH
is set to 100.
Word embeddings {e} are borrowed from
Senna (Collobert et al., 2011; Collobert, 2011).
The dimension for these embeddings is 50.
5 Experiments
We evaluate the proposed coherence model on two
common evaluation approaches adopted in exist-
ing work (Barzilay and Lapata, 2008; Louis and
Nenkova, 2012; Elsner et al., 2007; Lin et al.,
2011): Sentence Ordering and Readability Assess-
ment.
5.1 Sentence Ordering
We follow (Barzilay and Lapata, 2008; Louis and
Nenkova, 2012; Elsner et al., 2007; Lin et al.,
5
For more details on backpropagation through RNNs, see
Socher et al. (2010).
2043
2011) that all use pairs of articles, one containing
the original document order and the other a ran-
dom permutation of the sentences from the same
document. The pairwise approach is predicated
on the assumption that the original article is al-
ways more coherent than a random permutation;
this assumption has been verified in Lin et al.?s
work (2011).
We need to define the coherence score S
d
for
a given document d, where d is comprised of a
series of sentences, d = {s
1
, s
2
, .., s
N
d
}, and N
d
denotes the number of sentences within d. Based
on our clique definition, document d is comprised
of N
d
cliques. Taking window size L = 3 as ex-
ample, cliques generated from document d appear
as follows:
< s
start
, s
1
, s
2
>,< s
1
, s
2
, s
3
>, ...,
< s
N
d
?2
, s
N
d
?1
, s
N
d
>,< s
N
d
?1
, s
N
d
, s
end
>
The coherence score for a given document S
d
is
the probability that all cliques within d are coher-
ent, which is given by:
S
d
=
?
C?d
p(y
C
= 1) (8)
For document pair < d
1
, d
2
> in our task, we
would say document d
1
is more coherent than d
2
if
S
d
1
> S
d
2
(9)
5.1.1 Dataset
We use two corpora that are widely employed
for coherence prediction (Barzilay and Lee, 2004;
Barzilay and Lapata, 2008; Elsner et al., 2007).
One contains reports on airplane accidents from
the National Transportation Safety Board and the
other contains reports about earthquakes from the
Associated Press. These articles are about 10
sentences long and usually exhibit clear sentence
structure. For preprocessing, we only lowercase
the capital letters to match with tokens in Senna
word embeddings. In the recursive network, sen-
tences are parsed using the Stanford Parser
6
and
then transformed into binary trees. The accident
corpus ends up with a vocabulary size of 4758 and
an average of 10.6 sentences per document. The
earthquake corpus contains 3287 distinct terms
and an average of 11.5 sentences per document.
6
http://nlp.stanford.edu/software/
lex-parser.shtml
For each of the two corpora, we have 100 arti-
cles for training and 100 (accidents) and 99 (earth-
quakes) for testing. A maximum of 20 random
permutations were generated for each test arti-
cle to create the pairwise data (total of 1986 test
pairs for the accident corpus and 1956 for earth-
quakes)
7
.
Positive cliques are taken from original training
documents. For easy training, rather than creating
negative examples by replacing centered sentences
randomly, the negative dataset contains cliques
where centered sentences are replaced only by
other sentences within the same document.
5.1.2 Training and Testing
Despite the numerous parameters in the deep
learning framework, we tune only two principal
ones for each setting: window size L (tried on
{3, 5, 7}) and regularization parameterQ (tried on
{0.01, 0.1, 0.25, 0.5, 1.0, 1.25, 2.0, 2.5, 5.0}). We
trained parameters using 10-fold cross-validation
on the training data. Concretely, in each setting,
90 documents were used for training and evalua-
tion was done on the remaining articles, following
(Louis and Nenkova, 2012). After tuning, the final
model was tested on the testing set.
5.1.3 Model Comparison
We report performance of recursive and recurrent
networks. We also report results from some popu-
lar approaches in the literature, including:
Entity Grid Model : Grid model (Barzilay and
Lapata, 2008) obtains the best performance when
coreference resolution, expressive syntactic infor-
mation, and salience-based features are incorpo-
rated. Entity grid models represent each sentence
as a column of a grid of features and apply ma-
chine learning methods (e.g., SVM) to identify the
coherent transitions based on entity features (for
details of entity models see (Barzilay and Lapata,
2008)). Results are directly taken from Barzilay
and Lapata?s paper (2008).
HMM : Hidden-Markov approach proposed by
Louis and Nenkova (2012) to model the state
(cluster) transition probability in the coherent con-
text using syntactic features. Sentences need to be
clustered in advance where the number of clus-
ters is tuned as a parameter. We directly take
7
Permutations are downloaded from http:
//people.csail.mit.edu/regina/coherence/
CLsubmission/.
2044
Acci Earthquake Average
Recursive 0.864 0.976 0.920
Recurrent 0.840 0.951 0.895
Entity Grid 0.904 0.872 0.888
HMM 0.822 0.938 0.880
HMM+Entity 0.842 0.911 0.877
HMM+Content 0.742 0.953 0.848
Graph 0.846 0.635 0.740
Table 1: Comparison of Different Coherence
Frameworks. Reported baseline results are among
the best performance regarding each approach is
reprinted from prior work from (Barzilay and Lap-
ata, 2008; Louis and Nenkova, 2012; Guinaudeau
and Strube, 2013).
the results from Louis and Nenkova?s paper and
report the best results among different combi-
nations of parameter and feature settings
8
. We
also report performances of models from Louis
and Nenkova?s work that combine HMM and en-
tity/content models in a unified framework.
Graph Based Approach : Guinaudeau and
Strube (2013) extended the entity grid model to
a bipartite graph representing the text, where the
entity transition information needed for local co-
herence computation is embedded in the bipartite
graph. The Graph Based Approach outperforms
the original entity approach in some of feature set-
tings (Guinaudeau and Strube, 2013).
As can be seen in Table 1, the proposed frame-
works (both recurrent and recursive) obtain state-
of-art performance and outperform all existing
baselines by a large margin. One interpretation
is that the abstract sentence vector representations
computed by the deep learning framework is more
powerful in capturing exactly the relevant the se-
mantic/logical/syntactic features in coherent con-
texts than features or other representations devel-
oped by human feature engineering are.
Another good quality of the deep learning
framework is that it can be trained easily and
makes unnecessary the effort required of feature
engineering. In contrast, almost all existing base-
lines and other coherence methods require sophis-
ticated feature selection processes and greatly rely
on external feature extraction algorithm.
The recurrent network is easier to implement
than the recursive network and does not rely on
external resources (i.e., parse trees), but the recur-
sive network obtains better performance by build-
8
The details for information about parameter and feature
of best setting can be found in (Louis and Nenkova, 2012).
ing the convolution on parse trees rather than sim-
ply piling up terms within the sentence, which is
in line with common expectation.
Both recurrent and recursive models obtain bet-
ter performance on the Earthquake than the Acci-
dent dataset. Scrutiny of the corpus reveals that
articles reporting earthquakes exhibit a more con-
sistent structure: earthquake outbreak, describing
the center and intensity of the earthquake, injuries
and rescue operations, etc., while accident articles
usually exhibit more diverse scenarios.
5.2 Readability Assessment
Barzilay and Lapata (2008) proposed a readability
assessment task for stylistic judgments about the
difficulty of reading a document. Their approach
combines a coherence system with Schwarm and
Ostendorf?s (2005) readability features to clas-
sify documents into two categories, more read-
able (coherent) documents and less readable ones.
The evaluation accesses the ability to differentiate
?easy to read? documents from difficult ones of
each model.
5.2.1 Dataset
Barzilay and Lapata?s (2008) data corpus is
from the Encyclopedia Britannica and the
Britannica Elementary, the latter being a new
version targeted at children. Both versions con-
tain 107 articles. The Encyclopedia Britannica
corpus contains an average of 83.1 sentences
per document and the Britannica Elementary
contains 36.6. The encyclopedia lemmas are
written by different authors and consequently
vary considerably in structure and vocabulary
choice. Early researchers assumed that the chil-
dren version (Britannica Elementary) is easier
to read, hence more coherent than documents in
Encyclopedia Britannica. This is a somewhat
questionable assumption that needs further inves-
tigation.
5.2.2 Training and Testing
Existing coherence approaches again apply a pair-
wise ranking strategy and the article associated
with the higher score is considered to be the more
readable. As the replacement strategy for gener-
ating negative example is apparently not well fit-
ted to this task, we adopted the following training
framework: we use all sliding windows of sen-
tences from coherent documents (documents from
Britannica Elementary) as positive examples,
2045
Approach Accuracy
Recurrent 0.803
Recursive 0.828
Graph Approach 0.786
Entity 0.509
S&O 0.786
Entity+S&O 0.888
Table 2: Comparison of Different Coherence
Frameworks on Readability Assessment. Re-
ported baselines results are are taken from (Barzi-
lay and Lapata, 2008; Guinaudeau and Strube,
2013). S&O: Schwarm and Ostendorf (2005).
and cliques from Encyclopedia Britannica as
negative examples, and again apply Eq. 6 for train-
ing and optimization. During testing, we turn to
Equations 8 and 9 for pairwise comparison. We
adopted five-fold cross-validation in the same way
as in (Barzilay and Lapata, 2008; Guinaudeau and
Strube, 2013) for fair comparison. Parameters
were tuned within each training set also using five-
fold cross-validation. Parameters to tune included
window size L and regularization parameter Q.
5.3 Results
We report results of the proposed approaches in
the work along with entity model (Barzilay and
Lapata, 2008) and graph based approach (Elsner
and Charniak, 2008) in Table 2. The tabs shows
that deep learning approaches again significantly
outperform Entry and Global Approach baselines
and are nearly comparable to the combination of
entity and S&O features. Again, the recursive
network outperforms the recurrent network in this
task.
6 Conclusion
In this paper, we apply two neural network
approaches to the sentence-ordering (coherence)
task, using compositional sentence representations
learned by recurrent and recursive composition.
The proposed approach obtains state-of-art per-
formance on the standard coherence evaluation
tasks.
Acknowledgements
The authors want to thank Richard Socher and
Pradeep Dasigi for the clarification of deep learn-
ing techniques. We also thank the three anony-
mous EMNLP reviewers for helpful comments.
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL,
pages 113?120.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Micha Eisner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
125?129. Association for Computational Linguis-
tics.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179?211.
Micha Elsner and Eugene Charniak. 2008.
Coreference-inspired coherence modeling. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Hu-
man Language Technologies: Short Papers, pages
41?44. Association for Computational Linguistics.
Micha Elsner, Joseph L Austerweil, and Eugene Char-
niak. 2007. A unified local and global model for
discourse coherence. In HLT-NAACL, pages 436?
443.
2046
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 139?142. Association for Computational Lin-
guistics.
Barbara J Grosz and Candace L Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational linguistics, 12(3):175?204.
Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational linguis-
tics, 21(2):203?225.
Camille Guinaudeau and Michael Strube. 2013.
Graph-based local coherence modeling. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 93?103.
Laura Hasler. 2004. An investigation into the use of
centering transitions for summarisation. In Proceed-
ings of the 7th Annual CLUK Research Colloquium,
pages 100?107.
Jerry R Hobbs, Mark Stickel, Paul Martin, and Dou-
glas Edwards. 1988. Interpretation as abduction. In
Proceedings of the 26th annual meeting on Associ-
ation for Computational Linguistics, pages 95?103.
Association for Computational Linguistics.
Eduard H Hovy. 1988. Planning coherent multisenten-
tial text. In Proceedings of the 26th annual meet-
ing on Association for Computational Linguistics,
pages 163?169. Association for Computational Lin-
guistics.
Ozan Irsoy and Claire Cardie. 2013. Bidirectional re-
cursive neural networks for token-level labeling with
structure. arXiv preprint arXiv:1312.0493.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. arXiv preprint arXiv:1306.3584.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and represen-
tations. In IJCAI, volume 5, pages 1085?1090.
Alex Lascarides and Nicholas Asher. 1991. Discourse
relations and defeasible knowledge. In Proceedings
of the 29th annual meeting on Association for Com-
putational Linguistics, pages 55?62. Association for
Computational Linguistics.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 997?1006. Association for Computational
Linguistics.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157?1168. As-
sociation for Computational Linguistics.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Kathleen R McKeown. 1985. Discourse strategies for
generating natural-language text. Artificial Intelli-
gence, 27(1):1?41.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. Interspeech.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Eleni Miltsakaki and Karen Kukich. 2000. The role
of centering theory?s rough-shift in the teaching and
evaluation of writing skills. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 408?415. Association for
Computational Linguistics.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
Johanna D Moore and Cecile L Paris. 1989. Planning
text for advisory dialogues. In Proceedings of the
27th annual meeting on Association for Computa-
tional Linguistics, pages 203?211. Association for
Computational Linguistics.
Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL.
Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional linguistics, 30(3):309?363.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors. MIT Press, Cambridge, MA,
USA.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673?2681.
2047
Sarah E Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 523?530. Association
for Computational Linguistics.
Noah A Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
354?362. Association for Computational Linguis-
tics.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1?9.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS, vol-
ume 24, pages 801?809.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129?136.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631?1642.
Michael Strube and Udo Hahn. 1999. Functional
centering: Grounding referential coherence in in-
formation structure. Computational linguistics,
25(3):309?344.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017?1024.
Marilyn A Walker, Aravind Krishna Joshi, and
Ellen Friedman Prince. 1998. Centering theory in
discourse. Oxford University Press.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90?94. As-
sociation for Computational Linguistics.
Houfeng Wang, Longkai Zhang, Li Li, He Zhengyan,
and Ni Sun. 2013. Improving chinese word seg-
mentation on micro-blog using rich punctuations.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013).
2048
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061?2069,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Recursive Deep Models for Discourse Parsing
Jiwei Li
1
, Rumeng Li
2
and Eduard Hovy
3
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
2
School of EECS, Peking University, Beijing 100871, P.R. China
3
Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
jiweil@stanford.edu alicerumeng@foxmail.com ehovy@andrew.cmu.edu
Abstract
Text-level discourse parsing remains a
challenge: most approaches employ fea-
tures that fail to capture the intentional, se-
mantic, and syntactic aspects that govern
discourse coherence. In this paper, we pro-
pose a recursive model for discourse pars-
ing that jointly models distributed repre-
sentations for clauses, sentences, and en-
tire discourses. The learned representa-
tions can to some extent learn the seman-
tic and intentional import of words and
larger discourse units automatically,. The
proposed framework obtains comparable
performance regarding standard discours-
ing parsing evaluations when compared
against current state-of-art systems.
1 Introduction
In a coherent text, units (clauses, sentences, and
larger multi-clause groupings) are tightly con-
nected semantically, syntactically, and logically.
Mann and Thompson (1988) define a text to be
coherent when it is possible to describe clearly
the role that each discourse unit (at any level of
grouping) plays with respect to the whole. In a
coherent text, no unit is completely isolated. Dis-
course parsing tries to identify how the units are
connected with each other and thereby uncover the
hierarchical structure of the text, from which mul-
tiple NLP tasks can benefit, including text sum-
marization (Louis et al., 2010), sentence compres-
sion (Sporleder and Lapata, 2005) or question-
answering (Verberne et al., 2007).
Despite recent progress in automatic discourse
segmentation and sentence-level parsing (e.g.,
(Fisher and Roark, 2007; Joty et al., 2012; Sori-
cut and Marcu, 2003), document-level discourse
parsing remains a significant challenge. Recent
attempts (e.g., (Hernault et al., 2010b; Feng and
Hirst, 2012; Joty et al., 2013)) are still consid-
erably inferior when compared to human gold-
standard discourse analysis. The challenge stems
from the fact that compared with sentence-level
dependency parsing, the set of relations between
discourse units is less straightforward to define.
Because there are no clause-level ?parts of dis-
course? analogous to word-level parts of speech,
there is no discourse-level grammar analogous to
sentence-level grammar. To understand how dis-
course units are connected, one has to understand
the communicative function of each unit, and the
role it plays within the context that encapsulates it,
taken recursively all the way up for the entire text.
Manually developed features relating to words and
other syntax-related cues, used in most of the re-
cent prevailing approaches (e.g., (Feng and Hirst,
2012; Hernault et al., 2010b)), are insufficient for
capturing such nested intentionality.
Recently, deep learning architectures have been
applied to various natural language processing
tasks (for details see Section 2) and have shown
the advantages to capture the relevant semantic
and syntactic aspects of units in context. As word
distributions are composed to form the meanings
of clauses, the goal is to extend distributed clause-
level representations to the single- and multi-
sentence (discourse) levels, and produce the hier-
archical structure of entire texts.
Inspired by this idea, we introduce in this pa-
per a deep learning approach for discourse pars-
ing. The proposed parsing algorithm relies on
a recursive neural network to decide (1) whether
two discourse units are connected and if so (2)
by what relation they are connected. Concretely,
the parsing algorithm takes as input a document of
any length, and first obtains the distributed repre-
sentation for each of its sentences using recursive
convolution based on the sentence parse tree. It
then proceeds bottom-up, applying a binary clas-
sifier to determine the probability of two adjacent
2061
discourse units being merged to form a new sub-
tree followed by a multi-class classifier to select
the appropriate discourse relation label, and cal-
culates the distributed representation for the sub-
tree so formed, gradually unifying subtrees un-
til a single overall tree spans the entire sentence.
The compositional distributed representation en-
ables the parser to make accurate parsing decisions
and capture relations between different sentences
and units. The binary and multi-class classifiers,
along with parameters involved in convolution, are
jointly trained from a collection of gold-standard
discourse structures.
The rest of this paper is organized as follows.
We present related work in Section 2 and de-
scribe the RST Discourse Treebank in Section 3.
The sentence convolution approach is illustrated in
Section 4 and the discourse parser model in Sec-
tion 5. We report experimental results in Section 6
and conclude in Section 7.
2 Related Work
2.1 Discourse Analysis and Parsing
The basis of discourse structure lies in the recog-
nition that discourse units (minimally, clauses) are
related to one another in principled ways, and that
the juxtaposition of two units creates a joint mean-
ing larger than either unit?s meaning alone. In a
coherent text this juxtaposition is never random,
but serves the speaker?s communicative goals.
Considerable work on linguistic and computa-
tional discourse processing in the 1970s and 80s
led to the development of several proposals for re-
lations that combine units; for a compilation see
(Hovy and Maier, 1997). Of these the most influ-
ential is Rhetorical Structure Theory RST (Mann
and Thompson, 1988) that defines about 25 rela-
tions, each containing semantic constraints on its
component parts plus a description of the overall
functional/semantic effect produced as a unit when
the parts have been appropriately connected in the
text. For example, the SOLUTIONHOOD relation
connects one unit describing a problem situation
with another describing its solution, using phrases
such as ?the answer is?; in successful communi-
cation the reader will understand that a problem is
described and its solution is given.
Since there is no syntactic definition of a prob-
lem or solution (they can each be stated in a sin-
gle clause, a paragraph, or an entire text), one has
to characterize discourse units by their commu-
nicative (rhetorical) function. The functions are
reflected in text as signals of the author?s inten-
tions, and take various forms (including expres-
sions such as ?therefore?, ?for example?, ?the an-
swer is?, and so on; patterns of tense or pronoun
usage; syntactic forms; etc.). The signals govern
discourse blocks ranging from a clause to an en-
tire text , each one associated with some discourse
relation.
In order to build a text?s hierarchical structure,
a discourse parser needs to recognize these signals
and use them to appropriately compose the rela-
tionship and nesting. Early approaches (Marcu,
2000a; LeThanh et al., 2004) rely mainly on overt
discourse markers (or cue words) and use hand-
coded rules to build text structure trees, bottom-up
from clauses to sentences to paragraphs. . . . Since
a hierarchical discourse tree structure is analo-
gous to a constituency based syntactic tree, mod-
ern research explored syntactic parsing techniques
(e.g., CKY) for discourse parsing based on mul-
tiple text-level or sentence-level features (Soricut
and Marcu, 2003; Reitter, 2003; Baldridge and
Lascarides, 2005; Subba and Di Eugenio, 2009;
Lin et al., 2009; Luong et al., 2014).
A recent prevailing idea for discourse parsing
is to train two classifiers, namely a binary struc-
ture classifier for determining whether two adja-
cent text units should be merged to form a new
subtree, followed by a multi-class relation classi-
fier for determining which discourse relation label
should be assigned to the new subtree. The idea is
proposed by Hernault and his colleagues (Duverle
and Prendinger, 2009; Hernault et al., 2010a) and
followed by other work using more sophisticated
features (Feng and Hirst, 2012; Hernault et al.,
2010b). Current state-of-art performance for re-
lation identification is achieved by the recent rep-
resentation learning approach proposed by (Ji and
Eisenstein, 2014). The proposed framework pre-
sented in this paper is similar to (Ji and Eisenstein,
2014) for transforming the discourse units to the
abstract representations.
2.2 Recursive Deep Learning
Recursive neural networks constitute one type of
deep learning frameworks which was first pro-
posed in (Goller and Kuchler, 1996). The recur-
sive framework relies and operates on structured
inputs (e.g., a parse tree) and computes the rep-
resentation for each parent based on its children
2062
iteratively in a bottom-up fashion. A series of vari-
ations of RNN has been proposed to tailor differ-
ent task-specific requirements, including Matrix-
Vector RNN (Socher et al., 2012) that represents
every word as both a vector and a matrix, or Recur-
sive Neural Tensor Network (Socher et al., 2013)
that allows the model to have greater interactions
between the input vectors. Many tasks have ben-
efited from the recursive framework, including
parsing (Socher et al., 2011b), sentiment analysis
(Socher et al., 2013), textual entailment (Bowman,
2013), segmentation (Wang and Mansur, 2013;
Houfeng et al., 2013), and paraphrase detection
(Socher et al., 2011a).
3 The RST Discourse Treebank
There are today two primary alternative discourse
treebanks suitable for training data: the Rhetor-
ical Structure Theory Discourse Treebank RST-
DT (Carlson et al., 2003) and the Penn Discourse
Treebank (Prasad et al., 2008). In this paper, we
select the former. In RST (Mann and Thompson,
1988), a coherent context or a document is repre-
sented as a hierarchical tree structure, the leaves
of which are clause-sized units called Elementary
Discourse Units (EDUs). Adjacent nodes (siblings
in the tree) are linked with discourse relations that
are either binary (hypotactic) or multi-child (parat-
actic). One child of each hypotactic relation is al-
ways more salient (called the NUCLEUS); its sib-
ling (the SATELLITE) is less salient compared and
may be omitted in summarization. Multi-nuclear
relations (e.g., CONJUNCTION) exhibit no distinc-
tion of salience between the units.
The RST Discourse Treebank contains 385 an-
notated documents (347 for training and 38 for
testing) from the Wall Street Journal. A total
of 110 fine-grained relations defined in (Marcu,
2000b) are used for tagging relations in RST-DT.
They are subtypes of 18 original high-level RST
categories. For fair comparison with existing sys-
tems, we use in this work the 18 coarse-grained re-
lation classes, which with nuclearity attached form
a set of 41 distinct relations. Non-binary relations
are converted into a cascade of right-branching bi-
nary relations.
Conventionally, discourse parsing in RST-DT
involves the following sub-tasks: (1) EDU seg-
mentation to segment the raw text into EDUs, (2)
tree-building. Since the segmentation task is es-
sentially clause delimitation and hence relatively
easy (with state-of-art accuracy at most 95%),
we focus on the latter problem. We assume that
the gold-standard EDU segmentations are already
given, as assumed in other past work (Feng and
Hirst, 2012).
4 EDU Model
In this section, we describe how we compute
the distributed representation for a given sentence
based on its parse tree structure and contained
words. Our implementation is based on (Socher
et al., 2013). As the details can easily be found
there, we omit them for brevity.
Let s denote any given sentence, comprised of a
sequence of tokens s = {w
1
, w
2
, ..., w
n
s
}, where
n
s
denotes the number of tokens in s. Each to-
ken w is associated with a specific vector embed-
ding e
w
= {e
1
w
, e
2
w
, ..., e
K
w
}, where K denotes the
dimension of the word embedding. We wish to
compute the vector representation h
s
for current
sentence, where h
s
= {h
1
s
, h
2
s
, ..., h
K
s
}.
Parse trees are obtained using the Stanford
Parser
1
, and each clause is treated as an EDU. For
a given parent p in the tree and its two children c
1
(associated with vector representation h
c
1
) and c
2
(associated with vector representation h
c
2
), stan-
dard recursive networks calculate the vector for
parent p as follows:
h
p
= f(W ? [h
c
1
, h
c
2
] + b) (1)
where [h
c
1
, h
c
2
] denotes the concatenating vector
for children representations h
c
1
and h
c
2
; W is a
K ? 2K matrix and b is the 1 ? K bias vector;
and f(?) is the function tanh. Recursive neural
models compute parent vectors iteratively until the
root node?s representation is obtained, and use the
root embedding to represent the whole sentence.
5 Discourse Parsing
Since recent work (Feng and Hirst, 2012; Hernault
et al., 2010b) has demonstrated the advantage of
combining the binary structure classifier (deter-
mining whether two adjacent text units should be
merged to form a new subtree) with the multi-class
classifier (determining which discourse relation la-
bel to assign to the new subtree) over the older
single multi-class classifier with the additional la-
bel NO-REL, our approach follows the modern
1
http://nlp.stanford.edu/software/
lex-parser.shtml
2063
Figure 1: RST Discourse Tree Structure.
strategy but trains binary and multi-class classi-
fiers jointly based on the discourse structure tree.
Figure 2 illustrates the structure of a discourse
parse tree. Each node e in the tree is associated
with a distributed vector h
e
. e
1
, e
2
, e
3
and e
6
constitute the leaves of trees, the distributed vec-
tor representations of which are assumed to be al-
ready obtained from convolution in Section 4. Let
N
r
denote the number of relations and we have
N
r
= 41.
5.1 Binary (Structure) Classification
In this subsection, we train a binary (structure)
classifier, which aims to decide whether two EDUs
or spans should be merged during discourse tree
reconstruction.
Let t
binary
(e
i
, e
j
) be the binary valued variable
indicating whether e
i
and e
j
are related, or in other
words, whether a certain type of discourse rela-
tions holds between e
i
and e
j
. According to Fig-
ure 2, the following pairs constitute the training
data for binary classification:
t
binary
(e
1
, e
2
) = 1, t
binary
(e
3
, e
4
) = 1,
t
binary
(e
2
, e
3
) = 0, t
binary
(e
3
, e
6
) = 0,
t
binary
(e
5
, e
6
) = 1
To train the binary classifier, we adopt a three-
layer neural network structure, i.e., input layer,
hidden layer, and output layer. Let H = [h
e
i
, h
e
j
]
denote the concatenating vector for two spans e
i
and e
j
. We first project the concatenating vector
H to the hidden layer withN
binary
hidden neurons.
The hidden layer convolutes the input with non-
linear tanh function as follows:
L
binary
(e
i
,e
j
)
= f(G
binary
? [h
e
i
, h
e
j
] + b
binary
)
where G
binary
is an N
binary
? 2K convolution ma-
trix and b
binary
denotes the bias vector.
The output layer takes as input L
binary
(e
i
,e
j
)
and gen-
erates a scalar using the linear function U
binary
?
L
binary
(e
i
,e
j
)
+ b. A sigmod function is then adopted to
project the value to a [0,1] probability space. The
execution at the output layer can be summarized
as:
p[t
binary
(e
i
, e
j
) = 1] = g(U
binary
?L
binary
(e
i
,e
j
)
+b
?
binary
)
(2)
where U
binary
is an N
binary
? 1 vector and b
?
binary
denotes the bias. g(?) is the sigmod function.
5.2 Multi-class Relation Classification
If t
binary
(e
i
, e
j
) is determined to be 1, we next
use variable r(e
i
, e
j
) to denote the index of rela-
tion that holds between e
i
and e
j
. A multi-class
classifier is train based on a three-layer neural net-
work, in the similar way as binary classification in
Section 5.1. Concretely, a matrix G
Multi
and bias
vector b
Multi
are first adopted to convolute the con-
catenating node vectors to the hidden layer vector
L
multi
(e
i
,e
j
)
:
L
multi
(e
i
,e
j
)
= f(G
multi
? [h
e
i
, h
e
j
] + b
multi
) (3)
We then compute the posterior probability over
labels given the hidden layer vector L using the
softmax and obtain the N
r
dimensional probabil-
ity vector P
(e
1
,e
2
)
for each EDU pair as follows:
S
(e
i
,e
j
)
= U
multi
? L
multi
(e
i
,e
j
)
(4)
P
(e
1
,e
2
)
(i) =
exp(S
(e
1
,e
2
)
(i))
?
k
exp(S
(e
1
,e
2
)
)(k)
(5)
where U
multi
is the N
r
? 2K matrix. The i
th
ele-
ment in P
(e
1
,e
2
)
denotes the probability that i
t
h re-
lation holds between e
i
and e
j
. To note, binary and
multi-class classifiers are trained independently.
5.3 Distributed Vector for Spans
What is missing in the previous two subsections
are the distributed vectors for non-leaf nodes (i.e.,
e
4
and e
5
in Figure 1), which serve as structure and
relation classification. Again, we turn to recursive
deep learning network to obtain the distributed
vector for each node in the tree in a bottom-up
fashion.
Similar as for sentence parse-tree level compo-
sitionally, we extend a standard recursive neural
network by associating each type of relations r
with one specific K?2K convolution matrix W
r
.
2064
Figure 2: System Overview.
The representation for each node within the tree is
calculated based on the representations for its chil-
dren in a bottom-up fashion. Concretely, for a par-
ent node p, given the distributed representation h
e
i
for left child, h
e
j
for right child, and the relation
r(e
1
, e
2
), its distributed vector h
p
is calculated as
follows:
h
p
= f(W
r(e
1
,e
2
)
? [h
e
i
, h
e
j
] + b
r(e
1
,e
2
)
) (6)
where b
r(e
1
,e
2
)
is the bias vector and f(?) is the
non-linear tanh function.
To note, our approach does not make any dis-
tinction between within-sentence text spans and
cross-sentence text spans, different from (Feng
and Hirst, 2012; Joty et al., 2013)
5.4 Cost Function
The parameters to optimize include sentence-
level convolution parameters [W , b],
discourse-level convolution parameters
[{W
r
}, {b
r
}], binary classification parameters
[G
binary
, b
binary
, U
binary
, b
?
binary
], and multi-class
parameters [G
multi
, b
multi
, U
multi
].
Suppose we have M
1
binary training samples
and M
2
multi-class training examples (M
2
equals
the number of positive examples in M
1
, which
is also the non-leaf nodes within the training dis-
course trees). The cost function for our framework
with regularization on the training set is given by:
J(?
binary
) =
?
(e
i
,e
j
)?{binary}
J
binary
(e
i
, e
j
)
+Q
binary
?
?
???
binary
?
2
(7)
J(?
multi
) =
?
(e
i
,e
j
)?{multi}
J
multi
(e
i
, e
j
)
+Q
multi
?
?
???
multi
?
2
(8)
where
J
binary
(e
i
, e
j
) = ?t(e
i
, e
j
) log p(t(e
i
, e
j
) = 1)
? (1? t(e
i
, e
j
)) log[1? p(t(e
i
, e
j
) = 1)]
J
multi
(e
i
, e
j
) = ? log[p(r(e
i
, e
j
) = r)]
(9)
5.5 Backward Propagation
The derivative for parameters involved is com-
puted through backward propagation. Here we
illustrate how we compute the derivative of
J
multi
(e
i
, e
j
) with respect to different parameters.
For each pair of nodes (e
i
, e
j
) ? multi, we
associate it with a N
r
dimensional binary vector
R(e
i
, e
j
), which denotes the ground truth vector
with a 1 at the correct label r(e
i
, e
j
) and all other
entries 0. Integrating softmax error vector, for any
parameter ?, the derivative of J
multi
(e
i
, e
j
) with re-
spect to ? is given by:
?J
multi
(e
i
, e
j
)
??
= [P
(e
i
,e
j
)
?R
(e
i
,e
j
)
]?
?S
(e
i
,e
j
)
??
(10)
where ? denotes the Hadamard product between
the two vectors. Each training pair recursively
backpropagates its error to some node in the dis-
course tree through [{W
r
}, {b
r
}], and then to
nodes in sentence parse tree through [W, b], and
the derivatives can be obtained according to stan-
dard backpropagation (Goller and Kuchler, 1996;
Socher et al., 2010).
2065
5.6 Additional Features
When determining the structure/multi relation be-
tween individual EDUs, additional features are
also considered, the usefulness of which has been
illustrated in a bunch of existing work (Feng and
Hirst, 2012; Hernault et al., 2010b; Joty et al.,
2012). We consider the following simple text-level
features:
? Tokens at the beginning and end of the EDUs.
? POS at the beginning and end of the EDUs.
? Whether two EDUs are in the same sentence.
5.7 Optimization
We use the diagonal variant of AdaGrad (Duchi et
al., 2011) with minibatches, which is widely ap-
plied in deep learning literature (e.g.,(Socher et
al., 2011a; Pei et al., 2014)). The learning rate
in AdaGrad is adapted differently for different pa-
rameters at different steps. Concretely, let g
i
?
de-
note the subgradient at time step t for parameter
?
i
obtained from backpropagation, the parameter
update at time step t is given by:
?
?
= ?
??1
?
?
?
?
t=0
?
g
i2
?
g
i
?
(11)
where ? denotes the learning rate and is set to 0.01
in our approach.
Elements in {W
r
}, W , G
binary
, G
multi
, U
binary
,
U
multi
are initialized by randomly drawing from
the uniform distribution [?, ], where  is calcu-
lated as suggested in (Collobert et al., 2011). All
bias vectors are initialized with 0. Word embed-
dings {e} are borrowed from Senna (Collobert et
al., 2011; Collobert, 2011).
5.8 Inference
For inference, the goal is to find the most proba-
ble discourse tree given the EDUs within the doc-
ument. Existing inference approach basically in-
clude the approach adopted in (Feng and Hirst,
2012; Hernault et al., 2010b) that merges the most
likely spans at each step and SPADE (Fisher and
Roark, 2007) that first finds the tree structure that
is globally optimal, then assigns the most probable
relations to the internal nodes.
In this paper, we implement a probabilistic
CKY-like bottom-up algorithm for computing the
most likely parse tree using dynamic program-
ming as are adopted in (Joty et al., 2012; Joty
et al., 2013; Jurafsky and Martin, 2000) for the
search of global optimum. For a document with
n EDUs, as different relations are characterized
with different compositions (thus leading to dif-
ferent vectors), we use a N
r
?n?n dynamic pro-
gramming table Pr, the cell Pr[r, i, j] of which
represents the span contained EDUs from i to j
and stores the probability that relation r holds be-
tween the two spans within i to j. Pr[r, i, j] is
computed as follows:
Pr[r, i, j] =max
r
1
,r
2
,k
Pr[r
1
, i, k] ? Pr[r
2
, k, j]
?P (t
binary
(e
[i,k]
, e
[k,j]
) = 1)
?P (r(e
[i,k]
, e
[k,j]
) = 1)
(12)
At each merging step, a distributed vector for the
merged point is calculated according to Eq. 13 for
different relations. The CKY-like algorithms finds
the global optimal. To note, the worst-case run-
ning time of our inference algorithm is O(N
2
r
n
3
),
where n denotes the number of sentences within
the document, which is much slower than the
greedy search. In this work, for simplification, we
simplify the framework by maintaining the top 10
options at each step.
6 Experiments
A measure of the performance of the system is
realized by comparing the structure and labeling
of the RS-tree produced by our algorithm to gold-
standard annotations.
Standard evaluation of discourse parsing output
computes the ratio of the number of identical tree
constituents shared in the generated RS-trees and
the gold-standard trees against the total number
of constituents in the generated discourse trees
2
,
which is further divided to three matrices: Span
(on the blank tree structure), nuclearity (on the
tree structure with nuclearity indication), and rela-
tion (on the tree structure with rhetorical relation
indication but no nuclearity indication).
The nuclearity and relation decisions are made
based on the multi-class output labels from the
deep learning framework. As we do not consider
nuclearity when classifying different discourse re-
lations, the two labels attribute[N][S] and at-
tribute[S][N] made by multi-class classifier will
be treated as the same relation label ATTRIBUTE.
2
Conventionally, evaluation matrices involve precision,
recall and F-score in terms of the comparison between tree
structures. But these are the same when manual segmenta-
tion is used (Marcu, 2000b).
2066
Approach Span Nuclearity Relation
HILDA 75.3 60.0 46.8
Joty et al. 82.5 68.4 55.7
Feng and Hirst 85.7 71.0 58.2
Ji and Eisenstein 82.1 71.1 61.6
Unified (with feature) 82.0 70.0 57.1
Ours (no feature) 82.4 69.2 56.8
Ours (with feature) 84.0 70.8 58.6
human 88.7 77.7 65.7
Table 1: Performances for different approaches.
Performances for baselines are reprinted from
(Joty et al., 2013; Feng and Hirst, 2014; Ji and
Eisenstein, 2014).
Also, we do not train a separate classifier for NU-
CLEUS and SATELLITE identification. The nucle-
arity decision is made based on the relation type
produced by the multi-class classifier.
6.1 Parameter Tuning
The regularization parameter Q constitutes the
only parameter to tune in our framework. We tune
it on the 347 training documents. Concretely, we
employ a five-fold cross validation on the RST
dataset and tune Q on 5 different values: 0.01,
0.1, 0.5, 1.5, 2.5. The final model was tested on
the testing set after parameter tuning.
6.2 Baselines
We compare our model against the following
currently prevailing discourse parsing baselines:
HILDA A discourse parser based on support
vector machine classification introduced by Her-
nault et al. (Hernault et al., 2010b). HILDA uses
the binary and multi-class classifier to reconstruct
the tree structure in a greedy way, where the
most likely nodes are merged at each step. The
results for HILDA are obtained by running the
system with default settings on the same inputs
we provided to our system.
Joty et al The discourse parser introduced by
Joty et al. (Joty et al., 2013). It relies on CRF
and combines intra-sentential and multi-sentential
parsers in two different ways. Joty et al. adopt
the global optimal inference as in our work. We
reported the performance from their paper (Joty et
al., 2013).
Feng and Hirst The linear-time discourse
parser introduced in (Feng and Hirst, 2014) which
relies on two linear-chain CRFs to obtain a se-
quence of discourse constituents.
Ji and Eisenstein The shift-reduce discourse
parser introduced in (Ji and Eisenstein, 2014)
which parses document by relying on the dis-
tributed representations obtained from deep learn-
ing framework.
Additionally, we implemented a simplified ver-
sion of our model called unified where we use
a unified convolutional function with unified pa-
rameters [W
sen
, b
sen
] for span vector computation.
Concretely, for a parent node p, given the dis-
tributed representation h
e
i
for left child, h
e
j
for
right child, and the relation r(e
1
, e
2
), rather than
taking the inter relation between two children, its
distributed vector h
p
is calculated:
h
p
= f(W
sen
? [h
e
i
, h
e
j
] + b
sen
) (13)
6.3 Performance
Performances for different models approaches re-
ported in Table 1. And as we can observe, al-
though the proposed framework obtains compa-
rable result compared with existing state-of-state
performances regarding all evaluating parameters
for discourse parsing. Specifically, as for the three
measures, no system achieves top performance on
all three, though some systems outperform all oth-
ers for one of the measures. The proposed system
achieves high overall performance on all three, al-
though it does not achieve top score on any mea-
sure. The system gets a little bit performance
boost by considering text-level features illustrated
in Section 5.6. The simplified version of the orig-
inal model underperforms against the original ap-
proach due to lack of expressive power in convo-
lution. Performance plummets when different re-
lations are uniformly treated, which illustrates the
importance of taking into consideration different
types of relations in the span convolution proce-
dure.
7 Conclusion
In this paper, we describe an RST-style text-level
discourse parser based on a neural network model.
The incorporation of sentence-level distributed
vectors for discourse analysis obtains compara-
ble performance compared with current state-of-
art discourse parsing system.
Our future work will focus on extending
discourse-level distributed presentations to related
2067
tasks, such as implicit discourse relation identifi-
cation or dialogue analysis. Further, once the tree
structure for a document can be determined, the
vector for the entire document can be obtained
in bottom-up fashion, as in this paper. One can
now investigate whether the discourse parse tree
is useful for acquiring a single document-level
vector representation, which would benefit mul-
tiple tasks, such as document classification or
macro-sentiment analysis.
Acknowledgements
The authors want to thank Vanessa Wei Feng and
Shafiq Joty for helpful discussions regarding RST
dataset. We also want to thank Richard Socher,
Zhengyan He and Pradeep Dasigi for the clarifica-
tion of deep learning techniques.
References
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning, pages 96?103.
Association for Computational Linguistics.
Samuel R Bowman. 2013. Can recursive neural tensor
networks learn logical reasoning? arXiv preprint
arXiv:1312.6192.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
Springer.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
David A Duverle and Helmut Prendinger. 2009. A
novel discourse parser based on support vector ma-
chine classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 665?673. Association for Compu-
tational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic fea-
tures. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 60?68. Association
for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2014. A lin-
ear time bottom-up discourse parser with constraints
and post-editing. In ACL.
Seeger Fisher and Brian Roark. 2007. The utility of
parse-derived features for automatic discourse seg-
mentation. In ANNUAL MEETING-ASSOCIATION
FOR COMPUTATIONAL LINGUISTICS, vol-
ume 45, page 488.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347?352. IEEE.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010a. A semi-supervised approach to
improve classification of infrequent discourse rela-
tions using feature vector extension. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 399?409. As-
sociation for Computational Linguistics.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,
et al. 2010b. Hilda: a discourse parser using sup-
port vector machine classification. Dialogue & Dis-
course, 1(3).
Wang Houfeng, Longkai Zhang, and Ni Sun. 2013.
Improving chinese word segmentation on micro-
blog using rich punctuations.
Eduard H Hovy and Elisabeth Maier. 1997. Parsimo-
nious or profligate: How many and which discourse
structure relations. Discourse Processes.
Yangfeng Ji and Jacob Eisenstein. 2014. Representa-
tion learning for text-level discourse parsing.
Shafiq Joty, Giuseppe Carenini, and Raymond T
Ng. 2012. A novel discriminative framework for
sentence-level discourse analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 904?915. Asso-
ciation for Computational Linguistics.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra-and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st annual
meeting of the association for computational lin-
guistics (ACL), pages 486?496.
Dan Jurafsky and James H Martin. 2000. Speech &
Language Processing. Pearson Education India.
2068
Huong LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Generating discourse structures for
written texts. In Proceedings of the 20th inter-
national conference on Computational Linguistics,
page 329. Association for Computational Linguis-
tics.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343?351.
Association for Computational Linguistics.
Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue, pages 147?156. Association for Computa-
tional Linguistics.
Minh-Thang Luong, Michael C Frank, and Mark John-
son. 2014. Parsing entire discourses as very long
strings: Capturing topic continuity in grounded lan-
guage learning.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu. 2000a. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Daniel Marcu. 2000b. The theory and practice of dis-
course parsing and summarization. MIT Press.
Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
David Reitter. 2003. Simple signals for complex
rhetorics: On rhetorical analysis with rich-feature
support vector models. In LDV Forum, volume 18,
pages 38?52.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1?9.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS, vol-
ume 24, pages 801?809.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129?136.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631?1642.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149?156. Association
for Computational Linguistics.
Caroline Sporleder and Mirella Lapata. 2005. Dis-
course chunking and its application to sentence com-
pression. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 257?264.
Association for Computational Linguistics.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566?574. Association for
Computational Linguistics.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2007. Evaluating discourse-
based answer extraction for why-question answer-
ing. In Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 735?736. ACM.
Longkai Zhang Houfeng Wang and Xu Sun Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for chinese word seg-
mentation.
2069
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 217?221,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
TopicSpam: a Topic-Model-Based Approach for Spam Detection
Jiwei Li , Claire Cardie
School of Computer Science
Cornell University
Ithaca, NY, 14853
jl3226@cornell.edu
cardie@cs.cornell.edu
Sujian Li
Laboratory of Computational Linguistics
Peking University
Bejing, P.R.China, 150001
lisujian@pku.edu.cn
Abstract
Product reviews are now widely used by
individuals and organizations for decision
making (Litvin et al, 2008; Jansen, 2010).
And because of the profits at stake, peo-
ple have been known to try to game the
system by writing fake reviews to promote
target products. As a result, the task of de-
ceptive review detection has been gaining
increasing attention. In this paper, we pro-
pose a generative LDA-based topic mod-
eling approach for fake review detection.
Our model can aptly detect the subtle dif-
ferences between deceptive reviews and
truthful ones and achieves about 95% ac-
curacy on review spam datasets, outper-
forming existing baselines by a large mar-
gin.
1 Introduction
Consumers rely increasingly on user-generated
online reviews to make purchase decisions. Pos-
itive opinions can result in significant financial
gains. This gives rise to deceptive opinion spam
(Ott et al, 2011; Jindal et al, 2008), fake reviews
written to sound authentic and deliberately mis-
lead readers. Previous research has shown that
humans have difficulty distinguishing fake from
truthful reviews, operating for the most part at
chance (Ott et al, 2011). Consider, for example,
the following two hotel reviews. One is truthful
and the other is deceptive1:
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate, and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided. Their service was amazing,
1The first example is a deceptive review.
and we absolutely loved the beautiful indoor pool. I
would recommend staying here to anyone.
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The hotel
is central to both Navy Pier and Michigan Ave. so we
walked, trolleyed, and cabbed all around the area. We
ate the breakfast buffet on both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 (instead of the adult $20). The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we stay in Chicago next time.
Because of the difficulty of recognizing deceptive
opinions, there has been a widespread and growing
interest in developing automatic, usually learning-
based methods to help users identify deceptive re-
views (Ott et al, 2011; Jindal et al, 2008; Jindal
et al, 2010; Li et al, 2011; Lim et al, 2011; Wang
et al, 2011).
The state-of-the-art approach treats the task of
spam detection as a text categorization prob-
lem and was first introduced by Jindal and Liu
(2009) who trained a supervised classifier to dis-
tinguish duplicated reviews (assumed deceptive)
from original ones (assumed truthful). Since then,
many supervised approaches have been proposed
for spam detection. Ott et al (2011) employed
standard word and part-of-speech (POS) n-gram
features for supervised learning and built a gold?
standard opinion dataset of 800 reviews. Lim et
al. (2010) proposed the inclusion of user behavior-
based features and found that behavior abnormali-
ties of reviewers could predict spammers, without
using any textual features. Li et al (2011) care-
fully explored review-related features based on
content and sentiment, training a semi-supervised
classifier for opinion spam detection. However,
the disadvantages of standard supervised learning
methods are obvious. First, they do not gener-
ally provide readers with a clear probabilistic pre-
217
diction of how likely a review is to be deceptive
vs. truthful. Furthermore, identifying features that
provide direct evidence against deceptive reviews
is always a hard problem.
LDA topic models (Blei et al, 2003) have
widely been used for their ability to model latent
topics in document collection. In LDA, each docu-
ment is presented as a mixture distribution of top-
ics and each topic is presented as a mixture distri-
bution of words. Researchers also integrated dif-
ferent levels of information into LDA topic mod-
els to model the specific knowledge that they are
interested in, such as user-specific information
(Rosen-zvi et al, 2006), document-specific infor-
mation (Li et al, 2010) and time-specific infor-
mation (Diao et al, 2012). Ramage et al (2009)
developed a Labeled LDA model to define a one-
to-one correspondence between LDA latent topics
and tags. Chemudugunta et al (2008) illustrated
that by considering background information and
document-specific information, we can largely im-
prove the performance of topic modeling.
In this paper, we propose a Bayesian approach
called TopicSpam for deceptive review detection.
Our approach, which is a variation of Latent
Dirichlet Allocation (LDA) (Blei et al, 2003),
aims to detect the subtle differences between the
topic-word distributions of deceptive reviews vs.
truthful ones. In addition, our model can give
a clear probabilistic prediction on how likely a
review should be treated as deceptive or truth-
ful. Performance is tested on dataset from Ott et
al.(2011) that contains 800 reviews of 20 Chicago
hotels. Our model achieves more than 94% accu-
racy on that dataset.
2 TopicSpam
We are presented with four subsets of ho-
tel reviews, M = {Mi}i=4i=1, representing
deceptive train, truthful train, deceptive test
and truthful test data, respectively. Each re-
view r is comprised of a number of words r =
{wt}t=nrt=1 . Input for the TopicSpam algorithm is
the datasets M ; output is the label (deceptive,
truthful) for each review inM3 andM4. V denotes
vocabulary size.
2.1 Details of TopicSpam
In TopicSpam, each document is modeled as a
bag of words, which are assumed to be gener-
ated from a mixture of latent topics. Each word
is associated with a latent variable that specifies
Figure 1: Graphical Model for TopicSpam
the topic from which it is generated. Words in a
document are assumed to be conditionally inde-
pendent given the hidden topics. A general back-
ground distribution ?B and hotel-specific distri-
butions ?Hj (j = 1, ..., 20) are first introduced
to capture the background information and hotel-
specific information. To capture the difference
between deceptive reviews and truthful reviews,
TopicSpam also learns a deceptive topic distribu-
tion ?D and truthful topic distribution ?T . The
generative model of TopicSpam is shown as fol-
lows:
? For a training review in r1j ? M1, words are
originated from one of the three different top-
ics: ?B , ?Hj and ?D.
? For a training review in r2j ? M2, words are
originated from one of the three different top-
ics: ?B , ?Hj and ?T .
? For a test review in rmj ? Mm,m = 3, 4,
words are originated from one of the four dif-
ferent topics: ?B , ?Hj ?D and ?T .
The generation process of TopicSpam is shown
in Figure 1 and the corresponding graphical
model is illustrated in Figure 2. We use
? = (?G, ?Hi , ?D, ?T ) to represent the asym-
metric priors for topic-word distribution genera-
tion. In our experiments, we set ?G = 0.1,
and ?Hi = ?D = ?T = 0.01. The intu-
ition for the asymmetric priors is that there should
be more words assigned to the background topic.
? = [?B, ?Hi , ?D, ?T ] denotes the priors for
the document-level topic distribution in the LDA
model. We set ?B = 2 and ?T = ?D = ?Hi = 1,
reflecting the intuition that more words in each
document should cover the background topic.
2.2 Inference
We adopt the collapsed Gibbs sampling strategy to
infer the latent parameters in TopicSpam. In Gibbs
218
1. sample ?G ? Dir(?G)
2. sample ?D ? Dir(?D)
3. sample ?T ? Dir(?T )
4. for each hotel j ? [1, N ]: sample ?Hj ? ?H
5. for each review r
if i=1: sample ?r ? Dir(?B, ?Hj , ?D)
if i=2: sample ?r ? Dir(?B, ?Hj , ?T )
if i=3: sample ?r ? Dir(?B, ?Hj , ?D, ?T )
if i=4: sample ?r ? Dir(?B, ?Hj , ?D, ?T )
for each word w in R
sample z ? ?r sample w ? ?z
Figure 2: Generative Model for TopicSpam
sampling, for each word w in review r, we need
to calculate P (zw|w, z?w, ?, ?) in each iteration,
where z?w denotes the topic assignments for all
words except that of the current word zw.
P (zw = m|z?w, i, j, ?, ?)
Nmr + ?m?
m?(Nm
?
r + ??m)
? E
w
m + ?m?V
w? Ewm + V ?m
(1)
where Nmr denotes the number of times that topic
m appears in current review r and Ewm denotes the
number of times that word w is assigned to topic
m. After each sampling iteration, the latent pa-
rameters can be estimated using the following for-
mulas:
?mr =
Nmr + ?m?
m?(Nm
?
r + ?m)
?(w)m =
Ewm + ?m?
w? Ew
?
m + V ?m(2)
2.3 Labeling the Test Data
For each review r in the test data, let NDr denote
the number of words generated from the decep-
tive topic and NTr , the number of words generated
from the truthful topic. The decision for whether a
review is deceptive or truthful is made as follows:
? if NDr > NTr , r is deceptive.
? if NDr < NTr , r is truthful.
? if NDr = NTr , it is hard to decide.
Let P(D) denote the probability that r is deceptive
and P(T) denote the probability that r is truthful.
P (D) = N
D
r
NDr +NTr
P (T ) = N
T
r
NDr +NTr
(3)
3 Experiments
3.1 System Description
Our experiments are conducted on the dataset
from Ott et al(2011), which contains reviews of
the 20 most popular hotels on TripAdvisor in the
Chicago areas. There are 20 truthful and 20 decep-
tive reviews for each of the chosen hotels (800 re-
views total). Deceptive reviews are gathered using
Amazon Mechanical Turk2. In our experiments,
we adopt the same 5-fold cross-validation strat-
egy as in Ott et al, using the same data partitions.
Words are stemmed using PorterStemmer3.
3.2 Baselines
We employ a number of techniques as baselines:
TopicTD: A topic-modeling approach that only
considers two topics: deceptive and truthful.
Words in deceptive train are all generated from
the deceptive topic and words in truthful train
are generated from the truthful topic. Test docu-
ments are presented with a mixture of the decep-
tive and truthful topics.
TopicTDB: A topic-modeling approach that
only considers background, deceptive and truthful
information.
SVM-Unigram: Using SVMlight(Joachims,
1999) to train linear SVM models on unigram fea-
tures.
SVM-Bigram: Using SVMlight(Joachims,
1999) to train linear SVM models on bigram fea-
tures.
SVM-Unigram-Removal1: In SVM-Unigram-
Removal, we first train TopicSpam. Then words
generated from hotel-specific topics are removed.
We use the remaining words as features in SVM-
light.
SVM-Unigram-Removal2: Same as SVM-
Unigram-removal-1 but removing all background
words and hotel-specific words.
Experimental results are shown in Table 14.
As we can see, the accuracy of TopicSpam is
0.948, outperforming TopicTD by 6.4%. This il-
lustrates the effectiveness of modeling background
and hotel-specific information for the opinion
spam detection problem. We also see that Top-
icSpam slightly outperforms TopicTDB, which
2https://www.mturk.com/mturk/.
3http://tartarus.org/martin/PorterStemmer/
4Reviews with NDr = NTr are regarded as incorrectly
classified by TopicSpam.
219
Approach Accuracy T-P T-R T-F D-P D-R D-F
TopicSpam 0.948 0.954 0.942 0.944 0.941 0.952 0.946
TopicTD 0.888 0.901 0.878 0.889 0.875 0.897 0.886
TopicTDB 0.931 0.938 0.926 0.932 0.925 0.937 0.930
SVM-Unigram 0.884 0.899 0.865 0.882 0.870 0.903 0.886
SVM-Bigram 0.896 0.901 0.890 0.896 0.891 0.903 0.897
SVM-Unigram-Removal1 0.895 0.906 0.889 0.898 0.887 0.907 0.898
SVM-Unigram-Removal2 0.822 0.852 0.806 0.829 0.793 0.840 0.817
Table 1: Performance for different approaches based on nested 5-fold cross-validation experiments.
neglects hotel-specific information. By check-
ing the results of Gibbs sampling, we find that
this is because only a small number of words
are generated by the hotel-specific topics. Top-
icTD and SVM-Unigram get comparative accu-
racy rates. This can be explained by the fact
that both models use unigram frequency as fea-
tures for the classifier or topic distribution train-
ing. SVM-Unigram-Removal1 is also slightly
better than SVM-Unigram. In SVM-Unigram-
removal1, hotel-specific words are removed for
classifier training. So the first-step LDA model
can be viewed as a feature selection process for the
SVM, giving rise to better results. We can also see
that the performance of SVM-Unigram-removal2
is worse than other baselines. This can be ex-
plained as follows: for example, word ?my? has
large probability to be generated from the back-
ground topic. However it can also be generated by
deceptive topic occasionaly but can hardly be gen-
erated from the truthful topic. So the removal of
these words results in the loss of useful informa-
tion, and leads to low accuracy rate.
Our topic-modeling approach uses word fre-
quency as features and does not involve any fea-
ture selection process. Here we present the re-
sults of the sample reviews from Section 1. Stop
words are labeled in black, background topics (B)
in blue, hotel specific topics (H) in orange, de-
ceptive topics (D) in red and truthful topic (T) in
green.
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate,and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided not like most hotel shampoos.
Their service was amazing,and we absolutely loved the
beautiful indoor pool. I would recommend staying here
to anyone.
[B,H,D,T]=[41,6,10,1] p(D)=0.909 P(T)=0.091
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The ho-
tel is central to both Navy Pier and Michigan Ave. so
we walked, trolleyed, and cabbed all around the area.
We ate the breakfast buffet both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 ( instead of the adult $20) The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we?re in Chicago next time.
[B,H,D,T]=[80,15,3,18] p(D)=0.143 P(T)=0.857
background deceptive truthful Hilton
hotel hotel room Hilton
stay my ) palmer
we chicago ( millennium
room will but lockwood
! room $ park
Chicago very bathroom lobby
my visit location line
great husband night valet
I city walk shampoo
very experience park dog
Omni Amalfi Sheraton James
Omni Amalfi tower James
pool breakfast Sheraton service
plasma view pool spa
sundeck floor river bar
chocolate bathroom lake upgrade
indoor cocktail navy primehouse
request morning indoor design
pillow wine shower overlook
suitable great kid romantic
area room theater home
Table 2: Top words in different topics from Topic-
Spam
4 Conclusion
In this paper, we propose a novel topic model
for deceptive opinion spam detection. Our model
achieves an accuracy of 94.8%, demonstrating its
effectiveness on the task.
5 Acknowledgements
We thank Myle Ott for his insightful comments and sugges-
tions. This work was supported in part by NSF Grant BCS-
0904822, a DARPA Deft grant, and by a gift from Google.
220
References
David Blei, Andrew Ng and Micheal Jordan. Latent
Dirichlet alocation. 2003. In Journal of Machine
Learning Research.
Carlos Castillo, Debora Donato, Luca Becchetti, Paolo
Boldi, Stefano Leonardi Massimo Santini, and Se-
bastiano Vigna. A reference collection for web
spam. In Proceedings of annual international ACM
SIGIR conference on Research and development in
information retrieval, 2006.
Chaltanya Chemudugunta, Padhraic Smyth and Mark
Steyers. Modeling General and Specific Aspects of
Documents with a Probabilistic Topic Model.. In
Advances in Neural Information Processing Systems
19: Proceedings of the 2006 Conference.
Paul-Alexandru Chirita, Jorg Diederich, and Wolfgang
Nejdl. MailRank: using ranking for spam detection.
In Proceedings of ACM international conference on
Information and knowledge management. 2005.
Harris Drucke, Donghui Wu, and Vladimir Vapnik.
2002. Support vector machines for spam categoriza-
tion. In Neural Networks.
Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim.
In Proceeding of the 50th Annual Meeting of the As-
sociation for Computational Linguistics. 2012
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods.
Jack Jansen. 2010. Online product research. In Pew In-
ternet and American Life Project Report.
Nitin Jindal, and Bing Liu. Opinion spam and analysis.
2008. In Proceedings of the international conference
on Web search and web data mining
Nitin Jindal, Bing Liu, and Ee-Peng Lim. Finding
Unusual Review Patterns Using Unexpected Rules.
2010. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment
Pranam Kolari, Akshay Java, Tim Finin, Tim Oates and
Anupam Joshi. Detecting Spam Blogs: A Machine
Learning Approach. In Proceedings of Association
for the Advancement of Artificial Intelligence. 2006.
Peng Li, Jing Jiang and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
Learning to identify review Spam. 2011. In Proceed-
ings of the Twenty-Second international joint confer-
ence on Artificial Intelligence.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. Detecting Product Re-
view Spammers Using Rating Behavior. 2010. In
Proceedings of the 19th ACM international confer-
ence on Information and knowledge management.
Stephen Litvina, Ronald Goldsmithb and Bing Pana.
2008. Electronic word-of-mouth in hospitality
and tourism management. Tourism management,
29(3):458468.
Juan Martinez-Romo and Lourdes Araujo. Web Spam
Identification Through Language Model Analysis.
In AIRWeb. 2009.
Arjun Mukherjee, Bing Liu and Natalie Glance. Spot-
ting Fake Reviewer Groups in Consumer Reviews.
In Proceedings of the 18th international conference
on World wide web, 2012.
Alexandros Ntoulas, Marc Najork, Mark Manasse and
Dennis Fetterly. Detecting Spam Web Pages through
Content Analysis. In Proceedings of international
conference on World Wide Web 2006
Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Han-
cock. Finding deceptive opinion spam by any stretch
of the imagination. 2011. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
Bo Pang and Lillian Lee. Opinion mining and senti-
ment analysis. In Found. Trends Inf. Retr.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher D. Manning. Labeled LDA: a super-
vised topic model for credit attribution in multi-
labeled corpora. 2009. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing 2009.
Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and
Padhraic Smyth. The author-topic model for authors
and documents. In Proceedings of the 20th confer-
ence on Uncertainty in artificial intelligence.
Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Re-
view Graph based Online Store Review Spammer
Detection. 2011. In Proceedings of 11th Interna-
tional Conference of Data Mining.
Baoning Wu, Vinay Goel and Brian Davison. Topical
TrustRank: using topicality to combat Web spam.
In Proceedings of international conference on World
Wide Web 2006 .
Kyang Yoo and Ulrike Gretzel. 2009. Compari-
son of Deceptive and Truthful Travel Reviews.
InInformation and Communication Technologies in
Tourism 2009.
221
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 556?560,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evolutionary Hierarchical Dirichlet Process for Timeline Summarization
Jiwei Li
School of Computer Science
Cornell University
Ithaca, NY, 14853
jl3226@cornell.edu
Sujian Li
Laboratory of Computational Linguistics
Peking University
Bejing, P.R.China, 150001
lisujian@pku.edu.cn
Abstract
Timeline summarization aims at generat-
ing concise summaries and giving read-
ers a faster and better access to under-
stand the evolution of news. It is a new
challenge which combines salience rank-
ing problem with novelty detection. Pre-
vious researches in this field seldom ex-
plore the evolutionary pattern of topics
such as birth, splitting, merging, develop-
ing and death. In this paper, we develop
a novel model called Evolutionary Hier-
archical Dirichlet Process(EHDP) to cap-
ture the topic evolution pattern in time-
line summarization. In EHDP, time vary-
ing information is formulated as a series
of HDPs by considering time-dependent
information. Experiments on 6 different
datasets which contain 3156 documents
demonstrates the good performance of our
system with regard to ROUGE scores.
1 Introduction
Faced with thousands of news articles, people usu-
ally try to ask the general aspects such as the
beginning, the evolutionary pattern and the end.
General search engines simply return the top rank-
ing articles according to query relevance and fail
to trace how a specific event goes. Timeline sum-
marization, which aims at generating a series of
concise summaries for news collection published
at different epochs can give readers a faster and
better access to understand the evolution of news.
The key of timeline summarization is how to
select sentences which can tell readers the evolu-
tionary pattern of topics in the event. It is very
common that the themes of a corpus evolve over
time, and topics of adjacent epochs usually exhibit
strong correlations. Thus, it is important to model
topics across different documents and over differ-
ent time periods to detect how the events evolve.
The task of timelime summarization is firstly
proposed by Allan et al(2001) by extracting clus-
ters of noun phases and name entities. Chieu et
al.(2004) built a similar system in unit of sentences
with interest and burstiness. However, these meth-
ods seldom explored the evolutionary character-
istics of news. Recently, Yan et al(2011) ex-
tended the graph based sentence ranking algorithm
used in traditional multi-document summarization
(MDS) to timeline generation by projecting sen-
tences from different time into one plane. They
further explored the timeline task from the opti-
mization of a function considering the combina-
tion of different respects such as relevance, cover-
age, coherence and diversity (Yan et al, 2011b).
However, their approaches just treat timeline gen-
eration as a sentence ranking or optimization prob-
lem and seldom explore the topic information lied
in the corpus.
Recently, topic models have been widely used
for capturing the dynamics of topics via time.
Many dynamic approaches based on LDA model
(Blei et al, 2003) or Hierarchical Dirichelt Pro-
cesses(HDP) (Teh et al, 2006) have been pro-
posed to discover the evolving patterns in the cor-
pus as well as the snapshot clusters at each time
epoch (Blei and Lafferty, 2006; Chakrabarti et al,
2006; Wang and McCallum, 2007; Caron et al,
2007; Ren et al, 2008; Ahmed and Xing, 2008;
Zhang et al, 2010).
In this paper, we propose EHDP: a evolution-
ary hierarchical Dirichlet process (HDP) model
for timeline summarization. In EHDP, each HDP
is built for multiple corpora at each time epoch,
and the time dependencies are incorporated into
epochs under the Markovian assumptions. Topic
popularity and topic-word distribution can be in-
ferred from a Chinese Restaurant Process (CRP).
Sentences are selected into timelines by consider-
ing different aspects such as topic relevance, cov-
erage and coherence. We built the evaluation sys-
556
tems which contain 6 real datasets and perfor-
mance of different models is evaluated accord-
ing to the ROUGE metrics. Experimental results
demonstrate the effectiveness of our model .
2 EHDP for Timeline Summarization
2.1 Problem Formulation
Given a general query Q = {wqi}i=Qni=1 , we firstly
obtain a set of query related documents. We no-
tate different corpus as C = {Ct}t=Tt=1 according
to their published time where Ct = {Dti}i=Nti=1 de-
notes the document collection published at epoch
t. Document Dti is formulated as a collection of
sentences {stij}j=Ntij=1 . Each sentence is presented
with a series of words stij = {wtijl}
l=Ntij
l=1 and as-
sociated with a topic ?tij . V denotes the vocabu-
lary size. The output of the algorithm is a series
of timelines summarization I = {It}t=Tt=1 where
It ? Ct
2.2 EHDP
Our EHDP model is illustrated in Figure 2. Specif-
ically, each corpus Ct is modeled as a HDP. These
HDP shares an identical base measure G0, which
serves as an overall bookkeeping of overall mea-
sures. We use Gt0 to denote the base measure at
each epoch and draw the local measureGti for each
document at time t from Gt0. In EHDP, each sen-
tence is assigned to an aspect ?tij with the consid-
eration of words within current sentence.
To consider time dependency information in
EHDP, we link all time specific base measures Gt0
with a temporal Dirichlet mixture model as fol-
lows:
Gt0 ? DP (?t,
1
KG0+
1
K
??
?=0
F (v, ?)?Gt??0 ) (1)
where F (v, ?) = exp(??/v) denotes the expo-
nential kernel function that controls the influence
of neighboring corpus. K denotes the normaliza-
tion factor where K = 1 + ???=0 F (v, ?). ? is
the time width and ? is the decay factor. In Chi-
nese Restaurant Process (CRP), each document is
referred to a restaurant and sentences are com-
pared to customers. Customers in the restaurant
sit around different tables and each table btin is as-
sociated with a dish (topic) ?tin according to the
dish menu. Let mtk denote the number of ta-
bles enjoying dish k in all restaurants at epoch t,
mtk =
?Nt
i=1
?Ntib
n=1 1(?tin = k). We redefine
for each epoch t ? [1, T ]
1. draw global measure
Gt0 ? DP (?, 1KG0 + 1K
??
?=0 F (v, ?)Gt??0 )2. for each document Dti at epoch t,
2.1 draw local measure Gti ? DP (?,Gt0)
2.2 for each sentence stij in Dti
draw aspect ?tij ? Gti
for w ? stij draw w ? f(w)|?tij
Figure 1: Generation process for EHDP
another parameter Mtk to incorporate time depen-
dency into EHDP.
Mtk =
??
?=0
F (v, ?) ?mt??,k (2)
Let ntib denote the number of sentences sitting
around table b, in document i at epoch t. In CRP
for EHDP, when a new customer stij comes in,
he can sit on the existing table with probability
ntib/(nti?1+?), sharing the dish (topic) ?tib served
at that table or picking a new table with probabil-
ity ?/(nti ? 1 + ?). The customer has to select
a dish from the global dish menu if he chooses a
new table. A dish that has already been shared in
the global menu would be chosen with probability
M tk/(
?
kM tk+?) and a new dish with probability
?/(
?
kM tk + ?).
?tij |?ti1, ..., ?tij?1, ? ?
?
?tb=?ij
ntib
nti ? 1 + ?
??jb +
?
nti ? 1 + ?
??newjb
?newti |?, ? ?
?
k
Mtk?
iMti + ?
??k +
??
iMti + ?
G0 (3)
We can see that EHDP degenerates into a series of
independent HDPs when ? = 0 and one global
HDP when ? = T and v = ?, as discussed in
Amred and Xings work (2008).
2.3 Sentence Selection Strategy
The task of timeline summarization aims to pro-
duce a summary for each time and the generated
summary should meet criteria such as relevance ,
coverage and coherence (Li et al, 2009). To care
for these three criteria, we propose a topic scoring
algorithm based on Kullback-Leibler(KL) diver-
gence. We introduce the decreasing logistic func-
tion ?(x) = 1/(1 + ex) to map the distance into
interval (0,1).
557
Figure 2: Graphical model of EHDP.
Relevance: the summary should be related with
the proposed query Q.
FR(It) = ?(KL(It||Q))
Coverage: the summary should highly generalize
important topics mentioned in document collec-
tion at epoch t.
FCv(It) = ?(KL(It||Ct))
Coherence: News evolves over time and a good
component summary is coherent with neighboring
corpus so that a timeline tracks the gradual evolu-
tion trajectory for multiple correlative news.
FCh(It) =
??=?/2
?=??/2 F (v, ?) ? ?(KL(It||Ct??))
??=?/2
?=??/2 F (v, ?)
Let Score(It) denote the score of the summary
and it is calculated in Equ.(4).
Score(It) = ?1FR(It)+?2FCv(It)+?3FCh(It)
(4)?
i ?i = 1. Sentences with higher score are se-
lected into timeline. To avoid aspect redundancy,
MMR strategy (Goldstein et al, 1999) is adopted
in the process of sentence selection.
3 Experiments
3.1 Experiments set-up
We downloaded 3156 news articles from selected
sources such as BBC, New York Times and CNN
with various time spans and built the evaluation
systems which contains 6 real datasets. The news
belongs to different categories of Rule of Interpre-
tation (ROI) (Kumaran and Allan, 2004). Detailed
statistics are shown in Table 1. Dataset 2(Deep-
water Horizon oil spill), 3(Haiti Earthquake) and
5(Hurricane Sandy) are used as training data and
New Source Nation News Source Nation
BBC UK New York Times US
Guardian UK Washington Post US
CNN US Fox News US
ABC US MSNBC US
Table 1: New sources of datasets
News Subjects (Query) #docs #epoch
1.Michael Jackson Death 744 162
2.Deepwater Horizon oil spill 642 127
3.Haiti Earthquake 247 83
4.American Presidential Election 1246 286
5.Hurricane Sandy 317 58
6.Jerry Sandusky Sexual Abuse 320 74
Table 2: Detailed information for datasets
the rest are used as test data. Summary at each
epoch is truncated to the same length of 50 words.
Summaries produced by baseline systems and
ours are automatically evaluated through ROUGE
evaluation metrics (Lin and Hovy, 2003). For
the space limit, we only report three ROUGE
ROUGE-2-F and ROUGE-W-F score. Reference
timeline in ROUGE evaluation is manually gener-
ated by using Amazon Mechanical Turk1. Work-
ers were asked to generate reference timeline for
news at each epoch in less than 50 words and we
collect 790 timelines in total.
3.2 Parameter Tuning
To tune the parameters ?(i = 1, 2, 3) and v in our
system, we adopt a gradient search strategy. We
firstly fix ?i to 1/3. Then we perform experiments
on with setting different values of v/#epoch in
the range from 0.02 to 0.2 at the interval of 0.02.
We find that the Rouge score reaches its peak at
round 0.1 and drops afterwards in the experiments.
Next, we set the value of v is set to 0.1 ? #epoch
and gradually change the value of ?1 from 0 to 1
with interval of 0.05, with simultaneously fixing
?2 and ?3 to the same value of (1 ? ?1)/2. The
performance gets better as ?1 increases from 0 to
0.25 and then declines. Then we set the value of
?1 to 0.25 and change the value of ?2 from 0 to
0.75 with interval of 0.05. And the value of ?2 is
set to 0.4, and ?3 is set to 0.35 correspondingly.
3.3 Comparison with other topic models
In this subsection, we compare our model with
4 topic model baselines on the test data. Stand-
HDP(1): A topic approach that models different
time epochs as a series of independent HDPs with-
out considering time dependency. Stand-HDP(2):
1http://mturk.com
558
M.J. Death US Election S. Sexual Abuse
System R2 RW R2 RW R2 RW
EHDP 0.089 0.130 0.081 0.154 0.086 0.152
Stand-HDP(1) 0.080 0.127 0.075 0.134 0.072 0.138
Stand-HDP(2) 0.077 0.124 0.072 0.127 0.071 0.131
Dyn-LDA 0.080 0.129 0.073 0.130 0.077 0.134
Stan-LDA 0.072 0.117 0.065 0.122 0.071 0.121
Table 3: Comparison with topic models
M.J. Death US Election S. Sexual Abuse
System R2 RW R2 RW R2 RW
EHDP 0.089 0.130 0.081 0.154 0.086 0.152
Centroid 0.057 0.101 0.054 0.098 0.060 0.132
Manifold 0.053 0.108 0.060 0.111 0.069 0.128
ETS 0.078 0.120 0.073 0.130 0.075 0.135
Chieu 0.064 0.107 0.064 0.122 0.071 0.131
Table 4: Comparison with other baselines
A global HDP which models the whole time span
as a restaurant. The third baseline, Dynamic-
LDA is based on Blei and Laffery(2007)?s work
and Stan-LDA is based on standard LDA model.
In LDA based models, aspect number is prede-
fined as 80 2. Experimental results of different
models are shown in Table 2. As we can see,
EHDP achieves better results than the two stan-
dard HDP baselines where time information is not
adequately considered. We also find an interesting
result that Stan-HDP performs better than Stan-
LDA. This is partly because new aspects can be
automatically detected in HDP. As we know, how
to determine topic number in the LDA-based mod-
els is still an open problem.
3.4 Comparison with other baselines
We implement several baselines used in tradi-
tional summarization or timeline summarization
for comparison. (1) Centroid applies the MEAD
algorithm (Radev et al, 2004) according to the
features including centroid value, position and
first-sentence overlap. (2) Manifold is a graph
based unsupervised method for summarization,
and the score of each sentence is got from the
propagation through the graph (Wan et al, 2007).
(3) ETS is the timeline summarization approach
developed by Yan et al, (2011a), which is a graph
based approach with optimized global and local
biased summarization. (4) Chieu is the time-
line system provided by (Chieu and Lee, 2004)
utilizing interest and bursty ranking but neglect-
ing trans-temporal news evolution. As we can
see from Table 3, Centroid and Manifold get
the worst results. This is probably because meth-
ods in multi-document summarization only care
2In our experiments, the aspect number is set as 50, 80,
100 and 120 respectively and we select the best performed
result with the aspect number as 80
about sentence selection and neglect the novelty
detection task. We can also see that EHDP under
our proposed framework outputs existing timeline
summarization approaches ETS and chieu. Our
approach outputs Yan et al,(2011a)s model by
6.9% and 9.3% respectively with regard to the av-
erage score of ROUGE-2-F and ROUGE-W-F.
4 Conclusion
In this paper we present an evolutionary HDP
model for timeline summarization. Our EHDP ex-
tends original HDP by incorporating time depen-
dencies and background information. We also de-
velop an effective sentence selection strategy for
candidate in the summaries. Experimental results
on real multi-time news demonstrate the effective-
ness of our topic model.
Oct. 3, 2012
S1: The first debate between President Obama and Mitt Rom-
ney, so long anticipated, quickly sunk into an unenlightening
recitation of tired talking points and mendacity. S2. Mr. Rom-
ney wants to restore the Bush-era tax cut that expires at the end
of this year and largely benefits the wealthy
Oct. 11, 2012
S1: The vice presidential debate took place on Thursday, Oc-
tober 11 at Kentucky?sCentre College, and was moderated by
Martha Raddatz. S2: The first and only debate between Vice
President Joe Biden and Congressman Paul Ryan focused on
domestic and foreign policy. The domestic policy segments in-
cluded questions on health care, abortion
Oct. 16, 2012
S1. President Obama fights back in his second debate with Mitt
Romney, banishing some of the doubts he raised in their first
showdown. S2: The second debate dealt primarily with domes-
tic affairs and include some segues into foreign policy. includ-
ing taxes, unemployment, job creation, the national debt, energy
and women?s rights, both legal and
Table 5: Selected timeline summarization gener-
ated by EHDP for American Presidential Election
5 Acknowledgement
This research has been supported by NSFC grants
(No.61273278), National Key Technology RD
Program (No:2011BAH1B0403), National 863
Program (No.2012AA011101) and National So-
cial Science Foundation (No.12ZD227).
References
Amr Ahmed and Eric Xing. Dynamic non-parametric
mixture models and the recurrent chinese restaurant
process. 2008. In SDM.
559
James Allan, Rahul Gupta and Vikas Khandelwal.
Temporal summaries of new topics. 2001. In Pro-
ceedings of the 24th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval
David Blei, Andrew Ng and Micheal Jordan. 2003.
Latent dirichlet alocation. In Journal of Machine
Learning Research.
David Blei and John Lafferty. Dynamic topic models.
2006. In Proceedings of the 23rd international con-
ference on Machine learning.
Francois Carol, Manuel Davy and Arnaud Doucet.
Generalized poly urn for time-varying dirichlet pro-
cess mixtures. 2007. In Proceedings of the Interna-
tional Conference on Uncertainty in Artificial Intel-
ligence.
Deepayan Chakrabarti, Ravi Kumar and Andrew
Tomkins. Evolutionary Clustering. InProceedings of
the 12th ACM SIGKDD international conference
Knowledge discoveryand data mining.
Hai-Leong Chieu and Yoong-Keok Lee. Query based
event extraction along a timeline. In Proceedings of
the 27th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval
Giridhar Kumaran and James Allan. 2004. Text classifi-
cation and named entities for new event detection. In
Proceedings of the 27th annual international ACM
SIGIR04.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha
and Yong Yu. Enhancing diversity, coverage and bal-
ance for summarization through structure learning.
In Proceedings of the 18th international conference
on World wide web.
Chin-Yew Lin and Eduard Hovy. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the Human Language Technology
Conference of the NAACL. 2003.
Dragomar Radev, Hongyan. Jing, and Malgorzata Stys.
2004. Centroid-based summarization of multiple
documents. In Information Processing and Manage-
ment.
Lu Ren, David Dunson and Lawrence Carin. The dy-
namic hierarchical Dirichlet process. 2008. In Pro-
ceedings of the 25th international conference on
Machine Learning.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao.
2007. Manifold-ranking based topic-focused multi-
document summarization. In Proceedings of Inter-
national Joint Conference on Artificial Intelligence.
Xuerui Wang and Andrew MaCallum. Topics over
time: a non-Markov continuous-time model of topi-
cal trends. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining.
Yee Whye Teh, Michael Jordan, Matthew Beal and
David Blei. Hierarchical Dirichlet Processes. In
American Statistical Association.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li and Yan Zhang. 2011a. Evolutionary
Timeline Summarization: a Balanced Optimization
Framework via Iterative Substitution. In Proceed-
ings of the 34th international ACM SIGIR confer-
ence on Research and development in Information
Retrieval.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Jahna Otterbacher, Xiaoming Li and Yan Zhang.
Timeline Generation Evolutionary Trans-Temporal
Summarization. 2011b. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Jianwen Zhang, Yangqiu Song, Changshui Zhang and
Shixia Liu. 2010. Evolutionary Hierarchical Dirich-
let Processes for Multiple Correlated Time-varying
Corpora. In Proceedings of the 16th ACM SIGKDD
international conference on Knowledge discovery
and data mining.
560
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 165?174,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Weakly Supervised User Profile Extraction from Twitter
Jiwei Li
1
, Alan Ritter
2
, Eduard Hovy
1
1
Language Technology Institute,
2
Machine Learning Department
Carnegie Mellon University, Pittsburgh, PA 15213, USA
bdlijiwei@gmail.com, rittera@cs.cmu.edu, ehovy@andrew.cmu.edu
Abstract
While user attribute extraction on social
media has received considerable attention,
existing approaches, mostly supervised,
encounter great difficulty in obtaining gold
standard data and are therefore limited
to predicting unary predicates (e.g., gen-
der). In this paper, we present a weakly-
supervised approach to user profile extrac-
tion from Twitter. Users? profiles from so-
cial media websites such as Facebook or
Google Plus are used as a distant source
of supervision for extraction of their at-
tributes from user-generated text. In addi-
tion to traditional linguistic features used
in distant supervision for information ex-
traction, our approach also takes into ac-
count network information, a unique op-
portunity offered by social media. We test
our algorithm on three attribute domains:
spouse, education and job; experimental
results demonstrate our approach is able
to make accurate predictions for users? at-
tributes based on their tweets.
1
1 Introduction
The overwhelming popularity of online social me-
dia creates an opportunity to display given as-
pects of oneself. Users? profile information in
social networking websites such as Facebook
2
or
Google Plus
3
provides a rich repository personal
information in a structured data format, making it
amenable to automatic processing. This includes,
for example, users? jobs and education, and pro-
vides a useful source of information for applica-
tions such as search
4
, friend recommendation, on-
1
Both code and data are available at http://aclweb.
org/aclwiki/index.php?title=Profile_data
2
https://www.facebook.com/
3
https://plus.google.com/
4
https://www.facebook.com/about/
graphsearch
@[shanenicholson] has taken all the kids today so
I can go shopping-CHILD FREE! #iloveyoushano
#iloveyoucreditcard
Tamworth promo day with my handsome classy husband
@[shanenicholson]
Spouse: shanenicholson
I got accepted to be part of the UofM engineering safety
pilot program in [FSU]
Here in class. (@ [Florida State University] - Williams
Building)
Don?t worry , guys ! Our beloved [FSU] will always con-
tinue to rise ? to the top !
Education: Florida State University (FSU)
first day of work at [HuffPo], a sports bar woo come visit
me yo..
start to think we should just add a couple desks to the
[HuffPo] newsroom for Business Insider writers
just back from [HuffPo], what a hell !
Job: HuffPo
Table 1: Examples of Twitter message clues for
user profile inference.
line advertising, computational social science and
more.
Although profiles exist in an easy-to-use, struc-
tured data format, they are often sparsely popu-
lated; users rarely fully complete their online pro-
files. Additionally, some social networking ser-
vices such as Twitter don?t support this type of
structured profile data. It is therefore difficult to
obtain a reasonably comprehensive profile of a
user, or a reasonably complete facet of information
(say, education level) for a class of users. While
many users do not explicitly list all their personal
information in their online profile, their user gen-
erated content often contains strong evidence to
suggest many types of user attributes, for example
education, spouse, and employment (See Table 1).
Can one use such information to infer more de-
tails? In particular, can one exploit indirect clues
from an unstructured data source like Twitter to
obtain rich, structured user profiles?
In this paper we demonstrate that it is feasi-
ble to automatically extract Facebook-style pro-
165
files directly from users? tweets, thus making
user profile data available in a structured format
for upstream applications. We view user profile
inference as a structured prediction task where
both text and network information are incorpo-
rated. Concretely, we cast user profile predic-
tion as binary relation extraction (Brin, 1999),
e.g., SPOUSE(User
i
, User
j
), EDUCATION(User
i
,
Entity
j
) and EMPLOYER(User
i
, Entity
j
). Inspired
by the concept of distant supervision, we collect
training tweets by matching attribute ground truth
from an outside ?knowledge base? such as Face-
book or Google Plus.
One contribution of the work presented here is
the creation of the first large-scale dataset on three
general Twitter user profile domains (i.e., EDUCA-
TION, JOB, SPOUSE). Experiments demonstrate
that by simultaneously harnessing both text fea-
tures and network information, our approach is
able to make accurate user profile predictions. We
are optimistic that our approach can easily be ap-
plied to further user attributes such as HOBBIES
and INTERESTS (MOVIES, BOOKS, SPORTS or
STARS), RELIGION, HOMETOWN, LIVING LOCA-
TION, FAMILY MEMBERS and so on, where train-
ing data can be obtained by matching ground truth
retrieved from multiple types of online social me-
dia such as Facebook, Google Plus, or LinkedIn.
Our contributions are as follows:
? We cast user profile prediction as an informa-
tion extraction task.
? We present a large-scale dataset for this task
gathered from various structured and unstruc-
tured social media sources.
? We demonstrate the benefit of jointly rea-
soning about users? social network structure
when extracting their profiles from text.
? We experimentally demonstrate the effective-
ness of our approach on 3 relations: SPOUSE,
JOB and EDUCATION.
The remainder of this paper is organized as fol-
lows: We summarize related work in Section 2.
The creation of our dataset is described in Section
3. The details of our model are presented in Sec-
tion 4. We present experimental results in Section
5 and conclude in Section 6.
2 Related Work
While user profile inference from social media has
received considerable attention (Al Zamal et al,
2012; Rao and Yarowsky, 2010; Rao et al, 2010;
Rao et al, 2011), most previous work has treated
this as a classification task where the goal is to pre-
dict unary predicates describing attributes of the
user. Examples include gender (Ciot et al, 2013;
Liu and Ruths, 2013; Liu et al, 2012), age (Rao et
al., 2010), or political polarity (Pennacchiotti and
Popescu, 2011; Conover et al, 2011).
A significant challenge that has limited previous
efforts in this area is the lack of available training
data. For example, researchers obtain training data
by employing workers from Amazon Mechanical
Turk to manually identify users? gender from pro-
file pictures (Ciot et al, 2013). This approach is
appropriate for attributes such as gender with a
small numbers of possible values (e.g., male or fe-
male), for which the values can be directly iden-
tified. However for attributes such as spouse or
education there are many possible values, making
it impossible to manually search for gold standard
answers within a large number of tweets which
may or may not contain sufficient evidence.
Also related is the Twitter user timeline extrac-
tion algorithm of Li and Cardie (2013). This work
is not focused on user attribute extraction, how-
ever.
Distant Supervision Distant supervision, also
known as weak supervision, is a method for learn-
ing to extract relations from text using ground
truth from an existing database as a source of
supervision. Rather than relying on mention-
level annotations, which are expensive and time
consuming to generate, distant supervision lever-
ages readily available structured data sources as
a weak source of supervision for relation ex-
traction from related text corpora (Craven et
al., 1999). For example, suppose r(e
1
, e
2
) =
IsIn(Paris, France) is a ground tuple in the
database and s =?Paris is the capital of France?
contains synonyms for both ?Paris? and ?France?,
then we assume that s may express the fact
r(e
1
, e
2
) in some way and can be used as pos-
itive training examples. In addition to the wide
use in text entity relation extraction (Mintz et al,
2009; Ritter et al, 2013; Hoffmann et al, 2011;
Surdeanu et al, 2012; Takamatsu et al, 2012),
distant supervision has been applied to multiple
166
Figure 1: Illustration of Goolge Plus ?knowledge
base?.
fields such as protein relation extraction (Craven
et al, 1999; Ravikumar et al, 2012), event extrac-
tion from Twitter (Benson et al, 2011), sentiment
analysis (Go et al, 2009) and Wikipedia infobox
generation (Wu and Weld, 2007).
Homophily Online social media offers a rich
source of network information. McPherson et
al. (2001) discovered that people sharing more
attributes such as background or hobby have
a higher chance of becoming friends in social
media. This property, known as HOMOPHILY
(summarized by the proverb ?birds of a feather
flock together?) (Al Zamal et al, 2012) has been
widely applied to community detection (Yang and
Leskovec, 2013) and friend recommendation (Guy
et al, 2010) on social media. In the user attribute
extraction literature, researchers have considered
neighborhood context to boost inference accuracy
(Pennacchiotti and Popescu, 2011; Al Zamal et al,
2012), where information about the degree of their
connectivity to their pre-labeled users is included
in the feature vectors. A related algorithm by Mis-
love et al (2010) crawled Facebook profiles of
4,000 Rice University students and alumni and in-
ferred attributes such as major and year of ma-
triculation purely based on network information.
Mislove?s work does not consider the users? text
stream, however. As we demonstrate below, rely-
ing solely on network information is not enough to
enable inference about attributes.
3 Dataset Creation
We now describe the generation of our distantly
supervised training dataset in detail. We make
use of Google Plus and Freebase to obtain ground
facts and extract positive/negative bags of post-
ings from users? twitter streams according to the
ground facts.
Figure 2: Example of fetching tweets containing
entity USC mention from Miranda Cosgrove (an
American actress and singer-songwriter)?s twitter
stream.
Education/Job We first used the Google Plus
API
5
(shown in Figure 1) to obtain a seed set
of users whose profiles contain both their educa-
tion/job status and a link to their twitter account.
6
Then, we fetched tweets containing the mention of
the education/job entity from each correspondent
user?s twitter stream using Twitter?s search API
7
(shown in Figure 2) and used them to construct
positive bags of tweets expressing the associated
attribute, namely EDUCATION(User
i
, Entity
j
), or
EMPLOYER(User
i
, Entity
j
). The Freebase API
8
is employed for alias recognition, to match terms
such as ?Harvard University?, ?Harvard?, ?Har-
vard U? to a single The remainder of each corre-
sponding user?s entire Twitter feed is used as neg-
ative training data.
9
We expanded our dataset from the seed users
according to network information provided by
Google Plus and Twitter. Concretely, we crawled
circle information of users in the seed set from
both their Twitter and Google Plus accounts and
performed a matching to pick out shared users
between one?s Twitter follower list and Google
Plus Circle. This process assures friend identity
and avoids the problem of name ambiguity when
matching accounts across websites. Among candi-
date users, those who explicitly display Job or Ed-
ucation information on Google Plus are preserved.
We then gathered positive and negative data as de-
scribed above.
Dataset statistics are presented in Table 2. Our
5
https://developers.google.com/+/api/
6
An unambiguous twitter account link is needed here be-
cause of the common phenomenon of name duplication.
7
https://twitter.com/search
8
http://wiki.freebase.com/wiki/
Freebase_API
9
Due to Twitter user timeline limit, we crawled at most
3200 tweets for each user.
167
education dataset contains 7,208 users, 6,295 of
which are connected to others in the network. The
positive training set for the EDUCATION is com-
prised of 134,060 tweets.
Spouse Facebook is the only type of social me-
dia where spouse information is commonly dis-
played. However, only a tiny amount of individ-
ual information is publicly accessible from Face-
book Graph API
10
. To obtain ground truth for the
spouse relation at large scale, we turned to Free-
base
11
, a large, open-domain database, and gath-
ered instances of the /PEOPLE/PERSON/SPOUSE
relation. Positive/negative training tweets are ob-
tained in the same way as was previously de-
scribed for EDUCATION and JOB. It is worth
noting that our Spouse dataset is not perfect, as
individuals retrieved from Freebase are mostly
celebrities, and thus it?s not clear whether this
group of people are representative of the general
population.
SPOUSE is an exception to the ?ho-
mophily? effect. But it exhibits another
unique property, known as, REFLEXIVITY: fact
IsSpouseOf(e
1
, e
2
) and IsSpouseOf(e
2
, e
1
)
will hold or not hold at the same time. Given train-
ing data expressing the tuple IsSpouseOf(e
1
, e
2
)
from user e
1
?s twitter stream, we also gather user
e
2
?s tweet collection, and fetch tweets with the
mention of e
1
. We augment negative training
data from e
2
as in the case of Education and Job.
Our Spouse dataset contains 1,636 users, where
there are 554 couples (1108 users). Note that
the number of positive entities (3,121) is greater
than the number of users as (1) one user can have
multiple spouses at different periods of time (2)
multiple entities may point to the same individual,
e.g., BarackObama, Barack Obama and Barack.
4 Model
We now describe our approach to predicting user
profile attributes.
4.1 Notation
Message X: Each user i ? [1, I] is associ-
ated with his Twitter ID and his tweet corpus
X
i
. X
i
is comprised of a collection of tweets
X
i
= {x
i,j
}
j=N
i
j=1
, where N
i
denotes the number
of tweets user i published.
10
https://developers.facebook.com/docs/
graph-api/
11
http://www.freebase.com/
Education Job Spouse
#Users 7,208 1,806 1,636
#Users Con-
nected
6,295 1,407 1,108
#Edges 11,167 3,565 554
#Pos Entities 451 380 3121
#Pos Tweets 124,801 65,031 135,466
#Aver Pos
Tweets per User
17.3 36.6 82.8
#Neg Entity 6,987,186 4,405,530 8,840,722
#Neg Tweets 16,150,600 10,687,403 12,872,695
Table 2: Statistics for our Dataset
Tweet Collection L
e
i
: L
e
i
denotes the collection
of postings containing the mention of entity e from
user i. L
e
i
? X
i
.
Entity attribute indicator z
k
i,e
and z
k
i,x
: For
each entity e ? X
i
, there is a boolean variable z
k
i,e
,
indicating whether entity e expresses attribute k of
user i. Each posting x ? L
e
i
is associated with at-
tribute indicator z
k
i,x
indicating whether posting x
expresses attribute k of user i. z
k
i,e
and z
k
i,x
are
observed during training and latent during testing.
Neighbor set F
k
i
: F
k
i
denotes the neighbor set
of user i. For Education (k = 0) and Job (k = 1),
F
k
i
denotes the group of users within the network
that are in friend relation with user i. For Spouse
attribute, F
k
i
denote current user?s spouse.
4.2 Model
The distant supervision assumes that if entity e
corresponds to an attribute for user i, at least one
posting from user i?s Twitter stream containing a
mention of emight express that attribute. For user-
level attribute prediction, we adopt the following
two strategies:
(1) GLOBAL directly makes aggregate (entity)
level prediction for z
k
i,e
, where features for all
tweets from L
e
i
are aggregated to one vector for
training and testing, following Mintz et al (2009).
(2) LOCAL makes local tweet-level predictions
for each tweet z
e
i,x
, x ? L
k
i
in the first place, mak-
ing the stronger assumption that all mentions of an
entity in the users? profile are expressing the asso-
ciated attribute. An aggregate-level decision z
k
i,e
is
then made from the deterministic OR operators.
z
e
i,x
=
{
1 ?x ? L
e
i
, s.t.z
k
i,x
= 1
0 Otherwise
(1)
The rest of this paper describes GLOBAL in de-
tail. The model and parameters with LOCAL are
identical to those in GLOBAL except that LOCAL
168
encode a tweet-level feature vector rather than an
aggregate one. They are therefore excluded for
brevity. For each attribute k, we use a model that
factorizes the joint distribution as product of two
distributions that separately characterize text fea-
tures and network information as follows:
?(z
k
i,e
, X
i
, F
k
i
: ?) ?
?
text
(z
k
i,e
, X
i
)?
Neigh
(z
k
i,e
, F
k
i
)
(2)
Text Factor We use ?
text
(z
k
e
, X
i
) to capture the
text related features which offer attribute clues:
?
text
(z
k
e
, , X
i
) = exp[(?
k
text
)
T
? ?
text
(z
k
i,e
, X
i
)]
(3)
The feature vector ?
text
(z
k
i,e
, X
i
) encodes the fol-
lowing standard general features:
? Entity-level: whether begins with capital let-
ter, length of entity.
? Token-level: for each token t ? e, word iden-
tity, word shape, part of speech tags, name
entity tags.
? Conjunctive features for a window of k
(k=1,2) words and part of speech tags.
? Tweet-level: All tokens in the correspondent
tweet.
In addition to general features, we employ
attribute-specific features, such as whether the en-
tity matches a bag of words observed in the list
of universities, colleges and high schools for Edu-
cation attribute, whether it matches terms in a list
of companies for Job attribute
12
. Lists of universi-
ties and companies are taken from knowledge base
NELL
13
.
Neighbor Factor For Job and Education, we
bias friends to have a larger possibility to share
the same attribute. ?
Neigh
(z
k
i,e
, F
k
i
) captures such
influence from friends within the network:
?
Neigh
(z
k
i,e
, F
k
i
) =
?
j?F
k
i
?
Neigh
(z
k
e
, X
j
)
?
Neigh
(z
k
i,e
, X
j
)
= exp[(?
k
Neigh
)
T
? ?
Neigh
(z
k
i,e
, X
j
)]
(4)
Features we explore include the whether entity e
is also the correspondent attribute with neighbor
user j, i.e., I(z
e
j,k
= 0) and I(z
e
j,k
= 1).
12
Freebase is employed for alias recognition.
13
http://rtw.ml.cmu.edu/rtw/kbbrowser/
Input: Tweet Collection {X
i
}, Neighbor set
{F
k
i
}
Initialization:
? for each user i:
for each candidate entity e ? X
i
z
k
i,e
= argmax
z
?
?(z
?
, X
i
) from text
features
End Initialization
while not convergence:
? for each user i:
update attribute values for j ? F
k
i
for each candidate entity e ? X
i
z
k
i,e
= argmax
z
?
?(z
?
, X
i
, F
k
i
)
end while:
Figure 3: Inference for NEIGH-LATENT setting.
For Spouse, we set F
spouse
i
= {e} and the
neighbor factor can be rewritten as:
?
Neigh
(z
k
i,e
, X
j
) = ?
Neigh
(C
i
, X
e
) (5)
It characterizes whether current user C
i
to be the
spouse of user e (if e corresponds to a Twitter
user). We expect clues about whether C
i
being en-
tity e?s spouse from e?s Twitter corpus will in turn
facilitate the spouse inference procedure of user i.
?
Neigh
(C
i
, X
e
) encodes I(C
i
? S
e
), I(C
i
6? S
e
).
Features we explore also include whether C
i
?s
twitter ID appears in e?s corpus.
4.3 Training
We separately trained three classifiers regarding
the three attributes. All variables are observed
during training; we therefore take a feature-based
approach to learning structure prediction models
inspired by structure compilation (Liang et al,
2008). In our setting, a subset of the features
(those based on network information) are com-
puted based on predictions that will need to be
made at test time, but are observed during train-
ing. This simplified approach to learning avoids
expensive inference; at test time, however, we still
need to jointly predict the best attribute values for
friends as is described in section 4.4.
4.4 Inference
Job and Education Our inference algorithm
for Job/Education is performed on two settings,
depending on whether neighbor information is
169
observed (NEIGH-OBSERVED) or latent (NEIGH-
LATENT). Real world applications, where network
information can be partly retrieved from all types
of social networks, can always falls in between.
Inference in the NEIGH-OBSERVED setting is
trivial; for each entity e ? G
i
, we simply predict
it?s candidate attribute values using Equ.6.
z
k
i,e
= argmax
z
?
?(z
?
, X
i
, F
k
i
) (6)
For NEIGH-LATENT setting, attributes for each
node along the network are treated latent and user
attribute prediction depends on attributes of his
neighbors. The objective function for joint infer-
ence would be difficult to optimize exactly, and
algorithms for doing so would be unlikely to scale
to network of the size we consider. Instead, we use
a sieve-based greedy search approach to inference
(shown in Figure 3) inspired by recent work on
coreference resolution (Raghunathan et al, 2010).
Attributes are initialized using only text features,
maximizing ?
text
(e,X
i
), and ignoring network
information. Then for each user we iteratively re-
estimate their profile given both their text features
and network features (computed based on the cur-
rent predictions made for their friends) which pro-
vide additional evidence.
In this way, highly confident predictions will be
made strictly from text in the first round, then the
network can either support or contradict low con-
fidence predictions as more decisions are made.
This process continues until no changes are made
at which point the algorithm terminates. We em-
pirically found it to work well in practice. We ex-
pect that NEIGH-OBSERVED performs better than
NEIGH-LATENT since the former benefits from
gold network information.
Spouse For Spouse inference, if candidate entity
e has no correspondent twitter account, we directly
determine z
k
i,e
= argmax
z
?
?(z
?
, X
i
) from text
features. Otherwise, the inference of z
k
i,e
depends
on the z
k
e,C
i
. Similarly, we initialize z
k
i,e
and z
k
e,C
i
by maximizing text factor, as we did for Educa-
tion and Job. Then we iteratively update z
k
given
by the rest variables until convergence.
5 Experiments
In this Section, we present our experimental re-
sults in detail.
Education Job
AFFINITY 74.3 14.5
Table 3: Affinity values for Education and Job.
5.1 Preprocessing and Experiment Setup
Each tweet posting is tokenized using Twitter NLP
tool introduced by Noah?s Ark
14
with # and @
separated following tokens. We assume that at-
tribute values should be either name entities or
terms following @ and #. Name entities are ex-
tracted using Ritter et al?s NER system (2011).
Consecutive tokens with the same named entity
tag are chunked (Mintz et al, 2009). Part-of-
speech tags are assigned based on Owoputi et als
tweet POS system (Owoputi et al, 2013).
Data is divided in halves. The first is used as
training data and the other as testing data.
5.2 Friends with Same Attribute
Our network intuition is that users are much more
likely to be friends with other users who share at-
tributes, when compared to users who have no at-
tributes in common. In order to statistically show
this, we report the value of AFFINITY defined by
Mislove et al(2010), which is used to quantita-
tively evaluate the degree of HOMOPHILY in the
network. AFFINITY is the ratio of the fraction of
links between attribute (k)-sharing users (S
k
), rel-
ative to what is expected if attributes are randomly
assigned in the network (E
k
).
S
k
=
?
i
?
j?F
k
i
I(P
k
i
= P
k
j
)
?
i
?
j?F
k
i
I
E
k
=
?
m
T
k
m
(T
k
m
? 1)
U
k
(U
k
? 1)
(7)
where T
k
m
denotes the number of users with m
value for attribute k and U
k
=
?
m
T
k
m
. Table 3
shows the affinity value of the Education and Job.
As we can see, the property of HOMOPHILY in-
deed exists among users in the social network with
respect to Education and Job attribute, as signifi-
cant affinity is observed. In particular, the affinity
value for Education is 74.3, implying that users
connected by a link in the network are 74.3 times
more likely affiliated in the same school than as
expected if education attributes are randomly as-
signed. It is interesting to note that Education ex-
hibits a much stronger HOMOPHILY property than
14
https://code.google.com/p/
ark-tweet-nlp/downloads/list
170
Job. Such affinity demonstrates that our approach
that tries to take advantage of network information
for attribute prediction of holds promise.
5.3 Evaluation and Discussion
We evaluate settings described in Section 4.2 i.e.,
GLOBAL setting, where user-level attribute is pre-
dicted directly from jointly feature space and LO-
CAL setting where user-level prediction is made
based on tweet-level prediction along with differ-
ent inference approaches described in Section 4.4,
i.e. NEIGH-OBSERVED and NEIGH-LATENT, re-
garding whether neighbor information is observed
or latent.
Baselines We implement the following base-
lines for comparison and use identical processing
techniques for each approach for fairness.
? Only-Text: A simplified version of our algo-
rithm where network/neighbor influence is ig-
nored. Classifier is trained and tested only based
on text features.
? NELL: For Job and Education, candidate is se-
lected as attribute value once it matches bag of
words in the list of universities or companies
borrowed from NELL. For Education, the list is
extended by alias identification based on Free-
base. For Job, we also fetch the name abbrevia-
tions
15
. NELL is only implemented for Educa-
tion and Job attribute.
For each setting from each approach, we report
the (P)recision, (R)ecall and (F)1-score. For LO-
CAL setting, we report the performance for both
entity-level prediction (Entity) and posting-level
prediction (Tweet). Results for Education, Job and
Spouse from different approaches appear in Table
4, 5 and 6 respectively.
Local or Global For horizontal comparison, we
observe that GLOBAL obtains a higher Precision
score but a lower Recall than LOCAL(ENTITY).
This can be explained by the fact that LOCAL(U)
sets z
k
i,e
= 1 once one posting x ? L
e
i
is identified
as attribute related, while GLOBAL tend to be more
meticulous by considering the conjunctive feature
space from all postings.
Homophile effect In agreement with our ex-
pectation, NEIGH-OBSERVED performs better than
NEIGH-LATENT since erroneous predictions in
15
http://www.abbreviations.com/
NEIGH-LATENT setting will have negative in-
fluence on further prediction during the greedy
search process. Both NEIGH-OBSERVED and
NEIGH-LATENT where network information is
harnessed, perform better than Only-Text, which
the prediction is made independently on user?s text
features. The improvement of NEIGH-OBSERVED
over Only-Text is 22.7% and 6.4% regarding F-
1 score for Education and Job respectively, which
further illustrate the usefulness of making use of
Homophile effect for attribute inference on online
social media. It is also interesting to note the im-
provement much more significant in Education in-
ference than Job inference. This is in accord with
what we find in Section 5.2, where education net-
work exhibits stronger HOMOPHILE property than
Job network, enabling a significant benefit for ed-
ucation inference, but limited for job inference.
Spouse prediction also benefits from neighbor-
ing effect and the improvement is about 12% for
LOCAL(ENTITY) setting. Unlike Education and
Job prediction, for which in NEIGH-OBSERVED
setting all neighboring variables are observed, net-
work variables are hidden during spouse predic-
tion. By considering network information, the
model benefits from evident clues offered by tweet
corpus of user e?s spouse when making prediction
for e, but also suffers when erroneous decision are
made and then used for downstream predictions.
NELL Baseline Notably, NELL achieves high-
est Recall score for Education inference. It is
also worth noting that most of education men-
tions that NELL fails to retrieve are those in-
volve irregular spellings, such as HarvardUniv and
Cornell U, which means Recall score for NELL
baseline would be even higher if these irregular
spellings are recognized in a more sophisticated
system. The reason for such high recall is that as
our ground truths are obtained from Google plus,
the users from which are mostly affiliated with de-
cent schools found in NELL dictionary. However,
the high recall from NELL is sacrificed at preci-
sion, as users can mention school entities in many
of situations, such as paying a visit or reporting
some relevant news. NELL will erroneously clas-
sify these cases as attribute mentions.
NELL does not work out for Job, with a fairly
poor 0.0156 F1 score for LOCAL(ENTITY) and
0.163 for LOCAL(TWEET). Poor precision is ex-
pected for as users can mention firm entity in a
great many of situations. The recall score for
171
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach
NEIGH-OBSERVED 0.804 0.515 0.628 0.524 0.780 0.627 0.889 0.729 0.801
NEIGH-LATENT 0.755 0.440 0.556 0.420 0.741 0.536 0.854 0.724 0.783
Only-Text ?- 0.735 0.393 0.512 0.345 0.725 0.467 0.809 0.724 0.764
NELL ?- ?- ?- ?- 0.170 0.798 0.280 0.616 0.848 0.713
Table 4: Results for Education Prediction
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach
NEIGH-OBSERVED 0.643 0.330 0.430 0.374 0.620 0.467 0.891 0.698 0.783
NEIGH-LATENT 0.617 0.320 0.421 0.226 0.544 0.319 0.804 0.572 0.668
Only-Text ?- 0.602 0.304 0.404 0.155 0.501 0.237 0.764 0.471 0.583
NELL ?- ?- ?- ?- 0.0079 0.509 0.0156 0.094 0.604 0.163
Table 5: Results for Job Prediction
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach ?- 0.870 0.560 0.681 0.593 0.857 0.701 0.904 0.782 0.839
Only-Text ?- 0.852 0.448 0.587 0.521 0.781 0.625 0.890 0.729 0.801
Table 6: Results for Spouse Prediction
NELL in job inference is also quite low as job
related entities exhibit a greater diversity of men-
tions, many of which are not covered by the NELL
dictionary.
Vertical Comparison: Education, Job and
Spouse Job prediction turned out to be much
more difficult than Education, as shown in Ta-
bles 4 and 5. Explanations are as follows: (1)
Job contains a much greater diversity of mentions
than Education. Education inference can benefit a
lot from the dictionary relevant feature which Job
may not. (2) Education mentions are usually asso-
ciated with clear evidence such as homework, ex-
ams, studies, cafeteria or books, while situations
are much more complicated for job as vocabular-
ies are usually specific for different types of jobs.
(3) The boundary between a user working in and
a fun for a specific operation is usually ambigu-
ous. For example, a Google engineer may con-
stantly update information about outcome prod-
ucts of Google, so does a big fun. If the aforemen-
tioned engineer barely tweets about working con-
ditions or colleagues (which might still be ambigu-
ous), his tweet collection, which contains many of
mentions about outcomes of Google product, will
be significantly similar to tweets published by a
Google fun. Such nuisance can be partly solved
by the consideration of network information, but
not totally.
The relatively high F1 score for spouse predic-
tion is largely caused by the great many of non-
individual related entities in the dataset, the iden-
tification of which would be relatively simpler. A
deeper look at the result shows that the classifier
frequently makes wrong decisions for entities such
as userID and name entities. Significant as some
spouse relevant features are, such as love, hus-
band, child, in most circumstances, spouse men-
tions are extremely hard to recognize. For exam-
ple, in tweets ?Check this out, @alancross, it?s
awesome bit.ly/1bnjYHh.? or ?Happy Birth-
day @alancross !?. alancross can reasonably be
any option among current user?s friend, colleague,
parents, child or spouse. Repeated mentions add
no confidence. Although we can identify alan-
cross as spouse attribute once it jointly appear
with other strong spouse indicators, they are still
many cases where they never co-appear. How to
integrate more useful side information for spouse
recognition constitutes our future work.
6 Conclusion and Future Work
In this paper, we propose a framework for user at-
tribute inference on Twitter. We construct the pub-
licly available dataset based on distant supervision
and experiment our model on three useful user
profile attributes, i.e., Education, Job and Spouse.
Our model takes advantage of network informa-
tion on social network. We will keep updating the
dataset as more data is collected.
One direction of our future work involves ex-
ploring more general categories of user profile at-
172
tributes, such as interested books, movies, home-
town, religion and so on. Facebook would an
ideal ground truth knowledge base. Another direc-
tion involves incorporating richer feature space for
better inference performance, such as multi-media
sources (i.e. pictures and video).
7 Acknowledgments
A special thanks is owned to Dr. Julian McAuley
and Prof. Jure Leskovec from Stanford University
for the Google+ circle/network crawler, without
which the network analysis would not have been
conducted. This work was supported in part by
DARPA under award FA8750-13-2-0005.
References
Faiyaz Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of twitter users from neighbors. In
ICWSM.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In The World Wide Web
and Databases.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Mark Craven and Johan Kumlien 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Ido Guy, Naama Zwerdling, Inbal Ronen, David
Carmel, and Erel Uziel. 2010. Social media recom-
mendation based on people and tags. In Proceedings
of the 33rd international ACM SIGIR conference on
Research and development in information retrieval,
pages 194?201. ACM.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL, pages 541?550.
Jiwei Li and Claire Cardie. 2013. Timeline generation:
Tracking individuals on twitter. Proceedings of the
23rd international conference on World wide web.
Percy Liang, Hal Daum?e III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning.
Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter. In 2013 AAAI Spring Symposium Series.
Wendy Liu, Faiyaz Zamal, and Derek Ruths. 2012.
Using social media to infer gender composition of
commuter populations. In Proceedings of the When
the City Meets the Citizen Workshop, the Interna-
tional Conference on Weblogs and Social Media.
Miller McPherson, Lynn Smith-Lovin, and James M
Cook. 2001. Birds of a feather: Homophily in social
networks. Annual review of sociology, pages 415?
444.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
Alan Mislove, Bimal Viswanath, Krishna Gummadi,
and Peter Druschel. 2010. You are who you know:
inferring user profiles in online social networks. In
Proceedings of the third ACM international confer-
ence on Web search and data mining, pages 251?
260. ACM.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Marco Pennacchiotti and Ana Popescu. 2011. A ma-
chine learning approach to twitter user classification.
In ICWSM.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing.
Delip Rao and David Yarowsky. 2010. Detecting latent
user properties in social media. In Proc. of the NIPS
MLSN Workshop.
173
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37?44. ACM.
Delip Rao, Michael Paul, Clayton Fink, David
Yarowsky, Timothy Oates, and Glen Coppersmith.
2011. Hierarchical bayesian models for latent at-
tribute detection in social media. In ICWSM.
Haibin Liu, Michael Wall, Karin Verspoor, et al 2012.
Literature mining of protein-residue associations
with graph rules learned through distant supervision.
Journal of biomedical semantics, 3(Suppl 3):S2.
Alan Ritter, Sam Clark, Mausam, Oren Etzioni, et al
2011. Named entity recognition in tweets: an ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1524?1534. Association for Compu-
tational Linguistics.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
721?729. Association for Computational Linguis-
tics.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the six-
teenth ACM conference on Conference on infor-
mation and knowledge management, pages 41?50.
ACM.
Jaewon Yang and Jure Leskovec. 2013. Overlapping
community detection at scale: A nonnegative matrix
factorization approach. In Proceedings of the sixth
ACM international conference on Web search and
data mining, pages 587?596. ACM.
174
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1566?1576,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Towards a General Rule for Identifying Deceptive Opinion Spam
Jiwei Li
1
, Myle Ott
2
, Claire Cardie
2
, Eduard Hovy
1
1
Language Technology Institute, Carnegie Mellon University, Pittsburgh, P.A. 15213, USA
2
Department of Computer Science, Cornell University, Ithaca, N.Y., 14853, USA
bdlijiwei@gmail.com, myleott@cs.cornell.edu
cardie@cs.cornell.edu, ehovy@andrew.cmu.edu
Abstract
Consumers? purchase decisions are in-
creasingly influenced by user-generated
online reviews. Accordingly, there has
been growing concern about the poten-
tial for posting deceptive opinion spam?
fictitious reviews that have been deliber-
ately written to sound authentic, to de-
ceive the reader. In this paper, we ex-
plore generalized approaches for identify-
ing online deceptive opinion spam based
on a new gold standard dataset, which is
comprised of data from three different do-
mains (i.e. Hotel, Restaurant, Doctor),
each of which contains three types of re-
views, i.e. customer generated truthful re-
views, Turker generated deceptive reviews
and employee (domain-expert) generated
deceptive reviews. Our approach tries to
capture the general difference of language
usage between deceptive and truthful re-
views, which we hope will help customers
when making purchase decisions and re-
view portal operators, such as TripAdvisor
or Yelp, investigate possible fraudulent ac-
tivity on their sites.
1
1 Introduction
Consumers increasingly rely on user-generated
online reviews when making purchase deci-
sion (Cone, 2011; Ipsos, 2012). Unfortunately,
the ease of posting content to the Web, poten-
tially anonymously, creates opportunities and in-
centives for unscrupulous businesses to post de-
ceptive opinion spam?fictitious reviews that are
deliberately written to sound authentic, in order to
deceive the reader.
2
Accordingly, there appears
1
Dataset available by request from the first author.
2
Manipulating online reviews may also have legal conse-
quences. For example, the Federal Trade Commission (FTC)
to be widespread and growing concern among
both businesses and the public about this poten-
tial abuse (Meyer, 2009; Miller, 2009; Streitfeld,
2012; Topping, 2010; Ott, 2013).
Existing approaches for spam detection are usu-
ally focused on developing supervised learning-
based algorithms to help users identify decep-
tive opinion spam, which are highly dependent
upon high-quality gold-standard labeled data (Jin-
dal and Liu, 2008; Jindal et al, 2010; Lim et al,
2010; Wang et al, 2011; Wu et al, 2010). Stud-
ies in the literature rely on a couple of approaches
for obtaining labeled data, which usually fall into
two categories. The first relies on the judge-
ments of human annotators (Jindal et al, 2010;
Mukherjee et al, 2012). However, recent stud-
ies show that deceptive opinion spam is not eas-
ily identified by human readers (Ott et al, 2011).
An alternative approach, as introduced by Ott et
al. (2011), crowdsourced deceptive reviews using
Amazon Mechanical Turk.
3
A couple of follow-up
works have been introduced based on Ott et al?s
dataset, including estimating prevalence of decep-
tion in online reviews (Ott et al, 2012), identifica-
tion of negative deceptive opinion spam (Ott et al,
2013), and identifying manipulated offerings (Li
et al, 2013b).
Despite the advantages of soliciting deceptive
gold-standard material from Turkers (it is easy,
large-scale, and affordable), it is unclear whether
Turkers are representative of the general popula-
tion that generate fake reviews, or in other words,
Ott et al?s data set may correspond to only one
type of online deceptive opinion spam ? fake re-
views generated by people who have never been
to offerings or experienced the entities. Specifi-
cally, according to their findings (Ott et al, 2011;
has updated their guidelines on the use of endorsements and
testimonials in advertising to suggest that posting deceptive
reviews may be unlawful in the United States (FTC, 2009).
3
http://www.mturk.com
1566
Li et al, 2013a), truthful hotel reviews encode
more spatial details, characterized by terms such
as ?bathroom? and ?location?, while deceptive re-
views talk about general concepts such as why or
with whom they went to the hotel. However, a
hotel can instead solicit fake reviews from their
employees or customers who possess substantial
domain knowledge to write fake reviews and en-
code more spatial details in their lies. Indeed,
cases have been reported where hotel owners bribe
guests in return for good reviews on TripAdvi-
sor
4
, or companies ordered employees to pretend
they were satisfied customers and write glowing
reviews of its face-lift procedure on Web sites.
5
The domain knowledge possessed by domain ex-
perts enables them to craft reviews that are much
more difficult for classifiers to detect, compared to
the crowdsourced fake reviews.
Additionally, existing supervised algorithms in
the literature are usually narrowed to one spe-
cific domain and heavily rely on domain-specific
vocabulary. For example, classifiers assign high
weights to domain-specific terms such as ?hotel?,
?rooms?, or even the name of the hotels such as
?Hilton? when trained on reviews on hotels. It
is unclear whether these classifiers will perform
well at detecting deception in other domains, e.g.,
Restaurant or Doctor reviews. Even in a single do-
main, e.g., Hotel, classifiers trained from reviews
of one city (e.g., Chicago) may not be effective if
directly applied to reviews from other cities (e.g.,
New York City) (Li et al, 2013b). In the exam-
ples in Table 1, we trained a linear SVM clas-
sifier on Ott?s Chicago-hotel dataset on unigram
features and tested it on a couple of different do-
mains (the details of data acquisition are illustrated
in Section 3). Good performance is obtained on
Chicago-hotel reviews (Ott et al, 2011), but not as
good on New York City ones. The performance is
reasonable in Restaurant reviews due to the many
shared properties among restaurants and hotels,
but suffers in Doctor settings.
In this paper, we try to obtain a deeper under-
standing of the general nature of deceptive opin-
ion spam. One contribution of the work presented
here is the creation of the cross-domain (i.e., Ho-
tel, Restaurant and Doctor) gold-standard dataset.
4
http://www.dailymail.co.uk/travel/article-
2013391/Tripadvisor-Hotel-owners-bribe-guests-return-
good-reviews.html
5
http://www.nytimes.com/2009/07/15/
technology/internet/15lift.html?_r=0
Accuracy Precision Recall F1
NYC-Hotel 0.799 0.794 0.758 0.766
Chicago-Restaurant 0.785 0.813 0.742 0.778
Doctor 0.550 0.537 0.725 0.617
Table 1: SVM performance on datasets for a clas-
sifier trained on Chicago hotel review based on
Unigram feature.
In contrast to existing work (Ott et al, 2011; Li et
al., 2013b), our new gold standard includes three
types of reviews: domain expert deceptive opinion
spam (Employee), crowdsourced deceptive opin-
ion spam (Turker), and truthful Customer reviews
(Customer). In addition, some of domains contain
both positive (P) and negative (N) reviews.
6
To explore the general rule of deceptive opinion
spam, we extended SAGE Model (Eisenstein et
al., 2011), a bayesian generative approach that can
capture the multiple generative facets (i.e., decep-
tive vs truthful, positive vs negative, experienced
vs non-experienced, hotel vs restaurant vs doctor)
in the text collection. We find that more general
features, such as LIWC and POS, are more robust
when modeled using SAGE, compared with just
bag-of-words.
We additionally make theoretical contributions
that may shed light on a longstanding debate in the
literature about deception. For example, in con-
trast to existing findings that highlight the lack of
spatial detail in deceptive reviews (Ott et al, 2011;
Li et al, 2013b), we find that a lack of spatial de-
tail may not be a universal cue to deception, since
it does not apply to fake reviews written by domain
experts. Instead, our finding suggest that other lin-
guistic features may offer more robust cues to de-
ceptive opinion spam, such as overly highlighted
sentiment in the review or the overuse of first-
person singular pronouns.
The rest of this paper is organized as follows.
In Section 2, we briefly go over related work. We
describe the creation of our data set in Section 3
and present our model in Section 4. Experimental
results are shown in Section 5. We present anal-
ysis of general cues to deception in Section 6 and
conclude this paper in Section 7.
6
For example, a hotel manager could hire people to write
positive reviews to increase the reputation of his own hotel
or post negative ones to degrade his competitors. Identify-
ing positive/negative opinion spam is explored in (Ott et al,
2011; Ott et al, 2013)
1567
2 Related Work
Spam has been historically studied in the contexts
of Web text (Gy?ongyi et al, 2004; Ntoulas et al,
2006) or email (Drucker et al, 1999). Recently
there has been increasing concern about deceptive
opinion spam (Jindal and Liu, 2008; Ott et al,
2011; Wu et al, 2010; Mukherjee et al, 2013b;
Wang et al, 2012).
Jindal and Liu (2008) first studied the deceptive
opinion problem and trained models using features
based on the review text, reviewer, and product
to identify duplicate opinions, i.e., opinions that
appear more than once in the corpus with simi-
lar contexts. Wu et al (2010) propose an alter-
native strategy to detect deceptive opinion spam
in the absence of a gold standard. Yoo and Gret-
zel (2009) gathered 40 truthful and 42 deceptive
hotel reviews and manually compare the linguis-
tic differences between them. Ott et al created
a gold-standard collection by employing Turkers
to write fake reviews, and follow-up research was
based on their data (Ott et al, 2012; Ott et al,
2013; Li et al, 2013b; Feng and Hirst, 2013). For
example, Song et al (2012) looked into syntactic
features from Context Free Grammar parse trees
to improve the classifier performance. A step fur-
ther, Feng and Hirst (2013) make use of degree
of compatibility between the personal experiment
and a collection of reference reviews about the
same product rather than simple textual features.
In addition to exploring text or linguistic fea-
tures in deception, some existing work looks
into customers? behavior to identify deception
(Mukherjee et al, 2013a). For example, Mukher-
jee et al (2011; 2012) delved into group behavior
to identify group of reviewers who work collabo-
ratively to write fake reviews. Qian and Liu (2013)
identified multiple user IDs that are generated by
the same author, as these authors are more likely
to generate deceptive reviews.
In the psychological literature, researchers have
looked into possible linguistic cues to deception
(Newman et al, 2003), such as decreased spatial
detail, which is consistent with theories of reality
monitoring (Johnson and Raye, 1981), increased
negative emotion terms (Newman et al, 2003), or
the writing style difference between informative
(truthful) and imaginative (deceptive) writings in
(Rayson et al, 2001). The former typically con-
sists of more nouns, adjectives, prepositions, de-
terminers, and coordinating conjunctions, while
the latter consists of more verbs, adverbs, pro-
nouns, and pre-determiners.
SAGE (Sparse Additive Generative Model):
SAGE is an generative bayesian approach in-
troduced by Eisenstein et al (2011), which
can be viewed as an combination of topic mod-
els (Blei et al, 2003) and generalized additive
models (Hastie and Tibshirani, 1990). Unlike
other derivatives of topic models, SAGE drops
the Dirichlet-multinomial assumption and adopts
a Laplacian prior, triggering sparsity in topic-word
distribution. The reason why SAGE is tailored for
our task is that SAGE constructs multi-faceted la-
tent variable models by simply adding together the
component vectors rather than incorporating mul-
tiple switching latent variables in multiple facets.
3 Dataset Construction
In this section, we report our efforts to gather gold-
standard opinion spam datasets. Our datasets con-
tain the following domains, namely Hotel, Restau-
rant, and Doctor.
3.1 Turker set, using Mechanical Turk
Crowdsourcing services such as AMT greatly fa-
cilitate large-scale data annotation and collection
efforts. Anyone with basic programming skills can
create Human Intelligence Tasks (HITs) and ac-
cess a marketplace of anonymous online workers
(Turkers) willing to complete the tasks. We bor-
rowed some rules used by Ott et al to create their
dataset, such as restricting task to Turkers located
in the United States, and who maintain an approval
rating of at least 90%.
Hotel-Turker : We directly borrowed datasets
from Ott
7
and Li.
8
Restaurant-Turker : We gathered 20 positive
(P) deceptive reviews for each of 10 of the most
popular restaurants in Chicago, for a total of 200
positive deceptive restaurant reviews.
Doctor-Turker : We gathered a total number of
200 positive reviews from Turkers.
3.2 Employee set, by domain experts
We seek deceptive opinion spam written by people
with expert-level domain knowledge. It is not ap-
propriate to use crowdsourcing to obtain this data,
7
http://myleott.com/op_spam/
8
http://www.cs.cmu.edu/
?
jiweil/html/
four_city.html
1568
Turker Expert Customer
Hotel (P/N) 400/400 140/140 400/400
Restaurant (P/N) 200/0 120/0 200/200
Doctor (P/N) 200/0 32/0 200/0
Table 2: Statistics for our dataset.
so instead we solicit reviews written by employees
in each domain.
Hotel-Employee: We asked two hotel employ-
ees from each of seven hotels (14 employees to-
tal) each to write 10 deceptive positive-sentiment
reviews of their own hotel, and 10 deceptive
negative-sentiment reviews of their biggest local
competitor?s hotel. In total, we obtained 280 de-
ceptive reviews of 14 hotels, including a balanced
mix of positive- and negative-sentiment reviews.
Restaurant-Employee: We asked employees
from selected restaurants (a waiter/waitress or
cook) to each write positive-sentiment reviews of
their restaurant.
Doctor-Employee: We asked real doctors to
write positive fake reviews about themselves. In
total we obtained 32 reviews from 15 doctors.
3.3 Customer set from Actual Customers
Hotel-Customer: We borrowed from Ott et al?s
dataset.
Restaurant/Doctor-Customer: We solicited
data by matching a set of truthful reviews as Ott
et al did in collecting truthful hotel reviews.
3.4 Summary for Data Creation
Statistics for our data set is presented in Table 2.
Due to the difficulty in obtaining gold-standard
data in the literature, there is no doubt that our data
set is not perfect. Some parts are missing, some
are unbalanced, participants in the survey may not
be representative of the general population. How-
ever, as far as we know, this is the most compre-
hensive dataset for deceptive opinion spam so far,
and may to some extent shed insights on the nature
of online deception.
4 Feature-based Additive Model
In this section, we briefly describe our model.
Since mathematics are not the main theme of this
paper, we omit the exact details for inference,
which can be found in (Eisenstein et al, 2011).
Before describing the model in detail, we note
the following advantages of the SAGE model, and
our reasons for using it in this paper:
1. the ?additive? nature of SAGE allows a better
understanding of which features contribute
most to each type of deceptive review and
how much each such feature contributes to
the final decision jointly. If we instead use
SVM, for example, we would have to train
classifiers one by one (due to the distinct fea-
tures from different sources) to draw con-
clusions regarding the differences between
Turker vs Expert vs truthful reviews, positive
expert vs negative expert reviews, or reviews
from different domains. This would not only
become intractable, but would make the con-
clusions less clear.
2. For cross-domain classification task, standard
machine learning approaches may suffer due
to domain-specific properties (See Section
5.2).
4.1 Model
In SAGE, each termw is drawn from a distribution
proportional to exp(m
(w)
+ ?
(T )(w)
y
d
+ ?
(A)(w)
z
n
+
?
(I)(w)
y
d
,z
n
), where m
(w)
is the observed background
term frequency, ?
y
d
, ?
z
n
and ?
y
d
,z
n
denote the log
frequency deviation representing topic z
n
, facet
y
d
, and the second-order interaction part respec-
tively. Superscripts T ,A and I respectively denote
the index of the topic, facet, and second-order in-
teraction. In our task, we adapt the SAGE model
as follows:
Y = {y
Sentiment
? {positive, negative},
y
Domain
? {hotel, restaurant, doctor},
y
Source
? {employee, turker, customer}}
We model three ??s, one for each type of y. Let
i, j, k denote the index of the different types of y,
so that each term w is drawn as follows:
P (w|i, j, k) ? exp(m
(w)
+ ?
(i)(w)
y
Sentiment
+?
(j)(w)
y
Domain
+ ?
(k)(w)
y
Scource
+ higher order)
where the higher order parts denote the interac-
tions between different facets.
In our approach each document-level feature f
is drawn from the following distribution:
P (f |i, j, k) ? exp(m
(f)
+ ?
(i)(f)
y
Sentiment
+ ?
(j)(f)
y
Domain
+ ?
(k)(f)
y
Scource
+ higher order)
(1)
1569
where m
(f)
can be interpreted as the background
value of feature f . For each review d, the proba-
bility that it is drawn from facets with index i, j, k
is as follows:
P (d|i, j, k) =
?
f?d
P (f |i, j, k)
?
w?d
P (w|i, j, k) (2)
In the training process, parameters ?
(w)
y
and ?
(f)
y
are to be learned by maximizing the posterior
distribution following the original SAGE training
procedure. For prediction, we estimate y
Source
for
each document given all or part of ?
(w)
y
and ?
(f)
y
as follows:
y
Source
=
argmax
y
?
Source
P (d|y
?
Source
, y
Sentiment
, y
Domain
),
where we assume y
Sentiment
and y
Domain
are
given for each document d. Note that we as-
sume conditional independence between features
and words given y, similar to other topic mod-
els (Blei et al, 2003). Notably, our revised SAGE
model degenerates into a model similar to Gen-
eralized Additive Model (Hastie and Tibshirani,
1990) when word features are not considered.
5 Experiments
In this section, we report our experimental results.
We first restrict experiments to the within-domain
task and see what features most characterize the
deceptive reviews, and how. We later extend it to
cross domains to explore a more general classifier
of deceptive opinion spam.
5.1 Intra-Domain Classification
We explore the effect of both domain experts
and crowdsourcing workers on intra-domain de-
ception. Specifically, we reframe it as a intra-
domain multi-class classification task, where
given the labeled training data from one domain,
we learn a classifier to classify reviews accord-
ing to their source, i.e., Employee, Turker and
Customer. Since the machine learning classi-
fier is trained and tested within the same domain,
?
(j)(w)
y
Domain
and ?
(i)(f)
y
Domain
are not considered here.
We use a One-Versus-Rest (OvR) scheme, in
which we train m classifiers using SAGE, such
that each classifier f
i
, for i ? [1,m], is trained to
distinguish between class i on the one hand, and
all classes except i on the other. To make an m-
way decision, we then choose the class c with the
most confident prediction. OvR approaches have
been shown to produce state-of-art performance
compared to other multi-class approaches such as
Multinomial Naive Bayes or One-Versus-One clas-
sification scheme. We train the OvR classifier on
three sets of features, LIWC, Unigram, and POS.
9
Multi-class classification results are given at Ta-
ble 3. We report both OvR performance and the
performance of three One-versus-One binary clas-
sifiers, trained to distinguish between each pair
of classes. In particular, the three-class classifier
is around 65% accurate at distinguishing between
Employee, Customer, and Turker for each of the
domains using Unigram, significantly higher than
random guess. We also observe that each of the
three One-versus-One binary classifications per-
forms significantly better than chance, suggesting
that Employee, Customer, and Turker are in fact
three different classes. In particular, the two-class
classifier is around 0.76 accurate in distinguish-
ing between Turker and Employee reviews, de-
spite both kinds of reviews being deceptive opin-
ion spam.
Best performance is achieved on Unigram fea-
tures, constantly outperforming LIWC and POS
features in both three-class and two-class settings
in the hotel domain. Similar results are observed
for restaurant and doctor domains and details are
excluded for brevity. This suggests that a universal
set of keyword-based deception cues (e.g., LIWC)
is not the best approach for Intra-Domain Classifi-
cation. Similar results were also reported in previ-
ous work (Ott et al, 2012; Ott, 2013).
5.2 Cross-domain Classification
In this subsection, we frame our problem as a
domain adaptation task (Pan and Yang, 2010).
Again, we explore 3 feature sets: LIWC, Uni-
gram and POS. We train a classifier on hotel re-
views, and evaluate the performance on other do-
mains. For simplicity, we focus on truthful (Cus-
tomer) versus deceptive (Turker) binary classifi-
cation rather than a multi-class classification.
We report results from SAGE and SVM
10
in Ta-
ble 4. We first observe that classifiers trained on
hotel reviews apply well in the restaurant domain,
which is reasonable due to the many shared prop-
9
Part-of-speech tags were assigned based on Stan-
ford Parser http://nlp.stanford.edu/software/
lex-parser.shtml
10
We use SVMlight (Joachims, 1999) to train our linear
SVM classifiers
1570
Domain Setting Features
Customer Employee Turker
A P R P R P R
Hotel
Three-Class
Unigram 0.664 0.678 0.669 0.589 0.610 0.641 0.582
LIWC 0.602 0.617 0.613 0.541 0.598 0.590 0.511
POS 0.517 0.532 0.669 0.481 0.479 0.482 0.416
Customer vs Turker
Unigram 0.818 0.812 0.840 - - 0.820 0.809
LIWC 0.764 0.774 0.771 - - 0.723 0.749
POS 0.729 0.748 0.692 - - 0.707 0.759
Customer vs Employee
Unigram 0.799 0.832 0.784 0.804 0.820 - -
LIWC 0.732 0.746 0.751 0.714 0.722 - -
POS 0.728 0.713 0.742 0.707 0.754 - -
Employee vs Turker
Unigram 0.762 - - 0.786 0.806 0.826 0.794
LIWC 0.720 - - 0.728 0.726 0.698 0.739
POS 0.701 - - 0.688 0.710 0.701 0.697
Restaurant
Three-Class
Unigram
0.647 0.692 0.725 0.625 0.648 0.686 0.702
Customer vs Turker 0.817 0.842 0.816 - - 0.804 0.812
Customer vs Employee 0.785 0.790 0.814 0.769 0.826 - -
Employee vs Turker 0.774 - - 0.784 0.804 0.802 0.763
Doctor Customer vs Turker 0.745 0.772 0.701 - - 0.752 0.718
Table 3: Within-domain multi-class classifier performance.
Model Features Domain A P R F1 Domain A P R F1
SVM
Unigram Restaurant 0.785 0.813 0.742 0.778 Doctor 0.550 0.537 0.725 0.617
LIWC Restaurant 0.745 0.692 0.840 0.759 Doctor 0.521 0.512 0.965 0.669
POS Restaurant 0.735 0.697 0.815 0.751 Doctor 0.540 0.521 0.975 0.679
SAGE
Unigram Restaurant 0.770 0.793 0.750 0.784 Doctor 0.520 0.547 0.705 0.616
LIWC Restaurant 0.742 0.728 0.749 0.738 Doctor 0.647 0.650 0.608 0.628
POS Restaurant 0.746 0.732 0.687 0.701 Doctor 0.634 0.623 0.682 0.651
Table 4: Classifier performance in cross-domain adaptation.
erties among restaurants and hotels. Among three
types of features, Unigram still performs best.
POS and LIWC features are also robust across do-
mains.
In the doctor domain, we observe that models
trained on Unigram features from the hotels do-
main do not generalize well to doctor reviews, and
the performance is a little bit better than random
guess with only 0.55 accuracy. For SVM, models
trained on POS and LIWC features achieve even
lower accuracy than Unigram. POS and LIWC
features obtain around 0.5 precision and 1.0 re-
call, indicating that all doctor reviews are classi-
fied as deceptive by the classifier. One plausible
explanation could be doctor reviews generally en-
code some type of positive-weighted (deceptive)
features more than hotel reviews and these types
of features dominate the decision making proce-
dures, leading all reviews to be classified as de-
ceptive.
Tables 5 and 6 give the top weighted LIWC and
POS features. We observe that many features are
indeed shared among doctor and hotel domains.
Notably, POS features are more robust than LIWC
as more shared features are observed. As domain
specific properties will be considered in the in-
teraction part (?
LIWC
domain
and ?
POS
domain
) of the addi-
LIWC (hotel) LIWC (doctor)
deceptive truthful deceptive truthful
i AllPct Sixletters present
family number past AllPct
pronoun hear work social
Sixletters we health shehe
see space i number
posemo dash friend time
certain human posemo we
leisure exclusive feel you
future past perceptual negemo
perceptual home leisure Period
feel otherpunct insight relativ
comma negemo comma ingest
cause dash future money
Table 5: Top weighted LIWC features for Turker
vs Customer in Doctor and Hotel reviews. Blue
denotes shared positive (deceptive) features and
red denotes negative (truthful) features.
tive model, SAGE achieve much better results than
SVM, and is around 0.65 accurate in the cross-
domain task.
6 General Linguistic Cues of Deceptive
Opinion Spam
In this section, we examine a number of general
POS and LIWC features that may shed light on
a general rule for identifying deceptive opinion
1571
Figure 1: Visualization of the ? for POS features: Horizontal axes correspond to the values ? and are
NORMALIZED from the log-frequency function.
POS (hotel) POS (doctor)
deceptive truthful deceptive truthful
PRP$ CD VBD CD
PRP RRB NNP VBZ
VB LRB VB VBP
TO CC TO FW
NNP NNS VBG RRB
VBG RP PRP$ LRB
MD VBN JJS RB
VBP IN JJ LS
RB EX WRB PDT
JJS VBZ PRP VBN
Table 6: Top weighted POS features for Turker vs
Customer in Doctor and Hotel reviews. Blue de-
notes shared positive (deceptive) features and red
denotes negative (truthful) features.
spam. Our modified SAGE model provides us
with a tailored tool for this analysis. Specifically,
each feature f is associated with a background
valuem
f
. For each facetA, ?
f
A
, presents the facet-
specific preference value for feature f . Note that
sentiments are separated into positive and negative
dimensions, which is necessary because hotel em-
ployee authors wrote positive-sentiment reviews
when reviewing their own hotels, and negative-
sentiment reviews when reviewing their competi-
tors? hotels.
6.1 POS features
Early findings in the literature (Rayson et al,
2001; Buller and Burgoon, 1996; Biber et al,
1999) found that informative (truthful) writings
typically consist of more nouns, adjectives, prepo-
sitions, determiners, and coordinating conjunc-
tions, while imaginative (deceptive) writing con-
sist of more verbs, adverbs, pronouns, and pre-
determiners (with a few exceptions). Our find-
ings with POS features are largely in agreement
with these findings when distinguishing between
Turker and Customer reviews, but are violated in
the Employee set.
We present the eight types of POS features in
Figure 1, namely, N (Noun), JJ (Adjective), IN
(Preposition or subordinating conjunction) and DT
(Determiner), V (Verb), RB (Adverb), PRP (Pro-
nouns, both personal and possessive) and PDT
(Pre-Determiner).
From Figures 1(a)(b)(e)(f), we observe that with
the exception of PDT, the word frequency of
which is too small to draw a conclusion, Turker
and Customer reviews exhibit linguistic patterns in
agreement with previous findings in the literature,
where truthful reviews (Customer) tend to include
more N, JJ, IN and DT, while deceptive writings
tend to encode more V, RB and PRP.
However, in the case of the Employee-Positive
dataset, which is equally deceptive, most of these
rules are violated. Notably, reviews from the
Employee-Positive set did not encode fewer N, JJ
and DT terms, as expected (see Figures 1(a)(c)).
Instead, they encode even more N, JJ and DT
vocabularies than truthful reviews from the Cus-
tomer reviews. Also, fewer V and RB are found
in Employee-Positive reviews compared with Cus-
tomer reviews (see Figures 1(e)(g)).
One explanation for these observations is that
informative (truthful) writing tends to be more in-
troductory and descriptive, encoding more con-
crete details, when compared with imaginary writ-
ings. As domain experts possess considerable
knowledge of their own offerings, they highlight
1572
Figure 2: Visualization of the ? for LIWC features: Horizontal axes correspond to the values ? and are
normalized from the log-frequency function.
the details and their lies may be even more in-
formative and descriptive than those generated by
real customers! This explains why Employee-
Positive contains more N, IN and DT. Meanwhile,
as domain experts are engaged more in talking
about the details, they inevitably overlook other
information, possibly leading to fewer V and RB.
For Employee-Positive reviews, shown in Fig-
ures 1(d)(h), it turns out that domain experts do
not compensate for their lack of prior experience
when writing negative reviews for competitors? of-
ferings, as we will see again with LIWC features
in the next subsection.
6.2 LIWC features
We explore 3 LIWC categories (from left to right
in subfigures of Figure 2): sentiment (neg emo and
pos emo), spatial detail (space), and first-person
singular pronouns (first-person).
Space: Note that spatial details are more spe-
cific in the Hotel and Restaurant domains,
which is reflected in the high positive value of
?
Hotel,space
domain
(see Figure 2(g)) and negative value
of ?
Doctor,space
domain
(see Figure 2(h)). It illustrates how
domain-specific details can be predictive of decep-
tive text. Similarly predictive LIWC features are
home for the Hotel domain, ingest for the Restau-
rant domain, and health and body for the Doctor
domain.
In Figure 2(i)(j)(k)(l), we can easily see that
both actual customers and domain experts encode
more spatial details in their reviews (positive value
of ?), which is in agreement with our expectation.
This further demonstrates that a lack of spatial de-
tails would not be a general cue for deception.
Moreover, it appears that general domain expertise
does not compensate for the lack of prior experi-
ence when writing deceptive negative reviews for
competitors? hotels, as demonstrated by the lack
of spatial details in the negative-sentiment reviews
by employees shown in Figure 2(k).
Sentiment: According to our findings, the pres-
ence of sentiment is a general cue to deceptive
opinion spam, as observed when comparing Fig-
ure 2(b) to Figure 2(c) and (d). Participants, both
Employees and Turkers, tend to exaggerate senti-
ment, and include more sentiment-related vocabu-
laries in their lies. In other words, positive decep-
tive reviews were generally more positive and neg-
ative deceptive reviews were more negative in sen-
timent when compared with the truthful reviews
generated by actual customers. A similar pattern
can also be observed when comparing Figure 2(i)
to Figure 2(j).
1573
First-Person Singular Pronouns: The litera-
ture also associates deception with decreased us-
age of first-person singular pronouns, an effect at-
tributed to psychological distancing, whereby de-
ceivers talk less about themselves due either to a
lack of personal experience, or to detach them-
selves from the lie (Newman et al, 2003; Zhou
et al, 2004; Buller et al, 1996; Knapp and Co-
maden, 1979). However, according to our find-
ings, we find the opposite to hold. Increased first
person singular is an apparent indicator of decep-
tion, when comparing Figure 2(b) to 2(c) and 2(e).
We suspect that this relates to an effect observed
in previous studies of deception, where liars inad-
vertently undermine their lies by overemphasizing
aspects of their deception that they believe reflect
credibility (Bond and DePaulo, 2006; DePaulo et
al., 2003). One interpretation for this phenomenon
would be that deceivers try to overemphasize their
physical presence because they believe that this in-
creases their credibility.
7 Conclusion and Discussion
In this work, we have developed a multi-domain
large-scale dataset containing gold-standard de-
ceptive opinion spam. It includes reviews of Ho-
tels, Restaurants and Doctors, generated through
crowdsourcing and domain experts. We study this
data using SAGE, which enables us to make ob-
servations about the respects in which truthful and
deceptive text differs. Our model includes sev-
eral domain-independent features that shed light
on these differences, which further allows us to
formulate some general rules for recognizing de-
ceptive opinion spam.
We also acknowledge several important caveats
to this work. By soliciting fake reviews from par-
ticipants, including crowd workers and domain
experts, we have found that is possible to de-
tect fake reviews with above-chance accuracy, and
have used our models to explore several psycho-
logical theories of deception. However, it is still
very difficult to estimate the practical impact of
such methods, as it is very challenging to obtain
gold-standard data in the real world. Moreover,
by soliciting deceptive opinion spam in an arti-
ficial environment, we are endorsing the decep-
tion, which may influence the cues that we ob-
serve (Feeley and others, 1998; Frank and Ekman,
1997; Newman et al, 2003; Ott, 2013). Finally, it
may be possible to train people to tell more con-
vincing lies. Many of the characteristics regard-
ing fake review generation might be overcome by
well-trained fake review writers, which would re-
sults in opinion spam that is harder for detect. Fu-
ture work may wish to consider some of these ad-
ditional challenges.
8 Acknowledgement
We thank Wenjie Li and Xun Wang for useful dis-
cussions and suggestions. This work was sup-
ported in part by National Science Foundation
Grant BCS-0904822, a DARPA Deft grant, as well
as a gift from Google. We also thank the ACL re-
viewers for their helpful comments and advice.
References
Douglas Biber, Stig Johansson, Geoffrey Leech, Su-
san Conrad, Edward Finegan, and Randolph Quirk.
1999. Longman grammar of spoken and written En-
glish, volume 2. MIT Press.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. the Journal of machine
Learning research, 3:993?1022.
Charles Bond and Bella DePaulo. 2006. Accuracy of
deception judgments. Personality and Social Psy-
chology Review, 10(3):214?234.
David B Buller and Judee K Burgoon. 1996. Inter-
personal deception theory. Communication theory,
6(3):203?242.
David B Buller, Judee K Burgoon, Aileen Buslig, and
James Roiger. 1996. Testing interpersonal decep-
tion theory: The language of interpersonal decep-
tion. Communication theory, 6(3):268?289.
Paul-Alexandru Chirita, J?org Diederich, and Wolfgang
Nejdl. 2005. Mailrank: using ranking for spam
detection. In Proceedings of the 14th ACM inter-
national conference on Information and knowledge
management, pages 373?380. ACM.
Cone. 2011. 2011 Online Influence Trend Tracker.
http://www.coneinc.com/negative-reviews-online-
reverse-purchase-decisions, August.
Bella DePaulo, James Lindsay, Brian Malone, Laura
Muhlenbruck, Kelly Charlton, and Harris Cooper.
2003. Cues to deception. Psychological bulletin,
129(1):74.
Harris Drucker, Donghui Wu, and Vladimir Vapnik.
1999. Support vector machines for spam catego-
rization. Neural Networks, IEEE Transactions on,
10(5):1048?1054.
1574
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 1041?1048.
Thomas Feeley. 1998. The behavioral correlates of
sanctioned and unsanctioned deceptive communica-
tion. Journal of Nonverbal Behavior, 22(3):189?
204.
Vanessa Feng and Graeme Hirst. 2013. Detecting de-
ceptive opinions with profile compatibility. In Pro-
ceedings of the 6th International Joint Conference
on Natural Language Processing, Nagoya, Japan,
pages 14?18.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2, pages 171?175. Association for
Computational Linguistics.
Mark Frank and Paul Ekman. 1997. The ability to de-
tect deceit generalizes across different types of high-
stake lies. Journal of personality and social psychol-
ogy, 72(6):1429.
Zolt?an Gy?ongyi, Hector Garcia-Molina, and Jan Ped-
ersen. 2004. Combating web spam with trustrank.
In Proceedings of the Thirtieth international con-
ference on Very large data bases-Volume 30, pages
576?587. VLDB Endowment.
Trevor J Hastie and Robert J Tibshirani. 1990. Gener-
alized additive models, volume 43. CRC Press.
Ipsos. 2012. Socialogue: Five Stars? Thumbs Up? A+
or Just Average? http://www.ipsos-na.com/news-
polls/pressrelease.aspx?id=5929.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of the international con-
ference on Web search and web data mining, pages
219?230. ACM.
Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010. Find-
ing unusual review patterns using unexpected rules.
In Proceedings of the 19th ACM international con-
ference on Information and knowledge management,
pages 1549?1552. ACM.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Marcia K Johnson and Carol L Raye. 1981. Reality
monitoring. Psychological review, 88(1):67.
Mark Knapp and Mark Comaden. 1979. Telling it like
it isn?t: A review of theory and research on decep-
tive communications. Human Communication Re-
search, 5(3):270?285.
Jiwei Li, Claire Cardie, and Sujian Li. 2013a. Top-
icspam: a topic-model-based approach for spam de-
tection. In Proceedings of the 51th Annual Meeting
of the Association for Computational Linguis-tics.
Jiwei Li, Myle Ott, and Claire Cardie. 2013b. Iden-
tifying manipulated offerings on review portals. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, Seattle,
Wash, pages 18?21.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. 2010. Detecting prod-
uct review spammers using rating behaviors. In Pro-
ceedings of the 19th ACM international conference
on Information and knowledge management, pages
939?948. ACM.
Juan Martinez-Romo and Lourdes Araujo. 2009. Web
spam identification through language model analy-
sis. In Proceedings of the 5th International Work-
shop on Adversarial Information Retrieval on the
Web, pages 21?28. ACM.
David Meyer. 2009. Fake reviews prompt belkin apol-
ogy.
Claire Miller. 2009. Company settles case of reviews
it faked. New York Times.
Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie
Glance, and Nitin Jindal. 2011. Detecting group
review spam. In Proceedings of the 20th interna-
tional conference companion on World wide web,
pages 93?94. ACM.
Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012.
Spotting fake reviewer groups in consumer reviews.
In Proceedings of the 21st international conference
on World Wide Web, pages 191?200. ACM.
Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui
Wang, Meichun Hsu, Malu Castellanos, and Riddhi-
man Ghosh. 2013a. Spotting opinion spammers us-
ing behavioral footprints. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 632?640.
ACM.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Natalie Glance. 2013b. What yelp fake review fil-
ter might be doing. In Seventh International AAAI
Conference on Weblogs and Social Media.
Matthew L Newman, James W Pennebaker, Diane S
Berry, and Jane M Richards. 2003. Lying words:
Predicting deception from linguistic styles. Person-
ality and social psychology bulletin, 29(5):665?675.
Alexandros Ntoulas, Marc Najork, Mark Manasse, and
Dennis Fetterly. 2006. Detecting spam web pages
through content analysis. In Proceedings of the 15th
international conference on World Wide Web, pages
83?92. ACM.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 309?319.
1575
Myle Ott, Claire Cardie, and Jeff Hancock. 2012. Esti-
mating the prevalence of deception in online review
communities. In Proceedings of the 21st interna-
tional conference on World Wide Web, pages 201?
210. ACM.
Myle Ott, Claire Cardie, and Jeffrey T. Hancock. 2013.
Negative deceptive opinion spam. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Short Papers, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Myle Ott. 2013. Computational lingustic models of
deceptive opinion spam. PHD, thesis.
Sinno Pan and Qiang Yang. 2010. A survey on transfer
learning. Knowledge and Data Engineering, IEEE
Transactions on, 22(10):1345?1359.
Tieyun Qian and Bing Liu. 2013. Identifying multiple
userids of the same author. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, Seattle, Wash, pages 18?21.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within
the british national corpus sampler. Language and
Computers, 36(1):295?306.
David Streitfeld. 2012. For 2 a star, an online retailer
gets 5-star product reviews. New York Times., 26.
Alexandra Topping. 2010. Historian orlando figes
agrees to pay damages for fake reviews. The
Guardian., 16.
Guan Wang, Sihong Xie, Bing Liu, and Philip Yu.
2011. Review graph based online store review
spammer detection. In Data Mining (ICDM),
2011 IEEE 11th International Conference on, pages
1242?1247. IEEE.
Guan Wang, Sihong Xie, Bing Liu, and Philip Yu.
2012. Identify online store review spammers via so-
cial review graph. ACM Transactions on Intelligent
Systems and Technology (TIST), 3(4):61.
Guangyu Wu, Derek Greene, Barry Smyth, and P?adraig
Cunningham. 2010. Distortion as a validation cri-
terion in the identification of suspicious reviews. In
Proceedings of the First Workshop on Social Media
Analytics, pages 10?13. ACM.
Kyung-Hyan Yoo and Ulrike Gretzel. 2009. Com-
parison of deceptive and truthful travel reviews.
In Information and communication technologies in
tourism 2009, pages 37?47. Springer.
Lina Zhou, Judee K Burgoon, Douglas P Twitchell,
Tiantian Qin, and Jay F Nunamaker Jr. 2004. A
comparison of classification methods for predict-
ing deception in computer-mediated communica-
tion. Journal of Management Information Systems,
20(4):139?166.
1576
Transactions of the Association for Computational Linguistics, 1 (2013) 89?98. Action Editor: Noah Smith.
Submitted 12/2012; Published 5/2013. c?2013 Association for Computational Linguistics.
A Novel Feature-based Bayesian Model for Query Focused Multi-document
Summarization
Jiwei Li
School of Computer Science
Carnegie Mellon University
bdlijiwei@gmail.com
Sujian Li
Laboratory of Computational Linguistics
Peking University
lisujian@pku.edu.cn
Abstract
Supervised learning methods and LDA based topic
model have been successfully applied in the field of
multi-document summarization. In this paper, we
propose a novel supervised approach that can in-
corporate rich sentence features into Bayesian topic
models in a principled way, thus taking advantages of
both topic model and feature based supervised learn-
ing methods. Experimental results on DUC2007,
TAC2008 and TAC2009 demonstrate the effective-
ness of our approach.
1 Introduction
Query-focused multi-document summarization
(Nenkova et al, 2006; Wan et al, 2007; Ouyang et
al., 2010) can facilitate users to grasp the main idea
of documents. In query-focused summarization, a
specific topic description, such as a query, which
expresses the most important topic information is
proposed before the document collection, and a
summary would be generated according to the given
topic.
Supervised models have been widely used in sum-
marization (Li, et al, 2009, Shen et al, 2007,
Ouyang et al, 2010). Supervised models usually re-
gard summarization as a classification or regression
problem and use various sentence features to build a
classifier based on labeled negative or positive sam-
ples. However, existing supervised approaches sel-
dom exploit the intrinsic structure among sentences.
This disadvantage usually gives rise to serious prob-
lems such as unbalance and low recall in summaries.
Recently, LDA-based (Blei et al, 2003) Bayesian
topic models have widely been applied in multi-
document summarization in that Bayesian ap-
proaches can offer clear and rigorous probabilis-
tic interpretations for summaries(Daume and Marcu,
2006; Haghighi and Vanderwende, 2009; Jin et al,
2010; Mason and Charniak, 2011; Delort and Alfon-
seca, 2012). Exiting Bayesian approaches label sen-
tences or words with topics and sentences which are
closely related with query or can highly generalize
documents are selected into summaries. However,
LDA topic model suffers from the intrinsic disad-
vantages that it only uses word frequency for topic
modeling and can not use useful text features such as
position, word order etc (Zhu and Xing, 2010). For
example, the first sentence in a document may be
more important for summary since it is more likely
to give a global generalization about the document.
It is hard for LDA model to consider such informa-
tion, making useful information lost.
It naturally comes to our minds that we can im-
prove summarization performance by making full
use of both useful text features and the latent seman-
tic structures from by LDA topic model. One related
work is from Celikyilmaz and Hakkani-Tur (2010).
They built a hierarchical topic model called Hybh-
sum based on LDA for topic discovery and assumed
this model can produce appropriate scores for sen-
tence evaluation. Then the scores are used for tun-
ing the weights of various features that helpful for
summary generation. Their work made a good step
of combining topic model with feature based super-
vised learning. However, what their approach con-
fuses us is that whether a topic model only based
on word frequency is good enough to generate an
appropriate sentence score for regression. Actually,
how to incorporate features into LDA topic model
has been a open problem. Supervised topic models
such as sLDA(Blei and MacAuliffe 2007) give us
some inspiration. In sLDA, each document is asso-
ciated with a labeled feature and sLDA can integrate
such feature into LDA for topic modeling in a prin-
89
cipled way.
With reference to the work of supervised LDA
models, in this paper, we propose a novel sentence
feature based Bayesian model S-sLDA for multi-
document summarization. Our approach can natu-
rally combine feature based supervised methods and
topic models. The most important and challeng-
ing problem in our model is the tuning of feature
weights. To solve this problem, we transform the
problem of finding optimum feature weights into an
optimization algorithm and learn these weights in
a supervised way. A set of experiments are con-
ducted based on the benchmark data of DUC2007,
TAC2008 and TAC2009, and experimental results
show the effectiveness of our model.
The rest of the paper is organized as follows. Sec-
tion 2 describes some background and related works.
Section 3 describes our details of S-sLDA model.
Section 4 demonstrates details of our approaches,
including learning, inference and summary gener-
ation. Section 5 provides experiments results and
Section 6 concludes the paper.
2 Related Work
A variety of approaches have been proposed
for query-focused multi-document summarizations
such as unsupervised (semi-supervised) approaches,
supervised approaches, and Bayesian approaches.
Unsupervised (semi-supervised) approaches such
as Lexrank (Erkan and Radex, 2004), manifold
(Wan et al, 2007) treat summarization as a graph-
based ranking problem. The relatedness between
the query and each sentence is achieved by impos-
ing querys influence on each sentence along with
the propagation of graph. Most supervised ap-
proaches regard summarization task as a sentence
level two class classification problem. Supervised
machine learning methods such as Support Vector
Machine(SVM) (Li, et al, 2009), Maximum En-
tropy (Osborne, 2002) , Conditional Random Field
(Shen et al, 2007) and regression models (Ouyang
et al, 2010) have been adopted to leverage the rich
sentence features for summarization.
Recently, Bayesian topic models have shown their
power in summarization for its clear probabilistic
interpretation. Daume and Marcu (2006) proposed
Bayesum model for sentence extraction based on
query expansion concept in information retrieval.
Haghighi and Vanderwende (2009) proposed topic-
sum and hiersum which use a LDA-like topic model
and assign each sentence a distribution over back-
ground topic, doc-specific topic and content topics.
Celikyilmaz and Hakkani-Tur (2010) made a good
step in combining topic model with supervised fea-
ture based regression for sentence scoring in sum-
marization. In their model, the score of training
sentences are firstly got through a novel hierarchi-
cal topic model. Then a featured based support vec-
tor regression (SVR) is used for sentence score pre-
diction. The problem of Celikyilmaz and Hakkani-
Turs model is that topic model and feature based re-
gression are two separate processes and the score of
training sentences may be biased because their topic
model only consider word frequency and fail to con-
sider other important features. Supervised feature
based topic models have been proposed in recent
years to incorporate different kinds of features into
LDA model. Blei (2007) proposed sLDA for doc-
ument response pairs and Daniel et al (2009) pro-
posed Labeled LDA by defining a one to one corre-
spondence between latent topic and user tags. Zhu
and Xing (2010) proposed conditional topic random
field (CTRF) which addresses feature and indepen-
dent limitation in LDA.
3 Model description
3.1 LDA and sLDA
The hierarchical Bayesian LDA (Blei et al, 2003)
models the probability of a corpus on hidden topics
as shown in Figure 1(a). Let K be the number of
topics , M be the number of documents in the cor-
pus and V be vocabulary size. The topic distribution
of each document ?m is drawn from a prior Dirichlet
distribution Dir(?), and each document word wmn
is sampled from a topic-word distribution ?z spec-
ified by a drawn from the topic-document distribu-
tion ?m. ? is a K ?M dimensional matrix and each
?k is a distribution over the V terms. The generat-
ing procedure of LDA is illustrated in Figure 2. ?m
is a mixture proportion over topics of document m
and zmn is a K dimensional variable that presents
the topic assignment distribution of different words.
Supervised LDA (sLDA) (Blei and McAuliffe
2007) is a document feature based model and intro-
90
Figure 1: Graphical models for (a) LDA model and (b)
sLDA model.
1. Draw a document proportion vector ?m|? ? Dir(?)
2. For each word in m
(a)draw topic assignment zmn|? ?Multi(?zmn)
(b)draw word wmn|zmn, ? ?Multi(?zmn)
Figure 2: Generation process for LDA
duces a response variable to each document for topic
discovering, as shown in Figure 1(b). In the gener-
ative procedure of sLDA, the document pairwise la-
bel is draw from y|??zm, ?, ?2 ? p(y|??zm, ?, ?2), where??zm = 1N
?N
n=1 zm,n.
3.2 Problem Formulation
Here we firstly give a standard formulation of the
task. Let K be the number of topics, V be the vo-
cabulary size and M be the number of documents.
Each document Dm is represented with a collection
of sentence Dm = {Ss}s=Nms=1 where Nm denotes
the number of sentences in mth document. Each
sentence is represented with a collection of words
{wmsn}n=Nmsn=1 where Nms denotes the number of
words in current sentence. ???Yms denotes the feature
vector of current sentence and we assume that these
features are independent.
3.3 S-sLDA
zms is the hidden variable indicating the topic of
current sentence. In S-sLDA, we make an assump-
tion that words in the same sentence are generated
from the same topic which was proposed by Gruber
(2007). zmsn denotes the topic assignment of cur-
rent word. According to our assumption, zmsn =
Figure 3: Graph model for S-sLDA model
1. Draw a document proportion vector ?m|? ? Dir(?)
2. For each sentence in m
(a)draw topic assignment zms|? ?Multi(?zmn)
(b)draw feature vector ???Yms|zms, ? ? p(???Yms|zms, ?)
(c)for each word wmsn in current sentence
draw wmsn|zms, ? ?Multi(?zms)
Figure 4: generation process for S-sLDA
zms for any n ? [1, Nms]. The generative approach
of S-sLDA is shown in Figure 3 and Figure 4. We
can see that the generative process involves not only
the words within current sentence, but also a series
of sentence features. The mixture weights over fea-
tures in S-sLDA are defined with a generalized lin-
ear model (GLM).
p(???Yms|zms, ?) =
exp(zTms?)
???Yms?
zms exp(zTms?)
???Yms
(1)
Here we assume that each sentence has T features
and ???Yms is a T ? 1 dimensional vector. ? is a
K ? T weight matrix of each feature upon topics,
which largely controls the feature generation proce-
dure. Unlike s-LDA where ? is a latent variable esti-
mated from the maximum likelihood estimation al-
gorithm, in S-sLDA the value of ? is trained through
a supervised algorithm which will be illustrated in
detail in Section 3.
3.4 Posterior Inference and Estimation
Given a document and labels for each sentence, the
posterior distribution of the latent variables is:
p(?, z1:N |w1:N , Y, ?, ?1:K , ?) =
?
m p(?m|?)
?
s[p(zms|?m)p(
???Yms|zms, ?)
?
n p(wmsn|zmsn, ?zmsn ]?
d?p(?m|?)
?
z
?
s[p(zms|?m)p(
???Yms|zms, ?)
?
n p(wmsn|?zmsn)](2)
Eqn. (2) cannot be efficiently computed. By
applying the Jensens inequality, we obtain a
lower bound of the log likelihood of document
p(?, z1:N |w1:N ,
???Yms, ?, ?1:K , ?) ? L, where
L =
?
ms
E[logP (zms|?)] +
?
ms
E[logP (???Yms|zms, ?)]+
?
m
E[logP (?|?)] +
?
msn
E[logP (wmsn|zms, ?)] +H(q)
(3)
91
where H(q) = ?E[logq] and it is the entropy of
variational distribution q is defined as
q(?, z|?, ?) =
?
mk
q(?m|?)
?
sn
q(zmsn|?ms) (4)
here ? a K-dimensional Dirichlet parameter vector
and multinomial parameters. The first, third and
forth terms of Eqn. (3) are identical to the corre-
sponding terms for unsupervised LDA (Blei et al,
2003). The second term is the expectation of log
probability of features given the latent topic assign-
ments.
E[logP (???Yms|zms, ?)] =
E(zms)T ?
???Yms ? log
?
zms
exp(zTms?
???Yms)
(5)
where E(zms)T is a 1 ? K dimensional vector
[?msk]k=Kk=1 . The Bayes estimation for S-sLDA
model can be got via a variational EM algorithm. In
EM procedure, the lower bound is firstly minimized
with respect to ? and ?, and then minimized with ?
and ? by fixing ? and ?.
E-step:
The updating of Dirichlet parameter ? is identical
to that of unsupervised LDA, and does not involve
feature vector ???Yms.
?newm ? ?+
?
s?m
?s (6)
?newsk ? exp{E[log?m|?] +
Nms?
n=1
E[log(wmsn|?1:K)]+
T?
t=1
?ktYst} = exp[?(?mk)??(
K?
k=1
?mk) +
T?
t=1
?ktYst]
(7)
where ?(?) denotes the log ? function. ms denotes
the document that current sentence comes from and
Yst denotes the tth feature of sentence s.
M-step:
The M-step for updating ? is the same as the pro-
cedure in unsupervised LDA, where the probability
of a word generated from a topic is proportional to
the number of times this word assigned to the topic.
?newkw =
M?
m=1
Nm?
s=1
Nms?
n=1
1(wmsn = w)?kms (8)
4 Our Approach
4.1 Learning
In this subsection, we describe how we learn the fea-
ture weight ? in a supervised way. The learning pro-
cess of ? is a supervised algorithm combined with
variational inference of S-sLDA. Given a topic de-
scription Q1 and a collection of training sentences S
from related documents, human assessors assign a
score v(v = ?2,?1, 0, 1, 1) to each sentence in S.
The score is an integer between?2 (the least desired
summary sentences) and +2 (the most desired sum-
mary sentences), and score 0 denotes neutral atti-
tude. Ov = {ov1, ov2, ..., vvk}(v = ?2,?1, 0, 1, 2)
is the set containing sentences with score v. Let ?Qk
denote the probability that query is generated from
topic k. Since query does not belong to any docu-
ment, we use the following strategy to leverage ?Qk
?Qk =
?
w?Q
?kw?
1
M
M?
m=1
exp[?(?mk)??(
K?
k=1
?mk)]
(9)
In Equ.(9), ?w?Q ?kw denotes the probability that
all terms in query are generated from topic k
and 1M
?M
m=1 exp[?(?mk)??(
?K
k=1 ?mk)] can be
seen as the average probability that all documents in
the corpus are talking about topic k. Eqn. (9) is
based on the assumption that query topic is relevant
to the main topic discussed by the document corpus.
This is a reasonable assumption and most previous
LDA summarization models are based on similar as-
sumptions.
Next, we define ?Ov ,k for sentence set Ov, which
can be interpreted as the probability that all sen-
tences in collection Ov are generated from topic k.
?Ov,k =
1
|Ov|
?
s?Ov
?sk, k ? [1,K], v ? [?2, 2] (10)
|Ov| denotes the number of sentences in set Ov. In-
spired by the idea that desired summary sentences
would be more semantically related with the query,
we transform problem of finding optimum ? to the
following optimization problem:
min?L(?) =
v=2?
v=?2
v ?KL(Ov||Q);
T?
t=1
?kt = 1 (11)
1We select multiple queries and their related sentences for
training
92
where KL(Ov||Q) is the Kullback-Leibler diver-
gence between the topic and sentence set Ov as
shown in Eqn.(12).
KL(Ov||Q) =
K?
k=1
?Ovklog
?Ovk
?Qk
(12)
In Eqn. (11), we can see that O2, which contain de-
sirable sentences, would be given the largest penalty
for its KL divergence from Query. The case is just
opposite for undesired set.
Our idea is to incorporate the minimization pro-
cess of Eqn.(11) into variational inference process
of S-sLDA model. Here we perform gradient based
optimization method to minimize Eqn.(11). Firstly,
we derive the gradient of L(?) with respect to ?.
?L(?)
?xy
=
v=2?
v=?2
v ? ?KL(Qv||Q)??xy
(13)
?KL(Qv||Q)
??xy
=
K?
k=1
1
|Qv|
(1 + log
?
s?Qv
|Qv|
)
?
s?Qv
??sk
??xy
?
K?
k=1
1
|Qv|
?
s?Qv
?Qsk
?xy
?
K?
k=1
1
Qv
?
s?Qv?sk
?Qk
??sk
??xy
(14)
For simplification, we regard ? and ? as constant
during updating process of ?, so ??Qk??xy = 0.2 We canfurther get first derivative for each labeled sentence.
??sk
?xy
?
?
???????
???????
Ysyexp[?(?msi)??(
K?
k=1
?msk) +
T?
t=1
?ktYsy]
?
?
w?s
?kw if k = x
0 if k 6= x
(15)
4.2 Feature Space
Lots of features have been proven to be useful for
summarization (Louis et al, 2010). Here we dis-
cuss several types of features which are adopted in
S-sLDA model. The feature values are either binary
or normalized to the interval [0,1]. The following
features are used in S-sLDA:
Cosine Similarity with query: Cosine similarity is
based on the tf-idf value of terms.
2This is reasonable because the influence of ? and ? have
been embodied in ? during each iteration.
Local Inner-document Degree Order: Local Inner
document Degree Order is a binary feature which
indicates whether Inner-document Degree (IDD) of
sentence s is the largest among its neighbors. IDD
means the edge number between s and other sen-
tences in the same document.
Document Specific Word: 1 if a sentence contains
document specific word, 0 otherwise.
Average Unigram Probability (Nenkova and Van-
derwende, 2005; Celikyilmaz and Hakkani-Tur
2010): As for sentence s, p(s) = ?w?s 1|s|pD(w),
where pD(w) is the observed unigram probability in
document collection.
In addition, we also use the commonly used fea-
tures including sentence position, paragraph po-
sition, sentence length and sentence bigram fre-
quency.
E-step
initialize ?0sk := 1/K for all i and s.
initialize ?mi := ?mi +N)m/K for all i.
initialize ?kt = 0 for all k and t.
while not convergence
for m = 1 : M
update ?t+1m according to Eqn.(6)
for s = 1 : Nm
for k = 1 : K
update ?t+1sk according to Eqn.(7)
normalize the sum of ?t+1sk to 1.Minimize L(?) according to Eqn.(11)-(15).
M-step:
update ? according to Eqn.(8)
Figure 5: Learning process of ? in S-sLDA
4.3 Sentence Selection Strategy
Next we explain our sentence selection strategy. Ac-
cording to our intuition that the desired summary
should have a small KL divergence with query, we
propose a function to score a set of sentences Sum.
We use a decreasing logistic function ?(x) = 1/(1+
ex) to refine the score to the range of (0,1).
Score(Sum) = ?(KL(sum||Q)) (16)
Let Sum? denote the optimum update summary. We
can get Sum? by maximizing the scoring function.
Sum? = arg max
Sum?S&&words(Sum)?L
Score(Sum)
(17)
93
1. Learning: Given labeled set Ov, learn the feature
weight vector ? using algorithm in Figure 5.
2. Given new data set and ?, use algorithm in section
3.3 for inference. (The only difference between
this step and step (1) is that in this step we do not
need minimize L(?).
3. Select sentences for summarization from algo-
rithm in Figure 6.
Figure 6: Summarization Generation by S-sLDA.
A greedy algorithm is applied by adding sentence
one by one to obtain Sum?. We use G to denote
the sentence set containing selected sentences. The
algorithm first initializes G to ? and X to SU . Dur-
ing each iteration, we select one sentence from X
which maximize Score(sm ?G). To avoid topic re-
dundancy in the summary, we also revise the MMR
strategy (Goldstein et al, 1999; Ouyang et al, 2007)
in the process of sentence selection. For each sm,
we compute the semantic similarity between sm and
each sentence st in set Y in Eqn.(18).
cos?sem(sm, st) =
?
k ?smk?stk??
k ?2smk
??
k ?2stk
(18)
We need to assure that the value of semantic similar-
ity between two sentences is less than Thsem. The
whole procedure for summarization using S-sLDA
model is illustrated in Figure 6. Thsem is set to 0.5
in the experiments.
5 Experiments
5.1 Experiments Set-up
The query-focused multi-document summarization
task defined in DUC3(Document Understanding
Conference) and TAC4(Text Analysis Conference)
evaluations requires generating a concise and well
organized summary for a collection of related news
documents according to a given query which de-
scribes the users information need. The query
usually consists of a title and one or more narra-
tive/question sentences. The system-generated sum-
maries for DUC and TAC are respectively limited to
3http://duc.nist.gov/.
4http://www.nist.gov/tac/.
250 words and 100 words. Our experiment data is
composed of DUC 2007, TAC5 2008 and TAC 2009
data which have 45, 48 and 44 collections respec-
tively. In our experiments, DUC 2007 data is used
as training data and TAC (2008-2009) data is used
as the test data.
Stop-words in both documents and queries are
removed using a stop-word list of 598 words, and
the remaining words are stemmed by Porter Stem-
mer6. As for the automatic evaluation of summa-
rization, ROUGE (Recall-Oriented Understudy for
Gisting Evaluation) measures, including ROUGE-
1, ROUGE-2, and ROUGE-SU47 and their corre-
sponding 95% confidence intervals, are used to eval-
uate the performance of the summaries. In order to
obtain a more comprehensive measure of summary
quality, we also conduct manual evaluation on TAC
data with reference to (Haghighi and Vanderwende,
2009; Celikyilmaz and Hakkani-Tur, 2011; Delort
and Alfonseca, 2011).
5.2 Comparison with other Bayesian models
In this subsection, we compare our model with the
following Bayesian baselines:
KL-sum: It is developed by Haghighi and
Vanderwende (Lin et al, 2006) by using a KL-
divergence based sentence selection strategy.
KL(Ps||Qd) =
?
w
P (w)logP (w)Q(w) (19)
where Ps is the unigram distribution of candidate
summary andQd denotes the unigram distribution of
document collection. Sentences with higher ranking
score is selected into the summary.
HierSum: A LDA based approach proposed by
Haghighi and Vanderwende (2009), where unigram
distribution is calculated from LDA topic model in
Equ.(14).
Hybhsum: A supervised approach developed by
Celikyilmaz and Hakkani-Tur (2010).
For fair comparison, baselines use the same pro-
precessing methods with our model and all sum-
5Here, we only use the docset-A data in TAC, since TAC
data is composed of docset-A and docset-B data, and the docset-
B data is mainly for the update summarization task.
6http://tartarus.org/ martin/PorterStemmer/.
7Jackknife scoring for ROUGE is used in order to compare
with the human summaries.
94
maries are truncated to the same length of 100
words. From Table 1 and Table 2, we can
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3724 0.1030 0.1342
approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394)
Hybhsum 0.3703 0.1007 0.1314(0.3600-0.3806) (0.0952-0.1059) (0.1241-0.1387)
HierSum 0.3613 0.0948 0.1278(0.3374-0.3752) (0.0899-0.0998) (0.1197-0.1359)
KLsum 0.3504 0.0917 0.1234(0.3411-0.3597) (0.0842-0.0992) (0.1155-0.1315)
StandLDA 0.3368 0.0797 0.1156(0.3252-0.3386) (0.0758-0.0836) (0.1072-0.1240)
Table 1: Comparison of Bayesian models on TAC2008
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3903 0.1223 0.1488
approach (0.3819-0.3987) (0.1167-0.1279) (0.1446-0.1530)
Hybhsum 0.3824 0.1173 0.1436(0.3686-0.3952) (0.1132-0.1214) (0.1358-0.1514)
HierSum 0.3706 0.1088 0.1386(0.3624-0.3788) (0.0950-0.1144) (0.1312-0.1464)
KLsum 0.3619 0.0972 0.1299(0.3510-0.3728) (0.0917-0.1047) (0.1213-0.1385)
StandLDA 0.3552 0.0847 0.1214(0.3447-0.3657) (0.0813-0.0881) (0.1141-0.1286)
Table 2: Comparison of Bayesian models on TAC2009
see that among all the Bayesian baselines, Hybh-
sum achieves the best result. This further illus-
trates the advantages of combining topic model with
supervised method. In Table 1, we can see that
our S-sLDA model performs better than Hybhsum
and the improvements are 3.4% and 3.7% with re-
spect to ROUGE-2 and ROUGE-SU4 on TAC2008
data. The comparison can be extended to TAC2009
data as shown in Table 2: the performance of S-
sLDA is above Hybhsum by 4.3% in ROUGE-2
and 5.1% in ROUGE-SU4. It is worth explaining
that these achievements are significant, because in
the TAC2008 evaluation, the performance of the top
ranking systems are very close, i.e. the best system
is only 4.2% above the 4th best system on ROUGE-
2 and 1.2% on ROUGE-SU4.
5.3 Comparison with other baselines.
In this subsection, we compare our model with some
widely used models in summarization.
Manifold: It is the one-layer graph based semi-
supervised summarization approach developed by
Wan et al(2008). The graph is constructed only con-
sidering sentence relations using tf-idf and neglects
topic information.
LexRank: Graph based summarization approach
(Erkan and Radev, 2004), which is a revised version
of famous web ranking algorithm PageRank. It is
an unsupervised ranking algorithms compared with
Manifold.
SVM: A supervised method - Support Vector Ma-
chine (SVM) (Vapnik 1995) which uses the same
features as our approach.
MEAD: A centroid based summary algorithm by
Radev et al (2004). Cluster centroids in MEAD
consists of words which are central not only to one
article in a cluster, but to all the articles. Similarity
is measure using tf-idf.
At the same time, we also present the top three
participating systems with regard to ROUGE-2 on
TAC2008 and TAC2009 for comparison, denoted as
(denoted as SysRank 1st, 2nd and 3rd)(Gillick et al,
2008; Zhang et al, 2008; Gillick et al, 2009; Varma
et al, 2009). The ROUGE scores of the top TAC
system are directly provided by the TAC evaluation.
From Table 3 and Table 4, we can see that
our approach outperforms the baselines in terms of
ROUGE metrics consistently. When compared with
the standard supervised method SVM, the relative
improvements over the ROUGE-1, ROUGE-2 and
ROUGE-SU4 scores are 4.3%, 13.1%, 8.3% respec-
tively on TAC2008 and 7.2%, 14.9%, 14.3% on
TAC2009. Our model is not as good as top par-
ticipating systems on TAC2008 and TAC2009. But
considering the fact that our model neither uses sen-
tence compression algorithm nor leverage domain
knowledge bases like Wikipedia or training data,
such small difference in ROUGE scores is reason-
able.
5.4 Manual Evaluations
In order to obtain a more accurate measure of sum-
mary quality for our S-sLDA model and Hybhsum,
we performed a simple user study concerning the
following aspects: (1) Overall quality: Which sum-
mary is better overall? (2) Focus: Which summary
contains less irrelevant content? (3)Responsiveness:
Which summary is more responsive to the query.
(4) Non-Redundancy: Which summary is less re-
dundant? 8 judges who specialize in NLP partic-
ipated in the blind evaluation task. Evaluators are
presented with two summaries generated by S-sLDA
95
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3724 0.1030 0.1342
approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394)
SysRank 1st 0.3742 0.1039 0.1364(0.3639-0.3845) (0.0974-0.1104) (0.1285-0.1443)
SysRank 2nd 0.3717 0.0990 0.1326(0.3610-0.3824 (0.0944-0.1038) (0.1269-0.1385)
SysRank 3rd 0.3710 0.0977 0.1329(0.3550-0.3849) (0.0920-0.1034) (0.1267-0.1391)
PageRank 0.3597 0.0879 0.1221(0.3499-0.3695) (0.0809-0.0950) (0.1173-0.1269)
Manifold 0.3621 0.0931 0.1243(0.3506-0.3736) (0.0868-0.0994) (0.1206-0.1280)
SVM 0.3588 0.0921 0.1258(0.3489-0.3687) (0.0882-0.0960) (0.1204-0.1302)
MEAD 0.3558 0.0917 0.1226(0.3489-0.3627) (0.0882-0.0952) (0.1174-0.1278)
Table 3: Comparison with baselines on TAC2008
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3903 0.1223 0.1488
approach (0.3819-0.3987) (0.1167-0.1279) (0.1446-0.1530)
SysRank 1st 0.3917 0.1218 0.1505(0.3778-0.4057) (0.1122-0.1314) (0.1414-0.1596)
SysRank 2nd 0.3914 0.1212 0.1513(0.3808-0.4020) (0.1147-0.1277) (0.1455-0.1571)
SysRank 3rd 0.3851 0.1084 0.1447(0.3762-0.3932) (0.1025-0.1144) (0.1398-0.1496)
PageRank 0.3616 0.0849 0.1249(0.3532-0.3700) (0.0802-0.0896) (0.1221-0.1277)
Manifold 0.3713 0.1014 0.1342(0.3586-0.3841) (0.0950-0.1178) (0.1299-0.1385)
SVM 0.3649 0.1028 0.1319(0.3536-0.3762) (0.0957-0.1099) (0.1258-0.1380)
MEAD 0.3601 0.1001 0.1287(0.3536-0.3666) (0.0953-0.1049) (0.1228-0.1346)
Table 4: Comparison with baselines on TAC2009
and Hybhsum, as well as the four questions above.
Then they need to answer which summary is better
(tie). We randomly select 20 document collections
from TAC 2008 data and randomly assign two sum-
maries for each collection to three different evalua-
tors to judge which model is better in each aspect.
As we can see from Table 5, the two models al-
most tie with respect to Non-redundancy, mainly
because both models have used appropriate MMR
strategies. But as for Overall quality, Focus and
Our(win) Hybhsum(win) Tie
Overall 37 14 9
Focus 32 18 10
Responsiveness 33 13 14
Non-redundancy 13 11 36
Table 5: Comparison with baselines on TAC2009
Responsiveness, S-sLDA model outputs Hybhsum
based on t-test on 95% confidence level. Ta-
ble 6 shows the example summaries generated re-
spectively by two models for document collection
D0803A-A in TAC2008, whose query is ?Describe
the coal mine accidents in China and actions taken?.
From table 6, we can see that each sentence in these
two summaries is somewhat related to topics of coal
mines in China. We also observe that the summary
in Table 6(a) is better than that in Table 6(b), tend-
ing to select shorter sentences and provide more in-
formation. This is because, in S-sLDA model, topic
modeling is determined simultaneously by various
features including terms and other ones such as sen-
tence length, sentence position and so on, which
can contribute to summary quality. As we can see,
in Table 6(b), sentences (3) and (5) provide some
unimportant information such as ?somebody said?,
though they contain some words which are related
to topics about coal mines.
(1)China to close at least 4,000 coal mines this year:
official (2)By Oct. 10 this year there had been 43 coal
mine accidents that killed 10 or more people, (3)Offi-
cials had stakes in coal mines. (4)All the coal mines
will be closed down this year. (5) In the first eight
months, the death toll of coal mine accidents rose
8.5 percent last year. (6) The government has issued
a series of regulations and measures to improve the
coun.try?s coal mine safety situation. (7)The mining
safety technology and equipments have been sold to
countries. (8)More than 6,000 miners died in accidents
in China
(1) In the first eight months, the death toll of coal mine
accidents across China rose 8.5 percent from the same
period last year. (2)China will close down a number of
ill-operated coal mines at the end of this month, said
a work safety official here Monday. (3) Li Yizhong,
director of the National Bureau of Production Safety
Supervision and Administration, has said the collusion
between mine owners and officials is to be condemned.
(4)from January to September this year, 4,228 people
were killed in 2,337 coal mine accidents. (5) Chen
said officials who refused to register their stakes in
coal mines within the required time
Table 6: Example summary text generated by systems
(a)S-sLDA and (b) Hybhsum. (D0803A-A, TAC2008)
96
6 Conclusion
In this paper, we propose a novel supervised ap-
proach based on revised supervised topic model for
query-focused multi document summarization. Our
approach naturally combines Bayesian topic model
with supervised method and enjoy the advantages of
both models. Experiments on benchmark demon-
strate good performance of our model.
Acknowledgments
This research work has been supported by
NSFC grants (No.90920011 and No.61273278),
National Key Technology R&D Program
(No:2011BAH1B0403), and National High Tech-
nology R&D Program (No.2012AA011101). We
also thank the three anonymous reviewers for their
helpful comments. Corresponding author: Sujian
Li.
References
David Blei and Jon McAuliffe. Supervised topic models.
2007. In Neural Information Processing Systems
David Blei, Andrew Ng and Micheal Jordan. Latent
dirichlet alocation. In The Journal of Machine Learn-
ing Research, page: 993-1022.
Charles Broyden. 1965. A class of methods for solv-
ing nonlinear simultaneous equations. In Math. Comp.
volume 19, page 577-593.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A Hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics. page:
815-825
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal and
Jaime Carbonell. 1999. Summarizing Text Docu-
ments: Sentence Selection and Evaluation Metrics. In
Proceedings of the 22nd annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, page: 121-128.
Amit Grubber, Micheal Rosen-zvi and Yair Weiss. 2007.
Hidden Topic Markov Model. In Artificial Intelligence
and Statistics.
Hal Daume and Daniel Marcu H. 2006. Bayesian Query-
Focused Summarization. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, page 305-312.
Gune Erkan and Dragomir Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summariza-
tion. In J. Artif. Intell. Res. (JAIR), page 457-479.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, The ICSI
Summarization System at TAC, TAC 2008.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur,
Berndt Bohnet, Yang Liu, Shasha Xie. The ICSI/UTD
Summarization System at TAC 2009. TAC 2009
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362370.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. The
summarization systems at tac 2010. In Proceedings of
the third Text Analysis Conference, TAC-2010.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha and
Yong Yu. 2009. Enhancing diversity, coverage and bal-
ance for summarization through structure learning. In
Proceedings of the 18th international conference on
World wide web, page 71-80.
Chin-Yew Lin, Guihong Gao, Jianfeng Gao and Jian-Yun
Nie. 2006. An information-theoretic approach to au-
tomatic evaluation of summaries. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, page:462-470.
Annie Louis, Aravind Joshi, Ani Nenkova. 2010. Dis-
course indicators for content selection in summariza-
tion. In Proceedings of the 11th Annual Meeting of
the Special Interest Group on Discourse and Dialogue,
page:147-156.
Tengfei Ma, Xiaojun Wan. 2010. Multi-document sum-
marization using minimum distortion, in Proceedings
of International Conference of Data Mining. page
354363.
Rebecca Mason and Eugene Charniak. 2011. Extractive
multi-document summaries should explicitly not con-
tain document-specific content. In proceedings of ACL
HLT, page:49-54.
Ani Nenkova and Lucy Vanderwende. The impact of fre-
quency on summarization. In Tech. Report MSR-TR-
2005-101, Microsoft Research, Redwood, Washing-
ton, 2005.
Ani Nenkova, Lucy Vanderwende and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that inu-
ence summarization. In Proceedings of the 29th an-
nual International ACM SIGIR Conference on Re-
97
search and Development in Information Retrieval,
page 573-580.
Miles Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Work-
shop on Automatic Summarization, Volume 4 page:1-
8.
Jahna Otterbacher, Gunes Erkan and Dragomir Radev.
2005. Using random walks for question-focused sen-
tence retrieval. In Proceedings of the Conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, page 915-922
You Ouyang, Wenjie Li, Sujian Li and Qin Lua. 2011.
Applying regression models to query-focused multi-
document summarization. In Information Processing
and Management, page 227-237.
You Ouyang, Sujian. Li, and Wenjie. Li. 2007, Develop-
ing learning strategies for topic-based summarization.
In Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, page: 7986.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher Manning. 2009. Labeled LDA: A super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Vol 1, page 248-256.
Dou She, Jian-Tao Sun, Hua Li, Qiang Yang and
Zheng Chen. 2007. Document summarization using
conditional random elds. In Proceedings of Inter-
national Joint Conference on Artificial Intelligence,
page: 28622867.
V. Varma, V. Bharat, S. Kovelamudi, P. Bysani, S. GSK,
K. Kumar N, K. Reddy, N. Maganti , IIIT Hyderabad
at TAC 2009. TAC2009
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
Summarization using cluster-based link analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, page: 299-306.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2007.
Manifold-ranking based topic-focused multi-
document summarization. In Proceedings of In-
ternational Joint Conference on Artificial Intelligence,
page 2903-2908.
Furu Wei, Wenjie Li, Qin Lu and Yanxiang He. 2008. Ex-
ploiting Query-Sensitive Similarity for Graph-Based
Query-Oriented Summarization. In Proceedings of the
31st annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
page 283-290.
Jin Zhang, Xueqi Cheng, Hongbo Xu, Xiaolei Wang, Yil-
ing Zeng. ICTCAS?s ICTGrasper at TAC 2008: Sum-
marizing Dynamic Information with Signature Terms
Based Content Filtering, TAC 2008.
Dengzhong Zhou, Jason Weston, Arthur Gretton, Olivier
Bousquet and Bernhard Schlkopf. 2003. Ranking on
Data Manifolds. In Proceedings of the Conference on
Advances in Neural Information Processing Systems,
page 169-176.
Jun Zhu and Eric Xing. 2010. Conditional Topic Random
Fields. In Proceedings of the 27th International Con-
ference on Machine Learning.
Xiaojin Zhu, Zoubin Ghahramani and John Laf-
ferty. 2003. Semi-supervised Learning using Gaussian
Fields and Harmonic Functions. In Proceedings of In-
ternational Conference of Machine Learning, page:
912-919.
98
