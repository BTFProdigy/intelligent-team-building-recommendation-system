Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 345?351,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Tri-Training for Authorship Attribution with Limited Training Data 
 
Tieyun Qian 
State Key Laboratory 
of Software Eng., 
Wuhan University 
430072, Hubei, China 
qty@whu.edu.cn 
Bing Liu 
Dept. of Computer Sci-
ence, Univ. of Illinois at 
Chicago 
IL, USA, 60607 
liub@cs.uic.edu 
Li Chen 
State Key Laboratory of 
Software Eng., 
Wuhan University 
430072, Hubei, China 
ccnuchenli@163.com 
Zhiyong Peng 
Computer School, 
Wuhan University 
430072, Hubei, China 
peng@whu.edu.cn 
 
  
 
Abstract 
Authorship attribution (AA) aims to identify 
the authors of a set of documents. Traditional 
studies in this area often assume that there are 
a large set of labeled documents available for 
training. However, in the real life, it is often 
difficult or expensive to collect a large set of 
labeled data. For example, in the online review 
domain, most reviewers (authors) only write a 
few reviews, which are not enough to serve as 
the training data for accurate classification. In 
this paper, we present a novel three-view tri-
training method to iteratively identify authors 
of unlabeled data to augment the training set. 
The key idea is to first represent each docu-
ment in three distinct views, and then perform 
tri-training to exploit the large amount of un-
labeled documents. Starting from 10 training 
documents per author, we systematically eval-
uate the effectiveness of the proposed tri-
training method for AA. Experimental results 
show that the proposed approach outperforms 
the state-of-the-art semi-supervised method 
CNG+SVM and other baselines.  
1 Introduction 
Existing approaches to authorship attribution 
(AA) are mainly based on supervised classifica-
tion (Stamatatos, 2009, Kim et al, 2011, Serous-
si et al, 2012). Although this is an effective ap-
proach, it has a major weakness, i.e., for each 
author a large number of his/her articles are 
needed as the training data. This is possible if the 
author has written a large number of articles, but 
will be difficult if he/she has not. For example, in 
the online review domain, most authors (review-
ers) only write a few reviews (documents). It was 
shown that on average each reviewer only has 
2.72 reviews in amazon.com, and only 8% of the 
reviewers have at least 5 reviews (Jindal and Liu, 
2008). The small number of labeled documents 
makes it extremely challenging for supervised 
learning to train an accurate classifier. 
In this paper, we consider AA with only a few 
labeled examples. By exploiting the redundancy 
in human languages, we tackle the problem using 
a new three-view tri-training algorithm (TTA). 
Specifically, we first represent each document in 
three distinct views, and then tri-train three clas-
sifiers in these views. The predictions of two 
classifiers on unlabeled examples are used to 
augment the training set for the third classifier. 
This process repeats until a termination condition 
is met. The enlarged labeled sets are finally used 
to train classifiers to classify the test data.  
To our knowledge, no existing work has ad-
dressed AA in a tri-training framework. The AA 
problem with limited training data was attempted 
in (Stamatatos, 2007; Luyckx and Daelemans, 
2008). However, neither of them used a semi-
supervised approach to augment the training set 
with additional documents. Kourtis and Stama-
tatos (2011) introduced a variant of the self-
training method in (Nigam and Ghani, 2000). 
Note that the original self-training uses one clas-
sifier on one view. However, the self-training 
method in (Kourtis and Stamatatos, 2011) uses 
two classifiers (CNG and SVM) on one view. 
Both the self-training and tri-training are semi-
supervised learning methods. However, the pro-
posed approach is not a simple extension of the 
self-training method CNG+SVM of (Kourtis and 
Stamatatos, 2011). There are key differences. 
First, in their experimental setting, about 115 
and 129 documents per author on average are 
used for two experimental corpora. This number 
of labeled documents is still very large. We con-
sider a much more realistic problem, where the 
size of the training set is very small. Only 10 
samples per author are used in training.  
Second, CNG+SVM uses two learning methods 
on a single character n-gram view. In contrast, 
besides the character n-gram view, we also make 
use of the lexical and syntactic views. That is, 
345
three distinct views are used for building classi-
fiers. The redundant information in human lan-
guage is combined in the tri-training procedure.  
Third, in each round of self-training in 
CNG+SVM, each classifier is refined by the same 
newly labeled examples. However, in the pro-
posed tri-training method (TTA), the examples 
labeled by the classifiers of every two views are 
added to the third view. By doing so, each classi-
fier can borrow information from the other two 
views. And the predictions made by two classifi-
ers are more reliable than those by one classifier. 
The main contribution of this paper is thus the 
proposed three-view tri-training scheme which 
has a much better generalization ability by ex-
ploiting three different views of the same docu-
ment. Experimental results on the IMDb review 
dataset show that the proposed method dramati-
cally improves the CNG+SVM method. It also 
outperforms the co-training method (Blum and 
Mitchell, 1998) based on our proposed views.  
2 Related Work 
Existing AA methods either focused on finding 
suitable features or on developing effective 
techniques. Example features include function 
words (Argamon et al, 2007), richness features 
(Gamon 2004), punctuation frequencies (Graham 
et al, 2005), character (Grieve, 2007), word 
(Burrows, 1992) and POS n-grams (Gamon, 
2004; Hirst and Feiguina, 2007), rewrite rules 
(Halteren et al, 1996), and similarities (Qian and 
Liu, 2013). On developing effective learning 
techniques, supervised classification has been the 
dominant approach, e.g., neural networks 
(Graham et al, 2005; Zheng et al, 2006), 
decision tree (Uzuner and Katz, 2005; Zhao and 
Zobel, 2005), logistic regression (Madigan et al, 
2005), SVM (Diederich et al, 2000; Gamon 
2004; Li et al, 2006; Kim et al, 2011), etc. 
The main problem in the traditional research is 
the unrealistic size of the training set. A size of 
about 10,000 words per author is regarded as a 
reasonable training set size (Argamon et al, 
2007, Burrows, 2003). When no long documents 
are available, tens or hundreds of short texts are 
used (Halteren, 2007; Hirst and Feiguina, 2007; 
Schwartz et al, 2013).  
Apart from the existing works dealing with 
limited data discussed in the introduction, our 
preliminary study in (Qian et al, 2014) used one 
learning method on two views, but it is inferior 
to the proposed method in this paper.  
 
3 Proposed Tri-Training Algorithm  
3.1 Overall Framework 
We represent each document in three feature 
views: the character view, the lexical view and 
the syntactic view. Each view consists of a set of 
features in the respective type. A classifier can 
be learned from any of these views. We propose 
a three-view training algorithm to deal with the 
problem of limited training data. Logistic 
regression (LR) is used as the learner. The 
overall framework is shown in Figure 1. 
Given the labeled, unlabeled, and test sets L, 
U, and T, step 1 extracts the character, lexical, 
and syntactic views from L, U, and T, 
respectively. Steps 2-13 iteratively tri-train three 
classifiers by adding the data which are assigned 
the same label by two classifiers into the training 
set of the third classifier. The algorithm first 
randomly selects u unlabeled documents from U 
to create a pool U? of examples. Note that we can 
directly select from the large unlabeled set U. 
However, it is shown in (Blum and Mitchell 
2008) that a smaller pool can force the classifiers 
to select instances that are more representative of 
the underlying distribution that generates U. 
Hence we set the parameter u to a size of about 
1% of the whole unlabeled set, which allows us 
to observe the effects of different number of 
iterations. It then iterates over the following 
steps. First, use character, lexical and syntactic 
views on the current labeled set to train three 
classifiers C1, C2, and C3. See Steps 4-9. Second, 
Input: A small set of labeled documents L = {l1,?, lr}, a large 
set of unlabeled documents U = {u1,?, us}, and a set of test 
documents T = {t1,?, tt}, 
Parameters: the number of iterations k, the size of selected un-
labeled documents u 
Output: tk?s class assignment 
1   Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T 
2  Loop for k iterations: 
3  Randomly select u unlabeled documents U' from U; 
4       Learn the first view classifier C1 from L1 (L1=Lc, Ll, or Ls); 
5        Use C1 to label docs in U' based on U1(U1=Uc, Ul, or Us) 
6        Learn the second view classifier C2 from L2 (L2?L1) 
7        Use C2 to label documents in U' based on U2 (U2?U1); 
8        Learn the third view classifier C3 from L3 (L2?L1, L2) 
9        Use C3 to label documents in U' based on U3 (U2?U1, U2); 
10      Up1 = {u | u? U', u.label by C2 = u.label by C3}; 
11      Up2 = {u | u? U', u.label by C1 = u.label by C3}; 
12      Up3 = {u | u? U', u.label by C1 = u.label by C2}; 
13      U = U - U', Li = Li ? Upi (i=1..3);             
14 Learn three classifiers C1, C2, C3 from L1, L2, L3; 
15 Use Ci to label tk in Ti (i=1..3); 
16  Aggregate results from three views 
Figure 1: The tri-training algorithm (TTA) 
346
allow two of these three classifiers to classify the 
unlabeled set U? and choose p documents with 
agreed labels. See Steps 10-12. The selected 
documents are then added to the third labeled set 
for the label assigned (a label is an author here), 
and the u documents are removed from the 
unlabeled pool U? (line 13). We call this way of 
augmenting the training sets InterAdding. The 
one used in (Kourtis and Stamatatos, 2011) is 
called SelfAdding as it uses only a single view 
and adds to the same training set. Steps 14-15 
assign the test document to a category (author) 
using the classifier learned from the three views 
in the augmented labeled data, respectively. Step 
16 aggregates the results from three classifiers. 
3.2 Character View 
The features in the character view are the 
character n-grams of a document. Character n-
grams are simple and easily available for any 
natural language. For a fair comparison with the 
previous work in (Kourtis and Stamatatos, 2011), 
we extract frequencies of 3-grams at the 
character-level. The vocabulary size for character 
3-grams in our experiment is 28584.  
3.3 Lexical View 
The lexical view consists of word unigrams of a 
document. We represent each article by a vector 
of word frequencies. The vocabulary size for 
unigrams in our experiment is 195274.  
3.4 Syntactic View 
The syntactic view consists of the syntactic 
features of a document. We use four content-
independent structures including n-grams of POS 
tags (n = 1..3) and rewrite rules (Kim et al, 
2011). The vocabulary sizes for POS 1-grams, 
POS 2-grams, POS 3-grams, and rewrite rules in 
our experiment are 63, 1917, 21950, and 19240, 
respectively. These four types of syntactic 
structures are merged into a single vector. Hence 
the syntactic view of a document is represented 
as a vector of 43140 components. 
3.5 Aggregating Results from Three Views 
In testing, once we obtain the prediction values 
from three classifiers for a test document tk, an 
additional algorithm is used to decide the final 
author attribution. One simple method is voting. 
However, this method is weaker than the three 
methods below. It is also hard to compare with 
the self-training method CNG+SVM in (Kourtis 
and Stamatatos, 2011) as it only has two classifi-
ers. Hence we present three other strategies to 
further aggregate the results from the three 
views. These methods require the classifier to 
produce a numeric score to reflect the positive or 
negative certainty. Many classification algo-
rithms give such scores, e.g., SVM and logistic 
regression. The three methods are as follows:  
1)  ScoreSum: The learned model first classifies 
all test cases in T. Then for each test case tk, 
this method sums up all scores of positive 
classifications from the three views. It then 
assigns tk to the author with the highest score. 
2)  ScoreSqSum: This method works similarly to 
ScoreSum above except that it sums up the 
squared scores of positive classifications. 
3)  ScoreMax: This method works similarly to the 
ScoreSum method as well except that it finds 
the maximum classification score for each test 
document. 
4 Experimental Evaluation  
We now evaluate the proposed method. We use 
logistic regression (LR) with L2 regularization 
(Fan et al, 2008) and the SVMmulticlass (SVM) 
system (Joachims, 2007) with its default settings 
as the classifiers.  
4.1 Experiment Setup 
We conduct experiments on the IMDb dataset 
(Seroussi et al, 2010). This data set contains the 
IMDb reviews in May 2009. It has 62,000 re-
views by 62 users (1,000 reviews per user). For 
each author/reviewer, we further split his/her 
documents into the labeled, unlabeled, and test 
sets. 1% of one author?s documents, i.e., 10 doc-
uments per author, are used as the labeled data 
for training, 79% are used as unlabeled data, and 
the rest 20% are used for testing. We extract and 
compute the character and lexical features direct-
ly from the raw data, and use the Stanford PCFG 
parser (Klein and Manning, 2003) to generate the 
grammar structures of sentences in each review 
for extracting syntactic features. We normalize 
each feature?s value to the [0, 1] interval by di-
viding the maximum value of this feature in the 
training set. We use the micro-averaged classifi-
cation accuracy as the evaluation metric. 
4.2 Baseline methods 
We use six self-training baselines and three co-
training baselines. Self-training in (Kourtis and 
Stamatatos, 2011) uses two different classifiers 
on one view, and co-training uses one classifier 
on two views. All baselines except CNG+SVM 
347
on the character view are our extensions. 
Self-training using CNG+SVM on character, 
lexical and syntactic views respectively: This 
gives three baselines. It self-trains two classifi-
ers from the character 3-gram, lexical, and syn-
tactic views using CNG and SVM classifiers 
(Kourtis and Stamatatos, 2011). CNG is a pro-
file-based method which represents the author 
as the N most frequent character n-grams of all 
his/her training texts. The original method ap-
plied only CNG and SVM on the character n-
gram view. Since our results show that its per-
formance is extremely poor, we are curious 
what the reason is. Can this be due to the clas-
sifier or to the view? In order to differentiate 
the effects of views and classifiers, we present 
two additional types of baselines. The first type 
is to extend CNG+SVM method to lexical and 
syntactic views as well. The second type is to 
extend CNG+SVM method by replacing CNG 
with LR to show a fair comparison with our 
framework.  
Self-training using LR+SVM on character, lexi-
cal, and syntactic views: This is the second 
type extension. It also gives us three baselines. 
It again uses the character, lexical and syntac-
tic view and SVM as one of the two classifiers. 
The other classifier uses LR rather than CNG.  
Co-training using LR on Char+Lex, Char+Syn, 
and Lex+Syn views: This also gives us three 
baselines. Each baseline co-trains two classifi-
ers from every two views of the character 3-
gram, lexical, and syntactic views. 
4.3 Results and analysis 
(1) Effects of learning algorithms  
We first evaluate the effects of learning algo-
rithms on tri-training. We use SVM and LR as 
the learners as they are among the best methods.  
Figure 2. Effects of SVM and LR on tri-training 
The effects of SVM and LR on tri-training are 
shown in Fig. 2. For the aggregation results, we 
draw the curves for ScoreSum. The results for 
other two stratigies are similar. It is clear that LR 
outperforms SVM by a large margin for tri-
training when the number of iterations (k) is 
small. One possible reason is that LR is more 
tolerant to over-fitting caused by the small 
number of training samples. Hence, we use LR 
for tri-training in all experiments. 
(2) Effects of aggregation strategies  
We show the effects of the three proposed 
aggregation strategies. Table 1 indicates that 
ScoreSum (SS) is the best.  
Table 1. Effects of three aggregation strategies: 
ScoreMax(SM), ScoreSum(SS), and ScoreSq-Sum(SQ) 
We also observe that both ScoreSum and 
ScoreSqSum (SQ) perform better than ScoreMax 
(SM) and all single view cases. This suggests 
that the decision made from a number of scores 
is much more reliable than that made from only 
one score. ScoreSum is our default strategy. 
(3) Effects of data augmenting strategies  
We now see the effects of data adding methods 
to augment the labeled set in Fig. 3.  
 
Figure 3. Effects of data augmenting methods on 
tri-training 
We use two strategies. One is our InterAdding 
approach and the other is the SelfAdding 
approach in (Kourtis and Stamatatos, 2011), as 
introduced in Section 3.1. We can see that by 
adding newly classified samples by two 
classifiers to the third view, tri-training gets 
better and better results rapidly. For example, the 
accuracy for k = 10 iterations grows from 61.24 
for SelfAdding to 78.82 for InterAdding, an 
absolute increase of 17.58%. This implies that by 
integrating more information from other views, 
learning can improve greatly.  
(4) Comparison with self-training baselines 
We show the results of CNG+SVM in Table 2. It 
is clear that CNG is almost unable to correctly 
k 
Single  View Results Aggregated Results 
Lex Char Syn SM SS SQ 
0 45.75 32.88 33.96 41.11 46.85 44.61 
10 74.63 66.05 56.99 73.41 78.82 76.41 
20 82.30 74.92 65.05 81.63 86.19 84.05 
30 86.86 79.12 68.85 85.29 89.69 87.74 
40 89.16 81.81 70.85 87.83 91.52 89.99 
50 90.56 83.14 72.06 89.11 92.58 91.17 
60 91.69 84.13 73.23 90.05 93.15 91.82 
348
classify any test case. Its accuracy is only 1.26% 
at the start. This directly leads to the failure of 
the self-training. The reason is that the other 
classifier SVM can augment nearly 0 documents 
from the unlabeled set. We also tuned the param-
eter N for CNG, but it makes little difference. 
k 
Self-Training on Char  Aggregated Results 
CNG SVM SM SS SQ 
0 1.26 33.22 32.35 32.47 27.00 
10 1.26 32.35 32.35 32.47 27.00 
20 1.26 32.35 32.35 32.47 27.00 
30 1.26 32.35 32.35 32.47 27.00 
40 1.26 33.60 33.60 33.69 29.07 
50 1.26 33.60 33.60 33.69 29.07 
60 1.27 33.54 33.60 33.69 29.07 
Table 2. Results for the CNG+SVM baseline 
To distinguish the effects of views from classi-
fiers, we conduct two more types of experiments. 
First, we apply CNG+SVM to the lexical and 
syntactic views. The results are even worse. Its 
accuracy drops to 0.58% and 1.21%, respectively. 
Next, we replace CNG with LR and apply 
LR+SVM to all three views. We only show their 
best results in Table 3, either on a single view or 
aggregation. The details are omitted due to space 
limitations. We can see significant improvements 
over their corresponding results of CNG+SVM. 
This demonstrates that the learning methods are 
critical to self-training as well.  
k Tri 
Train 
SelfTrain:CNG+SVM SelfTrain:LR+SVM 
Char lex Syn Char Lex Syn 
0 46.85 33.22 45.44 34.50 33.22 45.75 34.48 
10 78.82 32.47 45.44 34.50 62.56 73.78 51.94 
20 86.19 32.47 45.44 34.09 71.21 81.44 59.88 
30 89.69 32.47 45.44 34.09 75.21 84.68 63.70 
40 91.52 33.69 45.44 34.09 77.46 88.25 65.74 
50 92.58 33.69 45.44 34.09 78.64 88.25 67.45 
60 93.15 33.69 45.44 34.09 79.54 89.31 68.37 
Table 3. Self-training variations 
From Table 3, we can also see that our tri-
training approach outperforms all self-training 
baselines by a large margin. For example, the 
accuracy for LR+SVM on the lexical view is 
89.31%.Although this is the best for self-training, 
it is worse than 93.15% of tri-training.  
The reason that self-training does not work 
well in general is the following: When the train-
ing set is small, the available data may not reflect 
the true distribution of the whole data. Then clas-
sifiers will be biased and their classifications will 
be biased too. In testing, the biased classifiers 
will not have good accuracy. However, in tri-
training, and co-training, each individual view 
may be biased but the views are independent. 
Then each view is more likely to produce ran-
dom samples for the other views and thus reduce 
the bias of each view as the iterations progress.  
(5) Comparison with co-training baselines 
We now compare tri-training with co-training 
(Blum and Mitchell, 1998) in Table 4. Again, tri-
training beats co-training consistently. The best 
performance of co-training is 92.81% achieved 
on the character and lexical views after 60 itera-
tions. However, the accuracy is worse than that 
of tri-training. The key reason is that tri-training 
considers three views, while co-training uses on-
ly two. Also, the predictions by two classifiers 
are more reliable than those by one classifier. 
k Tri 
Train 
Co-Train 
Char+Lex Char+Syn Lex+Syn 
0 46.85 45.75 42.02 45.75 
10 78.82 78.84 75.89 78.85 
20 86.19 86.02 82.59 85.63 
30 89.69 89.32 85.77 88.98 
40 91.52 91.14 87.52 91.16 
50 92.58 92.19 88.46 92.02 
60 93.15 92.81 89.21 92.50 
Table 4. Co-training vs. tri-training 
In (Qian, et al, 2014), we systematically inves-
tigated the effects of learning methods and views 
using a special co-training approach with two 
views. Learning was applied on two views but 
the data augmentation method was like that in 
self-training. The best result there was 91.23%, 
worse than 92.81% of co-training here in Table 4, 
which is worse than 93.15% of Tri-Training.   
Overall, Tri-training performs the best and co-
training is better than self-training and co-self-
training. This indicates that learning on different 
views can better exploit the redundancy in texts 
to achieve superior classification results. 
5 Conclusion  
In this paper, we investigated the problem of au-
thorship attribution with very few labeled exam-
ples. A novel three-view tri-training method was 
proposed to utilize natural views of human lan-
guages, i.e., the character, lexical and syntactic 
views, for classification. We evaluated the pro-
posed method and compared it with state-of-the-
art baselines. Results showed that the proposed 
method outperformed all baseline methods.  
Our future work will extend the work by in-
cluding more views such as the stylistic and vo-
cabulary richness views. Additional experiments 
will also be conducted to determine the general 
behavior of the tri-training approach. 
Acknowledgements 
This work was supported in part by the NSFC 
projects (61272275, 61232002, 61379044), and 
the 111 project (B07037).  
349
References  
S. Argamon, C. Whitelaw,  P. Chase, S. R. Hota,  N. 
Garg, and S. Levitan. 2007. Stylistic text 
classification using functional lexical features. 
JASIST 58, 802?822 
 
A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In: COLT. pp. 
92?100  
 
J. Burrows. 1992. Not unless you ask nicely: The 
interpretative nexus between analysis and 
information. Literary and Linguistic Computing 
7:91-109. 
 
J. Burrows. 2007. All the way through: Testing for 
authorship in different frequency data. LLC 22, 
27?47 
  
R-E. Fan, K-W. Chang,   C-J. Hsieh,  X-R. Wang, and 
C-J. Lin. 2008. Liblinear: A library for large linear 
classification. JMLR 9, 1871?1874 
 
J. Diederich, J. Kindermann, E. Leopold, G. Paass, G. 
F. Informationstechnik, and D-S. Augustin. 2000. 
Authorship attribution with support vector 
machines. Applied Intelligence 19:109-123. 
 
M. Gamon. 2004. Linguistic correlates of style: 
authorship classification with deep linguistic 
analysis features. In COLING. 
 
N. Graham, G. Hirst, and B. Marthi. 2005. 
Segmenting documents by stylistic character. 
Natural Language Engineering, 11:397-415.  
 
J. Grieve. 2007. Quantitative authorship attribution: 
An evaluation of techniques. LLC 22:251-270. 
 
H. van Halteren, F. Tweedie, and H. Baayen. 1996. 
Outside the cave of shadows: using syntactic 
annotation to enhance authorship attribution.  
Literary and Linguistic Computing 11:121-132. 
 
H. van Halteren. 2007. Author verification by 
linguistic profiling: An exploration of the 
parameter space. TSLP 4, 1?17 
 
G. Hirst, and O. Feiguina. 2007. Bigrams of syntactic 
labels for authorship discrimination of short texts. 
LLC 22, 405?417  
 
N. Jindal and B. Liu. 2008. Opinion spam and 
analysis. In: WSDM. pp. 29?230 
 
T. Joachims. 2007. www.cs.cornell.edu/people 
/tj/svmlight/old/svmmulticlassv2.12.html  
 
S. Kim,  H. Kim,  T. Weninger,  J. Han, and H. D. 
Kim. 2011. Authorship classification: a 
discriminative syntactic tree mining approach. In: 
SIGIR. pp. 455?464 
  
D. Klein and C. D. Manning. 2003 Accurate 
unlexicalized parsing. In: ACL. pp. 423?430  
 
I. Kourtis and E. Stamatatos, 2011. Author 
identification using semi-supervised learning. In: 
Notebook for PAN at CLEF 2011  
 
J. Li, R. Zheng, and H. Chen. 2006. From fingerprint 
to writeprint. Communications of the ACM 49:76-
82. 
 
K. Luyckx and W. Daelemans, 2008. Authorship 
attribution and verification with many authors and 
limited data. In: COLING. pp. 513?520 
  
D. Madigan, A. Genkin, D. Lewis, A. Argamon, D. 
Fradkin, and L. Ye, 2005. Author Identification on 
the Large Scale. In CSNA. 
 
K. Nigam and R. Ghani. 2000. Analyzing the 
effectiveness and applicability  of co-training.  In 
Proc. of CIKM, pp.86?93  
 
T. Qian, B. Liu. 2013 Identifying Multiple Userids of 
the Same Author. EMNLP, pp. 1124-1135 
 
T. Qian, B. Liu, M. Zhong, G. He. 2014. Co-Training 
on Authorship Attribution with Very Few Labeled 
Examples: Methods. vs. Views. In SIGIR, to 
appear. 
 
R. Schwartz, O. Tsur, A. Rappoport, M. Koppel. 2013. 
Authorship Attribution of Micro-Messages. 
EMNLP. pp. 1880-1891 
 
Y. Seroussi, F. Bohnert and Zukerman,.2012. 
Authorship attribution with author-aware topic 
models. In: ACL. pp. 264?269  
 
Y. Seroussi,  I. Zukerman, and F. Bohnert. 2010. 
Collaborative inference of sentiments from texts. 
In: UMAP. pp. 195?206 
 
E. Stamatatos. 2007. Author identification using 
imbalanced and limited training texts. In: TIR. pp. 
237?241 
 
E. Stamatatos. 2009. A survey of modern authorship 
attribution methods. JASIST 60:538?556 
 
?. Uzuner and B. Katz. 2005. A comparative study of 
language models for book and author recognition. 
Proc. of the 2nd IJCNLP, 969-980. 
350
 Y. Zhao and J. Zobel. 2005. Effective and scalable 
authorship attribution using function words. In 
Proc. of Information Retrival Technology, 174-
189.  
 
R. Zheng, J. Li, H. Chen, and Z. Huang. 2006. A 
framework for authorship identification of online 
messages: Writing style features and classification 
techniques. JASIST 57:378-393. 
351
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 223?230,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Preliminary Work on Symptom Name Recognition from Free-Text 
Clinical Records of Traditional Chinese Medicine using Conditional 
Random Fields and Reasonable Features 
 
Yaqiang Wang, Yiguang Liu, Zhonghua Yu*, Li Chen Yongguang Jiang 
Department of Computer Science Department of Preclinical Medicine 
Sichuan University Chengdu University of TCM 
Chengdu, Sichuan 610064, China Chengdu, Sichuan 610075, China 
yaq.wang@yahoo.com,lygpapers@yahoo.com.cn,  
yuzhonghua@scu.edu.cn,cl@scu.edu.cn  
cdtcm@163.com  
 
 
 
 
Abstract 
A preliminary work on symptom name recog-
nition from free-text clinical records (FCRs) 
of traditional Chinese medicine (TCM) is de-
picted in this paper. This problem is viewed as 
labeling each character in FCRs of TCM with 
a pre-defined tag (?B-SYC?, ?I-SYC? or ?O-
SYC?) to indicate the character?s role (a be-
ginning, inside or outside part of a symptom 
name). The task is handled by Conditional 
Random Fields (CRFs) based on two types of 
features. The symptom name recognition F-
Measure can reach up to 62.829% with recog-
nition rate 93.403% and recognition error rate 
52.665% under our experiment settings. The 
feasibility and effectiveness of the methods 
and reasonable features are verified, and sev-
eral interesting and helpful results are shown. 
A detailed analysis for recognizing symptom 
names from FCRs of TCM is presented 
through analyzing labeling results of CRFs. 
1 Introduction* 
Traditional Chinese medicine (TCM), a comple-
mentary medical theory to western medicine, pro-
vides a distinct way to view our human life (Pal, 
2002; Barnes, et al, 2004; Molassiotis, et al, 
2005). Moreover, it has shown that TCM 
knowledge, which is accumulated in clinical prac-
tice, has become one of the most important sources 
of modern biomedical research (Zhou, et al, 2010). 
                                                          
*Corresponding author 
In recent years, Data Mining and Machine 
Learning have been more than ever before applied 
to TCM clinical research, such as establishing 
TCM diagnosis expert systems for supporting deci-
sion making (Wang, et al, 2004; Huang and Chen, 
2007; Zhang, et al, 2008). However, most of the 
works are based on manually well-structured da-
tasets. 
Because of the high cost of manually structuring 
and maintaining free-text clinical records (FCRs) 
of TCM, large volume of such datasets has not 
been exploited effectively (Zhou, et al, 2010), alt-
hough they are significant for discovering new 
knowledge or capturing medical regularities. 
Therefore, developing appropriate information ex-
traction methods for handling FCRs of TCM is an 
urgent need to reduce the manual labor for re-
searchers. 
Automatically extracting meaningful infor-
mation and knowledge from FCRs of TCM is chal-
lenging in Data Mining and Machine Learning 
fields (Zhou, et al, 2010). As the basis, symptom 
name recognition or extraction from FCRs of TCM 
is in an early stage. To the best of our knowledge, 
there has little work to solve this problem (Wang, 
et al, 2010; Wang, et al, 2012). Symptom name 
recognition from FCRs of TCM was firstly at-
tempted in (Wang, et al, 2010) through normaliz-
ing the symptom names in clinical records based 
on literal similarity and remedy-based similarity 
methods but not directly recognizing original clini-
cal symptom names from FCRs of TCM. In 2012, 
Wang, et al proposed a framework of automatic 
diagnosis of TCM for practice. Symptom name 
recognition is one part of the framework and simp-
223
ly attempted through a literal similarity method 
without detailed analysis (summarized procedures 
for the previous wok are shown in figure 1). 
 
Figure 1. Simple Conclusions of the Previous Work. 
Named Entity Recognition (NER) has been 
widely studied. There have been lots of methods 
for Chinese NER (Zhang, et al, 2003; Wu, et al, 
2003; Gao, et al, 2005; Fu and Luke, 2005; Zhou, 
2006; Duan and Zhang, 2011). However, these 
methods cannot be directly applied on symptom 
name recognition from FCRs of TCM due to big 
differences of characteristics of the corpus (Wang, 
et al, 2012). There are also several related work on 
English NER, but Chinese NER has more chal-
lenges because of the distinct characteristics of 
Chinese (Wu, et al, 2003). 
In this paper, the task of symptom name recog-
nition from FCRs of TCM is studied. The symp-
tom names are recognized through finding their 
description boundaries from FCRs of TCM, and 
the method is described in section 2. Several rea-
sonable and helpful features are introduced for 
CRFs to label the characters in FCRs of TCM with 
pre-defined boundary tags to indicate their roles (a 
beginning, inside or outside part of a symptom 
name) (presented in section 3). At last, several in-
teresting and valuable experimental results are 
shown in section 4 and a conclusion is given in 
section 5. 
2 Symptom Name Recognition from FCRs 
of TCM 
The task of symptom name recognition from FCRs 
of TCM can be treated as detecting the boundaries 
of the symptom name descriptions in the sentences 
of FCRs of TCM. Therefore, this task can be 
viewed as labeling each tagging unit (e.g. word) in 
the sentences with a pre-defined tag indicating 
whether the unit is a beginning, inside, or outside 
part of a symptom name. 
Generally, the tagging unit is word (Ramshaw 
and Marcus, 1995). However, there is no natural 
segmentation for words in Chinese sentences. 
Therefore, Chinese word segmentation problem 
has to face up firstly (Gao, et al, 2005). Because of 
the characteristics of FCRs of TCM (Wang, et al, 
2012), automatically segmenting FCRs of TCM 
into words is not trivial and common Chinese word 
segmentation methods are not suitable. In order to 
tackle this problem, Chinese character is settled as 
the basic tagging unit. An example sentence of the 
labeling task is shown in figure 2. 
 
Figure 2. An Example Sentence of the Symptom Name 
Recognition Task. 
In figure 2, each character is labeled with a pre-
defined tag (?B-SYC?, ?I-SYC? or ?O-SYC?). The 
meaning of each tag is defined in table 1. 
Tag Meaning 
B-SYC Beginning of a TCM symptom name 
I-SYC Inside a TCM symptom name 
O-SYC Outside the TCM symptom names 
Table 1. Meanings of the Pre-defined Tags. 
Consequently, a recognized symptom name 
should start with a character labeled with ?B-SYC? 
and end before the character whose corresponding 
label changes from ?I-SYC? to ?B-SYC? or ?O-
SYC? for the first time. The labeling task can be 
formulated as follows: 
Given a FCR
1 2x ,x ,...,xn?x , where x i  is a 
Chinese character, the goal is to build a annotator 
p  to accurately label x  with the credible corre-
sponding tag sequence ( )p?y x , where 
1 2= y ,y ,..., yny  and y {B-SYC,I-SYC,O-SYC}n ? . 
This task can be effectively done by CRFs (Sha 
and Pereira, 2003) based on a training dataset 
which is consisted of pairs of sequences ( , )x y . 
224
3 Conditional Random Fields for Symp-
tom Name Recognition 
3.1 Conditional Random Fields 
A Conditional Random Field can be defined as an 
undirected graphical model (see figure 3) which 
consists of a sequence of vertices representing ran-
dom variables 
1 2( , ,..., )nY Y Y?Y  and edges repre-
senting conditional dependencies, conditioned on 
1 2( , ,..., )nX X X?X . The random variable iY  only 
has edges with its predecessor 
1iY ?  and successor 
1iY ? , thus, random variables 1 2, ,..., nY Y Y  obey the 
Markov property and form a linear Markov chain. 
 
Figure 3. An Undirected Graphical Structure for a Con-
ditional Random Field. 
Then the conditional probability of a label se-
quence given an input sequence can be defined as:  
1
exp ( , , )
( , )
( )
n
i
i
p
Z
?
?
?
? y x
y x
x
f
?
?
?
 
Where f  is a global feature vector (Sha and 
Pereira, 2003) and each element of f  is an arbi-
trary feature selection function 
kf  ( [1, ]k K? , 
where K  is the number of feature functions). ?  is 
a weight vector comprised by the learned weight 
k?  for each feature function. More detailed de-
scription is that,  
1
1 1
exp (y , y , , )
( | )
( )
n K
k k i i
i k
f i
p
Z
? ?
? ?
? ?
? ?? ??
?? x
y x
x
 
( )Z x  in the equation is a normalization factor 
which is the sum over all possible label sequences 
S :  
1
1 1
( ) exp (y ,y , , )n K k k i i
i k
Z f i? ?
? ?
? ?? ? ?? ?? ??x xS
 
The most likely label sequence for an input se-
quence x  is:  
argmax ( | )p?
y
y y x
 
It can be found with the Viterbi algorithm. We 
use the CRF++ tool in the experiments, which pro-
vides an efficient implementation for CRFs by us-
ing the limited-memory quasi-Newton algorithm 
for training the models (Sha and Pereira, 2003; 
Lafferty, et al, 2001) and the default settings of 
CRF++ are used. 
3.2 Features for Labeling 
It is difficult to analyze the syntactic structure of 
the content in FCRs of TCM which has narrative 
form, concise style and nonstandard description 
characteristics. Therefore, no higher level syntactic 
features, such as POS tags or NP chunks, can be 
used at the moment. Through analyzing FCRs of 
TCM, two types of representative and reasonable 
features (i.e. literal features and positional features) 
are exploited. The features are introduced and their 
reasonableness is explained by examples as fol-
lows. 
Literal Features: the simplest and the most ob-
vious features for determining the boundaries of 
symptom name descriptions are literal features. For 
example, according to the observation that after a 
word which is used to specify time (e.g. ???? 
(yesterday)) there would usually follow a symptom 
name description, such as ???? (borborygmus). 
The best approach to get such features is to di-
vide the content of FCRs of TCM into words. 
However, as described before, Chinese word seg-
mentation is not trivial work. Fortunately, seg-
menting the content into n-grams is considerable 
and reasonable, because the indicating words 
would be mixed in the n-gram segments and could 
be helpful to determine the boundaries of symptom 
name descriptions. 
Furthermore, the FCRs of TCM have a concise 
style, i.e. the length of the clauses in FCRs of TCM 
is short and words are usually used in their brief 
form. Therefore, the n-grams as the literal features 
need not be too long. In general, the average length 
of a Chinese word approximates 2 (Nie, et al, 
2000). Consequently, the value of n  should set to 
range from 1 to 3. Moreover, according to the intu-
ition that ?the distance between current character 
and its related n-grams in FCRs of TCM would not 
be too far?, the context window size, which is the 
fragment scope picking up literal features (i.e. n-
225
grams (see examples in table 2)) in FCRs of TCM, 
would not be too large. Otherwise it would bring 
about noisy information, thereby reducing the la-
beling precision. The context window size in our 
experiment is specified smaller then 4. 
Feature 
Type 
Context Window 
Size (CWS) 
Literal feature examples 
under different CWS 
Unigram 
Features 
(Uni) 
1 Ci-1, Ci, Ci+1 
2 Ci-2, Ci-1, Ci, Ci+1, Ci+2 
3 
Ci-3, Ci-2, Ci-1, Ci,  
Ci+1, Ci+2, Ci+3 
4 
Ci-4, Ci-3, Ci-2, Ci-1,  
Ci, Ci+1, Ci+2, Ci+3, Ci+4 
Bigram 
Features 
(Big) 
1 Ci-1Ci, Ci Ci+1 
2 
Ci-2Ci-1, Ci-1Ci,  
CiCi+1, Ci+1Ci+2 
3 
Ci-3Ci-2, Ci-2Ci-1, Ci-1Ci, 
CiCi+1, Ci+1Ci+2, Ci+2Ci+3 
4 
Ci-4Ci-3, Ci-3Ci-2, Ci-2Ci-1,  
Ci-1Ci, CiCi+1, Ci+1Ci+2,  
Ci+2Ci+3, Ci+3Ci+4 
Trigram 
Features 
(Tri) 
1 Ci-1CiCi+1 
2 
Ci-2Ci-1Ci, Ci-1CiCi+1,  
CiCi+1Ci+2 
3 
Ci-3Ci-2Ci-1, Ci-2Ci-1Ci,  
Ci-1CiCi+1, CiCi+1Ci+2, 
Ci+1Ci+2Ci+3 
4 
Ci-4Ci-3Ci-2Ci-1,  
Ci-3Ci-2Ci-1Ci,  
Ci-2Ci-1CiCi+1,  
Ci-1CiCi+1Ci+2,  
CiCi+1Ci+2Ci+3,  
Ci+1Ci+2Ci+3Ci+4 
Table 2. Literal Feature Examples Used in the Experi-
ments. Ci is the character at current position i in one 
clause. 
Positional Features: positions of characters in 
FCRs of TCM are also helpful. They are assistant 
features to determine the boundaries of symptom 
name descriptions. 
The start of a sentence would be usually a com-
mon character (i.e. its corresponding label is ?O-
SYC?) rather than the beginning of a symptom 
name description. On the contrary, the starting po-
sitions of the following clauses have higher proba-
bilities to be labeled with ?B-SYC?. Taking the 
FCR ?????, ???, ????? (Yesterday, 
the patient had borborygmus and more farting, and 
his/her heart was uncomfortable) as an example, it 
starts with a common word ???? (yesterday) 
followed by a symptom name ???? (borboryg-
mus). And at the same time, following clauses all 
start with symptom name descriptions. 
The example of positional features is shown in 
figure 4. 
 
Figure 4. Example of Positional Features. 
In figure 4, one ?[SubSID-POS]? represents a 
positional feature, and SubSID is the index of cur-
rent clause in a FCR and POS indicates the posi-
tion of a character in current clause. 
4 Experiments 
In this section, the proposed method for symptom 
name recognition from TCM FCRs is evaluated, 
and the usefulness of the introduced features is 
verified based on a TCM clinical dataset. The re-
sults are depicted bellow. 
4.1 Experimental Datasets 
In this paper, a clinical record dataset (CRD) is 
used. It contains 11613 FCRs of TCM and was 
collected by TCM doctors during their routine di-
agnostic work. The Chinese characters in FCRs of 
CRD are annotated with tags ?B-SYC?, ?I-SYC?, 
and ?O-SYC?. The number of each type of tags is 
69193, 104243 and 142860, respectively. There are 
4235 unique symptom names in CRD, and the 
amount of annotated symptom names is 69193. 
 Training 
Data 
Test 
Data 
Number of Unique Symptom 
Names 
1813 3463 
Amount of Symptom Names 17339 51854 
Number of Each Type of Tags 
(?B-SYC?, ?I-SYC?, ?O-SYC?) 
17339, 
25738, 
35995 
51854, 
78505, 
106865 
Table 3. Detailed Information of the Training and Test 
Datasets. 
CRD is divided into two sub-datasets (i.e. a 
training dataset (3483 FCRs, 25% of CRD) and a 
test dataset (8130 FCRs, 75% of CRD)). For con-
226
venience, all numbers (e.g. integers, decimals and 
fractions, etc.) in CRD are uniformly replaced by a 
English character ?N? in advance. Detailed infor-
mation of training and test datasets is listed in table 
3. 
4.2 Evaluation Metrics 
A new method for symptom name recognition 
from FCRs of TCM is proposed and two types of 
features are introduced. To evaluate the feasibility 
and effectiveness of the method and features, two 
groups of evaluation metrics are designed: (1) for 
assessing the ability of symptom name recognition, 
symptom name recognition rate, recognition error 
rate and recognition F-Measure are defined; (2) for 
giving a detailed analysis, the labeling precision, 
recall, and F-Measure are exercised. The detailed 
explanations of these metrics are described below. 
Symptom name recognition rate (RRdet), 
recognition error rate (RERdet) and recognition 
F-Measure (RFMdet): these metrics are designed 
for assessing capability of the proposed method for 
symptom name recognition from TCM FCRs. If 
and only if the boundary of a symptom name is 
labeled accurately (i.e. starting with ?B-SYC? and 
ending with the first change from ?I-SYC? to ?B--
SYC? or ?O-SYC?), the recognized symptom 
name is correct. Higher RRdet and lower RERdet are 
achieved; better symptom name recognition per-
formance RFMdet would be obtained. RRdet, RERdet 
and RFMdet are formulated as follows.  
| |
| |det
NSDCRR NCS?
 
| | | |
| |det
SD NSDCRER SD
??
 
2 (1 )
1
det det
det
det det
DR DERRFM DR DER
? ? ?? ? ?
 
Where | |NSDC  is the number of symptom 
name recognized correctly from the test dataset, 
| |NCS  is the number of clinical symptom names 
in the test dataset, and | |SD  is the number of 
symptom name recognized. 
Labeling precision (Prelab), recall (Reclab) and 
F-Measure (FMlab): the metrics (Prelab, Reclab and 
FMlab) are used to evaluate the performance of la-
beling Chinese character sequences of FCRs of 
TCM for giving a detailed analysis. They are de-
fined below. 
| |
| |lab
NCLCPre NCL?
 
| |
| |lab
NCLRec NC?
 
2 lab lab
lab
lab lab
Pre RecFM Pre Rec
? ?? ?
 
Where | |NCLC  is the number of characters la-
beled correctly with their corresponding tags, 
| |NCL  is the number of characters labeled with 
tags, and | |NC  is the number of characters should 
be labeled. 
4.3 Evaluation of Symptom Name Recogni-
tion Ability 
Comprehensive evaluations of symptom name 
recognition ability using CRFs with reasonable 
features are shown in figure 5, 6 and 7. These fig-
ures show that CRFs with reasonable features for 
symptom name recognition from FCRs of TCM is 
feasible. The best RFMdet 62.829% (RRdet 93.403% 
and RERdet 52.665%) is achieved under settings 
CWS 3?  and features Uni+Big+Tri used. 
 
Figure 5. Symptom Name Recognition Rate. 
It obviously shows in figures 5, 6 and 7 that lit-
eral features and positional features are helpful to 
symptom name recognition from FCRs of TCM. 
More types of features are used; better recognition 
performance would be obtained in most cases. 
When CWS 1?  and referred features changed 
from unigram literal features to the combination of 
unigram and bigram literal features, the highest 
growth about 3.925% of RFMdet is achieved (the 
227
RRdet increases from 87.586% to 93.034% and the 
RERdet decreases from 56.173% to 53.118%). 
 
Figure 6. Symptom Name Recognition Error Rate. 
 
Figure 7. Symptom Name Recognition F-Measure. 
As described previously, the context information 
is helpful to symptom name recognition. However, 
the context window size should not be too large. In 
figures 5, 6 and 7, it clearly shows that when CWS 
increase RRdet and RFMdet are improved and 
RFMdet is reduced. When CWS grows too large 
(larger than 3 here), RRdet and RFMdet begin, never-
theless, to descend and RERdet is raised in most 
every cases. 
Moreover, positional features are complemen-
tary features to literal features for symptom name 
recognition from FCRs of TCM. It vividly shows 
in figures 5, 6 and 7 that RRdet and RFMdet would 
be improved and RERdet would be reduced more or 
less when literal features combined with positional 
features. The highest elevation can reach 0.297% if 
the combination features of trigram literal features 
and positional features are used and CWS 1? . 
4.4 Evaluation of Labeling Performance and 
Detailed Analysis for Symptom Name 
Recognition 
In this part, firstly, an evaluation for labeling per-
formance is given, and then a detailed analysis for 
symptom name recognition from FCRs of TCM 
using CRFs with reasonable features would be de-
scribed. 
The results of Prelab and FMlab under different 
situations are shown in figure 8 and 9, respectively. 
The Reclab here are all 100%. It can be seen from 
these figures that the FMlab can reach nearly up to 
97.596% with corresponding Prelab 95.305%. The 
results can also demonstrate the feasibility of the 
proposed method for symptom name recognition 
from FCRs of TCM and the worth of the repre-
sentative and reasonable features introduced in this 
paper. The properties of literal features and posi-
tional features, which are just described in section 
4.3, are also reflected in figures 8 and 9. 
 
Figure 8. Results of Prelab under Different Situations. 
 
Figure 9. Results of FMlab under Different Situations. 
Although RRdet can achieve a very high perfor-
mance, however, RERdet is also too high. In figures 
8 and 9, high labeling results was gotten. It implies 
that the probable position of the symptom name 
can be found in TCM FCRs, but the exact bounda-
ries of the symptom name descriptions cannot be 
detected accurately yet. 
More careful results are listed in table 4. In this 
table, the average labeling Prelab of labels ?B-
228
SYC? and ?O-SYC? are always higher than the 
global average precision, but the average Prelab of 
?I-SYC? is lower than the global average precision. 
It implies that the performance of labeling the end 
position of a symptom name description is worse 
than the other position?s. In other words, the judg-
ment on whether ?I-SYC? or ?O-SYC? is more 
difficult. Therefore, as the future work, how to ac-
curately determine the end of a symptom name 
description should be paid more attention to. 
 CWS = 1 CWS = 2 CWS = 3 CWS = 4 
Global 
P 
94.186% 94.526% 94.616% 94.540% 
B 
P 95.184% 95.472% 95.519% 95.429% 
R 94.135% 94.243% 94.238% 94.113% 
F 94.656% 94.853% 94.873% 94.765% 
I 
P 93.085% 93.586% 93.772% 93.713% 
R 93.791% 94.181% 94.267% 94.201% 
F 93.434% 93.879% 94.016% 93.953% 
O 
P 94.533% 94.781% 94.819% 94.738% 
R 94.501% 94.916% 95.056% 94.996% 
F 94.514% 94.845% 94.934% 94.864% 
Table 4. Detailed Results of Average Prelab, Reclab and 
FMlab for Each Type of Labels. ?B?, ?I? and ?O? are 
short forms of ?B-SYC?, ?I-SYC? and ?O-SYC?, re-
spectively. 
5 Conclusion 
In this paper, a preliminary work on symptom 
name recognition from FCRs of TCM is described, 
and a feasible method based on CRFs with reason-
able features is investigated. Through the experi-
ments, the specialties, usage and effectiveness of 
the introduced features are verified. 
In future, particular syntactic structure and 
grammatical rules for FCRs of TCM need to be 
defined and studied based on the characteristics of 
FCRs of TCM. On the one hand, they can help the 
TCM doctors and researchers to understand the 
clinical records deeper (Spasic, et al, 2005; Zhou, 
et al, 2010), and on the other hand, technically, 
they are good for filtering and reducing feature size 
and providing basics and adequate evidence for 
symptom name normalization process and auto-
matic diagnosis procedure. 
Acknowledgments 
The authors would like to thank M.S. Xuehong 
Zhang and M.S. Shengrong Zhou for their helpful 
suggestions to this work and their valuable work 
on manually structuring the clinical records for us. 
The authors are grateful to Ms. Fang Yu and B.S. 
Yuheng Karen Chen for their helpful paper revis-
ing. The authors are also pleased to acknowledge 
the National Natural Science Foundation of China 
(Grant No. 61173182 and 61179071), the Provin-
cial Science and Technology Foundation of Si-
chuan Province (Grant No. 2008SZ0049), the 
Specialized Research Fund for the Doctoral Pro-
gram (Grant No. 20090181110052), and the New 
Century Excellent Talents Fund (Grant No. NCET-
08-0370) for their supporting to this work. 
References 
P.M. Barnes, E. Powell-Griner, K. McFann, R.L. Nahin. 
2004. Complementary and alternative medicine use 
among adults: United States, 2002. Seminars in Inte-
grative Medicine, 2(2):54-71. 
H. Duan, Y. Zheng. 2011. A study on features of the 
CRFs-based Chinese Named Entity Recognition. In-
ternational Journal of Advanced Intelligence, 
3(2):287-294. 
G. Fu, K.K. Luke. 2005. Chinese named entity recogni-
tion using lexicalized HMMs. SIGKDD Explorations, 
7(1):19-25. 
J. Gao, M. Li, A. Wu, C.-N. Huang. 2005.Chinese word 
segmentation and named entity recognition: a prag-
matic approach. Computational Linguistics, 
31(4):531-574. 
M. Huang, M. Chen. 2007. Integrated design of the in-
telligent web-based Chinese medical system 
(CMDS)-systematic development for digestive health. 
Expert System with Applications, 32:658-673. 
J. Lafferty, A. McCallum, F. Pereira. 2001. Conditional 
random fields: probabilistic models for segmenting 
and labeling sequence data. In Proceedings of the 
18th International Conference on Machine Learning. 
D. Li, K. Kipper-Schuler, G. Savova. 2008. Conditioal 
Random Fields and Support Vector Machine for dis-
order named entity recognition in clinical texts. In 
BioNLP 2008: Current Trends in Biomedical Natural 
Language Processing, pp:94-95. 
A. McCallum, W. Li. 2003. Early results for named 
entity recognition with conditional random fields, 
feature induction and web-enhanced lexicons. In 
Proceedings of the 7th Conference on Natural Lan-
guage Learning (CoNLL) at HLT-NAACL. 
M. Molassiotis, P. Fernadez-Ortega, D. Pud, G. Ozden, 
J.A. Scott, V. Panteli, A. Margulies, M. Browall, M. 
229
Magri, S. Selvekerova, E. Madsen, L. Milovics, I. 
Bruyns, G. Gudmundsdottir, S. Hummerston, A. M.-
A. Ahmad, N. Platin, N. Kearney, E. Pariraki. 2005. 
Use of complementary and alternative medicine in 
cancer patients: a European survey. Annals of Oncol-
ogy, 16(4):655-663. 
J.-J. Nie, J. Gao, J. Zhang, M. Zhou. 2000. On the use of 
words and n-grams for Chinese information retrieval. 
In Proceedings of the fifth international workshop on 
Information Retrieval with Asian Languages. 
S.K. Pal. 2002. Complementary and alternative medi-
cine: an overview. Current Science, 82(5):518-524. 
L.A. Ramshaw, M.P. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings of 
the Third Workshop on Very Large Corpora. ACL. 
F. Sha, F. Pereira. 2003. Shallow parsing with condi-
tional random fields. Proceedings of the 2003 Con-
ference of the North American Chapter of the 
Association of Computer Linguistics on Human Lan-
guage Technology. 
I. Spasic, S. Ananiadou, J. McNaught, A. Kumar. 2005. 
Text mining and ontologies in biomedicine: making 
sense of raw text. Briefings in Bioinformatics, 
6(3):239-251. 
X. Wang, H. Qu, P. Liu. 2004. A self-learning expert 
system for diagnosis in traditional Chinese medicine. 
Expert System with Applications, 26:557-566. 
Y. Wang, Z. Yu, Y. Jiang, K. Xu, X. Chen. 2010. Au-
tomatic symptom name normalization in clinical rec-
ords of traditional Chinese medicine. BMC 
Bioinformatics, 11:40. 
Y. Wang, Z. Yu, Y. Jiang, Y. Liu, L. Chen, Y. Liu. 
2012. A framework and its empirical study of auto-
matic diagnosis of traditional Chinese medicine uti-
lizing raw free-text clinical records. Journal of 
Biomedical Informatics, 45:210-223. 
Y. Wu, J. Zhao, B. Xu. 2003. Chinese named entity 
recognition combining a statistical model with hu-
man knowledge. In Proceedings of the ACL 2003 
Workshop on Multilingual and Mixed-Language 
Named Entity Recognition (MultiNER?03), pp:65-72. 
K. Yoshida, J. Tsujii. 2007. Reranking for biomedical 
named-entity recognition. In BioNLP 2007: Biologi-
cal, translational, and clinical language processing, 
pp:209-216. 
H.-P. Zhang, Q. Liu, H.-K. Yu, X.-Q. Cheng, S. Bai. 
2003. Chinese named entity recognition using role 
model. Computational Linguistics and Chinese Lan-
guage Processing, 8(2):29-60. 
N.L. Zhang, S. Yuan, Y. Wang. 2008. Latent tree mod-
els and diagnosis in traditional Chinese medicine. Ar-
tificial Intelligence in Medicine, 42:229-245. 
J. Zhou, L. He, X. Dai, J. Chen. 2006. Chinese named 
entity recognition with a multi-phase model. In Pro-
ceedings of the fifth Workshop on Chinese Language 
Processing, pp:213-216. 
X. Zhou, Y. Peng, B. Liu. 2010. Text mining for tradi-
tional Chinese medical knowledge discovery: a sur-
vey. Joural of Biomedical Informatics, 43:650-660. 
G.D. Zhou, J. Su. 2002. Named entity recognition using 
an HMM-based Chunk Tagger. In Proceedings of the 
40th Annual Meeting of the Association for Computa-
tional Linguistics. 
230
