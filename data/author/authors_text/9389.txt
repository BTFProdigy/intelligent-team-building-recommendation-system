Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 391?399,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
When Harry Met Harri,  and : 
Cross-lingual Name Spelling Normalization 
 
Fei Huang , Ahmad Emami and Imed Zitouni 
IBM T. J. Watson Research Center 
1101 Kitchawan Road 
Yorktown Heights, NY 10598 
{huangfe, emami, izitouni}@us.ibm.com 
 
Abstract 
Foreign name translations typically include 
multiple spelling variants. These variants 
cause data sparseness problems, increase 
Out-of-Vocabulary (OOV) rate, and present 
challenges for machine translation, 
information extraction and other NLP tasks. 
This paper aims to identify name spelling 
variants in the target language using the 
source name as an anchor. Based on word-
to-word translation and transliteration 
probabilities, as well as the string edit 
distance metric, target name translations with 
similar spellings are clustered. With this 
approach tens of thousands of high precision 
name translation spelling variants are 
extracted from sentence-aligned bilingual 
corpora. When these name spelling variants 
are applied to Machine Translation and 
Information Extraction tasks, improvements 
over strong baseline systems are observed in 
both cases. 
1 Introduction 
Foreign names typically have multiple spelling 
variants after translation, as seen in the 
following examples:   
He confirmed that "al-Kharroub 
province is at the top of our 
priorities."  
?for the Socialist Progressive 
Party in upper Shuf and the Al-
Kharrub region,? 
?during his tour of a number of 
villages in the region of Al-
Kharub,? 
?Beirut and its suburbs and 
Iqlim al-Khurub,? 
  
 
 
 
Such name spelling variants also frequently 
appear in other languages, such as (bushi) / 
(bushu) / (buxi) (for Bush) in Chinese, 
and 	 (sbrngfyld) /
	  (sbryngfyld) / 
	 (sbrynjfyld) (for Springfield) in Arabic.  
These spelling variants present challenges for 
many NLP tasks, increasing vocabulary size and 
OOV rate, exacerbating the data sparseness 
problem and reducing the readability of MT 
output when different spelling variants are 
generated for the same name in one document. 
We address this problem by replacing each 
spelling variant with its corresponding canonical 
form. Such text normalization could potentially 
benefit many NLP tasks including information 
retrieval, information extraction, question 
answering, speech recognition and machine 
translation. 
Research on name spelling variants has been 
studied mostly in Information Retrieval research, 
especially in query expansion and cross-lingual 
IR.  Baghat and Hovy (2007) proposed two 
approaches for spelling variants generation, 
based on the letters-to-phonemes mapping and 
Soundex algorithm (Knuth 1973). Raghaven and 
Allan (2005) proposed several techniques to 
group names in ASR output and evaluated their 
effectiveness in spoken document retrieval 
(SDR). Both approaches use a named entity 
extraction system to automatically identify 
names. For multi-lingual name spelling variants, 
Linden (2005) proposed to use a general edit 
distance metric with a weighted FST to find 
technical term translations (which were referred 
to as ?cross-lingual spelling variants?). These 
391
variants are typically translated words with 
similar stems in another language. Toivonen and 
colleagues (2005) proposed a two-step fuzzy 
translation technique to solve similar problems. 
Al-Onaizan and Knight (2002), Huang (2003) 
and Ji and Grishman (2007) investigated the 
general name entity translation problem, 
especially in the context of machine translation. 
This paper aims to identify mono-lingual 
name spelling variants using cross-lingual 
information. Instead of using a named entity 
tagger to identify name spelling variants, we 
treat names in one language as the anchor of 
spelling variants in another language. From 
sentence-aligned bilingual corpora we collect 
word co-occurrence statistics and calculate word 
translation1 probabilities. For each source word, 
we group its target translations into clusters 
according to string edit distances, then calculate 
the transliteration cost between the source word 
and each target translation cluster. Word pairs 
with small transliteration costs are considered as 
name translations, and the target cluster contains 
multiple spelling variants corresponding to the 
source name.  
We apply this approach to extract name 
transliteration spelling variants from bilingual 
corpora. We obtained tens of thousands of high 
precision name translation pairs. We further 
apply these spelling variants to Machine 
Translation (MT) and Information Extraction (IE) 
tasks, and observed statistically significant 
improvement on the IE task, and close to oracle 
improvement on the MT task.  
The rest of the paper is organized as follows. 
In section 2 we describe the technique to 
identify name spelling variants from bilingual 
data. In section 3 and 4 we address their 
application to MT and IE respectively. We 
present our experiment results and detailed 
analysis in section 5. Section 6 concludes this 
paper with future work. 
2 Finding Name Translation Variants 
                                                          
1
 In this paper, the translation cost measures the semantic 
difference between source and target names, which are 
estimated from their co-occurrence statistics. The 
transliteration cost measures their phonetic distance and are 
estimated based on a character transliteration model. 
Starting from sentence-aligned parallel data, we 
run HMM alignment (Vogel et. al. 1996 & Ge 
2004) to obtain a word translation model. For 
each source word this model generates target 
candidate translations as well as their translation 
probabilities. A typical entry is shown in Table 1.  
It can be observed that the Arabic name?s 
translations include several English words with 
similar spellings, all of which are correct 
translations. However, because the lexical 
translation probabilities are distributed among 
these variants, none of them has the highest 
probability. As a result, the incorrect translation, 
iqlim, is assigned the highest probability and 
often selected in MT output. To fix this problem, 
it is desirable to identify and group these target 
spelling variants, convert them into a canonical 
form and merge their translation probabilities.  
 | Alxrwb
iqlim 
[0.22] 
al-kharrub 
[0.16] 
al-kharub 
[0.11] 
overflew 
[0.09] 
junbulat 
[0.05] 
al-khurub 
[0.05] 
hours 
[0.04] 
al-kharroub 
[0.03] 
 
Table 1. English translations of a Romanized Arabic 
name Alxrwb with translation probabilities. 
   For each source word in the word translation 
model, we cluster its target translations based on 
string edit distances using group average 
agglomerative clustering algorithm (Manning 
and Sch?tze, 2000). Initially each target word is 
a single word cluster. We calculate the average 
editing distance between any two clusters, and 
merge them if the distance is smaller than a 
certain threshold. This process repeats until the 
minimum distance between any two clusters is 
above a threshold. In the above example, al-
kharrub, al-kharub, al-khurub and al-kharroub 
are grouped into a single cluster, and each of the 
ungrouped words remains in its single word 
cluster. Note that the source word may not be a 
name while its translations may still have similar 
spellings. An example is the Arabic word   
which is aligned to English words brief, briefing, 
briefed and briefings. To detect whether a source 
word is a name, we calculate the transliteration 
cost between the source word and its target 
translation cluster, which is defined as the 
average transliteration cost between the source 
word and each target word in the cluster. As 
392
many names are translated based on their 
pronunciations, the source and target names 
have similar phonetic features and lower 
transliteration costs. Word pairs whose 
transliteration cost is lower than an empirically 
selected threshold are considered as name 
translations. 
2.1 Name Transliteration Cost 
The transliteration cost measures the phonetic 
similarity between a source word and a target 
word. It is calculated based on the character 
transliteration model, which can be trained from 
bilingual name translation pairs. We segment the 
source and target names into characters, then run 
monotone2 HMM alignment on the source and 
target character pairs. After the training, 
character transliteration probabilities can be 
estimated from the relevant frequencies of 
character alignments. 
Suppose the source word f contains m 
characters, f1, f2, ?, fm,  and the target word e 
contains n characters, e1, e2, ?, en. For j=1, 2,?, 
n, letter  ej is aligned to character jaf according 
to the HMM aligner. Under the assumption that 
character alignments are independent, the word 
transliteration probability is calculated as 
 ?
=
=
n
j
aj jfepfeP
1
)|()|(          (2.1) 
where )|(
jaj fep is the character transliteration 
probability.  Note that in the above configuration 
one target character can be aligned to only one 
source character, and one source character can 
be aligned to multiple target characters.  
An example of the trained A-E character 
transliteration model is shown in Figure 1. The 
Arabic character  is aligned with high 
probabilities to English letters with similar 
pronunciation. Because Arabic words typically 
omit vowels, English vowels are also aligned to 
Arabic characters. Given this model, the 
characters within a Romanized Arabic name and 
its English translation are aligned as shown in 
Figure 1.   
                                                          
2
 As name are typically phonetically translated, the 
character alignment are often monotone. There is no cross-
link in character alignments. 
2.2 Transliteration Unit Selection 
The transliteration units are typically characters. 
The Arabic alphabet includes 32 characters, and 
the English alphbet includes 56 letters 3 . 
However, Chinese has about 4000 frequent 
characters. The imbalance of Chinese and 
English vocabulary sizes results in suboptimal 
transliteration model estimation. Each Chinese 
character also has a pinyin, the Romanized 
representation of its pronunciation. Segmenting 
the Chinese pinyin into sequence of Roman 
letters, we now have comparable vocabulary 
sizes for both Chinese and English. We build a 
pinyin transliteration model using Chinese-
English name translation pairs, and compare its 
performance with a character transliteration 
model in Experiment section 5.1. 

h 
[0.44] 
K 
[0.29] 
k 
[0.21] 
a 
[0.03] 
u 
[0.015] 
i 
[0.004] 
 
Figure 1. Example of the learned A-E character 
transliteration model with probabilities, and its 
application in the alignment between an Romanized 
Arabic name and an English translation. 
3 Application to Machine Translation 
We applied the extracted name translation 
spelling variants to the machine translation task. 
Given the name spelling variants, we updated 
both the translation and the language model, 
adding variants? probabilities to the canonical 
form. 
   Our baseline MT decoder is a phrase-based 
decoder as described in (Al-Onaizan and 
Papineni 2006). Given a source sentence, the 
decoder tries to find the translation hypothesis 
with minimum translation cost, which is defined 
as the log-linear combination of different feature 
functions, such as translation model cost, 
language model cost, distortion cost and 
                                                          
3Uppercase and lowercase letters plus some special 
symbols such as ?_?, ?-?. 
393
sentence length cost. The translation cost 
includes word translation probability and phrase 
translation probability. 
3.1 Updating The Translation Model 
Given target name spelling variants { mttt ,...,, 21  
} for a source name s, here mttt ,...,, 21 are sorted 
based on their lexical translation probabilities, 
).|(...)|()|( 21 stpstpstp m???  
We select 1t  as the canonical spelling, and 
merge other spellings? translation probabilities 
with this one: 
?
=
=
m
j
m stpstp
1
1 ).|()|(  
Other spelling variants get zero probability. 
Table 2 shows the updated word translation 
probabilities for ?|Alxwrb?. Compared 
with Figure 1, the translation probabilities from 
several spelling variants are merged with the 
canonical form, al-kharrub, which now has the 
highest probability in the new model. 

Table 2. English translations of an Arabic name |Alxrwb with the updated word translation 
model. 
 
   The phrase translation table includes source 
phrases, their target phrase translations and the 
frequencies of the bilingual phrase pair 
alignment. The phrase translation probabilities 
are calculated based on their alignment 
frequencies, which are collected from word 
aligned parallel data. To update the phrase 
translation table, for each phrase pair including a 
source name and its spelling variant in the target 
phrase, we replace the target name with its 
canonical spelling. After the mapping, two target 
phrases differing only in target names may end 
up with the identical target phrase, and their 
alignment frequencies are added. Phrase 
translation probabilities are re-estimated with the 
updated frequencies. 
3.2 Updating The Language Model 
The machine translation decoder uses a language 
model as a measure of a well-formedness of the 
output sentence. Since the updated translation 
model can produce only the canonical form of a 
group of spelling variants, the language model 
should be updated in that all m-grams 
( Nm ??1 ) that are spelling variants of each 
other are merged (and their counts added), 
resulting in the canonical form of the m-gram. 
Two m-grams are considered spelling variants of 
each other if they contain words it1 , 
it2 ( ii tt 21 ? ) 
at the same position i in the m-gram, and that it1  
and it2 belong to the same spelling variant group. 
   An easy way to achieve this update is to 
replace every spelling variant in the original 
language model training data with its 
corresponding canonical form, and then build 
the language model again. However, since we do 
not want to replace words that are not names we 
need to have a mechanism for detecting names.  
For simplicity, in our experiments we assumed a 
word is a name if it is capitalized, and we 
replaced spelling variants with their canonical 
forms only for words that start with a capital 
letter.  
4 Applying to Information Extraction 
Information extraction is a crucial step toward 
understanding a text, as it identifies the 
important conceptual objects in a discourse. We 
address here one important and basic task of 
information extraction: mention detection4: we 
call instances of textual references to objects 
mentions, which can be either named (e.g. John 
Smith), nominal (the president) or pronominal 
(e.g. he, she). For instance, in the sentence  
? President John Smith said he has no 
comments.   
there are two mentions: John Smith and he. 
Similar to many classical NLP tasks, we 
formulate the mention detection problem as a 
classification problem, by assigning to each 
token in the text a label, indicating whether it 
starts a specific mention, is inside a specific 
mention, or is outside any mentions. Good 
                                                          
4We adopt here the ACE (NIST 2007) nomenclature. 
 | Alxwrb
al-kharrub 
 [0.35] 
 iqlim 
 [0.22] 
al-kharub 
[0.0] 
overflew 
[0.09] 
junbulat 
[0.05] 
al-khurub 
[0.0] 
hours 
[0.04] 
al-kharroub 
[0.0] 
 
394
performance in many natural language 
processing tasks has been shown to depend 
heavily on integrating many sources of 
information (Florian et al 2007). We select an 
exponential classifier, the Maximum Entropy 
(MaxEnt henceforth) classifier that can integrate 
arbitrary types of information and make a 
classification decision by aggregating all 
information available for a given classification 
(Berger et al 1996). In this paper, the MaxEnt 
model is trained using the sequential conditional 
generalized iterative scaling (SCGIS) technique 
(Goodman, 2002), and it uses a Gaussian prior 
for regularization (Chen and Rosenfeld, 2000). 
   In ACE, there are seven possible mention 
types: person, organization, location, facility, 
geopolitical entity (GPE), weapon, and vehicle. 
Experiments are run on Arabic and English. Our 
baseline system achieved very competitive result 
among systems participating in the ACE 2007 
evaluation. It uses a large range of features, 
including lexical, syntactic, and the output of 
other information extraction models. These 
features were described in (Zitouni and Florian, 
2008 & Florian et al 2007), and are not 
discussed here. In this paper we focus on 
examining the effectiveness of name spelling 
variants in improving mention detection 
systems. We add a new feature that for each 
token xi  to process we fire its canonical form 
(class label) C(xi) ,  representative of name 
spelling variants of xi . This name spelling 
variant feature is also used in conjunction with 
the lexical (e.g., words and morphs in a 3-word 
window, prefixes and suffixes of length up to 4, 
stems in a 4-word window for Arabic) and 
syntactic (POS tags, text chunks) features. 
5 Experiments 
5.1 Evaluating the precision of name 
spelling variants 
We extracted Arabic-English and English-
Arabic name translation variants from sentence-
aligned parallel corpora released by LDC. The 
accuracy of the extracted name translation 
spelling variants are judged by proficient Arabic 
and Chinese speakers. 
   The Arabic-English parallel corpora include 
5.6M sentence pairs, 845K unique Arabic words 
and 403K unique English words. We trained a 
word translation model by running HMM 
alignment on the parallel data, grouped target 
translation with similar spellings and computed 
the average transliteration cost between the 
Arabic word and each English word in the 
translation clusters according to Formula 2.1. 
We sorted the name translation groups according 
to their transliteration costs, and selected 300 
samples at different ranking position for 
evaluation (20 samples at each ranking position). 
The quality of the name translation variants are 
judged as follows: for each candidate name 
translation group }|,...,,{ 21 sttt m , if the source 
word s is a name and all the target spelling 
variants are correct translations, it gets a credit 
of 1. If s is not a name, the credit is 0. If s is a 
name but only part of the target spelling variants 
are correct, it gets partial credit n/m, where n is 
the number of correct target translations. We 
evaluate only the precision of the extracted 
spelling variants 5 . As seen in Figure 2, the 
precision of the top 22K A-E name translations 
is 96.9%. Among them 98.5% of the Arabic 
words are names. The precision gets lower and 
lower when more non-name Arabic words are 
included. On average, each Arabic name has 
2.47 English spelling variants, although there are 
some names with more than 10 spelling variants. 
   Switching the source and target languages, we 
obtained English-Arabic name spelling variants, 
i.e., one English name with multiple Arabic 
spellings. As seen in Figure 3, top 20K E-A 
name pairs are obtained with a precision above 
87.9%, and each English name has 3.3 Arabic 
spellings on average. Table 3 shows some A-E 
and E-A name spelling variants, where Arabic 
words are represented in their Romanized form. 
  We conduct a similar experiment on the 
Chinese-English language pair, extracting 
Chinese-English and English-Chinese name 
spelling variants from 8.7M Chinese-English 
sentence pairs. After word segmentation, the 
Chinese vocabulary size is 1.5M words, and 
English vocabulary size is 1.4M words. With the  
                                                          
5
 Evaluating recall requires one to manually look through 
the space of all possible transliterations (hundreds of 
thousands of entries), which is impractical. 
395
Chinese pinyin transliteration model, we extract 
64K C-E name spelling variants with 93.6% 
precision. Figure 4 also shows the precision 
curve of the Chinese character transliteration 
model. On average the pinyin transliteration 
model has about 6% higher precision than the 
character transliteration model. The pinyin 
transliteration model is particularly better on the 
tail of the curve, extracting more C-E 
transliteration variants. Figure 5 shows the 
precision curve for E-C name spelling variants, 
where 20K name pairs are extracted using letter-
to-character transliteration model, and obtaining 
a precision of 74.3%. 
 Table 4 shows some C-E and E-C name 
spelling variants. We observed errors due to 
word segmentation. For example, the last two 
Chinese words corresponding to ?drenica? have 
additional Chinese characters, meaning ?drenica 
region? and ?drenica river?. Similarly for tenet, 
the last two Chinese words also have 
segmentation errors due to missing or spurious 
characters. Note that in the C-E spelling variants, 
the source word ? ? has 14 spelling 
variants. Judge solely from the spelling, it is 
hard to tell whether they are the same person 
name with different spellings. 
5.2   Experiments on Machine Translation 
We apply the Arabic-English name spelling 
variants on the machine translation task. Our 
baseline system is trained with 5.6M Arabic- 
English sentence pairs, the same training data 
used to extract A-E spelling variants. The 
language model is a modified Kneser-Ney 5-
gram model trained on roughly 3.5 billion words. 
After pruning (using count cutoffs), it contains a 
total of 935 million N-grams. We updated the 
translation models and the language model with 
the name spelling variant class. 
   Table 5 shows a Romanized Arabic sentence, 
the translation output from the baseline system 
and the output from the updated models. In the 
baseline system output, the Arabic name 
?Alxrwb? was incorrectly translated into 
?regional?. This error was fixed in the updated 
model, where both translation and language 
models assign higher probabilities to the correct 
translation ?al-kharroub? after spelling variant 
normalization.  
 
 
	
	
		

























	

















	
	













	











	









	

 
Figure 2. Arabic-English name spelling variants 
precision curve (Precision of evaluation sample at 
different ranking positions. The larger square indicates 
the cutoff point). 
	
	
		

























	

















	
	













	











	









	
ranking
pinyin char
 
Figure 4. Chinese-English name spelling variants 
precision curve. 
	
	
		

























	

















	
	













	











	




 
Figure 3. English-Arabic name spelling variants 
precision curve. 
	
	
		

























	

















	
	













	











	




 
Figure 5. English-Chinese name spelling variants 
precision curve. 
396
Source Alm&tmr AlAwl lAqlym Alxrwb AlErby AlmqAwm 
Reference the first conference of the Arab resistance in Iqlim Kharoub 
Baseline the first conference of the Arab regional resistance 
Updated model first conference of the Al-Kharrub the Arab resistance 
 
Table 5. English translation output with the baseline MT system and the system with updated models 
 
    
 BLEU 
r1n4 TER 
Baseline 0.2714 51.66 
Baseline+ULM+UTM 0.2718 51.46 
Ref. Normalization 0.2724 51.40 
Table 6. MT scores with updated TM and LM 
  We also evaluated the updated MT models on a 
MT test set. The test set includes 70 documents 
selected from GALE 2007 Development set. It 
contains 42 newswire documents and 28 weblog 
and newsgroup documents. There are 669 
sentences with 16.3K Arabic words in the test 
data. MT results are evaluated against one 
reference human translation using BLEU 
(Papineni et. al. 2001) and TER (Snover et. al. 
2006) scores. The results using the baseline 
decoder and the updated models are shown in 
Table 6. Applying the updated language model 
(ULM) and the translation model (UTM) lead to 
a small reduction in TER. After we apply similar 
name spelling normalization on the reference 
translation, we observed some additional 
improvements. Overall, the BLEU score is 
increased by 0.1 BLEU point and TER is 
reduced by 0.26. 
   Although the significance of correct name 
translation can not be fully represented by 
 
Table 3. Arabic-English and English-Arabic name spelling variant examples. Italic words represent different 
persons with similar spelling names. 
 
Lang. Pair Source Name Target Spelling Variants 
Alxmyny khomeini al-khomeini al-khomeni khomeni khomeyni khamenei khameneh'i 
krwby     karroubi karrubi krobi karubi karoubi kroubi 
Arabic-
English 
gbryAl     gabriel gabrielle gabrial ghobrial ghybrial 
cirebon   syrybwn syrbwn syrbn kyrybwn bsyrybwn bsyrwbwn 
mbinda     mbyndA mbndA mbydA AmbyndA AmbAndA mbynydA  
English-
Arabic 
nguyen     njwyn ngwyn ngwyyn ngyyn Angwyn nygwyyn nygwyn wnjwyn njwyyn 
nyjyn bnjwyn wngyyn ngwyAn njyn nykwyn  
 
Table 4. Chinese-English and English-Chinese name spelling variant examples with pinyin for Chinese characters. 
Italic words represent errors due to word segmentation. 
Lang. Pair Source Name  Target Spelling Variants 
	

(yan/duo/wei/ci/ji) 
endovitsky jendovitski yendovitski endovitski 
  
(si/te/fan/ni) 
stefani steffani stephani stefanni stefania 
Chinese-
English 
 
(wei/er/man) 
woermann wellman welman woellmann wohrmann wormann velman 
wollmann wehrmann verman woehrmann wellmann welmann wermann 
tenet (te/ni/te) (te/nei/te) (tai/nei/te) (te/nai/te) 
(te/nai/te) (te/nei/te/yu) (te/nei) 
drenica (de/lei/ni/cha) (de/lei/ni/ka) (te/lei/ni/cha) 
(te/lei/ni/cha) (de/lei/ni/cha/qu) 	
(de/lei/ni/cha/he) 
English-
Chinese 
ahmedabad Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 600?609,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Mention Detection Crossing the Language Barrier
Imed Zitouni and Radu Florian
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598
{izitouni, raduf}@us.ibm.com
Abstract
While significant effort has been put into an-
notating linguistic resources for several lan-
guages, there are still many left that have
only small amounts of such resources. This
paper investigates a method of propagat-
ing information (specifically mention detec-
tion information) into such low resource
languages from richer ones. Experiments
run on three language pairs (Arabic-English,
Chinese-English, and Spanish-English) show
that one can achieve relatively decent perfor-
mance by propagating information from a lan-
guage with richer resources such as English
into a foreign language alone (no resources
or models in the foreign language). Fur-
thermore, while examining the performance
using various degrees of linguistic informa-
tion in a statistical framework, results show
that propagated features from English help
improve the source-language system perfor-
mance even when used in conjunction with all
feature types built from the source language.
The experiments also show that using propa-
gated features in conjunction with lexically-
derived features only (as can be obtained di-
rectly from a mention annotated corpus) yields
similar performance to using feature types de-
rived from many linguistic resources.
1 Introduction
Information extraction is a crucial step toward un-
derstanding a text, as it identifies the important con-
ceptual objects and relations between them in a dis-
course. It includes classification, filtering, and se-
lection based on the language content of the source
data, i.e., based on the meaning conveyed by the
data. It is a crucial step for several applications,
such as summarization, information retrieval, data
mining, question answering, language understand-
ing, etc. This paper addresses an important and basic
task of information extraction: mention detection1 :
the identification and classification of textual refer-
ences to objects/abstractions mentions, which can be
either named (e.g. John Smith), nominal (the presi-
dent) or pronominal (e.g. he, she). For instance, in
the sentence
President John Smith said he has no
comments.
there are three mentions: President, John Smith and
he. This is similar to the named entity recognition
(NER) task with the additional twist of also identi-
fying nominal and pronominal mentions.
A few languages have received a lot of attention
in terms of natural language resources that were cre-
ated ? for instance, in English one has access to la-
beled part-of-speech data, word sense information,
parse tree structure, discourse, semantic role labeles,
named entity data, to name just a few (our apologies
if we missed your favorite resource). There are a few
other languages that also have annotated resources
(such as Arabic, Chinese, German, French, Spanish,
etc), but also a very large number of languages with
few resources. It would be very useful if one could
make use of the resources in the former languages
to help bootstrapping (or just the projection) of re-
source in any resource-challenged language.
Information transfer from a language to another
can be very useful when the ?donor? language has
more resources than the receiving one. As resources
grow in quantity and quality in the receiving lan-
guage, it becomes less and less likely that there will
be a gain in performance by transfering information,
as there are several sources of noise involved in the
1We adopt here the ACE (NIST, 2007) nomenclature
600
process - such as the translation (machine generated
or not) and the inherent imperfection of the mention
detection in the donor language. To test this hypoth-
esis, we conducted experiments on systems build
with a varied amount of resources in the receiv-
ing language, starting with the case where there are
none2 (all information is transferred through transla-
tion alignment), and ending with the case where we
used all the resources we could gather for that lan-
guage. The experiments will show that the gain in
performance decreases with the amount of resources
used in the source language, but, still, even when all
resources were used, a statistically significant gain
was still observed.
Similarly to classical NLP tasks such as text
chunking (Ramshaw and Marcus, 1995) and named
entity recognition (Tjong Kim Sang, 2002), we for-
mulate mention detection as a sequence classifica-
tion problem, by assigning a label to each token in
the text, indicating whether it starts a specific men-
tion, is inside a specific mention, or is outside any
mentions. The classification is performed with a sta-
tistical approach, built around the maximum entropy
(MaxEnt) principle (Berger et al, 1996), that has the
advantage of combining arbitrary types of informa-
tion in making a classification decision.
2 Previous Work
There are several investigations in literature that
explore using parallel corpora to transfer informa-
tion content from one language (most of the time
English) to another. The earliest investigations of
the subject have been performed, on word sense
disambiguation (Dagan et al, 1991; P.F.Brown et
al., 1991; Gale et al, 1992) (perhaps unsurpris-
ingly given its close connection to machine trans-
lation) ? all propose and (lightly) evaluate methods
to use word sense information extracted from the
target language to help the sense resolution in the
source language and machine translation. (Dagan
and Itai, 1994) explicitly suggests performing word
sense disambiguation in the target language (English
in the article) with the goal of resolving ambiguity in
the source language (Hebrew), and show moderate
2While applying this method in the case where the source
language has absolutely no resources might be an interesting
test case, we don?t see it as being realistic. Resources are build
nowadays in a large variety of languages, and not making use
of them is rather foolish (a certain big bird and sand comes to
mind).
improvement on a small data set3. More recently,
(Diab and Resnik, 2001) presents a method for per-
forming word sense tagging in both the source and
target texts of parallel bilingual corpora with the En-
glish WordNet sense inventory, by using translation
correspondences.
On more general cross-language information
transfer, (Yarowsky et al, 2001) proposed and eval-
uated a method of propagating POS tagging, named
mention, base noun phrase, and morphological in-
formation from English into a foreign language,
which is very similar to the one presented in this
article (experiments were run on French, Chinese,
Czech, and Spanish ? on human-generated transla-
tions). Their results show a significant improvement
in performance while building an automatic classi-
fier on the projected annotations over the same au-
tomatic classifier trained on a small amount of an-
notated data in the source language. (Riloff et al,
2002) extends the ideas in (Yarowsky et al, 2001),
by showing how it can be used, in conjunction with
an automatically trained information extraction sys-
tem on the source language, to bootstrap the annota-
tion of resources in the target language. They show
that they can obtain 48 F-measure on a information
extraction task identifying locations, vehicles and
victims in plane crashes. (Hwa et al, 2002) proposes
a framework that enables the acquisition of syntactic
dependency trees for low-resource languages by im-
porting linguistic annotation from rich-resource lan-
guages (English). The authors run a large-scale ex-
periment in which Chinese dependency parses were
induced from English, and show that a parser trained
on the resulting trees outperformed simple baselines.
(Cabezas et al, 2001) investigates a similar method
of propagating syntactic treebank-like annotations
from English to Spanish.
Finally, a large body of research has been done
on cross-language information retrieval, where the
goal is to find information in one language (e.g. Chi-
nese newswire) corresponding to a query in a differ-
ent language (e.g. English) ? although the list of rel-
evant papers is too long to be mentioned here (see,
for instance, (Grefenstette, 1998)).
The work presented here differs from the infor-
mation extraction investigations presented above in
two aspects:
? it handles unrestricted text and a full set of
3Very small by ?modern? standards - 137 examples. Prob-
ably because at the time the article was written, there were no
large publicly annotated databases, such as Semcor.
601
mention types (the ACE entity types) during the
information transfer
? it investigates whether using a resource-rich
language (English) can improve on the perfor-
mance obtained by using various degrees of ex-
istent resources in the source language (Arabic,
Chinese, Spanish)
? the information transfer is performed over ma-
chine generated translations and alignments.
3 Mention Detection
As mentioned in the introduction, the mention detec-
tion problem is formulated as a classification prob-
lem, by assigning to each token in the text a label,
indicating whether it starts a specific mention, is in-
side a specific mention, or is outside any mentions.
Good performance in many natural language pro-
cessing tasks has been shown to depend heavily on
integrating many sources of information (Florian et
al., 2004).4 Given this observation, we are interested
in algorithms that can easily integrate and make ef-
fective use of diverse input types. We select a ex-
ponential classifier, the Maximum Entropy (MaxEnt
henceforth) classifier that integrates arbitrary types
of information and makes a classification decision
by aggregating all information available for a given
classification. But the reader can replace it with her
favorite feature-based classifier throughout the pa-
per.
To help with the presentation, we introduce some
notations: let Y = {y1, . . . , yn} be the set of pre-
dicted classes, X be the example space and F =
{0, 1}m be a feature space. Each example x ? X
has associated a vector of m binary features f (x) =
(f1 (x) , . . . , fm (x)). The goal of the training pro-
cess is to associate examples x ? X with either
a probability distribution over the labels from Y ,
P (?|x)(if we are interested in soft classification) or
associate one label y ? Y (if we are interested in
hard classification).
The MaxEnt algorithm associates a set of weights
{?ij}i=1...nj=1...m with the features (fj)i, and computes
the probability distribution as
P (yi|x) =
1
Z(x)
m
?
j=1
?fj(x,yi)ij , (1)
Z(x) =
?
i
?
j
?fj(x,yi)ij
4In fact, the feature set used for classification has a much
larger impact on the performance of the resulting system than
the classifier method itself.
where Z(x) is a normalization factor. The
{?ij}j=1...m weights are estimated during the train-
ing phase to maximize the likelihood of the
data (Berger et al, 1996). In this paper, the Max-
Ent model is trained using the sequential condi-
tional generalized iterative scaling (SCGIS) tech-
nique (Goodman, 2002), and it uses a Gaussian
prior for regularization (Chen and Rosenfeld, 2000).
Now take xN1 = (x1, x2, . . . xN ), a sequence of
contiguous tokens (i.e., a sentence or a document) in
the source language. The goal of mention detection
system is to find the most likely sequence of labels
yN1 = (y1, y2 . . . yN ) that best matches the input xN1 .
In the mention detection case, each token xi in xN1
is tagged with a label yi as follows:5
? if it?s not part of any entity, yi = O (O for ?out-
side any mentions?)
? if it is part of an entity, it is composed of a sub-
tag specifying whether it starts a mention (B-)
or is inside a mention (I-), and a sub-type cor-
responding to mention type (e.g. B-PERSON).
In ACE, there are seven possible types: person,
organization, location, facility, geopolitical en-
tity (GPE), weapon, and vehicle.
To compute the best sequence yN1 , we use
yN1 = arg max
y?N1
P
(
y?N1 |xN1
)
= arg max
y?
?
P
(
y?j |xN1 , y?j?11
)
= arg max
y?
?
j
P
(
y?j |xN1 , yj?1j?k
)
where P
(
y?j|xN1 , yj?1j?k
)
has an exponential form of
the type (2). We also used the standard Markov as-
sumption that the probability P
(
y?j|xN1 , y?
j?1
1
)
only
depends on the previous k classifications. This
model is similar to the MEMM model (McCallum
et al, 2000), but it does not separate the probability
into generation probabilities and transition probabil-
ities, and, crucially, has access to ?future? observed
features (i.e. it can examine the entire xN1 sequence,
though in practice it will only examine some small
part of it) ? which is one way of eliminating label
5The mention encoding is the IOB2 encoding presented in
(Tjong Kim Sang and Veenstra, 1999) and introduced by
(Ramshaw and Marcus, 1994) for base noun phrase chunking.
602
bias observed by (Lafferty et al, 2001).6
The experiments are run on four languages, part
of the ACE-2007 evaluation (NIST, 2007): Arabic,
Chinese, English and Spanish.7 Systems across the
languages use a large range of features, including
lexical (words and morphs in a 3-word window, pre-
fixes and suffixes of length up to 4 characters, Word-
Net (Miller, 1995) for English), syntactic (POS tags,
text chunks), and the output of other information ex-
traction models. These features were described in
(Florian et al, 2004), and are not discussed here. In
this paper we focus on the examining the benefit of
cross-language mention propagation information in
improving mention detection systems.
Besides generic types of features, we also have
implemented language-specific features:
? In Arabic, blank-delimited words are com-
posed of zero or more prefixes, followed by a
stem and zero or more suffixes. Each prefix,
stem or suffix is a token; any contiguous se-
quence of tokens can represent a mention. Sim-
ilar to the approaches described in (Florian et
al., 2004) and (Zitouni et al, 2005), we decided
to ?condition? the output of the system on the
segmented data: the text is segmented first into
tokens and classification is then performed on
tokens. The segmentation model is similar to
the one presented by (Lee et al, 2003) and ob-
tains an accuracy of 98%.
? In Chinese text, unlike in Indo-European lan-
guages, words neither are white-space delim-
ited nor do they have capitalization markers.
Instead of a word-based model, we build a
character-based one, since word segmentation
errors can lead to irrecoverable mention detec-
tion errors; Jing et al (2003) also observes that
character-based models are better performing
than word-based ones. Word segmentation in-
formation is still useful and is integrated as an
additional feature stream.
? In English and in Spanish mention detection
systems are similar to those described in (Flo-
rian et al, 2004) where words are the tokens to
classify.
6In fact their example of label bias can be trivially solved
by allowing the classifier to examine features for subsequent
words.
7The ACE data has the nice property of being consistent in
annotations across these languages.
4 Cross-Language Mention Propagation
The approach proposed in this article requires a
mention detection system build in a resource-rich
language, and a translation from the source lan-
guage to the resource-rich language, together with
word alignment. This assumption is realistic: while
truly parallel data (humanly created) might be in
short supply or harder to acquire, adapting statis-
tical machine translation (SMT) systems from one
language-pair to another is not as challenging as it
used to be (Al-Onaizan and Papineni, 2006). We
also find that there is a large number of parallel
corpora available these days which cover many lan-
guage pairs. For example, for the European Union?s
23 official languages we find 253 language pairs;
each document in one language might have to be
translated in all other 22 languages. This is in ad-
dition to parallel corpora one could get from books,
including religious texts such as the Bible, that are
translated to a large number of languages. On the
other hand, even though mention detection system
is important for many natural language processing
applications, we still find lack of mention-annotated
corpora in many languages. In the approach we pro-
pose below, the annotated corpus used to train the
mention detection classifier does not have to be part
of a parallel corpus.
To start the process, we first use a SMT system
to translate the source unit (document or sentence)
xN1 into the resource-rich language, yielding the se-
quence ?M1 = (?1, ?2, . . . ?M ). Taking the sequence
of tokens ?M1 as input, the MaxEnt classifier assigns
a mention label to each token, building the label se-
quence ?M1 = (?1, ?2 . . . ?M ). Using the SMT-
produced word alignment between source text xN1
and translated text ?M1 (Koehn, 2004),we propagate
the target labels ?M1 to the source language build-
ing the label sequence y?N1 = (y?1, y?2 . . . y?N ).8 As
an example, if a sequence of tokens in the resource-
rich language ?i?i+1?i+2 is aligned to xjxj+1 in the
source language and if ?i?i+1?i+2 is tagged as a lo-
cation mention, then the sequence xjxj+1 can be la-
beled as a location mention: B-LOC, I-LOC. Hence,
each token xi in xN1 is tagged with a corresponding
propagated label y?i in y?N1 , y?i = ?
(
i, A, ?M1
)
, where
A is the alignment between the source and resource-
rich languages. In cases when the alignment is 1-
to-1 the function becomes the identity, but one can
imagine different scenarios which can be used in
8Or by using Giza++ if your favorite engine does not give
you word alignment.
603
 El soldado nepal?s fue baleado              por ex soldados haitianos cuando patrullaba la zona central de Haiti , inform? Minustah .
The Nepalese soldier was gunned down by former Haitian soldiers when patrullaba  the central area of Haiti , reported minustah .
GPELOCPERGPE
GPEPER ORGGPELOCGPEPER
PERGPE
Figure 1: Word alignment for a Spanish sentence and its English machine-translation. The mention labels shown are
the gold-standard ones for Spanish and the automatically detected ones for English. If mentions were to be propagated
from English to Spanish, the last mention would be a miss, due to the fact that the English mention detection failed to
identify ?minustah? as an organization.
many-to-many alignment cases. The alignement we
use in this paper is 1-to-many ({1...n}) from the
source language (eg., Arabic) to the resource-rich
language (e.g., English). Once we use SMT word
alignment to propagate label sequence ?M1 of ?M1 to
the corresponding text xN1 in the target language, we
end up with a sequence of labels y?N1 where for each
token xi in xN1 we attach its label y?i in y?N1 . Hence,
we label te entire span and if the strategy results in
two mentions where one contains the other, we elim-
inate the inner one.
Figure 1 displays the alignment between a Span-
ish sentence and its English automatic translation. It
also shows a good match between the gold-standard
tags in Spanish and the automatically extracted tags
in English.
There are three ways in which we propose using
these propagated labels:
1. Consider y?N1 as the result of propagating the
detected mentions in the original text xN1 , basi-
cally selecting yN1 = y?N1 . This situation corre-
sponds to a case where no resources (annotated
data) are available/needed on the source side,
where the propagated labels are the output of
the system.
2. Use the label sequence y?N1 as an additional fea-
ture in the MaxEnt framework when predicting
P
(
yj|xN1 , yj?1j?k
)
, together with other features
built from resources available on the source
language. We will call this model CDP (Con-
text Dependent Propagation).
3. Starting with a large corpus (possibly including
the training data), translate it into the resource-
rich language and run mention detection. Then
select the word sequences in the source lan-
guage associated with the found mentions in
the translation and add them to a machine-
generated gazetteer G9. This gazetteer G is then
used to construct features for classification. We
will call this model CIP (Context Independent
Propagation).
From a runtime point of view, the CIP method has
the advantage that there is no need to perform ma-
chine translation, and it can incorporate data from a
very large amount of text. The CDP method, on the
other hand, has the advantage that features are com-
puted in context, and will not fire unless the corre-
sponding mentions were found in the translated ver-
sion (hence the name). Of course, the CDP method
can incorporate features generated in the dictionary
G. The experimental section analyzes the impact of
each of these techniques on mention detection task
performance.
5 Resources
Experiments are conducted on the ACE 2007 data
sets10, in four languages: Arabic, Chinese, English,
and Spanish. This data is selected from a variety
of sources (broadcast news, broadcast conversations,
newswire, web log, newswire, conversational tele-
phony) and is labeled with 7 types: person, organi-
zation, location, facility, GPE (geo-political entity),
vehicle and weapon. Besides mention level informa-
tion, also labeled are coreference between the men-
tions, relations, events, and time resolution.
Since the evaluation tests set are not publicly
available, we have split the publicly available train-
ing corpus into an 85%/15% data split. To facilitate
future comparisons with work presented here, and
to simulate a realistic scenario, the splits are created
based on article dates: the test data is selected as the
latest 15% of the data in chronological order, in each
of the covered genres. This way, the documents in
9This is in fact a way to automatically construct a source-
side mention dictionary.
10Same data as for ACE 2008.
604
Language Training Test
Arabic 323 56
Chinese 538 95
English 499 100
Spanish 467 52
Table 1: Datasets size (number of documents)
the training and test data sets do not overlap in time,
and the content of the test data is more recent than
the training data. Table 1 presents the number of
documents in the training/test datasets for each of
the four languages.
While performance on the ACE data is usually
evaluated using a special-purpose measure - the
ACE value metric (NIST, 2007), given that we are
interested in the mention detection task only, we
decided to use the more intuitive and popular (un-
weighted) F-measure, the harmonic mean of preci-
sion and recall.
6 Resource-Rich Languages
From the set of four languages in ACE 2007, we
will unsurprisingly select English as the resource-
rich language. Table 2 shows the performance of
mention detection systems in all 4 languages one
can obtain by using all available resources in that
language, including lexical (words and morphs in a
3-word window, prefixes and suffixes of length up
to 4, WordNet (Miller, 1995) for English), syntac-
tic (POS tags, text chunks), and the output of other
information extraction models.
N P R F
Arabic 3566 83.6 76.8 80.0
Chinese 4791 81.1 71.3 75.8
English 8170 84.6 80.8 82.7
Spanish 2487 79.1 73.5 76.2
Table 2: Performance of Arabic, Chinese, English and
Spanish mention detection systems. Performance is pre-
sented in terms of Precision (P), Recall (R), and F-
measure (F). The column (N) displays the number of
mentions in the test set.
Results show that the English mention detection
system has a better performance when compared to
systems dealing with other languages such as Ara-
bic, Chinese and Spanish. These results are not un-
expected since the English model has access to a
larger training data and uses richer set of informa-
tion such as WordNet (Miller, 1995) and the output
Language Pair BLEU Score
Arabic-English 0.55
Chinese-English 0.32
Spanish-English 0.55
Table 3: BLEU performance of the SMT systems on the
3 language pairs
of a larger set of information extraction models.
7 Experiments
To show the effectiveness of cross-language mention
propagation information in improving mention de-
tection system performance in Arabic, Chinese and
Spanish, we use three SMT systems with very com-
petitive performance in terms of BLEU11 (Papineni
et al, 2002).
To give an idea of the SMT performance, Table 3
shows the performance of the translation systems on
the three language pairs, computed on standard test
sets. The Arabic to English SMT system is similar to
the one described in (Huang and Papineni, 2007); it
has 0.55 BLEU score on NIST 2003 Arabic-English
machine translation evaluation test set. The Chi-
nese to English SMT system has similar architecture
to the one described in (Al-Onaizan and Papineni,
2006). This system obtains a score of 0.32 cased
BLUE on NIST 2003 Arabic-English machine trans-
lation evaluation test set. The Spanish to English
SMT system is similar to the one described in (Lee et
al., 2006); it has a 0.55 BLEU score on the final text
edition of the European Parliament Plenary Speech
corpus in TC-STAR 2006 evaluation. As mentioned
earlier, these three SMT systems have very compet-
itive performance and are ranked among top 2 sys-
tems participating to NIST or TC-STAR evaluations.
Also, the English mention detection system used for
experiments has an F-measure of 82.7 and that has
very competitive results among systems participat-
ing in the ACE 2007 evaluation.
Experiments are conducted under several con-
ditions in order to investigate the effectiveness of
our approach in improving mention detection sys-
tem performance on languages with different levels
of resource availability (from simple to more com-
plex):
1. the system does not have access to any train-
ing data in the source language (no resources
11BLEU is an automatic measure for the translation quality
which makes good use of multiple reference translations.
605
needed besides the MT system);
2. the system has access to only lexical informa-
tion (information that can be directly derived
exclusively from mention-labeled text);
3. the system has access to lexical and syntactic
(e.g., POS tags, text chunks) information (re-
quires mention-labeled text, and models to pre-
dict POS tags, etc);
4. the system that has access to lexical, syntactic,
and semantic information (requires even more
models and labeled data).
The rest of this section examines in detail these four
cases.
To measure whether the improvement in per-
formance of a particular system over another
one is statistically significant or not, we use
the stratified bootstrap re-sampling significance
test (Noreen, 1989). This approach was used in the
named entity recognition shared task of CoNNL-
2002 (http://www.cnts.ua.ac.be/conll2002/ner/,
2002). In the following tables, we add a dagger sign
? to results that are not statistically significant when
compared to the baseline results.
7.1 No Source Language Training Data
In this first case, as described in Section 4, the men-
tion labels in the source language are obtained di-
rectly through the alignment from the mentions in
the translated text. This is a very simple scenario,
which can be implemented with ease, and, as we will
see, yields reasonable performance out-of-the-box.
N P R F
Arabic 3566 52.7 49.6 51.1
Chinese 4791 66.4 52.2 58.5
Spanish 2487 63.4 63.6 63.5
Table 4: Performance of the cross-language propagation
from English mention detection system onto Arabic, Chi-
nese and Spanish texts. Performance is presented in terms
of Precision (P), Recall (R), and F-measure (F). The col-
umn (N) shows the number of mentions in the test set.
Experimental results presented in Table 4 show
the performance of applying this information trans-
fer approach. For each source language (Arabic,
Chinese, or Arabic), we show the performance of
propagating mentions from the English text. Even
though no training data to build a source language
mention classifier is available, we still can detect
mentions with reasonably high accuracy. We con-
sider the obtained accuracy as reasonably good be-
cause, as an example, the performance of a sys-
tem that attaches to every word its most frequent
label (unigram) is around 25% F-measure on Ara-
bic. Results in Table 4 also show that even though
the Chinese-to-English SMT system is lower in term
of BLEU than the Arbic-to-English SMT system
(0.32 vs. 0.55), performance of the cross-language
propagation from English mention detection system
onto Chinese is better than the performance of the
propagation from English mention detection system
onto Arabic. One reason for this is that we notice
that Chinese-to-English SMT system translates and
aligns ACE categories better than Arabic-to-English
SMT system.
7.2 Lexical Resources
In this section, we consider the case when we have
available training data in the source language to be
able to train a statistical classifier. We also consider
that the classifier has access to lexical information
only. Our goal here is to study the effectiveness of
adding cross-language mention propagation infor-
mation to improve mention detection performance
on languages with limited resources.
Table 5 shows the performance of the 3 languages
with and without cross-language mention propaga-
tion information from English, with the 3 propa-
gation methods described in Section 4. One can
see that propagating mention propagation informa-
tion results in system performance increase12. When
systems use the CIP method, no improvement can
be observed on Arabic and Chinese, while a small
improvement of 0.5F point is obtained on Spanish
(74.5 vs. 75.0). In contrast, when systems use the
CDP method an improvement is obtained in recall
? which is to be expected, given the method ? lead-
ing to systems with better performance in terms of
F-measure: 1.6F points improvement for Arabic,
1.5F points improvement for Chinese and almost 3F
points improvement for Spanish. The results for all
the CDP transfers and the CIP for Spanish are statis-
tically significant.
7.3 Lexical and Syntactic Resources
We represent in Table 6 mention detection system
performance when syntactic resources are available
in the source language, in addition to lexical re-
12Only systems? performance marked with ? is not statisti-
cally significantly better.
606
Baseline CIP CDP
N P R F P R F P R F
Arabic: 3566 81.8 71.7 76.4 82.2 71.3 76.4? 82.6 73.9 78.0
Chinese: 4791 79.3 70.2 74.5 79.4 70.5 74.7? 79.8 72.5 76.0
Spanish: 2478 79.1 70.4 74.5 79.7 70.8 75.0 80.4 74.6 77.4
Table 5: Performance of Arabic, Chinese and Spanish mention detection using lexical features (?Baseline? column).
Columns ?CIP? stands for systems that add cross-language context independent mention propagation information and
column ?CDP? is for systems that add cross-language context dependent mention propagation information.
Baseline CIP CDP
N P R F P R F P R F
Arabic: 3566 82.2 72.6 77.1 82.7 72.9 77.5 83.2 74.5 78.6
Chinese: 4791 80.0 71.3 75.5 79.9 71.5 75.5? 81.0 72.4 76.5
Spanish: 2487 79.1 71.2 74.9 79.9 71.9 75.7 80.7 74.6 77.5
Table 6: Performance of Arabic, Chinese and Spanish mention detection using lexical and syntactic features (POS
tags, chunk information, etc).
sources available in the previous Subsection. This
experiment is important because it tests the effec-
tiveness of the propagation approach in improving
performance on languages with a typical level of re-
sources.
Results show that even in this situation, the use
of cross language mention propagation informa-
tion still lead to considerable improvement: using
the CDP transfer method yields improvements from
1.1F in Chinese to 2.6F in Spanish. Similar to the
previous section, the use of CIP information did not
improve performance significantly on Arabic (77.5
vs. 77.1) and Chinese (75.5 vs. 75.5) systems, but
we notice an improvement in Spanish13.
7.4 Lexical, Syntactic and Semantic Resources
This final section investigates whether the access
to cross-language mention propagation information
can still improve the performance of existing com-
petitive mention detection systems trained on lan-
guages with large resources. In this case, systems
have access to a full array of lexical, syntax, seman-
tic information, including the output from other in-
formation extraction models. Table 7 presents the
performance of mention detection systems on the
three languages, in the familiar 3 propagation meth-
ods: again, results show that better performance
is obtained when cross language mention informa-
tion is used. Under CIP, almost no change in terms
of performance is obtained for Arabic and Span-
13The dagger sign ? marks the systems that are not statisti-
cally significantly better.
ish, though a slight improvement can be observed
for Chinese (76.9F vs. 75.8F). When CDP is used
the performance of mention detection systems is im-
proved by 0.9F for Arabic (80.9 vs. 80.0), 2.3F
for Chinese (78.1F vs. 75.8F) and 1.9F for Span-
ish (78.1 vs. 76.2F). Once again, the results prove
that the use of cross language mention propagation
information, especially through CDP, is effective in
improving the performance even in this case.
By comparing results across tables, one can note
that systems having access to only lexical and cross
language mention propagation information are as ef-
fective as systems having access to large set of in-
formation. For Chinese, we obtain a performance of
75.8F when the system has access to lexical, syntac-
tic and output of other information extraction mod-
els. On the other hand, the same system has a
slightly better performance of 76.0 when it has ac-
cess to lexical and cross language mention propa-
gation information. The same behavior is observed
for Spanish, we obtain a performance of 76.2F when
the system has access to lexical, syntactic and output
of other information extraction models; compared to
77.4F when lexical and cross language mention in-
formation are used. This is not true for Arabic where
having access to larger set of information led to bet-
ter performance when compared to systems having
access to lexical information and CDP information
(80.0F vs. 78.0). We attribute this difference to
the fact that in Arabic we use the output of larger
number of information extraction models, and con-
sequently a richer set of information.
607
Baseline CIP CDP
N P R F P R F P R F
Arabic: 3566 83.6 76.8 80.0 83.9 77.0 80.2? 84.2 77.8 80.9
Chinese: 4791 81.1 71.3 75.8 81.4 73.0 76.9 81.7 74.8 78.1
Spanish: 2487 79.1 73.5 76.2 79.3 73.4 76.2? 80.1 76.2 78.1
Table 7: Performance of Arabic, Chinese and Spanish mention detection using lexical, syntactic and output of other
information extraction models: full-blown systems.
The other observation that is worth making is that
the improvement in performance has a decreasing
tendency as more resources are available. The per-
formance gain for CDP in Arabic goes from 1.6 to
1.5 to 0.9, and the one on Spanish goes from 2.9 to
2.6 to 1.9. The one on Chinese follows part of this
trend, as it goes from 1.4 to 1.1 to 2.3. While the
evidence here is not definitive, one can indeed note
the reduced effectiveness of the method as more re-
sources are available, which was indeed what we ex-
pected.
Results obtained by all these experiments help
answer an important question: when trying to im-
prove mention detection systems in a resource-poor
language, should we invest in building resources or
should we use propagation from a resource-rich lan-
guage to (at least) bootstrap the process? The answer
seems to be the latter.
8 Conclusion
This paper presents a new approach to mention de-
tection in low, medium or high-resource languages,
which benefits from projecting the output from a
resource-rich language such as English. We show
that even when no training data is available in one
source language, we can still build a decently per-
forming baseline mention detection system by only
using resources from English. This approach re-
quires a mention detection system on a resource-
rich language and an SMT system that translate text
from the source to the resource-rich language, both
of which can be attained.
In cases when large resources are available in the
source language, our cross language mention propa-
gation technique is still able to further improve men-
tion detection system performance. Experiments
performed on the four languages of ACE 2007, with
English chosen as the resource-rich language, show
consistent and significant improvements across con-
ditions and levels of linguistic sophistication. The
experiments are conducted on clearly specified par-
titions of the ACE 2007 data set, so future compar-
isons against the presented work can be correctly
and accurately made. We also note that systems
that have access to lexical and cross language men-
tion propagation information are as accurate as those
that have access to lexical, syntactic and output of
other information extraction models in the source
language (but no cross-language resources). As fu-
ture work, we plan to extend this work to use semi-
supervised and unsupervised approaches that can
make use of cross-language information propaga-
tion.
We believe that it is important for the research
community to continue to invest in building better
resources in ?source? languages, as it looks the most
promising approach. However, using a propagation
approach can definitely help bootstrap the process.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-2-0001 under the GALE program.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 529?536, Sydney, Australia, July. Association
for Computational Linguistics.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
C. Cabezas, B. Dorr, and P. Resnik. 2001. Spanish lan-
guage processing at university of maryland: Building
infrastructure for multilingual applications. In Pro-
ceedings of the 2nd International Workshop on Span-
ish Language Processing and Language Technologies.
Stanley Chen and Ronald Rosenfeld. 2000. A survey of
smoothing techniques for me models. IEEE Trans. on
Speech and Audio Processing.
I. Dagan and A. Itai. 1994. Word sense disambiguation
using a second language monolingual corpus. Compu-
tational Linguistics, 20(4):563?596.
608
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Meet-
ing of the Association for Computational Linguistics,
pages 130?137.
Mona Diab and Philip Resnik. 2001. An unsupervised
method for word sense tagging using parallel corpora.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
255?262, Morristown, NJ, USA. Association for Com-
putational Linguistics.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics:
HLT-NAACL 2004, pages 1?8.
W. Gale, K. Church, and D. Yarowsky. 1992. A method
for disambiguating word senses in a large corpus.
Computers and the Humanities, 26:415?439.
Joshua Goodman. 2002. Sequential conditional general-
ized iterative scaling. In Proceedings of ACL?02.
Gregory Grefenstette. 1998. Cross-Language Informa-
tion Retrieval, volume 079238122X. Kluwer Aca-
demic Publishers.
http://www.cnts.ua.ac.be/conll2002/ner/. 2002.
Fei Huang and Kishore Papineni. 2007. Hierarchi-
cal system combination for machine translation. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 277?286.
Rebecca Hwa, Philip Resnik, and Amy Weinberg. 2002.
Breaking the resource bottleneck for multilingual pars-
ing. In Proceedings of the Workshop on Linguis-
tic Knowledge Acquisition and Representation: Boot-
strapping Annotated Language Data.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. Itty-
cheriah. 2003. HowtogetaChineseName(Entity): Seg-
mentation and combination issues. In Proceedings of
EMNLP?03, pages 200?207.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA?04, Washington
DC, September-October.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In Proceedings of the ACL?03, pages 399?
406.
Young-Suk Lee, Yaser Al-Onaizan, Kishore Papineni,
and Salim Roukos. 2006. Ibm spoken language trans-
lation system. In TC-STAR Workshop on Speech-to-
Speech Translation, pages 13?18, Barcelona, Spain,
June.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In ICML.
G. A. Miller. 1995. WordNet: A lexical database. Com-
munications of the ACM, 38(11).
NIST. 2007. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley Sons.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
P.F.Brown, S.A.Della Pietra, V.J. Della Pietra, and
R.L.Mercer. 1991. Word-sense disambiguation using
statistical methods. In Proceedings of ACL?91.
L. Ramshaw and M. Marcus. 1994. Exploring the sta-
tistical derivation of transformational rule sequences
for part-of-speech tagging. In Proceedings of the ACL
Workshop on Combining Symbolic and Statistical Ap-
proaches to Language, pages 128?135.
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In David Yarowsky
and Kenneth Church, editors, Proceedings of the Third
Workshop on Very Large Corpora, pages 82?94, Som-
erset, New Jersey. Association for Computational Lin-
guistics.
E. Riloff, C. Schafer, and D. Yarowsky. 2002. Inducing
information extraction systems for new languages via
cross-language projection. In Proceedings of Coling
2002, Taipei, Taiwan.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Represent-
ing text chunks. In Proceedings of EACL?99.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independentnamed entity
recognition. In Proceedings of CoNLL-2002, pages
155?158.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of HLT
2001, San Diego, California, USA.
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and Radu
Florian. 2005. The impact of morphological stem-
ming on Arabic mention detection and coreference res-
olution. In Proceedings of the ACL Workshop on Com-
putational Approaches to Semitic Languages, pages
63?70, Ann Arbor, June.
609
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 660?667, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Multi-Lingual Coreference Resolution With Syntactic Features
Xiaoqiang Luo and Imed Zitouni
1101 Kitchawan Road
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598, U.S.A.
{xiaoluo, izitouni}@us.ibm.com
Abstract
In this paper, we study the impact of a
group of features extracted automatically from
machine-generated parse trees on coreference
resolution. One focus is on designing syn-
tactic features using the binding theory as the
guideline to improve pronoun resolution, al-
though linguistic phenomenon such as apposi-
tion is also modeled. These features are ap-
plied to the Arabic, Chinese and English coref-
erence resolution systems and their effective-
ness is evaluated on data from the Automatic
Content Extraction (ACE) task. The syntactic
features improve the Arabic and English sys-
tems significantly, but play a limited role in the
Chinese one. Detailed analyses are done to un-
derstand the syntactic features? impact on the
three coreference systems.
1 Introduction
A coreference resolution system aims to group together
mentions referring to the same entity, where a mention is
an instance of reference to an object, and the collection of
mentions referring to the same object in a document form
an entity. In the following example:
(I) ?John believes himself to be the best student.?
mentions are underlined. The three mentions ?John?,
?himself?, ?the best student? are of type name, pronoun 1,
and nominal, respectively. They form an entity since they
all refer to the same person.
Syntactic information plays an important role in corefer-
ence resolution. For example, the binding theory (Haege-
man, 1994; Beatrice and Kroch, 2000) provides a good
account of the constraints on the antecedent of English
pronouns. The theory relies on syntactic parse trees to de-
termine the governing category which defines the scope
1?Pronoun? in this paper refers to both anaphor and normal
pronoun.
of binding constraints. We will use the theory as a guide-
line to help us design features in a machine learning
framework.
Previous pronoun resolution work (Hobbs, 1976; Lappin
and Leass, 1994; Ge et al, 1998; Stuckardt, 2001) explic-
itly utilized syntactic information before. But there are
unique challenges in this study: (1) Syntactic informa-
tion is extracted from parse trees automatically generated.
This is possible because of the availability of statistical
parsers, which can be trained on human-annotated tree-
banks (Marcus et al, 1993; Xia et al, 2000; Maamouri
and Bies, 2004) for multiple languages; (2) The bind-
ing theory is used as a guideline and syntactic structures
are encoded as features in a maximum entropy corefer-
ence system; (3) The syntactic features are evaluated on
three languages: Arabic, Chinese and English (one goal
is to see if features motivated by the English language can
help coreference resolution in other languages). All con-
trastive experiments are done on publicly-available data;
(4) Our coreference system resolves coreferential rela-
tionships among all the annotated mentions, not just for
pronouns.
Using machine-generated parse trees eliminates the need
of hand-labeled trees in a coreference system. How-
ever, it is a major challenge to extract useful informa-
tion from these noisy parse trees. Our approach is encod-
ing the structures contained in a parse tree into a set of
computable features, each of which is associated with a
weight automatically determined by a machine learning
algorithm. This contrasts with the approach of extracting
rules and assigning weights to these rules by hand (Lap-
pin and Leass, 1994; Stuckardt, 2001). The advantage
of our approach is robustness: if a particular structure is
helpful, it will be assigned a high weight; if a feature is
extracted from a highly noisy parse tree and is not in-
formative in coreference resolution, it will be assigned
a small weight. By avoiding writing rules, we automati-
cally incorporate useful information into our model and at
the same time limit the potentially negative impact from
noisy parsing output.
660
2 Statistical Coreference Resolution Model
Our coreference system uses a binary entity-mention
model PL(?|e, m) (henceforth ?link model?) to score the
action of linking a mention m to an entity e. In our im-
plementation, the link model is computed as
PL(L = 1|e, m) ? max
m??e
P?L(L = 1|e, m?, m), (1)
where m? is one mention in entity e, and the basic model
building block P?L(L = 1|e, m?, m) is an exponential or
maximum entropy model (Berger et al, 1996):
P?L(L|e, m?, m) =
exp
{
?
i ?igi(e, m?, m, L)
}
Z(e, m?, m) , (2)
where Z(e, m?, m) is a normalizing factor to ensure that
P?L(?|e, m?, m) is a probability, {gi(e, m?, m, L)} are fea-
tures and {?i} are feature weights.
Another start model is used to score the action of creating
a new entity with the current mention m. Since starting
a new entity depends on all the partial entities created in
the history {ei}ti=1, we use the following approximation:
PS(S = 1|e1, e2, ? ? ? , et, m) ?
1 ? max
1?i?t
PL(L = 1|ei, m) (3)
In the maximum-entropy model (2), feature (typically bi-
nary) functions {gi(e, m?, m, ?)} provide us with a flex-
ible framework to encode useful information into the
the system: it can be as simple as ?gi(e, m?, m, L =
1) = 1 if m? and m have the same surface string,? or
?gj(e, m?, m, L = 0) = 1 if e and m differ in num-
ber,? or as complex as ?gl(e, m?, m, L = 1) = 1 if m?
c-commands m and m? is a NAME mention and m is a
pronoun mention.? These feature functions bear similar-
ity to rules used in other coreference systems (Lappin and
Leass, 1994; Mitkov, 1998; Stuckardt, 2001), except that
the feature weights {?i} are automatically trained over a
corpus with coreference information. Learning feature
weights automatically eliminates the need of manually
assigning the weights or precedence of rules, and opens
the door for us to explore rich features extracted from
parse trees, which is discussed in the next section.
3 Syntactic Features
In this section, we present a set of features extracted
from syntactic parse trees. We discuss how we approx-
imately compute linguistic concepts such as governing
category (Haegeman, 1994), apposition and dependency
relationships from noisy syntactic parse trees. While
parsing and parse trees depend on the target language,
the automatic nature of feature extraction from parse trees
makes the process language-independent.
V
(1)
(GC)
(Sub) (gov)
likes
NP1 NP2
S
John
VP
himself.
VP
V
John likes
NP1 NP2
S (GC)
(Sub)
(2)
(gov)
him.
V
VP
description
NP5
Miss Smith?sbelievesJohn
S
(gov)
(Sub)
(GC)
(3)
NP1
NP2
NP6
NP3
of herself.
NP4P
PP
Figure 1: GC examples.
3.1 Features Inspired by Binding Theory
The binding theory (Haegeman, 1994) concerning pro-
nouns can be summarized with the following principles:
1. A reflexive or reciprocal pronoun (e.g., ?herself? or
?each other?) must be bound in its governing cate-
gory (GC).
2. A normal pronoun must be free in its governing cat-
egory.
The first principle states that the antecedent of a reflexive
or reciprocal pronoun is within its GC, while the second
principle says that the antecedent of a normal pronoun is
outside its GC. While the two principles are simple, they
all rely on the concept of governing category, which is
defined as the minimal domain containing the pronoun in
question, its governor, and an accessible subject.
The concept GC can best be explained with a few exam-
ples in Figure 1, where the label of a head constituent
is marked within a box, and GC, accessible subject, and
governor constituents are marked in parentheses with
?GC?, ?Sub? and ?gov.? Noun-phrases (NP) are num-
bered for the convenience of referencing. For example,
in sub-figure (1) of Figure 1, the governor of ?himself?
is ?likes,? the subject is ?John,? hence the GC is the en-
tire sentence spanned by the root ?S.? Since ?himself?
is reflexive, its antecedent must be ?John? by Principle
1. The parse tree in sub-figure (2) is the same as that
in sub-figure (1), but since ?him? is a normal pronoun,
its antecedent, according to Principle 2, has to be out-
side the GC, that is, ?him? cannot be coreferenced with
?John.?. Sentence in sub-figure (3) is slightly more com-
plicated: the governor of ?herself? is ?description,? and
the accessible subject is ?Miss Smith.? Thus, the govern-
ing category is NP6. The first principle implies that the
antecedent of ?herself? must be ?Miss Smith.?
It is clear from these examples that GC is very useful
in finding the antecedent of a pronoun. But the last ex-
ample shows that determining GC is not a trivial matter.
Not only is the correct parse tree required, but extra in-
formation is also needed to identify the head governor
661
and the minimal constituent dominating the pronoun, its
governor and an accessible subject. Determining the ac-
cessible subject itself entails checking other constraints
such as number and gender agreement. The complexity
of computing governing category, compounded with the
noisy nature of machine-generated parse tree, prompts us
to compute a set of features that characterize the struc-
tural relationship between a candidate mention and a pro-
noun, as opposed to explicitly identify GC in a parse tree.
These features are designed to implicitly model the bind-
ing constraints.
Given a candidate antecedent or mention m1 and a pro-
noun mention m2 within a parsed sentence, we first test
if they have c-command relation, and then a set of count-
ing features are computed. The features are detailed as
follows:
(1) C-command ccmd(m1, m2) : A constituent X c-
commands another constituent Y in a parse tree if the first
branching node dominating X also dominates Y . The bi-
nary feature ccmd(m1, m2) is true if the minimum NP
dominating m1 c-commands the minimum NP dominat-
ing m2. In sub-figure (1) of Figure 1, NP1 c-commands
NP2 since the first branching node dominating NP1 is S
and it dominates NP2.
If ccmd(m1, m2) is true, we then define the c-command
path T (m1, m2) as the path from the minimum NP dom-
inating m2 to the first branching node that dominates the
minimum NP dominating m1. In sub-figure (1) of Fig-
ure 1, the c-command path T (?John?, ?himself?) would
be ?NP2-VP-S.?
(2) NP count(m1, m2): If ccmd(m1, m2) is true,
then NP count(m1, m2) counts how many NPs are
seen on the c-command path T (m1, m2), exclud-
ing two endpoints. In sub-figure (1) of Figure 1,
NP count(?John?, ?himself?) = 0 since there is no NP
on T (?John?, ?himself?).
(3) V P count(m1, m2): similar to NP count(m1, m2),
except that this feature counts how many verb phrases
(VP) are seen on the c-command path. In sub-figure (1)
of Figure 1, V P count(?John?, ?himself?) is true since
there is one VP on T (?John?, ?himself?).
(4) S count(m1, m2): This feature counts how many
clauses are seen on the c-command path when
ccmd(m1, m2) is true. In sub-figure (1) of Figure 1,
S count(?John?, ?himself?) = 0 since there is no clause
label on T (?John?, ?himself?).
These features are designed to capture information in the
concept of governing category when used in conjunction
with attributes (e.g., gender, number, reflexiveness) of in-
dividual pronouns. Counting the intermediate NPs, VPs
and sub-clauses implicitly characterizes the governor of
a pronoun in question; the presence or absence of a sub-
clause indicates whethere or not a coreferential relation is
across clause boundary.
3.2 Dependency Features
In addition to features inspired by the binding theory, a
set of dependency features are also computed with the
help of syntactic parse trees. This is motivated by exam-
ples such as ?John is the president of ABC Corporation,?
where ?John? and ?the president? refer to the same per-
son and should be in the same entity. In scenarios like
this, lexical features do not help, while the knowledge
that ?John? left-modifies the verb ?is? and the ?the presi-
dent? right-modifies the same verb would be useful.
Given two mentions m1 and m2 in a sentence, we com-
pute the following dependency features:
(1)same head(m1, m2): The feature compares the bi-
lexical dependencies ?m1, h(m1)?, and ?m2, h(m2)?,
where h(x) is the head word which x modifies. The fea-
ture is active only if h(m1) = h(m2), in which case it
returns h(m1).
(2)same POS(m1, m2): To get good coverage of de-
pendencies, we compute a feature same POS(m1, m2),
which examines the same dependency as in (1) and
returns the common head part-of-speech (POS) tag if
h(m1) = h(m2).
The head child nodes are marked with boxes in
Figure 1. For the parse tree in sub-figure (1),
same head(?John?, ?him?) would return ?likes? as
?John? left-modifies ?likes? while ?him? right-modifies
?likes,? and same POS(?John?, ?him?) would return
?V? as the POS tag of ?likes? is ?V.?
(3) mod(m1, m2): the binary feature is true if m1
modifies m2. For parse tree (2) of Figure 1,
mod(?John?, ?him?) returns false as ?John? does not
modify ?him? directly. A reverse order feature
mod(m2, m1) is computed too.
(4) same head2(m1, m2): this set of features examine
second-level dependency. It compares the head word of
h(m1), or h(h(m1)), with h(m2) and returns the com-
mon head if h(h(m1)) = h(m2). A reverse order feature
same head2(m2, m1) is also computed.
(5) same POS2(m1, m2): similar to (4), except that it
computes the second-level POS. A reverse order feature
same POS2(m2, m1) is computed too.
(6) same head22(m1, m2): it returns the common
second-level head if h(h(m1)) = h(h(m2)).
3.3 Apposition and Same-Parent Features
Apposition is a phenomenon where two adjacent NPs re-
fer to the same entity, as ?Jimmy Carter? and ?the former
president? in the following example:
(II) ?Jimmy Carter, the former president of US, is visit-
ing Europe.?
Note that not all NPs separated by a comma are neces-
sarily appositive. For example, in ?John called Al, Bob,
and Charlie last night,? ?Al? and ?Bob? share a same NP
662
parent and are separated by comma, but they are not ap-
positive.
To compute the apposition feature appos(m1, m2) for
mention-pair (m1, m2), we first determine the minimum
dominating NP of m1 and m2. The minimum dominating
NP of a mention is the lowest NP, with an optional modi-
fying phrase or clause, that spans the mention. If the two
minimum dominating NPs have the same parent NP, and
they are the only two NP children of the parent, the value
of appos(m1, m2) is true. This would exclude ?Al? and
?Bob? in ?John called Al, Bob, and Charlie last night?
from being computed as apposition.
We also implement a feature same parent(m1, m2)
which tests if two mentions m1 and m2 are dominated
by a common NP. The feature helps to prevent the system
from linking ?his? with ?colleague? in the sentence ?John
called his colleague.?
All the features described in Section 3.1-3.3 are com-
puted from syntactic trees generated by a parser. While
the parser is language dependent, feature computation
boils down to encoding the structural relationship of two
mentions, which is language independent. To test the ef-
fectiveness of the syntactic features, we integrate them
into 3 coreference systems processing Arabic, Chinese
and English.
4 Experimental Results
4.1 Data and System Description
All experiments are done on true mentions of the
ACE (NIST, 2004) 2004 data. We reserve part of LDC-
released 2004 data as the development-test set (hence-
forth ?devtest?) as follows: documents are sorted by their
date and time within each data source (e.g., broadcast
news (bnews) and news wire (nwire) are two different
sources) and the last 25% documents of each data source
are reserved as the devtest set. Splitting data on chrono-
logical order simulates the process of a system?s devel-
opment and deployment in the real world. The devtest
set statistics of three languages (Arabic, Chinese and
English) is summarized in Table 1, where the number
of documents, mentions and entities is shown on row 2
through 4, respectively. The rest of 2004 ACE data to-
gether with earlier ACE data is used as training.
Arabic Chinese English
#-docs 178 166 114
#-mentions 11358 8524 7008
#-entities 4428 3876 2929
Table 1: Devtest Set Statistics by Language
The official 2004 evaluation test set is used as the blind
test set on which we run our system once after the system
development is finished. We will report summary results
on this test set.
As for parser, we train three off-shelf maximum-entropy
parsers (Ratnaparkhi, 1999) using the Arabic, Chinese
and English Penn treebank (Maamouri and Bies, 2004;
Xia et al, 2000; Marcus et al, 1993). Arabic words
are segmented while the Chinese parser is a character-
based parser. The three parsers have a label F-measure
of 77%, 80%, and 86% on their respective test sets. The
three parsers are used to parse both ACE training and test
data. Features described in Section 3 are computed from
machine-generated parse trees.
Apart from features extracted from parse trees, our coref-
erence system also utilizes other features such as lex-
ical features (e.g., string matching), distance features
characterized as quantized word and sentence distances,
mention- and entity-level attribute information (e.g, ACE
distinguishes 4 types of mentions: NAM(e), NOM(inal),
PRE(modifier) and PRO(noun)) found in the 2004 ACE
data. Details of these features can be found in (Luo et
al., 2004).
4.2 Performance Metrics
The official performance metric in the ACE task is ACE-
Value (NIST, 2004). The ACE-Value is an entity-based
metric computed by subtracting a normalized cost from
1 (so it is unbounded below). The cost of a system is
a weighted sum of costs associated with entity misses,
false alarms and errors. This cost is normalized against
the cost of a nominal system that outputs no entity. A
perfect coreference system gets 100% ACE-Value while
a system outputting many false-alarm entities could get a
negative value.
The default weights in ACE-Value emphasize names, and
severely discount pronouns: the relative importance of a
pronoun is two orders of magnitude less than that of a
name. So the ACE-Value will not be able to accurately re-
flect a system?s improvement on pronouns2. For this rea-
son, we compute an unweighted entity-constrained men-
tion F-measure (Luo, 2005) and report all contrastive
experiments with this metric. The F-measure is com-
puted by first aligning system and reference entities such
that the number of common mentions is maximized
and each system entity is constrained to align with at
most one reference entity, and vice versa. For exam-
ple, suppose that a reference document contains three
entities: {[m1], [m2, m3], [m4]} while a system outputs
four entities: {[m1, m2], [m3], [m5], [m6]}, where {mi :
i = 1, 2, ? ? ? , 6} are mentions, then the best alignment
from reference to system would be [m1] ? [m1, m2],
[m2, m3] ? [m3] and other entities are not aligned. The
number of common mentions of the best alignment is 2
2Another possible choice is the MUC F-measure (Vilain et
al., 1995). But the metric has a systematic bias for systems
generating fewer entities (Bagga and Baldwin, 1998) ? see Luo
(2005). Another reason is that it cannot score single-mention
entity.
663
(i.e., m1 and m3), thus the recall is 24 and precision is
2
5 . Due to the one-to-one entity alignment constraint, theF-measure here is more stringent than the accuracy (Ge
et al, 1998; Mitkov, 1998; Kehler et al, 2004) computed
on antecedent-pronoun pairs.
4.3 Effect of Syntactic Features
We first present the contrastive experimental results on
the devtest described in sub-section 4.1.
Two coreference systems are trained for each language:
a baseline without syntactic features, and a system in-
cluding the syntactic features. The entity-constrained F-
measures with mention-type breakdown are presented in
Table 2. Rows marked with Nm contain the number of
mentions, while rows with ?base? and ?+synt? are F-
measures for the baseline and the system with the syn-
tactic features, respectively.
The syntactic features improve pronoun mentions across
three languages ? not surprising since features inspired
by the binding theory are designed to improve pronouns.
The pronoun improvement on the Arabic (from 73.2%
to 74.6%) and English (from 69.2% to 72.0%) system is
statistically significant (at above 95% confidence level),
but change on the Chinese system is not. For Arabic,
the syntactic features improve Arabic NAM, NOM and
PRE mentions, probably because Arabic pronouns are
sometimes attached to other types of mentions. For Chi-
nese and English, the syntactic features do not practically
change the systems? performance.
As will be shown in Section 4.5, the baseline systems
without syntactic features are already competitive, com-
pared with the results on the coreference evaluation track
(EDR-coref) of the ACE 2004 evaluation (NIS, 2004). So
it is nice to see that syntactic features further improve a
good baseline on Arabic and English.
Arabic
Mention Type
NAM NOM PRE PRO Total
Nm 2843 3438 1291 3786 11358
base 86.8 73.2 86.7 73.2 78.2
+synt 88.4 76.4 87.4 74.6 80.1
Chinese
Nm 4034 3696 - 794 8524
base 95.4 77.8 - 65.9 85.0
+synt 95.2 77.7 - 66.5 84.9
English
Nm 2069 2173 835 1931 7008
base 92.0 73.4 88.7 69.2 79.6
+synt 92.0 75.3 87.8 72.0 80.8
Table 2: F-measure(%) Breakdown by Mention Type:
NAM(e), NOM(inal), PRE(modifier) and PRO(noun).
Chinese data does not have the PRE type.
4.4 Error Analyses
From the results in Table 2, we know that the set of syn-
tactic features are working in the Arabic and English sys-
tem. But the results also raise some questions: Are there
interactions among the the syntactic features and other
features? Why do the syntactic features work well for
Arabic and English, but not Chinese? To answer these
questions, we look into each system and report our find-
ings in the following sections.
4.4.1 English System
Our system uses a group of distance features. One ob-
servation is that information provided by some syntactic
features (e.g., V P count(m1, m2) etc) may have over-
lapped with some of the distance features. To test if this
is the case, we take out the distance features from the En-
glish system, and then train two systems, one with the
syntactic features, one without. The results are shown
in Table 3, where numbers on the row ?b-dist? are F-
measures after removing the distance features from the
baseline, and numbers on the row ?b-dist+synt? are with
the syntactic features.
Mention Type
NAM NOM PRE PRO Total
b-dist 84.2 68.8 74.6 63.3 72.5
b-dist+synt 90.7 74.2 87.8 69.0 79.3
Table 3: Impact of Syntactic Features on English Sys-
tem After Taking out Distance Features. Numbers are
F-measures(%).
As can be seen, the impact of the syntactic features is
much larger when the distance features are absent in the
system: performance improves across all the four men-
tion types after adding the syntactic features, and the
overall F-measure jumps from 72.5% to 79.3%. The
PRE type gets the biggest improvement since features ex-
tracted from parse trees include apposition, same-parent
test, and dependency features, which are designed to help
mention pairs in close distance, just as in the case of PRE
mentions.
Comparing the numbers in Table 3 with the English base-
line of Table 2, we can also conclude that distance fea-
tures and syntactic features lead to about the same level
of performance when the other set of features is not
used. When the distance features are used, the syntac-
tic features further help to improve the performance of
the NOM and PRO mention type, albeit to a less degree
because of information overlap between the two sets of
features.
4.4.2 Chinese System
Results in Table 2 show that the syntactic features are not
so effective for Chinese as for Arabic and English. The
664
first thing we look into is if there is any idiosyncrasy in
the Chinese language.
In Table 4, we list the statistics collected over the training
sets of the three languages: the second row are the total
number of mentions, the third row the number of pronoun
mentions, the fourth row the number of events where the
c-command feature ccmd(m1, m2) is used, and the last
row the average number of c-command features per pro-
noun (i.e., the fourth row divided by the third row). A
pronouns event is defined as a tuple of training instance
(e, m1, m2) where m1 is a mention in entity e, and the
second mention m2 is a pronoun.
From Table 4, it is clear that Chinese pronoun distribution
is very different: pronoun mentions account for about
8.7% of the total mentions in Chinese, while 29.0% of
Arabic mentions and 25.1% of English mentions are pro-
nouns (the same disparity can be observed in the devtest
set in Table 2). This is because Chinese is a pro-drop lan-
guage (Huang, 1984): for example, in the Chinese Penn
treebank version 4, there are 4933 overt pronouns, but
5750 pro-drops! The ubiquity of pro-drops in Chinese
results in signigicantly less pronoun training events. Con-
sequently, the pronoun-related features are not trained as
well as in English and Arabic. One way to quantify this
is by looking at the average number of c-command fea-
tures on a per-pronoun basis: as shown in the last row of
Table 4, the c-command feature is seen more than twice
often in Arabic and English as in Chinese. Since low-
count features are filtered out, the sparsity of pronoun
events prevent many compound features (e.g., conjunc-
tion of syntactic and distance features) from being trained
in the Chinese system, which explains why the syntactic
features do not help Chinese pronouns.
Arabic Chinese English
#total-mentions 31706 33851 58202
#pron-mentions 9183 2941 14635
#-ccmd-event 10236 1260 13691
#ccmd/pron 1.14 0.428 0.936
Table 4: Distribution of Pronoun Mentions and Fre-
quency of c-command Features
4.4.3 Arabic System
As stated in Table 4, 29.0% of Arabic mentions are pro-
nouns, compared to a slightly lower number (25.1%) for
English. This explains the relatively high positive impact
of the syntactic features on the Arabic coreference sys-
tem, compared to English and Chinese systems. To un-
derstand how syntactic features work in the Arabic sys-
tem, we examine two examples extracted from the de-
vtest set: (1) the first example shows the negative impact
of syntactic features because of the noisy parsing output,
and (2) the second example proves the effectiveness of
the syntactic features to find the dependency between two
mentions. In both examples, the baseline system and the
system with syntactic features give different results.
Let?s consider the following sentence:
. . . A ? D???A ? ?Y??@ ?J
KQ?? @ Q.
J?K? . . .
... its-capital? Jerusalem? Israel? consider? and ...
. . . ? 	JK
Y??? ?

?Q??? @ Q? ??@ 	??J
 	J
??? 	?? @ YK
QK
 A ?J
 	?
of-the-city? the-Eastern? the-half? the-Palestininan? want? while
The English text shown above is a word-to-word trans-
lation of the Arabic text (read from right-to-left). In this
example, the parser wrongly put the nominal mention
?Y ? ?

@ (Jerusalem) and the pronominal mention? 	JK
Y?? @ (the-city) under the same constituent, which acti-vates the same parent feature. The use of the feature
same parent(?Y??

@, ? 	JK
Y?? @) leads to the two mentionsbeing put into different entities. This is because there
are many cases in the training data where two mentions
under the same parent are indeed in different entities: a
similar English example is ?John called his sister?, where
?his? and ?sister? belong to two different entities. The
same parent feature is a strong indicator of not putting
them into the same entity.
	?? + ??A g + ?
 +
	??J
? A
? 	P + ?@ + 	?A

?
? + ?
 PA
m.Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 1?6,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Classifier Combination Techniques Applied to Coreference Resolution
Smita Vemulapalli1, Xiaoqiang Luo2, John F. Pitrelli2 and Imed Zitouni2
1Center for Signal and Image Processing (CSIP) 2IBM T. J. Watson Research Center
School of ECE, Georgia Institute of Technology 1101 Kitchawan Road
Atlanta, GA 30332, USA Yorktown Heights, NY 10598, USA
smita@ece.gatech.edu {xiaoluo,pitrelli,izitouni}@us.ibm.com
Abstract
This paper examines the applicability of clas-
sifier combination approaches such as bagging
and boosting for coreference resolution. To
the best of our knowledge, this is the first ef-
fort that utilizes such techniques for corefer-
ence resolution. In this paper, we provide ex-
perimental evidence which indicates that the
accuracy of the coreference engine can po-
tentially be increased by use of bagging and
boosting methods, without any additional fea-
tures or training data. We implement and eval-
uate combination techniques at the mention,
entity and document level, and also address is-
sues like entity alignment, that are specific to
coreference resolution.
1 Introduction
Coreference resolution is the task of partitioning a
set of mentions (i.e. person, organization and loca-
tion) into entities. A mention is an instance of textual
reference to an object, which can be either named
(e.g. Barack Obama), nominal (e.g. the president) or
pronominal (e.g. he, his, it). An entity is an aggre-
gate of all the mentions (of any level) which refer to
one conceptual entity. For example, in the following
sentence:
John said Mary was his sister.
there are four mentions: John, Mary, his, and
sister.
John and his belong to the one entity since they
refer to the same person; Mary and sister both
refer to another person entity. Furthermore, John
and Mary are named mentions, sister is a nomi-
nal mention and his is a pronominal mention.
In this paper, we present a potential approach for
improving the performance of coreference resolu-
tion by using classifier combination techniques such
as bagging and boosting. To the best of our knowl-
edge, this is the first effort that utilizes classifier
combination for improving coreference resolution.
Combination methods have been applied to many
problems in natural-language processing (NLP). Ex-
amples include the ROVER system (Fiscus, 1997)
for speech recognition, the Multi-Engine Machine
Translation (MEMT) system (Jayaraman and Lavie,
2005), and part-of-speech tagging (Brill and Wu,
1998; Halteren et al, 2001). Most of these tech-
niques have shown a considerable improvement over
the performance of a single classifier and, therefore,
lead us to consider implementing such a multiple-
classifier system for coreference resolution as well.
Using classifier combination techniques one can
potentially achieve a classification accuracy that is
superior to that of the single best classifier. This
is based on the assumption that the errors made by
each of the classifiers are not identical, and there-
fore if we intelligently combine multiple classifier
outputs, we may be able to correct some of these er-
rors.
The main contributions of this paper are:
? Demonstrating the potential for improvement in
the baseline ? By implementing a system that
behaves like an oracle, we have shown that the
output of the combination of multiple classifiers
has the potential to be significantly higher in ac-
curacy than any of the individual classifiers.
? Adapting traditional bagging techniques ? Mul-
tiple classifiers, generated using bagging tech-
niques, were combined using an entity-level sum
1
rule and mention-level majority voting.
? Implementing a document-level boosting algo-
rithm ? A boosting algorithm was implemented
in which a coreference resolution classifier was
iteratively trained using a re-weighted training
set, where the reweighting was done at the doc-
ument level.
? Addressing the problem of entity alignment ?
In order to apply combination techniques to
multiple classifiers, we need to address entity-
alignment issues, explained later in this paper.
The baseline coreference system we use is sim-
ilar to the one described by Luo et al (Luo et al,
2004). In such a system, mentions are processed
sequentially, and at each step, a mention is either
linked to one of existing entities, or used to create a
new entity. At the end of this process, each possible
partition of the mentions corresponds to a unique se-
quence of link or creation actions, each of which is
scored by a statistical model. The one with the high-
est score is output as the final coreference result.
2 Classifier Combination Techniques
2.1 Bagging
One way to obtain multiple classifiers is via bagging
or bootstrap aggregating (Breiman, 1996). These
classifiers, obtained using randomly-sampled train-
ing sets, may be combined to improve classification.
We generated several classifiers by two tech-
niques. In the first technique, we randomly sample
the set of documents (training set) to generate a few
classifiers. In the second technique, we need to re-
duce the feature set and this is not done in a random
fashion. Instead, we use our understanding of the in-
dividual features and also their relation to other fea-
tures to decide which features may be dropped.
2.2 Oracle
In this paper, we refer to an oracle system which
uses knowledge of the truth. Here, truth, called the
gold standard henceforth, refers to mention detec-
tion and coreference resolution done by a human for
each document. It is possible that the gold standard
may have errors and is not perfect truth, but, as in
most NLP systems, it is considered the reference for
evaluating computer-based coreference resolution.
To understand the oracle, consider an example in
which the outputs of two classifiers for the same in-
put document are C1 and C2, as shown in Figure 1.
 
    C2-EP 
C2-EQ 
C2-ER 
C2-ES 
C1-EA 
C1-EB 
C1-EC 
C1-ED 
G-E1 
G-E2 
G-E3 
G-E4 
C1-EA 
C2-ER 
C1-ED 
C2-ES 
0.72 0.66 
1.0 0.85 
0.88 
0.78 
0.75 
Gold 
G 
Classifier C2 
File X File X File X File X 
Classifier C1 Oracle Output 
Figure 1: Working of the oracle
The number of entities in C1 and C2 may not be the
same and even in cases where they are, the number
of mentions in corresponding entities may not be the
same. In fact, even finding the corresponding entity
in the other classifier output or in the gold standard
output G is not a trivial problem and requires us to
be able to align any two classifier outputs.
The alignment between any two coreference la-
belings, say C1 and G, for a document is the best
one-to-one map (Luo, 2005) between the entities of
C1 and G. To align the entities of C1 with those of
G, under the assumption that an entity in C1 may
be aligned with at most only one entity in G and
vice versa, we need to generate a bipartite graph
between the entities of C1 and G. Now the align-
ment task is a maximum bipartite matching prob-
lem. This is solved by using the Kuhn-Munkres al-
gorithm (Kuhn, 1955; Munkres, 1957). The weights
of the edges of the graph are entity-level alignment
measures. The metric we use is a relative mea-
sure of the similarity between the two entities. To
compute the similarity metric ? (Luo, 2005) for the
entity pair (R,S), we use the formula shown in
Equation 1, where (?) represents the commonal-
ity with attribute-weighted partial scores. Attributes
are things such as (ACE) entity type, subtype, entity
class, etc.
?(R,S) = 2 |R ? S||R|+ |S| (1)
The oracle output is a combination of the entities
in C1 and C2 with the highest entity-pair alignment
measures with the entities in G.1 We can see in Fig-
ure 1 that the entity G-E1 is aligned with entities C1-
EA and C2-EP. We pick the entity with the highest
entity-pair alignment measure (highlighted in gray)
which, in this case, is C1-EA. This is repeated for
1A mention may be repeated across multiple output entities,
which is not an unwarranted advantage as the scorer insists on
one-to-one entity alignment. So if there are two entities con-
taining mention A, at most one mention A is credited and the
other will hurt the score.
2
    F-E1 
F-E2 
F-E3 
F-E4 
C2-EP 
C2-EQ 
C2-ER 
C2-ES 
C1-EA 
C1-EB 
C1-EC 
C1-ED 
0.72 0.6
1.0 0.85 
0.88 0.78 
0.75 
Full F 
File X File X File X 
Classifier C1 
             C2-ES F-E4 
C1-ED C2-EQ F-E3 
C1-EB  C2-ER F-E2 
C1-EA  C2-EP F-E1 
Entity-level 
Alignment Table 
Classifier C2 
Figure 2: Entity alignment between classifier outputs
every entity in G. The oracle output can be seen in
the right-hand side of Figure 1. This technique can
be scaled up to work for any number of classifiers.
2.3 Preliminary Combination Approaches
Imitating the oracle. Making use of the existing
framework of the oracle, we implement a combina-
tion technique that imitates the oracle except that in
this case, we do not have the gold standard. If we
have N classifiers Ci, i = 1 to N , then we replace
the gold standard by each of theN classifiers in suc-
cession, to get N outputs Combi, i = 1 to N .
The task of generating multiple classifier combi-
nation outputs that have a higher accuracy than the
original classifiers is often considered to be easier
than the task of determining the best of these out-
puts. We used the formulas in Equations 2, 3 and 4
to assign a score Si to each of the N combination
outputs Combi, and then we pick the one with the
highest score. The function Sc (which corresponds
to the function ? in Equation 1) gives the similarity
between the entities in the pair (R,S).
Si = 1N ? 1
?
j = 1 to N
j 6= i
Sc(Combi, Cj) (2)
Si = Sc(Combi, Ci) (3)
Si = 1N ? 1
?
j = 1 to N
j 6= i
Sc(Combi, Combj) (4)
Entity-level sum-rule. We implemented a basic sum-
rule at the entity level, where we generate only one
combination classifier output by aligning the entities
in the N classifiers and picking only one entity at
each level of alignment. In the oracle, the reference
for entity-alignment was the gold standard. Here,
we use the baseline/full system (generated using the
entire training and feature set) to do this. The entity-
level alignment is represented as a table in Figure 2.
Let Ai, i = 1 to M be the aligned entities in one
row of the table in Figure 2. Here, M ? N if
 
 
A A1   A2   A3   A4  ? 
B B1   B2           B4  ? 
C C1   C2   C3   C4  ? 
D        D2   D3   D4  ? 
3 
0 
1 
0 
A{m1,m2,m6} 
B{ m3} 
C{ m4,m5} 
D{m7 } 
Entity-level Alignment Table 
Mention m1 
Mention Count for m1 Output Majority Voting for mention m1 
Figure 3: Mention-level majority voting
we exclude the baseline from the combination and
M ? N + 1 if we include it. To pick one entity
out of these M entities, we use the traditional sum
rule (Tulyakov et al, 2008), shown in Equation 5, to
compute the S(Ai) for each Ai and pick the entity
with the highest S(Ai) value.
S(Ai) =
?
j = 1 to N
j 6= i
Sc(Ai, Aj) (5)
2.4 Mention-level Majority Voting
In the previous techniques, entities are either picked
or rejected as a whole but never broken down fur-
ther. In the mention-level majority voting technique,
we work at the mention level, so the entities created
after combination may be different from the entities
of all the classifiers that are being combined.
In the entity-level alignment table (shown in Fig-
ure 3), A, B, C and D refer to the entities in the base-
line system and A1, A2, ..., D4 represent the enti-
ties of the input classifiers that are aligned with each
of the baseline classifier entities. Majority voting is
done by counting the number of times a mention is
found in a set of aligned entities. So for every row
in the table, we have a mention count. The row with
the highest mention count is assigned the mention in
the output. This is repeated for each mention in the
document. In Figure 3, we are voting for the men-
tion m1, which is found to have a voting count of 3
(the majority vote) at the entity-level A and a count
of 1 at the entity-level C, so the mention is assigned
to the entity A. It is important to note that some clas-
sifier entities may not align with any baseline clas-
sifier entity as we allow only a one-to-one mapping
during alignment. Such entities will not be a part of
the alignment table. If this number is large, it may
have a considerable effect on the combination.
2.5 Document-level Boosting
Boosting techniques (Schapire, 1999) combine mul-
tiple classifiers, built iteratively and trained on
re-weighted data, to improve classification accu-
racy. Since coreference resolution is done for a
whole document, we can not split a document fur-
3
Test 
docu
ment
s
with 
perce
ntile 
< P th
resh
and F
-mea
sure 
< F th
resh
bc bn cts nw un wl
# Training documents : # Test documents 
ratio for every genre is maintained
Train Test
Train Test
Train Test
Train Test
Train Test
Docu
ment
s
to bo
ost
Train
ing S
et
Shuf
fle
Boos
ting o
f Tra
ining
 Set
Train
ing S
et
Figure 4: Document-level boosting
ther. So when we re-weight the training set, we
are actually re-weighting the documents (hence the
name document-level boosting). Figure 4 shows an
overview of this technique.
The decision of which documents to boost is
made using two thresholds: percentile threshold
Pthresh and the F-measure threshold Fthresh. Doc-
uments in the test set that are in the lowest Pthresh
percentile and that have a document F-measure less
than Fthresh will be boosted in the training set for
the next iteration. We shuffle the training set to cre-
ate some randomness and then divide it into groups
of training and test sets in a round-robin fashion such
that a predetermined ratio of the number of training
documents to the number of test documents is main-
tained. In Figure 4, the light gray regions refer to
training documents and the dark gray regions refer
to test documents. Another important consideration
is that it is difficult to achieve good coreference res-
olution performance on documents of some genres
compared to others, even if they are boosted signif-
icantly. In an iterative process, it is likely that doc-
uments of such genres will get repeatedly boosted.
Also our training set has more documents of some
genres and fewer of others. So we try to maintain, to
some extent, the ratio of documents from different
genres in the training set while splitting this training
set further into groups of training and test sets.
3 Evaluation
This section describes the general setup used to con-
duct the experiments and presents an evaluation of
the combination techniques that were implemented.
Experimental setup. The coreference resolution
system used in our experiments makes use of a Max-
imum Entropy model which has lexical, syntacti-
cal, semantic and discourse features (Luo et al,
Table 1: Statistics of ACE 2005 data
DataSet #Docs #Words #Mentions #Entities
Training 499 253771 46646 16102Test 100 45659 8178 2709Total 599 299430 54824 18811
Table 2: Accuracy of generated and baseline classifiers
Classifier Accuracy (%)
C1 ? C15 Average 77.52Highest 79.16Lowest 75.81C0 Baseline 78.53
2004). Experiments are conducted on ACE 2005
data (NIST, 2005), which consists of 599 documents
from rich and diversified sources. We reserve the
last 16% documents of each source as the test set,
and use the rest of the documents as the training set.
The ACE 2005 data split is tabulated in Table 1.
Bagging A total of 15 classifiers (C1 to C15) were
generated, 12 of which were obtained by sampling
the training set and the remaining 3 by sampling
the feature set. We also make use of the base-
line classifier C0. The accuracy of C0 to C15 has
been summarized in Table 2. The agreement be-
tween the classifiers? output was found to be in the
range of 93% to 95%. In this paper, the metric used
to compute the accuracy of the coreference resolu-
tion is the Constrained Entity-Alignment F-Measure
(CEAF) (Luo, 2005) with the entity-pair similarity
measure in Equation 1.
Oracle. To conduct the oracle experiment, we train
1 to 15 classifiers and align their output to the gold
standard. For all entities aligned with a gold entity,
we pick the one with the highest score as the output.
We measure the performance for varying number of
classifiers, and the result is plotted in Figure 5.
First, we observe a steady and significant increase
in CEAF for every additional classifier, because ad-
ditional classifiers can only improve the alignment
score. Second, we note that the oracle accuracy is
87.58% for a single input classifier C1, i.e. an abso-
lute gain of 9% compared to C0. This is because the
availability of gold entities makes it possible to re-
move many false-alarm entities. Finally, the oracle
accuracy when all 15 classifiers are used as input is
94.59%, a 16.06% absolute improvement.
This experiment helps us to understand the perfor-
mance bound of combining multiple classifiers and
the contribution of every additional classifier.
Preliminary combination approaches. While the
oracle results are encouraging, a natural question is
4
 
75
 
80
 
85
 
90
 
95
 
100
 
0
 
2
 
4
 
6
 
8
 
10
 
12
 
14
 
16
Accuracy (%)
Num
ber
 of C
lass
ifier
s
bas
elin
e
Figure 5: Oracle performance vs. number of classifiers
     
 
 
 
 
 
 
 
 
7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 
Classifier 
C1 
Classifier  
C2 
Combination 
Output Classifier  C3 
7-10 7-17 7-18 7-19 7-27 7-30 15-22 20-33 20-68 37-56 
    7-10 7-17 7-27 
Legend: 
Type I  
mentions 
Type II  
mentions 
Type III  
mentions 
Type IV  
mentions 
7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 
Baseline 
C   7-17 7-27 7-61 7-63 7-64 20-39 20-62 20-66  
Figure 6: A real example showing the working of
mention-level majority voting
how much performance gain can be attained if the
gold standard is not available. To answer this ques-
tion, we replace the gold standard with one of the
classifiers C1 to C15, and align the classifiers. This
is done in a round robin fashion as described in Sec-
tion 2.3. The best performance of this procedure is
77.93%. The sum-rule combination output had an
accuracy of 78.65% with a slightly different base-
line of 78.81%. These techniques do not yield a sta-
tistically significant increase in CEAF but this is not
surprising as C1 to C15 are highly correlated.
Mention-level majority voting. This experiment is
conducted to evaluate the mention-level majority
voting technique. The results are not statistically
better than the baseline, but they give us valuable
insight into the working of the combination tech-
nique. The example in Figure 6 shows a single
entity-alignment level for the baselineC0 and 3 clas-
sifiers C1, C2, and C3 and the combination output
by mention-level majority voting. The mentions are
denoted by the notation ?EntityID - MentionID?, for
example 7-10 is the mention with EntityID=7 and
MentionID=10. Here, we use the EntityID in the
gold file. The mentions with EntityID=7 are ?cor-
rect? i.e. they belong in this entity, and the others
are ?wrong? i.e. they do not belong in this entity.
The aligned mentions are of four types:
? Type I mentions ? These mentions have a highest
voting count of 2 or more at the same entity-level
alignment and hence appear in the output.
? Type II mentions ? These mentions have a high-
est voting count of 1. But they are present in
more than one input classifier and there is a tie
between the mention counts at different entity-
level alignments. The rule to break the tie is
that mentions are included if they are also seen
in the full system C0. As can been seen, this rule
brings in correct mentions such as 7-61, 7-63,
7-64, but it also admits 20-33,20-39 and 20-62.
In the oracle, the gold standard helps to remove
entities with false-alarm mentions, whereas the
full system output is noisy and it is not strong
enough to reliably remove undesired mentions.
? Type III mentions ? There is only one mention
20-66 which is of this type. It is selected in the
combination output since it is present in C2 and
the baseline C0, although it has been rejected as
a false-alarm in C1 and C3.
? Type IV mentions ? These false-alarm mentions
(relative to C0) are rejected in the output. As can
be seen, this correctly rejects mentions such as
15-22 and 20-68, but it also rejects correct men-
tions 7-18, 7-19 and 7-30.
In summary, the current implementation of this
technique has a limited ability to distinguish correct
mentions from wrong ones due to the noisy nature
of C0 which is used for alignment. We also observe
that mentions spread across different alignments of-
ten have low-count and they are often tied in count.
Therefore, it is important to set a minimum thresh-
old for accepting these low-count majority votes and
also investigate better tie-breaking techniques.
Document-level Boosting This experiment is con-
ducted to evaluate the document-level boosting tech-
nique. Table 3 shows the results with the ratio
of the number of training documents to the num-
ber of test documents equal to 80:20, F-measure
threshold Fthresh = 74% and percentile threshold
Pthresh = 25%. The accuracy increases by 0.7%,
relative to the baseline. Due to computational com-
plexity considerations, we used fixed values for the
parameters. Therefore, these values may be sub-
optimal and may not correspond to the best possible
increase in accuracy.
4 Related Work
A large body of literature related to statistical meth-
ods for coreference resolution is available (Ng and
Cardie, 2003; Yang et al, 2003; Ng, 2008; Poon and
5
Table 3: Results of document-level boosting
Iteration Accuracy (%)
1 78.532 78.823 79.084 78.37
Domingos, 2008; McCallum and Wellner, 2003).
Poon and Domingos (Poon and Domingos, 2008)
use an unsupervised technique based on joint infer-
ence across mentions and Markov logic as a repre-
sentation language for their system on both MUC
and ACE data. Ng (Ng, 2008) proposed a genera-
tive model for unsupervised coreference resolution
that views coreference as an EM clustering process.
In this paper, we make use of a coreference engine
similar to the one described by Luo et al (Luo et al,
2004), where a Bell tree representation and a Maxi-
mum entropy framework are used to provide a natu-
rally incremental framework for coreference resolu-
tion. To the best of our knowledge, this is the first ef-
fort that utilizes classifier combination techniques to
improve coreference resolution. Combination tech-
niques have earlier been applied to various applica-
tions including machine translation (Jayaraman and
Lavie, 2005), part-of-speech tagging (Brill and Wu,
1998) and base noun phrase identification (Sang et
al., 2000). However, the use of these techniques for
coreference resolution presents a unique set of chal-
lenges, such as the issue of entity alignment between
the multiple classifier outputs.
5 Conclusions and Future Work
In this paper, we examined and evaluated the ap-
plicability of bagging and boosting techniques to
coreference resolution. We also provided empir-
ical evidence that coreference resolution accuracy
can potentially be improved by using multiple clas-
sifiers. In future, we plan to improve (1) the entity-
alignment strategy, (2) the majority voting technique
by setting a minimum threshold for the majority-
vote and better tie-breaking, and (3) the boosting
algorithm to automatically optimize the parameters
that have been manually set in this paper. Another
possible avenue for future work would be to test
these combination techniques with other coreference
resolution systems.
Acknowledgments
The authors would like to acknowledge Ganesh N.
Ramaswamy for his guidance and support in con-
ducting the research presented in this paper.
References
L. Breiman. 1996. Bagging predictors. In Machine
Learning.
E. Brill and J. Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In Proc. of COLING.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recogniser output voting error
reduction (rover). In Proc. of ASRU.
H. V. Halteren et al 2001. Improving accuracy in
word class tagging through the combination of ma-
chine learning systems. Computational Linguistics,
27.
S. Jayaraman and A. Lavie. 2005. Multi-engine machine
translation guided by explicit word matching. In Proc.
of ACL.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistics Quarterly, 2.
X. Luo et al 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proc. of
ACL.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. of EMNLP.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proc. of IJCAI/IIWeb.
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. Journal of the Society of In-
dustrial and Applied Mathematics, 5(1).
V. Ng and C. Cardie. 2003. Bootstrapping coreference
classifiers with multiple machine learning algorithms.
In Proc. of EMNLP.
V. Ng. 2008. Unsupervised models for coreference reso-
lution. In Proc. of EMNLP.
NIST. 2005. ACE?05 evaluation. www.nist.gov/
speech/tests/ace/ace05/index.html.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with Markov Logic. In Proc.
of EMNLP.
E. F. T. K. Sang et al 2000. Applying system combi-
nation to base noun phrase identification. In Proc. of
COLING 2000.
R.E. Schapire. 1999. A brief introduction to boosting. In
Proc. of IJCAI.
S. Tulyakov et al 2008. Review of classifier combi-
nation methods. In Machine Learning in Document
Analysis and Recognition.
X. Yang et al 2003. Coreference resolution using com-
petition learning approach. In Proc. of ACL.
6
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473?480,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Factorizing Complex Models: A Case Study in Mention
Detection
Radu Florian, Hongyan Jing, Nanda Kambhatla and Imed Zitouni
IBM TJ Watson Research Center
Yorktown Heights, NY 10598
{raduf,hjing,nanda,izitouni}@us.ibm.com
Abstract
As natural language understanding re-
search advances towards deeper knowledge
modeling, the tasks become more and more
complex: we are interested in more nu-
anced word characteristics, more linguistic
properties, deeper semantic and syntactic
features. One such example, explored in
this article, is the mention detection and
recognition task in the Automatic Content
Extraction project, with the goal of iden-
tifying named, nominal or pronominal ref-
erences to real-world entities?mentions?
and labeling them with three types of in-
formation: entity type, entity subtype and
mention type. In this article, we investi-
gate three methods of assigning these re-
lated tags and compare them on several
data sets. A system based on the methods
presented in this article participated and
ranked very competitively in the ACE?04
evaluation.
1 Introduction
Information extraction is a crucial step toward un-
derstanding and processing natural language data,
its goal being to identify and categorize impor-
tant information conveyed in a discourse. Exam-
ples of information extraction tasks are identifi-
cation of the actors and the objects in written
text, the detection and classification of the rela-
tions among them, and the events they participate
in. These tasks have applications in, among other
fields, summarization, information retrieval, data
mining, question answering, and language under-
standing.
One of the basic tasks of information extraction
is the mention detection task. This task is very
similar to named entity recognition (NER), as the
objects of interest represent very similar concepts.
The main difference is that the latter will identify,
however, only named references, while mention de-
tection seeks named, nominal and pronominal ref-
erences. In this paper, we will call the identified
references mentions ? using the ACE (NIST, 2003)
nomenclature ? to differentiate them from entities
which are the real-world objects (the actual person,
location, etc) to which the mentions are referring
to1.
Historically, the goal of the NER task was to find
named references to entities and quantity refer-
ences ? time, money (MUC-6, 1995; MUC-7, 1997).
In recent years, Automatic Content Extraction
evaluation (NIST, 2003; NIST, 2004) expanded the
task to also identify nominal and pronominal refer-
ences, and to group the mentions into sets referring
to the same entity, making the task more compli-
cated, as it requires a co-reference module. The set
of identified properties has also been extended to
include the mention type of a reference (whether it
is named, nominal or pronominal), its subtype (a
more specific type dependent on the main entity
type), and its genericity (whether the entity points
to a specific entity, or a generic one2), besides the
customary main entity type. To our knowledge,
little research has been done in the natural lan-
guage processing context or otherwise on investi-
gating the specific problem of how such multiple la-
bels are best assigned. This article compares three
methods for such an assignment.
The simplest model which can be considered for
the task is to create an atomic tag by ?gluing? to-
gether the sub-task labels and considering the new
label atomic. This method transforms the prob-
lem into a regular sequence classification task, sim-
ilar to part-of-speech tagging, text chunking, and
named entity recognition tasks. We call this model
the all-in-one model. The immediate drawback
of this model is that it creates a large classifica-
tion space (the cross-product of the sub-task clas-
sification spaces) and that, during decoding, par-
tially similar classifications will compete instead of
cooperate - more details are presented in Section
3.1. Despite (or maybe due to) its relative sim-
plicity, this model obtained good results in several
instances in the past, for POS tagging in morpho-
logically rich languages (Hajic and Hladka?, 1998)
1In a pragmatic sense, entities are sets of mentions
which co-refer.
2This last attribute, genericity, depends only loosely
on local context. As such, it should be assigned while
examining all mentions in an entity, and for this reason
is beyond the scope of this article.
473
and mention detection (Jing et al, 2003; Florian
et al, 2004).
At the opposite end of classification methodol-
ogy space, one can use a cascade model, which per-
forms the sub-tasks sequentially in a predefined or-
der. Under such a model, described in Section 3.3,
the user will build separate models for each sub-
task. For instance, it could first identify the men-
tion boundaries, then assign the entity type, sub-
type, and mention level information. Such a model
has the immediate advantage of having smaller
classification spaces, with the drawback that it re-
quires a specific model invocation path.
In between the two extremes, one can use a joint
model, which models the classification space in the
same way as the all-in-one model, but where the
classifications are not atomic. This system incor-
porates information about sub-model parts, such
as whether the current word starts an entity (of
any type), or whether the word is part of a nomi-
nal mention.
The paper presents a novel contrastive analysis
of these three models, comparing them on several
datasets in three languages selected from the ACE
2003 and 2004 evaluations. The methods described
here are independent of the underlying classifiers,
and can be used with any sequence classifiers. All
experiments in this article use our in-house imple-
mentation of a maximum entropy classifier (Flo-
rian et al, 2004), which we selected because of its
flexibility of integrating arbitrary types of features.
While we agree that the particular choice of classi-
fier will undoubtedly introduce some classifier bias,
we want to point out that the described procedures
have more to do with the organization of the search
space, and will have an impact, one way or another,
on most sequence classifiers, including conditional
random field classifiers.3
The paper is organized as follows: Section 2 de-
scribes the multi-task classification problem and
prior work, Section 3.3 presents and contrasts the
three meta-classification models. Section 4 outlines
the experimental setup and the obtained results,
and Section 5 concludes the paper.
2 Multi-Task Classification
Many tasks in Natural Language Processing in-
volve labeling a word or sequence of words with
a specific property; classic examples are part-of-
speech tagging, text chunking, word sense disam-
biguation and sentiment classification. Most of the
time, the word labels are atomic labels, containing
a very specific piece of information (e.g. the word
3While not wishing to delve too deep into the issue
of label bias, we would also like to point out (as it
was done, for instance, in (Klein, 2003)) that the label
bias of MEMM classifiers can be significantly reduced
by allowing them to examine the right context of the
classification point - as we have done with our model.
is noun plural, or starts a noun phrase, etc). There
are cases, though, where the labels consist of sev-
eral related, but not entirely correlated, properties;
examples include mention detection?the task we
are interested in?, syntactic parsing with func-
tional tag assignment (besides identifying the syn-
tactic parse, also label the constituent nodes with
their functional category, as defined in the Penn
Treebank (Marcus et al, 1993)), and, to a lesser
extent, part-of-speech tagging in highly inflected
languages.4
The particular type of mention detection that we
are examining in this paper follows the ACE gen-
eral definition: each mention in the text (a refer-
ence to a real-world entity) is assigned three types
of information:5
? An entity type, describing the type of the en-
tity it points to (e.g. person, location, organi-
zation, etc)
? An entity subtype, further detailing the type
(e.g. organizations can be commercial, gov-
ernmental and non-profit, while locations can
be a nation, population center, or an interna-
tional region)
? A mention type, specifying the way the en-
tity is realized ? a mention can be named
(e.g. John Smith), nominal (e.g. professor),
or pronominal (e.g. she).
Such a problem ? where the classification consists
of several subtasks or attributes ? presents addi-
tional challenges, when compared to a standard
sequence classification task. Specifically, there are
inter-dependencies between the subtasks that need
to be modeled explicitly; predicting the tags inde-
pendently of each other will likely result in incon-
sistent classifications. For instance, in our running
example of mention detection, the subtype task is
dependent on the entity type; one could not have a
person with the subtype non-profit. On the other
hand, the mention type is relatively independent of
the entity type and/or subtype: each entity type
could be realized under any mention type and vice-
versa.
The multi-task classification problem has been
subject to investigation in the past. Caruana
et al (1997) analyzed the multi-task learning
4The goal there is to also identify word properties
such as gender, number, and case (for nouns), mood
and tense (for verbs), etc, besides the main POS tag.
The task is slightly different, though, as these proper-
ties tend to have a stronger dependency on the lexical
form of the classified word.
5There is a fourth assigned type ? a flag specifying
whether a mention is specific (i.e. it refers at a clear
entity), generic (refers to a generic type, e.g. ?the sci-
entists believe ..?), unspecified (cannot be determined
from the text), or negative (e.g. ?no person would do
this?). The classification of this type is beyond the
goal of this paper.
474
(MTL) paradigm, where individual related tasks
are trained together by sharing a common rep-
resentation of knowledge, and demonstrated that
this strategy yields better results than one-task-at-
a-time learning strategy. The authors used a back-
propagation neural network, and the paradigm was
tested on several machine learning tasks. It also
contains an excellent discussion on how and why
the MTL paradigm is superior to single-task learn-
ing. Florian and Ngai (2001) used the same multi-
task learning strategy with a transformation-based
learner to show that usually disjointly handled
tasks perform slightly better under a joint model;
the experiments there were run on POS tagging
and text chunking, Chinese word segmentation and
POS tagging. Sutton et al (2004) investigated
the multitask classification problem and used a dy-
namic conditional random fields method, a gener-
alization of linear-chain conditional random fields,
which can be viewed as a probabilistic generaliza-
tion of cascaded, weighted finite-state transducers.
The subtasks were represented in a single graphi-
cal model that explicitly modeled the sub-task de-
pendence and the uncertainty between them. The
system, evaluated on POS tagging and base-noun
phrase segmentation, improved on the sequential
learning strategy.
In a similar spirit to the approach presented in
this article, Florian (2002) considers the task of
named entity recognition as a two-step process:
the first is the identification of mention boundaries
and the second is the classification of the identified
chunks, therefore considering a label for each word
being formed from two sub-labels: one that spec-
ifies the position of the current word relative in a
mention (outside any mentions, starts a mention, is
inside a mention) and a label specifying the men-
tion type . Experiments on the CoNLL?02 data
show that the two-process model yields consider-
ably higher performance.
Hacioglu et al (2005) explore the same task, in-
vestigating the performance of the AIO and the
cascade model, and find that the two models have
similar performance, with the AIO model having a
slight advantage. We expand their study by adding
the hybrid joint model to the mix, and further in-
vestigate different scenarios, showing that the cas-
cade model leads to superior performance most of
the time, with a few ties, and show that the cas-
cade model is especially beneficial in cases where
partially-labeled data (only some of the component
labels are given) is available. It turns out though,
(Hacioglu, 2005) that the cascade model in (Ha-
cioglu et al, 2005) did not change to a ?mention
view? sequence classification6 (as we did in Section
3.3) in the tasks following the entity detection, to
allow the system to use longer range features.
6As opposed to a ?word view?.
3 Classification Models
This section presents the three multi-task classifi-
cation models, which we will experimentally con-
trast in Section 4. We are interested in performing
sequence classification (e.g. assigning a label to
each word in a sentence, otherwise known as tag-
ging). Let X denote the space of sequence elements
(words) and Y denote the space of classifications
(labels), both of them being finite spaces. Our goal
is to build a classifier
h : X+ ? Y+
which has the property that |h (x?)| = |x?| ,?x? ? X+
(i.e. the size of the input sequence is preserved).
This classifier will select the a posteriori most likely
label sequence y? = argmaxy?? p
(y??|x?); in our case
p (y?|x?) is computed through the standard Markov
assumption:
p (y1,m| x?) =
?
i
p (yi|x?, yi?n+1,i?1) (1)
where yi,j denotes the sequence of labels yi..yj .
Furthermore, we will assume that each label y
is composed of a number of sub-labels y =(y1y2 . . . yk)7; in other words, we will assume the
factorization of the label space into k subspaces
Y = Y1 ? Y2 ? . . .? Yk.
The classifier we used in the experimental sec-
tion is a maximum entropy classifier (similar to
(McCallum et al, 2000))?which can integrate sev-
eral sources of information in a rigorous manner.
It is our empirical observation that, from a perfor-
mance point of view, being able to use a diverse
and abundant feature set is more important than
classifier choice, and the maximum entropy frame-
work provides such a utility.
3.1 The All-In-One Model
As the simplest model among those presented here,
the all-in-one model ignores the natural factoriza-
tion of the output space and considers all labels as
atomic, and then performs regular sequence clas-
sification. One way to look at this process is the
following: the classification space Y = Y1 ? Y2 ?
. . . ? Yk is first mapped onto a same-dimensional
space Z through a one-to-one mapping o : Y ? Z;
then the features of the system are defined on the
space X+ ?Z, instead of X+ ? Y.
While having the advantage of being simple, it
suffers from some theoretical disadvantages:
? The classification space can be very large, be-
ing the product of the dimensions of sub-task
spaces. In the case of the 2004 ACE data
there are 7 entity types, 4 mention types and
many subtypes; the observed number of actual
7We can assume, without any loss of generality, that
all labels have the same number of sub-labels.
475
All-In-One Model Joint Model
B-PER
B-LOC
B-ORG B-
B-MISC
Table 1: Features predicting start of an entity in
the all-in-one and joint models
sub-label combinations on the training data is
401. Since the dynamic programing (Viterbi)
search?s runtime dependency on the classifica-
tion space is O (|Z|n) (n is the Markov depen-
dency size), using larger spaces will negatively
impact the decoding run time.8
? The probabilities p (zi|x?, zi?n,i?1) require
large data sets to be computed properly. If
the training data is limited, the probabilities
might be poorly estimated.
? The model is not friendly to partial evaluation
or weighted sub-task evaluation: different, but
partially similar, labels will compete against
each other (because the system will return a
probability distribution over the classification
space), sometimes resulting in wrong partial
classification.9
? The model cannot directly use data that is
only partially labeled (i.e. not all sub-labels
are specified).
Despite the above disadvantages, this model has
performed well in practice: Hajic and Hladka?
(1998) applied it successfully to find POS se-
quences for Czech and Florian et al (2004) re-
ports good results on the 2003 ACE task. Most
systems that participated in the CoNLL 2002 and
2003 shared tasks on named entity recognition
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003) applied this model, as they
modeled the identification of mention boundaries
and the assignment of mention type at the same
time.
3.2 The Joint Model
The joint model differs from the all-in-one model
in the fact that the labels are no longer atomic: the
features of the system can inspect the constituent
sub-labels. This change helps alleviate the data
8From a practical point of view, it might not be very
important, as the search is pruned in most cases to only
a few hypotheses (beam-search); in our case, pruning
the beam only introduced an insignificant model search
error (0.1 F-measure).
9To exemplify, consider that the system outputs the
following classifications and probabilities: O (0.2), B-
PER-NAM (0.15), B-PER-NOM (0.15); even the latter
2 suggest that the word is the start of a person mention,
the O label will win because the two labels competed
against each other.
Detect Boundaries   & Entity Types
Assemble full tag
Detect Entity Subtype Detect Mention Type
Figure 1: Cascade flow example for mention detec-
tion.
sparsity encountered by the previous model by al-
lowing sub-label modeling. The joint model the-
oretically compares favorably with the all-in-one
model:
? The probabilities p (yi|x?, yi?n,i?1) =
p
((y1i , . . . , yki
) |x?,
(
yji?n,i?1
)
j=1,k
)
might
require less training data to be properly
estimated, as different sub-labels can be
modeled separately.
? The joint model can use features that predict
just one or a subset of the sub-labels. Ta-
ble 1 presents the set of basic features that
predict the start of a mention for the CoNLL
shared tasks for the two models. While the
joint model can encode the start of a mention
in one feature, the all-in-one model needs to
use four features, resulting in fewer counts per
feature and, therefore, yielding less reliably es-
timated features (or, conversely, it needs more
data for the same estimation confidence).
? The model can predict some of the sub-tags
ahead of the others (i.e. create a dependency
structure on the sub-labels). The model used
in the experimental section predicts the sub-
labels by using only sub-labels for the previous
words, though.
? It is possible, though computationally expen-
sive, for the model to use additional data
that is only partially labeled, with the model
change presented later in Section 3.4.
3.3 The Cascade Model
For some tasks, there might already exist a natural
hierarchy among the sub-labels: some sub-labels
could benefit from knowing the value of other,
primitive, sub-labels. For example,
? For mention detection, identifying the men-
tion boundaries can be considered as a primi-
tive task. Then, knowing the mention bound-
aries, one can assign an entity type, subtype,
and mention type to each mention.
? In the case of parsing with functional tags, one
can perform syntactic parsing, then assign the
functional tags to the internal constituents.
476
Words Since Donna Karan International went public in 1996 ...
Labels O B-ORG I-ORG I-ORG O O O O ...
Figure 2: Sequence tagging for mention detection: the case for a cascade model.
? For POS tagging, one can detect the main
POS first, then detect the other specific prop-
erties, making use of the fact that one knows
the main tag.
The cascade model is essentially a factorization
of individual classifiers for the sub-tasks; in this
framework, we will assume that there is a more
or less natural dependency structure among sub-
tasks, and that models for each of the subtasks
will be built and applied in the order defined by
the dependency structure. For example, as shown
in Figure 1, one can detect mention boundaries and
entity type (at the same time), then detect mention
type and subtype in ?parallel? (i.e. no dependency
exists between these last 2 sub-tags).
A very important advantage of the cascade
model is apparent in classification cases where
identifying chunks is involved (as is the case with
mention detection), similar to advantages that
rescoring hypotheses models have: in the second
stage, the chunk classification stage, it can switch
to a mention view, where the classification units
are entire mentions and words outside of mentions.
This allows the system to make use of aggregate
features over the mention words (e.g. all the words
are capitalized), and to also effectively use a larger
Markov window (instead of 2-3 words, it will use 2-
3 chunks/words around the word of interest). Fig-
ure 2 contains an example of such a case: the cas-
cade model will have to predict the type of the
entire phrase Donna Karan International, in the
context ?Since <chunk> went public in ..?, which
will give it a better opportunity to classify it as an
organization. In contrast, because the joint model
and AIO have a word view of the sentence, will lack
the benefit of examining the larger region, and will
not have access at features that involve partial fu-
ture classifications (such as the fact that another
mention of a particular type follows).
Compared with the other two models, this clas-
sification method has the following advantages:
? The classification spaces for each subtask are
considerably smaller; this fact enables the cre-
ation of better estimated models
? The problem of partially-agreeing competing
labels is completely eliminated
? One can easily use different/additional data to
train any of the sub-task models.
3.4 Adding Partially Labeled Data
Annotated data can be sometimes expensive to
come by, especially if the label set is complex. But
not all sub-tasks were created equal: some of them
might be easier to predict than others and, there-
fore, require less data to train effectively in a cas-
cade setup. Additionally, in realistic situations,
some sub-tasks might be considered to have more
informational content than others, and have prece-
dence in evaluation. In such a scenario, one might
decide to invest resources in annotating additional
data only for the particularly interesting sub-task,
which could reduce this effort significantly.
To test this hypothesis, we annotated additional
data with the entity type only. The cascade model
can incorporate this data easily: it just adds it
to the training data for the entity type classifier
model. While it is not immediately apparent how
to incorporate this new data into the all-in-one and
joint models, in order to maintain fairness in com-
paring the models, we modified the procedures to
allow for the inclusion. Let T denote the original
training data, and T ? denote the additional train-
ing data.
For the all-in-one model, the additional training
data cannot be incorporated directly; this is an in-
herent deficiency of the AIO model. To facilitate a
fair comparison, we will incorporate it in an indi-
rect way: we train a classifier C on the additional
training data T ?, which we then use to classify the
original training data T . Then we train the all-
in-one classifier on the original training data T ,
adding the features defined on the output of ap-
plying the classifier C on T .
The situation is better for the joint model: the
new training data T ? can be incorporated directly
into the training data T .10 The maximum entropy
model estimates the model parameters by maxi-
mizing the data log-likelihood
L =
?
(x,y)
p? (x, y) log q? (y|x)
where p? (x, y) is the observed probability dis-
tribution of the pair (x, y) and q? (y|x) =
1
Z
?
j exp (?j ? fj (x, y)) is the conditional ME
probability distribution as computed by the model.
In the case where some of the data is partially an-
notated, the log-likelihood becomes
L =
?
(x,y)?T ?T ?
p? (x, y) log q? (y|x)
10The solution we present here is particular for
MEMM models (though similar solutions may exist for
other models as well). We also assume the reader is fa-
miliar with the normal MaxEnt training procedure; we
present here only the differences to the standard algo-
rithm. See (Manning and Schu?tze, 1999) for a good
description.
477
=
?
(x,y)?T
p? (x, y) log q? (y|x)
+
?
(x,y)?T ?
p? (x, y) log q? (y|x) (2)
The only technical problem that we are faced with
here is that we cannot directly estimate the ob-
served probability p? (x, y) for examples in T ?, since
they are only partially labeled. Borrowing the
idea from the expectation-maximization algorithm
(Dempster et al, 1977), we can replace this proba-
bility by the re-normalized system proposed prob-
ability: for (x, yx) ? T ?, we define
q? (x, y) = p? (x) ? (y ? yx) q? (y|x)?
y??yx q? (y?|x)? ?? ?
=q??(y|x)
where yx is the subset of labels from Y which are
consistent with the partial classification of x in T ?.
? (y ? yx) is 1 if and only if y is consistent with
the partial classification yx.11 The log-likelihood
computation in Equation (2) becomes
L =
?
(x,y)?T
p? (x, y) log q? (y|x)
+
?
(x,y)?T ?
q? (x, y) log q? (y|x)
To further simplify the evaluation, the quantities
q? (x, y) are recomputed every few steps, and are
considered constant as far as finding the optimum
? values is concerned (the partial derivative com-
putations and numerical updates otherwise become
quite complicated, and the solution is no longer
unique). Given this new evaluation function, the
training algorithm will proceed exactly the same
way as in the normal case where all the data is
fully labeled.
4 Experiments
All the experiments in this section are run on the
ACE 2003 and 2004 data sets, in all the three
languages covered: Arabic, Chinese, and English.
Since the evaluation test set is not publicly avail-
able, we have split the publicly available data into
a 80%/20% data split. To facilitate future compar-
isons with work presented here, and to simulate a
realistic scenario, the splits are created based on
article dates: the test data is selected as the last
20% of the data in chronological order. This way,
the documents in the training and test data sets
do not overlap in time, and the ones in the test
data are posterior to the ones in the training data.
Table 2 presents the number of documents in the
training/test datasets for the three languages.
11For instance, the full label B-PER is consistent
with the partial label B, but not with O or I.
Language Training Test
Arabic 511 178
Chinese 480 166
English 2003 658 139
English 2004 337 114
Table 2: Datasets size (number of documents)
Each word in the training data is labeled with
one of the following properties:12
? if it is not part of any entity, it?s labeled as O
? if it is part of an entity, it contains a tag spec-
ifying whether it starts a mention (B -) or is
inside a mention (I -). It is also labeled with
the entity type of the mention (seven possible
types: person, organization, location, facility,
geo-political entity, weapon, and vehicle), the
mention type (named, nominal, pronominal,
or premodifier13), and the entity subtype (de-
pends on the main entity type).
The underlying classifier used to run the experi-
ments in this article is a maximum entropy model
with a Gaussian prior (Chen and Rosenfeld, 1999),
making use of a large range of features, includ-
ing lexical (words and morphs in a 3-word win-
dow, prefixes and suffixes of length up to 4, Word-
Net (Miller, 1995) for English), syntactic (POS
tags, text chunks), gazetteers, and the output of
other information extraction models. These fea-
tures were described in (Florian et al, 2004), and
are not discussed here. All three methods (AIO,
joint, and cascade) instantiate classifiers based on
the same feature types whenever possible. In terms
of language-specific processing, the Arabic system
uses as input morphological segments, while the
Chinese system is a character-based model (the in-
put elements x ? X are characters), but it has
access to word segments as features.
Performance in the ACE task is officially eval-
uated using a special-purpose measure, the ACE
value metric (NIST, 2003; NIST, 2004). This
metric assigns a score based on the similarity be-
tween the system?s output and the gold-standard
at both mention and entity level, and assigns dif-
ferent weights to different entity types (e.g. the
person entity weights considerably more than a fa-
cility entity, at least in the 2003 and 2004 evalu-
ations). Since this article focuses on the mention
detection task, we decided to use the more intu-
itive (unweighted) F-measure: the harmonic mean
of precision and recall.
12The mention encoding is the IOB2 encoding pre-
sented in (Tjong Kim Sang and Veenstra, 1999) and
introduced by (Ramshaw and Marcus, 1994) for the
task of base noun phrase chunking.
13This is a special class, used for mentions that mod-
ify other labeled mentions; e.g. French in ?French
wine?. This tag is specific only to ACE?04.
478
For the cascade model, the sub-task flow is pre-
sented in Figure 1. In the first step, we identify
the mention boundaries together with their entity
type (e.g. person, organization, etc). In prelimi-
nary experiments, we tried to ?cascade? this task.
The performance was similar on both strategies;
the separated model would yield higher recall at
the expense of precision, while the combined model
would have higher precision, but lower recall. We
decided to use in the system with higher precision.
Once the mentions are identified and classified with
the entity type property, the data is passed, in par-
allel, to the mention type detector and the subtype
detector.
For English and Arabic, we spent three person-
weeks to annotate additional data labeled with
only the entity type information: 550k words for
English and 200k words for Arabic. As mentioned
earlier, adding this data to the cascade model is a
trivial task: the data just gets added to the train-
ing data, and the model is retrained. For the AIO
model, we have build another mention classifier on
the additional training data, and labeled the orig-
inal ACE training data with it. It is important
to note here that the ACE training data (called
T in Section 3.4) is consistent with the additional
training data T ?: the annotation guidelines for T ?
are the same as for the original ACE data, but we
only labeled entity type information. The result-
ing classifications are then used as features in the
final AIO classifier. The joint model uses the addi-
tional partially-labeled data in the way described
in Section 3.4; the probabilities q? (x, y) are updated
every 5 iterations.
Table 3 presents the results: overall, the cascade
model performs significantly better than the all-
in-one model in four out the six tested cases - the
numbers presented in bold reflect that the differ-
ence in performance to the AIO model is statisti-
cally significant.14 The joint model, while manag-
ing to recover some ground, falls in between the
AIO and the cascade models.
When additional partially-labeled data was
available, the cascade and joint models receive a
statistically significant boost in performance, while
the all-in-one model?s performance barely changes.
This fact can be explained by the fact that the en-
tity type-only model is in itself errorful; measuring
the performance of the model on the training data
yields a performance of 82 F-measure;15 therefore
the AIO model will only access partially-correct
14To assert the statistical significance of the results,
we ran a paired Wilcoxon test over the series obtained
by computing F-measure on each document in the test
set. The results are significant at a level of at least
0.009.
15Since the additional training data is consistent in
the labeling of the entity type, such a comparison is in-
deed possible. The above mentioned score is on entity
types only.
Language Data+ A-I-O Joint Cascade
Arabic?04 no 59.2 59.1 59.7
yes 59.4 60.0 60.7
English?04 no 72.1 72.3 73.7
yes 72.5 74.1 75.2
Chinese?04 no 71.2 71.7 71.7
English ?03 no 79.5 79.5 79.7
Table 3: Experimental results: F-measure on the
full label
Language Data+ A-I-O Joint Cascade
Arabic?04 no 66.3 66.5 67.5
yes 66.4 67.9 68.9
English?04 no 77.9 78.1 79.2
yes 78.3 80.5 82.6
Chinese?04 no 75.4 76.1 76.8
English ?03 no 80.4 80.4 81.1
Table 4: F-measure results on entity type only
data, and is unable to make effective use of it.
In contrast, the training data for the entity type
in the cascade model effectively triples, and this
change is reflected positively in the 1.5 increase in
F-measure.
Not all properties are equally valuable: the en-
tity type is arguably more interesting than the
other properties. If we restrict ourselves to eval-
uating the entity type output only (by projecting
the output label to the entity type only), the differ-
ence in performance between the all-in-one model
and cascade is even more pronounced, as shown in
Table 4. The cascade model outperforms here both
the all-in-one and joint models in all cases except
English?03, where the difference is not statistically
significant.
As far as run-time speed is concerned, the AIO
and cascade models behave similarly: our imple-
mentation tags approximately 500 tokens per sec-
ond (averaged over the three languages, on a Pen-
tium 3, 1.2Ghz, 2Gb of memory). Since a MaxEnt
implementation is mostly dependent on the num-
ber of features that fire on average on a example,
and not on the total number of features, the joint
model runs twice as slow: the average number of
features firing on a particular example is consider-
ably higher. On average, the joint system can tag
approximately 240 words per second. The train
time is also considerably longer; it takes 15 times as
long to train the joint model as it takes to train the
all-in-one model (60 mins/iteration compared to
4 mins/iteration); the cascade model trains faster
than the AIO model.
One last important fact that is worth mention-
ing is that a system based on the cascade model
participated in the ACE?04 competition, yielding
very competitive results in all three languages.
479
5 Conclusion
As natural language processing becomes more so-
phisticated and powerful, we start focus our at-
tention on more and more properties associated
with the objects we are seeking, as they allow for
a deeper and more complex representation of the
real world. With this focus comes the question of
how this goal should be accomplished ? either de-
tect all properties at once, one at a time through
a pipeline, or a hybrid model. This paper presents
three methods through which multi-label sequence
classification can be achieved, and evaluates and
contrasts them on the Automatic Content Extrac-
tion task. On the ACE mention detection task,
the cascade model which predicts first the mention
boundaries and entity types, followed by mention
type and entity subtype outperforms the simple all-
in-one model in most cases, and the joint model in
a few cases.
Among the proposed models, the cascade ap-
proach has the definite advantage that it can easily
and productively incorporate additional partially-
labeled data. We also presented a novel modifica-
tion of the joint system training that allows for the
direct incorporation of additional data, which in-
creased the system performance significantly. The
all-in-one model can only incorporate additional
data in an indirect way, resulting in little to no
overall improvement.
Finally, the performance obtained by the cas-
cade model is very competitive: when paired with a
coreference module, it ranked very well in the ?En-
tity Detection and Tracking? task in the ACE?04
evaluation.
References
R. Caruana, L. Pratt, and S. Thrun. 1997. Multitask
learning. Machine Learning, 28:41.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, Computer Sci-
ence Department, Carnegie Mellon University.
A. P. Dempster, N. M. Laird, , and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal statistical Soci-
ety, 39(1):1?38.
R. Florian and G. Ngai. 2001. Multidimensional
transformation-based learning. In Proceedings of
CoNLL?01, pages 1?8.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N Nicolov, and S Roukos.
2004. A statistical model for multilingual entity de-
tection and tracking. In Proceedings of the Human
Language Technology Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004, pages 1?8.
R. Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceedings
of CoNLL-2002, pages 175?178.
Kadri Hacioglu, Benjamin Douglas, and Ying Chen.
2005. Detection of entity mentions occuring in en-
glish and chinese text. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 379?386, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Kadri Hacioglu. 2005. Private communication.
J. Hajic and Hladka?. 1998. Tagging inflective lan-
guages: Prediction of morphological categories for a
rich, structured tagset. In Proceedings of the 36th
Annual Meeting of the ACL and the 17th ICCL,
pages 483?490, Montre?al, Canada.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. It-
tycheriah. 2003. HowtogetaChineseName(Entity):
Segmentation and combination issues. In Proceed-
ings of EMNLP?03, pages 200?207.
Dan Klein. 2003. Maxent models, conditional estima-
tion, and optimization, without the magic. Tutorial
presented at NAACL-03 and ACL-03.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19:313?330.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models
for information extraction and segmentation. In Pro-
ceedings of ICML-2000.
G. A. Miller. 1995. WordNet: A lexical database.
Communications of the ACM, 38(11).
MUC-6. 1995. The sixth mes-
sage understanding conference.
www.cs.nyu.edu/cs/faculty/grishman/muc6.html.
MUC-7. 1997. The seventh mes-
sage understanding conference.
www.itl.nist.gov/iad/894.02/related projects/
muc/proceedings/muc 7 toc.html.
NIST. 2003. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
NIST. 2004. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
L. Ramshaw and M. Marcus. 1994. Exploring the sta-
tistical derivation of transformational rule sequences
for part-of-speech tagging. In Proceedings of the
ACL Workshop on Combining Symbolic and Statis-
tical Approaches to Language, pages 128?135.
C. Sutton, K. Rohanimanesh, and A. McCallum.
2004. Dynamic conditional random fields: Factor-
ized probabilistic models for labeling and segment-
ing sequence data. In In Proceedings of the Twenty-
First International Conference on Machine Learning
(ICML-2004).
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmonton,
Canada.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Repre-
senting text chunks. In Proceedings of EACL?99.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named en-
tity recognition. In Proceedings of CoNLL-2002,
pages 155?158.
480
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 577?584,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Maximum Entropy Based Restoration of Arabic Diacritics
Imed Zitouni, Jeffrey S. Sorensen, Ruhi Sarikaya
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598
{izitouni, sorenj, sarikaya}@us.ibm.com
Abstract
Short vowels and other diacritics are not
part of written Arabic scripts. Exceptions
are made for important political and reli-
gious texts and in scripts for beginning stu-
dents of Arabic. Script without diacritics
have considerable ambiguity because many
words with different diacritic patterns ap-
pear identical in a diacritic-less setting. We
propose in this paper a maximum entropy
approach for restoring diacritics in a doc-
ument. The approach can easily integrate
and make effective use of diverse types of
information; the model we propose inte-
grates a wide array of lexical, segment-
based and part-of-speech tag features. The
combination of these feature types leads
to a state-of-the-art diacritization model.
Using a publicly available corpus (LDC?s
Arabic Treebank Part 3), we achieve a di-
acritic error rate of 5.1%, a segment error
rate 8.5%, and a word error rate of 17.3%.
In case-ending-less setting, we obtain a di-
acritic error rate of 2.2%, a segment error
rate 4.0%, and a word error rate of 7.2%.
1 Introduction
Modern Arabic written texts are composed of
scripts without short vowels and other diacritic
marks. This often leads to considerable ambigu-
ity since several words that have different diacritic
patterns may appear identical in a diacritic-less
setting. Educated modern Arabic speakers are able
to accurately restore diacritics in a document. This
is based on the context and their knowledge of the
grammar and the lexicon of Arabic. However, a
text without diacritics becomes a source of confu-
sion for beginning readers and people with learning
disabilities. A text without diacritics is also prob-
lematic for applications such as text-to-speech or
speech-to-text, where the lack of diacritics adds
another layer of ambiguity when processing the
data. As an example, full vocalization of text is
required for text-to-speech applications, where the
mapping from graphemes to phonemes is simple
compared to languages such as English and French;
where there is, in most cases, one-to-one relation-
ship. Also, using data with diacritics shows an
improvement in the accuracy of speech-recognition
applications (Afify et al, 2004). Currently, text-to-
speech, speech-to-text, and other applications use
data where diacritics are placed manually, which
is a tedious and time consuming excercise. A di-
acritization system that restores the diacritics of
scripts, i.e. supply the full diacritical markings,
would be of interest to these applications. It also
would greatly benefit nonnative speakers, sufferers
of dyslexia and could assist in restoring diacritics
of children?s and poetry books, a task that is cur-
rently done manually.
We propose in this paper a statistical approach
that restores diacritics in a text document. The
proposed approach is based on the maximum en-
tropy framework where several diverse sources of
information are employed. The model implicitly
learns the correlation between these types of infor-
mation and the output diacritics.
In the next section, we present the set of diacrit-
ics to be restored and the ambiguity we face when
processing a non-diacritized text. Section 3 gives
a brief summary of previous related works. Sec-
tion 4 presents our diacritization model; we ex-
plain the training and decoding process as well as
the different feature categories employed to restore
the diacritics. Section 5 describes a clearly defined
and replicable split of the LDC?s Arabic Treebank
Part 3 corpus, used to built and evaluate the sys-
tem, so that the reproduction of the results and
future comparison can accurately be established.
Section 6 presents the experimental results. Sec-
tion 7 reports a comparison of our approach to
the finite state machine modeling technique that
showed promissing results in (Nelken and Shieber,
2005). Finally, section 8 concludes the paper and
discusses future directions.
2 Arabic Diacritics
The Arabic alphabet consists of 28 letters that can
be extended to a set of 90 by additional shapes,
marks, and vowels (Tayli and Al-Salamah, 1990).
The 28 letters represent the consonants and long
577
vowels such as A, ? (both pronounced as /a:/),
?
  (pronounced as /i:/), and ? (pronounced as
/u:/). Long vowels are constructed by combin-
ing A, ?, ?
 , and ? with the short vowels. The
short vowels and certain other phonetic informa-
tion such as consonant doubling (shadda) are not
represented by letters, but by diacritics. A dia-
critic is a short stroke placed above or below the
consonant. Table 1 shows the complete set of Ara-
Diacritic Name Meaning/
on ? Pronunciation
Short vowels? fatha /a/
? damma /u/
? kasra /i/
Doubled case ending (?tanween?)? tanween al-fatha /an/
? tanween al-damma /un/
? tanween al-kasra /in/
Syllabification marks? shadda consonant
doubling? sukuun vowel
absence
Table 1: Arabic diacritics on the letter ? consonant
? ? (pronounced as /t/).
bic diacritics. We split the Arabic diacritics into
three sets: short vowels, doubled case endings, and
syllabification marks. Short vowels are written as
symbols either above or below the letter in text
with diacritics, and dropped all together in text
without diacritics. We find three short vowels:
? fatha: it represents the /a/ sound and is an
oblique dash over a consonant as in
? (c.f.
fourth row of Table 1).
? damma: it represents the /u/ sound and is
a loop over a consonant that resembles the
shape of a comma (c.f. fifth row of Table 1).
? kasra: it represents the /i/ sound and is an
oblique dash under a consonant (c.f. sixth row
of Table 1).
The doubled case ending diacritics are vowels used
at the end of the words to mark case distinction,
which can be considered as a double short vowels;
the term ?tanween? is used to express this phe-
nomenon. Similar to short vowels, there are three
different diacritics for tanween: tanween al-fatha,
tanween al-damma, and tanween al-kasra. They
are placed on the last letter of the word and have
the phonetic effect of placing an ?N? at the end
of the word. Text with diacritics contains also two
syllabification marks:
? shadda: it is a gemination mark placed above
the Arabic letters as in
?. It denotes the dou-
bling of the consonant. The shadda is usually
combined with a short vowel such as in
?.
? sukuun: written as a small circle as in
?. It is
used to indicate that the letter doesn?t contain
vowels.
Figure 1 shows an Arabic sentence transcribed with
and without diacritics. In modern Arabic, writing
scripts without diacritics is the most natural way.
Because many words with different vowel patterns
may appear identical in a diacritic-less setting,
considerable ambiguity exists at the word level.
The word I. J?, for example, has 21 possible forms
that have valid interpretations when adding dia-
critics (Kirchhoff and Vergyri, 2005). It may have
the interpretation of the verb ?to write? in I.
J

?
(pronounced /kataba/). Also, it can be interpreted
as ?books? in the noun form I.
J

? (pronounced /ku-
tubun/). A study made by (Debili et al, 2002)
shows that there is an average of 11.6 possible di-
acritizations for every non-diacritized word when
analyzing a text of 23,000 script forms.
. ?Q? 	Y?? @ ?
KQ? @ I. J?
. ? Q

?
	Y?? @ ?
K Q? @ I.
J

?
Figure 1: The same Arabic sentence without (up-
per row) and with (lower row) diacritics. The En-
glish translation is ?the president wrote the docu-
ment.?
Arabic diacritic restoration is a non-trivial task as
expressed in (El-Imam, 2003). Native speakers of
Arabic are able, in most cases, to accurately vo-
calize words in text based on their context, the
speaker?s knowledge of the grammar, and the lex-
icon of Arabic. Our goal is to convert knowledge
used by native speakers into features and incor-
porate them into a maximum entropy model. We
assume that the input text does not contain any
diacritics.
3 Previous Work
Diacritic restoration has been receiving increas-
ing attention and has been the focus of several
studies. In (El-Sadany and Hashish, 1988), a rule
based method that uses morphological analyzer for
578
vowelization was proposed. Another, rule-based
grapheme to sound conversion approach was ap-
peared in 2003 by Y. El-Imam (El-Imam, 2003).
The main drawbacks of these rule based methods is
that it is difficult to maintain the rules up-to-date
and extend them to other Arabic dialects. Also,
new rules are required due to the changing nature
of any ?living? language.
More recently, there have been several new stud-
ies that use alternative approaches for the diacriti-
zation problem. In (Emam and Fisher, 2004) an
example based hierarchical top-down approach is
proposed. First, the training data is searched hi-
erarchically for a matching sentence. If there is
a matching sentence, the whole utterance is used.
Otherwise they search for matching phrases, then
words to restore diacritics. If there is no match at
all, character n-gram models are used to diacritize
each word in the utterance.
In (Vergyri and Kirchhoff, 2004), diacritics in
conversational Arabic are restored by combining
morphological and contextual information with an
acoustic signal. Diacritization is treated as an un-
supervised tagging problem where each word is
tagged as one of the many possible forms provided
by the Buckwalter?s morphological analyzer (Buck-
walter, 2002). The Expectation Maximization
(EM) algorithm is used to learn the tag sequences.
Y. Gal in (Gal, 2002) used a HMM-based diacriti-
zation approach. This method is a white-space
delimited word based approach that restores only
vowels (a subset of all diacritics).
Most recently, a weighted finite state machine
based algorithm is proposed (Nelken and Shieber,
2005). This method employs characters and larger
morphological units in addition to words. Among
all the previous studies this one is more sophisti-
cated in terms of integrating multiple information
sources and formulating the problem as a search
task within a unified framework. This approach
also shows competitive results in terms of accuracy
when compared to previous studies. In their algo-
rithm, a character based generative diacritization
scheme is enabled only for words that do not occur
in the training data. It is not clearly stated in the
paper whether their method predict the diacritics
shedda and sukuun.
Even though the methods proposed for diacritic
restoration have been maturing and improving over
time, they are still limited in terms of coverage and
accuracy. In the approach we present in this paper,
we propose to restore the most comprehensive list
of the diacritics that are used in any Arabic text.
Our method differs from the previous approaches
in the way the diacritization problem is formulated
and because multiple information sources are inte-
grated. We view the diacritic restoration problem
as sequence classification, where given a sequence
of characters our goal is to assign diacritics to each
character. Our appoach is based on Maximum
Entropy (MaxEnt henceforth) technique (Berger
et al, 1996). MaxEnt can be used for sequence
classification, by converting the activation scores
into probabilities (through the soft-max function,
for instance) and using the standard dynamic pro-
gramming search algorithm (also known as Viterbi
search). We find in the literature several other
approaches of sequence classification such as (Mc-
Callum et al, 2000) and (Lafferty et al, 2001).
The conditional random fields method presented
in (Lafferty et al, 2001) is essentially a MaxEnt
model over the entire sequence: it differs from the
Maxent in that it models the sequence informa-
tion, whereas the Maxent makes a decision for each
state independently of the other states. The ap-
proach presented in (McCallum et al, 2000) com-
bines Maxent with Hidden Markov models to allow
observations to be presented as arbitrary overlap-
ping features, and define the probability of state
sequences given observation sequences.
We report in section 7 a comparative study be-
tween our approach and the most competitive dia-
critic restoration method that uses finite state ma-
chine algorithm (Nelken and Shieber, 2005). The
MaxEnt framework was successfully used to com-
bine a diverse collection of information sources and
yielded a highly competitive model that achieves a
5.1% DER.
4 Automatic Diacritization
The performance of many natural language pro-
cessing tasks, such as shallow parsing (Zhang et
al., 2002) and named entity recognition (Florian
et al, 2004), has been shown to depend on inte-
grating many sources of information. Given the
stated focus of integrating many feature types, we
selected the MaxEnt classifier. MaxEnt has the
ability to integrate arbitrary types of information
and make a classification decision by aggregating
all information available for a given classification.
4.1 Maximum Entropy Classifiers
We formulate the task of restoring diacritics as
a classification problem, where we assign to each
character in the text a label (i.e., diacritic). Be-
fore formally describing the method1, we introduce
some notations: let Y = {y1, . . . , yn} be the set of
diacritics to predict or restore, X be the example
space and F = {0, 1}m be a feature space. Each ex-
ample x ? X has associated a vector of binary fea-
tures f (x) = (f1 (x) , . . . , fm (x)). In a supervised
framework, like the one we are considering here, we
have access to a set of training examples together
with their classifications: {(x1, y1) , . . . , (xk, yk)}.
1This is not meant to be an in-depth introduction
to the method, but a brief overview to familiarize the
reader with them.
579
The MaxEnt algorithm associates a set of weights
(?ij)i=1...nj=1...m with the features, which are estimated
during the training phase to maximize the likeli-
hood of the data (Berger et al, 1996). Given these
weights, the model computes the probability dis-
tribution over labels for a particular example x as
follows:
P (y|x) = 1Z(x)
m
?
j=1
?fj (x)ij , Z(x) =
?
i
?
j
?fj (x)ij
where Z(X ) is a normalization factor. To esti-
mate the optimal ?j values, we train our Max-
Ent model using the sequential conditional gener-
alized iterative scaling (SCGIS) technique (Good-
man, 2002). While the MaxEnt method can nicely
integrate multiple feature types seamlessly, in cer-
tain cases it is known to overestimate its confidence
in especially low-frequency features. To overcome
this problem, we use the regularization method
based on adding Gaussian priors as described in
(Chen and Rosenfeld, 2000). After computing the
class probability distribution, the chosen diacritic
is the one with the most aposteriori probability.
The decoding algorithm, described in section 4.2,
performs sequence classification, through dynamic
programming.
4.2 Search to Restore Diacritics
We are interested in finding the diacritics of all
characters in a script or a sentence. These dia-
critics have strong interdependencies which can-
not be properly modeled if the classification is per-
formed independently for each character. We view
this problem as sequence classification, as con-
trasted with an example-based classification prob-
lem: given a sequence of characters in a sentence
x1x2 . . . xL, our goal is to assign diacritics (labels)
to each character, resulting in a sequence of diacrit-
ics y1y2 . . . yL. We make an assumption that dia-
critics can be modeled as a limited order Markov
sequence: the diacritic associated with the char-
acter i depends only on the diacritics associated
with the k previous diacritics, where k is usually
equal to 3. Given this assumption, and the nota-
tion xL1 = x1 . . . xL, the conditional probability of
assigning the diacritic sequence yL1 to the character
sequence xL1 becomes
p
(
yL1 |xL1
)
=
p
(
y1|xL1
)
p
(
y2|xL1 , y1
)
. . . p
(
yL|xL1 , yL?1L?k+1
)
(1)
and our goal is to find the sequence that maximizes
this conditional probability
y?L1 = arg max
yL1
p
(
yL1 |xL1
)
(2)
While we restricted the conditioning on the classi-
fication tag sequence to the previous k diacritics,
we do not impose any restrictions on the condition-
ing on the characters ? the probability is computed
using the entire character sequence xL1 .
To obtain the sequence in Equation (2), we create
a classification tag lattice (also called trellis), as
follows:
? Let xL1 be the input sequence of character and
S = {s1, s2, . . . , sm} be an enumeration of Yk
(m = |Y|k) - we will call an element sj a state.
Every such state corresponds to the labeling
of k successive characters. We find it useful
to think of an element si as a vector with k
elements. We use the notations si [j] for jth
element of such a vector (the label associated
with the token xi?k+j+1) and si [j1 . . . j2] for
the sequence of elements between indices j1
and j2.
? We conceptually associate every character
xi, i = 1, . . . , L with a copy of S, Si =
{
si1, . . . , sim
}
; this set represents all the possi-
ble labelings of characters xii?k+1 at the stage
where xi is examined.
? We then create links from the set Si to the
Si+1, for all i = 1 . . . L? 1, with the property
that
w
(
sij1 , s
i+1
j2
)
=
?
?
?
p
(
si+1j1 [k] |x
L
1 , si+1j2 [1..k ? 1]
)
if sij1 [2..k] = s
i+1
j2 [1..k ? 1]
0 otherwise
These weights correspond to probability of a
transition from the state sij1 to the state s
i+1
j2 .
? For every character xi, we compute recur-
sively2
?0 (sj) = 0, j = 1, . . . , k
?i (sj) = max
j1=1,...,M
?i?1 (sj1 ) + log w
(
si?1j1 , s
i
j
)
?i (sj) =
arg max
j1=1,...,M
?i?1 (sj1 ) + log w
(
si?1j1 , s
i
j
)
Intuitively, ?i (sj) represents the log-
probability of the most probable path through
the lattice that ends in state sj after i steps,
and ?i (sj) represents the state just before sj
on that particular path.
? Having computed the (?i)i values, the algo-
rithm for finding the best path, which corre-
sponds to the solution of Equation (2) is
1. Identify s?LL = arg maxj=1...L ?L (sj)
2. For i = L ? 1 . . . 1, compute
s?ii = ?i+1
(
s?i+1i+1
)
2For convenience, the index i associated with state
sij is moved to ?; the function ?i (sj) is in fact ?
(
sij
)
.
580
3. The solution for Equation (2) is given by
y? =
{
s?11[k], s?22[k], . . . , s?LL [k]
}
The runtime of the algorithm is ?
(
|Y|k ? L
)
, linear
in the size of the sentence L but exponential in the
size of the Markov dependency, k. To reduce the
search space, we use beam-search.
4.3 Features Employed
Within the MaxEnt framework, any type of fea-
tures can be used, enabling the system designer to
experiment with interesting feature types, rather
than worry about specific feature interactions. In
contrast, with a rule based system, the system de-
signer would have to consider how, for instance,
lexical derived information for a particular exam-
ple interacts with character context information.
That is not to say, ultimately, that rule-based sys-
tems are in some way inferior to statistical mod-
els ? they are built using valuable insight which
is hard to obtain from a statistical-model-only ap-
proach. Instead, we are merely suggesting that the
output of such a rule-based system can be easily
integrated into the MaxEnt framework as one of
the input features, most likely leading to improved
performance.
Features employed in our system can be divided
into three different categories: lexical, segment-
based, and part-of-speech tag (POS) features. We
also use the previously assigned two diacritics as
additional features.
In the following, we briefly describe the different
categories of features:
? Lexical Features: we include the charac-
ter n-gram spanning the curent character xi,
both preceding and following it in a win-
dow of 7: {xi?3, . . . , xi+3}. We use the cur-
rent word wi and its word context in a win-
dow of 5 (forward and backward trigram):
{wi?2, . . . , wi+2}. We specify if the character
of analysis is at the beginning or at the end
of a word. We also add joint features between
the above source of information.
? Segment-Based Features : Arabic blank-
delimited words are composed of zero or more
prefixes, followed by a stem and zero or more
suffixes. Each prefix, stem or suffix will be
called a segment in this paper. Segments are
often the subject of analysis when processing
Arabic (Zitouni et al, 2005). Syntactic in-
formation such as POS or parse information
is usually computed on segments rather than
words. As an example, the Arabic white-space
delimited word ?? D?K. A
? contains a verb ?K. A
?, a
third-person feminine singular subject-marker
H (she), and a pronoun suffix ?? (them); it
is also a complete sentence meaning ?she met
them.? To separate the Arabic white-space
delimited words into segments, we use a seg-
mentation model similar to the one presented
by (Lee et al, 2003). The model obtains an
accuracy of about 98%. In order to simulate
real applications, we only use segments gener-
ated by the model rather than true segments.
In the diacritization system, we include the
current segment ai and its word segment con-
text in a window of 5 (forward and backward
trigram): {ai?2, . . . , ai+2}. We specify if the
character of analysis is at the beginning or at
the end of a segment. We also add joint infor-
mation with lexical features.
? POS Features : we attach to the segment
ai of the current character, its POS: POS(ai).
This is combined with joint features that in-
clude the lexical and segment-based informa-
tion. We use a statistical POS tagging system
built on Arabic Treebank data with MaxEnt
framework (Ratnaparkhi, 1996). The model
has an accuracy of about 96%. We did not
want to use the true POS tags because we
would not have access to such information in
real applications.
5 Data
The diacritization system we present here is
trained and evaluated on the LDC?s Arabic Tree-
bank of diacritized news stories ? Part 3 v1.0: cata-
log number LDC2004T11 and ISBN 1-58563-298-8.
The corpus includes complete vocalization (includ-
ing case-endings). We introduce here a clearly de-
fined and replicable split of the corpus, so that the
reproduction of the results or future investigations
can accurately and correctly be established. This
corpus includes 600 documents from the An Nahar
News Text. There are a total of 340,281 words. We
split the corpus into two sets: training data and de-
velopment test (devtest) data. The training data
contains 288,000 words approximately, whereas the
devtest contains close to 52,000 words. The 90
documents of the devtest data are created by tak-
ing the last (in chronological order) 15% of docu-
ments dating from ?20021015 0101? (i.e., October
15, 2002) to ?20021215 0045? (i.e., December 15,
2002). The time span of the devtest is intention-
ally non-overlapping with that of the training set,
as this models how the system will perform in the
real world.
Previously published papers use proprietary cor-
pus or lack clear description of the training/devtest
data split, which make the comparison to other
techniques difficult. By clearly reporting the split
of the publicly available LDC?s Arabic Treebank
581
corpus in this section, we want future comparisons
to be correctly established.
6 Experiments
Experiments are reported in terms of word error
rate (WER), segment error rate (SER), and di-
acritization error rate (DER). The DER is the
proportion of incorrectly restored diacritics. The
WER is the percentage of incorrectly diacritized
white-space delimited words: in order to be
counted as incorrect, at least one character in the
word must have a diacritization error. The SER
is similar to WER but indicates the proportion of
incorrectly diacritized segments. A segment can
be a prefix, a stem, or a suffix. Segments are often
the subject of analysis when processing Arabic (Zi-
touni et al, 2005). Syntactic information such as
POS or parse information is based on segments
rather than words. Consequently, it is important
to know the SER in cases where the diacritization
system may be used to help disambiguate syntactic
information.
Several modern Arabic scripts contains the con-
sonant doubling ?shadda?; it is common for na-
tive speakers to write without diacritics except the
shadda. In this case the role of the diacritization
system will be to restore the short vowels, doubled
case ending, and the vowel absence ?sukuun?. We
run two batches of experiments: a first experiment
where documents contain the original shadda and
a second one where documents don?t contain any
diacritics including the shadda. The diacritization
system proceeds in two steps when it has to pre-
dict the shadda: a first step where only shadda is
restored and a second step where other diacritics
(excluding shadda) are predicted.
To assess the performance of the system under dif-
ferent conditions, we consider three cases based on
the kind of features employed:
1. system that has access to lexical features only;
2. system that has access to lexical and segment-
based features;
3. system that has access to lexical, segment-
based and POS features.
The different system types described above use the
two previously assigned diacritics as additional fea-
ture. The DER of the shadda restoration step is
equal to 5% when we use lexical features only, 0.4%
when we add segment-based information, and 0.3%
when we employ lexical, POS, and segment-based
features.
Table 2 reports experimental results of the diacriti-
zation system with different feature sets. Using
only lexical features, we observe a DER of 8.2%
and a WER of 25.1% which is competitive to a
True shadda Predicted shadda
WER SER DER WER SER DER
Lexical features
24.8 12.6 7.9 25.1 13.0 8.2
Lexical + segment-based features
18.2 9.0 5.5 18.8 9.4 5.8
Lexical + segment-based + POS features
17.3 8.5 5.1 18.0 8.9 5.5
Table 2: The impact of features on the diacriti-
zation system performance. The columns marked
with ?True shadda? represent results on docu-
ments containing the original consonant doubling
?shadda? while columns marked with ?Predicted
shadda? represent results where the system re-
stored all diacritics including shadda.
state-of-the-art system evaluated on Arabic Tree-
bank Part 2: in (Nelken and Shieber, 2005) a DER
of 12.79% and a WER of 23.61% are reported.
The system they described in (Nelken and Shieber,
2005) uses lexical, segment-based, and morpholog-
ical information. Table 2 also shows that, when
segment-based information is added to our sys-
tem, a significant improvement is achieved: 25%
for WER (18.8 vs. 25.1), 38% for SER (9.4 vs.
13.0), and 41% for DER (5.8 vs. 8.2). Similar be-
havior is observed when the documents contain the
original shadda. POS features are also helpful in
improving the performance of the system. They
improved the WER by 4% (18.0 vs. 18.8), SER by
5% (8.9 vs. 9.4), and DER by 5% (5.5 vs. 5.8).
Case-ending in Arabic documents consists of the
diacritic attributed to the last character in a white-
space delimited word. Restoring them is the most
difficult part in the diacritization of a document.
Case endings are only present in formal or highly
literary scripts. Only educated speakers of mod-
ern standard Arabic master their use. Technically,
every noun has such an ending, although at the
end of a sentence no inflection is pronounced, even
in formal speech, because of the rules of ?pause?.
For this reason, we conduct another experiment in
which case-endings were stripped throughout the
training and testing data without the attempt to
restore them.
We present in Table 3 the performance of the di-
acritization system on documents without case-
endings. Results clearly show that when case-
endings are omitted, the WER declines by 58%
(7.2% vs. 17.3%), SER is decreased by 52% (4.0%
vs. 8.5%), and DER is reduced by 56% (2.2% vs.
5.1%). Also, Table 3 shows again that a richer
set of features results in a better performance;
compared to a system using lexical features only,
adding POS and segment-based features improved
the WER by 38% (7.2% vs. 11.8%), the SER by
39% (4.0% vs. 6.6%), and DER by 38% (2.2% vs.
582
True shadda Predicted shadda
WER SER DER WER SER DER
Lexical features
11.8 6.6 3.6 12.4 7.0 3.9
Lexical + segment-based features
7.8 4.4 2.4 8.6 4.8 2.7
Lexical + segment-based + POS features
7.2 4.0 2.2 7.9 4.4 2.5
Table 3: Performance of the diacritization system
based on employed features. System is trained
and evaluated on documents without case-ending.
Columns marked with ?True shadda? represent re-
sults on documents containing the original con-
sonant doubling ?shadda? while columns marked
with ?Predicted shadda? represent results where
the system restored all diacritics including shadda.
3.6%). Similar to the results reported in Table 2,
we show that the performance of the system are
similar whether the document contains the origi-
nal shadda or not. A system like this trained on
non case-ending documents can be of interest to
applications such as speech recognition, where the
last state of a word HMM model can be defined to
absorb all possible vowels (Afify et al, 2004).
7 Comparison to other approaches
As stated in section 3, the most recent and ad-
vanced approach to diacritic restoration is the one
presented in (Nelken and Shieber, 2005): they
showed a DER of 12.79% and a WER of 23.61% on
Arabic Treebank corpus using finite state transduc-
ers (FST) with a Katz language modeling (LM) as
described in (Chen and Goodman, 1999). Because
they didn?t describe how they split their corpus
into training/test sets, we were not able to use the
same data for comparison purpose.
In this section, we want essentially to duplicate
the aforementioned FST result for comparison us-
ing the identical training and testing set we use for
our experiments. We also propose some new vari-
ations on the finite state machine modeling tech-
nique which improve performance considerably.
The algorithm for FST based vowel restoration
could not be simpler: between every pair of char-
acters we insert diacritics if doing so improves
the likelihood of the sequence as scored by a sta-
tistical n-gram model trained upon the training
corpus. Thus, in between every pair of charac-
ters we propose and score all possible diacritical
insertions. Results reported in Table 4 indicate
the error rates of diacritic restoration (including
shadda). We show performance using both Kneser-
Ney and Katz LMs (Chen and Goodman, 1999)
with increasingly large n-grams. It is our opinion
that large n-grams effectively duplicate the use of
a lexicon. It is unfortunate but true that, even for
a rich resource like the Arabic Treebank, the choice
of modeling heuristic and the effects of small sam-
ple size are considerable. Using the finite state ma-
chine modeling technique, we obtain similar results
to those reported in (Nelken and Shieber, 2005): a
WER of 23% and a DER of 15%. Better perfor-
mance is reached with the use of Kneser-Ney LM.
These results still under-perform those obtained
by MaxEnt approach presented in Table 2. When
all sources of information are included, the Max-
Ent technique outperforms the FST model by 21%
(22% vs. 18%) in terms of WER and 39% (9% vs.
5.5%) in terms of DER.
The SER reported on Table 2 and Table 3 are based
on the Arabic segmentation system we use in the
MaxEnt approach. Since, the FST model doesn?t
use such a system, we found inappropriate to re-
port SER in this section.
Katz LM Kneser-Ney LM
n-gram size WER DER WER DER
3 63 31 55 28
4 54 25 38 19
5 51 21 28 13
6 44 18 24 11
7 39 16 23 11
8 37 15 23 10
Table 4: Error Rate in % for n-gram diacritic
restoration using FST.
We propose in the following an extension to the
aforementioned FST model, where we jointly de-
termines not only diacritics but segmentation into
affixes as described in (Lee et al, 2003). Table 5
gives the performance of the extended FST model
where Kneser-Ney LM is used, since it produces
better results. This should be a much more dif-
ficult task, as there are more than twice as many
possible insertions. However, the choice of diacrit-
ics is related to and dependent upon the choice of
segmentation. Thus, we demonstrate that a richer
internal representation produces a more powerful
model.
8 Conclusion
We presented in this paper a statistical model for
Arabic diacritic restoration. The approach we pro-
pose is based on the Maximum entropy framework,
which gives the system the ability to integrate dif-
ferent sources of knowledge. Our model has the ad-
vantage of successfully combining diverse sources
of information ranging from lexical, segment-based
and POS features. Both POS and segment-based
features are generated by separate statistical sys-
tems ? not extracted manually ? in order to sim-
ulate real world applications. The segment-based
features are extracted from a statistical morpho-
logical analysis system using WFST approach and
the POS features are generated by a parsing model
583
True Shadda Predicted Shadda
n-gram size Kneser-Ney Kneser-Ney
WER DER WER DER
3 49 23 52 27
4 34 14 35 17
5 26 11 26 12
6 23 10 23 10
7 23 9 22 10
8 23 9 22 10
Table 5: Error Rate in % for n-gram dia-
critic restoration and segmentation using FST
and Kneser-Ney LM. Columns marked with ?True
shadda? represent results on documents contain-
ing the original consonant doubling ?shadda? while
columns marked with ?Predicted shadda? repre-
sent results where the system restored all diacritics
including shadda.
that also uses Maximum entropy framework. Eval-
uation results show that combining these sources of
information lead to state-of-the-art performance.
As future work, we plan to incorporate Buckwalter
morphological analyzer information to extract new
features that reduce the search space. One idea will
be to reduce the search to the number of hypothe-
ses, if any, proposed by the morphological analyzer.
We also plan to investigate additional conjunction
features to improve the accuracy of the model.
Acknowledgments
Grateful thanks are extended to Radu Florian for
his constructive comments regarding the maximum
entropy classifier.
References
M. Afify, S. Abdou, J. Makhoul, L. Nguyen, and B. Xi-
ang. 2004. The BBN RT04 BN Arabic System. In
RT04 Workshop, Palisades NY.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996.
A maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
T. Buckwalter. 2002. Buckwalter Arabic morpholog-
ical analyzer version 1.0. Technical report, Linguis-
tic Data Consortium, LDC2002L49 and ISBN 1-58563-
257-0.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for language
modeling. computer speech and language. Computer
Speech and Language, 4(13):359?393.
Stanley Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for me models. IEEE Trans.
on Speech and Audio Processing.
F. Debili, H. Achour, and E. Souissi. 2002. De
l?etiquetage grammatical a? la voyellation automatique
de l?arabe. Technical report, Correspondances de
l?Institut de Recherche sur le Maghreb Contemporain
17.
Y. El-Imam. 2003. Phonetization of arabic: rules and
algorithms. Computer Speech and Language, 18:339?
373.
T. El-Sadany and M. Hashish. 1988. Semi-automatic
vowelization of Arabic verbs. In 10th NC Conference,
Jeddah, Saudi Arabia.
O. Emam and V. Fisher. 2004. A hierarchical ap-
proach for the statistical vowelization of Arabic text.
Technical report, IBM patent filed, DE9-2004-0006, US
patent application US2005/0192809 A1.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N Nicolov, and S Roukos. 2004.
A statistical model for multilingual entity detection
and tracking. In Proceedings of HLT-NAACL 2004,
pages 1?8.
Y. Gal. 2002. An HMM approach to vowel restora-
tion in Arabic and Hebrew. In ACL-02 Workshop on
Computational Approaches to Semitic Languages.
Joshua Goodman. 2002. Sequential conditional gener-
alized iterative scaling. In Proceedings of ACL?02.
K. Kirchhoff and D. Vergyri. 2005. Cross-dialectal
data sharing for acoustic modeling in Arabic speech
recognition. Speech Communication, 46(1):37?51, May.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and
H. Hassan. 2003. Language model based Arabic word
segmentation. In Proceedings of the ACL?03, pages
399?406.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In ICML.
Rani Nelken and Stuart M. Shieber. 2005. Arabic
diacritization using weighted finite-state transducers.
In ACL-05 Workshop on Computational Approaches to
Semitic Languages, pages 79?86, Ann Arbor, Michigan.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Conference on
Empirical Methods in Natural Language Processing.
M. Tayli and A. Al-Salamah. 1990. Building bilingual
microcomputer systems. Communications of the ACM,
33(5):495?505.
D. Vergyri and K. Kirchhoff. 2004. Automatic dia-
critization of Arabic for acoustic modeling in speech
recognition. In COLING Workshop on Arabic-script
Based Languages, Geneva, Switzerland.
Tong Zhang, Fred Damerau, and David E. Johnson.
2002. Text chunking based on a generalization of Win-
now. Journal of Machine Learning Research, 2:615?
637.
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and Radu
Florian. 2005. The impact of morphological stemming
on Arabic mention detection and coreference resolu-
tion. In Proceedings of the ACL Workshop on Compu-
tational Approaches to Semitic Languages, pages 63?
70, Ann Arbor, June.
584
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 63?70,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Impact of Morphological Stemming on Arabic Mention
Detection and Coreference Resolution
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, Radu Florian
{izitouni, sorenj, xiaoluo, raduf}@watson.ibm.com
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA
Abstract
Arabic presents an interesting challenge to
natural language processing, being a highly
inflected and agglutinative language. In
particular, this paper presents an in-depth
investigation of the entity detection and
recognition (EDR) task for Arabic. We
start by highlighting why segmentation is
a necessary prerequisite for EDR, continue
by presenting a finite-state statistical seg-
menter, and then examine how the result-
ing segments can be better included into
a mention detection system and an entity
recognition system; both systems are statis-
tical, build around the maximum entropy
principle. Experiments on a clearly stated
partition of the ACE 2004 data show that
stem-based features can significantly im-
prove the performance of the EDT system
by 2 absolute F-measure points. The sys-
tem presented here had a competitive per-
formance in the ACE 2004 evaluation.
1 Introduction
Information extraction is a crucial step toward un-
derstanding and processing language. One goal of
information extraction tasks is to identify important
conceptual information in a discourse. These tasks
have applications in summarization, information re-
trieval (one can get al hits for Washington/person
and not the ones for Washington/state or Washing-
ton/city), data mining, question answering, language
understanding, etc.
In this paper we focus on the Entity Detection and
Recognition task (EDR) for Arabic as described in
ACE 2004 framework (ACE, 2004). The EDR has
close ties to the named entity recognition (NER) and
coreference resolution tasks, which have been the fo-
cus of several recent investigations (Bikel et al, 1997;
Miller et al, 1998; Borthwick, 1999; Mikheev et al,
1999; Soon et al, 2001; Ng and Cardie, 2002; Florian
et al, 2004), and have been at the center of evalu-
ations such as: MUC-6, MUC-7, and the CoNLL?02
and CoNLL?03 shared tasks. Usually, in computa-
tional linguistics literature, a named entity is an in-
stance of a location, a person, or an organization, and
the NER task consists of identifying each of these
occurrences. Instead, we will adopt the nomencla-
ture of the Automatic Content Extraction program
(NIST, 2004): we will call the instances of textual
references to objects/abstractions mentions, which
can be either named (e.g. John Mayor), nominal
(the president) or pronominal (she, it). An entity is
the aggregate of all the mentions (of any level) which
refer to one conceptual entity. For instance, in the
sentence
President John Smith said he has no com-
ments
there are two mentions (named and pronomial) but
only one entity, formed by the set {John Smith, he}.
We separate the EDR task into two parts: a men-
tion detection step, which identifies and classifies all
the mentions in a text ? and a coreference resolution
step, which combinines the detected mentions into
groups that refer to the same object. In its entirety,
the EDR task is arguably harder than traditional
named entity recognition, because of the additional
complexity involved in extracting non-named men-
tions (nominal and pronominal) and the requirement
of grouping mentions into entities. This is particu-
larly true for Arabic where nominals and pronouns
are also attached to the word they modify. In fact,
most Arabic words are morphologically derived from
a list of base forms or stems, to which prefixes and
suffixes can be attached to form Arabic surface forms
(blank-delimited words). In addition to the differ-
ent forms of the Arabic word that result from the
63
derivational and inflectional process, most preposi-
tions, conjunctions, pronouns, and possessive forms
are attached to the Arabic surface word. It is these
orthographic variations and complex morphological
structure that make Arabic language processing chal-
lenging (Xu et al, 2001; Xu et al, 2002).
Both tasks are performed with a statistical frame-
work: the mention detection system is similar to
the one presented in (Florian et al, 2004) and
the coreference resolution system is similar to the
one described in (Luo et al, 2004). Both systems
are built around from the maximum-entropy tech-
nique (Berger et al, 1996). We formulate the men-
tion detection task as a sequence classification prob-
lem. While this approach is language independent,
it must be modified to accomodate the particulars of
the Arabic language. The Arabic words may be com-
posed of zero or more prefixes, followed by a stem and
zero or more suffixes. We begin with a segmentation
of the written text before starting the classification.
This segmentation process consists of separating the
normal whitespace delimited words into (hypothe-
sized) prefixes, stems, and suffixes, which become the
subject of analysis (tokens). The resulting granular-
ity of breaking words into prefixes and suffixes allows
different mention type labels beyond the stem label
(for instance, in the case of nominal and pronominal
mentions). Additionally, because the prefixes and
suffixes are quite frequent, directly processing unseg-
mented words results in significant data sparseness.
We present in Section 2 the relevant particularities
of the Arabic language for natural language process-
ing, especially for the EDR task. We then describe
the segmentation system we employed for this task in
Section 3. Section 4 briefly describes our mention de-
tection system, explaining the different feature types
we use. We focus in particular on the stem n-gram,
prefix n-gram, and suffix n-gram features that are
specific to a morphologically rich language such as
Arabic. We describe in Section 5 our coreference
resolution system where we also describe the advan-
tage of using stem based features. Section 6 shows
and discusses the different experimental results and
Section 7 concludes the paper.
2 Why is Arabic Information
Extraction difficult?
The Arabic language, which is the mother tongue of
more than 300 million people (Center, 2000), present
significant challenges to many natural language pro-
cessing applications. Arabic is a highly inflected and
derived language. In Arabic morphology, most mor-
phemes are comprised of a basic word form (the root
or stem), to which many affixes can be attached to
form Arabic words. The Arabic alphabet consists
of 28 letters that can be extended to ninety by ad-
ditional shapes, marks, and vowels (Tayli and Al-
Salamah, 1990). Unlike Latin-based alphabets, the
orientation of writing in Arabic is from right to left.
In written Arabic, short vowels are often omitted.
Also, because variety in expression is appreciated
as part of a good writing style, the synonyms are
widespread. Arabic nouns encode information about
gender, number, and grammatical cases. There are
two genders (masculine and feminine), three num-
bers (singular, dual, and plural), and three gram-
matical cases (nominative, genitive, and accusative).
A noun has a nominative case when it is a subject,
accusative case when it is the object of a verb, and
genitive case when it is the object of a preposition.
The form of an Arabic noun is consequently deter-
mined by its gender, number, and grammatical case.
The definitive nouns are formed by attaching the
Arabic article ?

@ to the immediate front of the
nouns, such as in the word ??Q???

@ (the company).
Also, prepositions such as H. (by), and ? (to) can beattached as a prefix as in ??Q???? (to the company).
A noun may carry a possessive pronoun as a suffix,
such as in ?? D?Q?? (their company). For the EDR task,
in this previous example, the Arabic blank-delimited
word ?? D?Q?? should be split into two tokens: ??Q?? and
??. The first token ??Q?? is a mention that refers to
an organization, whereas the second token ?? is also
a mention, but one that may refer to a person. Also,
the prepositions (i.e., H. and ?) not be considered a
part of the mention.
Arabic has two kinds of plurals: broken plurals and
sound plurals (Wightwick and Gaafar, 1998; Chen
and Gey, 2002). The formation of broken plurals is
common, more complex and often irregular. As an
example, the plural form of the noun ?g. P (man) is
?A g. P (men), which is formed by inserting the infix
@. The plural form of the noun H. A
J? (book) is I. J?
(books), which is formed by deleting the infix @. The
plural form and the singular form may also be com-
pletely different (e.g. ?

@Q?@ for woman, but ZA
?	 for
women). The sound plurals are formed by adding
plural suffixes to singular nouns (e.g., IkAK. meaning
researcher): the plural suffix is H@ for feminine nouns
in grammatical cases (e.g., HA
JkAK.), 	?? for masculine
nouns in the nominative case (e.g., 	??JkAK.), and 	?K

for masculine nouns in the genitive and accusative
cases (e.g., 	?
JkAK.). The dual suffix is 	?@ for the nom-
inative case (e.g., 	?A
JkAK.), and 	?K
 for the genitive or
accusative (e.g., 	?
JkAK.).
Because we consider pronouns and nominals as men-
tions, it is essential to segment Arabic words into
these subword tokens. We also believe that the in-
64
formation denoted by these affixes can help with the
coreference resolution task1.
Arabic verbs have perfect and imperfect tenses (Ab-
bou and McCarus, 1983). Perfect tense denotes com-
pleted actions, while imperfect denotes ongoing ac-
tions. Arabic verbs in the perfect tense consist of a
stem followed by a subject marker, denoted as a suf-
fix. The subject marker indicates the person, gender,
and number of the subject. As an example, the verb
?K. A
? (to meet) has a perfect tense I?K. A
? for the third
person feminine singular, and @?

?K. A
? for the third per-
son masculine plural. We notice also that a verb with
a subject marker and a pronoun suffix can be by itself
a complete sentence, such us in the word ?? D?K. A
?: it
has a third-person feminine singular subject-markerH (she) and a pronoun suffix ?? (them). It is also
a complete sentence meaning ?she met them.? The
subject markers are often suffixes, but we may find
a subject marker as a combination of a prefix and a
suffix as in ???K. A
?K (she meets them). In this example,
the EDR system should be able to separate ???K. A
?K,
to create two mentions ( H and ??). Because the
two mentions belong to different entities, the EDR
system should not chain them together. An Arabic
word can potentially have a large number of vari-
ants, and some of the variants can be quite complex.
As an example, consider the word A ?D
JkAJ. ?? (and to
her researchers) which contains two prefixes and one
suffix ( A ? + ?

?kAK. + ? + ?).
3 Arabic Segmentation
Lee et al (2003) demonstrates a technique for seg-
menting Arabic text and uses it as a morphological
processing step in machine translation. A trigram
language model was used to score and select among
hypothesized segmentations determined by a set of
prefix and suffix expansion rules.
In our latest implementation of this algorithm, we
have recast this segmentation strategy as the com-
position of three distinct finite state machines. The
first machine, illustrated in Figure 1 encodes the pre-
fix and suffix expansion rules, producing a lattice of
possible segmentations. The second machine is a dic-
tionary that accepts characters and produces identi-
fiers corresponding to dictionary entries. The final
machine is a trigram language model, specifically a
Kneser-Ney (Chen and Goodman, 1998) based back-
off language model. Differing from (Lee et al, 2003),
we have also introduced an explicit model for un-
1As an example, we do not chain mentions with dif-
ferent gender, number, etc.
known words based upon a character unigram model,
although this model is dominated by an empirically
chosen unknown word penalty. Using 0.5M words
from the combined Arabic Treebanks 1V2, 2V2 and
3V1, the dictionary based segmenter achieves a exact
word match 97.8% correct segmentation.
SEP/epsilon
a/A#
epsilon/#
a/epsilon
a/epsilon
b/epsilon
b/B
UNK/epsilon
c/C
b/epsilon
c/BC
e/+E
epsilon/+
d/epsilon
d/epsilon
epsilon/epsilon
b/AB#
b/A#B#
e/+DE
c/epsilon d/BCD e/+D+E
Figure 1: Illustration of dictionary based segmenta-
tion finite state transducer
3.1 Bootstrapping
In addition to the model based upon a dictionary of
stems and words, we also experimented with models
based upon character n-grams, similar to those used
for Chinese segmentation (Sproat et al, 1996). For
these models, both arabic characters and spaces, and
the inserted prefix and suffix markers appear on the
arcs of the finite state machine. Here, the language
model is conditioned to insert prefix and suffix mark-
ers based upon the frequency of their appearance in
n-gram character contexts that appear in the train-
ing data. The character based model alone achieves
a 94.5% exact match segmentation accuracy, consid-
erably less accurate then the dictionary based model.
However, an analysis of the errors indicated that the
character based model is more effective at segment-
ing words that do not appear in the training data.
We seeked to exploit this ability to generalize to im-
prove the dictionary based model. As in (Lee et al,
2003), we used unsupervised training data which is
automatically segmented to discover previously un-
seen stems. In our case, the character n-gram model
is used to segment a portion of the Arabic Giga-
word corpus. From this, we create a vocabulary of
stems and affixes by requiring that tokens appear
more than twice in the supervised training data or
more than ten times in the unsupervised, segmented
corpus.
The resulting vocabulary, predominately of word
stems, is 53K words, or about six times the vo-
cabulary observed in the supervised training data.
This represents about only 18% of the total num-
ber of unique tokens observed in the aggregate
training data. With the addition of the automat-
ically acquired vocabulary, the segmentation accu-
racy achieves 98.1% exact match.
65
3.2 Preprocessing of Arabic Treebank Data
Because the Arabic treebank and the gigaword cor-
pora are based upon news data, we apply some
small amount of regular expression based preprocess-
ing. Arabic specific processing include removal of
the characters tatweel (), and vowels. Also, the fol-
lowing characters are treated as an equivalence class
during all lookups and processing: (1) ? ,?
 , and
(2)

@ , @ ,

@ ,

@. We define a token and introduce whites-
pace boundaries between every span of one or more
alphabetic or numeric characters. Each punctuation
symbol is considered a separate token. Character
classes, such as punctuation, are defined according
to the Unicode Standard (Aliprand et al, 2004).
4 Mention Detection
The mention detection task we investigate identifies,
for each mention, four pieces of information:
1. the mention type: person (PER), organiza-
tion (ORG), location (LOC), geopolitical en-
tity (GPE), facility (FAC), vehicle (VEH), and
weapon (WEA)
2. the mention level (named, nominal, pronominal,
or premodifier)
3. the mention class (generic, specific, negatively
quantified, etc.)
4. the mention sub-type, which is a sub-category
of the mention type (ACE, 2004) (e.g. OrgGov-
ernmental, FacilityPath, etc.).
4.1 System Description
We formulate the mention detection problem as a
classification problem, which takes as input seg-
mented Arabic text. We assign to each token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is outside
any mentions. We use a maximum entropy Markov
model (MEMM) classifier. The principle of maxi-
mum entropy states that when one searches among
probability distributions that model the observed
data (evidence), the preferred one is the one that
maximizes the entropy (a measure of the uncertainty
of the model) (Berger et al, 1996). One big advan-
tage of this approach is that it can combine arbitrary
and diverse types of information in making a classi-
fication decision.
Our mention detection system predicts the four la-
bels types associated with a mention through a cas-
cade approach. It first predicts the boundary and
the main entity type for each mention. Then, it uses
the information regarding the type and boundary in
different second-stage classifiers to predict the sub-
type, the mention level, and the mention class. Af-
ter the first stage, when the boundary (starting, in-
side, or outside a mention) has been determined, the
other classifiers can use this information to analyze
a larger context, capturing the patterns around the
entire mentions, rather than words. As an example,
the token sequence that refers to a mention will be-
come a single recognized unit and, consequently, lex-
ical and syntactic features occuring inside or outside
of the entire mention span can be used in prediction.
In the first stage (entity type detection and classifica-
tion), Arabic blank-delimited words, after segment-
ing, become a series of tokens representing prefixes,
stems, and suffixes (cf. section 2). We allow any
contiguous sequence of tokens can represent a men-
tion. Thus, prefixes and suffixes can be, and often
are, labeled with a different mention type than the
stem of the word that contains them as constituents.
4.2 Stem n-gram Features
We use a large set of features to improve the predic-
tion of mentions. This set can be partitioned into
4 categories: lexical, syntactic, gazetteer-based, and
those obtained by running other named-entity clas-
sifiers (with different tag sets). We use features such
as the shallow parsing information associated with
the tokens in a window of 3 tokens, POS, etc.
The context of a current token ti is clearly one of
the most important features in predicting whether ti
is a mention or not (Florian et al, 2004). We de-
note these features as backward token tri-grams and
forward token tri-grams for the previous and next
context of ti respectively. For a token ti, the back-
ward token n-gram feature will contains the previous
n ? 1 tokens in the history (ti?n+1, . . . ti?1) and the
forward token n-gram feature will contains the next
n ? 1 tokens (ti+1, . . . ti+n?1).
Because we are segmenting arabic words into
multiple tokens, there is some concern that tri-
gram contexts will no longer convey as much
contextual information. Consider the following
sentence extracted from the development set:
H. 	Qj?? ?
??A
J
??@ I. J?
??? Q
??? @ ?J??
 @
	Y? (transla-
tion ?This represents the location for Political
Party Office?). The ?Political Party Office? is
tagged as an organization and, as a word-for-word
translation, is expressed as ?to the Office of the
political to the party?. It is clear in this example
that the word Q?? (location for) contains crucial
information in distinguishing between a location
and an organization when tagging the token I. J?
?
66
(office). After segmentation, the sentence becomes:
+ I. J?
? + ?

@ + ? + Q?? + ?

@ + ?J? + ?
 + @
	Y?
.H. 	Qk + ?

@ + ? + ?
??A
J
? + ?

@
When predicting if the token I. J?
? (office) is the
beginning of an organization or not, backward and
forward token n-gram features contain only ?

@ + ?
(for the) and ?
??A
J
? + ?

@ (the political). This is
most likely not enough context, and addressing the
problem by increasing the size of the n-gram context
quickly leads to a data sparseness problem.
We propose in this paper the stem n-gram features as
additional features to the lexical set. If the current
token ti is a stem, the backward stem n-gram feature
contains the previous n ? 1 stems and the forward
stem n-gram feature will contain the following n? 1
stems. We proceed similarly for prefixes and suffixes:
if ti is a prefix (or suffix, respectively) we take the
previous and following prefixes (or suffixes)2. In the
sentence shown above, when the system is predict-
ing if the token I. J?
? (office) is the beginning of an
organization or not, the backward and forward stem
n-gram features contain Q?? ?J? (represent location
of) and H. 	Qk ?
??A
J
? (political office). The stem fea-
tures contain enough information in this example to
make a decision that I. J?
? (office) is the beginning of
an organization. In our experiments, n is 3, therefore
we use stem trigram features.
5 Coreference Resolution
Coreference resolution (or entity recognition) is de-
fined as grouping together mentions referring to the
same object or entity. For example, in the following
text,
(I) ?John believes Mary to be the best student?
three mentions ?John?, ?Mary?, ?student? are un-
derlined. ?Mary? and ?student? are in the same en-
tity since both refer to the same person.
The coreference system system is similar to the Bell
tree algorithm as described by (Luo et al, 2004).
In our implementation, the link model between a
candidate entity e and the current mention m is com-
puted as
PL(L = 1|e, m) ? maxmk?e P?L(L = 1|e, mk, m), (1)
2Thus, the difference to token n-grams is that the to-
kens of different type are removed from the streams, be-
fore the features are created.
where mk is one mention in entity e, and the basic
model building block P?L(L = 1|e, mk, m) is an ex-
ponential or maximum entropy model (Berger et al,
1996).
For the start model, we use the following approxima-
tion:
PS(S = 1|e1, e2, ? ? ? , et, m) ?
1 ? max
1?i?t
PL(L = 1|ei, m) (2)
The start model (cf. equation 2) says that the prob-
ability of starting a new entity, given the current
mention m and the previous entities e1, e2, ? ? ? , et, is
simply 1 minus the maximum link probability be-
tween the current mention and one of the previous
entities.
The maximum-entropy model provides us with a
flexible framework to encode features into the the
system. Our Arabic entity recognition system uses
many language-indepedent features such as strict
and partial string match, and distance features (Luo
et al, 2004). In this paper, however, we focus on the
addition of Arabic stem-based features.
5.1 Arabic Stem Match Feature
Features using the word context (left and right to-
kens) have been shown to be very helpful in corefer-
ence resolution (Luo et al, 2004). For Arabic, since
words are morphologically derived from a list of roots
(stems), we expected that a feature based on the
right and left stems would lead to improvement in
system accuracy.
Let m1 and m2 be two candidate mentions where
a mention is a string of tokens (prefixes, stems,
and suffixes) extracted from the segmented text.
In order to make a decision in either linking the
two mentions or not we use additional features
such as: do the stems in m1 and m2 match, do
stems in m1 match all stems in m2, do stems
in m1 partially match stems in m2. We proceed
similarly for prefixes and suffixes. Since prefixes and
suffixes can belong to different mention types, we
build a parse tree on the segmented text and we can
explore features dealing with the gender and number
of the token. In the following example, between
parentheses we make a word-for-word translations in
order to better explain our stemming feature. Let us
take the two mentions H. 	Qj?? ?
??A
J
??@ I. J?
???
(to-the-office the-politic to-the-party) and
?
G.
	Qm?'@ I. J?
? (office the-party?s) segmented as
H. 	Qk + ?

@ + ? + ?
??A
J
? + ?

@ + I. J?
? + ?

@ + ?
and ?
 + H. 	Qk + ?

@ + I. J?
? respectively. In our
67
development corpus, these two mentions are chained
to the same entity. The stemming match feature
in this case will contain information such us all
stems of m2 match, which is a strong indicator
that these mentions should be chained together.
Features based on the words alone would not help
this specific example, because the two strings m1
and m2 do not match.
6 Experiments
6.1 Data
The system is trained on the Arabic ACE 2003 and
part of the 2004 data. We introduce here a clearly
defined and replicable split of the ACE 2004 data,
so that future investigations can accurately and cor-
rectly compare against the results presented here.
There are 689 Arabic documents in LDC?s 2004 re-
lease (version 1.4) of ACE data from three sources:
the Arabic Treebank, a subset of the broadcast
(bnews) and newswire (nwire) TDT-4 documents.
The 178-document devtest is created by taking
the last (in chronological order) 25% of docu-
ments in each of three sources: 38 Arabic tree-
bank documents dating from ?20000715? (i.e., July
15, 2000) to ?20000815,? 76 bnews documents from
?20001205.1100.0489? (i.e., Dec. 05 of 2000 from
11:00pm to 04:89am) to ?20001230.1100.1216,? and
64 nwire documents from ?20001206.1000.0050? to
?20001230.0700.0061.? The time span of the test
set is intentionally non-overlapping with that of the
training set within each data source, as this models
how the system will perform in the real world.
6.2 Mention Detection
We want to investigate the usefulness of stem n-
gram features in the mention detection system. As
stated before, the experiments are run in the ACE?04
framework (NIST, 2004) where the system will iden-
tify mentions and will label them (cf. Section 4)
with a type (person, organization, etc), a sub-type
(OrgCommercial, OrgGovernmental, etc), a mention
level (named, nominal, etc), and a class (specific,
generic, etc). Detecting the mention boundaries (set
of consecutive tokens) and their main type is one of
the important steps of our mention detection sys-
tem. The score that the ACE community uses (ACE
value) attributes a higher importance (outlined by
its weight) to the main type compared to other sub-
tasks, such as the mention level and the class. Hence,
to build our mention detection system we spent a lot
of effort in improving the first step: detecting the
mention boundary and their main type. In this pa-
per, we report the results in terms of precision, recall,
and F-measure3.
Lexical features
Precision Recall F-measure
(%) (%) (%)
Total 73.3 58.0 64.7
FAC 76.0 24.0 36.5
GPE 79.4 65.6 71.8
LOC 57.7 29.9 39.4
ORG 63.1 46.6 53.6
PER 73.2 63.5 68.0
VEH 83.5 29.7 43.8
WEA 77.3 25.4 38.2
Lexical features + Stem
Precision Recall F-measure
(%) (%) (%)
Total 73.6 59.4 65.8
FAC 72.7 29.0 41.4
GPE 79.9 67.2 73.0
LOC 58.6 31.9 41.4
ORG 62.6 47.2 53.8
PER 73.8 64.6 68.9
VEH 81.7 35.9 49.9
WEA 78.4 29.9 43.2
Table 1: Performance of the mention detection sys-
tem using lexical features only.
To assess the impact of stemming n-gram features
on the system under different conditions, we consider
two cases: one where the system only has access to
lexical features (the tokens and direct derivatives in-
cluding standard n-gram features), and one where
the system has access to a richer set of information,
including lexical features, POS tags, text chunks,
parse tree, and gazetteer information. The former
framework has the advantage of being fast (making
it more appropriate for deployment in commercial
systems). The number of parameters to optimize in
the MaxEnt framework we use when only lexical fea-
tures are explored is around 280K parameters. This
number increases to 443K approximately when all in-
formation is used except the stemming feature. The
number of parameters introduced by the use of stem-
ming is around 130K parameters. Table 1 reports
experimental results using lexical features only; we
observe that the stemming n-gram features boost the
performance by one point (64.7 vs. 65.8). It is im-
portant to notice the stemming n-gram features im-
proved the performance of each category of the main
type.
In the second case, the systems have access to a large
amount of feature types, including lexical, syntac-
tic, gazetteer, and those obtained by running other
3The ACE value is an important factor for us, but its
relative complexity, due to different weights associated
with the subparts, makes for a hard comparison, while
the F-measure is relatively easy to interpret.
68
AllFeatures
Precision Recall F-measure
(%) (%) (%)
Total 74.3 64.0 68.8
FAC 72.3 36.8 48.8
GPE 80.5 70.8 75.4
LOC 61.1 35.4 44.8
ORG 61.4 50.3 55.3
PER 75.3 70.2 72.7
VEH 83.2 38.1 52.3
WEA 69.0 36.6 47.8
All-Features + Stem
Precision Recall F-measure
(%) (%) (%)
Total 74.4 64.6 69.2
FAC 68.8 38.5 49.4
GPE 80.8 71.9 76.1
LOC 60.2 36.8 45.7
ORG 62.2 51.0 56.1
PER 75.3 70.2 72.7
VEH 81.4 41.8 55.2
WEA 70.3 38.8 50.0
Table 2: Performance of the mention detection sys-
tem using lexical, syntactic, gazetteer features as well
as features obtained by running other named-entity
classifiers
named-entity classifiers (with different semantic tag
sets). Features are also extracted from the shal-
low parsing information associated with the tokens
in window of 3, POS, etc. The All-features system
incorporates all the features except for the stem n-
grams. Table 2 shows the experimental results with
and without the stem n-grams features. Again, Ta-
ble 2 shows that using stem n-grams features gave
a small boost to the whole main-type classification
system4. This is true for all types. It is interesting to
note that the increase in performance in both cases
(Tables 1 and 2) is obtained from increased recall,
with little change in precision. When the prefix and
suffix n-gram features are removed from the feature
set, we notice in both cases (Tables 1 and 2) a in-
significant decrease of the overall performance, which
is expected: what should a feature of preceeding (or
following) prepositions or finite articles captures?
As stated in Section 4.1, the mention detection sys-
tem uses a cascade approach. However, we were curi-
ous to see if the gain we obtained at the first level was
successfully transfered into the overall performance
of the mention detection system. Table 3 presents
the performance in terms of precision, recall, and F-
measure of the whole system. Despite the fact that
the improvement was small in terms of F-measure
(59.4 vs. 59.7), the stemming n-gram features gave
4The difference in performance is not statistically sig-
nificant
interesting improvement in terms of ACE value to
the hole EDR system as showed in section 6.3.
Precision Recall F-measure
(%) (%) (%)
All-Features 64.2 55.3 59.4
All-Features+Stem 64.4 55.7 59.7
Lexical 64.4 50.8 56.8
Lexical+Stem 64.6 52.0 57.6
Table 3: Performance of the mention detection sys-
tem including all ACE?04 subtasks
6.3 Coreference Resolution
In this section, we present the coreference results on
the devtest defined earlier. First, to see the effect of
stem matching features, we compare two coreference
systems: one with the stem features, the other with-
out. We test the two systems on both ?true? and
system mentions of the devtest set. ?True? men-
tions mean that input to the coreference system are
mentions marked by human, while system mentions
are output from the mention detection system. We
report results with two metrics: ECM-F and ACE-
Value. ECM-F is an entity-constrained mention F-
measure (cf. (Luo et al, 2004) for how ECM-F is
computed), and ACE-Value is the official ACE eval-
uation metric. The result is shown in Table 4: the
baseline numbers without stem features are listed un-
der ?Base,? and the results of the coreference system
with stem features are listed under ?Base+Stem.?
On true mention, the stem matching features im-
prove ECM-F from 77.7% to 80.0%, and ACE-value
from 86.9% to 88.2%. The similar improvement is
also observed on system mentions.The overall ECM-
F improves from 62.3% to 64.2% and the ACE value
improves from 61.9 to 63.1%. Note that the increase
on the ACE value is smaller than ECM-F. This is
because ACE-value is a weighted metric which em-
phasizes on NAME mentions and heavily discounts
PRONOUN mentions. Overall the stem features give
rise to consistent gain to the coreference system.
7 Conclusion
In this paper, we present a fully fledged Entity Detec-
tion and Tracking system for Arabic. At its base, the
system fundamentally depends on a finite state seg-
menter and makes good use of the relationships that
occur between word stems, by introducing features
which take into account the type of each segment.
In mention detection, the features are represented as
stem n-grams, while in coreference resolution they
are captured through stem-tailored match features.
69
Base Base+Stem
ECM-F ACEVal ECM-F ACEVal
Truth 77.7 86.9 80.0 88.2
System 62.3 61.9 64.2 63.1
Table 4: Effect of Arabic stemming features on coref-
erence resolution. The row marked with ?Truth?
represents the results with ?true? mentions while the
row marked with ?System? represents that mentions
are detected by the system. Numbers under ?ECM-
F? are Entity-Constrained-Mention F-measure and
numbers under ?ACE-Val? are ACE-values.
These types of features result in an improvement in
both the mention detection and coreference resolu-
tion performance, as shown through experiments on
the ACE 2004 Arabic data. The experiments are per-
formed on a clearly specified partition of the data, so
comparisons against the presented work can be cor-
rectly and accurately made in the future. In addi-
tion, we also report results on the official test data.
The presented system has obtained competitive re-
sults in the ACE 2004 evaluation, being ranked
amongst the top competitors.
8 Acknowledgements
This work was partially supported by the Defense
Advanced Research Projects Agency and monitored
by SPAWAR under contract No. N66001-99-2-8916.
The views and findings contained in this material are
those of the authors and do not necessarily reflect
the position of policy of the U.S. government and no
official endorsement should be inferred.
References
Peter F. Abbou and Ernest N. McCarus, editors. 1983.
Elementary modern standard Arabic. Cambridge Univer-
sity Press.
ACE. 2004. Automatic content extraction.
http://www.ldc.upenn.edu/Projects/ACE/.
Joan Aliprand, Julie Allen, Joe Becker, Mark Davis,
Michael Everson, Asmus Freytag, John Jenkins, Mike
Ksar, Rick McGowan, Eric Muller, Lisa Moore, Michel
Suignard, and Ken Whistler. 2004. The unicode stan-
dard. http://www.unicode.org/.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language process-
ing. Computational Linguistics, 22(1):39?71.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-finder.
In Proceedings of ANLP-97, pages 194?201.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York Uni-
versity.
Egyptian Demographic Center. 2000.
http://www.frcu.eun.eg/www/homepage/cdc/cdc.htm.
Aitao Chen and Fredic Gey. 2002. Building an arabic
stemmer for information retrieval. In Proceedings of the
Eleventh Text REtrieval Conference (TREC 2002), Na-
tional Institute of Standards and Technology, November.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techinques for language modeling. Techni-
cal Report TR-10-98, Center for Research in Comput-
ing Technology, Harvard University, Cambridge, Mas-
sachusettes, August.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A statisti-
cal model for multilingual entity detection and tracking.
In Proceedings of HLT-NAACL 2004, pages 1?8.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word segmen-
tation. In Proceedings of the ACL?03, pages 399?406.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on
the bell tree. In Proc. of ACL?04.
A. Mikheev, M. Moens, and C. Grover. 1999. Named
entity recognition without gazetteers. In Proceedings of
EACL?99.
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz,
R. Stone, and R. Weischedel. 1998. Bbn: Description of
the SIFT system as used for MUC-7. In MUC-7.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings of
the ACL?02, pages 104?111.
NIST. 2004. Proceedings of ace evaluation and pi meet-
ing 2004 workshop. Alexandria, VA, September. NIST.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
R. Sproat, C. Shih, W. Gale, and N. Chang. 1996. A
stochastic finite-state word-segmentation algorithm for
Chinese. Computational Linguistics, 22(3).
M. Tayli and A. Al-Salamah. 1990. Building bilingual
microcomputer systems. Communications of the ACM,
33(5):495?505.
J. Wightwick and M. Gaafar. 1998. Arabic Verbs and
Essentials of Grammar. Passport Books.
J. Xu, A. Fraser, and R. Weischedel. 2001. Trec2001
cross-lingual retrieval at bbn. In TREC 2001, Gaithers-
burg: NIST.
J. Xu, A. Fraser, and R. Weischedel. 2002. Empirical
studies in strategies for arabic information retrieval. In
SIGIR 2002, Tampere, Finland.
70
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 335?345,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Mention Detection Robustness to Noisy Input
Radu Florian, John F. Pitrelli, Salim Roukos and Imed Zitouni
IBM T.J. Watson Research Center
Yorktown Heights, NY, U.S.A.
{raduf,pitrelli,roukos,izitouni}us.ibm.com
Abstract
Information-extraction (IE) research typically
focuses on clean-text inputs. However, an IE
engine serving real applications yields many
false alarms due to less-well-formed input.
For example, IE in a multilingual broadcast
processing system has to deal with inaccu-
rate automatic transcription and translation.
The resulting presence of non-target-language
text in this case, and non-language mate-
rial interspersed in data from other applica-
tions, raise the research problem of making
IE robust to such noisy input text. We ad-
dress one such IE task: entity-mention de-
tection. We describe augmenting a statistical
mention-detection system in order to reduce
false alarms from spurious passages. The di-
verse nature of input noise leads us to pursue
a multi-faceted approach to robustness. For
our English-language system, at various miss
rates we eliminate 97% of false alarms on in-
puts from other Latin-alphabet languages. In
another experiment, representing scenarios in
which genre-specific training is infeasible, we
process real financial-transactions text con-
taining mixed languages and data-set codes.
On these data, because we do not train on data
like it, we achieve a smaller but significant im-
provement. These gains come with virtually
no loss in accuracy on clean English text.
1 Introduction
Information-extraction (IE) research is typically per-
formed on clean text in a predetermined language.
Lately, IE has improved to the point of being usable
for some real-world tasks whose accuracy require-
ments are reachable with current technology. These
uses include media monitoring, topic alerts, sum-
marization, population of databases for advanced
search, etc. These uses often combine IE with tech-
nologies such as speech recognition, machine trans-
lation, topic clustering, and information retrieval.
The propagation of IE technology from isolated
use to aggregates with such other technologies, from
NLP experts to other types of computer scientists,
and from researchers to users, feeds back to the IE
research community the need for additional inves-
tigation which we loosely refer to as ?information-
extraction robustness? research. For example:
1. Broadcast monitoring demands that IE handle
as input not only clean text, but also the tran-
scripts output by speech recognizers.
2. Multilingual applications, and the imperfection
of translation technology, require IE to contend
with non-target-language text input (Pitrelli et
al., 2008).
3. Naive users at times input to IE other material
which deviates from clean text, such as a PDF
file that ?looks? like plain text.
4. Search applications require IE to deal with
databases which not only possess clean text but
at times exhibit other complications like mark-
up codes particular to narrow, application-
specific data-format standards, for example, the
excerpt from a financial-transactions data set
shown in Figure 1.
Legacy industry-specific standards, such as il-
lustrated in this example, are part of long-
established processes which are cumbersome
to convert to a more-modern database format.
Transaction data sets typically build up over a
period of years, and as seen here, can exhibit
335
:54D://121000358
BANK OF BOSTON
:55D:/0148280005
NEVADA DEPT.OF VET.94C RECOV.FD
-5:MAC:E19DECA8CHK:641EB09B8968
USING OF FIELD 59: ONLY /INS/ WHEN
FOLLOWED BY BCC CODE IN CASE
OF QUESTIONS DONT HESITATE TO
CONTACT US QUOTING REFERENCE
NON-STC CHARGES OR VIA E-MAIL:
YOVANKA(UL)BRATASOVA(AT)BOA.CZ.
BEST REGARDS
BANKA OBCHODNIKA, A.S. PRAGUE, CZ
:58E::ADTX//++ ADDITIONAL
INFORMATION ++ PLEASE BE
INFORMED THAT AS A RESULT OF
THE PURCHASE OFFER ENDED ON 23
MAR 2008 CALDRADE LTD. IS
POSSESSING WITH MORE THEN 90
PER CENT VOTING RIGHT OF SLICE.
THEREFOR CALDRADE LTD. IS
EXERCISING PURCHASE RIGHTS
FOR ALL SLICE SHARES WHICH ARE
CURRENTLY NOT INHIS OWN.
PURCHASE PRICE: HUF 1.940 PER
SHARE. PLEASE :58E::ADTX//NOTE
THAT THOSE SHARES WHICH WILL
NOT BE PRESENTED TO THE OFFER
WILL BE CANCELLED AND INVALID.
:58:SIE SELBST
TRN/REF:515220 035
:78:RUECKGABE DES BETRAGES LT.
ANZBA43 M ZWECKS RUECKGABE IN
AUD. URSPR. ZU UNSEREM ZA MIT
REF. 0170252313279065 UND IHRE
RUECKG. :42:/BNF/UNSERE REF:
Figure 1: Example application-specific text, in this
case from financial transactions.
peculiar mark-up interspersed with meaning-
ful text. They also suffer complications arising
from limited-size entry fields and a diversity
of data-entry personnel, leading to effects like
haphazard abbreviation and improper spacing,
as shown. These issues greatly complicate the
IE problem, particularly considering that adapt-
ing IE to such formats is hampered by the exis-
tence of a multitude of such ?standards? and by
lack of sufficient annotated data in each one.
A typical state-of-the-art statistical IE engine will
happily process such ?noisy? inputs, and will typ-
ically provide garbage-in/garbage-out performance,
embarrassingly reporting spurious ?information? no
human would ever mistake. Yet it is also inappro-
priate to discard such documents wholesale: even
poor-quality inputs may have relevant information
interspersed. This information can include accurate
speech-recognition output, names which are recog-
nizable even in wrong-language material, and clean
target-language passages interleaved with the mark-
up. Thus, here we address methods to make IE ro-
bust to such varied-quality inputs. Specifically, our
overall goals are
? to skip processing non-language material such
as standard or database-specific mark-up,
? to process all non-target-language text cau-
tiously, catching interspersed target-language
text as well as text which is compatible with
the target language, e.g. person names which
are the same in the target- and non-target lan-
guage, and
? to degrade gracefully when processing anoma-
lous target-language material,
while minimizing any disruption of the processing
of clean, target-language text, and avoiding any ne-
cessity for explicit pre-classification of the genre of
material being input to the system. Such explicit
classification would be impractical in the presence
of the interleaving and the unconstrained data for-
mats from unpredetermined sources.
We begin our robustness work by addressing an
important and basic IE task: mention detection
(MD). MD is the task of identifying and classifying
textual references to entities in open-domain texts.
Mentions may be of type ?named? (e.g. John, Las
Vegas), ?nominal? (e.g. engineer, dentist)
or ?pronominal? (e.g. they, he). A mention also
336
has a specific class which describes the type of en-
tity it refers to. For instance, consider the following
sentence:
Julia Gillard, prime
minister of Australia,
declared she will enhance
the country?s economy.
Here we see three mentions of one person en-
tity: Julia Gillard, prime minister, and
she; these mentions are of type named, nominal,
and pronominal, respectively. Australia and
country are mentions of type named and nominal,
respectively, of a single geopolitical entity. Thus, the
MD task is a more general and complex task than
named-entity recognition, which aims at identifying
and classifying only named mentions.
Our approach to IE has been to use language-
independent algorithms, in order to facilitate reuse
across languages, but we train them with language-
specific data, for the sake of accuracy. Therefore, in-
put is expected to be predominantly in a target lan-
guage. However, real-world data genres inevitably
include some mixed-language/non-linguistic input.
Genre-specific training is typically infeasible due
to such application-specific data sets being unanno-
tated, motivating this line of research. Therefore, the
goal of this study is to investigate schemes to make a
language-specific MD engine robust to the types of
interspersed non-target material described above. In
these initial experiments, we work with English as
the target language, though we aim to make our ap-
proach to robustness as target-language-independent
as possible.
While our ultimate goal is a language-
independent approach to robustness, in these
initial experiments, English is the target language.
However, we process mixed-language material
including real-world data with its own peculiar
mark-up, text conventions including abbreviations,
and mix of languages, with the goal of English MD.
We approach robust MD using a multi-stage strat-
egy. First, non-target-character-set passages (here,
non-Latin-alphabet) are identified and marked for
non-processing. Then, following word-tokenization,
we apply a language classifier to a sliding variable-
length set of windows in order to generate fea-
tures for each word indicative of how much the text
around that word resembles good English, primar-
ily in comparison to other Latin-alphabet languages.
These features are used in a separate maximum-
entropy classifier whose output is a single feature to
add to the MD classifier. Additional features, pri-
marily to distinguish English from non-language in-
put, are added to MD as well. An example is the
minimum of the number of letters and the number of
digits in the ?word?, which when greater than zero
often indicates database detritus. Then we run the
MD classifier enhanced with these new robustness-
oriented features. We evaluate using a detection-
error-trade-off (DET) (Martin et al, 1997) anal-
ysis, in addition to traditional precision/recall/F -
measure.
This paper is organized as follows. Section 2 dis-
cusses previous work. Section 3 describes the base-
line maximum-entropy-based MD system. Section 4
introduces enhancements to the system to achieve
robustness. Section 5 describes databases used for
experiments, which are discussed in Section 6, and
Section 7 draws conclusions and plots future work.
2 Previous work on mention detection
The MD task has close ties to named-entity recog-
nition, which has been the focus of much recent re-
search (Bikel et al, 1997; Borthwick et al, 1998;
Tjong Kim Sang, 2002; Florian et al, 2003; Bena-
jiba et al, 2009), and has been at the center of sev-
eral evaluations: MUC-6, MUC-7, CoNLL?02 and
CoNLL?03 shared tasks. Usually, in computational-
linguistics literature, a named entity represents an
instance of either a location, a person, an organi-
zation, and the named-entity-recognition task con-
sists of identifying each individual occurrence of
names of such an entity appearing in the text. As
stated earlier, in this paper we are interested in
identification and classification of textual references
to object/abstraction mentions, which can be either
named, nominal or pronominal. This task has been
a focus of interest in ACE since 2003. The recent
ACE evaluation campaign was in 2008.
Effort to handle noisy data is still limited, espe-
cially for scenarios in which the system at decoding
time does not have prior knowledge of the input data
source. Previous work dealing with unstructured
data assumes the knowledge of the input data source.
As an example, E. Minkov et al (Minkov et al,
2005) assume that the input data is text from e-mails,
and define special features to enhance the detection
of named entities. Miller et al (Miller et al, 2000)
assume that the input data is the output of a speech
or optical character recognition system, and hence
extract new features for better named-entity recog-
nition. In a different research problem, L. Yi et al
eliminate the noisy text from the document before
337
performing data mining (Yi et al, 2003). Hence,
they do not try to process noisy data; instead, they
remove it. The approach we propose in this paper
does not assume prior knowledge of the data source.
Also we do not want to eliminate the noisy data, but
rather attempt to detect the appropriate mentions, if
any, that appear in that portion of the data.
3 Mention-detection algorithm
Similarly to classical NLP tasks such as base phrase
chunking (Ramshaw and Marcus, 1999) and named-
entity recognition (Tjong Kim Sang, 2002), we for-
mulate the MD task as a sequence-classification
problem, by assigning to each word token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is out-
side any mentions. We also assign to every non-
outside label a class to specify entity type e.g. per-
son, organization, location, etc. We are interested
in a statistical approach that can easily be adapted
for several languages and that has the ability to
integrate easily and make effective use of diverse
sources of information to achieve high system per-
formance. This is because, similar to many NLP
tasks, good performance has been shown to depend
heavily on integrating many sources of informa-
tion (Florian et al, 2004). We choose a Maximum
Entropy Markov Model (MEMM) as described pre-
viously (Florian et al, 2004; Zitouni and Florian,
2009). The maximum-entropy model is trained us-
ing the sequential conditional generalized iterative
scaling (SCGIS) technique (Goodman, 2002), and it
uses a Gaussian prior for regularization (Chen and
Rosenfeld, 2000)1.
3.1 Mention detection: standard features
The featues used by our mention detection systems
can be divided into the following categories:
1. Lexical Features Lexical features are imple-
mented as token n-grams spanning the current
token, both preceding and following it. For a
token xi, token n-gram features will contain the
previous n?1 tokens (xi?n+1, . . . xi?1) and the
following n? 1 tokens (xi+1, . . . xi+n?1). Set-
ting n equal to 3 turned out to be a good choice.
2. Gazetteer-based Features The gazetteer-
based features we use are computed on tokens.
1Note that the resulting model cannot really be called a
maximum-entropy model, as it does not yield the model which
has the maximum entropy (the second term in the product), but
rather is a maximum-a-posteriori model.
The gazetteers consist of several class of
dictionaries: including person names, country
names, company names, etc. Dictionar-
ies contain single names such as John or
Boston, and also phrases such as Barack
Obama, New York City, or The United
States. During both training and decoding,
when we encounter in the text a token or a
sequence of tokens that completely matches an
entry in a dictionary, we fire its corresponding
class.
The use of this framework to build MD systems
for clean English text has given very competitive re-
sults at ACE evaluations (Florian et al, 2006). Try-
ing other classifiers is always a good experiment,
which we didn?t pursue here for two reasons: first,
the MEMM system used here is state-of-the-art, as
proven in evaluations and competitions ? while it is
entirely possible that another system might get better
results, we don?t think the difference would be large.
Second, we are interested in ways of improving per-
formance on noisy data, and we expect any system
to observe similar degradation in performance when
presented with unexpected input ? showing results
for multiple classifier types might very well dilute
the message, so we stuck to one classifier type.
4 Enhancements for robustness
As stated above, our goal is to skip spans of charac-
ters which do not lend themselves to target-language
MD, while minimizing impact on MD for target-
language text, with English as the initial target lan-
guage for our experiments. More specifically, our
task is to process data automatically in any unprede-
termined format from any source, during which we
strive to avoid outputting spurious mentions on:
? non-language material, such as mark-up tags
and other data-set detritus, as well as non-text
data such as code or binaries likely mistakenly
submitted to the MD system,
? non-target-character-set material, here, non-
Latin-alphabet material, such as Arabic and
Chinese in their native character sets, and
? target-character-set material not in the target
language, here, Latin-alphabet languages other
than English.
It is important to note that this is not merely
a document-classification problem; this non-target
data is often interspersed with valid input text.
338
Mark-up is the obvious example of interspersing;
however, other categories of non-target data can also
interleave tightly with valid input. A few examples:
? English text is sometimes infixed right in a Chi-
nese sentence, such as
? some translation algorithms will leave un-
changed an untranslatable word, or will
transliterate it into the target language using a
character convention which may not be a stan-
dard known to the MD engine, and
? some target-alphabet-but-non-target-language
material will be compatible with the target
language, particularly people?s names. An
example with English as the target lan-
guage is Barack Obama in the Spanish
text ...presidente de Estados
Unidos, Barack Obama, dijo el
da 24 que ....
Therefore, to minimize needless loss of process-
able material, a robustness algorithm ideally does a
sliding analysis, in which, character-by-character or
word-by-word, material may be deemed to be suit-
able to process. Furthermore, a variety of strategies
will be needed to contend with the diverse nature of
non-target material and the patterns in which it will
appear among valid input.
Accordingly, the following is a summary of algo-
rithmic enhancements to MD:
1. detection of standard file formats, such as
SGML, and associated detagging,
2. segmentation of the file into target- vs. non-
target-character-set passages, such that the lat-
ter not be processed further,
3. tokenization to determine word and sentence
units, and
4. MD, augmented as follows:
? Sentence-level categorization of likeli-
hood of good English.
? If ?clean? English was detected, run the
same clean baseline model as described in
Section 3.
? If the text is determined to be a
bad fit to English, run an alternate
maximum-entropy model that is heavily
based on gazetteers, using only context-
independent (e.g. primarily gazetteer-
based) features, to catch isolated ob-
vious English/English-compatible names
embedded in otherwise-foreign text.
? If in between ?clean? and ?bad?, use
a ?mixed? maximum-entropy MD model
whose training data and feature set are
augmented to handle interleaving of En-
glish with mark-up and other languages.
These MD-algorithm enhancements will be de-
scribed in the following subsections.
4.1 Detection and detagging for standard file
formats
Some types of mark-up are well-known standards,
such as SGML (Warmer and van Egmond, 1989).
Clearly the optimal way of dealing with them is to
apply detectors of these specific formats, and associ-
ated detaggers, as done previously (Yi et al, 2003).
For this reason, standard mark-up is not a subject of
the current study; rather, our concern is with mark-
up peculiar to specific data sets, as described above,
and so while this step is part of our overall strategy,
it is not employed in the present experiments.
4.2 Character-set segmentation
Some entity mentions may be recognizable in a non-
target language which shares the target-language?s
character set, for example, a person?s name recog-
nizable by English speakers in an otherwise-not-
understandable Spanish sentence. However, non-
target character sets, such as Arabic and Chinese
when processing English, represent pure noise for
an IE system. Therefore, deterministic character-
set segmentation is applied, to mark non-target-
character-set passages for non-processing by the re-
mainder of the system, or, in a multilingual system,
to be diverted to a subsystem suited to process that
character set. Characters which can be ambiguous
with regard to character set, such as some punctua-
tion marks, are attached to target-character-set pas-
sages when possible, but are not considered to break
non-target-character-set passages surrounding them
on both sides.
4.3 Tokenization
Subsequent processing is based on determination of
the language of target-alphabet text. The fundamen-
tal unit of such processing is target-alphabet word,
necessitating tokenization at this point into word-
level units. This step includes punctuation sepa-
339
ration as well as the detction of sentence bound-
ary (Zimmerman et al, 2006).
4.4 Robust mention detection
After preprocessing steps presented earlier, we de-
tect mentions using a cascaded approach that com-
bines several MD classifiers. Our goal is to select
among maximum-entropy MD classifiers trained
separately to represent different degrees of ?nois-
iness? occurring in many genres of data, includ-
ing machine-translation output, informal communi-
cations, mixed-language material, varied forms of
non-standard database mark-up, etc. We somewhat-
arbitrarily choose to employ three classifiers as de-
scribed below. We select a classifier based on a
sentence-level determination of the material?s fit to
the target language. First, we build an n-gram lan-
guage model on clean target-language training text.
This language model is used to compute the perplex-
ity (PP ) of each sentence during decoding. The
PP indicates the quality of the text in the target-
language (i.e. English) (Brown et al, 1992); the
lower the PP , the cleaner the text. A sentence
with a PP lower than a threshold ?1 is considered
?clean? and hence the ?clean? baseline MD model
described in Section 3 is used to detect mentions
of this sentence. The clean MD model has access
to standard features described in Section 3.1. In
the case where a sentence looks particularly badly
matched to the target language, defined as PP > ?2,
we use a ?gazetteer-based? model based on a dic-
tionary look-up to detect mentions; we retreat to
seeking known mentions in a context-independent
manner reflecting that most of the context consists
of out-of-vocabulary words. The gazetteer-based
MD model has access only to gazetteer information
and does not look to lexical context during decod-
ing, reflecting the likelihood that in this poor ma-
terial, words surrounding any recognizable mention
are foreign and therefore unusable. In the case of an
in-between determination, that is, a sentence with
?1 < PP < ?2, we use a ?mixed? MD model, based
on augmenting the training data set and the feature
set as described in the next section. The values of ?1
and ?2 are estimated empirically on a separate devel-
opment data set that is also used to tune the Gaussian
prior (Chen and Rosenfeld, 2000). This set contains
a mix of clean English and Latin-alphabet-but-non-
English text that is not used for traning and evalua-
tion.
The advantage of this combination strategy is that
we do not need pre-defined knowledge of the text
source in order to apply an appropriate model. The
selection of the appropriate model to use for de-
coding is done automatically based on PP value of
the sentence. We will show in the experiments sec-
tion how this combination strategy is effective not
only in maintaining good performance on a clean
English text but also in improving performance on
non-English data when compared to other source-
specific MD models.
4.5 Mixed mention detection model
The mixed MD model is designed to process ?sen-
tences? mixing English with non-English, whether
foreign-language or non-language material. Our
approach is to augment model training compared
to the clean baseline by adding non-English,
mixed-language, and non-language material, and
to augment the model?s feature set with language-
identification features more localized than the
sentence-level perplexity described above, as well as
other features designed primarily to distinguish non-
language material such as mark-up codes.
4.5.1 Language-identification features
We apply an n-gram-based language classi-
fier (Prager, 1999) to variable-length sliding win-
dows as follows. For each word, we run 1- through
6-preceding-word windows through the classifier,
and 1- through 6-word windows beginning with the
word, for a total of 12 windows, yielding for each
window a result like:
0.235 Swedish
0.148 English
0.134 French
...
For each of the 12 results, we extract three fea-
tures: the identity of the top-scoring language, here,
Swedish; the confidence score in the top-scoring
language, here, 0.235; and the score difference be-
tween the target language (English for these ex-
periments) and the top-scoring non-target language,
here, 0.148 ? 0.235 = ?0.087. Thus we have
a 36-feature vector for each word. We bin these
and use them as input to a maximum-entropy clas-
sifier (separate from the MD classifier) which out-
puts ?English? or ?Non-English?, and a confidence
score. These scores in turn are binned into six cate-
gories to serve as a ?how-English-is-it? feature in the
augmented MD model. The language-identification
classifier and the maximum-entropy ?how-English?
classifier are each trained on text data separate from
340
each other and from the training and test sets for
MD.
4.5.2 Additional features
The following features are designed to capture
evidence of whether a ?word? is in fact linguistic
material or not: number of alphabetic characters,
number of characters, maximum consecutive rep-
etitions of a character, numbers of non-alphabetic
and non-alphanumeric characters, fraction of char-
acters which are alphabetic, fraction alphanumeric,
and number of vowels. These features are part of the
augmentation of the mixed MD model relative to the
clean MD model.
5 Data sets
Four data sets are used for our initial experiments.
One, ?English?, consists of 367 documents total-
ing 170,000 words, drawn from web news stories
from various sources and detagged to be plain text.
This set is divided into 340 documents as a train-
ing set and 27 for testing, annotated as described in
more detail elsewhere (Han, 2010). These data av-
erage approximately 21 annotated mentions per 100
words.
The second set, ?Latin?, consists of 23 detagged
web news articles from 11 non-English Latin-
alphabet languages totaling 31,000 words. Of these
articles, 12 articles containing 19,000 words are
used as a training set, with the remaining used for
testing, and each set containing all 11 languages.
They are annotated using the same annotation con-
ventions as ?English?, and from the perspective of
English; that is, only mentions which would be clear
to an English speaker are labeled, such as Barack
Obama in the Spanish example in Section 4. For
this reason, these data average only approximately 5
mentions per 100 words.
The third, ?Transactions?, consists of approxi-
mately 60,000 words drawn from a text data set
logging real financial transactions. Figure 1 shows
example passages from this database, anonymized
while preserving the character of the content.
This data set logs transactions by a staff of
customer-service representatives. English is the pri-
mary language, but owing to international clientele,
occasionally representatives communicate in other
languages, such as the German here, or in English
but mentioning institutions in other countries, here, a
Czech bank. Interspersed among text are codes spe-
cific to this application which delineate and identify
various information fields and punctuate long pas-
sages. The application also places constraints on
legal characters, leading to the unusual representa-
tion of underline and the ?at? sign as shown, mak-
ing for an e-mail address which is human-readable
but likely not obvious to a machine. Abbreviations
represent terms particularly common in this appli-
cation area, though they may not be obvious with-
out adapting to the application; these include stan-
dards like HUF, a currency code which stands for
Hungarian forint, and financial-transaction peculiar-
ities like BNF for ?beneficiary? as seen in Figure 1.
In short, good English is interspersed with non-
language content, foreign-language text, and rough
English like data-entry errors and haphazard abbre-
viations. These data average 4 mentions per 100
words.
Data sets with peculiarities analogous to those in
this Transactions set are commonplace in a variety
of settings. Training specific to data sets like this is
often infeasible due to lack of labeled data, insuffi-
cient data for training, and the multitude of such data
formats. For this reason, we do not train on Transac-
tions, letting our testing on this data set serve as an
example of testing on such data formats unseen.
6 Experiments
MD systems were trained to recognize the 116
entity-mention types shown in Table 1, annotated as
described previously (Han, 2010). The clean-data
classifier was trained on the English training data us-
ing the feature set described in Section 3.1. The clas-
sifier for ?mixed?-quality data and the ?gazetteer?
model were each trained on that set plus the ?Latin?
training set and the supplemental set. In addition,
?mixed? training included the additional features de-
scribed in Section 4.5. The framework used to build
the baseline MD system is similar to the one we used
in the ACE evaluation2. This system has achieved
competitive results with an F -measure of 82.7 when
trained on the seven main types of ACE data with
access to wordnet and part-of-speech-tag informa-
tion as well as output of other MD and named-entity
recognizers (Zitouni and Florian, 2008).
It is instructive to evaluate on the individual com-
ponent systems as well as the combination, despite
the fact that the individual components are not well-
suited to all the data sets, for example, the mixed
and gazetteer systems being a poorer fit to the En-
glish task than the baseline, and vice versa for the
2NIST?s ACE evaluation plan:
http://www.nist.gov/speech/tests/ace/index.htm
341
age event-custody facility people date
animal event-demonstration food percent duration
award event-disaster geological-object person e-mail-address
cardinal event-legal geo-political product measure
disease event-meeting law substance money
event event-performance location title-of-a-work phone-number
event-award event-personnel ordinal vehicle ticker-symbol
event-communication event-sports organ weapon time
event-crime event-violence organization web-address
Table 1: Entity-type categories used in these experiments. The eight in the right-most column are not
further distinguished by mention type, while the remaining 36 are further classified as named, nominal or
pronominal, for a total of 36 ? 3 + 8 = 116 mention labels.
English Latin Transactions
P R F P R F P R F
Clean 78.7 73.6 76.1 16.0 40.0 22.9 19.5 32.2 24.3
Mixed 77.9 69.7 73.6 78.5 55.9 65.3 37.1 47.8 41.7
Gazetteer 76.9 66.2 71.1 77.8 55.5 64.8 36.5 47.5 41.3
Combination 78.1 73.2 75.6 80.4 56.0 66.0 38.5 49.1 43.2
Table 2: Performance of clean, mixed, and gazetteer-based mention detection systems as well as their com-
bination. Performance is presented in terms of Precision (P), Recall (R), and F -measure (F).
non-target data sets. Precision/recall/F -measure re-
sults are shown in Table 2. Not surprisingly, the
baseline system, intended for clean data, performs
poorly on noisy data. The mixed and gazetteer sys-
tems, having a variety of noisy data in their train-
ing set, perform much better on the noisy conditions,
particularly on Latin-alphabet-non-English data be-
cause that is one of the conditions included in its
training, while Transactions remains a condition not
covered in the training set and so shows less im-
provement. However, because the mixed classifier,
and moreso the gazetteer classifier, are oriented to
noisy data, on clean data they suffer in performance
by 2.5 and 5 F -measure points, respectively. But
system combination serves us well: it recovers all
but 0.5 F -measure point of this loss, while also ac-
tually performing better on the noisy data sets than
the two classifiers specifically targeted toward them,
as can be seen in Table 2. It is important to note
that the major advantage of using the combination
model is the fact that we do not have to know the
data source in order to select the appropriate MD
model to use. We assume that the data source is
unknown, which is our claim in this work, and we
show that we obtain better performance than using
source-specific MD models. This reflects the fact
that a noisy data set will in fact have portions with
varying degrees of ?noise?, so the combination out-
performs any single model targeted to a single par-
ticular level of noise, enabling the system to con-
tend with such variability without the need for pre-
segregating sub-types of data for noise level. The
obtained improvement from the system combination
over all other models is statistically significant based
on the stratified bootstrap re-sampling significance
test (Noreen, 1989). We consider results statistically
significant when p < 0.05, which is the case in this
paper. This approach was used in the named-entity-
recognition shared task of CoNNL-20023.
It should be noted that some completely-non-
target types of data, such as non-target-character set
data, have been omitted from analysis here. In-
cluding them would make our system look compar-
atively stronger, as they would have only spurious
mentions and so generate false alarms but no correct
mentions in the baseline system, while our system
deterministically removes them.
As mentioned above, we view MD robustness pri-
marily as an effort to eliminate, relative to a base-
line system, large volumes of spurious ?mentions?
detected in non-target input content, while minimiz-
3http://www.cnts.ua.ac.be/conll2002/ner/
342
(a) DET plot for clean (baseline), mixed, gazetteer,
and combination MD systems on the Latin-alphabet-
non-English text. The clean system (upper curve)
performs far worse than the other three systems de-
signed to provide robustness; these systems in turn
perform nearly indistinguishably.
(b) DET plot for clean (baseline), mixed, gazetteer,
and combination MD systems on the Transactions
data set. The clean system (upper/longer curve)
reaches far higher false-alarm rates, while never ap-
proaching the lower miss rates achievable by any of
the other three systems, which in turn perform com-
parably to each other.
Figure 2: DET plots for Latin-alphabet-non-English and Transactions data sets
ing disruption of detection in target input. A sec-
ondary goal is recall in the event of occasional valid
mentions in such non-target material. Thus, as in-
put material degrades, precision increases in impor-
tance relative to recall. As such, we view precision
and recall asymmetrically on this task, and so rather
than evaluating purely in terms of F -measure, we
perform a detection-error-trade-off (DET) (Martin
et al, 1997) analysis, in which we plot a curve of
miss rate on valid mentions vs. false-alarm rate, with
the curve traced by varying a confidence threshold
across its range. We measure false-alarm and miss
rates relative to the number of actual mentions anno-
tated in the data set:
FA rate = # false alarms# annotated mentions (1)
Miss rate = # misses# annotated mentions (2)
where false alarms are ?mentions? output by the sys-
tem but not appearing in annotation, while misses
are mentions which are annotated but do not ap-
pear in the system output. Each mention is treated
equally in this analysis, so frequently-recurring en-
tity/mention types weigh on the results accordingly.
Figure 2a shows a DET plot for the clean, mixed,
gazetteer, and combination systems on the ?Latin?
data set, while Figure 2b shows the analogous plot
for the ?Transactions? data set. The drastic gains
made over the baseline system by the three experi-
mental systems are evident in the plots. For exam-
ple, on Latin, choosing an operating point of a miss
rate of 0.6 (nearly the best achievable by the clean
system), we find that the robustness-oriented sys-
tems eliminate 97% of the false alarms of the clean
baseline system, as the plot shows false-alarm rates
near 0.07 compared to the baseline?s of 2.08. Gains
on Transaction data are more modest, owing to this
case representing a data genre not included in train-
ing. It should be noted that the jaggedness of the
Transaction curves traces to the repetitive nature of
some of the terms in this data set.
In making a system more oriented toward robust-
ness in the face of non-target inputs, it is important
to quantify the effect of these systems being less-
oriented toward clean, target-language text. Figure 3
shows the analogous DET plot for the English test
set, showing that achieving robustness through the
combination system comes at a small cost to accu-
racy on the text the original system is trained to pro-
cess.
7 Conclusions
For information-extraction systems to be useful,
their performance must degrade gracefully when
confronted with inputs which deviate from ideal
and/or derive from unknown sources in unknown
formats. Imperfectly-translated, mixed-language,
marked-up text and non-language material must not
343
Figure 3: DET plot for clean (baseline), mixed,
gazetteer, and combination MD systems on clean English
text, verifying that performance by the clean system (low-
est curve) is very closely approximated by the combina-
tion system (second-lowest curve), while the mixed sys-
tem performs somewhat worse and the gazetteer system
(top curve), worse still, reflecting that these systems are
increasingly oriented toward noisy inputs.
be processed in a garbage-in-garbage-out fashion
merely because the system was designed only to
handle clean text in one language. Thus we have em-
barked on information-extraction-robustness work,
to improve performance on imperfect inputs while
minimizing disruption of processing of clean text.
We have demonstrated that for one IE task, mention
detection, a multi-faceted approach, motivated by
the diversity of input data imperfections, can elimi-
nate a large proportion of the spurious outputs com-
pared to a system trained on the target input, at a
relatively small cost of accuracy on that target input.
This outcome is achieved by a system-combination
approach in which a perplexity-based measure of
how well the input matches the target language is
used to select among models designed to deal with
such varying levels of noise. Rather than relying on
explicit recognition of genre of source data, the ex-
perimental system merely does its own assessment
of how much each sentence-sized chunk matches the
target language, an important feature in the case of
unknown text sources.
Chief among directions for further work is to con-
tinue to improve performance on noisy data, and to
strengthen our findings via larger data sets. Addi-
tionally, we look forward to expanding analysis to
different types of imperfect input, such as machine-
translation output, different types of mark-up, and
different genres of real data. Further work should
also explore the degree to which the approach to
achieving robustness must vary according to the tar-
get language. Finally, robustness work should be ex-
panded to other information-extraction tasks.
Acknowledgements
The authors thank Ben Han, Anuska Renta,
Veronique Baloup-Kovalenko and Owais Akhtar for
their help with annotation. This work was supported
in part by DARPA under contract HR0011-08-C-
0110.
References
Y. Benajiba, M. Diab, and P. Rosso. 2009. Arabic named
entity recognition: A feature-driven study. In the spe-
cial issue on Processing Morphologically Rich Lan-
guages of the IEEE Transaction on Audio, Speech and
Language.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of ANLP-97, pages 194?201.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, J. C.
Lai, and R. L. Mercer. 1992. An estimate of an up-
per bound for the entropy of English. Computational
Linguistics, 18(1), March.
S. Chen and R. Rosenfeld. 2000. A survey of smooth-
ing techniques for ME models. IEEE Transaction on
Speech and Audio Processing.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In Conference on Computational Natural Lan-
guage Learning - CoNLL-2003, Edmonton, Canada,
May.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proceedings of HLT-NAACL 2004, pages
1?8.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in men-
tion detection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 473?480, Sydney, Australia,
July. Association for Computational Linguistics.
J. Goodman. 2002. Sequential conditional generalized
iterative scaling. In Proceedings of ACL?02.
D. B. Han. 2010. Klue annotation guidelines - version
2.0. Technical Report RC25042, IBM Research, Au-
gust.
344
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki. 1997. The DET curve in assessment
of detection task performance. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), pages 1895?1898. Rhodes,
Greece.
D. Miller, S. Boisen, R. Schwartz, R. Stone, and
R. Weischedel. 2000. Named entity extraction from
noisy input: speech and OCR. In Proceedings of the
sixth conference on Applied natural language process-
ing, pages 316?324, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from email: Applying named
entity recognition to informal text. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 443?450, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley Sons.
J. F. Pitrelli, B. L. Lewis, E. A. Epstein, M. Franz,
D. Kiecza, J. L. Quinn, G. Ramaswamy, A. Srivas-
tava, and P. Virga. 2008. Aggregating Distributed
STT, MT, and Information Extraction Engines: The
GALE Interoperability-Demo System. In Interspeech.
Brisbane, NSW, Australia.
J. M. Prager. 1999. Linguini: Language identification for
multilingual documents. In Journal of Management
Information Systems, pages 1?11.
L. Ramshaw and M. Marcus. 1999. Text chunking using
transformation-based learning. In S. Armstrong, K.W.
Church, P. Isabelle, S. Manzi, E. Tzoukermann, and
D. Yarowsky, editors, Natural Language Processing
Using Very Large Corpora, pages 157?176. Kluwer.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independentnamed entity
recognition. In Proceedings of CoNLL-2002, pages
155?158. Taipei, Taiwan.
J. Warmer and S. van Egmond. 1989. The implementa-
tion of the Amsterdam SGML parser. Electron. Publ.
Origin. Dissem. Des., 2(2):65?90.
L. Yi, B. Liu, and X. Li. 2003. Eliminating noisy in-
formation in web pages for data mining. In KDD ?03:
Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 296?305, New York, NY, USA. ACM.
M. Zimmerman, D. Hakkani-Tur, J. Fung, N. Mirghafori,
L. Gottlieb, E. Shriberg, and Y. Liu. 2006. The
ICSI+ multilingual sentence segmentation system. In
Interspeech, pages 117?120, Pittsburgh, Pennsylvania,
September.
I. Zitouni and R. Florian. 2008. Mention detection
crossing the language barrier. In Proceedings of
EMNLP?08, Honolulu, Hawaii, October.
I. Zitouni and R. Florian. 2009. Cross-language informa-
tion propagation for Arabic mention detection. ACM
Transactions on Asian Language Information Process-
ing (TALIP), 8(4):1?21.
345
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 993?1001,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Enhancing Mention Detection
using Projection via Aligned Corpora
Yassine Benajiba
Center for Computational Learning Systems
Columbia University, NY
ybenajiba@ccls.columbia.edu
Imed Zitouni
IBM T.J. Watson Research Center
Yorktown Heights, NY
izitouni@us.ibm.com
Abstract
The research question treated in this paper
is centered on the idea of exploiting rich re-
sources of one language to enhance the per-
formance of a mention detection system of an-
other one. We successfully achieve this goal
by projecting information from one language
to another via a parallel corpus. We exam-
ine the potential improvement using various
degrees of linguistic information in a statisti-
cal framework and we show that the proposed
technique is effective even when the target
language model has access to a significantly
rich feature set. Experimental results show
up to 2.4F improvement in performance when
the system has access to information obtained
by projecting mentions from a resource-rich-
language mention detection system via a par-
allel corpus.
1 Introduction
The task of identifying and classifying entity textual
references in open-domain texts, i.e. the Mention
Detection (MD) task, has become one of the most
important subtasks of Information Extraction (IE).
It might intervene both as one step to structure nat-
ural language texts or as a text enrichment prepro-
cessing step to help other Natural Language Process-
ing (NLP) applications reach higher accuracy. Simi-
larly to the Automatic Content Extraction (ACE) 1
nomenclature, we consider that a mention can be
either named (e.g., John, Chicago), nominal (e.g.,
president, activist) or pronominal (e.g., he, she). It
has also a specific class which describes the type of
the entity it refers to. For instance, in the sentence:
1http://www.itl.nist.gov/iad/mig/tests/ace/2007/doc/ace07-
evalplan.v1.3a.pdf
Michael Bloomberg, the Mayor of NYC, declared
his war on tobacco and sugary drinks in the city.
we find the mentions ?Michael Bloomberg?, ?Mayor?
and ?his? of the same person entity. Their types
are named, nominal and pronominal, respectively.
?NYC? and ?city?, on the other hand, are mentions
of the same geopolitical (GPE) entity of type named
and nominal, respectively. Consequently, MD is a
more general and complex task than the well known
Named Entity Recognition (NER) task which aims
solely at the identification and classification of the
named mentions.
The difficulty of the MD task is directly related
to the nature of the language and the linguistic re-
sources available, i.e. it is easier to build accu-
rate MD systems for languages with a simple mor-
phology and a high amount of linguistic resources.
For this reason, we explore the idea of using an
MD system, which has been designed and built for
a resource-rich language (RRL), to help enhance
the performance of an MD system in a target lan-
guage (TL). More specifically, the goal of the re-
search work we present in this paper is to employ
the richness of English, in terms of natural lan-
guage resources, to raise the accuracy of MD sys-
tems in other languages. For instance, an English
MD system might achieve a performance of F?=1-
measure=82.7 (Zitouni and Florian, 2009) when it
resorts to a rich set of features extracted from di-
verse resources, namely: part-of-speech, chunk in-
formation, syntactic parse trees, word sense infor-
mation, WordNet information and information from
the output of other mention detection classifiers. In
this paper, our research question revolves around in-
vestigating an adequate approach to use such a sys-
tem to the benefit of other languages such as Arabic,
Chinese, French or Spanish MD systems, which also
993
have annotated resources but not of the same quan-
tity and/or quality as English.
In this paper, we have targeted English and Arabic
as the RRL and TL, respectively, because:
1. We have a very competitive English MD system;
2. The linguistic resources available for the Arabic
language allow a simulation of different TL richness
levels; and
3. The use of two languages of an utterly different
nature makes the extrapolation of the results to other
languages possible.
Our hypothesis might be expressed as follows: us-
ing an MD system resorting to a rich feature set (i.e.
the RRL MD system) to boost a MD system perfor-
mance in a TL can be very beneficial if the ?donor?
system surpasses its TL counterpart in terms of re-
sources. To test this hypothesis, we have projected
MD tags from RRL to TL via a parallel corpus, and
then extracted several linguistic features about the
automatically tagged words. Thereafter, we have
conducted experiments adding these new features to
the TL baseline MD system. In order to have a com-
plete picture on the impact of these new features, we
have used TL baseline systems resorting to a varied
amount of features, starting with a case employing
only lexical information to a case where we use all
the resources we could gather for the TL. Experi-
ments show that the gain is always statistically sig-
nificant and it reaches its maximum when only very
basic features are used in the baseline TL MD sys-
tem.
2 Mention Detection
Similarly to classical NLP tasks, such as Base
Phrase Chunking (Ramshaw and Marcus, 1999)
(BPC) or NER (Tjong Kim Sang, 2002), we formu-
late the MD task as a sequence classification prob-
lem, i.e. the classifier assigns to each token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is outside
any mentions. It also assigns to every non outside
mention a class to specify its type: e.g., person, or-
ganization, location, etc. In this study, we chose the
Maximum Entropy Markov Model (MEMM hence-
forth) approach because it can easily integrate arbi-
trary types of information in order to make a clas-
sification decision. To train our models, we have
used the Sequential Conditional Generalized Itera-
tive Scaling (SCGIS) technique (Goodman, 2002).
This techniques uses a Gaussian prior for regular-
ization (Chen and Rosenfeld, 2000). The features
used by our MD systems can be divided into the fol-
lowing categories:
1- Lexical: these are token n-grams directly neigh-
boring the current token on both sides, i.e. left and
right. Empirical results have shown that the optimal
span is n = 3.
2- Syntactic: they consist of the outcomes of several
Part-Of-Speech (POS) taggers and BPCs trained on
different corpora and different tag-sets in order to
provide the MD system with a wider variety of in-
formation. Our model uses the POS and BPC in-
formation appearing in window of 5 (current, two
previous, and two next) jointly with the tokens.
Both the English and the Arabic MD systems have
access to lexical and syntactic features. The former
one, however, also employs a set of features ob-
tained from the output of other MD classifiers. In
order to provide the MD system with complemen-
tary information, these classifiers are trained on dif-
ferent datasets annotated for different mention types,
e.g. dates or occupation references (not used in our
task).
3 Annotation, Projection and Feature
Extraction
We remind the reader that our main goal is to use
an RRL MD system to enhance the performance of
an MD system in another language, i.e. the TL. In
order to achieve this goal, we propose an approach
that uses an RRL-to-TL parallel corpus to bridge be-
tween these two languages. This approach performs
in three main steps, namely: annotation, projection
and feature extraction. In this section, we describe
in details each of these steps.
3.1 Annotation
This first step consists of MD tagging of the RRL
side of the parallel corpus. Because in our case study
we have chosen English as the RRL, we have used
an accurate English MD system to perform the an-
notation step. Our English MD system achieves an
F-measure of 82.7 (Zitouni and Florian, 2009) and
has achieved significantly competitive results at the
ACE evaluation campaign.
3.2 Projection
Once the RRL side of the parallel corpus is accu-
rately augmented with MD tags, the projection step
comes to transfer those tags to the TL side, Arabic
in our case study, using the word alignment informa-
tion. We illustrate the projection step with a relevant
example. Let consider the following MD tagged En-
glish sentence:
994
Bill/B-PER-NAM Clinton/I-PER-NAM is visiting
North/B-GPE-NAM Korea/I-GPE-NAM today
where ?Bill Clinton? is a named person mention and
?North Korea? is a named geopolitical entity (GPE)
one. A potential Arabic translation of this sentence
would be:
??J
? @

?J
?A?
??@ AK
P?? P?
	QK

	
??

J
	
J
?? ?J
K.
which might be transliterated as:
byl klyntwn yzwr kwryA Al$mAlyA Alywm
After projecting the English mentions to the Ara-
bic text, we obtain the following:
byl/B-PER-NAM klyntwn/I-PER-NAM yzwr
kwryA/B-GPE-NAM Al$mAlyp/I-GPE-NAM
Alywm
This tagged version of the Arabic text is provided to
the third module of the process responsible on fea-
ture extraction (see Subsection 3.3). It is, however,
pertinent to point out that the example we have used
for illustration is relatively simple in the sense that
almost all English and Arabic words have a 1-to-1
mapping. In real world translation (both human and
automatic), one should expect to see 1-to-n, n-to-1
mappings as well as unmapped words on both sides
of the parallel corpus rather frequently.
As stated by (Klementiev and Roth, 2006), the pro-
jection of NER tags is easier in comparison to pro-
jecting other types of annotations such as POS-tags
and BPC2, mainly because:
1. Not all the words are mentions: once we have pro-
jected the tags of the mentions from the RRL to TL
side, the rest of tokens are simply considered as out-
side any mentions. This is different from the POS-
tag and BPC where all the words are assigned a tag
and thus when a word is unmapped, further process-
ing is required (Yarowsky et al, 2001);
2. In case of a 1-to-n mapping, the target n
words are assigned the same class: for instance, let
consider the English GPE named mention ?North-
Korea?. The segmented version of its Arabic transla-
tion would be ?

?J
?A??
? ?@ AK
P??? (kwrya Al $mAlyp).
The projection process consists in simply assigning
the same class, i.e. GPE, to all Arabic tokens. The
problem takes another dimension, however, in the
case of propagating the POS-tags, because ?North?
is a NNP aligned with the determinant (DET) ?Al?
and the NNP ?$mAlyp?. Additional processing is
needed to handle this difference of tags on the two
2The claim is also valid for MD because it is the same type
of annotation.
sides.
3. In case of n-to-1 mapping, the TL side word is
simply assigned the class propagated from the RRL
side. For instance, if on the English side we have the
named person multi-word mention ?Ben Moussa?,
translated into the one-word mention ?????	JK. (bn-
mwsY) on the Arabic side, then projection consists
of simply assigning the person named tag to the Ara-
bic word.
However, in our research study, new challenges
arose because our RRL data are automatically an-
notated, which is different from what has been re-
ported in the research works we have mentioned be-
fore, i.e. (Yarowsky et al, 2001) and (Klementiev
and Roth, 2006), where gold annotated data were
used. In order to relax the impact of the noise intro-
duced by the English MD system, we :
1. use mention ?splits? to filter annotation errors:
We assume that when a sequence of tokens is tagged
as a mention on the RRL side, its TL counterpart
should be an uninterrupted sequence of tokens as
well. When the RRL MD system captures incor-
rectly the span of a mention, e.g. in the sentence
?Dona Karan international reputation of ...?, the
RRL MD system might mistakenly tag ?Dona Karan
international? as an organization mention instead of
tagging ?Dona Karan? as a person mention. It is pos-
sible to detect this type of errors on the TL side be-
cause ?dwnA kArAn? (Dona Karan) is distant from
?Al EAlmyp? (international), i.e. they do not form
an uninterrupted token sequence. We use this ?split?
in the mentions as information in order to not use
these mentions in the feature extraction step (see
Subsection 3.3).
2. do not use the projected mentions directly for
training: Instead, we use these tags as additional
features to our TL baseline model and allow our
MEMM classifier to weigh them according to their
relevance to each mention type.
3.3 Feature Extraction
At this point, the parallel corpus should be anno-
tated with mentions on both of its sides. Where
the RRL side is tagged using the English MD
system during the annotation step (c.f section 3.1)
while the TL side is annotated by the propagation
of these MD tags via the parallel corpus in the
projection step (c.f. section 3.2). In this third step,
the goal is to extract pertinent linguistic features
of the automatically tagged TL corpus to enhance
MD model in the TL. The explored features are as
follows:
995
1. Gazetteers: we group mentions by class in
different dictionaries. During both training and
decoding, when we encounter a token or a sequence
of tokens that is part of a dictionary, we fire its
corresponding class; the feature is fired only when
we find a complete match between sequence of
tokens in the text and in the dictionary.
2. Model-based features: it consists of building a
model on the automatically tagged TL side of the
parallel corpus. The output of this model is used
as a feature to enhance MD model in the target
language. However, it is also possible to use this
model to directly tag text in the TL. This would
be useful in cases where we do not have any TL
annotated data.
3. n-gram context features: it consists of using
the annotated corpus in the TL to collect n-gram
tokens surrounding a mention. We organize those
contexts by mention type and we use them to
tag tokens which appear in the same context
in both the training and decoding sets. These
tags will be used as additional feature in the
MD model. For instance, if we consider that
the person mention 	?
?k ?@Y? (SdAm Hsyn -
Sadam Husein) appears in the following sentence:
C ?A
	
? A?A
	
?
	
 ?

@Q

K

	?
?k ?@Y?
	
?

@ ??

@ hQ??
which might be transliterated as: SrH Ams An SdAm
Hsyn ytrAs nZAmA fA$lA and translated to English
as: declared yesterday that Sadam Husein governs
a failed system
the context n-grams that would be extracted are:
. Left n-grams: W?1=
	
?

@ (An - that),
W?2=
	
?

@ ??

@ (Ams An - yesterday that), etc.
. Right n-grams: W+1=?

@Q

K
 (ystrAs - governs),
W+2= A?A
	
?
	
 ?

@Q

K
 (ytrAs nZAmA - governs a sys-
tem), etc.
. Left and right n-grams: a joint of the two previ-
ous features, W?i and W+i.
For both training and test data we create a new
feature stream where we indicate that a token se-
quence is a mention if it appears in the same n-gram
context.
4. Head-word based features: it considers that
the lexical context in which the mention appeared
is the sequence of the parent sub-trees head words
in a parse-tree. For instance, if we consider the sen-
tence which we have used in the previous example,
the corresponding parse tree is shown in Figure 1.
The parent sub-tree heads of ?SdAm Hsyn? are
S
VPp3hhhhhhh

(((((((
SrHh3 NP
Ams
SBARp2hhhhhhhh
((((((((
Anh2 Sp1PPPP

NP
aaa
!!!
SdAm Hsyn
VP
Q
Q


ytrAsh1 NP
T
? ? ?
Figure 1: Parse tree
marked with hi on the tree. Similarly to the other
features, in both training and decoding sets, we
create a new feature stream where we tag those
token sequences which appear with the same n first
parent sub-tree head words as a person mention in
the annotated TL data.
5. Parser-based features: it attempts to use the
syntactic environment in which a mention might ap-
pear. In order to do so, for each mention in the tar-
get language corpus we consider only labels of the
parent non-terminals .We mark parent non-terminal
labels of ?SdAm Hsyn? on the tree with pi. Simi-
larly to the features described above, we create dur-
ing both training and test a new feature stream where
we indicate the token sequences which appear in the
same parent non-terminal labels.
Gazetteers and model-based features are the most
natural and expected kind of features that one would
extract from the automatically MD tagged version of
the TL text. Our motivation of using n-gram context
features, on one hand, and the head-word based and
parse-based features on the other is to: (i) contrast
the impact of local and global context features; and
(ii) experiment the possibility of employing both of
them jointly in order to test their complementarity.
4 The Target Language Mention Detection
System
- The Arabic language: In our research study, we
have intentionally chosen a TL which is differs from
English in its strategy in forming words and sen-
tences. By doing so, we are seeking to avoid ob-
taining results which are biased by the similarity of
the employed languages. For this reason, we have
996
chosen Arabic as a TL.
Due to its Semitic origins, the Arabic language is
both derivational, i.e. it uses a templatic strategy
to form a word, and highly inflectional, i.e. addi-
tional affixes might be added to a word in order to
obtain further meaning. Whereas the former char-
acteristic is common in most languages, the latter,
however, results in increasing sparseness in data
and consequently forming an obstacle to achieve a
high performance for most of the NLP tasks (Diab
et al, 2004; Benajiba et al, 2008; Zitouni et al,
2005; Zitouni and Florian, 2008). From a NLP
viewpoint, especially the supervised tasks such as
the one we are dealing with in this paper, this im-
plies that a huge amount of training data is nec-
essary in order to build a robust model. In our
study, to tackle the data sparseness problem, we have
performed the word segmentation. This segmenta-
tion pre-processing step consists of separating the
normal white-space delimited words into prefixes,
stems, and suffixes. Thus, from a modeling view-
point, the unit of analysis becomes the segments. We
use a technique similar to the one introduced in (Lee
et al, 2003) for segmentation with an accuracy of
98%.
- The Arabic MD system: Our Arabic MD system
employs the same technique presented in Section 2.
Compared to English MD model, Arabic MD sys-
tem has access to morphological information (Stem)
as we will explain next. Features used by the Arabic
MD system are divided in three categories:
1. Lexical: Similar to the lexical features used by
our English MD system (c.f. section 2);
2. Stem: This feature has been introduced in (Zitouni
et al, 2005) as stem n-grams spanning the current
stem; both preceding and following it. If the current
token xi is a stem, stem n-gram features contain the
previous n? 1 stems and the following n? 1 stems.
Stem n-gram features represent a lexical generaliza-
tion that reduce data sparseness;
3. Syntactic: it consists of the output of POS taggers
and the BPCs.
As we describe with more details in the experiments
section (see Section 6), once we have extracted the
new features from the parallel corpus, we contrast
their impact with the level of richness in features of
the TL MD system, i.e. we measure the impact of
each feature fi when the TL MD system uses: (i)
only lexical features; (ii) both lexical and stem fea-
tures; and (iii) lexical, stem and syntactic features.
5 Evaluation Data
Experiments are conducted on the Arabic ACE 2007
data. There are 379 Arabic documents and al-
most 98, 000 words. We find seven classes of men-
tions: Person (PER), Organization (ORG), Geo-
Political Entity (GPE), Location (LOC), Facility
(FAC), Vehicle (VEH) and Weapon (WEA). Since
the evaluation test sets are not publicly available,
we have split the publicly available training cor-
pus into an 85%/15% data split. We use 323 doc-
uments (80, 000 words) for training and 56 docu-
ments (18, 000 words) as a test set. This results
in 17, 634 mentions (7, 816 named, 8, 831 nominal
and 987 pronominal) for training and 3, 566 for test
(1, 673 named, 1, 682 nominal and 211 pronominal).
To facilitate future comparisons with work presented
here, and to simulate a realistic scenario, the splits
are created based on article dates: the test data is se-
lected as the latest 15% of the data in chronological
order, in each of the covered genres (newswire and
webblog). Performance on the ACE data is usually
evaluated using a special-purpose measure, i.e. the
ACE value metric. However, given that we are inter-
ested in the mention detection task only, we decided
to use the more intuitive and popular (un-weighted)
F-measure, the harmonic mean of precision and re-
call.
6 Experiments and Results
As we have stated earlier, our main goal is to in-
vestigate how an MD model of a TL might bene-
fit from additional information about the mentions
obtained by propagation from an RRL. In our re-
search study we have chosen Arabic as the TL and
English as the RRL. The English MD system we use
has access to a large set of information (Zitouni and
Florian, 2009) and has achieved a performance of
82.7F on ACE?07 data. In order to simulate differ-
ent levels of resource-richness for the TL, we have
employed four baseline systems which use different
feature-sets. Following we present these feature-sets
ranked from the resource-poorest to the resource-
richest one: 1- Lex.: lexical features; 2- Stem.:
Lex. + stem features; and 3- Syntac.: Stem. + syn-
tactic features.
For each of these baseline systems, we study the im-
pact of features extracted from the parallel corpus
(c.f. Section 3) separately. We report the following
results:
1- Base.: baseline system without the use of
parallel-data extracted features;
2- n? Lex.: Base. + n-gram context features;
997
Lex. Stem Syntac
Base. 74.14 74.47 75.53
n? Lex. 74.71 75.25 76.20
n?Head 74.63 75.29 75.93
n? Pars. 75.32 75.19 75.74
Gaz 74.90 74.79 75.66
Model 74.60 75.50 76.22
Comb. 76.01 76.74 77.18
Table 1: Obtained results when the features were ex-
tracted from a hand-aligned parallel corpus
3- n?Head: Base. + head-word based features;
4- n? Pars.: Base. + parser-related features;
5- Gaz.: Base. + automatically extracted
gazetteers from the parallel corpus;
6- Model: Base. + output of model trained on the
Arabic part of the parallel corpus;
7- Comb.: combination of all the above.
In the rest of the paper, to measure whether the im-
provement in performance of a system using fea-
tures from parallel data over baseline is statistically
significant or not, we use the stratified bootstrap re-
sampling significance test (Noreen, 1989) used in
the NER shared task of CoNLL-20023. We consider
results as statistically significant when p < 0.02.
6.1 Hand-aligned Data
In our first experiment-set, we use a hand-aligned
English-to-Arabic parallel corpus of approximately
one million words. After tagging the Arabic side
by projection we obtain 86.5K mentions. As we
have previously mentioned, in order to generate
the model-based feature, Model, we have trained a
model on the Arabic side of the parallel corpus. This
model achieved an F-measure of 57.7F. This shows
the performance that might be achieved when no hu-
man annotated data is available in the TL.
Results in Table 1 show that a significant improve-
ment is obtained when the TL is poor in resources;
for instance an improvement of ?1.9 points was
achieved when the TL used only lexical features.
The use of n ? Pars. features alone yielded 1.2
points of improvement. when the TL model uses a
rich feature-set, we still can obtain ?1.7 points im-
provement. When the TL baseline model employs
the Syntac feature-set, the greatest improvement
is obtained when we add the model-based feature.
Improvement obtained by the system using Comb.
3http://www.cnts.ua.ac.be/conll2002/ner/
features is statistically significant compared to the
baseline model. This system also outperforms sys-
tems using the new feature set separately across the
board. According to our error-analysis, the signif-
icant amount of Arabic mentions observed in the
parallel corpus, where many of them do not appear
in the training corpus, has significantly helped the
Lex., Stem and SyntacMD models to capture new
mentions and/or correct the type assigned. Some of
the relevant examples in our data are: (i) the facility
mention P?
	
??K. ?
	
?J.? (mbnY blfwr - Belvoir Build-
ing); (ii) the GPE mention ??K. A? (kAbwl - Kabul);
and (iii) the person mention 	?




J?J. ? @ (AlbEvyyn - the
Baathists). These mentions have only been tagged
correctly when we have added the new extracted fea-
tures to our model.
In other words, the error-analysis clearly points out
that one possible way to get further improvement is
to increase the parallel data in order to increase the
number of matches between (1) the number of men-
tions which are wrongly tagged by the TL MD model
and (2) the number of mentions in the TL side of the
parallel corpus. The second parameter can be, indi-
rectly, increased by increasing the size of the paral-
lel data. Getting 10 or 20 times more of parallel data
that is hand-aligned is expensive and requires sev-
eral months of human/hours work. For this reason
we opted for using an unsupervised approach by se-
lecting a parallel corpus that is automatically aligned
as we discuss in the next section.
6.2 Automatically-aligned Data
We have used for this experiment-set an Arabic-to-
English parallel data of 22 million words. The data
in this corpus is automatically aligned using a tech-
nique presented in (Ittycheriah and Roukos, 2005).
The alignment is one-to-many with a performance
around 87 F-measure.
Because we are dealing with a large amount of
data and the word alignment is done automatically,
meaning more noise, we have used the English MD
model confidence for additional filtering. Such fil-
tering consists in keeping, from the parallel corpus,
only sentences which have all tokens tagged with a
confidence greater than ?. In this paper, we use a
value of ? = 0.94, which results in a corpus of 17
million words. We notice that a lower value of ? re-
sults in a radical increase in noise. Because of space
limitation, we will report results only with this value
of ?.
Table 2 shows the obtained results for parallel-
998
Lex. Stem Syntac
Base. 74.14 74.47 75.53
n? Lex. 74.27 74.74 75.24
n?Head. 74.07 74.95 75.33
n? Pars. 75.62 75.22 76.02
Gaz 73.96 74.11 74.94
Model 74.87 75.12 75.76
Comb. 75.56 75.93 76.46
Table 2: Obtained results when the features were ex-
tracted from a automatically-aligned parallel corpus
data based features using the 17M subset. Differ-
ently from experiments using hand-aligned data, the
best results have been obtained when we have used
the parser-based feature, i.e. n ? Pars. On one
hand, the overall behavior is comparable to the one
obtained when using the 1M hand-aligned parallel
data (see Table 1), i.e. (i) the greatest improve-
ment has been obtained when the TL uses a poor
feature-set; and (ii) when the TL baseline model is
rich in resources, we still obtain 0.45 points absolute
improvement when using n ? Pars. On the other
hand, features extracted from automatically-aligned
data, in comparison with the ones extracted from the
hand aligned data, have helped the MD model to cor-
rect many of the TL baseline model false negatives.
This has been observed when the TL baseline sys-
tem uses a rich feature set as well. A side effect of
the noisy word alignment, however, was an increase
in the number of false positives. For instance, the
word H@Q?	?jJ?? (mstHDrAt - preparations) which
appeared in the following sentence:
?Q
	
k

@ H@Q?
	
?j

J??? hA???@ ?Y?
which might be transliterated as:
Edm AlsmAH lmstHDrAt AxrY
and translated to English as:
not to allow other preparations
has been tagged as an organization mention because
it has been mistakenly aligned, in the parallel cor-
pus, with the word ?A?, KO, in the sentence:

?J
?J
?j.

J? @ H@Q?
	
?j

J??

?? ?Q.??@ ?A?

??Q??
meaning:
The big cosmetics company KO.
In order to validate our results, we run our exper-
iments on a blind test-set. We have selected the
latest 5% of each genre of the hand-aligned data
Class Num. of mentions
FAC 285
GPE 2,145
LOC 239
ORG 1,135
PER 2,474
VEH 65
WEA 138
Table 3: Distribution over the classes of the blind test
mentions
Lex. Stem Syntac
Base. 74.26 73.54 73.61
n? Lex. 74.04 73.72 73.83
n?Head 74.14 73.64 73.83
n? Pars. 74.32 74.18 74.32
Gaz 71.49 72.13 73.39
Model 75.01 74.66 74.78
Table 4: Obtained results on blind test
and they have been manually annotated by a hu-
man. The blind test-set consists of 51,781 tokens of
which 6,481 are mentions. Table 3 shows the distri-
bution of these mentions over the different classes.
The results are shown in Table 4. These results con-
firm the conclusions we have deduced from the ones
previously presented in Table 2, i.e.: (i) the highest
improvement is obtained when the TL is resource-
scarce.
6.3 Combining Hand-aligned and
Automatically-aligned Data
Table 5 shows that combining both features
extracted from hand-aligned and automatically-
aligned corpora has led to better results. The im-
Lex. Stem Syntac
Base. 74.14 74.47 75.53
n? Lex. 74.60 75.08 75.58
n?Head 74.51 75.32 75.56
n? Pars. 75.46 75.90 76.22
Gaz 74.85 74.83 75.92
Model 74.83 75.59 75.40
Comb. 76.39 76.85 77.23
Table 5: Obtained results when the features were
extracted from both hand-aligned and automatically-
aligned parallel corpora
999
provement of using Comb. compared to baseline is
statistically significant. We notice again that when
the TL baseline MD model uses a richer feature set,
the obtained improvement from using RRL becomes
smaller. We also observed that automatically aligned
data helped capture most of the unseen mentions
whereas the hand-aligned features helped decrease
the number of false-alarms. It is important to notice
that when features Comb. is used with Stem base-
line model, the obtained F-measure (76.85) is 1.3
higher than the baseline model which uses lexical,
stem and syntactic features ? Syntac (75.53). The
type of errors which mostly occur and has not been
fixed neither by using hand-aligned data, automati-
cally aligned data nor the combination of both are
the nominal mentions whose class depends fully on
the context. For instance, the word
	
?
	
??? (mwZf -
employee) which was considered as O by the MD
model because it has not been seen in any of the par-
allel data in a context such as the following:
. . . 	?A? ?


Q???? @
	
?
	
???? @ ?? ?
	
?K
Q?

K
transliterated as:
tEryf $kl AlmwZf AlmSry ...
and translated as: ?defining the life of the Egyptian
employee ...?
7 Previous Works
Several research works, in different NLP tasks, have
shown that the use of an RRL to achieve a better
performance in a resource-challenged language
yields to successful results. In (Rogati et al, 2003),
authors used a statistical machine translation (MT)
system to build an Arabic stemmer. The obtained
stemmer has a performance of 87.5%. In (Ide et al,
2002), authors use the aligned versions of George
Orwell?s Nineteen Eighty-Four in seven languages
in order to determine sense distinctions which can
be used in the Word Sense Disambiguation (WSD)
task. They report that the automatically obtained
tags are at least as reliable as the one made by hu-
man annotators. Similarly, (Ng et al, 2003) report a
research study which uses an English-Chinese par-
allel corpus in order to extract sense-tagged training
data. In (Hwa et al, 2002), authors report promising
results of inducing Chinese dependency trees from
English. The obtained model outperformed the
baseline.
One of the significant differences between these
works and the one we present in this paper is that
instead of using the propagated annotation directly
as training data we use it as an additional feature and
thus allow the MEMM model to weigh each one of
them. By doing so, the model is able to distinguish
between the relevant and the irrelevant information
propagated from the RRL.
Authors in (Zitouni and Florian, 2008) attempt to
enhance an MD model of a foreign language by us-
ing an English MD system. They have used an MT
system to (i) translate the text to English; (ii) run the
English model on the translated text; (iii) and prop-
agate outcome to the original text. The approach
in (Zitouni and Florian, 2008) requires a MT system
that needs more effort and resources to build when
compared to a parallel corpus (used in our experi-
ments); not all institutions may have access to MT
and MD systems in plenty of language pairs.
8 Conclusions and Future Works
In this paper, we presented a novel approach that al-
lows to exploit the richness, in terms of resources, of
one language (English) to the benefit of a target lan-
guage (Arabic). We achieved successful results by
adopting a novel approach performing in three main
steps, namely: (i) Annotate the English side of an
English-to-Arabic parallel corpus automatically; (ii)
Project the obtained annotation from English to Ara-
bic via the parallel corpus; and (iii) Extract features
of different linguistic motivations of the automati-
cally tagged Arabic tokens. Thereafter, each of the
extracted features is used to bootstrap Arabic MD
system. We use different Arabic baseline MD mod-
els which employ different feature sets representing
different levels of richness in resources. We also use
both a one million word hand-aligned parallel cor-
pus and a 22 million word automatically aligned one
in order to study size vs. noise trade-off.
Results show that a statistically significant improve-
ment is always observed even when the Arabic base-
line MD model uses all the available resources.
When we use the hand-aligned parallel corpus, we
obtain up to 2.2 points improvement when the Ara-
bic MD model has access to very limited resources.
It decreases to 1.7 points when we use all the re-
sources we could gather for the Arabic language.
When no human-annotated data is available in the
TL, we show that we can obtain a performance of
57.6 using only mention propagation from RRL.
The results also show that a greater improvement
is achieved when using a small hand-aligned corpus
than using a 20 times bigger automatically aligned
data. However, in case both of them are available,
combining them leads to even higher results.
1000
References
Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008.
Arabic named entity recognition using optimized fea-
ture sets. In Proc. of EMNLP?08, pages 284?293.
Stanley Chen and Ronald Rosenfeld. 2000. A survey of
smoothing techniques for ME models. IEEE Transac-
tion on Speech and Audio Processing.
Mona Diab, Kadri Hacioglu, and Dan Jurafsky. 2004.
Automatic tagging of arabic text: from raw text to base
phrase chunks. In Proc. of HLT/NAACL?04.
Joshua Goodman. 2002. Sequential conditional general-
ized iterative scaling. In Proceedings of ACL?02.
Rebecca Hwa, Philip Resnik, and Amy Weinberg. 2002.
Breaking the resource bottleneck for multilingual pars-
ing. In Proceedings of LREC.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense
discrimination with parallel corpora. In Proceedings
of the SIGLEX/SENSEVAL Workshop on Word Sense
Disambiguation, pages 54?60.
Abe Ittycheriah and Salim Roukos. 2005. A maximum
entropy word aligner for arabic-english machine trans-
lation. In Proceedings of HLT/EMNLP?05, pages 89?
96.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of ACL?06, pages 817?824, Sydney, Australia.
Association for Computational Linguistics.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In Proc. of
the ACL?03, pages 399?406.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL?03, pages
455?462.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley Sons.
Lance Ramshaw and Mitchell Marcus. 1999. Text
chunking using transformation-based learning. In
S. Armstrong, K.W. Church, P. Isabelle, S. Manzi,
E. Tzoukermann, and D. Yarowsky, editors, Natu-
ral Language Processing Using Very Large Corpora,
pages 157?176. Kluwer.
Monica Rogati, Scott McCarley, and Yiming Yang. 2003.
Unsupervised learning of arabic stemming using a par-
allel corpus. In Proceedings of ACL?03, pages 391?
398.
Eric. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named entity
recognition. In Proceedings of CoNLL-2002, pages
155?158. Taipei, Taiwan.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT?01, pages 1?8.
Imed Zitouni and Radu Florian. 2008. Mention detection
crossing the language barrier. In Proc. of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Honolulu, Hawaii, October.
Imed Zitouni and Radu Florian. 2009. Cross-language
information propagation for arabic mention detection.
ACM Transactions on Asian Language Information
Processing (TALIP), 8(4):1?21.
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and Radu
Florian. 2005. The impact of morphological stem-
ming on arabic mention detection and coreference res-
olution. In Proc. of the ACL Workshop on Computa-
tional Approaches to Semitic Languages, pages 63?70.
1001
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 709?712,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Arabic Mention Detection: Toward Better Unit of Analysis
Yassine Benajiba
Center for Computational Learning Systems
Columbia University
ybenajiba@ccls.columbia.edu
Imed Zitouni
IBM T. J. Watson Research Center
izitouni@us.ibm.com
Abstract
We investigate in this paper the adequate unit
of analysis for Arabic Mention Detection. We
experiment different segmentation schemes
with various feature-sets. Results show that
when limited resources are available, models
built on morphologically segmented data out-
perform other models by up to 4F points. On
the other hand, when more resources extracted
from morphologically segmented data become
available, models built with Arabic TreeBank
style segmentation yield to better results. We
also show additional improvement by combin-
ing different segmentation schemes.
1 Introduction
This paper addresses an important and basic task of
information extraction: Mention Detection (MD)1:
the identification and classification of textual refer-
ences to objects/abstractions (i.e., mentions). These
mentions can be either named (e.g. Mohammed,
John), nominal (city, president) or pronominal (e.g.
he, she). For instance, in the sentence ?President
Obama said he will visit ...? there are three men-
tions: President, Obama and he. This is similar
to the Named Entity Recognition (NER) task with
the additional twist of also identifying nominal and
pronominal mentions. We formulate the mention de-
tection problem as a classification problem, by as-
signing to each token in the text a label, indicating
whether it starts a specific mention, is inside a spe-
cific mention, or is outside all mentions. The se-
lection of the unit of analysis is an important step
toward a better classification. When processing lan-
guages, such as English, using the word itself as the
1We adopt here the ACE nomenclature:
http://www.nist.gov/speech/tests/ace/index.html
unit of analysis (after separating punctuations) leads
to a good performance (Florian et al, 2004). For
other languages, such as Chinese, character is con-
sidered as the adequate unit of analysis (Jing et al,
2003). In this paper, we investigate different seg-
mentation schemes in order to define the best unit of
analysis for Arabic MD. Arabic adopts a very com-
plex morphology, i.e. each word is composed of zero
or more prefixes, one stem and zero or more suffixes.
Consequently, the Arabic data is sparser than other
languages, such as English, and it is necessary to
?segment? the words into several units of analysis in
order to achieve a good performance.
(Zitouni et al, 2005) used Arabic morphologically
segmented data and claimed to have very competi-
tive results in ACE 2003 and ACE 2004 data. On the
other hand, (Benajiba et al, 2008) report good re-
sults for Arabic NER on ACE 2003, 2004 and 2005
data using Arabic TreeBank (ATB) segmentation. In
all published works, authors do not mention a spe-
cific motivation for the segmentation scheme they
have adopted. Only for the Machine Translation
task, (Habash and Sadat, 2006) report several results
using different Arabic segmentation schemes. They
report that the best results were obtained when the
ATB-like segmentation was used. We explore here
the four known and linguistically-motivated sorts of
segmentation: punctuation separation, ATB, mor-
phological and character-level segmentations. To
our knowledge, this is the first paper which inves-
tigates different segmentation schemes to define the
unit of analysis which best fits Arabic MD.
2 Arabic Segmentation Schemes
Character-level Segmentation: considers that each
character is a separate token.
Morphological Segmentation : aims at segmenting
709
all affixes of a word. The morphological segmenta-
tion for the word I.

J??? @? (wAlmktb ? and the of-
fice)2 could be: ?I.

J??+ ?@+ ?? (w +Al +mktb).
Arabic TreeBank (ATB) segmentation : This seg-
mentation considers splitting the word into affixes
only if it projects an independent phrasal constituent
in the parse tree. As an example, in the word shown
above I.

J??? @?, the phrasal independent constituents
are: the conjunction ? (w ? and) and the noun
I.

J??? @ (Almktb ? the office). The morphological
segmentation of this word would lead to the follow-
ing parse tree:
S
HH
CONJ
w
NP
b
b
"
"
Al +mktb
Since the ?@ (Al, the definite article) is not an in-
dependent constituent, it is not considered for ATB
segmentation. Hence, for I.

J??? @?, the ATB segmen-
tation would be I.

J??? @+ ? (w +Almktb).
Punctuation separation : it consists of separating
the punctuation marks from the word.
Both ATB and morphological segmentation sys-
tems are based on weighted finite state transducers
(WFST). The decoder implements a general Bell-
man dynamic programming search for the best path
on a lattice of segmentation hypotheses that match
the input characters (Benajiba and Zitouni, 2009).
ATB and morphological segmentation systems have
a performance of 99.4 and 98.1 F-measure respec-
tively on ATB data.
The unit of analysis when doing classification de-
pends on the used segmentation. When using the
punctuation separation or character-based segmen-
tations, the unit of analysis is the word itself (with-
out the punctuation marks attached) or the character,
respectively. The ATB and morphological segmen-
tations are language specific and are based on dif-
ferent linguistic viewpoint. When using one of these
two segmentation schemes, the unit of analysis is the
morph (i.e. prefix, stem or suffix). Our goal in this
paper is to find the unit of analysis that fits best Ara-
bic MD.
2Throughout the paper, for each Arabic example we show
between parenthesis its transliteration and English translation
separated by ???.
3 Mention Detection System
As explained earlier, we consider the MD task as a
sequence classification problem where the class we
predict for each unit of analysis (i.e., token) is the
type of the entity which it refers to. We chose the
maximum entropy (MaxEnt) classifier that can in-
tegrate arbitrary types of information and make a
classification decision by aggregating all informa-
tion available for a given classification. For more
details about the system architecture, reader may re-
fer to (Zitouni et al, 2009). The features used in our
MD system can be divided into four categories:
Lexical Features: n-grams spanning the current to-
ken; both preceding and following it. A number of
n equal to 3 turned out to be a good choice.
Stem n-gram Features: stem trigram spanning the
current stem; both preceding and following it (Zi-
touni et al, 2005).
Syntactic Features: POS tags and shallow parsing
information in a ?2 window.
Features From Other Classifiers: outputs of MD
and NER taggers trained on other data-sets different
from the one we used here. They may identify types
of mentions different from the mentions of interest
in our task. For instance, such a tagger may identify
dates or occupation references (not used in our task),
among other types. Our hypothesis is that combin-
ing classifiers from diverse sources will boost per-
formance by injecting complementary information
into the mention detection models. We also use the
two previously assigned classification tags as addi-
tional feature.
4 Data
Experiments are conducted on the Arabic ACE 2007
data. Since the evaluation tests set are not publicly
available, we have split the publicly available train-
ing corpus into an 85%/15% data split. We use 323
documents (80, 000 words, 17, 634 mentions) for
training and 56 documents (18, 000 words, 3, 566
mentions) as a test set. We are interested in 7 types
of mentions: facility, Geo-Political Entity (GPE),
location, organization, person, vehicle and weapon.
We segmented the training and test set with four dif-
ferent styles building the following corpora:
Words: a corpus which is the result of running
punctuation separation;
ATBs: a corpus obtained by running punctuation
separation and ATB segmentation;
Mophs: a corpus where we conduct punctuation
separation and morphological segmentation;
Chars: a corpus where the original text is separated
710
into a sequence of characters.
When building MD systems on Words, ATBs,
Morphs and Chars, the unit of analysis is the word,
the ATB token, the morph and the character, respec-
tively.
5 Experiments
We show in this section the experimental results
when using Arabic MD system with different seg-
mentation schemes and different feature sets. We
explore in this paper four categories of features (c.f.
Section 3):
Lexf : lexical features;
Stemf : Lexf + morphological features;
Syntf : Stemf + syntactic features;
Semf : Syntf + output of other MD classifiers.
Lexf and Stemf features are directly extracted
from the appropriate corpus based on the used seg-
mentation style. This is different for Semf : we first
run classifiers on the morphologically segmented
data. Thereafter, we project those labels to other
corpora. This is because, we use classifiers initially
trained on morphologically segmented data such as
ACE 2003, 2004 and 2005 data. In such data, two
morphs belonging to the same word or ATB token
may have 2 different mentions. During transfer, a
token will have the label of the corresponding stem
in the morphologically segmented data. One moti-
vation to not re-train classifiers on each corpus sep-
arately is to be able to extract Semf features from
classifiers with similar performance.
Table 1: Results in terms of F-measure per feature-set and
segmentation scheme
Lexf Stemf Syntf Semf
Words 66.4 66.6 69.0 77.1
ATBs 70.1 69.8 72.1 79.0
Morphs 74.1 74.5 75.5 78.3
Chars 22.3 22.4 22.5 22.6
Results in Table 1 show that classifiers built on
ATBs and Morphs have shown to perform better
than classifiers trained on data with other segmenta-
tion styles. When the system uses character as the
unit of analysis, performance is poor. This is be-
cause the token itself becomes insignificant informa-
tion to the classifier. On the other hand, when only
punctuation separation is performed (Words), the
data is significantly sparse and the obtained results
achieves high F-measure (77.1) only when outputs
of other classifiers are used. As mentioned earlier,
classifiers used to extract those features are trained
on Morphs (less sparse), which explains their re-
markable positive impact since they resolve part of
the data sparseness problem in Words. When us-
ing full morphological segmentation, the data is less
sparse, which leads to less Out-Of-Vocabulary to-
kens (OOVs): the number of OOVs in the Morphs
data is 1,518 whereas it is 2,464 in the ATBs.
As an example, the word

?
	
JJ
?Q?@ (Alrhynp ? the
hostage), which is person mention in the training
data. This word is kept unchanged after ATB seg-
mentation and is segmented to ?

?+
	?
?P
+ ?@? (Al+
rhyn +p) in Morphs. In the development set the
same word appears in its dual form without defi-
nite article, i.e. 	?


J
	
J
?P. This word is unchanged in
ATBs and is segmented to ? 	?K
+
H+
	?
?P? (rhyn
+p +yn) in Morphs. For the model built on ATBs,
this word is an OOV, whereas for the model built
on Morphs the stem has been seen as part of a per-
son mention and consequently has a better chance
to tag it correctly. These phenomena are frequent,
which make the classifier trained on Morphs more
robust for such cases. Also, we observed that mod-
els trained on ATBs perform better on long span
mentions. We think this is because a model trained
on ATBs has access to larger context. One may
argue that a similar behavior of the model built on
the Morphs might be obtained if we use a wider
context window than the one used for ATBs in or-
der to have similar contextual information. In or-
der to confirm this statement, we have carried out a
set of experiments using all features over Morphs
data for a context window up to ?5/ + 5, the ob-
tained results show no improvement. Similar behav-
ior is observed when looking to results on identi-
fied named (Nam.), nominal (Nom.) and pronomi-
nal (Pro.) mentions on ATBs and Morphs (c.f. Ta-
ble 2); we remind the reader that NER is about rec-
ognizing named mentions. When limited resources
are available (e.g. Lexf , Stemf or Syntf ), we be-
lieve that it is more effective to morphologically seg-
ment the text (Morphs) as a pre-processing step.
The use of morph as a unit of analysis reduces the
data sparseness issue and at the same time allows
better context handling when compared to character.
On the other hand, when a larger set of resources
are available (e.g., Semf ), the use of the ATB to-
ken as a unit of analysis combined with morph-
based features leads to better performance (79.0 vs.
78.3 on Morphs). This is because (1) classifiers
trained on ATBs handle better the context and (2)
the use of morph-based features (output of classi-
711
fiers trained on morphologically segmented data) re-
moves some of the data sparseness from which clas-
sifiers trained on ATBs suffer. The obtained im-
provement in performance is statistically significant
when using the stratified bootstrap re-sampling sig-
nificance test (Noreen, 1989). We consider results
as statistically significant when p < 0.02, which is
the case in this paper. For an accurate MD system,
we think it is appropriate to benefit from ATBs to-
kens and Morphs. We investigate in the following
the combination of these two segmentation styles.
Table 2: Performance in terms of F-measure per level on
ATBs and Morphs
Seg. Lexf Stemf Syntf Semf
Nam.
ATBs 68.2 69.0 72.8 79.1
Morphs 73.4 73.8 75.3 78.7
Nom.
ATBs 65.6 64.6 66.9 75.8
Morphs 71.7 72.2 72.9 75.4
Pro.
ATBs 60.7 60.1 59.9 66.3
Morphs 63.0 67.2 65.7 65.1
5.1 Combination of ATB and Morph
We trained a model on ATBs that uses output of the
model trained on Morphs as additional information
(M2Af feature). We proceed similarly by training a
model on Morphs using output of the model trained
on ATBs (A2Mf feature). We have obtained the
features by a 15-way round-robin. Table 3 shows
the obtained results.
Table 3: Results in terms of F-measure of the combina-
tion experiments
Lexf Stemf Syntf Semf
ATBs 70.1 69.8 72.1 79.0
ATBs+M2Af 70.7 70.8 73.1 79.1
Morphs 74.1 74.5 75.5 78.3
Morphs+A2Mf 74.9 75.2 75.4 78.6
Results show a significant improvement for mod-
els that are trained on ATBs using information from
Morphs in addition to Lexf , Stemf and Syntf
features. This again confirms our claim that the use
of features from morphologically segmented text re-
duces the data sparseness and consequently leads to
better performance. For Semf features, only a 0.1
F-measure points have been gained. This is because
we are already using output of classifiers trained
on morphologically segmented data, which resolve
some of the data sparseness issue. The Morphs
side shows that the obtained performance when the
ATBs output is employed together with the Stemf
(75.2) is only 0.3 points below the performance of
the system using Syntf (75.5).
6 Conclusions
We have shown a comparative study aiming at defin-
ing the adequate unit of analysis for Arabic MD.
We conducted our study using four segmentation
schemes with four different feature-sets. Results
show that when only limited resources are available,
using morphological segmentation leads to the best
results. On the other hand, model trained on ATB
segmented data become more powerful and effective
when data sparseness is reduced by the use of other
classifier outputs trained on morphologically seg-
mented data. More improvement is obtained when
both segmentation styles are combined.
References
Y. Benajiba and I. Zitouni. 2009. Morphology-
based segmentation combination for arabic men-
tion detection. Special Issue on Arabic Nat-
ural Language Processing of ACM Transac-
tions on Asian Language Information Processing
(TALIP), 8(4).
Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic
named entity recognition using optimized feature
sets. In Proc. of EMNLP?08, pages 284?293.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and
S. Roukos. 2004. A statistical model for
multilingual entity detection and tracking. In
Proc.eedings of HLT-NAACL?04, pages 1?8.
N. Habash and F. Sadat. 2006. Combination of ara-
bic preprocessing schemes for statistical machine
translation. In Proceedings of ACL?06, pages 1?8.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. Itty-
cheriah. 2003. HowtogetaChineseName(Entity):
Segmentation and combination issues. In Pro-
ceedings of EMNLP?03, pages 200?207.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses. John Wiley Sons.
I. Zitouni, J. Sorensen, X. Luo, and R. Florian.
2005. The impact of morphological stemming on
arabic mention detection and coreference resolu-
tion. In Proc. of the ACL Workshop on Compu-
tational Approaches to Semitic Languages, pages
63?70.
I. Zitouni, X. Luo, and R. Florian. 2009. A cascaded
approach to mention detection and chaining in
arabic. IEEE Transactions on Audio, Speech and
Language Processing, 17:935?944.
712
Proceedings of the ACL 2010 Conference Short Papers, pages 281?285,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Arabic Named Entity Recognition:
Using Features Extracted from Noisy Data
Yassine Benajiba1 Imed Zitouni2 Mona Diab1 Paolo Rosso3
1 Center for Computational Learning Systems, Columbia University
2 IBM T.J. Watson Research Center, Yorktown Heights
3 Natural Language Engineering Lab. - ELiRF, Universidad Polite?cnica de Valencia
{ybenajiba,mdiab}@ccls.columbia.edu, izitouni@us.ibm.com, prosso@dsic.upv.es
Abstract
Building an accurate Named Entity
Recognition (NER) system for languages
with complex morphology is a challeng-
ing task. In this paper, we present research
that explores the feature space using both
gold and bootstrapped noisy features to
build an improved highly accurate Arabic
NER system. We bootstrap noisy features
by projection from an Arabic-English par-
allel corpus that is automatically tagged
with a baseline NER system. The feature
space covers lexical, morphological, and
syntactic features. The proposed approach
yields an improvement of up to 1.64
F-measure (absolute).
1 Introduction
Named Entity Recognition (NER) has earned an
important place in Natural Language Processing
(NLP) as an enabling process for other tasks.
When explicitly taken into account, research
shows that it helps such applications achieve bet-
ter performance levels (Babych and Hartley, 2003;
Thompson and Dozier, 1997). NER is defined as
the computational identification and classification
of Named Entities (NEs) in running text. For in-
stance, consider the following text:
Barack Obama is visiting the Middle East.
A NER system should be able to identify Barack
Obama and Middle East as NEs and classify them
as Person (PER) and Geo-Political Entity (GPE),
respectively. The class-set used to tag NEs may
vary according to user needs. In this research,
we adopt the Automatic Content Extraction (ACE)
2007 nomenclature1.
According to (Nadeau and Sekine, 2007), opti-
mization of the feature set is the key component in
enhancing the performance of a global NER sys-
tem. In this paper we investigate the possibil-
ity of building a high performance Arabic NER
system by using a large space of available feature
sets that go beyond the explored shallow feature
sets used to date in the literature for Arabic NER.
1http://www.nist.gov/speech/tests/ace/index.htm
Given current state-of-the-art syntactic processing
of Arabic text and the relative small size of man-
ually annotated Arabic NER data, we set out to
explore a main concrete research goal: to fully ex-
ploit the level of advancement in Arabic lexical
and syntactic processing to explore deeper linguis-
tic features for the NER task. Realizing that the
gold data available for NER is quite limited in size
especially given the diverse genres in the set, we
devise a method to bootstrap additional instances
for the new features of interest from noisily NER
tagged Arabic data.
2 Our Approach
We use our state-of-the-art NER system described
in (Benajiba et al, 2008) as our baseline sys-
tem (BASE) since it yields, to our knowledge, the
best performance for Arabic NER . BASE em-
ploys Support Vector Machines (SVMs) and Con-
ditional Random Fields (CRFs) as Machine Learn-
ing (ML) approaches. BASE uses lexical, syn-
tactic and morphological features extracted using
highly accurate automatic Arabic POS-taggers.
BASE employs a multi-classifier approach where
each classifier is tagging a NE class separately.
The feature selection is performed by using an in-
cremental approach selecting the top n features
(the features are ranked according to their individ-
ual impact) at each iteration and keeping the set
that yields the best results. In case of conflict - a
word is classified with more than one class/tag si-
multaneously - the global NER system selects the
output of the classifier with the highest precision.
The following is the feature set used in (Bena-
jiba et al, 2008) and accordingly in the BASE sys-
tem. 1. Context: a?/+1 token window; 2. Lex-
ical: character n ? grams where n ranges from
1? 3; 3. Gazetteers: automatically harvested and
manually cleaned Person NE class (PER), Geopo-
litical Entity NE class (GPE), and Organization
NE class (ORG) lexica; 4. POS-tag and Base
Phrase Chunk (BPC): automatically tagged us-
ing AMIRA (Diab et al, 2007) which yields F-
measures for both tasks in the high 90?s; 5. Mor-
phological features: automatically tagged using
the Morphological Analysis and Disambiguation
for Arabic (MADA) tool to extract information
about gender, number, person, definiteness and as-
281
pect for each word (Habash and Rambow, 2005);
6. Capitalization: derived as a side effect from
running MADA. MADA chooses a specific mor-
phological analysis given the context of a given
word. As part of the morphological information
available in the underlying lexicon that MADA ex-
ploits. As part of the information present, the un-
derlying lexicon has an English gloss associated
with each entry. More often than not, if the word
is a NE in Arabic then the gloss will also be a NE
in English and hence capitalized.
We devise an extended Arabic NER system (EX-
TENDED) that uses the same architecture as
BASE but employs additional features to those in
BASE. EXTENDED defines new additional syn-
tagmatic features.
We specifically investigate the space of the sur-
rounding context for the NEs. We explore gener-
alizations over the kinds of words that occur with
NEs and the syntactic relations NEs engage in. We
use an off-the-shelf Arabic syntactic parser. State-
of-the-art for Arabic syntactic parsing for the most
common genre (with the most training data) of
Arabic data, newswire, is in the low 80%s. Hence,
we acknowledge that some of the derived syntactic
features will be noisy.
Similar to all supervised ML problems, it is de-
sirable to have sufficient training data for the rele-
vant phenomena. The size of the manually anno-
tated gold data typically used for training Arabic
NER systems poses a significant challenge for ro-
bustly exploring deeper syntactic and lexical fea-
tures. Accordingly, we bootstrap more NE tagged
data via projection over Arabic-English parallel
data. The role of this data is simply to give us more
instances of the newly defined features (namely
the syntagmatic features) in the EXTENDED sys-
tem as well as more instances for the Gazetteers
and Context features defined in BASE. It is worth
noting that we do not use the bootstrapped NE
tagged data directly as training data with the gold
data.
2.1 Syntagmatic Features
For deriving our deeper linguistic features, we
parse the Arabic sentences that contain an NE. For
each of the NEs, we extract a number of features
described as follows:
- Syntactic head-word (SHW): The idea here
is to look for a broader relevant context.
Whereas the feature lexical n-gram context fea-
ture used in BASE, and hence here for EX-
TENDED, considers the linearly adjacent neigh-
boring words of a NE, SHW uses a parse tree
to look at farther, yet related, words. For
instance, in the Arabic phrase ?SrH Ams An
Figure 1: Example for the head word and syntactic
environment feature
bArAk AwbAma ytrAs?, which means ?de-
clared yesterday that Barack Obama governs
...?, glossed ?SrH/declared Ams/yesterday An/that
bArAk/Barack AwbAmA/Obama ytrAs/governs
...?, is parsed in Figure 1. According to the phrase
structure parse, the first parent sub-tree headword
of the NE ?bArAk AwbAmA? is the verb ?ytrAs?
(governs), the second one is ?An? (that) and the
third one is the verb ?SrH? (declared). This exam-
ple illustrates that the word ?Ams? is ignored for
this feature set since it is not a syntactic head. This
is a lexicalized feature.
- Syntactic Environment (SE): This follows in the
same spirit as SHW, but expands the idea in that
it looks at the parent non-terminal instead of the
parent head word, hence it is not a lexicalized fea-
ture. The goal being to use a more abstract repre-
sentation level of the context in which a NE ap-
pears. For instance, for the same example pre-
sented in Figure 1, the first, second, and third non-
terminal parents of the NE ?bArAk AwbAmA? are
?S?, ?SBAR? and ?VP?, respectively.
In our experiments we use the Bikel implementa-
tion (Bikel, 2004) of the Collins parser (Collins,
1999) which is freely available on the web2. It is a
head-driven CFG-style parser trained to parse En-
glish, Arabic, and Chinese.
2.2 Bootstrapping Noisy Arabic NER Data
Extracting the syntagmatic features from the
training data yields relatively small number of
instances. Hence the need for additional tagged
data. The new Arabic NER tagged data is derived
via projection exploiting parallel Arabic English
data. The process depends on the availability
of two key components: a large Arabic English
parallel corpus that is sentence and word aligned,
and a robust high performing English NER
system. The process is as follows. We NE tag the
2http://www.cis.upenn.edu/?dbikel/software.html#stat-
parser
282
English side of the parallel corpus. We project
the automatically tagged NER tags from the
English side to the Arabic side of the parallel
corpus. In our case, we have access to a large
manually aligned parallel corpus, therefore the
NER projection is direct. However, the English
side of the parallel corpus is not NER tagged,
hence we use an off-the-shelf competitive robust
automatic English NER system which has a
published performance of 92% (Zitouni and
Florian, 2009). The result of these two processes
is a large Arabic NER, albeit noisy, tagged data
set. As mentioned earlier this data is used only
for deriving additional instances for training
for the syntagmatic features and for the context
and gazetteer features.3 Given this additional
source of data, we changed the lexical features
extracted from the BASE to the EXTENDED. We
added two other lexical features: CBG and NGC,
described as follows: - Class Based Gazetteers
(CBG): This feature focuses on the surface form
of the NEs. We group the NEs encountered on the
Arabic side of the parallel corpus by class as they
are found in different dictionaries. The difference
between this feature and that in BASE is that the
Gazetteers are not restricted to Wikipedia sources.
- N-gram context (NGC): Here we disregard
the surface form of the NE, instead we focus on its
lexical context. For each n, where n varies from 1
to 3, we compile a list of the ?n, +n, and ?/+ n
words surrounding the NE. Similar to the CBG
feature, these lists are also separated by NE class.
It is worth highlighting that the NCG feature is
different from the Context feature in BASE in
that the window size is different +/ ? 1 ? 3 for
EXTENDED versus +/? 1 for BASE.
3 Experiments and Results
3.1 Gold Data for training and evaluation
We use the standard sets of ACE 2003, ACE
2004 and ACE 2005.4 The ACE data is annotated
for many tasks: Entity Detection and Tracking
(EDT), Relation Detection and Recognition
(RDR), Event Detection and Recognition (EDR).
All the data sets comprise Broadcast News
(BN) and Newswire (NW) genres. ACE 2004
includes an additional NW data set from the
Arabic TreeBank (ATB). ACE 2005 includes
a different genre of Weblogs (WL). The NE
classes adopted in the annotation of the ACE
2003 data are: Person (PER), Geo Political Entity
(GPE), Organization (ORG) and Facility (FAC).
3Therefore, we did not do the full feature extraction for
the other features described in BASE for this data.
4http://www.nist.gov/speech/tests/ace/
Additionally for the ACE 2004 and 2005 data, two
NE classes are added to the ACE 2003 tag-set:
Vehicles (e.g. Rotterdam Ship) and Weapons (e.g.
Kalashnikof). We use the same split for train, de-
velopment, and test used in (Benajiba et al, 2008).
3.2 Parallel Data
Most of the hand-aligned Arabic-English parallel
data used in our experiments is from the Language
Data Consortium (LDC).5. Another set of the par-
allel data is annotated in-house by professional an-
notators. The corpus has texts of five different gen-
res, namely: newswire, news groups, broadcast
news, broadcast conversation and weblogs corre-
sponding to the data genres in the ACE gold data.
The Arabic side of the parallel corpus contains
941,282 tokens. After projecting the NE tags from
the English side to the Arabic side of the paral-
lel corpus, we obtain a total of 57,290 Arabic NE
instances. Table 1 shows the number of NEs for
each class.
Class Number of NEs Class Number of NEs
FAC 998 PER 17,964
LOC 27,651 VEH 85
ORG 10,572 WEA 20
Table 1: Number of NEs per class in the Arabic
side of the parallel corpus
3.3 Individual Feature Impact
Across the board, all the features yield improved
performance. The highest obtained result is ob-
served where the first non-terminal parent is used
as a feature, a Syntactic Environment (SE) fea-
ture, yielding an improvement of up to 4 points
over the baseline. We experiment with different
sizes for the SE, i.e. taking the first parent versus
adding neighboring non-terminal parents. We note
that even though we observe an overall increase
in performance, considering both the {first, sec-
ond} or the {first, second, and third} non-terminal
parents decreases performance by 0.5 and 1.5 F-
measure points, respectively, compared to consid-
ering the first parent information alone. The head
word features, SHW, show a higher positive im-
pact than the lexical context feature, NGC. Finally,
the Gazetteer feature, CBG, impact is comparable
to the obtained improvement of the lexical context
feature.
3.4 Feature Combination Experiments
Table 2 illustrates the final results. It shows for
each data set and each genre the F-measure ob-
tained using the best feature set and ML approach.
It shows results for both the dev and test data us-
ing the optimal number of features selected from
5All the LDC data are publicly available
283
ACE 2003 ACE 2004 ACE 2005
BN NW BN NW ATB BN NW WL
FreqBaseline 73.74 67.61 62.17 51.67 62.94 70.18 57.17 27.66
dev
All-Synt. 83.41 79.11 76.90 72.90 74.82 81.42 76.07 54.49
All 83.93 79.72 78.54 72.80 74.97 81.82 75.92 55.65
test
All-Synt. 83.50 78.90 76.70 72.40 73.50 81.31 75.30 57.30
All 84.32 79.4 78.12 72.13 74.54 81.73 75.67 58.11
Table 2: Final Results obtained with selected features contrasted against all features combined
the all the features except the syntagmatic ones
(All-Synt.) contrasted against the system in-
cluding the semantic features, i.e. All the features,
per class All . The baseline results, FreqBaseline,
assigns a test token the most frequent tag observed
for it in the gold training data, if a test token is
not observed in the training data, it is assigned the
most frequent tag which is the O tag.
4 Results Discussion
Individual feature impact results show that the
syntagmatic features are helpful for most of the
data sets. The highest improvements are obtained
for the 2003 BN and 2005 WL data-sets. The im-
provement varies significantly from one data-set
to another because it highly depends on the num-
ber of NEs which the model has not been able to
capture using the contextual, lexical, syntactic and
morphological features.
Impact of the features extracted from the paral-
lel corpus per class: The syntagmatic features
have varied in their influence on the different NE
classes. Generally, the LOC and PER classes ben-
efitted more from the head word features, SHW),
than the other classes. On the other hand for the
syntactic environment feature (SE), the PER class
seemed not to benefit much from the presence of
this feature. Weblogs: Our results show that the
random contexts in which the NEs tend to ap-
pear in the WL documents stand against obtain-
ing a significant improvement. Consequently, the
features which use a more global context (syntac-
tic environment, SE, and head word, SHW, fea-
tures) have helped obtain better results than the
ones which we have obtained using local context
namely CBG and NGC.
5 Related Work
Projecting explicit linguistic tags from another
language via parallel corpora has been widely used
in the NLP tasks and has proved to contribute sig-
nificantly to achieving better performance. Dif-
ferent research works report positive results when
using this technique to enhance WSD (Diab and
Resnik, 2002; Ng et al, 2003). In the latter two
works, they augment training data from parallel
data for training supervised systems. In (Diab,
2004), the author uses projections from English
into Arabic to bootstrap a sense tagging system
for Arabic as well as a seed Arabic WordNet
through projection. In (Hwa et al, 2002), the
authors report promising results of inducing Chi-
nese dependency trees from English. The ob-
tained model outperformed the baseline. More re-
cently, in (Chen and Ji, 2009), the authors report
their comparative study between monolingual and
cross-lingual bootstrapping. Finally, in Mention
Detection (MD), a task which includes NER and
adds the identification and classification of nom-
inal and pronominal mentions, (Zitouni and Flo-
rian, 2008) show the impact of using a MT sys-
tem to enhance the performance of an Arabic MD
model. The authors report an improvement of up
to 1.6F when the baseline system uses lexical fea-
tures only. Unlike the work we present here, their
approach requires the availability of an accurate
MT system which is a more expensive process.
6 Conclusion and Future Directions
In this paper we investigate the possibility of
building a high performance Arabic NER system
by using lexical, syntactic and morphological fea-
tures and augmenting the model with deeper lexi-
cal features and more syntagmatic features. These
extra features are extracted from noisy data ob-
tained via projection from an Arabic-English par-
allel corpus. Our results show that we achieve a
significantly high performance for almost all the
data-sets. The greatest impact of the syntagmatic
features (1.64 points of F-measure) is obtained for
the ACE 2004, BN genre. Also, the WL genre
yields an improvement of 1.16 F1 points absolute.
Acknowledgments
This work has been partially funded by DARPA GALE
project. The research of the last author was funded
by MICINN research project TEXT-ENTERPRISE 2.0
TIN2009-13391-C04-03 (Plan I+D+i).
284
References
B. Babych and A. Hartley. 2003. Improving Machine
Translation Quality with Automatic Named Entity
Recognition. In Proc. of EACL-EAMT.
Y. Benajiba, M. Diab, and P. Rosso. 2008. Ara-
bic named entity recognition using optimized feature
sets. In Proceedings of EMNLP?08, pages 284?293.
Daniel M. Bikel. 2004. On the parameter space
of generative lexicalized statistical parsing models.
University of Pennsylvania, Philadelphia, PA, USA.
Supervisor-Marcus, Mitchell P.
Z. Chen and H. Ji. 2009. Can one language bootstrap
the other: A case study of event extraction. In Pro-
ceedings of NAACL?09.
M. Collins. 1999. Head-Driven Statistical Models for
Nat- ural Language Parsing. University of Pennsyl-
vania, Philadelphia, PA, USA.
Mona Diab and Philip Resnik. 2002. An unsuper-
vised method for word sense tagging using parallel
corpora. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 255?262, Philadelphia, Pennsylvania, USA,
July. Association for Computational Linguistics.
M. Diab, K. Hacioglu, and D. Jurafsky, 2007. Arabic
Computational Morphology: Knowledge-based and
Empirical Methods, chapter 9. Springer.
Mona Diab. 2004. Bootstrapping a wordnet taxonomy
for arabic. In Proceedings of First Arabic Language
Technology Conference (NEMLAR), Cairo Egypt,.
N. Habash and O. Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morpholog-
ical Disambiguation in One Fell Swoop. In Proc.
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
R. Hwa, P. Resnik, and A. Weinberg. 2002. Break-
ing the resource bottleneck for multilingual parsing.
In In Proceedings of the Workshop on Linguistic
Knowledge Acquisition and Representation: Boot-
strapping Annotated Language Data.
D. Nadeau and S. Sekine. 2007. A Survey of Named
Entity Recognition and Classification. Linguisticae
Investigationes, 30(7).
H.-T. Ng, B. Wang, and Y.-S. Chan. 2003. Exploit-
ing parallel texts for word sense disambiguation: An
empirical study. In ACL?03, pages 455?462, Sap-
poro, Japan.
P. Thompson and C. Dozier. 1997. Name Searching
and Information Retrieval. In In Proc. of Second
Conference on Empirical Methods in Natural Lan-
guage Processing, Providence, Rhode Island.
I. Zitouni and R. Florian. 2008. Mention detection
crossing the language barrier. In Proceedings of
EMNLP?08, Honolulu, Hawaii, October.
Imed Zitouni and Radu Florian. 2009. Cross language
information propagation for arabic mention detec-
tion. Journal of ACM Transactions on Asian Lan-
guage Information Processing, December.
285
