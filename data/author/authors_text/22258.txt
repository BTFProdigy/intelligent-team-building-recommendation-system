Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 287?295, Prague, June 2007. c?2007 Association for Computational Linguistics
Using RBMT Systems to Produce Bilingual Corpus for SMT 
Xiaoguang Hu, Haifeng Wang, Hua Wu 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza 
No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{huxiaoguang, wanghaifeng, wuhua}@rdc.toshiba.com.cn 
 
 
Abstract 
This paper proposes a method using the ex-
isting Rule-based Machine Translation 
(RBMT) system as a black box to produce 
synthetic bilingual corpus, which will be 
used as training data for the Statistical Ma-
chine Translation (SMT) system. We use 
the existing RBMT system to translate the 
monolingual corpus into synthetic bilingual 
corpus. With the synthetic bilingual corpus, 
we can build an SMT system even if there 
is no real bilingual corpus. In our experi-
ments using BLEU as a metric, the system 
achieves a relative improvement of 11.7% 
over the best RBMT system that is used to 
produce the synthetic bilingual corpora. 
We also interpolate the model trained on a 
real bilingual corpus and the models 
trained on the synthetic bilingual corpora. 
The interpolated model achieves an abso-
lute improvement of 0.0245 BLEU score 
(13.1% relative) as compared with the in-
dividual model trained on the real bilingual 
corpus. 
1 Introduction 
Within the Machine Translation (MT) field, by far 
the most dominant paradigm is SMT, but many 
existing commercial systems are rule-based. In this 
research, we are interested in answering the ques-
tion of whether the existing RBMT systems could 
be helpful to the development of an SMT system. 
To find the answer, let us first consider the follow-
ing facts: 
? Existing RBMT systems are usually pro-
vided as a black box. To make use of such 
systems, the most convenient way might 
be working on the translation results di-
rectly. 
? SMT methods rely on bilingual corpus. As 
a data driven method, SMT usually needs 
large bilingual corpus as the training data. 
Based on the above facts, in this paper we pro-
pose a method using the existing RBMT system as 
a black box to produce a synthetic bilingual cor-
pus1, which will be used as the training data for the 
SMT system. 
For a given language pair, the monolingual cor-
pus is usually much larger than the real bilingual 
corpus. We use the existing RBMT system to 
translate the monolingual corpus into synthetic 
bilingual corpus. Then, even if there is no real bi-
lingual corpus, we can train an SMT system with 
the monolingual corpus and the synthetic bilingual 
corpus. If there exist n available RBMT systems 
for the desired language pair, we use the n systems 
to produce n synthetic bilingual corpora, and n 
translation models are trained with the n corpora 
respectively. We name such a model the synthetic 
model. An interpolated translation model is built 
by linear interpolating the n synthetic models. In 
our experiments using BLEU (Papineni et al, 2002) 
as the metric, the interpolated synthetic model 
achieves a relative improvement of 11.7% over the 
best RBMT system that is used to produce the syn-
thetic bilingual corpora.  
                                                 
1 In this paper, to be distinguished from the real bilingual cor-
pus, the bilingual corpus generated by the RBMT system is 
called a synthetic bilingual corpus.  
287
Moreover, if a real bilingual corpus is available 
for the desired language pair, we build another 
translation model, which is named the standard 
model. Then we can build an interpolated model 
by interpolating the standard model and the syn-
thetic models. Experimental results show that the 
interpolated model achieves an absolute improve-
ment of 0.0245 BLEU score (13.1% relative) as 
compared with the standard model. 
The remainder of this paper is organized as fol-
lows. In section 2 we summarize the related work. 
We then describe our method Using RBMT sys-
tems to produce bilingual corpus for SMT in sec-
tion 3. Section 4 describes the resources used in the 
experiments. Section 5 presents the experiment 
result, followed by the discussion in section 6. Fi-
nally, we conclude and present the future work in 
section 7. 
2 Related Work 
In the MT field, by far the most dominant 
paradigm is SMT. SMT has evolved from the 
original word-based approach (Brown et al, 1993) 
into phrase-based approaches (Koehn et al, 2003; 
Och and Ney, 2004) and syntax-based approaches 
(Wu, 1997; Alshawi et al, 2000; Yamada and 
Knignt, 2001; Chiang, 2005). On the other hand, 
much important work continues to be carried out in 
Example-Based Machine Translation (EBMT) 
(Carl et al, 2005; Way and Gough, 2005), and 
many existing commercial systems are rule-based. 
Although we are not aware of any previous at-
tempt to use an existing RBMT system as a black 
box to produce synthetic bilingual training corpus 
for general purpose SMT systems, there exists a 
great deal of work on MT hybrids and Multi-
Engine Machine Translation (MEMT). 
Research into MT hybrids has increased over the 
last few years. Some research focused on the hy-
brid of various corpus-based MT methods, such as 
SMT and EBMT (Vogel and Ney, 2000; Marcu, 
2001; Groves and Way, 2006; Menezes and Quirk, 
2005). Others tried to exploit the advantages of 
both rule-based and corpus-based methods. Habash 
et al (2006) built an Arabic-English generation-
heavy MT system and boosted it with SMT com-
ponents. METIS-II is a hybrid machine translation 
system, in which insights from SMT, EBMT, and 
RBMT are used (Vandeghinste et al, 2006). Seneff 
et al (2006) combined an interlingual translation 
framework with phrase-based SMT for spoken 
language translation in a limited domain. They 
automatically generated a corpus of English-
Chinese pairs from the same interlingual represen-
tation by parsing the English corpus and then para-
phrasing each utterance into both English and Chi-
nese. 
Frederking and Nirenburg (1994) produced the 
first MEMT system by combining outputs from 
three different MT engines based on their knowl-
edge of the inner workings of the engines. Nomoto 
(2004) used voted language models to select the 
best output string at sentence level. Some recent 
approaches to MEMT used word alignment tech-
niques for comparison between the MT systems 
(Jayaraman and Lavie, 2005; Zaanen and Somers, 
2005; Matusov et al 2006). All the above MEMT 
systems operate on MT outputs for complete input 
sentences. Mellebeek et al (2006) presented a dif-
ferent approach, using a recursive decomposition 
algorithm that produces simple chunks as input to 
the MT engines. A consensus translation is pro-
duced by combining the best chunk translation. 
This paper uses RBMT outputs to improve the 
performance of SMT systems. Instead of RBMT 
outputs, other researchers have used SMT outputs 
to boost translation quality. Callision-Burch and 
Osborne (2003) used co-training to extend existing 
parallel corpora, wherein machine translations are 
selectively added to training corpora with multiple 
source texts. They also created training data for a 
language pair without a parallel corpus by using 
multiple source texts. Ueffing (2006) explored 
monolingual source-language data to improve an 
existing machine translation system via self-
training. The source data is translated by a SMT 
system, and the reliable translations are automati-
cally identified. Both of the methods improved 
translation quality. 
3 Method 
In this paper, we use the synthetic and real bilin-
gual corpus to train the phrase-based translation 
models. 
3.1  Phrase-Based Models 
According to the translation model presented in 
(Koehn et al, 2003), given a source sentence f , 
the best target translation  can be obtained 
using the following model 
beste
288
)()()(maxarg
)(maxarg
|
|
e
e
e
eef
fee
length
LM
best
?pp
p
=
=
 (1)
Where the translation model can be 
decomposed into  
)( | efp
?
=
??=
I
i
iiiiii
II
aefpbadef
efp
1
1
11
),|()()|(
)|(
?? w
(2)
Where )|( ii ef?  is the phrase translation prob-
ability.  denotes the start position of the source 
phrase that was translated into the ith target phrase, 
and  denotes the end position of the source 
phrase translated into the (i-1)th target phrase. 
 is the distortion probability. 
ia
1?ib
)( 1?? ii bad
),|( aefp iiw  is the lexical weight, and ?  is the 
strength of the lexical weight. 
3.2 Interpolated Models 
We train synthetic models with the synthetic bilin-
gual corpus produced by the RBMT systems. We 
can also train a translation model, namely standard 
model, if a real bilingual corpus is available. In 
order to make full use of these two kinds of cor-
pora, we conduct linear interpolation between them. 
In this paper, the distortion probability in equa-
tion (2) is estimated during decoding, using the 
same method as described in Pharaoh (Koehn, 
2004). For the phrase translation probability and 
lexical weight, we interpolate them as shown in (3) 
and (4). 
?
=
=
n
i
ii efef
0
)|()|( ???  (3)
?
=
=
n
i
ii aefpaefp
0
),|(),|( w,w ?  (4)
Where )|(0 ef?  and ),|( aefpw,0  denote the 
phrase translation probability and lexical weight 
trained with the real bilingual corpus, respectively. 
)|( efi?  and ),|( aefp iw,  ( ) are the 
phrase translation probability and lexical weight 
estimated by n  synthetic corpora produced by the 
RBMT systems. 
ni ,...,1=
i?  and i?  are interpolation coef-
ficients, ensuring  and . 1
0
=?
=
n
i
i? 1
0
=?
=
n
i
i?
4 Resources Used in Experiments 
4.1 Data 
In the experiments, we take English-Chinese trans-
lation as a case study. The real bilingual corpus 
includes 494,149 English-Chinese bilingual sen-
tence pairs. The monolingual English corpus is 
selected from the English Gigaword Second Edi-
tion, which is provided by Linguistic Data Consor-
tium (LDC) (catalog number LDC2005T12). The 
selected monolingual corpus includes 1,087,651 
sentences. 
For language model training, we use part of the 
Chinese Gigaword Second Edition provided by 
LDC (catalog number LDC2005T14). We use 
41,418 documents selected from the ZaoBao 
Newspaper and 992,261 documents from the Xin-
Hua News Agency to train the Chinese language 
model, amounting to 5,398,616 sentences. 
The test set and the development set are from 
the corpora distributed for the 2005 HTRDP 2  
evaluation of machine translation.  It can be ob-
tained from Chinese Linguistic Data Consortium 
(catalog number 2005-863-001). We use the same 
494 sentences in the test set and 278 sentences in 
the development set. Each source sentence in the 
test set and the development set has 4 different ref-
erences. 
4.2 Tools 
In this paper, we use two off-the-shelf commercial 
English to Chinese RBMT systems to produce the 
synthetic bilingual corpus. 
We also need a trainer and a decoder to perform 
phrase-based SMT. We use Koehn's training 
scripts 3  to train the translation model, and the 
SRILM toolkit (Stolcke, 2002) to train language 
model. For the decoder, we use Pharaoh (Koehn, 
2004). We run the decoder with its default settings 
(maximum phrase length 7) and then use Koehn's 
implementation of minimum error rate training 
(Och, 2003) to tune the feature weights on the de-
                                                 
2 The full name of HTRDP is National High Technology Re-
search and Development Program of China, also named as 863 
Program. 
3  It is located at http://www.statmt.org/wmt06/shared-
task/baseline.html. 
289
velopment set. The translation quality is evaluated 
using a well-established automatic measure: BLEU 
score (Papineni et al, 2002). We use the same 
method described in (Koehn and Monz, 2006) to 
perform the significance test. 
5 Experimental Results 
5.1 Results on Synthetic Corpus Only 
With the monolingual English corpus and the Eng-
lish side of the real bilingual corpus, we translate 
them into Chinese using the two commercial 
RBMT systems and produce two synthetic bilin-
gual corpora. With the corpora, we train two syn-
thetic models as described in section 3.1. Based on 
the synthetic models, we also perform linear inter-
polation as shown in section 3.2, without the stan-
dard models. We tune the interpolation weights 
using the development set, and achieve the best 
performance when 58.01 =? , 42.02 =? , 
58.01 =? , and 42.02 =? . The translation results 
on the test set are shown in Table 1. Synthetic 
model 1 and 2 are trained using the synthetic bilin-
gual corpora produced by RBMT system 1 and 
RBMT system 2, respectively. 
Method BLEU 
RBMT system 1 0.1681 
RBMT system 2 0.1453 
Synthetic Model 1 0.1644 
Synthetic Model 2 0.1668 
Interpolated Synthetic Model 0.1878 
Table 1. Translation Results Using Synthetic Bi-
lingual Corpus 
From the results, it can be seen that the interpo-
lated synthetic model obtains the best result, with 
an absolute improvement of the 0.0197 BLEU 
(11.7% relative) as compared with RBMT system 
1, and 0.0425 BLEU (29.2% relative) as compared 
with RBMT system 2. It is very promising that our 
method can build an SMT system that significantly 
outperforms both of the two RBMT systems, using 
the synthetic bilingual corpus produced by two 
RBMT systems. 
5.2 Results on Real and Synthetic Corpus 
With the real bilingual corpus, we build a standard 
model. We interpolate the standard model with the 
two synthetic models built in section 5.1 to obtain 
interpolated models. The translation results are 
shown in Table 2. The interpolation coefficients 
are both for phrase table probabilities and lexical 
weights. They are also tuned using the develop-
ment set.  
From the results, it can be seen that all the three 
interpolated models perform not only better than 
the RBMT systems but also better than the SMT 
system trained on the real bilingual corpus. The 
interpolated model combining the standard model 
and the two synthetic models performs the best, 
achieving a statistically significant improvement of 
about 0.0245 BLEU (13.1% relative) as compared 
with the standard model with no synthetic corpus. 
It also achieves 26.1% and 45.8% relative im-
provement as compared with the two RBMT sys-
tems respectively. The results indicate that using 
the corpus produced by RBMT systems, the per-
formance of the SMT system can be greatly im-
proved. The results also indicate that the more the 
RBMT systems are used, the better the translation 
quality is. 
Interpolation Coefficients 
Standard 
model 
Synthetic 
Model 1 
Synthetic 
Model 2 
BLEU 
1 ? ? 0.1874 
0.90 0.10 ? 0.2056 
0.86 ? 0.14 0.2040 
0.70 0.12 0.18 0.2119 
Table 2. Translation Results Using Standard and 
Synthetic Bilingual Corpus 
5.3 Effect of Synthetic Corpus Size 
To explore the relationship between the translation 
quality and the scale of the synthetic bilingual cor-
pus, we interpolate the standard model with the 
synthetic models trained with synthetic bilingual 
corpus of different sizes. In order to simplify the 
procedure, we only use RBMT system 1 to trans-
late the 1,087,651 monolingual English sentences 
to produce the synthetic bilingual corpus.  
We randomly select 20%, 40%, 60%, 80%, and 
100% of the synthetic bilingual corpus to train dif-
ferent synthetic models. The translation results of 
the interpolated models are shown in Figure 1. The 
results indicate that the larger the synthetic bilin-
gual corpus is, the better translation performance 
would be. 
290
0.13
0.15
0.17
0.19
0.21
20 40 60 80 100
Synthetic Bilingual Corpus (%)
B
L
E
U
Interpolated 
Standard
Synthetic
 
0.12
0.14
0.16
0.18
0.2
0.22
20 40 60 80 100
Real Bilingual Corpus (%)
B
L
E
U
Interpolated 
Standard
Synthetic
 
Figure 1. Comparison of Translation Results Using 
Synthetic Bilingual Corpus of Different Sizes 
Figure 2. Comparison of Translation Results Using 
Real Bilingual Corpus of Different Sizes 
5.4 Effect of Real Corpus Size Interpolation Coefficients 
Standard 
model 
Synthetic 
Model 1 
Synthetic 
Model 2 
BLEU 
1 ? ? 0.1874 
? 1 ? 0.1560 
? ? 1 0.1522 
0.80 0.10 0.10 0.1972 
Another issue is the relationship between the SMT 
performance and the size of the real bilingual cor-
pus. To train different standard models, we ran-
domly build five corpora of different sizes, which 
contain 20%, 40%, 60%, 80%, and 100% sentence 
pairs of the real bilingual corpus, respectively. As 
to the synthetic model, we use the same synthetic 
model 1 that is described in section 5.1. Then we 
build five interpolated models by performing linear 
interpolation between the synthetic model and the 
five standard models respectively.  The translation 
results are shown in Figure 2.  
Table 3. Translation Results without Additional 
Monolingual Corpus 
 Standard Model 
Synthetic 
Model 1 
Synthetic 
Model 2 
Standard 
Model 6,105,260 ? ? 
Synthetic 
Model 1 356,795 12,062,068 ? 
Synthetic 
Model 2 357,489 881,921 9,216,760
From the results, we can see that the larger the 
real bilingual corpus is, the better the performance 
of both standard models and interpolated models 
would be. The relative improvement of BLEU 
scores is up to 27.5% as compared with the corre-
sponding standard models. 
Table 4. Numbers of Phrase Pairs  5.5 Results without Additional Monolingual 
Corpus cant improvement of about 0.01 BLEU (5.2% rela-
tive) as compared with the standard model without 
using the synthetic corpus. In all the above experiments, we use an additional English monolingual corpus to get more synthetic 
bilingual corpus. We are also interested in the re-
sults without the additional monolingual corpus. In 
such case, the only English monolingual corpus is 
the English side of the real bilingual corpus. We 
use this smaller size of monolingual corpus and the 
real bilingual corpus to conduct similar experi-
ments as in section 5.2. The translation results are 
shown in Table 3. 
In order to further analyze the translation results, 
we examine the overlap and the difference among 
the phrase tables. The analytic results are shown in 
Table 4. More phrase pairs are extracted by the 
synthetic models, about twice by the synthetic 
model 1 in particular, than those extracted by the 
standard model. The overlap between each model 
is very low. For example, about 6% phrase pairs 
extracted by the standard model make appearance 
in both the standard model and the synthetic model 
1. This also explains why the interpolated model 
outperforms that of the standard model in Table 3.  
From the results, it can be seen that our method 
works well even if no additional monolingual cor-
pus is available. We achieve a statistically signifi- 
291
Methods English Sentence / Chinese Translations BLEU
 
This move helps spur the enterprise to strengthen technical innovation, man-
agement innovation and the creation of a brand name and to strengthen mar-
keting, after-sale service, thereby fundamentally enhance the enterprise's 
competitiveness; 
 
Standard 
model 
? ? ?? ??? ?? ?? ?? ?? ?? ? ?? ?? ? ?? ?? 
?? ? ?? ? ?? ? ?? ? ?? ?? ? ??? ? ? ?? ?? 0.5022
RBMT Sys-
tem 1 
?? ?? ?? ?? ?? ?? ?? ? ?? ? ?? ?? ? ?? ?? 
?? ?? ?? ?? ? ? ?? ?? ? ??? ?? ?? ?? ? ?
? ? 
0.1535
RBMT Sys-
tem 2 
?? ?? ?? ?? ?? ?? ?? ?? ?? ? ?? ?? ? ? ? 
?? ? ?? ? ?? ?? ?? ? ?? ???? ?? ?? ?? ?? 
? ?? ? 
0.1485
Interpolated 
Model 
? ? ?? ??? ?? ?? ?? ?? ?? ? ?? ?? ? ?? ?? 
? ?? ?? ?? ? ???? ? ?? ? ?? ? ?? ?? ? ??
? ? 
0.7198
Table 5. Translation Example 
This move  ? ? ?? This move  ? ? ?? 
helps  ??? helps  ??? 
spur  ?? spur  ?? 
the enterprise  ?? the enterprise  ?? 
to strengthen  ?? to strengthen  ?? 
technical  ?? technical  ?? 
innovation  ?? innovation  ?? 
, management  ? ?? , management  ? ?? 
innovation  ?? innovation  ?? 
and the creation of a  ? ?? and the creation of  ? ?? 
  (he jianli)  (he chuangzao) 
brand name  ?? a brand name  ?? 
  (pinpai)  (pinpai) 
and to strengthen  ?? ? and to strengthen  ? ?? 
marketing ,  ?? marketing ,  ?? ?? ? 
  (fuwu) after-sale service  ???? 
after-sale  ? ??  (shouhoufuwu) 
service  ? ?? ? , thereby  ? ?? 
, thereby  ?? fundamentally  ? ?? ? 
fundamentally  ?? enhance the  ?? 
enhance  ? ??? enterprise 's  ?? ? 
the enterprise  ? competitiveness  ??? 
's competitiveness  ? ?? ;  ? 
;  ??   
  (shouhou)   
(a) Results Produced by the Standard Model (b) Results Produced by the Interpolated Model 
Figure 3. Phrase Pairs Used for Translation 
 
292
6 Discussion 
6.1 Model Interpolation vs. Corpus Merge 
In section 5, we make use of the real bilingual cor-
pus and the synthetic bilingual corpora by perform-
ing model interpolation. Another available way is 
directly combining these two kinds of corpora to 
train a translation model, namely corpus merge. In 
order to compare these two methods, we use 
RBMT system 1 to translate the 1,087,651 mono-
lingual English sentences to produce synthetic bi-
lingual corpus. Then we train an SMT system with 
the combination of this synthetic bilingual corpus 
and the real bilingual corpus. The BLEU score of 
such system is 0.1887, while that of the model in-
terpolation system is 0.2020. It indicates that the 
model interpolation method is significantly better 
than the corpus merge method. 
6.2 Result Analysis 
As discussed in Section 5.5, the number of the 
overlapped phrase pairs among the standard model 
and the synthetic models is very small. The newly 
added phrase pairs from the synthetic models can 
assist to improve the translation results of the in-
terpolated model. In this section, we will use an 
example to further discuss the reason behind the 
improvement of the SMT system by using syn-
thetic bilingual corpus. Table 5 shows an English 
sentence and its Chinese translations produced by 
different methods. And Figure 3 shows the phrase 
pairs used for translation. The results show that 
imperfect translations of RBMT systems can be 
also used to boost the performance of an SMT sys-
tem. 
 Phrase Pairs 
Phrase 
Pairs 
Used 
New 
Pairs 
Used 
Standard 
Model 6,105,260 5,509 ? 
Interpolated 
Model 73,221,525 5,306 1993 
Table 6. Statistics of Phrase Pairs 
Further analysis is shown in Table 6. After add-
ing the synthetic corpus produced by the RBMT 
systems, the interpolated model outperforms the 
standard models mainly for the following two rea-
sons: (1) some new phrase pairs are added into the 
interpolated model. 37.6% phrase pairs (1993 out 
of 5306) are newly learned and used for translation. 
For example, the phrase pair "after-sale service <-> 
???? (shouhoufuwu)" is added; (2) The prob-
ability distribution of the phrase pairs is changed. 
For example, the probabilities of the two pairs "a 
brand name <-> ?? (pinpai)" and "and the crea-
tion of <-> ? ?? (he chuangzao)" increase. The 
probabilities of the other two pairs "brand name <-
> ?? (pinpai)" and "and the creation of a <-> ? 
??  (he jianli)" decrease. We found that 930 
phrase pairs, which are also in the phrase table of 
the standard model, are used by the interpolated 
model for translation but not used by the standard 
model. 
6.3 Human Evaluation 
According to (Koehn and Monz, 2006; Callison-
Burch et al, 2006), the RBMT systems are usually 
not adequately appreciated by BLEU. We also 
manually evaluated the RBMT systems and SMT 
systems in terms of both adequacy and fluency as 
defined in (Koehn and Monz, 2006). The evalua-
tion results show that the SMT system with the 
interpolated model, which achieves the highest 
BLEU scores in Table 2, achieves slightly better 
adequacy and fluency scores than the two RBMT 
systems. 
7 Conclusion and Future Work 
We presented a method using the existing RBMT 
system as a black box to produce synthetic bilin-
gual corpus, which was used as training data for 
the SMT system. We used the existing RBMT sys-
tem to translate the monolingual corpus into a syn-
thetic bilingual corpus. With the synthetic bilingual 
corpus, we could build an SMT system even if 
there is no real bilingual corpus. In our experi-
ments using BLEU as the metric, such a system 
achieves a relative improvement of 11.7% over the 
best RBMT system that is used to produce the syn-
thetic bilingual corpora. It indicates that using the 
existing RBMT systems to produce a synthetic bi-
lingual corpus, we can build an SMT system that 
outperforms the existing RBMT systems. 
We also interpolated the model trained on a real 
bilingual corpus and the models trained on the syn-
thetic bilingual corpora, the interpolated model 
achieves an absolute improvement of 0.0245 
BLEU score (13.1% relative) as compared with the 
individual model trained on the real bilingual cor-
293
pus. It indicates that we can build a better SMT 
system by leveraging the real and the synthetic bi-
lingual corpus. 
Further result analysis shows that after adding 
the synthetic corpus produced by the RBMT sys-
tems, the interpolated model outperforms the stan-
dard models mainly because of two reasons: (1) 
some new phrase pairs are added to the interpo-
lated model; (2) the probability distribution of the 
phrase pairs is changed. 
In the future work, we will investigate the possi-
bility of training a reverse SMT system with the 
RBMT systems. For example, we will investigate 
to train Chinese-to-English SMT system based on 
natural English and RBMT-generated synthetic 
Chinese. 
References 
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 
2000. Learning Dependency Translation Models as 
Collections of Finite-State Head Transducers. Com-
putational Linguistics, 26(1): 45-60. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 19(2): 
263-311. 
Chris Callison-Burch and Miles Osborne. 2003. Boot-
strapping Parallel Corpora. In Proceedings of the 
Human Language Technology conference / North 
American chapter of the Association for Computa-
tional Linguistics (HLT/NAACL-2003) Workshop on 
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 44-49. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn, 2006. Re-evaluating the Role of Bleu in Ma-
chine Translation Research. In Proceedings of the 
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
2006), pages 249-256. 
Michel Carl, Paul Schmidt, and Jorg Schutz. 2005. Re-
versible Template-based Shake & Bake Generation. 
In Proceedings of the 10th Machine Translation 
Summit Workshop on Example-Based Machine 
Translation, pages 17-25. 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2005), 
pages 263-270. 
Robert Frederking and Sergei Nirenburg. 1994. Three 
Heads Are Better Than One. In Proceedings of the 
4th Applied Natural Language Processing Confer-
ence (ANLP-1994), pages 95-100. 
Declan Groves and Andy Way. 2006. Hybridity in MT: 
Experiments on the Europarl Corpus. In Proceedings 
of the 11th Annual Conference of the European As-
sociation for Machine Translation (EAMT-2006), 
pages 115-124. 
Nizar Habash, Bonnie Dorr, and Christof Monz. 2006 
Challenges in Building an Arabic-English GHMT 
System with SMT Components. In Proceedings of 
the 11th Annual Conference of the European Asso-
ciation for Machine Translation (EAMT-2006), pages 
56-65. 
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine Machine Translation Guided by Explicit 
Word Matching. In Proceedings of the 10th Annual 
Conference of the European Association for Machine 
Translation (EAMT-2005), pages 143-152. 
Philipp Koehn. 2004. Pharaoh: A Beam Search Decoder 
For Phrase-Based Statistical Machine Translation 
Models. In Proceedings of the 6th Conference of the 
Association for Machine Translation in the Americas 
(AMTA-2004), pages 115-124. 
Philipp Koehn and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. In Proceedings of the 
Human Language Technology conference / North 
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL-2006) Workshop on 
Statistical Machine Translation, pages 102-121. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Human Language Technology con-
ference / North American Chapter of the Association 
for Computational Linguistics (HLT/NAACL-2003), 
pages 127-133. 
Daniel Marcu. 2001. Towards a Unified Approach to 
Memory- and Statistical-based Machine Translation. 
In Proceedings of the Association for Computational 
Linguistics / European Chapter of the Association for 
Computational Linguistics (ACL/EACL-2001), pages 
378-385. 
Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 
2006. Computing Consensus Translation from Multi-
ple Machine Translation Systems Using Enhanced 
Hypotheses Alignment. In Proceedings of the 11th 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006), 
pages 33-40. 
294
Bart Mellebeek, Karolina Owczarzak, Josef Van 
Genabith, and Andy Way. 2006. Multi-engine Ma-
chine Translation by Recursive Sentence Decomposi-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas 
(AMTA-2006), pages 110-118. 
Arul Menezes and Chris Quirk. 2005. Dependency 
treelet translation: the convergence of statistical and 
example-based machine-translation? In Proceedings 
of the 10th Machine Translation Summit Workshop 
on Example-Based Machine Translation, pages 99-
108. 
Tadashi Nomoto. 2004. Multi-Engine machine transla-
tion with voted language model. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL-2004), pages 494-
501. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-2003), pages 160-167. 
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach To Statistical Machine 
Translation. Computational Linguistics, 30(4):417-
449. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL-2002), pages 311-
318. 
Stephanie Seneff, Chao Wang, and John Lee. 2006. 
Combining Linguistic and Statistical Methods for Bi-
Directional English Chinese Translation in the Flight 
Domain. In Proceedings of the 7th Conference of the 
Association for Machine Translation in the Americas 
(AMTA-2006), pages 213-222. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the 5th 
International Conference on Spoken Language Proc-
essing (ICSLP-2002), pages 901-904. 
Nicola Ueffing. 2006. Using Monolingual Source-
Language Data to Improve MT Performance.  In 
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT-2006), pages 174-
181. 
Vincent Vandeghinste, Ineka Schuurman, Michael Carl, 
Stella Markantonatou, and Toni Badia. 2006. Metis-
II: Machine Translation for Low-Resource Lan-
guages. In Proceedings of the 5th International Con-
ference on Language Resources and Evaluation (L-
REC-2006), pages 1284-1289. 
Stephan Vogel and Hermann Ney. 2000. Construction 
of a Hierarchical Translation Memory. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING-2000), pages 1131-
1135. 
Andy Way and Nano Gough. 2005. Comparing Exam-
ple-Based and Statistical Machine Translation. Natu-
ral Language Engineering, 11(3): 295-309. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3): 377-403. 
Kenji Yamada and Kevin Knight. 2001. A Syntax Based 
Statistical Translation Model. In Proceedings of the 
Association for Computational Linguistics / Euro-
pean Chapter of the Association for Computational 
Linguistics (ACL/EACL-2001), pages 523-530. 
Menno van Zaanen and Harold Somers. 2005. DE-
MOCRAT: Deciding between Multiple Outputs Cre-
ated by Automatic Translation. In Proceedings of the 
10th Machine Translation Summit, pages 173-180. 
 
 
295
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142?146,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Improve Statistical Machine Translation with Context-Sensitive
Bilingual Semantic Embedding Model
Haiyang Wu
1
Daxiang Dong
1
Wei He
1
Xiaoguang Hu
1
Dianhai Yu
1
Hua Wu
1
Haifeng Wang
1
Ting Liu
2
1
Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2
Harbin Institute of Technology, Harbin, China
wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai,
wu hua,wanghaifeng@baidu.com
tliu@ir.hit.edu.cn
Abstract
We investigate how to improve bilingual
embedding which has been successfully
used as a feature in phrase-based sta-
tistical machine translation (SMT). De-
spite bilingual embedding?s success, the
contextual information, which is of criti-
cal importance to translation quality, was
ignored in previous work. To employ
the contextual information, we propose
a simple and memory-efficient model for
learning bilingual embedding, taking both
the source phrase and context around the
phrase into account. Bilingual translation
scores generated from our proposed bilin-
gual embedding model are used as features
in our SMT system. Experimental results
show that the proposed method achieves
significant improvements on large-scale
Chinese-English translation task.
1 Introduction
In Statistical Machine Translation (SMT) sys-
tem, it is difficult to determine the translation of
some phrases that have ambiguous meanings.For
example, the phrase??? jieguo? can be trans-
lated to either ?results?, ?eventually? or ?fruit?,
depending on the context around it. There are two
reasons for the problem: First, the length of phrase
pairs is restricted due to the limitation of model
size and training data. Another reason is that SMT
systems often fail to use contextual information
in source sentence, therefore, phrase sense disam-
biguation highly depends on the language model
which is trained only on target corpus.
To solve this problem, we present to learn
context-sensitive bilingual semantic embedding.
Our methodology is to train a supervised model
where labels are automatically generated from
phrase-pairs. For each source phrase, the aligned
target phrase is marked as the positive label
whereas other phrases in our phrase table are
treated as negative labels. Different from previ-
ous work in bilingual embedding learning(Zou et
al., 2013; Gao et al., 2014), our framework is a
supervised model that utilizes contextual informa-
tion in source sentence as features and make use
of phrase pairs as weak labels. Bilingual seman-
tic embeddings are trained automatically from our
supervised learning task.
Our learned bilingual semantic embedding
model is used to measure the similarity of phrase
pairs which is treated as a feature in decoding. We
integrate our learned model into a phrase-based
translation system and experimental results indi-
cate that our system significantly outperform the
baseline system. On the NIST08 Chinese-English
translation task, we obtained 0.68 BLEU improve-
ment. We also test our proposed method on much
larger web dataset and obtain 0.49 BLEU im-
provement against the baseline.
2 Related Work
Using vectors to represent word meanings is
the essence of vector space models (VSM). The
representations capture words? semantic and syn-
tactic information which can be used to measure
semantic similarities by computing distance be-
tween the vectors. Although most VSMs represent
one word with only one vector, they fail to cap-
ture homonymy and polysemy of word. Huang
et al. (2012) introduced global document context
and multiple word prototypes which distinguishes
and uses both local and global context via a joint
training objective. Much of the research focus
on the task of inducing representations for sin-
gle languages. Recently, a lot of progress has
142
been made at representation learning for bilin-
gual words. Bilingual word representations have
been presented by Peirsman and Pad?o (2010) and
Sumita (2000). Also unsupervised algorithms
such as LDA and LSA were used by Boyd-Graber
and Resnik (2010), Tam et al. (2007) and Zhao and
Xing (2006). Zou et al. (2013) learn bilingual em-
beddings utilizes word alignments and monolin-
gual embeddings result, Le et al. (2012) and Gao et
al. (2014) used continuous vector to represent the
source language or target language of each phrase,
and then computed translation probability using
vector distance. Vuli?c and Moens (2013) learned
bilingual vector spaces from non-parallel data in-
duced by using a seed lexicon. However, none
of these work considered the word sense disam-
biguation problem which Carpuat and Wu (2007)
proved it is useful for SMT. In this paper, we learn
bilingual semantic embeddings for source content
and target phrase, and incorporate it into a phrase-
based SMT system to improve translation quality.
3 Context-Sensitive Bilingual Semantic
Embedding Model
We propose a simple and memory-efficient
model which embeds both contextual information
of source phrases and aligned phrases in target cor-
pus into low dimension. Our assumption is that
high frequent words are likely to have multiple
word senses; therefore, top frequent words are se-
lected in source corpus. We denote our selected
words as focused phrase. Our goal is to learn a
bilingual embedding model that can capture dis-
criminative contextual information for each fo-
cused phrase. To learn an effective context sensi-
tive bilingual embedding, we extract context fea-
tures nearby a focused phrase that will discrimi-
nate focused phrase?s target translation from other
possible candidates. Our task can be viewed as
a classification problem that each target phrase is
treated as a class. Since target phrases are usu-
ally in very high dimensional space, traditional
linear classification model is not suitable for our
problem. Therefore, we treat our problem as a
ranking problem that can handle large number of
classes and optimize the objectives with scalable
optimizer stochastic gradient descent.
3.1 Bilingual Word Embedding
We apply a linear embedding model for bilin-
gual embedding learning. Cosine similarity be-
tween bilingual embedding representation is con-
sidered as score function. The score function
should be discriminative between target phrases
and other candidate phrases. Our score function
is in the form:
f(x,y; W,U) = cos(W
T
x,U
T
y) (1)
where x is contextual feature vector in source sen-
tence, and y is the representation of target phrase,
W ? R
|X|?k
,U ? R
|Y|?k
are low rank ma-
trix. In our model, we allow y to be bag-of-words
representation. Our embedding model is memory-
efficient in that dimensionality of x and y can be
very large in practical setting. We use |X| and |Y|
means dimensionality of random variable x and y,
then traditional linear model such as max-entropy
model requires memory space of O(|X||Y|). Our
embedding model only requires O(k(|X|+ |Y|))
memory space that can handle large scale vocabu-
lary setting. To score a focused phrase and target
phrase pair with f(x,y), context features are ex-
tracted from nearby window of the focused phrase.
Target words are selected from phrase pairs. Given
a source sentence, embedding of a focused phrase
is estimated from W
T
x and target phrase embed-
ding can be obtained through U
T
y.
3.2 Context Sensitive Features
Context of a focused phrase is extracted from
nearby window, and in our experiment we choose
window size of 6 as a focused phrase?s con-
text. Features are then extracted from the focused
phrase?s context. We demonstrate our feature
extraction and label generation process from the
Chinese-to-English example in figure 1. Window
size in this example is three. Position features
and Part-Of-Speech Tagging features are extracted
from the focused phrase?s context. The word fruit
Figure 1: Feature extraction and label generation
143
is the aligned phrase of our focused phrase and is
treated as positive label. The phrase results is a
randomly selected phrase from phrase table results
of ??. Note that feature window is not well de-
fined near the beginning or the end of a sentence.
To conquer this problem, we add special padding
word to the beginning and the end of a sentence to
augment sentence.
3.3 Parameter Learning
To learn model parameter W and U, we ap-
ply a ranking scheme on candidates selected from
phrase table results of each focused phrase. In par-
ticular, given a focus phrase w, aligned phrase is
treated as positive label whereas phrases extracted
from other candidates in phrase table are treated
as negative label. A max-margin loss is applied in
this ranking setting.
I(?) =
1
m
m
?
i=1
(? ? f(x
i
, y
i
; ?)? f(x
i
, y
?
i
; ?))+
(2)
Where f(x
i
,y
i
) is previously defined, ? =
{W,U} and + means max-margin hinge loss. In
our implementation, a margin of ? = 0.15 is used
during training. Objectives are minimized through
stochastic gradient descent algorithm. For each
randomly selected training example, parameters
are updated through the following form:
? := ?? ?
?l(?)
??
(3)
where ? = {W,U}. Given an instance with pos-
itive and negative label pair {x,y,y
?
}, gradients
of parameter W and U are as follows:
?l(W,U)
?W
= qsx(W
T
x)
T
? pqs
3
x(U
T
y) (4)
?l(W,U)
?U
= qsy(U
T
y)
T
? pqs
3
y(W
T
x) (5)
Where we set p = (W
T
x)
T
(U
T
y), q =
1
||W
T
x||
2
and s =
1
||U
T
y||
2
. To initialize our model param-
eters with strong semantic and syntactic informa-
tion, word vectors are pre-trained independently
on source and target corpus through word2vec
(Mikolov et al., 2013). And the pre-trained word
vectors are treated as initial parameters of our
model. The learned scoring function f(x,y) will
be used during decoding phase as a feature in log-
linear model which we will describe in detail later.
4 Integrating Bilingual Semantic
Embedding into Phrase-Based SMT
Architectures
To incorporate the context-sensitive bilingual
embedding model into the state-of-the-art Phrase-
Based Translation model, we modify the decoding
so that context information is available on every
source phrase. For every phrase in a source sen-
tence, the following tasks are done at every node
in our decoder:
? Get the focused phrase as well as its context in the
source sentence.
? Extract features from the focused phrase?s context.
? Get translation candidate extracted from phrase pairs of
the focused phrase.
? Compute scores for any pair of the focused phrase and
a candidate phrase.
We get the target sub-phrase using word align-
ment of phrase, and we treat NULL as a common
target word if there is no alignment for the focused
phrase. Finally we compute the matching score for
source content and target word using bilingual se-
mantic embedding model. If there are more than
one word in the focus phrase, then we add all score
together. A penalty value will be given if target is
not in translation candidate list. For each phrase in
a given SMT input sentence, the Bilingual Seman-
tic score can be used as an additional feature in
log-linear translation model, in combination with
other typical context-independent SMT bilexicon
probabilities.
5 Experiment
Our experiments are performed using an in-
house phrase-based system with a log-linear
framework. Our system includes a phrase trans-
lation model, an n-gram language model, a lexi-
calized reordering model, a word penalty model
and a phrase penalty model, which is similar to
Moses (Koehn et al., 2007). The evaluation metric
is BLEU (Papineni et al., 2002).
5.1 Data set
We test our approach on LDC corpus first. We
just use a subset of the data available for NIST
OpenMT08 task
1
. The parallel training corpus
1
LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards )
144
Method
OpenMT08 WebData
BLEU BLEU
Our Baseline 26.24 29.32
LOC 26.78** 29.62*
LOC+POS 26.82** 29.81*
Table 1: Results of lowercase BLEU on NIST08
task. LOC is the location feature and POS is
the Part-of-Speech feature * or ** equals to sig-
nificantly better than our baseline(? < 0.05 or
? < 0.01, respectively)
contains 1.5M sentence pairs after we filter with
some simple heuristic rules, such as sentence be-
ing too long or containing messy codes. As mono-
lingual corpus, we use the XinHua portion of the
English GigaWord. In monolingual corpus we fil-
ter sentence if it contain more than 100 words
or contain messy codes, Finally, we get mono-
lingual corpus containing 369M words. In order
to test our approach on a more realistic scenario,
we train our models with web data. Sentence
pairs obtained from bilingual website and com-
parable webpage. Monolingual corpus is gained
from some large website such as WiKi. There are
50M sentence pairs and 10B words monolingual
corpus.
5.2 Results and Analysis
For word alignment, we align all of the train-
ing data with GIZA++ (Och and Ney, 2003), us-
ing the grow-diag-final heuristic to improve recall.
For language model, we train a 5-gram modified
Kneser-Ney language model and use Minimum
Error Rate Training (Och, 2003) to tune the SMT.
For both OpenMT08 task and WebData task, we
use NIST06 as the tuning set, and use NIST08 as
the testing set. Our baseline system is a standard
phrase-based SMT system, and a language model
is trained with the target side of bilingual corpus.
Results on Chinese-English translation task are re-
ported in Table 1. Word position features and part-
of-speech tagging features are both useful for our
bilingual semantic embedding learning. Based on
our trained bilingual embedding model, we can
easily compute a translation score between any
bilingual phrase pair. We list some cases in table
2 to show that our bilingual embedding is context
sensitive.
Contextual features extracted from source sen-
tence are strong enough to discriminate different
Source Sentence
4 Nearest Neighbor from
bilingual embedding
??????????
?????????
?????(Investors
can only get down to
business in a stable so-
cial environment)
will be, can only, will, can
??????????
?????????
?????(In compe-
titions, the Chinese Dis-
abled have shown ex-
traordinary athletic abil-
ities)
skills, ability, abilities, tal-
ent
??????????
?????????
????(In the natu-
ral environment of Costa
Rica, grapes do not nor-
mally yield fruit.)
fruit, outcome of, the out-
come, result
? ? ??????
???????(As
a result, Eastern District
Council passed a pro-
posal)
in the end, eventually, as a
result, results
Table 2: Top ranked focused phrases based on
bilingual semantic embedding
word senses. And we also observe from the word
??? jieguo? that Part-Of-Speech Tagging fea-
tures are effective in discriminating target phrases.
6 Conlusion
In this paper, we proposed a context-sensitive
bilingual semantic embedding model to improve
statistical machine translation. Contextual infor-
mation is used in our model for bilingual word
sense disambiguation. We integrated the bilingual
semantic model into the phrase-based SMT sys-
tem. Experimental results show that our method
achieves significant improvements over the base-
line on large scale Chinese-English translation
task. Our model is memory-efficient and practical
for industrial usage that training can be done on
large scale data set with large number of classes.
Prediction time is also negligible with regard to
SMT decoding phase. In the future, we will ex-
plore more features to refine the model and try to
utilize contextual information in target sentences.
Acknowledgments
We thank the three anonymous reviewers for
their valuable comments, and Niu Gang and Wu
Xianchao for discussions. This paper is supported
by 973 program No. 2014CB340505.
145
References
Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 45?55,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In Proc. ACL.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 39?48, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111?3119.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. In Computational Linguistics, Volume 29,
Number 1, March 2003. Computational Linguistics,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Yves Peirsman and Sebastian Pad?o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 921?929, Los Ange-
les, California, June. Association for Computational
Linguistics.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-lsa based lm adaptation for spoken lan-
guage translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 520?527, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Ivan Vuli?c and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 106?116, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969?976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
146
