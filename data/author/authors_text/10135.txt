Lexical Chains as Document Features
Dinakar Jayarajan, Dipti Deodhare
Centre for Artificial Intelligence and Robotics,
Defence R & D Organisation,
Bangalore, INDIA.
[dinakarj, dipti]@cair.drdo.in
B Ravindran
Dept. of CSE,
IIT Madras,
Chennai, INDIA.
ravi@cse.iitm.ac.in
Abstract
Document clustering and classification is
usually done by representing the documents
using a bag of words scheme. This scheme
ignores many of the linguistic and semantic
features contained in text documents. We
propose here an alternative representation
for documents using Lexical Chains. We
compare the performance of the new repre-
sentation against the old one on a cluster-
ing task. We show that Lexical Chain based
features give better results than the Bag of
Words based features, while achieving al-
most 30% reduction in the dimensionality of
the feature vectors resulting in faster execu-
tion of the algorithms.
1 Introduction
Text data usually contains complex semantic infor-
mation which is communicated using a combination
of words. Ideally, the representation used should
capture and reflect this fact in order to semantically
drive the clustering algorithm and obtain better re-
sults.
The Bag of Words (BoW) (Salton et al, 1975)
scheme is a very popular scheme which has been
used for representing documents. But, this scheme
ignores many of the linguistic and semantic features
contained in text documents. This paper explores
an alternative representation for documents, using
lexical chains, which encodes some of the semantic
information contained in the document. This rep-
resentation results in improved performance on the
clustering tasks and achieves a drastic reduction in
the size of the feature space as well.
The BoW scheme was originally designed for the
Information Retrieval domain (Salton, 1989) where
the aim was to ?index? the document and not nec-
essarily to model the topic distribution. This rep-
resentation has since been adopted as the defacto
document representation scheme for supervised and
unsupervised learning on documents. The BoW
scheme represents features as an unordered set of
words contained in the document, along with their
frequency count.
The BoW scheme assumes that the distribution
of words in a document reflect the underlying dis-
tribution of topics and hence if the documents are
grouped on the basis of the similarity of the words
contained in them, it will implicitly result in a clus-
tering based on topics. This representation, using
a simple frequency count alone, does not capture
all the underlying information present in the doc-
uments. Moreover, it ignores information such as
position, relations and co-occurrences among the
words. In addition, the feature space formed will
be very huge and sparse resulting in time and space
costs as well.
Lexical Chaining is a technique which seeks to
identify and exploit the semantic relatedness of
words in a document. It is based on the phe-
nomenon of lexical cohesion (Halliday and Hasan,
1976) and works on the premise that semantically
related words co-occur close together in a passage
more than ?just by chance?. Lexical chaining is the
process of identifying and grouping such words to-
gether to form chains which in turn will help in iden-
tifying and representing the topic and content of the
document.
Lexical chains have been used as an intermediate
representation of text for various tasks such as au-
111
tomatic text summarisation (Barzilay and Elhadad,
1997; Silber and McCoy, 2002), malapropism de-
tection and correction (Hirst and St-Onge, 1997),
and hypertext construction (Green, 1998). An al-
gorithm for computing lexical chains was first given
by (Morris and Hirst, 1991) using the Roget?s The-
saurus (Kirkpatrick, 1998). Since an electronic ver-
sion of the Roget?s Thesaurus was not available then,
later algorithms were based on the WordNet lexical
database (Fellbaum, 1998).
We present here a two pass algorithm to com-
pute a representation of documents using lexical
chains and use these lexical chains to derive fea-
ture vectors. These lexical chain based feature vec-
tors are used to cluster the documents using two dif-
ferent algorithms - k-Means and Co-clustering. k-
Means is a well studied clustering algorithm widely
used in the text domain. Co-clustering, also known
as bi-clustering (Madeira and Oliveira, 2004), is
a clustering approach which was developed in the
bioinformatics domain for clustering gene expres-
sions. Since the text domain shares a lot of char-
acteristics (high dimensionality, sparsity, etc.) of
gene expression data, a lot of interest has been
generated recently in applying the co-clustering ap-
proaches (Dhillon et al, 2003) to the text domain
with promising results. Co-clustering (Dhillon et al,
2003; Sra et al, 2004) exploits the duality between
rows and columns of the document-term matrix used
to represent the features, by simultaneously cluster-
ing both the rows and columns.
We compare the clustering results obtained from
document features extracted using lexical chains
against those obtained by using the traditional
method of bag of words.
2 Lexical Chains
Lexical chains are groups of words which exhibit
lexical cohesion. Cohesion as given by (Halliday
and Hasan, 1976) is a way of getting text to ?hang
together as a whole?. Lexical cohesion is exhib-
ited through cohesive relations. They (Halliday and
Hasan, 1976) have classified these relations as:
1. Reiteration with identity of reference
2. Reiteration without identity of reference
3. Reiteration by means of super ordinate
4. Systematic semantic relation
5. Non systematic semantic relation
The first three relations involve reiteration which
includes repetition of the same word in the same
sense (e.g., car and car), the use of a synonym for a
word (e.g., car and automobile) and the use of hyper-
nyms (or hyponyms) for a word (e.g., car and vehi-
cle) respectively. The last two relations involve col-
locations i.e, semantic relationships between words
that often co-occur (e.g., football and foul). Lexi-
cal chains in a text are identified by the presence of
strong semantic relations between the words in the
text.
Algorithms for building lexical chains work by
considering candidate words for inclusion in the
chains constructed so far. Usually these candidate
words are nouns and compound nouns. Lexical
Chains can be computed at various granularities -
across sentences, paragraphs or documents. In gen-
eral, to compute lexical chains, each candidate word
in the sentence/paragraph/document is compared,
with each lexical chain identified so far. If a candi-
date word has a ?cohesive relation? with the words in
the chain it is added to the chain. On the other hand,
if a candidate word is not related to any of the chains,
a new chain is created for the candidate word. Thus
a lexical chain is made up of a set of semantically
related words. The lexical chains obtained are then
evaluated based on a suitable criteria and the better
chains are selected and used to further processing.
Naturally, the computation of lexical chains is predi-
cated on the availability of a suitable database which
maps relations between words.
Several algorithms have been proposed for com-
puting lexical chains. Prominent among them are
those by (Hirst and St-Onge, 1997; Barzilay and El-
hadad, 1997; Silber and McCoy, 2002; Jarmasz and
Szpakowicz, 2003). Except for the one by Jarmasz
and Szpakowicz, all others use WordNet (Fellbaum,
1998) to identify relations among words. A brief
overview of these algorithms is given in (Jayarajan
et al, 2007).
WordNet is a lexical database which organises
words into synonym sets or synsets. Each synset
contains one or more words that have the same
meaning. A word may appear in many synsets, de-
pending on the number of senses that it has. The
112
synsets are connected by links that indicate differ-
ent semantic relations such as generalisation (hy-
pernyms), specialisation (hyponyms), part relations
(holonyms1 and meronyms2), etc.
Our approach to computing lexical chains differs
from those listed above and is described in the next
section.
3 Lexical Chains based Feature Vectors
All the algorithms mentioned in the previous sec-
tion, try to disambiguate the sense of the word as
part of the chaining process. Both Word Sense Dis-
ambiguation (WSD) and lexical chaining are very
profound processes. The aim of computing the lex-
ical chains here is to try and identify the topics in
a document. If WSD has be performed as an im-
plicit step in the lexical chain computing algorithm,
it tends to deteriorate the outcome of both. We feel
that the words should be disambiguated by looking
at their context in a sentence/paragraph as a whole.
As such, we propose to perform WSD as a prepro-
cessing step, before the word is considered for lex-
ical chaining. We use an algorithm by (Patwardhan
et al, 2003) to disambiguate the senses of the words
in reference to Wordnet. We then filter out all non-
noun words identified in the WSD stage. This is
based on the assumption that nouns are better at re-
flecting the topics contained in a document than the
other parts of speech. The result is a set of nouns
which appear in the text along with its sense. We
refer to these as ?candidate words?.
Our algorithm is based on the WordNet Lexical
Database. WordNet is used to identify the relations
among the words. We use only the identity and syn-
onymy relations to compute the chains. A word has
a identity or synonymy relation with another word,
only if both the words occur in the same synset in
Wordnet. Empirically, we found that usage of only
these two relations, resulted in chains representing
crisp topics.
A lexical chain contains a list of words which are
related to each other and is identified using a unique
numeric identifier. Each word in turn is represented
as a 4-tuple   term, pos, sense, rel , where ?pos? is
1part of, member of, substance of relations, e.g., ?wheel? is
part of a ?vehicle?
2has part, has member, has substance relations, e.g., ?wheel?
has part ?rim?
the part-of-speech of the term, ?sense? is the Word-
net sense number and ?rel? is the relation of this word
to the chain. In this case, we treat the two rela-
tions - identity and synonymy, as a single relation
and hence this is uniformly ?IS? for all the words.
Definition 1 Length of a lexical chain  is defined
as the number of words in the chain.
    	
 
               