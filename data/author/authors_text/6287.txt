Two-Phase Biomedical Named Entity
Recognition Using A Hybrid Method
Seonho Kim1, Juntae Yoon2, Kyung-Mi Park1, and Hae-Chang Rim1
1 Dept. of Computer Science and Engineering,
Korea University, Seoul, Korea
2 NLP Lab. Daumsoft Inc. Seoul, Korea
Abstract. Biomedical named entity recognition (NER) is a difficult
problem in biomedical information processing due to the widespread am-
biguity of terms out of context and extensive lexical variations. This pa-
per presents a two-phase biomedical NER consisting of term boundary
detection and semantic labeling. By dividing the problem, we can adopt
an effective model for each process. In our study, we use two exponential
models, conditional random fields and maximum entropy, at each phase.
Moreover, results by this machine learning based model are refined by
rule-based postprocessing implemented using a finite state method. Ex-
periments show it achieves the performance of F-score 71.19% on the
JNLPBA 2004 shared task of identifying 5 classes of biomedical NEs.
1 Introduction
Due to dynamic progress in biomedical literature, a vast amount of new infor-
mation and research results have been published and many of them are available
in the electronic form - for example, like the PubMed MedLine database. Thus,
automatic knowledge discovery and efficient information access are strongly de-
manded to curate domain databases, to find out relevant information, and to
integrate/update new information across an increasingly large body of scien-
tific articles. In particular, since most biomedical texts introduce specific no-
tations, acronyms, and innovative names to represent new concepts, relations,
processes, functions, locations, and events, automatic extraction of biomedical
terminologies and mining of their diverse usage are major challenges in biomed-
ical information processing system. In these processes, biomedical named entity
recognition (NER) is the core step to access the higher level of information.
In fact, there has been a wide range of research on NER like the NER task on
the standard newswire domain in the Message Understanding Conference (MUC-
6). In this task, the best system reported 95% accuracy in identifying seven types
of named entities (person, organization, location, time, date, money, and per-
cent). While the performance in the standard domain turned out to be quite good
as shown in the papers, that in the biomedical domain is not still satisfactory,
which is mainly due to the following characteristics of biomedical terminologies:
First, NEs have various naming conventions. For instance, some entities have
descriptive and expanded forms such as ?activated B cell lines, 47 kDa sterol
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 646?657, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Two-Phase Biomedical Named Entity Recognition 647
regulatory element binding factor?, whereas some entities appear in shortened
or abbreviated forms like ?EGFR? and ?EGF receptor? representing epidermal
growth factor receptor. Second, biomedical NEs have the widespread ambiguity
out of context. For instance, ?IL-2? can be doubly classified as ?protein? and
?DNA? according to its context. Third, biomedical NEs often comprise a nested
structure, for example ??DNA??protein?TNF alpha?/protein?gene?/DNA??. Ac-
cording to [13], 16.57% of biomedical terms in GENIA have cascaded construc-
tions. In the case, recognition of the longest terms is the main target in general.
However, in our evaluation task, when the embedded part of a term is regarded
as the meaningful or important class in the context, the term is labeled only with
the class of embedded one. Thus, identification of internal structures of NEs is
helpful to recognize correct NEs. In addition, more than one NE often share the
same head noun with a conjunction/disjunction or enumeration structure, for
instance, ?IFN-gamma and GM-CSF mRNA?, ?CD33+, CD56+, CD16- acute
leukemia?or ?antigen- or cAMP-activated Th2 cell?. Last, there is a lot of inter-
annotator disagreement. [7] reported that the inter-annotator agreement rate of
human experts was just 77.6% when performing gene/protein/mRNA classifica-
tion task manually.
Thus, a lot of term occurrences in real text would not be identified with sim-
ple dictionary look-up, despite the availability of many terminological databases,
as claimed in [12]. That is one of the reasons why machine learning approaches
are more dominant in biomedical NER than rule-based or dictionary-based ap-
proaches [5], even though existence of reliable training resources is very critical.
Accordingly, much work has been done on biomedical NER, based on ma-
chine learning techniques. [3] and [13] have used hidden Markov Model (HMM)
for biomedical NER where state transitions are made by semantic trigger fea-
tures. [4] and [11] have applied maximum entropy plus Markovian sequence based
models such as maximum entropy markov model (MEMM) and conditional ran-
dom fields (CRFs), which present a way for integrating different features such
as internal word spellings and morphological clues within an NE string and con-
textual clues surrounding the string in the sentence.
These works took an one-phase based approach where boundary detection
of named entities and semantic labeling come together. On the other hand, [9]
proposed a two-phase model in which the biomedical named entity recognition
process is divided into two processes of distinguishing biomedical named entities
from general terms and labeling the named entities with semantic classes that
they belong to. They use support vector machines (SVM) for each phase. How-
ever, the SVM does not provide an easy way for labeling Markov sequence data
like B following O and I following B in named entities. Furthermore, since this
system is tested on the GENIA corpus rather than JNLPBA 2004 shared task, we
cannot confirm the effectiveness of this approach on the ground of experiments
for common resources.
In this paper, we present a two-phase named entity recognition model: (1)
boundary detection for NEs and (2) term classification by semantic labeling.
The advantage of dividing the recognition process into two phase is that we can
648 S. Kim et al
select separately a discriminative feature set for each subtask, and moreover can
measure effectiveness of models at each phase. We use two exponential models
for this work, namely conditional random fields for boundary detection having
Markov sequence, and the maximum entropy model for semantic labeling. In ad-
dition, results from the machine learning based model are refined by a rule-based
postprocessing, which is implemented using a finite state transducer (FST). The
FST is constructed with the GENIA 3.02 corpus. We here focus on identification
of five classes of NEs, i.e. ?protein?, ?RNA?, ?DNA?, ?cell line?, and ?cell type?
and experiments are conducted on the training and evaluation set provided by
the shared task in COLING 2004 JNLPBA.
2 Training
2.1 Maximum Entropy and Conditional Random Fields
Before we describe the features used in our model, we briefly introduce the ME
and CRF model which we make use of. In the ME framework, the conditional
probability of predicting an outcome o given a history h is defined as follows:
p?(o|h) =
1
Z?(h)
exp
(
k
?
i=1
?ifi(h, o)
)
(1)
where fi(h, o) is a binary-valued feature function, ?i is the weighting parameter
of fi(h, o), k is the number of features, and Z?(h) is a normalization factor
for ?op?(o|h)=1. That is, the probability p?(o|h) is calculated by the weighted
sum of active features. Given an exponential model with k features and a set
of training data, empirical distribution, weights of the k features are trained to
maximize the model?s log-likelihood:
L(p) =
?
o,h
p?(h, o)log(o|h) (2)
Although the maximum entropy model above provides a powerful tool for
classification by integrating different features, it is not easy to model the Markov
sequence data. In this case, the CRF is used for a task of assigning label sequences
to a set of observation sequences. Based on the principle of maximum entropy,
a CRF has a single exponential model for the joint probability of the entire
sequence of labels given the observation sequence. The CRF is a special case of
the linear chain that corresponds to conditionally trained finite-state machine
and define conditional probability distributions of a particular label sequence s
given observation sequence o
p?(s|o) = 1Z(o)exp(
?k
j=1 ?jFj(s,o))
Fj(s,o) =
?n
i=1 fj(si?1, si,o, i)
(3)
Two-Phase Biomedical Named Entity Recognition 649
where s = s1 . . . sn, and o = o1 . . . on, Z(o) is a normalization factor, and each
feature is a transition function [8]. For example, we can think of the following
feature function.
fj(si?1, si,o, i) =
?
?
?
1 if si?1=B and si=I,
and the observation word at position i is ?gene??
0 otherwise
(4)
Our CRFs for term boundary detection have a first-order Markov dependency
between output tags. The label at position i, si is one of B, I and O. In contrast
to the ME model, since B is the beginning of a term, the transition from O to I
is not possible. CRFs constrain results to consider only reasonable paths. Thus,
total 8 combinations are possible for (si?1,si) and the most likely s can be found
with the Viterbi algorithm. The weights are set to maximize the conditional log
likelihood of labeled sequences in the training set using a quasi-Newton method
called L-BFGS [2].
2.2 Features for Term Boundary Detection
Table 1 shows features for the step of finding the boundary of biomedical terms.
Here, we give a supplementary description of a part of the features.
Table 1. Feature set for boundary detection (+:conjunction)
Model Feature Description
CRF, MEmarkov Word wi?1, wi?2, wi, wi+1, wi+2
CRF, MEmarkov Word Normalization normalization forms of the 5 words
CRF, MEmarkov POS POSwi?1 , POSwi , POSwi+1
CRF, MEmarkov Word Construction form WFwi
CRF, MEmarkov Word Characteristics WCwi?1 , WCwi , WCwi+1
CRF, MEmarkov Contextual Bigrams wi?1 + wi
wi + wi+1
wi+1 + wi+2
CRF, MEmarkov Contextual Trigrams wi?1 + wi + wi+1
CRF, MEmarkov Bigram POS POSwi?1 + POSwi
POSwi + POSwi+1
CRF, MEmarkov Trigram POS POSwi?1 + POSwi + POSwi+1
CRF, MEmarkov Modifier MODI(wi)
CRF, MEmarkov Header HEAD(wi)
CRF, MEmarkov SUFFIX SUFFIX(wi)
CRF, MEmarkov Chunk Type CTypewi
CRF, MEmarkov Chunk Type + Pre POS CTypewi + POSwi?1
MEmarkov Pre label labelwi?1
MEmarkov Pre label + Cur Word labelwi?1 + wi
? word and POS: 5 words(target word(wi), left two words, and right two
words) and three POS(POSwi?1 , POSwi , POSwi+1) are considered.
650 S. Kim et al
? word normalization: This feature contributes to word normalization. We
attempt to reduce a word to its stem or root form with a simple algorithm
which has rules for words containing plural, hyphen, and alphanumeric let-
ters. Specifically, the following patterns are considered.
(1) ?lymphocytes?, ?cells? ? ?lymphocyte?, ?cell?
(2) ?il-2?, ?il-2a?, ?il2a? ? ?il?
(3) ?5-lipoxygenase?, ?v-Abl? ? ?lipoxygenase?, ?abl?
(4) ?peri-kappa?or ?t-cell? has two normalization forms of ?peri?and?kappa?
and ?t? and ?cell? respectively.
(5) ?Ca2+-independent? has two roots of ?ca? and ?independent?.
(6) The root of digits is ?D?.
? informative suffix: This feature appears if a target word has a salient suffix
for boundary detection. The list of salient suffixes is obtained by relative
entropy [10].
? word construction form: This feature indicates how a target word is or-
thographically constructed. Word shapes refer to a mapping of each word
on equivalence classes that encodes with dashes, numerals, capitalizations,
lower letters, symbols, and so on. All spellings are represented with combina-
tions of the attributes1. For instance, the word construction form of ?IL-2?
would become ?IDASH-ALPNUM?.
? word characteristics: This feature appears if a word represents a DNA
sequence of ?A?,?C?,?G?,?T? or Greek letter such as beta or alpha, ordinal
index such as I, II or unit such as BU/ml, micron/mL. It is encoded with
?ACGT?, ?GREEK?, ?INDEX?, ?UNIT?.
? head/modifying information: If a word prefers the rightmost position
of terminologies, we regard it has the property of a head noun. On the
other hand, if a word frequently occurs in other positions, we regard it has
the property of a modifying noun. It can help to establish the beginning
and ending point of multi-word entities. We automatically extract 4,382
head nouns and 7,072 modifying nouns from the training data as shown in
Table 2.
? chunk-type information: This feature is also effective in determining the
position of a word in NEs, ?B?, ?I?, ?O? which means ?begin chunk?, ?in
chunk? and ?others?, respectively. We consider the chunk type of a target
word and the conjunction of the current chunk type and the POS of the
previous word to represent the structure of an NE.
We also tested an ME-based model for boundary detection. For this, we add
two special features : previous state (label) and conjunction of previous label
1 ?IDASH? (inter dash), ?EDASH? (end dash), ?SDASH? (start dash),
?CAP?(capitalization), ?LOW?(lowercase), ?MIX?(lowercase and capitaliza-
tion letters), ?NUM?(digit), ?ALPNUM?(alpha-numeric), ?SYM?(symbol),
?PUNC?(punctuation),and ?COMMA?(comma)
Two-Phase Biomedical Named Entity Recognition 651
Table 2. Examples of Head/Modifying Nouns
Modifying Nouns Head Nouns
nf-kappa cytokines
nuclear elements
activated assays
normal complexes
phorbol macrophages
viral molecules
inflammatory pathways
murine extracts
electrophoretic glucocorticoids
acute levels
intracellular responses
epstein-barr clones
cytoplasmic motifs
and current word to consider state transition. That is, a previous label can be
represented as a feature function in our model as follows:
fi(h, o) =
{
1 if pre label+tw=B+gene,o=I
0 otherwise
(5)
It means that the target word is likely to be inside a term (I), when the word
is ?gene? and the previous label is ?B?. In our model, the current label is de-
terministically assigned to the target word with considering the previous state
with the highest probability.
2.3 Features for Semantic Labeling
Table 3 shows features for semantic labeling with respect to recognized NEs.
? word contextual feature: We make use of three kinds of internal and ex-
ternal contextual features: words within identified NEs, their word normal-
ization forms, and words surrounding the NEs. In Table 3, NEw0 denotes
the rightmost word in an identified NE region. Moreover, the presence of
specific head nouns acting as functional words takes precedence when de-
termining the term class, even though many terms do not contain explicit
term category information. For example, functional words, such as ?factor?,
?receptor?, and ?protein? are very useful in determining protein class, and
?gene?, ?promoter?, and ?motif ? are clues for classifying DNA [5]. In gen-
eral, such functional words are often the last word of an entity. This is the
reason we consider the position where a word occurs in NEs along with the
word. For inside context features, we use non-positional word features as
well. As non-positional features, all words inside NEs are used.
? internal bigrams and trigrams: We consider the rightmost bigrams/
trigrams inside identified NEs and the normalized bigrams/trigrams.
652 S. Kim et al
Table 3. Feature Set for Semantic Classification
Feature description
Word Features (positional) NEwothers , NEw?3 , NEw?2 , NEw?1 , NEw0
Word Features (non-positional) AllNEw
Word Normalization (positional) WFNEw?3 , WFNEw?2 , WFNEw?1 , WFNEw0
Left Context(Words Surrounding an NE) LCW?2, LCW?1
Right Context RCW+1, RCW+2
Internal Bigrams NEw?1 + NEw0
Internal Trigrams NEw?2 + NEw?1 + NEw0
Normalized Internal Bigrams WFNEw?1 + WFNEw0
Normalized Internal Trigrams NEw?2 + NEw?1 + NEw0
IDASH-word related Bigrams/Trigrams
Keyword KEYWORD(NEi)
? IDASH-word related bigrams/trigrams: This feature appears if NEw0
or NEw?1 contains dash characters. In this case, the bigram/trigram are
additionally formed by removing all dashes from the spelling. It is useful to
deal with lexical variants.
? keywords: This feature appears if the identified NE is informative key-
word with respect to a specific class. The keywords set comprises terms
obtained by the relative entropy between general and biomedical domain
corpora.
3 Rule-Based Postprocessing
A rule-based method can be used to correct errors by NER based on machine
learning. For example, the CRFs tag ?IL-2 receptor expression? as ?B I I?,
since the NEs ended with ?receptor expression? in training data almost belong
to ?other name? class even if the NEs ended with ?receptor? belong to ?pro-
tein? class. It should be actually tagged as ?B I O?. That kind of errors is
caused mainly by the cascaded phenomenon in biomedical names. Since our sys-
tem considers all NEs belonging to other classes in the recognition phase, it
tends to recognize the longest ones. That is, in the term classification phase,
such NEs are classified as ?other? class and are ignored. Thus, the system
losts embedded NEs although the training and evaluation set in fact tends to
consider only the embedded NE when the embedded one is more meaningful
or important.
This error correction is conducted by the rule-based method, i.e. If condi-
tion THEN action. For example, the rule ?IF wi?2=IL-2, wi?1=receptor and
wi=expression THEN replace the tag of wi with O? can be applied for the above
case. We use a finite state transducer for this rule-based transformation, which
is easy to understand with given lexical rules, and very efficient. Rules used for
the FST are acquired from the GENIA corpus. We first retrieved all NEs in-
cluding embedded NEs and longest NEs from GENIA 3.02 corpus and change
Two-Phase Biomedical Named Entity Recognition 653
IL-2/B gene/I
IL-2/O gene/O expression/O
Fig. 1. Non-Deterministic FST
IL-2/? gene/ ?
expression/OOO
? /BI
Fig. 2. Deterministic FST
the outputs of all other classes except the target 5 classes to O. That is, the
input of FST is a sequence of words in a sentence and the output is categories
corresponding to the words.
Then, we removed the rules in conflict with NE information from the training
corpus. These rules are non-deterministic (Figure 1), and we can change it to
the deterministic FST (Figure 2) since the lengths of NEs are finite. The deter-
ministic FST is made by defining the final output function for the deterministic
behavior of the transducer, delaying the output. The deterministic FST is de-
fined as follows: (?1, ?2, Q, i, F, ?, ?, ?), where ?1 is a finite input alphabet; ?2
is a finite output alphabet; Q is a finite set of states or vertices; i ? Q is the
initial state; F ? Q is the set of final states; ? is the deterministic state transi-
tion function that maps Q ? ?1 on Q; ? is the deterministic emission function
that maps Q ? ?1 on ??2 and ? : F ? ??2 is the final output function for the
deterministic behavior of the transducer.
4 Evaluation
4.1 Experimental Environments
In the shared task, only biomedical named entities which belong to 5 specific
classes are annotated in the given training data. That is, terms belonging to
other classes in GENIA are excluded from the recognition target. However, we
consider all NEs in the boundary detection step since we separate the NER
task into two phases. Thus, in order to utilize other class terms, we additionally
annotated ?O? class words in the training data where they corresponds to other
classes such as other organic compound, lipid, and multi cell in GENIA 3.02p
version corpus. During the annotation, we only consider the longest NEs on
654 S. Kim et al
Table 4. Number of training examples
RNA DNA cell line cell type protein other
472 5,370 2,236 2,084 16,042 11,475
GENIA. As a consequence, we find all biomedical named entities in text at the
term detection phase. Then, biomedical NEs classified as other class are changed
to O at the semantic labeling phase. The total words that belong to other class
turned out to be 25,987. Table 4 shows the number of NEs with respect to each
class on the training data. In our experiments, a quasi-Newton method called the
L-BFGS with Gaussian Prior smoothing is applied for parameter estimation [2].
4.2 Experimental Results
Table 5 shows the overall performance on the evaluation data. Our system
achieves an F-score of 71.19%. As shown in the table, the performance of NER
for cell line class was not good, because its boundary recognition is not so good
as other classes. Also, Table 6 shows the results of semantic classification. In par-
ticular, the system often confuses protein with DNA, and cell line with cell type.
Among the correctly identified 7,093 terms, 790 terms were misclassified.
Table 7 shows the performance of each phase. Our system obtains 76.88%
F-score in the boundary detection task and, using 100% correctly recognized
terms from annotated test data, 90.54% F-score in the semantic classification
task. Currently, since we cannot directly assess the accuracy of the term detection
process on the evaluation set because of other class words, the 75% of the training
data were used for training and the rest for testing.
Table 5. Overall performance on the evaluation data
Fully Correct Left Correct Right Correct
Class Recall Precision F-score F-score F-score
protein 76.30 69.71 72.85 77.60 79.15
DNA 67.80 64.91 66.33 68.36 74.57
RNA 73.73 63.04 67.97 71.09 74.22
cell line 57.40 54.88 56.11 59.04 65.69
cell type 70.12 77.64 73.69 74.89 81.51
overall 72.77 69.68 71.19 74.75 78.23
Table 6. Confusion matrix over evaluation data
gold/sys protein DNA RNA cell line cell type other
protein 0 72 3 1 4 267
DNA 97 0 0 0 0 49
RNA 11 0 0 0 0 0
cell line 10 1 0 0 63 37
cell type 21 0 0 92 0 57
Two-Phase Biomedical Named Entity Recognition 655
Table 7. Performance of term detection and semantic classification
Recall Precision F-score
term detection (MEMarkov) 74.03 75.31 74.67
term detection (CRF) 76.14 77.64 76.88
semantic classification 87.50 93.81 90.54
overall NER 72.77 69.68 71.19
Table 8. Performance of NE recognition methods (one-phase vs. two-phase)
method Recall Precision F-score
one-phase 64.23 63.13 63.68
two-phase(baseline2) 66.24 64.54 65.38
(only 5 classes)
two-phase(baseline2) 68.51 67.58 68.04
(5 classes+other class)
Also, we compared our model with the one-phase model. The detailed results
are presented in Table 8. Both of them have pros and cons. The best-reported
system presented by [13] uses one-phase strategy. In our evaluation, the two-
phase method shows a better result than the one-phase method, although direct
comparison is not possible since we tested with a maximum entropy based expo-
nential models in all cases. The features for one-phase method are identical with
the recognition features except that the local context of a word is extended as
previous 4 words and next 4 words. In addition, we investigate whether the con-
sideration of ?other? class words is helpful in the recognition performance. Table
8 shows explicit annotations of other NE classes much improve the performance
of existing entity types.
In the next experiment, we test how individual methods have an effect on the
performance in the term detection step. Table 9 shows the results obtained by com-
bining different methods in the NER process. At the semantic labeling phase, all
methods employed the ME model using the features described in 2.3. Baseline1
is the two-phase ME model which restrict the inspection of NE candidates to the
NPs which include at least one biomedical salient word. Baseline2 is the two-phase
ME model considering all words. In order to retrieve domain salient words, we
utilized a relative frequency ratio of word distribution in the domain corpus and
that in the general corpus [10]. We used the Penn II raw corpus as out-of-domain
corpus. Both models do not use the features related to previous labels. As a re-
sult, usage of salient words decrease the performance and it only speeds up the
training process. Baseline2+FST indicates boundary extension/contraction using
FST are applied as postprocessing step in baseline2 recognition. In addition, we
compared use of CRFs and ME with Markov process features. For this, we added
features of previous labels to the feature set for ME. Baseline2+MEMarkov is the
two-phase ME model considering all features including previous label related fea-
tures. Baseline2+CRF is a model exploiting CRFs and baseline2+CRF+FST is a
model using CRFand FST as postprocessing.As shown in Table 9, the CRFs based
656 S. Kim et al
Table 9. F-score for different methods
Method Recall Precision F-score
baseline1(salientNP ) 66.21 66.34 66.27
baseline2(all) 68.51 67.58 68.04
baseline2 + FST 68.89 68.53 68.71
baseline2 + MEMarkov 70.30 67.65 68.95
baseline2 + MEMarkov + FST 70.61 68.40 69.49
baseline2 + CRF 72.44 68.77 70.56
baseline2 + CRF + FST 72.77 69.68 71.19
Table 10. Comparisons with other systems
System Precision Recall F-score
Zhou et. al (2004) 69.42 75.99 72.55
Our system 72.77 69.68 71.19
Finkel et. al (2004) 71.62 68.56 70.06
Settles (2004) 70.0 69.0 69.5
model outperforms the ME based model. Our system reached F-score 71.19% on
the baseline2 + CRF + FST model.
Table 10 shows the comparison with top-ranked systems in JNLPBA 2004
shared task. The top-ranked systems made use of external knowledge from
gazetteers and abbreviation handling routines, which were reported to be ef-
fective. Zhou et. al reported the usage of gazetteers and abbreviation handling
improves the performance of the NER system by 4.8% in F-score [13]. Finkel
et. al made use of a number of external resources, including gazetteers, web-
querying, use of the surrounding abstract, abbreviation handling, and frequency
counts from BNC corpus [4]. Settles utilized semantic domain knowledge of 17
kinds of lexicons [11]. Although the performance of our system is a bit lower than
the best system, the results are very promising since most systems use external
gazetteers, and abbreviation and conjunction/disjunction handling scheme. This
suggests areas for further work.
5 Conclusion and Discussion
We presented a two-phase biomedical NE recognition model, term boundary
detection and semantic labeling. We proposed two exponential models for each
phase. That is, CRFs are used for term detection phase including Markov process
and ME is used for semantic labeling. The benefit of dividing the whole process
into two processes is that, by separating the processes with different characteris-
tics, we can select separately the discriminative feature set for each subtask, and
moreover measure effectiveness of models at each phase. Furthermore, we use
the rule-based method as postprocessing to refine the result. The rules are ex-
tracted from the GENIA corpus, which is represented by the deterministic FST.
The rule-based approach is effective to correct errors by cascading structures
Two-Phase Biomedical Named Entity Recognition 657
of biomedical NEs. The experimental results are quite promising. The system
achieved 71.19% F-score without Gazetteers or abbreviation handling process.
The performance could be improved by utilizing lexical database and testing
various classification models.
Acknowledgements
This work was supported by Korea Research Foundation Grant, KRF-2004-037-
D00017.
References
1. Thorten Brants. TnT A Statistical Part-of-Speech Tagger. In Proceedings of the
6th Applied Natural Language Processing.; 2000.
2. Stanley F. Chen and Ronald Rosenfeld. A Gaussian prior for smoothing maximum
entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.
3. Nigel Collier, Chikashi Nobata and Jun-ichi Tsujii. Extracting the Names of Genes
and Gene Products with a Hidden Markov Model. In Proceedings of COLING 2000;
201-207.
4. Jenny Finkel, Shipra Dingare, and Huy Nguyen. Exploiting Context for Biomedical
Entity Recognition From Syntax to thw Web. In Proceedings of JNLPBA/BioNLP
2004; 88-91.
5. K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. Toward information extrac-
tion: identifying protein names from biological papers. In Proceedins of the Pacific
Symposium on Biocomputing 98; 707-718.
6. Junichi Kazama, Takaki Makino, Yoshihiro Ohta and Junichi Tsujii. Tuning Sup-
port Vector Machines for Biomedical Named Entity Recognition, Proceedings of the
ACL Workshop on Natural Language Processing in the Biomedical Domain 2002;
1-8.
7. Michael Krauthammer and Goran Nenadic. Term Identification in the Biomedical
literature. Journal of Biomedical Informatics. 2004; 37(6):512-526.
8. John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional Random
Fields: probabilistic models for segmenting and labeling sequence data. In Proceed-
ings of ICML-01; 282-289.
9. Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, Hae-Chang Rim. Biomedical
named entity recognition using two-phase model based on SVMs. Journal of
Biomedical Informatics 2004; 37(6):436-447.
10. Kyung-Mi Park, Seonho Kim, Ki-Joong Lee, Do-Gil Lee, and Hae-Chang Rim.
Incorportating Lexical Knowledge into Biomedical NE Recognition. In Proceedings
of Natural Language Processing in Biomedicine and its Applications Post-COLING
Workshop 2004; 76-79.
11. Burr Settles. Biomedical Named Entity Recognition Using Conditional Random
Fields and Rich Feature Sets. In Proceedings of JNLPBA/BioNLP 2004; 104-107.
12. Olivia Tuason, Lifeng Chen, Hongfang Liu, Judith A. Blake, Carol Friedman. Bi-
ological Nomenclatures: A Source of Lexical Knowledge and Ambiguity. In Pacific
Symposium on Biocomputing 2004; 238-249.
13. GuoDong Zhou, Jie Zhang, Jian Su, Chew-Lim Tan. Exploring Deep Knowledge
Resources in Biomedical Name Recognition. In Proceedings of JNLPBA/BioNLP
2004; 99-102.
76
77
78
79
Two-Phase Semantic Role Labeling based on Support Vector Machines
Kyung-Mi Park and Young-Sook Hwang and Hae-Chang Rim
Department of Computer Science & Engineering, Korea University
5-ka, Anam-dong, SEOUL, 136-701, KOREA
{kmpark, yshwang, rim}@nlp.korea.ac.kr
Abstract
In this study, we try to apply SVMs to the se-
mantic role labeling task, which is one of the
multiclass problems. As a result, we propose a
two-phase semantic role labeling model which
consists of the identification phase and the clas-
sification phase. We first identify semantic ar-
guments, and then assign semantic roles to the
identified semantic arguments. By taking the
two-phase approach, we can alleviate the un-
balanced class distribution problem, and select
the features appropriate for each task.
1 Introduction
A semantic role in a language is a semantic relation-
ship between a syntactic constituent and a predicate. The
shared task of CoNLL-2004 relates to recognize seman-
tic roles in English (X. Carreras, 2004). Given a sentence,
the task is to analyze a proposition expressed by a target
verb of a sentence. Especially, for each target verb, all
constituents in a sentence which fill semantic roles of the
verb have to be recognized. This task is based only on
partial parsing information, avoiding use of a full parser
and external lexico-semantic knowledge base. According
to previous results of the CoNLL shared task, the POS
tagged, chunked, clause identified, and named-entity rec-
ognized sentences are given as an input (Figure 1).
SVM is a well-known machine learning algorithm with
high generalization performance in high dimensional fea-
ture spaces (H. Yamada, 2003). Also, learning with com-
bination of multiple features is possible by virtue of poly-
nomial kernel functions. However, since it is a binary
classifier, we are often confronted with the unbalanced
class distribution problem in a multiclass classification
task. The larger the number of classes, the more severe
the problem is. The semantic role labeling can be formu-
lated as a multiclass classification problem. If we try to
apply SVMs in the semantic role labeling problem, we
have to find a method of resolving the unbalanced class
distribution problem.
Conceptually, semantic role labeling can be divided
into two subtasks: the identification task which finds
the boundary of semantic arguments in a given sentence,
and the classificiation task which determines the seman-
tic role of the argument. This provides us a hint of using
SVMs with less severe unbalanced class distribution. In
this paper, we present a two-phase semantic role label-
ing method which consists of an identification phase and
a classification phase. By taking two phase model based
on SVMs, we can alleviate the unbalanced class distri-
bution problem. That is, since we find only the bound-
ary of an argument in the identification phase, the num-
ber of classes is decreased into two (ARG, NON-ARG)
or three (B-ARG, I-ARG, O). Therefore, we have to build
only one or three SVM classifiers. We can alleviate the
unbalanced class distribution problem by decreasing the
number of negative examples, which is much larger than
the number of positive exampels without two-phase mod-
eling. In the classification phase, we classify only the
identified argument into a proper semantic role. This en-
ables us to reduce the computational cost by ignoring the
non-argument constitutents.
Since features for identifying arguments are different
from features for classifying a role, we need to determine
different feature sets appropriate for the tasks. For iden-
tification, we focus on the features to detect the depen-
dency between a constituent and a predicate because the
arguments are dependent on the predicate. For seman-
tic role labeling, we consider both the syntactic and the
semantic information such as the sentential form of the
target predicate, the head of a constituent, and so on. In
the following sections, we will explain the two phase se-
mantic role labeling method in detail and show some ex-
perimental results.
Figure 1: An example of semantic role labeling. The columns contain: the word, its POS, its chunk type, clause
boundary, its named-entity tag, the target predicate, the result of semantic role labeling in the target predicate exist,
and deliver.
2 Two Phase Semantic Role Labeling
based on SVMs
We regard the semantic role labeling as a classification
problem of a syntactic constituent. However, a syntac-
tic constituent can be a chunk, or a clause. Therefore, we
have to identify the boundaries of semantic arguments be-
fore we assign roles to the arguments.
2.1 Semantic Argument Identification
This phase is the step of finding the boundary of seman-
tic arguments. A sequence of chunks or a subclause in
the immediate clause of a predicate can be a semantic ar-
gument of the predicate. A chunk or a subclause of the
predicate becomes a unit of the constituent of an argu-
ment. The chunks within the subclause are ignored.
For identifying the semantic arguments of a target
predicate, it is necessary to find the dependency rela-
tion between each constituent and a predicate. Identify-
ing a dependency relation is important for identifying a
subject/object relation (S. Buchholz, 2002) and also for
identifying the semantic arguments of a target predicate.
Therefore, the features for finding dependency relations
are implicitly represented in the feature set for the identi-
fication task.
For implementing the method based on the SVMs, we
represent a constituent of an argument with B/I/O nota-
tion, and assign one of the following classes to each con-
stituent: B-ARG class representing the beginning of se-
mantic argument, I-ARG class representing a part of a
semantic argument, or O class indicating that the con-
stituent does not belong to the semantic arguments.
Because we decide the unit of a constituent as a chunk
or a subclause, words except the predicate in the target
phrase 1 do not belong to constituent. Therefore, these
words have to be handled independently. In the training
data, we often observed that the beginning of semantic
arguments starts from the word right after the predicate.
For the agreement with the chunk boundary, we regard
the word following a predicate as the beginning word of
a new chunk. Namely, when the beginning of chunk tag
is I, we change I to B. Also, the words located in front of
the predicate in the target phrase are post-processed by 4
hand-crafted rules 2 and 211 automated rules 3 based on
frequency in the training data.
In order to restrict the search space in terms of the con-
stituents, we use the clause boundaries. The left search
boundary for identifying the semantic argument is set to
the left boundary of the second upper clause, and the right
search boundary is set to the right boundary of the imme-
diate clause.
2.1.1 Features for Identifying Semantic Argument
For this phase, we use 29 features for representing syn-
tactic and semantic information related to constituent and
predicate. Table1 shows a set of features employed. The
features can be described as follows:
? position: This is a binary feature identifying
whether the constituent is before (-1) or after (1) the
predicate in the immediate clause. The feature value
1The chunk containing a predicate is referred to as target
phrase.
2For example, if a word in target phrase is n?t, not or Not,
and POS tag of the word is RB and the distance between the
word and the predicate is less than 4, then the semantic role is
AM-NEG.
3For example, if a word in target phrase is already, and POS
tag of the word is RB, then the semantic role is AM-TMP.
Features examples
position -2, -1, 1
distance 0, 1, 2, . . .
predicate-candidate # of VP, NP, SBAR 0, 1, 2, . . .
(intervening features) # of POS [CC], [,], [:] 0, 1, 2, . . .
POS [?] & POS [?] -1, 0, 1
path VP-PP-NP, . . .
headword, headword?s POS, chunk type
predicate itself & context beginning word?s POS MD, TO, VBZ, . . .
context-1: headword, headword?s POS, chunk type
headword, headword?s POS, chunk type
candidate itself & context context-2: headword, headword?s POS, chunk type
context-1: headword, headword?s POS, chunk type
context+1: headword, headword?s POS, chunk type
Table 1: Features for Identifying a semantic argument
Figure 2: Two-phase semantic role labeling procedure using the example sentence presented in Figure 1. (P means the
target phrase containing the predicate deliver, and C means the constituent such as a chunk (e.g. Under) or a subclause
(e.g. Rockwell said))
(-2) means that the constituent is out of the immedi-
ate clause.
? distance: The distance is measured by the number
of chunks between the predicate and the constituent.
? # of VP, NP, SBAR: These are numeric features rep-
resenting the number of the specific chunk types be-
tween the predicate and the constituent.
? # of POS [CC], [,], [:]: These are numeric features
representing the number of the specific POS types
between the predicate and the constituent.
? POS [?] & POS [?]: This is used as a feature rep-
resenting the difference between # of POS[?] and #
of POS[?] counted in the range from the predicate
to the constituent. In Table 1, the feature value (-1)
means that # of POS[?] is larger than # of POS[?].
The feature value (1) conversly means that # of
POS[?] is larger than # of POS[?]. The featue value
(0) means that # of POS[?] is equal to # of POS[?].
? path: This is the syntactic path from the predicate to
the constituent, and is a symbolic feature comprising
all the elements (chunk or subclause) between the
predicate and the constituent.
? beginning word?s POS: In the target phrase, these
values appear only with VPs and represent the POS
of the syntactic head (MD, TO, VB, VBD, VBG,
VBN, VBP, VBZ). This represents the property of the
target phrase, for example, the feature value TO in-
dicates that the target phrase is to-infinitive.
? context: These are information for the predicate it-
self, the left context of the predicate, the constituent
itself, and the left and right context of the con-
stituent. In Table 1, - means the left context, and +
means the right context. In case that the constituent
is the subclause, the chunk type of the constituent is
set to the first chunk type of the subclause.
2.2 Semantic Role Assignment
In this phase, we assign appropriate semantic roles to the
identified semantic arguments. For learning SVM classi-
fiers, we consider not all semantic roles, but only 18 se-
mantic roles based on frequency in the training data (Ta-
ble 2). The (AM-MOD, AM-NEG) are post-processed by
hand-crafted rules. As we decrease the number of SVM
classifiers to be learned in the training data, the training
cost of classifiers can be reduced. Furthermore, we can
alleviate the unbalanced class distribution problem by ex-
semantic role
A0, A1, A2, A3, A4, R-A0, R-A1, R-A2, C-A1
AM-TMP, AM-ADV, AM-MNR, AM-LOC, AM-DIS
AM-PNC, AM-CAU, AM-DIR, AM-EXT
Table 2: 18 semantic roles
cluding the infrequent classes.
2.2.1 Features for Assigning Semantic Role
This phase also uses all features applied in the seman-
tic argument identification phase, except for # of POS [:]
and POS[?] & POS[?]. In addition, we use the following
feature.
? voice: This is a binary feature identifying whether
the target phrase is active or passive.
In Figure 2, we show two-phase semantic role labeling
procedure using the example sentence in Figure 1.
3 Experiments
For experiments, we utilized the SVM light package (T.
Joachims, 2002). In both the semantic argument identifi-
cation and the semantic role assignment phase, we used a
polynomial kernel (degree 2) with the one-vs-rest classi-
fication method. Table 3 shows the experimental results
on the test set and Table 4 shows the experimental results
on the development set. Table 4 also shows the perfor-
mance of each phase.
For improving the performance, we try to select the
discrminative features for each subtask. Especially, since
the performance of the identification phase is critical
to the total performance, we concentrate on improving
the identification performance. Our system obtains a F-
measure of 74.08 in the identification phase, as present-
eded in Table 4. For the argument classification task, the
our system obtains a classification accuracy (A) of 85.45.
4 Conclusion
In this paper, we present a method of two phase seman-
tic role labeling based on the support vector machines.
We found that SVM is useful to incorporate the hetero-
geneous features for the semantic role labeling. Also, by
applying the two phase model, we can alleviate the unbal-
anced class distribution problem caused by the the nega-
tive examples. Experimental results show that our system
obtains a F-measure of 63.99 on the test set and 65.78 on
the development set.
Precision Recall F?=1
Overall 65.63% 62.43% 63.99
A0 78.24% 74.60% 76.38
A1 65.83% 66.46% 66.14
A2 49.84% 43.70% 46.57
A3 56.04% 34.00% 42.32
A4 62.86% 44.00% 51.76
A5 0.00% 0.00% 0.00
AM-ADV 45.18% 44.30% 44.74
AM-CAU 36.67% 22.45% 27.85
AM-DIR 20.00% 20.00% 20.00
AM-DIS 56.62% 58.22% 57.41
AM-EXT 61.54% 57.14% 59.26
AM-LOC 26.01% 31.14% 28.34
AM-MNR 43.54% 35.69% 39.22
AM-MOD 97.46% 91.10% 94.17
AM-NEG 94.92% 88.19% 91.43
AM-PNC 40.00% 28.24% 33.10
AM-PRD 0.00% 0.00% 0.00
AM-TMP 51.83% 45.38% 48.39
R-A0 80.49% 83.02% 81.73
R-A1 75.00% 51.43% 61.02
R-A2 100.00% 33.33% 50.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 96.66% 96.66% 96.66
Table 3: Results on the test set: closed challenge
Precision Recall F?=1 A
Overall 67.27% 64.36% 65.78 -
identification 75.96% 72.30% 74.08 -
classification - - - 85.45
Table 4: Results on the development set: closed chal-
lenge. (A means accuracy.)
References
X. Carreras and L. Marquez. 2004. Introduction to the
CoNLL-2004 Shared Task: Semantic Role Labeling.
CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical
Dependency Analysis with Support Vector Machines.
IWPT03.
S. Buchholz 2002. Memory-Based Grammatical Rela-
tion Finding. PhD. thesis, Tilburg University.
T. Joachims 2002. SVM Light available at http:// svm-
light.joachims.org/.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 209?212, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Maximum Entropy based Semantic Role Labeling
Kyung-Mi Park and Hae-Chang Rim
Department of Computer Science & Engineering, Korea University
5-ka, Anam-dong, SeongBuk-gu, SEOUL, 136-701, KOREA
{kmpark, rim}@nlp.korea.ac.kr
1 Introduction
The semantic role labeling (SRL) refers to finding
the semantic relation (e.g. Agent, Patient, etc.) be-
tween a predicate and syntactic constituents in the
sentences. Especially, with the argument informa-
tion of the predicate, we can derive the predicate-
argument structures, which are useful for the appli-
cations such as automatic information extraction. As
previous work on the SRL, there have been many
machine learning approaches. (Gildea and Jurafsky,
2002; Pradhan et al, 2003; Lim et al, 2004).
In this paper, we present a two-phase SRL method
based on a maximum entropy (ME) model. We first
identify parse constituents that represent valid se-
mantic arguments of a given predicate, and then as-
sign appropriate semantic roles to the the identified
parse constituents. In the two-phase SRL method,
the performance of the argument identification phase
is very important, because the argument classifica-
tion is performed on the region identified at the iden-
tification phase. In this study, in order to improve the
performance of identification, we try to incorporate
clause boundary restriction and tree distance restric-
tion into pre-processing of the identification phase.
Since features for identifying arguments are dif-
ferent from features for classifying a role, we need
to determine different feature sets appropriate for the
tasks. We determine final feature sets for each phase
with experiments. We participate in the closed chal-
lenge of the CoNLL-2005 shared task and report re-
sults on both development and test sets. A detailed
description of the task, data and related work can be
found in Carreras and Ma`rquez (2005).
2 System Description
In this section, we describe our system that iden-
tifies and classifies semantic arguments. First, we
explain pre-processing of the identification phase.
Next, we describe features employed. Finally, we
explain classifiers used in each phase.
2.1 Pre-processing
We thought that the occurrence of most semantic
arguments are sensitive to the boundary of the im-
mediate clause or the upper clauses of a predicate.
Also, we assumed that they exist in the uniform dis-
tance on the parse tree from the predicate?s parent
node (called Pp) to the parse constituent?s parent
node (called Pc). Therefore, for identifying seman-
tic arguments, we do not need to examine all parse
constituents in a parse tree. In this study, we use
the clause boundary restriction and the tree distance
restriction, and they can provide useful information
for spotting the probable search space which include
semantic arguments.
In Figure 1 and Table 1, we show an example of
applying the tree distance restriction. We show the
distance between Pp=VP and the nonterminals of a
parse tree in Figure 1. For example, NP2:d=3 means
3 times downward movement through the parse tree
from Pp=VP to Pc=NP2. NP4 does not have the dis-
tance from Pp because we allow to move only up-
ward or only downward through the tree from Pp to
Pc. In Table 1, we indicate all 14 argument can-
didates that correspond to tree distance restriction
(d?3). Only 2 of the 14 argument candidates are
actually served to semantic arguments (NP4, PP).
209
Figure 1: Distance between Pp=VP and Pc.
distance direction Pc argument candidates
d=1 UP S NP4
d=0 - VP PP
d=1 DOWN PP IN, NP3
d=2 DOWN NP3 NP1, CONJP, NP2
d=3 DOWN NP1 JJ, NNS, NN
d=3 DOWN CONJP RB, IN
d=3 DOWN NP2 NNS, NN
Table 1: Probable argument candidates (d?3).
2.2 Features
The following features describe properties of the
verb predicate. These featues are shared by all the
parse constituents in the tree.
? pred lex: this is the predicate itself.
? pred POS: this is POS of the predicate.
? pred phr: this is the syntactic category of Pp.
? pred type: this represents the predicate usage
such as to-infinitive form, the verb predicate of
a main clause, and otherwise.
? voice: this is a binary feature identifying
whether the predicate is active or passive.
? sub cat: this is the phrase structure rule ex-
panding the predicate?s parent node in the tree.
? pt+pl: this is a conjoined feature of pred type
and pred lex. Because the maximum entropy
model assumes the independence of features,
we need to conjoin the coherent features.
The following features characterize the internal
structure of a argument candidate. These features
change with the constituent under consideration.
? head lex: this is the headword of the argument
candidate. We extracts the headword by using
the Collins?s headword rules.
? head POS: this is POS of the headword.
? head phr: this is the syntactic category of Pc.
? cont lex: this is the content word of the argu-
ment candidate. We extracts the content word
by using the head table of the chunklink.pl 1.
? cont POS: this is POS of the content word.
? gov: this is the governing category introduced
by Gildea and Jurafsky (2002).
The following features capture the relations be-
tween the verb predicate and the constituent.
? path: this is the syntactic path through the parse
tree from the parse constituent to the predicate.
? pos: this is a binary feature identifying whether
the constituent is before or after the predicate.
? pos+clau: this, conjoined with pos, indicates
whether the constituent is located in the imme-
diate clause, in the first upper clause, in the sec-
ond upper clause, or in the third upper clause.
? pos+VP, pos+NP, pos+SBAR: these are nu-
meric features representing the number of the
specific chunk types between the constituent
and the predicate.
? pos+CC, pos+comma, pos+colon, pos+quote:
these are numeric features representing the
number of the specific POS types between the
constituent and the predicate .
? pl+hl (pred lex + head lex), pl+cl (pred lex +
cont lex), v+gov (voice + gov).
2.3 Classifier
The ME classifier for the identification phase clas-
sifies each parse constituent into one of the follow-
ing classes: ARG class or NON-ARG class. The ME
classifier for the classification phase classifies the
identified argument into one of the pre-defined se-
mantic roles (e.g. A0, A1, AM-ADV, AM-CAU, etc.).
1http://pi0657.kub.nl/s?abine/chunklink/chunklink 2-2-
2000 for conll.pl
210
#exa. %can. #can. %arg. F?=1
no restriction
All1 3,709,080 - 233,394 96.06 79.37
All2 2,579,278 - 233,004 95.90 79.52
All3 1,598,726 100.00 231,120 95.13 79.92
restriction on clause boundary
1/0 1,303,596 81.54 222,238 91.47 78.97
1/1 1,370,760 85.74 223,571 92.02 79.14
2/0 1,403,630 87.80 228,891 94.21 79.66
2/1 1,470,794 92.00 230,224 94.76 79.89
3/0 1,439,755 90.06 229,548 94.48 79.63
3/1 1,506,919 94.26 230,881 95.03 79.79
restriction on tree distance
6/1 804,413 50.32 226,875 93.38 80.17
6/2 936,021 58.55 227,637 93.69 79.94
7/1 842,453 52.70 228,129 93.90 80.44
7/2 974,061 60.93 228,891 94.21 80.03
8/1 871,541 54.51 228,795 94.17 80.24
8/2 1,003,149 62.75 229,557 94.48 80.04
restriction on clause boundary & tree distance
2/1,7/1 786,951 49.22 227,523 93.65 80.12
2/1,8/1 803,040 50.23 228,081 93.88 80.11
3/1,7/1 800,740 50.09 227,947 93.82 80.28
3/1,8/1 822,225 51.43 228,599 94.09 80.06
Table 2: Different ways of reducing candidates.
3 Experiments
To test the proposed method, we have experimented
with CoNLL-2005 datasets (Wall Street sections 02-
21 as training set, Charniak? trees). The results have
been evaluated by using the srl-eval.pl script pro-
vided by the shared task organizers. For building
classifiers, we utilized the Zhang le?s MaxEnt toolkit
2
, and the L-BFGS parameter estimation algorithm
with Gaussian Prior smoothing.
Table 2 shows the different ways of reducing the
number of argument candidates. The 2nd and 3rd
columns (#can., %can.) indicate the number of ar-
gument candidates and the percentage of argument
candidates that satisfy each restriction on the train-
ing set. The 4th and 5th columns (#arg., %arg.)
indicate the number of correct arguments and the
percentage of correct arguments that satisfy each re-
striction on the training set. The last column (F?=1)
indicates the performance of the identification task
on the development set by applying each restriction.
In no restriction, All1 extracts candidates from all
the nonterminals?s child nodes of a tree. All2 fil-
ter the nonterminals which include at least one non-
2http://www.nlplab.cn/zhangle/maxent toolkit.html
Prec. Recall F?=1 Accu.
All 82.57 78.41 80.44 86.00
All-(pred lex) 82.80 77.78 80.21 84.93
All-(pred POS) 83.40 76.72 79.92 85.95
All-(pred phr) 83.11 77.57 80.24 85.87
All-(pred type) 82.76 77.91 80.26 85.99
All-(voice) 82.87 77.88 80.30 85.88
All-(sub cat) 82.48 77.68 80.00 84.88
All-(pt+pl) 83.20 77.40 80.20 85.62
All-(head lex) 82.58 77.87 80.16 85.61
All-(head POS) 82.66 77.88 80.20 85.89
All-(head phr) 83.52 76.82 80.03 85.81
All-(cont lex) 82.57 77.87 80.15 85.64
All-(cont POS) 82.65 77.92 80.22 86.09
All-(gov) 82.69 78.34 80.46 85.91
All-(path) 78.39 67.96 72.80 85.69
All-(pos) 82.70 77.74 80.14 85.85
All-(pos+clau) 82.94 78.34 80.57 86.19
All-(pos+VP) 82.69 77.87 80.20 85.87
All-(pos+NP) 82.78 77.69 80.15 85.77
All-(pos+SBAR) 82.51 78.00 80.19 85.83
All-(pos+CC) 82.84 78.10 80.40 85.70
All-(pos+comma) 82.78 77.69 80.15 85.70
All-(pos+colon) 82.67 77.96 80.25 85.72
All-(pos+quote) 82.63 77.98 80.24 85.66
All-(pl+hl) 82.62 77.71 80.09 84.98
All-(pl+cl) 82.72 77.79 80.18 85.24
All-(v+gov) 82.93 77.81 80.29 85.85
Prec. Recall F?=1 Accu.
Iden. 82.56 78.72 80.59 -
clas. - - - 87.16
Iden.+Clas. 72.68 69.16 70.87 -
Table 3: Performance of various feature combina-
tions (top) and performance of each phase (bottom).
terminal child 3. All3 filter the nonterminals which
include at least one nonterminal child and have dis-
tance from Pp. We use All3 as a baseline.
In restriction on clause boundary, for example,
2/0 means that the left search boundary for identi-
fying the argument is set to the left boundary of the
second upper clause, and the right search boundary
is set to the right boundary of the immediate clause.
In restriction on tree distance, for example, 7/1
means that it is possible to move up to 7 times up-
ward (d?7) through the parse tree from Pp to Pc, and
it is possible to move up to once downward (d?1)
through the parse tree from Pp to Pc.
In clause boundary & tree distance, for example,
3/1,7/1 means the case when we use both the clause
boundary (3/1) and the tree distance (7/1).
3We ignore the nonterminals that have only pre-terminal
children (e.g. in Figure 1, NP1, CONJP, NP2).
211
Precision Recall F?=1
Development 72.68% 69.16% 70.87
Test WSJ 74.69% 70.78% 72.68
Test Brown 64.58% 60.31% 62.38
Test WSJ+Brown 73.35% 69.37% 71.31
Test WSJ Precision Recall F?=1
Overall 74.69% 70.78% 72.68
A0 85.02% 81.53% 83.24
A1 73.98% 72.25% 73.11
A2 63.20% 57.57% 60.25
A3 62.96% 49.13% 55.19
A4 73.40% 67.65% 70.41
A5 100.00% 40.00% 57.14
AM-ADV 56.73% 50.00% 53.15
AM-CAU 70.21% 45.21% 55.00
AM-DIR 46.48% 38.82% 42.31
AM-DIS 70.95% 65.62% 68.18
AM-EXT 87.50% 43.75% 58.33
AM-LOC 44.09% 46.28% 45.16
AM-MNR 55.56% 52.33% 53.89
AM-MOD 97.59% 95.64% 96.61
AM-NEG 96.05% 95.22% 95.63
AM-PNC 40.68% 41.74% 41.20
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 70.11% 61.73% 65.66
R-A0 84.68% 83.93% 84.30
R-A1 73.33% 70.51% 71.90
R-A2 50.00% 31.25% 38.46
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 85.71% 57.14% 68.57
R-AM-MNR 16.67% 16.67% 16.67
R-AM-TMP 72.50% 55.77% 63.04
V 97.32% 97.32% 97.32
Table 4: Overall results (top) and detailed results on
the WSJ test (bottom).
Precision Recall F?=1
one-phase 71.94 68.70 70.29
two-phase 72.68 69.16 70.87
Table 5: Performance of one-phase vs. two-phase.
According to the experimental results, we use
7/1 tree distance restriction for all following ex-
periments. By applying the restriction, we can re-
move about 47.3% (%can.=52.70%) of total argu-
ment candidates as compared with All3. 93.90%
(%arg.) corresponds to the upper bound on recall.
In order to estimate the relative contribution of
each feature, we measure performance of each phase
on the development set by leaving out one feature at
a time, as shown in the top of Table 3. Precision,
Recall, and F?=1 represent the performance of the
identification task, and Accuracy represent the per-
formance of the classification task only with 100%
correct argument identification respectively. All rep-
resents the performance of the experiment when all
26 features introduced by section 2.2 are considered.
Finally, for identification, we use 24 features except
gov and pos+clau, and obtain an F?=1 of 80.59%, as
shown in the bottom of Table 3. Also, for classifica-
tion, we use 23 features except pred type, cont POS,
and pos+clau, and obtain an Accuracy of 87.16%.
Table 4 presents our best system performance on
the development set, and the performance of the
same system on the test set. Table 5 shows the
performance on the development set using the one-
phase method and the two-phase method respec-
tively. The one-phase method is implemented by in-
corporating the identification into the classification.
one-phase shows the performance of the experiment
when 25 features except pos+clau are used. Exper-
imental results show that the two-phase method is
better than the one-phase method in our evaluation.
4 Conclusion
We have presented a two-phase SRL method based
on a ME model. In the two-phase method, in order to
improve the performance of identification that dom-
inate the overall performance, we have performed
pre-processing. Experimental results show that our
system obtains an F?=1 of 72.68% on the WSJ test
and that the introduction of pre-processing improves
the performance, as compared with the case when
all parse constituents are considered.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. Pro-
ceedings of CoNLL-2005.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling
of Semantic Roles. Computational Linguistics.
Joon-Ho Lim, Young-Sook Hwang, So-Young Park and Hae-
Chang Rim. 2004. Semantic Role Labeling using Maximum
Entropy Model. Proceedings of CoNLL.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James H. Martin and Daniel Jurafsky. 2003. Shallow
Semantic Parsing Using Support Vector Machines. Techni-
cal Report, TR-CSLR-2003-03.
212
