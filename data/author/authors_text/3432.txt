WordNet Nouns: Classes and Instances
George A. Miller
Princeton University
Florentina Hristea
University of Bucharest
WordNet, a lexical database for English that is extensively used by computational linguists, has
not previously distinguished hyponyms that are classes from hyponyms that are instances. This
note describes an attempt to draw that distinction and proposes a simple way to incorporate the
results into future versions of WordNet.
If you were to say ?Women are numerous,? you would not wish to imply that any
particular woman is numerous. Instead, you would probably mean something like ?The
class of women contains numerous instances.? To say, on the other hand, ?Rosa Parks
is numerous,? would be nonsense. Whereas the noun woman denotes a class, the proper
noun Rosa Parks is an instance of that class. As Quirk et al (1985, page 288) point out,
proper nouns normally lack number contrast.
This important distinction between classes and instances underlies the present dis-
cussion of WordNet nouns. Some nouns are understood to refer to classes; membership
in those classes determines the semantic relation of hyponymy that is basic for the
organization of nouns in WordNet (WN). Other nouns, however, are understood to refer
to particular individuals. In many cases the distinction is clear, but not always.
The distinction to be discussed here is between words ordinarily understood as
referring to classes and words ordinarily understood as referring to particular individ-
uals and places. In the literature on knowledge representation, the classic discussion of
this distinction is provided by Woods (1975). The distinction was not drawn in initial
versions of WN (Miller 1990; Fellbaum 1998), which used the ?is a? relation in both
cases. That is to say, both ?A heroine is a woman? and ?Rosa Parks is a woman? were
considered to be occurrences of the ?is a? relation and were encoded in the WN database
in the same manner.
Requests to incorporate a distinction between classes and instances have come from
ontologists, among others. In their discussion of WN, for example, Gangemi et al (2001)
and Oltramari et al (2002) complain about the confusion between individuals and
concepts. They suggest that if there was an ?instance of? relation, they could distinguish
between a concept-to-concept relation of subsumption and an individual-to-concept
relation of instantiation. That is, essentially, the suggestion we follow in the present
work, but in some cases the distinction was not easy to draw.
Incorporating this distinction was resisted at first because WN was not initially
conceived as an ontology, but rather as a description of lexical knowledge. WN in-
cludes verbs, adjectives, and adverbs in addition to nouns. Although no ontology was
intended, the organization of nouns in WN bore many similarities to an ontology.
As the importance of ontology became more apparent, requests to convert the WN
noun hierarchy could no longer be ignored. Version 2.1 of WN takes a step in that
direction: the Tops file is reorganized to have a single unique beginner: entity. In a
reasonable ontology, however, all terms might be expected to conform to the mem-
bership relation of set theory and would not contain individuals or placenames. The
confounding of classes and instances in WN posed a problem. The obvious way to solve
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 1
that problem was to distinguish them. That is to say, the instances in WN had to be
identified.
There are three characteristics that all words denoting instances share. They are,
first of all, nouns. Second, they are proper nouns, which means that they should be
capitalized. And finally, the referent should be a unique entity, which implies that they
should not have hyponyms; it is meaningless to have an instance of an instance.
Unfortunately, these characteristics are shared by many words that are not in-
stances. In clear-cut cases, such as persons or cities, there is little problem identify-
ing instances. But there are many other proper nouns that are not instances. It was
decided that there was no alternative to inspecting the 24,073 sets of synonyms that
contained candidate nouns, one at a time. This was done manually by the authors, FH
and GM.
The strategy agreed on for assigning ?instance? tags was to concentrate on a word?s
referent. When they knew of a unique referent, it was considered a clear case of an
instance. Otherwise it was considered a class. For example, when Beethoven is used to
refer to the German composer, it is an instance, but when Beethoven is used to refer to the
composer?s music (as in ?She loved to listen to Beethoven?) the same word refers to a
class of musical compositions. Moreover, just to be clear, when there were two different
referents, both were tagged as instances. For example, Bethlehem in the Holy Land and
Bethlehem in Pennsylvania were both tagged as instances. And when an instance had
two or more hypernyms, it was tagged as an instance of all of them. For example, Mars
is an instance of a terrestrial planet (having a compact rocky surface) and also as an
instance of a superior planet (its orbit lies outside the Earth?s orbit).
The basic entries in WN are sets of synonyms, or synsets. A problem reported by
both taggers was the occurrence of capitalized and lower-case words in the same synset.
It makes no sense for a word to refer to an instance and for its synonym to refer to a
class, so in these cases the entire synset was considered to denote a class. For example,
acetaminophen and Tylenol are synonyms in WN and both were considered to denote
classes. The possibility that Tylenol might be an instance of acetaminophen seemed to be
refuted by such usages as ?She took two Tylenol and went to bed.? In short, giving
something a trade name does not change it from a class to an instance. The street names
of drugs were also considered to denote classes.
The two taggers disagreed in their treatment of sacred texts. Whereas they agreed
that Adi Granth, Zend Vesta, Bhagavadgita, Mahabharata, and others were instances of
sacred texts, when they came to the Christian Bible they disagreed. FH considered it
an instance, no different from other sacred texts; GM called it a class term because
there are many hyponyms of Bible: Vulgate, Douay, King James, Revised Version, American
Revised Version, etc. But GM?s decision made the Bible a special case, which may have
resulted from WN?s compilers knowing more about the Bible than they knew about
other sacred texts. It was decided that this was a case in which a sacred text could be
a class: Bible was tagged as a class of a sacred text and its hyponyms were tagged as
instances.
Languages posed another problem. For example, are Old Italian, Sardinian, and
Tuscan instances of Italian? It was decided that, from an ontological point of view,
languages are not instances. Only speech acts are instances.
Placenames included many geographical regions that do not have well-defined po-
litical boundaries: Andalusia, Appalachia, Antarctic Zone, Badlands, Barbary Coast, Bithynia,
Caucasia, etc., but the terms still have geographical significance and are in general use.
Although vague in denotation, the taggers considered them instances. The names of
cities and states, islands and continents, rivers and lakes, mountain peaks and mountain
2
Miller and Hristea WordNet Nouns: Classes and Instances
ranges, seas and oceans, planets and satellites, stars and constellations, were tagged as
instances, as were the signs of the zodiac.
The names of numbers and the names of monetary units were all considered as
classes; the Hong Kong dollar, for example, is not an instance of dollar.
Overall, there were 7,671 synsets in WN that the taggers finally agreed should be
tagged as instances. Version 2.1 of WordNet contains these distinctions and it will be
subjected to helpful criticism by WN users, as are all the other lexical relations in WN.
Finally, a word about the notation that will represent this distinction in WN. The
symbol used to code hypernyms has been @. That is to say, {peach, drupe,@} has repre-
sented ?a peach is a drupe? or ?all peaches are drupes.? This notation is appropriate
for representing relations between classes but it is not appropriate for representing
relations of instantiation. When {Berlin, city,@} is used to represent ?Berlin is a city,?
the instance Berlin is treated inappropriately as a class. A different symbol is needed to
code instances. It has been decided, therefore, simply to add an i to the @; to represent
?Berlin is an instance of a city? by {Berlin, city,@i} in the new notation.
Since WN 2.1 contains the distinctions between classes and instances described
here, it will be possible to treat WN nouns as a semi-ontology by simply ignoring all
nouns tagged with @i. It is hoped that this modification will make WN more useful to
future users.
Acknowledgments
Florentina Hristea is grateful to the
Romanian-U.S. Fulbright Commission for
the Fulbright Grant that made it possible for
her to collaborate in this research. Work by
the Cognitive Science Laboratory was
supported by a contract between Princeton
University and the Advanced Research
Development Activity (AQUAINT Program
Phase 2, Contract No. NBCHC40012). The
authors are indebted to Benjamin Haskell for
developing the interface that was used to tag
instances and to Christiane Fellbaum, Helen
Langone, and Randee Tengi for comments on
the manuscript.
References
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Gangemi, Aldo, Nicola Guarino, and
Alessandro Oltramari. 2001. Conceptual
analysis of lexical taxonomies: The case
of WordNet top-level. In C. Welty and
B. Smith, editors, Formal Ontology in
Information Systems. Proceedings of
FOIS2001. ACM Press, 285?296.
Oltramari, Alessandro, Aldo Gangemi,
Nicola Guarino, and Claudio Masolo.
2002. Restructuring WordNet?s top-level:
The OntoClean approach. In Proceedings of
LREC2002 (OntoLex workshop), Las
Palmas, Spain.
Miller, George A., editor. 1990. WordNet: An
on-line lexical database [Special Issue].
International Journal of Lexicography,
3:235?312.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1985.
A Comprehensive Grammar of the English
Language. Longman, London and
New York.
Woods, William A. 1975. What?s in a link:
Foundations for semantic networks.
In Daniel G. Bobrow and Alan Collins,
editors, Representation and Understanding:
Studies in Cognitive Science. Academic
Press, New York.
3

An Evaluation Exercise for Romanian Word Sense Disambiguation
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu
Vivi Na?stase
School of Computer Science
University of Ottawa
Ottawa, ON, Canada
vnastase@site.uottawa.ca
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA, USA
timc@isi.edu
Doina Ta?tar
Department of Computer Science
Babes?-Bolyai University
Cluj-Napoca, Romania
dtatar@ubb.ro
Dan Tufis?
Romanian Academy Center
for Artificial Intelligence
Bucharest, Romania
tufis@racai.ro
Florentina Hristea
Department of Computer Science
University of Bucharest
Bucharest, Romania
fhristea@mailbox.ro
Abstract
This paper presents the task definition, resources,
participating systems, and comparative results for a
Romanian Word Sense Disambiguation task, which
was organized as part of the SENSEVAL-3 evaluation
exercise. Five teams with a total of seven systems
were drawn to this task.
1 Introduction
SENSEVAL is an evaluation exercise of the lat-
est word-sense disambiguation (WSD) systems. It
serves as a forum that brings together researchers in
WSD and domains that use WSD for various tasks.
It allows researchers to discuss modifications that
improve the performance of their systems, and an-
alyze combinations that are optimal.
Since the first edition of the SENSEVAL competi-
tions, a number of languages were added to the orig-
inal set of tasks. Having the WSD task prepared for
several languages provides the opportunity to test
the generality of WSD systems, and to detect dif-
ferences with respect to word senses in various lan-
guages.
This year we have proposed a Romanian WSD
task. Five teams with a total of seven systems have
tackled this task. We present in this paper the data
used and how it was obtained, and the performance
of the participating systems.
2 Open Mind Word Expert
The sense annotated corpus required for this task
was built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted to Roma-
nian1.
To overcome the current lack of sense tagged
data and the limitations imposed by the creation of
such data using trained lexicographers, the Open
Mind Word Expert system enables the collection
of semantically annotated corpora over the Web.
Sense tagged examples are collected using a Web-
based application that allows contributors to anno-
tate words with their meanings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, who are asked to
select the most appropriate sense for the target word
in each sentence. The selection is made using check-
boxes, which list all possible senses of the current
target word, plus two additional choices, ?unclear?
and ?none of the above.? Although users are en-
couraged to select only one meaning per word, the
selection of two or more senses is also possible. The
results of the classification submitted by other users
are not presented to avoid artificial biases.
3 Sense inventory
For the Romanian WSD task, we have chosen a set
of words from three parts of speech - nouns, verbs
and adjectives. Table 1 presents the number of
words under each part of speech, and the average
number of senses for each class.
The senses were (manually) extracted from a Ro-
manian dictionary (Dict?ionarul EXplicativ al limbii
roma?ne - DEX (Coteanu et al, 1975)). These senses
1Romanian Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/romanian
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Number Avg senses Avg senses
Class words (fine) (coarse)
Nouns 25 8.92 4.92
Verbs 9 8.7 4.6
Adjectives 5 9 4
Total 39 8.875 4.725
Table 1: Sense inventory
and their dictionary definitions were incorporated in
the Open Mind Word Expert. For each annotation
task, the contributors could choose from this list of
39 words. For each chosen word, the system dis-
plays the associated senses, together with their def-
initions, and a short (1-4 words) description of the
sense. After the user gets familiarized with these
senses, the system displays each example sentence,
and the list of senses together with their short de-
scription, to facilitate the tagging process.
For the coarse grained WSD task, we had the op-
tion of using the grouping provided by the dictio-
nary. A manual analysis however showed that some
of the senses in the same group are quite distinguish-
able, while others that were separated were very
similar.
For example, for the word circulatie (roughly, cir-
culation). The following two senses are grouped in
the dictionary:
2a. movement, travel along a communication
line/way
2b. movement of the sap in plants or the cytoplasm
inside cells
Sense 2a fits better with sense 1 of circulation:
1. the event of moving about
while sense 2b fits better with sense 3:
3. movement or flow of a liquid, gas, etc. within a
circuit or pipe.
To obtain a better grouping, a linguist clustered
the similar senses for each word in our list of forty.
The average number of senses for each class is al-
most halved.
Notice that Romanian is a language that uses dia-
critics, and the the presence of diacritics may be cru-
cial for distinguishing between words. For example
peste without diacritics may mean fish or over. In
choosing the list of words for the Romanian WSD
task, we have tried to avoid such situations. Al-
though some of the words in the list do have dia-
critics, omitting them does not introduce new ambi-
guities.
4 Corpus
Examples are extracted from the ROCO corpus, a
400 million words corpus consisting of a collection
of Romanian newspapers collected on the Web over
a three years period (1999-2002).
The corpus was tokenized and part-of-speech
tagged using RACAI?s tools (Tufis, 1999). The to-
kenizer recognizes and adequately segments various
constructs: clitics, dates, abbreviations, multiword
expressions, proper nouns, etc. The tagging fol-
lowed the tiered tagging approach with the hidden
layer of tagging being taken care of by Thorsten
Brants? TNT (Brants, 2000). The upper level of
the tiered tagger removed from the assigned tags all
the attributes irrelevant for this WSD exercise. The
estimated accuracy of the part-of-speech tagging is
around 98%.
5 Sense Tagged Data
While several sense annotation schemes have been
previously proposed, including single or dual anno-
tations, or the ?tag until two agree? scheme used dur-
ing SENSEVAL-2, we decided to use a new scheme
and collect four tags per item, which allowed us
to conduct and compare inter-annotator agreement
evaluations for two-, three-, and four-way agree-
ment. The agreement rates are listed in Table 3.
The two-way agreement is very high ? above 90%
? and these are the items that we used to build the
annotated data set. Not surprisingly, four-way agree-
ment is reached for a significantly smaller number of
cases. While these items with four-way agreement
were not explicitly used in the current evaluation,
we believe that this represents a ?platinum standard?
data set with no precedent in the WSD research com-
munity, which may turn useful for a range of future
experiments (for bootstrapping, in particular).
Agreement type Total (%)
TOTAL ITEMS 11,532 100%
At least two agree 10,890 94.43%
At least three agree 8,192 71.03%
At least four agree 4,812 41.72%
Table 3: Inter-agreement rates for two-, three-, and
four-way agreement
Table 2 lists the target words selected for this task,
together with their most common English transla-
tions. For each word, we also list the number of
senses, as defined in the DEX sense inventory (col-
locations included), and the number of annotated ex-
amples made available to task participants.
Word Main English senses senses Train Test Word Main English senses senses Train Test
translation (fine) (coarse) size size translation (fine) (coarse) size size
NOUNS
ac needle 16 7 127 65 accent accent 5 3 172 87
actiune action 10 7 261 128 canal channel 6 5 134 66
circuit circuit 7 5 200 101 circulatie circulation 9 3 221 114
coroana crown 15 11 252 126 delfin doplhin 5 4 31 15
demonstratie demonstration 6 3 229 115 eruptie eruption 2 2 54 27
geniu genius 5 3 106 54 nucleu nucleus 7 5 64 33
opozitie opposition 12 7 266 134 perie brush 5 3 46 24
pictura painting 5 2 221 111 platforma platform 11 8 226 116
port port 7 3 219 108 problema problem 6 4 262 131
proces process 11 3 166 82 reactie reaction 7 6 261 131
stil style 14 4 199 101 timbru stamp 7 3 231 116
tip type 7 4 263 131 val wave 15 9 242 121
valoare value 23 9 251 125
VERBS
cistiga win 5 4 227 115 citi read 10 4 259 130
cobori descend 11 6 252 128 conduce drive 7 6 265 134
creste grow 14 6 209 103 desena draw 3 3 54 27
desface untie 11 5 115 58 fierbe boil 11 4 83 43
indulci sweeten 7 4 19 10
ADJECTIVES
incet slow 6 3 224 113 natural natural 12 5 242 123
neted smooth 7 3 34 17 oficial official 5 3 185 96
simplu simple 15 6 153 82
Table 2: Target words in the SENSEVAL-3 Romanian Lexical Sample task
Team System name Reference (this volume)
Babes-Bolyai University, Cluj-Napoca (1) ubb nbc ro (Csomai, 2004)
Babes-Bolyai University, Cluj-Napoca (2) UBB (Serban and Tatar, 2004)
Swarthmore College swat-romanian (Wicentowski et al, 2004a)
Swarthmore College / Hong Kong Polytechnic University swat-hk-romanian (Wicentowski et al, 2004b)
Hong Kong University of Science and Technology romanian-swat hk-bo
University of Maryland, College Park UMD SST6 (Cabezas et al, 2004)
University of Minnesota, Duluth Duluth-RomLex (Pedersen, 2004)
Table 4: Teams participating in the SENSEVAL-3 Romanian Word Sense Disambiguation task
In addition to sense annotated examples, partici-
pants have been also provided with a large number
of unlabeled examples. However, among all partici-
pating systems, only one system ? described in (Ser-
ban and Ta?tar 2004) ? attempted to integrate this ad-
ditional unlabeled data set into the learning process.
6 Participating Systems
Five teams participated in this word sense disam-
biguation task. Table 4 lists the names of the par-
ticipating systems, the corresponding institutions,
and references to papers in this volume that provide
detailed descriptions of the systems and additional
analysis of their results.
There were no restrictions placed on the number
of submissions each team could make. A total num-
ber of seven submissions was received for this task.
Table 5 shows all the submissions for each team, and
gives a brief description of their approaches.
7 Results and Discussion
Table 6 lists the results obtained by all participating
systems, and the baseline obtained using the ?most
frequent sense? (MFS) heuristic. The table lists pre-
cision and recall figures for both fine grained and
coarse grained scoring.
The performance of all systems is significantly
higher than the baseline, with the best system per-
forming at 72.7% (77.1%) for fine grained (coarse
grained) scoring, which represents a 35% (38%) er-
ror reduction with respect to the baseline.
The best system (romanian-swat hk-bo) relies on
a Maximum Entropy classifier with boosting, using
local context (neighboring words, lemmas, and their
part of speech), as well as bag-of-words features for
surrounding words.
Not surprisingly, several of the top perform-
ing systems are based on combinations of multi-
ple sclassifiers, which shows once again that voting
System Description
romanian-swat hk-bo Supervised learning using Maximum Entropy with boosting, using bag-of-words
and n-grams around the head word as features
swat-hk-romanian The swat-romanian and romanian-swat hk-bo systems combined with majority voting.
Duluth-RLSS An ensemble approach that takes a vote among three bagged decision trees,
based on unigrams, bigrams and co-occurrence features
swat-romanian Three classifiers: cosine similarity clustering, decision list, and Naive Bayes,
using bag-of-words and n-grams around the head word as features
combined with a majority voting scheme.
UMD SST6 Supervised learning using Support Vector Machines, using contextual features.
ubb nbc ro Supervised learning using a Naive Bayes learning scheme, and features extracted
using a bag-of-words approach.
UBB A k-NN memory-based learning approach, with bag-of-words features.
Table 5: Short description of the systems participating in the SENSEVAL-3 Romanian Word Sense Disam-
biguation task. All systems are supervised.
Fine grained Coarse grained
System P R P R
romanian-swat hk-bo 72.7% 72.7% 77.1% 77.1%
swat-hk-romanian 72.4% 72.4% 76.1% 76.1%
Duluth-RLSS 71.4% 71.4% 75.2% 75.2%
swat-romanian 71.0% 71.0% 74.9% 74.9%
UMD SST6 70.7% 70.7% 74.6% 74.6%
ubb nbc ro 71.0% 68.2% 75.0% 72.0%
UBB 67.1% 67.1% 72.2% 72.2%
Baseline (MFS) 58.4% 58.4% 62.9% 62.9%
Table 6: System results on the Romanian Word Sense Disambiguation task.
schemes that combine several learning algorithms
outperform the accuracy of individual classifiers.
8 Conclusion
A Romanian Word Sense Disambiguation task
was organized as part of the SENSEVAL-3 eval-
uation exercise. In this paper, we presented
the task definition, and resources involved, and
shortly described the participating systems. The
task drew the participation of five teams, and in-
cluded seven different systems. The sense an-
notated data used in this exercise is available
online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the
Romanian Open Mind Word Expert project, mak-
ing this task possible. Special thanks to Bog-
dan Harhata, from the Institute of Linguistics Cluj-
Napoca, for building a coarse grained sense map.
We are also grateful to all the participants in this
task, for their hard work and involvement in this
evaluation exercise. Without them, all these com-
parative analyses would not be possible.
References
T. Brants. 2000. Tnt - a statistical part-of-speech
tagger. In Proceedings of the 6th Applied NLP
Conference, ANLP-2000, Seattle, WA, May.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with Open Mind Word Ex-
pert. In Proceedings of the Workshop on ?Word
Sense Disambiguation: Recent Successes and Fu-
ture Directions?, ACL 2002, Philadelphia, July.
I. Coteanu, L. Seche, M. Seche, A. Burnei,
E. Ciobanu, E. Contras?, Z. Cret?a, V. Hristea,
L. Mares?, E. St??ngaciu, Z. S?tefa?nescu, T. T?ugulea,
I. Vulpescu, and T. Hristea. 1975. Dict?ionarul
Explicativ al Limbii Roma?ne. Editura Academiei
Republicii Socialiste Roma?nia.
D. Tufis. 1999. Tiered tagging and combined classi-
fiers. In Text, Speech and Dialogue, Lecture Notes
in Artificial Intelligence.
