Memory-Efficient and Thread-Safe Quasi-Destructive Graph
Unification
Marcel P. van Lohuizen
Department of Information Technology and Systems
Delft University of Technology
mpvl@acm.org
Abstract
In terms of both speed and mem-
ory consumption, graph unification
remains the most expensive com-
ponent of unification-based gram-
mar parsing. We present a tech-
nique to reduce the memory usage
of unification algorithms consider-
ably, without increasing execution
times. Also, the proposed algorithm
is thread-safe, providing an efficient
algorithm for parallel processing as
well.
1 Introduction
Both in terms of speed and memory consump-
tion, graph unification remains the most ex-
pensive component in unification-based gram-
mar parsing. Unification is a well known algo-
rithm. Prolog, for example, makes extensive
use of term unification. Graph unification is
slightly different. Two different graph nota-
tions and an example unification are shown in
Figure 1 and 2, respectively.
In typical unification-based grammar
parsers, roughly 90% of the unifications
fail. Any processing to create, or copy, the
result graph before the point of failure is
b
e
A C
F
D
?
?
A = b
C = 1
[
D = e
]
F = 1
?
?
Figure 1: Two ways to represent an identical
graph.
redundant. As copying is the most expensive
part of unification, a great deal of research
has gone in eliminating superfluous copying.
Examples of these approaches are given in
(Tomabechi, 1991) and (Wroblewski, 1987).
In order to avoid superfluous copying, these
algorithms incorporate control data in the
graphs. This has several drawbacks, as we
will discuss next.
Memory Consumption To achieve the
goal of eliminating superfluous copying, the
aforementioned algorithms include adminis-
trative fields?which we will call scratch
fields?in the node structure. These fields
do not attribute to the definition of the graph,
but are used to efficiently guide the unifica-
tion and copying process. Before a graph is
used in unification, or after a result graph has
been copied, these fields just take up space.
This is undesirable, because memory usage
is of great concern in many unification-based
grammar parsers. This problem is especially
of concern in Tomabechi?s algorithm, as it in-
creases the node size by at least 60% for typ-
ical implementations.
In the ideal case, scratch fields would be
stored in a separate buffer allowing them to be
reused for each unification. The size of such a
buffer would be proportional to the maximum
number of nodes that are involved in a single
unification. Although this technique reduces
memory usage considerably, it does not re-
duce the amount of data involved in a single
unification. Nevertheless, storing and loading
nodes without scratch fields will be faster, be-
cause they are smaller. Because scratch fields
are reused, there is a high probability that
they will remain in cache. As the difference
[
A =
[
B = c
]
D =
[
E = f
]
]
unionsq
?
?
A = 1
[
B = c
]
D = 1
G =
[
H = j
]
?
??
?
?
?
?
A = 1
[
B = c
E = f
]
D = 1
G =
[
H = j
]
?
?
?
?
Figure 2: An example unification in attribute value matrix notation.
in speed between processor and memory con-
tinues to grow, caching is an important con-
sideration (Ghosh et al, 1997).1
A straightforward approach to separate the
scratch fields from the nodes would be to use
a hash table to associate scratch structures
with the addresses of nodes. The overhead
of a hash table, however, may be significant.
In general, any binding mechanism is bound
to require some extra work. Nevertheless,
considering the difference in speed between
processors and memory, reducing the mem-
ory footprint may compensate for the loss of
performance to some extent.
Symmetric Multi Processing Small-
scale desktop multiprocessor systems (e.g.
dual or even quad Pentium machines) are be-
coming more commonplace and affordable. If
we focus on graph unification, there are two
ways to exploit their capabilities. First, it is
possible to parallelize a single graph unifica-
tion, as proposed by e.g. (Tomabechi, 1991).
Suppose we are unifying graph a with graph b,
then we could allow multiple processors to
work on the unification of a and b simulta-
neously. We will call this parallel unifica-
tion. Another approach is to allow multiple
graph unifications to run concurrently. Sup-
pose we are unifying graph a and b in addi-
tion to unifying graph a and c. By assigning
a different processor to each operation we ob-
tain what we will call concurrent unifica-
tion. Parallel unification exploits parallelism
inherent of graph unification itself, whereas
concurrent unification exploits parallelism at
the context-free grammar backbone. As long
as the number of unification operations in
1Most of today?s computers load and store data in
large chunks (called cache lines), causing even unini-
tialized fields to be transported.
one parse is large, we believe it is preferable
to choose concurrent unification. Especially
when a large number of unifications termi-
nates quickly (e.g. due to failure), the over-
head of more finely grained parallelism can be
considerable.
In the example of concurrent unification,
graph a was used in both unifications. This
suggests that in order for concurrent unifica-
tion to work, the input graphs need to be
read only. With destructive unification al-
gorithms this does not pose a problem, as
the source graphs are copied before unifica-
tion. However, including scratch fields in the
node structure (as Tomabechi?s and Wrob-
lewski?s algorithms do) thwarts the imple-
mentation of concurrent unification, as differ-
ent processors will need to write different val-
ues in these fields. One way to solve this prob-
lem is to disallow a single graph to be used
in multiple unification operations simultane-
ously. In (van Lohuizen, 2000) it is shown,
however, that this will greatly impair the abil-
ity to achieve speedup. Another solution is to
duplicate the scratch fields in the nodes for
each processor. This, however, will enlarge
the node size even further. In other words,
Tomabechi?s and Wroblewski?s algorithms are
not suited for concurrent unification.
2 Algorithm
The key to the solution of all of the above-
mentioned issues is to separate the scratch
fields from the fields that actually make up
the definition of the graph. The result-
ing data structures are shown in Figure 3.
We have taken Tomabechi?s quasi-destructive
graph unification algorithm as the starting
point (Tomabechi, 1995), because it is often
considered to be the fastest unification algo-
arc list
type
ArcNode
Unification data Copy data
Reusable scratch
structures
copyforward
comp-arc list
value
label
offset
indexindex
only structures
Permanent, read-
Figure 3: Node and Arc structures and the
reusable scratch fields. In the permanent
structures we use offsets. Scratch structures
use index values (including arcs recorded in
comp-arc list). Our implementation derives
offsets from index values stored in nodes.
rithm for unification-based grammar parsing
(see e.g. (op den Akker et al, 1995)). We
have separated the scratch fields needed for
unification from the scratch fields needed for
copying.2
We propose the following technique to asso-
ciate scratch structures with nodes. We take
an array of scratch structures. In addition,
for each graph we assign each node a unique
index number that corresponds to an element
in the array. Different graphs typically share
the same indexes. Since unification involves
two graphs, we need to ensure that two nodes
will not be assigned the same scratch struc-
ture. We solve this by interleaving the index
positions of the two graphs. This mapping is
shown in Figure 4. Obviously, the minimum
number of elements in the table is two times
the number of nodes of the largest graph. To
reduce the table size, we allow certain nodes
to be deprived of scratch structures. (For ex-
ample, we do not forward atoms.) We denote
this with a valuation function v, which re-
turns 1 if the node is assigned an index and 0
otherwise.
We can associate the index with a node by
including it in the node structure. For struc-
ture sharing, however, we have to use offsets
between nodes (see Figure 4), because other-
wise different nodes in a graph may end up
having the same index (see Section 3). Off-
2The arc-list field could be used for permanent for-
ward links, if required.
c_
Left graph
offset: 0
g4
e3 f _
Right graph
offset: 1
2j
h0
_l
3k1b 1i
2 x 0 + 0
a h b ji k
0 1 2 3 4 5 6 7 8 9 10 11 12
d e g
a0
d2
+1
+1 +1
2 x 1 + 1
+1 -2 +0
+3+1
2 x 4 + 0
+4
-2+1+0
Figure 4: The mechanism to associate index
numbers with nodes. The numbers in the
nodes represent the index number. Arcs are
associated with offsets. Negative offsets indi-
cate a reentrancy.
sets can be easily derived from index values
in nodes. As storing offsets in arcs consumes
more memory than storing indexes in nodes
(more arcs may point to the same node), we
store index values and use them to compute
the offsets. For ease of reading, we present our
algorithm as if the offsets were stored instead
of computed. Note that the small index val-
ues consume much less space than the scratch
fields they replace.
The resulting algorithm is shown in Fig-
ure 5. It is very similar to the algorithm in
(Tomabechi, 1991), but incorporates our in-
dexing technique. Each reference to a node
now not only consists of the address of the
node structure, but also its index in the ta-
ble. This is required because we cannot derive
its table index from its node structure alone.
The second argument of Copy indicates
the next free index number. Copy returns
references with an offset, allowing them to
be directly stored in arcs. These offsets will
be negative when Copy exits at line 2.2,
resembling a reentrancy. Note that only
AbsArc explicitly defines operations on off-
sets. AbsArc computes a node?s index using
its parent node?s index and an offset.
Unify(dg1, dg2)
1. try Unify1((dg1, 0), (dg2, 1))a
1.1. (copy, n)? Copy((dg1, 0), 0)
1.2. Clear the fwtab and cptab table.b
1.3. return copy
2. catch
2.1. Clear the fwtab table.b
2.2. return nil
Unify1(ref in1, ref in2)
1. ref1? (dg1, idx1)? Dereference(ref in1)
2. ref2? (dg2, idx2)? Dereference(ref in2)
3. if dg1 ?addr dg2 and idx1 = idx2c then
3.1. return
4. if dg1.type = bottom then
4.1. Forward(ref1, ref2)
5. elseif dg2.type = bottom then
5.1. Forward(ref2, ref1)
6. elseif both dg1 and dg2 are atomic then
6.1. if dg1.arcs 6= dg2.arcs then
throw UnificationFailedException
6.2. Forward(ref2, ref1)
7. elseif either dg1 or dg2 is atomic then
7.1. throw UnificationFailedException
8. else
8.1. Forward(ref2, ref1)
8.2. shared? IntersectArcs(ref1, ref2)
8.3. for each (( , r1), ( , r2)) in shared do
Unify1(r1, r2)
8.4. new? ComplementArcs(ref1, ref2)
8.5. for each arc in new do
Push arc to fwtab[idx1].comp arcs
Forward((dg1, idx1), (dg2, idx2))
1. if v(dg1) = 1 then
fwtab[idx1].forward? (dg2, idx2)
AbsArc((label, (dg, off)), current idx)
return (label, (dg, current idx + 2 ? off))d
Dereference((dg, idx))
1. if v(dg1) = 1 then
1.1. (fwd-dg, fwd-idx)? fwtab[idx].forward
1.2. if fwd-dg 6= nil then
Dereference(fwd-dg, fwd-idx)
1.3. else
return (dg, idx)
IntersectArcs(ref1, ref2)
Returns pairs of arcs with index values for each pair
of arcs in ref1 resp. ref2 that have the same label.
To obtain index values, arcs from arc-list must be
converted with AbsArc.
ComplementArcs(ref1, ref2)
Returns node references for all arcs with labels that
exist in ref2, but not in ref1. The references are com-
puted as with IntersectArcs.
Copy(ref in, new idx)
1. (dg, idx)? Dereference(ref in)
2. if v(dg) = 1 and cptab[idx].copy 6= nil then
2.1. (dg1, idx1)? cptab[idx].copy
2.2. return (dg1, idx1? new idx + 1)
3. newcopy? new Node
4. newcopy.type? dg.type
5. if v(dg) = 1 then
cptab[idx].copy? (newcopy, new idx)
6. count? v(newcopy)e
7. if dg.type = atomic then
7.1. newcopy.arcs? dg.arcs
8. elseif dg.type = complex then
8.1. arcs? {AbsArc(a, idx) | a ? dg.arcs}
? fwtab[idx].comp arcs
8.2. for each (label, ref) in arcs do
ref1? Copy(ref, count + new idx)f
Push (label, ref1) into newcopy.arcs
if ref1.offset > 0g then
count? count + ref1.offset
9. return (newcopy, count)
aWe assign even and odd indexes to the nodes of dg1 and dg2, respectively.
bTables only needs to be cleared up to point where unification failed.
cCompare indexes to allow more powerful structure sharing. Note that indexes uniquely identify a node in
the case that for all nodes n holds v(n) = 1.
dNote that we are multiplying the offset by 2 to account for the interleaved offsets of the left and right graph.
eWe assume it is known at this point whether the new node requires an index number.
fNote that ref contains an index, whereas ref1 contains an offset.
gIf the node was already copied (in which case it is < 0), we need not reserve indexes.
Figure 5: The memory-efficient and thread-safe unification algorithm. Note that the arrays
fwtab and cptab?which represent the forward table and copy table, respectively?are defined
as global variables. In order to be thread safe, each thread needs to have its own copy of these
tables.
Contrary to Tomabechi?s implementation,
we invalidate scratch fields by simply reset-
ting them after a unification completes. This
simplifies the algorithm. We only reset the
table up to the highest index in use. As table
entries are roughly filled in increasing order,
there is little overhead for clearing unused el-
ements.
A nice property of the algorithm is that
indexes identify from which input graph a
node originates (even=left, odd=right). This
information can be used, for example, to
selectively share nodes in a structure shar-
ing scheme. We can also specify additional
scratch fields or additional arrays at hardly
any cost. Some of these abilities will be used
in the enhancements of the algorithm we will
discuss next.
3 Enhancements
Structure Sharing Structure sharing is an
important technique to reduce memory us-
age. We will adopt the same terminology as
Tomabechi in (Tomabechi, 1992). That is,
we will use the term feature-structure sharing
when two arcs in one graph converge to the
same node in that graph (also refered to as
reentrancy) and data-structure sharing when
arcs from two different graphs converge to the
same node.
The conditions for sharing mentioned in
(Tomabechi, 1992) are: (1) bottom and
atomic nodes can be shared; (2) complex
nodes can be shared unless they are modified.
We need to add the following condition: (3)
all arcs in the shared subgraph must have the
same offsets as the subgraph that would have
resulted from copying. A possible violation
of this constraint is shown in Figure 6. As
long as arcs are processed in increasing order
of index number,3 this condition can only be
violated in case of reentrancy. Basically, the
condition can be violated when a reentrancy
points past a node that is bound to a larger
subgraph.
3This can easily be accomplished by fixing the or-
der in which arcs are stored in memory. This is a good
idea anyway, as it can speedup the ComplementArcs
and IntersectArcs operations.
h0a0
1i
3k
s6
t
G +1
7
Node could be shared Node violates condition 3
1b j 4
+3+1 +2F
K +1
G H
c2 d
e4 f 5
g6
+4
+1 +1
+5F
F G +1
HG
+1
K L
b 2j1
3
o2 p3
+4
+1 +1
+5F HG
+1
K L
F
0
q4
+1
1n
m
r 5
result without sharing result with sharing
F
0m
+1F G +4
s6
-3
+6H
G +1K
Specialized sharing arc
-3
-2
3d g7
4l
Figure 6: Sharing mechanism. Node f cannot
be shared, as this would cause the arc labeled
F to derive an index colliding with node q.
Contrary to many other structure sharing
schemes (like (Malouf et al, 2000)), our algo-
rithm allows sharing of nodes that are part of
the grammar. As nodes from the different in-
put graphs are never assigned the same table
entry, they are always bound independently
of each other. (See the footnote for line 3 of
Unify1.)
The sharing version of Copy is similar to
the variant in (Tomabechi, 1992). The extra
check can be implemented straightforwardly
by comparing the old offset with the offset for
the new nodes. Because we derive the offsets
from index values associated with nodes, we
need to compensate for a difference between
the index of the shared node and the index it
should have in the new graph. We store this
information in a specialized share arc. We
need to adjust Unify1 to handle share arcs
accordingly.
Deferred Copying Just as we use a table
for unification and copying, we also use a ta-
ble for subsumption checking. Tomabechi?s
algorithm requires that the graph resulting
01
2
3
4
5
6
4 5 6 7 8 9 10 11 12 13 14 15 16 17
Tim
e (
sec
on
ds)
 
Sentence length (no. words)
"basic"
"tomabechi"
"packed"
"pack+deferred_copy"
"pack+share"
"packed_on_dual_proc"
Figure 7: Execution time (seconds).
from unification be copied before it can be
used for further processing. This can result
in superfluous copying when the graph is sub-
sumed by an existing graph. Our technique
allows subsumption to use the bindings gener-
ated by Unify1 in addition to its own table.
This allows us to defer copying until we com-
pleted subsumption checking.
Packed Nodes With a straightforward im-
plementation of our algorithm, we obtain a
node size of 8 bytes.4 By dropping the con-
cept of a fixed node size, we can reduce the
size of atom and bottom nodes to 4 bytes.
Type information can be stored in two bits.
We use the two least significant bits of point-
ers (which otherwise are 0) to store this type
information. Instead of using a pointer for
the value field, we store nodes in place. Only
for reentrancies we still need pointers. Com-
plex nodes require 8 bytes, as they include
a pointer to the first node past its children
(necessary for unification). This scheme re-
quires some extra logic to decode nodes, but
significantly reduces memory consumption.
4We do not have a type hierarchy.
0
5
10
15
20
25
30
35
40
4 5 6 7 8 9 10 11 12 13 14 15 16 17
He
ap
 si
ze
 (M
B)
 
Sentence length (no. words)
"basic"
"tomabechi"
"packed"
"pack+share"
Figure 8: Memory used by graph heap (MB).
4 Experiments
We have tested our algorithm with a medium-
sized grammar for Dutch. The system was
implemented in Objective-C using a fixed ar-
ity graph representation. We used a test set
of 22 sentences of varying length. Usually, ap-
proximately 90% of the unifications fails. On
average, graphs consist of 60 nodes. The ex-
periments were run on a Pentium III 600EB
(256 KB L2 cache) box, with 128 MB mem-
ory, running Linux.
We tested both memory usage and execu-
tion time for various configurations. The re-
sults are shown in Figure 7 and 8. It includes
a version of Tomabechi?s algorithm. The
node size for this implementation is 20 bytes.
For the proposed algorithm we have included
several versions: a basic implementation, a
packed version, a version with deferred copy-
ing, and a version with structure sharing.
The basic implementation has a node size of
8 bytes, the others have a variable node size.
Whenever applicable, we applied the same op-
timizations to all algorithms. We also tested
the speedup on a dual Pentium II 266 Mhz.5
Each processor was assigned its own scratch
tables. Apart from that, no changes to the
5These results are scaled to reflect the speedup rel-
ative to the tests run on the other machine.
algorithm were required. For more details on
the multi-processor implementation, see (van
Lohuizen, 1999).
The memory utilization results show signif-
icant improvements for our approach.6 Pack-
ing decreased memory utilization by almost
40%. Structure sharing roughly halved this
once more.7 The third condition prohibited
sharing in less than 2% of the cases where it
would be possible in Tomabechi?s approach.
Figure 7 shows that our algorithm does not
increase execution times. Our algorithm even
scrapes off roughly 7% of the total parsing
time. This speedup can be attributed to im-
proved cache utilization. We verified this by
running the same tests with cache disabled.
This made our algorithm actually run slower
than Tomabechi?s algorithm. Deferred copy-
ing did not improve performance. The addi-
tional overhead of dereferencing during sub-
sumption was not compensated by the savings
on copying. Structure sharing did not sig-
nificantly alter the performance as well. Al-
though, this version uses less memory, it has
to perform additional work.
Running the same tests on machines with
less memory showed a clear performance ad-
vantage for the algorithms using less memory,
because paging could be avoided.
5 Related Work
We reduce memory consumption of graph uni-
fication as presented in (Tomabechi, 1991)
(or (Wroblewski, 1987)) by separating scratch
fields from node structures. Pereira?s
(Pereira, 1985) algorithm also stores changes
to nodes separate from the graph. However,
Pereira?s mechanism incurs a log(n) overhead
for accessing the changes (where n is the
number of nodes in a graph), resulting in
an O(n logn) time algorithm. Our algorithm
runs in O(n) time.
6The results do not include the space consumed
by the scratch tables. However, these tables do not
consume more than 10 KB in total, and hence have
no significant impact on the results.
7Because the packed version has a variable node
size, structure sharing yielded less relative improve-
ments than when applied to the basic version. In
terms of number of nodes, though, the two results
were identical.
With respect to over and early copying (as
defined in (Tomabechi, 1991)), our algorithm
has the same characteristics as Tomabechi?s
algorithm. In addition, our algorithm allows
to postpone the copying of graphs until after
subsumption checks complete. This would re-
quire additional fields in the node structure
for Tomabechi?s algorithm.
Our algorithm allows sharing of grammar
nodes, which is usually impossible in other
implementations (Malouf et al, 2000). A
weak point of our structure sharing scheme
is its extra condition. However, our experi-
ments showed that this condition can have a
minor impact on the amount of sharing.
We showed that compressing node struc-
tures allowed us to reduce memory consump-
tion by another 40% without sacrificing per-
formance. Applying the same technique to
Tomabechi?s algorithm would yield smaller
relative improvements (max. 20%), because
the scratch fields cannot be compressed to the
same extent.
One of the design goals of Tomabechi?s al-
gorithm was to come to an efficient imple-
mentation of parallel unification (Tomabechi,
1991). Although theoretically parallel uni-
fication is hard (Vitter and Simons, 1986),
Tomabechi?s algorithm provides an elegant
solution to achieve limited scale parallelism
(Fujioka et al, 1990). Since our algorithm is
based on the same principles, it allows paral-
lel unification as well. Tomabechi?s algorithm,
however, is not thread-safe, and hence cannot
be used for concurrent unification.
6 Conclusions
We have presented a technique to reduce
memory usage by separating scratch fields
from nodes. We showed that compressing
node structures can further reduce the mem-
ory footprint. Although these techniques re-
quire extra computation, the algorithms still
run faster. The main reason for this was the
difference between cache and memory speed.
As current developments indicate that this
difference will only get larger, this effect is not
just an artifact of the current architectures.
We showed how to incoporate data-
structure sharing. For our grammar, the ad-
ditional constraint for sharing did not pose
a problem. If it does pose a problem, there
are several techniques to mitigate its effect.
For example, one could reserve additional in-
dexes at critical positions in a subgraph (e.g.
based on type information). These can then
be assigned to nodes in later unifications with-
out introducing conflicts elsewhere. Another
technique is to include a tiny table with re-
pair information in each share arc to allow a
small number of conflicts to be resolved.
For certain grammars, data-structure shar-
ing can also significantly reduce execution
times, because the equality check (see line 3 of
Unify1) can intercept shared nodes with the
same address more frequently. We did not ex-
ploit this benefit, but rather included an offset
check to allow grammar nodes to be shared as
well. One could still choose, however, not to
share grammar nodes.
Finally, we introduced deferred copying.
Although this technique did not improve per-
formance, we suspect that it might be benefi-
cial for systems that use more expensive mem-
ory allocation and deallocation models (like
garbage collection).
Since memory consumption is a major con-
cern with many of the current unification-
based grammar parsers, our approach pro-
vides a fast and memory-efficient alternative
to Tomabechi?s algorithm. In addition, we
showed that our algorithm is well suited for
concurrent unification, allowing to reduce ex-
ecution times as well.
References
[Fujioka et al1990] T. Fujioka, H. Tomabechi,
O. Furuse, and H. Iida. 1990. Parallelization
technique for quasi-destructive graph unifica-
tion algorithm. In Information Processing So-
ciety of Japan SIG Notes 90-NL-80.
[Ghosh et al1997] S. Ghosh, M. Martonosi, and
S. Malik. 1997. Cache miss equations: An
analytical representation of cache misses. In
Proceedings of the 11th International Confer-
ence on Supercomputing (ICS-97), pages 317?
324, New York, July 7?11. ACM Press.
[Malouf et al2000] Robert Malouf, John Carroll,
and Ann Copestake. 2000. Efficient feature
structure operations witout compilation. Nat-
ural Language Engineering, 1(1):1?18.
[op den Akker et al1995] R. op den Akker, H. ter
Doest, M. Moll, and A. Nijholt. 1995. Parsing
in dialogue systems using typed feature struc-
tures. Technical Report 95-25, Dept. of Com-
puter Science, University of Twente, Enschede,
The Netherlands, September. Extended version
of an article published in E...
[Pereira1985] Fernando C. N. Pereira. 1985. A
structure-sharing representation for unification-
based grammar formalisms. In Proc. of the
23 rd Annual Meeting of the Association for
Computational Linguistics. Chicago, IL, 8?12
Jul 1985, pages 137?144.
[Tomabechi1991] H. Tomabechi. 1991. Quasi-
destructive graph unifications. In Proceedings
of the 29th Annual Meeting of the ACL, Berke-
ley, CA.
[Tomabechi1992] Hideto Tomabechi. 1992. Quasi-
destructive graph unifications with structure-
sharing. In Proceedings of the 15th Interna-
tional Conference on Computational Linguis-
tics (COLING-92), Nantes, France.
[Tomabechi1995] Hideto Tomabechi. 1995. De-
sign of efficient unification for natural lan-
guage. Journal of Natural Language Process-
ing, 2(2):23?58.
[van Lohuizen1999] Marcel van Lohuizen. 1999.
Parallel processing of natural language parsers.
In PARCO ?99. Paper accepted (8 pages), to
appear soon.
[van Lohuizen2000] Marcel P. van Lohuizen. 2000.
Exploiting parallelism in unification-based
parsing. In Proc. of the Sixth International
Workshop on Parsing Technologies (IWPT
2000), Trento, Italy.
[Vitter and Simons1986] Jeffrey Scott Vitter and
Roger A. Simons. 1986. New classes for paral-
lel complexity: A study of unification and other
complete problems for P. IEEE Transactions
on Computers, C-35(5):403?418, May.
[Wroblewski1987] David A. Wroblewski. 1987.
Nondestructive graph unification. In Howard
Forbus, Kenneth; Shrobe, editor, Proceedings
of the 6th National Conference on Artificial In-
telligence (AAAI-87), pages 582?589, Seattle,
WA, July. Morgan Kaufmann.
A Generic Approach to Parallel Chart Parsing with an
Application to LinGO
Marcel van Lohuizen
Faculty of Information Technology and Systems Delft University of Technology
Delft, The Netherlands
mpvl@acm.org
Abstract
Multi-processor systems are becom-
ing more commonplace and afford-
able. Based on analyses of ac-
tual parsings, we argue that to ex-
ploit the capabilities of such ma-
chines, unification-based grammar
parsers should distribute work at the
level of individual unification oper-
ations. We present a generic ap-
proach to parallel chart parsing that
meets this requirement, and show
that an implementation of this tech-
nique for LinGO achieves consider-
able speedups.
1 Introduction
The increasing demand for accuracy and ro-
bustness for today?s unification-based gram-
mar parsers brings on an increasing demand
for computing power. In addition, as these
systems are increasingly used in applications
that require direct user interaction, e.g. web-
based applications, responsiveness is of major
concern. In the mean time, small-scale desk-
top multiprocessor systems (e.g. dual or even
quad Pentium machines) are becoming more
commonplace and affordable. In this paper
we will show that exploiting the capabilities
of these machines can speed up parsers con-
siderably, and can be of major importance in
achieving the required performance.
There are certain requirements the design
of a parallel parser should meet. Over the
past years, many improvements to existing
parsing techniques have boosted the perfor-
mance of parsers by many factors (Oepen and
Callmeier, 2000). If a design of a parallel
parser is tied too much to a particular ap-
proach to parsing, it may be hard to incorpo-
rate such improvements as they become avail-
able. For this reason, a solution to parallel
parsing should be as general as possible. One
obvious way to ensure that optimizations for
sequential parsers can be used in a parallel
parser as well is to let a parallel parser mimic
a sequential parser as much as possible. This
is basically the approach we will take.
The parser that we will present in this pa-
per uses the LinGO grammar. LinGO is an
HPSG-based grammar which was developed
at Stanford (Copestake, 2000). It is currently
used by many research institutions. This al-
lows our results to be compared with that of
other research groups.
In Section 2, we explore the possibilities for
parallelism in natural language parsing by an-
alyzing the computational structure of pars-
ings. Section 3 and 4 discuss respectively the
design and the performance of our system.
Finally, we compare our work with other re-
search on parallel parsing.
2 Analysis of Parsings
To analyze the possibilities for parallelism in
computations they are often represented as
task graphs. A task graph is a directed acyclic
graph, where the nodes represent some unit
of computation, called a task, and the arcs
represent the execution dependencies between
the tasks. Task graphs can be used to an-
alyze the critical path, which is the mini-
mal time required to complete a computa-
tion, given an infinite amount of processors.
From Brent (1974) and Graham (1969) we
know that there exist P -processor schedulings
where the execution time TP is bound as fol-
lows:
TP ? T1/P + T?, (1)
where T1 is the total work, or the execution
time for the one processor case, and T? is
the critical path. Furthermore, to effectively
use P processors, the average parallelism P? =
T1/T? should be larger than P .
The first step of the analysis is to find an
appropriate graph representation for parsing
computations. According to Caroll (1994),
performing a complexity analysis solely at the
level of grammars and parsing schemata can
give a distorted image of the parsing pro-
cess in practice. For this reason, we based
our analysis on actual parsings. The experi-
ments were based on the fuse test suite, which
is a balanced extract from four appointment
scheduling (spoken) dialogue corpora (incl.
VerbMobil). Fuse contains over 2000 sen-
tences with an average length of 11.6.
We define a task graph for a single pars-
ing computation as follows. First, we distin-
guish two types of tasks: unification tasks and
match tasks. A unification task executes a
single unification operation. A match task is
responsible for all the actions that are taken
when a unification succeeds: matching the
resulting edge with other edges in the chart
and putting resulting unification tasks on the
agenda. The match task is also responsible
for applying filtering techniques like the quick
check (Malouf et al, 2000). The tasks are
connected by directed arcs that indicate the
execution dependencies.
We define the cost of each unification task
as the number of nodes visited during the
unification and successive copying operation.
Unification operations are typically responsi-
ble for over 90% of the total work. In addi-
tion, the cost of the match tasks are spread
out over succeeding unification tasks. We
therefore simply neglect the cost for match op-
erations, and assume that this does not have a
significant impact on our measurements. The
length of a path in the graph can now be de-
fined as the sum of the costs of all nodes on
Figure 1: Task graphs for two different ap-
proaches to parallel chart parsing.
T1 T? P?
Type 1 1014247 3487 187
Average type 2 1014247 11004 54
Worst case type 2 1014247 69300 13
Table 1: Critical path analysis for type 1 and
type 2 task graphs (average and worst case).
the path. The critical path length T? can be
defined as the longest path between any two
nodes in the graph.
The presented model resembles a very fine-
grained scheme for distributing work, where
each single unification tasks to be scheduled
independently. In a straightforward imple-
mentation of such a scheme, the scheduling
overhead can become significant. Limiting
the scheduling overhead is crucial in obtaining
considerable speedup. It might therefore be
tempting to group related tasks into a single
unit of execution to mitigate this overhead.
For this reason we also analyzed a task graph
representation where only match tasks spawn
a new unit of execution. The top graph in
Figure 1 shows an example of a task graph
for the first approach. The bottom graph of
Figure 1 shows the corresponding task graph
for the second approach. Note that because
a unification task may depend on more than
one match task, a choice has to be made in
which unit of execution the unification task is
put.
Table 1 shows the results of the critical path
analysis of both approaches. For the first ap-
proach, the critical path is uniquely defined.
For the second approach we show both the
worst case, considering all possible schedul-
ings, and an average case. The results for T1,
T?, and P? are averaged over all sentences.1
The results show that, using the first ap-
proach, there is a considerable amount of par-
allelism in the parsing computations. The re-
sults also show that a small change in the de-
sign of a parallel parser can have a signifi-
cant impact on the value for P? . To obtain a
speedup of P , in practice, there should be a
safety margin between P and P? . This sug-
gests that the first approach is a considerably
saver choice, especially when one is consider-
ing using more than a dozen of processors.
3 Design and Implementation
Based on the discussion in the preceding sec-
tions, we can derive two requirements for the
design of a parallel parser: it should be close
in design to a sequential parser and it should
allow each single unification operation to be
scheduled dynamically. The parallel parser we
will present in this section meets both require-
ments.
Let us first focus on how to meet the first
requirement. Basically, we let each processor,
run a regular sequential parser augmented
with a mechanism to combine the results of
the different parsers. Each sequential parser
component is contained in a different thread.
By using threads, we allow each parser to
share the same memory space. Initially, each
thread is assigned a different set of work, for
example, resembling a different part of the in-
put string. A thread will process the unifica-
tion tasks on the agenda and, on success, will
perform the resulting match task to match the
new edge with the edges on its chart. After
completing the work on its agenda, a thread
will match the edges on its chart with the
edges derived so far by the other threads. This
may produce new unification tasks, which the
thread puts on its agenda. After the commu-
nication phase is completed, it returns to nor-
mal parsing mode to execute the work on its
agenda. This process continues until all edges
1Note that since
?
T1/
?
T? 6=
?
T1/T?, the re-
sults for P? turn out slightly lower than might have
been expected from the values of T1 and T?.
Figure 2: Architecture of MACAMBA.
of all threads have been matched against each
other and all work has been completed.
3.1 Data Structures
Figure 2 shows an outline of our approach in
terms of data structures. Each thread con-
tains an agenda, which can be seen as a queue
of unification tasks, a chart, which stores the
derived edges, and a heap, which is used to
store the typed-feature structures that are ref-
erenced by the edges. Each thread has full ac-
cess to its own agenda, chart, and heap, and
has read-only access to the respective struc-
tures of all other threads. Grammars are
read-only and can be read by all threads.
In the communication phase, threads need
read-only access to the edges derived by other
threads. This is especially problematic for
the feature structures. Many unification al-
gorithms need write access to scratch fields
in the graph structures. Such algorithms are
therefore not thread-safe.2 For this reason we
use the thread-safe unification algorithm pre-
sented by Van Lohuizen (2000), which is com-
parable in performance to Tomabechi?s algo-
rithm (Tomabechi, 1991).
Note that each thread also has its own
agenda. Some parsing systems require strict
control over the order of evaluation of tasks.
The distributed agendas that we use in our
approach may make it hard to implement such
a strict control. One solution to the problem
would be to use a centralized agenda. The dis-
advantage of such a solution is that it might
increase the synchronization overhead. Tech-
niques to reduce the synchronization overhead
2In this context, thread safe means that the same
data structure can be involved in more than one op-
eration, of more than one thread, simultaneously.
global shared NrThreadsIdle, Generation, IdleGen
Sched()
var threadGen, newWork, isIdle
threadGen?Generation?Generation+1
while NrThreadsIdle 6= P do
1. newWork? not IsEmpty(agenda).
2. Process the agenda as in the sequential
case. In addition, stamp each newly de-
rived I edge by setting I.generation to the
current value for threadGen and add I to this
thread?s edge list.
3. Examine all the other threads for newly
derived edges. For each new edge I and for
each edge J on the chart for which holds
I.generation > J.generation, add the cor-
responding task to the agenda if it passes
the filter. If any edge was processed, set
newWork to true.
4. if not newWork then
newWork?Steal()
5. lock GlobalLock
6. if newWork then
Generation? Generation + 1
threadGen? Generation
NrThreadsIdle? 0
7. else
if Generation 6= IdleGen then
isIdle? false
Generation? Generation + 1
threadGen? IdleGen? Generation
elseif threadGen 6= IdleGen then
isIdle? false
threadGen? IdleGen
elseif not isIdle then
isIdle? true
NrThreadsIdle? NrThreadsIdle + 1
8. unlock GlobalLock
Figure 3: Scheduling algorithm.
in such a setup can be found in (Markatos and
LeBlanc, 1992).
3.2 Scheduling Algorithm
At startup, each thread calls the scheduling
algorithm shown in Figure 3. This algorithm
can be seen as a wrapper around an existing
sequential parser that takes care of combin-
ing the results of the individual threads. The
functionality of the sequential parser is em-
bedded in step 2. After this step, the agenda
will be empty. The communication between
threads takes place in step 3. Each time a
thread executes this step, it will proceed over
all the newly derived edges of other threads
(foreign edges) and match them with the
edges on its own chart (local edges). Checking
the newly derived edges of other threads can
simply be done by proceeding over a linked list
of derived edges maintained by the respective
threads. Threads record the last visited edge
of the list of each other thread. This ensures
that each newly derived item needs to be vis-
ited only once by each thread.
As a result of step 3, the agenda may be-
come non-empty. In this case, newWork will
be set and step 2 is executed again. This cycle
continues until all work is completed.
The remaining steps serve several purposes:
load balancing, preventing double work, and
detecting termination. We will explain each of
these aspects in the following sections. Note
that step 6 and 7 are protected by a lock.
This ensures that no two threads can execute
this code simultaneously. This is necessary
because Step 6 and 7 write to variables that
are shared amongst threads. The overhead
incurred by this synchronization is minimal,
as a thread typically iterates over this part
only a small number of times. This is because
the depth of the derivation graph of any edge
is limited (average 14, maximum 37 for the
fuse test set).
3.3 Work Stealing
In the design as presented so far, each thread
exclusively executes the unification tasks on
its agenda. Obviously, this violates the re-
quirement that each unification task should
be scheduled dynamically.
In (Blumofe and Leiserson, 1993), it is
shown that for any multi-threaded compu-
tation with work T1 and task graph depth
T?, and for any number P of processors, a
scheduling will achieve TP ? T1/P +T? if for
the scheduling holds that whenever there are
more than P tasks ready, all P threads are
executing work. In other words, as long as
there is work on any queue, no thread should
be idle.
An effective technique to ensure the above
requirement is met is work stealing (Frigo et
al., 1998). With this technique, a thread will
first attempt to steal work from the queue
of another thread before denouncing itself to
be idle. If it succeeds, it will resume nor-
mal execution as if the stolen tasks were its
own. Work stealing incurs less synchroniza-
tion overhead than, for example, a centralized
work queue.
In our implementation, a thread becomes a
thief by calling Steal, at step 4 of Sched.
Steal allows stealing from two types of
queues: the agendas, which contain outstand-
ing unification tasks, and the unchecked for-
eign edges, which resemble outstanding match
tasks between threads.
A thief first picks a random victim to steal
from. It first attempts to steal the victim?s
match tasks. If it succeeds, it will perform
the matches and put any resulting unification
tasks on its own agenda. If it cannot gain
exclusive access to the lists of unchecked for-
eign edges, or if there were no matches to be
performed, it will attempt to steal work from
the victim?s agenda. A thief will steal half of
the work on the agenda. This balances the
load between the two threads and minimizes
the chance that either thread will have to call
the expensive steal operation soon thereafter.
Note that idle threads will keep calling Steal
until they either obtain new work or all other
threads become idle.
Obviously, stealing eliminates the exclusive
ownership of the agenda and unchecked for-
eign edge lists of the respective threads. As a
consequence, a thread needs to lock its agenda
and edge lists each time it needs to access
it. We use an asymmetric mutual exclusion
scheme, as presented in (Frigo et al, 1998), to
minimize the cost of locking for normal pro-
cessing and move more of the overhead to the
side of the thief.
3.4 Preventing Duplicate Matches
When two matching edges are stored on the
charts of two different threads, it should be
prevented that both threads will perform the
corresponding match. Failing to do so can
cause the derivation of duplicate edges and
eventually a combinatorial explosion of work.
Our solution is based on a generation scheme.
Each newly derived edge is stamped with the
current generation of the respective thread,
threadGen (see step 2). In addition, a thread
will only perform the match for two edges if
the edge on its chart has a lower generation
than the foreign edge (see step 3). Obviously,
because the value of threadGen is unique for
the thread (see step 6), this scheme prevents
two edges from being matched twice.
Sched also ensures that two matching
edges will always be matched by at least one
thread. After a thread completes step 3, it
will always raise its generation. The new gen-
eration will be greater than that of any for-
eign edge processed before. This ensures that
when an edge is put on the chart, no for-
eign edge with a higher generation has been
matched against the respective chart before.
3.5 Termination
A thread may terminate when all work is com-
pleted, that is, if and only if the following
conditions hold simultaneously: all agendas
of all threads are empty, all possible matches
between edges have been processed, and all
threads are idle. Step 7 of Sched enforces
that these conditions hold before any thread
leaves Sched. Basically, each thread deter-
mines for itself whether its queues are empty
and raises the global counter NrThreadsIdle
accordingly. When all threads are idle simul-
taneously, the parser is finished.
A thread?s agenda is guaranteed to be
empty whenever newWork is false at step 7.
The same does not hold for the unchecked
foreign edges. Whenever a thread derives a
new edge, all other edges need to perform the
corresponding matches. The following mecha-
nism enforces this. The first thread to become
idle raises the global generation and records
it in IdleGen. Subsequent idle threads will
adopt this as their idle generation. When-
ever a thread derives a new edge, it will raise
Generation and reset NrThreadsIdle (step 6).
This invalidates IdleGen which implicitly re-
moves the idle status from all threads. Note
that step 7 lets each thread perform an addi-
tional iteration before raising NrThreadsIdle.
This allows a thread to check for foreign edges
that were derived after step 3 and before 7.
Once all work is done, detecting termination
P TP (s) speedup
1 1599.8 1
2 817.5 1.96
3 578.2 2.77
4 455.9 3.51
5 390.3 4.10
6 338.0 4.73
Table 2: Execution times for the fuse test
suite for various number of processors.
requires at most 2P synchronization steps.3
3.6 Implementation
The implementation of the system con-
sists of two parts: MACAMBA and CaLi.
MACAMBA stands for Multi-threading Ar-
chitecture for Chart And Memoization-Based
Applications. The MACAMBA framework
provides a set of objects that implement the
scheduling technique presented in the previ-
ous section. It also includes a set of sup-
port objects like charts and a thread-safe uni-
fication algorithm. CaLi is an instance of a
MACAMBA application that implements a
Chart parser for the LinGO grammar. The
design of CaLi was based on PET (Callmeier,
2000), one of the fastest parsers for LinGO.
It implements the quick check (Malouf et al,
2000), which, together with the rule check,
takes care of filtering over 90% of the failing
unification tasks before they are put on the
agenda. MACAMBA and CaLi were both im-
plemented in Objective-C and currently run
on Windows NT, Linux, and Solaris.
4 Performance Results
The performance of the sequential version of
CaLi is comparable to that of PET.4 In ad-
dition, for the single-processor parallel ver-
sion of CaLi the total overhead incurred by
scheduling is less than 1%.
The first set of experiments consisted of
running the fuse test suite on a SUN Ultra
Enterprise with 8 nodes, each with a 400 MHz
3No locking is required once a thread is idle.
4Respectively, 1231s and 1339s on a 500MHz P-III,
where both parsers used the same parsing schema.
UltraSparc processor, for a varying number of
processors. Table 2 shows the results of these
experiments.5 The execution times for each
parse are measured in wall clock time. The
time measurement of a parse is started be-
fore the first thread starts working and ends
only when all threads have stopped. The fuse
test suite contains a large number of small
sentences that are hard to parallelize. These
results indicate that deploying multiple pro-
cessors on all input sentences unconditionally
still gives a considerable overall speedup.
The second set of experiments were run on
a SUN Enterprise10000 with 64 250 MHz Ul-
traSparc II processors. To limit the amount of
data generated by the experiments, and to in-
crease the accuracy of the measurements, we
selected a subset of the sentences in the fuse
suite. The parser is able to parse many sen-
tences in the fuse suite in fewer than several
milliseconds. Measuring speedup is inaccu-
rate in these cases. We therefore eliminated
such sentences from the test suite. From the
remaining sentences we made a selection of
500 sentences of various lengths.
The results are shown in Figure 4. The fig-
ure includes a graph for the maximum, mini-
mum, and average speedup obtained over all
sentences. The maximum speedup of 31.4 is
obtained at 48 processors. The overall peak
is reached at 32 processors where the average
speedup is 17.3. One of the reasons for the
decline in speedup after 32 processors is the
overhead in the scheduling algorithm. Most
notably, the total number of top-level itera-
tions of Sched increases for larger P . The
minimum speedups of around 1 are obtained
for, often small, sentences that contain too lit-
tle inherent parallelism to be parallelized ef-
fectively.
Figure 4 shows a graph of the parallel ef-
ficiency, which is defined as speedup divided
by the number of processors. The average ef-
ficiency remains close to 80% up till 16 pro-
cessors. Note that super linear speedup is
achieved with up to 12 processors, repeat-
edly for the same set of sentences. Super lin-
5Because the system was shared with other users,
only 6 processors could be utilized.
Figure 4: Average, maximum, and minimum
speedup and parallel efficiency based on wall
clock time.
ear speedup can occur because increasing the
number of processors also reduces the amount
of data handled by each node. This reduces
the chance of cache misses.
5 Related Work
Parallel parsing for NLP has been researched
extensively. For example, Thompson (1994)
presented some implementations of parallel
chart parsers. Nijholt (1994) gives a more the-
oretical overview of parallel chart parsers. A
survey of parallel processing in NLP is given
by Adriaens and Hahn (1994).
Nevertheless, many of the presented solu-
tions either did not yield acceptable speedup
or were very specific to one application. Re-
cently, several NLP systems have been par-
allelized successfully. Pontelli et al (1998)
show how two existing NLP applications were
successfully parallelized using the parallel
Prolog environment ACE. The disadvantage
of this approach, though, is that it can only
be applied to parsers developed in Prolog.
Manousopoulou et al (1997) discuss a par-
allel parser generator based on the Eu-PAGE
system. This solution exploits coarse-grained
parallelism of the kind that is unusable for
many parsing applications, including our own
(see also Go?rz et. al. (1996)).
Nurkkala et al (1994) presented a parallel
parser for the UPenn TAG grammar, imple-
mented on the nCUBE. Although their best
results were obtained with random grammars,
speedups for the English grammar were also
considerable.
Yoshida et. al. (Yoshida et al, 1999) pre-
sented a 2-phase parallel FB-LTAG parser,
where the operations on feature structures
are all performed in the second phase. The
speedup ranged up to 8.8 for 20 processors,
Parallelism is mainly thwarted by a lack of
parallelism in the first phase.
Finally, Ninomiya et al (2001) developed
an agent-based parallel parser that achieves
speedups of up to 13.2. It is implemented
in ABCL/f and LiLFeS. They also provide a
generic solution that could be applied to many
parsers. The main difference with our system
is the distribution of work. This system uses
a tabular chart like distribution of matches
and a randomized distribution of unification
tasks. Experiments we conducted show that
the choice of distribution scheme can have a
significant influence on the cache utilization.
It should be mentioned, though, that it is
in general hard to compare the performance
of systems when different grammars are used.
On the scheduling side, our approach shows
close resemblance to the Cilk-5 system (Frigo
et al, 1998). It implements work stealing
using similar techniques. An important dif-
ference, though, is that our scheduler was
designed for chart parsers and tabular algo-
rithms in general. These types of applications
fall outside the class of applications that Cilk
is capable of handling efficiently.
6 Conclusions
We showed that there is sufficient parallelism
in parsing computations and presented a par-
allel chart parser for LinGO that can effec-
tively exploit this parallelism by achieving
considerable speedups. Also, the presented
techniques do not rely on a particular parsing
schema or grammar formalism, and can there-
fore be useful for other parsing applications.
Acknowledgements
Thanks to Makino Takaki and Takashi Ni-
nomiya of the Department of Information Sci-
ence, University of Tokyo, for running the
1?64 processor experiments at their depart-
ment?s computer.
References
[Adriaens and Hahn1994] Geert Adriaens and Udo
Hahn, editors. 1994. Parallel Natural Lan-
guage Processing. Ablex Publishing Corpora-
tion, Norwood, New Jersey.
[Blumofe and Leiserson1993] Robert D. Blumofe
and Charles E. Leiserson. 1993. Space-
efficient scheduling of multithreaded computa-
tions. In Proceedings of the Twenty-Fifth An-
nual ACM Symposium on the Theory of Com-
puting (STOC ?93), pages 362?371, San Diego,
CA, USA, May. Also submitted to SIAM Jour-
nal on Computing.
[Brent1974] Richard P. Brent. 1974. The paral-
lel evaluation of general arithmetic expressions.
Journal of the ACM, 21(2):201?206, April.
[Callmeier2000] Ulrich Callmeier. 2000. PET ?
A platform for experimentation with efficient
HPSG. Natural Language Engineering, 6(1):1?
18.
[Caroll1994] John Caroll. 1994. Relating complex-
ity to practical performance in parsing with
wide-coverage unification grammars. In Proc.
of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 287?294,
Las Cruces, NM, June27?30.
[Copestake2000] Ann Copestake, 2000. The (new)
LKB system, version 5.2. from Stanford site.
[Frigo et al1998] Matteo Frigo, Charles E. Leiser-
son, and Keigh H. Randall. 1998. The im-
plementation of the Cilk-5 multithreaded lan-
guage. ACM SIGPLAN Notices, 33(5):212?
223, May.
[Go?rz et al1996] Gu?nther Go?rz, Marcus Kesseler,
Jo?rg Spilker, and Hans Weber. 1996. Research
on architectures for integrated speech/language
systems in Verbmobil. In The 16th Interna-
tional Conference on Computational Linguis-
tics, volume 1, pages 484?489, Copenhagen,
Danmark, August5?9.
[Graham1969] R.L. Graham. 1969. Bounds on
multiprocessing timing anomalies. SIAM J.
Appl. Math., 17(2):416?429.
[Malouf et al2000] Robert Malouf, John Carroll,
and Ann Copestake. 2000. Efficient feature
structure operations witout compilation. Natu-
ral Language Engineering, 6(1):1?18.
[Manousopoulou et al1997] A.G. Manousopoulou,
G. Manis, P. Tsanakas, and G. Papakonstanti-
nou. 1997. Automatic generation of portable
parallel natural language parsers. In Proceed-
ings of the 9th Conference on Tools with Arti-
ficial Intelligence (ICTAI ?97), pages 174?177.
IEEE Computer Society Press.
[Markatos and LeBlanc1992] E. P. Markatos and
T. J. LeBlanc. 1992. Using processor affinity
in loop scheduling on shared-memory multipro-
cessors. In IEEE Computer Society. Technical
Committee on Computer Architecture, editor,
Proceedings, Supercomputing ?92: Minneapo-
lis, Minnesota, November 16-20, 1992, pages
104?113, 1109 Spring Street, Suite 300, Silver
Spring, MD 20910, USA. IEEE Computer So-
ciety Press.
[Nijholt1994] Anton Nijholt. 1994. Parallel ap-
proaches to context-free language parsing. In
Adriaens and Hahn (1994).
[Ninomiya et al2001] Takashi Ninomiya, Kentaro
Torisawa, and Jun?ichi Tsujii. 2001. An agent-
based parallel HPSG parser for shared-memory
parallel machines. Journal of Natural Language
Processing, 8(1), January.
[Nurkkala and Kumar1994] Tom Nurkkala and
Vipin Kumar. 1994. A parallel parsing algo-
rithm for natural language using tree adjoining
grammar. In Howard Jay Siegel, editor, Pro-
ceedings of the 8th International Symposium
on Parallel Processing, pages 820?829, Los
Alamitos, CA, USA, April. IEEE Computer
Society Press.
[Oepen and Callmeier2000] Stephan Oepen and
Ulrich Callmeier. 2000. Measure for mea-
sure: Parser cross-fertilization. In Proceedings
sixth International Workshop on Parsing Tech-
nologies (IWPT?2000), pages 183?194, Trento,
Italy.
[Pontelli et al1998] Enrico Pontelli, Gopal Gupta,
Janyce Wiebe, and David Farwell. 1998. Natu-
ral language multiprocessing: A case study. In
Proceedings of the 15th National Conference on
Artifical Intelligence (AAAI ?98), July.
[Thompson1994] Henry S. Thompson. 1994. Par-
allel parsers for context-free grammars?two ac-
tual implementations compared. In Adriaens
and Hahn (1994).
[Tomabechi1991] H. Tomabechi. 1991. Quasi-
destructive graph unifications. In Proceedings
of the 29th Annual Meeting of the ACL, Berke-
ley, CA.
[van Lohuizen2000] Marcel P. van Lohuizen.
2000. Memory-efficient and thread-safe quasi-
destructive graph unification. In Proceedings
of the 38th Meeting of the Association for
Computational Linguistics, Hong Kong, China.
[Yoshida et al1999] Minoru Yoshida, Takashi Ni-
nomiya, Kentaro Torisawa, Takaki Makino, and
Jun?ichi Tsujii. 1999. Proceedings of efficient
FB-LTAG parser and its parallelization. In Pro-
ceedings of Pacific Association for Computa-
tional Linguistics ?99, pages 90?103, Waterloo,
Canada, August.
