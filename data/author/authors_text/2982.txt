Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 45?48,
New York, June 2006. c?2006 Association for Computational Linguistics
A Finite-State Model of Georgian Verbal Morphology
Olga Gurevich
Department of Linguistics
University of California, Berkeley
3834 23rd Street
San Francisco, CA 94114
olya.gurevich@gmail.com
Abstract
Georgian is a less commonly studied lan-
guage with complex, non-concatenative
verbal morphology. We present a compu-
tational model for generation and recogni-
tion of Georgian verb conjugations, rely-
ing on the analysis of Georgian verb struc-
ture as a word-level template. The model
combines a set of finite-state transducers
with a default inheritance mechanism.1
1 Introduction
Georgian morphology is largely synthetic, with
complex verb forms that can often express the mean-
ing of a whole sentence. Descriptions of Georgian
verbal morphology emphasize the large number of
inflectional categories; the large number of elements
that a verb form can contain; the inter-dependencies
in the occurrence of various elements; and the large
number of regular, semi-regular, and irregular pat-
terns of formation of verb inflections (cf. Hewitt
1995). All of these factors make computational
modeling of Georgian morphology a rather daunting
task.
In this paper, we propose a computational model
for parsing and generation of a subset of Georgian
verbs that relies on a templatic, word-based analysis
of the verbal system rather than assuming compo-
sitional rules for combining individual morphemes.
We argue that such a model is viable, extensible, and
1This work was in part supported by the Berkeley Language
Center. I?d like to thank Lauri Karttunen for introducing me to
finite-state morphology and providing an updated version of the
software, and Shorena Kurtsikidze and Vakhtang Chikovani for
help with the Georgian data. All errors are my own.
capable of capturing the generalizations inherent in
the Georgian verbal system at various levels of reg-
ularity. To our knowledge, this is the only computa-
tional model of the Georgian verb currently in active
development and available to the non-Georgian aca-
demic community2.
2 Georgian Verbal Morphology
The Georgian verb forms are made up of several
kinds of morphological elements that recur in dif-
ferent formations. These elements can be formally
identified in a fairly straightforward fashion; how-
ever, their function and distribution defy a simple
compositional analysis but instead are determined
by the larger morphosyntactic and semantic contexts
in which the verbs appear (usually tense, aspect, and
mood) and the lexical properties of the verbs them-
selves.
2.1 Verb Structure
Georgian verbs are often divided into four conju-
gation classes, based mostly on valency (cf. Har-
ris 1981). In this brief report, we will concentrate
on transitive verbs, although our model can accom-
modate all four conjugation types. Verbs inflect
in tense/mood/aspect (TAM) paradigms (simplified
here as tenses). There are a total of 10 actively used
tenses in Modern Georgian, grouped into TAM se-
ries as in Table 1. Knowing the series and tense of a
verb form is essential for being able to conjugate it.
The structure of the verb can be described using
the following (simplified) template.
2See Tandashvili (1999) for an earlier model. Unfortunately,
the information in the available publications does not allow for
a meaningful comparison with the present model.
45
Series Tense 2SGSUBJ:3SGOBJ
PRESENT xat?-av
IMPERFECT xat?-av-di
PRES. SUBJ. xat?-av-de
FUTURE da-xat?-av
CONDITIONAL da-xat?-av-di
I
FUT. SUBJ. da-xat?-av-de
AORIST da-xat?-eII
AOR. SUBJ. da-xat?-o
PERFECT da-gi-xat?-av-sIII
PLUPERFECT da-ge-xat?-a
Table 1: Tenses of the verb ?to paint?. Root is in bold.
(Preverb)-(agreement1)-(version)-root-(thematic
suffix)-(tense)-(agreement)
The functions of some of the elements are dis-
cussed below. As an illustration, note the formation
of the verb xat?va ?paint? in Table 1.
2.2 Lexical and Semi-Regular Patterns
The complexity of the distribution of morphologi-
cal elements in Georgian is illustrated by preverbs,
thematic suffixes, and tense endings. The preverbs
(a closed class of about 8) indicate perfective aspect
and lexical derivations from roots, similar to verb
prefixes in Slavic or German. The association of a
verb with a particular preverb is lexical and must be
memorized. A preverb appears on forms from the
Future subgroup of series I, and on all forms of se-
ries II and III in transitive verbs. Table 2 demon-
strates some of the lexically-dependent morpholog-
ical elements, including several different preverbs
(row ?Future?).
Similarly, thematic suffixes form a closed class
and are lexically associated with verb roots. They
function as stem formants and distinguish inflec-
tional classes. In transitive verbs, thematic suffixes
appear in all series I forms. Their behavior in other
series differs by individual suffix: in series II, most
suffixes disappear, though some seem to leave par-
tial ?traces? (rows ?Present? and ?Perfect? in Table
2).
The next source of semi-regular patterns comes
from the inflectional endings in the individual tenses
and the corresponding changes in some verb roots
(row ?Aorist? in Table 2).
Finally, another verb form relevant for learners is
the masdar, or verbal noun. The masdar may or may
?Bring? ?Paint? ?Eat?
Present i-gh-eb-s xat?-av-s ch?am-?-s
Future c?amo-i-gh-eb-s da-xat?-av-s she-ch?am-s
Aorist c?amo-i-gh-o da-xat?-a she-ch?am-a
Perfect c?amo-u-gh-ia da-u-xat?-av-s she-u-ch?am-ia
Masdar c?amo-gh-eb-a da-xat?-v-a ch?-am-a
Table 2: Lexical Variation. Roots are in bold; lexically vari-
able affixes are in italics.
OBJSUBJ 1SG 1PL 2SG 2PL 3
1SG ? ? g?? g?t v??
1PL ? ? g?t g?t v?t
2SG m?? gv?? ? ? ???
2PL m?t gv?t ? ? ?t
3SG m?* gv?* g?* g?t ?*
3PL m?** gv?** g?** g?** ?**
Table 3: Subject/Object agreement. The 3sg and 3pl suffixes,
marked by * and **, are tense-dependent.
not include the preverb and/or some variation of the
thematic suffix (last row in Table 2).
2.3 Regular Patterns
Verb agreement in Georgian is a completely regu-
lar yet not entirely compositional phenomenon. A
verb can mark agreement with both the subject and
the object via a combination of prefixal and suffixal
agreement markers, as in Table 3.
The distribution and order of attachment of agree-
ment affixes has been the subject of much discus-
sion in theoretical morphological literature. To sim-
plify matters for the computational model, we as-
sume here that the prefixal and suffixal markers at-
tach to the verb stem at the same time, as a sort
of circumfix, and indicate the combined subject and
object properties of a paradigm cell.
Despite the amount of lexical variation, tense for-
mation in some instances is also quite regular. So,
the Imperfect and First Subjunctive tenses are regu-
larly formed from the Present. Similarly, the Condi-
tional and Future Subjunctive are formed from the
Future. And for most (though not all) transitive
verbs, the Future is formed from the Present via the
addition of a preverb.
Additionally, the number of possible combina-
tions of inflectional endings and other irregularities
is also finite, and some choices tend to predict other
choices in the paradigm of a given verb. Georgian
verbs can be classified according to several example
46
paradigms, or inflectional (lexical) classes, similar
to the distinctions made in Standard European lan-
guages; the major difference is that the number of
classes is much greater in Georgian. For instance,
Melikishvili (2001) distinguishes over 60 classes, of
which 17 are transitive. While the exact number of
inflectional classes is still in question, the general
example-based approach seems the only one viable
for Georgian.
3 Computational Model
3.1 Overview
Finite-state networks are currently one of the
most popular methods in computational morphol-
ogy. Many approaches are implemented as two-way
finite-state transducers (FST) in which each arc cor-
responds to a mapping of two elements, for exam-
ple a phoneme and its phonetic realization or a mor-
pheme and its meaning. As a result, FST morpholo-
gies often assume morpheme-level compositional-
ity. As demonstrated in the previous section, such
assumptions do not serve well to describe the ver-
bal morphology of Georgian. Instead, it can be de-
scribed as a series of patterns at various levels of
regularity. However, compositionality is not a neces-
sary assumption: finite-state models are well-suited
for representing mappings from strings of meaning
elements to strings of form elements without neces-
sarily pairing them one-to-one.
Our model was implemented using the xfst pro-
gram included in (Beesley and Karttunen 2003). The
core of the model consists of several levels of finite-
state transducer (FST) networks such that the result
of compiling a lower-level network serves as input to
a higher-level network. The levels correspond to the
division of templatic patterns into completely lexical
(Level 1) and semi-regular (Level 2). Level 3 con-
tains completely regular patterns that apply to the
results of both Level 1 and Level 2. The regular-
expression patterns at each level are essentially con-
straints on the templatic structure of verb forms at
various levels of generality. The FST model can be
used both for the generation of verbal inflections and
for recognition of complete forms.
The input to the model is a set of hand-written
regular expressions (written as FST patterns) which
identify the lexically specific information for a rep-
resentative of each verb class, as well as the more
regular rules of tense formation. In addition to divid-
ing verb formation patterns into lexical and regular,
our model also provides a mechanism for specifying
defaults and overrides in inflectional markers. Many
of the tense-formation patterns mentioned above can
be described as defaults with some lexical excep-
tions. In order to minimize the amount of manual
entry, we specify the exceptional features at the first
level and use the later levels to apply default rules in
all other cases.
3.2 Level 1: The Lexicon
The first level of the FST model contains lexically
specific information stored as several complete word
forms for each verb. In addition to the information
that is always lexical (such as the root and preverb),
this network also contains forms which are excep-
tional. For the most regular verbs, these are: Present,
Future, Aorist 2SgSubj, Aorist 3SgSubj, and Per-
fect.
The inflected forms are represented as two-level
finite-state arcs, with the verb stem and morphosyn-
tactic properties on the upper side, and the inflected
word on the lower side.
The forms at Level 1 contain a place holder
?+Agr1? for the prefixal agreement marker, which
is replaced by the appropriate marker in the later
levels (necessary because the prefixal agreement is
between the preverb and the root).
3.3 Level 2: Semi-regular Patterns
The purpose of Level 2 is to compile inflectional
forms that are dependent on other forms (introduced
in Level 1), and to provide default inflections for reg-
ular tense formation patterns.
An example of the first case is the Conditional
tense, formed predictably from the Future tense. The
FST algorithm is as follows:
? Compile a network consisting of Future forms.
? Add the appropriate inflectional suffixes.
? Replace the tense property ?+Fut? with
?+Cond?.
? Add the inflectional properties where needed.
An example of the second case is the Present
3PlSubj suffix, which is -en for most transitive verbs,
but -ian for a few others (see Fig. 1). Xfst provides a
simplified feature unification mechanism called flag
47
Lev. 1 paint+Pres
xat?-av
paint+Aor
da-xat?-a
open+PresPl
xsn-ian
Lev. 2 paint+Past+3Sg
xat?-av-da
paint+Pres+3Pl
xat?-av-en
default
overridden
Lev. 3 paint+3PlSubj+1SgObj
m-xat?-av-en
open+3PlSubj+1SgObj
m-xsn-ian
Figure 1: Verbs ?paint? and ?open? at three levels of the model.
New information contributed by each form is in bold.
diacritics. Using these flags, we specify exceptional
forms in Level 1, so that default inflections do not
apply to them in Level 2.
The patterns defined at Level 2 are compiled into
a single network, which serves as input to Level 3.
3.4 Level 3: Regular Patterns
The purpose of Level 3 is to affix regular inflection:
object and non-3rd person subject agreement. As
described in section 2, agreement in Georgian is ex-
pressed via a combination of a pre-stem affix and
a suffix, which are best thought of as attaching si-
multaneously and working in tandem to express both
subject and object agreement. Thus the compilation
of Level 3 consists of several steps, each of which
corresponds to a paradigm cell.
The operation of the model is partially illustrated
on forms of the verbs ?paint? and ?open? in Figure 1.
3.5 Treatment of Lexical Classes
The input to Level 1 contains a representative for
each lexical class, supplied with a diacritic feature
indicating the class number. Other verbs that belong
to those classes could, in principle, be inputted along
with the class number, and the FST model could
substitute the appropriate roots in the process of
compiling the networks. However, there are several
challenges to this straightforward implementation.
Verbs belonging to the same class may have dif-
ferent preverbs, thus complicating the substitution.
For many verbs, tense formation involves stem alter-
nations such as syncope or vowel epenthesis, again
complicating straightforward substitution. Supple-
tion is also quite common in Georgian, requiring
completely different stems for different tenses.
As a result, even for a verb whose lexical class is
known, several pieces of information must be sup-
plied to infer the complete inflectional paradigm.
The FST substitution mechanisms are fairly re-
stricted, and so the compilation of new verbs is done
in Java. The scripts make non-example verbs look
like example verbs in Level 1 of the FST network by
creating the necessary inflected forms, but the hu-
man input to the scripts need only include the infor-
mation necessary to identify the lexical class of the
verb.
4 Evaluation and Future Work
At the initial stages of modeling, we have concen-
trated on regular transitive verbs and frequent irreg-
ular verbs. The model currently contains several
verbs from each of the 17 transitive verb classes
mentioned in (Melikishvili 2001), and a growing
number of frequent irregular verbs from different
conjugation classes. Regular unaccusative, unerga-
tive, and indirect verbs will be added in the near fu-
ture, with the goal of providing full inflections for
200 most frequent Georgian verbs.
The model serves as the basis for an online
learner?s reference for Georgian conjugations (Gure-
vich 2005), which is the only such reference cur-
rently available.
A drawback of most finite-state models is their in-
ability to generalize to novel items the way a human
could. However, the output of our finite-state model
could potentially be used to generate training sets for
connectionist or statistical models.
References
Beesley, Kenneth and Lauri Karttunen. 2003. Finite-
State Morphology. Cambridge University Press.
Gurevich, Olga. 2005. Computing non-
concatenative morphology: The case of georgian.
In LULCL 2006. Bolzano, Italy.
Harris, Alice C. 1981. Georgian syntax: a study in
relational grammar. Cambridge University Press.
Hewitt, B. G. 1995. Georgian: a structural refer-
ence grammar. John Benjamins.
Melikishvili, Damana. 2001. Kartuli zmnis ugh-
lebis sist?ema [Conjugation system of the Geor-
gian verb]. Logos presi.
Tandashvili, M. 1999. Main Principles of
Computer-Aided Modeling, http://titus.uni-
frankfurt.de/personal/manana/refeng.htm. Tbilisi
Habilitation.
48
Proceedings of NAACL HLT 2007, Companion Volume, pages 49?52,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Document Similarity Measures to Distinguish  
Native vs. Non-Native Essay Writers 
Olga Gurevich 
Educational Testing Service 
Rosedale & Carter Roads,  
Turnbull 11R 
Princeton, NJ 08541 
ogurevich@ets.org 
Paul Deane 
Educational Testing Service 
Rosedale & Carter Roads,  
Turnbull 11R 
Princeton, NJ 08541 
pdeane@ets.org 
 
Abstract 
The ability to distinguish statistically dif-
ferent populations of speakers or writers 
can be an important asset in many NLP 
applications.  In this paper, we describe a 
method of using document similarity 
measures to describe differences in be-
havior between native and non-native 
speakers of English in a writing task.1 
1 Introduction 
The ability to distinguish statistically different 
populations of speakers or writers can be an impor-
tant asset in many NLP applications.  In this paper, 
we describe a method of using document similarity 
measures to describe differences in behavior be-
tween native and non-native speakers of English in 
a prompt response task. 
We analyzed results from the new TOEFL inte-
grated writing task, described in the next section.  
All task participants received the same set of 
prompts and were asked to summarize them.  The 
resulting essays are all trying to express the same 
?gist? content, so that any measurable differences 
between them must be due to differences in indi-
vidual language ability and style.  Thus the task is 
uniquely suited to measuring differences in linguis-
tic behavior between populations. 
Our measure of document similarity, described 
in section 3, is a combination of word overlap and 
syntactic similarity, also serving as a measure of 
syntactic variability.  The results demonstrate sig-
nificant differences between native and non-native 
                                                        
1 This research was funded while the first author was a Re-
search Postdoctoral Fellow at ETS in Princeton, NJ. 
speakers that cannot be attributed to any demo-
graphic factor other than the language ability itself. 
2 TOEFL Integrated Writing Task and 
Scoring 
The Test of English as a Foreign Language 
(TOEFL) is administered to foreign students wish-
ing to enroll in US or Canadian universities.  It 
aims to measure the extent to which a student has 
acquired English; thus native speakers should on 
average perform better on the test regardless of 
their analytical abilities.  The TOEFL now includes 
a writing component, and pilot studies were con-
ducted with native as well as non-native speakers. 
One of the writing components is an Integrated 
Writing Task.  Students first read an expository 
passage, which remains on the screen throughout 
the task.  Students then hear a segment of a lecture 
concerning the same topic.  However, the lecture 
contradicts and complements the information con-
tained in the reading.  The lecture is heard once; 
students then summarize the lecture and the read-
ing and describe any contradictions between them. 
The resulting essays are scored by human raters 
on a scale of 0 to 5, with 5 being the best possible 
score2.  The highest-scoring essays express ideas 
from both the lecture and the reading using correct 
grammar; the lowest-scoring essays rely on only 
one of the prompts for information and have 
grammatical problems; and the scores in between 
show a combination of both types of deficiencies. 
The test prompt contained passages about the 
advantages and disadvantages of working in 
groups; the reading was 260 words long, the lec-
ture 326 words.  540 non-native speakers and 950 
                                                        
2 Native speaker essays were initially scored with possible 
half-grades such as 2.5.  For purposes of comparison, these 
were rounded down to the nearest integer. 
49
native speakers were tested by ETS in 2004.  ETS 
also collected essential demographic data such as 
native language, educational level, etc., for each 
student.  For later validation, we excluded 1/3 of 
each set, selected at random, thus involving 363 
non-native speakers and 600 native speakers. 
Percent score frequencies
0
5
10
15
20
25
30
35
1 2 3 4 5
Sco re
Non-native
Native
 
Figure 1.  Relative score distributions. 
 
Among the non-native speakers, the most 
common score was 1 (see Fig. 1 for a histogram).  
By contrast, native speaker scores centered around 
3 and showed a normal-type distribution.  The dif-
ference in distributions confirms that the task is 
effective at separating non-native speakers by skill 
level, and is easier for native speakers.  The poten-
tial sources of difficulty include comprehension of 
the reading passage, listening ability and memory 
for the lecture, and the analytical ability to find 
commonalities and differences between the content 
of the reading and the lecture. 
3 Document Similarity Measure 
Due to the design of the TOEFL task, the content 
of the student essays is highly constrained.  The 
aim of the computational measures is to extract 
grammatical and stylistic differences between dif-
ferent essays.  We do this by comparing the essays 
to the reading and lecture prompts.  Our end goal is 
to determine to what extent speakers diverge from 
the prompts while retaining the content.   
The prediction is that native speakers are much 
more likely to paraphrase the prompts while keep-
ing the same gist, whereas non-native speakers are 
likely to either repeat the prompts close to verba-
tim, or diverge from them in ways that do not pre-
serve the gist.  This intuition conforms to previous 
studies of native vs. non-native speakers? text 
summarization (cf. Campbell 1987), although we 
are not aware of any related computational work. 
We begin by measuring lexico-grammatical 
similarity between each essay and the two prompts.  
An essay is represented as a set of features derived 
from its lexico-grammatical content, as described 
below.  The resulting comparison measure goes 
beyond simple word or n-gram overlap by provid-
ing a measure of structural similarity as well.  In 
essence, our method measures to what extent the 
essay expresses the content of the prompt in the 
same words, used in the same syntactic positions. 
3.1 C-rater tuples 
In order to get a measure of syntactic similarity, we 
relied on C-rater (Leacock & Chodorow 2003), an 
automatic scoring engine developed at ETS.  C-
rater includes several basic NLP components, in-
cluding POS tagging, morphological processing, 
anaphora resolution, and shallow parsing.  The 
parsing produces tuples for each clause, which de-
scribe each verb and its syntactic arguments (1). 
(1) CLAUSE: the group spreads responsibil-
ity for a decision to all the members 
TUPLE: :verb: spread :subj: the group :obj: 
responsible :pp.for: for a decide :pp.to: to all 
C-rater does not produce full-sentence trees or 
prepositional phrase attachment.  However, the 
tuples are reasonably accurate on non-native input. 
3.2 Lexical and Syntactic Features 
C-rater produces tuples for each document, often 
several per sentence.  For the current experiment, 
we used the main verb, its subject and object.  We 
then converted each tuple into a set of features, 
which included the following: 
? The verb, subject (pro)noun, and object 
(pro)noun as individual words; 
? All of the words together as a single feature; 
? The verb, subject, and object words with 
their argument roles. 
Each document can now be represented as a set 
of tuple-derived features, or feature vectors. 
3.3 Document Comparison 
Two feature vectors derived from tuples can be 
compared using a cosine measure (Salton 1989).  
The closer to 1 the cosine, the more similar the two 
feature sets.  To compensate for different frequen-
cies of the features and for varying document 
lengths, the feature vectors are weighted using 
standard tf*idf techniques. 
50
In order to estimate the similarity between two 
documents, we use the following procedure.  For 
each tuple vector in Document A, we find the tuple 
in Document B with the maximum cosine to the 
tuple in Document A.  The maximum cosine val-
ues for each tuple are then averaged, resulting in a 
single scalar value for Document A.  We call this 
measure Average Maximum Cosine (AMC). 
We calculated AMCs for each student response 
versus the reading, the lecture, and the reading + 
lecture combined.  This procedure was performed 
for both native and non-native essays.  A detailed 
examination of the resulting trends is in section 4. 
3.4 Other Measures of Document Similarity 
We also performed several measures of document 
similarity that did not include syntactic features. 
Content Vector Analysis 
The student essays and the prompts were compared 
using Content Vector Analysis (CVA), where each 
document was represented as a vector consisting of 
the words in it (Salton 1989).  The tf*idf-weighted 
vectors were compared by a cosine measure. 
For non-native speakers, there was a noticeable 
trend.  At higher score levels (where the score is 
determined by a human rater), student essays 
showed more similarity to both the reading and the 
lecture prompts.  Both the reading and lecture 
similarity trends were significant (linear trend; F= 
MSlinear trend/MSwithin-subjects=63 for the reading; F=71 
for the lecture at 0.05 significance level3).  Thus, 
the rate of vocabulary retention from both prompts 
increases with higher English-language skill level. 
Native speakers showed a similar pattern of in-
creasing cosine similarity between the essay and 
the reading (F=35 at 0.05 significance for the 
trend), and the lecture (F=35 at the 0.05 level). 
BLEU score 
In order to measure the extent to which whole 
chunks of text from the prompt are reproduced in 
the student essays, we used the BLEU score, 
known from studies of machine translation (Pap-
ineni et al 2002).  We used whole essays as sec-
tions of text rather than individual sentences. 
For non-native speakers, the trend was similar 
to that found with CVA: at higher score levels, the 
                                                        
3 All statistical calculations were performed as ANOVA-style 
trend analyses using SPSS. 
overlap between the essays and both prompts in-
creased (F=52.4 at the 0.05 level for the reading; 
F=53.6 for the lecture). 
Native speakers again showed a similar pattern, 
with a significant trend towards more similarity to 
the reading (F=35.6) and the lecture (F=31.3).  
These results were confirmed by a simple n-gram 
overlap measure. 
4 Results 
4.1 Overall similarity to reading and lecture 
The AMC similarity measure, which relies on syn-
tactic as well as lexical similarity, produced some-
what different results from simpler bag-of-word or 
n-gram measures.  In particular, there was a differ-
ence in behavior between native and non-native 
speakers: non-native speakers showed increased 
structural similarity to the lecture with increasing 
scores, but native speakers did not.   
For non-native speakers, the trend of increased 
AMC between the essay and the lecture was sig-
nificant (F=10.9).  On the other hand, there was no 
significant increase in AMC between non-native 
essays and the reading (F=3.4).  Overall, for non-
native speakers the mean AMC was higher for the 
reading than for the lecture (0.114 vs. 0.08). 
Native speakers, by contrast, showed no sig-
nificant trends for either the reading or the lecture.  
Overall, the average AMCs for the reading and the 
lecture were comparable (0.08 vs. 0.075). 
We know from results of CVA and BLEU 
analyses that for both groups of speakers, higher-
scoring essays are more lexically similar to the 
prompts.  Thus, the lack of a trend for native 
speakers must be due to lack of increase in struc-
tural similarity between higher-scoring essays and 
the prompts.  Since better essays are presumably 
better at expressing the content of the prompts, we 
can hypothesize that native speakers paraphrase the 
content more than non-native speakers. 
4.2 Difference between lecture and reading 
The most informative measure of speaker behavior 
was the difference between the Average Maximum 
Cosine with the reading and the lecture, calculated 
by subtracting the lecture AMC from the reading 
AMC.  Here, non-native speakers showed a sig-
nificant downward linear trend with increasing 
51
score (F=6.5; partial eta-squared 0.08), whereas the 
native speakers did not show any trend (F=1.5).  
The AMC differences are plotted in Figure 3. 
AMC difference between reading and 
lecture
-0.05
0
0.05
0.1
0.15
0 1 2 3 4 5
Score
Non-native
Native
 
Figure 2 - AMC difference between reading and 
lecture 
 
Non-native speakers with lower scores rely 
mostly on the reading to produce their response, 
whereas speakers with higher scores rely some-
what more on the lecture than on the reading.  By 
contrast, native speakers show no correlation be-
tween score and reading vs. lecture similarity.  
Thus, there is a significant difference in the overall 
distribution and behavior between native and non-
native speaker populations.  This difference also 
shows that human raters rely on information other 
than simple verbatim similarity to the lecture in 
assigning the overall scores. 
4.3 Other parameters of variation 
For non-native speakers, the best predictor of the 
human-rated score is the difference in AMC be-
tween the reading and the lecture. 
As demonstrated in the previous section, the 
AMC difference does not predict the score for na-
tive speakers.  We analyzed native speaker demo-
graphic data in order to find any other possible 
predictors.  The students? overall listening score, 
their status as monolingual vs. bilingual, their par-
ents? educational levels all failed to predict the es-
say scores.  
5 Discussion and Future Work 
The Average Maximum Cosine measure as de-
scribed in this paper successfully characterizes the 
behavior of native vs. non-native speaker popula-
tions on an integrated writing task.  Less skillful 
non-native speakers show a significant trend of 
relying on the easier, more available prompt (the 
reading) than on the harder prompt (the lecture), 
whereas more skillful readers view the lecture as 
more relevant and rely on it more than on the read-
ing.  This difference can be due to better listening 
comprehension for the lecture and/or better mem-
ory.  By contrast, native speakers rely on both the 
reading and the lecture about the same, and show 
no significant trend across skill levels.  Native 
speakers seem to deviate more from the structure 
of the original prompts while keeping the same 
content, signaling better paraphrasing skills. 
While not a direct measure of gist similarity, 
this technique represents a first step toward detect-
ing paraphrases in written text.  In the immediate 
future, we plan to extend the set of features to in-
clude non-verbatim similarity, such as synonyms 
and words derived by LSA-type comparison (Lan-
dauer et al 1998).  In addition, the syntactic fea-
tures will be expanded to include frequent 
grammatical alternations such as active / passive. 
A rather simple measure such as AMC has al-
ready revealed differences in population distribu-
tions for native vs. non-native speakers.  
Extensions of this method can potentially be used 
to determine if a given essay was written by a na-
tive or a non-native speaker.  For instance, a statis-
tical classifier can be trained to distinguish feature 
sets characteristic for different populations.  Such a 
classifier can be useful in a number of NLP-related 
fields, including information extraction, search, 
and, of course, educational measurement. 
References 
Campbell, C. 1987. Writing with Others? Words: Native 
and Non-Native University Students? Use of Infor-
mation from a Background Reading Text in Aca-
demic Compositions.  Technical Report, UCLA 
Center for Language Education and Research. 
Landauer, T.; Foltz, P. W; and Laham. D. 1998. Intro-
duction to Latent Semantic Analysis. Discourse 
Processes 25: 259-284. 
Leacock, C., & Chodorow, M. 2003. C-rater: Scoring of 
short-answer questions. Computers and the Humani-
ties, 37(4), 389-405. 
Papineni, K; Roukos, S.; Ward, T. and Zhu, W-J. 2002.  
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation.  ACL ?02, p. 311-318. 
Salton, G. 1989. Automatic Text Processing: The Trans-
formation, Analysis, and Retrieval of Information by 
Computer.  Reading, MA: Addison-Weley. 
52
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 19?27,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Mining of Parsed Data to Derive Deverbal Argument Structure
Olga Gurevich Scott A. Waterman
Microsoft / Powerset
475 Brannan Street, Ste. 330
San Francisco, CA 94107
{olya.gurevich,scott.waterman}@microsoft.com
Abstract
The availability of large parsed corpora
and improved computing resources now
make it possible to extract vast amounts
of lexical data. We describe the pro-
cess of extracting structured data and sev-
eral methods of deriving argument struc-
ture mappings for deverbal nouns that sig-
nificantly improves upon non-lexicalized
rule-based methods. For a typical model,
the F-measure of performance improves
from a baseline of about 0.72 to 0.81.
1 Introduction
There is a long-standing division in natural lan-
guage processing between symbolic, rule-based
approaches and data-driven, statistical ones. Rule-
based, human-curated approaches are thought to
be more accurate for linguistic constructions ex-
plicitly covered by the rules. However, such
approaches often have trouble scaling up to a
wider range of phenomena or different genres of
text. There have been repeated moves towards hy-
bridized approaches, in which rules created with
human linguistic intuitions are supplemented by
automatically derived corpus data (cf. (Klavans
and Resnik, 1996)).
Unstructured corpus data for English can eas-
ily be found on the Internet. Large corpora of
text annotated with part of speech information are
also available (such as the British National Cor-
pus). However, it is much harder to find widely
available, large corpora annotated for syntactic or
semantic structure. The Penn Treebank (Marcus
et al, 1993) has until recently been the only such
corpus, covering 4.5M words in a single genre of
financial reporting. At the same time, the accuracy
and speed of syntactic parsers has been improving
greatly, so that in recent years it has become possi-
ble to automatically create parsed corpora of rea-
sonable quality, using much larger amounts of text
with greater genre variation. For many NLP tasks,
having more training data greatly improves the
quality of the resulting models (Banko and Brill,
2001), even if the training data are not perfect.
We have access to the entire English-language
text of Wikipedia (about 2M pages) that was
parsed using the XLE parser (Riezler et al, 2002),
as well as an architecture for distributed data-
mining within this corpus, called Oceanography
(Waterman, 2009). Using the parsed corpus, we
extract a large volume of dependency relations and
derive lexical models that significantly improve
a rule-based system for determining the underly-
ing argument structure of deverbal noun construc-
tions.
2 Deverbal Argument Mapping
Deverbal nouns, or nominalizations, are nouns
that designate some aspect of the event referred
to by the verb from which they are morphologi-
cally derived (Quirk et al, 1985). For example,
the noun destruction refers to the action described
by the verb destroy, and destroyer may refer to the
agent of that event. Deverbal nouns are very com-
mon in English texts: by one count, about half of
all sentences in written text contain at least one
deverbal noun (Gurevich et al, 2008). Thus, a
computational system that aims to match multi-
ple ways of expressing the same underlying events
(such as question answering or search) must be
able to deal with deverbal nouns.
To interpret deverbal constructions, one must be
able to map nominal and prepositional modifiers to
the various roles in the verbal frame. For intran-
sitive verbs, almost any argument of the deverbal
noun is mapped to the verb?s subject, e.g. abun-
dance of food gives rise to subj(abound, food).
If the underlying verb is transitive, and the de-
verbal noun has two arguments, the mappings
are also fairly straightforward. For example, the
phrase Carthage?s defeat by Rome gives rise to
19
the arguments subj(defeat, Rome) and obj(defeat,
Carthage), based on knowledge that a ?by? argu-
ment usually maps to the subject, and the posses-
sive in the presence of a ?by? argument usually
maps to the object (Nunes, 1993).
However, in many cases a deverbal noun has
only one argument, even though the underlying
verb may be transitive. In such cases, our system
has to decide whether to map the lone argument
of the deverbal onto the subject or object of the
verb. This mapping is in many cases obvious to a
human: e.g., the king?s abdication corresponds to
subj(abdicate, king), whereas the room?s adorn-
ment corresponds to obj(adorn, room). In some
cases, the mapping is truly ambiguous, e.g., They
enjoyed the support of the Queen vs. They jumped
to the support of the Queen. Yet in other cases, the
lone argument of the deverbal noun is neither the
subject nor the object of the underlying verb, but it
may correspond to a different (e.g. prepositional)
argument of the verb, as in the travels of 1996 (cor-
responding to someone traveled in 1996). Finally,
in some cases the deverbal noun is being used in
a truly nominal sense, without an underlying map-
ping to a verb, as in Bill Gates? foundation, and
the possessive is not a verbal argument.
The predictive models in this paper focus on this
case of single arguments of deverbal nouns with
transitive underlying verbs. To constrain the scope
of the task, we focus on possessive arguments, like
the room?s adornment, and ?of? arguments, like
the support of the Queen. Our goal is to improve
the accuracy of verbal roles assigned in such cases
by creating lexically-specific preferences for indi-
vidual deverbal noun / verb pairs. Some of our
experiments also take into account some lexical
properties of the deverbal noun?s arguments. The
lexical preferences are derived by comparing ar-
gument preferences of verbs with those of related
deverbal nouns, derived from a large parsed cor-
pus using Oceanography.
2.1 Current Deverbal Mapping System
We have a list of approximately 4000 deverbal
noun / verb pairs, constructed from a combina-
tion of WordNet?s derivational links (Fellbaum,
1998), NomLex (Macleod et al, 1998), NomL-
exPlus (Meyers et al, 2004b) and some indepen-
dent curation. In the current system implementa-
tion, we attempt to map deverbal nouns onto cor-
responding verbs using a small set of heuristics
described in (Gurevich et al, 2008). We distin-
guish between event nouns like destruction, agen-
tive nouns like destroyer, and patient-like nouns
like employee.
If a deverbal noun maps onto a transitive verb
and has only one argument, the heuristics are as
follows. Arguments of agentive nouns become ob-
jects while the nouns themselves become subjects,
so the ship?s destroyer maps to subj(destroy, de-
stroyer); obj(destroy, ship). Arguments of patient-
like nouns become subjects while the nouns them-
selves become objects, so the company?s employee
becomes subj(employ, company); obj(employ, em-
ployee).
The difficult case of event nouns is currently
handled through default mappings: possessive ar-
guments become subjects (e.g., his confession 7?
subj(confess, he)), and ?of? arguments become ob-
jects (e.g., confession of sin 7? obj(confess, sin)).
However, as we have seen from examples above,
these defaults are not always correct. The correct
mapping depends on the lexical nature of the de-
verbal noun and its corresponding verb, and pos-
sibly on properties of the possessive or ?of? argu-
ment as well.
2.2 System Background
The deverbal argument mapping occurs in the con-
text of a larger semantic search application, where
the goal is to match alternate forms expressing
similar concepts. We are currently processing
the entire text of the English-language Wikipedia,
consisting of about 2M unique pages.
Parsing in this system is done using the XLE
parser (Kaplan and Maxwell, 1995) and a broad-
coverage grammar of English (Riezler et al, 2002;
Crouch et al, 2009), which produces constituent
structures and functional structures in accordance
with the theory of Lexical-Functional Grammar
(Dalrymple, 2001).
Parsing is followed by a semantic processing
phase, producing a more abstract argument struc-
ture. Semantic representations are created using
the Transfer system of successive rewrite rules
(Crouch and King, 2006). Numerous construc-
tions are normalized and rewritten (e.g., passives,
relative clauses, etc.) to maximize matching be-
tween alternate surface forms. This is the step in
which deverbal argument mapping occurs.
20
2.3 Evaluation Data
To evaluate the performance of the current and
experimental argument mappings, we extracted a
random set of 1000 sentences from the parsed
Wikipedia corpus in which a deverbal noun had
a single possessive argument. Each sentence was
manually annotated with the verb role mapping
between the deverbal and the possessive argu-
ments. One of six labels were assigned:
? Subject, e.g. John?s attention
? Object, e.g. arrangement of flowers
? Other: there is an underlying verb, but the
relationship between the verb and the argu-
ment is neither subject nor object; these rela-
tions often appear as prepositional arguments
in the verbal form, e.g. Declaration of Delhi
? Noun modifier: the argument modifies the
nominal sense of the deverbal noun, rather
than the underlying verb, although there is
still an underlying event, as in director of 25
years
? Not deverbal: the deverbal noun is not used
to designate an event in this context, e.g. the
rest of them
? Error: the parser incorrectly identified the ar-
gument as modifying the deverbal, or as be-
ing the only argument of the deverbal
Similarly, we extracted a sample of 750 sentences
in which a deverbal noun had a single ?of? argu-
ment, and annotated those manually.
The distribution of annotations is summarized
in Table 1. For possessive arguments, the preva-
lent role was subject, and for ?of? arguments it was
object.
The defaults will correctly assign the majority
of arguments roles.
Possessive ?Of?
total 1000 750
unique deverbals 423 338
subj 511 (51%) 158 (21%)
obj 335 (34%) 411 (55%)
other 28 (3%) 50 (7%)
noun mod 23 (2%) 18 (2%)
not deverbal 21 (2%) 40 (5%)
error 82 (8%) 73 (10%)
Table 1: Evaluation Role Judgements, with de-
faults in bold
2.4 Lexicalizing Role Mappings
Our basic premise is that knowledge about role-
mapping behavior of particular verbs will inform
the role-mapping behavior of their corresponding
deverbal nouns. For example, if a particular argu-
ment of a given verb surfaces as the verb?s sub-
ject more often than as object, we might also pre-
fer the subject role when the same argument oc-
curs as a modifier of the corresponding deverbal
noun. However, as nominal modification con-
structions impose their own role-mapping pref-
erences (e.g., possessives are more likely to be
subjects than objects), we expect different dis-
tributions of arguments to appear in the various
deverbal modification patterns. Making use of
this intuition requires collecting sufficient infor-
mation about corresponding arguments of verbs
and deverbal nouns. This is available, given a
large parsed corpus, a reasonably accurate and fast
parser, and enough computing capacity. The re-
mainder of the paper details our data extraction,
model-building methods, and the results of some
experiments.
3 Data Collection
Oceanography is a pattern extraction and statistics
language for analyzing structural relationships in
corpora parsed using XLE (Waterman, 2009). It
simplifies the task of programming for NL analy-
sis over large corpora, and the sorting, counting,
and distributional analysis that often characterizes
statistical NLP. This corpus processing language is
accompanied by a distributed runtime, which uses
cluster computing to match patterns and collect
statistics simultaneously across many machines.
This is implemented in a specialized distributed
framework for parsing and text analysis built on
top of Hadoop (D. Cutting et al, ). Oceanography
programs compile down to distributed programs
which run in this cluster environment, allowing the
NL researcher to state declaratively the data gath-
ering and analysis tasks.
A typical program consists of two declarative
parts, a pattern matching specification, and a set
of statistics declarations. The pattern matching
section is written using Transfer, a specialized lan-
guage for identifying subgraphs in the dependency
structures used in XLE (Crouch and King, 2006).
Transfer rules use a declarative syntax for spec-
ifying elements and their relations; in this way,
it is much like a very specialized awk or grep
for matching within parse trees and dependency
graphs.
Statistics over these matched structures are
21
also stated declaratively. The researcher states
which sub-elements or tuples are to be counted,
and the resulting compiled program will output
counts. Conditional distributions and comparisons
between distributions are available as well.
3.1 Training Data
Using Oceanography, we extracted two sets of re-
lations from the parsed Wikipedia corpus, Full-
Wiki, with approximately 2 million documents. A
smaller 10,000-document subset, the 10K set, was
used in initial experiments. Some comparative re-
sults are shown to indicate effects of corpus size
on results. Summary corpus statistics are shown
in table 2. The two sets were:
1. All verb-argument pairs, using verb and ar-
gument lemmas. We recorded the verb, the
argument, the kind of relation between them
(e.g., subject, object, etc.), and part of speech
of the argument, distinguishing also among
pronouns, names, and common nouns. For
each combination, we record its frequency of
occurrence.
2. All deverbal-argument pairs, using deverbal
noun and argument lemmas. We recorded the
deverbal noun, the argument, the kind of rela-
tion (e.g., possessive, ?of?, prenominal modi-
fier, etc.) and part of speech of the argument.
We record the frequency of occurrence for
each combination.
Some summary statistics about the extracted
data are in Table 2.
FullWiki training data
Documents 2 million
Sentences 121,428,873
Deverbal nouns with arguments 4,596
Unique verbs with deverbals 3,280
Verbs with arguments 7,682
Deverbal - role - argument sets 21,924,405
Deverbal - argument pairs 12,773,621
Deverbals with any poss argument 3,802
Possessive deverbal - argument pairs 611,192
Most frequent: poss(work, he) 75,343
Deverbals with any ?of? argument 4,075
?Of? deverbal- argument pairs 2,108,082
Most frequent: of(end, season) 15,282
Verb - role - argument sets 72,150,246
Verb - argument pairs 40,895,810
Overlapping pairs 5,069,479
Deverbals with overlapping arguments 3,211
Table 2: Training Data
4 Assigning Roles
The present method is based on projecting argu-
ment type preferences from the verbal usage to the
deverbal. The intuition is that if an argument X is
preferred as the subject (object) of verb V, then it
will also be preferred in the semantic frame of an
occurrence (N, X) with the corresponding dever-
bal noun N.
We model these preferences directly using the
relative frequency of subject and object occur-
rences of each possible argument with each verb.
Even with an extremely large corpus, it is unlikely
that one will find direct evidence for all such com-
binations, and one will need to generalize the pre-
diction.
4.1 Deverbal-only Model
The first model, all-arg, specializes only for the
deverbal, and generalizes over all arguments, re-
lying on the overall preference of subject v. ob-
ject for the set of arguments that appear with both
verb and deverbal forms. Take as an example
deverbal nouns with possessive arguments (e.g.,
the city?s destruction). Given the phrase (X?s N),
where N is a deverbal noun related to verb V,
Fd(N, V, X) is a function that assigns one of the
roles subj, obj, unknown to the pair (V, X). In
this deverbal only model, the function depends on
N and V only, and not on the argument X. Fd for
a any pair (N, V) is calculated as follows:
1. Find all arguments X that occur in the con-
struction ?N?s X? as well as either subj(V, X)
or obj(V, X). X, N, and V have all been lem-
matized. For example, poss(city, destruction)
occurs 10 times in the corpus; subj(destroy,
city) occurs 3 times, and obj(destroy, city) oc-
curs 12 times. This approach conflates in-
stances of the city?s destruction, the cities?
destruction, the city?s destructions, etc.
2. For each argument X, calculate the ratio be-
tween the number of occurrences of subj(V,
X) and obj(V, X). If the argument occurs
as subject more than 1.5 times as often as
the object, increment the count of subject-
preferring arguments of N by 1. If the ar-
gument occurs as object more than 1.5 times
as often as subject (as would be the case with
(destroy, city)), increment the count of object-
preferring arguments. If the ratio in frequen-
cies of occurrence is less than the cutoff ratio
22
of 1.5, neither count is incremented. In ad-
dition to the number of arguments with each
preference, we keep track of the total num-
ber of instances for each argument prefer-
ence, summed up over all individual argu-
ments with that preference.
3. Compare the number of subject-preferring ar-
guments of N with the number of object-
preferring arguments. If one is greater than
the other by more than 1.5 times, state that the
deverbal noun N has a preference for map-
ping its possessive arguments to the appro-
priate verbal role. We ignore cases where the
total number of occurrences of the winning
arguments is too small to be informative (in
the current model, we require it to be greater
than 1).
If there is insufficient evidence for a deverbal
N, we fall back to the default preference across all
deverbals. Subject and object co-occurrences with
the verb forms are always counted, regardless of
other arguments the verb may have in each sen-
tence, on the intuition that the semantic role pref-
erence of the argument is relatively unaffected and
that this will map to the deverbal construction even
when the possessive is the only argument. Sum-
mary preferences for all-args are shown in Ta-
ble 3.
The same algorithm was applied to detect ar-
gument preferences for deverbals with ?of? argu-
ments (such as destruction of the city). Summary
preferences are shown in Table 4.
4.2 Deverbal + Argument Animacy Model
The second model tries to capture the intuition that
animate arguments often behave differently than
inanimate ones: in particular, animate arguments
are more often agents, encoded syntactically as
subjects.
We calculated argument preferences separately
for two classes of arguments: (1) animate pro-
nouns such as he, she, I; and (2) nouns that were
not identified as names by our name tagger. We
assumed that arguments in the first group were
animate, whereas arguments in the second group
were not. In these experiments, we did not try to
classify named entities as animate or inanimate,
resulting in less training data for both classes of
arguments. This strategy also incorrectly classifies
common nouns that refer to people (e.g., occupa-
tion names such as teacher).
The results of running both models on the 10K
and FullWiki training sets are in Table 3 for pos-
sessive arguments and Table 4 for ?of? arguments.
For possessives, animate arguments preferred
subject role mappings much more than the average
across all arguments. Inanimate arguments also on
the whole preferred subject mappings, but much
less strongly.
For ?of? arguments, in most cases there were
more object-preferring verbs, except for verbs
with animate arguments, which overwhelmingly
preferred subjects. We might therefore expect
there to be a difference in performance between
the model that treats all arguments equally and the
model that takes argument animacy into account.
Model: all-arg
10K FullWiki
Subj-preferring 391 (65%) 1786 (67%)
Obj-preferring 207 (35%) 884 (33%)
Total 598 (100%) 2670 (100%)
Model: animacy
Subj-pref animate 370 (78%) 1941 (79%)
Obj-pref animate 106 (22%) 511 (21%)
Total animate 476 (100%) 2452 (100%)
Subj-pref inanimate 45 (47%) 990 (57%)
Obj-pref inanimate 51 (53%) 748 (43%)
Total inanimate 96 (100%) 1738 (100%)
Table 3: Possessive argument preferences
Model: all-arg
10K FullWiki
Subj-preferring 143 (30%) 839 (29%)
Obj-preferring 328 (70%) 2036 (71%)
Total 471 (100%) 2875 (100%)
Model: animacy
Subj-pref animate 70 (83%) 1196 (74%)
Obj-pref animate 14 (17%) 423 (26%)
Total animate 84 (100 %) 1619 (100%)
Subj-pref inanimate 83 (23%) 699 (25%)
Obj-pref inanimate 272 (77%) 2068 (75%)
Total inanimate 355 (100%) 2767 (100%)
Table 4: ?Of? argument preferences
5 Experiments
The base system against which we compare these
models uses the output of the parser, identifies de-
verbal nouns and their arguments, and applies the
heuristics described in Section 2.1 to obtain verb
roles. Recall that possessive arguments of transi-
tive deverbals map to the subject role, and ?of? ar-
guments map to object. Also recall that these rules
apply only to eventive deverbals; mapping rules
for known agentive and patient-like deverbals re-
main as before.
23
In the evaluation, the experimental models take
precedence: if the model predicts an outcome, it
is used. The default system behavior is used as a
fallback when the model does not have sufficient
evidence to make a prediction. This stacking of
models allows the use of corpus evidence when
available, and generalized defaults otherwise.
For the animacy model, we used our full sys-
tem to detect whether the argument of a deverbal
was animate (more precisely, human). In addi-
tion to the animate pronouns used to generate the
model, we also considered person names, as well
as common nouns that had the hypernym ?person?
in WordNet. If the argument was animate and the
model had a prediction, that was used. If no pre-
diction was available for animate arguments, then
the inanimate prediction was used. Failing that,
the prediction falls back to the general defaults.
5.1 Possessive Arguments of Deverbal Nouns
Model predictions were compared against the
hand-annotated evaluation set described in Sec-
tion 2.3. For each sentence in the evaluation set,
we used the models to make a two-way prediction
with respect to the default mapping: is the posses-
sive argument of the deverbal noun an underlying
subject or not. We ignored test sentences marked
as having erroneous parses, leaving 918 (of 1000
annotated). Since we were evaluating the accuracy
of the ?subject? label, all non-subject roles (object,
?other?, ?not a deverbal?, and ?nominal modifier?)
were in the same class. The baseline for compari-
son is the default ?subject? role.
The possible outcomes for each sentence
were:
? True Positive: Expected role and role pro-
duced by the system are ?subject?
? True Negative: Expected role is not subject,
and the model did not produce the label sub-
ject. Expected role and produced role may
differ (e.g. expected role may be ?other?, and
the model may produce ?object?, but since
neither one is ?subject?, this counts as correct
? False Positive: Expected role is not subject,
but the model produced subject
? False Negative: Expected role is subject, but
the model produced some other role
As a quick evaluation, we compared baseline
and model-predicted results directly in the surface
string of the sentences, without reparsing the sen-
tences or using the semantic rewrite rules. The
advantage of this evaluation is that it is very fast
to run and is easily reproducible outside of our
specialized environment. This evaluation differed
from the full-pipeline evaluation in two ways: (1)
it did not distinguish event deverbals from agen-
tive and patient-like deverbals, thus possibly in-
troducing errors, and (2) it did not look up all ar-
gument lemmas to find out their animacy. This
baseline had precision of 0.56; recall of 1.0, and
an F-measure of 0.72.
The complete evaluation uses our full NL
pipeline, reparsing the sentences and applying all
of our deverbal mapping rules as described above.
The baseline for this evaluation had a precision of
0.65, recall of 0.94, and F-measure of 0.77. The
differences in the two baselines are mostly due to
the full-pipeline evaluation having different map-
ping rules for agentive and patient-like deverbals.
5.1.1 Results
Results of applying the models are summarized
in Table 5, for all models, trained with both the
smaller and the larger data sets, and measured with
and without using the full pipeline.
All models performed better than the baseline.
The all-arg model did about the same as the an-
imacy model with both training sets. We suggest
some reasons for this in the next section.
It is unambiguously clear that adding lexical
knowledge to the rule system, even when this
knowledge is derived from a relatively small train-
ing set, significantly improves performance, and
also that more training data leads to greater im-
provements.
Model Training Precision Recall F-measure
Surface String Measure
Baseline - 0.56 1.00 0.72
all-arg 10K 0.64 0.92 0.76
animacy 10K 0.62 0.93 0.75
all-arg FullWiki 0.68 0.95 0.81
animacy FullWiki 0.70 0.92 0.79
Full NL pipeline
Baseline - 0.65 0.94 0.77
all-arg 10K 0.75 0.88 0.81
animacy 10K 0.73 0.90 0.80
all-arg FullWiki 0.78 0.90 0.84
animacy FullWiki 0.81 0.88 0.84
Table 5: Performance on deverbal nouns with one
possessive argument
5.1.2 Error Analysis and Discussion
We looked at the errors produced by the best-
performing model, all-arg trained on the FullWiki
24
set. There were 49 false negatives (i.e. cases
where the human judge decided that the underly-
ing relationship between the deverbal and its ar-
gument is ?subject?, but our system produced a
different relation or no relation at all), covering
39 unique deverbal nouns. Of these, 20 deverbal
nouns were predicted by the model to prefer ob-
jects (e.g., Hopkins? accusation), and 19 did not
get assigned either subject or object due to other
errors (including a few mislabeled evaluation sen-
tences).
Some of the false negatives involved deverbal
nouns that refer to reciprocal predicates such as
his marriage, or causative ones such as Berlin?s
unity, which could map to subject or objects. Our
current system does not allow us to express such
ambiguity, but it is a possible future improvement.
Looking at the false negatives produced by the
all-arg model, 3 deverbal nouns received more ac-
curate predictions with the animacy model (e.g.,
his sight; Peter Kay?s statement). Intuitively, the
animacy model should in general make more in-
formed decisions about the argument mappings
because it takes properties of individual arguments
into account. However, as we have seen, it does
not in fact outperform the model that treats all ar-
guments the same way.
We believe this is due to the fact that the an-
imacy model was trained on less data than the
all-arg model, because we only considered ani-
mate pronouns and common nouns when gener-
ating argument-mapping predictions. Excluding
all named entities and non-animate pronouns most
likely had an effect on the number of deverbals for
which the model was able to make accurate pre-
dictions. In the next iteration, we would like to
use all available arguments, relying on the named
entity type and information available in WordNet
for common nouns to distinguish between animate
and inanimate arguments.
The all-arg model evaluation resulted in 131
false positives (cases where the model predicted
the relation to be ?subject?, but the human judge
thought it was something else). Of these, 105 were
marked by the human judge as having objects, 8 as
having a verbal relation other than subject or ob-
ject, 9 as having nominal modifiers, 9 has having
no deverbal.
Altogether, false positives covered 85 unique
verbs. Of these, 48 had been explicitly predicted
by our model to prefer subjects, and the rest had
no explicit prediction, thus defaulting to having a
subject. 3 of these deverbals would have been cor-
rectly identified as having objects by the animacy
model (e.g., his composition; her representation).
Although it is hard to predict the outcome of a
statistical model, we feel that more reliable infor-
mation about the animacy of arguments at training
time would improve the performance of the ani-
macy model, potentially making it better than the
all-arg model.
5.2 ?Of? Arguments of Deverbal Nouns
The evaluation procedure for ?of? arguments was
the same as for possessive arguments, except that
the default argument mapping was ?object?, and
the evaluated decision was whether a particular
role was object or non-object. Ignoring sentence
with erroneous parses, we had 677 evaluation ex-
amples.
5.2.1 Results
Results for all models are summarized in Table 6.
All models outperformed the baseline on all train-
ing sets and on both the surface or full-pipeline
measures.
As with possessive arguments, the all-arg and
animacy models performed about the same, with
both the FullWiki and 10K training sets.
The 10K-trained animacy model did not do
as poorly as might have been expected given its
low prediction rate for deverbals with animate ar-
guments in our evaluation set. The better-than-
expected performance may be explained by low
incidence of animate arguments in this set.
Model Training Precision Recall F-measure
Surface String Measure
Baseline - 0.60 1.00 0.75
all-arg 10K 0.68 0.97 0.80
animacy 10K 0.66 0.94 0.78
all-arg FullWiki 0.71 0.97 0.82
animacy FullWiki 0.70 0.91 0.79
Full NL pipeline
Baseline - 0.61 0.89 0.73
all-arg 10K 0.71 0.86 0.78
animacy 10K 0.70 0.85 0.77
all-arg FullWiki 0.78 0.87 0.82
animacy FullWiki 0.80 0.85 0.82
Table 6: Performance on deverbal nouns with one
?of? argument
5.2.2 Error Analysis and Discussion
We looked at the errors produced by the best-
performing model, all-arg trained on the FullWiki
25
set. There were 53 false negatives (cases where
the human judged marked the relation as ?object?
but the system marked it as something else), cov-
ering 42 unique deverbal nouns. Of these 7 were
(incorrectly) predicted by the model to prefer sub-
jects (e.g., operation of a railway engine), and the
rest were misidentified due to other errors.
There were 101 false positives (cases where the
system marked the role as object, but the human
judge disagreed). Of these, the human judged
marked 54 as subject, 21 as other verbal role, 13
as nominal modifier, and 13 as non-deverbal.
Of the 72 unique deverbals in the false-positive
set, our model incorrectly predicted that 38 should
prefer objects (such as Adoration of the Magi; un-
der the direction of Bishop Smith)). For 30 de-
verbals, the model made no prediction, and the
default mapping to object turned out to be incor-
rect. It is unclear to what extent better information
about animacy would have helped.
6 Related Work
One of the earliest computational attempts to de-
rive argument structures for deverbal nouns is
(Hull and Gomez, 1996), with hand-crafted map-
ping rules for a small set of individual nouns, ex-
emplifying a highly precise but not easily scalable
method.
In recent years, NomBank (Meyers et al,
2004a) has provided a set of about 200,000 manu-
ally annotated instances of nominalizations with
arguments, giving rise to supervised machine-
learned approaches such as (Pradhan et al, 2004)
and (Liu and Ng, 2007), which perform fairly well
in the overall task of classifying deverbal argu-
ments. However, no evaluation results are pro-
vided for specific, problematic classes of nominal
arguments such as possessives; it is likely that the
amount of annotations in NomBank is insufficient
to reliably map such cases onto verbal arguments.
(Pado? et al, 2008) describe an unsupervised
approach that, like ours, uses verbal argument
patterns to deduce deverbal patterns, though the
resulting labels are semantic roles used in SLR
tasks (cf. (Gildea and Jurafsky, 2000)) rather
than syntactic roles. A combination of our much
larger training set and the sophisticated probabilis-
tic methods used by Pado? et al would most likely
improve performance for both syntactic and se-
mantic roles labelling tasks.
7 Conclusions and Future Work
We have demonstrated that large amounts of lexi-
cal data derived from an unsupervised parsed cor-
pus improve role assignment for deverbal nouns.
The improvements are significant even with a rel-
atively small training set, relying on parses that
have not been hand-corrected, using a very sim-
ple prediction model. Larger amounts of extracted
data improve performance even more.
There is clearly still headroom for improve-
ment in this method. In a pilot study, we used
argument preferences for individual deverbal-
argument pairs, falling back to deverbal-only gen-
eralizations when more specific patterns were not
available. This model had slightly higher preci-
sion and slightly lower recall than the deverbal-
only model, suggesting that a more sophisticcated
probabilistic prediction model may be needed.
In addition, performance should improve if we
allow non-binary decisions: in addition to map-
ping deverbal arguments to subject or object of the
underlying verb, we could allow mappings such
as ?unknown? or ?ambiguous?. The same training
sets can be used to produce a model that makes a
3- or 4-way split. In the possessive and ?of? sets,
the ?unknown / ambiguous? class would cover be-
tween 15% and 20% of all the data. This third
possibility becomes even more important for other
deverbal arguments. For example, if the deverbal
noun has a prenominal modifier (as in city destruc-
tion), in a third of the cases the underlying relation
is neither the subject nor the object (Lapata, 2002).
And, of course, the methodology of extracting
lexical preferences based on large parsed corpora
can be applied to many other NL tasks not related
to deverbal nouns.
Acknowledgments
We gratefully acknowledge the helpful advice and
comments of our colleagues Tracy Holloway King
and Dick Crouch, as well as the three anonymous
reviewers.
26
References
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In ACL, pages 26?33.
Richard S. Crouch and Tracy Holloway King. 2006.
Semantics via f-structure rewriting. In Proceed-
ings of the Lexical Functional Grammar Conference
2006.
Dick Crouch, Mary Dalrymple, Ron Kaplan,
Tracy Holloway King, John Maxwell, and Paula
Newman. 2009. XLE Documentation. Available
On-line.
D. Cutting et al Apache Hadoop Project.
http://hadoop.apache.org/.
Mary Dalrymple. 2001. Lexical Functional Grammar.
Academic Press. Syntax and Semantics, volume 34.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 512?520, Hong Kong,
October. Association for Computational Linguistics.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2008. Deverbal nouns in
knowledge representation. Journal of Logic and
Computation, 18:385?404.
Richard D. Hull and Fernando Gomez. 1996. Seman-
tic interpretation of nominalizations. In AAAI/IAAI,
Vol. 2, pages 1062?1068.
Ronald Kaplan and John T. Maxwell. 1995. A method
for disjunctive constraint satisfaction. In Formal Is-
sues in Lexical-Functional Grammar. CSLI Press.
Judith Klavans and Philip Resnik, editors. 1996. The
Balancing Act. Combining Symbolic and Statistical
Approaches to Language. The MIT Press.
Maria Lapata. 2002. The disambiguation of nomi-
nalizations. Computational Linguistics, 28(3):357?
388.
Chang Liu and Hwee Tou Ng. 2007. Learning pre-
dictive structures for semantic role labeling of nom-
bank. In ACL.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In Proceedings of
EURALEX?98.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004a.
The nombank project: An interim report. In
A. Meyers, editor, HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. As-
sociation for Computational Linguistics.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004b. The cross-breeding of
dictionaries. In Proceedings of LREC-2004.
Mary Nunes. 1993. Argument linking in english de-
rived nominals. In Robert Van Valin, editor, Ad-
vances in Role and Reference Grammar, pages 375?
432. John Benjamins.
Sebastian Pado?, Marco Pennacchiotti, and Caroline
Sporleder. 2008. Semantic role assignment for
event nominalisations by leveraging verbal data. In
Proceedings of CoLing08.
Sameer Pradhan, Honglin Sun, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2004. Parsing argu-
ments of nominalizations in english and chinese. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 141?
144, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman.
Stefan Riezler, Tracy Holloway King, Ronald Kaplan,
John T. Maxwell II, Richard Crouch, and Mark
Johnson. 2002. Parsing the Wall Street Journal
using a Lexical-Functional Grammar and discrimi-
native estimation techniques. In Proceedings of the
ACL?02.
Scott A. Waterman. 2009. Distributed parse mining.
In Software engineering, testing, and quality assur-
ance for natural language processing (SETQA-NLP
2009).
27
