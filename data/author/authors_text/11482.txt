Proceedings of the Third Workshop on Statistical Machine Translation, pages 224?232,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Optimizing Chinese Word Segmentation for Machine Translation
Performance
Pi-Chuan Chang, Michel Galley, and Christopher D. Manning
Computer Science Department, Stanford University
Stanford, CA 94305
pichuan,galley,manning@cs.stanford.edu
Abstract
Previous work has shown that Chinese word seg-
mentation is useful for machine translation to En-
glish, yet the way different segmentation strategies
affect MT is still poorly understood. In this pa-
per, we demonstrate that optimizing segmentation
for an existing segmentation standard does not al-
ways yield better MT performance. We find that
other factors such as segmentation consistency and
granularity of Chinese ?words? can be more impor-
tant for machine translation. Based on these find-
ings, we implement methods inside a conditional
random field segmenter that directly optimize seg-
mentation granularity with respect to the MT task,
providing an improvement of 0.73 BLEU. We also
show that improving segmentation consistency us-
ing external lexicon and proper noun features yields
a 0.32 BLEU increase.
1 Introduction
Word segmentation is considered an important first
step for Chinese natural language processing tasks,
because Chinese words can be composed of multi-
ple characters but with no space appearing between
words. Almost all tasks could be expected to ben-
efit by treating the character sequence ?Us? to-
gether, with the meaning smallpox, rather than deal-
ing with the individual characters ?U? (sky) and
?s? (flower). Without a standardized notion of a
word, traditionally, the task of Chinese word seg-
mentation starts from designing a segmentation stan-
dard based on linguistic and task intuitions, and then
aiming to building segmenters that output words that
conform to the standard. One widely used standard
is the Penn Chinese Treebank (CTB) Segmentation
Standard (Xue et al, 2005).
It has been recognized that different NLP ap-
plications have different needs for segmentation.
Chinese information retrieval (IR) systems benefit
from a segmentation that breaks compound words
into shorter ?words? (Peng et al, 2002), parallel-
ing the IR gains from compound splitting in lan-
guages like German (Hollink et al, 2004), whereas
automatic speech recognition (ASR) systems prefer
having longer words in the speech lexicon (Gao et
al., 2005). However, despite a decade of very in-
tense work on Chinese to English machine transla-
tion (MT), the way in which Chinese word segmen-
tation affects MT performance is very poorly under-
stood. With current statistical phrase-based MT sys-
tems, one might hypothesize that segmenting into
small chunks, including perhaps even working with
individual characters would be optimal. This is be-
cause the role of a phrase table is to build domain
and application appropriate larger chunks that are
semantically coherent in the translation process. For
example, even if the word for smallpox is treated as
two one-character words, they can still appear in a
phrase like ?U s?smallpox?, so that smallpox
will still be a candidate translation when the system
translates ?U? ?s?. Nevertheless, Xu et al (2004)
show that an MT system with a word segmenter out-
performs a system working with individual charac-
ters in an alignment template approach. On differ-
ent language pairs, (Koehn and Knight, 2003) and
(Habash and Sadat, 2006) showed that data-driven
methods for splitting and preprocessing can improve
Arabic-English and German-English MT.
Beyond this, there has been no finer-grained anal-
ysis of what style and size of word segmentation is
optimal for MT. Moreover, most discussion of seg-
mentation for other tasks relates to the size units to
identify in the segmentation standard: whether to
join or split noun compounds, for instance. People
224
generally assume that improvements in a system?s
word segmentation accuracy will be monotonically
reflected in overall system performance. This is the
assumption that justifies the concerted recent work
on the independent task of Chinese word segmenta-
tion evaluation at SIGHAN and other venues. How-
ever, we show that this assumption is false: aspects
of segmenters other than error rate are more criti-
cal to their performance when embedded in an MT
system. Unless these issues are attended to, sim-
ple baseline segmenters can be more effective inside
an MT system than more complex machine learning
based models, with much lower word segmentation
error rate.
In this paper, we show that even having a ba-
sic word segmenter helps MT performance, and we
analyze why building an MT system over individ-
ual characters doesn?t function as well. Based on
an analysis of baseline MT results, we pin down
four issues of word segmentation that can be im-
proved to get better MT performance. (i) While a
feature-based segmenter, like a support vector ma-
chine or conditional random field (CRF) model, may
have very good aggregate performance, inconsistent
context-specific segmentation decisions can be quite
harmful to MT system performance. (ii)A perceived
strength of feature-based systems is that they can
generate out-of-vocabulary (OOV) words, but these
can hurt MT performance, when they could have
been split into subparts from which the meaning of
the whole can be roughly compositionally derived.
(iii) Conversely, splitting OOV words into non-
compositional subparts can be very harmful to an
MT system: it is better to produce such OOV items
than to split them into unrelated character sequences
that are known to the system. One big source of such
OOV words is named entities. (iv) Since the opti-
mal granularity of words for phrase-based MT is un-
known, we can benefit from a model which provides
a knob for adjusting average word size.
We build several different models to address these
issues and to improve segmentation for the benefit of
MT. First, we emphasize lexicon-based features in
a feature-based sequence classifier to deal with seg-
mentation inconsistency and over-generating OOV
words. Having lexicon-based features reduced the
MT training lexicon by 29.5%, reduced the MT test
data OOV rate by 34.1%, and led to a 0.38 BLEU
point gain on the test data (MT05). Second, we ex-
tend the CRF label set of our CRF segmenter to iden-
tify proper nouns. This gives 3.3% relative improve-
ment on the OOV recall rate, and a 0.32 improve-
ment in BLEU. Finally, we tune the CRF model to
generate shorter or longer words to directly optimize
the performance of MT. For MT, we found that it
is preferred to have words slightly shorter than the
CTB standard.
The paper is organized as follows: we describe
the experimental settings for the segmentation task
and the task in Section 2. In Section 3.1 we demon-
strate that it is helpful to have word segmenters for
MT, but that segmentation performance does not di-
rectly correlate with MT performance. We analyze
what characteristics of word segmenters most affect
MT performance in Section 3.2. In Section 4 and
5 we describe how we tune a CRF model to fit the
?word? granularity and also how we incorporate ex-
ternal lexicon and information about named entities
for better MT performance.
2 Experimental Setting
2.1 Chinese Word Segmentation
For directly evaluating segmentation performance,
we train each segmenter with the SIGHAN Bake-
off 2006 training data (the UPUC data set) and then
evaluate on the test data. The training data contains
509K words, and the test data has 155K words. The
percentage of words in the test data that are unseen
in the training data is 8.8%. Detail of the Bakeoff
data sets is in (Levow, 2006). To understand how
each segmenter learns about OOV words, we will
report the F measure, the in-vocabulary (IV) recall
rate as well as OOV recall rate of each segmenter.
2.2 Phrase-based Chinese-to-English MT
The MT system used in this paper is Moses, a state-
of-the-art phrase-based system (Koehn et al, 2003).
We build phrase translations by first acquiring bidi-
rectional GIZA++ (Och and Ney, 2003) alignments,
and using Moses? grow-diag alignment symmetriza-
tion heuristic.1 We set the maximum phrase length
to a large value (10), because some segmenters
described later in this paper will result in shorter
1In our experiments, this heuristic consistently performed
better than the default, grow-diag-final.
225
words, therefore it is more comparable if we in-
crease the maximum phrase length. During decod-
ing, we incorporate the standard eight feature func-
tions of Moses as well as the lexicalized reordering
model. We tuned the parameters of these features
with Minimum Error Rate Training (MERT) (Och,
2003) on the NIST MT03 Evaluation data set (919
sentences), and then test the MT performance on
NIST MT03 and MT05 Evaluation data (878 and
1082 sentences, respectively). We report the MT
performance using the original BLEU metric (Pap-
ineni et al, 2001). All BLEU scores in this paper are
uncased.
The MT training data was subsampled from
GALE Year 2 training data using a collection
of character 5-grams and smaller n-grams drawn
from all segmentations of the test data. Since
the MT training data is subsampled with charac-
ter n-grams, it is not biased towards any particular
word segmentation. The MT training data contains
1,140,693 sentence pairs; on the Chinese side there
are 60,573,223 non-whitespace characters, and the
English sentences have 40,629,997 words.
Our main source for training our five-gram lan-
guage model was the English Gigaword corpus, and
we also included close to one million English sen-
tences taken from LDC parallel texts: GALE Year 1
training data (excluding FOUO data), Sinorama,
AsiaNet, and Hong Kong news. We restricted the
Gigaword corpus to a subsample of 25 million sen-
tences, because of memory constraints.
3 Understanding Chinese Word
Segmentation for Phrase-based MT
In this section, we experiment with three types
of segmenters ? character-based, lexicon-based and
feature-based ? to explore what kind of characteris-
tics are useful for segmentation for MT.
3.1 Character-based, Lexicon-based and
Feature-based Segmenters
The training data for the segmenter is two orders of
magnitude smaller than for the MT system, it is not
terribly well matched to it in terms of genre and
variety, and the information an MT system learns
about alignment of Chinese to English might be the
basis for a task appropriate segmentation style for
Chinese-English MT. A phrase-based MT system
Segmentation Performance
Segmenter F measure OOV Recall IV Recall
CharBased 0.334 0.012 0.485
MaxMatch 0.828 0.012 0.951
MT Performance
Segmenter MT03 (dev) MT05 (test)
CharBased 30.81 29.36
MaxMatch 31.95 30.73
Table 1: CharBased vs. MaxMatch
like Moses can extract ?phrases? (sequences of to-
kens) from a word alignment and the system can
construct the words that are useful. These observa-
tions suggest the first hypothesis.
Hypothesis 1. A phrase table should capture word
segmentation. Character-based segmentation for
MT should not underperform a lexicon-based seg-
mentation, and might outperform it.
Observation In the experiments we conducted,
we found that the phrase table cannot capture every-
thing a Chinese word segmenter can do, and there-
fore having word segmentation helps phrase-based
MT systems. 2
To show that having word segmentation helps
MT, we compare a lexicon-based maximum-
matching segmenter with character-based segmen-
tation (treating each Chinese character as a word).
The lexicon-based segmenter finds words by greed-
ily matching the longest words in the lexicon in a
left-to-right fashion. We will later refer to this seg-
menter as MaxMatch. The MaxMatch segmenter is a
simple and common baseline for the Chinese word
segmentation task.
The segmentation performance of MaxMatch is
not very satisfying because it cannot generalize to
capture words it has never seen before. How-
ever, having a basic segmenter like MaxMatch still
gives the phrase-based MT system a win over the
character-based segmentation (treating each Chinese
character as a word). We will refer to the character-
based segmentation as CharBased.
In Table 1, we can see that on the Chinese word
segmentation task, having MaxMatch is obviously
better than not trying to identify Chinese words at
all (CharBased). As for MT performance, in Ta-
ble 1 we see that having a segmenter, even as sim-
2Different phrase extraction heuristics might affect the re-
sults. In our experiments, grow-diag outperforms both one-to-
many and many-to-one for both MaxMatch and CharBased. We
report the results only on grow-diag.
226
ple as MaxMatch, can help phrase-based MT system
by about 1.37 BLEU points on all 1082 sentences
of the test data (MT05). Also, we tested the per-
formance on 828 sentences of MT05 where all el-
ements are in vocabulary3 for both MaxMatch and
CharBased. MaxMatch achieved 32.09 BLEU and
CharBased achieved 30.28 BLEU, which shows that
on the sentences where all elements are in vocabu-
lary, there MaxMatch is still significantly better than
CharBased. Therefore, Hypothesis 1 is refuted.
Analysis We hypothesized in Hypothesis 1 that
the phrase table in a phrase-basedMT system should
be able to capture the meaning by building ?phrases?
on top of character sequences. Based on the experi-
mental result in Table 1, we see that using character-
based segmentation (CharBased) actually performs
reasonably well, which indicates that the phrase ta-
ble does capture the meaning of character sequences
to a certain extent. However, the results also show
that there is still some benefit in having word seg-
mentation for MT. We analyzed the decoded out-
put of both systems (CharBased and MaxMatch) on
the development set (MT03). We found that the ad-
vantage of MaxMatch over CharBased is two-fold,
(i) lexical: it enhances the ability to disambiguate
the case when a character has very different meaning
in different contexts, and (ii) reordering: it is easier
to move one unit around than having to move two
consecutive units at the same time. Having words as
the basic units helps the reordering model.
For the first advantage, one example is the char-
acter ???, which can both mean ?intelligence?, or
an abbreviation for Chile (?|). The comparison
between CharBased and MaxMatch is listed in Ta-
ble 2. The word??w (dementia) is unknown for
both segmenters. However, MaxMatch gave a better
translation of the character?. The issue here is not
that the ?????intelligence? entry never appears in
the phrase table of CharBased. The real issue is,
when ? means Chile, it is usually followed by the
character |. So by grouping them together, Max-
Match avoided falsely increasing the probability of
translating the stand-alone ? into Chile. Based on
our analysis, this ambiguity occurs the most when
the character-based system is dealing with a rare or
unseen character sequence in the training data, and
also occurs more often when dealing with translit-
3Except for dates and numbers.
Reference translation:
scientists complete sequencing of the chromosome linked to
early dementia
CharBased segmented input:
? ? [ ? M ' ? ? ? ? w ff / ? N  ? ? S
MaxMatch segmented input:
??[ ? M' ?? ? ? w ff /? N ? ? S
Translation with CharBased segmentation:
scientists at the beginning of the stake of chile lost the genome
sequence completed
Translation with MaxMatch segmentation:
scientists at stake for the early loss of intellectual syndrome
chromosome completed sequencing
Table 2: An example showing that character-based segmenta-
tion provides weaker ability to distinguish character with mul-
tiple unrelated meanings.
erations. The reason is that characters composing
a transliterated foreign named entity usually doesn?t
preserve their meanings; they are just used to com-
pose a Chinese word that sounds similar to the orig-
inal word ? much more like using a character seg-
mentation of English words. Another example of
this kind is ?C_?%?w? (Alzheimer?s dis-
ease). The MT system using CharBased segmenta-
tion tends to translate some characters individually
and drop others; while the system using MaxMatch
segmentation is more likely to translate it right.
The second advantage of having a segmenter like
the lexicon-based MaxMatch is that it helps the re-
ordering model. Results in Table 1 are with the
linear distortion limit defaulted to 6. Since words
in CharBased are inherently shorter than MaxMatch,
having the same distortion limit means CharBased
is limited to a smaller context than MaxMatch. To
make a fairer comparison, we set the linear distor-
tion limit in Moses to unlimited, removed the lexi-
calized reordering model, and retested both systems.
With this setting, MaxMatch is 0.46 BLEU point bet-
ter than CharBased (29.62 to 29.16) on MT03. This
result suggests that having word segmentation does
affect how the reordering model works in a phrase-
based system.
Hypothesis 2. Better Segmentation Performance
Should Lead to Better MT Performance
Observation We have shown in Hypothesis 1 that
it is helpful to segment Chinese texts into words
first. In order to decide a segmenter to use, the
most intuitive thing to do is to find one that gives
higher F measure on segmentation. Our experiments
show that higher F measure does not necessarily
227
lead to higher BLEU score. In order to contrast
with the simple maximum matching lexicon-based
model (MaxMatch), we built another segmenter with
a CRF model. CRF is a statistical sequence model-
ing framework introduced by Lafferty et al (2001),
and was first used for the Chinese word segmenta-
tion task by Peng et al (2004), who treated word
segmentation as a binary decision task. We opti-
mized the parameters with a quasi-Newton method,
and used Gaussian priors to prevent overfitting.
The probability assigned to a label sequence for a
particular sequence of characters by a CRF is given
by the equation:
p? (y|x) =
1
Z(x)
exp
T
?
t=1
K
?
k=1
?k fk(x,yt?1,yt , t) (1)
x is a sequence of T unsegmented characters, Z(x) is
the partition function that ensures that Equation 1 is
a probability distribution, { fk}Kk=1 is a set of feature
functions, and y is the sequence of binary predic-
tions for the sentence, where the prediction yt = +1
indicates the t-th character of the sequence is pre-
ceded by a space, and where yt =?1 indicates there
is none. We trained a CRF model with a set of ba-
sic features: character identity features of the current
character, previous character and next character, and
the conjunction of previous and current characters in
the zero-order templates. We will refer to this seg-
menter as CRF-basic.
Table 3 shows that the feature-based segmenter
CRF-basic outperforms the lexicon-based MaxMatch
by 5.9% relative F measure. Comparing the OOV re-
call rate and the IV recall rate, the reason is that CRF-
basic wins a lot on the OOV recall rate. We see that
a feature-based segmenter like CRF-basic clearly has
stronger ability to recognize unseen words. On
MT performance, however, CRF-basic is 0.38 BLEU
points worse than MaxMatch on the test set. In Sec-
tion 3.2, we will look at how theMT training and test
data are segmented by each segmenter, and provide
statistics and analysis for why certain segmenters are
better than others.
3.2 Consistency Analysis of Different
Segmenters
In Section 3.1 we have refuted two hypotheses. Now
we know that: (i) phrase table construction does not
fully capture what a word segmenter can do. Thus it
Segmentation Performance
Segmenter F measure OOV Recall IV Recall
CRF-basic 0.877 0.502 0.926
MaxMatch 0.828 0.012 0.951
CRF-Lex 0.940 0.729 0.970
MT Performance
Segmenter MT03 (dev) MT05 (test)
CRF-basic 33.01 30.35
MaxMatch 31.95 30.73
CRF-Lex 32.70 30.95
Table 3: CRF-basic vs MaxMatch
Segmenter #MT Training Lexicon Size #MT Test Lexicon Size
CRF-basic 583147 5443
MaxMatch 39040 5083
CRF-Lex 411406 5164
MT Test Lexicon OOV rate Conditional Entropy
CRF-basic 7.40% 0.2306
MaxMatch 0.49% 0.1788
CRF-Lex 4.88% 0.1010
Table 4: MT Lexicon Statistics and Conditional Entropy of Seg-
mentation Variations of three segmetners
is useful to have word segmentation for MT. (ii) a
higher F measure segmenter does not necessarily
outperforms on the MT task.
To understand what factors other than segmen-
tation F measure can affect MT performance, we
introduce another CRF segmenter CRF-Lex that in-
cludes lexicon-based features by using external lex-
icons. More details of CRF-Lex will be described
in Section 5.1. From Table 3, we see that the seg-
mentation F measure is that CRF-Lex > CRF-basic >
MaxMatch. And now we know that the better seg-
mentation F measure does not always lead to better
MT BLEU score, because of in terms of MT perfor-
mance, CRF-Lex > MaxMatch > CRF-basic.
In Table 4, we list some statistics of each seg-
menter to explain this phenomenon. First we look
at the lexicon size of the MT training and test data.
While segmenting the MT data, CRF-basic gener-
ates an MT training lexicon size of 583K unique
word tokens, and MaxMatch has a much smaller lex-
icon size of 39K. CRF-Lex performs best on MT,
but the MT training lexicon size and test lexicon
OOV rate is still pretty high compared to MaxMatch.
Only examining the MT training and test lexicon
size still doesn?t fully explain why CRF-Lex outper-
forms MaxMatch. MaxMatch generates a smaller MT
lexicon and lower OOV rate, but for MT it wasn?t
better than CRF-Lex, which has a bigger lexicon and
higher OOV rate. In order to understand why Max-
Match performs worse on MT than CRF-Lex but bet-
228
ter than CRF-basic, we use conditional entropy of
segmentation variations to measure consistency.
We use the gold segmentation of the SIGHAN
test data as a guideline. For every work type wi,
we collect all the different pattern variations vi j in
the segmentation we want to examine. For exam-
ple, for a word ?ABC? in the gold segmentation, we
look at how it is segmented with a segmenter. There
are many possibilities. If we use cx and cy to indi-
cate other Chinese characters and to indicate white
spaces, ?cx ABC cy? is the correct segmentation,
because the three characters are properly segmented
from both sides, and they are concatenated with each
other. It can also be segmented as ?cx A BC cy?,
which means although the boundary is correct, the
first character is separated from the other two. Or,
it can be segmented as ?cxA BCcy?, which means
the first character was actually part of the previous
word, while BC are the beginning of the next word.
Every time a particular word type wi appears in the
text, we consider a segmenter more consistent if it
can segment wi in the same way every time, but it
doesn?t necessarily have to be the same as the gold
standard segmentation. For example, if ?ABC? is a
Chinese person name which appears 100 times in the
gold standard data, and one segmenter segment it as
cx A BC cy 100 times, then this segmenter is still
considered to be very consistent, even if it doesn?t
exactly match the gold standard segmentation. Us-
ing this intuition, the conditional entropy of segmen-
tation variations H(V |W ) is defined as follows:
H(V |W ) = ??
wi
P(wi)?
vi j
P(vi j|wi) logP(vi j|wi)
= ??
wi
?
vi j
P(vi j,wi) logP(vi j|wi)
Now we can look at the overall conditional en-
tropy H(V |W ) to compare the consistency of each
segmenter. In Table 4, we can see that even though
MaxMatch has a much smaller MT lexicon size than
CRF-Lex, when we examine the consistency of how
MaxMatch segments in context, we find the condi-
tional entropy is much higher than CRF-Lex. We can
also see that CRF-basic has a higher conditional en-
tropy than the other two. The conditional entropy
H(V |W ) shows how consistent each segmenter is,
and it correlates with the MT performance in Ta-
ble 4. Note that consistency is only one of the com-
peting factors of how good a segmentation is for
MT performance. For example, a character-based
segmentation will always have the best consistency
possible, since every word ABC will just have one
pattern: cx A B C cy. But from Section 3.1 we
see that CharBased performs worse than both Max-
Match and CRF-basic on MT, because having word
segmentation can help the granularity of the Chinese
lexicon match that of the English lexicon.
In conclusion, for MT performance, it is helpful
to have consistent segmentation, while still having a
word segmentation matching the granularity of the
segmented Chinese lexicon and the English lexicon.
4 Optimal Average Token Length for MT
We have shown earlier that word-level segmentation
vastly outperforms character based segmentation in
MT evaluations. Since the word segmentation stan-
dard under consideration (Chinese Treebank (Xue
et al, 2005)) was neither specifically designed nor
optimized for MT, it seems reasonable to investi-
gate whether any segmentation granularity in con-
tinuum between character-level and CTB-style seg-
mentation is more effective for MT. In this section,
we present a technique for directly optimizing a seg-
mentation property?characters per token average?
for translation quality, which yields significant im-
provements in MT performance.
In order to calibrate the average word length pro-
duced by our CRF segmenter?i.e., to adjust the rate
of word boundary predictions (yt = +1), we apply
a relatively simple technique (Minkov et al, 2006)
originally devised for adjusting the precision/recall
tradeoff of any sequential classifier. Specifically, the
weight vector w and feature vector of a trained lin-
ear sequence classifier are augmented at test time
to include new class-conditional feature functions to
bias the classifier towards particular class labels. In
our case, since we wish to increase the frequency of
word boundaries, we add a feature function:
f0(x,yt?1,yt , t) =
{
1 if yt = +1
0 otherwise
Its weight ?0 controls the extent of which the classi-
fier will make positive predictions, with very large
positive ?0 values causing only positive predic-
tions (i.e., character-based segmentation) and large
negative values effectively disabling segmentation
boundaries. Table 5 displays how changes of the
229
?0 ?1 0 1 2 4 8 32
len 1.64 1.62 1.61 1.59 1.55 1.37 1
Table 5: Effect of the bias parameter ?0 on the average number
of character per token on MT data.
bias parameter ?0 affect segmentation granularity.4
Since we are interested in analyzing the different
regimes of MT performance between CTB segmen-
tation and character-based, we performed a grid
search in the range between ?0 = 0 (maximum-
likelihood estimate) and ?0 = 32 (a value that is
large enough to produce only positive predictions).
For each ?0 value, we ran an entire MT training and
testing cycle, i.e., we re-segmented the entire train-
ing data, ran GIZA++, acquired phrasal translations
that abide to this new segmentation, and ran MERT
and evaluations on segmented data using the same
?0 values.
 30
 30.5
 31
 31.5
 32
 32.5
 33
-3 -2 -1  0  1  2  3  4  5  6  7  8
bias
BLEU[%] scores
MT03(dev)MT02MT05
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
-3 -2 -1  0  1  2  3  4  5  6  7  8
bias
Segmentation performance
PrecisionRecallF measure
Figure 1: A bias towards more segment boundaries (?0 > 0)
yields better MT performance and worse segmentation results.
Segmentation and MT results are displayed in
Figure 1. First, we observe that an adjustment of
the precision and recall tradeoff by setting nega-
4Note that character-per-token averages provided in the ta-
ble consider each non-Chinese word (e.g., foreign names, num-
bers) as one character, since our segmentation post-processing
prevents these tokens from being segmented.
tive bias values (?0 = ?2) slightly improves seg-
mentation performance. We also notice that rais-
ing ?0 yields relatively consistent improvements in
MT performance, yet causes segmentation perfor-
mance (F measure) to be increasingly worse. While
the latter finding is not particularly surprising, it fur-
ther confirms that segmentation and MT evaluations
can yield rather different outcomes. We chose the
?0 = 2 on another dev set (MT02). On the test set
MT05, ?0 = 2 yields 31.47 BLEU, which represents
a quite large improvement compared to the unbiased
segmenter (30.95 BLEU). Further reducing the av-
erage number of characters per token yields gradual
drops of performance until character-level segmen-
tation (?0 ? 32, 29.36 BLEU).
Here are some examples of how setting ?0 = 2
shortens the words in a way that can help MT.
? separating adjectives and pre-modifying adverbs:
??(very big) ??(very)?(big)
? separating nouns and pre-modifying adjectives:
p??(high blood pressure)
?p(high)??(blood pressure)
? separating compound nouns:
S?(Department of Internal Affairs)
?S(Internal Affairs)?(Department).
5 Improving Segmentation Consistency of
a Feature-based Sequence Model for
Segmentation
In Section 3.1 we showed that a statistical sequence
model with rich features can generalize better than
maximum matching segmenters. However, it also
inconsistently over-generates a big MT training lexi-
con and OOVwords in MT test data, and thus causes
a problem for MT. To improve a feature-based se-
quence model for MT, we propose 4 different ap-
proaches to deal with named entities, optimal length
of word for MT and joint search for segmentation
and MT decoding.
5.1 Making Use of External Lexicons
One way to improve the consistency of the CRF
model is to make use of external lexicons (which
are not part of the segmentation training data) to
add lexicon-based features. All the features we use
are listed in Table 6. Our linguistic features are
adopted from (Ng and Low, 2004) and (Tseng et
al., 2005). There are three categories of features:
230
Lexicon-based Features Linguistic Features
(1.1) LBegin(Cn),n ? [?2,1] (2.1) Cn,n ? [?2,1]
(1.2) LMid(Cn),n ? [?2,1] (2.2) Cn?1Cn,n ? [?1,1]
(1.3) LEnd(Cn),n ? [?2,1] (2.3) Cn?2Cn,n ? [1,2]
(1.4) LEnd(C?1)+LEnd(C0) (2.4) Single(Cn),n ? [?2,1]
+LEnd(C1) (2.5) UnknownBigram(C?1C0)
(1.5) LEnd(C?2)+LEnd(C?1) (2.6) ProductiveA f f ixes(C?1,C0)
+LBegin(C0)+LMid(C0) (2.7) Reduplication(C?1,Cn),n ? [0,1]
(1.6) LEnd(C?2)+LEnd(C?1)
+LBegin(C?1)
+LBegin(C0)+LMid(C0)
Table 6: Features for CRF-Lex
character identity n-grams, morphological and char-
acter reduplication features. Our lexicon-based fea-
tures are adopted from (Shi and Wang, 2007), where
LBegin(C0), LMid(C0) and LEnd(C0) represent the
maximum length of words found in a lexicon that
contain the current character as either the first, mid-
dle or last character, and we group any length equal
or longer than 6 together. The linguistic features
help capturing words that were unseen to the seg-
menter; while the lexicon-based features constrain
the segmenter with external knowledge of what se-
quences are likely to be words.
We built a CRF segmenter with all the features
listed in Table 6 (CRF-Lex). The external lexicons
we used for the lexicon-based features come from
various sources including named entities collected
from Wikipedia and the Chinese section of the UN
website, named entities collected by Harbin Institute
of Technology, the ADSO dictionary, EMM News
Explorer, Online Chinese Tools, Online Dictionary
from Peking University and HowNet. There are
423,224 distinct entries in all the external lexicons.
The MT lexicon consistency of CRF-Lex in Table
4 shows that the MT training lexicon size has been
reduced by 29.5% and the MT test data OOV rate is
reduced by 34.1%.
5.2 Joint training of Word Segmentation and
Proper Noun Tagging
Named entities are an important source for OOV
words, and in particular are ones which it is bad to
break into pieces (particularly for foreign names).
Therefore, we use the proper noun (NR) part-of-
speech tag information from CTB to extend the label
sets of our CRF model from 2 to 4 ({beginning of a
word, continuation of a word} ? {NR, not NR}).
This is similar to the ?all-at-once, character-based?
POS tagging in (Ng and Low, 2004), except that
Segmentation Performance
Segmenter F measure OOV Recall IV Recall
CRF-Lex-NR 0.943 0.753 0.970
CRF-Lex 0.940 0.729 0.970
MT Performance
Segmenter MT03 (dev) MT05 (test)
CRF-Lex-NR 32.96 31.27
CRF-Lex 32.70 30.95
Table 7: CRF-Lex-NR vs CRF-Lex
we are only tagging proper nouns. We call the 4-
label extension CRF-Lex-NR. The segmentation and
MT performance of CRF-Lex-NR is listed in Table 7.
With the 4-label extension, the OOV recall rate im-
proved by 3.29%; while the IV recall rate stays the
same. Similar to (Ng and Low, 2004), we found the
overall F measure only goes up a tiny bit, but we do
find a significant OOV recall rate improvement.
On the MT performance, CRF-Lex-NR has a 0.32
BLEU gain on the test set MT05. In addition to the
BLEU improvement, CRF-Lex-NR also provides ex-
tra information about proper nouns, which can be
combined with postprocessing named entity transla-
tion modules to further improve MT performance.
6 Conclusion
In this paper, we investigated what segmentation
properties can improve machine translation perfor-
mance. First, we found that neither character-based
nor a standard word segmentation standard are opti-
mal for MT, and show that an intermediate granular-
ity is much more effective. Using an already com-
petitive CRF segmentation model, we directly opti-
mize segmentation granularity for translation qual-
ity, and obtain an improvement of 0.73 BLEU point
on MT05 over our lexicon-based segmentation base-
line. Second, we augment our CRF model with
lexicon and proper noun features in order to im-
prove segmentation consistency, which provide a
0.32 BLEU point improvement.
7 Acknowledgement
The authors would like to thank Menqgiu Wang and
Huihsin Tseng for useful discussions. This paper is
based on work funded in part by the Defense Ad-
vanced Research Projects Agency through IBM.
231
References
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.
2005. Chinese word segmentation and named entity
recognition: A pragmatic approach. Computational
Linguistics.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Short Pa-
pers, pages 49?52, New York City, USA, June. Asso-
ciation for Computational Linguistics.
Vera Hollink, Jaap Kamps, Christof Monz, and Maarten
de Rijke. 2004. Monolingual document retrieval for
European languages. Information Retrieval, 7(1).
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In EACL ?03: Proceed-
ings of the tenth conference on European chapter of
the Association for Computational Linguistics, pages
187?193. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning.
Gina-Anne Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmentation
and named entity recognition. In Proc. of the Fifth
SIGHAN Workshop on Chinese Language Processing,
July.
Einat Minkov, Richard Wang, Anthony Tomasic, and
William Cohen. 2006. NER systems that suit user?s
preferences: Adjusting the recall-precision trade-off
for entity extraction. In Proc. of NAACL-HLT, Com-
panion Volume: Short Papers, New York City, USA,
June.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? Word-
based or character-based? In Proc. of EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Fuchun Peng, Xiangji Huang, Dale Schuurmans, and
Nick Cercone. 2002. Investigating the relationship
between word segmentation performance and retrieval
performance in Chinese IR. In Proc. of the 19th Inter-
national Conference on Computational Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proc. of COLING.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRFs based joint decoding method for cascaded seg-
mentation and labeling tasks. In IJCAI.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for Sighan bake-
off 2005. In Proc. of the Fourth SIGHAN Workshop on
Chinese Language Processing.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical ma-
chine translation. In Proc. of the Third SIGHAN Work-
shop on Chinese Language Learning.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. Building a large annotated Chinese
corpus: the Penn Chinese treebank. Journal of Nat-
ural Language Engineering, 11(2).
232
 A Conditional Random Field Word Segmenter  
for Sighan Bakeoff 2005 
Huihsin Tseng 
Dept. of Linguistics 
University of Colorado 
Boulder, CO 80302 
tseng@colorado.edu
Pichuan Chang, Galen Andrew,  
Daniel Jurafsky, Christopher Manning 
Stanford Natural Language Processing Group 
Stanford University 
Stanford, CA 94309 
{pichuan, pupochik, jurafsky, manning}@stanford.edu 
Abstract
We present a Chinese word seg-
mentation system submitted to the 
closed track of Sighan bakeoff 2005. 
Our segmenter was built using a condi-
tional random field sequence model 
that provides a framework to use a 
large number of linguistic features such 
as character identity, morphological 
and character reduplication features. 
Because our morphological features 
were extracted from the training cor-
pora automatically, our system was not 
biased toward any particular variety of 
Mandarin. Thus, our system does not 
overfit the variety of Mandarin most 
familiar to the system's designers. Our 
final system achieved a F-score of 
0.947 (AS), 0.943 (HK), 0.950 (PK) 
and 0.964 (MSR). 
1 Introduction 
The 2005 Sighan Bakeoff included four dif-
ferent corpora, Academia Sinica (AS), City 
University of Hong Kong (HK), Peking Univer-
sity (PK), and Microsoft Research Asia (MSR), 
each of which has its own definition of a word. 
In the 2003 Sighan Bakeoff (Sproat & Emer-
son 2003), no single model performed well on 
all corpora included in the task. Rather, systems 
tended to do well on corpora largely drawn from 
a set of similar Mandarin varieties to the one 
they were originally developed for. Across cor-
pora, variation is seen in both the lexicons and 
also in the word segmentation standards. We 
concluded that, for future systems, generaliza-
tion across such different Mandarin varieties is 
crucial. To this end, we proposed a new model 
using character identity, morphological and 
character reduplication features in a conditional 
random field modeling framework. 
2 Algorithm
Our system builds on research into condi-
tional random field (CRF), a statistical sequence 
modeling framework first introduced by Lafferty 
et al (2001). Work by Peng et al (2004) first 
used this framework for Chinese word segmen-
tation by treating it as a binary decision task, 
such that each character is labeled either as the 
beginning of a word or the continuation of one. 
Gaussian priors were used to prevent overfitting 
and a quasi-Newton method was used for pa-
rameter optimization.  
The probability assigned to a label sequence 
for a particular sequence of characters by a CRF 
is given by the equation below: 
( ) ( )??
?
??
?
= ??
?Cc k
c cXYkkXZ
XYP f ,,exp)(
1| ??
Y is the label sequence for the sentence, X is 
the sequence of unsegmented characters, Z(X) is 
a normalization term, fk is a feature function, and 
c indexes into characters in the sequence being 
labeled.
A CRF allows us to utilize a large number of 
n-gram features and different state sequence 
168
based features and also provides an intuitive 
framework for the use of morphological features.  
3 Feature engineering 
3.1 Features
The linguistic features used in our model fall 
into three categories: character identity n-grams,
morphological and character reduplication fea-
tures.
For each state, the character identity features 
(Ng & Low 2004, Xue & Shen 2003, Goh et al 
2003) are represented using feature functions 
that key off of the identity of the character in the 
current, proceeding and subsequent positions. 
Specifically, we used four types of unigram fea-
ture functions, designated as C0 (current charac-
ter), C1 (next character), C-1 (previous character), 
C-2 (the character two characters back). Fur-
thermore, four types of bi-gram features were 
used, and are notationally designated here as 
conjunctions of the previously specified unigram 
features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0.
Given that unknown words are normally 
more than one character long, when representing 
the morphological features as feature functions, 
such feature functions keyed off the morpho-
logical information extracted from both the pro-
ceeding state and the current state. Our morpho-
logical features are based upon the intuition re-
garding unknown word features given in Gao et 
al. (2004). Specifically, their idea was to use 
productive affixes and characters that only oc-
curred independently to predict boundaries of 
unknown words. To construct a table containing 
affixes of unknown words, rather than using 
threshold-filtered affix tables in a separate un-
known word model as was done in Gao et al 
(2004), we first extracted rare words from a cor-
pus and then collected the first and last charac-
ters to construct the prefix and suffix tables. For 
the table of individual character words, we col-
lected an individual character word table for 
each corpus of the characters that always oc-
curred alone as a separate word in the given cor-
pus. We also collected a list of bi-grams from 
each training corpus to distinguish known 
strings from unknown. Adopting all the features 
together in a model and using the automatically 
generated morphological tables prevented our 
system from manually overfitting the Mandarin 
varieties we are most familiar with.  
The tables are used in the following ways: 
1) C-1+C0 unknown word feature functions 
were created for each specific pair of characters 
in the bi-gram tables. Such feature functions are 
active if the characters in the respective states 
match the corresponding feature function?s 
characters. These feature functions are designed 
to distinguish known strings from unknown.  
2) C-1, C0, and C1 individual character feature 
functions were created for each character in the 
individual character word table, and are likewise 
active if the respective character matches the 
feature function?s character. 
3) C-1 prefix feature functions are defined 
over characters in the prefix table, and fire if the 
character in the proceeding state matches the 
feature function?s character. 
4) C0 suffix feature functions are defined 
over suffix table characters, and fire if the char-
acter in the current state matches the feature 
function?s character. 
Additionally, we also use reduplication fea-
ture functions that are active based on the repeti-
tion of a given character. We used two such fea-
ture functions, one that fires if the previous and 
the current character, C-1 and C0, are identical 
and one that does so if the subsequent and the 
previous characters, C-1 and C1, are identical.  
Most features appeared in the first-order tem-
plates with a few of character identity features in 
the both zero-order and first-order templates. 
We also did normalization of punctuations due 
to the fact that Mandarin has a huge variety of 
punctuations.  
Table 1 shows the number of data features 
and lambda weights in each corpus.  
Table 1 The number of features in each corpus 
# of data features # of lambda weights 
AS 2,558,840 8,076,916
HK 2,308,067 7,481,164
PK 1,659,654 5,377,146
MSR 3,634,585 12,468,890
3.2 Experiments 
3.2.1 Results on Sighan bakeoff 2003 
Experiments done while developing this sys-
tem showed that its performance was signifi-
cantly better than that of Peng et al (2004).  
As seen in Table 2, our system?s F-score was 
0.863 on CTB (Chinese Treebank from Univer-
169
sity of Pennsylvania) versus 0.849 F on Peng et 
al. (2004). We do not at present have a good 
understanding of which aspects of our system 
give it superior performance. 
Table 2 Comparisons of Peng et al (2004) and our F-
score on the closed track in Sighan bakeoff 2003 
Sighan  
Bakeoff 2003 
Our F-score F-score 
Peng et al (2004) 
CTB 0.863 0.849 
AS 0.970 0.956 
HK 0.947 0.928 
PK 0.953 0.941 
3.2.2 Results on Sighan bakeoff 2005 
Our final system achieved a F-score of 0.947 
(AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). 
This shows that our system successfully general-
ized and achieved state of the art performance 
on all four corpora. 
Table 3 Performance of the features cumulatively, 
starting with the n-gram.  
F-score AS HK PK MSR
n-gram 0.943 0.946 0.950 0.961
n-gram (PU fixed)  0.953   
+Unk&redupl 0.947 0.943 0.950 0.964
+Unk&redupl 
(PU fixed) 
 0.952   
Table 3 lists our results on the four corpora. 
We give our results using just character identity 
based features; character identity features plus 
unknown words and reduplication features. Our 
unknown word features only helped on AS and 
MSR. Both of these corpora have words that 
have more characters than HK and PK. This in-
dicates that our unknown word features were 
more useful for corpora with segmentation stan-
dards that tend to result in longer words. 
In the HK corpus, when we added in un-
known word features, our performance dropped. 
However, we found that the testing data uses 
different punctuation than the training set. Our 
system could not distinguish new word charac-
ters from new punctuation, since having a com-
plete punctuation list is considered external 
knowledge for closed track systems. If the new 
punctuation were not unknown to us, our per-
formance on HK data would have gone up to 
0.952 F and the unknown word features would 
have not hurt the system too much. 
Table 4 present recalls (R), precisions (P), f-
scores (F) and recalls on both unknown (Roov)
and known words (Riv).
Table 4 Detailed performances of each corpus 
R P F Roov Riv
AS 0.950 0.943 0.947? 0.718? 0.960
HK 0.941 0.946 0.943? 0.698? 0.961
HK
(PU-fix)
0.952 0.952 0.952 0.791 0.965
PK 0.946 0.954 0.950? 0.787? 0.956
MSR 0.962 0.966 0.964? 0.717? 0.968
3.3 Error analysis 
Our system performed reasonably well on 
morphologically complex new words, such as 
??? (CABLE in AS) and ??? (MUR-
DER CASE in PK), where ? (LINE) and ?
(CASE) are suffixes. However, it over-
generalized to words with frequent suffixes such 
as ?? (it should be ? ? ?to burn some-
one? in PK) and ?? (it should be? ? ?
?to look backward? in PK). For the corpora that 
considered 4 character idioms as a word, our 
system combined most of new idioms together. 
This differs greatly from the results that one 
would likely obtain with a more traditional 
MaxMatch based technique, as such an algo-
rithm would segment novel idioms. 
One short coming of our system is that it is 
not robust enough to distinguish the difference 
between ordinal numbers and numbers with 
measure nouns. For example, ?? (3rd year) 
and ?? (three years) are not distinguishable 
to our system. In order to avoid this problem, it 
might require having more syntactic knowledge 
than was implicitly given in the training data.  
Finally, some errors are due to inconsisten-
cies in the gold segmentation of non-hanzi char-
acter. For example, ?Pentium4? is a word, but 
?PC133? is two words. Sometimes, ?8? is a 
word, but sometimes it is segmented into two 
words.
170
4 Conclusion
Our system used a conditional random field 
sequence model in conjunction with character 
identity features, morphological features and 
character reduplication features. We extracted 
our morphological information automatically to 
prevent overfitting Mandarin from particular 
Mandarin-speaking area. Our final system 
achieved a F-score of 0.947 (AS), 0.943 (HK), 
0.950 (PK) and 0.964 (MSR).  
5 Acknowledgment 
Thanks to Kristina Toutanova for her gener-
ous help and to Jenny Rose Finkel who devel-
oped such a great conditional random field 
package. This work was funded by the Ad-
vanced Research and Development Activity's 
Advanced Question Answering for Intelligence 
Program, National Science Foundation award 
IIS-0325646 and a Stanford Graduate Fellow-
ship.
References
Lafferty, John, A. McCallum, and F. Pereira. 2001. 
Conditional Random Field: Probabilistic Models 
for Segmenting and Labeling Sequence Data. In 
ICML 18. 
Gao, Jianfeng Andi Wu, Mu Li, Chang-Ning Huang, 
Hongqiao Li, Xinsong Xia and Haowei Qin. 2004. 
Adaptive Chinese word segmentation. In ACL-
2004.
Goh, Chooi-Ling, Masayuki Asahara, Yuji Matsu-
moto. 2003. Chinese unknown word identification 
using character-based tagging and chunking. In 
ACL 2003 Interactive Poster/Demo Sessions. 
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once? 
Word-Based or Character-Based? In EMNLP 9.
Peng, Fuchun, Fangfang Feng and Andrew 
McCallum. 2004. Chinese segmentation and new 
word detection using conditional random fields. In 
COLING 2004.
Sproat, Richard and Tom Emerson. 2003. The first 
international Chinese word segmentation bakeoff. 
In SIGHAN 2. 
Xue, Nianwen and Libin Shen. 2003. Chinese Word 
Segmentation as LMR Tagging. In SIGHAN 2.
171
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 9?16,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Discriminative Syntactic Word Order Model for Machine Translation
Pi-Chuan Chang?
Computer Science Department
Stanford University
Stanford, CA 94305
pichuan@stanford.edu
Kristina Toutanova
Microsoft Research
Redmond, WA
kristout@microsoft.com
Abstract
We present a global discriminative statistical
word order model for machine translation.
Our model combines syntactic movement
and surface movement information, and is
discriminatively trained to choose among
possible word orders. We show that com-
bining discriminative training with features
to detect these two different kinds of move-
ment phenomena leads to substantial im-
provements in word ordering performance
over strong baselines. Integrating this word
order model in a baseline MT system results
in a 2.4 points improvement in BLEU for
English to Japanese translation.
1 Introduction
The machine translation task can be viewed as con-
sisting of two subtasks: predicting the collection of
words in a translation, and deciding the order of the
predicted words. For some language pairs, such as
English and Japanese, the ordering problem is es-
pecially hard, because the target word order differs
significantly from the source word order.
Previous work has shown that it is useful to model
target language order in terms of movement of syn-
tactic constituents in constituency trees (Yamada
and Knight, 2001; Galley et al, 2006) or depen-
dency trees (Quirk et al, 2005), which are obtained
using a parser trained to determine linguistic con-
stituency. Alternatively, order is modelled in terms
of movement of automatically induced hierarchical
structure of sentences (Chiang, 2005; Wu, 1997).
? This research was conducted during the author?s intern-
ship at Microsoft Research.
The advantages of modeling how a target lan-
guage syntax tree moves with respect to a source lan-
guage syntax tree are that (i) we can capture the fact
that constituents move as a whole and generally re-
spect the phrasal cohesion constraints (Fox, 2002),
and (ii) we can model broad syntactic reordering
phenomena, such as subject-verb-object construc-
tions translating into subject-object-verb ones, as is
generally the case for English and Japanese.
On the other hand, there is also significant amount
of information in the surface strings of the source
and target and their alignment. Many state-of-the-art
SMT systems do not use trees and base the ordering
decisions on surface phrases (Och and Ney, 2004;
Al-Onaizan and Papineni, 2006; Kuhn et al, 2006).
In this paper we develop an order model for machine
translation which makes use of both syntactic and
surface information.
The framework for our statistical model is as fol-
lows. We assume the existence of a dependency tree
for the source sentence, an unordered dependency
tree for the target sentence, and a word alignment
between the target and source sentences. Figure 1
(a) shows an example of aligned source and target
dependency trees. Our task is to order the target de-
pendency tree.
We train a statistical model to select the best or-
der of the unordered target dependency tree. An im-
portant advantage of our model is that it is global,
and does not decompose the task of ordering a tar-
get sentence into a series of local decisions, as in the
recently proposed order models for Machine Transi-
tion (Al-Onaizan and Papineni, 2006; Xiong et al,
2006; Kuhn et al, 2006). Thus we are able to define
features over complete target sentence orders, and
avoid the independence assumptions made by these
9
all constraints are satisfied
[??] [??] [?] [???][???] [????]
?restriction??condition? TOPIC ?all? ?satisfy? PASSIVE-PRES
c d e f g h
(a)
fe cd g h
fe cd gh
fe cdg h
(b)
Figure 1: (a) A sentence pair with source depen-
dency tree, projected target dependency tree, and
word alignments. (b) Example orders violating the
target tree projectivity constraints.
models. Our model is discriminatively trained to se-
lect the best order (according to the BLEU measure)
(Papineni et al, 2001) of an unordered target depen-
dency tree from the space of possible orders.
Since the space of all possible orders of an un-
ordered dependency tree is factorially large, we train
our model on N-best lists of possible orders. These
N-best lists are generated using approximate search
and simpler models, as in the re-ranking approach of
(Collins, 2000).
We first evaluate our model on the task of ordering
target sentences, given correct (reference) unordered
target dependency trees. Our results show that com-
bining features derived from the source and tar-
get dependency trees, distortion surface order-based
features (like the distortion used in Pharaoh (Koehn,
2004)) and language model-like features results in a
model which significantly outperforms models using
only some of the information sources.
We also evaluate the contribution of our model
to the performance of an MT system. We inte-
grate our order model in the MT system, by simply
re-ordering the target translation sentences output
by the system. The model resulted in an improve-
ment from 33.6 to 35.4 BLEU points in English-to-
Japanese translation on a computer domain.
2 Task Setup
The ordering problem in MT can be formulated as
the task of ordering a target bag of words, given a
source sentence and word alignments between tar-
get and source words. In this work we also assume
a source dependency tree and an unordered target
dependency tree are given. Figure 1(a) shows an ex-
ample. We build a model that predicts an order of
the target dependency tree, which induces an order
on the target sentence words. The dependency tree
constrains the possible orders of the target sentence
only to the ones that are projective with respect to
the tree. An order of the sentence is projective with
respect to the tree if each word and its descendants
form a contiguous subsequence in the ordered sen-
tence. Figure 1(b) shows several orders of the sen-
tence which violate this constraint.1
Previous studies have shown that if both the
source and target dependency trees represent lin-
guistic constituency, the alignment between subtrees
in the two languages is very complex (Wellington et
al., 2006). Thus such parallel trees would be difficult
for MT systems to construct in translation. In this
work only the source dependency trees are linguisti-
cally motivated and constructed by a parser trained
to determine linguistic structure. The target depen-
dency trees are obtained through projection of the
source dependency trees, using the word alignment
(we use GIZA++ (Och and Ney, 2004)), ensuring
better parallelism of the source and target structures.
2.1 Obtaining Target Dependency Trees
Through Projection
Our algorithm for obtaining target dependency trees
by projection of the source trees via the word align-
ment is the one used in the MT system of (Quirk
et al, 2005). We describe the algorithm schemat-
ically using the example in Figure 1. Projection
of the dependency tree through alignments is not at
all straightforward. One of the reasons of difficulty
is that the alignment does not represent an isomor-
phism between the sentences, i.e. it is very often
not a one-to-one and onto mapping.2 If the align-
ment were one-to-one we could define the parent of
a word wt in the target to be the target word aligned
to the parent of the source word si aligned to wt. An
additional difficulty is that such a definition could re-
sult in a non-projective target dependency tree. The
projection algorithm of (Quirk et al, 2005) defines
heuristics for each of these problems. In case of
one-to-many alignments, for example, the case of
?constraints? aligning to the Japanese words for ?re-
striction? and ?condition?, the algorithm creates a
1For example, in the first order shown, the descendants of
word 6 are not contiguous and thus this order violates the con-
straint.
2In an onto mapping, every word on the target side is asso-
ciated with some word on the source side.
10
subtree in the target rooted at the rightmost of these
words and attaches the other word(s) to it. In case of
non-projectivity, the dependency tree is modified by
re-attaching nodes higher up in the tree. Such a step
is necessary for our example sentence, because the
translations of the words ?all? and ?constraints? are
not contiguous in the target even though they form a
constituent in the source.
An important characteristic of the projection algo-
rithm is that all of its heuristics use the correct target
word order.3 Thus the target dependency trees en-
code more information than is present in the source
dependency trees and alignment.
2.2 Task Setup for Reference Sentences vs MT
Output
Our model uses input of the same form when
trained/tested on reference sentences and when used
in machine translation: a source sentence with a de-
pendency tree, an unordered target sentence with
and unordered target dependency tree, and word
alignments.
We train our model on reference sentences. In this
setting, the given target dependency tree contains the
correct bag of target words according to a reference
translation, and is projective with respect to the cor-
rect word order of the reference by construction. We
also evaluate our model in this setting; such an eval-
uation is useful because we can isolate the contribu-
tion of an order model, and develop it independently
of an MT system.
When translating new sentences it is not possible
to derive target dependency trees by the projection
algorithm described above. In this setting, we use
target dependency trees constructed by our baseline
MT system (described in detail in 6.1). The system
constructs dependency trees of the form shown in
Figure 1 for each translation hypothesis. In this case
the target dependency trees very often do not con-
tain the correct target words and/or are not projective
with respect to the best possible order.
3For example, checking which word is the rightmost for the
heuristic for one-to-many mappings and checking whether the
constructed tree is projective requires knowledge of the correct
word order of the target.
3 Language Model with Syntactic
Constraints: A Pilot Study
In this section we report the results of a pilot study to
evaluate the difficulty of ordering a target sentence if
we are given a target dependency tree as the one in
Figure 1, versus if we are just given an unordered
bag of target language words.
The difference between those two settings is that
when ordering a target dependency tree, many of the
orders of the sentence are not allowed, because they
would be non-projective with respect to the tree.
Figure 1 (b) shows some orders which violate the
projectivity constraint. If the given target depen-
dency tree is projective with respect to the correct
word order, constraining the possible orders to the
ones consistent with the tree can only help perfor-
mance. In our experiments on reference sentences,
the target dependency trees are projective by con-
struction. If, however, the target dependency tree
provided is not necessarily projective with respect
to the best word order, the constraint may or may
not be useful. This could happen in our experiments
on ordering MT output sentences.
Thus in this section we aim to evaluate the use-
fulness of the constraint in both settings: reference
sentences with projective dependency trees, and MT
output sentences with possibly non-projective de-
pendency trees. We also seek to establish a baseline
for our task. Our methodology is to test a simple
and effective order model, which is used by all state
of the art SMT systems ? a trigram language model
? in the two settings: ordering an unordered bag of
words, and ordering a target dependency tree.
Our experimental design is as follows. Given an
unordered sentence t and an unordered target de-
pendency tree tree(t), we define two spaces of tar-
get sentence orders. These are the unconstrained
space of all permutations, denoted by Permutations(t)
and the space of all orders of t which are projec-
tive with respect to the target dependency tree, de-
noted by TargetProjective(t,tree(t)). For both spaces
S, we apply a standard trigram target language
model to select a most likely order from the space;
i.e., we find a target order order?S (t) such that:
order?S (t) = argmaxorder(t)?SPrLM (order(t)).
The operator which finds order?S (t) is difficult to
implement since the task is NP-hard in both set-
11
Reference Sentences
Space BLEU Avg. Size
Permutations 58.8 261
TargetProjective 83.9 229
MT Output Sentences
Space BLEU Avg. Size
Permutations 26.3 256
TargetProjective 31.7 225
Table 1: Performance of a tri-gram language model
on ordering reference and MT output sentences: un-
constrained or subject to target tree projectivity con-
straints.
tings, even for a bi-gram language model (Eisner
and Tromble, 2006).4 We implemented left-to-right
beam A* search for the Permutations space, and a
tree-based bottom up beam A* search for the Tar-
getProjective space. To give an estimate of the search
error in each case, we computed the number of times
the correct order had a better language model score
than the order returned by the search algorithm.5
The lower bounds on search error were 4% for Per-
mutations and 2% for TargetProjective, computed on
reference sentences.
We compare the performance in BLEU of orders
selected from both spaces. We evaluate the perfor-
mance on reference sentences and on MT output
sentences. Table 1 shows the results. In addition
to BLEU scores, the table shows the median number
of possible orders per sentence for the two spaces.
The highest achievable BLEU on reference sen-
tences is 100, because we are given the correct bag
of words. The highest achievable BLEU on MT out-
put sentences is well below 100 (the BLEU score of
the MT output sentences is 33). Table 3 describes
the characteristics of the main data-sets used in the
experiments in this paper; the test sets we use in the
present pilot study are the reference test set (Ref-
test) of 1K sentences and the MT test set (MT-test)
of 1,000 sentences.
The results from our experiment show that the tar-
get tree projectivity constraint is extremely powerful
on reference sentences, where the tree given is in-
deed projective. (Recall that in order to obtain the
target dependency tree in this setting we have used
information from the true order, which explains in
part the large performance gain.)
4Even though the dependency tree constrains the space, the
number of children of a node is not bounded by a constant.
5This is an underestimate of search error, because we don?t
know if there was another (non-reference) order which had a
better score, but was not found.
The gain in BLEU due to the constraint was not
as large on MT output sentences, but was still con-
siderable. The reduction in search space size due
to the constraint is enormous. There are about 230
times fewer orders to consider in the space of tar-
get projective orders, compared to the space of all
permutations. From these experiments we conclude
that the constraints imposed by a projective target
dependency tree are extremely informative. We also
conclude that the constraints imposed by the target
dependency trees constructed by our baseline MT
system are very informative as well, even though
the trees are not necessarily projective with respect
to the best order. Thus the projectivity constraint
with respect to a reasonably good target dependency
tree is useful for addressing the search and modeling
problems for MT ordering.
4 A Global Order Model for Target
Dependency Trees
In the rest of the paper we present our new word or-
der model and evaluate it on reference sentences and
in machine translation. In line with previous work
on NLP tasks such as parsing and recent work on
machine translation, we develop a discriminative or-
der model. An advantage of such a model is that we
can easily combine different kinds of features (such
as syntax-based and surface-based), and that we can
optimize the parameters of our model directly for the
evaluation measures of interest.
Additionally, we develop a globally normalized
model, which avoids the independence assumptions
in locally normalized conditional models.6 We train
a global log-linear model with a rich set of syntactic
and surface features. Because the space of possible
orders of an unordered dependency tree is factori-
ally large, we use simpler models to generate N-best
orders, which we then re-rank with a global model.
4.1 Generating N-best Orders
The simpler models which we use to generate N-best
orders of the unordered target dependency trees are
the standard trigram language model used in Section
3, and another statistical model, which we call a Lo-
cal Tree Order Model (LTOM). The LTOM model
6Those models often assume that current decisions are inde-
pendent of future observations.
12
[??]
this-1 eliminates the six minute delay+1
[? ? -2] [?? ? ] [6] [?] [?] [? ] [?? -1] [? ] [? ?? ? ]
Pron Verb Det Funcw Funcw Noun
[kore] [niyori] [roku] [fun] [kan] [no] [okure] [ga] [kaishou] [saremasu]
Pron Posp Noun Noun Noun Posp Noun Posp Vn Auxv
?this? ?by? 6 ?minute? ?period? ?of? ?delay? ?eliminate? PASSIVE
Figure 2: Dependency parse on the source (English)
sentence, alignment and projected tree on the target
(Japanese) sentence. Notice that the projected tree
is only partial and is used to show the head-relative
movement.
uses syntactic information from the source and tar-
get dependency trees, and orders each local tree of
the target dependency tree independently. It follows
the order model defined in (Quirk et al, 2005).
The model assigns a probability to the position
of each target node (modifier) relative to its par-
ent (head), based on information in both the source
and target trees. The probability of an order of the
complete target dependency tree decomposes into a
product over probabilities of positions for each node
in the tree as follows:
P (order(t)|s, t) =
?
n?t
P (pos(n, parent(n))|s, t)
Here, position is modelled in terms of closeness
to the head in the dependency tree. The closest
pre-modifier of a given head has position ?1; the
closest post-modifier has a position 1. Figure 2
shows an example dependency tree pair annotated
with head-relative positions. A small set of features
is used to reflect local information in the dependency
tree to model P (pos(n, parent(n))|s, t): (i) lexical
items of n and parent(n), (ii) lexical items of the
source nodes aligned to n and parent(n), (iii) part-
of-speech of the source nodes aligned to the node
and its parent, and (iv) head-relative position of the
source node aligned to the target node.
We train a log-linear model which uses these fea-
tures on a training set of aligned sentences with
source and target dependency trees in the form of
Figure 2. The model is a local (non-sequence) clas-
sifier, because the decision on where to place each
node does not depend on the placement of any other
nodes.
Since the local tree order model learns to order
whole subtrees of the target dependency tree, and
since it uses syntactic information from the source, it
provides an alternative view compared to the trigram
language model. The example in Figure 2 shows
that the head word ?eliminates? takes a dependent
?this? to the left (position ?1), and on the Japanese
side, the head word ?kaishou? (corresponding to
?eliminates?) takes a dependent ?kore? (correspond-
ing to ?this?) to the left (position ?2). The trigram
language model would not capture the position of
?kore? with respect to ?kaishou?, because the words
are farther than three positions away.
We use the language model and the local tree or-
der model to create N-best target dependency tree
orders. In particular, we generate the N-best lists
from a simple log-linear combination of the two
models:
P (o(t)|s, t) ? PLM (o(t)|t)PLTOM (o(t)|s, t)?
where o(t) denotes an order of the target.7 We used
a bottom-up beam A* search to generate N-best or-
ders. The performance of each of these two models
and their combination, together with the 30-best or-
acle performance on reference sentences is shown in
Table 2. As we can see, the 30-best oracle perfor-
mance of the combined model (98.0) is much higher
than the 1-best performance (92.6) and thus there is
a lot of room for improvement.
4.2 Model
The log-linear reranking model is defined as fol-
lows. For each sentence pair spl (l = 1, 2, ..., L) in
the training data, we have N candidate target word
orders ol,1, ol,2, ..., ol,N , which are the orders gener-
ated from the simpler models. Without loss of gen-
erality, we define ol,1 to be the order with the highest
BLEU score with respect to the correct order.8
We define a set of feature functions fm(ol,n, spl)
to describe a target word order ol,n of a given sen-
tence pair spl. In the log-linear model, a correspond-
ing weights vector ? is used to define the distribution
over all possible candidate orders:
p(ol,n|spl, ?) = e
?F (ol,n,spl)
?
n? e
?F (ol,n? ,spl)
7We used the value ? = .5, which we selected on a devel-
opment set to maximize BLEU.
8To avoid the problem that all orders could have a BLEU
score of 0 if none of them contains a correct word four-gram,
we define sentence-level k-gram BLEU, where k is the highest
order, k ? 4, for which there exists a correct k-gram in at least
one of the N-Best orders.
13
We train the parameters ? by minimizing the neg-
ative log-likelihood of the training data plus a
quadratic regularization term:
L(?) = ??l log p(ol,1|spi, ?) + 12?2
?
m ?m2
We also explored maximizing expected BLEU as
our objective function, but since it is not convex, the
performance was less stable and ultimately slightly
worse, as compared to the log-likelihood objective.
4.3 Features
We design features to capture both the head-relative
movement and the surface sequence movement of
words in a sentence. We experiment with different
combinations of features and show their contribu-
tion in Table 2 for reference sentences and Table 4
in machine translation. The notations used in the ta-
bles are defined as follows:
Baseline: LTOM+LM as described in Section 4.1
Word Bigram: Word bigrams of the target sen-
tence. Examples from Figure 2: ?kore?+?niyori?,
?niyori?+?roku?.
DISP: Displacement feature. For each word posi-
tion in the target sentence, we examine the align-
ment of the current word and the previous word, and
categorize the possible patterns into 3 kinds: (a) par-
allel, (b) crossing, and (c) widening. Figure 3 shows
how these three categories are defined.
Pharaoh DISP: Displacement as used in Pharaoh
(Koehn, 2004). For each position in the sentence,
the value of the feature is one less than the difference
(absolute value) of the positions of the source words
aligned to the current and the previous target word.
POSs and POSt: POS tags on the source and target
sides. For Japanese, we have a set of 19 POS tags.
?+? means making conjunction of features and
prev() means using the information associated with
the word from position ?1.
In all explored models, we include the log-
probability of an order according to the language
model and the log-probability according to the lo-
cal tree order model, the two features used by the
baseline model.
5 Evaluation on Reference Sentences
Our experiments on ordering reference sentences
use a set of 445K English sentences with their ref-
erence Japanese translations. This is a subset of the
(N (N
-L -L
(a) parallel
(N (NQ
-L -L
(b) crossing
(N (NQ
-L -L
(c) widening
Figure 3: Displacement feature: different alignment
patterns of two contiguous words in the target sen-
tence.
set MT-train in Table 3. The sentences were anno-
tated with alignment (using GIZA++ (Och and Ney,
2004)) and syntactic dependency structures of the
source and target, obtained as described in Section
2. Japanese POS tags were assigned by an automatic
POS tagger, which is a local classifier not using tag
sequence information.
We used 400K sentence pairs from the complete
set to train the first pass models: the language model
was trained on 400K sentences, and the local tree
order model was trained on 100K of them. We gen-
erated N-best target tree orders for the rest of the
data (45K sentence pairs), and used it for training
and evaluating the re-ranking model. The re-ranking
model was trained on 44K sentence pairs. All mod-
els were evaluated on the remaining 1,000 sentence
pairs set, which is the set Ref-test in Table 3.
The top part of Table 2 presents the 1-best
BLEU scores (actual performance) and 30-best or-
acle BLEU scores of the first-pass models and their
log-linear combination, described in Section 4. We
can see that the combination of the language model
and the local tree order model outperformed either
model by a large margin. This indicates that combin-
ing syntactic (from the LTOM model) and surface-
based (from the language model) information is very
effective even at this stage of selecting N-best orders
for re-ranking. According to the 30-best oracle per-
formance of the combined model LTOM+LM, 98.0
BLEU is the upper bound on performance of our re-
ranking approach.
The bottom part of the table shows the perfor-
mance of the global log-linear model, when features
in addition to the scores from the two first-pass mod-
els are added to the model. Adding word-bigram
features increased performance by about 0.6 BLEU
points, indicating that training language-model like
features discriminatively to optimize ordering per-
formance, is indeed worthwhile. Next we compare
14
First-pass models
Model BLEU
1 best 30 best
Lang Model (Permutations) 58.8 71.2
Lang Model (TargetProjective) 83.9 95.0
Local Tree Order Model 75.8 87.3
Local Tree Order Model + Lang Model 92.6 98.0
Re-ranking Models
Features BLEU
Baseline 92.60
Word Bigram 93.19
Pharaoh DISP 92.94
DISP 93.57
DISP+POSs 94.04
DISP+POSs+POSt 94.14
DISP+POSs+POSt, prev(DISP)+POSs+POSt 94.34
DISP+POSs+POSt, prev(DISP)+POSs+POSt, WB 94.50
Table 2: Performance of the first-pass order models
and 30-best oracle performance, followed by perfor-
mance of re-ranking model for different feature sets.
Results are on reference sentences.
the Pharaoh displacement feature to the displace-
ment feature we illustrated in Figure 3. We can
see that the Pharaoh displacement feature improves
performance of the baseline by .34 points, whereas
our displacement feature improves performance by
nearly 1 BLEU point. Concatenating the DISP fea-
ture with the POS tag of the source word aligned to
the current word improved performance slightly.
The results show that surface movement features
(i.e. the DISP feature) improve the performance
of a model using syntactic-movement features (i.e.
the LTOM model). Additionally, adding part-of-
speech information from both languages in combi-
nation with displacement, and using a higher order
on the displacement features was useful. The per-
formance of our best model, which included all in-
formation sources, is 94.5 BLEU points, which is a
35% improvement over the fist-pass models, relative
to the upper bound.
6 Evaluation in Machine Translation
We apply our model to machine translation by re-
ordering the translation produced by a baseline MT
system. Our baseline MT system constructs, for
each target translation hypothesis, a target depen-
dency tree. Thus we can apply our model to MT
output in exactly the same way as for reference sen-
tences, but using much noisier input: a source sen-
tence with a dependency tree, word alignment and
an unordered target dependency tree as the example
shown in Figure 2. The difference is that the target
dependency tree will likely not contain the correct
data set num sent. English Japanese
avg. len vocab avg. len vocab
MT-train 500K 15.8 77K 18.7 79K
MT-test 1K 17.5 ? 20.9 ?
Ref-test 1K 17.5 ? 21.2 ?
Table 3: Main data sets used in experiments.
target words and/or will not be projective with re-
spect to the best possible order.
6.1 Baseline MT System
Our baseline SMT system is the system of Quirk et
al. (2005). It translates by first deriving a depen-
dency tree for the source sentence and then trans-
lating the source dependency tree to a target depen-
dency tree, using a set of probabilistic models. The
translation is based on treelet pairs. A treelet is a
connected subgraph of the source or target depen-
dency tree. A treelet translation pair is a pair of
word-aligned source and target treelets.
The baseline SMT model combines this treelet
translation model with other feature functions ? a
target language model, a tree order model, lexical
weighting features to smooth the translation prob-
abilities, word count feature, and treelet-pairs count
feature. These models are combined as feature func-
tions in a (log)linear model for predicting a target
sentence given a source sentence, in the framework
proposed by (Och and Ney, 2002). The weights
of this model are trained to maximize BLEU (Och
and Ney, 2004). The SMT system is trained using
the same form of data as our order model: parallel
source and target dependency trees as in Figure 2.
Of particular interest are the components in the
baseline SMT system contributing most to word or-
der decisions. The SMT system uses the same target
language trigram model and local tree order model,
as we are using for generating N-best orders for re-
ranking. Thus the baseline system already uses our
first-pass order models and only lacks the additional
information provided by our re-ranking order model.
6.2 Data and Experimental Results
The baseline MT system was trained on the MT-train
dataset described in Table 3. The test set for the MT
experiment is a 1K sentences set from the same do-
main (shown as MT-test in the table). The weights
in the linear model used by the baseline SMT system
were tuned on a separate development set.
Table 4 shows the performance of the first-pass
models in the top part, and the performance of our
15
First-pass models
Model BLEU
1 best 30 best
Baseline MT System 33.0 ?
Lang Model (Permutations) 26.3 28.7
Lang Model (TargetCohesive) 31.7 35.0
Local Tree Order Model 27.2 31.5
Local Tree Order Model + Lang Model 33.6 36.0
Re-ranking Models
Features BLEU
Baseline 33.56
Word Bigram 34.11
Pharaoh DISP 34.67
DISP 34.90
DISP+POSs 35.28
DISP+POSs+POSt 35.22
DISP+POSs+POSt, prev(DISP)+POSs+POSt 35.33
DISP+POSs+POSt, prev(DISP)+POSs+POSt, WB 35.37
Table 4: Performance of the first pass order models
and 30-best oracle performance, followed by perfor-
mance of re-ranking model for different feature sets.
Results are in MT.
re-ranking model in the bottom part. The first row
of the table shows the performance of the baseline
MT system, which is a BLEU score of 33. Our first-
pass and re-ranking models re-order the words of
this 1-best output from the MT system. As for ref-
erence sentences, the combination of the two first-
pass models outperforms the individual models. The
1-best performance of the combination is 33.6 and
the 30-best oracle is 36.0. Thus the best we could
do with our re-ranking model in this setting is 36
BLEU points.9 Our best re-ranking model achieves
2.4 BLEU points improvement over the baseline MT
system and 1.8 points improvement over the first-
pass models, as shown in the table. The trends here
are similar to the ones observed in our reference ex-
periments, with the difference that target POS tags
were less useful (perhaps due to ungrammatical can-
didates) and the displacement features were more
useful. We can see that our re-ranking model al-
most reached the upper bound oracle performance,
reducing the gap between the first-pass models per-
formance (33.6) and the oracle (36.0) by 75%.
7 Conclusions and Future Work
We have presented a discriminative syntax-based or-
der model for machine translation, trained to to se-
9Notice that the combination of our two first-pass models
outperforms the baseline MT system by half a point (33.6 ver-
sus 33.0). This is perhaps due to the fact that the MT system
searches through a much larger space (possible word transla-
tions in addition to word orders), and thus could have a higher
search error.
lect from the space of orders projective with respect
to a target dependency tree. We investigated a com-
bination of features modeling surface movement and
syntactic movement phenomena and showed that
these two information sources are complementary
and their combination is powerful. Our results on or-
dering MT output and reference sentences were very
encouraging. We obtained substantial improvement
by the simple method of post-processing the 1-best
MT output to re-order the proposed translation. In
the future, we would like to explore tighter integra-
tion of our order model with the SMT system and to
develop more accurate algorithms for constructing
projective target dependency trees in translation.
References
Y. Al-Onaizan and K. Papineni. 2006. Distortion models for
statistical machine translation. In ACL.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In ACL.
M. Collins. 2000. Discriminative reranking for natural language
parsing. In ICML, pages 175?182.
J Eisner and R. W. Tromble. 2006. Local search with very
large-scale neighborhoods for optimal permutations in ma-
chine translation. In HLT-NAACL Workshop.
H. Fox. 2002. Phrasal cohesion and statistical machine transla-
tion. In EMNLP.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models. In ACL.
P. Koehn. 2004. Pharaoh: A beam search decoder for phrase-
based statistical machine translation models. In AMTA.
R. Kuhn, D. Yuen, M. Simard, P. Paul, G. Foster, E. Joanis, and
H. Johnson. 2006. Segment choice models: Feature-rich
models for global distortion in statistical machine transla-
tion. In HLT-NAACL.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
ACL.
F. J. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguistics,
30(4).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. BLEU: a
method for automatic evaluation of machine translation. In
ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet
translation: Syntactically informed phrasal SMT. In ACL.
B. Wellington, S. Waxmonsky, and I. Dan Melamed. 2006.
Empirical lower bounds on the complexity of translational
equivalence. In ACL-COLING.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy based
phrase reordering model for statistical machine translation.
In ACL.
K. Yamada and Kevin Knight. 2001. A syntax-based statistical
translation model. In ACL.
16
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96?103,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Detecting Action Items in Audio Meeting Recordings
William Morgan Pi-Chuan Chang Surabhi Gupta
Department of Computer Science
Stanford University
353 Serra Mall
Stanford, CA 94305-9205
ruby@cs.stanford.edu
pcchang@cs.stanford.edu
surabhi@cs.stanford.edu
Jason M. Brenier
Department of Linguistics
Center for Spoken Language Research
Institute of Cognitive Science
University of Colorado at Boulder
594 UCB
Boulder, Colorado 80309-0594
jbrenier@colorado.edu
Abstract
Identification of action items in meeting
recordings can provide immediate access
to salient information in a medium noto-
riously difficult to search and summarize.
To this end, we use a maximum entropy
model to automatically detect action item-
related utterances from multi-party audio
meeting recordings. We compare the ef-
fect of lexical, temporal, syntactic, seman-
tic, and prosodic features on system per-
formance. We show that on a corpus of ac-
tion item annotations on the ICSI meeting
recordings, characterized by high imbal-
ance and low inter-annotator agreement,
the system performs at an F measure of
31.92%. While this is low compared to
better-studied tasks on more mature cor-
pora, the relative usefulness of the features
towards this task is indicative of their use-
fulness on more consistent annotations, as
well as to related tasks.
1 Introduction
Meetings are a ubiquitous feature of workplace
environments, and recordings of meetings pro-
vide obvious benefit in that they can be replayed
or searched through at a later date. As record-
ing technology becomes more easily available and
storage space becomes less costly, the feasibil-
ity of producing and storing these recordings in-
creases. This is particularly true for audio record-
ings, which are cheaper to produce and store than
full audio-video recordings.
However, audio recordings are notoriously diffi-
cult to search or to summarize. This is doubly true
of multi-party recordings, which, in addition to the
difficulties presented by single-party recordings,
typically contain backchannels, elaborations, and
side topics, all of which further confound search
and summarization processes. Making efficient
use of large meeting corpora thus requires intel-
ligent summary and review techniques.
One possible user goal given a corpus of meet-
ing recordings is to discover the action items de-
cided within the meetings. Action items are deci-
sions made within the meeting that require post-
meeting attention or labor. Rapid identification
of action items can provide immediate access to
salient portions of the meetings. A review of ac-
tion items can also function as (part of) a summary
of the meeting content.
To this end, we explore the task of applying
maximum entropy classifiers to the task of auto-
matically detecting action item utterances in au-
dio recordings of multi-party meetings. Although
available corpora for action items are not ideal, it
is hoped that the feature analysis presented here
will be of use to later work on other corpora.
2 Related work
Multi-party meetings have attracted a significant
amount of recent research attention. The creation
of the ICSI corpus (Janin et al, 2003), comprised
of 72 hours of meeting recordings with an average
of 6 speakers per meeting, with associated tran-
scripts, has spurred further annotations for var-
ious types of information, including dialog acts
(Shriberg et al, 2004), topic hierarchies and action
items (Gruenstein et al, 2005), and ?hot spots?
(Wrede and Shriberg, 2003).
The classification of individual utterances based
on their role in the dialog, i.e. as opposed to their
semantic payload, has a long history, especially
in the context of dialog act (DA) classification.
96
Research on DA classification initially focused
on two-party conversational speech (Mast et al,
1996; Stolcke et al, 1998; Shriberg et al, 1998)
and, more recently, has extended to multi-party
audio recordings like the ICSI corpus (Shriberg
et al, 2004). Machine learning techniques such
as graphical models (Ji and Bilmes, 2005), maxi-
mum entropy models (Ang et al, 2005), and hid-
den Markov models (Zimmermann et al, 2005)
have been used to classify utterances from multi-
party conversations.
It is only more recently that work focused
specifically on action items themselves has been
developed. SVMs have been successfully applied
to the task of extracting action items from email
messages (Bennett and Carbonell, 2005; Corston-
Oliver et al, 2004). Bennett and Carbonell, in par-
ticular, distinguish the task of action item detec-
tion in email from the more well-studied task of
text classification, noting the finer granularity of
the action item task and the difference of seman-
tics vs. intent. (Although recent work has begun to
blur this latter division, e.g. Cohen et al (2004).)
In the audio domain, annotations for action item
utterances on several recorded meeting corpora,
including the ICSI corpus, have recently become
available (Gruenstein et al, 2005), enabling work
on this topic.
3 Data
We use action item annotations produced by Gru-
enstein et al (2005). This corpus provides topic
hierarchy and action item annotations for the ICSI
meeting corpus as well as other corpora of meet-
ings; due to the ready availability of other types of
annotations for the ICSI corpus, we focus solely
on the annotations for these meetings. Figure 1
gives an example of the annotations.
The corpus covers 54 ICSI meetings annotated
by two human annotators, and several other meet-
ings annotated by one annotator. Of the 54 meet-
ings with dual annotations, 6 contain no action
items. For this study we consider only those meet-
ings which contain action items and which are an-
notated by both annotators.
As the annotations were produced by a small
number of untrained annotators, an immediate
question is the degree of consistency and reliabil-
ity. Inter-annotator agreement is typically mea-
sured by the kappa statistic (Carletta, 1996), de-
kappa
fre
qu
en
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
Figure 2: Distribution of ? (inter-annotator agree-
ment) across the 54 ICSI meetings tagged by two
annotators. Of the two meetings with ? = 1.0, one
has only two action items and the other only four.
fined as:
? = P (O) ? P (E)1 ? P (E)
where P (O) is the probability of the observed
agreement, and P (E) the probability of the ?ex-
pected agreement? (i.e., under the assumption the
two sets of annotations are independent). The
kappa statistic ranges from ?1 to 1, indicating per-
fect disagreement and perfect agreement, respec-
tively.
Overall inter-annotator agreement as measured
by ? on the action item corpus is poor, as noted in
Purver et al (2006), with an overall ? of 0.364 and
values for individual meetings ranging from 1.0 to
less than zero. Figure 2 shows the distribution of
? across all 54 annotated ICSI meetings.
To reduce the effect of poor inter-annotator
agreement, we focus on the top 15 meetings as
ranked by ?; the minimum ? in this set is 0.435.
Although this reduces the total amount of data
available, our intention is that this subset of the
most consistent annotations will form a higher-
quality corpus.
While the corpus classifies related action item
utterances into action item ?groups,? in this study
we wish to treat the annotations as simply binary
attributes. Visual analysis of annotations for sev-
eral meetings outside the set of chosen 15 suggests
that the union of the two sets of annotations yields
the most consistent resulting annotation; thus, for
this study, we consider an utterance to be an action
item if at least one of the annotators marked it as
such.
The 15-meeting subset contains 24,250 utter-
97
A1 A2
X X So that will be sort of the assignment for next week, is to?
X X to?for slides and whatever net you picked and what it can do and?and how far
you?ve gotten. Pppt!
X - Well, I?d like to also,
X X though, uh, ha- have a first cut at what the
X X belief-net looks like.
- X Even if it?s really crude.
- - OK? So, you know,
- - here a- here are?
- X So we?re supposed to @@ about features and whatnot, and?
Figure 1: Example transcript and action item annotations (marked ?X?) from annotators A1 and A2.
?@@? signifies an unintelligible word. This transcript is from an ICSI meeting recording and has ? =
0.373, ranking it 16th out of 54 meetings in annotator agreement.
0 500 1000 1500 2000 2500
Figure 3: Number of total and action item utter-
ances across the 15 selected meetings. There are
24,250 utterances total, 590 of which (2.4%) are
action item utterances.
ances total; under the union strategy above, 590 of
these are action item utterances. Figure 3 shows
the number of action item utterances and the num-
ber of total utterances in the 15 selected meetings.
One noteworthy feature of the ICSI corpus un-
derlying the action item annotations is the ?digit
reading task,? in which the participants of meet-
ings take turns reading aloud strings of digits.
This task was designed to provide a constrained-
vocabulary training set of speech recognition de-
velopers interested in multi-party speech. In this
study we did not remove these sections; the net
effect is that some portions of the data consist of
these fairly atypical utterances.
4 Experimental methodology
We formulate the action item detection task as one
of binary classification of utterances. We apply a
maximum entropy (maxent) model (Berger et al,
1996) to this task.
Maxent models seek to maximize the condi-
tional probability of a class c given the observa-
tions X using the exponential form
P (c|X) = 1Z(X) exp
[
?
i
?i,c fi,c(X)
]
where fi,c(X) is the ith feature of the data X
in class c, ?i,c is the corresponding weight, and
Z(X) is a normalization term. Maxent models
choose the weights ?i,c so as to maximize the en-
tropy of the induced distribution while remaining
consistent with the data and labels; the intuition is
that such a distribution makes the fewest assump-
tions about the underlying data.
Our maxent model is regularized by a quadratic
prior and uses quasi-Newton parameter optimiza-
tion. Due to the limited amount of training data
(see Section 3) and to avoid overfitting, we em-
ploy 10-fold cross validation in each experiment.
To evaluate system performance, we calculate
the F measure (F ) of precision (P ) and recall (R),
defined as:
P = |A ? C||A|
R = |A ? C||C|
F = 2PRP + R
where A is the set of utterances marked as action
items by the system, and C is the set of (all) cor-
rect action item utterances.
98
The use of precision and recall is motivated by
the fact that the large imbalance between posi-
tive and negative examples in the corpus (Sec-
tion 3) means that simpler metrics like accuracy
are insufficient?a system that simply classifies
every utterance as negative will achieve an accu-
racy of 97.5%, which clearly is not a good reflec-
tion of desired behavior. Recall and F measure for
such a system, however, will be zero.
Likewise, a system that flips a coin weighted in
proportion to the number of positive examples in
the entire corpus will have an accuracy of 95.25%,
but will only achieve P = R = F = 2.4%.
5 Features
As noted in Section 3, we treat the task of produc-
ing action item annotations as a binary classifica-
tion task. To this end, we consider the following
sets of features. (Note that all real-valued features
were range-normalized so as to lie in [0, 1] and that
no binning was employed.)
5.1 Immediate lexical features
We extract word unigram and bigram features
from the transcript for each utterance. We nor-
malize for case and for certain contractions; for
example, ?I?ll? is transformed into ?I will?.
Note that these are oracle features, as the tran-
scripts are human-produced and not the product
of automatic speech recognizer (ASR) system out-
put.
5.2 Contextual lexical features
We extract word unigram and bigram features
from the transcript for the previous and next ut-
terances across all speakers in the meeting.
5.3 Syntactic features
Under the hypothesis that action item utterances
will exhibit particular syntactic patterns, we use
a conditional Markov model part-of-speech (POS)
tagger (Toutanova and Manning, 2000) trained on
the Switchboard corpus (Godfrey et al, 1992) to
tag utterance words for part of speech. We use the
following binary POS features:
? Presence of UH tag, denoting the presence of
an ?interjection? (including filled pauses, un-
filled pauses, and discourse markers).
? Presence of MD tag, denoting presence of a
modal verb.
? Number of NN* tags, denoting the number of
nouns.
? Number of VB* tags, denoting the number of
verbs.
? Presence of VBD tag, denoting the presence
of a past-tense verb.
5.4 Prosodic features
Under the hypothesis that action item utterances
will exhibit particular prosodic behavior?for ex-
ample, that they are emphasized, or are pitched a
certain way?we performed pitch extraction using
an auto-correlation method within the sound anal-
ysis package Praat (Boersma and Weenink, 2005).
From the meeting audio files we extract the fol-
lowing prosodic features, on a per-utterance basis:
(pitch measures are in Hz; intensity in energy; nor-
malization in all cases is z-normalization)
? Pitch and intensity range, minimum, and
maximum.
? Pitch and intensity mean.
? Pitch and intensity median (0.5 quantile).
? Pitch and intensity standard deviation.
? Pitch slope, processed to eliminate halv-
ing/doubling.
? Number of voiced frames.
? Duration-normalized pitch and intensity
ranges and voiced frame count.
? Speaker-normalized pitch and intensity
means.
5.5 Temporal features
Under the hypothesis that the length of an utter-
ance or its location within the meeting as a whole
will determine its likelihood of being an action
item?for example, shorter statements near the
end of the meeting might be more likely to be ac-
tion items?we extract the duration of each utter-
ance and the time from its occurrence until the end
of the meeting. (Note that the use of this feature
precludes operating in an online setting, where the
end of the meeting may not be known in advance.)
5.6 General semantic features
Under the hypothesis that action item utterances
will frequently involve temporal expressions?e.g.
?Let?s have the paper written by next Tuesday??
we use Identifinder (Bikel et al, 1997) to mark
temporal expressions (?TIMEX? tags) in utterance
transcripts, and create a binary feature denoting
99
the existence of a temporal expression in each ut-
terance.
Note that as Identifinder was trained on broad-
cast news corpora, applying it to the very different
domain of multi-party meeting transcripts may not
result in optimal behavior.
5.7 Dialog-specific semantic features
Under the hypothesis that action item utterances
may be closely correlated with specific dialog
act tags, we use the dialog act annotations from
the ICSI Meeting Recorder Dialog Act Corpus.
(Shriberg et al, 2004) As these DA annotations
do not correspond one-to-one with utterances in
the ICSI corpus, we align them in the most liberal
way possible, i.e., if at least one word in an utter-
ance is annotated for a particular DA, we mark the
entirety of that utterance as exhibiting that DA.
We consider both fine-grained and coarse-
grained dialog acts.1 The former yields 56 fea-
tures, indicating occurrence of DA tags such
as ?appreciation,? ?rhetorical question,? and
?task management?; the latter consists of only
7 classes??disruption,? ?backchannel,? ?filler,?
?statement,? ?question,? ?unlabeled,? and ?un-
known.?
6 Results
The final performance for the maxent model
across different feature sets is given in Table 1.
F measures scores range from 13.81 to 31.92.
Figure 4 shows the interpolated precision-recall
curves for several of these feature sets; these
graphs display the level of precision that can be
achieved if one is willing to sacrifice some recall,
and vice versa.
Although ideally, all combinations of features
should be evaluated separately, the large number
of features in this precludes this strategy. The
combination of features explored here was cho-
sen so as to start from simpler features and suc-
cessively add more complex ones. We start with
transcript features that are immediate and context-
independent (?unigram?, ?bigram?, ?TIMEX?);
then add transcript features that require context
(?temporal?, ?context?), then non-transcript (i.e.
audio signal) features (?prosodic?), and finally add
features that require both the transcript and the au-
dio signal (?DA?).
1We use the map 01 grouping defined in the MRDA cor-
pus to collapse the tags.
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
recall
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
unigram
bigram
temporal
context+prosodic
fine?grained DAs
Figure 4: Interpolated precision-recall curve for
several (cumulative) feature sets. This graph sug-
gests the level of precision that can be achieved
if one is willing to sacrifice some recall, and vice
versa.
In total, nine combinations of features were
considered. In every case except that of syn-
tactic and coarse-grained dialog act features, the
additional features improved system performance
and these features were used in succeeding exper-
iments. Syntactic and coarse-grained DA features
resulted in a drop in performance and were dis-
carded from succeeding systems.
7 Analysis
The unigram and bigram features provide signif-
icant discriminative power. Tables 2 and 3 give
the top features, as determined by weight, for the
models trained only on these features. It is clear
from Table 3 that the detailed end-of-utterance
punctuation in the human-generated transcripts
provide valuable discriminative power.
The performance gain from adding TIMEX tag-
ging features is small and likely not statistically
significant. Post-hoc analysis of the TIMEX tag-
ging (Section 5.6) suggests that Identifinder tag-
ging accuracy is quite plausible in general, but ex-
hibits an unfortunate tendency to mark the digit-
reading (see Section 3) portion of the meetings as
temporal expressions. It is plausible that remov-
ing these utterances from the meetings would al-
low this feature a higher accuracy.
Based on the low feature weight assigned, utter-
ance length appears to provide no significant value
to the model. However, the time until the meet-
ing is over ranks as the highest-weighted feature
in the unigram+bigram+TIMEX+temporal feature
set. This feature is thus responsible for the 39.25%
100
features number F % imp.
unigram 6844 13.81
unigram+bigram 61281 16.72 21.07
unigram+bigram+TIMEX 61284 16.84 0.72
unigram+bigram+TIMEX+temporal 61286 23.45 39.25
unigram+bigram+TIMEX+temporal+syntactic 61291 21.94 -6.44
unigram+bigram+TIMEX+temporal+context 183833 25.62 9.25
unigram+bigram+TIMEX+temporal+context+prosodic 183871 27.44 7.10
unigram+bigram+TIMEX+temporal+context+prosodic+coarse DAs 183878 26.47 -3.53
unigram+bigram+TIMEX+temporal+context+prosodic+fine DAs 183927 31.92 16.33
Table 1: Performance of the maxent classifier as measured by F measure, the relative improvement from
the preceding feature set, and the number of features, across all feature sets tried. Italicized lines denote
the addition of features which do not improve performance; these are omitted from succeeding systems.
feature +/- ?
?pull? + 2.2100
?email? + 1.7883
?needs? + 1.7212
?added? + 1.6613
?mm-hmm? - 1.5937
?present? + 1.5740
?nine? - 1.5019
?!? - 1.5001
?five? - 1.4944
?together? + 1.4882
Table 2: Features, evidence type (positive denotes
action item), and weight for the top ten features
in the unigram-only model. ?Nine? and ?five? are
common words in the digit-reading task (see Sec-
tion 3).
feature +/- ?
?- $? - 1.4308
?i will? + 1.4128
?, $? - 1.3115
?uh $? - 1.2752
?w- $? - 1.2419
?. $? - 1.2247
?email? + 1.2062
?six $? - 1.1874
?* in? - 1.1833
?so $? - 1.1819
Table 3: Features, evidence type and weight for
the top ten features in the unigram+bigram model.
The symbol * denotes the beginning of an utter-
ance and $ the end. All of the top ten features are
bigrams except for the unigrams ?email?.
feature +/- ?
mean intensity (norm.) - 1.4288
mean pitch (norm.) - 1.0661
intensity range + 1.0510
?i will? + 0.8657
?email? + 0.8113
reformulate/summarize (DA) + 0.7946
?just go? (next) + 0.7190
?i will? (prev.) + 0.7074
?the paper? + 0.6788
understanding check (DA) + 0.6547
Table 4: Features, evidence type and weight for
the top ten features on the best-performing model.
Bigrams labeled ?prev.? and ?next? correspond to
the lexemes from previous and next utterances, re-
spectively. Prosodic features labeled as ?norm.?
have been normalized on a per-speaker basis.
boost in F measure in row 3 of Table 1.
The addition of part-of-speech tags actually de-
creases system performance. It is unclear why this
is the case. It may be that the unigram and bi-
gram features already adequately capture any dis-
tinctions these features make, or simply that these
features are generally not useful for distinguishing
action items.
Contextual features, on the other hand, im-
prove system performance significantly. A post-
hoc analysis of the action item annotations makes
clear why: action items are often split across mul-
tiple utterances (e.g. as in Figure 1), only a portion
of which contain lexical cues sufficient to distin-
guish them as such. Contextual features thus allow
utterances immediately surrounding these ?obvi-
ous? action items to be tagged as well.
101
Prosodic features yield a 7.10% increase in
F measure, and analysis shows that speaker-
normalized intensity and pitch, and the range in
intensity of an utterance, are valuable discrimina-
tive features. The subsequent addition of coarse-
grained dialog act tags does not further improve
system performance. It is likely this is due to rea-
sons similar to those for POS tags?either the cat-
egories are insufficient to distinguish action item
utterances, or whatever usefulness they provide is
subsumed by other features.
Table 4 shows the feature weights for the top-
ranked features on the best-scoring system. The
addition of the fine-grained DA tags results in a
significant increase in performance.The F measure
of this best feature set is 31.92%.
8 Conclusions
We have shown that several classes of features are
useful for the task of action item annotation from
multi-party meeting corpora. Simple lexical fea-
tures, their contextual versions, the time until the
end of the meeting, prosodic features, and fine-
grained dialog acts each contribute significant in-
creases in system performance.
While the raw system performance numbers of
Table 1 are low relative to other, better-studied
tasks on other, more mature corpora, we believe
the relative usefulness of the features towards this
task is indicative of their usefulness on more con-
sistent annotations, as well as to related tasks.
The Gruenstein et al (2005) corpus provides
a valuable and necessary resource for research in
this area, but several factors raise the question of
annotation quality. The low ? scores in Section 3
are indicative of annotation problems. Post-hoc
error analysis yields many examples of utterances
which are somewhat difficult to imagine as pos-
sible, never mind desirable, to tag. The fact that
the extremely useful oracular information present
in the fine-grained DA annotation does not raise
performance to the high levels that one might ex-
pect further suggests that the annotations are not
ideal?or, at the least, that they are inconsistent
with the DA annotations.2
This analysis is consistent with the findings of
Purver et al (2006), who achieve an F measure of
2Which is not to say they are devoid of significant value?
training and testing our best system on the corpus with the
590 positive classifications randomly shuffled across all ut-
terances yields an F measure of only 4.82.
less than 25% when applying SVMs to the classi-
fication task to the same corpus, and motivate the
development of a new corpus of action item anno-
tations.
9 Future work
In Section 6 we showed that contextual lexical
features are useful for the task of action item de-
tection, at least in the fairly limited manner em-
ployed in our implementation, which simply looks
at immediate previous and immediate next utter-
ances. It seems likely that applying a sequence
model such as an HMM or conditional random
field (CRFs) will act as a generalization of this fea-
ture and may further improve performance.
Addition of features such as speaker change and
?hot spots? (Wrede and Shriberg, 2003) may also
aid classification. Conversely, it is possible that
feature selection techniques may improve perfor-
mance by helping to eliminate poor-quality fea-
tures. In this work we have followed an ?ev-
erything but the kitchen sink? approach, in part
because we were curious about which features
would prove useful. The effect of adding POS and
coarse-grained DA features illustrates that this is
not necessarily the ideal strategy in terms of ulti-
mate system performance.
In general, the features evaluated in this
work are an indiscriminate mix of human- and
automatically-generated features; of the human-
generated features, some are plausible to generate
automatically, at some loss of quality (e.g. tran-
scripts) while others are unlikely to be automati-
cally generated in the foreseeable future (e.g. fine-
grained dialog acts). Future work may focus on
the effects that automatic generation of the former
has on overall system performance (although this
may require higher-quality annotations to be use-
ful.) For example, the detailed end-of-utterance
punctuation present in the human transcripts pro-
vides valuable discriminative power (Table 3), but
current ASR systems are not likely to be able to
provide this level of detail. Switching to ASR out-
put will have a negative effect on performance.
One final issue is that of utterance segmenta-
tion. The scheme used in the ICSI meeting corpus
does not necessarily correspond to the ideal seg-
mentation for other tasks. The action item annota-
tions were performed on these segmentations, and
in this study we did not attempt resegmentation,
but in the future it may prove valuable to collapse,
102
for example, successive un-interrupted utterances
from the same speaker into a single utterance.
In conclusion, while overall system perfor-
mance does not approach levels typical of better-
studied classification tasks such as named-entity
recognition, we believe that this is a largely a prod-
uct of the current action item annotation quality.
We believe that the feature analysis presented here
is useful, for this task and for other related tasks,
and that, provided with a set of more consistent
action item annotations, the current system can be
used as is to achieve better performance.
Acknowledgments
The authors wish to thank Dan Jurafsky, Chris
Manning, Stanley Peters, Matthew Purver, and
several anonymous reviewers for valuable advice
and comments.
References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classifica-
tion in multiparty meetings. In Proceedings of the
ICASSP.
Paul N. Bennett and Jaime Carbonell. 2005. Detecting
action-items in e-mail. In Proceedings of SIGIR.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of the Conference on Applied
NLP.
Paul Boersma and David Weenink. 2005. Praat: doing
phonetics by computer v4.4.12 (computer program).
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of EMNLP.
Simon Corston-Oliver, Eric Ringger, Michael Ga-
mon, and Richard Campbell. 2004. Task-focused
summarization of email. In Text Summarization
Branches Out: Proceedings of the ACL Workshop.
J. Godfrey, E. Holliman, and J.McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICAASP.
Alexander Gruenstein, John Niekrasz, and Matthew
Purver. 2005. Meeting structure annotation: Data
and tools. In Proceedings of the 6th SIGDIAL Work-
shop on Discourse and Dialogue.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
and Chuck Wooters. 2003. The ICSI meeting cor-
pus. In Proceedings of the ICASSP.
Gang Ji and Jeff Bilmes. 2005. Dialog act tag-
ging using graphical models. In Proceedings of the
ICASSP.
Marion Mast, R. Kompe, S. Harbeck, A. Kie?ling,
H. Niemann, E. No?th, E.G. Schukat-Talamazzini,
and V. Warnke. 1996. Dialog act classification with
the help of prosody. In Proceedings of the ICSLP.
Matthew Purver, Patrick Ehlen, and John Niekrasz.
2006. Detecting action items in multi-party meet-
ings: Annotation and initial experiments. In Pro-
ceedings of the 3rd Joint Workshop on MLMI.
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke,
Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coc-
caro, Rachel Martin, Marie Meteer, and Carol Van
EssDykema. 1998. Can prosody aid the auto-
matic classification of dialog acts in conversational
speech? Language and Speech, 41(3?4):439?487.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings
of the 5th SIGDIAL Workshop on Discourse and Di-
alogue.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,
Noah Coccaro, Daniel Jurafsky, Rachel Mar-
tin, Marie Meteer, Klaus Ries, Paul Taylor, and
Carol Van EssDykema. 1998. Dialog act model-
ing for conversational speech. In Proceedings of
the AAAI Spring Symposium on Applying Machine
Learning to Discourse Processing.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of EMNLP.
Britta Wrede and Elizabeth Shriberg. 2003. Spot-
ting ?hot spots? in meetings: Human judgments and
prosodic cues. In Proceedings of the European Con-
ference on Speech Communication and Technology.
Matthias Zimmermann, Yang Liu, Elizabeth Shriberg,
and Andreas Stolcke. 2005. Toward joint segmen-
tation and classification of dialog acts in multiparty
meetings. In Proceedings of the 2nd Joint Workshop
on MLMI.
103
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 215?223,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Disambiguating ?DE? for Chinese-English Machine Translation
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Manning
Computer Science Department, Stanford University
Stanford, CA 94305
pichuan,jurafsky,manning@stanford.edu
Abstract
Linking constructions involving { (DE) are ubiq-
uitous in Chinese, and can be translated into En-
glish in many different ways. This is a major source
of machine translation error, even when syntax-
sensitive translation models are used. This paper
explores how getting more information about the
syntactic, semantic, and discourse context of uses
of { (DE) can facilitate producing an appropriate
English translation strategy. We describe a finer-
grained classification of { (DE) constructions in
Chinese NPs, construct a corpus of annotated ex-
amples, and then train a log-linear classifier, which
contains linguistically inspired features. We use the
DE classifier to preprocess MT data by explicitly
labeling { (DE) constructions, as well as reorder-
ing phrases, and show that our approach provides
significant BLEU point gains on MT02 (+1.24),
MT03 (+0.88) and MT05 (+1.49) on a phrased-
based system. The improvement persists when a hi-
erarchical reordering model is applied.
1 Introduction
Machine translation (MT) from Chinese to En-
glish has been a difficult problem: structural dif-
ferences between Chinese and English, such as
the different orderings of head nouns and rela-
tive clauses, cause BLEU scores to be consis-
tently lower than for other difficult language pairs
like Arabic-English. Many of these structural
differences are related to the ubiquitous Chinese
{(DE) construction, used for a wide range of
noun modification constructions (both single word
and clausal) and other uses. Part of the solution
to dealing with these ordering issues is hierarchi-
cal decoding, such as the Hiero system (Chiang,
2005), a method motivated by {(DE) examples
like the one in Figure 1. In this case, the transla-
tion goal is to rotate the noun head and the preced-
ing relative clause around{(DE), so that we can
translate to ?[one of few countries]{ [have diplo-
matic relations with North Korea]?. Hiero can
learn this kind of lexicalized synchronous gram-
mar rule.
But use of hierarchical decoders has not solved
the DE construction translation problem. We ana-
lyzed the errors of three state-of-the-art systems
(the 3 DARPA GALE phase 2 teams? systems),
and even though all three use some kind of hier-
archical system, we found many remaining errors
related to reordering. One is shown here:
h? ? ?XProceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 51?59,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Discriminative Reordering with Chinese Grammatical Relations Features
Pi-Chuan Changa, Huihsin Tsengb, Dan Jurafskya, and Christopher D. Manninga
aComputer Science Department, Stanford University, Stanford, CA 94305
bYahoo! Inc., Santa Clara, CA 95054
{pichuan,jurafsky,manning}@stanford.edu, huihui@yahoo-inc.com
Abstract
The prevalence in Chinese of grammatical
structures that translate into English in dif-
ferent word orders is an important cause of
translation difficulty. While previous work has
used phrase-structure parses to deal with such
ordering problems, we introduce a richer set of
Chinese grammatical relations that describes
more semantically abstract relations between
words. Using these Chinese grammatical re-
lations, we improve a phrase orientation clas-
sifier (introduced by Zens and Ney (2006))
that decides the ordering of two phrases when
translated into English by adding path fea-
tures designed over the Chinese typed depen-
dencies. We then apply the log probabil-
ity of the phrase orientation classifier as an
extra feature in a phrase-based MT system,
and get significant BLEU point gains on three
test sets: MT02 (+0.59), MT03 (+1.00) and
MT05 (+0.77). Our Chinese grammatical re-
lations are also likely to be useful for other
NLP tasks.
1 Introduction
Structural differences between Chinese and English
are a major factor in the difficulty of machine trans-
lation from Chinese to English. The wide variety
of such Chinese-English differences include the or-
dering of head nouns and relative clauses, and the
ordering of prepositional phrases and the heads they
modify. Previous studies have shown that using syn-
tactic structures from the source side can help MT
performance on these constructions. Most of the
previous syntactic MT work has used phrase struc-
ture parses in various ways, either by doing syntax-
directed translation to directly translate parse trees
into strings in the target language (Huang et al,
2006), or by using source-side parses to preprocess
the source sentences (Wang et al, 2007).
One intuition for using syntax is to capture dif-
ferent Chinese structures that might have the same
(a) (ROOT  (IP     (LCP       (QP (CD ?)
        (CLP (M ?)))
      (LC ?))
    (PU ?)    (NP       (DP (DT ??))
      (NP (NN ??)))    (VP       (ADVP (AD ??))
      (VP (VV ??)        (NP           (NP             (ADJP (JJ ??))
            (NP (NN ??)))
          (NP (NN ??)))
        (QP (CD ?????)
          (CLP (M ?)))))
    (PU ?)))
(b) (ROOT  (IP     (NP       (DP (DT ??))
      (NP (NN ??)))    (VP       (LCP         (QP (CD ?)
          (CLP (M ?)))
        (LC ?))
      (ADVP (AD ??))
      (VP (VV ??)        (NP           (NP             (ADJP (JJ ??))
            (NP (NN ??)))
          (NP (NN ??)))
        (QP (CD ?????)
          (CLP (M ?)))))
    (PU ?)))
?
?
? ??
??
?? ?? ?
?? ??
??
?????
? (three) 
? (year) 
? (over; in) ?? (city)
??(complete)
??(collectively) ??(invest) ?(yuan)
 (these) ??(asset)
??(fixed)
?????(12 billion)
loc nsubj advmod dobj range
lobj det nn
nummod amod
nummod
Figure 1: Sentences (a) and (b) have the same mean-
ing, but different phrase structure parses. Both sentences,
however, have the same typed dependencies shown at the
bottom of the figure.
meaning and hence the same translation in English.
But it turns out that phrase structure (and linear or-
der) are not sufficient to capture this meaning rela-
tion. Two sentences with the same meaning can have
different phrase structures and linear orders. In the
example in Figure 1, sentences (a) and (b) have the
same meaning, but different linear orders and dif-
ferent phrase structure parses. The translation of
sentence (a) is: ?In the past three years these mu-
nicipalities have collectively put together investment
in fixed assets in the amount of 12 billion yuan.? In
sentence (b), ?in the past three years? has moved its
51
position. The temporal adverbial ??#u? (in the
past three years) has different linear positions in the
sentences. The phrase structures are different too: in
(a) the LCP is immediately under IP while in (b) it
is under VP.
We propose to use typed dependency parses in-
stead of phrase structure parses. Typed dependency
parses give information about grammatical relations
between words, instead of constituency informa-
tion. They capture syntactic relations, such as nsubj
(nominal subject) and dobj (direct object) , but also
encode semantic information such as in the loc (lo-
calizer) relation. For the example in Figure 1, if we
look at the sentence structure from the typed depen-
dency parse (bottom of Figure 1), ??#u? is con-
nected to the main verb q? (finish) by a loc (lo-
calizer) relation, and the structure is the same for
sentences (a) and (b). This suggests that this kind
of semantic and syntactic representation could have
more benefit than phrase structure parses.
Our Chinese typed dependencies are automati-
cally extracted from phrase structure parses. In En-
glish, this kind of typed dependencies has been in-
troduced by de Marneffe and Manning (2008) and
de Marneffe et al (2006). Using typed dependen-
cies, it is easier to read out relations between words,
and thus the typed dependencies have been used in
meaning extraction tasks.
We design features over the Chinese typed depen-
dencies and use them in a phrase-based MT sys-
tem when deciding whether one chunk of Chinese
words (MT system statistical phrase) should appear
before or after another. To achieve this, we train a
discriminative phrase orientation classifier follow-
ing the work by Zens and Ney (2006), and we use
the grammatical relations between words as extra
features to build the classifier. We then apply the
phrase orientation classifier as a feature in a phrase-
based MT system to help reordering.
2 Discriminative Reordering Model
Basic reordering models in phrase-based systems
use linear distance as the cost for phrase move-
ments (Koehn et al, 2003). The disadvantage of
these models is their insensitivity to the content of
the words or phrases. More recent work (Tillman,
2004; Och et al, 2004; Koehn et al, 2007) has in-
troduced lexicalized reordering models which esti-
mate reordering probabilities conditioned on the ac-
tual phrases. Lexicalized reordering models have
brought significant gains over the baseline reorder-
ing models, but one concern is that data sparseness
can make estimation less reliable. Zens and Ney
(2006) proposed a discriminatively trained phrase
orientation model and evaluated its performance as a
classifier and when plugged into a phrase-based MT
system. Their framework allows us to easily add in
extra features. Therefore we use it as a testbed to see
if we can effectively use features from Chinese typed
dependency structures to help reordering in MT.
2.1 Phrase Orientation Classifier
We build up the target language (English) translation
from left to right. The phrase orientation classifier
predicts the start position of the next phrase in the
source sentence. In our work, we use the simplest
class definition where we group the start positions
into two classes: one class for a position to the left of
the previous phrase (reversed) and one for a position
to the right (ordered).
Let c j, j? be the class denoting the movement from
source position j to source position j? of the next
phrase. The definition is:
c j, j? =
{ reversed if j? < j
ordered if j? > j
The phrase orientation classifier model is in the log-
linear form:
p?N1 (c j, j? | f J1 ,eI1, i, j)
= exp
(?Nn=1 ?nhn( f J1 ,eI1, i, j,c j, j?)
)
?c? exp
(?Nn=1 ?nhn( f J1 ,eI1, i, j,c?)
)
i is the target position of the current phrase, and f J1
and eI1 denote the source and target sentences respec-
tively. c? represents possible categories of c j, j? .
We can train this log-linear model on lots of la-
beled examples extracted from all of the aligned MT
training data. Figure 2 is an example of an aligned
sentence pair and the labeled examples that can be
extracted from it. Also, different from conventional
MERT training, we can have a large number of bi-
nary features for the discriminative phrase orienta-
tion classifier. The experimental setting will be de-
scribed in Section 4.1.
52
(21) </s>
(20) .
(19) world
(18) outside
(17) the
(16) to
(15) up
(14) opening
(13) of
(12) policy
(11) 's
(10) China
(9) from
(8) arising
(7) star
(6) bright
(5) a
(4) become
(3) already
(2) has
(1) Beihai
(0) <s>
(15)
</s>
(14)?(13)??(12)?(11)?(10)?(9)??(8)?(7)?5)?(6)?)?(4)??(3)??(2)?(1)??(0)<s>
ordered151420
ordered14618
ordered6516
reversed5715
reversed7810
reversed8109
ordered1098
reversed9137
ordered13126
ordered12115
ordered1134
ordered323
ordered211
ordered100
classj'ji
i j
Figure 2: An illustration of an alignment grid between a Chinese sentence and its English translation along with the
labeled examples for the phrase orientation classifier. Note that the alignment grid in this example is automatically
generated.
The basic feature functions are similar to what
Zens and Ney (2006) used in their MT experiments.
The basic binary features are source words within a
window of size 3 (d ? ?1,0,1) around the current
source position j, and target words within a window
of size 3 around the current target position i. In the
classifier experiments in Zens and Ney (2006) they
also use word classes to introduce generalization ca-
pabilities. In the MT setting it?s harder to incorpo-
rate the part-of-speech information on the target lan-
guage. Zens and Ney (2006) also exclude word class
information in the MT experiments. In our work
we will simply use the word features as basic fea-
tures for the classification experiments as well. As
a concrete example, we look at the labeled example
(i = 4, j = 3, j? = 11) in Figure 2. We include the
word features in a window of size 3 around j and i
as in Zens and Ney (2006), we also include words
around j? as features. So we will have nine word
features for (i = 4, j = 3, j? = 11):
Src?1:. Src0:?? Src1:?)
Src2?1:{ Src20: Src21:(
Tgt?1:already Tgt0:become Tgt1:a
2.2 Path Features Using Typed Dependencies
Assuming we have parsed the Chinese sentence that
we want to translate and have extracted the gram-
matical relations in the sentence, we design features
using the grammatical relations. We use the path be-
tween the two words annotated by the grammatical
relations. Using this feature helps the model learn
about what the relation is between the two chunks
of Chinese words. The feature is defined as follows:
for two words at positions p and q in the Chinese
53
Shared relations Chinese English
nn 15.48% 6.81%
punct 12.71% 9.64%
nsubj 6.87% 4.46%
rcmod 2.74% 0.44%
dobj 6.09% 3.89%
advmod 4.93% 2.73%
conj 6.34% 4.50%
num/nummod 3.36% 1.65%
attr 0.62% 0.01%
tmod 0.79% 0.25%
ccomp 1.30% 0.84%
xsubj 0.22% 0.34%
cop 0.07% 0.85%
cc 2.06% 3.73%
amod 3.14% 7.83%
prep 3.66% 10.73%
det 1.30% 8.57%
pobj 2.82% 10.49%
Table 1: The percentage of typed dependencies in files
1?325 in Chinese (CTB6) and English (English-Chinese
Translation Treebank)
sentence (p < q), we find the shortest path in the
typed dependency parse from p to q, concatenate all
the relations on the path and use that as a feature.
A concrete example is the sentences in Figure 3,
where the alignment grid and labeled examples are
shown in Figure 2. The glosses of the Chinese words
in the sentence are in Figure 3, and the English trans-
lation is ?Beihai has already become a bright star
arising from China?s policy of opening up to the out-
side world.? which is also listed in Figure 2.
For the labeled example (i = 4, j = 3, j? = 11),
we look at the typed dependency parse to find the
path feature between ?? and . The relevant
dependencies are: dobj(??, ?h), clf (?h, ()
and nummod( , ). Therefore the path feature is
PATH:dobjR-clfR-nummodR. We also use the direc-
tionality: we add an R to the dependency name if it?s
going against the direction of the arrow.
3 Chinese Grammatical Relations
Our Chinese grammatical relations are designed to
be very similar to the Stanford English typed depen-
dencies (de Marneffe and Manning, 2008; de Marn-
effe et al, 2006).
3.1 Description
There are 45 named grammatical relations, and a de-
fault 46th relation dep (dependent). If a dependency
matches no patterns, it will have the most generic
relation dep. The descriptions of the 45 grammat-
ical relations are listed in Table 2 ordered by their
frequencies in files 1?325 of CTB6 (LDC2007T36).
The total number of dependencies is 85748, and
other than the ones that fall into the 45 grammatical
relations, there are also 7470 dependencies (8.71%
of all dependencies) that do not match any patterns,
and therefore keep the generic name dep.
3.2 Chinese Specific Structures
Although we designed the typed dependencies to
show structures that exist both in Chinese and En-
glish, there are many other syntactic structures that
only exist in Chinese. The typed dependencies we
designed also cover those Chinese specific struc-
tures. For example, the usage of ?{? (DE) is one
thing that could lead to different English transla-
tions. In the Chinese typed dependencies, there
are relations such as cpm (DE as complementizer)
or assm (DE as associative marker) that are used
to mark these different structures. The Chinese-
specific ??? (BA) construction also has a relation
ba dedicated to it.
The typed dependencies annotate these Chinese
specific relations, but do not directly provide a map-
ping onto how they are translated into English. It
becomes more obvious how those structures affect
the ordering when Chinese sentences are translated
into English when we apply the typed dependencies
as features in the phrase orientation classifier. This
will be further discussed in Section 4.4.
3.3 Comparison with English
To compare the distribution of Chinese typed de-
pendencies with English, we extracted the English
typed dependencies from the translation of files 1?
325 in the English Chinese Translation Treebank
1.0 (LDC2007T02), which correspond to files 1?325
in CTB6. The English typed dependencies are ex-
tracted using the Stanford Parser.
There are 116,799 total English dependencies,
and 85,748 Chinese ones. On the corpus we use,
there are 45 distinct dependency types (not includ-
ing dep) in Chinese, and 50 in English. The cov-
erage of named relations is 91.29% in Chinese and
90.48% in English; the remainder are the unnamed
relation dep. We looked at the 18 shared relations
54
?? ? ?? ?? ? ? ?? ? ?? ? ? ? ?? ?
nsubj nsubjpobj lccomp loc rcmod
dobj
clfnummodadvmod
Beihai already become China to outside open during rising (DE) one measureword brightstar .prep cpm
punct
Figure 3: A Chinese example sentence labeled with typed dependencies
between Chinese and English in Table 1. Chinese
has more nn, punct, nsubj, rcmod, dobj, advmod,
conj, nummod, attr, tmod, and ccomp while English
uses more pobj, det, prep, amod, cc, cop, and xsubj,
due mainly to grammatical differences between Chi-
nese and English. For example, some determiners
in English (e.g., ?the? in (1b)) are not mandatory in
Chinese:
(1a)??=/import and export/total value
(1b) The total value of imports and exports
In another difference, English uses adjectives
(amod) to modify a noun (?financial? in (2b)) where
Chinese can use noun compounds (???/finance?
in (2a)).
(2a)?u/Tibet??/finance?/system??/reform
(2b) the reform in Tibet ?s financial system
We also noticed some larger differences between
the English and Chinese typed dependency distribu-
tions. We looked at specific examples and provide
the following explanations.
prep and pobj English has much more uses of prep
and pobj. We examined the data and found three
major reasons:
1. Chinese uses both prepositions and postposi-
tions while English only has prepositions. ?Af-
ter? is used as a postposition in Chinese exam-
ple (3a), but a preposition in English (3b):
(3a)??/1997??/after
(3b) after 1997
2. Chinese uses noun phrases in some cases where
English uses prepositions. For example, ??
-? (period, or during) is used as a noun phrase
in (4a), but it?s a preposition in English.
(4a)??/1997t/to??/1998?- /period
(4b) during 1997-1998
3. Chinese can use noun phrase modification in
situations where English uses prepositions. In
example (5a), Chinese does not use any prepo-
sitions between ?apple company? and ?new
product?, but English requires use of either
?of? or ?from?.
(5a)?*??/apple companyc??/new product
(5b) the new product of (or from) Apple
The Chinese DE constructions are also often
translated into prepositions in English.
cc and punct The Chinese sentences contain more
punctuation (punct) while the English translation
has more conjunctions (cc), because English uses
conjunctions to link clauses (?and? in (6b)) while
Chinese tends to use only punctuation (?,? in (6a)).
(6a) YJ/these?=/city??/social?/economic
0/development??/rapid??0/local
?/economic"?/strength?/clearly
/enhance
(6b) In these municipalities the social and economic de-
velopment has been rapid, and the local economic
strength has clearly been enhanced
rcmod and ccomp There are more rcmod and
ccomp in the Chinese sentences and less in the En-
glish translation, because of the following reasons:
1. Some English adjectives act as verbs in Chi-
nese. For example, c (new) is an adjectival
predicate in Chinese and the relation between
c (new) and ?? (system) is rcmod. But
?new? is an adjective in English and the En-
glish relation between ?new? and ?system? is
amod. This difference contributes to more rc-
mod in Chinese.
(7a)c/new{/(DE)X=/verify and write off
(7b) a new sales verification system
2. Chinese has two special verbs (VC): 4 (SHI)
and ? (WEI) which English doesn?t use. For
55
abbreviation short description Chinese example typed dependency counts percentagenn noun compound modifier q??e nn(?e,q?) 13278 15.48%punct punctuation 0:,?? punct(,?,?) 10896 12.71%nsubj nominal subject ?? nsubj(,??) 5893 6.87%conj conjunct (links two conjuncts) ??Z?a? conj(?a?,??) 5438 6.34%dobj direct object ???Y??G?G dobj(?Y,?G) 5221 6.09%advmod adverbial modifier \????G advmod(??,) 4231 4.93%prep prepositional modifier ?"B??Zq? prep(q?,?) 3138 3.66%nummod number modifier ?G?G nummod(G,?) 2885 3.36%amod adjectival modifier J-?? amod(??,J-) 2691 3.14%pobj prepositional object ???? pobj(??,?) 2417 2.82%rcmod relative clause modifier X?t,{<Y rcmod(<Y,?t) 2348 2.74%cpm complementizer ??{??? cpm(,{) 2013 2.35%assm associative marker ?{?? assm(?,{) 1969 2.30%assmod associative modifier ?{?? assmod(??,?) 1941 2.26%cc coordinating conjunction ??Z?a? cc(?a?,Z) 1763 2.06%clf classifier modifier ?G?G clf(?G,G) 1558 1.82%ccomp clausal complement Uq??Rzf~?? ccomp(??,Rz) 1113 1.30%det determiner YJ??? det(??,YJ) 1113 1.30%lobj localizer object ?#u lobj(u,?#) 1010 1.18%range dative object that is a quantifier phrase ?b ?7?? range(?b,?) 891 1.04%asp aspect marker ??*~ asp(?,?) 857 1.00%tmod temporal modifier 1X?t, tmod(?t,1) 679 0.79%plmod localizer modifier of a preposition ?Y?yH? plmod(?,?) 630 0.73%attr attributive ?4??7?? attr(?,??) 534 0.62%mmod modal verb modifier ?Czt?F mmod(zt,) 497 0.58%loc localizer 3??1? loc(3,1?) 428 0.50%top topic O?4??? top(4,O?) 380 0.44%pccomp clausal complement of a preposition ??\??? pccomp(?,??) 374 0.44%etc etc modifier )?s? etc(?s,) 295 0.34%lccomp clausal complement of a localizer ?)?i8??{?h lccomp(?,8) 207 0.24%ordmod ordinal number modifier ????? ordmod(?,??) 199 0.23%xsubj controlling subject Uq??Rzf~?? xsubj(Rz,Uq) 192 0.22%neg negative modifier 1X?t, neg(?t,X) 186 0.22%rcomp resultative complement ???? rcomp(??,??) 176 0.21%comod coordinated verb compound modifier ?Y"q comod(?Y,"q) 150 0.17%vmod verb modifier ??|?i??0?{*~ vmod(0?,|?) 133 0.16%prtmod particles such as?,1,u, ????Rz{?? prtmod(Rz,?) 124 0.14%ba ?ba? construction ?????5=? ba(?5,?) 95 0.11%dvpm manner DE(?) modifier ?H?3? dvpm(?H,?) 73 0.09%dvpmod a ?XP+DEV(?)? phrase that modifies VP ?H?3? dvpmod(3?,?H) 69 0.08%prnmod parenthetical modifier ???-? 1990 ? 1995? prnmod(?-, 1995) 67 0.08%cop copular ?4?{? cop(?,4) 59 0.07%pass passive marker ?????b? pass(??,?) 53 0.06%nsubjpass nominal passive subject 1??*S?{?	? nsubjpass(?*,1) 14 0.02%
Table 2: Chinese grammatical relations and examples. The counts are from files 1?325 in CTB6.
example, there is an additional relation, ccomp,
between the verb4/(SHI) and\?/reduce in
(8a). The relation is not necessary in English,
since4/SHI is not translated.
(8a) /second4/(SHI)??#/1996
?)/ChinaLl?/substantially
\?/reduce{/tariff
(8b) Second, China reduced tax substantially in
1996.
conj There are more conj in Chinese than in En-
glish for three major reasons. First, sometimes one
complete Chinese sentence is translated into sev-
eral English sentences. Our conj is defined for two
grammatical roles occurring in the same sentence,
and therefore, when a sentence breaks into multiple
ones, the original relation does not apply. Second,
we define the two grammatical roles linked by the
conj relation to be in the same word class. However,
words which are in the same word class in Chinese
may not be in the same word class in English. For
example, adjective predicates act as verbs in Chi-
nese, but as adjectives in English. Third, certain con-
structions with two verbs are described differently
between the two languages: verb pairs are described
as coordinations in a serial verb construction in Chi-
nese, but as the second verb being the complement
56
of the first verb in English.
4 Experimental Results
4.1 Experimental Setting
We use various Chinese-English parallel corpora1
for both training the phrase orientation classifier, and
for extracting statistical phrases for the phrase-based
MT system. The parallel data contains 1,560,071
sentence pairs from various parallel corpora. There
are 12,259,997 words on the English side. Chi-
nese word segmentation is done by the Stanford Chi-
nese segmenter (Chang et al, 2008). After segmen-
tation, there are 11,061,792 words on the Chinese
side. The alignment is done by the Berkeley word
aligner (Liang et al, 2006) and then we symmetrized
the word alignment using the grow-diag heuristic.
For the phrase orientation classifier experiments,
we extracted labeled examples using the parallel
data and the alignment as in Figure 2. We extracted
9,194,193 total valid examples: 86.09% of them are
ordered and the other 13.91% are reversed. To eval-
uate the classifier performance, we split these exam-
ples into training, dev and test set (8 : 1 : 1). The
phrase orientation classifier used in MT experiments
is trained with all of the available labeled examples.
Our MT experiments use a re-implementation of
Moses (Koehn et al, 2003) called Phrasal, which
provides an easier API for adding features. We
use a 5-gram language model trained on the Xin-
hua and AFP sections of the Gigaword corpus
(LDC2007T40) and also the English side of all the
LDC parallel data permissible under the NIST08
rules. Documents of Gigaword released during the
epochs of MT02, MT03, MT05, and MT06 were
removed. For features in MT experiments, we in-
corporate Moses? standard eight features as well as
the lexicalized reordering features. To have a more
comparable setting with (Zens and Ney, 2006), we
also have a baseline experiment with only the stan-
dard eight features. Parameter tuning is done with
Minimum Error Rate Training (MERT) (Och, 2003).
The tuning set for MERT is the NIST MT06 data
set, which includes 1664 sentences. We evaluate the
result with MT02 (878 sentences), MT03 (919 sen-
1LDC2002E18, LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85,
LDC2002L27 and LDC2005T34.
tences), and MT05 (1082 sentences).
4.2 Phrase Orientation Classifier
Feature Sets #features Train. Acc. Train. Dev DevAcc. (%) Macro-F Acc. (%) Macro-FMajority class - 86.09 - 86.09 -Src 1483696 89.02 71.33 88.14 69.03Src+Tgt 2976108 92.47 82.52 91.29 79.80Src+Src2+Tgt 4440492 95.03 88.76 93.64 85.58Src+Src2+Tgt+PATH 4691887 96.01 91.15 94.27 87.22
Table 3: Feature engineering of the phrase orientation
classifier. Accuracy is defined as (#correctly labeled ex-
amples) divided by (#all examples). The macro-F is an
average of the accuracies of the two classes.
The basic source word features described in Sec-
tion 2 are referred to as Src, and the target word
features as Tgt. The feature set that Zens and Ney
(2006) used in their MT experiments is Src+Tgt. In
addition to that, we also experimented with source
word features Src2 which are similar to Src, but take
a window of 3 around j? instead of j. In Table 3
we can see that adding the Src2 features increased
the total number of features by almost 50%, but also
improved the performance. The PATH features add
fewer total number of features than the lexical fea-
tures, but still provide a 10% error reduction and
1.63 on the macro-F1 on the dev set. We use the best
feature sets from the feature engineering in Table 3
and test it on the test set. We get 94.28% accuracy
and 87.17 macro-F1. The overall improvement of
accuracy over the baseline is 8.19 absolute points.
4.3 MT Experiments
In the MT setting, we use the log probability from
the phrase orientation classifier as an extra feature.
The weight of this discriminative reordering feature
is also tuned by MERT, along with other Moses
features. In order to understand how much the
PATH features add value to the MT experiments, we
trained two phrase orientation classifiers with differ-
ent features: one with the Src+Src2+Tgt feature set,
and the other one with Src+Src2+Tgt+PATH. The re-
sults are listed in Table 4. We compared to two
different baselines: one is Moses8Features which
has a distance-based reordering model, the other is
Baseline which also includes lexicalized reorder-
ing features. From the table we can see that using
the discriminative reordering model with PATH fea-
tures gives significant improvement over both base-
57
Setting #MERT features MT06(tune) MT02 MT03 MT05
Moses8Features 8 31.49 31.63 31.26 30.26Moses8Features+DiscrimRereorderNoPATH 9 31.76(+0.27) 31.86(+0.23) 32.09(+0.83) 31.14(+0.88)Moses8Features+DiscrimRereorderWithPATH 9 32.34(+0.85) 32.59(+0.96) 32.70(+1.44) 31.84(+1.58)
Baseline (Moses with lexicalized reordering) 16 32.55 32.56 32.65 31.89Baseline+DiscrimRereorderNoPATH 17 32.73(+0.18) 32.58(+0.02) 32.99(+0.34) 31.80(?0.09)Baseline+DiscrimRereorderWithPATH 17 32.97(+0.42) 33.15(+0.59) 33.65(+1.00) 32.66(+0.77)
Table 4: MT experiments of different settings on various NIST MT evaluation datasets. All differences marked in bold
are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005).
??? ?? ??
det
every level product
nn
? ? ?? ? ??
products of all level
??? ?? ?? ? ? ?? ? ??whole city this year industry total output value
det nn
gross industrial output value of the whole city this year
Figure 4: Two examples for the feature PATH:det-nn and
how the reordering occurs.
lines. If we use the discriminative reordering model
without PATH features and only with word features,
we still get improvement over the Moses8Features
baseline, but the MT performance is not signifi-
cantly different from Baseline which uses lexical-
ized reordering features. From Table 4 we see that
using the Src+Src2+Tgt+PATH features significantly
outperforms both baselines. Also, if we compare be-
tween Src+Src2+Tgt and Src+Src2+Tgt+PATH, the
differences are also statistically significant, which
shows the effectiveness of the path features.
4.4 Analysis: Highly-weighted Features in the
Phrase Orientation Model
There are a lot of features in the log-linear phrase
orientation model. We looked at some highly-
weighted PATH features to understand what kind
of grammatical constructions were informative for
phrase orientation. We found that many path fea-
tures corresponded to our intuitions. For example,
the feature PATH:prep-dobjR has a high weight for
being reversed. This feature informs the model that
in Chinese a PP usually appears before VP, but in
English they should be reversed. Other features
with high weights include features related to the
DE construction that is more likely to translate to
a relative clause, such as PATH:advmod-rcmod and
PATH:rcmod. They also indicate the phrases are
more likely to be chosen in reversed order. Another
frequent pattern that has not been emphasized in the
previous literature is PATH:det-nn, meaning that a
[DT NP1NP2] in Chinese is translated into English
as [NP2 DT NP1]. Examples with this feature are
in Figure 4. We can see that the important features
decided by the phrase orientation model are also im-
portant from a linguistic perspective.
5 Conclusion
We introduced a set of Chinese typed dependencies
that gives information about grammatical relations
between words, and which may be useful in other
NLP applications as well as MT. We used the typed
dependencies to build path features and used them to
improve a phrase orientation classifier. The path fea-
tures gave a 10% error reduction on the accuracy of
the classifier and 1.63 points on the macro-F1 score.
We applied the log probability as an additional fea-
ture in a phrase-based MT system, which improved
the BLEU score of the three test sets significantly
(0.59 on MT02, 1.00 on MT03 and 0.77 on MT05).
This shows that typed dependencies on the source
side are informative for the reordering component in
a phrase-based system. Whether typed dependen-
cies can lead to improvements in other syntax-based
MT systems remains a question for future research.
Acknowledgments
The authors would like to thank Marie-Catherine de
Marneffe for her help on the typed dependencies,
and Daniel Cer for building the decoder. This work
is funded by a Stanford Graduate Fellowship to the
first author and gift funding from Google for the
project ?Translating Chinese Correctly?.
58
References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 224?232, Columbus, Ohio, June.
Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8, Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC-06, pages 449?454.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Demonstration Session.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of HLT-NAACL.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004: Short Papers, pages 101?104.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 737?745, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
Association for Computational Linguistics.
59
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705?713,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Uptraining for Accurate Deterministic Question Parsing
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, Hiyan Alshawi
Google Research
{slav,pichuan,ringgaard,hiyan}@google.com
Abstract
It is well known that parsing accuracies drop
significantly on out-of-domain data. What is
less known is that some parsers suffer more
from domain shifts than others. We show
that dependency parsers have more difficulty
parsing questions than constituency parsers.
In particular, deterministic shift-reduce depen-
dency parsers, which are of highest interest
for practical applications because of their lin-
ear running time, drop to 60% labeled accu-
racy on a question test set. We propose an
uptraining procedure in which a deterministic
parser is trained on the output of a more ac-
curate, but slower, latent variable constituency
parser (converted to dependencies). Uptrain-
ing with 100K unlabeled questions achieves
results comparable to having 2K labeled ques-
tions for training. With 100K unlabeled and
2K labeled questions, uptraining is able to
improve parsing accuracy to 84%, closing
the gap between in-domain and out-of-domain
performance.
1 Introduction
Parsing accuracies on the popular Section 23 of the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank have been steadily improving over the past
decade. At this point, we have many different pars-
ing models that reach and even surpass 90% depen-
dency or constituency accuracy on this test set (Mc-
Donald et al, 2006; Nivre et al, 2007; Charniak and
Johnson, 2005; Petrov et al, 2006; Carreras et al,
2008; Koo and Collins, 2010). Quite impressively,
models based on deterministic shift-reduce parsing
algorithms are able to rival the other computation-
ally more expensive models (see Nivre (2008) and
references therein for more details). Their linear
running time makes them ideal candidates for large
scale text processing, and our model of choice for
this paper.
Unfortunately, the parsing accuracies of all mod-
els have been reported to drop significantly on out-
of-domain test sets, due to shifts in vocabulary and
grammar usage (Gildea, 2001; McClosky et al,
2006b; Foster, 2010). In this paper, we focus our
attention on the task of parsing questions. Questions
pose interesting challenges for WSJ-trained parsers
because they are heavily underrepresented in the
training data (there are only 334 questions among
the 39,832 training sentences). At the same time,
questions are of particular interest for user facing
applications like question answering or web search,
which necessitate parsers that can process questions
in a fast and accurate manner.
We start our investigation in Section 3 by train-
ing several state-of-the-art (dependency and con-
stituency) parsers on the standard WSJ training set.
When evaluated on a question corpus, we observe
dramatic accuracy drops exceeding 20% for the de-
terministic shift-reduce parsers. In general, depen-
dency parsers (McDonald et al, 2006; Nivre et al,
2007), seem to suffer more from this domain change
than constituency parsers (Charniak and Johnson,
2005; Petrov et al, 2006). Overall, the latent vari-
able approach of Petrov et al (2006) appears to gen-
eralize best to this new domain, losing only about
5%. Unfortunately, the parsers that generalize better
to this new domain have time complexities that are
cubic in the sentence length (or even higher), render-
ing them impractical for web-scale text processing.
705
SBARQ
WHNP
WP
What
SQ
VBZ
does
NP
DT
the
NNP
Peugeot
NN
company
VP
VB
manufacture
.
?
(a)
What does the Peugeot company manufacture ?ROOT
dobj aux det nn p   nsubjroot
(b)
Figure 1: Example constituency tree from the QuestionBank (a) converted to labeled Stanford dependencies (b).
We therefore propose an uptraining method, in
which a deterministic shift-reduce parser is trained
on the output of a more accurate, but slower parser
(Section 4). This type of domain adaptation is rem-
iniscent of self-training (McClosky et al, 2006a;
Huang and Harper, 2009) and co-training (Blum and
Mitchell, 1998; Sagae and Lavie, 2006), except that
the goal here is not to further improve the perfor-
mance of the very best model. Instead, our aim is
to train a computationally cheaper model (a linear
time dependency parser) to match the performance
of the best model (a cubic time constituency parser),
resulting in a computationally efficient, yet highly
accurate model.
In practice, we parse a large amount of unlabeled
data from the target domain with the constituency
parser of Petrov et al (2006) and then train a deter-
ministic dependency parser on this noisy, automat-
ically parsed data. The accuracy of the linear time
parser on a question test set goes up from 60.06%
(LAS) to 76.94% after uptraining, which is compa-
rable to adding 2,000 labeled questions to the train-
ing data. Combining uptraining with 2,000 labeled
questions further improves the accuracy to 84.14%,
fully recovering the drop between in-domain and
out-of-domain accuracy.
We also present a detailed error analysis in Sec-
tion 5, showing that the errors of the WSJ-trained
model are primarily caused by sharp changes in syn-
tactic configurations and only secondarily due to
lexical shifts. Uptraining leads to large improve-
ments across all error metrics and especially on im-
portant dependencies like subjects (nsubj).
2 Experimental Setup
We used the following experimental protocol
throughout the paper.
2.1 Data
Our main training set consists of Sections 02-21 of
the Wall Street Journal portion of the Penn Treebank
(Marcus et al, 1993), with Section 22 serving as de-
velopment set for source domain comparisons. For
our target domain experiments, we evaluate on the
QuestionBank (Judge et al, 2006), which includes
a set of manually annotated questions from a TREC
question answering task. The questions in the Ques-
tionBank are very different from our training data in
terms of grammatical constructions and vocabulary
usage, making this a rather extreme case of domain-
adaptation. We split the 4,000 questions contained
in this corpus in three parts: the first 2,000 ques-
tions are reserved as a small target-domain training
set; the remaining 2,000 questions are split in two
equal parts, the first serving as development set and
the second as our final test set. We report accuracies
on the developments sets throughout this paper, and
test only at the very end on the final test set.
We convert the trees in both treebanks from con-
stituencies to labeled dependencies (see Figure 1)
using the Stanford converter, which produces 46
types of labeled dependencies1 (de Marneffe et al,
2006). We evaluate on both unlabeled (UAS) and
labeled dependency accuracy (LAS).2
Additionally, we use a set of 2 million ques-
tions collected from Internet search queries as unla-
beled target domain data. All user information was
anonymized and only the search query string was re-
tained. The question sample is selected at random
after passing two filters that select queries that are
1We use the Stanford Lexicalized Parser v1.6.2.
2Because the QuestionBank does not contain function tags,
we decided to strip off the function tags from the WSJ be-
fore conversion. The Stanford conversion only uses the -ADV
and -TMP tags, and removing all function tags from the WSJ
changed less than 0.2% of the labels (primarily tmod labels).
706
Training on Evaluating on WSJ Section 22 Evaluating on QuestionBank
WSJ Sections 02-21 F1 UAS LAS POS F1 UAS LAS POS
Nivre et al (2007) ? 88.42 84.89 95.00 ? 74.14 62.81 88.48
McDonald et al (2006) ? 89.47 86.43 95.00 ? 80.01 67.00 88.48
Charniak (2000) 90.27 92.33 89.86 96.71 83.01 85.61 73.59 90.49
Charniak and Johnson (2005) 91.92 93.56 91.24 96.69 84.47 87.13 75.94 90.59
Petrov et al (2006) 90.70 92.91 90.48 96.27 85.52 88.17 79.10 90.57
Petrov (2010) 92.10 93.85 91.60 96.44 86.62 88.77 79.92 91.08
Our shift-reduce parser ? 88.24 84.69 95.00 ? 72.23 60.06 88.48
Our shift-reduce parser (gold POS) ? 90.51 88.53 100.00 ? 78.30 68.92 100.00
Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets.
similar in style to the questions in the QuestionBank:
(i) the queries must start with an English function
word that can be used to start a question (what, who
when, how, why, can, does, etc.), and (ii) the queries
have a maximum length of 160 characters.
2.2 Parsers
We use multiple publicly available parsers, as well
as our own implementation of a deterministic shift-
reduce parser in our experiments. The depen-
dency parsers that we compare are the determinis-
tic shift-reduce MaltParser (Nivre et al, 2007) and
the second-order minimum spanning tree algorithm
based MstParser (McDonald et al, 2006). Our shift-
reduce parser is a re-implementation of the Malt-
Parser, using a standard set of features and a lin-
ear kernel SVM for classification. We also train and
evaluate the generative lexicalized parser of Char-
niak (2000) on its own, as well as in combination
with the discriminative reranker of Charniak and
Johnson (2005). Finally, we run the latent variable
parser (a.k.a. BerkeleyParser) of Petrov et al (2006),
as well as the recent product of latent variable gram-
mars version (Petrov, 2010). To facilitate compar-
isons between constituency and dependency parsers,
we convert the output of the constituency parsers to
labeled dependencies using the same procedure that
is applied to the treebanks. We also report their F1
scores for completeness.
While the constituency parsers used in our experi-
ments view part-of-speech (POS) tagging as an inte-
gral part of parsing, the dependency parsers require
the input to be tagged with a separate POS tagger.
We use the TnT tagger (Brants, 2000) in our experi-
ments, because of its efficiency and ease of use. Tag-
ger and parser are always trained on the same data.
3 Parsing Questions
We consider two domain adaptation scenarios in this
paper. In the first scenario (sometimes abbreviated
as WSJ), we assume that we do not have any labeled
training data from the target domain. In practice, this
will always be the case when the target domain is
unknown or very diverse. The second scenario (ab-
breviated as WSJ+QB) assumes a small amount of
labeled training data from the target domain. While
this might be expensive to obtain, it is certainly fea-
sible for narrow domains (e.g. questions), or when a
high parsing accuracy is really important.
3.1 No Labeled Target Domain Data
We first trained all parsers on the WSJ training set
and evaluated their performance on the two domain
specific evaluation sets (newswire and questions).
As can be seen in the left columns of Table 1, all
parsers perform very well on the WSJ development
set. While there are differences in the accuracies,
all scores fall within a close range. The table also
confirms the commonly known fact (Yamada and
Matsumoto, 2003; McDonald et al, 2005) that con-
stituency parsers are more accurate at producing de-
pendencies than dependency parsers (at least when
the dependencies were produced by a deterministic
transformation of a constituency treebank, as is the
case here).
This picture changes drastically when the per-
formance is measured on the QuestionBank devel-
opment set (right columns in Table 1). As one
707
Evaluating on Training on WSJ + QB Training on QuestionBank
QuestionBank F1 UAS LAS POS F1 UAS LAS POS
Nivre et al (2007) ? 83.54 78.85 91.32 ? 79.72 73.44 88.80
McDonald et al (2006) ? 84.95 80.17 91.32 ? 82.52 77.20 88.80
Charniak (2000) 89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84
Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56
Petrov (2010) 92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79
Our shift-reduce parser ? 83.70 78.27 91.32 ? 80.44 74.29 88.80
Our shift-reduce parser (gold POS) ? 89.39 86.60 100.00 ? 87.31 84.15 100.00
Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.
might have expected, the accuracies are significantly
lower, however, the drop for some of the parsers
is shocking. Most notably, the deterministic shift-
reduce parsers lose almost 25% (absolute) on la-
beled accuracies, while the latent variable parsers
lose around 12%.3 Note also that even with gold
POS tags, LAS is below 70% for our determinis-
tic shift-reduce parser, suggesting that the drop in
accuracy is primarily due to a syntactic shift rather
than a lexical shift. These low accuracies are espe-
cially disturbing when one considers that the aver-
age question in the evaluation set is only nine words
long and therefore potentially much less ambiguous
than WSJ sentences. We will examine the main error
types more carefully in Section 5.
Overall, the dependency parsers seem to suf-
fer more from the domain change than the con-
stituency parsers. One possible explanation is that
they lack the global constraints that are enforced by
the (context-free) grammars. Even though the Mst-
Parser finds the globally best spanning tree, all con-
straints are local. This means for example, that it
is not possible to require the final parse to contain
a verb (something that can be easily expressed by
a top-level production of the form S ? NP VP in a
context free grammar). This is not a limitation of de-
pendency parsers in general. For example, it would
be easy to enforce such constraints in the Eisner
(1996) algorithm or using Integer Linear Program-
ming approaches (Riedel and Clarke, 2006; Martins
et al, 2009). However, such richer modeling capac-
ity comes with a much higher computational cost.
Looking at the constituency parsers, we observe
3The difference between our shift-reduce parser and the
MaltParser are due to small differences in the feature sets.
that the lexicalized (reranking) parser of Charniak
and Johnson (2005) loses more than the latent vari-
able approach of Petrov et al (2006). This differ-
ence doesn?t seem to be a difference of generative
vs. discriminative estimation. We suspect that the
latent variable approach is better able to utilize the
little evidence in the training data. Intuitively speak-
ing, some of the latent variables seem to get alo-
cated for modeling the few questions present in the
training data, while the lexicalization contexts are
not able to distinguish between declarative sentences
and questions.
To verify this hypothesis, we conducted two addi-
tional experiments. In the first experiment, we col-
lapsed the question specific phrasal categories SQ
and SBARQ to their declarative sentence equivalents
S and SBAR. When the training and test data are
processed this way, the lexicalized parser loses 1.5%
F1, while the latent variable parser loses only 0.7%.
It is difficult to examine the grammars, but one can
speculate that some of the latent variables were used
to model the question specific constructions and the
model was able to re-learn the distinctions that we
purposefully collapsed. In the second experiment,
we removed all questions from the WSJ training set
and retrained both parsers. This did not make a
significant difference when evaluating on the WSJ
development set, but of course resulted in a large
performance drop when evaluating on the Question-
Bank. The lexicalized parser came out ahead in this
experiment,4 confirming our hypothesis that the la-
tent variable model is better able to pick up the small
amount of relevant evidence that is present in the
WSJ training data (rather than being systematically
4The F1 scores were 52.40% vs. 56.39% respectively.
708
better suited for modeling questions).
3.2 Some Labeled Target Domain Data
In the above experiments, we considered a situation
where we have no labeled training data from the tar-
get domain, as will typically be the case. We now
consider a situation where a small amount of labeled
data (2,000 manually parsed sentences) from the do-
main of interest is available for training.
We experimented with two different ways of uti-
lizing this additional training data. In a first experi-
ment, we trained models on the concatenation of the
WSJ and QuestionBank training sets (we did not at-
tempt to weight the different corpora). As Table 2
shows (left columns), even a modest amount of la-
beled data from the target domain can significantly
boost parsing performance, giving double-digit im-
provements in some cases. While not shown in the
table, the parsing accuracies on the WSJ develop-
ment set where largely unaffected by the additional
training data.
Alternatively, one can also train models exclu-
sively on the QuestionBank data, resulting in ques-
tion specific models. The parsing accuracies of
these domain-specific models are shown in the right
columns of Table 2, and are significantly lower than
those of models trained on the concatenated training
sets. They are often times even lower than the results
of parsers trained exclusively on the WSJ, indicating
that 2,000 sentences are not sufficient to train accu-
rate parsers, even for quite narrow domains.
4 Uptraining for Domain-Adaptation
The results in the previous section suggest that
parsers without global constraints have difficul-
ties dealing with the syntactic differences between
declarative sentences and questions. A possible ex-
planation is that similar word configurations can ap-
pear in both types of sentences, but with very differ-
ent syntactic interpretation. Local models without
global constraints are therefore mislead into dead-
end interpretations from which they cannot recover
(McDonald and Nivre, 2007). Our approach will
therefore be to use a large amount of unlabeled data
to bias the model towards the appropriate distribu-
tion for the target domain. Rather than looking
for feature correspondences between the domains
 70
 75
 80
 85
 90
1M100K10K1K100100
U
A
S
WSJ+QB
WSJ
 60
 65
 70
 75
 80
 85
1M100K10K1K100100
LA
S
Number of unlabeled questions
WSJ+QB
WSJ
Figure 2: Uptraining with large amounts of unlabeled
data gives significant improvements over two different
supervised baselines.
(Blitzer et al, 2006), we propose to use automati-
cally labeled target domain data to learn the target
domain distribution directly.
4.1 Uptraining vs. Self-training
The idea of training parsers on their own output has
been around for as long as there have been statis-
tical parsers, but typically does not work well at
all (Charniak, 1997). Steedman et al (2003) and
Clark et al (2003) present co-training procedures
for parsers and taggers respectively, which are ef-
fective when only very little labeled data is avail-
able. McClosky et al (2006a) were the first to im-
prove a state-of-the-art constituency parsing system
by utilizing unlabeled data for self-training. In sub-
sequent work, they show that the same idea can be
used for domain adaptation if the unlabeled data is
chosen accordingly (McClosky et al, 2006b). Sagae
and Tsujii (2007) co-train two dependency parsers
by adding automatically parsed sentences for which
the parsers agree to the training data. Finally, Suzuki
et al (2009) present a very effective semi-supervised
approach in which features from multiple generative
models estimated on unlabeled data are combined in
a discriminative system for structured prediction.
All of these approaches have in common that their
ultimate goal is to improve the final performance.
Our work differs in that instead of improving the
709
Uptraining with Using only WSJ data Using WSJ + QB data
different base parsers UAS LAS POS UAS LAS POS
Baseline 72.23 60.06 88.48 83.70 78.27 91.32
Self-training 73.62 61.63 89.60 84.26 79.15 92.09
Uptraining on Petrov et al (2006) 86.02 76.94 90.75 88.38 84.02 93.63
Uptraining on Petrov (2010) 85.21 76.19 90.74 88.63 84.14 93.53
Table 3: Uptraining substantially improves parsing accuracies, while self-training gives only minor improvements.
performance of the best parser, we want to build
a more efficient parser that comes close to the ac-
curacy of the best parser. To do this, we parse
the unlabeled data with our most accurate parser
and generate noisy, but fairly accurate labels (parse
trees) for the unlabeled data. We refer to the parser
used for producing the automatic labels as the base
parser (unless otherwise noted, we used the latent
variable parser of Petrov et al (2006) as our base
parser). Because the most accurate base parsers are
constituency parsers, we need to convert the parse
trees to dependencies using the Stanford converter
(see Section 2). The automatically parsed sentences
are appended to the labeled training data, and the
shift-reduce parser (and the part-of-speech tagger)
are trained on this new training set. We did not
increase the weight of the WSJ training data, but
weighted the QuestionBank training data by a fac-
tor of ten in the WSJ+QB experiments.
4.2 Varying amounts of unlabeled data
Figure 2 shows the efficacy of uptraining as a func-
tion of the size of the unlabeled data. Both la-
beled (LAS) and unlabeled accuracies (UAS) im-
prove sharply when automatically parsed sentences
from the target domain are added to the training data,
and level off after 100,000 sentences. Comparing
the end-points of the dashed lines (models having
access only to labeled data from the WSJ) and the
starting points of the solid lines (models that have
access to both WSJ and QuestionBank), one can see
that roughly the same improvements (from 72% to
86% UAS and from 60% to 77% LAS) can be ob-
tained by having access to 2,000 labeled sentences
from the target domain or uptraining with a large
amount of unlabeled data from the target domain.
The benefits seem to be complementary and can be
combined to give the best results. The final accu-
racy of 88.63 / 84.14 (UAS / LAS) on the question
evaluation set is comparable to the in-domain per-
formance on newswire data (88.24 / 84.69).
4.3 Varying the base parser
Table 3 then compares uptraining on the output of
different base parsers to pure self-training. In these
experiments, the same set of 500,000 questions was
parsed by different base parsers. The automatic
parses were then added to the labeled training data
and the parser was retrained. As the results show,
self-training provides only modest improvements of
less than 2%, while uptraining gives double-digit
improvements in some cases. Interestingly, there
seems to be no substantial difference between up-
training on the output of a single latent variable
parser (Petrov et al, 2006) and a product of latent
variable grammars (Petrov, 2010). It appears that
the roughly 1% accuracy difference between the two
base parsers is not important for uptraining.
4.4 POS-less parsing
Our uptraining procedure improves parse quality on
out-of-domain data to the level of in-domain ac-
curacy. However, looking closer at Table 3, one
can see that the POS accuracy is still relatively low
(93.53%), potentially limiting the final accuracy.
To remove this limitation (and also the depen-
dence on a separate POS tagger), we experimented
with word cluster features. As shown in Koo et al
(2008), word cluster features can be used in con-
junction with POS tags to improve parsing accuracy.
Here, we use them instead of POS tags in order to
further reduce the domain-dependence of our model.
Similar to Koo et al (2008), we use the Brown clus-
tering algorithm (Brown et al, 1992) to produce a
deterministic hierarchical clustering of our input vo-
cabulary. We then extract features based on vary-
710
UAS LAS POS
Part-of-Speech Tags 88.35 84.05 93.53
Word Cluster Features 87.92 83.73 ?
Table 4: Parsing accuracies of uptrained parsers with and
without part-of-speech tags and word cluster features.
ing cluster granularities (6 and 10 bits in our experi-
ments). Table 4 shows that roughly the same level of
accuracy can be achieved with cluster based features
instead of POS tag features. This change makes our
parser completely deterministic and enables us to
process sentences in a single left-to-right pass.
5 Error Analysis
To provide a better understanding of the challenges
involved in parsing questions, we analyzed the er-
rors made by our WSJ-trained shift-reduce parser
and also compared them to the errors that are left
after uptraining.
5.1 POS errors
Many parsing errors can be traced back to POS tag-
ging errors, which are much more frequent on out-
of-domain data than on in-domain data (88.8% on
the question data compared to above 95.0% on WSJ
data). Part of the reason for the lower POS tagging
accuracy is the higher unknown word ratio (7.3% on
the question evaluation set, compared to 3.4% on the
WSJ evaluation set). Another reason is a change in
the lexical distribution.
For example, wh-determiners (WDT) are quite
rare in the WSJ training data (relative frequency
0.45%), but five times more common in the Ques-
tionBank training data (2.49%). In addition to this
frequency difference, 52.43% of the WDTs in the
WSJ are the word ?which? and 46.97% are?that?. In
the QuestionBank on the other hand, ?what? is by
far the most common WDT word (81.40%), while
?which? and ?that? account only for 13.65% and
4.94% respectively. Not surprisingly the most com-
mon POS error involves wh-determiners (typically
the word ?what?) being incorrectly labeled as Wh-
pronouns (WP), resulting in head and label errors
like the one shown in Figure 3(a).
To separate out POS tagging errors from parsing
errors, we also ran experiments with correct (gold)
Dep. Label Frequency WSJ Uptrained
nsubj 934 41.02 88.64
amod 556 78.21 86.00
dobj 555 70.10 83.12
attr 471 8.64 93.49
aux 467 77.31 82.56
Table 5: F1 scores for the most frequent labels in the
QuestionBank development set. Uptraining leads to huge
improvements compared to training only on the WSJ.
POS tags. The parsing accuracies of our shift-reduce
parser using gold POS tags are listed in the last rows
of Tables 1 and 2. Even with gold POS tags, the de-
terministic shift-reduce parser falls short of the ac-
curacies of the constituency parsers (with automatic
tags), presumably because the shift-reduce model is
making only local decisions and is lacking the global
constraints provided by the context-free grammar.
5.2 Dependency errors
To find the main error types, we looked at the most
frequent labels in the QuestionBank development
set, and analyzed the ones that benefited the most
from uptraining. Table 5 has the frequency and F-
scores of the dependency types that we are going to
discuss in the following. We also provide examples
which are illustrated in Figure 3.
nsubj: The WSJ-trained model is often producing
parses that are missing a subject (nsubj). Questions
like ?What is the oldest profession?? and ?When
was Ozzy Osbourne born?? should have ?profes-
sion? and ?Osbourne? as nsubjs, but in both cases
the WSJ-trained parser did not label any subj (see
Figures 3(b) and 3(c)). Another common error is to
mislabel nsubj. For example, the nsubj of ?What are
liver enzymes?? should be enzymes, but the WSJ-
trained parser labels ?What? as the nsubj, which
makes sense in a statement but not in a question.
amod: The model is overpredicting ?amod?, re-
sulting in low precision figures for this label. An
example is ?How many points make up a perfect
fivepin bowling score??. The Stanford dependency
uses ?How? as the head of ?many? in noun phrases
like ?How many points?, and the relation is a generic
?dep?. But in the WSJ model prediction, ?many?s?
head is ?points,? and the relation mislabeled as
amod. Since it?s an adjective preceding the noun,
711
What is the oldest profession ?
ROOT
det     amod proot attr nsubj
WP VBZ DT JJS NN .ROOT
det     amod proot   attrdep
WP VBZ DT JJS NN .
What is the oldest profession ?
When was Ozzy Osbourne born ?
ROOT WRB VBZ  NNP NNP VBN .
root padvmod aux nn nsubj
When was Ozzy Osbourne   born ?
ROOT WRB VBZ  NNP NNP   NNP .
   root    nnnn pcompl nsubj
What films featured the character ?
ROOT WDT NNS  VBD DT NN  NNP NNP .
Popeye Doyle
nsubj dep det    nn nn dobj
What films featured the character ?
ROOT WP NNS  VBD DT NN  NNP NNP .
Popeye Doyle
   nsubj    compl det    nn nn   root    ccomp(a)
(b)
(c)
(d)
How many people did Randy ?
ROOT WRB JJ  NNS VBD NNP  NNP VB .
Craft kill
  dobj dep aux    nn nsubj p
?
.
dep
How many people did Randy
ROOT WRB JJ  NNS VBD NNP  NNP VB
Craft kill
compl amod ccomp   nn nsubj pnsubjroot
Figure 3: Example questions from the QuestionBank development set and their correct parses (left), as well as the
predictions of a model trained on the WSJ (right).
the WSJ model often makes this mistake and there-
fore the precision is much lower when it doesn?t see
more questions in the training data.
dobj: The WSJ model doesn?t predict object ex-
traction well. For example, in ?How many people
did Randy Craft kill?? (Figure 3(d)), the direct ob-
ject of kill should be ?How many people.? In the
Stanford dependencies, the correct labels for this
noun phrase are ?dobj dep dep,? but the WSJ model
predicts ?compl amod nsubj.? This is a common
error caused by the different word order in ques-
tions. The uptrained model is much better at han-
dling these type of constructions.
attr: An attr (attributive) is a wh-noun phrase
(WHNP) complement of a copular verb. In the WSJ
training data, only 4,641 out of 950,028 dependen-
cies are attr (0.5%); in the QuestionBank training
data, 1,023 out of 17,069 (6.0%) are attr. As a con-
sequence, the WSJ model cannot predict this label
in questions very well.
aux: ?What does the abbreviation AIDS stand
for?? should have ?stand? as the main head of the
sentence, and ?does? as its aux. However, the WSJ
model labeled ?does? as the main head. Similar
patterns occur in many questions, and therefore the
WSJ has a very low recall rate.
In contrast, mostly local labels (that are not re-
lated to question/statement structure differences)
have a consistently high accuracy. For example: det
has an accuracy of 98.86% with the WSJ-trained
model, and 99.24% with the uptrained model.
6 Conclusions
We presented a method for domain adaptation of de-
terministic shift-reduce parsers. We evaluated mul-
tiple state-of-the-art parsers on a question corpus
and showed that parsing accuracies degrade substan-
tially on this out-of-domain task. Most notably, de-
terministic shift-reduce parsers have difficulty deal-
ing with the modified word order and lose more
than 20% in accuracy. We then proposed a simple,
yet very effective uptraining method for domain-
adaptation. In a nutshell, we trained a deterministic
shift-reduce parser on the output of a more accurate,
but slower parser. Uptraining with large amounts of
unlabeled data gives similar improvements as hav-
ing access to 2,000 labeled sentences from the target
domain. With 2,000 labeled questions and a large
amount of unlabeled questions, uptraining is able to
close the gap between in-domain and out-of-domain
accuracy.
712
Acknowledgements
We would like to thank Ryan McDonald for run-
ning the MstParser experiments and for many fruit-
ful discussions on this topic. We would also like to
thank Joakim Nivre for help with the MatlParser and
Marie-Catherine de Marneffe for help with the Stan-
ford Dependency Converter.
References
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP ?06.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT ?98.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In CoNLL ?08.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In AI ?97.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
S. Clark, J. Curran, and M. Osborne. 2003. Bootstrap-
ping pos-taggers using unlabelled data. In CoNLL ?03.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In COLING ?96.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
Z. Huang and M. Harper. 2009. Self-training PCFG
grammars with latent annotations across languages. In
EMNLP ?09.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
bank: creating a corpus of parse-annotated questions.
In ACL ?06.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In ACL ?10.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL ?08.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In ACL ?09.
D. McClosky, E. Charniak, and M. Johnson. 2006a. Ef-
fective self-training for parsing. In NAACL ?06.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
ACL ?06.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
EMNLP ?07.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In CoNLL ?06.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2).
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov. 2010. Products of random latent variable
grammars. In NAACL ?10.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP ?06.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In NAACL ?06.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In CoNLL ?07.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL ?03.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In EMNLP
?09.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In IWPT
?03.
713
Deterministic Statistical Mapping of
Sentences to Underspecified Semantics
Hiyan Alshawi
Google, Inc.
(hiyan@google.com)
Pi-Chuan Chang
Google, Inc.
(pichuan@google.com)
Michael Ringgaard
Google, Inc.
(ringgaard@google.com)
Abstract
We present a method for training a statistical model for mapping natural language sentences to
semantic expressions. The semantics are expressions of an underspecified logical form that has prop-
erties making it particularly suitable for statistical mapping from text. An encoding of the semantic
expressions into dependency trees with automatically generated labels allows application of exist-
ing methods for statistical dependency parsing to the mapping task (without the need for separate
traditional dependency labels or parts of speech). The encoding also results in a natural per-word
semantic-mapping accuracy measure. We report on the results of training and testing statistical mod-
els for mapping sentences of the Penn Treebank into the semantic expressions, for which per-word
semantic mapping accuracy ranges between 79% and 86% depending on the experimental condi-
tions. The particular choice of algorithms used also means that our trained mapping is deterministic
(in the sense of deterministic parsing), paving the way for large-scale text-to-semantic mapping.
1 Introduction
Producing semantic representations of text is motivated not only by theoretical considerations but also
by the hypothesis that semantics can be used to improve automatic systems for tasks that are intrinsically
semantic in nature such as question answering, textual entailment, machine translation, and more gen-
erally any natural language task that might benefit from inference in order to more closely approximate
human performance. Since formal logics have formal denotational semantics, and are good candidates
for supporting inference, they have often been taken to be the targets for mapping text to semantic
representations, with frameworks emphasizing (more) tractable inference choosing first order predicate
logic (Stickel, 1985) while those emphasizing representational power favoring one of the many available
higher order logics (van Benthem, 1995).
It was later recognized that in order to support some tasks, fully specifying certain aspects of a logic
representation, such as quantifier scope, or reference resolution, is often not necessary. For example, for
semantic translation, most ambiguities of quantifier scope can be carried over from the source language
to the target language without being resolved. This led to the development of underspecified semantic
representations (e.g. QLF, Alshawi and Crouch (1992) and MRS, Copestake et al(2005)) which are
easier to produce from text without contextual inference but which can be further specified as necessary
for the task being performed.
While traditionally mapping text to formal representations was predominantly rule-based, for both
the syntactic and semantic components (Montague (1973), Pereira and Shieber (1987), Alshawi (1992)),
good progress in statistical syntactic parsing (e.g. Collins (1999), Charniak (2000)) led to systems that
applied rules for semantic interpretation to the output of a statistical syntactic parser (e.g. Bos et al
(2004)). More recently researchers have looked at statistical methods to provide robust and trainable
methods for mapping text to formal representations of meaning (Zettlemoyer and Collins, 2005).
In this paper we further develop the two strands of work mentioned above, i.e. mapping text to
underspecified semantic representations and using statistical parsing methods to perform the analysis.
15
Here we take a more direct route, starting from scratch by designing an underspecified semantic repre-
sentation (Natural Logical Form, or NLF) that is purpose-built for statistical text-to-semantics mapping.
An underspecified logic whose constructs are motivated by natural language and that is amenable to
trainable direct semantic mapping from text without an intervening layer of syntactic representation. In
contrast, the approach taken by (Zettlemoyer and Collins, 2005), for example, maps into traditional logic
via lambda expressions, and the approach taken by (Poon and Domingos, 2009) depends on an initial
step of syntactic parsing.
In this paper, we describe a supervised training method for mapping text to NLF, that is, producing
a statistical model for this mapping starting from training pairs consisting of sentences and their corre-
sponding NLF expressions. This method makes use of an encoding of NLF expressions into dependency
trees in which the set of labels is automatically generated from the encoding process (rather than being
pre-supplied by a linguistically motivated dependency grammar). This encoding allows us to perform the
text-to-NLF mapping using any existing statistical methods for labeled dependency parsing (e.g. Eisner
(1996), Yamada and Matsumoto (2003), McDonald, Crammer, Pereira (2005)). A side benefit of the
encoding is that it leads to a natural per-word measure for semantic mapping accuracy which we use for
evaluation purposes. By combing our method with deterministic statistical dependency models together
with deterministic (hard) clusters instead of parts of speech, we obtain a deterministic statistical text-to-
semantics mapper, opening the way to feasible mapping of text-to-semantics at a large scale, for example
the entire web.
This paper concentrates on the text-to-semantics mapping which depends, in part, on some properties
of NLF. We will not attempt to defend the semantic representation choices for specific constructions il-
lustrated here. NLF is akin to a variable-free variant of QLF or an MRS in which some handle constraints
are determined during parsing. For the purposes of this paper it is sufficient to note that NLF has roughly
the same granularity of semantic representation as these earlier underspecified representations.
We outline the steps of our text-to-semantics mapping method in Section 2, introduce NLF in Sec-
tion 3, explain the encoding of NLF expressions as formal dependency trees in Section 4, and report on
experiments for training and testing statistical models for mapping text to NLF expressions in Section 5.
2 Direct Semantic Mapping
Our method for mapping text to natural semantics expressions proceeds as follows:
1. Create a corpus of pairs consisting of text sentences and their corresponding NLF semantic ex-
pressions.
2. For each of the sentence-semantics pairs in the corpus, align the words of the sentence to the tokens
of the NLF expressions.
3. ?Encode? each alignment pair as an ordered dependency tree in which the labels are generated by
the encoding process.
4. Train a statistical dependency parsing model with the set of dependency trees.
5. For a new input sentence S, apply the statistical parsing model to S, producing a labeled depen-
dency tree DS .
6. ?Decode? DS into a semantic expression for S.
For step 1, the experiments in this paper (Section 5) obtain the corpus by converting an existing
constituency treebank into semantic expressions. However, direct annotation of a corpus with semantic
expressions is a viable alternative, and indeed we are separately exploring that possibility for a different,
open domain, text corpus.
16
For steps 4 and 5, any method for training and applying a dependency model from a corpus of labeled
dependency trees may be used. As described in Section 5, for the experiments reported here we use an
algorithm similar to that of Nivre (2003).
For steps 2, 3 and 6, the encoding of NLF semantic expressions as dependency trees with automati-
cally constructed labels is described in Section 4.
3 Semantic Expressions
NLF expressions are by design amenable to facilitating training of text-to-semantics mappings. For this
purpose, NLF has a number of desirable properties:
1. Apart from a few built-in logical connectives, all the symbols appearing in NLF expressions are
natural language words.
2. For an NLF semantic expression corresponding to a sentence, the word tokens of the sentence
appear exactly once in the NLF expression.
3. The NLF notation is variable-free.
Technically, NLF expressions are expression of an underspecified logic, i.e. a semantic representation
that leaves open the interpretation of certain constructs (for example the scope of quantifiers and some
operators and the referents of terms such as anaphora, and certain implicit relations such as those for
compound nominals). NLF is similar in some ways to Quasi Logical Form, or QLF (Alshawi, 1992), but
the properties listed above keep NLF closer to natural language than QLF, hence natural logical form. 1
There is no explicit formal connection between NLF and Natural Logic (van Benthem, 1986), though it
may turn out that NLF is a convenient starting point for some Natural Logic inferences.
In contrast to statements of a fully specified logic in which denotations are typically taken to be
functions from possible worlds to truth values (Montague, 1973), denotations of a statement in an under-
specified logic are typically taken to be relations between possible worlds and truth values (Alshawi and
Crouch (1992), Alshawi (1996)). Formal denotations for NLF expressions are beyond the scope of this
paper and will be described elsewhere.
3.1 Connectives and Examples
A NLF expression for the sentence
In 2002, Chirpy Systems stealthily acquired two profitable companies producing pet acces-
sories.
is shown in Figure 1.
The NLF constructs and connectives are explained in Table 1. For variable-free abstraction, an NLF
expression [p, ?, a] corresponds to ?x.p(x, a). Note that some common logical operators are not
built-in since they will appear directly as words such as not.2 We currently use the unknown/unspecified
operator, %, mainly for linguistic constructions that are beyond the coverage of a particular semantic
mapping model. A simple example that includes % in our converted WSJ corpus is Other analysts are
nearly as pessimistic for which the NLF expression is
[are, analysts.other, pessimistic%nearly%as]
In Section 5 we give some statistics on the number of semantic expressions containing % in the data used
for our experiments and explain how it affects our accruracy results.
1The term QLF is now sometimes used informally (e.g. Liakata and Pulman (2002), Poon and Domingos (2009)) for any
logic-like semantic representation without explicit quantifier scope.
2NLF does include Horn clauses, which implictly encode negation, but since Horn clauses are not part of the experiments
reported in this paper, we will not discuss them further here.
17
[acquired
/stealthily
:[in, ?, 2002],
Chirpy+Systems,
companies.two
:profitable
:[producing,
?,
pet+accessories]]
Figure 1: Example of an NLF semantic expression.
Operator Example Denotation Language Constructs
[...] [sold, Chirpy, Growler] predication tuple clauses, prepositions, ...
: company:profitable intersection adjectives, relative clauses, ...
. companies.two (unscoped) quantification determiners, measure terms
? [in, ?, 2005] variable-free abstract prepositions, relatives, ...
_ [eating, _, apples] unspecified argument missing verb arguments, ...
{...} and{Chirpy, Growler} collection noun phrase coordination, ...
/ acquired/stealthily type-preserving operator adverbs, modals, ...
+ Chirpy+Systems implicit relation compound nominals, ...
@ meeting@yesterday temporal restriction bare temporal modifiers, ...
& [...] & [...] conjunction sentences, ...
|...| |Dublin, Paris, Bonn| sequence paragraphs, fragments, lists, ...
% met%as uncovered op constructs not covered
Table 1: NLF constructs and connectives.
4 Encoding Semantics as Dependencies
We encode NLF semantic expressions as labeled dependency trees in which the label set is generated
automatically by the encoding process. This is in contrast to conventional dependency trees for which
the label sets are presupplied (e.g. by a linguistic theory of dependency grammar). The purpose of
the encoding is to enable training of a statistical dependency parser and converting the output of that
parser for a new sentence into a semantic expression. The encoding involves three aspects: Alignment,
headedness, and label construction.
4.1 Alignment
Since, by design, each word token corresponds to a symbol token (the same word type) in the NLF ex-
pression, the only substantive issue in determining the alignment is the occurrence of multiple tokens
of the same word type in the sentence. Depending on the source of the sentence-NLF pairs used for
training, a particular word in the sentence may or may not already be associated with its corresponding
word position in the sentence. For example, in some of the experiments reported in this paper, this corre-
spondence is provided by the semantic expressions obtained by converting a constituency treebank (the
well-known Penn WSJ treebank). For situations in which the pairs are provided without this informa-
tion, as is the case for direct annotation of sentences with NLF expressions, we currently use a heuristic
greedy algorithm for deciding the alignment. This algorithm tries to ensure that dependents are near their
heads, with a preference for projective dependency trees. To guage the importance of including correct
alignments in the input pairs (as opposed to training with inferred alignments), we will present accuracy
results for semantic mapping for both correct and automatically infererred alignments.
18
4.2 Headedness
The encoding requires a definition of headedness for words in an NLF expression, i.e., a head-function
h from dependent words to head words. We define h in terms of a head-function g from an NLF
(sub)expression e to a word w appearing in that (sub)expression, so that, recursively:
g(w) = w
g([e1, ..., en]) = g(e1)
g(e1 : e2) = g(e1)
g(e1.e2) = g(e1)
g(e1/e2) = g(e1)
g(e1@e2) = g(e1)
g(e1&e2) = g(e1)
g(|e1, ..., en|) = g(e1)
g(e1{e2, ..., en}) = g(e1)
g(e1 + ...+ en) = g(en)
g(e1%e2) = g(e1)
Then a head word h(w) for a dependent w is defined in terms of the smallest (sub)expression e
containing w for which
h(w) = g(e) 6= w
For example, for the NLF expression in Figure 1, this yields the heads shown in Table 3. (The labels
shown in that table will be explained in the following section.)
This definition of headedness is not the only possible one, and other variations could be argued for.
The specific definition for NLF heads turns out to be fairly close to the notion of head in traditional
dependency grammars. This is perhaps not surprising since traditional dependency grammars are often
partly motivated by semantic considerations, if only informally.
4.3 Label Construction
As mentioned, the labels used during the encoding of a semantic expression into a dependency tree are
derived so as to enable reconstruction of the expression from a labeled dependency tree. In a general
sense, the labels may be regarded as a kind of formal semantic label, though more specifically, a label is
interpretable as a sequence of instructions for constructing the part of a semantic expression that links a
dependent to its head, given that part of the semantic expression, including that derived from the head,
has already been constructed. The string for a label thus consists of a sequence of atomic instructions,
where the decoder keeps track of a current expression and the parent of that expression in the expression
tree being constructed. When a new expression is created it becomes the current expression whose parent
is the old current expression. The atomic instructions (each expressed by a single character) are shown
in Table 2.
A sequence of instructions in a label can typically (but not always) be paraphrased informally as
?starting from head word wh, move to a suitable node (at or above wh) in the expression tree, add speci-
fied NLF constructs (connectives, tuples, abstracted arguments) and then add wd as a tuple or connective
argument.?
Continuing with our running example, the labels for each of the words are shown in Table 3.
Algorithmically, we find it convenient to transform semantic expressions into dependency trees and
vice versa via a derivation tree for the semantic expression in which the atomic instruction symbols listed
above are associated with individual nodes in the derivation tree.
The output of the statistical parser may contain inconsistent trees with formal labels, in particular
trees in which two different arguments are predicated to fill the same position in a semantic expression
tuple. For such cases, the decoder that produces the semantic expression applies the simple heuristic
19
Instruction Decoding action
[, {, | Set the current expression to a
newly created tuple, collection,
or sequence.
:, /, ., +, &, @, % Attach the current subexpression
to its parent with the specified
connective.
* Set the current expression to a
newly created symbol from the
dependent word.
0, 1, ... Add the current expression at the
specified parent tuple position.
?, _ Set the current subexpression to
a newly created abstracted-over or
unspecfied argument.
- Set the current subexpression to be
the parent of the current expression.
Table 2: Atomic instructions in formal label sequences.
Dependent Head Label
In acquired [:?1-*0
2002 in -*2
Chirpy Systems *+
Systems acquired -*1
stealthily acquired */
acquired [*0
two companies *.
profitable companies *:
companies acquired -*2
producing companies [:?1-*0
pet accessories *+
accessories producing -*2
Table 3: Formal labels for an example sentence.
20
Dataset Null Labels? Auto Align? WSJ sections Sentences
Train+Null-AAlign yes no 2-21 39213
Train-Null-AAlign no no 2-21 24110
Train+Null+AAlign yes yes 2-21 35778
Train-Null+AAlign no yes 2-21 22611
Test+Null-AAlign yes no 23 2416
Test-Null-AAlign no no 23 1479
Table 4: Datasets used in experiments.
of using the next available tuple position when such a conflicting configuration is predicated. In our
experiments, we are measuring per-word semantic head-and-label accuracy, so this heuristic does not
play a part in that evaluation measure.
5 Experiments
5.1 Data Preparation
In the experiments reported here, we derive our sentence-semantics pairs for training and testing from
the Penn WSJ Treebank. This choice reflects the lack, to our knowledge, of a set of such pairs for a
reasonably sized publicly available corpus, at least for NLF expressions. Our first step in preparing the
data was to convert the WSJ phrase structure trees into semantic expressions. This conversion is done
by programming the Stanford treebank toolkit to produce NLF trees bottom-up from the phrase structure
trees. This conversion process is not particularly noteworthy in itself (being a traditional rule-based
syntax-to-semantics translation process) except perhaps to the extent that the closeness of NLF to natural
language perhaps makes the conversion somewhat easier than, say, conversion to a fully resolved logical
form.
Since our main goal is to investigate trainable mappings from text strings to semantic expressions,
we only use the WSJ phrase structure trees in data preparation: the phrase structure trees are not used as
inputs when training a semantic mapping model, or when applying such a model. For the same reason,
in these experiments, we do not use the part-of-speech information associated with the phrase structure
trees in training or applying a semantic mapping model. Instead of parts-of-speech we use word cluster
features from a hierarchical clustering produced with the unsupervised Brown clustering method (Brown
et al 1992); specifically we use the publicly available clusters reported in Koo et al (2008).
Constructions in the WSJ that are beyond the explicit coverage of the conversion rules used for data
preparation result in expressions that include the unknown/unspecified (or ?Null?) operator %. We report
on different experimental settings in which we vary how we treat training or testing expressions with
%. This gives rise to the data sets in Table 4 which have +Null (i.e., including %), and -Null (i.e., not
including %) in the data set names.
Another attribute we vary in the experiments is whether to align the words in the semantic expressions
to the words in the sentence automatically, or whether to use the correct alignment (in this case preserved
from the conversion process, but could equally be provided as part of a manual semantic annotation
scheme, for example). In our current experiments, we discard non-projective dependency trees from
training sets. Automatic alignment results in additional non-projective trees, giving rise to different
effective training sets when auto-alignment is used: these sets are marked with +AAlign, otherwise -
AAlign. The training set numbers shown in Table 4 are the resulting sets after removal of non-projective
trees.
21
Training Test Accuracy(%)
+Null-AAlign +Null-AAlign 81.2
-Null-AAlign +Null-AAlign 78.9
-Null-AAlign -Null-AAlign 86.1
+Null-AAlign -Null-AAlign 86.5
Table 5: Per-word semantic accuracy when training with the correct alignment.
Training Test Accuracy(%)
+Null+AAlign +Null-AAlign 80.4
-Null+AAlign +Null-AAlign 78.0
-Null+AAlign -Null-AAlign 85.5
+Null+AAlign -Null-AAlign 85.8
Table 6: Per-word semantic accuracy when training with an auto-alignment.
5.2 Parser
As mentioned earlier, our method can make use of any trainable statistical dependency parsing algorithm.
The parser is trained on a set of dependency trees with formal labels as explained in Sections 2 and 4.
The specific parsing algorithm we use in these experiments is a deterministic shift reduce algorithm
(Nivre, 2003), and the specific implementation of the algorithm uses a linear SVM classifier for predict-
ing parsing actions (Chang et al, 2010). As noted above, hierarchical cluster features are used instead
of parts-of-speech; some of the features use coarse (6-bit) or finer (12-bit) clusters from the hierarchy.
More specifically, the full set of features is:
? The words for the current and next input tokens, for the top of the stack, and for the head of the
top of the stack.
? The formal labels for the top-of-stack token and its leftmost and rightmost children, and for the
leftmost child of the current token.
? The cluster for the current and next three input tokens and for the top of the stack and the token
below the top of the stack.
? Pairs of features combining 6-bit clusters for these tokens together with 12-bit clusters for the top
of stack and next input token.
5.3 Results
Tables 5 and 6 show the per-word semantic accuracy for different training and test sets. This measure is
simply the percentage of words in the test set for which both the predicted formal label and the head word
are correct. In syntactic dependency evaluation terminology, this corresponds to the labeled attachment
score.
All tests are with respect to the correct alignment; we vary whether the correct alignment (Table 5)
or auto-alignment (Table 6) is used for training to give an idea of how much our heuristic alignment
is hurting the semantic mapping model. As shown by comparing the two tables, the loss in accuracy
due to using the automatic alignment is only about 1%, so while the automatic alignment algorithm can
probably be improved, the resulting increase in accuracy would be relatively small.
As shown in the Tables 5 and 6, two versions of the test set are used: one that includes the ?Null?
operator %, and a smaller test set with which we are testing only the subset of sentences for which the
semantic expressions do not include this label. The highest accuracies (mid 80?s) shown are for the
22
# Labels # Train Sents Accuracy(%)
151 (all) 22611 85.5
100 22499 85.5
50 21945 85.5
25 17669 83.8
12 7008 73.4
Table 7: Per-word semantic accuracy after pruning label sets in Train-Null+AAlign (and testing with
Test-Null-AAlign).
(easier) test set which excludes examples in which the test semantic expressions contain Null operators.
The strictest settings, in which semantic expressions with Null are not included in training but included
in the test set effectively treat prediction of Null operators as errors. The lower accuracy (high 70?s) for
such stricter settings thus incorporates a penalty for our incomplete coverage of semantics for the WSJ
sentences. The less strict Test+Null settings in which % is treated as a valid output may be relevant to
applications that can tolerate some unknown operators between subexpressions in the output semantics.
Next we look at the effect of limiting the size of the automatically generated formal label set prior
to training. For this we take the configuration using the TrainWSJ-Null+AAlign training set and the
TestWSJ-Null-AAlign test set (the third row in Table refPerWordSemanticAccuracyAAlign for which
auto-alignment is used and only labels without the NULL operator % are included). For this training
set there are 151 formal labels. We then limit the training set to instances that only include the most
frequent k labels, for k = 100, 50, 25, 12, while keeping the test set the same. As can be seen in Table 7,
the accuracy is unaffected when the training set is limited to the 100 most frequent or 50 most frequent
labels. There is a slight loss when training is limited to 25 labels and a large loss if it is limited to 12
labels. This appears to show that, for this corpus, the core label set needed to construct the majority
of semantic expressions has a size somewhere between 25 and 50. It is perhaps interesting that this is
roughly the size of hand-produced traditional dependency label sets. On the other hand, it needs to be
emphasized that since Table 7 ignores beyond-coverage constructions that presently include Null labels,
it is likely that a larger label set would be needed for more complete semantic coverage.
6 Conclusion and Further Work
We?ve shown that by designing an underspecified logical form that is motivated by, and closely related to,
natural language constructions, it is possible to train a direct statistical mapping from pairs of sentences
and their corresponding semantic expressions, with per-word accuracies ranging from 79% to 86% de-
pending on the strictness of the experimental setup. The input to training does not require any traditional
syntactic categories or parts of speech. We also showed, more specifically, that we can train a model that
can be applied deterministically at runtime (using a deterministic shift reduce algorithm combined with
deterministic clusters), making large-scale text-to-semantics mapping feasible.
In traditional formal semantic mapping methods (Montague (1973), Bos et al (2004)), and even
some recent statistical mapping methods (Zettlemoyer and Collins, 2005), the semantic representation is
overloaded to performs two functions: (i) representing the final meaning, and (ii) composing meanings
from the meanings of subconstituents (e.g. through application of higher order lambda functions). In our
view, this leads to what are perhaps overly complex semantic representations of some basic linguistic
constructions. In contrast, in the method we presented, these two concerns (meaning representation and
semantic construction) are separated, enabling us to keep the semantics of constituents simple, while
turning the construction of semantic expressions into a separate structured learning problem (with its
own internal prediction and decoding mechanisms).
Although, in the experiments we reported here we do prepare the training data from a traditional
treebank, we are encouraged by the results and believe that annotation of a corpus with only semantic
23
expressions is sufficient for building an efficient and reasonably accurate text-to-semantics mapper. In-
deed, we have started building such a corpus for a question answering application, and hope to report
results for that corpus in the future. Other further work includes a formal denotational semantics of the
underspecified logical form and elaboration of practical inference operations with the semantic expres-
sions. This work may also be seen as a step towards viewing semantic interpretation of language as the
interaction between a pattern recognition process (described here) and an inference process.
References
Hiyan Alshawi and Richard Crouch. 1992. Monotonic Semantic Interpretation. Proceedings of the 30th Annual
Meeting of the Association for Computational Linguistics. Newark, Delaware, 32?39.
Hiyan Alshawi, ed. 1992. The Core Language Engine. MIT Press, Cambridge, Massachusetts.
Hiyan Alshawi. 1996. Underspecified First Order Logics. In Semantic Ambiguity and Underspecification, edited
by Kees van Deemter and Stanley Peters, CSLI Publications, Stanford, California.
Johan van Benthem. 1986. Essays in Logical Semantics. Reidel, Dordrecht.
Johan van Benthem. 1995. Language in Action: Categories, Lambdas, and Dynamic Logic. MIT Press, Cam-
bridge, Massachusetts.
Bos, Johan, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. Proceedings of the 20th International Conference on Computa-
tional Linguistics. Geneva, Switzerland, 1240?1246.
P. Brown, V. Pietra, P. Souza, J. Lai, and R. Mercer. 1992. Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Eugene Charniak. 2000. A maximum entropy inspired parser. Proceedings of the 1st Conference of the North
American Chapter of the Association for Computational Linguistics, Seattle, Washington.
Michael Collins. 1999. Head Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
A. Copestake, D. Flickinger, I. Sag, C. Pollard. 2005. Minimal Recursion Semantics, An Introduction. Research
on Language and Computation, 3(23):281-332.
D. Davidson. 1967. The Logical Form of Action Sentences. In The Logic of Decision and Action, edited by
N. Rescher, University of Pittsburgh Press, Pittsburgh, Pennsylvania.
Jason Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. Proceedings of
the 16th International Conference on Computational Linguistics (COLING-96, 340?345.
T. Koo, X. Carreras, and M. Collins. 2008. Simple Semisupervised Dependency Parsing. Proceedings of the
Annual Meeting of the Association for Computational Linguistics.
Maria Liakata and Stephen Pulman. 2002. From trees to predicate-argument structures. Proceedings of the 19th
International Conference on Computational Linguistics. Taipei, Taiwan, 563?569.
Chang, Y.-W., C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin. 2010. Training and Testing Low-degree
Polynomial Data Mappings via Linear SVM. Journal of Machine Learning Research, 11, 1471?1490.
Ryan McDonald, Koby Crammer and Fernando Pereira 2005. Online Large-Margin Training of Dependency
Parsers. Proceedomgs of the 43rd Annual Meeting of the Association for Computational Linguistics..
R. Montague. 1973. The Proper Treatment of Quantification in Ordinary English. In Formal Philosophy, edited
by R. Thomason, Yale University Press, New Haven.
Fernando Pereira and Stuart Shieber. 1987. Prolog and Natural Language Analysis. Center for the Study of
Language and Information, Stanford, California.
Joakim Nivre 2003 An Efficient Algorithm for Projective Dependency Parsing. Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, 149?160.
H. Poon and P. Domingos 2009. Unsupervised semantic parsing. Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, Singapore, 2009.
Mark Stickel. 1985. Automated deduction by theory resolution. Journal of Automated Reasoning, 1, 4.
Hiroyasu Yamada and Yuji Matsumoto 2003. Statistical dependency analysis with support vector machines.
Proceedings of the 8th International Workshop on Parsing Technologies, 195?206.
Zettlemoyer, Luke S. and Michael Collins. 2005. Learning to map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. Proceedings of the 21st Conference on Uncertainty in Artificial
Intelligence. Edinburgh, Scotland, 658?666.
24
