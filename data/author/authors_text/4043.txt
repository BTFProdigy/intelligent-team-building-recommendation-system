267
268
269
270
271
272
273
274
Book Review
Speech and Language Processing (second edition)
Daniel Jurafsky and James H. Martin
(Stanford University and University of Colorado at Boulder)
Pearson Prentice Hall, 2009, xxxi+988 pp; hardbound, ISBN 978-0-13-187321-6, $115.00
Reviewed by
Vlado Keselj
Dalhousie University
Speech and Language Processing is a general textbook on natural language processing,
with an excellent coverage of the area and an unusually broad scope of topics. It includes
statistical and symbolic approaches to NLP, as well as the main methods of speech
processing. I would rank it as the most appropriate introductory and reference textbook
for purposes such as an introductory fourth-year undergraduate or graduate course, a
general introduction for an interested reader, or an NLP reference for a researcher or
other professional working in an area related to NLP.
The book?s contents are organized in an order corresponding to different levels of
natural language processing. After the introductory chapter 1, there are five parts:
 Part I, Words: five chapters covering regular expressions, automata,
words, transducers, n-grams, part-of-speech tagging, hidden Markov
models, and maximum entropy models.
 Part II, Speech: five chapters covering phonetics, speech synthesis,
recognition, and phonology.
 Part III, Syntax: five chapters covering a formal grammar of English,
syntactic and statistical parsing, feature structures and unification, and
complexity of language classes.
 Part IV, Semantics and Pragmatics: five chapters covering representation
of meaning, computational semantics, lexical and computational lexical
semantics, and computational discourse.
 Part V, Applications: four chapters covering information extraction,
question answering, summarization, dialogue and conversational
agents, and machine translation.
The first edition of the book appeared in 2000 with the same title, and a very
similar size and structure. The structure has been changed by breaking the old part
?Words? into two parts ?Words? and ?Speech,? merging two old parts ?Semantics?
and ?Pragmatics? into one ?Semantics and Pragmatics,? and introducing one new part
?Applications.? I considered the old edition also to be the textbook of choice for a
course in NLP, but even though the changes may not appear to be significant, the
new edition is a marked improvement, both in overall content structure as well as in
presenting topics at a finer-grained level. Topics on speech synthesis and recognition
are significantly expanded; maximum entropy models are introduced and very well
Computational Linguistics Volume 35, Number 3
explained; and statistical parsing is covered better with an explanation of the principal
ideas in probabilistic lexicalized context-free grammars.
Both editions include very detailed examples, with actual numerical values and
computation, explaining various methods such as n-grams and smoothing. As another
example, maximum entropy modeling is a popular topic but in many books explained
only superficially, while here it is presented in a well-motivated and very intuitive way.
The learning method is not covered, and more details about it would be very useful. The
new edition conveniently includes the following useful reference tables on endpapers:
regular expression syntax, Penn Treebank POS tags, some WordNet 3.0 relations, and
major ARPAbet symbols.
The book was written with a broad coverage in mind (language and speech process-
ing; symbolic and stochastic approaches; and algorithmic, probabilistic, and signal-
processing methodology) and a wide audience: computer scientists, linguists, and
engineers. This has a positive side, because there is an educational need, especially in
computer science, to present NLP in a broad, integrated way; this has seemed to be
always very challenging and books with wide coverage were rare or non-existent. For
example, Allen?s (1995) Natural Language Understanding presented mostly a symbolic
approach to NLP, whereas Manning and Schu?tze?s (1999) Foundations of Statistical
Natural Language Processing presented an exclusively statistical approach. However,
there is also a negative side to the wide coverage?it is probably impossible to present
material in an order that would satisfy audiences from different backgrounds, in par-
ticular, linguists vs. computer scientists and engineers.
In my particular case, I started teaching a graduate course in Natural Language
Processing in 2002 at Dalhousie University, which later became a combined graduate/
undergraduate course. My goal was to present an integrated view of NLP with an
emphasis on two main paradigms: knowledge-based or symbolic, and probabilistic.
Not being aware of Jurafsky and Martin?s book at the time, I was using Manning and
Schu?tze?s book for the probabilistic part, and Sag and Wasow?s (1999) book Syntactic
Theory: A Formal Introduction for the symbolic part. I was very happy to learn about
Jurafsky and Martin?s book, since it fitted my course objectives very well. Although I
keep using the book, including this new edition in Fall 2008, and find it a very good
match with the course, there is quite a difference between the textbook and the course
in order of topics and the overall philosophy, so the book is used as a main supportive
reading reference and the course notes are used to navigate students through the mate-
rial. I will discuss some of the particular differences and similarities between Jurafsky
and Martin?s book and my course syllabus, as I believe my course is representative of
the NLP courses taught by many readers of this journal.
The book introduces regular expressions and automata in Chapter 2, and later
introduces context-free grammars in Chapter 12, followed by some general discussion
about formal languages and complexity in Chapter 16. This is a somewhat disrupted
sequence of topics from formal language theory, which should be covered earlier in
a typical undergraduate computer science program. Of course, it is not only a very
good idea but necessary to cover these topics in case a reader is not familiar with
them; however, they should be presented as one introductory unit. Additionally, a
presentation with an emphasis on theoretical background, rather than practical issues
of using regular expressions, would be more valuable. For example, the elegance of
the definition of regular sets, using elementary sets and closure of three operations,
is much more appealing and conceptually important than shorthand tricks of using
practical regular expressions, which are given more space and visibility. As another
example, it is hard to understand the choice of discussing equivalence of deterministic
464
Book Review
and non-deterministic finite automata in a small, note-like subsection (2.2.7), yet giv-
ing three-quarters of a page to an exponential algorithm for NFSA recognition (in
Figure 2.19), with a page-long discussion. It may be damaging to students even to
mention such a poor algorithm choice as the use of backtracking or a classical search
algorithm for NFSA acceptance. Context-free grammars are described in subsection
12.2.1; besides the need to have them earlier in the course, actually as a part of in-
troductory background review, more space should be given to this important formal-
ism. In addition to the concepts of derivation and ?syntactic parsing,? the following
concepts should be introduced as well: parse trees, left-most and right-most deriva-
tion, sentential forms, the language induced by a grammar, context-free languages,
grammar ambiguity, ambiguous sentences, bracketed representation of the parse trees,
and a grammar induced by a treebank. Some of these concepts are introduced in
other parts of the book. More advanced concepts would be desirable as well, such as
pumping lemmas, provable non-context-freeness of some languages, and push-down
automata.
As noted earlier, the order of the book contents follows the levels of NLP, starting
with words and speech, then syntax, and ending with semantics and pragmatics, fol-
lowed by applications. From my perspective, having applications at the end worked
well; however, while levels of NLP are an elegant and important view of the NLP
domain, it seems more important that students master the main methodological ap-
proaches to solving problems rather than the NLP levels of those problems. Hence, my
course is organized around topics such as n-gram models, probabilistic models, naive
Bayes, Bayesian networks, HMMs, unification-based grammars, and similar, rather than
following NLP levels and corresponding problems, such as POS tagging, word-sense
disambiguation, language modeling, and parsing. For example, HMMs are introduced
in Chapter 5, as a part of part-of-speech tagging; language modeling is discussed in
Chapter 4; and naive Bayes models are discussed in Chapter 20.
The discussion of unification in the book could be extended. It starts with feature
structures in Chapter 15, including discussion of unification, implementation, modeling
some natural language phenomena, and types and inheritance. The unification algo-
rithm (Figure 15.8, page 511) is poorly chosen. A better choice would be a standard,
elegant, and efficient algorithm, such as Huet?s (e.g., Knight 1989). The recursive algo-
rithm used in the book is not as efficient, elegant, nor easy to understand as Huet?s,
and it contains serious implementational traps. For example, it is not emphasized that
the proper way to maintain the pointers is to use the UNION-FIND data structure (e.g.,
Cormen et al 2002). If the pointers f1 and f2 are identical, there is no need to set f1.pointer
to f2. Finally, if f1 and f2 are complex structures, it is not a good idea to make a recursive
call before their unification is finished, since these structures may be accessed and
unified with other structures during the recursive call. The proper way to do it is to
use a stack or queue (usually called sigma) in Huet?s style, add pointers to structures to
be unified on the stack, and unify them after the unification of current feature structure
nodes is finished. Actually, this is similar to the use of ?agenda? earlier in the book, so
it would fit well with previous algorithms.
Regarding the order of the unification topics, I prefer an approach with a historical
order, starting from classical unification and resolution, followed by definite-clause
grammars, and then following with feature structures. The Prolog programming lan-
guage is a very important part in the story of unification, and should not be skipped,
as it is here. More could be written about type hierarchies and their implementation,
especially because they are conceptually very relevant to the recent popular use of
ontologies and the Semantic Web.
465
Computational Linguistics Volume 35, Number 3
As a final remark on the order, I found it useful in a computer science course
to present all needed linguistic background at the beginning, such as English word
classes (in Chapter 5), morphology (in Chapter 3), typical rules in English syntax (in
Chapter 12), and elements of semantics (in Chapter 19), and even a bit of pragmatics. As
can be seen, these pieces are placed throughout the book. The introduction of English
syntax in Chapter 12 is excellent and better than what can be typically found in NLP
books, but nonetheless, the ordering of the topics could be better: Agreement and other
natural language phenomena are intermixed with context-free rules, whereas in my
course those two were separated. The point should be that context-free grammars are
a very elegant formalism, but phenomena such as agreement, movement, and sub-
categorization are the issues that need to be addressed in natural languages and are
not handled by a context-free grammar (cf. Sag and Wasow 1999).
I also used the textbook in a graduate reading course on speech processing, with
emphasis on speech synthesis. The book was a useful reference, but the coverage was
sufficient for only a small part of the course.
The following are some minor remarks: The title of Chapter 13, ?Syntactic Pars-
ing,? is unusual because normally parsing is considered to be a synonym for syntactic
processing. The chapter describes the classical parsing algorithms for formal languages,
such as CKY and Earley?s, and the next chapter describes statistical parsing. Maybe
a title such as ?Classical Parsing,? ?Symbolic Parsing,? or simply ?Parsing? would be
better. The Good?Turing discounting on page 101 and the formula (4.24) are not well
explained. The formula (14.36) on page 479 for harmonic mean is not correct; the small
fractions in the denominator need to be added.
In conclusion, there are places that could be improved, and in particular, I did not
find that the order of material was the best possible. Nonetheless, the book is recom-
mended as first on the list for a textbook in a course in natural language processing.
References
Allen, James. 1995. Natural Language
Understanding. The Benjamin/Cummings
Publishing Company, Inc., Redwood City, CA.
Cormen, Thomas H., Leiserson, Charles E.,
Rivest, Ronald L., and Stein, Clifford. 2002.
Introduction to Algorithms, 2nd edition. The
MIT Press, Cambridge, MA.
Knight, Kevin. 1989. Unification: A
multidisciplinary survey. ACM Computing
Surveys, 21(1): 93?124.
Manning, Christopher D. and Schu?tze,
Hinrich. 1999. Foundations of Statistical
Natural Language Processing. The MIT
Press, Cambridge, MA.
Sag, Ivan A. and Wasow, Thomas. 1999.
Syntactic Theory: A Formal Introduction.
CSLI Publications, Stanford, CA.
Vlado Keselj is an Associate Professor of the Faculty of Computer Science at Dalhousie University.
His research interests include natural language processing, text processing, text mining, data
mining, and artificial intelligence. Keselj?s address is: Faculty of Computer Science, Dalhousie
University, 6050 University Ave, Halifax, NS, B3H 1W5 Canada; e-mail: vlado@cs.dal.ca.
466
R{j}ecnik.com: English?Serbo-Croatian Electronic Dictionary
Vlado KES?ELJ
Faculty of Computer Science
Dalhousie University, Halifax
Canada, vlado@cs.dal.ca
Tanja KES?ELJ
Korlex Software
Bedford NS, Canada
tanja@keselj.net
Larisa ZLATIC?
Larisa Zlatic Language Services
Austin, Texas, USA
larisaz@serbiantranslator.com
Abstract
The features of R{j}ecnik.com dictionary, as
one of the first on-line English-Serbo-Croatian
dictionaries are presented. The dictionary has
been on-line for the past five years and has been
frequently visited by the Internet users. We
evaluate and discuss the system based on the
analysis of the collected data about site vis-
its during this five-year period. The dictionary
structure is inspired by the WordNet basic de-
sign. The dictionary?s source knowledge base
and the software system provide interfaces to
producing an on-line dictionary, a printed-paper
dictionary, and several electronic resources use-
ful in Natural Language Processing.
1 Introduction
The dictionaries, monolingual, bilingual, or
multilingual, are the standard way of collecting
and presenting lexicographic knowledge about
one or more languages. The electronic dic-
tionaries (EDs) are not merely a straightfor-
ward extension of their printed counterparts,
but they entail additional purely computational
problems.
ED as marked-up text. An ED may be seen
simply as a long, marked-up text. The im-
portant computational issues arise around the
problem of efficient keyword search and appro-
priate presentation of the dictionary data. The
search is performed in the context of a markup
scheme, such as SGML or XML, and the query
model has to provide expressibility for search
queries within this scheme; e.g, searching for
a keyword within a certain text region. An
example of such research is the OED project
conducted from 1987 through 1994 (Tompa and
Gonnet, 1999; OED, 2004). One of the achieve-
ments of the OED project was that the search
software was able to retrieve all occurrences of
words and phrases within the dictionary corpus
of size 570 MB in less than a second (Tompa
and Gonnet, 1999).
Knowledge-base Structure of an ED.
The second aspect of EDs is the structure of
information represented in them. This struc-
ture is of interest to linguists, lexicographers,
and various dictionary users, but it is of chief
interest to computational linguists. A major
computational challenge is how to design the
dictionary structure in order to make its main-
tenance manageable and efficient. Various lex-
ical resources that were developed in the last
few decades have become invaluable in Natural
Language Processing (NLP), most notably the
WordNet. Another reason why efficiency in dic-
tionary maintenance is important is that natu-
ral languages change dynamically and good ED
should track these lexical innovations. Differ-
ent domains need to be covered, and the parts
of the dictionary that are becoming old and ar-
chaic need to be time-stamped and archived as
such.
In this paper, we present a bilingual bidirec-
tional on-line Serbo-Croatian (SC)-English dic-
tionary that has been available on the Internet
since 1999. This is the first published report de-
scribing this resource. The dictionary internal
structure is motivated by the WordNet struc-
ture, and it provides a way of producing mono-
lingual SC and bilingual SC-English wordnet.
2 Related Work
The OED project (Tompa and Gonnet, 1999;
OED, 2004) is a related project that was dis-
cussed in section 1. There are many on-line
dictionaries on the Internet: monolingual, bilin-
gual, and even multilingual. Probably the most
comprehensive list is given at the site YourDic-
tionary.com1, collected by Robert Beard from
the Bucknell University, which lists on-line dic-
tionaries for 294 languages, including two en-
tries for sign languages (ASL and Sign).
There are not that many on-line SC-English2
1http://www.YourDictionary.com
2Under language name ?Serbo-Croatian? (SC) we
assume labels Serbo-Croatian, Serbian, Croatian, or
Bosnian.
dictionaries. YourDictionary.com lists about
five such dictionaries. Most of them are narrow-
domain dictionaries. The Google directory3
lists seven dictionaries. Rjecnik.com4 is the old-
est one in these language pairs and is still active
and expanding. Tkusmic.com5 was created in
2003 and has a very similar interface. One of the
most popular dictionaries is Krstarica.com6. A
long list of dictionaries is given at Danko S?ipka?s
web site.7 Many of those are not active any
more, or they are textual dictionary files with a
limited domain.
The WordNet (Miller, 2004) project is rele-
vant to our work, since we propose a dictionary
structure based on the building blocks that fol-
low the WordNet structure. As a result, a di-
rect by-product of our ED is an SC WordNet.
The task of creating a Serbo-Croatian Word-
Net is already underway within the Balkanet
project (Christodoulakis, 2002).
3 Project Description
Project history. The on-line dictionary
R{j}ecnik.com has been active since 1999. One
of its most visible characteristics, also noted by
other users, is simplicity of the user interface.
There is one search textual field in which the
user enters the query and the dictionary reports
all dictionary entries matching the query on ei-
ther English or SC side. It provides an efficient
search mechanism, returning the results within
a second.
Lexical resources. As a lexicographic re-
source, this is a wide-coverage, up-to-date, bidi-
rectional, and bilingual dictionary covering not
only general, often used terms, but also over
8,000 computer and Internet terms,8 as well as
healthcare and medical vocabulary, including
useful abbreviations. The entries are grouped
by semantic meaning and part of speech, in
the WordNet fashion. The English lexemes are
associated with their phonetic representations,
and the entries are marked by domain of usage
(e.g., computers, business, finance, medicine).
Colloquial and informal expressions are marked
3http://directory.google.com
4http://rjecnik.com and http://recnik.com
5http://www.tkuzmic.com/dictionary/
6http://www.krstarica.com/recnik/
7http://www.public.asu.edu/?dsipka/rjeynici.html
8A number of the terms was collected
through public discussion at the e-mail list Ser-
bian Terminology maintained by Danko Sipka
(http://main.amu.edu.pl/mailman/listinfo/st-l).
with special symbols so that they can be eas-
ily identified. In addition, the dictionary con-
tains plenty of illustrative examples showing the
language in use. A suitable text encoding for
SC is used so that the software generates both
Latin (Roman) and Cyrillic script versions. Di-
alectical and geographical differences are also
marked.
Software overview. The dictionary software
is developed in the Perl programming language.
From the source dictionary file, the searchable
on-line resource file is generated. It is in tex-
tual format and it is indexed through an in-
verted file index for searchable terms in English
and SC. The searchable terms are chosen selec-
tively. The tags and descriptions are not search-
able since this would produce spurious search
results.
Dictionary structure. Following the ideas
from OED (Tompa and Gonnet, 1999), we
adopted the philosophy of modern text markup
systems that ?a computer-processable version
of text is well-represented by interleaving ?tags?
with the text of the original document, still leav-
ing the original words in proper sequence.? Ad-
ditionally, we adopted the ideas from the Word-
Net project (Miller, 2004) in structuring our
knowledge base around the basic entry unit be-
ing a meaning; i.e., one meaning = one en-
try. One source dictionary entry (vs. a printed,
or on-line dictionary entry) corresponds to one
synset in WordNet. It is represented in one
physical line in a textual file, or it may be stored
in several lines which are continued by having a
backslash (\) character at the end of each line
but the last one. An entry starts with the En-
glish lexemes separated by commas followed by
an equal sign (=), and the corresponding SC lex-
emes, also separated by commas. Additional
pertinent information is encoded using tags.
This representation is conceptually simple and
efficient in terms of manual maintenance and
memory use. It is also flexible, since it allows
tags to define features that refer to the whole
entry or just individual lexemes. Such rep-
resentation deviates from the commonly used
XML notation because we find the XML nota-
tion to be more ?machine-friendly? than user-
friendly, but it can be automatically converted
to XML. To illustrate the difference between
TEI (Sperberg-McQueen and Burnard, 2003),
the standard XML-based markup scheme, and
our markup scheme, we adopt an example from
(Erjavec, 1999), which is shown in in Fig. 1.
(A)
<entry key="bewilder">
<form> <orth type=?hw?>bewilder</orth>
<pron>bI"wIld@(r)</pron> </form>
<gramgrp><pos>vtr</pos></gramgrp>
<sense orig=?sem?>
<trans><tr>zbuniti</tr>, <tr>zaplesti</tr>,
<tr>zavesti</tr>, <tr>posramiti</tr>,
<tr>pobrkati</tr></trans>
<eg><quote>too much choice can bewilder a
small child</quote>
<trans><tr>prevelik izbor mo"ze zbuniti
malo d{ij}ete</tr></trans>
</eg>
</entry>
(B)
abash [\eb?ae"s], bewilder [biw?ild\er], \
confound [kanf?aund], confuse [k\enfj?u:z]\
= :v zbuniti, zaplesti, zavesti, posramiti,\
:coll pobrkati :/coll, :eg too much choice\
can bewilder a small child = prevelik izbor\
mo"ze zbuniti malo d{ij}ete
Figure 1: Comparative example with TEI
The entry (A) in Fig. 1 shows an entry with
TEI markup, in (B) we give our corresponding
entry. The tags are preceded with a colon (:).
English lexemes are associated with their pho-
netic representations within the square brack-
ets. The phonetic representation is encoded us-
ing the vfon encoding.9 All changes to the dic-
tionary can be easily tracked down using the key
:id tag and the standard CVS (Control Version
System) system. The encoding ipp is used to en-
code SC text fragments, since they include ad-
ditional letters beside the standard 7-bit ASCII
set. The on-line version of the dictionary is en-
coded using the dual1 encoding for simplicity
and efficiency reasons. The input query can be
entered using the ipp encoding, and is trans-
lated into the dual1 encoding before matching.
The krascii encoding10 is additionally accepted
in the input query as the most common tran-
scribing scheme, although it inherently leads to
some incorrect matches.
A very systematic variation in SC is
ekavian vs. ijekavian dialect; for example:
mleko/mlijeko (milk) and primeri/primjeri (ex-
amples), but also hteo/htio (wanted). The
text is converted via the following regular ex-
9The details about different encodings such as ipp,
vfon, and dual1 are provided in (Kes?elj and others, 2004).
10Krascii is a simple transcribing scheme that ignores
diacritics.
POS tags: noun (n), verb (v), adjective (a), ad-
verb (adv), article (art), preposition (prep), conjunction
(conj), interjection (interj), pronoun (pron), numeral
(num), noun phrase (np), verb phrase (vp), symbol or
special character (sym), and idiom (idiom).
Morpho-syntactic features: diminutive (dim), femi-
nine (fm), imperfective (ipf), intransitive (itv), mascu-
line (m), neuter (nt), past participle (pp), perfective (pf),
plural (pl), preterite or past tense (pret), singular (sl),
and transitive (tv).
Dialect tags: American (am), Bosnian (bos), British
(br), Croatian (hr), Serbian (sr), and Old Slavic (ssl).
Domain tags: agriculture (agr), archaeological (archl),
architecture (archt), biology (bio), botany (bot), com-
puter (c), diplomacy (dipl), electrical (elect), chemistry
(chem), culinary (cul), law (law), linguistic (ling), math-
ematics (mat), medicine (med), military (mil), mythol-
ogy (myt), music (mus), religion (rel), sports (sp), and
zoology (zoo).
Computer science subareas, cob tag (e.g., cob pl):
internet (int), programing languages (pl), computational
linguistics (cl), graph theory (gt), cryptography (crypt),
data structures (ds), formal languages (fl), computer net-
works (cn), information retrieval (ir), and object oriented
programming (oop).
Misc.: abbreviation (abb), abbreviation expansion
(abbE), colloquial (coll), description (desc), example
(eg), obsolete (obs), see (see), unique entry identifier
(id), and vulgar (vul).
Figure 2: List of tags
Avg.visits Avg.time Len. of the
Year per day b/w visits longest query
1999 106 13m 34s 953
2000 249 5m 47s 710
2001 402 3m 34s 1556
2002 662 2m 10s 2492
2003 1018 1m 25s 4958
2004 2158 40s 1249
Figure 3: Site visit statistics
pression substitutions for ekavian and ijeka-
vian: s/\{(([^\|\}]*)\|)?([^\}]*)\}/$2/g
and s/\{(([^\|\}]*)\|)?([^\}]*)\}/$3/g.
The list of tags used in the dictionary is given
in Fig. 2.
4 Dictionary and Usage Statistics
The dictionary has been on-line for five years
(since 22-Jul-99). As of 28-Apr-2004, it has
60,338 lexemes, organized in 20,911 entries. The
average system response time is 0.4 sec. Some
site statistics are given in Fig. 3. The inter-
face is supposed to be used only for short-word
queries, but long queries are also submitted in
hope that the system would do machine transla-
tion. As can be seen from the figure, the longest
submitted query had the length of 4958 bytes.
Still, the majority of the queries are below 100
bytes: in 1999 there were 0.03% queries sub-
1999 2000 2001 2002 2003 2004
95 love 522 love 854 love 1252 hello 1977 hello 607 hello
95 hello 499 hello 756 hello 1205 love 1776 love 590 love
70 you 346 you 521 you 892 you 1287 you 416 you
57 devojka 215 good 324 good 487 i 707 good 259 i
38 i 170 f. . . (en) 278 i 453 good 705 i 216 good
34 k. . . (sc) 158 i 264 devojka 341 f. . . (en) 578 thank you 204 da
34 djevojka 154 I 254 f. . . (en) 335 thank you 573 f. . . (en) 191 se
30 djak 148 devojka 252 thank you 333 happy 551 beautiful 191 thank you
30 f. . . (en) 144 are 243 happy 330 beautiful 499 are 189 beautiful
28 word 141 thank you 218 I 319 I 486 i love you 185 volim
Figure 5: The most commonly asked queries (f. . . (en) and k. . . (sc) denote obscene words
 0
 0.1
 0.2
 0  5  6  7  10  15  20  25  30
Dis
trib
utio
n in
 a y
ear
Query length
199920002001200220032004
Figure 4: Distribution of query lengths
mited longer than 100 bytes, 0.05% in 2000 and
2001, 0.14% in 2002, 0.27% in 2003, and 0.12%
in 2004. The distribution of query lengths less
than 30 bytes is given in Fig. 4. The most com-
monly asked queries are given in Fig. 5.
5 Conclusions and Future Work
We have presented the features of an electronic
English-SC dictionary. The dictionary is de-
signed to be multi-functional, providing the in-
terfaces to produce a printed dictionary copy
and an on-line searchable lexicon. We propose
a dictionary structure inspired by the WordNet,
which is flexible and easy to maintain. We also
report the site statistics of the on-line dictionary
during the last five years.
Future work. The plan for future work in-
cludes incorporating a lemmatizer that would
translate inflected word forms into their canoni-
cal representations. This is relevant for English,
but it is a more important issue in SC, which is a
highly-inflectional language. We do not know of
any lemmatizer or stemmer currently available
for SC. The software interfaces for producing a
wordnet form, and a TEI-encoded form will be
developed. An issue of long queries needs to be
addressed. Currently, if a user submits a long
query, which is usually a sentence or paragraph,
the dictionary reports ?zero entries found.? A
fall-back strategy should be provided, which will
consist of tokenizing the input and giving the
results on querying separate lexemes.
6 Acknowledgments
We thank Danko S?ipka, Dus?ko Vitas, and
anonymous reviewers for helpful feedback. The
first author is supported by the NSERC.
References
D. Christodoulakis. 2002. Balkanet: Design
and development of a multilingual Balkan
WordNet. WWW.
T. Erjavec. 1999. Encoding and presenting an
English-Slovene dictionary and corpus. In 4th
TELRI Seminar, Bratislava.
V. Kes?elj et al 2004. Report on R{j}ecnik.com:
An English ? Serbo-Croatian electronic dic-
tionary. Technical Report CS-2004-XX, Dal-
housie University. Forthcoming.
G.A. Miller. 2004. WordNet home page.
http://www.cogsci.princeton .edu/?wn/.
OED. 2004. Oxford English Dictionary.
WWW. http://www.oed.com/, Apr. 2004.
C.M. Sperberg-McQueen and L. Burnard. 2003.
Text encoding initiative. http://www.tei-
c.org/P4X/index.html, accessed in May 2004.
F. Tompa and G. Gonnet. 1999. UW cen-
tre for the new OED and text research.
http://db.uwaterloo.ca/OED/.
D. Vitas, C. Krstev, I. Obradovic?, Lj. Popovic?,
and G. Pavlovic?-Laz?etic?. 2003. An overview
of resources and basic tools for the process-
ing of Serbian written texts. In D. Piperidis,
editor, First workshop on Balkan Languages
and Resources, pages 1?8.
D. S?ipka. 1998. Osnovi Leksikologije i Srodnih
Disciplina. Matica srpska.
BioNLP 2007: Biological, translational, and clinical language processing, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Unsupervised Method for Extracting Domain-specific Affixes in
Biological Literature
Haibin Liu Christian Blouin Vlado Kes?elj
Faculty of Computer Science, Dalhousie University, Canada, {haibin,cblouin,vlado}@cs.dal.ca
Abstract
We propose an unsupervised method to au-
tomatically extract domain-specific prefixes
and suffixes from biological corpora based
on the use of PATRICIA tree. The method is
evaluated by integrating the extracted affixes
into an existing learning-based biological
term annotation system. The system based
on our method achieves comparable experi-
mental results to the original system in locat-
ing biological terms and exact term match-
ing annotation. However, our method im-
proves the system efficiency by significantly
reducing the feature set size. Additionally,
the method achieves a better performance
with a small training data set. Since the af-
fix extraction process is unsupervised, it is
assumed that the method can be generalized
to extract domain-specific affixes from other
domains, thus assisting in domain-specific
concept recognition.
1 Introduction
Biological term annotation is a preparatory step in
information retrieval in biological science. A bi-
ological term is generally defined as any technical
term related to the biological domain. Consider-
ing term structure, there are two types of biologi-
cal terms: single word terms and multi-word terms.
Many systems (Fukuda et al, 1998; Franzn et al,
2002) have been proposed to annotate biological
terms based on different methodologies in which de-
termining term boundaries is usually the first task. It
has been demonstrated (Jiampojamarn et al, 2005a),
however, that accurately locating term boundaries
is difficult. This is so because of the ambiguity of
terms, and the peculiarity of the language used in
biological literature.
(Jiampojamarn et al, 2005b) proposed an auto-
matic biological term annotation system (ABTA)
which applies supervised learning methods to an-
notate biological terms in the biological litera-
ture. Given unstructured texts in biological research,
the annotation system first locates biological terms
based on five word position classes, ?Start?, ?Mid-
dle?, ?End?, ?Single? and ?Non-relevant?. There-
fore, multi-word biological terms should be in a con-
sistent sequence of classes ?Start (Middle)* End?
while single word terms will be indicated by the
class ?Single?. Word n-grams (Cavnar and Tren-
kle, 1994) are used to define each input sentence
into classification instances. For each element in
an n-gram, the system extracts feature attributes as
input for creating the classification model. The ex-
tracted feature attributes include word feature pat-
terns(e.g., Greek letters, uppercase letters, digits and
other symbols), part-of-speech (POS) tag informa-
tion, prefix and suffix characters. Without using
other specific domain resources, the system achieves
comparable results to some other state-of-the-art
systems (Finkel et al, 2004; Settles, 2004) which
resort to external knowledge, such as protein dictio-
naries. It has been demonstrated (Jiampojamarn et
al., 2005b) that the part-of-speech tag information
is the most effective attribute in aiding the system
to annotate biological terms because most biologi-
cal terms are partial noun phrases.
The ABTA system learns the affix feature by
recording only the first and the last n characters (e.g.,
n = 3) of each word in classification instances, and
the authors claimed that the n characters could pro-
vide enough affix information for the term annota-
tion task. Instead of using a certain number of char-
acters to provide affix information, however, it is
more likely that a specific list of typically used pre-
fixes and suffixes of biological words would provide
more accurate information to classifying some bio-
logical terms and boundaries. We hypothesize that
33
a more flexible affix definition will improve the per-
formance of the taks of biological term annotation.
Inspired by (Jiampojamarn et al, 2005b), we
propose a method to automatically extract domain-
specific prefixes and suffixes from biological cor-
pora. We evaluate the effectiveness of the extracted
affixes by integrating them into the parametrization
of an existing biological term annotation system,
ABTA (Jiampojamarn et al, 2005b), to evaluate the
impact on performance of term annotation. The pro-
posed method is completely unsupervised. For this
reason, we suggest that our method can be gener-
alized for extracting domain-specific affixes from
many domains.
The rest of the paper is organized as follows: In
section 2, we review recent research advances in bi-
ological term annotation. Section 3 describes the
methodology proposed for affix extraction in detail.
The experiment results are presented and evaluated
in section 4. Finally, section 5 summarizes the paper
and introduces future work.
2 Related Work
Biological term annotation denotes a set of proce-
dures that are used to systematically recognize per-
tinent terms in biological literature, that is, to differ-
entiate between biological terms and non-biological
terms and to highlight lexical units that are related to
relevant biology concepts (Nenadic and Ananiadou,
2006).
Recognizing biological entities from texts allows
for text mining to capture their underlying meaning
and further extraction of semantic relationships and
other useful information. Because of the importance
and complexity of the problem, biological term an-
notation has attracted intensive research and there is
a large number of published work on this topic (Co-
hen and Hersh, 2005; Franzn et al, 2003).
Current approaches in biological term annota-
tion can be generalized into three main categories:
lexicon-based, rule-based and learning-based (Co-
hen and Hersh, 2005). Lexicon-based approaches
use existing terminological resources, such as dic-
tionaries or databases, in order to locate term oc-
currences in texts. Given the pace of biology re-
search, however, it is not realistic to assume that a
dictionary can be maintained up-to-date. A draw-
back of lexicon-based approaches is thus that they
are not able to annotate recently coined biological
terms. Rule-based approaches attempt to recover
terms by developing rules that describe associated
term formation patterns. However, rules are often
time-consuming to develop while specific rules are
difficult to adjust to other types of terms. Thus, rule-
based approaches are considered to lack scalability
and generalization.
Systems developed based on learning-based ap-
proaches use training data to learn features useful for
biological term annotation. Compared to the other
two methods, learning-based approaches are theo-
retically more capable to identify unseen or multi-
word terms, and even terms with various writing
styles by different authors. However, a main chal-
lenge for learning-based approaches is to select a set
of discriminating feature attributes that can be used
for accurate annotation of biological terms. The fea-
tures generally fall into four classes: (1) simple de-
terministic features which capture use of uppercase
letters and digits, and other formation patterns of
words, (2) morphological features such as prefix and
suffix, (3) part-of-speech features that provide word
syntactic information, and (4) semantic trigger fea-
tures which capture the evidence by collecting the
semantic information of key words, for instances,
head nouns or special verbs.
As introduced earlier, the learning-based biologi-
cal term annotation system ABTA obtained an 0.705
F-score in exact term matching on Genia corpus
(v3.02)1 which contains 2,000 abstracts of biolog-
ical literature. In fact, the morphological features
in ABTA are learned by recording only the first and
the last n characters of each word in classification
instances. This potentially leads to inaccurate affix
information for the term annotation task.
(Shen et al, 2003) explored an adaptation of a
general Hidden Markov Model-based term recog-
nizer to biological domain. They experimented with
POS tags, prefix and suffix information and noun
heads as features and reported an 0.661 F-score in
overall term annotation on Genia corpus. 100 most
frequent prefixes and suffixes are extracted as can-
didates, and evaluated based on difference in likeli-
hood of part of a biological term versus not. Their
method results in a modest positive improvement in
recognizing biological terms. Two limitations of this
method are: (1) use of only a biological corpus, so
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
34
that the general domain-independent affixes are not
removed, and (2) a supervised process of choosing a
score threshold that is used in affix selection.
(Lee et al, 2003) used prefix and suffix fea-
tures coupled with a dictionary-based refinement of
boundaries of the selected candidates in their exper-
iments for term annotation. They extracted affix fea-
tures in a similar way with (Shen et al, 2003). They
also reported that affix features made a positive ef-
fect on improving term annotation accuracy.
In this project, we consider the quality of domain-
specific affix features extracted via an unsupervised
method. Successful demonstration of the quality of
this extraction method implies that domain-specific
affixes can be identified for arbitrary corpora without
the need to manually generate training sets.
3 PATRICIA-Tree-based Affix Extraction
3.1 PATRICIA Tree
The method we propose to extract affixes from bio-
logical words is based on the use of PATRICIA tree.
?PATRICIA? stands for ?Practical Algorithm To Re-
trieve Information Coded In Alphanumeric?. It was
first proposed by (Morrison, 1968) as an algorithm
to provide a flexible means of storing, indexing, and
retrieving information in a large file. PATRICIA
tree uses path compression by grouping common se-
quences into nodes. This structure provides an ef-
ficient way of storing values while maintaining the
lookup time for a key of O(N) in the worst case,
where N is the length of the longest key. Meanwhile,
PATRICIA tree has little restriction on the format of
text and keys. Also it does not require rearrange-
ment of text or index when new material is added.
Because of its outstanding flexibility and efficiency,
PATRICIA tree has been applied to many large in-
formation retrieval problems (Morrison, 1968).
In our project, all biological words are inserted
and stored in a PATRICIA tree, using which we can
efficiently look up specific biological word or extract
biological words that share specified affixes and cal-
culated required statistics.
3.2 Experiment Design
In this work, we have designed the experiments to
extract domain-specific prefixes and suffixes of bio-
logical words from a biological corpus, and investi-
gate whether the extracted affix information could
facilitate better biological term annotation. The
overall design of our experiments consists of three
major processes: affix extraction, affix refining and
evaluation of experimental results. It is seen that
every node in PATRICIA tree contains exactly one
string of 1 or more characters, which is the preced-
ing substring of its descendant nodes. Meanwhile,
every word is a path of substrings from the root node
to a leaf. Therefore, we propose that every substring
that can be formed from traversing the internal nodes
of the tree is a potential affix.
In the affix extraction process, we first populate a
PATRICIA tree using all words in the combined cor-
pus(CC) of a Biological Corpus (BC) and a General
English Corpus (GEC). GEC is used against BC in
order to extract more accurate biological affix infor-
mation. Two PATRICIA trees are populated sepa-
rately for extracting prefixes and suffixes. The suffix
tree is based on strings derived by reversing all the
input words from the combined corpus. All the po-
tential prefixes and suffixes are then extracted from
the populated PATRICIA trees.
In the affix refining process, for each extracted
potential affix, we compute its joint probability of
being both an English affix and a biological affix,
P (D = Biology, A = Yes|PA), where D stands
for Domain, A stands for Affix and PA represents
Potential Affix. This joint probability can be fur-
ther decomposed as shown in Eq.(1). In the for-
mula, P (A = Yes|PA) denotes the probability that
a given potential affix is a true English affix while
P (D = Biology|A = Yes,PA) refers to the proba-
bility that a given English affix is actually a biologi-
cal affix.
P (D = Biology, A = Yes|PA) =
P (D=Biology|A=Yes,PA)? P (A=Yes|PA) (1)
To calculate P (A = Yes|PA), the probabilities of
prefixes and suffixes are measured separately. In
linguistics, a prefix is described as a type of affix
that precedes the morphemes to which it can attach
(Soanes and Stevenson, 2004). Simply speaking, a
prefix is a substring that can be found at the begin-
ning of a word. Our functional definition of a prefix
is a substring which precedes words existing in the
English language. This can be done by enumerating,
for each node, all descendant substring and assess-
ing their existence as stand-alone words. For exam-
ple, ?radioimmunoassay?, ?radioiodine? and ?radio-
35
labeled? are three words and have a common start-
ing string ?radio?. If we take out the remaining part
of each word, three new strings are obtained, ?im-
munoassay?, ?iodine? and ?labeled?. Since all the
input words are already stored in PATRICIA tree,
we lookup these three strings in PATRICIA tree and
find that ?immunoassay?, ?iodine? and ?labeled? are
also meaningful words in the tree. This indicates
that ?radio? is a prefix among the input words. On
the other hand, it is obvious that ?radioimmunoas-
say? and ?radioiodine? share another string ?radioi?.
However, ?mmunoassay? and ?odine? are not mean-
ingful words due to their absence in the PATRICIA
tree. This suggests that ?radioi? is not a prefix.
For each extracted potential prefix,
P (A = Yes|PA) is computed as the proportion of
strings formed by traversing all descendant nodes
that are meaningful terms. In our experiments,
the measure of determining a string meaningful
is to look up whether the string is an existing
word present in the built prefix PATRICIA tree.
Algorithm 1 shows the procedure of populating a
PATRICIA tree and calculating P (A = Yes|PA)
for each potential prefix.
Algorithm 1 P (A = Yes|PA) for Prefix
Input: words (w) ? Combined Corpus (CC)
Output: P (A = Yes|PA) for each potential prefix
PT = ? //PT : Patricia Trie
for all words w ? CC do
PT ? Insert(w) //Populating Patricia Trie
for all nodes ni ? PT do
PA? String(ni) //Concatenate strings
// in nodes from root to ni,
// which is a potential prefix
TPA ? PrefixSearch(PA)
//TPA : all words w ? CC beginning with PA
score ? 0
for all words w ? TPA do
if Extrstr(PA,w) in PT then
//Extrstr() returns the remaining string
// of w without PA
score ++
P (A = Yes|PA) ? score/|TPA|
//|TPA| is the number of words in TPA
Likewise, in linguistics a suffix is an affix that
follows the morphemes to which it can attach
(Soanes and Stevenson, 2004). Simply speaking,
a suffix of a word is a substring exactly match-
ing the last part of the word. Similar to the idea
of calculating P (A = Yes|PA) for potential pre-
fix, we conjecture that the extracted potential suf-
fix could be a reasonable English suffix if the in-
verted strings formed from traversing the descen-
dant nodes of the potential suffix in the suffix PA-
TRICIA tree are meaningful words. For instance,
?Calcium-dependent?, ?Erythropoietin-dependent?
and ?Ligand-dependent? share a common ending
string ?-dependent?. Since the remaining strings of
each word, ?Calcium?, ?Erythropoietin? and ?Lig-
and? can be found in the ?forward? PATRICIA tree,
?-dependent? is a potentially useful suffix.
However, it is often observable that some English
words do not begin with another meaningful word
but a typical prefix, for example, ?alpha-bound? and
?pro-glutathione?. It is known that ?-bound?and
?-glutathione? are good suffixes in biology. ?al-
pha? and ?pro?, however, are not meaningful words
but typical prefixes, and in fact have been extracted
when calculating P (A = Yes|PA) for potential pre-
fix. Therefore, in order to detect and capture such
potential suffixes, we further assume that if a word
begins with a recognized prefix instead of another
meaningful word, the remaining part of the word
still has the potential to be an informative suffix.
Therefore, strings ?-bound? and ?-glutathione? can
be successfully extracted as potential suffixes. In our
experiments, an extracted potential prefix is consid-
ered a recognized prefix if its P (A = Yes|PA) is
greater than 0.5.
To calculate P (D = Biology|A = Yes, PA), it
is necessary to first determine true English affixes
from extracted potential affixes. In our experiments,
we consider that an extracted potential prefix or suf-
fix is a recognized affix only if its P (A = Yes|PA)
is greater than 0.5. It is also necessary to consider
the biological corpus BC and the general English
corpus GEC separately. It is assumed that a biol-
ogy related affix tends to occur more frequently in
words of BC than GEC. Eq.(2) is used to estimate
P (D = Biology|A = Yes, PA).
P (D = Biology|A = Yes, PA) =
(#Words with PA in BC/Size (BC))/
(#Words with PA in BC/Size (BC) +
#Words with PA in GEC/Size (GEC)), (2)
36
where only PA with P (A = Yes|PA) greater than
0.5 are used, and the number of words with a certain
PA is further normalized by the size of each corpus.
Finally, the joint probability of each potential af-
fix, P (D = Biology, A = Yes|PA), can be used to
parametrize a word beginning or ending with PA.
In the evaluation process of our experiments, the
prefix-suffix pair with maximum joint probability
values is used to parametrize a word. Therefore,
each word in BC has exactly two values as affix fea-
ture: a joint probability value for its potential prefix
and a joint probability value for its potential suffix.
We then replace the original affix feature of ABTA
system with our obtained joint probability values,
and investigate whether these new affix information
leads to equivalent or better term annotation on BC.
4 Results and Evaluation
4.1 Dataset and Environment
For our experiments, it is necessary to use a corpus
that includes widely used biological terms and com-
mon English words. This dataset, therefore, will al-
low us to accurately extract the information of bi-
ology related affixes. As a proof-of-concept proto-
type, our experiments are conducted on two widely
used corpora: Genia corpus (v3.02) and Brown cor-
pus2.The Genia version 3.02 corpus is used as the
biological corpus BC in our experiments. It contains
2,000 biological research paper abstracts. They were
selected from the search results in the MEDLINE
database3, and each biological term has been an-
notated into different terminal classes based on the
opinions of experts in biology. Used as the general
English corpus GEC, Brown corpus includes 500
samples of common English words, totalling about
a million words drawn from 15 different text cate-
gories.
All the experiments were executed on a Sun So-
laris server Sun-Fire-880. Our experiments were
mainly implemented using Perl and Python.
4.2 Experimental Results
We extracted 15,718 potential prefixes and 21,282
potential suffixes from the combined corpus of Ge-
nia and Brown. Among them, there are 2,306 poten-
tial prefixes and 1,913 potential suffixes with joint
2http://clwww.essex.ac.uk/w3c/corpus ling/
3http://www.ncbi.nlm.nih.gov/PubMed/
probability value P (D = Biology, A = Yes|PA)
greater than 0.5. Table 1 shows a few examples
of extracted potential affixes whose joint probabil-
ity value is equal to 1.0. It is seen that most of
these potential affixes are understandable biological
affixes which directly carry specific semantic mean-
ings about certain biological terms. However, some
substrings are also captured as potential affixes al-
though they may not be recognized as ?affixes? in
linguistics, for example ?adenomyo? in prefixes, and
?mopoiesis? in suffixes. In Genia corpus, ?adeno-
myo? is the common beginning substring of biologi-
cal terms ?adenomyoma?, ?adenomyosis? and ?ade-
nomyotic? , while ?plasias? is the common ending
substring of biological terms ?neoplasias? and ?hy-
perplasias?. The whole list of extracted potential af-
fixes is available upon request.
In order to investigate whether the extracted af-
fixes improves the performance of biological term
annotation, it is necessary to obtain the experimen-
tal results of both original ABTA system and the
ABTA system using our extracted affix information.
In ABTA, the extraction of feature attributes is per-
formed on the whole 2000 abstracts of Genia cor-
pus, and then 1800 abstracts are used as training
set while the rest 200 abstracts are used as testing
set. The evaluation measures are precision, recall
and F-score. C4.5 decision tree classifier (Alpay-
din, 2004) is reported as the most efficient classi-
fier which leads to the best performance among all
the classifiers experimented in (Jiampojamarn et al,
2005b). Therefore, C4.5 is used as the main clas-
sifier in our experiments. The experimental results
of ABTA system with 10 fold cross-validation based
on different combinations of the original features are
presented in Table 2 in which feature ?WFP? is short
for Word Feature Patterns, feature ?AC? denotes Af-
fix Characters, and feature ?POS? refers to POS tag
information. The setting of parameters in the exper-
iments with ABTA is: the word n-gram size is 3, the
number of word feature patterns is 3, and the number
of affix characters is 4. We have reported the F-score
and the classification accuracy of the experiments in
the table. It is seen that there is a tendency with the
experimental performance that for a multi-word bi-
ological term, the middle position is most difficult
to detect while the ending position is generally eas-
ier to be identified than the starting position. The
assumed reason for this tendency is that for multi-
37
Potential Prefixes Potential Suffixes
13-acetate
B-cell
endotoxin
I-kappaB
macrophage
adenomyo
Rel/NF-kappaB
anti-CD28
VitD3
cytokine
3-kinase
CD28
HSV-1
ligand
N-alpha-tosyl-L
platelet
pharmaco
adenovirus
chromatin
hemoglobin
-T-cell
-coated
-expressed
-inducer
plasias
-alpha-activated
mopoiesis
-nonresponsive
coagulant
-soluble
cytoid
-bearing
-kappaB-mediated
-globin-encoding
-immortalized
-methyl
lyse
-receptor
glycemia
racrine
Table 1: Examples of Extracted Potential Affixes with Joint Probability Value 1.0
word biological terms, many middle words of are
seemingly unrelated to biology domain while many
ending words directly indicate their identity, for in-
stances, ?receptor?, ?virus? or ?expression?.
Table 3 shows the experimental results of ABTA
system after replacing the original affix feature with
our obtained joint probability values for each word
in Genia corpus. ?JPV? is used to denote Joint Prob-
ability Values. It is seen that based on all three
features the system achieves a classification accu-
racy of 87.5%, which is comparable to the results
of the original ABTA system. However, the size of
the feature set of the system is significantly reduced,
and the classification accuracy of 87.5% is achieved
based on only 18 parameters, which is 1/2 of the size
of the original feature set. Meanwhle, the execution
time of the experiments generally reduces to nearly
half of the original ABTA system (e.g., reduces from
4 hours to 1.7 hours). Furthermore, when the feature
set contains only our extracted affix information, the
system reaches a classification accuracy of 81.46%
based on only 6 parameters. It is comparable with
the classification accuracy achieved by using only
POS information in the system. In addition, Table 3
also presents the experimental results when our ex-
tracted affix information is used as an addtional fea-
ture to the original feature set. It is expected that the
system performance is further improved when the
four features are applied together. However, the size
of the feature set increases to 42 parameters, which
increases the data redundancy. This proves that the
extracted affix information has a positive impact on
locating biological terms, and it could be a good re-
placement of the original affix feature.
Moreover, we also evaluated the performance of
the exact matching biological term annotation based
on the obtained experimental results of ABTA sys-
tem. The exact matching annotation in ABTA sys-
tem is to accurately identify every biological term,
including both multi-word terms and single word
terms, therefore, all the word position classes of
a term have to be classified correctly at the same
time. An error occurring in any one of ?Start? ?Mid-
dle? and ?End? classes leads the system to annotate
multi-word terms incorrectly. Consequently, the ac-
cumulated errors will influence the exact matching
annotation performance. Table 4 presents the exact
matching annotation results of different combination
of features based on 10 fold cross-validation over
Genia corpus. It is seen that after replacing the orig-
inal affix feature of ABTA system with our obtained
joint probability values for each word in Genia cor-
pus, the system achieves an 0.664 F-score on exact
matching of biological term annotation, compara-
ble to the exact matching performance of the orig-
inal ABTA system. In addition, when the feature
set contains only our extracted affix information, the
system reaches an 0.536 F-score on exact matching.
Although it is a little lower than the exact matching
performance achieved by using only the original af-
fix features in the system, the feature set size of the
system is significantly reduced from 24 to 6.
In order to further compare our method with the
original ABTA system, we attempted eleven differ-
ent sizes of training data set to run the experiments
separately based on our method and the original
ABTA system. They can then be evaluated in terms
of their performance on each training set size. These
eleven different training set sizes are: 0.25%, 0.5%,
1%, 2.5%, 5%, 7.5%, 10%, 25%, 50%, 75% and
90%. For instance, 0.25% denotes that the train-
ing data set is 0.25% of Genia corpus while the
rest 99.75% becomes the testing data set for exper-
iments. It is observed that there are about 21 paper
abstracts in training set when its size is 1% , and 52
abstracts when its size is 2.5%. It is expected that
larger training set size leads to better classification
accuracy of experiments.
For each training set size, we randomly extracted
10 different training sets from Genia corpus to run
the experiments. We then computed the mean clas-
sification accuracy (MCA) of 10 obtained classifi-
cation accuracies. Figure 1 was drawn to illustrate
the distribution of MCA of each training set size
38
Feature F-Measure Classification #
sets Start Middle End Single Non Accuracy (%) Parameters
WFP 0.467 0.279 0.495 0.491 0.864 74.59 9
AC 0.709 0.663 0.758 0.719 0.932 85.67 24
POS 0.69 0.702 0.775 0.67 0.908 83.96 3
WFP+AC 0.717 0.674 0.762 0.730 0.933 86.02 33
WFP+POS 0.726 0.721 0.793 0.716 0.923 85.96 12
AC+POS 0.755 0.741 0.809 0.732 0.930 87.14 27
WFP+AC+POS 0.764 0.745 0.811 0.749 0.933 87.59 36
Table 2: Experimental Results of Original ABTA System
Feature F-Measure Classification #
sets Start Middle End Single Non Accuracy (%) Parameters
JPV 0.652 0.605 0.713 0.602 0.898 81.46 6
WFP+JPV 0.708 0.680 0.756 0.699 0.919 84.84 15
JPV+POS 0.753 0.740 0.805 0.722 0.928 86.92 9
WFP+JPV+POS 0.758 0.749 0.809 0.74 0.933 87.50 18
WFP+AC+POS+JPV 0.767 0.746 0.816 0.751 0.934 87.77 42
Table 3: Experimental Results of ABTA System with Extracted Affix Information
for both methods, with the incremental proportion of
training data. It is noted in Figure 1 that the change
patterns of MCA obtained by our method and the
original ABTA system are similar. It is also seen
that our method achieves marginally better classifi-
cation performance when the proportion of training
data is under 2.5%.
Figure 1: MCA Distribution
In order to determine if the classification perfor-
mance difference between our method and the origi-
nal ABTA system is statistically significant, we per-
formed one-tailed t-Test (Alpaydin, 2004) on the
classification results with our hypothesis that MCA
of our proposed method is higher than MCA of orig-
inal ABTA system. The significance level ? is set
to be the conventional value 0.05. As a result, the
classification performance difference between two
methods is statistically significant when the propor-
tion of training data is 0.25%, 0.5%, 1% or 2.5%.
Table 5 shows the P values of t-Test results for the
various training set sizes. This demonstrates that
the ABTA system adopting our method outperforms
the original ABTA system in classification accuracy
when the proportion of training data is lower than
2.5% of Genia corpus, and achieves comparable
classification performance with the original ABTA
system when the proportion continuously increases.
One-tailed Training set size
t-Test 0.25% 0.5% 1% 2.5%
P value 0.0298 0.0006 0.0002 0.0229
Table 5: One-tailed t-Test Results
5 Conclusions
In this paper, we have presented an unsupervised
method to extract domain-specific prefixes and suf-
fixes from the biological corpus based on the use
of PATRICIA tree. The ABTA system (Jiampoja-
marn et al, 2005b) adopting our method achieves
an overall classification accuracy of 87.5% in locat-
ing biological terms, and derives an 0.664 F-score in
exact term matching annotation, which are all com-
parable to the experimental results obtained by the
original ABTA system. However, our method helps
the system significantly reduce the size of feature set
and thus improves the system efficiency. The sys-
tem also obtains a classification accuracy of 81.46%
based only on our extracted affix information. This
39
Feature Exact Matching Annotation #
sets Precision Recall F-score Parameters
AC 0.548 0.571 0.559 24
WFP+AC+POS 0.661 0.673 0.667 36
JPV 0.527 0.545 0.536 6
WFP+JPV+POS 0.658 0.669 0.664 18
Table 4: Exact Matching Annotation Performance
demonstates that the affix information acheived by
the proposed method is important to accurately lo-
cating biological terms.
We further explored the reliability of our method
by gradually increasing the proportion of training
data from 0.25% to 90% of Genia corpus. One-tailed
t-Test results confirm that the ABTA system adopt-
ing our method achieves more reliable performance
than the original ABTA system when the training
corpus is small. The main result of this work is thus
that affix features can be parametrized from small
corpora at no cost in performance.
There are some aspects in which the proposed
method can be improved in our future work. We
are interested in investigating whether there exists
a certain threshold value for the joint probability
which might improve the classification accuracy of
ABTA system to some extent. However, this could
import supervised elements into our method. More-
over, we would like to incorporate our method into
other published learning-based biological term an-
notation systems to see if better system performance
will be achieved. However, superior parametriza-
tion will improve the annotation performance only
if the affix information is not redundant with other
features such as POS.
References
Ethem Alpaydin. 2004. Introduction to Machine Learning.
MIT Press.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based
text categorization. In Proc. SDAIR-94, 3rd Ann. Symposium
on Doc. Analysis and Inf. Retr., pages 161?175, Las Vegas,
USA.
Aaron Michael Cohen and William R. Hersh. 2005. A sur-
vey of current work in biomedical text mining. Briefings in
Bioinformatics, 5(1):57?71.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nissim,
Gail Sinclair, and Christopher Manning. 2004. Exploiting
context for biomedical entity recognition: From syntax to
the web. In Joint wsh. on NLP in Biomedicine and its Appli-
cations (JNLPBA-2004).
Kristofer Franzn, Gunnar Eriksson, Fredrik Olsson, Lars
Asker Per Lidn, and Joakim Cster. 2002. Protein names
and how to find them. International Journal of Medical In-
formatics special issue on NLP in Biomedical Applications,
pages 49?61.
Kristofer Franzn, Gunnar Eriksson, Fredrik Olsson, Lars
Asker Per Lidn, and Joakim Cster. 2003. Mining the Bio-
medical Literature in the Genomic Era: An Overview. J.
Comp. Biol., 10(6):821?855.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. 1998. To-
ward information extraction: Identifying protein names from
biological papers. In the Pacific Symposium on Biocomput-
ing, pages 707?718.
Sittichai Jiampojamarn, Nick Cercone, and Vlado Kes?elj.
2005a. Automatic Biological Term Annotation Using N-
gram and Classification Models. Master?s thesis, Faculty of
Comp.Sci., Dalhousie University.
Sittichai Jiampojamarn, Nick Cercone, and Vlado Kes?elj.
2005b. Biological Named Entity Recognition using N-
grams and Classification Methods. In Conf. of the Pacific
Assoc. for Computational Linguistics, PACLING?05, Tokyo,
Japan.
Ki-Joong Lee, Young-Sook Hwang, and Hae-Chang Rim.
2003. Two-phase biomedical NE recognition based on
SVMs. In Proc. of the ACL 2003 workshop on Natural lan-
guage processing in biomedicine, pages 33?40, Morristown,
NJ, USA. ACL.
Donald R. Morrison. 1968. Patricia - Practical Algorithm To
Retrieve Information Coded in Alphanumeric. Journal of
the ACM, 15(4):514?534.
Goran Nenadic and Sophia Ananiadou. 2006. Mining semanti-
cally related terms from biomedical literature. ACM Trans-
actions on Asian Language Information Processing (TALIP),
5(1):22 ? 43.
Burr Settles. 2004. Biomedical named entity recognition using
conditional random fields and novel feature sets. In Joint
wsh. on NLP in Biomedicine and its Applications (JNLPBA-
2004).
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and Chew-Lim
Tan. 2003. Effective adaptation of a Hidden Markov Model-
based named entity recognizer for biomedical domain. In
Proc. of the ACL 2003 wsh. on NLP in Biomedicine, pages
49?56, Morristown, NJ, USA.
Catherine Soanes and Angus Stevenson. 2004. Oxford Dictio-
nary of English. Oxford University Press.
40
Proceedings of the Workshop on BioNLP, pages 133?141,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Identifying Interaction Sentences from Biological Literature Using
Automatically Extracted Patterns
Haibin Liu
Faculty of Computer Science
Dalhousie University
Halifax, NS, Canada
haibin@cs.dal.ca
Christian Blouin
Faculty of Computer Science
Dalhousie University
Halifax, NS, Canada
cblouin@cs.dal.ca
Vlado Kes?elj
Faculty of Computer Science
Dalhousie University
Halifax, NS, Canada
vlado@cs.dal.ca
Abstract
An important task in information retrieval is to
identify sentences that contain important relation-
ships between key concepts. In this work, we
propose a novel approach to automatically extract
sentence patterns that contain interactions involv-
ing concepts of molecular biology. A pattern is
defined in this work as a sequence of specialized
Part-of-Speech (POS) tags that capture the struc-
ture of key sentences in the scientific literature.
Each candidate sentence for the classification task
is encoded as a POS array and then aligned to
a collection of pre-extracted patterns. The qual-
ity of the alignment is expressed as a pairwise
alignment score. The most innovative component
of this work is the use of a Genetic Algorithm
(GA) to maximize the classification performance
of the alignment scoring scheme. The system
achieves an F-score of 0.834 in identifying sen-
tences which describe interactions between bio-
logical entities. This performance is mostly af-
fected by the quality of the preprocessing steps
such as term identification and POS tagging.
1 Introduction
Recent research in information extraction (IE) in bio-
logical science has focused on extracting information
about interactions between biological entities from re-
search communications. The type of interaction of in-
terest includes protein-protein, protein-DNA, gene reg-
ulations and other interactions between macromole-
cules. This work broadens the definition of the term
?interaction? to include other types of concepts that
are semantically related to cellular components and
processes. This contrasts with the past efforts focus-
ing strictly on molecular interactions (Blaschke et al,
1999; Ono et al, 2001). We anticipate that identifying
the relationships between concepts of molecular biol-
ogy will facilitate the building of knowledge models,
improve the sensitivity of IE tasks and ultimately facil-
itate the formulation of new hypothesis by experimen-
talists.
The extraction of interactions is based on the heuris-
tic premise that interacting concepts co-occur within
a given section of text. The challenge is that co-
occurrence certainly does not guarantee that a passage
contains an interaction(Jang et al, 2006; Skusa et al,
2005). Co-occurrence is highly dependent on the de-
finition of the section of text within which the target
terms are expected to be found. A thorough compari-
son on the prediction of protein-protein interaction be-
tween abstract-level co-occurrence and sentence-level
co-occurrence was undertaken (Raychaudhuri, 2006).
It is demonstrated that abstract co-occurrence is more
sensitive but less specific for interactions. At the cost
of wide coverage, sentence co-occurrence increases the
accuracy of interaction prediction. Since the ultimate
goal of IE is to extract knowledge and accuracy is the
most important aspect in evaluating the performance
of such systems, it makes sense to focus the effort
in seeking interaction sentences rather than passages
or abstracts. Not every co-occurrence in sentences
implies a relationship that expresses a fact. In the
2005 Genomics Track dataset, 50% of all sentence co-
occurrences of entities correspond to definite relation-
ships while the rest of the co-occurrences only convey
some possible relationships or contain no relationship
of interest (Li et al, 2005). Therefore, more sophisti-
cated text mining strategies are required to classify sen-
tences that describe interactions between co-occurring
concepts.
In the BioCreative II challenge 1, teams were asked
to determine whether a given passage of text contained
information about the interaction between two proteins.
This classification task worked at the abstract level and
the interacting protein pairs were not required to be ex-
tracted. The task for the Learning Language in Logic
1http://biocreative.sourceforge.net/
133
(LLL?05) challenge 2 was to build systems that ex-
tract interactions between genes or proteins from bio-
logical literature. From individual sentences annotated
with agent-target relations, patterns or models had to be
learned to extract these interactions. The task focused
on extracting only the interacting partners. The context
of an interaction may also be critical to the validity of
the extracted knowledge since not all statements found
in the literature are always true.
In this work, we propose an approach to automati-
cally extract patterns containing relevant interaction be-
tween biological concepts. This extraction is based on
the assumption that biological interactions are articu-
lated by a limited number of POS patterns embedded
in sentences where entities/concepts are co-occurring.
The extracted patterns are then applied to identify inter-
action sentences which describe interactions between
biological entities. Our work aims to identify precise
sentences rather than passages. Because of the nature
of the patterns, we hope that some of the contextual in-
formation present in interaction sentences also play a
role in the classification task.
The rest of the paper is organized as follows: In Sec-
tion 2, we review recent research advances in extracting
biological interactions. Section 3 describes an experi-
mental system designed for our work. Sections 4, 5
and 6 elaborate the approaches and algorithms. Per-
formance is evaluated in Section 7. Finally, Section 8
summarizes the paper and introduces future work.
2 Related work
Early on, Blaschke (Blaschke et al, 1999) employed
patterns to predict the presence of a protein-protein in-
teraction. A series of patterns was developed manu-
ally to cover the most obvious descriptions of protein
functions. This process was based on a set of key-
words, including interaction verbs, that are commonly
used to describe this type of interaction. A sentence ex-
traction system BioIE (Divoli and Attwood, 2005) also
uses patterns to extract entire sentences related to pro-
tein families, protein structures, functions and diseases.
The patterns were manually defined and consisted of
single words, word pairs, and small phrases.
Although systems relying on hand-coded patterns
have achieved some success in extracting biological in-
teractions, the strict requirement of dedicated expert
work is problematic. Moreover, each type of interac-
tion may require a definition of many different patterns
including different arrangements and different variants
2http://genome.jouy.inra.fr/texte/LLLchallenge/
of the same keyword. Manually encoding all patterns
encountered in a corpus is time-consuming and poten-
tially impractical in real applications. Thus, automati-
cally learning such patterns is an attractive solution.
An approach which combines dynamic program-
ming and sequence alignment algorithms as normally
used for the comparison between nucleotide sequences
was introduced by Huang et al (Huang et al, 2004).
This approach is designed to generate patterns useful
for extracting protein-protein interactions. The main
problem with this approach is that the scoring scheme
that is required to implement the alignment algorithm is
difficult to define and contains a potentially large num-
ber of free parameters. We propose a method based
on Genetic Algorithm (GA) heuristics to maximize the
alignment procedure for the purpose of classification.
GAs were also used as a learning strategy to train finite
state automata for finding biological relation patterns
in texts(Plake et al, 2005). It was reported (Bunescu et
al., 2005; Hakenberg et al, 2005) that automatically
learned patterns identify biological interactions even
more accurately than hand-coded patterns.
3 Overview of system design
In this work, we have designed an experimental sys-
tem to facilitate the automatic extraction of biological
interaction patterns and the identification of interaction
sentences. It consists of three major modules: biolog-
ical text preprocessing, interaction pattern extraction,
and interaction sentence identification.
Biological text preprocessing reformats the original
biological texts into candidate sentences. A pattern
learning method is then proposed to automatically ex-
tract the representative patterns of biological interac-
tions. The obtained patterns are further used to iden-
tify instances that evidently describe biological inter-
actions. Poor performance during preprocessing will
have detrimental effects on later stages. In the follow-
ing sections, we will describe each component.
4 Biological text preprocessing
4.1 Sentence preparation
A heuristic method is implemented to detect sentence
boundaries (Mikheev, 2002) based on the assumption
that sentences are usually demarcated by some indica-
tive delimiting punctuation marks in order to segment
the biological texts into sentence units. Captions and
headings that are not grammatically valid sentences are
therefore detected and further eliminated for our work.
134
4.2 Part-of-Speech tagging
POS tagging is then performed to associate each word
in a sentence with its most likely POS tag. Because
subsequent processing steps typically depend on the
tagger?s output, high performance at this level is cru-
cial for success in later stages. A statistical tagger Lin-
gua::EN::Tagger 3 is used to perform this task.
4.3 Biological term annotation
A learning-based biological term annotation system,
ABTA (Jiampojamarn et al, 2005), is embedded in our
system. The type of terms includes molecules, such
as genes, proteins and cell lines, and also biological
processes. Examples of biological processes as entities
are: ?T cell activation? and ?IL-2 gene transcription?.
We consider that a broader definition of biological term
will include more facts from literature, thus leading to
more general use of interaction patterns for IE tasks.
ABTA considers the longest expression and ignores
embedded entities. Further, instead of distinguishing
terms from their relevant biology concepts, a unified
tag ?BIO? is assigned to all the identified terms. We
aim to discover patterns of the general interactions be-
tween biological concepts, not only the interactions be-
tween molecules, e.g., protein-protein interaction.
Tags like NN (noun) and VB (verb) are typically used
to define entities and the action type of interactions,
and thus they are indispensable. However, tags such
as JJ (adjective) and RB (adverb) could occur at differ-
ent positions in a sentence. We decided to remove these
tags to prevent the combinatorial effect that these would
induce within the set of extracted patterns.
4.4 Text chunking
Next, a rule-based text chunker (Ramshaw and Mar-
cus, 1995) is applied on the tagged sentences to fur-
ther identify phrasal units, such as base noun phrases
NP and verbal units VB. This allows us to focus on the
holistic structure of each sentence. Text chunking is not
applied on the identified biological terms. In order to
achieve more generalized interaction patterns, a unified
tag ?VB? is used to represent every verbal unit instead
of employing different tags for various tenses of verbs.
As a result of preprocessing, every sentence is rep-
resented by its generalized form as a sequence of cor-
responding tags consisting of POS tags and predefined
tags. Table 1 summarizes the main tags in the system.
A biological interaction tends to involve at least three
objects: a pair of co-occurring biological entities con-
3http://search.cpan.org/?acoburn
Tag name Tag description Tag type
BIO Biological entity Predefined
NP Base noun phrase Predefined
VB Verbal unit Predefined
IN Preposition POS
CC Coordinating conjunction POS
TO to POS
PPC Punctuation comma POS
PRP Possessive 2nd determiner POS
DET Determiner POS
POS Possessive POS
Table 1: Main tags used in the system
nected by a verb which specifies the action type of the
interaction. Thus, a constraint is applied that only sen-
tences satisfying form ?BioEntity A ? Verb ? BioEn-
tity B? will be preserved as candidate sentences to be
further processed in the system. It is possible that the
presence of two entities in different sentence structures
implies a relationship. However, this work assumes the
underlying co-occurrence of two concepts and a verb in
the interest of improving the classification accuracy.
The obtained candidate sentences are split into train-
ing and testing sets. The training set is used to ex-
tract the representative patterns of biological interac-
tions. The testing set is prepared for identifying sen-
tences that evidently describe biological interactions.
5 Interaction pattern extraction
5.1 PATRICIA trees
The method we propose to extract interaction patterns
from candidate sentences is based on the use of PATRI-
CIA trees (Morrison, 1968). A PATRICIA tree uses
path compression by grouping common sequences into
nodes. This structure provides an efficient way of stor-
ing values while maintaining the lookup time for a key
of O(N). It has been applied to many large information
retrieval problems (Chien, 1997; Chen et al, 1998).
In our work, a PATRICIA tree is used for the first
time to facilitate the automatic extraction of interaction
patterns. All training sentences are inserted and stored
in a generic PATRICIA tree from which the common
patterns of POS tags can be efficiently stored and the
tree structure used to compute relevant usage statistics.
5.2 Potential pattern extraction
Patterns of straightforward biological interactions are
frequently encountered in a range of actual sentences.
Conversely, vague relationships or complex interac-
tions patterns are seldom repeated. Therefore, the
135
premise of this work is that there is a set of frequently
occurring interaction patterns that matches a majority
of stated facts about molecular biology. In this work, a
biological interaction pattern is defined as follows:
Definition 5.1. A biological interaction pattern bip
is a sequence of tags defined in Table 1 that captures an
aggregate view of the description of certain types of bi-
ological interactions based on the consistently repeated
occurrences of this sequence of tags in different inter-
action sentences. BIP = {bip1, bip2, ? ? ? , bipk} repre-
sents the set of biological interaction patterns.
We first extract potential interaction patterns by
populating a PATRICIA tree using training sentences.
Every node in the tree contains one or more system
tags, which is the preceding tag sequence of its descen-
dant nodes in each sentence. Every sentence is com-
posed of a path of system tags from the root to a leaf.
Hence, we propose that the sequence of system tags
that can be formed from traversing the nodes of the tree
is a potential pattern of biological interactions. At the
same time, the occurrence frequency of each pattern is
also retrieved from the traversal of tree nodes.
A predefined frequency threshold fmin is used as
a constraint to filter out patterns that occur less than
fmin times. It has been demonstrated that if an interac-
tion is well recognized, it will be consistently repeated
(Blaschke et al, 1999; Ono et al, 2001). The general-
ization and the usability of patterns can be controlled by
tuning fmin. Further, some filtering rules are adapted
to control the form of a pattern and enhance the quality
of the discovered patterns, such as if a pattern ends with
a tag IN, VB, CC or TO, the pattern will be rejected.
Flexibility in setting this threshold can be applied to
meet special demands. Algorithm 1 shows our pattern
learning method which has a time complexity of O(n)
in the size of candidate sentences, n.
Algorithm 1 Patricia-Tree-based Extraction of Biolog-
ical Interaction Patterns
Input: Candidate Sentences CS ? Biological text; a prede-
fined threshold fmin; a set of filtering rules FR
Output: BIP : Set of biological interaction patterns
BIP ? ?; PT ? ? //PT : Patricia Trie
for all sentences s ? CS do
PT ? Insert(s) //Populating Patricia Tree
for all nodes ni ? PT do
bipi ? Pattern(ni) //Concatenating tags in nodes
from root to ni, which is a potential pattern
if Count(bipi) ? fmin and bipi does not meet FR
then
//Count(bipi) returns No. of occurrences of bipi;
BIP ? bipi
5.3 Interaction verb mining
Although the obtained patterns are derived from the
candidate sentences possessing the form ?BioEntity A
? Verb ? BioEntity B?, some of them may not contain
facts about biological interactions. This is possible if
the action verbs do not describe an interaction. Quite a
few verbs, such as ?report?, ?believe?, and ?discover?,
only serve a narrative discourse purpose. Therefore,
mining the correct interaction verbs becomes an impor-
tant step in the automatic discovery of patterns. We de-
cided to perform the method applied in (Huang et al,
2004) to mine a list of interaction verbs. This will be
used to further improve the relevance of achieved pat-
terns by filtering out patterns formed by the sentences
in which the action verbs are not on the list.
6 Interaction sentence identification
Once the biological interaction patterns are obtained,
we perform interaction sentence identification on test-
ing sentences. For our work, they are partitioned into
two sets: interaction sentences which explicitly discuss
interactions between entities, and non-interaction sen-
tences which do not describe interactions, or merely
imply some vague relationships between entities. The
task of interaction sentence identification is treated as a
classification problem to differentiate between interac-
tion sentences and non-interaction sentences.
6.1 Pattern matching scoring
We first perform pattern matching by iteratively apply-
ing the interaction patterns to each testing sentence.
This is done using sequence alignment which calculates
the degree of the similarity of a sentence to an inter-
action pattern. Since patterns capture various ways of
expressing interactions among sentences, a high simi-
larity between an interaction sentence and a pattern is
expected. Therefore, we conjecture that the alignment
scores can be used to discriminate some type of inter-
action sentences from other types of sentences.
The scoring scheme involved in the pattern match-
ing consists of penalties for introducing gaps, match re-
wards and mismatch penalties for different system tag
pairs. Table 2 presents an example scoring scheme for
main tags. Penalties and rewards are denoted respec-
tively by negative and positive values.
As a variation of global alignment, an end-space free
alignment algorithm is implemented to facilitate the
alignment between patterns and testing sentences. The
shortest pattern is always preferred for a sentence in
case that same alignment score is achieved by multiple
136
Tag Gap Match Mismatch
BIO -10 +8 -3
NP -8 +6 -3
VB -7 +7 -3
IN -6 +5 -1
CC -6 +5 -1
TO -1 +5 -1
PPC -1 +3 -1
PRP -1 +3 -1
DET -1 +3 -1
POS -1 +3 -1
Table 2: An alignment scoring scheme for system tags
patterns. As a result, each sentence is assigned to its
most appropriate pattern along with a maximum align-
ment score. Therefore, an interaction sentence will be
highlighted with a high alignment score by its most
similar interaction pattern, while a non-interaction sen-
tence will be characterized by a low alignment score
indicating rejections by all patterns. Essentially, this
procedure can be seen as a variation of the well-known
k Nearest Neighbors classification method, with k = 1.
6.2 Performance evaluation
We then evaluate whether the alignment scores can be
used to classify the testing sentences. We have pro-
posed two independent evaluation measures: statistical
analysis (SA) and classification accuracy (AC).
SA measures whether the scoring difference be-
tween the mean of interaction sentences and the mean
of non-interaction sentences is statistically significant.
If the difference is significant, there will be a tendency
that interaction sentences outscore non-interaction sen-
tences in alignment. Hence, it would be reliable to
use alignment scores to classify testing sentences. Al-
though non-interaction sentences could come from the
same documents as interaction sentences and discuss
concepts that are associated with the target interac-
tions, we assume that interaction sentences and non-
interaction sentences are two independent samples.
The statistical two-sample z test (Freund and Per-
les, 2006) is performed with the null hypothesis that
there is no scoring difference between the means of
interaction and non-interaction sentences. A compar-
atively large z will lead to the rejection of the null
hypothesis. Naturally, the increase of z value will in-
crease the difference between the means and therefore
conceptually keep pushing the overall scoring distrib-
utions of two samples further away from each other.
Consequently, interaction sentences can be separated
from non-interaction sentences according to alignment
scores. In reality, the distinction between interaction
and non-interaction sentences is not absolute. Thus,
the scoring distributions of two samples can only be
distanced by a certain maximum value of z depending
on the scoring scheme applied in pattern matching.
Conversely, AC measures the proportion of correctly
classified testing sentences, including both interaction
and non-interaction sentences, to the total testing sen-
tences. An appropriate threshold T is determined for
obtained alignment scores to differentiate between in-
teraction and non-interaction sentences, and to facili-
tate the calculation of classification accuracy.
It is not possible to evaluate the performance without
correctly pre-labeled testing sentences. We decided to
manually classify the testing sentences in advance by
assigning each sentence an appropriate label of inter-
action or non-interaction. This work was done by two
independent experts, both with Ph.D. degrees in mole-
cular biology or a related discipline.
6.3 Scoring scheme optimization
The scoring scheme applied in pattern matching has a
crucial impact on the performance of interaction sen-
tence identification. An interesting problem is whether
there exists an optimal scoring scheme covering the
costs of gap, match and mismatch for different sys-
tem tags in the pattern matching alignment, which is
destined to achieve the best performance on classify-
ing testing sentences. To the best of our knowledge,
no efforts have been made to investigate this problem.
Instead, an empirical or arbitrary scoring scheme was
adopted in previous research for the pairwise align-
ments (Huang et al, 2004; Hakenberg et al, 2005). We
have proved that the problem is NP-hard by reducing a
well-known NP-hard problem 3-SAT to this problem.
The proof is not presented in this work.
A genetic algorithm (GA) is used as a heuristic
method to optimize parameters of the scoring scheme
for sentence classification. The costs of penalties and
rewards for different system tags are encoded by inte-
ger values within two predefined ranges: [-50, 0) and
(0, 50], and assembled as a potential solution of scor-
ing scheme, which consists of 30 parameters covering
the costs for tags in the alignment as listed in Table 2.
The two evaluation measures SA and AC are used as
the fitness function for GA respectively with the goal
of maximizing z value or classification accuracy.
GA is set up to evolve for 100 generations, each of
which consists of a population of 100 potential solu-
tions of scoring scheme. GA starts with a randomly
137
generated population of 100 potential solutions and
proceeds until 100 generations are reached. The num-
ber of generations and the population size are decided
with consideration of the runtime cost of evaluating the
fitness function, which requires running the scoring al-
gorithm with each sentence. A large number of gener-
ations or a large population size would incur an expen-
sive runtime cost of evaluation.
In addition, we further divide the labeled set of can-
didate sentences into two subsets: The first dataset is
used to optimize parameters of the scoring scheme,
while the second dataset, testing set, is used to test the
achieved scheme on the task of sentence classification.
7 Results and evaluation
7.1 Dataset
Our experiments have been conducted on Genia cor-
pus (v3.02) 4, the largest, publicly available corpus in
molecular biology domain. It consists of 2,000 biolog-
ical research paper abstracts and is intended to cover
biological reactions concerning transcription factors in
human blood cells. The information of sentence seg-
mentation, word tokenization, POS tagging and biolog-
ical term annotation is also encoded in the corpus.
7.2 Biological text preprocessing results
Evaluated using the inherently equipped annotation in-
formation, our system achieves nearly 99% accuracy
on segmenting sentences. Further, it obtains an overall
POS tagging accuracy of 91.0% on 364,208 individ-
ual words. We noticed that the tagging information en-
coded in Genia corpus is not always consistent through-
out the whole corpus, thus introducing detrimental ef-
fects on the tagging performance. Also, considering
that the tagger is parameterized according to the gen-
eral English domain, porting this tagger to the biology
domain is accompanied by some loss in performance.
The system reaches an F-score of 0.705 on annotat-
ing all biological terms including both multi-word and
single word terms. After performing text chunking, the
system produces a set of candidate sentences. We fur-
ther perform text chunking on Genia corpus based on
its encoded annotations and use the resulting set of sen-
tences for the subsequent experiments to provide a gold
standard to which results produced based on our system
annotations can be compared. Table 3 presents some
statistics of the preprocessed dataset. For each type of
annotations, we randomized the candidate sentence set
4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
and chose 12,525 candidate sentences as the training
set to extract biological interaction patterns. The rest
of candidate sentences are prepared as the testing set.
Attributes Genia Our system
Total preprocessed sentences 18,545 18,355
Candidate sentences 16,272 17,525
Training set sentences 12,525 12,525
Testing set sentences 6,020 5,000
Table 3: Statistics of experimental dataset
7.3 Interaction pattern extraction results
fmin = 5 is used to filter out the potential patterns
that appear less than 5 times in the training set. Eval-
uated by domain experts, lists of 300 interaction verbs
and 700 non-interaction verbs are obtained from 12,525
training sentences with Genia annotations. Inflectional
variants of the verbs are also added into the lists.
Refined by the filtering rules and the interaction
verbs, a final set of representative patterns of biological
interactions are obtained from Algorithm 1. We per-
formed our proposed pattern learning method on train-
ing sentences of both the GENIA and our own anno-
tations. There are respectively 241 and 329 potential
patterns. Of these, 209 and 302 were extracted. Inter-
estingly, only 97 extracted patterns are common to both
annotation schemes.
Table 4 lists the 10 most frequent interaction patterns
based on Genia annotations. For instance, a training
sentence conforming to the second pattern is ?The ex-
pression of the QR gene is regulated by the transcrip-
tion factor AP-1.? (MEDLINE: 96146856).
Pattern count Pattern
264 BIO VB BIO IN BIO
261 NP IN BIO VB IN BIO
182 NP IN BIO VB BIO
162 BIO IN BIO VB IN BIO
160 BIO VB IN BIO IN BIO
143 NP IN BIO VB IN NP IN BIO
142 NP VB IN BIO VB BIO
138 PRP VB IN BIO VB BIO
126 BIO VB NP IN BIO IN BIO
121 NP IN BIO VB NP IN BIO
Table 4: Extracted Biological Interaction Patterns
7.4 Interaction sentence identification results
Since the total testing sentence set is large, we decided
to randomly extract 400 sentences from it as the sam-
ple set for our task. The 400 sentences were manu-
138
Figure 1: AC comparison between two measures
ally pre-labeled into two classes: interaction and non-
interaction. Further, a subset of 300 testing sentences
was used by GA to optimize parameters of the scor-
ing scheme, while the remaining 100 sentences were
prepared to test the achieved scheme on sentence clas-
sification. The distribution of class labels of the sample
sentences is shown in Table 5.
Class label 300 sentences 100 sentences
No. % No. %
Interaction 158 52.67 53 53
Non-interaction 142 47.33 47 47
Table 5: Class distribution of sample sentences
7.4.1 Comparison between two measures
We applied the evaluation measures, SA and AC,
respectively to the subset of 300 testing sentences as
the fitness function for GA, and recorded the scoring
scheme of every generation resulted from GA. Figure 1
presents the distribution of achieved classification ac-
curacy in terms of each scoring scheme optimized by
GA. This comparison is done with respect to the gener-
ation and evaluated on 300 testing sentences using the
annotations from the Genia corpus.
The achieved classification accuracy for AC gen-
erally outperforms the classification accuracy derived
by SA. It reaches its highest classification accuracy
80.33% from the 91th generation. Therefore, AC is
considered more efficient with the system and becomes
our final choice of fitness function for GA.
7.4.2 Results of sentence identification
GA results in an optimized performance on the 300
sentences. It also results in an optimized scoring
scheme along with its associated scoring threshold T ,
which are then applied together to the other 100 test-
ing sentences. Table 6 and 7 present the system perfor-
mance on the two sets respectively to both annotations.
Experimental Genia Our system
Results Interaction Non Interaction Non
Precision 0.757 0.887 0.704 0.702
Recall 0.928 0.665 0.761 0.640
F-score 0.834 0.750 0.731 0.670
Overall AC(%) 80.33 70.33
Table 6: Performance on 300 testing sentences
Experimental Genia Our system
Results Interaction Non Interaction Non
Precision 0.739 0.762 0.676 0.697
Recall 0.792 0.723 0.755 0.638
F-score 0.765 0.742 0.713 0.666
Overall AC(%) 75.96 70.00
Table 7: Performance on 100 testing sentences
Table 6 shows that when using the Genia annota-
tions the system achieves an 0.834 F-score in identify-
ing interaction sentences and an overall AC of 80.33%,
which is much higher than the proportion of either in-
teraction or non-interaction sentences in the 300 sen-
tence subset. This indicates that the system performs
well on both classes. In 100 generations GA is not able
to evolve a scoring scheme that leads to an AC above
80.33%. Moreover, our system annotations achieve
a lower performance than Genia annotations. We at-
tribute the difference to the accuracy loss of our system
annotations in the preprocessing steps as inaccurate an-
notations will lead to inappropriate patterns, thus harm-
ing the performance of sentence identification. For Ge-
nia annotations, the performance on the 100 testing sen-
tences suggests an overfitting problem.
There are a number of preprocessing steps that affect
the final classification performance. However, even as-
suming an ideal preprocessing of the unstructured text,
our method relies on the assumption that all interac-
tion sentences are articulated by a set of POS patterns
that are distinct to all other types of sentences. The
manual annotation of the training/testing set was a dif-
ficult task, so it is reasonable to assume that this will
also be difficult for the classifier. The use of passive
voice and the common use of comma splicing within
patterns makes sentence-level classification an espe-
cially difficult task. Another source of interactions that
our system cannot identify are implied and assume a
deeper semantic understanding of the concepts them-
139
selves. Other sentences are long enough that the inter-
action itself is merely a secondary purpose to another
idea. All of these factors pose interesting challenges
for future development of this work.
Moreover, we also experimented with 10 empirical
scoring schemes derived from previous experiments on
the 300 sentences respectively, including the scheme in
the Table 2. Several fixed thresholds were attempted for
obtained alignment scores to differentiate between in-
teraction and non-interaction sentences. Without using
GA to optimize parameters of the scoring scheme, the
best performance of 10 empirical schemes is an overall
AC of 65.67%, which is outperformed at the 3rd gen-
eration of the GA optimization with Genia annotations.
7.5 System performance comparison
Within the framework of our system, we further con-
ducted experiments on the same dataset for sentence
identification using interaction patterns generated by
another pattern generating algorithm (PGA) (Huang et
al., 2004) in order to compare with the performance of
patterns obtained by our pattern learning method.
In our implementation, PGA iterates over all pairs
of candidate sentences in the training set and calculates
the best alignment for each pair in terms of the cost
scheme of gap penalties proposed (Huang et al, 2004).
Each consensus sequence from the optimal alignment
of each pair forms a pattern. The filter rules proposed
are also applied. PGA has a time complexity of O(n2)
in the size of candidate sentences, n. Hence, our pro-
posed pattern learning method is much more efficient
when dealing with large collections of biological texts.
PGA produces a large number of patterns, even with
fmin = 5 and other filtering criteria. There are 37,319
common patterns between two types of annotations.
Attributes Genia Our system
Potential patterns (fmin = 5) 476,600 387,302
Extracted patterns (fmin = 5) 176,082 88,800
Table 8: Pattern extraction results of PGA
In order to make a direct comparison, we decided to
experiment with the same number of interaction pat-
terns. For Genia annotations, we chose the most fre-
quent 209 patterns generated by PGA to compare with
the 209 patterns by our method. For our system annota-
tions, two sets of 302 patterns are employed. Further, it
is found that there are 96 common patterns between the
two sets of 209 patterns for Genia annotations, and 153
common patterns between the two sets of 302 patterns
for our system annotations. Table 9 and 10 present the
results of sentence identification of PGA. The results
show that patterns generated by PGA do not perform
as well as patterns obtained by our method.
Experimental Genia Our system
Results Interaction Non Interaction Non
Precision 0.721 0.869 0.663 0.699
Recall 0.918 0.606 0.785 0.556
F-score 0.808 0.714 0.719 0.619
Overall AC(%) 77.00 67.67
Table 9: Performance of PGA on 300 testing sentences
Experimental Genia Our system
Results Interaction Non Interaction Non
Precision 0.664 0.796 0.698 0.635
Recall 0.849 0.574 0.566 0.766
F-score 0.745 0.667 0.625 0.694
Overall AC(%) 71.98 66.00
Table 10: Performance of PGA on 100 testing sentences
8 Conclusion and future work
In this paper, a novel approach is presented to auto-
matically extract the representative patterns of biologi-
cal interactions, which are used to detect sentences that
describe biological interactions. We conducted the ex-
periments on our designed system based on the Ge-
nia corpus. By means of a genetic algorithm, the sys-
tem achieves an 0.834 F-score using Genia annotations
and an 0.731 F-score using our system annotations in
identifying interaction sentences by evaluating 300 sen-
tences. By applying the optimized scoring scheme to
another set of 100 sentences, the system achieves com-
parable results for both types of annotations. Further-
more, by comparing with another pattern generating al-
gorithm, we infer that our proposed method is more ef-
ficient in producing patterns to identify interaction sen-
tences.
In our future work, we would like to employ the ob-
tained interaction patterns to guide the extraction of
specific interactions. The matching between patterns
and sentences will be performed and the matched parts
of each sentence will be extracted as candidate interac-
tions. Further reasoning processes can be performed
by means of available biological ontologies, such as
UMLS Semantic Network (Mccray and Bodenreider,
2002) and Gene Ontology (Consortium, 2001), to in-
fer new relations from the initial interactions. Such
processes can be employed to derive additional biolog-
ical knowledge from existing knowledge, or test for bi-
ological consistency of the newly entered data.
140
References
Christian Blaschke, Miguel A. Andrade, Christos Ouzounis,
and Alfonso Valencia. 1999. Automatic extraction of bi-
ological information from scientific text: Protein-protein
interactions. In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biology,
pages 60?67. AAAI Press.
Razvan Bunescu, Ruifang Ge, Rohit J Kate, Edward M Mar-
cotte, Raymond J Mooney, Arun K Ramani, and Yuk W
Wong. 2005. Comparative experiments on learning infor-
mation extractors for proteins and their interactions. Arti-
ficial Intelligence in Medicine, 33(2):139?155.
Keh-Jiann Chen, Wen Tsuei, and Lee-Feng Chien. 1998.
Pat-trees with the deletion function as the learning device
for linguistic patterns. In Proceedings of the 17th inter-
national conference on Computational linguistics, pages
244?250, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Lee-Feng Chien. 1997. Pat-tree-based keyword extrac-
tion for chinese information retrieval. SIGIR Forum,
31(SI):50?58.
Gene Ontology Consortium. 2001. Creating the gene ontol-
ogy resource: design and implementation. Genome Re-
search, 11(8):1425?1433.
Anna Divoli and Teresa K. Attwood. 2005. Bioie: extract-
ing informative sentences from the biomedical literature.
Bioinformatics, 21(9):2138?2139.
John E. Freund and Benjamin M. Perles. 2006. Modern
Elementary Statistics. Prentice Hall.
Jorg Hakenberg, Conrad Plake, Ulf Leser, Harald Kirsch,
and Dietrich Rebholz-Schuhmann. 2005. Lll?05 chal-
lenge: Genic interaction extraction with alignments and
finite state automata. In Proceedings of Learning Lan-
guage in Logic Workshop (LLL?05) at ICML, page 38C45,
Bonn, Germany.
Minlie Huang, Xiaoyan Zhu, Yu Hao, Donald G. Payan,
Kunbin Qu, and Ming Li. 2004. Discovering patterns to
extract protein-protein interactions from full texts. Bioin-
formatics, 20:3604?3612.
Hyunchul Jang, Jaesoo Lim, Joon-Ho Lim, Soo-Jun Park,
Kyu-Chul Lee, and Seon-Hee Park. 2006. Finding the ev-
idence for protein-protein interactions from pubmed ab-
stracts. Bioinformatics, 22(14):e220?e226.
Sittichai Jiampojamarn, Nick Cercone, and Vlado Kes?elj.
2005. Biological Named Entity Recognition using N-
grams and Classification Methods. In Proceedings of the
Conference Pacific Association for Computational Lin-
guistics, PACLING?05, Tokyo, Japan.
Jiao Li, Xian Zhang, Yu Hao, Minlie Huang, and Xiaoyan
Zhu. 2005. Learning domain-specific knowledge from
context?thuir at trec2005 genomics track. In Proceed-
ings of 14th Text Retrireval Conference (TREC2005),
Gaithersburg, USA.
Alexa T. Mccray and Olivier Bodenreider. 2002. A concep-
tual framework for the biomedical domain. In Semantics
of Relationships, Kluwer, pages 181?198. Kluwer Acad-
emic Publishers.
Andrei Mikheev. 2002. Periods, capitalized words, etc.
Comput. Linguist., 28(3):289?318.
Donald R. Morrison. 1968. Patricia ? Practical Algorithm
To Retrieve Information Coded in Alphanumeric. Jour-
nal of the ACM, 15(4):514?534.
Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami, and
Toshihisa Takagi. 2001. Automated extraction of infor-
mation on protein-protein interactions from the biological
literature. Bioinformatics, 17(2):155?161.
Conrad Plake, Jorg Hakenberg, and Ulf Leser. 2005. Learn-
ing patterns for information extraction from free text. In
Proceedings of AKKD 2005, Karlsruhe, Germany.
Lance Ramshaw and Mitch Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings of
the Third Workshop on Very Large Corpora, pages 82?94,
Somerset, New Jersey.
Soumya Raychaudhuri. 2006. Computational Text Analy-
sis: For Functional Genomics and Bioinformatics. Ox-
ford University Press.
Andre Skusa, Alexander Ruegg, and Jacob Kohler. 2005.
Extraction of biological interaction networks from scien-
tific literature. Brief Bioinform, 6(3):263?276.
141
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 387?397, Dublin, Ireland, August 23-29 2014.
Author Verification Using Common N-Gram Profiles of Text Documents
Magdalena Jankowska and Evangelos Milios and Vlado Ke
?
selj
Faculty of Computer Science, Dalhousie University
6050 University Avenue
Halifax, NS B3H 4R2, Canada
{jankowsk, eem, vlado}@cs.dal.ca
Abstract
Authorship verification is the problem of answering the question whether or not a sample text
document was written by a specific person, given a few other documents known to be authored by
them. We propose a proximity based method for one-class classification that applies the Common
N-Gram (CNG) dissimilarity measure. The CNG dissimilarity (Ke?selj et al., 2003) is based on
the differences in the frequencies of n-grams of tokens (characters, words) that are most common
in the considered documents. Our method utilizes the pairs of most dissimilar documents among
documents of known authorship. We evaluate various variants of the method in the setting of
a single classifier or an ensemble of classifiers, on a multilingual authorship verification corpus
of the PAN 2013 Author Identification evaluation framework. Our method yields competitive
results when compared to the results achieved by the participants of the PAN 2013 competition
on the entire set, as well as separately on two subsets ? English and Spanish ones ? out of the
three language subsets of the corpus.
1 Introduction
The task of computational detection of who wrote a given text is a widely studied linguistic and machine
learning problem with applications in domains such as forensics, security, criminal and civil law, or liter-
ary research. The authorship verification problem is a type of such a computational authorship analysis
task, in which, given a set of documents written by one author, and a sample document, we are asked
whether or not this sample document was written by this given author. This is different from the more
traditional problem of deciding who among a finite number of candidate authors for which we are given
sample writings, wrote a document in question, and, albeit more difficult, is often considered to better
reflect the real-life problems related to authorship detection (Koppel et al., 2012).
We describe our one-class proximity based classification method and evaluate it on the multilingual
dataset of the Authorship Identification competition task of PAN 2013 (evaluation lab on uncovering
plagiarism, authorship, and social software misuse) (Juola and Stamatatos, 2013).
During the competition, to which a variant of our method has been submitted (Jankowska et al., 2013),
it yielded ranking 5th (joint) out of 18 with respect to the accuracy, and 1st rank out of 10 in the secondary
ranking based on the area under the ROC curve (AUC), which evaluates the ordering of instances by the
confidence score. In this paper we show some further experiments on how a different way of tuning
the classifier parameters, using solely the training dataset of the competition, as well as an ensemble
of classifiers based on our method, without any parameter tuning, leads to competitive accuracy results
while still achieving high AUC values.
2 Related Work
The author analysis has been studied extensively in the context of the authorship attribution problem, in
which there is a small set of candidate authors out of which the author of a questioned document is to
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
387
be selected. There are several papers (Stamatatos, 2009; Juola, 2008; Koppel et al., 2009) presenting
excellent surveys of this area.
The two main categories (Stamatatos, 2009) of solutions for the problem are similarity based ap-
proaches, in which a classification is performed in a Neigherst Neighbour scheme, attributing a sample
text to the author whose writing is most similar according to some measure, and machine-learning based
approaches, in which each document by an author is treated as a data sample within a class, and a super-
vised classifier is trained on these data.
A more limited research has been performed on an open-set variant on this problem, in which it is
possible that none of the candidate authors wrote a document in question, with authorship verification
being the extreme case of an open-set problem with only one candidate. The ?unmasking method? for
authorship verification (Koppel and Schler, 2004) is successful for novel-length texts. This approach,
similarly as our method, falls into a category of intrinsic methods (Juola and Stamatatos, 2013); it uses
only the documents in question, without constructing classes of other authors. The ensemble of one-class
classifiers (Halvani et al., 2013), which achieved high accuracy at the PAN 2013 Author Identification
competition, is also an example of such an intrinsic method. It varies from our approach by using a
different scheme of creating the dissimilarity between an unknown document and the known authorship
set of texts, based on the Nearest Neighbour technique (Tax, 2001), as well as by a different distance
measure and features used.
Another way of approaching the author verification problem is to cast it into a binary or multi-class
classification, by creating a class or classes of other authors. The ?imposters? method (Koppel and Win-
ter, 2014) generates a very large set of texts by authors that did not write the questioned document,
to transform the problem into a open-set author attribution problem with many candidates, handled
by an ensemble-based similarity method (Koppel et al., 2011). A modified version of the imposters
method (Seidman, 2013) achieved first ranking in the PAN 2013 Authorship Identification competition.
The method (Veenman and Li, 2013), which achieved the highest accuracy on the English set in this
competition, is also of such an extrinsic type; its first step is a careful selection of online documents
similar to the ones in the problems. The method (Ghaeini, 2013), which produces competitive ordering
of verification instances, uses weighted k-NN approach using classes of other authors created from other
verification instances.
3 Methodology
The formulation of the authorship verification task for the Author Identification Task at PAN 2013 is the
following: ?Given a set of documents (no more than 10, possibly only one) by the same author, is an
additional (out-of-set) document also by that author?? (Juola and Stamatatos, 2013).
We approach this task with an algorithm based on the idea of proximity based methods for one-class
classification. In one-class classification framework, an object is classified as belonging or not belong-
ing to a target class, while only sample examples of objects from the target class are available during
the training phase. Our method resembles the idea of the k-centers algorithm for one-class classifica-
tion (Ypma et al., 1998; Tax, 2001), with k being equal to the number of all training documents in the
target set (i.e., written by the given author). The k-centers algorithm is suitable for cases when there
are many data points from the target class; it uses equal radius sphere boundaries around the target data
points and compares the sample document to the closest such centre. We propose a different classifica-
tion condition, described below, utilizing the pairs of most dissimilar documents within the set of known
documents.
Let A = {d
1
, ..., d
k
}, k ? 2, be the input set of documents written by a given author, which we will
call known documents. If only one known document is provided, we split it in half and treat these two
chunks as two known documents. Let u be the input sample document, of which the authorship we are to
verify, that is return the answer ?Yes? or ?No? to the posed question whether it was written by the given
author.
Our algorithm calculates for each known document d
i
, i = 1, 2, ..., k, the maximum dissimilar-
ity between this document and all other known documents: D
max
(d
i
, A), as well as the dissimilar-
388
ity between this document and the sample document u: D(d
i
, u), and finally the dissimilarity ratio
r(d
i
, u, A) =
D(d
i
,u)
D
max
(d
i
,A)
(and thus r(d
i
, u, A) < 1 means that there exists a known document more
dissimilar to d
i
than u, while r(d
i
, u, A) > 1 means that all the known documents are more similar to
d
i
than u). The average M(u,A) of the dissimilarity ratio over all known documents d
1
, d
2
, ..., d
k
from
A, is the subject of the thresholding: the sample u is classified as written by the same person as the
known documents if and only if M(u,A) is at most equal to a selected threshold ?. Notice that in this
framework the dissimilarity between the documents does not need to be a metric distance, i.e., it does
not need to fulfil the triangle inequality (as is the case for the dissimilarity measure we choose).
For the dissimilarity measure between documents we use the Common N-Gram (CNG) dissimilarity;
proposed by Ke?selj et al. (2003); this dissimilarity (or its variants) used in the Nearest Neighbour classi-
fication scheme (Common N-gram classifier) was successfully applied to authorship classification tasks
(Ke?selj et al., 2003; Juola, 2008; Stamatatos, 2007). The CNG dissimilarity is based on the differences
in the usage frequencies of the most common n-grams of tokens (usually characters, but possibly other
tokens) of the documents. Each document is represented by a profile: a sequence of the most common
character n-grams (strings of characters of the given length n from the document) coupled with their
frequencies (normalized by the length of the document). The dissimilarity between two documents of
the profiles P
1
and P
2
is defined as follows:
D(P
1
, P
2
) =
?
x?(P
1
?P
2
)
(
f
P
1
(x)? f
P
2
(x)
f
P
1
(x)+f
P
2
(x)
2
)
2
(1)
where x is a character n-gram from the union of two profiles, and f
P
i
(x) is the normalized frequency of
the n-gram x in the the profile P
i
, i = 1, 2 (f
P
i
(x) = 0 whenever x does not appear in the profile P
i
).
The parameters of the dissimilarity are the length of the n-grams n and the length of the profile L. As
our method is based on the ratios of dissimilarities between documents, we take care that the documents
in a given problem are always represented by profiles of the same length. We experiment with two ways
of selecting the length of the profiles. In the dynamic-length variant, the length of profiles is selected
separately for each problem, based on the number of n-grams in the documents in the given instance
(parametrized as a fraction f of all n-grams of the document that contains the least number of them). In
the fixed-length variant, we use a selected fixed length L of profiles. For a one-class classifier we need
to select two parameters defining the features used for dissimilarity (length of the n-grams n, and either
the fixed length L of a profile, or the fraction f defining the profile length), and the parameter ? (for
classifying by thresholding the average dissimilarity ratio M ).
We linearly scale the measure M to represent it as a confidence score in the range from 0 (the highest
confidence in the answer ?No? ) to 1 (the highest confidence in the answer ?Yes?), with the answer ?Yes?
given if and only if the confidence score is at least 0.5. The value of M equal to ? is transformed to
the score 0.5, values greater than ? to the scores between 0 and 0.5, and values less than ? to the scores
between 0.5 and 1 (a cutoff of 0.1 is applied, , i.e. all values of M(u,A) < ? ? cutoff are mapped to the
score 1, and all values of M(u,A) > ? + cutoff are mapped to the score 0).
4 Training and test datasets
We leverage the evaluation framework of the PAN 2013 competition task of Author Identification (Juola
and Stamatatos, 2013), the datasets of which were carefully created for authorship verification, with
effort made to match within each problem instance the texts by the same genre, register, theme and time
of writing. The dataset consists of English, Greek and Spanish subsets. In each instance, the number
of documents of known authorship is not greater than 10 (possibly only one). The dataset is divided
into the training set pan13-ai-train and the test set pan13-ai-test. The training set was made
available for the participants before the competition; the test set was used to evaluate the submissions
and subsequently published (PAN, 2013).
To enrich the training dataset for our competition submission, we also compiled ourselves two ad-
ditional datasets using existing sets for other authorship identification tasks. mod-pan12-aa-EN is
389
an English author verification set compiled from the fiction corpus for the Traditional Authorship At-
tribution sub task of the PAN 2012 competition (PAN, 2012; Juola, 2012). mod-Bpc-GR is a Greek
author verification set compiled from the Greek dataset of journal articles (Stamatatos et al., 2000). It is
important to note that these sets are different from the competition dataset in that we did not attempt to
match the theme or time of writing of the texts.
Table 1 presents characteristics of the datasets.
pan13-ai-train
total English Spanish Greek
number of problems 35 10 5 20
mean of the known document number per problem 4.4 3.2 2.4 5.5
mean length of documents in words 1226 1038 653 1362
genre textbooks editorials, fiction articles
pan13-ai-test
total English Spanish Greek
number of problems 85 30 25 30
mean of the known document number per problem 4.1 4.2 3.0 4.9
mean length of documents in words 1163 1043 890 1423
genre textbooks editorials, fiction articles
mod-pan12-aa-EN
total: English
number of problems 22
mean of the known document number per problem 2.0
mean length of documents in words 4799
genre fiction
mod-Bpc-GR
total: Greek
number of problems 76
mean of the known document number per problem 2.5
mean length of documents in words 1120
genre articles
Table 1: Characteristics of datasets used in our authorship verification experiments.
5 Evaluation measures
In our experiments we use two measures of evaluation, based on the measures proposed for the PAN 2013
competition. The accuracy is the fraction of all problems that have been answered correctly. The AUC
measure is the area under the ROC curve based on the confidence scores. It is the nature of applications
of authorship verification, such as forensics, that makes the confidence score and not only the binary
answer, an important aspect of a solution (Gollub et al., 2013).
For our method accuracy is equivalent to the measure that was used in the competition for the main
evaluation. This measure is F
1
, defined based on the fact that in the competition it was allowed to
withdraw an answer (i.e., use an ?I do not know? option). Precision and recall were defined as follows:
recall =
#correct answers
#problems
, precision =
#correct answers
#answers
, and F
1
is the harmonic mean of precision and
recall. For any method that, as our method, provides the answer ?Yes? or ?No? for all problem instances,
the accuracy and F
1
are equivalent.
390
6 Types of classifiers
A single classifier of our method requires two parameters defining the features to be used to represent
a document (the length of an n-gram and the length of a profile), as well as a selection of the threshold
for the dissimilarity for the classification decision. We tune and evaluate four version of such single
classifiers. Combining many such one-class classifiers, each using different combination of features
defining parameters, into one ensemble, allows to remove or mitigate the parameter tuning. We describe
the creation and the evaluation of four types of ensembles.
Table 2 reports the considered space for feature defining parameters. On a training set, for a given
combination of feature defining parameters (n,L) or (n,f ), we use the accuracy at the optimal threshold
(a threshold ? that maximizes the accuracy), as a measure of performance for these parameters.
Parameters
n
length of n-grams
L
# of n-grams: profile length (fixed-length)
f fraction of n-grams for profile length (dynamic-length)
?
threshold for classification
?
2+
threshold for classification if at least 2 known documents are given
?
1
threshold for classification if only one known document is given
Space of considered parameters
n for character n-grams
{3, 4, ..., 9, 10}
n for word n-grams
{1, 2, 3}
L
{200, 500, 1000, 1500, 2000, 2500, 3000}
f
{0.2, 0.3, ..., 0.9, 1}
single classifiers ensembles
English Spanish Greek English Spanish Greek
vD1 n 6 7 10 eC type character
f 0.75 (n,L) all in the considered space
? 1.02 1.005 1.002 ? 1
vF1 n 6 7 eW type word
L 2000 2000 (n,L) all in the considered space
?
2+
1.02 1.008 ? 1
?
1
1.06 1.04 eCW type character, word
vF2 n 7 3 9 (n,L) all in the considered space
L 3000 2000 3000 ? 1
?
2+
1.014 1.014 0.997 eCW type character, word
?
1
1.056 1.126 1.060 (n,L) selected based on training data
vD2 n 7 3 9 (61) (75) (43)
f 0.8 0.6 0.8 ? 1
?
2+
1.013 1.00530207 0.9966
?
1
1.053 1.089 1.059
Table 2: Parameters for four variants of single one-class classifiers and four ensembles of one-class
classifiers based on our method.
6.1 Single classifiers
For single character n-gram classifiers, we tuned the parameters for each language separately on training
data, by selecting feature defining parameters based on their performance, and selecting the thresholds
391
to correspond to the optimal thresholds. Table 2 reports the parameters of four variants of single classi-
fiers. We include our two submissions to the PAN 2013 Authorship Identification competition: the final
submission vF1 and the preliminary submission vD1. The other two classifiers were tuned and tested
after the competition.
Our preliminary submission vD1 (Table 2) is tuned on pan13-ai-train, with f chosen ad-hoc.
This is the only classifier among the reported variants that does not use a preprocessing of truncation of
all documents in a given problem instance to the length of the shortest document, which tend to increase
the accuracy for cases of a significant difference in the length of documents.
For tuning of parameters of the final submission vF1 (Table 2) we use not only pan13-ai-train,
but also additional training sets mod-pan12-aa-EN and mod-Bpc-GR. We also introduce two thresh-
old values: one for cases when there are at least two known documents, and another one for the cases
when there is only one known document (which has to be divided in two). The intuition behind this dou-
ble threshold approach is that when there is only one known document, the two halves of it can be more
similar to each other than in other cases. After the parameters are selected based on subsets of training
sets with only these problems that contain at least two known documents, the additional threshold is
selected based on the optimal threshold on a modified ?1-only? training set, from the problem of which
all known documents except of a random single one is removed. For Spanish, with only three training
instances with more than one known document, we use the same parameters as for English.
For tuning of vF2 and vD2 (Table 2) we use only competition training data, without the additional
corpora used for vF1. Feature parameters are selected based on the performance on the subsets contain-
ing at least two known documents, and on the ?1-only? modified sets (which allows us to use the Spanish
training set for tuning the Spanish classifiers).
6.2 Ensembles of classifiers
We test ensembles of single one-class classifiers based on our method, with the ensemble combining
answers of the classifiers, and each classifier using different set of features. An important advantage of
an ensemble is the alleviation of the problem of tuning the parameters. Each classifier uses a different
combination of parameters n and L defining the features. And as many classifiers are used, instead of
tuning the threshold of a single classifier based on some training data, the threshold of each classifier
is set to some fixed value, with 1 being a natural choice, as it corresponds to checking whether or not
the unknown document is (on average) less similar to each given known document than the author?s
document that is most dissimilar to this given known document.
We test majority voting and voting weighted by the confidence scores of single classifiers. For each
ensemble we combine answers of the classifiers in order to obtain the confidence score of the ensemble.
For majority voting the confidence score of the ensemble is the ratio of the number of classifiers that
output ?Yes? to the total number of classifiers, the confidence score of the weighted voting is the average
of the confidence scores of the single classifiers.
We experiment with n-grams being characters (utf8-encoded) and words (converted to uppercase).
Table 2 summarize the ensembles. The ensemble eC is of all character n-gram classifiers in our space
of considered parameters n and L; eW is of all word n-gram classifiers; eCW is of all classifiers of eC
and eW. These ensembles do not use any training data. We also create a classifier eCW sel (Table 2),
which is a subset of the classifiers of eCW, selected based on the performance of the single classifiers
on the training data of the competition. For each language separately, we remove classifiers that on the
training data achieved lowest accuracies at their respective optimal thresholds, while keeping at least half
of the character based classifiers and at least half of the word based classifiers. (For Spanish, eCW sel
and eCW differ just by one classifier: the only one that on the small Spanish training set has the optimal
accuracy less than 1.)
7 Results
The accuracy and the area under the ROC curve (AUC) values achieved by the variants of our method
on the PAN 2013 Author Identification test dataset are presented in Table 3. The table states also the
392
best PAN 2013 competition results of other participants
1
(that is the results of these participants that
achieved the highest accuracy or AUC on any (sub)set). There were 17 other participants for which there
are accuracy (or F
1
) results, 9 of which submitted also confidence scores evaluated by AUC.
PAN 2013 Author Identification test dataset
F
1
= accuracy except for Ghaeini,2013 AUC
all English Spanish Greek all English Spanish Greek
single classifiers
vD1 0.718 0.733 0.760 0.667 0.790 0.837 0.846 0.718
vF1 0.682 0.733 0.720 0.600 0.793 0.839 0.859 0.711
vD2 0.729 0.767 0.760 0.667 0.805 0.850 0.936 0.704
vF2 0.753 0.767 0.880 0.633 0.810 0.844 0.885 0.664
ensembles of classifiers
eC majority 0.729 0.800 0.840 0.567 0.754 0.777 0.833 0.620
weight 0.729 0.833 0.800 0.567 0.764 0.830 0.859 0.582
eW majority 0.718 0.733 0.720 0.700 0.763 0.830 0.805 0.700
weight 0.741 0.767 0.760 0.700 0.822 0.886 0.853 0.782
eCW majority 0.800 0.833 0.840 0.733 0.755 0.817 0.821 0.633
weight 0.741 0.800 0.840 0.600 0.780 0.842 0.853 0.622
eCW sel majority 0.800 0.833 0.840 0.733 0.778 0.826 0.814 0.682
weight 0.788 0.800 0.840 0.733 0.805 0.857 0.853 0.687
boxed values: best competition results of other PAN 2013 Author Identification participants
Seidman,2013 0.753 0.800 0.600 0.833 0.735 0.792 0.583 0.824
Veenman and Li,2013 ? 0.800 ? ? ? ? ? ?
Halvani et al.,2013 0.718 0.700 0.840 0.633 ? ? ? ?
Ghaeini,2013 0.606 0.691 0.667 0.461 0.729 0.837 0.926 0.527
Table 3: Area under the ROC curve (AUC) and F
1
(which is equal to accuracy for all algorithms except
for (Ghaeini, 2013)) on the test dataset of PAN 2013 Author Identification competition task. Results of
variants of our method compared with competition results of those among other competition participants
that achieved the highest value of any evaluation measure on any (sub)set. The highest result in any
category is bold; the highest result by other competition participants in any category is boxed.
All variants of our method perform better on the English and Spanish subset than on the Greek one,
both in terms of the accuracy and in terms of AUC. On the Greek subset they are all outperformed by
other competition participant(s). This is most likely due to the fact that the Greek subset was created in a
way that makes it especially difficult for algorithms that are based on CNG character-based dissimilarity
(Juola and Stamatatos, 2013), by using a variant of CNG dissimilarity for the character 3-grams in order
to select difficult cases. This particularity of the set may also be the reason why the ensemble eC of
character n-gram classifiers performed worse than other methods on this set.
The variants of our method are competitive in terms of the ordering of the verification instances ac-
cording to the confidence score as measured by AUC. During the competition, our final submission vF1
achieved the first ranking according to the AUC on the entire set, the highest AUC on the English subset,
and the second-highest AUC values on the Spanish and Greek subset, out of 10 participants that submit-
1
The results of our methods are on the published competition dataset. The results by other participants are the published
competition results. The actual competition evaluation set for Spanish may have some text in a different encoding then the
published set; our final submission method vF1 yielded on it a different result than on the published dataset.
393
ted confidence scores. All variants of our method perform better than any other competition participant
on the entire set. On the English subset the single classifiers and the ensembles with weighted voting
have AUC above 0.8, and out of those only eC has AUC lower than the best result by other participants.
On the Spanish subset all variants of our method achieved AUC above 0.8, with vD2 achieving AUC
higher than the best competition result on this subset.
In terms of overall accuracy on the entire set, the ensembles combining character and word based
classifiers: eCW with majority voting and eCW sel with both types of voting, achieve accuracy higher
then the best overall accuracy in the competition. They also match or surpass the best competition
accuracy on the English subset, and match the best competition accuracy on the Spanish subset. The
highest accuracy on the English subset was achieved by eC with weighted voting, eCW with majority
voting, and eCW sel with majority voting (higher than the best competition result). vF2 yields on the
Spanish subset accuracy higher than the best competition result.
For the ensembles of classifiers, on the English and Spanish subsets, the AUC for voting weighted by
the confidence scores are higher than the AUC for the majority voting, but not so on the Greek subset.
This is consistent with the fact that on the Greek subset the confidence scores for single classifier variants
yield worse ordering (AUC) than on other sets. Creation of eCW sel by removing from the ensemble
eCW the classifiers that perform worst on the training data improves the Greek results, and slightly the
English results.
We tested the statistical significance of accuracy differences between all pairs of accuracies reported
in Table 3 by the exact binomial McNemar?s test (Dietterich, 1998). Only few of these differences are
statistically significant. On the entire set these are: the difference between the accuracy of eCW with
majority voting and of eC with majority voting, vD1 and vF1, as well as the difference between the
accuracies of eCW sel with weighted voting and of vF1. On the Greek subset, this is the difference
between the accuracies of the submission (Seidman, 2013) and the lower accuracy of eC with weighted
voting.
English mod-pan12-aa-EN Greek mod-Bpc-GR
accuracy
AUC
accuracy
AUC
vD1 0.545 0.649 0.605 0.661
vD2 0.727 0.826 0.566 0.698
vF2 0.773 0.843 0.618 0.709
eC majority 0.636 0.843 0.658 0.694
weighted 0.682 0.806 0.671 0.703
eW majority 0.636 0.674 0.750 0.757
weighted 0.727 0.736 0.737 0.749
eCW majority 0.636 0.785 0.737 0.725
weighted 0.682 0.818 0.711 0.719
eCW sel majority 0.636 0.789 0.750 0.742
weighted 0.682 0.826 0.737 0.737
Table 4: Accuracy and area under ROC curve (AUC) of our method on other English and Greek datasets.
The sets were compiled by ourselves for the purpose of enriching training domain for other variant of
our classifier. The highest result in any category is bold.
The datasets mod-pan12-aa-EN and mod-Bpc-GR were compiled by ourselves from other au-
thorship attribution sets for the purpose of enriching the training corpora for our final submission vF1.
The comparison between results on the English and Greek subsets of vF1 with the results of vF2 (for
which these additional sets were not used), shows that vF2 achieved better results on English data. while
vF1 has higher AUC on Greek data.
Though these additional sets were not created specifically for authorship verification evaluation, we
394
examine the results of our methods on these sets (with the exception of vF1, which is tuned on them).
We present the results in Table 4. vD1 performs poorly on mod-pan12-aa-EN. This is in part due to
the fact that in this set the documents in a given problem instance can differ significantly with respect to
the length, and the variant vD1 does not use the preprocessing of truncation all files withing a problem
to the same length. The variants vD2 and vF2 (which apply this truncation) yielded accuracy and
AUC similar in value to the ones achieved on the PAN 2013 English subset. The ensembles containing
character n-gram classifiers yielded similar AUC on mod-pan12-aa-EN as on the PAN2013 English
subset, close in value to 0.8. But their accuracies are distinctly lower than the results on the English
competition subset, with values below 0.7 (for each such an ensemble, vast majority of the misclassified
instances are false negatives: cases classified as not written by the same person when in fact they are). For
mod-Bpc-GR the single classifiers (with parameters tuned on the competition Greek subset) perform
rather poorly, with results similar but lower in values than the results yielded on the competition Greek
test set. The ensembles containing word n-gram based classifiers perform better than the ensembles
containing only the character n-gram classifiers, yielding both AUC and accuracy in the range of 0.71 ?
0.75.
8 Future Work
It will be of interest to investigate the relation between the performance of our method and the number
and the length of the considered texts. An interesting direction indicated by results of our experiments is
also the analysis of the role of word n-grams and character n-grams for authorship verification depending
on the genre of the texts, and on the topical similarity between the documents.
9 Conclusions
We present our proximity based one-class classification method for authorship verification. The method
uses for each document of known authorship the most dissimilar document of the same author, and exam-
ines how much more or less similar is the questioned document. We use Common N-Gram dissimilarity
based on differences in frequencies of character and word n-grams.
We evaluate our method on the set of PAN 2013 Authorship Identification competition. One variant
of our method was submitted to the competition. The ordering by scores indicating the confidence that
the documents were written by the same person, yielded by our method, and evaluated by area under
ROC curve (AUC), is competitive with respect to other participants of the competition, overall, and on
the English and Spanish subsets. On the entire set, AUC by each variant of our method is higher than the
best result by other participants. In terms of accuracy, the method also performs better on the English
and Spanish subsets of the dataset, and worse on the Greek one. An ensemble combining character based
classifiers and word based classifiers yields the best accuracy, surpassing the best competition result on
the entire set and on the English subset, while matching the best competition result on the Spanish subset.
As all proximity based one-class classification algorithms, our method relies on a selected threshold on
the proximity between the questioned text and the set of documents of known authorship. Additionally,
a single classifier requires two parameters defining the features representing documents. Ensembles of
classifiers allow to alleviate the parameter tuning, by using many classifiers for many combinations of
feature defining parameters, with a threshold fixed to 1 (a natural, albeit arbitrary, value).
Acknowledgements
This research was funded by a contract from the Boeing Company, Killam Predoctoral Scholarship, and
a Collaborative Research and Development grant from the Natural Sciences and Engineering Research
Council of Canada.
References
Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
395
M.R. Ghaeini. 2013. Intrinsic Author Identification Using Modified Weighted KNN - Notebook for PAN at CLEF
2013. In Pamela Forner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ?
Working Notes Papers, September.
Tim Gollub, Martin Potthast, Anna Beyer, Matthias Busse, Francisco M. Rangel Pardo, Paolo Rosso, Efstathios
Stamatatos, and Benno Stein. 2013. Recent trends in digital text forensics and its evaluation - plagiarism
detection, author identification, and author profiling. In Pamela Forner, Henning M?uller, Roberto Paredes, Paolo
Rosso, and Benno Stein, editors, CLEF, volume 8138 of Lecture Notes in Computer Science, pages 282?302.
Springer.
Oren Halvani, Martin Steinebach, and Ralf Zimmermann. 2013. Authorship Verification via k-Nearest Neighbor
Estimation - Notebook for PAN at CLEF 2013. In Pamela Forner, Roberto Navigli, and Dan Tufis, editors,
CLEF 2013 Evaluation Labs and Workshop ? Working Notes Papers, September.
Magdalena Jankowska, Vlado Ke?selj, and Evangelos Milios. 2013. Proximity Based One-class Classification with
Common N-Gram Dissimilarity for Authorship Verification Task - Notebook for PAN at CLEF 2013. In Pamela
Forner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ? Working Notes
Papers, September.
Patrick Juola and Efstathios Stamatatos. 2013. Overview of the Author Identification Task at PAN 2013. In
Pamela Forner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ? Working
Notes Papers, September.
Patrick Juola. 2008. Authorship attribution. Foundations and Trends
R
? in Information Retrieval, 1(3):233?334.
Patrick Juola. 2012. An overview of the traditional authorship attribution subtask. In Pamela Forner, Jussi
Karlgren, and Christa Womser-Hacker, editors, CLEF (Online Working Notes/Labs/Workshop).
Vlado Ke?selj, Fuchun Peng, Nick Cercone, and Calvin Thomas. 2003. N-gram-based author profiles for au-
thorship attribution. In Proceedings of the Conference Pacific Association for Computational Linguistics, PA-
CLING?03, pages 255?264, Dalhousie University, Halifax, Nova Scotia, Canada, August.
Moshe Koppel and Jonathan Schler. 2004. Authorship verification as a one-class classification problem. In Pro-
ceedings of the 21st International Conference on Machine Learning, ICML ?04, page 489?495, Banf, Alberta,
Canada, July. ACM.
Moshe Koppel and Yaron Winter. 2014. Determining if two documents are written by the same author. Journal of
the Association for Information Science and Technology, 65(1):178?187.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2009. Computational methods in authorship attribution.
Journal of the American Society for Information Science and Technology, 60(1):9?26.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2011. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83?94, March.
Moshe Koppel, Jonathan Schler, Shlomo Argamon, and Yaron Winter. 2012. The ?Fundamental Problem? of
Authorship Attribution. English Studies, 93(3):284?291.
PAN. 2012. Dataset of PAN 2012, Author Identification task. http://www.uni-weimar.de/medien/
webis/research/events/pan-12/pan12-web/authorship.html. Accessed on Apr 2, 2013.
PAN. 2013. Dataset of PAN 2013, Author Identification task. http://www.uni-weimar.de/medien/
webis/research/events/pan-13/pan13-web/author-identification.html. Accessed
on Oct 8, 2013.
Shachar Seidman. 2013. Authorship Verification Using the Impostors Method - Notebook for PAN at CLEF
2013. In Pamela Forner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ?
Working Notes Papers, September.
Efstathios Stamatatos, George Kokkinakis, and Nikos Fakotakis. 2000. Automatic text categorization in terms of
genre and author. Computational Linguistics, 26(4):471?495, December.
Efstathios Stamatatos. 2007. Author identification using imbalanced and limited training texts. In Proceeding
of the 18th International Workshop on Database and Expert Systems Applications, DEXA?07, pages 237?241,
Regensburg, Germany, September.
Efstathios Stamatatos. 2009. A survey of modern authorship attribution methods. Journal of the American Society
for Information Science and Technology, 60(3):538?556.
396
David Tax. 2001. One Class Classification. Concept-learning in the absence of counter-examples. Ph.D. thesis,
Delft University of Technology, June.
Cor J. Veenman and Zhenshi Li. 2013. Authorship Verification with Compression Features. In Pamela Forner,
Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop ? Working Notes Papers,
September.
Alexander Ypma, Er Ypma, and Robert P.W. Duin. 1998. Support objects for domain approximation. In Pro-
ceedings of International Conference on Artificial Neural Networks, pages 2?4, Skovde, Sweden, September.
Springer.
397
