Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 915?932,
Prague, June 2007. c?2007 Association for Computational Linguistics
The CoNLL 2007 Shared Task on Dependency Parsing
Joakim Nivre?? Johan Hall? Sandra Ku?bler? Ryan McDonald??
Jens Nilsson? Sebastian Riedel?? Deniz Yuret??
?Va?xjo? University, School of Mathematics and Systems Engineering, first.last@vxu.se
?Uppsala University, Dept. of Linguistics and Philology, joakim.nivre@lingfil.uu.se
?Indiana University, Department of Linguistics, skuebler@indiana.edu
??Google Inc., ryanmcd@google.com
??University of Edinburgh, School of Informatics, S.R.Riedel@sms.ed.ac.uk
??Koc? University, Dept. of Computer Engineering, dyuret@ku.edu.tr
Abstract
The Conference on Computational Natural
Language Learning features a shared task, in
which participants train and test their learn-
ing systems on the same data sets. In 2007,
as in 2006, the shared task has been devoted
to dependency parsing, this year with both a
multilingual track and a domain adaptation
track. In this paper, we define the tasks of the
different tracks and describe how the data
sets were created from existing treebanks for
ten languages. In addition, we characterize
the different approaches of the participating
systems, report the test results, and provide
a first analysis of these results.
1 Introduction
Previous shared tasks of the Conference on Compu-
tational Natural Language Learning (CoNLL) have
been devoted to chunking (1999, 2000), clause iden-
tification (2001), named entity recognition (2002,
2003), and semantic role labeling (2004, 2005). In
2006 the shared task was multilingual dependency
parsing, where participants had to train a single
parser on data from thirteen different languages,
which enabled a comparison not only of parsing and
learning methods, but also of the performance that
can be achieved for different languages (Buchholz
and Marsi, 2006).
In dependency-based syntactic parsing, the task is
to derive a syntactic structure for an input sentence
by identifying the syntactic head of each word in the
sentence. This defines a dependency graph, where
the nodes are the words of the input sentence and the
arcs are the binary relations from head to dependent.
Often, but not always, it is assumed that all words
except one have a syntactic head, which means that
the graph will be a tree with the single independent
word as the root. In labeled dependency parsing, we
additionally require the parser to assign a specific
type (or label) to each dependency relation holding
between a head word and a dependent word.
In this year?s shared task, we continue to explore
data-driven methods for multilingual dependency
parsing, but we add a new dimension by also intro-
ducing the problem of domain adaptation. The way
this was done was by having two separate tracks: a
multilingual track using essentially the same setup
as last year, but with partly different languages, and
a domain adaptation track, where the task was to use
machine learning to adapt a parser for a single lan-
guage to a new domain. In total, test results were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). Not everyone submitted papers describ-
ing their system, and some papers describe more
than one system (or the same system in both tracks),
which explains why there are only (!) twenty-one
papers in the proceedings.
In this paper, we provide task definitions for the
two tracks (section 2), describe data sets extracted
from available treebanks (section 3), report results
for all systems in both tracks (section 4), give an
overview of approaches used (section 5), provide a
first analysis of the results (section 6), and conclude
with some future directions (section 7).
915
2 Task Definition
In this section, we provide the task definitions that
were used in the two tracks of the CoNLL 2007
Shard Task, the multilingual track and the domain
adaptation track, together with some background
and motivation for the design choices made. First
of all, we give a brief description of the data format
and evaluation metrics, which were common to the
two tracks.
2.1 Data Format and Evaluation Metrics
The data sets derived from the original treebanks
(section 3) were in the same column-based format
as for the 2006 shared task (Buchholz and Marsi,
2006). In this format, sentences are separated by a
blank line; a sentence consists of one or more to-
kens, each one starting on a new line; and a token
consists of the following ten fields, separated by a
single tab character:
1. ID: Token counter, starting at 1 for each new
sentence.
2. FORM: Word form or punctuation symbol.
3. LEMMA: Lemma or stem of word form, or an
underscore if not available.
4. CPOSTAG: Coarse-grained part-of-speech tag,
where the tagset depends on the language.
5. POSTAG: Fine-grained part-of-speech tag,
where the tagset depends on the language, or
identical to the coarse-grained part-of-speech
tag if not available.
6. FEATS: Unordered set of syntactic and/or mor-
phological features (depending on the particu-
lar language), separated by a vertical bar (|), or
an underscore if not available.
7. HEAD: Head of the current token, which is
either a value of ID or zero (0). Note that,
depending on the original treebank annotation,
there may be multiple tokens with HEAD=0.
8. DEPREL: Dependency relation to the HEAD.
The set of dependency relations depends on
the particular language. Note that, depending
on the original treebank annotation, the depen-
dency relation when HEAD=0 may be mean-
ingful or simply ROOT.
9. PHEAD: Projective head of current token,
which is either a value of ID or zero (0), or an
underscore if not available.
10. PDEPREL: Dependency relation to the
PHEAD, or an underscore if not available.
The PHEAD and PDEPREL were not used at all
in this year?s data sets (i.e., they always contained
underscores) but were maintained for compatibility
with last year?s data sets. This means that, in prac-
tice, the first six columns can be considered as input
to the parser, while the HEAD and DEPREL fields
are the output to be produced by the parser. Labeled
training sets contained all ten columns; blind test
sets only contained the first six columns; and gold
standard test sets (released only after the end of the
test period) again contained all ten columns. All data
files were encoded in UTF-8.
The official evaluation metric in both tracks was
the labeled attachment score (LAS), i.e., the per-
centage of tokens for which a system has predicted
the correct HEAD and DEPREL, but results were
also reported for unlabeled attachment score (UAS),
i.e., the percentage of tokens with correct HEAD,
and the label accuracy (LA), i.e., the percentage of
tokens with correct DEPREL. One important differ-
ence compared to the 2006 shared task is that all to-
kens were counted as ?scoring tokens?, including in
particular all punctuation tokens. The official eval-
uation script, eval07.pl, is available from the shared
task website.1
2.2 Multilingual Track
The multilingual track of the shared task was orga-
nized in the same way as the 2006 task, with an-
notated training and test data from a wide range of
languages to be processed with one and the same
parsing system. This system must therefore be able
to learn from training data, to generalize to unseen
test data, and to handle multiple languages, possi-
bly by adjusting a number of hyper-parameters. Par-
ticipants in the multilingual track were expected to
submit parsing results for all languages involved.
1http://depparse.uvt.nl/depparse-wiki/SoftwarePage
916
One of the claimed advantages of dependency
parsing, as opposed to parsing based on constituent
analysis, is that it extends naturally to languages
with free or flexible word order. This explains the
interest in recent years for multilingual evaluation
of dependency parsers. Even before the 2006 shared
task, the parsers of Collins (1997) and Charniak
(2000), originally developed for English, had been
adapted for dependency parsing of Czech, and the
parsing methodology proposed by Kudo and Mat-
sumoto (2002) and Yamada and Matsumoto (2003)
had been evaluated on both Japanese and English.
The parser of McDonald and Pereira (2006) had
been applied to English, Czech and Danish, and the
parser of Nivre et al (2007) to ten different lan-
guages. But by far the largest evaluation of mul-
tilingual dependency parsing systems so far was the
2006 shared task, where nineteen systems were eval-
uated on data from thirteen languages (Buchholz and
Marsi, 2006).
One of the conclusions from the 2006 shared task
was that parsing accuracy differed greatly between
languages and that a deeper analysis of the factors
involved in this variation was an important problem
for future research. In order to provide an extended
empirical foundation for such research, we tried to
select the languages and data sets for this year?s task
based on the following desiderata:
? The selection of languages should be typolog-
ically varied and include both new languages
and old languages (compared to 2006).
? The creation of the data sets should involve as
little conversion as possible from the original
treebank annotation, meaning that preference
should be given to treebanks with dependency
annotation.
? The training data sets should include at least
50,000 tokens and at most 500,000 tokens.2
The final selection included data from Arabic,
Basque, Catalan, Chinese, Czech, English, Greek,
Hungarian, Italian, and Turkish. The treebanks from
2The reason for having an upper bound on the training set
size was the fact that, in 2006, some participants could not train
on all the data for some languages because of time limitations.
Similar considerations also led to the decision to have a smaller
number of languages this year (ten, as opposed to thirteen).
which the data sets were extracted are described in
section 3.
2.3 Domain Adaptation Track
One well known characteristic of data-driven pars-
ing systems is that they typically perform much
worse on data that does not come from the train-
ing domain (Gildea, 2001). Due to the large over-
head in annotating text with deep syntactic parse
trees, the need to adapt parsers from domains with
plentiful resources (e.g., news) to domains with lit-
tle resources is an important problem. This prob-
lem is commonly referred to as domain adaptation,
where the goal is to adapt annotated resources from
a source domain to a target domain of interest.
Almost all prior work on domain adaptation as-
sumes one of two scenarios. In the first scenario,
there are limited annotated resources available in the
target domain, and many studies have shown that
this may lead to substantial improvements. This in-
cludes the work of Roark and Bacchiani (2003), Flo-
rian et al (2004), Chelba and Acero (2004), Daume?
and Marcu (2006), and Titov and Henderson (2006).
Of these, Roark and Bacchiani (2003) and Titov and
Henderson (2006) deal specifically with syntactic
parsing. The second scenario assumes that there are
no annotated resources in the target domain. This is
a more realistic situation and is considerably more
difficult. Recent work by McClosky et al (2006)
and Blitzer et al (2006) have shown that the exis-
tence of a large unlabeled corpus in the new domain
can be leveraged in adaptation. For this shared-task,
we are assuming the latter setting ? no annotated re-
sources in the target domain.
Obtaining adequate annotated syntactic resources
for multiple languages is already a challenging prob-
lem, which is only exacerbated when these resources
must be drawn from multiple and diverse domains.
As a result, the only language that could be feasibly
tested in the domain adaptation track was English.
The setup for the domain adaptation track was as
follows. Participants were provided with a large an-
notated corpus from the source domain, in this case
sentences from the Wall Street Journal. Participants
were also provided with data from three different
target domains: biomedical abstracts (development
data), chemical abstracts (test data 1), and parent-
child dialogues (test data 2). Additionally, a large
917
unlabeled corpus for each data set (training, devel-
opment, test) was provided. The goal of the task was
to use the annotated source data, plus any unlabeled
data, to produce a parser that is accurate for each of
the test sets from the target domains.3
Participants could submit systems in either the
?open? or ?closed? class (or both). The closed class
requires a system to use only those resources pro-
vided as part of the shared task. The open class al-
lows a system to use additional resources provided
those resources are not drawn from the same domain
as the development or test sets. An example might
be a part-of-speech tagger trained on the entire Penn
Treebank and not just the subset provided as train-
ing data, or a parser that has been hand-crafted or
trained on a different training set.
3 Treebanks
In this section, we describe the treebanks used in the
shared task and give relevant information about the
data sets created from them.
3.1 Multilingual Track
Arabic The analytical syntactic annotation
of the Prague Arabic Dependency Treebank
(PADT) (Hajic? et al, 2004) can be considered a
pure dependency annotation. The conversion, done
by Otakar Smrz, from the original format to the
column-based format described in section 2.1 was
therefore relatively straightforward, although not all
the information in the original annotation could be
transfered to the new format. PADT was one of the
treebanks used in the 2006 shared task but then only
contained about 54,000 tokens. Since then, the size
of the treebank has more than doubled, with around
112,000 tokens. In addition, the morphological
annotation has been made more informative. It
is also worth noting that the parsing units in this
treebank are in many cases larger than conventional
sentences, which partly explains the high average
number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
3Note that annotated development data for the target domain
was only provided for the development domain, biomedical ab-
stracts. For the two test domains, chemical abstracts and parent-
child dialogues, the only annotated data sets were the gold stan-
dard test sets, released only after test runs had been submitted.
Basque For Basque, we used the 3LB Basque
treebank (Aduriz et al, 2003). At present, the tree-
bank consists of approximately 3,700 sentences, 334
of which were used as test data. The treebank com-
prises literary and newspaper texts. It is annotated
in a dependency format and was converted to the
CoNLL format by a team led by Koldo Gojenola.
Catalan The Catalan section of the CESS-ECE
Syntactically and Semantically Annotated Cor-
pora (Mart?? et al, 2007) is annotated with, among
other things, constituent structure and grammatical
functions. A head percolation table was used for
automatically converting the constituent trees into
dependency trees. The original data only contains
functions related to the verb, and a function table
was used for deriving the remaining syntactic func-
tions. The conversion was performed by a team led
by Llu??s Ma`rquez and Anto`nia Mart??.
Chinese The Chinese data are taken from the
Sinica treebank (Chen et al, 2003), which con-
tains both syntactic functions and semantic func-
tions. The syntactic head was used in the conversion
to the CoNLL format, carried out by Yu-Ming Hsieh
and the organizers of the 2006 shared task, and the
syntactic functions were used wherever it was pos-
sible. The training data used is basically the same
as for the 2006 shared task, except for a few correc-
tions, but the test data is new for this year?s shared
task. It is worth noting that the parsing units in this
treebank are sometimes smaller than conventional
sentence units, which partly explains the low aver-
age number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
Czech The analytical syntactic annotation of the
Prague Dependency Treebank (PDT) (Bo?hmova? et
al., 2003) is a pure dependency annotation, just as
for PADT. It was also used in the shared task 2006,
but there are two important changes compared to
last year. First, version 2.0 of PDT was used in-
stead of version 1.0, and a conversion script was
created by Zdenek Zabokrtsky, using the new XML-
based format of PDT 2.0. Secondly, due to the upper
bound on training set size, only sections 1?3 of PDT
constitute the training data, which amounts to some
450,000 tokens. The test data is a small subset of the
development test set of PDT.
918
English For English we used the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993). In particular, we used sections 2-11 for train-
ing and a subset of section 23 for testing. As a pre-
processing stage we removed many functions tags
from the non-terminals in the phrase structure repre-
sentation to make the representations more uniform
with out-of-domain test sets for the domain adapta-
tion track (see section 3.2). The resulting data set
was then converted to dependency structures using
the procedure described in Johansson and Nugues
(2007a). This work was done by Ryan McDonald.
Greek The Greek Dependency Treebank
(GDT) (Prokopidis et al, 2005) adopts a de-
pendency structure annotation very similar to those
of PDT and PADT, which means that the conversion
by Prokopis Prokopidis was relatively straightfor-
ward. GDT is one of the smallest treebanks in
this year?s shared task (about 65,000 tokens) and
contains sentences of Modern Greek. Just like PDT
and PADT, the treebank contains more than one
level of annotation, but we only used the analytical
level of GDT.
Hungarian For the Hungarian data, the Szeged
treebank (Csendes et al, 2005) was used. The tree-
bank is based on texts from six different genres,
ranging from legal newspaper texts to fiction. The
original annotation scheme is constituent-based, fol-
lowing generative principles. It was converted into
dependencies by Zo?ltan Alexin based on heuristics.
Italian The data set used for Italian is a subset
of the balanced section of the Italian Syntactic-
Semantic Treebank (ISST) (Montemagni et al,
2003) and consists of texts from the newspaper Cor-
riere della Sera and from periodicals. A team led
by Giuseppe Attardi, Simonetta Montemagni, and
Maria Simi converted the annotation to the CoNLL
format, using information from two different anno-
tation levels, the constituent structure level and the
dependency structure level.
Turkish For Turkish we used the METU-Sabanc?
Turkish Treebank (Oflazer et al, 2003), which was
also used in the 2006 shared task. A new test set of
about 9,000 tokens was provided by Gu?ls?en Eryig?it
(Eryig?it, 2007), who also handled the conversion to
the CoNLL format, which means that we could use
all the approximately 65,000 tokens of the original
treebank for training. The rich morphology of Turk-
ish requires the basic tokens in parsing to be inflec-
tional groups (IGs) rather than words. IGs of a single
word are connected to each other deterministically
using dependency links labeled DERIV, referred to
as word-internal dependencies in the following, and
the FORM and the LEMMA fields may be empty
(they contain underscore characters in the data files).
Sentences do not necessarily have a unique root;
most internal punctuation and a few foreign words
also have HEAD=0.
3.2 Domain Adaptation Track
As mentioned previously, the source data is drawn
from a corpus of news, specifically the Wall Street
Journal section of the Penn Treebank (Marcus et al,
1993). This data set is identical to the English train-
ing set from the multilingual track (see section 3.1).
For the target domains we used three different
labeled data sets. The first two were annotated
as part of the PennBioIE project (Kulick et al,
2004) and consist of sentences drawn from either
biomedical or chemical research abstracts. Like the
source WSJ corpus, this data is annotated using the
Penn Treebank phrase structure scheme. To con-
vert these sets to dependency structures we used the
same procedure as before (Johansson and Nugues,
2007a). Additional care was taken to remove sen-
tences that contained non-WSJ part-of-speech tags
or non-terminals (e.g., HYPH part-of-speech tag in-
dicating a hyphen). Furthermore, the annotation
scheme for gaps and traces was made consistent with
the Penn Treebank wherever possible. As already
mentioned, the biomedical data set was distributed
as a development set for the training phase, while
the chemical data set was only used for final testing.
The third target data set was taken from the
CHILDES database (MacWhinney, 2000), in partic-
ular the EVE corpus (Brown, 1973), which has been
annotated with dependency structures. Unfortu-
nately the dependency labels of the CHILDES data
were inconsistent with those of the WSJ, biomedi-
cal and chemical data sets, and we therefore opted
to only evaluate unlabeled accuracy for this data
set. Furthermore, there was an inconsistency in how
main and auxiliary verbs were annotated for this data
set relative to others. As a result of this, submitting
919
Multilingual Domain adaptation
Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES
Language family Sem. Isol. Rom. Sin. Sla. Ger. Hel. F.-U. Rom. Tur. Ger.
Annotation d d c+f c+f d c+f d c+f c+f d c+f d
Training data Development data
Tokens (k) 112 51 431 337 432 447 65 132 71 65 5
Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2
Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1
LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No
No. CPOSTAG 15 25 17 13 12 31 18 16 14 14 25
No. POSTAG 21 64 54 294 59 45 38 43 28 31 37
No. FEATS 21 359 33 0 71 0 31 50 21 78 0
No. DEPREL 29 35 42 69 46 20 46 49 22 25 18
No. DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1
% HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0
% HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0
% HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0
HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0
% Non-proj. arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4
% Non-proj. sent. 10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0
Punc. attached S S A S S A S A A S A
DEPRELS for punc. 10 13 6 29 16 13 15 1 10 12 8
Test data PCHEM CHILDES
Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999
Sentences 131 334 167 690 286 214 197 390 249 300 195 666
Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9
% New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10
% New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a
Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development
set and the two test sets of the domain adaptation track.
920
results for the CHILDES data was considered op-
tional. Like the chemical data set, this data set was
only used for final testing.
Finally, a large corpus of unlabeled in-domain
data was provided for each data set and made avail-
able for training. This data was drawn from theWSJ,
PubMed.com (specific to biomedical and chemical
research literature), and the CHILDES data base.
The data was tokenized to be as consistent as pos-
sible with the WSJ training set.
3.3 Overview
Table 1 describes the characteristics of the data sets.
For the multilingual track, we provide statistics over
the training and test sets; for the domain adaptation
track, the statistics were extracted from the develop-
ment set. Following last year?s shared task practice
(Buchholz and Marsi, 2006), we use the following
definition of projectivity: An arc (i, j) is projective
iff all nodes occurring between i and j are dominated
by i (where dominates is the transitive closure of the
arc relation).
In the table, the languages are abbreviated to their
first two letters. Language families are: Semitic,
Isolate, Romance, Sino-Tibetan, Slavic, Germanic,
Hellenic, Finno-Ugric, and Turkic. The type of the
original annotation is either constituents plus (some)
functions (c+f) or dependencies (d). For the train-
ing data, the number of words and sentences are
given in multiples of thousands, and the average
length of a sentence in words (including punctua-
tion tokens). The following rows contain informa-
tion about whether lemmas are available, the num-
ber of coarse- and fine-grained part-of-speech tags,
the number of feature components, and the number
of dependency labels. Then information is given on
how many different dependency labels can co-occur
with HEAD=0, the percentage of HEAD=0 depen-
dencies, and the percentage of heads preceding (left)
or succeeding (right) a token (giving an indication of
whether a language is predominantly head-initial or
head-final). This is followed by the average number
of HEAD=0 dependencies per sentence and the per-
centage of non-projective arcs and sentences. The
last two rows show whether punctuation tokens are
attached as dependents of other tokens (A=Always,
S=Sometimes) and specify the number of depen-
dency labels that exist for punctuation tokens. Note
that punctuation is defined as any token belonging to
the UTF-8 category of punctuation. This means, for
example, that any token having an underscore in the
FORM field (which happens for word-internal IGs
in Turkish) is also counted as punctuation here.
For the test sets, the number of words and sen-
tences as well as the ratio of words per sentence are
listed, followed by the percentage of new words and
lemmas (if applicable). For the domain adaptation
sets, the percentage of new words is computed with
regard to the training set (Penn Treebank).
4 Submissions and Results
As already stated in the introduction, test runs were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). In the result tables below, systems are
identified by the last name of the teammember listed
first when test runs were uploaded for evaluation. In
general, this name is also the first author of a paper
describing the system in the proceedings, but there
are a few exceptions and complications. First of all,
for four out of twenty-seven systems, no paper was
submitted to the proceedings. This is the case for the
systems of Jia, Maes et al, Nash, and Zeman, which
is indicated by the fact that these names appear in
italics in all result tables. Secondly, two teams sub-
mitted two systems each, which are described in a
single paper by each team. Thus, the systems called
?Nilsson? and ?Hall, J.? are both described in Hall et
al. (2007a), while the systems called ?Duan (1)? and
?Duan (2)? are both described in Duan et al (2007).
Finally, please pay attention to the fact that there
are two teams, where the first author?s last name is
Hall. Therefore, we use ?Hall, J.? and ?Hall, K.?,
to disambiguate between the teams involving Johan
Hall (Hall et al, 2007a) and Keith Hall (Hall et al,
2007b), respectively.
Tables 2 and 3 give the scores for the multilingual
track in the CoNLL 2007 shared task. The Average
column contains the average score for all ten lan-
guages, which determines the ranking in this track.
Table 4 presents the results for the domain adapta-
tion track, where the ranking is determined based on
the PCHEM results only, since the CHILDES data
set was optional. Note also that there are no labeled
921
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2)
Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5)
Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1)
Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10)
Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3)
Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11)
Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7)
Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9)
Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12)
Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6)
Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15)
Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4)
Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8)
Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14)
Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17)
Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13)
Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16)
Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18)
Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19)
Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20)
Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6)
Nash 8.65(22)* 86.49(8)
Shimizu 7.20(23) 72.02(20)
Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task. Teams
are denoted by the last name of their first member, with italics indicating that there is no corresponding
paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a score
in the Average column indicates a statistically significant difference with the next lower rank.
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3)
Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2)
Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1)
Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9)
Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10)
Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5)
Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7)
Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12)
Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4)
Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13)
Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15)
Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6)
Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8)
Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14)
Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17)
Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16)
Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11)
Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18)
Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19)
Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20)
Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8)
Nash 8.77(22)* 87.71(9)
Shimizu 7.79(23) 77.91(20)
Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.
Teams are denoted by the last name of their first member, with italics indicating that there is no correspond-
ing paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a
score in the Average column indicates a statistically significant difference with the next lower rank.
922
LAS UAS
Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o
Sagae 81.06(1) 83.42(1)
Attardi 80.40(2) 83.08(3) 58.67(3)
Dredze 80.22(3) 83.38(2) 61.37(1)
Nguyen 79.50(4)* 82.04(4)*
Jia 76.48(5)* 78.92(5)* 57.43(5)
Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1)
Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)*
Zeman 50.61(8) 54.57(8) 58.89(2)
Schneider 63.01(3)* 66.53(3)* 60.27(2)
Watson 55.47(4) 62.79(4) 45.61(3)
Wu 52.89(6)
Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of
the domain adaptation track in the CoNLL 2007 shared task. Teams are denoted by the last name of their
first member, with italics indicating that there is no corresponding paper in the proceedings. The number
in parentheses next to each score gives the rank. A star next to a score in the PCHEM columns indicates a
statistically significant difference with the next lower rank.
attachment scores for the CHILDES data set, for rea-
sons explained in section 3.2. The number in paren-
theses next to each score gives the rank. A star next
to a score indicates that the difference with the next
lower rank is significant at the 5% level using a z-
test for proportions. A more complete presentation
of the results, including the significance results for
all the tasks and their p-values, can be found on the
shared task website.4
Looking first at the results in the multilingual
track, we note that there are a number of systems
performing at almost the same level at the top of the
ranking. For the average labeled attachment score,
the difference between the top score (Nilsson) and
the fifth score (Hall, J.) is no more than half a per-
centage point, and there are generally very few sig-
nificant differences among the five or six best sys-
tems, regardless of whether we consider labeled or
unlabeled attachment score. For the closed class of
the domain adaptation track, we see a very similar
pattern, with the top system (Sagae) being followed
very closely by two other systems. For the open
class, the results are more spread out, but then there
are very few results in this class. It is also worth not-
ing that the top scores in the closed class, somewhat
unexpectedly, are higher than the top scores in the
4http://nextens.uvt.nl/depparse-wiki/AllScores
open class. But before we proceed to a more detailed
analysis of the results (section 6), we will make an
attempt to characterize the approaches represented
by the different systems.
5 Approaches
In this section we give an overview of the models,
inference methods, and learning methods used in the
participating systems. For obvious reasons the dis-
cussion is limited to systems that are described by
a paper in the proceedings. But instead of describ-
ing the systems one by one, we focus on the basic
methodological building blocks that are often found
in several systems although in different combina-
tions. For descriptions of the individual systems, we
refer to the respective papers in the proceedings.
Section 5.1 is devoted to system architectures. We
then describe the two main paradigms for learning
and inference, in this year?s shared task as well as in
last year?s, which we call transition-based parsers
(section 5.2) and graph-based parsers (section 5.3),
adopting the terminology of McDonald and Nivre
(2007).5 Finally, we give an overview of the domain
adaptation methods that were used (section 5.4).
5This distinction roughly corresponds to the distinction
made by Buchholz and Marsi (2006) between ?stepwise? and
?all-pairs? approaches.
923
5.1 Architectures
Most systems perform some amount of pre- and
post-processing, making the actual parsing compo-
nent part of a sequential workflow of varying length
and complexity. For example, most transition-
based parsers can only build projective dependency
graphs. For languages with non-projective depen-
dencies, graphs therefore need to be projectivized
for training and deprojectivized for testing (Hall et
al., 2007a; Johansson and Nugues, 2007b; Titov and
Henderson, 2007).
Instead of assigning HEAD and DEPREL in a
single step, some systems use a two-stage approach
for attaching and labeling dependencies (Chen et al,
2007; Dredze et al, 2007). In the first step unlabeled
dependencies are generated, in the second step these
are labeled. This is particularly helpful for factored
parsing models, in which label decisions cannot be
easily conditioned on larger parts of the structure
due to the increased complexity of inference. One
system (Hall et al, 2007b) extends this two-stage ap-
proach to a three-stage architecture where the parser
and labeler generate an n-best list of parses which in
turn is reranked.6
In ensemble-based systems several base parsers
provide parsing decisions, which are added together
for a combined score for each potential dependency
arc. The tree that maximizes the sum of these com-
bined scores is taken as the final output parse. This
technique is used by Sagae and Tsujii (2007) and in
the Nilsson system (Hall et al, 2007a). It is worth
noting that both these systems combine transition-
based base parsers with a graph-based method for
parser combination, as first described by Sagae and
Lavie (2006).
Data-driven grammar-based parsers, such as Bick
(2007), Schneider et al (2007), and Watson and
Briscoe (2007), need pre- and post-processing in or-
der to map the dependency graphs provided as train-
ing data to a format compatible with the grammar
used, and vice versa.
5.2 Transition-Based Parsers
Transition-based parsers build dependency graphs
by performing sequences of actions, or transitions.
Both learning and inference is conceptualized in
6They also flip the order of the labeler and the reranker.
terms of predicting the correct transition based on
the current parser state and/or history. We can fur-
ther subclassify parsers with respect to the model (or
transition system) they adopt, the inference method
they use, and the learning method they employ.
5.2.1 Models
The most common model for transition-based
parsers is one inspired by shift-reduce parsing,
where a parser state contains a stack of partially
processed tokens and a queue of remaining input
tokens, and where transitions add dependency arcs
and perform stack and queue operations. This type
of model is used by the majority of transition-based
parsers (Attardi et al, 2007; Duan et al, 2007; Hall
et al, 2007a; Johansson and Nugues, 2007b; Man-
nem, 2007; Titov and Henderson, 2007; Wu et al,
2007). Sometimes it is combined with an explicit
probability model for transition sequences, which
may be conditional (Duan et al, 2007) or generative
(Titov and Henderson, 2007).
An alternative model is based on the list-based
parsing algorithm described by Covington (2001),
which iterates over the input tokens in a sequen-
tial manner and evaluates for each preceding token
whether it can be linked to the current token or not.
This model is used by Marinov (2007) and in com-
ponent parsers of the Nilsson ensemble system (Hall
et al, 2007a). Finally, two systems use models based
on LR parsing (Sagae and Tsujii, 2007; Watson and
Briscoe, 2007).
5.2.2 Inference
The most common inference technique in transition-
based dependency parsing is greedy deterministic
search, guided by a classifier for predicting the next
transition given the current parser state and history,
processing the tokens of the sentence in sequen-
tial left-to-right order7 (Hall et al, 2007a; Mannem,
2007; Marinov, 2007; Wu et al, 2007). Optionally
multiple passes over the input are conducted until no
tokens are left unattached (Attardi et al, 2007).
As an alternative to deterministic parsing, several
parsers use probabilistic models and maintain a heap
or beam of partial transition sequences in order to
pick the most probable one at the end of the sentence
7For diversity in parser ensembles, right-to-left parsers are
also used.
924
(Duan et al, 2007; Johansson and Nugues, 2007b;
Sagae and Tsujii, 2007; Titov and Henderson, 2007).
One system uses as part of their parsing pipeline a
?neighbor-parser? that attaches adjacent words and
a ?root-parser? that identifies the root word(s) of a
sentence (Wu et al, 2007). In the case of grammar-
based parsers, a classifier is used to disambiguate
in cases where the grammar leaves some ambiguity
(Schneider et al, 2007; Watson and Briscoe, 2007)
5.2.3 Learning
Transition-based parsers either maintain a classifier
that predicts the next transition or a global proba-
bilistic model that scores a complete parse. To train
these classifiers and probabilitistic models several
approaches were used: SVMs (Duan et al, 2007;
Hall et al, 2007a; Sagae and Tsujii, 2007), modified
finite Newton SVMs (Wu et al, 2007), maximum
entropy models (Sagae and Tsujii, 2007), multiclass
averaged perceptron (Attardi et al, 2007) and max-
imum likelihood estimation (Watson and Briscoe,
2007).
In order to calculate a global score or probabil-
ity for a transition sequence, two systems used a
Markov chain approach (Duan et al, 2007; Sagae
and Tsujii, 2007). Here probabilities from the output
of a classifier are multiplied over the whole sequence
of actions. This results in a locally normalized
model. Two other entries used MIRA (Mannem,
2007) or online passive-aggressive learning (Johans-
son and Nugues, 2007b) to train a globally normal-
ized model. Titov and Henderson (2007) used an in-
cremental sigmoid Bayesian network to model the
probability of a transition sequence and estimated
model parameters using neural network learning.
5.3 Graph-Based Parsers
While transition-based parsers use training data to
learn a process for deriving dependency graphs,
graph-based parsers learn a model of what it means
to be a good dependency graph given an input sen-
tence. They define a scoring or probability function
over the set of possible parses. At learning time
they estimate parameters of this function; at pars-
ing time they search for the graph that maximizes
this function. These parsers mainly differ in the
type and structure of the scoring function (model),
the search algorithm that finds the best parse (infer-
ence), and the method to estimate the function?s pa-
rameters (learning).
5.3.1 Models
The simplest type of model is based on a sum of
local attachment scores, which themselves are cal-
culated based on the dot product of a weight vector
and a feature representation of the attachment. This
type of scoring function is often referred to as a first-
order model.8 Several systems participating in this
year?s shared task used first-order models (Schiehlen
and Spranger, 2007; Nguyen et al, 2007; Shimizu
and Nakagawa, 2007; Hall et al, 2007b). Canisius
and Tjong Kim Sang (2007) cast the same type of
arc-based factorization as a weighted constraint sat-
isfaction problem.
Carreras (2007) extends the first-order model to
incorporate a sum over scores for pairs of adjacent
arcs in the tree, yielding a second-order model. In
contrast to previous work where this was constrained
to sibling relations of the dependent (McDonald and
Pereira, 2006), here head-grandchild relations can
be taken into account.
In all of the above cases the scoring function is
decomposed into functions that score local proper-
ties (arcs, pairs of adjacent arcs) of the graph. By
contrast, the model of Nakagawa (2007) considers
global properties of the graph that can take multi-
ple arcs into account, such as multiple siblings and
children of a node.
5.3.2 Inference
Searching for the highest scoring graph (usually a
tree) in a model depends on the factorization cho-
sen and whether we are looking for projective or
non-projective trees. Maximum spanning tree al-
gorithms can be used for finding the highest scor-
ing non-projective tree in a first-order model (Hall
et al, 2007b; Nguyen et al, 2007; Canisius and
Tjong Kim Sang, 2007; Shimizu and Nakagawa,
2007), while Eisner?s dynamic programming algo-
rithm solves the problem for a first-order factoriza-
tion in the projective case (Schiehlen and Spranger,
2007). Carreras (2007) employs his own exten-
sion of Eisner?s algorithm for the case of projective
trees and second-order models that include head-
grandparent relations.
8It is also known as an edge-factored model.
925
The methods presented above are mostly efficient
and always exact. However, for models that take
global properties of the tree into account, they can-
not be applied. Instead Nakagawa (2007) uses Gibbs
sampling to obtain marginal probabilities of arcs be-
ing included in the tree using his global model and
then applies a maximum spanning tree algorithm to
maximize the sum of the logs of these marginals and
return a valid cycle-free parse.
5.3.3 Learning
Most of the graph-based parsers were trained using
an online inference-based method such as passive-
aggressive learning (Nguyen et al, 2007; Schiehlen
and Spranger, 2007), averaged perceptron (Carreras,
2007), or MIRA (Shimizu and Nakagawa, 2007),
while some systems instead used methods based on
maximum conditional likelihood (Nakagawa, 2007;
Hall et al, 2007b).
5.4 Domain Adaptation
5.4.1 Feature-Based Approaches
One way of adapting a learner to a new domain with-
out using any unlabeled data is to only include fea-
tures that are expected to transfer well (Dredze et
al., 2007). In structural correspondence learning a
transformation from features in the source domain
to features of the target domain is learnt (Shimizu
and Nakagawa, 2007). The original source features
along with their transformed versions are then used
to train a discriminative parser.
5.4.2 Ensemble-Based Approaches
Dredze et al (2007) trained a diverse set of parsers
in order to improve cross-domain performance by
incorporating their predictions as features for an-
other classifier. Similarly, two parsers trained with
different learners and search directions were used
in the co-learning approach of Sagae and Tsujii
(2007). Unlabeled target data was processed with
both parsers. Sentences that both parsers agreed on
were then added to the original training data. This
combined data set served as training data for one of
the original parsers to produce the final system. In
a similar fashion, Watson and Briscoe (2007) used a
variant of self-training to make use of the unlabeled
target data.
5.4.3 Other Approaches
Attardi et al (2007) learnt tree revision rules for the
target domain by first parsing unlabeled target data
using a strong parser; this data was then combined
with labeled source data; a weak parser was applied
to this new dataset; finally tree correction rules are
collected based on the mistakes of the weak parser
with respect to the gold data and the output of the
strong parser.
Another technique used was to filter sentences of
the out-of-domain corpus based on their similarity
to the target domain, as predicted by a classifier
(Dredze et al, 2007). Only if a sentence was judged
similar to target domain sentences was it included in
the training set.
Bick (2007) used a hybrid approach, where a data-
driven parser trained on the labeled training data was
given access to the output of a Constraint Grammar
parser for English run on the same data. Finally,
Schneider et al (2007) learnt collocations and rela-
tional nouns from the unlabeled target data and used
these in their parsing algorithm.
6 Analysis
Having discussed the major approaches taken in the
two tracks of the shared task, we will now return to
the test results. For the multilingual track, we com-
pare results across data sets and across systems, and
report results from a parser combination experiment
involving all the participating systems (section 6.1).
For the domain adaptation track, we sum up the most
important findings from the test results (section 6.2).
6.1 Multilingual Track
6.1.1 Across Data Sets
The average LAS over all systems varies from 68.07
for Basque to 80.95 for English. Top scores vary
from 76.31 for Greek to 89.61 for English. In gen-
eral, there is a good correlation between the top
scores and the average scores. For Greek, Italian,
and Turkish, the top score is closer to the average
score than the average distance, while for Czech, the
distance is higher. The languages that produced the
most stable results in terms of system ranks with re-
spect to LAS are Hungarian and Italian. For UAS,
Catalan also falls into this group. The language that
926
Setup Arabic Chinese Czech Turkish
2006 without punctuation 66.9 90.0 80.2 65.7
2007 without punctuation 75.5 84.9 80.0 71.6
2006 with punctuation 67.0 90.0 80.2 73.8
2007 with punctuation 76.5 84.7 80.2 79.8
Table 5: A comparison of the LAS top scores from 2006 and 2007. Official scoring conditions in boldface.
For Turkish, scores with punctuation also include word-internal dependencies.
produced the most unstable results with respect to
LAS is Turkish.
In comparison to last year?s languages, the lan-
guages involved in the multilingual track this year
can be more easily separated into three classes with
respect to top scores:
? Low (76.31?76.94):
Arabic, Basque, Greek
? Medium (79.19?80.21):
Czech, Hungarian, Turkish
? High (84.40?89.61):
Catalan, Chinese, English, Italian
It is interesting to see that the classes are more easily
definable via language characteristics than via char-
acteristics of the data sets. The split goes across
training set size, original data format (constituent
vs. dependency), sentence length, percentage of un-
known words, number of dependency labels, and ra-
tio of (C)POSTAGS and dependency labels. The
class with the highest top scores contains languages
with a rather impoverished morphology. Medium
scores are reached by the two agglutinative lan-
guages, Hungarian and Turkish, as well as by Czech.
The most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection. Based on these characteristics, one would
expect to find Czech in the last class. However, the
Czech training set is four times the size of the train-
ing set for Arabic, which is the language with the
largest training set of the difficult languages.
However, it would be wrong to assume that train-
ing set size alone is the deciding factor. A closer
look at table 1 shows that while Basque and Greek
in fact have small training data sets, so do Turk-
ish and Italian. Another factor that may be asso-
ciated with the above classification is the percent-
age of new words (PNW) in the test set. Thus, the
expectation would be that the highly inflecting lan-
guages have a high PNW while the languages with
little morphology have a low PNW. But again, there
is no direct correspondence. Arabic, Basque, Cata-
lan, English, and Greek agree with this assumption:
Catalan and English have the smallest PNW, and
Arabic, Basque, and Greek have a high PNW. But
the PNW for Italian is higher than for Arabic and
Greek, and this is also true for the percentage of
new lemmas. Additionally, the highest PNW can be
found in Hungarian and Turkish, which reach higher
scores than Arabic, Basque, and Greek. These con-
siderations suggest that highly inflected languages
with (relatively) free word order need more training
data, a hypothesis that will have to be investigated
further.
There are four languages which were included in
the shared tasks on multilingual dependency pars-
ing both at CoNLL 2006 and at CoNLL 2007: Ara-
bic, Chinese, Czech, and Turkish. For all four lan-
guages, the same treebanks were used, which allows
a comparison of the results. However, in some cases
the size of the training set changed, and at least one
treebank, Turkish, underwent a thorough correction
phase. Table 5 shows the top scores for LAS. Since
the official scores excluded punctuation in 2006 but
includes it in 2007, we give results both with and
without punctuation for both years.
For Arabic and Turkish, we see a great improve-
ment of approximately 9 and 6 percentage points.
For Arabic, the number of tokens in the training
set doubled, and the morphological annotation was
made more informative. The combined effect of
these changes can probably account for the substan-
tial improvement in parsing accuracy. For Turkish,
the training set grew in size as well, although only by
600 sentences, but part of the improvement for Turk-
ish may also be due to continuing efforts in error cor-
927
rection and consistency checking. We see that the
choice to include punctuation or not makes a large
difference for the Turkish scores, since non-final IGs
of a word are counted as punctuation (because they
have the underscore character as their FORM value),
which means that word-internal dependency links
are included if punctuation is included.9 However,
regardless of whether we compare scores with or
without punctuation, we see a genuine improvement
of approximately 6 percentage points.
For Chinese, the same training set was used.
Therefore, the drop from last year?s top score to this
year?s is surprising. However, last year?s top scor-
ing system for Chinese (Riedel et al, 2006), which
did not participate this year, had a score that was
more than 3 percentage points higher than the sec-
ond best system for Chinese. Thus, if we compare
this year?s results to the second best system, the dif-
ference is approximately 2 percentage points. This
final difference may be attributed to the properties of
the test sets. While last year?s test set was taken from
the treebank, this year?s test set contains texts from
other sources. The selection of the textual basis also
significantly changed average sentence length: The
Chinese training set has an average sentence length
of 5.9. Last year?s test set alo had an average sen-
tence length of 5.9. However, this year, the average
sentence length is 7.5 tokens, which is a significant
increase. Longer sentences are typically harder to
parse due to the increased likelihood of ambiguous
constructions.
Finally, we note that the performance for Czech
is almost exactly the same as last year, despite the
fact that the size of the training set has been reduced
to approximately one third of last year?s training set.
It is likely that this in fact represents a relative im-
provement compared to last year?s results.
6.1.2 Across Systems
The LAS over all languages ranges from 80.32 to
54.55. The comparison of the system ranks aver-
aged over all languages with the ranks for single lan-
9The decision to include word-internal dependencies in this
way can be debated on the grounds that they can be parsed de-
terministically. On the other hand, they typically correspond to
regular dependencies captured by function words in other lan-
guages, which are often easy to parse as well. It is therefore
unclear whether scores are more inflated by including word-
internal dependencies or deflated by excluding them.
guages show considerably more variation than last
year?s systems. Buchholz and Marsi (2006) report
that ?[f]or most parsers, their ranking differs at most
a few places from their overall ranking?. This year,
for all of the ten best performing systems with re-
spect to LAS, there is at least one language for which
their rank is at least 5 places different from their
overall rank. The most extreme case is the top per-
forming Nilsson system (Hall et al, 2007a), which
reached rank 1 for five languages and rank 2 for
two more languages. Their only outlier is for Chi-
nese, where the system occupies rank 14, with a
LAS approximately 9 percentage points below the
top scoring system for Chinese (Sagae and Tsujii,
2007). However, Hall et al (2007a) point out that
the official results for Chinese contained a bug, and
the true performance of their system was actually
much higher. The greatest improvement of a sys-
tem with respect to its average rank occurs for En-
glish, for which the system by Nguyen et al (2007)
improved from the average rank 15 to rank 6. Two
more outliers can be observed in the system of Jo-
hansson and Nugues (2007b), which improves from
its average rank 12 to rank 4 for Basque and Turkish.
The authors attribute this high performance to their
parser?s good performance on small training sets.
However, this hypothesis is contradicted by their re-
sults for Greek and Italian, the other two languages
with small training sets. For these two languages,
the system?s rank is very close to its average rank.
6.1.3 An Experiment in System Combination
Having the outputs of many diverse dependency
parsers for standard data sets opens up the interest-
ing possibility of parser combination. To combine
the outputs of each parser we used the method of
Sagae and Lavie (2006). This technique assigns to
each possible labeled dependency a weight that is
equal to the number of systems that included the de-
pendency in their output. This can be viewed as
an arc-based voting scheme. Using these weights
it is possible to search the space of possible depen-
dency trees using directed maximum spanning tree
algorithms (McDonald et al, 2005). The maximum
spanning tree in this case is equal to the tree that on
average contains the labeled dependencies that most
systems voted for. It is worth noting that variants
of this scheme were used in two of the participating
928
5 10 15 20Number of Systems
80
82
84
86
88
Accu
racy
Unlabeled AccuracyLabeled Accuracy
Figure 1: System Combination
systems, the Nilsson system (Hall et al, 2007a) and
the system of Sagae and Tsujii (2007).
Figure 1 plots the labeled and unlabeled accura-
cies when combining an increasing number of sys-
tems. The data used in the plot was the output of all
competing systems for every language in the mul-
tilingual track. The plot was constructed by sort-
ing the systems based on their average labeled accu-
racy scores over all languages, and then incremen-
tally adding each system in descending order.10 We
can see that both labeled and unlabeled accuracy are
significantly increased, even when just the top three
systems are included. Accuracy begins to degrade
gracefully after about ten different parsers have been
added. Furthermore, the accuracy never falls below
the performance of the top three systems.
6.2 Domain Adaptation Track
For this task, the results are rather surprising. A look
at the LAS and UAS for the chemical research ab-
stracts shows that there are four closed systems that
outperform the best scoring open system. The best
system (Sagae and Tsujii, 2007) reaches an LAS of
81.06 (in comparison to their LAS of 89.01 for the
English data set in the multilingual track). Consider-
ing that approximately one third of the words of the
chemical test set are new, the results are noteworthy.
The next surprise is to be found in the relatively
low UAS for the CHILDES data. At a first glance,
this data set has all the characteristics of an easy
10The reason that there is no data point for two parsers is
that the simple voting scheme adopted only makes sense with at
least three parsers voting.
set; the average sentence is short (12.9 words), and
the percentage of new words is also small (6.10%).
Despite these characteristics, the top UAS reaches
62.49 and is thus more than 10 percentage points
below the top UAS for the chemical data set. One
major reason for this is that auxiliary and main
verb dependencies are annotated differently in the
CHILDES data than in the WSJ training set. As a
result of this discrepancy, participants were not re-
quired to submit results for the CHILDES data. The
best performing system on the CHILDES corpus is
an open system (Bick, 2007), but the distance to
the top closed system is approximately 1 percent-
age point. In this domain, it seems more feasible to
use general language resources than for the chemi-
cal domain. However, the results prove that the extra
effort may be unnecessary.
7 Conclusion
Two years of dependency parsing in the CoNLL
shared task has brought an enormous boost to the
development of dependency parsers for multiple lan-
guages (and to some extent for multiple domains).
But even though nineteen languages have been cov-
ered by almost as many different parsing and learn-
ing approaches, we still have only vague ideas about
the strengths and weaknesses of different methods
for languages with different typological characteris-
tics. Increasing our knowledge of the multi-causal
relationship between language structure, annotation
scheme, and parsing and learning methods probably
remains the most important direction for future re-
search in this area. The outputs of all systems for all
data sets from the two shared tasks are freely avail-
able for research and constitute a potential gold mine
for comparative error analysis across languages and
systems.
For domain adaptation we have barely scratched
the surface so far. But overcoming the bottleneck
of limited annotated resources for specialized do-
mains will be as important for the deployment of
human language technology as being able to handle
multiple languages in the future. One result from
the domain adaptation track that may seem surpris-
ing at first is the fact that closed class systems out-
performed open class systems on the chemical ab-
stracts. However, it seems that the major problem in
929
adapting pre-existing parsers to the new domain was
not the domain as such but the mapping from the
native output of the parser to the kind of annotation
provided in the shared task data sets. Thus, find-
ing ways of reusing already invested development
efforts by adapting the outputs of existing systems
to new requirements, without substantial loss in ac-
curacy, seems to be another line of research that may
be worth pursuing.
Acknowledgments
First and foremost, we want to thank all the peo-
ple and organizations that generously provided us
with treebank data and helped us prepare the data
sets and without whom the shared task would have
been literally impossible: Otakar Smrz, Charles
University, and the LDC (Arabic); Maxux Aranz-
abe, Kepa Bengoetxea, Larraitz Uria, Koldo Go-
jenola, and the University of the Basque Coun-
try (Basque); Ma. Anto`nia Mart?? Anton??n, Llu??s
Ma`rquez, Manuel Bertran, Mariona Taule?, Difda
Monterde, Eli Comelles, and CLiC-UB (Cata-
lan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming
Hsieh, and Academia Sinica (Chinese); Jan Hajic?,
Zdenek Zabokrtsky, Charles University, and the
LDC (Czech); Brian MacWhinney, Eric Davis, the
CHILDES project, the Penn BioIE project, and
the LDC (English); Prokopis Prokopidis and ILSP
(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun-
garian); Giuseppe Attardi, Simonetta Montemagni,
Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril
Ribarov, Alessandro Lenci, Nicoletta Calzolari,
ILC, and ELRA (Italian); Gu?ls?en Eryig?it, Kemal
Oflazer, and Ruket C?ak?c? (Turkish).
Secondly, we want to thank the organizers of last
year?s shared task, Sabine Buchholz, Amit Dubey,
Erwin Marsi, and Yuval Krymolowski, who solved
all the really hard problems for us and answered all
our questions, as well as our colleagues who helped
review papers: Jason Baldridge, Sabine Buchholz,
James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju-
lia Hockenmaier, Yuval Krymolowski, Erwin Marsi,
Bea?ta Megyesi, Yannick Versley, and Alexander
Yeh. Special thanks to Bertjan Busser and Erwin
Marsi for help with the CoNLL shared task website
and many other things, and to Richard Johansson for
letting us use his conversion tool for English.
Thirdly, we want to thank the program chairs
for EMNLP-CoNLL 2007, Jason Eisner and Taku
Kudo, the publications chair, Eric Ringger, the
SIGNLL officers, Antal van den Bosch, Hwee Tou
Ng, and Erik Tjong Kim Sang, and members of the
LDC staff, Tony Castelletto and Ilya Ahtaridis, for
great cooperation and support.
Finally, we want to thank the following people,
who in different ways assisted us in the organi-
zation of the CoNLL 2007 shared task: Giuseppe
Attardi, Eckhard Bick, Matthias Buch-Kromann,
Xavier Carreras, Tomaz Erjavec, Svetoslav Mari-
nov, Wolfgang Menzel, Xue Nianwen, Gertjan van
Noord, Petya Osenova, Florian Schiel, Kiril Simov,
Zdenka Uresova, and Heike Zinsmeister.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT).
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
E. Bick. 2007. Hybrid ways to improve domain inde-
pendence in an ML dependency parser. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
the Tenth Conf. on Computational Natural Language
Learning (CoNLL).
S. Canisius and E. Tjong Kim Sang. 2007. A constraint
satisfaction approach to dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
930
X. Carreras. 2007. Experiments with a high-order pro-
jective dependency parser. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of the First Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
W. Chen, Y. Zhang, and H. Isahara. 2007. A two-stage
parser for multilingual dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
H. Daume? and D. Marcu. 2006. Domain adaptation for
statistical classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
M. Dredze, J. Blitzer, P. P. Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly hard do-
main adaptation for dependency parsing. In Proc. of
the CoNLL 2007 Shared Task. EMNLP-CoNLL.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
G. Eryig?it. 2007. ITU validation set for Metu-Sabanc?
Turkish Treebank. URL: http://www3.itu.edu.tr/
?gulsenc/papers/validationset.pdf.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, A. Luo, N. Nicolov, and S. Roukos. 2004. A
statisical model for multilingual entity detection and
tracking. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP).
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools.
J. Hall, J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007a. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
K. Hall, J. Havelka, and D. Smith. 2007b. Log-linear
models of non-projective trees, k-best MST parsing
and tree-ranking. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Johansson and P. Nugues. 2007a. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conf. on Computational Lin-
guistics (NODALIDA).
R. Johansson and P. Nugues. 2007b. Incremental depen-
dency parsing using online learning. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of the Sixth
Conf. on Computational Language Learning (CoNLL).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
P. R. Mannem. 2007. Online learning for determinis-
tic dependency parsing. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
S. Marinov. 2007. Covington variations. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
931
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of the 11th Conf. of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Methods
in Natural Language Processing (HLT/EMNLP).
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
T. Nakagawa. 2007. Multilingual dependency parsing
using Gibbs sampling. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
L.-M. Nguyen, T.-P. Nguyen, and A. Shimazu. 2007. A
multilingual dependency analysis system using online
passive-aggressive learning. In Proc. of the CoNLL
2007 Shared Task. EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13:95?135.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT).
S. Riedel, R. C?ak?c?, and I. Meza-Ruiz. 2006. Multi-
lingual dependency parsing with incremental integer
linear programming. In Proc. of the Tenth Conf. on
Computational Natural Language Learning (CoNLL).
B. Roark and M. Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In
Proc. of the Human Language Technology Conf. and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conf. of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT/NAACL).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
M. Schiehlen and Kristina Spranger. 2007. Global learn-
ing of labelled dependency trees. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
G. Schneider, K. Kaljurand, F. Rinaldi, and T. Kuhn.
2007. Pro3Gres parser in the CoNLL domain adap-
tation shared task. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
N. Shimizu and H. Nakagawa. 2007. Structural corre-
spondence learning for dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL).
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Watson and T. Briscoe. 2007. Adapting the RASP
system for the CoNLL07 domain-adaptation task. In
Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
Y.-C. Wu, J.-C. Yang, and Y.-S. Lee. 2007. Multi-
lingual deterministic dependency parsing framework
using modified finite Newton method support vector
machines. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT).
932
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 328?334,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning Morphological Disambiguation Rules for Turkish
Deniz Yuret
Dept. of Computer Engineering
Koc? University
Istanbul, Turkey
dyuret@ku.edu.tr
Ferhan Tu?re
Dept. of Computer Engineering
Koc? University
Istanbul, Turkey
fture@ku.edu.tr
Abstract
In this paper, we present a rule based
model for morphological disambiguation
of Turkish. The rules are generated by a
novel decision list learning algorithm us-
ing supervised training. Morphological
ambiguity (e.g. lives = live+s or life+s)
is a challenging problem for agglutinative
languages like Turkish where close to half
of the words in running text are morpho-
logically ambiguous. Furthermore, it is
possible for a word to take an unlimited
number of suffixes, therefore the number
of possible morphological tags is unlim-
ited. We attempted to cope with these
problems by training a separate model for
each of the 126 morphological features
recognized by the morphological analyzer.
The resulting decision lists independently
vote on each of the potential parses of a
word and the final parse is selected based
on our confidence on these votes. The
accuracy of our model (96%) is slightly
above the best previously reported results
which use statistical models. For compari-
son, when we train a single decision list on
full tags instead of using separate models
on each feature we get 91% accuracy.
1 Introduction
Morphological disambiguation is the task of select-
ing the correct morphological parse for a given word
in a given context. The possible parses of a word
are generated by a morphological analyzer. In Turk-
ish, close to half the words in running text are mor-
phologically ambiguous. Below is a typical word
?masal?? with three possible parses.
masal+Noun+A3sg+Pnon+Acc (= the story)
masal+Noun+A3sg+P3sg+Nom (= his story)
masa+Noun+A3sg+Pnon+Nom?DB+Adj+With
(= with tables)
Table 1: Three parses of the word ?masal??
The first two parses start with the same root,
masal (= story, fable), but the interpretation of the
following +? suffix is the Accusative marker in one
case, and third person possessive agreement in the
other. The third parse starts with a different root,
masa (= table) followed by a derivational suffix +l?
(= with) which turns the noun into an adjective. The
symbol ?DB represents a derivational boundary and
splits the parse into chunks called inflectional groups
(IGs).1
We will use the term feature to refer to individual
morphological features like +Acc and +With; the
term IG to refer to groups of features split by deriva-
tional boundaries (?DB), and the term tag to refer to
the sequence of IGs following the root.
Morphological disambiguation is a useful first
step for higher level analysis of any language but it
is especially critical for agglutinative languages like
Turkish, Czech, Hungarian, and Finnish. These lan-
guages have a relatively free constituent order, and
1See (Oflazer et al, 1999) for a detailed description of the
morphological features used in this paper.
328
syntactic relations are partly determined by morpho-
logical features. Many applications including syn-
tactic parsing, word sense disambiguation, text to
speech synthesis and spelling correction depend on
accurate analyses of words.
An important qualitative difference between part
of speech tagging in English and morphological dis-
ambiguation in an agglutinative language like Turk-
ish is the number of possible tags that can be as-
signed to a word. Typical English tag sets include
less than a hundred tag types representing syntac-
tic and morphological information. The number of
potential morphological tags in Turkish is theoret-
ically unlimited. We have observed more than ten
thousand tag types in our training corpus of a mil-
lion words. The high number of possible tags poses
a data sparseness challenge for the typical machine
learning approach, somewhat akin to what we ob-
serve in word sense disambiguation.
One way out of this dilemma could be to ignore
the detailed morphological structure of the word and
focus on determining only the major and minor parts
of speech. However (Oflazer et al, 1999) observes
that the modifier words in Turkish can have depen-
dencies to any one of the inflectional groups of a
derived word. For example, in ?mavi masal? oda? (=
the room with a blue table) the adjective ?mavi? (=
blue) modifies the noun root ?masa? (= table) even
though the final part of speech of ?masal?? is an ad-
jective. Therefore, the final part of speech and in-
flection of a word do not carry sufficient information
for the identification of the syntactic dependencies
it is involved in. One needs the full morphological
analysis.
Our approach to the data sparseness problem is
to consider each morphological feature separately.
Even though the number of potential tags is un-
limited, the number of morphological features is
small: The Turkish morphological analyzer we use
(Oflazer, 1994) produces tags that consist of 126
unique features. For each unique feature f , we take
the subset of the training data in which one of the
parses for each instance contain f . We then split this
subset into positive and negative examples depend-
ing on whether the correct parse contains the feature
f . These examples are used to learn rules using the
Greedy Prepend Algorithm (GPA), a novel decision
list learner.
To predict the tag of an unknown word, first the
morphological analyzer is used to generate all its
possible parses. The decision lists are then used to
predict the presence or absence of each of the fea-
tures contained in the candidate parses. The results
are probabilistically combined taking into account
the accuracy of each decision list to select the best
parse. The resulting tagging accuracy is 96% on a
hand tagged test set.
A more direct approach would be to train a single
decision list using the full tags as the target classifi-
cation. Given a word in context, such a decision list
assigns a complete morphological tag instead of pre-
dicting individual morphological features. As such,
it does not need the output of a morphological ana-
lyzer and should be considered a tagger rather than
a disambiguator. For comparison, such a decision
list was built, and its accuracy was determined to be
91% on the same test set.
The main reason we chose to work with decision
lists and the GPA algorithm is their robustness to ir-
relevant or redundant features. The input to the deci-
sion lists include the suffixes of all possible lengths
and character type information within a five word
window. Each instance ends up with 40 attributes on
average which are highly redundant and mostly irrel-
evant. GPA is able to sort out the relevant features
automatically and build a fairly accurate model. Our
experiments with Naive Bayes resulted in a signif-
icantly worse performance. Typical statistical ap-
proaches include the tags of the previous words as
inputs in the model. GPA was able to deliver good
performance without using the previous tags as in-
puts, because it was able to extract equivalent infor-
mation implicit in the surface attributes. Finally, un-
like most statistical approaches, the resulting models
of GPA are human readable and open to interpreta-
tion as Section 3.1 illustrates.
The next section will review related work. Sec-
tion 3 introduces decision lists and the GPA training
algorithm. Section 4 presents the experiments and
the results.
2 Related Work
There is a large body of work on morphological dis-
ambiguation and part of speech tagging using a va-
riety of rule-based and statistical approaches. In the
329
rule-based approach a large number of hand crafted
rules are used to select the correct morphological
parse or POS tag of a given word in a given context
(Karlsson et al, 1995; Oflazer and Tu?r, 1997). In
the statistical approach a hand tagged corpus is used
to train a probabilistic model which is then used to
select the best tags in unseen text (Church, 1988;
Hakkani-Tu?r et al, 2002). Examples of statisti-
cal and machine learning approaches that have been
used for tagging include transformation based learn-
ing (Brill, 1995), memory based learning (Daele-
mans et al, 1996), and maximum entropy models
(Ratnaparkhi, 1996). It is also possible to train sta-
tistical models using unlabeled data with the ex-
pectation maximization algorithm (Cutting et al,
1992). Van Halteren (1999) gives a comprehensive
overview of syntactic word-class tagging.
Previous work on morphological disambiguation
of inflectional or agglutinative languages include
unsupervised learning for of Hebrew (Levinger
et al, 1995), maximum entropy modeling for Czech
(Hajic? and Hladka?, 1998), combination of statistical
and rule-based disambiguation methods for Basque
(Ezeiza et al, 1998), transformation based tagging
for Hungarian (Megyesi, 1999).
Early work on Turkish used a constraint-based ap-
proach with hand crafted rules (Oflazer and Kuruo?z,
1994). A purely statistical morphological disam-
biguation model was recently introduced (Hakkani-
Tu?r et al, 2002). To counter the data sparseness
problem the morphological parses are split across
their derivational boundaries and certain indepen-
dence assumptions are made in the prediction of
each inflectional group.
A combination of three ideas makes our approach
unique in the field: (1) the use of decision lists and
a novel learning algorithm that combine the statis-
tical and rule based techniques, (2) the treatment of
each individual feature separately to address the data
sparseness problem, and (3) the lack of dependence
on previous tags and relying on surface attributes
alone.
3 Decision Lists
We introduce a new method for morphological dis-
ambiguation based on decision lists. A decision list
is an ordered list of rules where each rule consists
of a pattern and a classification (Rivest, 1987). In
our application the pattern specifies the surface at-
tributes of the words surrounding the target such as
suffixes and character types (e.g. upper vs. lower
case, use of punctuation, digits). The classification
indicates the presence or absence of a morphological
feature for the center word.
3.1 A Sample Decision List
We will explain the rules and their patterns using the
sample decision list in Table 2 trained to identify the
feature +Det (determiner).
Rule Class Pattern
1 1 W=?c?ok R1=+DA
2 1 L1=?pek
3 0 W=+AzI
4 0 W=?c?ok
5 1 ?
Table 2: A five rule decision list for +Det
The value in the class column is 1 if word W
should have a +Det feature and 0 otherwise. The
pattern column describes the required attributes of
the words surrounding the target word for the rule
to match. The last (default) rule has no pattern,
matches every instance, and assigns them +Det.
This default rule captures the behavior of the ma-
jority of the training instances which had +Det in
their correct parse. Rule 4 indicates a common
exception: the frequently used word ?c?ok? (mean-
ing very) should not be assigned +Det by default:
?c?ok? can be also used as an adjective, an adverb,
or a postposition. Rule 1 introduces an exception to
rule 4: if the right neighbor R1 ends with the suffix
+DA (the locative suffix) then ?c?ok? should receive
+Det. The meanings of various symbols in the pat-
terns are described below.
When the decision list is applied to a window of
words, the rules are tried in the order from the most
specific (rule 1) to the most general (rule 5). The first
rule that matches is used to predict the classification
of the center word. The last rule acts as a catch-all;
if none of the other rules have matched, this rule as-
signs the instance a default classification. For exam-
ple, the five rule decision list given above classifies
the middle word in ?pek c?ok alanda? (matches rule
330
W target word A [ae]
L1, L2 left neighbors I [?iuu?]
R1, R2 right neighbors D [dt]
== exact match B [bp]
=? case insensitive match C [cc?]
=+ is a suffix of K [kgg?]
Table 3: Symbols used in the rule patterns. Capital
letters on the right represent character groups useful
in identifying phonetic variations of certain suffixes,
e.g. the locative suffix +DA can surface as +de, +da,
+te, or +ta depending on the root word ending.
1) and ?pek c?ok insan? (matches rule 2) as +Det,
but ?insan c?ok daha? (matches rule 4) as not +Det.
One way to interpret a decision list is as a se-
quence of if-then-else constructs familiar from pro-
gramming languages. Another way is to see the last
rule as the default classification, the previous rule as
specifying a set of exceptions to the default, the rule
before that as specifying exceptions to these excep-
tions and so on.
3.2 The Greedy Prepend Algorithm (GPA)
To learn a decision list from a given set of training
examples the general approach is to start with a de-
fault rule or an empty decision list and keep adding
the best rule to cover the unclassified or misclassi-
fied examples. The new rules can be added to the
end of the list (Clark and Niblett, 1989), the front of
the list (Webb and Brkic, 1993), or other positions
(Newlands and Webb, 2004). Other design decisions
include the criteria used to select the ?best rule? and
how to search for it.
The Greedy Prepend Algorithm (GPA) is a variant
of the PREPEND algorithm (Webb and Brkic, 1993).
It starts with a default rule that matches all instances
and classifies them using the most common class in
the training data. Then it keeps prepending the rule
with the maximum gain to the front of the grow-
ing decision list until no further improvement can be
made. The algorithm can be described as follows:
GPA(data)
1 dlist ? NIL
2 default -class ? MOST-COMMON-CLASS(data)
3 rule ? [if TRUE then default -class ]
4 while GAIN(rule , dlist , data) > 0
5 do dlist ? prepend(rule , dlist)
6 rule ? MAX-GAIN-RULE(dlist , data)
7 return dlist
The gain of a candidate rule in GPA is defined
as the increase in the number of correctly classified
instances in the training set as a result of prepend-
ing the rule to the existing decision list. This is
in contrast with the original PREPEND algorithm
which uses the less direct Laplace preference func-
tion (Webb and Brkic, 1993; Clark and Boswell,
1991).
To find the next rule with the maximum gain, GPA
uses a heuristic search algorithm. Candidate rules
are generated by adding a single new attribute to the
pattern of each rule already in the decision list. The
candidate with the maximum gain is prepended to
the decision list and the process is repeated until no
more positive gain rules can be found. Note that if
the best possible rule has more than one extra at-
tribute compared to the existing rules in the decision
list, a suboptimal rule will be selected. The origi-
nal PREPEND uses an admissible search algorithm,
OPUS, which is guaranteed to find the best possible
candidate (Webb, 1995), but we found OPUS to be
too slow to be practical for a problem of this scale.
We picked GPA for the morphological disam-
biguation problem because we find it to be fast and
fairly robust to the existence of irrelevant or redun-
dant attributes. The average training instance has
40 attributes describing the suffixes of all possible
lengths and character type information in a five word
window. Most of this information is redundant or
irrelevant to the problem at hand. The number of
distinct attributes is on the order of the number of
distinct word-forms in the training set. Nevertheless
GPA is able to process a million training instances
for each of the 126 unique morphological features
and produce a model with state of the art accuracy
in about two hours on a regular desktop PC.2
2Pentium 4 CPU 2.40GHz
331
4 Experiments and Results
In this section we present the details of the data,
the training and testing procedures, the surface at-
tributes used, and the accuracy results.
4.1 Training Data
documents 2383
sentences 50673
tokens 948404
parses 1.76 per token
IGs 1.33 per parse
features 3.29 per IG
unique tokens 111467
unique tags 11084
unique IGs 2440
unique features 126
ambiguous tokens 399223 (42.1%)
Table 4: Statistics for the training data
Our training data consists of about 1 million
words of semi-automatically disambiguated Turkish
news text. For each one of the 126 unique morpho-
logical features, we used the subset of the training
data in which instances have the given feature in at
least one of their generated parses. We then split this
subset into positive and negative examples depend-
ing on whether the correct parse contains the given
feature. A decision list specific to that feature is cre-
ated using GPA based on these examples.
Some relevant statistics for the training data are
given in Table 4.
4.2 Input Attributes
Once the training data is selected for a particular
morphological feature, each instance is represented
by surface attributes of five words centered around
the target word. We have tried larger window sizes
but no significant improvement was observed. The
attributes computed for each word in the window
consist of the following:
1. The exact word string (e.g. W==Ali?nin)
2. The lowercase version (e.g. W=?ali?nin) Note:
all digits are replaced by 0?s at this stage.
3. All suffixes of the lowercase version (e.g.
W=+n, W=+In, W=+nIn, W=+?nIn, etc.) Note:
certain characters are replaced with capital let-
ters representing character groups mentioned in
Table 3. These groups help the algorithm rec-
ognize different forms of a suffix created by the
phonetic rules of Turkish: for example the loca-
tive suffix +DA can surface as +de, +da, +te, or
+ta depending on the ending of the root word.
4. Attributes indicating the types of characters at
various positions of the word (e.g. Ali?nin
would be described with W=UPPER-FIRST,
W=LOWER-MID, W=APOS-MID, W=LOWER-
LAST)
Each training instance is represented by 40 at-
tributes on average. The GPA procedure is responsi-
ble for picking the attributes that are relevant to the
decision. No dictionary information is required or
used, therefore the models are fairly robust to un-
known words. One potentially useful source of at-
tributes is the tags assigned to previous words which
we plan to experiment with in future work.
4.3 The Decision Lists
At the conclusion of the training, 126 decision lists
are produced of the form given in Table 2. The num-
ber of rules in each decision list range from 1 to
6145. The longer decision lists are typically for part
of speech features, e.g. distinguishing nouns from
adjectives, and contain rules specific to lexical items.
The average number of rules is 266. To get an esti-
mate on the accuracy of each decision list, we split
the one million word data into training, validation,
and test portions using the ratio 4:1:1. The train-
ing set accuracy of the decision lists is consistently
above 98%. The test set accuracies of the 126 deci-
sion lists range from 80% to 100% with the average
at 95%. Table 5 gives the six worst features with test
set accuracy below 89%; these are the most difficult
to disambiguate.
4.4 Correct Tag Selection
To evaluate the candidate tags, we need to combine
the results of the decision lists. We assume that the
presence or absence of each feature is an indepen-
dent event with a probability determined by the test
set accuracy of the corresponding decision list. For
example, if the +P3pl decision list predicts YES,
we assume that the +P3pl feature is present with
332
87.89% +Acquire To acquire (noun)
86.18% +PCIns Postposition subcat.
85.11% +Fut Future tense
84.08% +P3pl 3. plural possessive
80.79% +Neces Must
79.81% +Become To become (noun)
Table 5: The six features with the worst test set ac-
curacy.
probability 0.8408 (See Table 5). If the +Fut deci-
sion list predicts NO, we assume the +Fut feature is
present with probability 1 ? 0.8511 = 0.1489. To
avoid zero probabilities we cap the test set accura-
cies at 99%.
Each candidate tag indicates the presence of cer-
tain features and the absence of others. The prob-
ability of the tag being correct under our indepen-
dence assumption is the product of the probabilities
for the presence and absence of each of the 126 fea-
tures as determined by our decision lists. For effi-
ciency, one can neglect the features that are absent
from all the candidate tags because their contribu-
tion will not effect the comparison.
4.5 Results
The final evaluation of the model was performed on
a test data set of 958 instances. The possible parses
for each instance were generated by the morpholog-
ical analyzer and the correct one was picked manu-
ally. 40% of the instances were ambiguous, which
on the average had 3.9 parses. The disambiguation
accuracy of our model was 95.82%. The 95% confi-
dence interval for the accuracy is [0.9457, 0.9708].
An analysis of the mistakes in the test data show
that at least some of them are due to incorrect tags
in our training data. The training data was semi-
automatically generated and thus contained some er-
rors. Based on hand evaluation of the differences be-
tween the training data tags and the GPA generated
tags, we estimate the accuracy of the training data to
be below 95%. We ran two further experiments to
see if we could improve on the initial results.
In our first experiment we used our original model
to re-tag the training data. The re-tagged training
data was used to construct a new model. The result-
ing accuracy on the test set increased to 96.03%, not
a statistically significant improvement.
In our second experiment we used only unam-
biguous instances for training. Decision list training
requires negative examples, so we selected random
unambiguous instances for positive and negative ex-
amples for each feature. The accuracy of the result-
ing model on the test set was 82.57%. The problem
with selecting unambiguous instances is that certain
common disambiguation decisions are never repre-
sented during training. More careful selection of
negative examples and a sophisticated bootstrapping
mechanism may still make this approach workable.
Finally, we decided to see if our decision lists
could be used for tagging rather than disambigua-
tion, i.e. given a word in a context decide on the full
tag without the help of a morphological analyzer.
Even though the number of possible tags is unlim-
ited, the most frequent 1000 tags cover about 99%
of the instances. A single decision list trained with
the full tags was able to achieve 91.23% accuracy
using 10000 rules. This is a promising result and
will be explored further in future work.
5 Contributions
We have presented an automated approach to learn
morphological disambiguation rules for Turkish us-
ing a novel decision list induction algorithm, GPA.
The only input to the rules are the surface attributes
of a five word window. The approach can be gener-
alized to other agglutinative languages which share
the common challenge of a large number of poten-
tial tags. Our approach for resolving the data sparse-
ness problem caused by the large number of tags is
to generate a separate model for each morphologi-
cal feature. The predictions for individual features
are probabilistically combined based on the accu-
racy of each model to select the best tag. We were
able to achieve an accuracy around 96% using this
approach.
Acknowledgments
We would like to thank Kemal Oflazer of Sabanc?
University for providing us with the Turkish mor-
phological analyzer, training and testing data for dis-
ambiguation, and valuable feedback.
333
References
Brill, E. (1995). Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Church, K. W. (1988). A stochastic parts program and
noun phrase parser for unrestricted text. In Proceed-
ings of the Second Conference on Applied Natural
Language Processing, pages 136?143.
Clark, P. and Boswell, R. (1991). Rule induction with
CN2: Some recent improvements. In Kodratoff,
Y., editor, Machine Learning ? Proceedings of the
Fifth European Conference (EWSL-91), pages 151?
163, Berlin. Springer-Verlag.
Clark, P. and Niblett, T. (1989). The CN2 induction al-
gorithm. Machine Learning, 3:261?283.
Cutting, D., Kupiec, J., Pedersen, J., and Sibun, P. (1992).
A practical part-of-speech tagger. In Proceedings of
the 3rd Conference on Applied Language Processing,
pages 133?140.
Daelemans, W. et al (1996). MBT: A memory-based
part of speech tagger-generator. In Ejerhead, E. and
Dagan, I., editors, Proceedings of the Fourth Workshop
on Very Large Corpora, pages 14?27.
Ezeiza, N. et al (1998). Combining stochastic and rule-
based methods for disambiguation in agglutinative lan-
guages. In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics (COL-
ING/ACL98), pages 379?384.
Hajic?, J. and Hladka?, B. (1998). Tagging inflective lan-
guages: Prediction of morphological categories for a
rich, structured tagset. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics (COLING/ACL98), pages 483?490, Mon-
treal, Canada.
Hakkani-Tu?r, D. Z., Oflazer, K., and Tu?r, G. (2002).
Statistical morphological disambiguation for aggluti-
native languages. Computers and the Humanities,
36:381?410.
Karlsson, F., Voutialinen, A., Heikkila?, J., and Anttila, A.
(1995). Constraint Grammar - A Language Indepen-
dent System for Parsing Unrestricted Text. Mouton de
Gruyter.
Levinger, M., Ornan, U., and Itai, A. (1995). Learning
morpho-lexical probabilities from an untagged corpus
with an application to hebrew. Computational Lin-
guistics, 21(3):383?404.
Megyesi, B. (1999). Improving brill?s pos tagger for an
agglutinative language. In Pascale, F. and Joe, Z., ed-
itors, Proceedings of the Joing SIGDAT Conference
on Empirical Methods in Natural Language and Very
Large Corpora, pages 275?284, College Park, Mary-
land, USA.
Newlands, D. and Webb, G. I. (2004). Alternative strate-
gies for decision list construction. In Proceedings of
the Fourth Data Mining Conference (DM IV 03), pages
265?273.
Oflazer, K. (1994). Two-level description of turkish
morphology. Literary and Linguistic Computing,
9(2):137?148.
Oflazer, K., Hakkani-Tu?r, D. Z., and Tu?r, G. (1999).
Design for a turkish treebank. In Proceedings of
the Workshop on Linguistically Interpreted Corpora,
EACL 99, Bergen, Norway.
Oflazer, K. and Kuruo?z, I?. (1994). Tagging and morpho-
logical disambiguation of turkish text. In Proceedings
of the 4th Applied Natural Language Processing Con-
ference, pages 144?149. ACL.
Oflazer, K. and Tu?r, G. (1997). Morphological disam-
biguation by voting constraints. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL97, EACL97), Madrid, Spain.
Ratnaparkhi, A. (1996). A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Rivest, R. L. (1987). Learning decision lists. Machine
Learning, 2:229?246.
van Halteren, H., editor (1999). Syntactic Wordclass Tag-
ging. Text, Speech and Language Technology. Kluwer
Academic Publishers.
Webb, G. I. (1995). Opus: An efficient admissible algo-
rithm for unordered search. JAIR, 3:431?465.
Webb, G. I. and Brkic, N. (1993). Learning decision lists
by prepending inferred rules. In Proceedings of the AI
93 Workshop on Machine Learning and Hybrid Sys-
tems, pages 6?10, Melbourne.
334
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 141?144,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Smoothing a Tera-word Language Model
Deniz Yuret
Koc? University
dyuret@ku.edu.tr
Abstract
Frequency counts from very large corpora,
such as the Web 1T dataset, have recently be-
come available for language modeling. Omis-
sion of low frequency n-gram counts is a prac-
tical necessity for datasets of this size. Naive
implementations of standard smoothing meth-
ods do not realize the full potential of such
large datasets with missing counts. In this pa-
per I present a new smoothing algorithm that
combines the Dirichlet prior form of (Mackay
and Peto, 1995) with the modified back-off es-
timates of (Kneser and Ney, 1995) that leads to
a 31% perplexity reduction on the Brown cor-
pus compared to a baseline implementation of
Kneser-Ney discounting.
1 Introduction
Language models, i.e. models that assign probabili-
ties to sequences of words, have been proven useful
in a variety of applications including speech recog-
nition and machine translation (Bahl et al, 1983;
Brown et al, 1990). More recently, good results
on lexical substitution and word sense disambigua-
tion using language models have also been reported
(Yuret, 2007).
The recently introduced Web 1T 5-gram dataset
(Brants and Franz, 2006) contains the counts of
word sequences up to length five in a 1012 word cor-
pus derived from publicly accessible Web pages. As
this corpus is several orders of magnitude larger than
the ones used in previous language modeling stud-
ies, it holds the promise to provide more accurate
domain independent probability estimates. How-
ever, naive application of the well-known smooth-
ing methods do not realize the full potential of this
dataset.
In this paper I present experiments with modifica-
tions and combinations of various smoothing meth-
ods using the Web 1T dataset for model building and
the Brown corpus for evaluation. I describe a new
smoothing method, Dirichlet-Kneser-Ney (DKN),
that combines the Bayesian intuition of MacKay and
Peto (1995) and the improved back-off estimation of
Kneser and Ney (1995) and gives significantly better
results than the baseline Kneser-Ney discounting.
The next section describes the general structure
of n-gram models and smoothing. Section 3 de-
scribes the data sets and the experimental methodol-
ogy used. Section 4 presents experiments with adap-
tations of various smoothing methods. Section 5 de-
scribes the new algorithm.
2 N-gram Models and Smoothing
N-gram models are the most commonly used lan-
guage modeling tools. They estimate the probability
of each word using the context made up of the previ-
ous n?1 words. Let abc represent an n-gram where
a is the first word, c is the last word, and b repre-
sents zero or more words in between. One way to
estimate Pr(c|ab) is to look at the number of times
word c has followed the previous n? 1 words ab,
Pr(c|ab) =
C(abc)
C(ab)
(1)
where C(x) denotes the number of times x has been
observed in the training corpus. This is the max-
imum likelihood (ML) estimate. Unfortunately it
141
does not work very well because it assigns zero
probability to n-grams that have not been observed
in the training corpus. To avoid the zero probabil-
ities, we take some probability mass from the ob-
served n-grams and distribute it to unobserved n-
grams. Such redistribution is known as smoothing
or discounting.
Most existing smoothing methods can be ex-
pressed in one of the following two forms:
Pr(c|ab) = ?(c|ab) + ?(ab) Pr(c|b) (2)
Pr(c|ab) =
{
?(c|ab) if C(abc) > 0
?(ab) Pr(c|b) otherwise (3)
Equation 2 describes the so-called interpolated
models and Equation 3 describes the back-off mod-
els. The highest order distributions ?(c|ab) and
?(c|ab) are typically discounted to be less than the
ML estimate so we have some leftover probability
for the c words unseen in the context ab. Different
methods mainly differ on how they discount the ML
estimate. The back-off weights ?(ab) are computed
to make sure the probabilities are normalized. The
interpolated models always incorporate the lower or-
der distribution Pr(c|b) whereas the back-off models
consider it only when the n-gram abc has not been
observed in the training data.
3 Data and Method
All the models in this paper are interpolated mod-
els built using the counts obtained from the Web 1T
dataset and evaluated on the million word Brown
corpus using cross entropy (bits per token). The low-
est order model is taken to be the word frequencies
in the Web 1T corpus. The Brown corpus was re-
tokenized to match the tokenization style of the Web
1T dataset resulting in 1,186,262 tokens in 52,108
sentences. The Web 1T dataset has a 13 million
word vocabulary consisting of words that appear 100
times or more in its corpus. 769 sentences in Brown
that contained words outside this vocabulary were
eliminated leaving 1,162,052 tokens in 51,339 sen-
tences. Capitalization and punctuation were left in-
tact. The n-gram patterns of the Brown corpus were
extracted and the necessary counts were collected
from the Web 1T dataset in one pass. The end-of-
sentence tags were not included in the entropy cal-
culation. For parameter optimization, numerical op-
timization was performed on a 1,000 sentence ran-
dom sample of Brown.
4 Experiments
In this section, I describe several smoothing meth-
ods and give their performance on the Brown corpus.
Each subsection describes a single idea and its im-
pact on the performance. All methods use interpo-
lated models expressed by ?(c|ab) and ?(ab) based
on Equation 2. The Web 1T dataset does not include
n-grams with counts less than 40, and I note the spe-
cific implementation decisions due to the missing
counts where appropriate.
4.1 Absolute Discounting
Absolute discounting subtracts a fixed constant D
from each nonzero count to allocate probability for
unseen words. A different D constant is chosen for
each n-gram order. Note that in the original study, D
is taken to be between 0 and 1, but because the Web
1T dataset does not include n-grams with counts less
than 40, the optimized D constants in our case range
from 0 to 40. The interpolated form is:
?(c|ab) =
max(0, C(abc)?D)
C(ab?)
(4)
?(ab) =
N(ab?)D
C(ab?)
The ? represents a wildcard matching any word and
C(ab?) is the total count of n-grams that start with
the n ? 1 words ab. If we had complete counts,
we would have C(ab?) =
?
c C(abc) = C(ab).
However because of the missing counts in general
C(ab?) ? C(ab) and we need to use the former for
proper normalization. N(ab?) denotes the number
of distinct words following ab in the training data.
Absolute discounting achieves its best performance
with a 3-gram model and gives 8.53 bits of cross en-
tropy on the Brown corpus.
4.2 Kneser-Ney
Kneser-Ney discounting (Kneser and Ney, 1995)
has been reported as the best performing smooth-
ing method in several comparative studies (Chen and
Goodman, 1999; Goodman, 2001). The ?(c|ab)
and ?(ab) expressions are identical to absolute dis-
counting (Equation 4) for the highest order n-grams.
142
However, a modified estimate is used for lower order
n-grams used for back-off. The interpolated form is:
Pr(c|ab) = ?(c|ab) + ?(ab)Pr?(c|b) (5)
Pr?(c|ab) = ??(c|ab) + ??(ab)Pr?(c|b)
Specifically, the modified estimate Pr?(c|b) for a
lower order n-gram is taken to be proportional to the
number of unique words that precede the n-gram in
the training data. The ?? and ?? expressions for the
modified lower order distributions are:
??(c|b) =
max(0, N(?bc)?D)
N(?b?)
(6)
??(b) =
R(?b?)D
N(?b?)
where R(?b?) = |c : N(?bc) > 0| denotes the num-
ber of distinct words observed on the right hand side
of the ?b? pattern. A different D constant is chosen
for each n-gram order. The lowest order model is
taken to be Pr(c) = N(?c)/N(??). The best results
for Kneser-Ney are achieved with a 4-gram model
and its performance on Brown is 8.40 bits.
4.3 Correcting for Missing Counts
Kneser-Ney takes the back-off probability of a lower
order n-gram to be proportional to the number of
unique words that precede the n-gram in the training
data. Unfortunately this number is not exactly equal
to the N(?bc) value given in the Web 1T dataset be-
cause the dataset does not include low count abc n-
grams. To correct for the missing counts I used the
following modified estimates:
N ?(?bc) = N(?bc) + ?(C(bc)? C(?bc))
N ?(?b?) = N(?b?) + ?(C(b?)? C(?b?))
The difference between C(bc) and C(?bc) is due
to the words preceding bc less than 40 times. We
can estimate their number to be a fraction of this
difference. ? is an estimate of the type token ra-
tio of these low count words. Its valid range is be-
tween 1/40 and 1, and it can be optimized along with
the other parameters. The reader can confirm that
?
c N
?(?bc) = N ?(?b?) and |c : N ?(?bc) > 0| =
N(b?). The expression for the Kneser-Ney back-off
estimate becomes
??(c|b) =
max(0, N ?(?bc)?D)
N ?(?b?)
(7)
??(b) =
N(b?)D
N ?(?b?)
Using the corrected N ? counts instead of the plain N
counts achieves its best performance with a 4-gram
model and gives 8.23 bits on Brown.
4.4 Dirichlet Form
MacKay and Peto (1995) show that based on Dirich-
let priors a reasonable form for a smoothed distribu-
tion can be expressed as
?(c|ab) =
C(abc)
C(ab?) +A
(8)
?(ab) =
A
C(ab?) +A
The parameter A can be interpreted as the extra
counts added to the given distribution and these ex-
tra counts are distributed as the lower order model.
Chen and Goodman (1996) suggest that these ex-
tra counts should be proportional to the number of
words with exactly one count in the given context
based on the Good-Turing estimate. The Web 1T
dataset does not include one-count n-grams. A rea-
sonable alternative is to take A to be proportional
to the missing count due to low-count n-grams:
C(ab)? C(ab?).
A(ab) = max(1,K(C(ab)? C(ab?)))
A different K constant is chosen for each n-gram
order. Using this formulation as an interpolated 5-
gram language model gives a cross entropy of 8.05
bits on Brown.
4.5 Dirichlet with KN Back-Off
Using a modified back-off distribution for lower or-
der n-grams gave us a big boost in the baseline re-
sults from 8.53 bits for absolute discounting to 8.23
bits for Kneser-Ney. The same idea can be applied
to the missing-count estimate. We can use Equa-
tion 8 for the highest order n-grams and Equation 7
for lower order n-grams used for back-off. Such a
5-gram model gives a cross entropy of 7.96 bits on
the Brown corpus.
5 A New Smoothing Method: DKN
In this section, I describe a new smoothing method
that combines the Dirichlet form of MacKay and
143
Peto (1995) and the modified back-off distribution
of Kneser and Ney (1995). We will call this new
method Dirichlet-Kneser-Ney, or DKN for short.
The important idea in Kneser-Ney is to let the prob-
ability of a back-off n-gram be proportional to the
number of unique words that precede it. However
we do not need to use the absolute discount form for
the estimates. We can use the Dirichlet prior form
for the lower order back-off distributions as well as
the highest order distribution. The extra counts A
in the Dirichlet form are taken to be proportional
to the missing counts, and the coefficient of pro-
portionality K is optimized for each n-gram order.
Where complete counts are available, A should be
taken to be proportional to the number of one-count
n-grams instead. This smoothing method with a 5-
gram model gives a cross entropy of 7.86 bits on
the Brown corpus achieving a perplexity reduction
of 31% compared to the naive implementation of
Kneser-Ney.
The relevant equations are repeated below for the
reader?s convenience.
Pr(c|ab) = ?(c|ab) + ?(ab)Pr?(c|b)
Pr?(c|ab) = ??(c|ab) + ??(ab)Pr?(c|b)
?(c|b) =
C(bc)
C(b?) +A(b)
?(b) =
A(b)
C(b?) +A(b)
??(c|b) =
N ?(?bc)
N ?(?b?) +A(b)
??(b) =
A(b)
N ?(?b?) +A(b)
A(b) = max(1,K(C(b)? C(b?)))
or max(1,K|c : C(bc) = 1|)
6 Summary and Discussion
Frequency counts based on very large corpora can
provide accurate domain independent probability es-
timates for language modeling. I presented adapta-
tions of several smoothing methods that can prop-
erly handle the missing counts that may exist in
such datasets. I described a new smoothing method,
DKN, combining the Bayesian intuition of MacKay
and Peto (1995) and the modified back-off distri-
bution of Kneser and Ney (1995) which achieves a
significant perplexity reduction compared to a naive
implementation of Kneser-Ney smoothing. This
is a surprizing result because Chen and Goodman
(1999) partly attribute the performance of Kneser-
Ney to the use of absolute discounting. The re-
lationship between Kneser-Ney smoothing to the
Bayesian approach have been explored in (Goldwa-
ter et al, 2006; Teh, 2006) using Pitman-Yor pro-
cesses. These models still suggest discount-based
interpolation with type frequencies whereas DKN
uses Dirichlet smoothing throughout. The condi-
tions under which the Dirichlet form is superior is
a topic for future research.
References
Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.
1983. A maximum likelihood approach to continu-
ous speech recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 5(2):179?190.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Linguistic Data Consortium, Philadelphia.
LDC2006T13.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the ACL.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech and Language.
S. Goldwater, T.L. Griffiths, and M. Johnson. 2006. In-
terpolating between types and tokens by estimating
power-law generators. In Advances in Neural Infor-
mation Processing Systems, volume 18. MIT Press.
Joshua Goodman. 2001. A bit of progress in language
modeling. Computer Speech and Language.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In International Confer-
ence on Acoustics, Speech, and Signal Processing.
David J. C. Mackay and Linda C. Bauman Peto. 1995. A
hierarchical Dirichlet language model. Natural Lan-
guage Engineering, 1(3):1?19.
Y.W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the ACL, pages 985?992.
Deniz Yuret. 2007. KU: Word sense disambiguation
by substitution. In SemEval-2007: 4th International
Workshop on Semantic Evaluations.
144
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 345?348,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Modeling Morphologically Rich Languages Using Split Words and
Unstructured Dependencies
Deniz Yuret
Koc? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Abstract
We experiment with splitting words into
their stem and suffix components for mod-
eling morphologically rich languages. We
show that using a morphological ana-
lyzer and disambiguator results in a sig-
nificant perplexity reduction in Turkish.
We present flexible n-gram models, Flex-
Grams, which assume that the n?1 tokens
that determine the probability of a given
token can be chosen anywhere in the sen-
tence rather than the preceding n?1 posi-
tions. Our final model achieves 27% per-
plexity reduction compared to the standard
n-gram model.
1 Introduction
Language models, i.e. models that assign prob-
abilities to sequences of words, have been proven
useful in a variety of applications including speech
recognition and machine translation (Bahl et al,
1983; Brown et al, 1990). More recently, good re-
sults on lexical substitution and word sense disam-
biguation using language models have also been
reported (Hawker, 2007; Yuret, 2007). Morpho-
logically rich languages pose a challenge to stan-
dard modeling techniques because of their rela-
tively large out-of-vocabulary rates and the regu-
larities they possess at the sub-word level.
The standard n-gram language model ignores
long-distance relationships between words and
uses the independence assumption of a Markov
chain of order n ? 1. Morphemes play an im-
portant role in the syntactic dependency structure
in morphologically rich languages. The depen-
dencies are not only between stems but also be-
tween stems and suffixes and if we use complete
words as unit tokens, we will not be able to rep-
resent these sub-word dependencies. Our work-
ing hypothesis is that the performance of a lan-
guage model is correlated by how much the prob-
abilistic dependencies mirror the syntactic depen-
dencies. We present flexible n-grams, FlexGrams,
in which each token can be conditioned on tokens
anywhere in the sentence, not just the preceding
n?1 tokens. We also experiment with words split
into their stem and suffix forms, and define stem-
suffix FlexGrams where one set of offsets is ap-
plied to stems and another to suffixes. We evaluate
the performance of these models on a morpholog-
ically rich language, Turkish.
2 The FlexGram Model
The FlexGram model relaxes the contextual as-
sumption of n-grams and assumes that the n ? 1
tokens that determine the probability of a given to-
ken can be chosen anywhere in the sentence rather
than at the preceding n? 1 positions. This allows
the ability to model long-distance relationships be-
tween tokens without a predefined left-to-right or-
dering and opens the possibility of using different
dependency patterns for different token types.
Formal definition An order-n FlexGram model
is specified by a tuple of dependency offsets
[d
1
, d
2
, . . . , d
n?1
] and decomposes the probability
of a given sequence of tokens into a product of
conditional probabilities for every token:
p(w
1
, . . . , w
k
) =
?
w
i
?S
p(w
i
|w
i+d
1
. . . w
i+d
n?1
)
The offsets can be positive or negative and the
same set of offsets is applied to all tokens in the
sequence. In order to represent a properly nor-
malized probability model over the set of all finite
length sequences, we check that the offsets of a
FlexGram model does not result in a cycle. We
show that using differing dependency offsets for
stems and suffixes can improve the perplexity.
345
3 Dataset
We used the Turkish newspaper corpus of Milliyet
after removing sentences with 100 or more tokens.
The dataset contains about 600 thousand sentences
in the training set and 60 thousand sentences in the
test set (giving a total of about 10 million words).
The versions of the corpus we use developed by
using different word-split strategies along with a
sample sentence are explained below:
1. The unsplit dataset contains the raw corpus:
Kasparov b?ukemedi?gi eli ?opecek
(Kasparov is going to kiss the hand he cannot bend)
2. The morfessor dataset was prepared using the
Morfessor (Creutz et al, 2007) algorithm:
Kasparov b?uke +medi?gi eli ?op +ecek
3. The auto-split dataset is obtained after using
our unsupervised morphological splitter:
Kaspar +ov b?uk +emedi?gi eli ?op +ecek
4. The split dataset contains words that are split
into their stem and suffix forms by using a
highly accurate supervised morphological an-
alyzer (Yuret and T?ure, 2006):
Kasparov b?uk +yAmA+dHk+sH el +sH ?op
+yAcAk
5. The split+0 version is derived from the split
dataset by adding a zero-suffix to any stem that
is not followed by a suffix:
Kasparov +0 b?uk +yAmA+dHk+sH el +sH
?op +yAcAk
Some statistics of the dataset are presented in
Table 1. The vocabulary is taken to be the to-
kens that occur more than once in the training set
and the OOV column shows the number of out-
of-vocabulary tokens in the test set. The unique
and 1-count columns give the number of unique
tokens and the number of tokens that only occur
once in the training set. Approximately 5% of the
tokens in the unsplit test set are OOV tokens. In
comparison, the ratio for a comparably sized En-
glish dataset is around 1%. Splitting the words
into stems and suffixes brings the OOV ratio closer
to that of English.
Model evaluation When comparing language
models that tokenize data differently:
1. We take into account the true cost of the OOV
tokens using a separate character-based model
similar to Brown et al (1992).
2. When reporting averages (perplexity, bits-per-
word) we use a common denominator: the
number of unsplit words.
Table 1: Dataset statistics (K for thousands, M for millions)
Dataset Train Test OOV Unique 1-count
unsplit 8.88M 0.91M 44.8K (4.94%) 430K 206K
morfessor 9.45M 0.98M 10.3K (1.05%) 167K 34.4K
auto-split 14.3M 1.46M 13.0K (0.89%) 128K 44.8K
split 12.8M 1.31M 17.1K (1.31%) 152K 75.4K
split+0 17.8M 1.81M 17.1K (0.94%) 152K 75.4K
4 Experiments
In this section we present a number of experiments
that demonstrate that when modeling a morpho-
logically rich language like Turkish, (i) splitting
words into their stem and suffix forms is beneficial
when the split is performed using a morphologi-
cal analyzer and (ii) allowing the model to choose
stem and suffix dependencies separately and flex-
ibly results in a perplexity reduction, however the
reduction does not offset the cost of zero suffixes.
We used the SRILM toolkit (Stolcke, 2002) to
simulate the behavior of FlexGram models by us-
ing count files as input. The interpolated Kneser-
Ney smoothing was used in all our experiments.
Table 2: Total log probability (M for millions of bits).
Split Dataset Unsplit Dataset
N Word logp OOV logp Word logp OOV logp
1 14.2M 0.81M 11.7M 2.32M
2 10.5M 0.64M 9.64M 1.85M
3 9.79M 0.56M 9.46M 1.59M
4 9.72M 0.53M 9.45M 1.38M
5 9.71M 0.51M 9.45M 1.25M
6 9.71M 0.50M 9.45M 1.19M
4.1 Using a morphological tagger and
disambiguator
The split version of the corpus contains words
that are split into their stem and suffix forms by
using a previously developed morphological an-
alyzer (Oflazer, 1994) and morphological disam-
biguator (Yuret and T?ure, 2006). The analyzer
produces all possible parses of a Turkish word us-
ing the two-level morphological paradigm and the
disambiguator chooses the best parse based on the
analysis of the context using decision lists. The in-
tegrated system was found to discover the correct
morphological analysis for 96% of the words on
a hand annotated out-of-sample test set. Table 2
gives the total log-probability (using log
2
) for the
split and unsplit datasets using n-gram models
of different order. We compute the perplexity
of the two datasets using a common denomina-
tor: 2
? log
2
(p)/N
where N=906,172 is taken to be
the number of unsplit tokens. The best combina-
tion (order-6 word model combined with an order-
9 letter model) gives a perplexity of 2,465 for
the split dataset and 3,397 for the unsplit dataset,
346
which corresponds to a 27% improvement.
4.2 Separation of stem and suffix models
Only 45% of the words in the split dataset have
suffixes. Each sentence in the split+0 dataset has
a regular [stem suffix stem suffix ...] structure. Ta-
ble 3 gives the average cost of stems and suffixes in
the two datasets for a regular 6-gram word model
(ignoring the common OOV words). The log-
probability spent on the zero suffixes in the split+0
dataset has to be spent on trying to decide whether
to include a stem or suffix following a stem in the
split dataset. As a result the difference in total log-
probability between the two datasets is small (only
6% perplexity difference). The set of OOV tokens
is the same for both the split and split+0 datasets;
therefore we ignore the cost of the OOV tokens as
is the default SRILM behavior.
Table 3: Total log probability for the 6-gram word models
on split and split+0 data.
split dataset split+0 dataset
token number of total number of total
type tokens ? log
2
p tokens ? log
2
p
stem 0.91M 7.80M 0.91M 7.72M
suffix 0.41M 1.89M 0.41M 1.84M
0-suffix ? ? 0.50M 0.21M
all 1.31M 9.69M 1.81M 9.78M
4.3 Using the FlexGram model
We perform a search over the space of dependency
offsets using the split+0 dataset and considered n-
gram orders 2 to 6 and picked the dependency off-
sets within a window of 4n + 1 tokens centered
around the target. Table 4 gives the best mod-
els discovered for stems and suffixes separately
and compares them to the corresponding regular
n-gram models on the split+0 dataset. The num-
bers in parentheses give perplexity and significant
reductions can be observed for each n-gram order.
Table 4: Regular ngram vs FlexGram models.
N ngram-stem ngram-suffix
2 -1 (1252) -1 (5.69)
3 -2,-1 (418) -2,-1 (5.29)
4 -3,-2,-1 (409) -3,-2,-1 (4.79)
5 -4,-3,-2,-1 (365) -4,-3,-2,-1 (4.80)
6 -5,-4,-3,-2,-1 (367) -5,-4,-3,-2,-1 (4.79)
N flexgram-stem flexgram-suffix
2 -2 (596) -1 (5.69)
3 +1,-2 (289) +1,-1 (4.21)
4 +2,+1,-1 (189) -2,+1,-1 (4.19)
5 +4,+2,+1,-1 (176) -3,-2,+1,-1 (4.12)
6 +4,+3,+2,+1,-1 (172) -4,-3,-2,+1,-1 (4.13)
However, some of these models cannot be used
in combination because of cycles as we depict on
the left side of Figure 1 for order 3. Table 5 gives
the best combined models without cycles. We
were able to exhaustively search all the patterns
for orders 2 to 4 and we used beam search for or-
ders 5 and 6. Each model is represented by its
offset tuple and the resulting perplexity is given
in parentheses. Compared to the regular n-gram
models from Table 4 we see significant perplexity
reductions up to order 4. The best order-3 stem-
suffix FlexGram model can be seen on the right
side of Figure 1.
Table 5: Best stem-suffix flexgram model combinations for
the split+0 dataset.
N flexgram-stem flexgram-suffix perplexity reduction
2 -2 (596) -1 (5.69) 52.3%
3 -4,-2 (496) +1,-1 (4.21) 5.58%
4 -4,-2,-1 (363) -3,-2,-1 (4.79) 11.3%
5 -6,-4,-2,-1 (361) -3,-2,-1 (4.79) 1.29%
6 -6,-4,-2,-1 (361) -3,-2,-1 (4.79) 1.52%
5 Related work
Several approaches attempt to relax the rigid or-
dering enforced by the standard n-gram model.
The skip-gram model (Siu and Ostendorf, Jan
2000) allows the skipping of one word within a
given n-gram. Variable context length language
modeling (Kneser, 1996) achieves a 10% per-
plexity reduction when compared to the trigrams
by varying the order of the n-gram model based
on the context. Dependency models (Rosenfeld,
2000) use the parsed dependency structure of sen-
tences to build the language model as in grammat-
ical trigrams (Lafferty et al, 1992), structured lan-
guage models (Chelba and Jelinek, 2000), and de-
pendency language models (Chelba et al, 1997).
The dependency model governs the whole sen-
tence and each word in a sentence is likely to have
a different dependency structure whereas in our
experiments with FlexGrams we use two connec-
tivity patterns: one for stems and one for suffixes
without the need for parsing.
6 Contributions
We have analyzed the effect of word splitting and
unstructured dependencies on modeling Turkish, a
morphologically complex language. Table 6 com-
pares the models we have tested on our test corpus.
We find that splitting words into their stem and
suffix components using a morphological analyzer
and disambiguator results in significant perplexity
reductions of up to 27%. FlexGram models out-
perform regular n-gram models (Tables 4 and 5)
347
Figure 1: Two FlexGram models where W represents a stem, s represents a suffix, and the arrows represent dependencies.
The left model has stem offsets [+1,-2] and suffix offsets [+1,-1] and cannot be used as a directed graphical model because
of the cycles. The right model has stem offsets [-4,-2] and suffix offsets [+1,-1] and is the best order-3 FlexGram model for
Turkish.
Table 6: Perplexity for compared models.
N unsplit split flexgram
2 3929 4360 5043
3 3421 2610 3083
4 3397 2487 2557
5 3397 2468 2539
6 3397 2465 2539
when using an alternating stem-suffix representa-
tion of the sentences; however Table 6 shows that
the cost of the alternating stem-suffix representa-
tion (zero-suffixes) offsets this gain.
References
Lalit R. Bahl, Frederick Jelinek, and Robert L.
Mercer. A maximum likelihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelli-
gence, 5(2):179?190, 1983.
Peter F. Brown, John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra, Frederick
Jelinek, John D. Lafferty, Robert L. Mercer, and
Paul S. Roossin. A statistical approach to ma-
chine translation. Computational Linguistics,
16(2):79?85, 1990.
Peter F. Brown et al An estimate of an upper
bound for the entropy of english. Computa-
tional Linguistics, 18(1):31?40, 1992.
Ciprian Chelba and Frederick Jelinek. Recog-
nition performance of a structured language
model. CoRR, cs.CL/0001022, 2000.
Ciprian Chelba, David Engle, Frederick Jelinek,
Victor M. Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald
Rosenfeld, Andreas Stolcke, and Dekai Wu.
Structure and performance of a dependency lan-
guage model. In Proc. Eurospeech ?97, pages
2775?2778, Rhodes, Greece, September 1997.
Mathias Creutz, Teemu Hirsim?aki, Mikko Ku-
rimo, Antti Puurula, Janne Pylkk?onen, Vesa
Siivola, Matti Varjokallio, Ebru Arisoy, Mu-
rat Saraclar, and Andreas Stolcke. Morph-
based speech recognition and modeling of out-
of-vocabulary words across languages. TSLP, 5
(1), 2007.
Tobias Hawker. USYD: WSD and lexical substitu-
tion using the Web1T corpus. In SemEval-2007:
4th International Workshop on Semantic Evalu-
ations, 2007.
R. Kneser. Statistical language modeling using a
variable context length. In Proc. ICSLP ?96,
volume 1, pages 494?497, Philadelphia, PA,
October 1996.
John Lafferty, Daniel Sleator, and Davy Tem-
perley. Grammatical trigrams: a probabilistic
model of link grammar. In AAAI Fall Sym-
posium on Probabilistic Approaches to NLP,
1992.
Kemal Oflazer. Two-level description of turkish
morphology. Literary and Linguistic Comput-
ing, 9(2):137?148, 1994.
Ronald Rosenfeld. Two decades of statistical lan-
guage modeling: Where do we go from here.
In Proceedings of the IEEE, volume 88, pages
1270?1278, 2000.
Manhung Siu and M. Ostendorf. Variable n-grams
and extensions for conversational speech lan-
guage modeling. Speech and Audio Processing,
IEEE Transactions on, 8(1):63?75, Jan 2000.
ISSN 1063-6676. doi: 10.1109/89.817454.
Andreas Stolcke. Srilm ? an extensible language
modeling toolkit. In Proc. Int. Conf. Spoken
Language Processing (ICSLP 2002), 2002.
Deniz Yuret. KU: Word sense disambiguation by
substitution. In SemEval-2007: 4th Interna-
tional Workshop on Semantic Evaluations, June
2007.
Deniz Yuret and Ferhan T?ure. Learning mor-
phological disambiguation rules for turkish. In
HLT-NAACL 06, June 2006.
348
 
	Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 246?250, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing as a Classication Problem
Deniz Yuret
Koc? University
Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
This paper presents an approach to depen-
dency parsing which can utilize any stan-
dard machine learning (classification) al-
gorithm. A decision list learner was used
in this work. The training data provided
in the form of a treebank is converted to a
format in which each instance represents
information about one word pair, and the
classification indicates the existence, di-
rection, and type of the link between the
words of the pair. Several distinct mod-
els are built to identify the links between
word pairs at different distances. These
models are applied sequentially to give the
dependency parse of a sentence, favoring
shorter links. An analysis of the errors,
attribute selection, and comparison of dif-
ferent languages is presented.
1 Introduction
This paper presents an approach to supervised learn-
ing of dependency relations in a language using stan-
dard machine learning techniques. The treebanks
(Hajic? et al, 2004; Chen et al, 2003; Bo?hmova?
et al, 2003; Kromann, 2003; van der Beek et al,
2002; Brants et al, 2002; Kawata and Bartels, 2000;
Afonso et al, 2002; Dz?eroski et al, 2006; Civit Tor-
ruella and Mart?? Anton??n, 2002; Nilsson et al, 2005;
Oflazer et al, 2003; Atalay et al, 2003) provided for
the CoNLL shared task(Buchholz et al, 2006) were
converted to a set of instances each of which con-
sists of the attributes of a candidate word pair with
a classification that indicates the existence, direction
and type of the dependency link between the pair.
An initial model is built to identify dependency
relations between adjacent word pairs using a deci-
sion list learning algorithm. To identify longer dis-
tance relations, the adjacent modifiers are dropped
from the sentence and a second order model is built
based on the word pairs that come into contact. A
total of three models were built using this technique
successively and used for parsing.
All given attributes are considered as candidates
in an attribute selection process before building each
model. In addition, attributes indicating suffixes of
various lengths and character type information were
constructed and used.
To parse a given sentence, the models are applied
sequentially, each one considering candidate word
pairs and adding new links without deleting the ex-
isting links or creating conflicts (cycles or crossings)
with them. Thus, the algorithm can be considered a
bottom-up, multi-pass, deterministic parser. Given
a candidate word pair, the models may output ?no
link?, or give a link with a specified direction and
type. Thus labeling is an integrated step. Word
pair candidates that may form cycles or crossings
are never considered, so the parser will only gen-
erate projective structures.
Section 2 gives the details of the learning algo-
rithm. Section 3 describes the first pass model of
links between adjacent words. Section 4 details
the approach for identifying long distance links and
presents the parsing results.
246
2 The Learning Algorithm
The Greedy Prepend Algorithm (Yuret and Ture,
2006) was used to build decision lists to identify de-
pendency relations. A decision list is an ordered list
of rules where each rule consists of a pattern and a
classification (Rivest, 1987). The first rule whose
pattern matches a given instance is used for its clas-
sification. In our application the pattern specifies the
attributes of the two words to be linked such as parts
of speech and morphological features. The classi-
fication indicates the existence and the type of the
dependency link between the two words.
Table 1 gives a subset of the decision list that iden-
tifies links between adjacent words in German. The
class column indicates the type of the link, the pat-
tern contains attributes of the two candidate words X
and Y, as well as their neighbors (XL1 indicates the
left neighbor of X). For example, given the part of
speech sequence APPR-ART-NN, there would be an
NK link between APPR and ART (matches rule 3), but
there would be no link between ART and NN (rule 1
overrides rule 2).
Rule Class Pattern
1 NONE XL1:postag=APPR
2 L:NK X:postag=ART Y:postag=NN
3 R:NK X:postag=APPR
4 NONE
Table 1: A four rule decision list for adjacent word
dependencies in German
The average training instance for the depen-
dency problem has over 40 attributes describing the
two candidate words including suffixes of different
lengths, parts of speech and information on neigh-
boring words. Most of this information may be re-
dundant or irrelevant to the problem at hand. The
number of distinct attribute values is on the order
of the number of distinct word-forms in the train-
ing set. GPA was picked for this problem because
it has proven to be fairly efficient and robust in the
presence of irrelevant or redundant attributes in pre-
vious work such as morphological disambiguation
in Turkish (Yuret and Ture, 2006) and protein sec-
ondary structure prediction (Kurt, 2005).
3 Dependency of Adjacent Words
We start by looking at adjacent words and try to pre-
dict whether they are linked, and if they are, what
type of link they have. This is a nice subproblem to
study because: (i) It is easily converted to a standard
machine learning problem, thus amenable to com-
mon machine learning techniques and analysis, (ii)
It demonstrates the differences between languages
and the impact of various attributes. The machine
learning algorithm used was GPA (See Section 2)
which builds decision lists.
Table 2 shows the percentage of adjacent tokens
that are linked in the training sets for the languages
studied1 . Most languages have approximately half
of the adjacent words linked. German, with 42.15%
is at the low end whereas Arabic and Turkish with
above 60% are at the high end. The differences may
be due to linguistic factors such as the ubiquity of
function words which prefer short distance links, or
it may be an accident of data representation: for ex-
ample each token in the Turkish data represents an
inflectional group, not a whole word.
Arabic 61.02 Japanese 54.81
Chinese 56.59 Portuguese 50.81
Czech 48.73 Slovene 45.62
Danish 55.93 Spanish 51.28
Dutch 55.54 Swedish 48.26
German 42.15 Turkish 62.60
Table 2: Percentage of adjacent tokens linked.
3.1 Attributes
The five attributes provided for each word in the
treebanks were the wordform, the lemma, the
coarse-grained and fine-grained parts of speech, and
a list of syntactic and/or morphological features. In
addition I generated two more attributes for each
word: suffixes of up to n characters (indicated
by suffix[n]), and character type information, i.e.
whether the word contains any punctuation charac-
ters, upper case letters, digits, etc.
Two questions to be answered empirically are: (i)
How much context to include in the description of
each instance, and (ii) Which attributes to use for
each language.
1Including non-scoring tokens
247
Table 3 shows the impact of using varying
amounts of context in Spanish. I used approximately
10,000 instances for training and 10,000 instances
for testing. Only the postag feature is used for
each word in this experiment. As an example, con-
sider the word sequence w1 . . . wiwi+1 . . . wn, and
the two words to be linked are wi and wi+1. Con-
text=0 means only information about wi and wi+1
is included, context=1 means we also include wi?1
and wi+2, etc. The table also includes the number
of rules in each decision list. The results are typical
of the experiments performed with other languages
and other attribute combinations: there is a statisti-
cally significant improvement going from context=0
to context=1. Increasing the context size further
does not have a significant effect.
Context Rules Accuracy
0 161 83.17
1 254 87.31
2 264 87.05
3 137 87.14
Table 3: Context size vs. accuracy in Spanish.
A number of experiments were run to determine
the best attribute combinations for each language.
Table 4 gives a set of results for single attributes in
Spanish. These results are based on 10,000 training
instances and all experiments use context=1. Postag
was naturally the most informative single attribute
on all languages tested, however the second best
or the best combination varied between languages.
Suffix[3] indicates all suffixes up to three characters
in length. The FEATS column was split into its con-
stituent features each of which was treated as a bi-
nary attribute.
Attributes Rules Accuracy
postag 254 87.31
cpostag 154 85.72
suffix[3] 328 77.15
lemma 394 76.78
form 621 75.06
feats 66 71.95
ctype 47 53.40
Table 4: Attributes vs. accuracy in Spanish.
There are various reasons for performing at-
tribute selection. Intuitively, including more infor-
mation should be good, so why not use all the at-
tributes? First, not every machine learning algo-
rithm is equally tolerant of redundant or irrelevant
attributes. Naive Bayes gets 81.54% and C4.5 gets
86.32% on the Spanish data with the single postag
attribute using context=1. One reason I chose GPA
was its relative tolerance to redundant or irrelevant
attributes. However, no matter how robust the algo-
rithm, the lack of sufficient training data will pose a
problem: it becomes difficult to distinguish informa-
tive attributes from non-informative ones if the data
is sparse. About half of the languages in this study
had less than 100,000 words of training data. Fi-
nally, studying the contribution of each attribute type
in each language is an interesting research topic in
its own right. The next section will present the best
attribute combinations and the resulting accuracy for
each language.
3.2 Results
Language Attributes Accuracy
Arabic ALL 76.87
Chinese postag, cpostag 84.51
Czech postag, lemma 79.25
Danish postag, form 86.96
Dutch postag, feats 85.36
German postag, form 87.97
Japanese postag, suffix[2] 95.56
Portuguese postag, lemma 90.18
Slovene ALL 85.19
Spanish postag, lemma 89.01
Swedish postag, form 83.20
Turkish ALL 85.27
Table 5: Adjacent word link accuracy.
Table 5 gives the best attribute combinations for
determining adjacent word links for each language
studied. The attribute combinations and the corre-
sponding models were determined using the training
sets, and the accuracy reported is on the test sets.
These attribute combinations were used as part of
the model in the final evaluation. I used context=1
for all the models. Because of time limitations at-
tribute combinations with more than two attributes
248
could not be tested and only the first 100,000 train-
ing instances were used. Exceptions are indicated
with ?ALL?, where all attributes were used in the
model ? these are cases where using all the attributes
outperformed other subsets tried.
For most languages, the adjacent word link accu-
racy is in the 85-90% range. The outliers are Ara-
bic and Czech at the lower end, and Japanese at the
higher end. It is difficult to pinpoint the exact rea-
sons: Japanese has the smallest set of link types,
and Arabic has the greatest percentage of adjacent
word links. Some of the differences between the
languages come from linguistic origins, but many
are due to the idiosyncrasies of our particular data
set: number of parts of speech, types of links, qual-
ity of the treebank, amount of data are all arbitrary
factors that effect the results. One observation is that
the ranking of the languages in Table 5 according to
performance is close to the ranking of the best re-
sults in the CoNLL shared task ? the task of linking
adjacent words via machine learning seems to be a
good indicator of the difficulty of the full parsing
problem.
4 Long Distance Dependencies
Roughly half of the dependency links are between
non-adjacent words in a sentence. To illustrate how
we can extend the previous section?s approach to
long distance links, consider the phrase ?kick the
red ball?. The adjacent word linker can only find
the red-ball link even if it is 100% accurate. How-
ever once that link has been correctly identified, we
can drop the modifier ?red? and do a second pass
with the words ?kick the ball?. This will identify the
link the-ball, and dropping the modifier again leaves
us with ?kick ball?. Thus, doing three passes over
this word sequence will bring all linked words into
contact and allow us to use our adjacent word linker.
Table 6 gives the percentage of the links discovered
in each pass by a perfect model in Spanish.
Pass: 1 2 3 4 5
Link%: 51.09 23.56 10.45 5.99 3.65
Table 6: Spanish links discovered in multiple passes.
We need to elaborate a bit on the operation of
?dropping the modifiers? that lead from one pass to
the next. After the discovery of the red-ball link
in the above example, it is true that ?red? can no
longer link with any other words to the right (it can-
not cross its own head), but it can certainly link with
the words to the left. To be safe, in the next pass
we should consider both the-red and the-ball as can-
didate links. In the actual implementation, given a
partial linkage, all ?potentially adjacent? word pairs
that do not create cycles or link crossings were con-
sidered as candidate pairs for the next pass.
There are significant differences between the first
pass and the second pass. Some word pairs will
rarely be seen in contact during the first pass (e.g.
?kick ball?). Maybe more importantly, we will
have additional ?syntactic? context during the sec-
ond pass, i.e. information about the modifiers dis-
covered in the first pass. All this argues for building
a separate model for the second pass, and maybe for
further passes as well.
In the actual implementation, models for three
passes were built for each language. To create the
training data for the n?th pass, all the links that can
be discovered with (n-1) passes are taken as given,
and all word pairs that are ?potentially adjacent?
given this partial linkage are used as training in-
stances. To describe each training instance, I used
the attributes of the two candidate words, their sur-
face neighbors (i.e. the words they are adjacent to
in the actual sentence), and their syntactic neighbors
(i.e. the words they have linked with so far).
To parse a sentence the three passes were run se-
quentially, with the whole sequence repeated twice2.
Each pass adds new links to the existing partial link-
age, but does not remove any existing links. Table 7
gives the labeled and unlabeled attachment score for
the test set of each language using this scheme.
5 Conclusion
I used standard machine learning techniques to in-
vestigate the lower bound accuracy and the impact
of various attributes on the subproblem of identify-
ing dependency links between adjacent words. The
technique was then extended to identify long dis-
tance dependencies and used as a parser. The model
gives average results for Turkish and Japanese but
2This counterintuitive procedure was used because it gave
the best results on the training set.
249
Language LAS UAS
Arabic 52.42 68.82
Chinese 72.72 78.37
Czech 51.86 66.36
Danish 71.56 78.16
Dutch 62.75 66.17
German 63.82 67.71
Japanese 84.35 87.31
Portuguese 70.35 79.46
Slovene 55.06 70.60
Spanish 69.63 73.89
Swedish 65.23 73.25
Turkish 60.31 71.54
Table 7: Labeled and unlabeled attachment scores.
generally performs below average. The lack of a
specialized parsing algorithm taking into account
sentence wide constraints and the lack of a prob-
abilistic component in the model are probably to
blame. Nevertheless, the particular decomposition
of the problem and the simplicity of the resulting
models provide some insight into the difficulties as-
sociated with individual languages.
References
A. Abeille?, editor. 2003. Treebanks: Building and Us-
ing Parsed Corpora, volume 20 of Text, Speech and
Language Technology. Kluwer Academic Publishers,
Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Flo-
resta sinta?(c)tica?: a treebank for Portuguese. In Proc.
of the Third Intern. Conf. on Language Resources and
Evaluation (LREC), pages 1698?1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annota-
tion process in the Turkish treebank. In Proc. of the 4th
Intern. Workshop on Linguistically Interpreteted Cor-
pora (LINC).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. In Proc. of the Tenth Conf. on Com-
putational Natural Language Learning (CoNLL-X).
SIGNLL.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
M. Civit Torruella and Ma A. Mart?? Anton??n. 2002. De-
sign principles for a Spanish treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. Z?abokrtsky, and A. Z?ele. 2006. Towards a Slovene
dependency treebank. In Proc. of the Fifth Intern.
Conf. on Language Resources and Evaluation (LREC).
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese treebank in VERBMOBIL. Verbmobil-
Report 240, Seminar fu?r Sprachwissenschaft, Univer-
sita?t Tu?bingen.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of the
Second Workshop on Treebanks and Linguistic Theo-
ries (TLT).
Volkan Kurt. 2005. Protein structure prediction using
decision lists. Master?s thesis, Koc? University.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from an-
tiquity. In Proc. of the NODALIDA Special Session on
Treebanks.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15.
Ronald L. Rivest. 1987. Learning decision lists. Ma-
chine Learning, 2:229?246.
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In Com-
putational Linguistics in the Netherlands (CLIN).
Deniz Yuret and Ferhan Ture. 2006. Learning mor-
phological disambiguation rules for Turkish. In HLT-
NAACL 06.
250
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 13?18,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 04:
Classification of Semantic Relations between Nominals
Roxana Girju
Univ. of Illinois
at Urbana-Champaign
Urbana, IL 61801
girju@uiuc.edu
Preslav Nakov
Univ. of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Vivi Nastase
EML Research gGmbH
Heidelberg, Germany 69118
nastase@eml-research.de
Stan Szpakowicz
University of Ottawa
Ottawa, ON K1N 6N5
szpak@site.uottawa.ca
Peter Turney
National Research Council of Canada
Ottawa, ON K1A 0R6
peter.turney@nrc-cnrc.gc.ca
Deniz Yuret
Koc? University
Istanbul, Turkey 34450
dyuret@ku.edu.tr
Abstract
The NLP community has shown a renewed
interest in deeper semantic analyses, among
them automatic recognition of relations be-
tween pairs of words in a text. We present an
evaluation task designed to provide a frame-
work for comparing different approaches to
classifying semantic relations between nom-
inals in a sentence. This is part of SemEval,
the 4th edition of the semantic evaluation
event previously known as SensEval. We de-
fine the task, describe the training/test data
and their creation, list the participating sys-
tems and discuss their results. There were
14 teams who submitted 15 systems.
1 Task Description and Related Work
The theme of Task 4 is the classification of semantic
relations between simple nominals (nouns or base
noun phrases) other than named entities ? honey
bee, for example, shows an instance of the Product-
Producer relation. The classification occurs in the
context of a sentence in a written English text. Al-
gorithms for classifying semantic relations can be
applied in information retrieval, information extrac-
tion, text summarization, question answering and so
on. The recognition of textual entailment (Tatu and
Moldovan, 2005) is an example of successful use of
this type of deeper analysis in high-end NLP appli-
cations.
The literature shows a wide variety of methods
of nominal relation classification. They depend as
much on the training data as on the domain of ap-
plication and the available resources. Rosario and
Hearst (2001) classify noun compounds from the
domain of medicine, using 13 classes that describe
the semantic relation between the head noun and
the modifier in a given noun compound. Rosario
et al (2002) classify noun compounds using the
MeSH hierarchy and a multi-level hierarchy of se-
mantic relations, with 15 classes at the top level.
Nastase and Szpakowicz (2003) present a two-level
hierarchy for classifying noun-modifier relations in
base noun phrases from general text, with 5 classes
at the top and 30 classes at the bottom; other re-
searchers (Turney and Littman, 2005; Turney, 2005;
Nastase et al, 2006) have used their class scheme
and data set. Moldovan et al (2004) propose a 35-
class scheme to classify relations in various phrases;
the same scheme has been applied to noun com-
pounds and other noun phrases (Girju et al, 2005).
Chklovski and Pantel (2004) introduce a 5-class set,
designed specifically for characterizing verb-verb
semantic relations. Stephens et al (2001) propose
17 classes targeted to relations between genes. La-
pata (2002) presents a binary classification of rela-
tions in nominalizations.
There is little consensus on the relation sets and
algorithms for analyzing semantic relations, and it
seems unlikely that any single scheme could work
for all applications. For example, the gene-gene re-
lation scheme of Stephens et al (2001), with rela-
tions like X phosphorylates Y, is unlikely to be trans-
ferred easily to general text.
We have created a benchmark data set to allow the
evaluation of different semantic relation classifica-
tion algorithms. We do not presume to propose a sin-
gle classification scheme, however alluring it would
13
Relation Training data Test data Agreement Example
positive set size positive set size (independent tagging)
Cause-Effect 52.1% 140 51.3% 80 86.1% laugh (cause) wrinkles (effect)
Instrument-Agency 50.7% 140 48.7% 78 69.6% laser (instrument) printer (agency)
Product-Producer 60.7% 140 66.7% 93 68.5% honey (product) bee (producer)
Origin-Entity 38.6% 140 44.4% 81 77.8% message (entity) from outer-space (origin)
Theme-Tool 41.4% 140 40.8% 71 47.8% news (theme) conference(tool)
Part-Whole 46.4% 140 36.1% 72 73.2% the door (part) of the car (whole)
Content-Container 46.4% 140 51.4% 74 69.1% the apples (content) in the basket (container)
Table 1: Data set statistics
be to try to design a unified standard ? it would be
likely to have shortcomings just as any of the others
we have just reviewed. Instead, we have decided to
focus on separate semantic relations that many re-
searchers list in their relation sets. We have built an-
notated data sets for seven such relations. Every data
set supports a separate binary classification task.
2 Building the Annotated Data Sets
Ours is a new evaluation task, so we began with data
set creation and annotation guidelines. The data set
that Nastase and Szpakowicz (2003) created had re-
lation labels and part-of-speech and WordNet sense
annotations, to facilitate classification. (Moldovan
et al, 2004; Girju et al, 2005) gave the annotators
an example of each phrase in a sentence along with
WordNet senses and position of arguments. Our
annotations include all these, to support a variety
of methods (since we work with relations between
nominals, the part of speech is always noun). We
have used WordNet 3.0 on the Web and sense index
tags.
We chose the following semantic relations:
Cause-Effect, Content-Container, Instrument-
Agency, Origin-Entity, Part-Whole, Product-
Producer and Theme-Tool. We wrote seven detailed
definitions, including restrictions and conventions,
plus prototypical positive and near-miss negative
examples. For each relation separately, we based
data collection on wild-card search patterns that
Google allows. We built the patterns manually,
following Hearst (1992) and Nakov and Hearst
(2006). Instances of the relation Content-Container,
for example, come up in response to queries such as
?* contains *?, ?* holds *?, ?the * in the *?. Fol-
lowing the model of the Senseval-3 English Lexical
Sample Task, we set out to collect 140 training and
at least 70 test examples per relation, so we had a
number of different patterns to ensure variety. We
also aimed to collect a balanced number of positive
and negative examples. The use of heuristic patterns
to search for both positive and negative examples
should naturally result in negative examples that
are near misses. We believe that near misses are
more useful for supervised learning than negative
examples that are generated randomly.
?Among the contents of the <e1>vessel</e1>
were a set of carpenter?s <e2>tools</e2>, sev-
eral large storage jars, ceramic utensils, ropes and
remnants of food, as well as a heavy load of ballast
stones.?
WordNet(e1) = ?vessel%1:06:00::?,
WordNet(e2) = ?tool%1:06:00::?,
Content-Container(e2, e1) = ?true?,
Query = ?contents of the * were a?
Figure 1: Annotations illustrated
Figure 1 illustrates the annotations. We tag the
nominals, so parsing or chunking is not necessary.
For Task 4, we define a nominal as a noun or base
noun phrase, excluding names entities. A base noun
phrase, e.g., lawn or lawn mower, is a noun with pre-
modifiers. We also exclude complex noun phrases
(e.g., with attached prepositional phrases ? the en-
gine of the lawn mower).
The procedure was the same for each relation.
One person gathered the sample sentences (aim-
ing approximately for a similar number of positive
and negative examples) and tagged the entities; two
other people annotated the sentences with WordNet
senses and classified the relations. The detailed re-
lation definitions and the preliminary discussions of
positive and negative examples served to maximize
the agreement between the annotators. They first
classified the data independently, then discussed ev-
ery disagreement and looked for consensus. Only
the agreed-upon examples went into the data sets.
Next, we split each data set into 140 training and
no fewer than 70 test examples. (We published the
training set for the Content-Container relation as de-
velopment data two months before the test set.) Ta-
ble 1 shows the number of positive and negative ex-
14
amples for each relation.1
The average inter-annotator agreement on rela-
tions (true/false) after the independent annotation
step was 70.3%, and the average agreement on
WordNet sense labels was 71.9%. In the process of
arriving at a consensus between annotators, the def-
inition of each relation was revised to cover explic-
itly cases where there had been disagreement. We
expect that these revised definitions would lead to
much higher levels of agreement than the original
definitions did.
3 The Participants
The task of classifying semantic relations between
nominals has attracted the participation of 14 teams
who submitted 15 systems. Table 4 lists the sys-
tems, the authors and their affiliations, and brief de-
scriptions. The systems? performance information
in terms of precision, recall, F -measure and accu-
racy, macroaveraged over all relations, appears in
Table 3. We computed these measures as described
in Lewis (1991).
We distinguish four categories of systems based
on the type of information used ? WordNet senses
and/or Google queries:
A ? WordNet = NO & Query = NO;
B ? WordNet = YES & Query = NO;
C ? WordNet = NO & Query = YES;
D ? WordNet = YES & Query = YES.
WordNet = ?YES? or WordNet = ?NO? tells us
only whether a system uses the WordNet sense la-
bels in the data sets. A system may use WordNet
internally for varied purposes, but ignore our sense
labels; such a system would be in category A or C .
Based on the input variation, each submitted system
may have up to 4 variations ? A,B,C,D.
Table 2 presents three baselines for a relation.
Majority always guesses either ?true? or ?false?,
whichever is the majority in the test set (maximizes
accuracy). Alltrue always guesses ?true? (maxi-
mizes recall). Probmatch randomly guesses ?true?
(?false?) with the probability matching the distribu-
tion of ?true? (?false?) in the test dataset (balances
precision and recall).
We present the results in Table 3 grouped by cat-
egory, to facilitate system comparison.
1As this paper serves also as a documentation of the data set,
the order of relations in the table is the same as in the data set.
Type P R F Acc
majority 81.3 42.9 30.8 57.0
alltrue 48.5 100.0 64.8 48.5
probmatch 48.5 48.5 48.5 51.7
Table 2: Baselines: precision, recall, F -measure and
accuracy averaged over the 7 binary classifications.
Team P R F Acc
A ? WordNet = NO & Query = NO
UCD-FC 66.1 66.7 64.8 66.0
ILK 60.5 69.5 63.8 63.5
UCB? 62.7 63.0 62.7 65.4
UMELB-B 61.5 55.7 57.8 62.7
UTH 56.1 57.1 55.9 58.8
UC3M 48.2 40.3 43.1 49.9
avg?stdev 59.2?6.3 58.7?10.5 58.0?8.1 61.1?6.0
B ? WordNet = YES & Query = NO
UIUC? 79.7 69.8 72.4 76.3
FBK-IRST 70.9 73.4 71.8 72.9
ILK 72.8 70.6 71.5 73.2
UCD-S1 69.9 64.6 66.8 71.4
UCD-PN 62.0 71.7 65.4 67.0
UC3M 66.7 62.8 64.3 67.2
CMU-AT 55.7 66.7 60.4 59.1
UCD-FC 66.4 58.1 60.3 63.6
UMELB-A 61.7 56.8 58.7 62.5
UVAVU 56.8 56.3 56.1 57.7
LCC-SRN 55.9 57.8 51.4 53.7
avg ? stdev 65.3?7.7 64.4?6.5 63.6?6.9 65.9?7.2
C ? WordNet = NO & Query = YES
UCB? 64.2 66.5 65.1 67.0
UCD-FC 66.1 66.7 64.8 66.0
UC3M 49.4 43.9 45.3 50.1
avg?stdev 59.9?9.1 59.0?13.1 58.4?11.3 61.0?9.5
D ? WordNet = YES & Query = YES
UTD-HLT-CG 67.3 65.3 62.6 67.2
UCD-FC 66.4 58.1 60.3 63.6
UC3M 60.9 57.8 58.8 62.3
avg?stdev 64.9?3.5 60.4?4.2 60.6?1.9 64.4?2.5
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 3: System performance grouped by category.
Precision, recall, F -measure and accuracy macro-
averaged over each system?s performance on all 7
relations.
4 Discussion
The highest average accuracy on Task 4 was 76.3%.
Therefore, the average initial agreement between an-
notators (70.3%), before revising the definitions, is
not an upper bound on the accuracy that can be
achieved. That the initial agreement between anno-
tators is not a good indicator of the accuracy that can
be achieved is also supported by the low correlation
15
System Institution Team Description System Type
UVAVU Univ. of Amsterdam
TNO Science & Industry
Free Univ. Amsterdam
Sophia Katrenko
Willem Robert van
Hage
similarity measures in WordNet; syn-
tactic dependencies; lexical patterns;
logical combination of attributes
B
CMU -AT Carnegie Mellon Univ. Alicia Tribble
Scott E. Fahlman
WordNet; manually-built ontologies;
Scone Knowledge Representation Lan-
guage; semantic distance
B
ILK Tilburg University Caroline Sporleder
Roser Morante
Antal van den Bosch
semantic clusters based on noun simi-
larity; WordNet supersenses; grammat-
ical relation between entities; head of
sentence; WEKA
A, B
FBK-IRST Fondazione Bruno
Kessler - IRST
Claudio Giuliano
Alberto Lavelli
Daniele Pighin
Lorenza Romano
shallow and deep syntactic information;
WordNet synsets and hypernyms; ker-
nel methods; SVM
B
LCC-SRN Language Computer
Corp.
Adriana Badulescu named entity recognition; lexical, se-
mantic, syntactic features; decision tree
and semantic scattering
B
UMELB-A Univ. of Melbourne Su Kim
Timothy Baldwin
sense collocations; similarity of con-
stituents; extending training and testing
data using similar words
B
UMELB-B Univ. of Melbourne Su Kim
Timothy Baldwin
similarity of nearest-neighbor matching
over the union of senses for the two
nominals; cascaded tagging with de-
creasing thresholds
A
UCB? Univ. of California at
Berkeley
Preslav Nakov
Marti Hearst
VSM; joining terms; KNN-1 A, C
UC3M Univ. Carlos III of Madrid Isabel Segura Bedmar
Doaa Sammy
Jose? Luis Mart??nez
Ferna?ndez
WordNet path; syntactic features; SVM A, B, C, D
UCD-S1 Univ. College Dublin Cristina Butnariu
Tony Veale
lexical-semantic categories from Word-
Net; syntactic patterns from corpora,
SVM
B
UCD-FC Univ. College Dublin Fintan Costello WordNet; additional noun compounds
tagged corpus; Naive Bayes
A, B, C, D
UCD-PN Univ. College Dublin Paul Nulty WordNet supersenses; web-based fre-
quency counts for specific joining
terms; WEKA (SMO)
B
UIUC? Univ. of Illinois at Urbana
Champaign
Roxana Girju
Brandon Beamer
Suma Bhat
Brant Chee
Andrew Fister
Alla Rozovskaya
features based on WordNet, NomLex-
PLUS, grammatical roles, lexico-
syntactic patterns, semantic parses
B
UTD-HLT-CG Univ. of Texas at Dallas Cristina Nicolae
Garbiel Nicolae
Sanda Harabagiu
lexico-semantic features from Word-
Net, VerbNet; semantic features from a
PropBank parser; dependency features
D
UTH Univ. of Tokio Eiji Aramaki
Takeshi Imai
Kengo Miyo
Kazuhiko Ohe
joining phrases; physical size for enti-
ties; web-mining; SVM
A
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 4: Short description of the teams and the participating systems.
16
Relation Team Type P R F Acc Test size Base-F Base-Acc Avg. rank
Cause-Effect UIUC B4 69.5 100.0 82.0 77.5 80 67.8 51.2 3.4
Instrument-Agency FBK-IRST B4 76.9 78.9 77.9 78.2 78 65.5 51.3 3.4
Product-Producer UCD-S1 B4 80.6 87.1 83.7 77.4 93 80.0 66.7 1.7
Origin-Entity ILK B3 70.6 66.7 68.6 72.8 81 61.5 55.6 6.0
Theme-Tool ILK B4 69.0 69.0 69.0 74.6 71 58.0 59.2 6.0
Part-Whole UC3M B4 72.4 80.8 76.4 81.9 72 53.1 63.9 4.5
Content-Container UIUC B4 93.1 71.1 80.6 82.4 74 67.9 51.4 3.1
Table 5: The best results per relation. Precision, recall, F -measure and accuracy macro-averaged over each
system?s performance on all 7 relations. Base-F shows the baseline F -measure (alltrue), Base-Acc ? the
baseline accuracy score (majority). The last column shows the average rank for each relation.
of 0.15 between the Acc column in Table 5 and the
Agreement column in Table 1.
We performed various analyses of the results,
which we summarize here in four questions. We
write Xi to refer to four possible system categories
(Ai, Bi, Ci, and Di) with four possible amounts of
training data (X1 for training examples 1 to 35, X2
for 1 to 70, X3 for 1 to 105, and X4 for 1 to 140).
Does more training data help?
Overall, the results suggest that more training data
improves the performance. There were 17 cases in
which we had results for all four possible amounts
of training data. All average F -measure differences,
F (X4)?F (Xi) where X = A to D, i = 1 to 3, for
these 17 sets of results are statistically significant:
F (X4)?F (X1): N = 17, avg = 8.3, std = 5.8, min =
1.1, max = 19.6, t-value = ?5.9, p-value = 0.00001.
F (X4)?F (X2): N = 17, avg = 4.0, std = 3.7, min =
?3.5, max = 10.5, t-value = 4.5, p-value = 0.0002.
F (X4)?F (X3): N = 17, avg = 0.9, std = 1.7, min =
?2.6, max = 4.7, t-value = 2.1, p-value = 0.03.
Does WordNet help?
The statistics show that WordNet is important, al-
though the contribution varies across systems. Three
teams submitted altogether 12 results both for A1?
A4 and B1?B4. The average F -measure difference,
F (Bi)?F (Ai), i = 1 to 4, is significant:
F (Bi)?F (Ai): N = 12, avg = 6.1, std = 8.4, min =
?4.5, max = 21.2, t-value = ?2.5, p-value = 0.01.
The results of the UCD-FC system actually went
down when WordNet was used. The statistics for the
remaining two teams, however, are a bit better:
F (Bi)?F (Ai): N = 8, avg = 10.4, std = 6.7, min =
?1.0, max = 21.2, t-value = ?4.4, p-value = 0.002.
Does knowing the query help?
Overall, knowing the query did not seem to improve
the results. Three teams submitted 12 results both
for A1?A4 and C1?C4. The average F -measure dif-
ference, F (Ci)?F (Ai) , i = 1 to 4, is not significant:
F (Ci)?F (Ai): N = 12, avg = 0.9, std = 1.8, min =
?2.0, max = 5.0, t-value = ?1.6, p-value = 0.06.
Again, the UCD-FC system differed from the
other systems in that the A and C scores were iden-
tical, but even averaging over the remaining two sys-
tems and 8 cases does not show a statistically signif-
icant advantage:
F (Ci)?F (Ai): N = 8, avg = 1.3, std = 2.2, min =
?2.0, max = 5.0, t-value = ?1.7, p-value = 0.07.
Are some relations harder to classify?
Table 5 shows the best results for each relation in
terms of precision, recall, and F -measure, per team
and system category. Column Base-F presents the
baseline F -measure (alltrue), while Base-Acc the
baseline accuracy score (majority). For all seven re-
lations, the best team significantly outperforms the
baseline. The category of the best-scoring system
in almost every case is B4 (only the ILK B4 system
scored second on the Origin-Entity relation).
Table 5 suggests that some relations are more dif-
ficult to classify than others. The best F -measure
ranges from 83.7 for Product?Producer to 68.6 for
Origin?Entity. The difference between the best F -
measure and the baseline F -measure ranges from
23.3 for Part-Whole to 3.7 for Product-Producer.
The difference between the best accuracy and the
baseline accuracy ranges from 31.0 for Content-
Container to 10.7 for Product-Producer.
The F column shows the best result for each rela-
tion, but similar differences among the relations may
be observed when all results are pooled. The Avg.
rank column computes the average rank of each re-
lation in the ordered list of relations generated by
each system. For example, Product?Producer is of-
ten listed as the first or the second easiest relation
(with an average rank of 1.7), while Origin?Entity
and Theme?Tool are identified as the most difficult
17
relations to classify (with average ranks of 6.0).
5 Conclusion
This paper describes a new semantic evaluation task,
Classification of Semantic Relations between Nom-
inals. We have accomplished our goal of providing
a framework and a benchmark data set to allow for
comparisons of methods for this task. The data in-
cluded different types of information ? lexical se-
mantic information, context, query used ? meant to
facilitate the analysis of useful sources of informa-
tion for determining the semantic relation between
nominals. The results that the participating systems
have reported show successful approaches to this
difficult task, and the advantages of using lexical se-
mantic information.
The success of the task ? measured in the inter-
est of the community and the results of the partici-
pating systems ? shows that the framework and the
data are useful resources. By making this collection
freely accessible, we encourage further research into
this domain and integration of semantic relation al-
gorithms in high-end applications.
Acknowledgments
We thank Eneko Agirre, Llu??s Ma`rquez and Richard
Wicentowski, the organizers of SemEval 2007, for
their guidance and prompt support in all organiza-
tional matters. We thank Marti Hearst for valu-
able advice throughout the task description and de-
bates on semantic relation definitions. We thank the
anonymous reviewers for their helpful comments.
References
T. Chklovski and P. Pantel. 2004. Verbocean: Mining the
web for fine-grained semantic verb relations. In Proc.
Conf. on Empirical Methods in Natural Language Pro-
cessing, EMNLP-04, pages 33?40, Barcelona, Spain.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19:479?496.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. 14th International
Conf. on Computational Linguistics (COLING-92),
pages 539?545.
M. Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
D.D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the Speech and Natural Language
Workshop, pages 312?318, Asilomar.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and
R. Girju. 2004. Models for the semantic classification
of noun phrases. In Proc. Computational Lexical Se-
mantics Workshop at HLT-NAACL 2004, pages 60?67,
Boston, MA.
P. Nakov and M. Hearst. 2006. Using verbs to char-
acterize noun-noun relations. In Proc. Twelfth Inter-
national Conf. in Artificial Intelligence (AIMSA-06),
pages 233?244, Varna,Bulgaria.
V. Nastase and S. Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301, Tilburg, The Netherlands.
V. Nastase, J. Sayyad-Shirabad, M. Sokolova, and S. Sz-
pakowicz. 2006. Learning noun-modifier semantic
relations with corpus-based and WordNet-based fea-
tures. In Proc. 21st National Conf. on Artificial Intel-
ligence (AAAI 2006), pages 781?787, Boston, MA.
B. Rosario and M. Hearst. 2001. Classifying the seman-
tic relations in noun-compounds via domain-specific
lexical hierarchy. In Proc. 2001 Conf. on Empirical
Methods in Natural Language Processing (EMNLP-
01), pages 82?90.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The de-
scent of hierarchy, and selection in relational seman-
tics. In Proc. 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages 417?
424, Philadelphia, PA.
M. Stephens, M. Palakal, S. Mukhopadhyay, and R. Raje.
2001. Detecting gene relations from MEDLINE ab-
stracts. In Proc. Sixth Annual Pacific Symposium on
Biocomputing, pages 483?496.
M. Tatu and D. Moldovan. 2005. A semantic approach to
recognizing textual entailment. In Proc. Human Lan-
guage Technology Conf. and Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP
2005), pages 371?378, Vancouver, Canada.
P.D. Turney and M.L. Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60(1-3):251?278.
P.D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. Nineteenth Interna-
tional Joint Conf. on Artificial Intelligence (IJCAI-05),
pages 1136?1141, Edinburgh, Scotland.
18
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 207?214,
Prague, June 2007. c?2007 Association for Computational Linguistics
KU: Word Sense Disambiguation by Substitution
Deniz Yuret
Koc? University
Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
Data sparsity is one of the main factors that
make word sense disambiguation (WSD)
difficult. To overcome this problem we need
to find effective ways to use resources other
than sense labeled data. In this paper I de-
scribe a WSD system that uses a statistical
language model based on a large unanno-
tated corpus. The model is used to evalu-
ate the likelihood of various substitutes for a
word in a given context. These likelihoods
are then used to determine the best sense for
the word in novel contexts. The resulting
system participated in three tasks in the Se-
mEval 2007 workshop. The WSD of prepo-
sitions task proved to be challenging for the
system, possibly illustrating some of its lim-
itations: e.g. not all words have good sub-
stitutes. The system achieved promising re-
sults for the English lexical sample and En-
glish lexical substitution tasks.
1 Introduction
A typical word sense disambiguation system is
trained on a corpus of manually sense tagged text.
Machine learning algorithms are then employed to
find the best sense for a word in a novel context
by generalizing from the training examples. The
training data is costly to generate and inter-annotator
agreement is difficult to achieve. Thus there is very
little training data available: the largest single cor-
pus of sense tagged text, SemCor, has 41,497 sense
tagged words. (Yuret, 2004) observed that approxi-
mately half of the test instances do not match any of
the contextual features learned from the training data
for an all words disambiguation task. (Yarowsky and
Florian, 2002) found that each successive doubling
of the training data only leads to a 3-4% error reduc-
tion within their experimental range.
Humans do not seem to be cursed with an expo-
nential training data requirement to become profi-
cient with the use of a word. Dictionaries typically
contain a definition and one or two examples of us-
age for each sense. This seems to be sufficient for
a human to use the word correctly in contexts that
share no surface features with the dictionary exam-
ples. The 108 waking seconds it takes a person to
become proficient in a language does not seem suf-
ficient to master all the words and their different
senses. We need models that do not require large
amounts of annotated text to perform WSD.
What possible process can explain our proficiency
without relying on a lot of labeled data? Let us look
at a concrete example: The two most frequent senses
of the word ?board? according to WordNet 3.0 (Fell-
baum, 1998) are the ?committee? sense, and the
?plank? sense. When we hear a sentence like ?There
was a board meeting?, it is immediately obvious that
the first sense is intended. One hypothesis is that a
common sense inference engine in your brain rules
out the second sense. Maybe you visualize pieces
of timber sitting around a meeting table and decide
that it is absurd. Another hypothesis is that the plank
sense does not even occur to you because you hear
this sentence in the middle of a conversation about
corporate matters. Therefore the plank sense is not
psychologically ?primed?. Finally, maybe you sub-
consciously perform a substitution and the sentence
207
?There was a plank meeting? just sounds bad to your
linguistic ?ear?.
In this paper I will describe a system that judges
potential substitutions in a given context using a sta-
tistical language model as a surrogate for the linguis-
tic ?ear?. The likelihoods of the various substitutes
are used to select the best sense for a target word.
The use of substitutes for WSD is not new. (Lea-
cock et al, 1998) demonstrated the use of related
monosemous words (monosemous relatives) to col-
lect examples for a given sense from the Internet.
(Mihalcea, 2002) used the monosemous relatives
technique for bootstrapping the automatic acquisi-
tion of large sense tagged corpora. In both cases, the
focus was on collecting more labeled examples to be
subsequently used with supervised machine learn-
ing techniques. (Martinez et al, 2006) extended the
method to make use of polysemous relatives. More
importantly, their method places these relatives in
the context of the target word to query a search en-
gine and uses the search results to predict the best
sense in an unsupervised manner.
There are three areas that distinguish my system
from the previous work: (i) The probabilities for
substitutes in context are determined using a statisti-
cal language model rather than search hits on heuris-
tically constructed queries, (ii) The set of substitutes
are derived from multiple sources and optimized us-
ing WSD performance as the objective function, and
(iii) A probabilistic generative model is used to se-
lect the best sense rather than typical machine learn-
ing algorithms or heuristics. Each of these areas is
explained further below.
Probabilities for substitutes: Statistical language
modeling is the art of determining the probability of
a sequence of words. According to the model used
in this study, the sentence ?There was a committee
meeting? is 17,629 times more likely than the sen-
tence ?There was a plank meeting?. Thus, a statis-
tical language model can be used as a surrogate for
your inner ear that decides what sounds good and
what sounds bad. I used a language model based on
the Web 1T 5-gram dataset (Brants and Franz, 2006)
which gives the counts of 1 to 5-grams in a web cor-
pus of 1012 words. The details of the Web1T model
are given in the Appendix.
Given that I criticize existing WSD algorithms for
using too much data, it might seem hypocritical to
employ a data source with 1012 words. In my de-
fense, from an engineering perspective, an unanno-
tated 1012 word corpus exists, whereas large sense
tagged corpora do not. From a scientific perspective,
it is clear that no human ever comes close to expe-
riencing 1012 words, but they do outperform simple
n-gram language models based on that much data in
predicting the likelihood of words in novel contexts
(Shannon, 1951). So, even though we do not know
how humans do it, we do know that they have the
equivalent of a powerful statistical language model
in their heads.
Selecting the best substitutes: Perhaps more im-
portant for the performance of the system is the deci-
sion of which substitutes to try. We never thought of
using ?monkey? as a potential substitute for ?board?.
One possibility is to use the synonyms in Word-
Net which were selected such that they can be in-
terchanged in at least some contexts. However 54%
of WordNet synsets do not have any synonyms. Be-
sides, synonymous words would not always help if
they share similar ambiguities in meaning. Substi-
tutes that are not synonyms, on the other hand, may
be very useful such as ?hot? vs. ?cold? or ?car?
vs. ?truck?. In general we are looking for potential
substitutes that have a high likelihood of appearing
in contexts that are associated with a specific sense
of the target word. The substitute selection method
used in this work is described in Section 3.
Selecting the best sense: Once we have a lan-
guage model and a set of substitutes to try, we need
a decision procedure that picks the best sense of a
word in a given context. An unsupervised system
can be designed to keep track of the sense associ-
ated with each substitute based on the lexical re-
source used. However since I used multiple lexical
resources, and had training data available, I chose a
supervised approach. For each instance in the train-
ing set, the likelihood of each substitute is deter-
mined. Then instances of a single sense are grouped
together to yield a probability distribution over the
substitutes for that sense. When a test instance is
encountered its substitute distribution is compared
to that of each sense to select the most appropriate
one. Section 2 describes the sense selection proce-
dure in detail.
208
We could say each context is represented with
the likelihood it assigns to various substitutes rather
than its surface features. That way contexts that do
not share any surface features can be related to each
other.
Results: To summarize the results, in the Word
Sense Disambiguation of Prepositions Task, the sys-
tem achieved 54.7% accuracy1 . This is 15.1% above
the baseline of picking the most frequent sense
but 14.6% below the best system. In the Coarse
Grained English Lexical Sample WSD Task, the sys-
tem achieved 85.1% accuracy, which is 6.4% above
the baseline of picking the most frequent sense and
3.6% below the best system. Finally, in the English
Lexical Substitution Task, the system achieved the
top result for picking the best substitute for each
word.
2 Sense Selection Procedure
Consider a target word w0 with n senses S =
{s1, . . . , sn}. Let Cj = {cj1, cj2, . . .} be the set
of contexts in the training data where w0 has been
tagged with sense sj . The prior probability of a
sense sj will be defined as:
P (sj) =
|Cj |
?n
k=1 |Ck|
Suppose we decide to use m substitutes W =
{w1, . . . , wm}. The selection of the possible sub-
stitutes is discussed in Section 3. Let P (wi, c) de-
note the probability of the context c where the target
word has been replaced with wi. This probability is
obtained from the Web1T language model. The con-
ditional probability of a substitute wi in a particular
context c is defined as:
P (wi|c) =
P (wi, c)
?
w?W P (w, c)
The conditional probability of a substitute wi for
a particular sense sj is defined as:
P (wi|sj) =
1
|Cj |
?
c?Cj
P (wi|c)
1In all the tasks participated, the system submitted a unique
answer for each instance. Therefore precision, recall, F-
measure, and accuracy have the same value. I will use the term
accuracy to represent them all.
Given a test context ct, we would like to find out
which sense sj it is most likely to represent:
argmaxj P (sj |ct) ? P (ct|sj)P (sj)
To calculate the likelihood of the test context
P (ct|sj), we first find the conditional probability
distribution of the substitutes P (wi|ct), as described
above. Treating these probabilities as fractional
counts we can express the likelihood as:
P (ct|sj) ?
?
w?W
P (w|sj)P (w|ct)
Thus we choose the sense that maximizes the pos-
terior probability:
argmaxjP (sj)
?
w?W
P (w|sj)P (w|ct)
3 Substitute Selection Procedure
Potential substitutes for a word were selected from
WordNet 3.0 (Fellbaum, 1998), and the Roget The-
saurus (Thesaurus.com, 2007).
When selecting the WordNet substitutes, the pro-
gram considered all synsets of the target word and
neighboring synsets accessible following a single
link. All words contained within these synsets and
their glosses were considered as potential substi-
tutes.
When selecting the Roget substitutes, the program
considered all entries that included the target word.
By default, the entries that included the target word
as part of a multi word phrase and entries that had
the wrong part of speech were excluded.
I observed that the particular set of substitutes
used had a large impact on the disambiguation per-
formance in cross validation. Therefore I spent a
considerable amount of effort trying to optimize the
substitute sets. The union of the WordNet and Ro-
get substitutes were first sorted based on their dis-
criminative power measured by the likelihood ratio
of their best sense:
LR(wi) = maxj
P (wi|sj)
P (wi|sj)
The following optimization algorithms were then
run to maximize the leave-one-out cross validation
(loocv) accuracy on the lexical sample WSD train-
ing data.
209
1. Each substitute was temporarily deleted and the
resulting gain in loocv was noted. The sub-
stitute that led to the highest gain was perma-
nently deleted. The procedure was repeated un-
til no further loocv gain was possible.
2. Each pair of substitutes were tried alone and
the pair that gave the highest loocv score was
chosen as the initial list. Other substitutes were
then greedily added to this list until no further
loocv gain was possible.
3. Golden section search was used to find the ideal
cutoff point in the list of substitutes sorted by
likelihood ratio. Substitutes below the cutoff
point were deleted.
None of these algorithms consistently gave the
best result. Thus, each algorithm was run for each
target word and the substitute set that gave the best
loocv result was used for the final testing. The loocv
gain from using the optimized substitute sets instead
of the initial union of WordNet and Roget substi-
tutes was significant. For example the average gain
was 9.4% and the maximum was 38% for the En-
glish Lexical Sample WSD task.
4 English Lexical Substitution
The English Lexical Substitution Task (McCarthy
and Navigli, 2007), for both human annotators and
systems is to replace a target word in a sentence with
as close a word as possible. It is different from the
standard WSD tasks in that there is no sense repos-
itory used, and even the identification of a discrete
sense is not necessary.
The task used a lexical sample of 171 words with
10 instances each. For each instance the human
annotators selected several substitutes. There were
three subtasks: best: scoring the best substitute for
a given item, oot: scoring the best ten substitutes for
a given item, and mw: detection and identification
of multi-words. The details of the subtasks and scor-
ing can be found in (McCarthy and Navigli, 2007).
My system participated in the first two subtasks.
Because there is no training set, the supervised
optimization of the substitute set using the algo-
rithms described in Section 3 is not applicable.
Based on the trial data, I found that the Roget substi-
tutes work better than the WordNet substitutes most
BEST P R Mode P Mode R
all 12.90 12.90 20.65 20.65
Further Analysis
NMWT 13.39 13.39 21.20 21.20
NMWS 14.33 13.98 21.88 21.42
RAND 12.67 12.67 20.34 20.34
MAN 13.16 13.16 21.01 21.01
OOT P R Mode P Mode R
all 46.15 46.15 61.30 61.30
Further Analysis
NMWT 48.43 48.43 63.42 63.42
NMWS 49.72 49.72 63.74 63.74
RAND 47.80 47.80 62.84 62.84
MAN 44.23 44.23 59.55 59.55
Table 1: BEST and OOT results: P is precision, R
is recall, Mode indicates accuracy selecting the sin-
gle preferred substitute when there is one, NMWT
is the score without items identified as multi-words,
NMWS is the score using only single word substi-
tutes, RAND is the score for the items selected ran-
domly, and MAN is the score for the items selected
manually.
of the time. The antonyms in each entry and the
entries that did not have the target word as the head
were filtered out to improve the accuracy. Antonyms
happen to be good substitutes for WSD, but not so
good for lexical substitution.
For the final output of the system, the substitutes
wi in a context c were simply sorted by P (wi, c)
which is calculated based on the Web1T language
model.
In the best subtask the system achieved 12.9% ac-
curacy, which is the top score and 2.95% above the
baseline. The system was able to find the mode (a
single substitute preferred to the others by the anno-
tators) in 20.65% of the cases when there was one,
which is 5.37% above the baseline and 0.08% be-
low the top score. The top part of Table 1 gives
the breakdown of the best score, see (McCarthy and
Navigli, 2007) for details.
The low numbers here are partly a consequence of
the scoring formula used. Specifically, the score for
a single item is bounded by the frequency of the best
substitute in the gold standard file. Therefore, the
210
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 10  100  1000  10000
Ac
cu
ra
cy
 a
bo
ve
 b
as
el
in
e
Number of training instances
Figure 1: Training set size vs. accuracy above base-
line for the English lexical sample task.
highest achievable score was not 100%, but 45.76%.
A more intuitive way to look at the result may be
the following: Human annotators assigned 4.04 dis-
tinct substitutes for each instance on average, and
my system was able to guess one of these as the best
in 33.73% of the cases.
In the oot subtask the system achieved 46.15%
accuracy, which is 16.45% above the baseline and
22.88% below the top result. The system was able to
find the mode as one of its 10 guesses in 61.30% of
the cases when there was a mode, which is 20.73%
above the best baseline and 4.96% below the top
score. Unlike the best scores, 100% accuracy is pos-
sible for oot. Each item had 1 to 9 distinct substi-
tutes in the gold standard, so an ideal system could
potentially cover them all with 10 guesses. The sec-
ond part of Table 1 gives the breakdown of the oot
score.
In conclusion, selecting substitutes based on a
standard repository like Roget and ranking them us-
ing the ngram language model gives a good base-
line for this task. To improve the performance along
these lines we need better language models, and bet-
ter substitute selection procedures. Even the best
language model will only tell us which words are
most likely to replace our target word, not which
ones preserve the meaning. Relying on reposito-
ries like Roget for the purpose of substitute selection
seems ad-hoc and better methods are needed.
5 English Lexical Sample WSD
The Coarse-Grained English Lexical Sample WSD
Task (Palmer et al, 2007), provided training and
test data for sense disambiguation of 65 verbs and
35 nouns. On average there were 223 training and
49 testing instances for each word tagged with an
OntoNote sense tag (Hovy et al, 2006). OntoNote
sense tags are groupings of WordNet senses that
are more coarse-grained than traditional WN entries,
and which have achieved on average 90% inter-
annotator agreement. The number of senses for a
word ranged from 1 to 13 with an average of 3.6.
I used substitute sets optimized for each word as
described in Section 3. Then a single best sense for
each test instance was selected based on the model
given in Section 2. The system achieved 85.05% ac-
curacy, which is 6.39% above the baseline of pick-
ing the most frequent sense and 3.65% below the top
score.
These numbers seem higher than previous Sen-
seval lexical sample tasks. The best system in
Senseval-3 (Mihalcea et al, 2004; Grozea, 2004)
achieved 72.9% fine grained, 79.3% coarse grained
accuracy. Many factors may have played a role but
the most important one is probably the sense inven-
tory. The nouns and verbs in Senseval-3 had 6.1 fine
grained and 4.5 coarse grained senses on average.
The leave-one-out cross-validation result of my
system on the training set was 83.21% with the un-
filtered union of Roget and WordNet substitutes, and
90.69% with the optimized subset. Clearly there is
some over-fitting in the substitute optimization pro-
cess which needs to be improved.
Table 2 details the performance on individual
words. The accuracy is 88.67% on the nouns and
81.02% on the verbs. One can clearly see the rela-
tion of the performance with the number of senses
(decreasing) and the frequency of the first sense (in-
creasing). Interestingly no clear relation exists be-
tween the training set size and the accuracy above
the baseline. Figure 1 plots the relationship between
training set size vs. the accuracy gain above the most
frequent sense baseline. This could indicate that the
system peaks at a low training set size and general-
izes well because of the language model. However,
it should be noted that each point in the plot rep-
resents a different word, not experiments with the
211
same word at different training set sizes. Thus the
difficulty of each word may be the overriding factor
in determining performance. A more detailed study
similar to (Yarowsky and Florian, 2002) is needed to
explore the relationship in more detail.
6 WSD of Prepositions
The Word Sense Disambiguation of Prepositions
Task (Litkowski and Hargraves, 2007), provided
training and test data for sense disambiguation of
34 prepositions. On average there were 486 train-
ing and 234 test instances for each preposition. The
number of senses for a word ranged from 1 to 20
with an average of 7.4.
The system described in Sections 2 and 3 were
applied to this task as well. WordNet does not have
information about prepositions, so most of the can-
didate substitutes were obtained from Roget and The
Preposition Project (Litkowski, 2005). After opti-
mizing the substitute sets the system achieved 54.7%
accuracy which is 15.1% above the most frequent
sense baseline and 14.6% below the top result. Un-
fortunately there were only three teams that partic-
ipated in this task. The detailed breakdown of the
results can be seen in the second part of Table 2.
The loocv result on the training data with the ini-
tial unfiltered set of substitutes was 51.70%. Opti-
mizations described in Section 3 increased this to
59.71%. This increase is comparable to the one
in the lexical substitution task. The final result of
54.7% shows signs of overfitting in the substitute se-
lection process.
The average gain above the baseline for preposi-
tions (39.6% to 54.7%) is significantly higher than
the English lexical sample task (78.7% to 85.1%).
However the preposition numbers are generally
lower compared to the nouns and verbs because they
are more ambiguous: the number of senses is higher
and the first sense frequency is lower.
Good quality substitutes are difficult to find for
prepositions. Unlike common nouns and verbs,
common prepositions play unique roles in language
and are difficult to replace. Open class words have
synonyms, hypernyms, antonyms etc. that provide
good substitutes: it is easy to come up with ?I ate
halibut? when you see ?I ate fish?. It is not as easy
to replace ?of? in the phrase ?the president of the
company?. Even when there is a good substitute,
e.g. ?over? vs. ?under?, the two prepositions usually
share the exact same ambiguities: they can both ex-
press a physical direction or a quantity comparison.
Therefore the substitution based model presented in
this work may not be a good match for preposition
disambiguation.
7 Contributions and Future Work
A WSD method employing a statistical language
model was introduced. The language model is used
to evaluate the likelihood of possible substitutes for
the target word in a given context. Each context is
represented with its preferences for possible substi-
tutes, thus contexts with no surface features in com-
mon can nevertheless be related to each other.
The set of substitutes used for a word had a large
effect on the performance of the resulting system. A
substitute selection procedure that uses the language
model itself rather than external lexical resources
may work better.
I hypothesize that the model would be advanta-
geous on tasks like ?all words? WSD, where data
sparseness is paramount, because it is able to link
contexts with no surface features in common. It can
be used in an unsupervised manner where the sub-
stitutes and their associated senses can be obtained
from a lexical resource. Work along these lines was
not completed due to time limitations.
Finally, there are two failure modes for the algo-
rithm: either there are no good substitutes that dif-
ferentiate the various senses (as I suspect is the case
for some prepositions), or the language model does
not yield accurate preferences among the substitutes
that correspond to our intuition. In the first case we
have to fall back on other methods, as the substi-
tutes obviously are of limited value. The correspon-
dence between the language model and our intuition
requires further study.
Appendix: Web1T Language Model
The Web 1T 5-gram dataset (Brants and Franz,
2006) that was used to build a language model for
this work consists of the counts of word sequences
up to length 5 in a 1012 word corpus derived from
the Web. The data consists of mostly English words
that have been tokenized and sentence tagged. To-
212
kens that appear less than 200 times and ngrams that
appear less than 40 times have been filtered out.
I used a smoothing method loosely based on the
one-count method given in (Chen and Goodman,
1996). Because ngrams with low counts are not in-
cluded in the data I used ngrams with missing counts
instead of ngrams with one counts. The missing
count is defined as:
m(wi?1i?n+1) = c(wi?1i?n+1) ?
?
wi
c(wii?n+1)
where wii?n+1 indicates the n-word sequence end-
ing with wi, and c(wii?n+1) is the count of this se-
quence. The corresponding smoothing formula is:
P (wi|wi?1i?n+1) =
c(wii?n+1) + (1 + ?n)m(wi?1i?n+1)P (wi|wi?1i?n+2)
c(wi?1i?n+1) + ?nm(wi?1i?n+1)
The parameters ?n > 0 for n = 2 . . . 5 was opti-
mized on the Brown corpus to yield a cross entropy
of 8.06 bits per token. The optimized parameters are
given below:
?2 = 6.71, ?3 = 5.94, ?4 = 6.55, ?5 = 5.71
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Linguistic Data Consortium, Philadelphia.
LDC2006T13.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the ACL.
Christiane Fellbaum, editor. 1998. Wordnet: An Elec-
tronic Lexical Database. MIT Press.
Cristian Grozea. 2004. Finding optimal parameter set-
tings for high performance word sense disambigua-
tion. In Proceedings of Senseval-3: The Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text.
Eduard H. Hovy, M. Marcus, M. Palmer, S. Pradhan,
L. Ramshaw, and R. Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology / North American Association of
Computational Linguistics conference (HLT-NAACL
2006), New York, NY. Short paper.
Claudia Leacock, Martin Chodorow, and George A.
Miller. 1998. Using corpus statistics and wordnet
relations for sense identification. Computational Lin-
guistics, 24(1):147?166, March.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word sense disambiguation of preposi-
tions. In SemEval-2007: 4th International Workshop
on Semantic Evaluations.
K. C. Litkowski. 2005. The preposition project. In
Proceedings of the Second ACL-SIGSEM Workshop on
The Linguistic Dimensions of Prepositions and their
Use in Computational Linguistics Formalisms and Ap-
plications, Colchester, England, April. University of
Essex.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop (ALTW 2006), pages
42?50.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 Task 10: English lexical substitution task. In
SemEval-2007: 4th International Workshop on Se-
mantic Evaluations.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of Senseval-3: The Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text.
Rada Mihalcea. 2002. Bootstrapping large sense tagged
corpora. In Proceedings of the 3rd International
Conference on Languages Resources and Evaluations
LREC 2002, Las Palmas, Spain, May.
Martha Palmer, Sameer Pradhan, and Edward Loper.
2007. SemEval-2007 Task 17: English lexical sample,
English SRL and English all-words tasks. In SemEval-
2007: 4th International Workshop on Semantic Evalu-
ations.
Claude Elwood Shannon. 1951. Prediction and entropy
of printed English. The Bell System Technical Journal,
30:50?64.
Thesaurus.com. 2007. Roget?s New
MillenniumTMThesaurus, First Edition (v
1.3.1). Lexico Publishing Group, LLC.
http://thesaurus.reference.com.
David Yarowsky and Radu Florian. 2002. Evaluating
sense disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293?310.
Deniz Yuret. 2004. Some experiments with a Naive
Bayes WSD system. In ACL 2004 Senseval-3 Work-
shop, Barcelona, Spain, July.
213
English Lexical Sample WSD
lexelt trn/tst s mfs acc lexelt trn/tst s mfs acc
affect.v 45/19 1 1.000 1.000 allow.v 108/35 2 0.971 0.971
announce.v 88/20 2 1.000 1.000 approve.v 53/12 2 0.917 0.917
area.n 326/37 3 0.703 0.838 ask.v 348/58 6 0.517 0.759
attempt.v 40/10 1 1.000 1.000 authority.n 90/21 4 0.238 0.714
avoid.v 55/16 1 1.000 1.000 base.n 92/20 5 0.100 0.650
begin.v 114/48 4 0.562 0.792 believe.v 202/55 2 0.782 0.836
bill.n 404/102 3 0.755 0.902 build.v 119/46 3 0.739 0.543
buy.v 164/46 5 0.761 0.783 capital.n 278/57 4 0.965 0.982
care.v 69/7 3 0.286 1.000 carrier.n 111/21 7 0.714 0.667
cause.v 73/47 1 1.000 1.000 chance.n 91/15 4 0.400 0.667
claim.v 54/15 3 0.800 0.800 come.v 186/43 10 0.233 0.372
complain.v 32/14 2 0.857 0.857 complete.v 42/16 2 0.938 0.938
condition.n 132/34 2 0.765 0.765 contribute.v 35/18 2 0.500 0.500
defense.n 120/21 7 0.286 0.476 describe.v 57/19 3 1.000 1.000
development.n 180/29 3 0.621 0.759 disclose.v 55/14 1 0.929 0.929
do.v 207/61 4 0.902 0.934 drug.n 205/46 2 0.870 0.935
effect.n 178/30 3 0.767 0.800 end.v 135/21 4 0.524 0.619
enjoy.v 56/14 2 0.571 0.643 estimate.v 74/16 1 1.000 1.000
examine.v 26/3 3 1.000 1.000 exchange.n 363/61 5 0.738 0.902
exist.v 52/22 2 1.000 1.000 explain.v 85/18 2 0.889 0.944
express.v 47/10 1 1.000 1.000 feel.v 347/51 3 0.686 0.765
find.v 174/28 5 0.821 0.821 fix.v 32/2 5 0.500 0.500
future.n 350/146 3 0.863 0.829 go.v 244/61 12 0.459 0.426
grant.v 19/5 2 0.800 0.400 hold.v 129/24 8 0.375 0.542
hope.v 103/33 1 1.000 1.000 hour.n 187/48 4 0.896 0.771
improve.v 31/16 1 1.000 1.000 job.n 188/39 3 0.821 0.795
join.v 68/18 4 0.389 0.556 keep.v 260/80 7 0.562 0.562
kill.v 111/16 4 0.875 0.875 lead.v 165/39 6 0.385 0.513
maintain.v 61/10 2 0.900 0.800 management.n 284/45 2 0.711 0.978
move.n 270/47 4 0.979 0.979 need.v 195/56 2 0.714 0.857
negotiate.v 25/9 1 1.000 1.000 network.n 152/55 3 0.909 0.836
occur.v 47/22 2 0.864 0.864 order.n 346/57 7 0.912 0.930
part.n 481/71 4 0.662 0.901 people.n 754/115 4 0.904 0.948
plant.n 347/64 2 0.984 0.984 point.n 469/150 9 0.813 0.920
policy.n 331/39 2 0.974 0.949 position.n 268/45 7 0.467 0.556
power.n 251/47 3 0.277 0.766 prepare.v 54/18 2 0.778 0.833
president.n 879/177 3 0.729 0.927 produce.v 115/44 2 0.750 0.750
promise.v 50/8 2 0.750 0.750 propose.v 34/14 2 0.857 1.000
prove.v 49/22 3 0.318 0.818 purchase.v 35/15 1 1.000 1.000
raise.v 147/34 7 0.147 0.441 rate.n 1009/145 2 0.862 0.917
recall.v 49/15 3 0.867 0.933 receive.v 136/48 2 0.958 0.958
regard.v 40/14 3 0.714 0.643 remember.v 121/13 2 1.000 1.000
remove.v 47/17 1 1.000 1.000 replace.v 46/15 2 1.000 1.000
report.v 128/35 3 0.914 0.914 rush.v 28/7 2 1.000 1.000
say.v 2161/541 5 0.987 0.987 see.v 158/54 6 0.444 0.574
set.v 174/42 9 0.286 0.500 share.n 2536/525 2 0.971 0.973
source.n 152/35 5 0.371 0.829 space.n 67/14 5 0.786 0.929
start.v 214/38 6 0.447 0.447 state.n 617/72 3 0.792 0.819
system.n 450/70 5 0.486 0.586 turn.v 340/62 13 0.387 0.516
value.n 335/59 3 0.983 0.983 work.v 230/43 7 0.558 0.721
AVG 222.8/48.5 3.6 0.787 0.851
Preposition WSD
lexelt trn/tst s mfs acc lexelt trn/tst s mfs acc
about.p 710/364 6 0.885 0.934 above.p 48/23 5 0.609 0.522
across.p 319/151 2 0.960 0.960 after.p 103/53 6 0.434 0.585
against.p 195/92 6 0.435 0.793 along.p 364/173 3 0.954 0.954
among.p 100/50 3 0.300 0.680 around.p 334/155 6 0.452 0.535
as.p 173/84 1 1.000 1.000 at.p 715/367 12 0.425 0.662
before.p 47/20 3 0.450 0.850 behind.p 138/68 4 0.662 0.676
beneath.p 57/28 3 0.571 0.679 beside.p 62/29 1 1.000 1.000
between.p 211/102 7 0.422 0.765 by.p 509/248 10 0.371 0.556
down.p 332/153 3 0.438 0.647 during.p 81/39 2 0.385 0.564
for.p 950/478 13 0.238 0.395 from.p 1204/578 16 0.279 0.415
in.p 1391/688 13 0.362 0.436 inside.p 67/38 4 0.526 0.579
into.p 604/297 8 0.451 0.539 like.p 266/125 7 0.768 0.808
of.p 3000/1478 17 0.205 0.374 off.p 161/76 4 0.763 0.776
on.p 872/441 20 0.206 0.469 onto.p 117/58 3 0.879 0.879
over.p 200/98 12 0.327 0.510 round.p 181/82 7 0.378 0.512
through.p 440/208 15 0.495 0.538 to.p 1182/572 10 0.322 0.579
towards.p 214/102 4 0.873 0.873 with.p 1187/578 15 0.249 0.455
AVG 486.3/238.1 7.4 0.397 0.547
Table 2: English Lexical Sample and Preposition WSD Results: lexelt is the lexical item, trn/tst is the
number of training and testing instances, s is the number of senses in the training set, mfs is the most
frequent sense baseline, and acc is the final accuracy.
214
Coling 2010: Poster Volume, pages 1391?1398,
Beijing, August 2010
Unsupervised Part of Speech Tagging Using Unambiguous Substitutes
from a Statistical Language Model
Mehmet Ali Yatbaz
Dept. of Computer Engineering
Koc? University
?myatbaz@ku.edu.tr dyuret@ku.edu.tr
Deniz Yuret
Dept. of Computer Engineering
Koc? University
?
Abstract
We show that unsupervised part of speech
tagging performance can be significantly
improved using likely substitutes for tar-
get words given by a statistical language
model. We choose unambiguous substi-
tutes for each occurrence of an ambiguous
target word based on its context. The part
of speech tags for the unambiguous sub-
stitutes are then used to filter the entry for
the target word in the word?tag dictionary.
A standard HMM model trained using the
filtered dictionary achieves 92.25% accu-
racy on a standard 24,000 word corpus.
1 Introduction
We define the unsupervised part-of-speech (POS)
tagging problem as predicting the correct part-of-
speech tag of a word in a given context using
an unlabeled corpus and a dictionary with possi-
ble word?tag pairs0 The performance of an un-
supervised POS tagging system depends highly
on the quality of the word?tag dictionary (Banko
and Moore, 2004). We propose a dictionary fil-
tering procedure based on likely substitutes sug-
gested by a statistical language model. The pro-
cedure reduces the word?tag dictionary size and
leads to significant improvement in the accuracy
of the POS models.
Probabilistic models such as the hidden Markov
model (HMM) trained by expectation maximiza-
tion (EM), maximum a posteriori (MAP) esti-
mation, and Bayesian methods have been used
0In the POS literature the term ?unsupervised? is typi-
cally used to describe systems that do not directly use the
tagged data. However, many of the unsupervised systems,
including ours, uses the tag?word dictionary.
to solve the unsupervised POS tagging problem
(Merialdo, 1994; Goldwater and Griffiths, 2007).
All of these approaches first learn the parameters
connecting the hidden structure to the observed
sequence of variables and then identify the most
probable values of the hidden structure for a given
observed sequence. They differ in the way they
estimate the model parameters. HMM-EM esti-
mates model parameters by using the maximum
likelihood estimation (MLE), MAP defines a prior
distribution over parameters and finds the param-
eter values that maximize the posterior distribu-
tion given data, and Bayesian methods integrate
over the posterior of the parameters to incorporate
all possible parameter settings into the estimation
process. Some baseline results and performance
reports from the literature are presented in Table 1.
(Johnson, 2007) criticizes the standard EM
based HMM approaches because of their poor per-
formance on the unsupervised POS tagging and
their tendency to assign equal number of words
to each hidden state. (Mitzenmacher, 2004) fur-
ther claims that words have skewed POS tag dis-
tributions, and a Bayesian method with sparse pri-
ors over the POS tags may perform better than
HMM estimated with EM. (Goldwater and Grif-
fiths, 2007) uses a fully Bayesian HMM model
that averages over all possible parameter values.
Their model achieves 86.8% tagging accuracy
with sparse POS priors and outperforms 74.50%
accuracy of the standard second order HMM-EM
(3-gram tag model) on a 24K word subset of
the Penn Treebank corpus. (Smith and Eisner,
2005) take a different approach and use the con-
ditional random fields estimated using contrastive
estimation which outperforms the HMM-EM and
1391
Accuracy System
64.2 Random baseline
74.4 Second order HMM
82.0 First order HMM
86.8 Fully Bayesian approach with sparse priors (Goldwater and Griffiths, 2007)
88.6 CRF/CE (Smith and Eisner, 2005)
91.4 EM-HMM with language specific information, good initialization and manual adjustments to standard
dictionary (Goldberg et al, 2008)
91.8 Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009).
94.0 Most frequent tag baseline
Table 1: Tagging accuracy on a 24K-word corpus. All the systems ? except (Goldwater and Griffiths,
2007) ? use the same 45 tag dictionary that is constructed from the Penn Treebank.
Bayesian methods by achieving 88.6% accuracy
on the same 24K corpus.
Despite the fact that HMM-EM has a poor repu-
tation in POS literature (Goldberg et al, 2008) has
shown that with good initialization together with
some language specific features and language de-
pendent constraints HMM-EM achieves 91.4%
accuracy. Aside from the language specific infor-
mation and the good initialization, they also man-
ually reduce the noise in the word?tag dictionary.
(Ravi and Knight, 2009) focus on the POS tag
collection to find the smallest POS model that ex-
plain the data. They apply integer programming
to construct a minimal bi-gram POS tag set and
use this set to constrain the training phase of the
EM algorithm. The model trained by EM is used
to reduce the dictionary and these steps are iter-
atively repeated until no further improvement is
observed. Their model achieves 91.6% accuracy
on the 24K word corpus (with 100 random starts
this goes up to 91.8%). The main advantage of
this model is the restriction of the tag set so that
rare POS tags or the noise in the corpus do not get
incorporated into the estimation process.
Language models for disambiguation: Recent
work has shown that statistical language models
trained on large amounts of unlabeled text can
be used to improve the performance on various
disambiguation problems. The language model
is used to generate likely substitutes for the tar-
get word in the given context and these benefit
the disambiguation process to the extent that the
likely substitutes are unambiguous or have dif-
ferent ambiguities compared to the target word.
Using statistical language models based on large
corpora for unsupervised word sense disambigua-
tion and lexical substitution has been explored in
(Yuret, 2007; Hawker, 2007; Yuret and Yatbaz,
2010). Unsupervised morphological disambigua-
tion in agglutinative languages using likely sub-
stitutes has been shown to improve on standard
methods in (Yatbaz and Yuret, 2009).
In this paper we use the statistical language
model to reduce the possible number of tags per
word to help the disambiguation process. Specif-
ically we assume that the same hidden tag se-
quence that has generated a particular test sen-
tence can also generate artificial sentences where
one of the words has been replaced with a likely
substitute. POS tags of the likely substitutes can
then be used to reduce the tag set of the target
word. Thus, the substitutes are implicitly incorpo-
rated into the disambiguation process for reducing
the noise and the rare tags in the dictionary.
Currency gyrations can whipsaw(VB/NN) the funds .
Currency gyrations can withdraw(VB) the funds .
Currency gyrations can restore(VB) the funds .
Currency gyrations can modify(VB) the funds .
Currency gyrations can justify(VB) the funds .
Currency gyrations can regulate(VB) the funds .
Table 2: Sample artificial sentences generated for
a test sentence from the Penn Treebank.
Table 2 presents an example where the likely
unambiguous replacements of the target word
?whipsaw? for a given sentence taken from the
Penn Treebank (Marcus et al, 1994) are listed. In
this example each substitute is an unambiguous
verb (VB), confirming our assumption that each
artificial sentence comes from the same hidden se-
quence. For all occurrences of the word ?whip-
saw?, our reduction algorithm will count the POS
tags of the likely substitutes and remove the tags
1392
that have not been observed from the dictionary.
Assuming that the first sentence in Table 2 is the
only sentence in which we observe ?whipsaw?,
the ?NN? tag of ?whipsaw? will be removed.
The next section describes the details of our
dictionary reduction method. Section 3 explains
the details of statistical language model. We ex-
perimentally demonstrate that the word?tag dic-
tionary reduced by the substitutes improve the
performance by constraining the unsupervised
model in Section 4. Finally, Section 5 comments
on the results and discusses the possible exten-
sions of our method.
2 Dictionary Reduction
Our main assumption is that likely replacements
of a target word should have the same POS tag
as the target word in a given context. Motivated
by this idea we propose a new procedure that au-
tomatically reduces the dictionary size by using
the unambiguous replacements of the target word.
For all occurrences of the target word the pro-
cedure counts the POS tags of the replacement
words and removes the unobserved POS tags of
the target word from the dictionary.
Our approach is based on the idea that similar
words in a given context should have the same tag
sequence. To reduce the dictionary with the help
of the replacement words similar to a target word
w, we follow three rules:
1. Choose the replacement word from unam-
biguous substitutes that are likely to appear
in the target word context.
2. Substitutes must be observed in the training
corpus.
3. Count the tags of the replacement for all oc-
currences of the target word.
4. Remove the tags that are not observed as the
tag of replacements in any occurrences of the
target word.
The first rule is used to increase the likelihood
of getting a replacement word with the same POS
tag. The second rule makes sure that the size of
the vocabulary does not change. The third rule
determines the unused POS tags in all occurrences
of w and finally, last rule removes the unobserved
tags of w from the dictionary.
We use the standard first order HMM to test the
performance of our method. In a standard nth or-
der HMM each hidden state is conditioned by its
n preceding hidden states and each observation is
conditioned by its corresponding hidden state. In
POS tagging, the observed variable sequence is
a sentence s and the hidden variables ti are the
POS tags of the words wi in s. The HMM pa-
rameters ? can be estimated by using Baum-Welch
EM algorithm on an unlabeled training corpus D
(Baum, 1972). The tag sequence that maximizes
Pr(t|s, ??) can be identified by the Viterbi search
algorithm.
3 Statistical Language Modeling
In order to estimate highly probable replacement
words for a given wordw in the context cw, we use
an n-gram language model. The context is defined
as the 2n?1 word windoww?n+1 . . . w0 . . . wn?1
and it is centered at the target word position. The
probability of a word in a given context can be
estimated as:
P (w0 = w|cw) ? P (w?n+1 . . . w0 . . . wn?1) (1)
= P (w?n+1)P (w?n+2|w?n+1)
. . . P (wn?1|wn?2?n+1) (2)
? P (w0|w?1?n+1)P (w1|w0?n+2)
. . . P (wn?1|wn?20 ) (3)
where wji represents the sequence of words
wiwi+1 . . . wj . In Equation 1, Pr(w|cw) is pro-
portional to Pr(w?n+1 . . . w0 . . . wn+1) since the
context of the target word replacements is fixed.
Terms without w0 are common for every replace-
ment in Equation 2 therefore they have been
dropped in Equation 3. Finally, because of the
Markov property of n-gram language model, only
n? 1 words are used as a conditional context.
The probabilities in Equation 3 are estimated
using a 4 gram language model for all the words
in the vocabulary of D that are unambiguous and
have a common tag with the target word w. The
words with the highest Pr(r|cw) where r ? D are
selected as the replacement words of w in cw.
1393
To get accurate domain independent proba-
bility estimates we used the Web 1T data-set
(Brants and Franz, 2006), which contains the
counts of word sequences up to length five in a
1012 word corpus derived from publicly accessi-
ble Web pages. The SRILM toolkit is used to train
5-gram language model (Stolcke, 2002). The lan-
guage model parameters are optimized by using a
randomly selected 24K words corpus from Penn
Treebank. In order to efficiently apply the lan-
guage model to a given test corpus, the vocabulary
size is limited to the words seen in the test corpus.
4 Experiments
In this section we present a number of experi-
ments measuring the performance of several vari-
ants of our algorithm. The models in this sec-
tion are trained1 and tested on the same unlabeled
data therefore there aren?t any out-of-vocabulary
words. The experiments in this section focus on:
(1) the analysis of the dictionary reduction (2) the
number of the substitutes used for each ambigu-
ous word and (3) the size of the word?tag dictio-
nary.
4.1 Dataset
We trained HMM-EM models on a corpus that
consists of the first 24K words of the Penn Tree-
bank corpus. To be consistent with the POS tag-
ging literature, the tag dictionary is constructed by
listing all observed tags for each word in the entire
Penn Treebank. Nearly 55% of the words in Penn
Treebank corpus are ambiguous and the average
number of tags is 2.3.
Groups Member POS tags Count %
Noun NN/NNP/NNS/NNPS 7511 31.30
Verb VBD/VB/VBZ/VBN/VBG/VBP 3285 13.69
Adj JJ/JJR/JJS 1718 7.16
Adv RB/RBR 742 3.09
Pronoun CD/PRP/PRP$ 1397 5.82
Content Noun/Verb/Adj/Adv/Pronoun 14653 61.05
Function Other 9347 38.95
Total All 45 POS tags 24K 100.00
Table 3: Group names, members, number and per-
centage of the words according to their gold POS
tags.
1The GMTK tool is used to train HMM-EM model on an
unlabeled corpus (Bilmes and Zweig, 2002).
Table 3 shows the POS speech groups and their
distributions in the 24K word corpus. We report
the model accuracy on several POS groups. Our
motivation is to determine HMM-EM model ac-
curacies on the subgroups before and after imple-
menting the dictionary reduction procedure.
4.2 Baseline
Table 4 presents some standard baselines for com-
parison. We define a random and a most frequent
tag (MFT) baseline on the 24K corpus. The ran-
dom baseline is calculated by randomly picking
one of the tags of each word and it also represents
the amount of ambiguity in the corpus. The MFT
baseline simply selects the most frequent POS tag
of each word from the 1M word Penn Treebank
corpus (counts of the first 24K words is not in-
cluded in the 1M word corpus). If the target word
does not exist in the training set, the MFT base-
line randomly picks one of the possible tags of the
missing word.
The first and second order HMMs can be
treated as the unsupervised baselines. These unsu-
pervised baselines are calculated by training uni-
formly initialized first and second order HMMs on
the target corpus without any smoothing. All the
initial parameters of HMM-EM are uniformly ini-
tialized to observe only the effects of the artificial
sentences on the performance of HMM-EM.
The success of the MFT baseline on the Noun,
Adj, Pronoun and function word groups shows
that tag distributions of the words in these groups
are more skewed towards to one of the available
tags. The MFT baseline performs poorly, com-
pared to the above groups, on V erb, and Adv
which is due to the less skewed POS tag behav-
ior of these tags.
The POS tagging literature widely uses the sec-
ond order HMM as the baseline model; how-
ever, the performance of this model can be out-
performed by an unsupervised first order HMM
model or a simple MFT baseline as presented in
Table 4. A point worth noting is that although the
first order HMM and the MFT baseline have sim-
ilar content word accuracies, the MFT baseline is
significantly better on the function words. This
is expected since EM tends to assign words uni-
formly to the available POS tags. Thus EM can
1394
Noun Verb Adj Adv Pronoun Content Function Total(%)
Random Baseline 76.98 53.87 68.46 72.98 87.64 71.59 52.64 64.21
3-gram HMM 77.43 68.16 78.06 73.32 94.85 76.88 70.45 74.38
2-gram HMM 92.22 83.84 85.22 83.96 95.56 89.42 70.49 82.05
MFT Baseline 96.11 80.30 88.56 83.15 98.75 91.28 98.25 93.99
Table 4: Percentages of words tagged correctly by different models using standard dictionary.
not capture the highly skewed behavior of func-
tion words. Moreover the amount of skewness af-
fects the accuracy of the EM such that the perfor-
mance gain of the MFT baseline over the first or-
der HMM on function words is around 28%-30%
while the performance gain on Noun, Adj and
Pronoun is around 3%-4%.
4.3 Reduced Dictionary
EM can not capture the sparse structure of the
word distributions therefore it tends to assign
equal number of words to each POS tag. Together
with the noisy word?tag dictionary great portion
of the function words are tagged with very rare
POS tags. The abuse of the rare tags is presented
in Table 5 in a similar fashion with (Ravi and
Knight, 2009). The count of replacement word
POS tags and the removed rare POS tags of 2 er-
roneous function words are also shown in Table 5.
Word Tag Gold EM Replacement
dictionary tagging tagging POS counts
of {RB,RP,IN} IN(632) IN(0) IN(2377)
RP(0) RP(632) RP(0)
RB(0) RB(0) RB(850)
a {LS,SYM,NNP, DT(458) DT(0) DT(513)
FW,JJ,IN,DT} IN(1) IN(0) IN(317)
JJ(2) JJ(0) JJ(1329)
SYM(1) SYM(258) SYM(0)
LS(0) LS(230) LS(0)
Table 5: Removed POS tags of the given words
are shown in bold.
The results obtained with the dictionary that is
reduced by using 5 replacements are presented
in Table 6. Note that with reduced dictionary
the uniformly initialized first order HMM-EM
achieves 91.85% accuracy. Dictionary reduction
also removes some of the useful tags therefore
the upper?bound (oracle score) of the 24K dataset
becomes 98.15% after the dictionary reduction.
We execute 100 random restarts of the EM algo-
rithm and select the model with the highest corpus
likelihood, our model achieves 92.25% accuracy
which is the highest accuracy reported for the 24K
corpus so far.
As Table 6 shows, the effect of the dictionary
reduction on the function words is higher than
the effect on the content words. The main reason
for this situation is, function words are frequently
tagged with one of its tags which is also the reason
for the high accuracy of the majority voting based
baseline on the function words.
The reduced dictionary (RD) removes the rare
problematic POS tags of the words as a result the
accuracy on the content and function words shows
a drastic improvement compared to HMM models
trained with the original dictionary.
Pos 2-gram HMM 2-gram HMM RD
groups accuracy(%) accuracy(%)
Noun 92.22 94.01
Verb 83.84 84.90
Adj 85.22 89.52
Adv 83.96 85.18
Pronoun 95.56 95.92
Content 89.42 91.18
Function 70.49 92.92
All 82.05 91.85
Table 6: Percentages of the correctly tagged
words by different models with modified dictio-
nary. The dictionary size is reduced by using the
top 5 replacements of each target word.
4.4 More Data
In this set of experiments we doubled the size of
the data and trained HMM-EM models on a cor-
pus that consists of the first 48K words of the Penn
Treebank corpus. Our aim is to observe the effect
of more data on our dictionary reduction proce-
1395
dure. Using the 5 replacements of each ambigu-
ous word we reduce the dictionary and train a new
HMM-EM model using this dictionary. The ad-
ditional data together with 100 random starts in-
creases the model accuracy to 92.47% on the 48K
corpus.
Pos 3-gram HMM RD 2-gram HMM RD
groups accuracy(%) accuracy(%)
Noun 89.45 93.47
Verb 85.56 88.99
Adj 86.02 87.53
Adv 94.44 95.92
Pronoun 94.08 94.04
Content 88.91 91.97
Function 92.44 92.26
All 90.31 92.09
Table 7: Percentages of the correctly tagged
words by the first and second order HMM-EM
model trained on the 48K corpus with reduced
dictionary. The dictionary size is reduced by using
the top 5 replacements of each target word.
As we mentioned before, when the model is
trained using the original dictionary, the perfor-
mance gap between the first order HMM the sec-
ond order HMM is around 8% as presented in Ta-
ble 4. On the other hand, when we use the re-
duced dictionary together with more data the ac-
curacy gap between the second order and the first
order HMM-EM becomes less than 2% as shown
in Table 7. This confirms the hypothesis that the
low performance of the second order HMM is due
to data sparsity in the 24K-word dataset, and bet-
ter results may be achieved with the second order
HMM in larger datasets.
4.5 Number of Replacements
In this set of experiments we vary the number of
artificial replacement words per each ambiguous
word in s. We run our method on the 24K corpus
with 1, 5, 10, 25 and 50 replacement words per
ambiguous word and we present the results in Ta-
ble 8. The performance of our method affected by
the the number of replacements and highest score
is achieved when 5 replacements are used. Incor-
porating the probability of the substitutes into the
model rather than using a hard cutoff may be a
better solution.
Number of 2-gram HMM RD
replacements accuracy(%)
none 82.05
1 89.65
5 91.85
10 90.09
25 89.97
50 89.83
Table 8: Percentages of the correctly tagged
words by the models trained on the 24K corpus
with different reduced dictionaries. The dictio-
nary size is reduced by using different number re-
placements.
4.6 17-Tagset
To observe the effect our method on a model with
coarse grained dictionary, we collapsed the 45?
tagset treebank dictionary to a 17?tagset coarse
dictionary (Smith and Eisner, 2005). The POS
literature after the work of Smith and Eisner fol-
lows this tradition and also tests the models on this
17?tagset. Table 9 summarizes the previously re-
ported results on coarse grained POS tagging. Our
system achieves 92.9% accuracy where the ora-
cle accuracy of 24K dataset with the reduced 17?
tagset dictionary is 98.3% and the state-of-the-art
system IP+EM scores 96.8%.
Model Accuracy Data Size
BHMM 87.3 24K
CE+spl 88.7 24K
RD 92.9 24K
LDA+AC 93.4 1M
InitEM-HMM 93.8 1M
IP+EM 96.8 24K
Table 9: Performance of different systems using
the coarse grained dictionary.
The IP+EM system constructs a model that de-
scribes the data by using minimum number of bi-
gram POS tags then uses this model to reduce the
dictionary size (Ravi and Knight, 2009). InitEM-
HMM uses the language specific information to-
gether with good initialization and it achieves
93.8% accuracy on the 1M word treebank corpus.
LDA+AC semi-supervised Bayesian model with
strong ambiguity class component given the mor-
phological features of words and scores 93.4% on
the 1M word treebank corpus. (Toutanova and
Johnson, 2007). CE+spl is HMM model estimated
1396
by contrastive estimation method and achieves
88.7% accuracy (Smith and Eisner, 2005). Fi-
nally, BHMM is a fully Bayesian approach that
uses sparse POS priors and scores 87.3% (Gold-
water and Griffiths, 2007).
5 Contributions
In this paper we proposed a dictionary reduction
method that can be applied to unsupervised tag-
ging problems. With the help of a statistical lan-
guage model, our system creates artificial replace-
ments that are assumed to have the same POS tag
as the target word and use them to reduce the size
of the word?tag dictionary. To test our method
we used HMM-EM as the unsupervised model.
Our method significantly improves the prediction
accuracy of the unsupervised first order HMM-
EM system in all of the POS groups and achieves
92.25% and 92.47% word tagging accuracy on the
24K and 48K word corpora respectively. We also
tested our model on a coarse grained dictionary
with 17 tags and achieved an accuracy of 92.8%.
In this work, we show that unambiguous re-
placements of an ambiguous word can reduce the
amount of the ambiguity thus replacement words
might also be incorporated into the other unsuper-
vised disambiguation problems.
Acknowledgments
This work was supported in part by the Scien-
tific and Technical Research Council of Turkey
(TU?BI?TAK Project 108E228).
References
Banko, Michele and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING ?04: Pro-
ceedings of the 20th international conference on
Computational Linguistics, page 556, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Baum, L.E. 1972. An inequality and associated maxi-
mization technique in statistical estimation for prob-
abilistic functions of Markov processes. Inequali-
ties, 3(1):1?8.
Bilmes, J. and G. Zweig. 2002. The Graphical Models
Toolkit: An open source software system for speech
and time-series processing. In IEEE International
Conference On Acoustics Speech and Signal Pro-
cessing, volume 4, pages 3916?3919.
Brants, T. and A. Franz. 2006. Web 1T 5-gram Ver-
sion 1. Linguistic Data Consortium, Philadelphia.
Goldberg, Y., M. Adler, and M. Elhadad. 2008. Em
can find pretty good hmm pos-taggers (when given
a good start). Proceedings of ACL-08. Columbus,
OH, pages 746?754.
Goldwater, S. and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging.
In Annual Meeting-Assosiation for Computational
Linguistics, volume 45, page 744.
Hawker, Tobias. 2007. Usyd: Wsd and lexical substi-
tution using the web1t corpus. In Proceedings of the
Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 446?453, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Johnson, M. 2007. Why doesnt EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 296?305.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguis-
tics, 19(2):313?330.
Merialdo, B. 1994. Tagging english text with
a probabilistic model. Computational linguistics,
20(2):155?171.
Mitzenmacher, M. 2004. A brief history of generative
models for power law and lognormal distributions.
Internet mathematics, 1(2):226?251.
Ravi, Sujith and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
ACL-IJCNLP ?09: Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1, pages
504?512, Morristown, NJ, USA. Association for
Computational Linguistics.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In ACL ?05: Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 354?362, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Stolcke, A. 2002. SRILM-an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing, volume 3.
1397
Toutanova, K. and M. Johnson. 2007. A Bayesian
LDA-based model for semi-supervised part-of-
speech tagging. In Proceedings of NIPS, volume 20.
Yatbaz, Mehmet Ali and Deniz Yuret. 2009. Unsuper-
vised morphological disambiguation using statisti-
cal language models. In NIPS 2009 Workshop on
Grammar Induction, Representation of Language
and Language Learning.
Yuret, Deniz and Mehmet Ali Yatbaz. 2010. The
noisy channel model for unsupervised word sense
disambiguation. Computational Linguistics, 36(1),
March.
Yuret, Deniz. 2007. KU: Word sense disambigua-
tion by substitution. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 207?214, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
1398
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2303?2313, Dublin, Ireland, August 23-29 2014.
Unsupervised Instance-Based Part of Speech Induction
Using Probable Substitutes
Mehmet Ali Yatbaz Enis R?fat Sert Deniz Yuret
Koc? University Artificial Intelligence Laboratory,
?
Istanbul
{myatbaz,esert,dyuret}@ku.edu.tr
Abstract
We develop an instance (token) based extension of the state of the art word (type) based part-of-
speech induction system introduced in (Yatbaz et al., 2012). Each word instance is represented
by a feature vector that combines information from the target word and probable substitutes
sampled from an n-gram model representing its context. Modeling ambiguity using an instance
based model does not lead to significant gains in overall accuracy in part-of-speech tagging be-
cause most words in running text are used in their most frequent class (e.g. 93.69% in the
Penn Treebank). However it is important to model ambiguity because most frequent words are
ambiguous and not modeling them correctly may negatively affect upstream tasks. Our main
contribution is to show that an instance based model can achieve significantly higher accuracy on
ambiguous words at the cost of a slight degradation on unambiguous ones, maintaining a compa-
rable overall accuracy. On the Penn Treebank, the overall many-to-one accuracy of the system is
within 1% of the state-of-the-art (80%), while on highly ambiguous words it is up to 70% better.
On multilingual experiments our results are significantly better than or comparable to the best
published word or instance based systems on 15 out of 19 corpora in 15 languages. The vector
representations for words used in our system are available for download for further experiments.
1 Introduction
Unsupervised part-of-speech (POS) induction aims to classify words into syntactic categories using un-
labeled, plain text input. The problem of induction is important for studying under-resourced languages
that lack labeled corpora and high quality dictionaries. It is also essential in modeling child language
acquisition because every child manages to induce syntactic categories without access to labeled sen-
tences, labeled prototypes, or dictionary constraints (Ambridge and Lieven, 2011). Categories induced
from data may point to shortcomings or inconsistencies of hand-labeled categories as discussed in Sec-
tion 4. Finally, the induced categories or the vector representations generated by the induction algorithms
may improve natural language processing systems when used as additional features.
Word-based POS induction systems classify different instances of a word in a single category (which
we will refer to as the one-tag-per-word assumption). Instance-based systems classify each occurence of
a word separately and can handle ambiguous words.
Examples of word-based systems include ones that represent each word with the vector of neighboring
words (context vectors) and cluster them (Sch?utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), use
a prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003),
restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn,
2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), or
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2303





	


        











Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 940?951, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Syntactic Categories Using Paradigmatic Representations of
Word Context
Mehmet Ali Yatbaz Enis Sert Deniz Yuret
Artificial Intelligence Laboratory
Koc? University, I?stanbul, Turkey
{myatbaz,esert,dyuret}@ku.edu.tr
Abstract
We investigate paradigmatic representations
of word context in the domain of unsupervised
syntactic category acquisition. Paradigmatic
representations of word context are based on
potential substitutes of a word in contrast to
syntagmatic representations based on prop-
erties of neighboring words. We compare
a bigram based baseline model with several
paradigmatic models and demonstrate signif-
icant gains in accuracy. Our best model based
on Euclidean co-occurrence embedding com-
bines the paradigmatic context representation
with morphological and orthographic features
and achieves 80% many-to-one accuracy on a
45-tag 1M word corpus.
1 Introduction
Grammar rules apply not to individual words (e.g.
dog, eat) but to syntactic categories of words (e.g.
noun, verb). Thus constructing syntactic categories
(also known as lexical or part-of-speech categories)
is one of the fundamental problems in language ac-
quisition.
Syntactic categories represent groups of words
that can be substituted for one another without alter-
ing the grammaticality of a sentence. Linguists iden-
tify syntactic categories based on semantic, syntac-
tic, and morphological properties of words. There is
also evidence that children use prosodic and phono-
logical features to bootstrap syntactic category ac-
quisition (Ambridge and Lieven, 2011). However
there is as yet no satisfactory computational model
that can match human performance. Thus identify-
ing the best set of features and best learning algo-
rithms for syntactic category acquisition is still an
open problem.
Relationships between linguistic units can be
classified into two types: syntagmatic (concerning
positioning), and paradigmatic (concerning substitu-
tion). Syntagmatic relations determine which units
can combine to create larger groups and paradig-
matic relations determine which units can be sub-
stituted for one another. Figure 1 illustrates the
paradigmatic vs syntagmatic axes for words in a
simple sentence and their possible substitutes.
In this study, we represent the paradigmatic axis
directly by building substitute vectors for each word
position in the text. The dimensions of a substi-
tute vector represent words in the vocabulary, and
the magnitudes represent the probability of occur-
rence in the given position. Note that the substitute
vector for a word position (e.g. the second word in
Fig. 1) is a function of the context only (i.e. ?the
cried?), and does not depend on the word that
does actually appear there (i.e. ?man?). Thus substi-
Figure 1: Syntagmatic vs. paradigmatic axes for words
in a simple sentence (Chandler, 2007).
940
tute vectors represent individual word contexts, not
word types. We refer to the use of features based on
substitute vectors as paradigmatic representations of
word context.
Our preliminary experiments indicated that using
context information alone without the identity or the
features of the target word (e.g. using dimension-
ality reduction and clustering on substitute vectors)
has limited success and modeling the co-occurrence
of word and context types is essential for inducing
syntactic categories. In the models presented in this
paper, we combine paradigmatic representations of
word context with features of co-occurring words
within the co-occurrence data embedding (CODE)
framework (Globerson et al 2007; Maron et al
2010). The resulting embeddings for word types are
split into 45 clusters using k-means and the clusters
are compared to the 45 gold tags in the 1M word
Penn Treebank Wall Street Journal corpus (Mar-
cus et al 1999). We obtain many-to-one accura-
cies up to .7680 using only distributional informa-
tion (the identity of the word and a representation of
its context) and .8023 using morphological and or-
thographic features of words improving the state-of-
the-art in unsupervised part-of-speech tagging per-
formance.
The high probability substitutes reflect both se-
mantic and syntactic properties of the context as
seen in the example below (the numbers in paren-
theses give substitute probabilities):
?Pierre Vinken, 61 years old, will join the
board as a nonexecutive director Nov. 29.?
the: its (.9011), the (.0981), a (.0006), . . .
board: board (.4288), company (.2584),
firm (.2024), bank (.0731), . . .
Top substitutes for the word ?the? consist of
words that can act as determiners. Top substitutes
for ?board? are not only nouns, but specifically
nouns compatible with the semantic context.
This example illustrates two concerns inherent in
all distributional methods: (i) words that are gener-
ally substitutable like ?the? and ?its? are placed in
separate categories (DT and PRP$) by the gold stan-
dard, (ii) words that are generally not substitutable
like ?do? and ?put? are placed in the same category
(VB). Freudenthal et al(2005) point out that cat-
egories with unsubstitutable words fail the standard
linguistic definition of a syntactic category and chil-
dren do not seem to make errors of substituting such
words in utterances (e.g. ?What do you want?? vs.
*?What put you want??). Whether gold standard
part-of-speech tags or distributional categories are
better suited to applications like parsing or machine
translation can be best decided using extrinsic eval-
uation. However in this study we follow previous
work and evaluate our results by comparing them to
gold standard part-of-speech tags.
Section 2 gives a detailed review of related work.
Section 3 describes the dataset and the construction
of the substitute vectors. Section 4 describes co-
occurrence data embedding, the learning algorithm
used in our experiments. Section 5 describes our
experiments and compares our results with previ-
ous work. Section 6 gives a brief error analysis
and Section 7 summarizes our contributions. All the
data and the code to replicate the results given in
this paper is available from the authors? website at
http://goo.gl/RoqEh.
2 Related Work
There are several good reviews of algorithms
for unsupervised part-of-speech induction
(Christodoulopoulos et al 2010; Gao and Johnson,
2008) and models of syntactic category acquisition
(Ambridge and Lieven, 2011).
This work is to be distinguished from supervised
part-of-speech disambiguation systems, which use
labeled training data (Church, 1988), unsupervised
disambiguation systems, which use a dictionary of
possible tags for each word (Merialdo, 1994), or
prototype driven systems which use a small set
of prototypes for each class (Haghighi and Klein,
2006). The problem of induction is important for
studying under-resourced languages that lack la-
beled corpora and high quality dictionaries. It is also
essential in modeling child language acquisition be-
cause every child manages to induce syntactic cat-
egories without access to labeled sentences, labeled
prototypes, or dictionary constraints.
Models of unsupervised part-of-speech induction
fall into two broad groups based on the information
they utilize. Distributional models only use word
941
types and their context statistics. Word-feature mod-
els incorporate additional morphological and ortho-
graphic features.
2.1 Distributional models
Distributional models can be further categorized into
three subgroups based on the learning algorithm.
The first subgroup represents each word type with its
context vector and clusters these vectors accordingly
(Schu?tze, 1995). Work in modeling child syntac-
tic category acquisition has generally followed this
clustering approach (Redington et al 1998; Mintz,
2003). The second subgroup consists of proba-
bilistic models based on the Hidden Markov Model
(HMM) framework (Brown et al 1992). A third
group of algorithms constructs a low dimensional
representation of the data that represents the empir-
ical co-occurrence statistics of word types (Glober-
son et al 2007), which is covered in more detail in
Section 4.
Clustering: Clustering based methods represent
context using neighboring words, typically a sin-
gle word on the left and a single word on the right
called a ?frame? (e.g., the dog is; the cat is). They
cluster word types rather than word tokens based on
the frames they occupy thus employing one-tag-per-
word assumption from the beginning (with the ex-
ception of some methods in (Schu?tze, 1995)). They
may suffer from data sparsity caused by infrequent
words and infrequent contexts. The solutions sug-
gested either restrict the set of words and set of con-
texts to be clustered to the most frequently observed,
or use dimensionality reduction. Redington et al
(1998) define context similarity based on the num-
ber of common frames bypassing the data sparsity
problem but achieve mediocre results. Mintz (2003)
only uses the most frequent 45 frames and Biemann
(2006) clusters the most frequent 10,000 words us-
ing contexts formed from the most frequent 150-200
words. Schu?tze (1995) and Lamar et al(2010b)
employ SVD to enhance similarity between less fre-
quently observed words and contexts. Lamar et al
(2010a) represent each context by the currently as-
signed left and right tag (which eliminates data spar-
sity) and cluster word types using a soft k-means
style iterative algorithm. They report the best clus-
tering result to date of .708 many-to-one accuracy
on a 45-tag 1M word corpus.
HMMs: The prototypical bitag HMM model max-
imizes the likelihood of the corpus w1 . . . wn
expressed as P (w1|c1)
?n
i=2 P (wi|ci)P (ci|ci?1)
where wi are the word tokens and ci are their (hid-
den) tags. One problem with such a model is its ten-
dency to distribute probabilities equally and the re-
sulting inability to model highly skewed word-tag
distributions observed in hand-labeled data (John-
son, 2007). To favor sparse word-tag distributions
one can enforce a strict one-tag-per-word solution
(Brown et al 1992; Clark, 2003), use sparse pri-
ors in a Bayesian setting (Goldwater and Griffiths,
2007; Johnson, 2007), or use posterior regulariza-
tion (Ganchev et al 2010). Each of these techniques
provide significant improvements over the standard
HMM model: for example Gao and Johnson (2008)
show that sparse priors can gain from 4% (.62 to .66
with a 1M word corpus) in cross-validated many-
to-one accuracy. However Christodoulopoulos et al
(2010) show that the older one-tag-per-word models
such as (Brown et al 1992) outperform the more
sophisticated sparse prior and posterior regulariza-
tion methods both in speed and accuracy (the Brown
model gets .68 many-to-one accuracy with a 1M
word corpus). Given that close to 95% of the word
occurrences in human labeled data are tagged with
their most frequent part of speech (Lee et al 2010),
this is probably not surprising; one-tag-per-word is
a fairly good first approximation for induction.
2.2 Word-feature models
One problem with the algorithms in the previous
section is the poverty of their input features. Of the
syntactic, semantic, and morphological information
linguists claim underlie syntactic categories, con-
text vectors or bitag HMMs only represent limited
syntactic information in their input. Experiments
incorporating morphological and orthographic fea-
tures into HMM based models demonstrate signifi-
cant improvements. (Clark, 2003; Berg-Kirkpatrick
and Klein, 2010; Blunsom and Cohn, 2011) incor-
porate similar orthographic features and report im-
provements of 3, 7, and 10% respectively over the
baseline Brown model. Christodoulopoulos et al
(2010) use prototype based features as described in
(Haghighi and Klein, 2006) with automatically in-
942
duced prototypes and report an 8% improvement
over the baseline Brown model. Christodoulopou-
los et al(2011) define a type-based Bayesian multi-
nomial mixture model in which each word instance
is generated from the corresponding word type mix-
ture component and word contexts are represented
as features. They achieve a .728 MTO score by ex-
tending their model with additional morphological
and alignment features gathered from parallel cor-
pora. To our knowledge, nobody has yet tried to
incorporate phonological or prosodic features in a
computational model for syntactic category acquisi-
tion.
2.3 Paradigmatic representations
Sahlgren (2006) gives a detailed analysis of paradig-
matic and syntagmatic relations in the context of
word-space models used to represent word mean-
ing. Sahlgren?s paradigmatic model represents word
types using co-occurrence counts of their frequent
neighbors, in contrast to his syntagmatic model that
represents word types using counts of contexts (doc-
uments, sentences) they occur in. Our substitute
vectors do not represent word types at all, but con-
texts of word tokens using probabilities of likely sub-
stitutes. Sahlgren finds that in word-spaces built by
frequent neighbor vectors, more nearest neighbors
share the same part-of-speech compared to word-
spaces built by context vectors. We find that rep-
resenting the paradigmatic axis more directly using
substitute vectors rather than frequent neighbors im-
prove part-of-speech induction.
Our paradigmatic representation is also related to
the second order co-occurrences used in (Schu?tze,
1995). Schu?tze concatenates the left and right con-
text vectors for the target word type with the left con-
text vector of the right neighbor and the right con-
text vector of the left neighbor. The vectors from the
neighbors include potential substitutes. Our method
improves on his foundation by using a 4-gram lan-
guage model rather than bigram statistics, using the
whole 78,498 word vocabulary rather than the most
frequent 250 words. More importantly, rather than
simply concatenating vectors that represent the tar-
get word with vectors that represent the context we
use S-CODE to model their co-occurrence statistics.
2.4 Evaluation
We report many-to-one and V-measure scores for
our experiments as suggested in (Christodoulopou-
los et al 2010). The many-to-one (MTO) evaluation
maps each cluster to its most frequent gold tag and
reports the percentage of correctly tagged instances.
The MTO score naturally gets higher with increas-
ing number of clusters but it is an intuitive met-
ric when comparing results with the same number
of clusters. The V-measure (VM) (Rosenberg and
Hirschberg, 2007) is an information theoretic met-
ric that reports the harmonic mean of homogeneity
(each cluster should contain only instances of a sin-
gle class) and completeness (all instances of a class
should be members of the same cluster). In Sec-
tion 6 we argue that homogeneity is perhaps more
important in part-of-speech induction and suggest
MTO with a fixed number of clusters as a more in-
tuitive metric.
3 Substitute Vectors
In this study, we predict the part of speech of a word
in a given context based on its substitute vector. The
dimensions of the substitute vector represent words
in the vocabulary, and the entries in the substitute
vector represent the probability of those words be-
ing used in the given context. Note that the substi-
tute vector is a function of the context only and is
indifferent to the target word. This section details
the choice of the data set, the vocabulary and the es-
timation of substitute vector probabilities.
The Wall Street Journal Section of the Penn Tree-
bank (Marcus et al 1999) was used as the test cor-
pus (1,173,766 tokens, 49,206 types). The tree-
bank uses 45 part-of-speech tags which is the set we
used as the gold standard for comparison in our ex-
periments. To compute substitute probabilities we
trained a language model using approximately 126
million tokens of Wall Street Journal data (1987-
1994) extracted from CSR-III Text (Graff et al
1995) (we excluded the test corpus). We used
SRILM (Stolcke, 2002) to build a 4-gram language
model with Kneser-Ney discounting. Words that
were observed less than 20 times in the language
model training data were replaced by UNK tags,
which gave us a vocabulary size of 78,498. The per-
plexity of the 4-gram language model on the test cor-
943
pus is 96.
It is best to use both left and right context when
estimating the probabilities for potential lexical sub-
stitutes. For example, in ?He lived in San Francisco
suburbs.?, the token San would be difficult to guess
from the left context but it is almost certain look-
ing at the right context. We define cw as the 2n ? 1
word window centered around the target word posi-
tion: w?n+1 . . . w0 . . . wn?1 (n = 4 is the n-gram
order). The probability of a substitute word w in a
given context cw can be estimated as:
P (w0 = w|cw) ? P (w?n+1 . . . w0 . . . wn?1)(1)
= P (w?n+1)P (w?n+2|w?n+1)
. . . P (wn?1|w
n?2
?n+1) (2)
? P (w0|w
?1
?n+1)P (w1|w
0
?n+2)
. . . P (wn?1|w
n?2
0 ) (3)
where wji represents the sequence of words
wiwi+1 . . . wj . In Equation 1, P (w|cw) is pro-
portional to P (w?n+1 . . . w0 . . . wn+1) because the
words of the context are fixed. Terms without w0
are identical for each substitute in Equation 2 there-
fore they have been dropped in Equation 3. Finally,
because of the Markov property of n-gram language
model, only the closest n ? 1 words are used in the
experiments.
Near the sentence boundaries the appropriate
terms were truncated in Equation 3. Specifically, at
the beginning of the sentence shorter n-gram con-
texts were used and at the end of the sentence terms
beyond the end-of-sentence token were dropped.
For computational efficiency only the top 100
substitutes and their unnormalized probabilities
were computed for each of the 1,173,766 positions
in the test set1. The probability vectors for each po-
sition were normalized to add up to 1.0 giving us the
final substitute vectors used in the rest of this study.
1The substitutes with unnormalized log probabilities can be
downloaded from http://goo.gl/jzKH0. For a descrip-
tion of the FASTSUBS algorithm used to generate the substitutes
please see http://arxiv.org/abs/1205.5407v1.
FASTSUBS accomplishes this task in about 5 hours, a naive
algorithm that looks at the whole vocabulary would take more
than 6 days on a typical 2012 workstation.
4 Co-occurrence Data Embedding
The general strategy we follow for unsupervised
syntactic category acquisition is to combine features
of the context with the identity and features of the
target word. Our preliminary experiments indicated
that using the context information alone (e.g. clus-
tering substitute vectors) without the target word
identity and features had limited success.2 It is the
co-occurrence of a target word with a particular type
of context that best predicts the syntactic category.
In this section we review the unsupervised meth-
ods we used to model co-occurrence statistics: the
Co-occurrence Data Embedding (CODE) method
(Globerson et al 2007) and its spherical extension
(S-CODE) introduced by (Maron et al 2010).
Let X and Y be two categorical variables with fi-
nite cardinalities |X| and |Y |. We observe a set of
pairs {xi, yi}ni=1 drawn IID from the joint distribu-
tion of X and Y . The basic idea behind CODE and
related methods is to represent (embed) each value
of X and each value of Y as points in a common
low dimensional Euclidean space Rd such that val-
ues that frequently co-occur lie close to each other.
There are several ways to formalize the relationship
between the distances and co-occurrence statistics,
in this paper we use the following:
p(x, y) =
1
Z
p?(x)p?(y)e?d
2
x,y (4)
where d2x,y is the squared distance between the em-
beddings of x and y, p?(x) and p?(y) are empirical
probabilities, and Z =
?
x,y p?(x)p?(y)e
?d2x,y is a
normalization term. If we use the notation ?x for
the point corresponding to x and ?y for the point
corresponding to y then d2x,y = ??x ? ?y?
2. The
log-likelihood of a given embedding `(?, ?) can be
2A 10-nearest-neighbor supervised baseline using cosine
distance between substitute vectors gives .7213 accuracy. Clus-
tering substitute vectors using various distance metrics and di-
mensionality reduction methods give results inferior to this up-
per bound.
944
expressed as:
`(?, ?) =
?
x,y
p?(x, y) log p(x, y) (5)
=
?
x,y
p?(x, y)(? logZ + log p?(x)p?(y)? d2x,y)
= ? logZ + const ?
?
x,y
p?(x, y)d2x,y
The likelihood is not convex in ? and ?. We use
gradient ascent to find an approximate solution for
a set of ?x, ?y that maximize the likelihood. The
gradient of the d2x,y term pulls neighbors closer in
proportion to the empirical joint probability:
?
??x
?
x,y
?p?(x, y)d2x,y =
?
y
2p?(x, y)(?y ? ?x)
(6)
The gradient of the Z term pushes neighbors apart
in proportion to the estimated joint probability:
?
??x
(? logZ) =
?
y
2p(x, y)(?x ? ?y) (7)
Thus the net effect is to pull pairs together if their
estimated probability is less than the empirical prob-
ability and to push them apart otherwise. The gradi-
ents with respect to ?y are similar.
S-CODE (Maron et al 2010) additionally re-
stricts all ?x and ?y to lie on the unit sphere. With
this restriction, Z stays around a fixed value dur-
ing gradient ascent. This allows S-CODE to sub-
stitute an approximate constant Z? in gradient calcu-
lations for the real Z for computational efficiency.
In our experiments, we used S-CODE with its sam-
pling based stochastic gradient ascent algorithm and
smoothly decreasing learning rate.
5 Experiments
In this section we present experiments that evaluate
substitute vectors as representations of word con-
text within the S-CODE framework. Section 5.1
replicates the bigram based S-CODE results from
(Maron et al 2010) as a baseline. The S-CODE
algorithm works with discrete inputs. The substi-
tute vectors as described in Section 3 are high di-
mensional and continuous. We experimented with
two approaches to use substitute vectors in a dis-
crete setting. Section 5.2 presents an algorithm that
partitions the high dimensional space of substitute
vectors into small neighborhoods and uses the par-
tition id as a discrete context representation. Sec-
tion 5.3 presents an even simpler model which pairs
each word with a random substitute. When the left-
word ? right-word pairs used in the bigram model
are replaced with word ? partition-id or word ? sub-
stitute pairs we see significant gains in accuracy.
These results support our running hypothesis that
paradigmatic features, i.e. potential substitutes of
a word, are better determiners of syntactic category
compared to left and right neighbors. Section 5.4
explores morphologic and orthographic features as
additional sources of information and its results im-
prove the state-of-the-art in the field of unsupervised
syntactic category acquisition.
Each experiment was repeated 10 times with dif-
ferent random seeds and the results are reported
with standard errors in parentheses or error bars in
graphs. Table 1 summarizes all the results reported
in this paper and the ones we cite from the literature.
5.1 Bigram model
In (Maron et al 2010) adjacent word pairs (bi-
grams) in the corpus are fed into the S-CODE algo-
rithm as X,Y samples. The algorithm uses stochas-
tic gradient ascent to find the ?x, ?y embeddings for
left and right words in these bigrams on a single 25-
dimensional sphere. At the end each word w in the
vocabulary ends up with two points on the sphere,
a ?w point representing the behavior of w as the
left word of a bigram and a ?w point representing
it as the right word. The two vectors for w are con-
catenated to create a 50-dimensional representation
at the end. These 50-dimensional vectors are clus-
tered using an instance weighted k-means algorithm
and the resulting groups are compared to the cor-
rect part-of-speech tags. Maron et al(2010) report
many-to-one scores of .6880 (.0016) for 45 clusters
and .7150 (.0060) for 50 clusters (on the full PTB45
tag-set). If only ?w vectors are clustered without
concatenation we found the performance drops sig-
nificantly to about .62.
To make a meaningful comparison we re-ran the
bigram experiments using our default settings and
obtained a many-to-one score of .7314 (.0096) and
the V-measure is .6558 (.0052) for 45 clusters. The
following default settings were used: (i) each word
945
Distributional Models MTO VM
(Lamar et al 2010a) .708 -
(Brown et al 1992)* .678 .630
(Goldwater et al 2007) .632 .562
(Ganchev et al 2010)* .625 .548
(Maron et al 2010) .688 (.0016) -
Bigrams (Sec. 5.1) .7314 (.0096) .6558 (.0052)
Partitions (Sec. 5.2) .7554 (.0055) .6703 (.0037)
Substitutes (Sec. 5.3) .7680 (.0038) .6822 (.0029)
Models with Additional Features MTO VM
(Clark, 2003)* .712 .655
(Christodoulopoulos et al 2011) .728 .661
(Berg-Kirkpatrick and Klein, 2010) .755 -
(Christodoulopoulos et al 2010) .761 .688
(Blunsom and Cohn, 2011) .775 .697
Substitutes and Features (Sec. 5.4) .8023 (.0070) .7207 (.0041)
Table 1: Summary of results in terms of the MTO and VM scores. Standard errors are given in parentheses when
available. Starred entries have been reported in the review paper (Christodoulopoulos et al 2010). Distributional
models use only the identity of the target word and its context. The models on the right incorporate orthographic and
morphological features.
was kept with its original capitalization, (ii) the
learning rate parameters were adjusted to ?0 =
50, ?0 = 0.2 for faster convergence in log likeli-
hood, (iii) the number of s-code iterations were in-
creased from 12 to 50 million, (iv) k-means initial-
ization was improved using (Arthur and Vassilvit-
skii, 2007), and (v) the number of k-means restarts
were increased to 128 to improve clustering and re-
duce variance.
5.2 Random partitions
Instead of using left-word ? right-word pairs as in-
puts to S-CODE we wanted to pair each word with a
paradigmatic representation of its context to get a di-
rect comparison of the two context representations.
To obtain a discrete representation of the context,
the random?partitions algorithm first designates a
random subset of substitute vectors as centroids to
partition the space, and then associates each context
with the partition defined by the closest centroid in
cosine distance. Each partition thus defined gets a
unique id, and word (X) ? partition-id (Y ) pairs are
given to S-CODE as input. The algorithm cycles
through the data until we get approximately 50 mil-
lion updates. The resulting ?x vectors are clustered
using the k-means algorithm (no vector concatena-
tion is necessary). Using default settings (64K ran-
dom partitions, 25 s-code dimensions, Z = 0.166)
the many-to-one accuracy is .7554 (.0055) and the
V-measure is .6703 (.0037).
To analyze the sensitivity of this result to our spe-
cific parameter settings we ran a number of experi-
ments where each parameter was varied over a range
of values.
Figure 2 gives results where the number of initial
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 10000  100000
number of random partitions
m2o
Figure 2: MTO is not sensitive to the number of partitions
used to discretize the substitute vector space within our
experimental range.
random partitions is varied over a large range and
shows the results to be fairly stable across two orders
of magnitude.
Figure 3 shows that at least 10 embedding dimen-
sions are necessary to get within 1% of the best re-
sult, but there is no significant gain from using more
than 25 dimensions.
Figure 4 shows that the constant Z? approximation
can be varied within two orders of magnitude with-
out a significant performance drop in the many-to-
one score. For uniformly distributed points on a 25
dimensional sphere, the expected Z ? 0.146. In the
experiments where we tested we found the real Z al-
ways to be in the 0.140-0.170 range. When the con-
stant Z? estimate is too small the attraction in Eq. 6
dominates the repulsion in Eq. 7 and all points tend
to converge to the same location. When Z? is too
high, it prevents meaningful clusters from coalesc-
946
 0.71
 0.2
 0.21
 0.1
 0.11
 0.3
 0.31
 0.4
 0.41
 0.5
 6  60  600
89numb er ofaedm dpnm8ope8o
nte
Figure 3: MTO falls sharply for less than 10 S-CODE
dimensions, but more than 25 do not help.
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 0.01  0.1  1
number o faadbptifs tb
i2b
Figure 4: MTO is fairly stable as long as the Z? constant
is within an order of magnitude of the real Z value.
ing.
We find the random partition algorithm to be
fairly robust to different parameter settings and the
resulting many-to-one score significantly better than
the bigram baseline.
5.3 Random substitutes
Another way to use substitute vectors in a dis-
crete setting is simply to sample individual substi-
tute words from them. The random-substitutes al-
gorithm cycles through the test data and pairs each
word with a random substitute picked from the pre-
computed substitute vectors (see Section 3). We ran
the random-substitutes algorithm to generate 14 mil-
lion word (X) ? random-substitute (Y ) pairs (12
substitutes for each token) as input to S-CODE.
Clustering the resulting ?x vectors yields a many-
to-one score of .7680 (.0038) and a V-measure of
.6822 (.0029).
This result is close to the previous result by the
random-partition algorithm, .7554 (.0055), demon-
strating that two very different discrete represen-
tations of context based on paradigmatic features
give consistent results. Both results are significantly
above the bigram baseline, .7314 (.0096). Figure 5
illustrates that the random-substitute result is fairly
robust as long as the training algorithm can observe
more than a few random substitutes per word.
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 1  10  100
number of random pubptitutep ser ord
m2o
Figure 5: MTO is not sensitive to the number of random
substitutes sampled per word token.
5.4 Morphological and orthographic features
Clark (2003) demonstrates that using morpholog-
ical and orthographic features significantly im-
proves part-of-speech induction with an HMM
based model. Section 2 describes a number other ap-
proaches that show similar improvements. This sec-
tion describes one way to integrate additional fea-
tures to the random-substitute model.
The orthographic features we used are similar to
the ones in (Berg-Kirkpatrick et al 2010) with small
modifications:
? Initial-Capital: this feature is generated for cap-
italized words with the exception of sentence
initial words.
? Number: this feature is generated when the to-
ken starts with a digit.
? Contains-Hyphen: this feature is generated for
lowercase words with an internal hyphen.
947
? Initial-Apostrophe: this feature is generated for
tokens that start with an apostrophe.
We generated morphological features using the
unsupervised algorithm Morfessor (Creutz and La-
gus, 2005). Morfessor was trained on the WSJ sec-
tion of the Penn Treebank using default settings, and
a perplexity threshold of 300. The program induced
5 suffix types that are present in a total of 10,484
word types. These suffixes were input to S-CODE
as morphological features whenever the associated
word types were sampled.
In order to incorporate morphological and ortho-
graphic features into S-CODE we modified its in-
put. For each word ? random-substitute pair gen-
erated as in the previous section, we added word ?
feature pairs to the input for each morphological and
orthographic feature of the word. Words on average
have 0.25 features associated with them. This in-
creased the number of pairs input to S-CODE from
14.1 million (12 substitutes per word) to 17.7 mil-
lion (additional 0.25 features on average for each of
the 14.1 million words).
Using similar training settings as the previous
section, the addition of morphological and ortho-
graphic features increased the many-to-one score of
the random-substitute model to .8023 (.0070) and
V-measure to .7207 (.0041). Both these results im-
prove the state-of-the-art in part-of-speech induction
significantly as seen in Table 1.
6 Error Analysis
Figure 6 is the Hinton diagram showing the rela-
tionship between the most frequent tags and clusters
from the experiment in Section 5.4. In general the
errors seem to be the lack of completeness (multi-
ple large entries in a row), rather than lack of ho-
mogeneity (multiple large entries in a column). The
algorithm tends to split large word classes into sev-
eral clusters. Some examples are:
? Titles like Mr., Mrs., and Dr. are split from the
rest of the proper nouns in cluster (39).
? Auxiliary verbs (10) and the verb ?say? (22)
have been split from the general verb clusters
(12) and (7).
? Determiners ?the? (40), ?a? (15), and capital-
ized ?The?, ?A? (6) have been split into their
own clusters.
? Prepositions ?of? (19), and ?by?, ?at? (17) have
been split from the general preposition cluster
(8).
Nevertheless there are some homogeneity errors as
well:
? The adjective cluster (5) also has some noun
members probably due to the difficulty of sep-
arating noun-noun compounds from adjective
modification.
? Cluster (6) contains capitalized words that span
a number of categories.
Most closed-class items are cleanly separated into
their own clusters as seen in the lower right hand
corner of the diagram. The completeness errors are
not surprising given that the words that have been
split are not generally substitutable with the other
members of their Penn Treebank category. Thus it
can be argued that metrics that emphasize homo-
geneity such as MTO are more appropriate in this
context than metrics that average homogeneity and
completeness such as VM as long as the number of
clusters is controlled.
7 Contributions
Our main contributions can be summarized as fol-
lows:
? We introduced substitute vectors as paradig-
matic representations of word context and
demonstrated their use in syntactic category ac-
quisition.
? We demonstrated that using paradigmatic rep-
resentations of word context and modeling co-
occurrences of word and context types with
the S-CODE learning framework give superior
results when compared to a baseline bigram
model.
? We extended the S-CODE framework to in-
corporate morphological and orthographic fea-
tures and improved the state-of-the-art in un-
supervised part-of-speech induction to 80%
many-to-one accuracy.
948
Figure 6: Hinton diagram comparing most frequent tags and clusters.
? All our code and data, including the sub-
stitute vectors for the one million word
Penn Treebank Wall Street Journal dataset,
is available at the authors? website at
http://goo.gl/RoqEh.
References
B. Ambridge and E.V.M. Lieven, 2011. Child Language
Acquisition: Contrasting Theoretical Approaches,
chapter 6.1. Cambridge University Press.
D. Arthur and S. Vassilvitskii. 2007. k-means++: The
advantages of careful seeding. In Proceedings of the
eighteenth annual ACM-SIAM symposium on Discrete
algorithms, pages 1027?1035. Society for Industrial
and Applied Mathematics.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1288?1297, Uppsala, Sweden, July.
Association for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582?590, Los Angeles,
California, June. Association for Computational Lin-
guistics.
C. Biemann. 2006. Unsupervised part-of-speech tagging
employing efficient graph clustering. In Proceedings
of the 21st International Conference on computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics: Student Research
Workshop, pages 7?12. Association for Computational
Linguistics.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479, December.
D. Chandler. 2007. Semiotics: the basics. The Basics
Series. Routledge.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
pos induction: how far have we come? In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 575?
584, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A bayesian mixture model
for pos induction using multiple features. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 638?647, Edin-
burgh, Scotland, UK., July. Association for Computa-
tional Linguistics.
949
Kenneth Ward Church. 1988. A stochastic parts pro-
gram and noun phrase parser for unrestricted text. In
Proceedings of the second conference on Applied nat-
ural language processing, ANLC ?88, pages 136?143,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth conference on Eu-
ropean chapter of the Association for Computational
Linguistics - Volume 1, EACL ?03, pages 59?66,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In Proceedings of AKRR?05, Inter-
national and Interdisciplinary Conference on Adap-
tive Knowledge Representation and Reasoning, pages
106?113, Espoo, Finland, June.
D. Freudenthal, J.M. Pine, and F. Gobet. 2005. On the
resolution of ambiguities in the extraction of syntactic
categories through chunking. Cognitive Systems Re-
search, 6(1):17?25.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. J. Mach. Learn. Res.,
99:2001?2049, August.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 344?352, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Amir Globerson, Gal Chechik, Fernando Pereira, and
Naftali Tishby. 2007. Euclidean embedding of co-
occurrence data. J. Mach. Learn. Res., 8:2265?2295,
December.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
David Graff, Roni Rosenfeld, and Doug Paul. 1995. Csr-
iii text. Linguistic Data Consortium, Philadelphia.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, HLT-NAACL
?06, pages 320?327, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Michael Lamar, Yariv Maron, and Elie Bienenstock.
2010a. Latent-descriptor clustering for unsupervised
pos induction. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 799?809, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michael Lamar, Yariv Maron, Mark Johnson, and Elie
Bienenstock. 2010b. Svd and clustering for unsuper-
vised pos tagging. In Proceedings of the ACL 2010
Conference Short Papers, pages 215?219, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 853?861, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-of-
speech induction. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1567?1575.
Bernard Merialdo. 1994. Tagging english text with a
probabilistic model. Comput. Linguist., 20:155?171,
June.
T.H. Mintz. 2003. Frequent frames as a cue for gram-
matical categories in child directed speech. Cognition,
90(1):91?117.
M. Redington, N. Crater, and S. Finch. 1998. Distribu-
tional information: A powerful cue for acquiring syn-
tactic categories. Cognitive Science, 22(4):425?469.
A. Rosenberg and J. Hirschberg. 2007. V-measure: A
conditional entropy-based external cluster evaluation
measure. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 410?420.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the seventh conference
950
on European chapter of the Association for Compu-
tational Linguistics, EACL ?95, pages 141?148, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286, November.
951
The Noisy Channel Model for Unsupervised
Word Sense Disambiguation
Deniz Yuret?
Koc? University
Mehmet Ali Yatbaz
Koc? University
We introduce a generative probabilistic model, the noisy channel model, for unsupervised word
sense disambiguation. In our model, each context C is modeled as a distinct channel through
which the speaker intends to transmit a particular meaning S using a possibly ambiguous word
W. To reconstruct the intended meaning the hearer uses the distribution of possible meanings
in the given context P(S|C) and possible words that can express each meaning P(W|S). We
assume P(W|S) is independent of the context and estimate it using WordNet sense frequencies.
The main problem of unsupervised WSD is estimating context-dependent P(S|C) without access
to any sense-tagged text. We show one way to solve this problem using a statistical language
model based on large amounts of untagged text. Our model uses coarse-grained semantic classes
for S internally and we explore the effect of using different levels of granularity on WSD per-
formance. The system outputs fine-grained senses for evaluation, and its performance on noun
disambiguation is better than most previously reported unsupervised systems and close to the
best supervised systems.
1. Introduction
Word sense disambiguation (WSD) is the task of identifying the correct sense of an
ambiguous word in a given context. An accurate WSD system would benefit appli-
cations such as machine translation and information retrieval. The most successful
WSD systems to date are based on supervised learning and trained on sense-tagged
corpora. In this article we present an unsupervised WSD algorithm that can leverage
untagged text and can perform at the level of the best supervised systems for the all-
nouns disambiguation task.
The main drawback of the supervised approach is the difficulty of acquiring
considerable amounts of training data, also known as the knowledge acquisition
bottleneck. Yarowsky and Florian (2002) report that each successive doubling of the
training data for WSD only leads to a 3?4% error reduction within their experimental
range. Banko and Brill (2001) experiment with the problem of selection among
confusable words and show that the learning curves do not converge even after
? Koc? University, Department of Computer Engineering, 34450 Sar?yer, I?stanbul, Turkey.
E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr.
Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication:
12 September 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
a billion words of training data. They suggest unsupervised, semi-supervised, or
active learning to take advantage of large data sets when labeling is expensive. Yuret
(2004) observes that in a supervised naive Bayes WSD system trained on SemCor,
approximately half of the test instances do not contain any of the contextual features
(e.g., neighboring content words or local collocation patterns) observed in the training
data. SemCor is the largest publicly available corpus of sense-tagged text, and has only
about a quarter million sense-tagged words. In contrast, our unsupervised system uses
the Web1T data set (Brants and Franz 2006) for unlabeled examples, which contains
counts from a 1012 word corpus derived from publicly-available Web pages.
A note on the term ?unsupervised? may be appropriate here. In the WSD literature
?unsupervised? is typically used to describe systems that do not directly use sense-
tagged corpora for training. However, many of these unsupervised systems, including
ours, use sense ordering or sense frequencies from WordNet (Fellbaum 1998) or other
dictionaries. Thus it might be more appropriate to call them weakly supervised or
semi-supervised. More specifically, context?sense pairs or context?word?sense triples
are not observed in the training data, but context-word frequencies (from untagged
text) and word-sense frequencies (from dictionaries or other sources) are used in model
building. One of the main problems we explore in this study is the estimation of context-
dependent sense probabilities when no context?sense pairs have been observed in the
training data.
The first contribution of this article is a probabilistic generative model for word
sense disambiguation that seamlessly integrates unlabeled text data into the model
building process. Our approach is based on the noisy channel model (Shannon 1948),
which has been an essential ingredient in fields such as speech recognition and machine
translation. In this study we demonstrate that the noisy channel model can also be the
key component for unsupervised word sense disambiguation, provided we can solve
the context-dependent sense distribution problem. In Section 2.1 we show one way
to estimate the context-dependent sense distribution without using any sense-tagged
data. Section 2.2 outlines the complete unsupervised WSD algorithm using this model.
We estimate the distribution of coarse-grained semantic classes rather than fine-grained
senses. The solution uses the two distributions for which we do have data: the distribu-
tion of words used to express a given sense, and the distribution of words that appear
in a given context. The first can be estimated using WordNet sense frequencies, and the
second can be estimated using an n-gram language model as described in Section 2.3.
The second contribution of this article is an exploration of semantic classes at differ-
ent levels of granularity for word sense disambiguation. Using fine-grained senses for
model building is inefficient both computationally and from a learning perspective. The
noisy channel model can take advantage of the close distribution of similar senses if they
are grouped into semantic classes. We take semantic classes to be groups of WordNet
synsets defined using the hypernym hierarchy. In each experiment we designate a
number of synsets high in the WordNet hypernym hierarchy as ?head synsets? and
use their descendants to partition the senses into separate semantic classes. In Section 3
we present performance bounds for such class-based WSD and describe our method of
exploring the different levels of granularity.
In Section 4 we report on our actual experiments and compare our results with
the best supervised and unsupervised systems from SensEval-2 (Cotton et al 2001),
SensEval-3 (Mihalcea and Edmonds 2004), and SemEval-2007 (Agirre, Ma`rquez, and
Wicentowski 2007). Section 5 discusses these results and the idiosyncrasies of the
data sets, baselines, and evaluation metrics used. Section 6 presents related work, and
Section 7 summarizes our contributions.
112
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
2. The Noisy Channel Model for WSD
2.1 Model
The noisy channel model has been the foundation of standard models in speech recog-
nition (Bahl, Jelinek, and Mercer 1983) and machine translation (Brown et al 1990).
In this article we explore its application to WSD. The noisy channel model can be
used whenever a signal received does not uniquely identify the message being sent.
Bayes? Law is used to interpret the ambiguous signal and identify the most probable
intended message. In WSD, we model each context as a distinct channel where the
intended message is a word sense (or semantic class) S, and the signal received is an
ambiguous wordW. In this section we will describe how to model a given context C as
a noisy channel, and in particular how to estimate the context-specific sense distribution
without using any sense-tagged data.
Equation (1) expresses the probability of a sense S of wordW in a given context C.
This is the well-known Bayes? formula with an extra P(.|C) in each term indicating the
dependence on the context.
P(S|W,C) =
P(W|S,C)P(S|C)
P(W|C)
(1)
To perform WSD we need to find the sense S that maximizes the probability P(S|W,C).
This is equivalent to the maximization of the product P(W|S,C)P(S|C) because the
denominator P(W|C) does not depend on S. To perform the maximization, the two
distributions P(W|S,C) and P(S|C) need to be estimated for each context C.
The main challenge is to estimate P(S|C), the distribution of word senses that can
be expressed in the given context. In unsupervised WSD we do not have access to any
sense-tagged data, thus we do not know what senses are likely to be expressed in any
given context. Therefore it is not possible to estimate P(S|C) directly.
What we do have is the word frequencies for each sense P(W|S), and the word
frequencies for the given context P(W|C). We use the WordNet sense frequencies to
estimate P(W|S) and a statistical language model to estimate P(W|C) as detailed in
Section 2.3. We make the independence assumption P(W|S,C) = P(W|S), that is, the
distribution of words used to express a particular sense is the same for all contexts.
Finally, the relationship between the three distributions, P(S|C), P(W|S,C), and P(W|C)
is given by the total probability theorem:
P(W|C) =
?
S
P(S|C)P(W|S,C) (2)
We can solve for P(S|C) using linear algebra. Let WS be a matrix, s and w two vectors
such that:
WSij = P(W = i|S = j)
sj = P(S = j|C = k)
wi = P(W = i|C = k) (3)
113
Computational Linguistics Volume 36, Number 1
Using this new form, we can see that Equation (2) is equivalent to the linear equation
w =WS? s and s can be solved using a linear solver. Typically WS is a tall matrix and
the system has no exact solutions. We use the Moore?Penrose pseudoinverse WS+ to
compute an approximate solution:
s =WS+ ? w (4)
Appendix A discusses possible scaling issues of this solution and offers alternative
solutions. We use the pseudoinverse solution in all our experiments because it can be
computed fast and none of the alternatives we tried made a significant difference in
WSD performance.
2.2 Algorithm
Section 2.1 described how to apply the noisy channel model for WSD in a single context.
In this section we present the steps we follow in our experiments to simultaneously
apply the noisy channel model to all the contexts in a given word sense disambiguation
task.
Algorithm 1
1. Let W be the vocabulary. In this study we took the vocabulary to be the
approximately 12,000 nouns in WordNet that have non-zero sense
frequencies.
2. Let S be the set of senses or semantic classes to be used. In this study we
used various partitions of noun synsets as semantic classes.
3. Let C be the set of contexts (nine-word windows for a 5-gram model)
surrounding each target word in the given WSD task.
4. Compute the matrix WC where WCik = P(W = i|C = k). Here i ranges
over the vocabulary W and k ranges over the contexts C. This matrix
concatenates the (w) word distribution vectors from Equation (4) for each
context. The entries of the matrix are computed using the n-gram language
model described in Section 2.3. This is the most expensive step in the
algorithm (see Appendix B for a discussion of implementation efficiency).
5. Compute the matrix WS where WSij = P(W = i|S = j). Here i ranges over
the vocabulary W and j ranges over the semantic classes S. The entries of
the matrix are computed using the WordNet sense frequencies.
6. Compute the matrix SC =WS+ ?WC where SCjk = P(S = j|C = k).
Here j ranges over the semantic classes S and k ranges over the contexts C.
This step computes the pseudoinverse solution described in Section 2.1
simultaneously for all the contexts, and the resulting SC matrix is a
concatenation of the (s) solution vectors from Equation (4) for each context.
WS+ is the pseudoinverse of the matrix WS.
7. Compute the best semantic class for each WSD instance by using
argmaxSP(S|W,C) ? P(W|S)P(S|C). Here P(S|C) comes from the column
of the SC matrix that corresponds to the context of the WSD instance
114
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
and P(W|S) comes from the row of the WS matrix that corresponds to the
word to be disambiguated.
8. Compute the fine-grained answer for each WSD instance by taking the
most frequent (lowest numbered) sense in the chosen semantic class.
9. Apply the one sense per discourse heuristic: If a word is found to have
multiple senses in a document, replace them with the majority answer.
2.3 Estimation Procedure
In Section 2.1, we showed how the unsupervised WSD problem expressed as a noisy
channel model can be decomposed into the estimation of two distributions: P(W|S) and
P(W|C). In this section we detail our estimation procedure for these two distributions.
To estimate P(W|S), the distribution of words that can be used to express a given
meaning, we used the WordNet sense frequencies.1 We did not perform any smoothing
for the zero counts and used the maximum likelihood estimate: count(W,S)/count(S).
As described in later sections, we also experimented with grouping similar WordNet
senses into semantic classes. In this case S stands for the semantic class, and the counts
from various senses of a word in the same semantic class are added together to estimate
P(W|S).
To estimate the distribution of words in a given context, P(W|C), we used a 5-gram
language model. We define the context as the nine-word window centered on the target
word w1w2 . . .w9, whereW = w5. The probability of a word in the given context can be
expressed as:
P(W = w5) ? P(w1 . . .w9) (5)
= P(w1)P(w2|w1) . . . P(w9|w1 . . .w8) (6)
? P(w5|w1 . . .w4)P(w6|w2 . . .w5)P(w7|w3 . . .w6) (7)
P(w8|w4 . . .w7)P(w9|w5 . . .w8)
Equation (5) indicates that P(W|C) is proportional to P(w1 . . .w9) because the other
words in the context are fixed for a given WSD instance. Equation (6) is the standard
decomposition of the probability of a word sequence into conditional probabilities.
The first four terms do not include the target word w5, and have been dropped in Equa-
tion (7). We also truncate the remaining conditionals to four words reflecting the Markov
assumption of the 5-gram model. Finally, using an expression that is proportional to
P(W|C) instead of P(W|C) itself will not change the WSD result because we are taking
the argmax in Equation (1).
Each term on the right hand side of Equation (7) is estimated using a 5-gram
language model. To get accurate domain-independent probability estimates we used the
Web 1T data set (Brants and Franz 2006), which contains the counts of word sequences
up to length five in a 1012 word corpus derived from publicly-accessible Web pages.
Estimation of P(W|C) is the most computationally expensive step of the algorithm, and
some implementation details are given in Appendix B.
1 The sense frequencies were obtained from the index.sense file included in the WordNet distribution.
We had to correct the counts of three words (person, group, and location) whose WordNet counts
unfortunately include the corresponding named entities and are thus inflated.
115
Computational Linguistics Volume 36, Number 1
Figure 1
Upper bound on fine-grained accuracy for a given number of semantic classes.
3. Semantic Classes
Our algorithm internally differentiates semantic classes rather than fine-grained senses.
Using fine-grained senses in the noisy channel model would be computationally ex-
pensive because the word?sense matrix needs to be inverted (see Equation [4]). It is
also unclear whether using fine-grained senses for model building will lead to better
learning performance: The similarity between the distributions of related senses is
ignored and the data becomes unnecessarily fragmented.
Even though we use coarse-grained semantic classes for model building, we use
fine-grained senses for evaluation. During evaluation, the coarse-grained semantic
classes predicted by the model are mapped to fine-grained senses by picking the lowest
numbered WordNet sense in the chosen semantic class.2 This is necessary to perform a
meaningful comparison with published results.
We take semantic classes to be groups of WordNet synsets defined using the hyper-
nym hierarchy (see Section 6 for alternative definitions). Section 4 presents three WSD
experiments using different sets of semantic classes at different levels of granularity.
In each experiment we designate a number of synsets high in the WordNet hypernym
hierarchy as ?head synsets? and use their descendants to form the separate semantic
classes.
An arbitrary set of head synsets will not necessarily have mutually exclusive and
collectively exhaustive descendants. To assign every synset to a unique semantic class,
we impose an ordering on the semantic classes. Each synset is assigned only to the first
semantic class whose head it is a descendant of according to this ordering. If there are
synsets that are not descendants of any of the heads, they are collected into a separate
semantic class created for that purpose.
Using the coarse-grained semantic classes for prediction, Algorithm 1 will be unable
to return the correct fine-grained sense when this is not the lowest numbered sense in
a semantic class. To quantify the restrictive effect of working with a small number of
semantic classes, Figure 1 plots the number of semantic classes versus the best possible
2 The sense numbers are ordered by the frequency of occurrence in WordNet.
116
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
oracle accuracy for the nouns in the SemCor corpus. To compute the oracle accuracy, we
assume that the program can find the correct semantic class for each instance, but has to
pick the first sense in that class as the answer. To construct a given number of semantic
classes, we used the following algorithm:
Algorithm 2
1. Initialize all synsets to be in a single ?default? semantic class.
2. For each synset, compute the following score: the oracle accuracy achieved
if that synset and all its descendants are split into a new semantic class.
3. Take the synset with the highest score and split that synset and its
descendants into a new semantic class.
4. Repeat steps 2 and 3 until the desired number of semantic classes is
achieved.
The upper bound on fine-grained accuracy given a small number of semantic
classes is surprisingly high. In particular, the best reported noun WSD accuracy (78%)
is achievable if we could perfectly distinguish between five semantic classes.
4. Three Experiments
We ran three experiments with the noisy channel model using different sets of semantic
classes. The first experiment uses the 25 WordNet semantic categories for nouns, the
second experiment looks at what happens when we group all the senses to just two
or three semantic classes, and the final experiment optimizes the number of semantic
classes using one data set (which gives 135 classes) and reports the out-of-sample result
using another data set.
The noun instances from the last three SensEval/SemEval English all-words tasks
are used for evaluation. We focus on the disambiguation of nouns for several reasons.
Nouns constitute the largest portion of content words (48% of the content words in the
Brown corpus [Kucera and Francis 1967] are nouns). For many tasks and applications
(e.g., Web queries [Jansen, Spink, and Pfaff 2000]) nouns are the most frequently encoun-
tered and important part of speech. Finally, WordNet has a more complete coverage
of noun semantic relations than other parts of speech, which is important for our
experiments with semantic classes.
As described in Section 2.2 we use the model to assign each ambiguous word to its
most likely semantic class in all the experiments. The lowest numbered sense in that
class is taken as the fine-grained answer. Finally we apply the one sense per discourse
heuristic: If the same word has been assigned more than one sense within the same
document, we take a majority vote and use sense numbers to break the ties.
Table 1 gives some baselines for comparison. The performance of the best super-
vised and unsupervised systems on noun disambiguation for each data set are given.
The first-sense baseline (FSB) is obtained by always picking the lowest numbered sense
for the word in the appropriate WordNet version. We prefer the FSB baseline over the
commonly used most-frequent-sense baseline because the tie breaking is unambiguous.
All the results reported are for fine-grained sense disambiguation. The top three systems
given in the table for each task are all supervised systems; the result for the best
117
Computational Linguistics Volume 36, Number 1
Table 1
Baselines for the three SensEval English all-words tasks; the WordNet version used (WN);
number of noun instances (Nouns); percentage accuracy of the first-sense baseline (FSB); the top
three supervised systems; and the best unsupervised system (Unsup). The last row gives the
total score of the best systems on the three tasks.
Task WN Nouns FSB 1st 2nd 3rd Unsup
senseval2 1.7 1,067 71.9 78.0 74.5 70.0 61.8
senseval3 1.7.1 892 71.0 72.0 71.2 71.0 62.6
semeval07 2.1 159 64.2 68.6 66.7 66.7 63.5
total 2,118 70.9 74.4 72.5 70.2 62.2
unsupervised system is given in the last column. The reported unsupervised systems
do use the sense ordering and frequency information from WordNet.
4.1 First Experiment: The 25 WordNet Categories
In previous work, descendants of 25 special WordNet synsets (known as the unique
beginners) have been used as the coarse-grained semantic classes for nouns (Crestan,
El-Be`ze, and De Loupy 2001; Kohomban and Lee 2005). These unique beginners were
used to organize the nouns into 25 lexicographer files based on their semantic category
during WordNet development. Figure 2 shows the synsets at the top of the noun
hierarchy in WordNet. The 25 unique beginners have been shaded, and the two graphics
show how the hierarchy evolved between the two WordNet versions used in this study.
Figure 2
The top of the WordNet noun hypernym hierarchy for version 1.7 (left) and version 2.1 (right).
The 25 WordNet noun categories are shaded.
118
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
Table 2
The performance of the noisy channel model with the 25 semantic classes based on WordNet
lexicographer files. The columns give the data set, the percentage of times the model picks the
correct semantic class, maximum possible fine-grained score if the model had always picked the
correct class, and the actual score.
Data Set CorrClass MaxScore Score
senseval2 85.1 90.3 77.7
senseval3 78.0 88.7 70.1
semeval07 75.5 86.2 64.8
total 81.4 89.3 73.5
We ran our initial experiments using these 25 WordNet categories as semantic
classes. The distribution of words for each semantic class, P(W|S), is estimated based
on WordNet sense frequencies. The distribution of words for each context, P(W|C), is
estimated using a 5-gram model based on the Web 1T corpus. The system first finds the
most likely semantic class based on the noisy channel model, then picks the first sense in
that class. Table 2 gives the results for the three data sets, which are significantly higher
than the previously reported unsupervised results.
To illustrate which semantic classes are the most difficult to disambiguate, Table 3
gives the confusion matrix for the Senseval2 data set. We can see that frequently occur-
ring concrete classes like person and body are disambiguated well. The largest source
of errors are the abstract classes like act, attribute, cognition, and communication. These
25 classes may not be the ideal candidates for word sense disambiguation. Even though
they allow a sufficient degree of fine-grained distinction (Table 2 shows that we can get
Table 3
Confusion matrix for Senseval2 data with the 25 WordNet noun classes. The rows are actual
classes, the columns are predicted classes. Column names have been abbreviated to save space.
The last two columns give the frequency of the class (F) and the accuracy of the class (A).
ac an ar at bo co co ev fe fo gr lo mo ob pe ph po pr qu re sh st su ti F A
act 58 0 4 7 0 7 2 3 2 0 5 0 0 0 0 0 1 4 1 1 0 2 0 0 9.1 59.8
animal 0 17 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.1 77.3
artifact 0 0 66 2 0 0 6 5 0 0 5 1 0 1 0 0 0 0 0 0 3 1 0 0 8.4 73.3
attribute 3 0 0 19 0 3 0 0 0 0 0 1 0 1 2 0 2 1 0 0 1 3 0 0 3.4 52.8
body 0 0 0 0 123 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 11.6 99.2
cognition 6 0 1 2 0 82 5 1 0 0 0 2 1 1 1 0 1 0 5 1 0 5 0 0 10.7 71.9
communicat 2 0 1 0 0 2 29 1 0 0 0 2 5 0 0 1 0 0 0 1 0 0 0 2 4.3 63.0
event 0 0 0 0 0 0 0 19 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 2.0 90.5
feeling 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.4 100.
food 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 100.
group 0 0 0 2 0 5 0 0 0 0 69 2 0 3 0 0 0 0 0 1 1 0 0 1 7.9 82.1
location 0 0 0 1 0 0 0 0 0 0 0 22 0 0 0 0 0 0 0 0 0 0 0 0 2.2 95.7
motive 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0.2 50.0
object 2 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0.7 14.3
person 2 4 0 0 0 1 1 0 0 0 1 0 0 0 168 0 0 0 0 0 0 0 0 0 16.6 94.9
phenomenon 1 0 0 1 0 1 0 2 0 0 0 0 0 0 0 3 0 0 0 3 0 0 0 0 1.0 27.3
possession 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0.4 100.
process 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 12 0 0 0 1 0 0 1.4 80.0
quantity 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 10 0 0 0 0 0 1.2 76.9
relation 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0.3 66.7
shape 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0.1 100.
state 1 0 1 5 0 1 1 2 0 0 1 0 0 0 1 0 0 0 0 0 0 98 0 0 10.4 88.3
substance 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 0 0.9 100.
time 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 44 4.8 86.3
119
Computational Linguistics Volume 36, Number 1
Table 4
The performance of the noisy channel model with two to three semantic classes. The columns
give the data set, the head synsets, the percentage of times the model picks the correct semantic
class, maximum possible fine-grained score if the model had always picked the correct class, and
the actual score.
Data Set Heads CorrClass MaxScore Score
senseval2 entity/default 86.6 76.8 74.9
senseval3 entity/default 94.2 75.8 71.2
senseval3 object/entity/default 93.8 77.4 72.9
semeval07 psychological-feature/default 91.2 74.8 68.6
85?90% if we could pick the right class every time), they seem too easy to confuse. In the
next few experiments we will use these observations to design better sets of semantic
classes.
4.2 Second Experiment: Distinguishing Mental and Physical Concepts
Figure 1 shows that the upper bound for fine-grained disambiguation is relatively high
even for a very small number of semantic classes. In our next experiment we look at
how well our approach can perform differentiating only two or three semantic classes.
We use Algorithm 2 applied to the appropriate version of SemCor to pick the head
synsets used to define the semantic classes. Figure 2 shows that the top level of the
hypernym hierarchy has changed significantly between the WordNet versions. Thus,
different head synsets are chosen for different data sets. However, the main distinction
captured by our semantic classes seems to be between mental and physical concepts.
Table 4 gives the results. The performance with a few semantic classes is comparable to
the top supervised algorithms in each of the three data sets.
4.3 Third Experiment: Tuning the Number of Classes
Increasing the number of semantic classes has two opposite effects on WSD perfor-
mance. The higher the number, the finer distinctions we can make, and the maximum
possible fine-grained accuracy goes up. However, the more semantic classes we define,
the more difficult it becomes to distinguish them from one another. For an empirical
analysis of the effect of semantic class granularity on the fine-grained WSD accuracy,
we generated different sets of semantic classes using the following algorithm.
Algorithm 3
1. Sort all the synsets according to their ?subtree frequency?: i.e., the total
frequency of each synset?s descendants in the hypernym tree.
2. Take the desired number of synsets with the highest subtree frequency and
use them as head synsets, that is, split their descendants into separate
semantic classes.
Figure 3 shows the fine-grained accuracy we achieved on the Senseval2 data set
with up to 600 semantic classes defined based on Algorithm 3. Note the differences: (i)
120
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
Figure 3
The fine-grained accuracy on Senseval2 data set for a given number of semantic classes.
Figure 1 gives the best possible oracle accuracy, Figure 3 gives the actual WSD accuracy;
(ii) Algorithm 2 chooses the head synsets based on their oracle score, Algorithm 3
chooses them based on their subtree frequency.
As we suspected, the relationship is not simple or monotonic. However, one can
identify distinct peaks at 3, 25, and 100?150 semantic classes. One hypothesis is that
these peaks correspond to ?natural classes? at different levels of granularity. Here are
some example semantic classes from each peak:
3 classes entity, abstraction
25 classes action, state, content, location, attribute, ...
135 classes food, day, container, home, word, business, feeling, material, job, man, ...
To test the out-of-sample effect of tuning the semantic classes based on the peaks
of Figure 3, we used the SemEval-2007 data set as our test sample. When the 135
semantic classes from the highest peak are used for the disambiguation of the nouns
in the SemEval-2007 data set, an accuracy of 69.8% was achieved. This is higher than
the accuracy of the best supervised system on this task (68.6%), although the difference
is not statistically significant.
5. Discussion
In this section we will address several questions raised by the results of the experi-
ments. Why do we get different results from different data sets? Are the best results
significantly different than the first-sense baseline? Can we improve our results using
better semantic classes?
5.1 Why Do We Get Different Results from Different Data Sets?
Table 5 summarizes our results from the three experiments of Section 4. There are some
significant differences between the data sets.
121
Computational Linguistics Volume 36, Number 1
Table 5
Result summary for the three data sets. The columns give the data set, the results of the three
experiments, best reported result, the first-sense baseline, and the number of instances.
Data set Exp1 Exp2 Exp3 Best FSB Instances
senseval2 77.7 74.9 - 78.0 71.9 1,067
senseval3 70.1 72.9 - 72.0 71.0 892
semeval07 64.8 68.6 69.8 68.6 64.2 159
The SemEval-2007 data set appears to be significantly different from the other two
with its generally lower baseline and scores. The difference in accuracy is probably
due to the difference in data preparation. In the two Senseval data sets all content
words were targeted for disambiguation. In the SemEval-2007 data set only verbs and
their noun arguments were selected, targeting only about 465 lemmas from about 3,500
words of text. For the Senseval-3 data set none of our results, or any published result
we know of, is significantly above the baseline for noun disambiguation. This may be
due to extra noise in the data?the inter-annotator agreement for nouns in this data set
was 74.9%.
5.2 Are the Best Results Significantly Different Than the FSB?
Among all the published results for these three data sets, our two results for the
Senseval-2 data set and the top supervised result for the Senseval-2 data set are the
only ones statistically significantly above the FSB for noun disambiguation at the 95%
confidence interval. This is partly because of the lack of sufficient data. For example, the
SemEval-2007 data set has only 159 nouns; and a result of 71.8% would be needed to
demonstrate a difference from the baseline of 64.2% at the 95% confidence interval.
More importantly, however, statistical significance should not be confused with
?significance? in general. A statistically significant difference may not be necessary or
sufficient for a significant impact on an application. Even a WSD system that is statis-
tically indistinguishable from the baseline according to the ?total accuracy? metric is
most probably providing significantly different answers compared to always guessing
the first sense. There are metrics that can reveal these differences, such as ?balanced
error rate? (i.e., arithmetic average of the error rates for different senses) or ?accuracy in
detecting the use of a non-dominant sense.?
Finally, the relatively high first-sense baseline (e.g., 71.0% for Senseval-3 nouns)
combined with the relatively low inter-annotator agreement (e.g., 74.9% for Senseval-
3 nouns) makes progress in the traditional WSD task difficult. Annotators who are
perfectly proficient in comprehending language nevertheless find it difficult to distin-
guish between artificially-created dictionary senses. If our long term goal is to model
human competence in language comprehension, it would make sense to focus on tasks
at which humans are naturally competent. Dictionary-independent tasks such as lexical
substitution or textual entailment may be the right steps in this direction.
5.3 Can We Improve Our Results Using Better Semantic Classes?
In order to get an upper bound for our approach, we searched for the best set of semantic
classes specific to each data set using the following greedy algorithm.
122
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
Table 6
The performance of the noisy channel model with the best set of semantic classes picked for each
data set. The columns give the data set, the number of classes, maximum possible score if the
model always picks the correct class, percentage of times it actually picks the correct class, and
its fine-grained accuracy.
Data Set NumClass MaxScore CorrClass Score
senseval2 23 89.2 88.8 80.1
senseval3 29 87.2 87.4 77.4
semeval07 12 84.9 89.9 79.2
Algorithm 4
1. Initialize all synsets to be in a single ?default? semantic class.
2. For each synset, compute the following score: the WSD accuracy achieved
if that synset and all its descendants are split into a new semantic class.
3. Take the synset with the highest score and split that synset and its
descendants into a new semantic class.
4. Repeat steps 2 and 3 until the WSD accuracy can no longer be improved.
Algorithm 4 was run for each of the three data sets, which resulted in three different
sets of semantic classes. The noisy channel model was applied with the best set of
semantic classes for each data set. Table 6 summarizes the results. Note that these results
are not predictive of out-of-sample accuracy because Algorithm 4 picks a specific set of
semantic classes optimal for a given data set. But the results do indicate that a better
set of semantic classes may lead to significantly better WSD accuracy. In particular
each result in Table 6 is significantly higher than previously reported supervised or
unsupervised results.
How to construct a good set of semantic classes that balance specificity and identi-
fiability is a topic of ongoing research. See Kohomban and Lee (2007) for a supervised
solution using feature-based clustering that tries to maintain feature?class coherence.
Non-parametric Bayesian approaches such as Teh et al (2006) applied to context distri-
butions could reveal latent senses in an unsupervised setting.
6. Related Work
For a general overview of different approaches to WSD, see Navigli (2009) and
Stevenson (2003). The Senseval and SemEval workshops (Cotton et al 2001; Mihalcea
and Edmonds 2004; Agirre, Ma`rquez, and Wicentowski 2007) are good sources of recent
work, and have been used in this article to benchmark our results.
Generative models based on the noisy channel framework have previously been
used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation
(Brown et al 1990), question answering (Echihabi and Marcu 2003), spelling correction
(Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among
others. To our knowledge our work is the first application of the noisy channel model
to unsupervised word sense disambiguation.
123
Computational Linguistics Volume 36, Number 1
Using statistical language models based on large corpora for WSD has been ex-
plored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this
article see Yuret (2008); for a more general review of statistical language modeling see
Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001).
Grouping similar senses into semantic classes for WSD has been explored in previ-
ous work. Senses that are similar have been identified using WordNet relations (Peters,
Peters, and Vossen 1998; Crestan, El-Be`ze, and De Loupy 2001; Kohomban and Lee
2005), discourse domains (Magnini et al 2003), annotator disagreements (Chklovski
and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE
(Dolan 1994), and ODE (Navigli 2006).
Ciaramita and Altun (2006) build a supervised HMM tagger using ?supersenses,?
essentially the 25 WordNet noun categories we have used in our first experiment in
addition to 15 verb categories similarly defined. They report a supersense precision of
67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for
Senseval-3 nouns. However, the results are not directly comparable because they do not
report the noun and verb scores separately or calculate the corresponding fine-grained
score to compare with other Senseval-3 results.
Kohomban and Lee (2007) go beyond the WordNet categories based on lexicogra-
pher files and experiment with clustering techniques to construct their semantic classes.
Their classes are based on local features from sense-labeled data and optimize feature?
class coherence rather than adhering to the WordNet hierarchy. Their supervised system
achieves an accuracy of 74.7% on Senseval-2 nouns and 73.6% on Senseval-3 nouns.
The systems mentioned so far are supervised WSD systems. Agirre and Martinez
(2004) explore the large-scale acquisition of sense-tagged examples from the Web and
train supervised, minimally supervised (requiring sense bias information from hand-
tagged corpora, similar to our system), and fully unsupervised WSD algorithms using
this corpus. They report good results on the Senseval-2 lexical sample data compared to
other unsupervised systems. Martinez, de Lacalle, and Agirre (2008) test a similar set of
systems trained using automatically acquired corpora on Senseval-3 nouns. Their mini-
mally supervised system obtains 63.9% accuracy on polysemous nouns from Senseval-3
(corresponding to 71.86% on all nouns).
7. Contributions
We have introduced a new generative probabilistic model based on the noisy channel
framework for unsupervised word sense disambiguation. The main contribution of this
model is the reduction of the word sense disambiguation problem to the estimation of
two distributions: the distribution of words used to express a given sense, and the dis-
tribution of words that appear in a given context. In this framework, context similarity
is determined by the distribution of words that can be placed in the given context. This
replaces the ad hoc contextual feature design process by a statistical language model,
allowing the advances in language modeling and the availability of large unlabeled
corpora to have a direct impact on WSD performance.
We have provided a detailed analysis of using coarse-grained semantic classes for
fine-grained WSD. The noisy channel model is a good fit for class-based WSD, where
the model decides on a coarse-grained semantic class instead of a fine-grained sense.
The chosen semantic class is then mapped to a specific sense based on the WordNet
ordering during evaluation. We show that the potential loss from using coarse-grained
classes is limited, and state-of-the-art performance is possible using only a few semantic
classes. We explore semantic classes at various levels of granularity and show that
124
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
the relationship between granularity and fine-grained accuracy is complex, thus more
work is needed to determine an ideal set of semantic classes.
In several experiments we compare the performance of our unsupervised WSD
system with the best systems from previous Senseval and SemEval workshops. We
consistently outperform any previously reported unsupervised results and achieve
comparable performance to the best supervised results.
Appendix A: Solutions for P(S|C)
To solve for P(S|C) using P(W|C) and P(W|S), we represent the first two as vectors: sj =
P(S = j|C = k) and wi = P(W = i|C = k), and the last one as a matrix: WSij = P(W =
i|S = j). Our problem becomes finding a solution to the linear equation w =WS? s.
Using the Moore?Penrose pseudoinverse, WS+, we find a solution s =WS+ ? w. This
solution minimizes the distance |WS? s? w|. There are two potential problems with
this pseudoinverse solution. First, it may violate the non-negativity and normalization
constraints of a probability distribution. Second, a maximum likelihood estimate should
minimize the cross entropy between WS? s and w, not the Euclidean distance. We
addressed the normalization problem using a constrained linear solver and the cross-
entropy problem using numerical optimization. However, our experiments showed the
difference in WSD performance to be less than 1% in each case. The pseudoinverse
solution, s =WS+ ? w, can be computed quickly and works well in practice, so this
is the solution that is used in all our experiments.
Appendix B: Estimating P(W|C)
Estimating P(W|C) for each context is expensive because the number of words that need
to be considered is large. The Web 1T data set contains 13.5 million unique words, and
WordNet defines about 150,000 lemmas. To make the computation feasible we needed
to limit the set of words for which P(W|C) needs to be estimated. We limited our set to
WordNet lemmas with the same part of speech as the target word. We further required
the word to have a non-zero count in WordNet sense frequencies. The inflection and
capitalization of each word W was automatically matched to the target word. As a
result, we estimated P(W|C) for about 10,000 words for each noun context and assumed
the other words had zero probability. The n-grams required for all the contexts were
listed, and their counts were extracted from the Web 1T data set in one pass. The P(W|C)
was estimated for all the words and contexts based on these counts. In the end, we only
used the 100 most likely words in each context for efficiency, as the difference in results
using the whole distribution was not significant. For more details on smoothing with a
large language model see Yuret (2008), although we did not see a significant difference
in WSD performance based on the smoothing method used.
Acknowledgments
This work was supported in part by
the Scientific and Technical Research
Council of Turkey (TU?BI?TAK Project
108E228). We would like to thank Peter
Turney, Rada Mihalcea, Diana McCarthy,
and the four anonymous reviewers
for their helpful comments and suggestions.
References
Agirre, E. and D. Martinez. 2004.
Unsupervised WSD based on
automatically retrieved examples:
The importance of bias. In Proceedings
of the Conference on Empirical Methods
in Natural Language Processing (EMNLP),
pages 25?32, Barcelona.
125
Computational Linguistics Volume 36, Number 1
Agirre, Eneko, Llu??s Ma`rquez, and Richard
Wicentowski, editors. 2007. Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007),
Prague.
Bahl, Lalit R., Frederick Jelinek, and Robert L.
Mercer. 1983. A maximum likelihood
approach to continuous speech
recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence,
5(2):179?190.
Banko, Michele and Eric Brill. 2001.
Scaling to very very large corpora
for natural language disambiguation.
In Proceedings of 39th Annual Meeting of
the Association for Computational Linguistics,
pages 26?33, Toulouse, France, July.
Association for Computational Linguistics.
Brants, Thorsten and Alex Franz. 2006. Web
1T 5-gram version 1. Linguistic Data
Consortium, Philadelphia. LDC2006T13.
Brill, Eric and Robert C. Moore. 2000. An
improved error model for noisy channel
spelling correction. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics, pages 286?293,
Hong Kong.
Brown, Peter F., John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Chen, S. F. and J. Goodman. 1999. An
empirical study of smoothing techniques
for language modeling. Computer
Speech and Language, 13(4):359?394.
Chklovski, Timothy and Rada Mihalcea. 2003.
Exploiting agreement and disagreement
of human annotators for word sense
disambiguation. In Proceedings of the
Conference on Recent Advances in Natural
Language Processing, pages 357?366,
Borovetz.
Ciaramita, Massimiliano and
Yasemin Altun. 2006. Broad-coverage
sense disambiguation and information
extraction with a supersense sequence
tagger. In Proceedings of the 2006
Conference on Empirical Methods in
Natural Language Processing,
pages 594?602, Sydney.
Cotton, Scott, Phil Edmonds,
Adam Kilgarriff, and Martha Palmer,
editors. 2001. SENSEVAL-2: Second
International Workshop on Evaluating Word
Sense Disambiguation Systems, Toulouse,
France.
Crestan, E., M. El-Be`ze, and C. De Loupy.
2001. Improving WSD with multi-level
view of context monitored by similarity
measure. In Proceedings of SENSEVAL-2:
Second International Workshop on Evaluating
Word Sense Disambiguation Systems,
Toulouse.
Daume III, Hal and Daniel Marcu. 2002.
A noisy-channel model for document
compression. In Proceedings of 40th
Annual Meeting of the Association for
Computational Linguistics, pages 449?456,
Philadelphia, PA.
Dolan, W. B. 1994. Word sense ambiguation:
clustering related senses. In Proceedings
of the 15th conference on Computational
Linguistics, pages 05?09, Kyoto.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 16?23,
Sapporo.
Fellbaum, Christiane, editor. 1998.Wordnet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Goodman, Joshua. 2001. A bit of progress in
language modeling. Computer Speech and
Language, 15:403?434.
Hawker, Tobias. 2007. Usyd: WSD and lexical
substitution using the Web1t corpus. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 446?453,
Prague.
Jansen, B. J., A. Spink, and A. Pfaff. 2000.
Linguistic aspects of Web queries. In
Proceedings of the ASIS Annual Meeting,
pages 169?176, Chicago, IL.
Kohomban, Upali Sathyajith and
Wee Sun Lee. 2005. Learning semantic
classes for word sense disambiguation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL?05), pages 34?41, Ann Arbor, MI.
Kohomban, Upali Sathyajith and
Wee Sun Lee. 2007. Optimizing classifier
performance in word sense disambiguation
by redefining word sense classes. In
Proceedings of the International Joint
Conference on Artificial Intelligence,
pages 1635?1640, Hyderabad.
Kucera, Henry and W. Nelson Francis. 1967.
Computational Analysis of Present-Day
American English. Brown University Press,
Providence, RI.
Magnini, B., C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2003. The role of domain
information in word sense disambiguation.
126
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
Natural Language Engineering,
8(04):359?373.
Martinez, D., O. Lopez de Lacalle, and
E. Agirre. 2008. On the use of automatically
acquired examples for all-nouns
word sense disambiguation. Journal
of Artificial Intelligence Research, 33:79?107.
Mihalcea, Rada and Phil Edmonds, editors.
2004. SENSEVAL-3: The Third International
Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona.
Navigli, Roberto. 2006. Meaningful clustering
of senses helps boost word sense
disambiguation performance. In
Proceedings of the 21st International
Conference on Computational Linguistics
and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 105?112, Sydney.
Navigli, Roberto. 2009. Word sense
disambiguation: A survey. ACM
Computing Surveys, 41(2):1?69.
Peters, W., I. Peters, and P. Vossen. 1998.
Automatic sense clustering in
EuroWordNet. In Proceedings of the
International Conference on Language
Resources and Evaluation, pages 409?416,
Granada.
Rosenfeld, Ronald. 2000. Two decades
of statistical language modeling:
Where do we go from here? Proceedings
of the IEEE, 88:1270?1278.
Shannon, Claude Elwood. 1948. A
mathematical theory of communication.
The Bell System Technical Journal,
27:379?423, 623?656.
Stevenson, Mark. 2003.Word Sense
Disambiguation: The Case for Combinations
of Knowledge Sources. CSLI, Stanford, CA.
Teh, Y. W., M. I. Jordan, M. J. Beal, and
D. M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical
Association, 101(476):1566?1581.
Yarowsky, David. 1992. Word sense
disambiguation using statistical
models of Roget?s categories trained
on large corpora. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 454?460, Nantes.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation across
diverse parameter spaces.Natural Language
Engineering, 8(4):293?310.
Yuret, Deniz. 2004. Some experiments
with a Naive Bayes WSD system. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 265?268,
Barcelona.
Yuret, Deniz. 2007. KU: Word sense
disambiguation by substitution. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 207?214, Prague.
Yuret, Deniz. 2008. Smoothing a tera-word
language model. In Proceedings of
ACL-08: HLT, Short Papers,
pages 141?144, Columbus, OH.
127

Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 51?56,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 12: Parser Evaluation using Textual Entailments
Deniz Yuret
Koc? University
?
Istanbul, Turkey
dyuret@ku.edu.tr
Ayd?n Han
Koc? University
?
Istanbul, Turkey
ahan@ku.edu.tr
Zehra Turgut
Koc? University
?
Istanbul, Turkey
zturgut@ku.edu.tr
Abstract
Parser Evaluation using Textual Entail-
ments (PETE) is a shared task in the
SemEval-2010 Evaluation Exercises on
Semantic Evaluation. The task involves
recognizing textual entailments based on
syntactic information alone. PETE intro-
duces a new parser evaluation scheme that
is formalism independent, less prone to
annotation error, and focused on semanti-
cally relevant distinctions.
1 Introduction
Parser Evaluation using Textual Entailments
(PETE) is a shared task that involves recognizing
textual entailments based on syntactic information
alone. Given two text fragments called ?text? and
?hypothesis?, textual entailment recognition is the
task of determining whether the meaning of the
hypothesis is entailed (can be inferred) from the
text. In contrast with general RTE tasks (Dagan
et al, 2009) the PETE task focuses on syntactic
entailments:
Text: The man with the hat was tired.
Hypothesis-1: The man was tired. (yes)
Hypothesis-2: The hat was tired. (no)
PETE is an evaluation scheme based on a natu-
ral human linguistic competence (i.e. the ability to
comprehend sentences and answer simple yes/no
questions about them). We believe systems should
try to model natural human linguistic competence
rather than their dubious competence in artificial
tagging tasks.
The PARSEVAL measures introduced nearly two
decades ago (Black et al, 1991) still dominate the
field of parser evaluation. These methods com-
pare phrase-structure bracketings produced by the
parser with bracketings in the annotated corpus, or
?treebank?. Parser evaluation using short textual
entailments has the following advantages com-
pared to treebank based evaluation.
Consistency: Recognizing syntactic entail-
ments is a more natural task for people than
treebank annotation. Focusing on a natural
human competence makes it practical to collect
high quality evaluation data from untrained
annotators. The PETE dataset was annotated by
untrained Amazon Mechanical Turk workers at
an insignificant cost and each annotation is based
on the unanimous agreement of at least three
workers. In contrast, of the 36306 constituent
strings that appear multiple times in the Penn
Treebank (Marcus et al, 1994), 5646 (15%) have
multiple conflicting annotations. If indicative of
the general level of inconsistency, 15% is a very
high number given that the state of the art parsers
claim f-scores above 90% (Charniak and Johnson,
2005).
Relevance: PETE automatically focuses atten-
tion on semantically relevant phenomena rather
than differences in annotation style or linguistic
convention. Whether a phrase is tagged ADJP vs
ADVP rarely affects semantic interpretation. At-
taching the wrong subject to a verb or the wrong
prepositional phrase to a noun changes the mean-
ing of the sentence. Standard treebank based eval-
uation metrics do not distinguish between seman-
tically relevant and irrelevant errors (Bonnema et
al., 1997). In PETE semantically relevant differ-
ences lead to different entailments, semantically
irrelevant differences do not.
Framework independence: Entailment recog-
nition is a formalism independent task. A com-
mon evaluation method for parsers that do not use
the Penn Treebank formalism is to automatically
convert the Penn Treebank to the appropriate for-
malism and to perform treebank based evaluation
(Nivre et al, 2007a; Hockenmaier and Steedman,
51
2007). The inevitable conversion errors compound
the already mentioned problems of treebank based
evaluation. In addition, manually designed tree-
banks do not naturally lend themselves to unsu-
pervised parser evaluation. Unlike treebank based
evaluation, PETE can compare phrase structure
parsers, dependency parsers, unsupervised parsers
and other approaches on an equal footing.
PETE was inspired by earlier work on represen-
tations of grammatical dependency, proposed for
ease of use by end users and suitable for parser
evaluation. These include the grammatical rela-
tions (GR) by (Carroll et al, 1999), the PARC rep-
resentation (King et al, 2003), and Stanford typed
dependencies (SD) (De Marneffe et al, 2006) (See
(Bos and others, 2008) for other proposals). Each
use a set of binary relations between words in
a sentence as the primary unit of representation.
They share some common motivations: usability
by people who are not (computational) linguists
and suitability for relation extraction applications.
Here is an example sentence and its SD represen-
tation (De Marneffe and Manning, 2008):
Bell, based in Los Angeles, makes and dis-
tributes electronic, computer and building prod-
ucts.
nsubj(makes-8, Bell-1)
nsubj(distributes-10, Bell-1)
partmod(Bell-1, based-3)
nn(Angeles-6, Los-5)
prep-in(based-3, Angeles-6)
conj-and(makes-8, distributes-10)
amod(products-16, electronic-11)
conj-and(electronic-11, computer-13)
amod(products-16, computer-13)
conj-and(electronic-11, building-15)
amod(products-16, building-15)
dobj(makes-8, products-16)
PETE goes one step further by translating most
of these dependencies into natural language entail-
ments.
Bell makes something.
Bell distributes something.
Someone is based in Los Angeles.
Someone makes products.
PETE has some advantages over representations
based on grammatical relations. For example SD
defines 55 relations organized in a hierarchy, and
it may be non-trivial for a non-linguist to under-
stand the difference between ccomp (clausal com-
plement with internal subject) and xcomp (clausal
complement with external subject) or between
nsubj (nominal subject) and xsubj (controlling
subject). In fact it could be argued that proposals
like SD replace one artificial annotation formal-
ism with another and no two such proposals agree
on the ideal set of binary relations to use. In con-
trast, untrained annotators have no difficulty unan-
imously agreeing on the validity of most PETE
type entailments.
However there are also significant challenges
associated with an evaluation scheme like PETE.
It is not always clear how to convert certain rela-
tions into grammatical hypothesis sentences with-
out including most of the original sentence in the
hypothesis. Including too much of the sentence in
the hypothesis would increase the chances of get-
ting the right answer with the wrong parse. Gram-
matical hypothesis sentences are especially diffi-
cult to construct when a (negative) entailment is
based on a bad parse of the sentence. Introduc-
ing dummy words like ?someone? or ?something?
alleviates part of the problem but does not help
in the case of clausal complements. In summary,
PETE makes the annotation phase more practical
and consistent but shifts the difficulty to the entail-
ment creation phase.
PETE gets closer to an extrinsic evaluation by
focusing on semantically relevant, application ori-
ented differences that can be expressed in natu-
ral language sentences. This makes the evaluation
procedure indirect: a parser developer has to write
an extension that can handle entailment questions.
However, given the simplicity of the entailments,
the complexity of such an extension is comparable
to one that extracts grammatical relations.
The balance of what is being evaluated is also
important. A treebank based evaluation scheme
may mix semantically relevant and irrelevant mis-
takes, but at least it covers every sentence at a uni-
form level of detail. In this evaluation, we focused
on sentences and relations where state of the art
parsers disagree. We hope this methodology will
uncover weaknesses that the next generation sys-
tems can focus on.
The remaining sections will go into more de-
tail about these challenges and the solutions we
have chosen to implement. Section 2 explains the
method followed to create the PETE dataset. Sec-
52
tion 3 evaluates the baseline systems the task or-
ganizers created by implementing simple entail-
ment extensions for several state of the art parsers.
Section 4 presents the participating systems, their
methods and results. Section 5 summarizes our
contribution.
2 Dataset
To generate the entailments for the PETE task we
followed the following three steps:
1. Identify syntactic dependencies that are chal-
lenging to state of the art parsers.
2. Construct short entailment sentences that
paraphrase those dependencies.
3. Identify the subset of the entailments with
high inter-annotator agreement.
2.1 Identifying Challenging Dependencies
To identify syntactic dependencies that are chal-
lenging for current state of the art parsers, we used
example sentences from the following sources:
? The ?Unbounded Dependency Corpus?
(Rimell et al, 2009). An unbounded de-
pendency construction contains a word or
phrase which appears to have been moved,
while being interpreted in the position of
the resulting ?gap?. An unlimited number
of clause boundaries may intervene between
the moved element and the gap (hence
?unbounded?).
? A list of sentences from the Penn Treebank
on which the Charniak parser (Charniak and
Johnson, 2005) performs poorly
1
.
? The Brown section of the Penn Treebank.
We tested a number of parsers (both phrase
structure and dependency) on these sentences and
identified the differences in their output. We took
sentences where at least one of the parsers gave a
different answer than the others or the gold parse.
Some of these differences reflected linguistic con-
vention rather than semantic disagreement (e.g.
representation of coordination) and some did not
represent meaningful differences that can be ex-
pressed with entailments (e.g. labeling a phrase
ADJP vs ADVP). The remaining differences typ-
ically reflected genuine semantic disagreements
1
http://www.cs.brown.edu/?ec/papers/badPars.txt.gz
that would effect downstream applications. These
were chosen to turn into entailments in the next
step.
2.2 Constructing Entailments
We tried to make the entailments as targeted as
possible by building them around two content
words that are syntactically related. When the two
content words were not sufficient to construct a
grammatical sentence we used one of the follow-
ing techniques:
? Complete the mandatory elements using the
words ?somebody? or ?something?. (e.g.
To test the subject-verb dependency in ?John
kissed Mary.? we construct the entailment
?John kissed somebody.?)
? Make a passive sentence to avoid using a spu-
rious subject. (e.g. To test the verb-object
dependency in ?John kissed Mary.? we con-
struct the entailment ?Mary was kissed.?)
? Make a copular sentence or use existen-
tial ?there? to express noun modification.
(e.g. To test the noun-modifier dependency
in ?The big red boat sank.? we construct the
entailment ?The boat was big.? or ?There was
a big boat.?)
2.3 Filtering Entailments
To identify the entailments that are clear to human
judgement we used the following procedure:
1. Each entailment was tagged by 5 untrained
annotators from the Amazon Mechanical
Turk crowdsourcing service.
2. The results from the annotators whose agree-
ment with the gold parse fell below 70% were
eliminated.
3. The entailments for which there was unani-
mous agreement of at least 3 annotators were
kept.
The instructions for the annotators were brief
and targeted people with no linguistic background:
Computers try to understand long sentences by
dividing them into a set of short facts. You will
help judge whether the computer extracted the
right facts from a given set of 25 English sen-
tences. Each of the following examples consists
of a sentence (T), and a short statement (H) de-
rived from this sentence by a computer. Please
53
read both of them carefully and choose ?Yes?
if the meaning of (H) can be inferred from the
meaning of (T). Here is an example:
(T) Any lingering suspicion that this was a trick
Al Budd had thought up was dispelled.
(H) The suspicion was dispelled. Answer: YES
(H) The suspicion was a trick. Answer: NO
You can choose the third option ?Not sure? when
the (H) statement is unrelated, unclear, ungram-
matical or confusing in any other manner.
The ?Not sure? answers were grouped with the
?No? answers during evaluation. Approximately
50% of the original entailments were retained after
the inter-annotator agreement filtering.
2.4 Dataset statistics
The final dataset contained 367 entailments which
were randomly divided into a 66 sentence devel-
opment test and a 301 sentence test set. 52% of
the entailments in the test set were positive.
Approximately half of the final entailments
were from the Unbounded Dependency Corpus,
a third were from the Brown section of the Penn
Treebank, and the remaining were from the Char-
niak sentences. Table 1 lists the most frequent
grammatical relations encountered in the entail-
ments.
GR Entailments
Direct object 42%
Nominal subject 33%
Reduced relative clause 21%
Relative clause 13%
Passive nominal subject 6%
Object of preposition 5%
Prepositional modifier 4%
Conjunct 2%
Adverbial modifier 2%
Free relative 2%
Table 1: Most frequent grammatical relations en-
countered in the entailments.
3 Baselines
In order to establish baseline results for this task,
we built an entailment decision system for CoNLL
format dependency files and tested several pub-
licly available parsers. The parsers used were the
Berkeley Parser (Petrov and Klein, 2007), Char-
niak Parser (Charniak and Johnson, 2005), Collins
Parser (Collins, 2003), Malt Parser (Nivre et al,
2007b), MSTParser (McDonald et al, 2005) and
Stanford Parser (Klein and Manning, 2003). Each
parser was trained on sections 02-21 of the WSJ
section of Penn Treebank. Outputs of phrase
structure parsers were automatically annotated
with function tags using Blaheta?s function tag-
ger (Blaheta and Charniak, 2000) and converted to
the dependency structure with LTH Constituent-
to-Dependency Conversion Tool (Johansson and
Nugues, 2007).
To decide the entailments both the test and
hypothesis sentences were parsed. All the con-
tent words in the hypothesis sentence were de-
termined by using part-of-speech tags and depen-
dency relations. After applying some heuristics
such as active-passive conversion, the extracted
dependency path between the content words was
searched in the dependency graph of the test sen-
tence. In this search process, same relation types
for the direct relations between the content word
pairs and isomorphic subgraphs in the test and hy-
pothesis sentences were required for the ?YES?
answer.
Table 2 lists the baseline results achieved. There
are significant differences in the entailment accu-
racies of systems that have comparable unlabeled
attachment scores. One potential reason for this
difference is the composition of the PETE dataset
which emphasizes challenging syntactic construc-
tions that some parsers may be better at. Another
reason is the complete indifference of treebank
based measures like UAS to the semantic signif-
icance of various dependencies and their impact
on potential applications.
System PETE UAS
Berkeley Parser 68.1% 91.2
Stanford Parser 66.1% 90.2
Malt Parser 65.5% 89.8
Charniak Parser 64.5% 93.2
Collins Parser 63.5% 91.6
MST Parser 59.8% 92.0
Table 2: Baseline systems: The second column
gives the performance on the PETE test set, the
third column gives the unlabeled attachment score
on section 23 of the Penn Treebank.
4 Systems
There were 20 systems from 7 teams participat-
ing in the PETE task. Table 3 gives the percent-
age of correct answers for each system. 12 sys-
54
System Accuracy Precision Recall F1
360-418-Cambridge 0.7243 0.7967 0.6282 0.7025
459-505-SCHWA 0.7043 0.6831 0.8013 0.7375
473-568-MARS-3 0.6678 0.6591 0.7436 0.6988
372-404-MDParser 0.6545 0.7407 0.5128 0.6061
372-509-MaltParser 0.6512 0.7429 0.5000 0.5977
473-582-MARS-5 0.6346 0.6278 0.7244 0.6726
166-415-JU-CSE-TASK12-2 0.5781 0.5714 0.7436 0.6462
166-370-JU-CSE-TASK12 0.5482 0.5820 0.4551 0.5108
390-433-Berkeley Parser Based 0.5415 0.5425 0.7372 0.6250
473-566-MARS-1 0.5282 0.5547 0.4551 0.5108
473-569-MARS-4 0.5249 0.5419 0.5385 0.5402
390-431-Brown Parser Based 0.5216 0.5349 0.5897 0.5610
473-567-MARS-2 0.5116 0.5328 0.4679 0.4983
363-450-VENSES 0.5083 0.5220 0.6090 0.5621
473-583-MARS-6 0.5050 0.5207 0.5641 0.5415
390-432-Brown Reranker Parser Based 0.5017 0.5217 0.4615 0.4898
390-435-Berkeley Parser with substates 0.5017 0.5395 0.2628 0.3534
390-434-Berkeley Parser with Self Training 0.4983 0.5248 0.3397 0.4125
390-437-Combined 0.4850 0.5050 0.3269 0.3969
390-436-Berkeley Parser with Viterbi Decoding 0.4784 0.4964 0.4359 0.4642
Table 3: Participating systems and their scores. The system identifier consists of the participant ID,
system ID, and the system name given by the participant. Accuracy gives the percentage of correct
entailments. Precision, Recall and F1 are calculated for positive entailments.
tems performed above the ?always yes? baseline
of 51.83%.
Most systems started the entailment decision
process by extracting syntactic dependencies,
grammatical relations, or predicates by parsing the
text and hypothesis sentences. Several submis-
sions, including the top two scoring systems used
the C&C Parser (Clark and Curran, 2007) which
is based on Combinatory Categorical Grammar
(CCG) formalism. Others used dependency struc-
tures produced by Malt Parser (Nivre et al,
2007b), MSTParser (McDonald et al, 2005) and
Stanford Parser (Klein and Manning, 2003).
After the parsing step, the decision for the en-
tailment was based on the comparison of relations,
predicates, or dependency paths between the text
and the hypothesis. Most systems relied on heuris-
tic methods of comparison. A notable exception is
the MARS-3 system which used an SVM-based
classifier to decide on the entailment using depen-
dency path features.
Table 4 lists the frequency of various grammati-
cal relations in the instances where the top system
made mistakes. A comparison with Table 1 shows
the direct objects and reduced relative clauses to
be the frequent causes of error.
5 Contributions
We introduced PETE, a new method for parser
evaluation using textual entailments. By basing
the entailments on dependencies that current state
GR Entailments
Direct object 51%
Reduced relative clause 36%
Nominal subject 20%
Object of preposition 7%
Passive nominal subject 7%
Table 4: Frequency of grammatical relations in en-
tailment instances that got wrong answers from the
Cambridge system.
of the art parsers disagree on, we hoped to cre-
ate a dataset that would focus attention on the
long tail of parsing problems that do not get suffi-
cient attention using common evaluation metrics.
By further restricting ourselves to differences that
can be expressed by natural language entailments,
we hoped to focus on semantically relevant deci-
sions rather than accidents of convention which
get mixed up in common evaluation metrics. We
chose to rely on untrained annotators on a natu-
ral inference task rather than trained annotators
on an artificial tagging task because we believe
(i) many subfields of computational linguistics are
struggling to make progress because of the noise
in artificially tagged data, and (ii) systems should
try to model natural human linguistic competence
rather than their dubious competence in artificial
tagging tasks. Our hope is datasets like PETE will
be used not only for evaluation but also for training
and fine-tuning of systems in the future. Further
55
work is needed to automate the entailment gener-
ation process and to balance the composition of
syntactic phenomena covered in a PETE dataset.
Acknowledgments
We would like to thank Laura Rimell, Stephan
Oepen and Anna Mac for their careful analysis and
valuable suggestions.
?
Onder Eker contributed to
the early development of the PETE task.
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, et al 1991. A Procedure for Quanti-
tatively Comparing the Syntactic Coverage of En-
glish Grammars. In Speech and natural language:
proceedings of a workshop, held at Pacific Grove,
California, February 19-22, 1991, page 306. Mor-
gan Kaufmann Pub.
D. Blaheta and E. Charniak. 2000. Assigning func-
tion tags to parsed text. In Proceedings of the 1st
North American chapter of the Association for Com-
putational Linguistics conference, page 240. Mor-
gan Kaufmann Publishers Inc.
R. Bonnema, R. Bod, and R. Scha. 1997. A DOP
model for semantic interpretation. In Proceedings
of the eighth conference on European chapter of the
Association for Computational Linguistics, pages
159?167. Association for Computational Linguis-
tics.
Johan Bos et al, editors. 2008. Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation. In connection with the 22nd In-
ternational Conference on Computational Linguis-
tics.
J. Carroll, G. Minnen, and T. Briscoe. 1999. Cor-
pus annotation for parser evaluation. In Proceedings
of the EACL workshop on Linguistically Interpreted
Corpora (LINC).
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, page 180.
Association for Computational Linguistics.
S. Clark and J.R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?552.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational linguis-
tics, 29(4):589?637.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(04).
M.C. De Marneffe and C.D. Manning, 2008. Stanford
typed dependencies manual.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
a corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Proc. of the 16th Nordic Conference on Compu-
tational Linguistics (NODALIDA).
T.H. King, R. Crouch, S. Riezler, M. Dalrymple, and
R. Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th Interna-
tional Workshop on Linguistically Interpreted Cor-
pora (LINC-03), pages 1?8.
D. Klein and C.D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423?430. Association
for Computational Linguistics.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguis-
tics, 19(2):313?330.
R. McDonald, F. Pereira, K. Ribarov, and J. Ha-
jic. 2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of
HLT/EMNLP, pages 523?530.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007a. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL, volume 7, pages 915?932.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K?ubler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95?135.
S. Petrov and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
L. Rimell, S. Clark, and M. Steedman. 2009. Un-
bounded dependency recovery for parser evaluation.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
813?821. Association for Computational Linguis-
tics.
56
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 300?306, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
AI-KU: Using Substitute Vectors and Co-Occurrence Modeling for Word
Sense Induction and Disambiguation
Osman Bas?kaya Enis Sert Volkan Cirik Deniz Yuret
Artificial Intelligence Laboratory
Koc? University, I?stanbul, Turkey
{obaskaya,esert,vcirik,dyuret}@ku.edu.tr
Abstract
Word sense induction aims to discover differ-
ent senses of a word from a corpus by us-
ing unsupervised learning approaches. Once a
sense inventory is obtained for an ambiguous
word, word sense discrimination approaches
choose the best-fitting single sense for a given
context from the induced sense inventory.
However, there may not be a clear distinction
between one sense and another, although for
a context, more than one induced sense can
be suitable. Graded word sense method al-
lows for labeling a word in more than one
sense. In contrast to the most common ap-
proach which is to apply clustering or graph
partitioning on a representation of first or sec-
ond order co-occurrences of a word, we pro-
pose a system that creates a substitute vec-
tor for each target word from the most likely
substitutes suggested by a statistical language
model. Word samples are then taken accord-
ing to probabilities of these substitutes and the
results of the co-occurrence model are clus-
tered. This approach outperforms the other
systems on graded word sense induction task
in SemEval-2013.
1 Introduction
There exists several drawbacks of representing the
word senses with a fixed-list of definitions of a man-
ually constructed lexical database. There is no guar-
antee that they reflect the exact meaning of a tar-
get word in a given context since they usually con-
tain definitions that are too general (Ve?ronis, 2004).
More so, lexical databases often include many rare
senses while missing corpus/domain-specific senses
(Pantel and Lin, 2004). The goal of Word Sense In-
duction (WSI) is to solve these problems by auto-
matically discovering the meanings of a target word
from a text, not pre-defined sense inventories. Word
Sense Discrimination (WSD) approaches determine
best-fitting sense among the meanings that are dis-
covered for an ambiguous word. However, (Erk
et al, 2009) suggested that annotators often gave
high ratings to more than one WordNet sense for the
same occurrence. They introduced a novel annota-
tion paradigm allowing that words have more than
one sense with a degree of applicability.
Unlike previous SemEval tasks in which systems
labeled a target word?s meaning with only one sense,
word sense induction task in SemEval-2013 relaxes
this by allowing a target word to have more than one
sense if applicable.
Word sense induction approaches can be catego-
rized into graph based models, bayesian, and vector-
space ones. In graph-based approaches, every con-
text word is represented as a vertex and if two con-
text words co-occur in one or more instances of a
target word, then two vertices are connected with
an edge. When the graph is obtained, one of the
graph clustering algorithm is employed. As a result,
different partitions indicate the different senses of a
target word (Ve?ronis, 2004). Agirre et al (2006) ex-
plored the use of two graph algorithms for unsuper-
vised induction and tagging of nominal word senses
based on corpora. Recently, Korkontzelos and Man-
andhar (2010) proposed a graph-based model which
achieved good results on word sense induction and
discrimination task in SemEval-2010.
300
Brody and Lapata (2009) proposed a Bayesian
approach modeling the contexts of the ambiguous
word as samples from a multinomial distribution
over senses which are in turn characterized as dis-
tributions over words.
Vector-space models, on the other hand, typically
create context vector by using first or second or-
der co-occurrences. Once context vector has been
constructed, different clustering algorithms may be
applied. However, representing the context with
first or second order co-occurrences can be difficult
since there are plenty of parameters to be consid-
ered such as the order of occurrence, context win-
dow size, statistical significance of words in the con-
text window and so on. Instead of dealing with
these, we suggest representing the context with the
most likely substitutes determined by a statistical
language model. Statistical language models based
on large corpora has been examined in (Yuret, 2007;
Hawker, 2007; Yuret and Yatbaz, 2010) for unsuper-
vised word sense disambiguation and lexical substi-
tution. Moreover, the best results in unsupervised
part-of-speech induction achieved by using substi-
tute vectors (Yatbaz et al, 2012).
In this paper, we propose a system that represents
the context of each target word by using high prob-
ability substitutes according to a statistical language
model. These substitute words and their probabili-
ties are used to create word pairs (instance id - sub-
stitute word) to feed our co-occurrence model. The
output of the co-occurrence model is clustered by k-
means algorithm. Our systems perform well among
other submitted systems in SemEval-2013.
Rest of the paper is organized as follows. Sec-
tion 2 describes the provided datasets and evalu-
ation measures of the task. Section 3 gives de-
tails of our algorithm and is divided into five con-
tiguous subsections that correspond to each step of
our system. In Section 4 we present the differ-
ences between our three systems and their perfor-
mances. Finally, Section 5 summarizes our work in
this task. The code to replicate this work is available
at http://goo.gl/jPTZQ.
2 Data and Evaluation Methodology
The test data for the graded word sense induction
task in SemEval-2013 includes 50 terms containing
20 verbs, 20 nouns and 10 adjectives. There are a
total of 4664 test instances provided. All evalua-
tion was performed on test instances only. In ad-
dition, the organizers provided sense labeled trial
data which can be used for tuning. This trial data
is a redistribution of the Graded Sense and Usage
data set provided by Katrin Erk, Diana McCarthy,
and Nicholas Gaylord (Erk et al, 2009). It consists
of 8 terms; 3 verbs, 3 nouns, and 2 adjectives all
with moderate polysemy (4-7 senses). Each term
in trial data has 50 contexts, in total 400 instances
provided. Lastly, participants can use ukWaC1, a 2-
billion word web-gathered corpus, for sense induc-
tion. Furthermore, unlike in previous WSI tasks, or-
ganizers allow participants to use additional contexts
not found in the ukWaC under the condition that they
submit systems for both using only the ukWaC and
with their augmented corpora.
The gold-standard of test data was prepared using
WordNet 3.1 by 10 annotators. Since WSI systems
report their annotations in a different sense inven-
tory than WordNet 3.1, a mapping procedure should
be used first. The organizers use the sense mapping
procedure explained in (Jurgens, 2012). This proce-
dure has adopted the supervised evaluation setting
of past SemEval WSI Tasks, but the main differ-
ence is that the former takes into account applica-
bility weights for each sense which is a necessary
for graded word sense.
Evaluation can be divided into two categories: (1)
a traditional WSD task for Unsupervised WSD and
WSI systems, (2) a clustering comparison setting
that evaluates the similarity of the sense inventories
for WSI systems. WSD evaluation is made accord-
ing to three objectives:
? Their ability to detect which senses are appli-
cable (Jaccard Index is used)
? Their ability to rank the applicable senses ac-
cording to the level of applicability (Weighted
Kendall?s ? is used)
? Their ability to quantify the level of applicabil-
ity for each sense (Weighted Normalized Dis-
counted Cumulative Gain is used)
Clustering comparison is made by using:
1Available here: http://wacky.sslmit.unibo.it
301
? Fuzzy Normalized Mutual Information: It cap-
tures the alignment of the two clusterings inde-
pendent of the cluster sizes and therefore serves
as an effective measure of the ability of an ap-
proach to accurately model rare senses.
? Fuzzy B-Cubed: It provides an item-based
evaluation that is sensitive to the cluster size
skew and effectively captures the expected per-
formance of the system on a dataset where the
cluster (i.e., sense) distribution would be equiv-
alent.
More details can be found on the task website.2
3 Algorithm
In this section, we explain our algorithm. First, we
describe data enrichment procedure then we will an-
swer how each instance?s substitute vector was con-
structed. In contrast to common practice which is
clustering the context directly, we first performed
word sampling on the substitute vectors and cre-
ated instance id - substitute word pairs as explained
in Subsection 3.3. These pairs were used in the
co-occurrence modeling step described in Subsec-
tion 3.4. Finally, we clustered these co-occurrence
modeling output with the k-means clustering algo-
rithm. It is worth noting that this pipeline is per-
formed on each target word separately.
SRILM (Stolcke, 2002) is employed on entire
ukWaC corpus for the 4-gram language model to
conduct all experiments.
3.1 Data Enrichment
Data enrichment aims to increase the number of in-
stances of target words. Our preliminary experi-
ments on the trial data showed that additional con-
texts increase the performance of our systems.
Assuming that our target word is book in noun
form. We randomly fetch 20,000 additional contexts
from ukWaC where our target word occurs with the
same part-of-speech tag. This implies that we skip
those sentences in which the word book functions as
a verb. These additional contexts are labeled with
unique numbers so that we can distinguish actual in-
stances in the test data. We follow this procedure for
2www.cs.york.ac.uk/semeval-2013/task13/
Substitute Probability
solve 0.305
complete 0.236
meet 0.096
overcome 0.026
counter 0.022
tackle 0.014
address 0.012
... ...
... ...
Table 1: The most likely substitutes for meet
every target word in the test data. In total, 1 mil-
lion additional instances were fetched from ukWac.
Hereafter we refer to this new dataset with as an ex-
panded dataset.
3.2 Substitute Vectors
Unlike other WSI methods which rely on the first or
the second order co-occurrences (Pedersen, 2010),
we represent the context of each target word instance
by finding the most likely substitutes suggested by
the 4-gram language model we built from ukWaC
corpus. The high probability substitutes reflect both
semantic and syntactic properties of the context as
seen in Table 1 for the following example:
And we need Your help to meet the chal-
lenge!
For every instance in our expanded dataset, we
use three tokens each on the left and the right side of
a target word as a context when estimating the prob-
abilities for potential lexical substitutes. This tight
window size might seem limited, however, tight con-
text windows give better scores for semantic simi-
larity, while larger context windows or second-order
context words are better for modeling general top-
ical relatedness (Sahlgren, 2006; Peirsman et al,
2008).
Fastsubs (Yuret, 2012) was used for this process
and the top 100 most likely substitutes were used for
representing each instance since the rest of the sub-
stitutes had negligible probabilities. These top 100
probabilities were normalized to add up to 1.0 giv-
ing us a final substitute vector for a particular target
word?s instance. Note that the substitute vector is a
302
Instance ID Substitute Word
meet1 complete
meet1 solve
meet1 solve
meet1 overcome
... ...
... ...
meet1 meet
meet1 complete
meet1 solve
meet1 solve
Table 2: Substitute word sampling for instance meet1
Figure 1: Co-Occcurrence Embedding Sphere for meet
function of the context only and is indifferent to the
target word.
At the end of this step, we had 1,004,466 sub-
stitute vectors. The next common step might be to
cluster these vectors either locally, which means ev-
ery target word will be clustered separately; or glob-
ally, which indicates all instances (approximately 1
million) will be clustered together. Both approaches
led us to lower scores than the presented method.
Therefore, instead of clustering substitute vectors di-
rectly, we relied on co-occurrence modeling.
3.3 Substitute Word Sampling
Before running S-CODE (Maron et al, 2010) to
model co-occurrence statistics, we needed to per-
form the substitute word sampling. For each target
word?s instance, we sample 100 substitutes from its
substitute vector. Assuming that our target word is
meet and its substitute vector is the one shown in
Instance ID Substitute Word
meet1 complete
meet1 solve
... ...
meet2 hold
meet2 visit
... ...
meet20100 assemble
... ...
meet20100 gather
Table 3: Substitute sampling for a target word meet.
Instance ID - Substitute word pairs
Table 1. We choose 100 substitutes from this in-
stance?s substitute vector by using individual proba-
bilities of substitutes. As seen in Table 2, those sub-
stitutes which have high probabilities dominate the
right column. Recall that Table 2 illustrates only one
instance (subscript denotes the instance number) for
the target word meet which has 20,000 and 100 in-
stances from the context enrichment procedure and
the test, respectively. We followed the same proce-
dure for every instance of each target word. Table 3
depicts instance id - substitute word pairs for the
target word meet rather than for only one instance
shown in Table 2.
3.4 Co-Occurrence Modeling
After sampling, we had approximately 20,000 in-
stance id - substitute word pairs. These pairs were
used to feed S-CODE. The premise is that words
with similar meanings will occur in similar contexts
(Harris, 1954), and at the end this procedure enables
us to put together words with similar meanings as
well as making the clustering procedure more accu-
rate. If two different instances have similar substi-
tute word pairs (i.e, similar contexts) then these two
word pairs attract each other and they will be located
closely on the unit sphere, otherwise they will repel
and eventually be far away from each other (see Fig-
ure 1).
3.5 Clustering
We used k-means clustering on S-CODE sphere.
Note that the procedures explained in the fore-
gone subsections were repeated for each target
303
System JI WKT WNDCG
A
ll
In
st
an
ce
s
ai-ku 0.759 0.804 0.432
ai-ku(a1000) 0.759 0.794 0.612
ai-ku(r5-a1000) 0.760 0.800 0.541
MFS 0.381 0.655 0.337
All-Senses 0.757 0.745 0.660
All-Senses-freq-ranked 0.757 0.789 0.671
All-Senses-avg-ranked 0.757 0.806 0.706
Random-3 0.776 0.784 0.306
Random-n 0.795 0.747 0.301
Table 4: Supervised results on the trial set using median
gold-standard (JI: Jaccard Index FScore, WKT: Weighted
Kendall?s Tau FScore, WNDCG: Weighted Normalized
Discounted Cumulative Gain FScore)
word. More precisely, the substitute sampling, co-
occurrence modeling and clustering were performed
one by one for each target word.
We picked 22 as k value since the test set con-
tained words with 3 to 22 senses. After all word
pairs were labeled, we counted all class labels for
each instance in the test set. For example, if meet1?s
50 word pairs are labeled with c1 and 30 word pairs
are labeled with c2 and finally 20 word pairs are la-
beled with c3, then this particular instance would
have 50% sense1, 30% sense2 and 20% sense3.
4 Evaluation Results
In this section, we will discuss evaluation scores and
the characteristics of the test and the trial data.
All three AI-KU systems followed the same pro-
cedures described in Section 3. After clustering,
some basic post-processing operations were per-
formed for ai-ku(a1000) and ai-ku(r5-a1000). For
ai-ku(a1000), we added 1000 to all sense labels
which were obtained from the clustering procedure;
for ai-ku(r5-a1000), those sense labels occurred less
than 5 times in clustering were removed since we
considered them to be unreliable labels, afterwards
we added 1000 for all remaining sense labels.
Supervised Metrics: Table 5 shows the perfor-
mance of our systems on the test data using all
instances (verbs, nouns, adjectives) for all super-
vised measures and in comparison with the sys-
tems that performed best and worst, most frequent
sense (MFS), all senses equally weighted, all senses
average weighted, random-3, and random-n base-
System JI WKT WNDCG
A
ll
In
st
an
ce
s
ai-ku 0.197 0.620 0.387
ai-ku(a1000) 0.197 0.606 0.215
ai-ku(r5-a1000) 0.244 0.642 0.332
Submitted-Best 0.244 0.642 0.387
All-Best 0.552 0.787 0.499
All-Worst 0.149 0.465 0.215
MFS 0.552 0.560 0.412
All-Senses-eq-weighted 0.149 0.787 0.436
All-Senses-avg-ranked 0.187 0.613 0.499
Random-3 0.244 0.633 0.287
Random-n 0.290 0.638 0.286
Table 5: Supervised results on the test set. (Submitted-
Best indicates the best scores among all submitted sys-
tem. All-Best indicates the best scores among all sub-
mitted systems and baselines. JI: Jaccard Index FS-
core, WKT: Weighted Kendall?s Tau FScore, WNDCG:
Weighted Normalized Discounted Cumulative Gain FS-
core)
Trial Data Test Data
Number of Sense 4.97 1.19
Sense Perplexity 5.79 3.78
Table 6: Average number of senses and average sense
perplexity for trial and test data
lines. Bold numbers indicate that ai-ku achieved
best scores among all submitted systems. Our sys-
tems performed generally well for all three super-
vised measures and slightly better for all submit-
ted systems. On the other hand, baselines achieved
better scores than all participants. More precisely,
on sense detection objective, MFS baseline obtained
0.552 which is the top score, while the best submit-
ted system could reach only 0.244. Why is it the case
that MFS had one of the worst sense detection score
on trial data (see Table 4), but best on test data? Un-
like the trial data, test data largely consists of only
one sense instances, MFS usually gives correct an-
swer. Table 6 illustrates the characteristics of the
test and trial data. Instances annotated with multiple
sense had a very small fraction in the test data. In
fact, 517 instances in the test set were annotated with
two senses (11%) and only 25 were annotated with
three senses (0.5%). However, trial data provided
by the organizers had almost 5 senses per instance
on the average. A similar results can be observed
in All-Senses baselines. On sense ranking objec-
304
System FScore FNMI FB-Cubed
A
ll
Si
ng
le
-s
en
se
In
st
an
ce
s
ai-ku 0.641 0.045 0.351
ai-ku(a1000) 0.601 0.023 0.288
ai-ku(r5-a1000) 0.628 0.026 0.421
Submitted-Best 0.641 0.045 0.441
All-Best 0.641 0.048 0.570
All-Worst 0.477 0.006 0.180
MFS 0.578 - -
SemCor-MFS 0.477 - -
One Sense 0.569 0.0 0.570
Random-3 0.555 0.010 0.359
Random-n 0.533 0.006 0.223
Table 7: Supervised and unsupervised results on the test
set using instances which have only one sense. Bold num-
bers indicate that ai-ku achieved the best submitted sys-
tem scores. (FScore: Supervised FScore, FNMI: Fuzzy
Normalized Mutual Information, FB-Cubed: Fuzzy B-
Cubed FScore)
tives, All-Sense-eq-weighted outperformed all other
systems. The reason is the same as the above. This
baseline ranks all senses equally and since most in-
stances had been annotated only one sense, the other
wrong senses were tied and placed at the second po-
sition in ranking. As a result, this baseline achieved
the highest score. Finally, for quantifying the level
of applicability for each sense, Weighted NDCG was
employed. ai-ku outperformed other submitted sys-
tems, but top score was achieved by all-sense-avg-
weighted baseline. Addition to these results, orga-
nizers provided scores for instances which have only
one sense. This setting contains 89% of the test data.
Table 7 shows supervised and unsupervised scores
for all single-sense instances. Our base system, ai-
ku, outperformed all other system and all baselines
for FScore. Moreover, it also achieved the second
best score (0.045) for Fuzzy NMI. Only one base-
line (one sense per instance) obtained slightly better
score (0.048) for this metric. For Fuzzy B-Cubed,
ai-ku(r5-a1000) obtained 0.421 which is the third
best score.
Clustering Comparison: This evaluation setting
aims to measure the similarity of the induced sense
inventories for WSI systems. Unlike supervised
metrics, it avoids potential loss of sense information
since this setting does not require any sense map-
ping procedure to convert induced senses to a Word-
System Fuzzy NMI Fuzzy B-Cubed
A
ll
In
st
an
ce
s
ai-ku 0.065 0.390
ai-ku(a1000) 0.035 0.320
ai-ku(r5-a1000) 0.039 0.451
Submitted-Best 0.065 0.483
All-Best 0.065 0.623
All-Worst 0.016 0.201
Random-2 0.028 0.474
Random-3 0.018 0.382
Random-n 0.016 0.245
Table 8: Scores on clustering measures (Fuzzy NMI:
Fuzzy Normalized Mutual Information, Fuzzy B-Cubed:
Fuzzy B-Cubed FScore)
All instances
ai-ku 7.72
ai-ku(a1000) 7.72
ai-ku(r5-a1000) 3.11
Table 9: Average number of senses for each ai-ku systems
on test data
Net sense. ai-ku performed best for Fuzzy NMI
among other systems included baselines. For Fuzzy
B-Cubed, ai-ku(r5a1000) outperformed random-3
and random-n baselines. Table 8 depicts the per-
formance of our systems, best and worst systems as
well as the random baselines.
The best scores for the graded word sense in-
duction task in SemEval-2013 are mostly achieved
by baselines in supervised setting. Major problem
is that there is huge sense differences between test
and trial data regarding to number of sense distribu-
tion. Participants that used trial data as for param-
eter tuning and picking the best algorithm achieved
lower scores than baselines since test data does not
show properties of trial data. Consequently, ai-ku
systems produce significantly more senses than the
gold-standard (see Table 9), and this mainly deterio-
rates our performance.
5 Conclusion
In this paper, we presented substitute vector repre-
sentation and co-occurrence modeling on WSI task.
Clustering substitute vectors directly gives lower
scores. Thus, taking samples from each target?s sub-
stitute vector, we obtained instance id - substitute
word pairs. These pairs were used by S-CODE. Fi-
305
nally we run k-means on the S-CODE. Although our
systems were highly ranked among the other submit-
ted systems, no system showed better performance
than the top baselines for all metrics. One explana-
tion is that trial data does not reflect the characteris-
tics of test data according to their number of sense
distributions. Systems used trial data biased to re-
turn more than one sense for each instance since av-
erage number of sense is almost five in trial data. In
addition, baselines (except random ones) know true
sense distribution in the test data beforehand which
make them harder to beat.
References
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 585-593.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 103-111, Athens, Greece.
Katrin Erk, Diana McCarthy, Nicholas Gaylord. 2009.
Investigations on Word Senses and Word Usages, In
Proceedings of ACL-09 Singapore.
Zellig S. Harris. 2012. Distributional structure. Word,
Vol. 10, pages 146-162.
Tobias Hawker. 2007. USYD: WSD and lexical substi-
tution using the Web 1T corpus In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 207214, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
UoY: Graphs of Unambiguous Vertices for Word
Sense Induction and Disambiguation. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation. Uppsala, Sweden.
David Jurgens. 2012. An Evaluation of Graded Sense
Disambiguation using Word Sense Induction. In Se-
mEval ?12 Proceedings of the First Joint Conference
on Lexical and Computational Semantics. pages 189-
198.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2012. Sphere embedding: An application to part-of-
speech induction. In J. Lafferty, C. K. I. Williams, J.
Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, In
Advances in Neural Information Processing Systems
23, pages 1567-1575.
Patrick Pantel and Dekang Lin. 2002. Discovering Word
Senses from Text. In Proceedings of the 8th ACM
SIGKDD Conference, pages 613-619, New York, NY,
USA. ACM.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters Ap-
plied to the Sense Induction Task of SemEval-2. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation. pages 363-366, Uppsala, Sweden.
Yves Peirsman, Kris Heylen and Dirk Geeraerts. 2008.
Size Matters. Tight and Loose Context Definitions in
English Word Space Models. In Proceedings of the
ESSLLI Workshop on Distributional Lexical Seman-
tics, Hamburg, Germany.
Magnus Sahlgren. 2002. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, De-
partment of Linguistics, Stockholm University.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings International
Conference on Spoken Language Processing, pages
257286.
Jean Ve?ronis. 2004. HyperLex: Lexical Cartography for
Information Retrieval. Computer Speech & Language,
18(3):223-252.
Mehmet Ali Yatbaz, Enis Sert and Deniz Yuret. 2012.
Learning Syntactic Categories Using Paradigmatic
Representations of Word Context. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL 2012,
July 12-14, 2012, Jeju Island, Korea.
Deniz Yuret. 2012. FASTSUBS: An Efficient Admis-
sible Algorithm for Finding the Most Likely Lexical
Substitutes Using a Statistical Language Model. Com-
puting Research Repository (CoRR).
Deniz Yuret. 2007. KU: Word sense disambiguation by
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 207214, Prague, Czech Republic, June.
Association for Computational Linguistics.
Deniz Yuret and Mehmet Ali Yatbaz. 2010. The noisy
channel model for unsupervised word sense disam-
biguation. Computational Linguistics, Volume 36 Is-
sue 1, March 2010, pages 111-127.
306
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 223?227
Manchester, August 2008
Discriminative vs. Generative Approaches in Semantic Role Labeling
Deniz Yuret
Koc? University
dyuret@ku.edu.tr
Mehmet Ali Yatbaz
Koc? University
myatbaz@ku.edu.tr
Ahmet Engin Ural
Koc? University
aural@ku.edu.tr
Abstract
This paper describes the two algorithms
we developed for the CoNLL 2008 Shared
Task ?Joint learning of syntactic and se-
mantic dependencies?. Both algorithms
start parsing the sentence using the same
syntactic parser. The first algorithm
uses machine learning methods to identify
the semantic dependencies in four stages:
identification and labeling of predicates,
identification and labeling of arguments.
The second algorithm uses a generative
probabilistic model, choosing the seman-
tic dependencies that maximize the proba-
bility with respect to the model. A hybrid
algorithm combining the best stages of
the two algorithms attains 86.62% labeled
syntactic attachment accuracy, 73.24% la-
beled semantic dependency F1 and 79.93%
labeled macro F1 score for the combined
WSJ and Brown test sets1.
1 Introduction
In this paper we describe the system we developed
for the CoNLL 2008 Shared Task (Surdeanu et al,
2008). Section 2 describes our approach for iden-
tifying syntactic dependencies. For semantic role
labeling (SRL), we pursued two independent ap-
proaches. Section 3 describes our first approach,
where we treated predicate identification and la-
beling, and argument identification and labeling as
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1These numbers are slightly higher than the official results
due to a small bug in our submission.
four separate machine learning problems. The fi-
nal program consists of four stages, each stage tak-
ing the answers from the previous stage as given
and performing its own identification or labeling
task based on a model generated from the train-
ing set. Section 4 describes our second approach
where we used a generative model based on the
joint distribution of the predicate, the arguments,
their labels and the syntactic dependencies con-
necting them. Section 5 summarizes our results
and suggests possible improvements.
2 Syntactic dependencies
We used a non-projective dependency parser based
on spanning tree algorithms. The parameters were
determined based on the experimental results of
the English task in (McDonald et al, 2005), i.e. we
used projective parsing and a first order feature set
during training. Due to the new representation of
hyphenated words in both training and testing data
of our shared task and the absence of the gold part
of speech (GPOS) column in the test data, the for-
mat of the CoNLL08 shared task is slightly differ-
ent from the format of the CoNLL05 shared task,
which is supported by the McDonald?s parser. We
reformatted the data accordingly. The resulting la-
beled attachment score on the test set is 87.39% for
WSJ and 80.46% for Brown.
3 The 4-stage discriminative approach
Our first approach to SRL consists of four distinct
stages: (1) predicate identification, (2) predicate
labeling, (3) argument identification, and (4) argu-
ment labeling.
A discriminative machine learning algorithm is
trained for each stage using the gold input and out-
put values from the training set. The following sec-
223
tions describe the machine learning algorithm, the
nature of its input/output, and the feature selection
process for each stage. The performance of each
stage is compared to a most frequent class base-
line and analyzed separately for the two test sets
and for nouns and verbs. In addition we look at the
performance given the input from the gold data vs.
the input from the previous stage.
3.1 Predicate identification
The task of this stage is to determine whether a
given word is a nominal or a verb predicate using
the dependency-parsed input. As potential predi-
cates we only consider words that appear as a pred-
icate in the training data or have a corresponding
PropBank or NomBank XML file. The method
constructs feature vectors for each occurrence of
a target word in the training and test data. It as-
signs class labels to the target words in the training
data depending on whether a target word is a pred-
icate or not, and finally classifies the test data. We
experimented with combinations of the following
features for each word in a 2k + 1 word window
around the target: (1) POS(W): the part of speech
of the word, (2) DEP(W, HEAD(W)): the syntac-
tic dependency of the word, (3) LEMMA(W): the
lemma of the word, (4) POS(HEAD(W)): the part
of speech of the syntactic head.
We empirically selected the combination that
gives the highest accuracy in terms of the precision
and recall scores on the development data. The
method achieved its highest score when we used
features 1-3 for the target word and features 1-2 for
the neighbors in a [-3 +3] word window. TiMBL
(Daelemans et al, 2004) was used as the learning
algorithm.
Table 1 (4-stage, All1) shows the results of our
learning method on the WSJ and Brown test data.
The noun and verb results are given separately
(Verb1, Noun1). To distinguish the mistakes com-
ing from parsing we also give the results of our
method after the gold parse (4-stage-gold). Our re-
sults are significantly above the most frequent class
baseline which gives 72.3% on WSJ and 65.3% on
Brown.
3.2 Predicate labeling
The task of the second stage is deciding the correct
frame for a word given that the word is a predicate.
The input of the stage is 11-column data, where the
columns contain part of speech, lemma and syn-
tactic dependency for each word. The first stage?s
decision for the frame is indicated by a string in
the predicate column. The output of the stage is
simply the replacement of that string with the cho-
sen frame of the word. The chosen frame of the
word may be word.X, where X is a valid number
in PropBank or NomBank.
The statistics of the training data show that by
picking the most frequent frame, the system can
pick the correct frame in a large percent of the
cases. Thus we decided to use the most frequent
frame baseline for this stage. If the word is never
seen in the training, first frame of the word is
picked as default.
In the test phase, the results are as the follow-
ing; in the Brown data, assuming that the stage 1
is gold, the score is 80.8%, noting that 11% of the
predicates are not seen in the training phase. In
WSJ, the score based on gold input is 88.3%, and
only 5% of the predicates are not seen in the train-
ing phase. Table 1 gives the full results for Stage 2
(4-stage, Verb2, Noun2, All2).
3.3 Argument identification
The input data at this stage contains the syntac-
tic dependencies, predicates and their frames. We
look at the whole sentence for each predicate and
decide whether each word should be an argument
of that predicate or not. We mark the words we
choose as arguments indicating which predicate
they belong to and leave the labeling of the ar-
gument type to the next stage. Thus, for each
predicate-word pair we have a yes/no decision to
make.
As input to the learning algorithm we experi-
mented with representations of the syntactic de-
pendency chain between the predicate and the
argument at various levels of granularity. We
identified the syntactic dependency chain between
the predicate and each potential argument using
breadth-first-search on the dependency tree. We
tried to represent the chain using various subsets
of the following elements: the argument lemma
and part-of-speech, the predicate frame and part-
of-speech, the parts-of-speech and syntactic de-
pendencies of the intermediate words linking the
argument to the predicate.
The syntactic dependencies leading from the ar-
gument to the predicate can be in the head-modifier
or the modifier-head direction. We marked the di-
rection associated with each dependency relation
in the chain description. We also experimented
224
with using fine-grained and coarse-grained parts of
speech. The coarse-grained part of speech consists
of the first two characters of the Penn Treebank
part of speech given in the training set.
We used a simple learning algorithm: choose
the answer that is correct for the majority of the
instances with the same chain description from
the training set. Not having enough detail in the
chain description leaves crucial information out
that would help with the decision process, whereas
having too much detail results in bad classifica-
tions due to sparse data. In the end, neither the ar-
gument lemma, nor the predicate frame improved
the performance. The best results were achieved
with a chain description including the coarse parts
of speech and syntactic dependencies of each word
leading from the argument to the predicate. The
results are summarized in Table 1 (4-stage, Verb3,
Noun3, All3).
3.4 Argument labeling
The task of this stage is choosing the correct argu-
ment tag for a modifier given that it is modifying
a particular predicate. Input data format has ad-
ditional columns indicating which words are argu-
ments for which predicates. There are 54 possible
values for a labeled argument. As a baseline we
take the most frequent argument label in the train-
ing data (All1) which gives 37.8% on the WSJ test
set and 33.8% on the Brown test set.
The features to determine the correct label of an
argument are either lexical or syntactic. In a few
cases, they are combined. The following list gives
the set we have used. Link is the type of the syntac-
tic dependency. Direction is left or right, depend-
ing the location of the head and the modifier in the
sentence. LastLink is the type of the dependency
at the end of the dependency chain and firstLink
is type of the dependency at the beginning of the
dependency chain.
Feature1 : modifierStem + headStem
Feature2 : modifierStem + coarsePosModifier +
headStem + coarsePosHead + direction
Feature3 : coarsePosModifier + headPos +
firstLink + lastLink + direction
Feature4: modifierStem + coarsePosModifier
The training phase includes building simple his-
tograms based on four features. Feature1 and Fea-
ture2 are sparser than the other two features and
are better features as they include lexical informa-
tion. Last two features are less sparse, covering
most of the development data, i.e. their histograms
give non-zero values in the development phase. In
order to match all the instances in the development
and use the semantic information, a cascade of the
features is implemented similar to the one done by
Gildea and Jurafsky(2002), although no weighting
and a kind of back-off smoothing is used. First,
a match is searched in the histogram of the first
feature, if not found it is searched in the following
histogram. After a match, the most frequent argu-
ment with that match is returned. Table 1 gives the
performance (4-stage, Verb4, Noun4, All4).
4 The generative approach
One problem with the four-stage approach is that
the later stages provide no feedback to the earlier
ones. Thus, a frame chosen because of its high
prior probability will not get corrected when we
fail to find appropriate arguments for it. A gen-
erative model, on the other hand, does not suffer
from this problem. The probability of the whole
assignment, including predicates, arguments, and
their labels, is evaluated together and the highest
probability combination is chosen.
4.1 The generative model
Figure 1: The graphical model depicting the con-
ditional independence assumptions.
Our generative model specifies the distribution
of the following random variables: P is the lemma
(stem+pos) of a candidate predicate. F is the
frame chosen for the predicate (could be null). A
i
is the argument label of word i with respect to a
given predicate (could be null). W
i
is the lemma
(stem+pos) of word i. L
i
is the syntactic depen-
dency chain leading from word i to the given pred-
icate (similar to Section 3.3).
We consider each word in the sentence as a can-
didate predicate and use the joint distribution of the
above variables to find the maximum probability F
225
WSJ Verb1 Verb2 Verb3 Verb4 Noun1 Noun2 Noun3 Noun4 All1 All2 All3 All4
4-stage 97.1 85.5 85.7 71.7 84.6 78.4 61.1 49.4 90.6 81.8 76.6 63.5
generative 96.1 88.4 83.4 74.0 82.8 79.5 69.8 63.2 89.0 83.6 77.4 69.2
4-stage-gold 97.4 88.3 95.2 82.7 85.2 92.7 70.5 81.9 91.1 90.5 86.0 82.4
generative-gold 96.3 92.6 91.1 88.0 83.4 95.5 80.7 86.9 89.4 94.0 86.7 87.5
hybrid 97.1 89.3 85.7 74.7 84.6 80.9 70.9 64.0 90.6 84.9 79.5 70.2
Brown Verb1 Verb2 Verb3 Verb4 Noun1 Noun2 Noun3 Noun4 All1 All2 All3 All4
4-stage 93.0 74.5 78.9 59.0 74.4 58.6 52.3 38.8 86.0 68.6 72.8 54.3
generative 91.4 71.7 76.1 60.0 70.8 59.3 54.0 45.3 83.1 66.6 69.6 55.7
4-stage-gold 93.0 80.8 93.7 73.2 75.7 80.3 70.1 70.5 86.5 80.8 88.2 72.4
generative-gold 91.6 80.6 85.8 78.05 71.2 85.9 70.5 75.1 83.5 82.6 81.8 77.1
hybrid 93.0 73.3 78.9 60.4 74.4 62.9 57.6 47.5 86.0 69.3 73.4 57.0
Table 1: The F1 scores for different datasets, models, stages, and predicate parts of speech. The ?Verb?
in the column heading indicates verbal predicates, ?Noun? indicates nominal predicates, ?All? indicates
all predicates. The numbers 1-4 in column headings indicate the 4 stages: (1) predicate identification, (2)
predicate labeling, (3) argument identification, (4) argument labeling. The gold results assume perfect
output from the previous stages. The highest number in each column is marked with boldface.
and A
i
labels given P , W
i
, and L
i
. The graphical
model in Figure 1 specifies the conditional inde-
pendence assumptions we make. Equivalently, we
take the following to be proportional to the joint
probability of a particular assignment:
Pr(F |P )
?
i
Pr(A
i
|F ) Pr(W
i
|FA
i
) Pr(L
i
|FA
i
)
4.2 Parameter estimation
To estimate the parameters of the generative model
we used the following methodology:
For Pr(F |P ) we use the maximum likelihood
estimate from the training data. As a consequence,
frames that were never observed in the training
data have zero probability. One exception is lem-
mas which have not been observed in the training
data, for which each frame is considered equally
likely.
For Pr(A
i
|F ) we also use the maximum like-
lihood estimate and normalize it using sentence
length. For a given argument label we find the
expected number of words in a sentence with that
label for frame F . We divide this expected num-
ber with the length of the given sentence to find
Pr(A
i
|F ) for a single word. Any leftover prob-
ability is given to the null label. If the sentence
length is shorter than the expected number of ar-
guments, all probabilities are scaled down propor-
tionally.
For the remaining two terms Pr(L
i
|F,A
i
) and
Pr(W
i
|F,A
i
) using the maximum likelihood esti-
mate is not effective because of data sparseness.
The arguments in the million word training data
contain about 16,000 unique words and 25,000
unique dependency chains. To handle the sparse-
ness problem we smoothed these two estimates us-
ing the part-of-speech argument distribution, i.e.
Pr(L
i
|POS, A
i
) and Pr(W
i
|POS, A
i
), where POS
represents the coarse part of speech of the predi-
cate.
5 Results and Analysis
Table 1 gives the F1 scores for the two models
(4-stage and generative), presented separately for
noun and verb predicates and the four stages of
predicate identification/labeling, argument identi-
fication/labeling. In order to isolate the perfor-
mance of each stage we also give their scores with
gold input. The rest of this section analyzes these
results and suggests possible improvements.
A hybrid algorithm: A comparison of the two
algorithms show that the 4-stage approach is su-
perior in predicate and verbal-argument identifica-
tion and the generative algorithm is superior in the
labeling of predicates and arguments and nominal-
argument identification. This suggests a hybrid al-
gorithm where we restrict the generative model to
take the answers for the better stages from the 4-
stage algorithm (Noun1, Verb1, Verb3) as given.
Tables 1 and 2 present the results for the hybrid
algorithm compared to the 4-stage and generative
models.
Parsing performance: In order to see the effect
of syntactic parsing performance, we ran the hy-
brid algorithm starting with the gold parse. The
labeled semantic score went up to 78.84 for WSJ
and 67.20 for Brown, showing that better parsing
226
Data/algorithm Unlabeled Labeled
WSJ 4-stage 81.15 69.44
WSJ generative 81.01 73.66
WSJ hybrid 82.94 74.74
Brown 4-stage 76.91 58.76
Brown generative 73.76 59.05
Brown hybrid 77.22 60.80
Table 2: Semantic scores for the 4-stage, genera-
tive, and hybrid algorithms
can add about 4-6% to the overall performance.
Syntactic vs lexical features: Our algorithms
use two broad classes of features: information
from the dependency parse provides syntactic ev-
idence, and the word pairs themselves provide se-
mantic evidence for a possible relation. To iden-
tify their relative contributions, we experimented
with two modifications of the generative algo-
rithm: gen-l does not use the Pr(W
i
|FA
i
) term
and gen-w does not use the Pr(L
i
|FA
i
) term. gen-
l, using only syntactic information and the pred-
icate, gets a labeled semantic score of 70.97 for
WSJ and 58.83 for Brown, a relatively small de-
crease. In contrast gen-w, using only lexical infor-
mation gets 43.06 for WSJ and 33.17 for Brown
causing almost a 40% decrease in performance.
On the other hand, we find that the lexical fea-
tures are essential for certain tasks. In labeling the
arguments of nominal predicates, finding an exact
match for the lexical pair guarantees a 90% accu-
racy. If there is no exact match, the 4-stage algo-
rithm falls back on a syntactic match, which only
gives a 75% accuracy.
Future work: The hybrid algorithm shows the
strengths and weaknesses of our two approaches.
The generative algorithm allows feedback from the
later stages to the earlier stages and the 4-stage ma-
chine learning approach allows the use of better
features. One way to improve the system could be
by adding feedback to the 4-stage algorithm (later
stages can veto input coming from previous ones),
or adding more features to the generative model
(e.g. information about neighbor words when pre-
dicting F ). More importantly, there is no feedback
between the syntactic parser and the semantic role
labeling in our systems. Treating both problems
under the same framework may lead to better re-
sults.
Another property of both models is the indepen-
dence of the argument label assignments from each
other. Even though we try to control the number of
arguments of a particular type by adjusting the pa-
rameters, there are cases when we end up with no
assignments for a mandatory argument or multiple
assignments where only one is allowed. A more
strict enforcement of valence constraints needs to
be studied.
The use of smoothing in the generative model
was critical, it added about 20% to our final F1
score. This raises the question of finding more
effective smoothing techniques. In particular, the
jump from specific frames to coarse parts of speech
is probably not optimal. There may be interme-
diate groups of noun and verb predicates which
share similar semantic or syntactic argument dis-
tributions. Identifying and using such groups will
be considered in future work.
References
Daelemans, W., J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg memory-
Based Learner. Tilburg University.
Gildea, D. and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245 288.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line Large-Margin Training of Dependency Parsers.
Ann Arbor, 100.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
227
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 282?289,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
L1 Regularized Regression for Reranking and System Combination in
Machine Translation
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Deniz Yuret
Koc? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
We use L1 regularized transductive regres-
sion to learn mappings between source
and target features of the training sets
derived for each test sentence and use
these mappings to rerank translation out-
puts. We compare the effectiveness of L1
regularization techniques for regression to
learn mappings between features given in
a sparse feature matrix. The results show
the effectiveness of using L1 regulariza-
tion versus L2 used in ridge regression.
We show that regression mapping is ef-
fective in reranking translation outputs and
in selecting the best system combinations
with encouraging results on different lan-
guage pairs.
1 Introduction
Regression can be used to find mappings be-
tween the source and target feature sets derived
from given parallel corpora. Transduction learn-
ing uses a subset of the training examples that
are closely related to the test set without using
the model induced by the full training set. In
the context of SMT, we select a few training in-
stances for each test instance to guide the transla-
tion process. This also gives us a computational
advantage when considering the high dimension-
ality of the problem. The goal in transductive
regression based machine translation (TRegMT)
is both reducing the computational burden of the
regression approach by reducing the dimension-
ality of the training set and the feature set and
also improving the translation quality by using
transduction. Transductive regression is shown to
achieve higher accuracy than L2 regularized ridge
regression on some machine learning benchmark
datasets (Chapelle et al, 1999).
In an idealized feature mapping matrix where
features are word sequences, we would like to ob-
serve few target features for each source feature
derived from a source sentence. In this setting, we
can think of feature mappings being close to per-
mutation matrices with one nonzero item for each
column. L1 regularization helps us achieve solu-
tions close to the permutation matrices by increas-
ing sparsity.
We show that L1 regularized regression map-
ping is effective in reranking translation outputs
and present encouraging results on different lan-
guage pairs in the translation task of WMT10. In
the system combination task, different translation
outputs of different translation systems are com-
bined to find a better translation. We model system
combination task as a reranking problem among
the competing translation models and present en-
couraging results with the TRegMT system.
Related Work: Regression techniques can
be used to model the relationship between
strings (Cortes et al, 2007). Wang et al (2007)
applies a string-to-string mapping approach
to machine translation by using ordinary least
squares regression and n-gram string kernels to
a small dataset. Later they use L2 regularized
least squares regression (Wang and Shawe-Taylor,
2008). Although the translation quality they
achieve is not better than Moses (Koehn et al,
2007), which is accepted to be the state-of-the-art,
they show the feasibility of the approach. Ser-
rano et al (2009) use kernel regression to find
translation mappings from source to target feature
vectors and experiment with translating hotel
front desk requests. Ueffing (2007) approaches
the transductive learning problem for SMT by
bootstrapping the training using the translations
produced by the SMT system that have a scoring
performance above some threshold as estimated
by the SMT system itself.
282
Outline: Section 2 gives an overview of regres-
sion based machine translation, which is used to
find the mappings between the source and target
features of the training set. In section 3 we present
L1 regularized transductive regression for align-
ment learning. Section 4 presents our experiments,
instance selection techniques, and results on the
translation task for WMT10. In section 5, we
present the results on the system combination task
using reranking. The last section concludes.
2 An Overview of Regression Based
Machine Translation
Let X and Y correspond to the token sets used to
represent source and target strings, then a train-
ing sample of m inputs can be represented as
(x1, y1), . . . , (xm, ym) ? X
? ? Y ?, where (xi, yi)
corresponds to a pair of source and target language
token sequences. Our goal is to find a mapping
f : X? ? Y ? that can convert a given set of
source tokens to a set of target tokens that share
the same meaning in the target language.
X? Y ?-
? R ?
-FX FY
g
?X ?Y
6
??1Y
f
h
Figure 1: String-to-string mapping.
Figure 1 depicts the mappings between different
representations. ?X : X? ? FX = RNX and
?Y : Y ? ? FY = RNY map each string sequence
to a point in high dimensional real number space
where dim(FX) = NX and dim(FY ) = NY .
Let MX ? RNX?m and MY ? RNY ?m such
that MX = [?X(x1), . . . ,?X(xm)] and MY =
[?Y (y1), . . . ,?Y (ym)]. The ridge regression so-
lution using L2 regularization is found as:
HL2 = arg min
H?RNY ?NX
?MY ?HMX ?2F +??H?
2
F .(1)
Proposition 1 Solution to the cost function given
in Equation 1 is found by the following identities:
H = MY MTX(MXM
T
X + ?INX )
?1 (primal)
H = MY (KX + ?Im)?1MTX (dual)
(2)
where KX = MTXMX is the Gram matrix with
KX(i, j) = kX(xi, xj) and kX(xi, xj) is the ker-
nel function defined as kX(xi, xj) = ?(xi)T?(xj).
The primal solution involves the inversion of the
covariance matrix in the feature space (O(N3X))
and the dual solution involves the inversion of the
kernel matrix in the instance space (O(m3)) and
L2 regularization term prevents the normal equa-
tions to be singular. We use the dual solution when
computing HL2 .
Two main challenges of the RegMT approach
are learning the regression function, g : X? ?
FY , and solving the pre-image problem, which,
given the features of the estimated target string se-
quence, g(x) = ?Y (y?), attempts to find y ? Y ?:
f(x) = arg miny?Y ? ||g(x)??Y (y)||
2. Pre-image
calculation involves a search over possible transla-
tions minimizing the cost function:
f(x) = arg min
y?Y ?
??Y (y)?H?X(x)?
2
= arg min
y?Y ?
kY (y, y)? 2(K
y
Y )
T (KX + ?Im)
?1KxX ,(3)
where KyY =[kY (y, y1), . . . , kY (y, ym)]
T ? Rm?1
and KxX ? R
m?1 is defined similarly.
We use n-spectrum weighted word ker-
nel (Shawe-Taylor and Cristianini, 2004) as fea-
ture mappers which consider all word sequences
up to order n:
k(x, x?)=
nX
p=1
|x|?p+1X
i=1
|x?|?p+1X
j=1
p I(x[i : i+p?1]=x?[j :j+p?1])
(4)
where x[i : j] denotes a substring of x with the
words in the range [i, j], I(.) is the indicator func-
tion, and p is the number of words in the feature.
3 L1 Regularized Regression
In statistical machine translation, parallel cor-
pora, which contain translations of the same doc-
uments in source and target languages, are used
to estimate a likely target translation for a given
source sentence based on the observed transla-
tions. String kernels lead to very sparse represen-
tations of the feature space and we examine the ef-
fectiveness of L1 regularized regression to find the
mappings between sparsely observed feature sets.
3.1 Sparsity in Translation Mappings
We would like to observe only a few nonzero tar-
get feature coefficients corresponding to a source
feature in the coefficient matrix. An example solu-
tion matrix representing a possible alignment be-
tween unigram source and target features could be
the following:
283
H e1 e2 e3
f1 1 1
f2 1
f3 1
Here ei represents unigram source features and fi
represent unigram target features. e1 and e3 have
unambiguous translations whereas e2 is ambigu-
ous. Even if unigram features lead to ambiguity,
we expect higher order features like bigrams and
trigrams to help us resolve the ambiguity. Typical
H matrices have thousands of features. L1 regu-
larization helps us achieve solutions close to per-
mutation matrices by increasing sparsity (Bishop,
2006). In contrast, L2 solutions give us dense ma-
trices.
3.2 L1 Regularized Regression for Learning
HL2 does not give us a sparse solution and most
of the coefficients remain non-zero. L1 norm be-
haves both as a feature selection technique and a
method for reducing coefficient values.
HL1 = arg min
H?RNY ?NX
?MY ?HMX ?2F +??H?1 .(5)
Equation 5 presents the lasso (least absolute
shrinkage and selection operator) (Tibshirani,
1996) solution where the regularization term is
now the L1 matrix norm defined as ? H ?1=?
i,j |Hi,j |. Since L1 regularization cost is not
differentiable, HL1 is found by optimization or ap-
proximation techniques. We briefly describe three
techniques to obtain L1 regularized regression co-
efficients.
Forward Stagewise Regression (FSR): We
experiment with forward stagewise regression
(FSR) (Hastie et al, 2006), which approximates
the lasso. The incremental forward stagewise re-
gression algorithm increases the weight of the pre-
dictor variable that is most correlated with the
residual by a small amount, , multiplied with
the sign of the correlation at each step. As
 ? 0, the profile of the coefficients resemble the
lasso (Hastie et al, 2009).
Quadratic Programming (QP): We also use
quadratic programming (QP) to find HL1 . We can
pose lasso as a QP problem as follows (M?rup
and Clemmensen, 2007). We assume that the
rows of MY are independent and solve for each
row i, Myi ? R
1?m, using non-negative variables
h+i ,h
?
i ? R
NX?1 such that hi = h+i ? h
?
i :
hi = arg min
h
?Myi ? hMX?
2
F +?
NXX
k=1
|hk|, (6)
hi = arg min
h?i
1
2
h?igMXgMX
T
h?i
T
? h?i(gMXM
T
yi ? ?1), (7)
s.t. h?i > 0, gMX =
?
MX
?MX
?
, h?i =
?
h+i h
?
i
?
.
Linear Programming (LP): L1 minimization
can also be posed as a linear programming (LP)
problem by interpreting the error term as the con-
straint (Chen et al, 1998) and solving for each row
i:
hi = arg min
h
?h?1 subject to Myi = hMX , (8)
which can again be solved using non-negative
variables. This is a slightly different optimization
and the results can be different but linear program-
ming solvers offer computational advantages.
3.3 Transductive Regression
Transduction uses test instances, which can some-
times be accessible at training time, to learn spe-
cific models tailored towards the test set. Trans-
duction has computational advantages by not us-
ing the full training set and by having to satisfy a
smaller set of constraints. For each test sentence,
we pick a limited number of training instances de-
signed to improve the coverage of correct features
to build a regression model. Section 4.2 details our
instance selection methods.
4 Translation Experiments
We perform experiments on the translation task
of the English-German, German-English, English-
French, English-Spanish, and English-Czech lan-
guage pairs using the training corpus provided in
WMT10.
4.1 Datasets and Baseline
We developed separate SMT models using
Moses (Koehn et al, 2007) with default settings
with maximum sentence length set to 80 using 5-
gram language model and obtained distinct 100-
best lists for the test sets. All systems were tuned
with 2051 sentences and tested with 2525 sen-
tences. We have randomly picked 100 instances
from the development set to be used in tuning the
regression experiments (dev.100). The translation
challenge test set contains 2489 sentences. Num-
ber of sentences in the training set of each system
284
and baseline performances for uncased output (test
set BLEU, challenge test set BLEU) are given in
Table 1.
Corpus # sent BLEU BLEU Challenge
en-de 1609988 .1471 .1309
de-en 1609988 .1943 .1556
en-fr 1728965 .2281 .2049
en-es 1715158 .2237 .2106
en-cz 7320238 .1452 .1145
Table 1: Initial uncased performances of the trans-
lation systems.
Feature mappers used are 3-spectrum counting
word kernels, which consider all N -grams up to
order 3 weighted by the number of tokens in the
feature. We segment sentences using some of the
punctuation for managing the feature set better and
do not consider N -grams that cross segments.
We use BLEU (Papineni et al, 2001) and
NIST (Doddington, 2002) evaluation metrics for
measuring the performance of translations auto-
matically.
4.2 Instance Selection
Proper selection of training instances plays an im-
portant role to learn feature mappings with limited
computational resources accurately. In previous
work (Wang and Shawe-Taylor, 2008), sentence
based training instances were selected using tf-idf
retrieval. We transform test sentences to feature
sets obtained by the kernel mapping before mea-
suring their similarities and index the sentences
based on the features. Given a source sentence
of length 20, its feature representation would have
a total of 57 uni/bi/tri-gram features. If we select
closest sentences from the training set, we may not
have translations for all the features in this repre-
sentation. But if we search for translations of each
feature, then we have a higher chance of covering
all the features found in the sentence we are try-
ing to translate. The index acts as a dictionary of
source phrases storing training set entries whose
source sentence match the given source phrase.
The number of instances per feature is chosen
inversely proportional to the frequency of the fea-
ture determined by the following formula:
#instance(f) = n/ ln(1 + idfScore(f)/9.0), (9)
where idfScore(f) sums the idf (inverse document
frequency) of the tokens in feature f and n is a
small number.
4.3 Addition of Brevity Penalty
Detailed analysis of the results shows TRegMT
score achieves better N -gram match percentages
than Moses translation but suffers from the brevity
penalty due to selecting shorter translations. Due
to using a cost function that minimizes the squared
loss, TRegMT score tends to select shorter trans-
lations when the coverage is low. We also observe
that we are able to achieve higher scores for NIST,
which suggests the addition of a brevity penalty to
the score.
Precision based BLEU scoring divides N -gram
match counts toN -gram counts found in the trans-
lation and this gives an advantage to shorter trans-
lations. Therefore, a brevity penalty (BP) is added
to penalize short translations:
BP = min(1?
ref-length
trans-length
, 0) (10)
BLEU = e(log(ngramprec)+BP) (11)
where ngramprec represent the sum of n-gram
precisions. Moses rarely incurs BP as it has a word
penalty parameter optimized against BLEU which
penalizes translations that are too long or too short.
For instance, Moses 1-best translation for en-de
system achieves .1309 BLEU versus .1320 BLEU
without BP.
We handle short translations in two ways. We
optimize the ? parameter of QP, which manages
the sparsity of the solution (larger ? values cor-
respond to sparser solutions) against BLEU score
rather than the squared loss. Optimization yields
? = 20.744. We alternatively add a BP cost to the
squared loss:
BP = e
?
min(1?
|?Y (y)|
|pH?X (x)+?BP q|
,0)
?
(12)
f(x) = arg min
y?Y ?
??Y (y)?H?X(x)?
2 +?BPBP (13)
where |.| denotes the length of the feature vector,
p.q rounds feature weights to integers, ?BP is a
constant weight added to the estimation, and ?BP
is the weight given for the BP cost. |pH?X(x) +
?BP q| represents an estimate of the length of the
reference as found by the TRegMT system. This
BP cost estimate is similar to the cost used in (Ser-
rano et al, 2009) normalized by the length of the
reference. We found ?BP = 0.1316 and ?BP =
?13.68 when optimized on the en-de system. We
add a BP penalty to all of the reranking results
given in the next section and QP results also use
optimized ?.
285
en-de de-en en-fr en-es en-cz
Score BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST
Baseline .1309 5.1417 .1556 5.4164 .2049 6.3194 .2106 6.3611 .1145 4.5008
Oracle .1811 6.0252 .2101 6.2103 .2683 7.2409 .2770 7.3190 .1628 5.4501
L2 .1319 5.1680 .1555 5.4344 .2044 6.3370 .2132 6.4093 .1148 4.5187
FSR .1317* 5.1639 .1559 5.4383 .2053 6.3458 .2144 6.4168 .1150 4.5172
LP .1317 5.1695 .1561 5.4304 .2048 6.3245 .2109 6.4176 .1124 4.5143
QP .1309 5.1664 .1550 5.4553 .2033 6.3354* .2121 6.4271 .1150 4.5264
Table 2: Reranking results using TRegMT, TM, and LM scores. We use approximate randomization
test (Riezler and Maxwell, 2005) with 1000 repetitions to determine score difference significance: results
in bold are significant with p ? 0.01 and italic results with (*) are significant with p ? .05. The
difference of the remaining from the baseline are not statistically significant.
4.4 Reranking Experiments
We rerank N -best lists by using linear combina-
tions of the following scoring functions:
1. TRegMT: Transductive regression based ma-
chine translation scores as found by Equa-
tion 3.
2. TM: Translation model scores we obtain
from the baseline SMT system that is used
to generate the N -best lists.
3. LM: 5-gram language model scores that the
baseline SMT system uses when calculating
the translation model scores.
The training set we obtain may not contain all
of the features of the reference target due to low
coverage. Therefore, when performing reranking,
we also add the cost coming from the features of
?Y (y) that are not represented in the training set
to the squared loss as in:
??Y (y) \ FY ?2 + ??Y (y)?H?X(x)?2, (14)
where ?Y (y) \ FY represent the features of y not
represented in the training set.
We note that TRegMT score only contains or-
dering information as present in the bi/tri-gram
features in the training set. Therefore, the ad-
dition of a 5-gram LM score as well as the TM
score, which also incorporates the LM score in
itself, improves the performance. We are not
able to improve the BLEU score when we use
TRegMT score by itself however we are able to
achieve improvements in the NIST and 1-WER
scores. The performance increase is important for
two reasons. First of all, we are able to improve
the performance using blended spectrum 3-gram
features against translations obtained with 5-gram
language model and higher order features. Out-
performing higher order n-gram models is known
to be a difficult task (Galley and Manning, 2009).
Secondly, increasing the performance with rerank-
ing itself is a hard task since possible translations
are already constrained by the ones observed inN -
best lists. Therefore, an increase in the N -best list
size may increase the score gaps.
Table 2 presents reranking results on all of the
language pairs we considered, using TRegMT,
TM, and LM scores with the combination weights
learned in the development set. We are able to
achieve better BLEU and NIST scores on all of the
listed systems. We are able to see up to .38 BLEU
points increase for the en-es pair. Oracle reranking
performances are obtained by using BLEU scoring
metric.
If we used only the TM and LM scores when
reranking with the en-de system, then we would
obtain .1309 BLEU and 5.1472 NIST scores. We
only see a minor increase in the NIST score and no
change in the BLEU score with this setting when
compared with the baseline given in Table 2.
Due to computational reasons, we do not use
the same number of instances to train different
models. In our experiments, we used n = 3 for
L2, n = 1.5 for FSR, and n = 1.2 for QP and
LP solutions to select the number of instances in
Equation 9. The average number of instances used
per sentence in training corresponding to these
choices are approximately 140, 74, and 61. Even
with these decreased number of training instances,
L1 regularized regression techniques are able to
achieve comparable scores to L2 regularized re-
gression model in Table 2.
5 System Combination Experiments
We perform experiments on the system com-
bination task for the English-German, German-
English, English-French, English-Spanish, and
English-Czech language pairs using the training
286
en-de de-en en-fr en-es en-cz
Score BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST
Random .1490 5.6555 .2088 6.4886 .2415 6.8948 .2648 7.2563 .1283 4.9238
Best model .1658 5.9610 .2408 6.9861 .2864 7.5272 .3047 7.7559 .1576 5.4480
L2 .1694 5.9974 .2336 6.9398 .2948 7.7037 .3036 7.8120 .1657 5.5654
FSR .1689 5.9638 .2357 6.9254 .2947 7.7107 .3049 7.8156 .1657 5.5632
LP .1694 5.9954 .2368 6.8850 .2928 7.7157 .3027 7.7838 .1659 5.5680
QP .1692 5.9983 .2368 6.9172 .2913 7.6949 .3040 7.8086 .1662 5.5785
Table 3: Reranking results using TRegMT, TM, and LM scores. bold correspond to the best score in
each rectangle of scores.
corpus provided in WMT10.
5.1 Datasets
We use the training set provided in WMT10 to in-
dex and select transductive instances from. The
challenge split the test set for the translation task
of 2489 sentences into a tuning set of 455 sen-
tences and a test set with the remaining 2034 sen-
tences. Translation outputs for each system is
given in a separate file and the number of sys-
tem outputs per translation pair varies. We have
tokenized and lowercased each of the system out-
puts and combined these in a singleN -best file per
language pair. We also segment sentences using
some of the punctuation for managing the feature
set better. We use these N -best lists for TRegMT
reranking to select the best translation model. Fea-
ture mappers used are 3-spectrum counting word
kernels, which consider all n-grams up to order 3
weighted by the number of tokens in the feature.
5.2 Experiments
We rerank N -best lists by using combinations of
the following scoring functions:
1. TRegMT: Transductive regression based ma-
chine translation scores as found by Equa-
tion 3.
2. TM?: Translation model scores are obtained
by measuring the average BLEU perfor-
mance of each translation relative to the other
translations in the N -best list.
3. LM: We calculate 5-gram language model
scores for each translation using the language
model trained over the target corpus provided
in the translation task.
Since we do not have access to the reference
translations nor to the translation model scores
each system obtained for each sentence, we es-
timate translation model performance (TM?) by
measuring the average BLEU performance of each
translation relative to the other translations in the
N -best list. Thus, each possible translation in the
N -best list is BLEU scored against other transla-
tions and the average of these scores is selected
as the TM score for the sentence. Sentence level
BLEU score calculation avoids singularities in n-
gram precisions by taking the maximum of the
match count and 12|si| for |si| denoting the length
of the source sentence si as used in (Macherey and
Och, 2007).
Table 3 presents reranking results on all of the
language pairs we considered, using TRegMT,
TM, and LM scores with the same combination
weights as above. Random model score lists the
random model performance selected among the
competing translations randomly and it is used as
a baseline. Best model score lists the performance
of the best model performance. We are able to
achieve better BLEU and NIST scores in all of the
listed systems except for the de-en language pair
when compared with the performance of the best
competing translation system. The lower perfor-
mance in the de-en language pair may be due to
having a single best translation system that outper-
forms others significantly. The difference between
the best model performance and the mean as well
as the variance of the scores in the de-en language
pair is about twice their counterparts in en-de lan-
guage pair.
Due to computational reasons, we do not use
the same number of instances to train different
models. In our experiments, we used n = 4 for
L2, n = 1.5 for FSR, and n = 1.2 for QP and
LP solutions to select the number of instances in
Equation 9. The average number of instances used
per sentence in training corresponding to these
choices are approximately 189, 78, and 64.
287
6 Contributions
We use transductive regression to learn mappings
between source and target features of given paral-
lel corpora and use these mappings to rerank trans-
lation outputs. We compare the effectiveness ofL1
regularization techniques for regression. TRegMT
score has a tendency to select shorter transla-
tions when the coverage is low. We incorporate a
brevity penalty to the squared loss and optimize ?
parameter of QP to tackle this problem and further
improve the performance of the system.
The results show the effectiveness of using L1
regularization versus L2 used in ridge regression.
Proper selection of training instances plays an im-
portant role to learn correct feature mappings with
limited computational resources accurately. We
plan to investigate better instance selection meth-
ods for improving the translation performance.
TRegMT score has a tendency to select shorter
translations when the coverage is low. We incor-
porate a brevity penalty to the score and optimize
the ? parameter of QP to tackle this problem.
Acknowledgments
The research reported here was supported in
part by the Scientific and Technological Research
Council of Turkey (TUBITAK).
References
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer.
Olivier Chapelle, Vladimir Vapnik, and Jason Weston.
1999. Transductive inference for estimating values
of functions. In NIPS, pages 421?427.
Scott Shaobing Chen, David L. Donoho, and
Michael A. Saunders. 1998. Atomic decomposition
by basis pursuit. SIAM Journal on Scientific Com-
puting, 20(1):33?61.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2007. A general regression framework for learn-
ing string-to-string mappings. In Gokhan H. Bakir,
Thomas Hofmann, and Bernhard Sch editors, Pre-
dicting Structured Data, pages 143?168. The MIT
Press, September.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 773?781,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Trevor Hastie, Jonathan Taylor, Robert Tibshirani, and
Guenther Walther. 2006. Forward stagewise regres-
sion and the monotone lasso. Electronic Journal of
Statistics, 1.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning:
Data Mining, Inference and Prediction. Springer-
Verlag, 2nd edition.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Assoc. for Compu-
tational Linguistics, pages 177?180, Prague, Czech
Republic, June.
Wolfgang Macherey and Franz J. Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems. In
EMNLP-CoNLL, pages 986?995.
M. M?rup and L. H. Clemmensen. 2007. Multiplica-
tive updates for the lasso. In Machine Learning for
Signal Processing MLSP, IEEE Workshop on, pages
33 ?38, Aug.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Nicolas Serrano, Jesus Andres-Ferrer, and Francisco
Casacuberta. 2009. On a kernel regression approach
to machine translation. In Iberian Conference on
Pattern Recognition and Image Analysis, pages 394?
401.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
288
Robert J. Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of the Royal Statisti-
cal Society, Series B, 58(1):267?288.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 25?32, Prague, Czech Republic,
June. The Association for Computer Linguistics.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared transla-
tion task. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 155?158,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine trans-
lation. In Human Language Technologies 2007:
The Conference of the North American Chapter
of the Association for Computational Linguistics;
Companion Volume, Short Papers, pages 185?188,
Rochester, New York, April. Association for Com-
putational Linguistics.
289
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 272?283,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Instance Selection for Machine Translation using Feature Decay
Algorithms
Ergun Bi?ici
Ko? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Deniz Yuret
Ko? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
We present an empirical study of instance se-
lection techniques for machine translation. In
an active learning setting, instance selection
minimizes the human effort by identifying
the most informative sentences for transla-
tion. In a transductive learning setting, se-
lection of training instances relevant to the
test set improves the final translation qual-
ity. After reviewing the state of the art in
the field, we generalize the main ideas in a
class of instance selection algorithms that use
feature decay. Feature decay algorithms in-
crease diversity of the training set by devalu-
ing features that are already included. We
show that the feature decay rate has a very
strong effect on the final translation quality
whereas the initial feature values, inclusion
of higher order features, or sentence length
normalizations do not. We evaluate the best
instance selection methods using a standard
Moses baseline using the whole 1.6 million
sentence English-German section of the Eu-
roparl corpus. We show that selecting the
best 3000 training sentences for a specific
test sentence is sufficient to obtain a score
within 1 BLEU of the baseline, using 5% of
the training data is sufficient to exceed the
baseline, and a? 2 BLEU improvement over
the baseline is possible by optimally selected
subset of the training data. In out-of-domain
translation, we are able to reduce the train-
ing set size to about 7% and achieve a similar
performance with the baseline.
1 Introduction
Statistical machine translation (SMT) makes use
of a large number of parallel sentences, sentences
whose translations are known in the target lan-
guage, to derive translation tables, estimate param-
eters, and generate the actual translation. Not all
of the parallel corpus nor the translation table that
is generated is used during decoding a given set
of test sentences and filtering is usually performed
for computational advantage (Koehn et al, 2007).
Some recent regression-based statistical machine
translation systems rely on a small sized training
data to learn the mappings between source and tar-
get features (Wang and Shawe-Taylor, 2008; Ser-
rano et al, 2009; Bicici and Yuret, 2010). Regres-
sion has some computational disadvantages when
scaling to large number of training instances.
Previous work shows that the more the training
data, the better the translations become (Koehn,
2006). However, with the increased size of the
parallel corpus there is also the added noise, mak-
ing relevant instance selection important. Phrase-
based SMT systems rely heavily on accurately
learning word alignments from the given parallel
corpus. Proper instance selection plays an impor-
tant role in obtaining a small sized training set with
which correct alignments can be learned. Word-
level translation accuracy is also affected by the
number of times a word occurs in the parallel cor-
pus (Koehn and Knight, 2001). Koehn and Knight
find that about 50 examples per word are required
to achieve a performance close to using a bilingual
lexicon in their experiments. Translation perfor-
mance can improve as we include multiple possi-
ble translations for a given word, which increases
272
the diversity of the training set.
Transduction uses test instances, which can
sometimes be accessible at training time, to learn
specific models tailored towards the test set which
also reduces computation by not using the full
training set. Transductive retrieval selects train-
ing data close to the test set given a parallel corpus
and a test set. This work shows that transductive
retrieval of the training set for statistical machine
translation allows us to achieve a performance bet-
ter than using all of the parallel corpus. When se-
lecting training data, we seek to maximize the cov-
erage or the percentage of test source and target
features (i.e. n-grams) found in the training set us-
ing minimal number of target training features and
a fixed number of training instances. Diversifying
the set of training sentences can help us increase
the coverage. We show that target coverage bounds
the achievable BLEU score with a given training
set and small increases can result in large increases
on this BLEU bound.
We develop the feature decay algorithms (FDA)
that aim to maximize the coverage of the target
language features and achieve significant gains in
translation performance. We find that decaying
feature weights has significant effect on the per-
formance. We achieve improvements of ?2 BLEU
points using about 20% of the available training
data in terms of target words and ?1 BLEU points
with only about 5%. We show that selecting 3000
instances for a test sentence is sufficient to obtain
a score within 1 BLEU of the baseline. In the out-
of-domain translation task, we are able to reduce
the training set size to its 7% to achieve a similar
performance with the baseline.
The next section reviews related previous work.
We discuss the FDA in section 3. Section 4
presents our coverage and translation results both
in and out-of-domain and includes an instance se-
lection method also designed for improving word
alignment results. We list our contributions in the
last section.
2 Related Work
Transductive learning makes use of test instances,
which can sometimes be accessible at training
time, to learn specific models tailored towards the
test set. Selection of training instances relevant to
the test set improves the final translation quality as
in transductive learning and decreases human ef-
fort by identifying the most informative sentences
for translation as in active learning. Instance se-
lection in a transductive learning framework se-
lects the best instances for a given test set (L? et
al., 2007). Active learning selects training samples
that will benefit the learning algorithm the most
over the unlabeled dataset U from a labeled train-
ing set L or from U itself after labeling (Banko and
Brill, 2001). Active learning in SMT selects which
instances to add to the training set to improve the
performance of a baseline system (Haffari et al,
2009; Ananthakrishnan et al, 2010). Recent work
involves selecting sentence or phrase translation
tasks for external human effort (Bloodgood and
Callison-Burch, 2010). Below we present exam-
ples of both with a label indicating whether they
follow an approach close to active learning [AL] or
transductive learning [TL] and in our experiments
we use the transductive framework.
TF-IDF [TL]: L? et al (2007) use tf-idf infor-
mation retrieval technique based cosine score to se-
lect a subset of the parallel corpus close to the test
set for SMT training. They outperform the baseline
system when the top 500 training instances per test
sentence are selected. The terms used in their tf-idf
measure correspond to words where this work fo-
cuses on bigram feature coverage. When the com-
bination of the top N selected sentences are used
as the training set, they show increase in the per-
formance at the beginning and decrease when 2000
sentences are selected for each test sentence.
N-gram coverage [AL]: Eck et al (2005) use
n-gram feature coverage to sort and select training
instances using the following score:
?NGRAM (S) =
?n
i=1
?
unseen x ? Xi(S) C(x)
|S|
,
(1)
for sentence S with Xi(S) storing the i-grams
found in S and C(x) returning the count of x in
the parallel corpus. ?NGRAM score sums over un-
seen n-grams to increase the coverage of the train-
ing set. The denominator involving the length of
the sentence takes the translation cost of the sen-
tence into account. Eck et al (2005) also note
that longer sentences are more difficult for train-
ing SMT models. In their experiments, they are
not able to reach a performance above the baseline
2
273
system?s BLEU score, which is using all of the par-
allel corpus, but they achieve close performance by
using about 15% of the parallel corpus.
DWDS [AL]: Density weighted diversity sam-
pling (DWDS) (Ambati et al, 2010) score tries to
select sentences containing the n-gram features in
the unlabeled dataset U while increasing the di-
versity among the sentences selected, L (labeled).
DWDS increases the score of a sentence with in-
creasing frequency of its n-grams found in U and
decreases with increasing frequency in the already
selected set of sentences, L, in favor of diversity.
Let PU (x) denote the probability of feature x in U
and CL(x) denote its count in L. Then:
d(S) =
?
x?X(S) PU (x)e
??CL(x)
|X(S)|
(2)
u(S) =
?
x?X(S) I(x 6? X(L))
|X(S)|
(3)
?DWDS(S) =
2d(S)u(S)
d(S) + u(S)
, (4)
where X(S) stores the features of S and ? is a
decay parameter. d(S) denotes the density of S
proportional to the probability of its features in U
and inversely proportional to their counts in L and
u(S) its uncertainty, measuring the percentage of
new features in S. These two scores are combined
using harmonic mean. DWDS tries to select sen-
tences containing similar features in U with high
diversity. In their active learning experiments, they
selected 1000 training instances in each iteration
and retrained the SMT system.
Log-probability ratios [AL]: Haffari et
al. (2009) develop sentence selection scores using
feature counts in L and U , increasing for frequent
features in U and decreasing for frequent features
in L. They use geometric and arithmetic averages
of log-probability ratios in an active learning
setting where 200 sentences from U are selected
and added to L with their translations for 25
iterations (Haffari et al, 2009). Later, Haffari
et al (2009) distinguish between features found
in the phrase table, xreg, and features not found,
xoov. OOV features are segmented into subfeatures
(i.e. feature ?go to school? is segmented as:
(go to school), (go)(to school), (go to)(school),
(go)(to)(school)). Expected log probability ratio
(ELPR) score is used:
?ELPR(S) = 0.4|Xreg(S)|
?
x?Xreg(S)
log
PU (x)
PL(x)
+ 0.6|Xoov(S)|
?
x?Xoov(S)
?
h?H(x)
1
|H(x)|
?
y?Yh(x)
log
PU (y)
PL(y)
,
(5)
where H(x) return the segmentations of x and
Yh(x) return the features found in segment h.
?ELPR performs better than geometric average in
their experiments (Haffari and Sarkar, 2009).
Perplexity [AL & TL]: Perplexity of the train-
ing instance as well as inter-SMT-system disagree-
ment are also used to select training data for trans-
lation models (Mandal et al, 2008). The increased
difficulty in translating a parallel sentence or its
novelty as found by the perplexity adds to its im-
portance for improving the SMT model?s perfor-
mance. A sentence having high perplexity (a rare
sentence) in L and low perplexity (a common sen-
tence) in U is considered as a candidate for addi-
tion. They are able to improve the performance
of a baseline system trained on some initial corpus
together with additional parallel corpora using the
initial corpus and part of the additional data.
Alignment [TL]: Uszkoreit et al (2010) mine
parallel text to improve the performance of a base-
line translation model on some initial document
translation tasks. They retrieve similar documents
using inverse document frequency weighted cosine
similarity. Then, they filter nonparallel sentences
using their word alignment performance, which is
estimated using the following score:
score(A) =
?
(s,t)?A
ln
p(s, t)
p(s)p(t)
, (6)
where A stands for an alignment between source
and target words and the probabilities are estimated
using a word aligned corpus. The produced paral-
lel data is used to expand a baseline parallel corpus
and shown to improve the translation performance
of machine translation systems.
3 Instance Selection with Feature
Decay
In this section we will describe a class of instance
selection algorithms for machine translation that
3
274
use feature decay, i.e. increase the diversity of the
training set by devaluing features that have already
been included. Our abstraction makes three com-
ponents of such algorithms explicit permitting ex-
perimentation with their alternatives:
? The value of a candidate training sentence as
a function of its features.
? The initial value of a feature.
? The update of the feature value as instances
are added to the training set.
A feature decay algorithm (FDA) aims to max-
imize the coverage of the target language features
(such as words, bigrams, and phrases) for the test
set. A target language feature that does not ap-
pear in the selected training instances will be dif-
ficult to produce regardless of the decoding algo-
rithm (impossible for unigram features). In gen-
eral we do not know the target language features,
only the source language side of the test set is avail-
able. Unfortunately, selecting a training instance
with a particular source language feature does not
guarantee the coverage of the desired target lan-
guage feature. There may be multiple translations
of a feature appropriate for different senses or dif-
ferent contexts. For each source language feature
in the test set, FDA tries to find as many train-
ing instances as possible to increase the chances
of covering the appropriate target language feature.
It does this by reducing the value of the features
that are already included after picking each train-
ing instance. Algorithm 1 gives the pseudo-code
for FDA.
The input to the algorithm is a parallel corpus,
the number of desired training instances, and the
source language features of the test set. We use
unigram and bigram features; adding trigram fea-
tures does not seem to significantly affect the re-
sults. The user has the option of running the algo-
rithm for each test sentence separately, then possi-
bly combining the resulting training sets. We will
present results with these variations in Section 4.
The first foreach loop initializes the value of
each test set feature. We experimented with ini-
tial feature values that are constant, proportional
to the length of the n-gram, or log-inverse of the
corpus frequency. We have observed that the ini-
tial value does not have a significant effect on the
Algorithm 1: The Feature Decay Algorithm
Input: Bilingual corpus U , test set features F ,
and desired number of training
instances N .
Data: A priority queue Q, sentence scores
score, feature values fvalue.
Output: Subset of the corpus to be used as the
training data L ? U .
foreach f ? F do1
fvalue(f)? init(f,U)2
foreach S ? U do3
score(S)?
?
f?features(S) fvalue(f)4
push(Q, S,score(S))5
while |L| < N do6
S ? pop(Q)7
score(S)?
?
f?features(S) fvalue(f)8
if score(S) ? topval(Q) then9
L ? L ? {S}10
foreach f ? features(S) do11
fvalue(f)? decay(f,U ,L)12
else13
push(Q, S,score(S))14
quality of training instances selected. The feature
decay rule dominates the behavior of the algorithm
after the first few iterations. However, we prefer
the log-inverse values because they lead to fewer
score ties among candidate instances and result in
faster running times.
The second foreach loop initializes the score for
each candidate training sentence and pushes them
onto a priority queue. The score is calculated as the
sum of the feature values. Note that as we change
the feature values, the sentence scores in the prior-
ity queue will no longer be correct. However they
will still be valid upper bounds because the fea-
ture values only get smaller. Features that do not
appear in the test set are considered to have zero
value. This observation can be used to speed up
the initialization by using a feature index and only
iterating over the sentences that have features in
common with the test set.
Finally the while loop populates the training set
by picking candidate sentences with the highest
scores. This is done by popping the top scoring
candidate S from the priority queue at each itera-
tion. We recalculate its score because the values
4
275
of its features may have changed. We compare the
recalculated score of S with the score of the next
best candidate. If the score of S is equal or better
we are sure that it is the top candidate because the
scores in the priority queue are upper bounds. In
this case we place S in our training set and decay
the values of its features. Otherwise we push S
back on the priority queue with its updated score.
The feature decay function on Line 12 is the
heart of the algorithm. Unlike the choice of fea-
tures (bigram vs trigram) or their initial values
(constant vs log?inverse?frequency) the rate of de-
cay has a significant effect on the performance. We
found it is optimal to reduce feature values at a rate
of 1/n where n is the current training set count
of the feature. The results get significantly worse
with no feature decay. They also get worse with
faster, exponential feature decay, e.g. 1/2n. Ta-
ble 1 presents the experimental results that support
these conclusions. We use the following settings
for the experiments in Section 4:
init(f,U) = 1 or log(|U|/cnt(f,U))
decay(f,U ,L) =
init(f,U)
1 + cnt(f,L)
or
init(f,U)
1 + 2cnt(f,L)
init decay en?de de?en
1 none .761 .484 .698 .556
log(1/f) none .855 .516 .801 .604
1 1/n .967 .575 .928 .664
log(1/f) 1/n .967 .570 .928 .656
1 1/2n .967 .553 .928 .653
log(1/f) 1/2n .967 .557 .928 .651
Table 1: FDA experiments. The first two columns
give the initial value and decay formula used for
features. f is the corpus frequency of a feature
and n is its count in selected instances. The next
four columns give the expected coverage of the
source and target language bigrams of a test sen-
tence when 100 training sentences are selected.
4 Experiments
We perform translation experiments on the
English-German language pair using the parallel
corpus provided in WMT?10 (Callison-Burch et
al., 2010). The English-German section of the Eu-
roparl corpus contains about 1.6 million sentences.
We perform in-domain experiments to discriminate
among different instance selection techniques bet-
ter in a setting with low out-of-vocabulary rate. We
randomly select the test set test with 2, 588 tar-
get words and separate development set dev with
26, 178 target words. We use the language model
corpus provided in WMT?10 (Callison-Burch et
al., 2010) to build a 5-gram model.
We use target language bigram coverage, tcov,
as a quality measure for a given training set, which
measures the percentage of the target bigram fea-
tures of the test sentence found in a given training
set. We compare tcov and the translation perfor-
mance of FDA with related work. We also perform
small scale SMT experiments where only a couple
of thousand training instances are used for each test
sentence.
4.1 The Effect of Coverage on Translation
BLEU (Papineni et al, 2001) is a precision based
measure and uses n-gram match counts up to or-
der n to determine the quality of a given transla-
tion. The absence of a given word or translating
it as another word interrupts the continuity of the
translation and decreases the BLEU score even if
the order among the words is determined correctly.
Therefore, the target coverage of an out-of-domain
test set whose translation features are not found in
the training set bounds the translation performance
of an SMT system.
We estimate this translation performance bound
from target coverage by assuming that the miss-
ing tokens can appear randomly at any location of
a given sentence where sentence lengths are nor-
mally distributed with mean 25.6 and standard de-
viation 14.1. This is close to the sentence length
statistics of the German side Europarl corpus used
in WMT?10 (WMT, 2010). We replace all un-
known words found with an UNK token and calcu-
late the BLEU score. We perform this experiment
for 10, 000 instances and repeat for 10 times.
The obtained BLEU scores for target cover-
age values is plotted in Figure 1 with label esti-
mate. We also fit a third order polynomial func-
tion of target coverage 0.025 BLEU scores above
the estimate values to show the similarity with the
5
276
0.0 0.2 0.4 0.6 0.8 1.0tcov
0.0
0.2
0.4
0.6
0.8
1.0
1.2
BLE
U
BLEU vs. tcov
estimatef(x)=ax^3 + bx^2 + cx + d
Figure 1: Effect of coverage on translation perfor-
mance. BLEU bound is a third-order function of
target coverage. High coverage? High BLEU.
BLEU scores bound estimated, whose parameters
are found to be [0.56, 0.53,?0.09, 0.003] with a
least-squares fit. Figure 1 shows that the BLEU
score bound obtained has a third-order polyno-
mial relationship with target coverage and small
increases in the target coverage can result in large
increases on this BLEU bound.
4.2 Coverage Results
We select N training instances per test sentence
using FDA (Algorithm 1), TF-IDF with bigram
features, NGRAM scoring (Equation 1), DWDS
(Equation 4), and ELPR (Equation 5) techniques
from previous work. For the active learning algo-
rithms, source side test corpus becomes U and the
selected training set L. For all the techniques, we
compute 1-grams and 2-grams as the features used
in calculating the scores and add only one sentence
to the training set at each iteration except for TF-
IDF. We set ? parameter of DWDS to 1 as given
in their paper. We adaptively select the top scor-
ing instance at each step from the set of possible
sentences U with a given scorer ?(.) and add the
instance to the training set, L, until the size of L
reaches N for the related work other than TF-IDF.
We test all algorithms in this transductive setting.
We measure the bigram coverage when all of
the training sentences selected for each test sen-
tence are combined. The results are presented in
Figure 2 where the x-axis is the number of words
of the training set and y-axis is the target cover-
age obtained. FDA has a steep slope in its increase
and it is able to reach target coverage of ? 0.84.
DWDS performs worse initially but its target cov-
erage improve after a number of instances are se-
lected due to its exponential feature decay proce-
dure. TF-IDF performs worse than DWDS and it
provides a fast alternative to FDA instance selec-
tion but with some decrease in coverage. ELPR
and NGRAM instance selection techniques per-
form worse. NGRAM achieves better coverage
than ELPR, although it lacks a decay procedure.
When we compare the sentences selected, we
observe that FDA prefers longer sentences due to
summing feature weights and it achieves larger tar-
get coverage value. NGRAM is not able to discrim-
inate between sentences well and a lot of sentences
of the same length get the same score when the un-
seen n-grams belong to the same frequency class.
The statistics of L obtained with the instance se-
lection techniques differ from each other as given
in Table 2, where N = 1000 training instances se-
lected per test sentence. We observe that DWDS
has fewer unique target bigram features than TF-
IDF although it selects longer target sentences.
NGRAM obtains a large number of unique target
bigrams although its selected target sentences have
similar lengths with DWDS and ELPR prefers short
sentences.
Technique Unique bigrams Words per sent tcov
FDA 827,928 35.8 .74
DWDS 412,719 16.7 .67
TF-IDF 475,247 16.2 .65
NGRAM 626,136 16.6 .55
ELPR 172,703 10.9 .35
Table 2: Statistics of the obtained target L forN =
1000.
4.3 Translation Results
We develop separate phrase-based SMT models
using Moses (Koehn et al, 2007) using default set-
tings with maximum sentence length set to 80 and
obtained baseline system score as 0.3577 BLEU.
We use the training instances selected by FDA in
6
277
104 105 106 107
Training Set Size (words)
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
tco
v
tcov vs. Training Set Size (words)
DWDS
ELPR
FDA
NGRAM
TFIDF
Figure 2: Target coverage curve comparison with previous work. Figure shows the rate of increase in
tcov as the size of L increase.
three learning settings:
L? L is the union of the instances selected for
each test sentence.
L?F L is selected using all of the features found
in the test set.
LI L is the set of instances selected for each test
sentence.
We develop separate Moses systems with each
training set and LI corresponds to developing a
Moses system for each test sentence. L? results
are plot in Figure 3 where we increasingly select
N ? {100, 200, 500, 1000, 2000, 3000, 5000,
10000} instances for each test sentence for train-
ing. The improvements over the baseline are sta-
tistically significant with paired bootstrap resam-
pling using 1000 samples (Koehn, 2004). As we
select more instances, the performance of the SMT
system increases as expected and we start to see a
decrease in the performance after selecting ?107
target words. We obtain comparable results for the
de-en direction. The performance increase is likely
to be due to the reduction in the number of noisy or
irrelevant training instances and the increased pre-
cision in the probability estimates in the generated
phrase tables.
105 106 107 108Training Set Size (words)
0.30
0.31
0.32
0.33
0.34
0.35
0.36
0.37
0.38
BLE
U
0.3058
0.3318
0.341
0.36450.3697
0.3758
0.36220.36530.3577
BLEU vs. Training Set Size (words)
Figure 3: BLEU vs. the number of target words in
L?.
L?F results given in Table 3 show that we can
achieve within 1 BLEU performance using about
3% of the parallel corpus target words (30,000 in-
stances) and better performance using only about
5% (50,000 instances).
The results with LI when building an individ-7
278
# sent # target words BLEU NIST
10,000 449,116 0.3197 5.7788
20,000 869,908 0.3417 6.0053
30,000 1,285,096 0.3492 6.0246
50,000 2,089,403 0.3711 6.1561
100,000 4,016,124 0.3648 6.1331
ALL 41,135,754 0.3577 6.0653
Table 3: Performance for en-de using L?F . ALL
corresponds to the baseline system using all of the
parallel corpus. bold correspond to statistically
significant improvement over the baseline result.
ual Moses model for each test sentence are given
in Table 4. Individual SMT training and transla-
tion can be preferable due to smaller computational
costs and high parallelizability. As we translate
a single sentence with each SMT system, tuning
weights becomes important. We experiment three
settings: (1) using 100 sentences for tuning, which
are randomly selected from dev.1000, (2) using the
mean of the weights obtained in (1), and (3) us-
ing the weights obtained in the union learning set-
ting (L?). We observe that we can obtain a perfor-
mance within 2 BLEU difference to the baseline
system by training on 3000 instances per sentence
(underlined) using the mean weights and 1 BLEU
difference using the union weights. We also exper-
imented with increasing the N -best list size used
during MERT optimization (Hasan et al, 2007),
with increased computational cost, and observed
some increase in the performance.
N 100 dev sents Mean Union
1000 0.3149 0.3242 0.3354
2000 0.3258 0.3352 0.3395
3000 0.3270 0.3374 0.3501
5000 0.3217 0.3303 0.3458
Table 4: LI performance for en-de using 100 sen-
tences for tuning or mean of the weights or dev
weights obtained with the union setting.
Comparison with related work: Table 5
presents the translation results compared with pre-
vious work selecting 1000 instances per test sen-
tence. We observe that coverage and translation
performance are correlated. Although the cover-
age increase of DWDS and FDA appear similar,
due to the third-order polynomial growth of BLEU
with respect to coverage, we achieve large BLEU
gains in translation. We observe increased BLEU
gains when compared with the results of TF-IDF,
NGRAM, and ELPR in order.
FDA DWDS TF-IDF NGRAM ELPR
0.3645 0.3547 0.3405 0.2572 0.2268
Table 5: BLEU results using different techniques
with N = 1000. High coverage? High BLEU.
We note that DWDS originally selects instances
using the whole test corpus to estimate PU (x) and
selects 1000 instances at each iteration. We exper-
imented with both of these settings and obtained
0.3058 and 0.3029 BLEU respectively. Lower
performance suggest the importance of updating
weights after each instance selection step.
4.4 Instance Selection for Alignment
We have shown that high coverage is an integral
part of training sets for achieving high BLEU per-
formance. SMT systems also heavily rely on the
word alignment of the parallel corpus to derive
a phrase table that can be used for translation.
GIZA++ (Och and Ney, 2003) is commonly used
for word alignment and phrase table generation,
which is prone to making more errors as the length
of the training sentence increase (Ravi and Knight,
2010). Therefore, we analyze instance selection
techniques that optimize coverage and word align-
ment performance and at the same time do not
produce very long sentences. Too few words per
sentence may miss the phrasal structure, whereas
too many words per sentence may miss the actual
word alignment for the features we are interested.
We are also trying to retrieve relevant training sen-
tences for a given test sentence to increase the fea-
ture alignment performance.
Shortest: A baseline strategy that can minimize
the training feature set?s size involves selecting the
shortest translations containing each feature.
Co-occurrence: We use co-occurrence of
words in the parallel corpus to retrieve sentences
containing co-occurring items. Dice?s coeffi-
cient (Dice, 1945) is used as a heuristic word align-
ment technique giving an association score for
each pair of word positions (Och and Ney, 2003).
8
279
We define Dice?s coefficient score as:
dice(x, y) =
2C(x, y)
C(x)C(y)
, (7)
where C(x, y) is the number of times x and y co-
occur and C(x) is the count of observing x in the
selected training set. Given a test source sentence,
SU , we can estimate the goodness of a training
sentence pair, (S, T ), by the sum of the alignment
scores:
?dice(SU , S, T ) =
X
x?X(SU )
|T |X
j=1
X
y?Y (x)
dice(y, Tj)
|T | log |S|
,
(8)
where X(SU ) stores the features of SU and Y (x)
lists the tokens in feature x. The difficulty of word
aligning a pair of training sentences, (S, T ), can be
approximated by |S||T |. We use a normalization
factor proportional to |T | log |S|.
The average target words per sentence using
?dice drops to 26.2 compared to 36.3 of FDA.
We still obtain a better performance than the base-
line en-de system with the union of 1000 train-
ing instances per sentence with 0.3635 BLEU and
6.1676 NIST scores. Coverage comparison with
FDA shows slight improvement with lower number
of target bigrams and similar trend for others (Fig-
ure 4). We note that shortest strategy achieves bet-
ter performance than both ELPR and NGRAM. We
obtain 0.3144 BLEU and 5.5 NIST scores in the
individual translation task with 1000 training in-
stances per sentence and 0.3171 BLEU and 5.4662
NIST scores when the mean of the weights is used.
4.5 Out-of-domain Translation Results
We have used FDA and dice algorithms to select
training sets for the out-of-domain challenge test
sets used in (Callison-Burch et al, 2011). The
parallel corpus contains about 1.9 million training
sentences and the test set contain 3003 sentences.
We built separate Moses systems using all of the
parallel corpus for the language pairs en-de, de-en,
en-es, and es-en. We created training sets using
all of the features of the test set to select train-
ing instances. The results given in Table 6 show
that we can achieve similar BLEU performance us-
ing about 7% of the parallel corpus target words
(200,000 instances) using dice and about 16% us-
ing FDA. In the out-of-domain translation task, we
are able to reduce the training set size to achieve
a performance close to the baseline. The sample
points presented in the table is chosen proportional
to the relative sizes of the parallel corpus sizes of
WMT?10 and WMT?11 datasets and the training
set size of the peak in Figure 3. We may be able
to achieve better performance in the out-of-domain
task as well. The sample points in Table 6 may be
on either side of the peak.
5 Contributions
We have introduced the feature decay algorithms
(FDA), a class of instance selection algorithms that
use feature decay, which achieves better target cov-
erage than previous work and achieves significant
gains in translation performance. We find that de-
caying feature weights has significant effect on the
performance. We demonstrate that target coverage
and translation performance are correlated, show-
ing that target coverage is also a good indicator of
BLEU performance. We have shown that target
coverage provides an upper bound on the transla-
tion performance with a given training set.
We achieve improvements of ?2 BLEU points
using about 20% of the available training data in
terms of target words with FDA and ? 1 BLEU
points with only about 5%. We have also shown
that by training on only 3000 instances per sen-
tence we can reach within 1 BLEU difference to
the baseline system. In the out-of-domain transla-
tion task, we are able to reduce the training set size
to achieve a similar performance with the baseline.
Our results demonstrate that SMT systems can
improve their performance by transductive train-
ing set selection. We have shown how to select in-
stances and achieved significant performance im-
provements.
References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner,
and Daniel Tapias, editors, Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).
9
280
104 105 106 107Total Training Set Size (words)
0.3
0.4
0.5
0.6
0.7
0.8
0.9
tco
v
tcov vs. Total Training Set Size (words)
FDAdiceshortest
0 5000 10000 15000 20000 25000 30000 35000 40000Average Training Set Size (words)
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
tco
v
tcov vs. Average Training Set Size (words)
FDAdiceshortest
Figure 4: Target coverage per target words comparison. Figure shows the rate of increase in tcov as
the size of L increase. Target coverage curves for total training set size is given on the left plot and for
average training set size per test sentence on the right plot.
en-de de-en en-es es-en
BLEU
ALL 0.1376 0.2074 0.2829 0.2919
FDA 0.1363 0.2055 0.2824 0.2892
dice 0.1374 0.2061 0.2834 0.2857
# target words ?106
ALL 47.4 49.6 52.8 50.4
FDA 7.9 8.0 8.7 8.2
dice 6.9 7.0 3.9 3.6
% of ALL
FDA 17 16 16 16
dice 14 14 7.4 7.1
Table 6: Performance for the out-of-domain task of (Callison-Burch et al, 2011). ALL corresponds to
the baseline system using all of the parallel corpus.
10
281
Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative sample selection for statistical machine
translation. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 626?635, Cambridge, MA, October.
Association for Computational Linguistics.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages 26?
33, Toulouse, France, July. Association for Computa-
tional Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the ACL 2010
Joint Fifth Workshop on Statistical Machine Transla-
tion and Metrics MATR, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 854?
864, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR. Associa-
tion for Computational Linguistics, Uppsala, Sweden,
July.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2011. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation. Edinburgh, England, July.
Lee R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?
302.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine transla-
tion based on n-gram coverage. In Proceedings of
the 10th Machine Translation Summit, MT Summit X,
pages 227?234, Phuket, Thailand, September.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 181?189, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-
chine translation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 415?423, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Sa?a Hasan, Richard Zens, and Hermann Ney. 2007.
Are very large N-best lists useful for SMT? In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 57?60, Rochester, New York, April. As-
sociation for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2001. Knowledge
sources for word-level translation models. In Pro-
ceedings of the 2001 Conference on Empirical Meth-
ods in Natural Language Processing.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Annual Meeting of the Assoc. for Computational Lin-
guistics, pages 177?180, Prague, Czech Republic,
June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2006. Statistical machine translation:
the basic, the novel, and the speculative. Tutorial at
EACL 2006.
Yajuan L?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
343?350, Prague, Czech Republic, June. Association
for Computational Linguistics.
A. Mandal, D. Vergyri, W. Wang, J. Zheng, A. Stol-
cke, G. Tur, D. Hakkani-Tur, and N.F. Ayan. 2008.
Efficient data selection for machine translation. In
Spoken Language Technology Workshop, 2008. SLT
2008. IEEE, pages 261 ?264.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
11
282
Computational Linguistics, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Sujith Ravi and Kevin Knight. 2010. Does giza++
make search errors? Computational Linguistics,
36(3):295?302.
Nicolas Serrano, Jesus Andres-Ferrer, and Francisco
Casacuberta. 2009. On a kernel regression approach
to machine translation. In Iberian Conference on Pat-
tern Recognition and Image Analysis, pages 394?401.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 1101?1109, Beijing, China,
August. Coling 2010 Organizing Committee.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155?158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
WMT. 2010. ACL Workshop: Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
July.
12
283
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 323?329,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
RegMT System for Machine Translation, System Combination, and
Evaluation
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Deniz Yuret
Koc? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
We present the results we obtain using our
RegMT system, which uses transductive re-
gression techniques to learn mappings be-
tween source and target features of given par-
allel corpora and use these mappings to gen-
erate machine translation outputs. Our train-
ing instance selection methods perform fea-
ture decay for proper selection of training in-
stances, which plays an important role to learn
correct feature mappings. RegMT uses L2
regularized regression as well as L1 regular-
ized regression for sparse regression estima-
tion of target features. We present transla-
tion results using our training instance selec-
tion methods, translation results using graph
decoding, system combination results with
RegMT, and performance evaluation with the
F1 measure over target features as a metric for
evaluating translation quality.
1 Introduction
Regression can be used to find mappings between
the source and target feature sets derived from given
parallel corpora. Transduction learning uses a sub-
set of the training examples that are closely related
to the test set without using the model induced by
the full training set. In the context of statistical ma-
chine translation, translations are performed at the
sentence level and this enables us to select a small
number of training instances for each test instance
to guide the translation process. This also gives us a
computational advantage when considering the high
dimensionality of the problem as each sentence can
be mapped to many features.
The goal in transductive regression based ma-
chine translation (RegMT) is both reducing the com-
putational burden of the regression approach by re-
ducing the dimensionality of the training set and the
feature set and also improving the translation quality
by using transduction.
We present translation results using our training
instance selection methods, translation results us-
ing graph decoding, system combination results with
RegMT, and performance evaluation with the F1
measure over target features as a metric for eval-
uating translation quality. RegMT work builds on
our previous regression-based machine translation
results (Bicici and Yuret, 2010) especially with in-
stance selection and additional graph decoding ca-
pability. We present our results to this year?s chal-
lenges.
Outline: Section 2 gives an overview of the
RegMT model. In section 3, we present our train-
ing instance selection techniques and WMT?11 re-
sults. In section 4, we present the graph decoding re-
sults on the Haitian Creole-English translation task.
Section 5 presents our system combination results
using reranking with the RegMT score. Section 6
evaluates the F1 measure that we use for the auto-
matic evaluation metrics challenge. The last section
present our contributions.
2 Machine Translation Using Regression
Let X and Y correspond to the sets of tokens
that can be used in the source and target strings,
then, m training instances are represented as
(x1, y1), . . . , (xm, ym) ? X
? ? Y ?, where (xi, yi)
corresponds to a pair of source and target language
323
token sequences for 1 ? i ? m. Our goal is to find
a mapping f : X? ? Y ? that can convert a source
sentence to a target sentence sharing the same mean-
ing in the target language (Figure 1).
X? Y ?-
? R ?
-FX FY
g
?X ?Y
6
??1Y
f
h
Figure 1: String-to-string mapping.
We define feature mappers ?X : X? ? FX =
RNX and ?Y : Y ? ? FY = RNY that map each
string sequence to a point in high dimensional real
number space. Let MX ? RNX?m and MY ?
RNY ?m such that MX = [?X(x1), . . . ,?X(xm)]
and MY = [?Y (y1), . . . ,?Y (ym)]. The ridge re-
gression solution usingL2 regularization is found by
minimizing the following cost:
WL2 = arg min
W?RNY ?NX
?MY ?WMX ?2F +??W?
2
F . (1)
Two main challenges of the regression based ma-
chine translation (RegMT) approach are learning
the regression function, h : FX ? FY , and
solving the pre-image problem, which, given the
features of the estimated target string sequence,
h(?X(x)) = ?Y (y?), attempts to find y ? Y ?:
y = arg miny?Y ? ||h(?X(x)) ? ?Y (y)||
2. Pre-
image calculation involves a search over possible
translations minimizing the cost function:
f(x) = arg min
y?Y ?
??Y (y)?W?X(x)?2 . (2)
2.1 L1 Regularized Regression
String kernels lead to sparse feature representations
and L1 regularized regression is effective to find the
mappings between sparsely observed features.We
would like to observe only a few nonzero target co-
efficients corresponding to a source feature in the co-
efficient matrix. L1 regularization helps us achieve
solutions close to permutation matrices by increas-
ing sparsity (Bishop, 2006) (page 145). In contrast,
L2 regularized solutions give us dense matrices.
WL2 is not a sparse solution and most of the coef-
ficients remain non-zero. We are interested in pe-
nalizing the coefficients better; zeroing the irrele-
vant ones leading to sparsification to obtain a solu-
tion that is closer to a permutation matrix. L1 norm
behaves both as a feature selection technique and a
method for reducing coefficient values.
WL1 = arg min
W?RNY ?NX
?MY ?WMX ?2F +??W?1 . (3)
Equation 3 presents the lasso (Tibshirani, 1996) so-
lution where the regularization term is now the L1
matrix norm defined as ?W?1=
?
i,j |Wi,j |. WL2
can be found by taking the derivative but since
L1 regularization cost is not differentiable, WL1 is
found by optimization or approximation techniques.
We use forward stagewise regression (FSR) (Hastie
et al, 2006), which approximates lasso for L1 regu-
larized regression.
2.2 Related Work:
Regression techniques can be used to model the
relationship between strings (Cortes et al, 2007).
Wang et al (2007) applies a string-to-string map-
ping approach to machine translation by using ordi-
nary least squares regression and n-gram string ker-
nels to a small dataset. Later they use L2 regularized
least squares regression (Wang and Shawe-Taylor,
2008). Although the translation quality they achieve
is not better than Moses (Koehn et al, 2007), which
is accepted to be the state-of-the-art, they show the
feasibility of the approach. Serrano et al (2009)
use kernel regression to find translation mappings
from source to target feature vectors and experiment
with translating hotel front desk requests. Locally
weighted regression solves separate weighted least
squares problems for each instance (Hastie et al,
2009), weighted by a kernel similarity function.
3 Instance Selection for Machine
Translation
Proper selection of training instances plays an im-
portant role for accurately learning feature mappings
with limited computational resources. Coverage of
the features is important since if we do not have the
correct features in the training matrices, we will not
be able to translate them. Coverage is measured by
the percentage of target features of the test set found
in the training set. For each test sentence, we pick
a limited number of training instances designed to
324
improve the coverage of correct features to build a
regression model.
We use two techniques for this purpose: (1)
Feature Decay Algorithm (FDA), which optimizes
source languge bigram coverage to maximize the
target coverage, (2) dice. Feature decay algorithms
(FDA) aim to maximize the coverage of the tar-
get language features (such as words, bigrams, and
phrases) for the test sentences. FDA selects training
instances one by one updating the coverage of the
features already added to the training set in contrast
to the features found in the test sentence.
We also use a technique that we call dice, which
optimizes source language bigram coverage such
that the difficulty of aligning source and target fea-
tures is minimized. We define Dice?s coefficient
score as:
dice(x, y) =
2C(x, y)
C(x)C(y)
, (4)
where C(x, y) is the number of times x and y co-
occurr and C(x) is the count of observing x in
the selected training set. Given a test source sen-
tence, SU , we can estimate the goodness of a train-
ing sentence pair, (S, T ), by the sum of the align-
ment scores:
?dice(SU , S, T ) =
?
x?X(SU )
|T |?
j=1
?
y?Y (x)
dice(y, Tj)
|T | log |S|
,
(5)
where X(SU ) stores the features of SU and Y (x)
lists the tokens in feature x. The difficulty of word
aligning a pair of training sentences, (S, T ), can be
approximated by |S||T |. We use a normalization fac-
tor proportional to |T | log |S|.
The details of both of these techniques and further
results can be found in (Bicici and Yuret, 2011).
3.1 Moses Experiments on the Translation
Task
We have used FDA and dice algorithms to select
training sets for the out-of-domain challenge test
sets used in (Callison-Burch et al, 2011). The par-
allel corpus contains about 1.9 million training sen-
tences and the test set contain 3003 sentences. We
built separate Moses systems using all of the paral-
lel corpus for the language pairs en-de, de-en, en-
es, and es-en. We created training sets using all
en-de de-en en-es es-en
BLEU
ALL .1376 .2074 .2829 .2919
FDA .1363 .2055 .2824 .2892
dice .1374 .2061 .2834 .2857
words
ALL 47.4 49.6 52.8 50.4
FDA 7.9 8.0 8.7 8.2
dice 6.9 7.0 3.9 3.6
% ALL
FDA 17 16 16 16
dice 14 14 7.4 7.1
Table 1: Performance for the out-of-domain task
of (Callison-Burch et al, 2011). ALL corresponds to the
baseline system using all of the parallel corpus. words
list the size of the target words used in millions.
of the features of the test set to select training in-
stances. The results given in Table 1 show that we
can achieve similar BLEU performance using about
7% of the parallel corpus target words (200,000 in-
stances) using dice and about 16% using FDA. In the
out-of-domain translation task, we are able to reduce
the training set size to achieve a performance close
to the baseline. We may be able to achieve better
performance in this out-of-domain task as well as
explained in (Bicici and Yuret, 2011).
4 Graph Decoding for RegMT
We perform graph-based decoding by first generat-
ing a De Bruijn graph from the estimated y? (Cortes et
al., 2007) and then finding Eulerian paths with max-
imum path weight. We use four features when scor-
ing paths: (1) estimation weight from regression, (2)
language model score, (3) brevity penalty as found
by e?(lR?|s|/|path|) for lR representing the length ra-
tio from the parallel corpus and |path| representing
the length of the current path, (4) future cost as in
Moses (Koehn et al, 2007) and weights are tuned
using MERT (Och, 2003) on the de-en dev set.
We demonstrate that sparse L1 regularized regres-
sion performs better than L2 regularized regression.
Graph based decoding can provide an alternative to
state of the art phrase-based decoding system Moses
in translation domains with small vocabulary and
training set size.
4.1 Haitian Creole to English Translation Task
with RegMT
We have trained a Moses system for the Haitian Cre-
ole to English translation task, cleaned corpus, us-
325
ing the options as described in section 3.1. Moses
achieves 0.3186 BLEU on this task. We observed
that graph decoding performs better where target
coverage is high such that the bigrams used lead
to a connected graph. To increase the connec-
tivity, we have included Moses translations in the
training set and performed graph decoding with
RegMT. RegMT with L2 regularized regression
achieves 0.2708 BLEU with graph decoding and
lasso achieves 0.26 BLEU.
Moses makes use of a number of distortion pa-
rameters and lexical weights, which are estimated
using all of the parallel corpus. Thus, our Moses
translation achieves a better performance than graph
decoding with RegMT using 100 training instances
for translating each source test sentence.
5 System Combination with RegMT
We perform experiments on the system com-
bination task for the English-German, German-
English, English-Spanish, and Spanish-English lan-
guage pairs using the training corpus provided in
WMT?11 (Callison-Burch et al, 2011). We have
tokenized and lowercased each of the system out-
puts and combined these in a single N -best file per
language pair. We use these N -best lists for rerank-
ing by RegMT to select the best translation model.
Feature mappers used are 2-spectrum counting word
kernels (Taylor and Cristianini, 2004).
We rerank N -best lists by a linear combination of
the following scoring functions:
1. RegMT: Regression based machine translation
scores as found by Equation 2.
2. CBLEU: Comparative BLEU scores we obtain
by measuring the average BLEU performance
of each translation relative to the other systems?
translations in the N -best list.
3. LM: We calculate 5-gram language model
scores for each translation using the language
model trained over the target corpus provided
in the translation task.
Since we do not have access to the reference trans-
lations nor to the translation model scores each sys-
tem obtained for each sentence, we estimate trans-
lation model performance (CBLEU) by measuring
the average BLEU performance of each translation
relative to the other translations in the N -best list.
Thus, each possible translation in the N -best list is
BLEU scored against other translations and the av-
erage of these scores is selected as the CBLEU score
for the sentence. Sentence level BLEU score calcu-
lation avoids singularities in n-gram precisions by
taking the maximum of the match count and 12|si| for
|si| denoting the length of the source sentence si as
used in (Macherey and Och, 2007).
Table 2 presents reranking results on all of the lan-
guage pairs we considered, using RegMT, CBLEU,
and LM scores with the same combination weights
as above. We also list the performance of the best
model (Max) as well as the worst (Min). We are
able to achieve close or better BLEU scores in all
of the listed systems when compared with the per-
formance of the best translation system except for
the ht-en language pair. The lower performance in
the ht-en language pair may be due to having a sin-
gle best translation system that outperforms others
significantly. This happens for instance when an un-
constrained model use external resources to achieve
a significantly better performance than the second
best model. 2nd best in Table 2 lists the second best
model?s performance to estimate how much the best
model?s performance is better than the rest.
BLEU en-de de-en en-es es-en ht-en
Min .1064 .1572 .2174 .1976 .2281
Max .1727 .2413 .3375 .3009 .3708
2nd best .1572 .2302 .3301 .2973 .3288
Average .1416 .1997 .292 .2579 .2993
Oracle .2529 .3305 .4265 .4233 .4336
RegMT .1631 .2322 .3311 .3052 .3234
Table 2: System combination results.
RegMT model may prefer sentences with lower
BLEU, which can sometimes cause it to achieve a
lower BLEU performance than the best model. This
is clearly the case for en-de with 1.6 BLEU points
difference with the second best model performance
and for de-en task with 1.11 BLEU points differ-
ence. Also this observation holds for en-es with
0.74 BLEU points difference and for ht-en with 4.2
BLEU points difference. For es-en task, there is 0.36
BLEU points difference with the second best model
and these models likely to complement each other.
326
The existence of complementing SMT models is
important for the reranking approach to achieve a
performance better than the best model, as there is
a need for the existence of a model performing bet-
ter than the best model on some test sentences. We
can use the competitive SMT model to achieve the
performance of the best with a guarantee even when
a single model is dominating the rest (Bicici and
Kozat, 2010). For competing translation systems
in an on-line machine translation setting adaptively
learning of model weights can be performed based
on the previous transaltion performance (Bicici and
Kozat, 2010).
6 Target F1 as a Performance Evaluation
Metric
We use target sentence F1 measure over the tar-
get features as a translation performance evaluation
metric. We optimize the parameters of the RegMT
model with the F1 measure comparing the target
vector with the estimate we get from the RegMT
model. F1 measure uses the 0/1-class predictions
over the target feature with the estimate vector,
?Y (y?). Let TP be the true positive, TN the true neg-
ative, FP the false positive, and FN the false negative
rates, we use the following measures for evaluation:
prec =
TP
TP + FP
, BER = ( FPTN+FP +
FN
TP+FN )/2 (6)
rec =
TP
TP + FN
, F1 =
2?prec?rec
prec+rec (7)
where BER is the balanced error rate, prec is pre-
cision, and rec is recall. The evaluation techniques
measure the effectiveness of the learning models in
identifying the features of the target sentence mak-
ing minimal error to increase the performance of the
decoder and its translation quality.
We use gapped word sequence kernels (Taylor
and Cristianini, 2004) when using F1 for evaluating
translations since a given translation system may not
be able to translate a given word but can correctly
identify the surrounding phrase. For instance, let the
reference translation be the following sentence:
a sound compromise has been reached
Some possible translations for the reference are
given in Table 3 together with their BLEU (Papineni
et al, 2001) and F1 scores for comparison. F1 score
does not have a brevity penalty but a brief transla-
tion is penalized by a low recall value. We use up
to 3 tokens as gaps. F1 measure is able to increase
the ranking of Trans4 by using a gapped sequence
kernel, which can be preferrable to Trans3.
We note that a missing token corresponds to vary-
ing decreases in the n-gram precision used in the
BLEU score. A sentence containing m tokens has
m 1-grams, m?1 2-grams, andm?n+1 n-grams.
A missing token degrades the performance more in
higher order n-gram precision values. A missing to-
ken decreases n-gram precision by 1m for 1-grams
and by nm?n+1 for n-grams. Based on this obser-
vation, we use F1 measure with gapped word se-
quence kernels to evaluate translations. Gapped fea-
tures allows us to consider the surrounding phrase
for a missing token as present in the translation.
Let the reference sentence be represented with
a b c d e f where a-f, x, y, z correspond to to-
kens in the sentence. Then, Trans3 has the form
a b x y f, and Trans4 has the form a c y f.
Then, F1 ranks Trans4 higher than Trans3 for orders
greater than 3 as there are two consecutive word er-
rors in Trans3. F1 can also prefer a missing token
rather than a word error as we see by comparing
Trans4 and Trans5 and it can still prefer contigu-
ity over a gapped sequence as we see by comparing
Trans5 and Trans6 in Table 3.
We calculate the correlation of F1 with BLEU on
the en-de development set. We use 5-grams with the
F1 measure as this increases the correlation with 4-
gram BLEU. Table 4 gives the correlation results us-
ing both Pearson?s correlation score and Spearman?s
correlation score. Spearman?s correlation score is a
better metric for comparing the relative orderings.
Metric No gaps Gaps
Pearson .8793 .7879
Spearman .9068 .8144
Table 4: F1 correlation with 4-gram BLEU using blended
5-gram gapped word sequence features on the develop-
ment set.
7 Contributions
We present the results we obtain using our RegMT
system, which uses transductive regression tech-
niques to learn mappings between source and tar-
327
Format BLEU F1
Ref: a sound compromise has been reached a b c d e f 4-grams 3-grams 4-grams 5-grams
Trans1: a sound agreement has been reached a b x d e f .2427 .6111 .5417 .5
Trans2: a compromise has reached a c d f .137 .44 .3492 .3188
Trans3: a sound agreement is reached a b x y f .1029 .2 .1558 .1429
Trans4: a compromise is reached a c y f .0758 .2 .1587 .1449
Trans5: a good compromise is reached a z c y f .0579 .1667 .1299 .119
Trans6: a good compromise is been a z c y e .0579 .2 .1558 .1429
Table 3: BLEU vs. F1 on sample sentence translation task.
get features of given parallel corpora and use these
mappings to generate machine translation outputs.
We also present translation results using our train-
ing instance selection methods, translation results
using graph decoding, system combination results
with RegMT, and performance evaluation with F1
measure over target features. RegMT work builds
on our previous regression-based machine transla-
tion results (Bicici and Yuret, 2010) especially with
instance selection and additional graph decoding ca-
pability.
References
Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings of
the ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized re-
gression for reranking and system combination in ma-
chine translation. In Proceedings of the ACL 2010
Joint Fifth Workshop on Statistical Machine Transla-
tion and Metrics MATR, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Ergun Bicici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the EMNLP 2011 Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, England, July.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2011. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation. Edinburgh, England, July.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2007. A general regression framework for learn-
ing string-to-string mappings. In Gokhan H. Bakir,
Thomas Hofmann, and Bernhard Sch editors, Predict-
ing Structured Data, pages 143?168. The MIT Press,
September.
Trevor Hastie, Jonathan Taylor, Robert Tibshirani, and
Guenther Walther. 2006. Forward stagewise regres-
sion and the monotone lasso. Electronic Journal of
Statistics, 1.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference and Prediction. Springer-Verlag,
2nd edition.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Assoc. for Computational Linguistics,
pages 177?180, Prague, Czech Republic, June.
Wolfgang Macherey and Franz Josef Och. 2007. An
empirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 986?995, Prague, Czech Republic,
June. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. Association for Com-
putational Linguistics, 1:160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Nicolas Serrano, Jesus Andres-Ferrer, and Francisco
Casacuberta. 2009. On a kernel regression approach
to machine translation. In Iberian Conference on Pat-
tern Recognition and Image Analysis, pages 394?401.
J. Shawe Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
328
Robert J. Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of the Royal Statistical
Society, Series B, 58(1):267?288.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155?158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine trans-
lation. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Companion
Volume, Short Papers, pages 185?188, Rochester, New
York, April. Association for Computational Linguis-
tics.
329
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 181?190,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Probabilistic Modeling of Joint-context in Distributional Similarity
Oren Melamud
?
, Ido Dagan
?
, Jacob Goldberger
?
, Idan Szpektor
?
, Deniz Yuret
?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
? Koc? University
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com, dyuret@ku.edu.tr
Abstract
Most traditional distributional similarity
models fail to capture syntagmatic patterns
that group together multiple word features
within the same joint context. In this work
we introduce a novel generic distributional
similarity scheme under which the power
of probabilistic models can be leveraged
to effectively model joint contexts. Based
on this scheme, we implement a concrete
model which utilizes probabilistic n-gram
language models. Our evaluations sug-
gest that this model is particularly well-
suited for measuring similarity for verbs,
which are known to exhibit richer syntag-
matic patterns, while maintaining compa-
rable or better performance with respect
to competitive baselines for nouns. Fol-
lowing this, we propose our scheme as a
framework for future semantic similarity
models leveraging the substantial body of
work that exists in probabilistic language
modeling.
1 Introduction
The Distributional Hypothesis is commonly
phrased as ?words which are similar in meaning
occur in similar contexts? (Rubenstein and Good-
enough, 1965). Distributional similarity models
following this hypothesis vary in two major as-
pects, namely the representation of the context and
the respective computational model. Probably the
most prominent class of distributional similarity
models represents context as a vector of word fea-
tures and computes similarity using feature vector
arithmetics (Lund and Burgess, 1996; Turney et
al., 2010). To construct the feature vectors, the
context of each target word token
1
, which is com-
monly a word window around it, is first broken
1
We use word type to denote an entry in the vocabulary,
and word token for a particular occurrence of a word type.
into a set of individual independent words. Then
the weights of the entries in the word feature vec-
tor capture the degree of association between the
target word type and each of the individual word
features, independently of one another.
Despite its popularity, it was suggested that
the word feature vector approach misses valu-
able information, which is embedded in the co-
location and inter-relations of words (e.g. word
order) within the same context (Ruiz-Casado et al.,
2005). Following this motivation, Ruiz-Casado
et al. (2005) proposed an alternative composite-
feature model, later adopted in (Agirre et al.,
2009). This model adopts a richer context repre-
sentation by considering entire word window con-
texts as features, while keeping the same compu-
tational vector-based model. Although showing
interesting potential, this approach suffers from a
very high-dimensional feature space resulting in
data sparseness problems. Therefore, it requires
exceptionally large learning corpora to consider
large windows effectively.
A parallel line of work adopted richer context
representations as well, with a different compu-
tational model. These works utilized neural net-
works to learn low dimensional continuous vector
representations for word types, which were found
useful for measuring semantic similarity (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
These vectors are trained by optimizing the pre-
diction of target words given their observed con-
texts (or variants of this objective). Most of these
models consider each observed context as a joint
set of context words within a word window.
In this work we follow the motivation in the pre-
vious works above to exploit richer joint-context
representations for modeling distributional simi-
larity. Under this approach the set of features in
the context of each target word token is consid-
ered to jointly reflect on the meaning of the target
word type. To further facilitate this type of mod-
181
eling we propose a novel probabilistic computa-
tional scheme for distributional similarity, which
leverages the power of probabilistic models and
addresses the data sparseness challenge associated
with large joint-contexts. Our scheme is based on
the following probabilistic corollary to the distri-
butional hypothesis:
(1)?words are similar in meaning if
they are likely to occur in the same contexts?
To realize this corollary, our distributional sim-
ilarity scheme assigns high similarity scores to
word pairs a and b, for which a is likely in the con-
texts that are observed for b and vice versa. The
scheme is generic in the sense that various under-
lying probabilistic models can be used to provide
the estimates for the likelihood of a target word
given a context. This allows concrete semantic
similarity models based on this scheme to lever-
age the capabilities of probabilistic models, such
as established language models, which typically
address the modeling of joint-contexts.
We hypothesize that an underlying model that
could capture syntagmatic patterns in large word
contexts, yet is flexible enough to deal with data
sparseness, is desired. It is generally accepted
that the semantics of verbs in particular are cor-
related with their syntagmatic properties (Levin,
1993; Hanks, 2013). This provides grounds to ex-
pect that such model has the potential to excel for
verbs. To capture syntagmatic patterns, we choose
in this work standard n-gram language models as
the basis for a concrete model implementing our
scheme. This choice is inspired by recent work on
learning syntactic categories (Yatbaz et al., 2012),
which successfully utilized such language mod-
els to represent word window contexts of target
words. However, we note that other richer types
of language models, such as class-based (Brown
et al., 1992) or hybrid (Tan et al., 2012), can be
seamlessly integrated into our scheme.
Our evaluations suggest that our model is in-
deed particularly advantageous for measuring se-
mantic similarity for verbs, while maintaining
comparable or better performance with respect to
competitive baselines for nouns.
2 Background
In this section we provide additional details re-
garding previous works that we later use as base-
lines in our evaluations.
To implement the composite-feature approach,
Ruiz-Casado et al. (2005) used a Web search en-
gine to compare entire window contexts of target
word types. For example, a single feature that
could be retrieved this way for the target word like
is ?Children cookies and milk?. They showed
good results on detecting synonyms in the 80
multiple-choice questions TOEFL test. Agirre et
al. (2009) constructed composite-feature vectors
using an exceptionally large 1.6 Teraword learn-
ing corpus. They found that this approach out-
performs the traditional independent feature vec-
tor approach on a subset of the WordSim353 test-
set (Finkelstein et al., 2001), which is designed to
test the more restricted relation of semantic simi-
larity (to be distinguished from looser semantic re-
latedness). We are not aware of additional works
following this approach, of using entire word win-
dows as features.
Neural networks have been used to train lan-
guage models that are based on low dimensional
continuous vector representations for word types,
also called word embeddings (Bengio et al., 2003;
Mikolov et al., 2010). Although originally de-
signed to improve language models, later works
have shown that such word embeddings are useful
in various other NLP tasks, including measuring
semantic similarity with vector arithmetics (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
Specifically, the recent work by Mikolov et al.
(2013) introduced the CBOW and Skip-gram mod-
els, achieving state-of-the-art results in detecting
semantic analogies. The CBOW model is trained
to predict a target word given the set of context
words in a word window around it, where this
context is considered jointly as a bag-of-words.
The Skip-gram model is trained to predict each of
the context words independently given the target
word.
3 Probabilistic Distributional Similarity
3.1 Motivation
In this section we briefly demonstrate the bene-
fits of considering joint-contexts of words. As an
illustrative example, we note that the target words
like and surround may share many individual word
features such as ?school? and ?campus? in the sen-
tences ?Mary?s son likes the school campus? and
?The forest surrounds the school campus?. This
potentially implies that individual features may
not be sufficient to accurately reflect the difference
182
between such words. Alternatively, we could use
the following composite features to model the con-
text of these words, ?Mary?s son the school
campus? and ?The forest the school campus?.
This would discriminate better between like and
surround. However, in this case sentences such as
?Mary?s son likes the school campus? and ?John?s
son loves the school campus? will not provide any
evidence to the similarity between like and love,
since ?Mary?s son the school campus? is a dif-
ferent feature than ?John?s son the school cam-
pus?.
In the remainder of this section we propose
a modeling scheme and then a concrete model,
which can predict that like and love are likely to
occur in each other?s joint-contexts, whereas like
and surround are not, and then assign similarity
scores accordingly.
3.2 The probabilistic similarity scheme
We now present a computational scheme that re-
alizes our proposed corollary (1) to the distribu-
tional hypothesis and facilitates robust probabilis-
tic modeling of joint contexts. First, we slightly
rephrase this corollary as follows: ?words a and
b are similar in meaning if word b is likely in
the contexts of a and vice versa?. We denote the
probability of an occurrence of a target word b
given a joint-context c by p(b|c). For example,
p(love|?Mary?s son the school campus?) is the
probability of the word love to be the filler of the
?place-holder? in the given joint-context ?Mary?s
son the school campus?. Similarly, we denote
p(c|a) as the probability of a joint-context c given
a word a, which fills its place-holder. We now
propose p
sim
(b|a) to reflect how likely b is in the
joint-contexts of a. We define this measure as:
(2)p
sim
(b|a) =
?
c
p(c|a) ? p(b|c)
where c goes over all possible joint-contexts in the
language.
To implement this measure we need to find
an efficient estimate for p
sim
(b|a). The most
straight forward strategy is to compute sim-
ple corpus count ratio estimates for p(b|c) and
p(c|a), denoted p
#
(b|c) =
count(b,c)
count(?,c)
and
p
#
(c|a) =
count(a,c)
count(a,?)
. However, when consid-
ering large joint-contexts for c, this approach be-
comes similar to the composite-feature approach
since it is based on co-occurrence counts of tar-
get words with large joint-contexts. Therefore, we
expect in this case to encounter the data sparse-
ness problems mentioned in Section 1, where se-
mantically similar word type pairs that share only
few or no identical joint-contexts yield very low
p
sim
(b|a) estimates.
To address the data sparseness challenge and
adopt more advanced context modeling, we aim to
use a more robust underlying probabilistic model
? for our scheme and denote the probabilities es-
timated by this model by p
?
(b|c) and p
?
(c|a). We
note that contrary to the count ratio model, given a
robust model ?, such as a language model, p
?
(b|c)
and p
?
(c|a) can be positive even if the target words
b and a were not observed with the joint-context c
in the learning corpus.
While using p
?
(b|c) and p
?
(c|a) to estimate the
value of p
sim
(b|a) addresses the sparseness chal-
lenge, it introduces a computational challenge.
This is because estimating p
sim
(b|a) would re-
quire computing the sum over all of the joint-
contexts in the learning corpus regardless of
whether they were actually observed with either
word type a or b. For that reason we choose a
middle ground approach, estimating p(b|c) with
?, while using a count ratio estimate for p(c|a),
as follows. We denote the collection of all joint-
contexts observed for the target word a in the
learning corpus by C
a
, where |C
a
|= count(a, ?).
For example, C
like
= {c
1
=?Mary?s son the
school campus?, c
2
=?John?s daughter to read
poetry?,...}. We note that this collection is a multi-
set, where the same joint-context can appear more
than once.
We now approximate p
sim
(b|a) from Equation
(2) as follows:
(3)
p?
sim
?
(b|a) =
?
c
p
#
(c|a) ? p
?
(b|c) =
1
|C
a
|
?
?
c?C
a
p
?
(b|c)
We note that this formulation still addresses
sparseness of data by using a robust model, such as
a language model, to estimate p
?
(b|c). At the same
time it requires our model to sum only over the
joint-contexts in the collection C
a
, since contexts
not observed for a yield p
#
(c|a) = 0. Even so,
since the size of these context collections grows
linearly with the corpus size, considering all ob-
served contexts may still present a scalability chal-
lenge. Nevertheless, we expect our approximation
p?
sim
?
(b|a) to converge with a reasonable sample
183
size from a?s joint-contexts. Therefore, in order
to bound computational complexity, we limit the
size of the context collections used to train our
model to a maximum of N by randomly sampling
N entries from larger collections. In all our ex-
periments we use N = 10, 000. Higher values
of N yielded negligible performances differences.
Overall we see that our model estimates p?
sim
?
(b|a)
as the average probability predicted for b in (a
large sample of) the contexts observed for a.
Finally, we define our similarity measure for tar-
get word types a and b:
(4)sim
?
(a, b) =
?
p?
sim
?
(b|a) ? p?
sim
?
(a|b)
As intended, this similarity measure promotes
word pairs in which both b is likely in the con-
texts of a and vice versa. Next, we describe a
model which implements this scheme with an n-
gram language model as a concrete choice for ?.
3.3 Probabilistic similarity using language
models
In this work we focus on the word window context
representation, which is the most common. We
define a word window of order k around a target
word as a window with up to k words to each side
of the target word, not crossing sentence bound-
aries. The word window does not include the tar-
get word itself, but rather a ?place-holder? for it.
Since word windows are sequences of words,
probabilistic language models are a natural choice
of a model ? for estimating p
?
(b|c). Language
models assign likelihood estimates to sequences
of words using approximation strategies. In
this work we choose n-gram language models,
aiming to capture syntagmatic properties of the
word contexts, which are sensitive to word or-
der. To approximate the probability of long se-
quences of words, n-gram language models com-
pute the product of the estimated probability of
each word in the sequence conditioned on at most
the n ? 1 words preceding it. Furthermore, they
use ?discounting? methods to improve the esti-
mates of conditional probabilities when learning
data is sparse. Specifically, in this work we use
the Kneser-Ney n-gram model (Kneser and Ney,
1995).
We compute p
?
(b|c) as follows:
(5)p
?
(b|c) =
p
?
(b, c)
p
?
(c)
where p
?
(b, c) is the probability of the word se-
quence comprising the word window c, in which
the word b fills the place-holder. For instance, for
c = ?I drive my to work every? and b = car,
p
?
(b, c) is the estimated language model probabil-
ity of ?I drive my car to work every?. p
?
(c) is the
marginal probability of p
?
(?, c) over all possible
words in the vocabulary.
2
4 Experimental Settings
Although sometimes used interchangeably, it is
common to distinguish between semantic simi-
larity and semantic relatedness (Budanitsky and
Hirst, 2001; Agirre et al., 2009). Semantic simi-
larity is used to describe ?likeness? relations, such
as the relations between synonyms, hypernym-
hyponyms, and co-hyponyms. Semantic relat-
edness refers to a broader range of relations in-
cluding also meronymy and various other asso-
ciative relations as in ?pencil-paper? or ?penguin-
Antarctica?. In this work we focus on semantic
similarity and evaluate all compared methods on
several semantic similarity tasks.
Following previous works (Lin, 1998; Riedl and
Biemann, 2013) we use Wordnet to construct large
scale gold standards for semantic similarity evalu-
ations. We perform the evaluations separately for
nouns and verbs to test our hypothesis that our
model is particularly well-suited for verbs. To fur-
ther evaluate our results on verbs we use the verb
similarity test-set released by (Yang and Powers,
2006), which contains pairs of verbs associated
with semantic similarity scores based on human
judgements.
4.1 Compared methods
We compare our model with a traditional fea-
ture vector model, the composite-feature model
(Agirre et al., 2009), and the recent state-of-the-art
word embedding models, CBOW and Skip-gram
(Mikolov et al., 2013), all trained on the same
learning corpus and evaluated on equal grounds.
We denote the traditional feature vector baseline
by IFV
W?k
, where IFV stands for ?Independent-
Feature Vector? and k is the order of the con-
text word window considered. Similarly, we
2
Computing p
?
(c) by summing over all possible place-
holder filler words, as we did in this work, is computation-
ally intensive. However, this can be done more efficiently
by implementing customized versions of (at least some) n-
gram language models with little computational overhead,
e.g. by counting the learning corpus occurrences of n-gram
templates, in which one of the elements matches any word.
184
denote the composite-feature vector baseline by
CFV
W?k
, where CFV stands for ?Composite-
Feature Vector?. This baseline constructs
traditional-like feature vectors, but considers en-
tire word windows around target word tokens as
single features. In both of these baselines we use
Cosine as the vector similarity measure, and posi-
tive pointwise mutual information (PPMI) for the
feature vector weights. PPMI is a well-known
variant of pointwise mutual information (Church
and Hanks, 1990), and the combination of Cosine
with PPMI was shown to perform particularly well
in (Bullinaria and Levy, 2007).
We denote Mikolov?s CBOW and Skip-gram
baseline models by CBOW
W?k
and SKIP
W?k
respectively, where k denotes again the order of
the window used to train these models. We used
Mikolov?s word2vec utility
3
with standard param-
eters (600 dimensions, negative sampling 15) to
learn the word embeddings, and Cosine as the vec-
tor similarity measure between them.
As the underlying probabilistic language model
for our method we use the Berkeley implementa-
tion
4
(Pauls and Klein, 2011) of the Kneser-Ney
n-gram model with the default discount parame-
ters. We denote our model PDS
W?k
, where PDS
stands for ?Probabilistic Distributional Similar-
ity?, and k is the order of the context word win-
dow. In order to avoid giving our model an un-
fair advantage of tuning the order of the language
model n as an additional parameter, we use a fixed
n = k + 1. This means that the conditional prob-
abilities that our n-gram model learns consider a
scope of up to half the size of the window, which
is the distance in words between the target word
and either end of the window. We note that this
is the smallest reasonable value for n, as smaller
values effectively mean that there will be context
words within the window that are more than n
words away from the target word, and therefore
will not be considered by our model.
As learning corpus we used the first CD of
the freely available Reuters RCV1 dataset (Rose
et al., 2002). This learning corpus contains ap-
proximately 100M words, which is comparable in
size to the British National Corpus (BNC) (As-
ton, 1997). We first applied part-of-speech tag-
ging and lemmatization to all words. Then we
represented each word w in the corpus as the pair
3
http://code.google.com/p/word2vec
4
http://code.google.com/p/berkeleylm/
[pos(w), lemma(w)], where pos(w) is a coarse-
grained part-of-speech category and lemma(w) is
the lemmatized form of w. Finally, we converted
every pair [pos(w), lemma(w)] that occurs less
than 100 times in the learning corpus to the pair
[pos(w), ? ], which represents all rare words of the
same part-of-speech tag. Ignoring rare words is a
common practice used in order to clean up the cor-
pus and reduce the vocabulary size (Gorman and
Curran, 2006; Collobert and Weston, 2008).
The above procedure resulted in a word vocabu-
lary of 27K words. From this vocabulary we con-
structed a target verb set with over 2.5K verbs by
selecting all verbs that exist in Wordnet (Fellbaum,
2010). We repeated this procedure to create a tar-
get noun set with over 9K nouns. We used our
learning corpus for all compared methods and had
them assign a semantic similarity score for every
pair of verbs and every pair of nouns in these tar-
get sets. These scores were later used in all of our
evaluations.
4.2 Wordnet evaluation
There is a shortage of large scale test-sets for se-
mantic similarity. Popular test-sets such as Word-
Sim353 and the TOEFL synonyms test contain
only 353 and 80 test items respectively, and there-
fore make it difficult to obtain statistically signif-
icant results. To automatically construct larger-
scale test-sets for semantic similarity, we sampled
large target word subsets from our corpus and used
Wordnet as a gold standard for their semantically
similar words, following related previous evalua-
tions (Lin, 1998; Riedl and Biemann, 2013). We
constructed two test-sets for our primary evalua-
tion, one for verb similarity and another for noun
similarity.
To perform the verb similarity evaluation, we
randomly sampled 1,000 verbs from the target
verb set, where the probability of each verb to be
sampled is set to be proportional to its frequency in
the learning corpus. Next, for each sampled verb
a we constructed a Wordnet-based gold standard
set of semantically similar words. In this set each
verb a
?
is annotated as a ?synonym? of a if at least
one of the senses of a
?
is a synonym of any of the
senses of a. In addition, each verb a
?
is annotated
as a ?semantic neighbor? of a if at least one of the
senses of a
?
is a synonym, co-hyponym, or a di-
rect hypernym/hyponym of any of the senses of a.
We note that by definition all verbs annotated as
185
synonyms of a are annotated as semantic neigh-
bors as well. Next, per each verb a and an evalu-
ated method, we generated a ranked list of all other
verbs, which was induced according to the similar-
ity scores of this method.
Finally, we evaluated the compared methods
on two tasks, ?synonym detection? and ?seman-
tic neighbor detection?. In the synonym detection
task we evaluated the methods? ability to retrieve
as much verbs annotated in our gold standard as
?synonyms?, in the top-n entries of their ranked
lists. Similarly, we evaluated all methods on the
?semantic neighbors? task. The synonym detec-
tion task is designed to evaluate the ability of the
compared methods to identify a more restrictive
interpretation of semantic similarity, while the se-
mantic neighbor detection task does the same for
a somewhat broader interpretation.
We repeated the above procedure for sam-
pling 1,000 target nouns, constructing the noun
Wordnet-based gold standards and evaluating on
the two semantic similarity tasks.
4.3 VerbSim evaluation
The publicly available VerbSim test-set contains
130 verb pairs, each annotated with an average of
6 human judgements of semantic similarity (Yang
and Powers, 2006). We extracted a 107 pairs sub-
set of this dataset for which all verbs are in our
learning corpus. We followed works such as (Yang
and Powers, 2007; Agirre et al., 2009) and com-
pared the Spearman correlations between the verb-
pair similarity scores assigned by the compared
methods and the manually annotated scores in this
dataset.
5 Results
For each method and verb a in our 1,000 tested
verbs, we used the Wordnet gold standard to com-
pute the precision at top-1, top-5 and top-10 of the
ranked list generated by this method for a. We
then computed mean precision values averaged
over all verbs for each of the compared methods,
denoted as P@1, P@5 and P@10. The detailed
report of P@10 results is omitted for brevity, as
they behave very similarly to P@5. We varied the
context window order used by all methods to test
its effect on the results. We measured the same
metrics for nouns.
The results of our Wordnet-based 1,000 verbs
evaluation are presented in the upper part of Fig-
ure 1. The results show significant improvement
of our method over all baselines, with a margin
between 2 to 3 points on the synonyms detection
task and 5 to 7 points on the semantic neighbors
detection task. Our best performing configura-
tions are PDS
W?3
and PDS
W?4
, outperform-
ing all other baselines on both tasks and in all pre-
cision categories. This difference is statistically
significant at p < 0.001 using a paired t-test in all
cases except for the P@1 in the synonyms detec-
tion task. Within the baselines, the composite fea-
ture vector (CFV) performs somewhat better than
the independent feature vector (IFV) baseline, and
both methods perform best around window order
of two, with gradual decline for larger windows.
The word embedding baselines, CBOW and SKIP,
perform comparably to the feature vector base-
lines and to one another, with best performance
achieved around window order of four.
When gradually increasing the context window
order within the range of up to 4 words, our PDS
model shows improvement. This is in contrast to
the feature vector baselines, whose performance
declines for context window orders larger than 2.
This suggests that our approach is able to take ad-
vantage of larger contexts in comparison to stan-
dard feature vector models. The decline in perfor-
mance for the independent feature vector baseline
(IFV) may be related to the fact that independent
features farther away from the target word are gen-
erally more loosely related to it. This seems con-
sistent with previous works, where narrow win-
dows of the order of two words performed well
(Bullinaria and Levy, 2007; Agirre et al., 2009;
Bruni et al., 2012) and in particular so when eval-
uating semantic similarity rather than relatedness.
On the other hand, the decline in performance for
the composite feature vector baseline (CFV) may
be attributed to the data sparseness phenomenon
associated with larger windows. The performance
of the word embedding baselines (CBOW and
SKIP) starts declining very mildly only for win-
dow orders larger than 4. This might be attributed
to the fact that these models assign lower weights
to context words the farther away they are from the
center of the window.
The results of our Wordnet-based 1,000 nouns
evaluation are presented in the lower part of Fig-
ure 1. These results are partly consistent with the
results achieved for verbs, but with a couple of
notable differences. First, though our model still
186
Figure 1: Mean precision scores as a function of window order, obtained against the Wordnet-based gold
standard, on both the verb and noun test-sets with both the synonyms and semantic neighbor detection
tasks. ?P@n? stands for precision in the top-n words of the ranked lists. Note that the Y-axis scale varies
between graphs.
outperforms or performs comparably to all other
baselines, in this case the advantage of our model
over the feature vector baselines is much more
moderate and not statistically significant. Second,
the word embedding baselines generally perform
worst (with CBOW performing a little better than
SKIP), and our model outperforms them in both
P@5 and P@10 with a margin of around 2 points
for the synonyms detection task and 3-4 points for
the neighbor detection task, with statistical signif-
icance at p < 0.001.
Next, to reconfirm the particular applicability
of our model to verb similarity as apparent from
the Wordnet evaluation, we performed the Verb-
Sim evaluation and present the results in Table 1.
We compared the Spearman correlation obtained
for the top-performing window order of each of
the evaluated methods in the Wordnet verbs eval-
uation. We present two sets of results. The ?all
scores? results follow the standard evaluation pro-
cedure, considering all similarity scores produced
by each method. In the ?top-100 scores? results,
for each method we converted to zero the scores
that it assigned to word pairs, where neither of
the words is in the top-100 most similar words
of the other. Then we performed the evaluation
with these revised scores. This procedure focuses
on evaluating the quality of the methods? top-
100 ranked word lists. The results show that our
method outperforms all baselines by a nice mar-
187
Method All scores top-100 scores
PDS W-4 0.616 0.625
CFV W-2 0.477 0.497
IFV W-2 0.467 0.546
SKIP W-4 0.469 0.512
CBOW W-5 0.528 0.469
Table 1: Spearman correlation values obtained for
the VerbSim evaluation. Each method was evalu-
ated with the optimal window order found in the
Wordnet verbs evaluation.
gin of more than 8 points with the score of 0.616
and 0.625 for the ?all scores? and ?top-100 scores?
evaluations respectively. Though not statistically
significant, due to the small test-set size, these re-
sults support the ones from the Wordnet evalu-
ation, suggesting that our model performs better
than the baselines on measuring verb similarity.
In summary, our results suggest that in lack of a
robust context modeling scheme it is hard for dis-
tributional similarity models to effectively lever-
age larger word window contexts for measuring
semantic similarity. It appears that this is some-
what less of a concern when it comes to noun sim-
ilarity, as the simple feature vector models reach
near-optimal performance with small word win-
dows of order 2, but it is an important factor for
verb similarity. In his recent book, Hanks (2013)
claims that contrary to nouns, computational mod-
els that are to capture the meanings of verbs must
consider their syntagmatic patterns in text. Our
particularly good results on verb similarity sug-
gest that our modeling approach is able to cap-
ture such information in larger context windows.
We further conjecture that the reason the word em-
bedding baselines did not do as well as our model
on verb similarity might be due to their particular
choice of joint-context formulation, which is not
sensitive to word order. However, these conjec-
tures should be further validated with additional
evaluations in future work.
6 Future Directions
In this paper we investigated the potential for im-
proving distributional similarity models by model-
ing jointly the occurrence of several features under
the same context. We evaluated several previous
works with different context modeling approaches
and suggest that the type of the underlying con-
text modeling may have significant effect on the
performance of the semantic model. Further-
more, we introduced a generic probabilistic distri-
butional similarity approach, which can leverage
the power of established probabilistic language
models to effectively model joint-contexts for the
purpose of measuring semantic similarity. Our
concrete model utilizing n-gram language models
outperforms several competitive baselines on se-
mantic similarity tasks, and appears to be partic-
ularly well-suited for verbs. In the remainder of
this section we describe some potential future di-
rections that can be pursued.
First, the performance of our generic scheme
is largely inherited from the nature of its under-
lying language model. Therefore, we see much
potential in exploring the use of other types of
language models, such as class-based (Brown et
al., 1992), syntax-based (Pauls and Klein, 2012)
or hybrid (Tan et al., 2012). Furthermore, a sim-
ilar approach to ours could be attempted in word
embedding models. For instance, our syntagmatic
joint-context modeling approach could be investi-
gated by word embedding models to generate bet-
ter embeddings for verbs.
Another direction relates to the well known ten-
dency of many words, and particularly verbs, to
assume different meanings (or senses) under dif-
ferent contexts. To address this phenomenon con-
text sensitive similarity and inference models have
been proposed (Dinu and Lapata, 2010; Melamud
et al., 2013). Similarly to many semantic similar-
ity models, our current model aggregates informa-
tion from all observed contexts of a target word
type regardless of its different senses. However,
we believe that our approach is well suited to ad-
dress context sensitive similarity with proper en-
hancements, as it considers joint-contexts that can
more accurately disambiguate the meaning of tar-
get words. As an example, it is possible to con-
sider the likelihood of word b to occur in a subset
of the contexts observed for word a, which is bi-
ased towards a particular sense of a.
Finally, we note that our model is not a classic
vector space model and therefore common vec-
tor composition approaches (Mitchell and Lap-
ata, 2008) cannot be directly applied to it. In-
stead, other methods, such as similarity of com-
positions (Turney, 2012), should be investigated to
extend our approach for measuring similarity be-
tween phrases.
188
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT) and the Scien-
tific and Technical Research Council of Turkey
(T
?
UB
?
ITAK, Grant Number 112E277).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL. Association for Computational Lin-
guistics.
Guy Aston. 1997. The BNC Handbook Exploring the
British National Corpus with SARA Guy Aston and
Lou Burnard.
Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of ACL.
Alexander Budanitsky and Graeme Hirst. 2001.
Semantic distance in wordnet: An experimental,
application-oriented evaluation of five measures. In
Workshop on WordNet and Other Lexical Resources.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510?526.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP.
Christiane Fellbaum. 2010. WordNet. Springer.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web. ACM.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Pro-
ceedings of ACL.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing. IEEE.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago press.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of ACL.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N -Gram Language Models. In Proceedings of ACL.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings
of ACL.
Martin Riedl and Chris Biemann. 2013. Scaling to
large?3 data: An efficient and effective method to
compute distributional thesauri. In Proceedings of
EMNLP.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volume 1-from Yester-
day?s News to Tomorrow?s Language Resources. In
LREC.
189
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Using context-window overlapping
in synonym discovery and ontology extension. Pro-
ceedings of RANLP.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2012. A scalable distributed syntactic, semantic,
and lexical language model. Computational Lin-
guistics, 38(3):631?671.
Peter D. Turney, Patrick Pantel, et al. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of artificial intelligence research.
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Researc,
44(1):533?585, May.
Dongqiang Yang and David M. W. Powers. 2006. Verb
similarity on the taxonomy of wordnet. In the 3rd
International WordNet Conference (GWC-06).
Dongqiang Yang and David M. W. Powers. 2007.
An empirical investigation into grammatically con-
strained contexts in predicting distributional similar-
ity. In Australasian Language Technology Workshop
2007, pages 117?124.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
EMNLP.
190
