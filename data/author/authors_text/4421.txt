Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 85?88,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Dictionary Definitions based Homograph Identification using a  
Generative Hierarchical Model 
 
 
Anagha Kulkarni Jamie Callan 
Language Technologies Institute 
School of Computer Science, Carnegie Mellon University 
5000 Forbes Ave, Pittsburgh, PA 15213, USA 
{anaghak, callan}@cs.cmu.edu 
 
 
 
 
 
 
Abstract 
A solution to the problem of homograph 
(words with multiple distinct meanings) iden-
tification is proposed and evaluated in this pa-
per. It is demonstrated that a mixture model 
based framework is better suited for this task 
than the standard classification algorithms ? 
relative improvement of 7% in F1 measure 
and 14% in Cohen?s kappa score is observed.  
1 Introduction 
Lexical ambiguity resolution is an important re-
search problem for the fields of information re-
trieval and machine translation (Sanderson, 2000; 
Chan et al, 2007). However, making fine-grained 
sense distinctions for words with multiple closely-
related meanings is a subjective task (Jorgenson, 
1990; Palmer et al, 2005), which makes it difficult 
and error-prone.  Fine-grained sense distinctions 
aren?t necessary for many tasks, thus a possibly-
simpler alternative is lexical disambiguation at the 
level of homographs (Ide and Wilks, 2006).  
Homographs are a special case of semantically 
ambiguous words:  Words that can convey multi-
ple distinct meanings. For example, the word bark 
can imply two very different concepts ? ?outer 
layer of a tree trunk?, or, ?the sound made by a 
dog? and thus is a homograph. Ironically, the defi-
nition of the word ?homograph? is itself ambiguous 
and much debated; however, in this paper we con-
sistently use the above definition.  
If the goal is to do word-sense disambiguation 
of homographs in a very large corpus, a manually-
generated homograph inventory may be impracti-
cal. In this case, the first step is to determine which 
words in a lexicon are homographs.  This problem 
is the subject of this paper. 
2 Finding the Homographs in a Lexicon 
Our goal is to identify the homographs in a large 
lexicon.  We assume that manual labor is a scarce 
resource, but that online dictionaries are plentiful 
(as is the case on the web).  Given a word from the 
lexicon, definitions are obtained from eight dic-
tionaries: Cambridge Advanced Learners Diction-
ary (CALD), Compact Oxford English Dictionary, 
MSN Encarta, Longman Dictionary of Contempo-
rary English (LDOCE), The Online Plain Text 
English Dictionary, Wiktionary, WordNet and 
Wordsmyth. Using multiple dictionaries provides 
more evidence for the inferences to be made and 
also minimizes the risk of missing meanings be-
cause a particular dictionary did not include one or 
more meanings of a word (a surprisingly common 
situation). We can now rephrase the problem defi-
nition as that of determining which words in the 
lexicon are homographs given a set of dictionary 
definitions for each of the words.  
2.1 Features 
We use nine meta-features in our algorithm. In-
stead of directly using common lexical features 
such as n-grams we use meta-features which are 
functions defined on the lexical features. This ab-
85
straction is essential in this setup for the generality 
of the approach. For each word w to be classified 
each of the following meta-features are computed. 
 
1. Cohesiveness Score: Mean of the cosine simi-
larities between each pair of definitions of w. 
2. Average Number of Definitions: The average 
number of definitions per dictionary. 
3. Average Definition Length: The average 
length (in words) of definitions of w. 
4. Average Number of Null Similarities: The 
number of definition pairs that have zero co-
sine similarity score (no word overlap). 
5. Number of Tokens: The sum of the lengths 
(in words) of the definitions of w. 
6. Number of Types: The size of the vocabulary 
used by the set of definitions of w. 
7. Number of Definition Pairs with n Word 
Overlaps: The number of definition pairs that 
have more than n=2 words in common. 
8. Number of Definition Pairs with m Word 
Overlaps: The number of definition pairs that 
have more than m=4 words in common. 
9. Post Pruning Maximum Similarity: (below) 
 
The last feature sorts the pair-wise cosine similar-
ity scores in ascending order, prunes the top n% of 
the scores, and uses the maximum remaining score 
as the feature value.  This feature is less ad-hoc 
than it may seem.  The set of definitions is formed 
from eight dictionaries, so almost identical defini-
tions are a frequent phenomenon, which makes the 
maximum cosine similarity a useless feature. A 
pruned maximum turns out to be useful informa-
tion. In this work n=15 was found to be most in-
formative using a tuning dataset.  
Each of the above features provides some 
amount of discriminative power to the algorithm. 
For example, we hypothesized that on average the 
cohesiveness score will be lower for homographs 
than for non-homographs. Figure 1 provides an 
illustration. If empirical support was observed for 
such a hypothesis about a candidate feature then 
the feature was selected. This empirical evidence 
was derived from only the training portion of the 
data (Section 3.1).  
The above features are computed on definitions 
stemmed with the Porter Stemmer. Closed class 
words, such as articles and prepositions, and dic-
tionary-specific stopwords, such as ?transitive?, 
?intransitive?, and ?countable?, were also removed. 
Figure 1. Histogram of Cohesiveness scores for Homo-
graphs and Non-homographs. 
2.2 Models 
We formulate the homograph detection process as 
a generative hierarchical model. Figure 2 provides 
the plate notation of the graphical model. The la-
tent (unobserved) variable Z models the class in-
formation: homograph or non-homograph. Node X 
is the conditioned random vector (Z is the condi-
tioning variable) that models the feature vector. 
 
Figure 2.  Plate notation for the proposed model. 
 
This setup results in a mixture model with two 
components, one for each class. The Z is assumed 
to be Bernoulli distributed and thus parameterized 
by a single parameter p. We experiment with two 
continuous multivariate distributions, Dirichlet and 
Multivariate Normal (MVN), for the conditional 
distribution of X|Z. 
Z ~ Bernoulli (p) 
X|Z ~ Dirichlet (az)   
OR 
X|Z ~ MVN (muz, covz) 
We will refer to the parameters of the condi-
tional distribution as ?z. For the Dirichlet distribu-
tion, ?z is a ten-dimensional vector az = (az1, .., 
az10). For the MVN, ?z represents a nine-
dimensional mean vector muz = (muz1, .., muz9) 
N 
p Z 
X ? 
 
86
and a nine-by-nine-dimensional covariance matrix 
covz. We use maximum likelihood estimators 
(MLE) for estimating the parameters (p, ?z). The 
MLEs for Bernoulli and MVN parameters have 
analytical solutions. Dirichlet parameters were es-
timated using an estimation method proposed and 
implemented by Tom Minka1. 
We experiment with three model setups: Super-
vised, semi-supervised, and unsupervised. In the 
supervised setup we use the training data described 
in Section 3.1 for parameter estimation and then 
use thus fitted models to classify the tuning and 
test dataset. We refer to this as the Model I. In 
Model II, the semi-supervised setup, the training 
data is used to initialize the Expectation-
Maximization (EM) algorithm (Dempster et al, 
1977) and the unlabeled data, described in Section 
3.1, updates the initial estimates. The Viterbi 
(hard) EM algorithm was used in these experi-
ments. The E-step was modified to include only 
those unlabeled data-points for which the posterior 
probability was above certain threshold. As a re-
sult, the M-step operates only on these high poste-
rior data-points. The optimal threshold value was 
selected using a tuning set (Section 3.1). The unsu-
pervised setup, Model III, is similar to the semi-
supervised setup except that the EM algorithm is 
initialized using an informed guess by the authors. 
3 Data 
In this study, we concentrate on recognizing 
homographic nouns, because homographic ambi-
guity is much more common in nouns than in 
verbs, adverbs or adjectives. 
3.1 Gold Standard Data 
A set of potentially-homographic nouns was identi-
fied by selecting all words with at least two noun 
definitions in both CALD and LDOCE.  This set 
contained 3,348 words. 
225 words were selected for manual annotation 
as homograph or non-homograph by random sam-
pling of words that were on the above list and used 
in prior psycholinguistic studies of homographs 
(Twilley et al, 1994; Azuma, 1996) or on the Aca-
demic Word List (Coxhead, 2000). 
Four annotators at, the Qualitative Data Analysis 
Program at the University of Pittsburgh, were 
                                                          
1
 http://research.microsoft.com/~minka/software/fastfit/ 
trained to identify homographs using sets of dic-
tionary definitions.  After training, each of the 225 
words was annotated by each annotator. On aver-
age, annotators categorized each word in just 19 
seconds.  The inter-annotator agreement was 0.68, 
measured by Fleiss? Kappa. 
23 words on which annotators disagreed (2/2 
vote) were discarded, leaving a set of 202 words 
(the ?gold standard?) on which at least 3 of the 4 
annotators agreed. The best agreement between the 
gold standard and a human annotator was 0.87 
kappa, and the worst was 0.78. The class distribu-
tion (homographs and non-homographs) was 0.63, 
0.37. The set of 3,123 words that were not anno-
tated was the unlabeled data for the EM algorithm. 
4 Experiments and Results 
A stratified division of the gold standard data in 
the proportion of 0.75 and 0.25 was done in the 
first step. The smaller portion of this division was 
held out as the testing dataset. The bigger portion 
was further divided into two portions of 0.75 and 
0.25 for the training set and the tuning set, respec-
tively. The best and the worst kappa between a 
human annotator and the test set are 0.92 and 0.78. 
Each of the three models described in Section 
2.2 were experimented with both Dirichlet and 
MVN as the conditional. An additional experiment 
using two standard classification algorithms ? Ker-
nel Based Na?ve Bayes (NB) and Support Vector 
Machines (SVM) was performed. We refer to this 
as the baseline experiment. The Na?ve Bayes clas-
sifier outperformed SVM on the tuning as well as 
the test set and thus we report NB results only. A 
four-fold cross-validation was employed for the all 
the experiments on the tuning set. The results are 
summarized in Table 1. The reported precision, 
recall and F1 values are for the homograph class.  
The na?ve assumption of class conditional fea-
ture independence is common to simple Na?ve 
Bayes classifier, a kernel based NB classifier; 
however, unlike simple NB it is capable of model-
ing non-Gaussian distributions. Note that in spite 
of this advantage the kernel based NB is outper-
formed by the MVN based hierarchical model. Our 
nine features are by definition correlated and thus 
it was our hypothesis that a multivariate distribu-
tion such as MVN which can capture the covari-
ance amongst the features will be a better fit. The 
above finding confirms this hypothesis. 
87
 Table 1. Results for the six models and the baseline on the tuning and test set.
One of the known situations when mixture mod-
els out-perform standard classification algorithms 
is when the data comes from highly overlapping 
distributions. In such cases the classification algo-
rithms that try to place the decision boundary in a 
sparse area are prone to higher error-rates than 
mixture model based approach. We believe that 
this is explanations of the observed results. On the 
test set a relative improvement of 7% in F1 and 
14% in kappa statistic is obtained using the MVN 
mixture model. 
The results for the semi-supervised models are 
non-conclusive. Our post-experimental analysis 
reveals that the parameter updation process using 
the unlabeled data has an effect of overly separat-
ing the two overlapping distributions. This is trig-
gered by our threshold based EM methodology 
which includes only those data-points for which 
the model is highly confident; however such data-
points are invariable from the non-overlapping re-
gions of the distribution, which gives a false view 
to the learner that the distributions are less over-
lapping. We believe that the unsupervised models 
also suffer from the above problem in addition to 
the possibility of poor initializations. 
5 Conclusions 
We have demonstrated in this paper that the prob-
lem of homograph identification can be ap-
proached using dictionary definitions as the source 
of information about the word. Further more, using 
multiple dictionaries provides more evidence for 
the inferences to be made and also minimizes the 
risk of missing few meanings of the word.  
We can conclude that by modeling the underly-
ing data generation process as a mixture model, the 
problem of homograph identification can be per-
formed with reasonable accuracy.  
The capability of identifying homographs from 
non-homographs enables us to take on the next 
steps of sense-inventory generation and lexical 
ambiguity resolution. 
Acknowledgments 
We thank Shay Cohen and Dr. Matthew Harrison for the 
helpful discussions. This work was supported in part by 
the Pittsburgh Science of Learning Center which is 
funded by the National Science Foundation, award 
number SBE-0354420.  
References  
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum 
likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series 
B, 39(1):1?38. 
A. Coxhead. 2000. A New Academic Word List. 
TESOL, Quarterly, 34(2): 213-238. 
J. Jorgenson. 1990. The psychological reality of word 
senses. Journal of Psycholinguistic Research 19:167-
190. 
L. Twilley, P. Dixon, D. Taylor, and K. Clark. 1994. 
University of Alberta norms of relative meaning fre-
quency for 566 homographs. Memory and Cognition. 
22(1): 111-126. 
M. Sanderson. 2000. Retrieving with good sense. In-
formation Retrieval, 2(1): 49-69. 
M. Palmer, H. Dang, C. Fellbaum, 2005. Making fine-
grained and coarse-grained sense distinctions. Jour-
nal of Natural Language Engineering. 13: 137-163.  
N. Ide and Y. Wilks. 2006. Word Sense Disambigua-
tion, Algorithms and Applications. Springer, 
Dordrecht, The Netherlands. 
T. Azuma. 1996. Familiarity and Relatedness of Word 
Meanings: Ratings for 110 Homographs. Behavior 
Research Methods, Instruments and Computers. 
28(1): 109-124. 
Y. Chan, H. Ng, and D. Chiang. 2007. Proceeding of 
Association for Computational Linguistics, Prague, 
Czech Republic. 
 Tuning Set Test Set 
 
Preci-
sion Recall F1 Kappa 
Preci-
sion Recall F1 Kappa 
Model I ? Dirichlet 0.84 0.74 0.78 0.47 0.81 0.62 0.70 0.34 
Model II ? Dirichlet 0.85 0.71 0.77 0.45 0.81 0.60 0.68 0.33 
Model III ? Dirichlet 0.78 0.74 0.76 0.37 0.82 0.56 0.67 0.32 
Model I ? MVN 0.70 0.75 0.78 0.32 0.80 0.73 0.76 0.41 
Model II ? MVN 0.74 0.82 0.78 0.34 0.71 0.79 0.74 0.25 
Model III ? MVN 0.69 0.89 0.77 0.22 0.64 0.84 0.72 0.22 
Baseline ? NB 0.82 0.73 0.77 0.43 0.82 0.63 0.71 0.36 
88
Selecting the ?Right? Number of Senses
Based on Clustering Criterion Functions
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
This paper describes an unsupervised
knowledge?lean methodology for auto-
matically determining the number of
senses in which an ambiguous word is
used in a large corpus. It is based on the
use of global criterion functions that assess
the quality of a clustering solution.
1 Introduction
The goal of word sense discrimination is to cluster
the occurrences of a word in context based on its
underlying meaning. This is often approached as a
problem in unsupervised learning, where the only
information available is a large corpus of text (e.g.,
(Pedersen and Bruce, 1997), (Schu?tze, 1998), (Pu-
randare and Pedersen, 2004)). These methods usu-
ally require that the number of clusters to be dis-
covered (k) be specified ahead of time. However,
in most realistic settings, the value of k is unknown
to the user.
Word sense discrimination seeks to cluster N
contexts, each of which contain a particular tar-
get word, into k clusters, where we would like
the value of k to be automatically selected. Each
context consists of approximately a paragraph of
surrounding text, where the word to be discrimi-
nated (the target word) is found approximately in
the middle of the context. We present a methodol-
ogy that automatically selects an appropriate value
for k. Our strategy is to perform clustering for suc-
cessive values of k, and evaluate the resulting solu-
tions with a criterion function. We select the value
of k that is immediately prior to the point at which
clustering does not improve significantly.
Clustering methods are typically either parti-
tional or agglomerative. The main difference is
that agglomerative methods start with 1 or N clus-
ters and then iteratively arrive at a pre?specified
number (k) of clusters, while partitional methods
start by randomly dividing the contexts into k clus-
ters and then iteratively rearranging the members
of the k clusters until the selected criterion func-
tion is maximized. In this work we have used K-
means clustering, which is a partitional method,
and the H2 criterion function, which is the ratio
of within cluster similarity to between cluster sim-
ilarity. However, our approach can be used with
any clustering algorithm and global criterion func-
tion, meaning that the criterion function should ar-
rive at a single value that assesses the quality of the
clustering for each value of k under consideration.
2 Methodology
In word sense discrimination, the number of con-
texts (N) to cluster is usually very large, and con-
sidering all possible values of k from 1...N would
be inefficient. As the value of k increases, the cri-
terion function will reach a plateau, indicating that
dividing the contexts into more and more clusters
does not improve the quality of the solution. Thus,
we identify an upper bound to k that we refer to as
deltaK by finding the point at which the criterion
function only changes to a small degree as k in-
creases.
According to the H2 criterion function, the
higher its ratio of within cluster similarity to be-
tween cluster similarity, the better the clustering.
A large value indicates that the clusters have high
internal similarity, and are clearly separated from
each other. Intuitively then, one solution to select-
ing k might be to examine the trend of H2 scores,
and look for the smallest k that results in a nearly
maximum H2 value.
However, a graph of H2 values for a clustering
111
of the 4 sense verb serve as shown in Figure 1 (top)
reveals the difficulties of such an approach. There
is a gradual curve in this graph and the maximum
value (plateau) is not reached until k values greater
than 100.
We have developed three methods that take as
input the H2 values generated from 1...deltaK
and automatically determine the ?right? value of
k, based on finding when the changes in H2 as k
increases are no longer significant.
2.1 PK1
The PK1 measure is based on (Mojena, 1977),
which finds clustering solutions for all values of
k from 1..N , and then determines the mean and
standard deviation of the criterion function. Then,
a score is computed for each value of k by sub-
tracting the mean from the criterion function, and
dividing by the standard deviation. We adapt this
technique by using the H2 criterion function, and
limit k from 1...deltaK:
PK1(k) = H2(k)?mean(H2[1...deltaK])std(H2[1...deltaK])
(1)
To select a value of k, a threshold must be set.
Then, as soon as PK1(k) exceeds this threshold,
k-1 is selected as the appropriate number of clus-
ters. We have considered setting this threshold us-
ing the normal distribution based on interpreting
PK1 as a z-score, although Mojena makes it clear
that he views this method as an ?operational rule?
that is not based on any distributional assumptions.
He suggests values of 2.75 to 3.50, but also states
they would need to be adjusted for different data
sets. We have arrived at an empirically determined
value of -0.70, which coincides with the point in
the standard normal distribution where 75% of the
probability mass is associated with values greater
than this.
We observe that the distribution of PK1 scores
tends to change with different data sets, making it
hard to apply a single threshold. The graph of the
PK1 scores shown in Figure 1 illustrates the dif-
ficulty - the slope of these scores is nearly linear,
and as such the threshold (as shown by the hori-
zontal line) is a somewhat arbitrary cutoff.
2.2 PK2
PK2 is similar to (Hartigan, 1975), in that both
take the ratio of a criterion function at k and k-1,
0.001
0.002
0.003
0.004
0.005
0.006
0.007
0.008
0.009
0 50 100 150 200
H2 vs k
s
4r
-2.000
-1.500
-1.000
-0.500
0.000
0.500
1.000
1.500
2 3 4 5 6 7 8 9 1011121314151617
PK1 vs k
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r r
24
0.900
1.000
1.100
1.200
1.300
1.400
1.500
1.600
1.700
1.800
1.900
2 3 4 5 6 7 8 9 1011121314 151617
PK2 vs kr
r
r
r
r
r
r r r r
r r r r r
r
2
4
0.990
0.995
1.000
1.005
1.010
1.015
1.020
1.025
1.030
1.035
1.040
2 3 4 5 6 7 8 9 1011121314 151617
PK3 vs k
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
2
4
Figure 1: Graphs of H2 (top) and PK 1-3 for
serve: Actual number of senses (4) shown as trian-
gle (all), predicted number as square (PK1-3), and
deltaK (17) shown as dot (H2) and upper limit of
k (PK1-3).
112
in order to assess the relative improvement when
increasing the number of clusters.
PK2(k) = H2(k)H2(k ? 1) (2)
When this ratio approaches 1, the clustering has
reached a plateau, and increasing k will have no
benefit. If PK2 is greater than 1, then an addi-
tional cluster improves the solution and we should
increase k. We compute the standard deviation of
PK2 and use that to establish a boundary as to
what it means to be ?close enough? to 1 to consider
that we have reached a plateau. Thus, PK2 will
select k where PK2(k) is the closest to (but not
less than) 1 + standard deviation(PK2[1...deltaK]).
The graph of PK2 in Figure 1 shows an el-
bow that is near the actual number of senses. The
critical region defined by the standard deviation is
shaded, and note that PK2 selected the value of
k that was outside of (but closest to) that region.
This is interpreted as being the last value of k that
resulted in a significant improvement in cluster-
ing quality. Note that here PK2 predicts 3 senses
(square) while in fact there are 4 actual senses (tri-
angle). It is significant that the graph of PK2 pro-
vides a clearer representation of the plateau than
does that of H2.
2.3 PK3
PK3 utilizes three k values, in an attempt to find
a point at which the criterion function increases
and then suddenly decreases. Thus, for a given
value of k we compare its criterion function to the
preceding and following value of k:
PK3(k) = 2?H2(k)H2(k ? 1) + H2(k + 1) (3)
PK3 is close to 1 if the three H2 values form
a line, meaning that they are either ascending, or
they are on the plateau. However, our use of
deltaK eliminates the plateau, so in our case values
of 1 show that k is resulting in consistent improve-
ments to clustering quality, and that we should
continue. When PK3 rises significantly above 1,
we know that k+1 is not climbing as quickly, and
we have reached a point where additional clus-
tering may not be helpful. To select k we chose
the largest value of PK3(k) that is closest to (but
still greater than) the critical region defined by the
standard deviation of PK3. This is the last point
where a significant increase in H2 was observed.
Note that the graph of PK3 in Figure 1 shows the
value of PK3 rising and falling dramatically in
the critical region, suggesting a need for additional
points to make it less localized.
PK3 is similar in spirit to (Salvador and Chan,
2004), which introduces the L measure. This tries
to find the point of maximum curvature in the cri-
terion function graph, by fitting a pair of lines to
the curve (where the intersection of these lines rep-
resents the selected k).
3 Experimental Results
We conducted experiments with words that have 2,
3, 4, and 6 actual senses. We used three words that
had been manually sense tagged, including the 3
sense adjective hard, the 4 sense verb serve, and
the 6 sense noun line. We also created 19 name
conflations where sets of 2, 3, 4, and 6 names of
persons, places, or organizations that are included
in the English GigaWord corpus (and that are typ-
ically unambiguous) are replaced with a single
name to create pseudo or false ambiguities. For
example, we replaced all mentions of Bill Clinton
and Tony Blair with a single name that can refer
to either of them. In general the names we used
in these sets are fairly well known and occur hun-
dreds or even thousands of times.
We clustered each word or name using four dif-
ferent configurations of our clustering approach,
in order to determine how consistent the selected
value of k is in the face of changing feature sets
and context representations. The four configura-
tions are first order feature vectors made up of un-
igrams that occurred 5 or more times, with and
without singular value decomposition, and then
second order feature vectors based on bigrams that
occurred 5 or more times and had a log?likelihood
score of 3.841 or greater, with and without sin-
gular value decomposition. Details on these ap-
proaches can be found in (Purandare and Peder-
sen, 2004).
Thus, in total there are 22 words to be discrim-
inated, 7 with 2 senses, 6 words with 3 senses, 6
with 4 senses, and 3 words with 6 senses. Four
different configurations of clustering are run for
each word, leading to a total of 88 experiments.
The results are shown in Tables 1, 2, and 3. In
these tables, the actual numbers of senses are in
the columns, and the predicted number of senses
are in the rows.
We see that the predicted value of PK1 agreed
113
Table 1: k Predicted by PK1 vs Actual k
2 3 4 6
1 6 6 3 3 18
2 5 5 1 3 14
3 4 1 7 2 14
4 6 5 7 1 19
5 4 2 1 7
6 2 3 3 2 10
7 1 1 2
8 1 1
9 1 1 2
11 1 1
28 24 24 12 88
Table 2: k Predicted by PK2 vs Actual k
2 3 4 6
1 3 1 4
2 8 5 7 6 26
3 8 10 8 2 30
4 4 2 3 9
5 1 3 2 6
6 1 2 1 4
7 2 2
9 1 1 2
10 1 2 3
11 1 1
12 1 1
17 2 2
28 24 24 12 88
with the actual value in 15 cases, whereas PK3
agreed in 17 cases, and PK2 agreed in 22 cases.
We observe that PK1 and PK3 also experienced
considerable confusion, in that their predictions
were in many cases several clusters off of the cor-
rect value. While PK2 made various mistakes,
it was generally closer to the correct values, and
had fewer spurious responses (very large or very
small predictions). We note that the distribution
of PK2?s predictions were most like those of the
actual senses.
4 Conclusions
This paper shows how to use clustering criterion
functions as a means of automatically selecting the
number of senses k in an ambiguous word. We
have found that PK2, a ratio of the criterion func-
tions for the current and previous value of k, is
Table 3: k Predicted by PK3 vs Actual k
2 3 4 6
1 3 4 1 1 9
2 13 9 12 4 38
3 4 3 4 4 15
4 2 2 1 1 6
5 2 1 1 1 5
6 1 2 3 6
7 1 1 1 3
9 1 1
10 1 1
11 2 2
12 1 1
13 1 1
28 24 24 12 88
most effective, although there are many opportu-
nities for future improvements to these techniques.
5 Acknowledgments
This research is supported by a National Science
Foundation Faculty Early CAREER Development
Award (#0092784). All of the experiments in
this paper were carried out with the SenseClusters
package, which is freely available from the URL
on the title page.
References
J. Hartigan. 1975. Clustering Algorithms. Wiley, New
York.
R. Mojena. 1977. Hierarchical grouping methods and
stopping rules: An evaluation. The Computer Jour-
nal, 20(4):359?363.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 197?207, Providence,
RI, August.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41?48, Boston, MA.
S. Salvador and P. Chan. 2004. Determining the
number of clusters/segments in hierarchical cluster-
ing/segmentation algorithms. In Proceedings of the
16th IEEE International Conference on Tools with
AI, pages 576?584.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
114
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 276?279,
New York City, June 2006. c?2006 Association for Computational Linguistics
Automatic Cluster Stopping with Criterion Functions and the Gap Statistic
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
SenseClusters is a freely available sys-
tem that clusters similar contexts. It can
be applied to a wide range of problems,
although here we focus on word sense
and name discrimination. It supports
several different measures for automati-
cally determining the number of clusters
in which a collection of contexts should
be grouped. These can be used to discover
the number of senses in which a word is
used in a large corpus of text, or the num-
ber of entities that share the same name.
There are three measures based on clus-
tering criterion functions, and another on
the Gap Statistic.
1 Introduction
Word sense and name discrimination are problems
in unsupervised learning that seek to cluster the oc-
currences of a word (or name) found in multiple con-
texts based on their underlying meaning (or iden-
tity). The assumption is made that each discovered
cluster will represent a different sense of a word, or
the underlying identity of a person or organization
that has an ambiguous name.
Existing approaches to this problem usually re-
quire that the number of clusters to be discovered
(k) be specified ahead of time. However, in most re-
alistic settings, the value of k is unknown to the user.
Here we describe various cluster stopping measures
that are now implemented in SenseClusters (Puran-
dare and Pedersen, 2004) that will group N contexts
into k clusters, where the value of k will be automat-
ically determined.
Cluster stopping can be viewed as a problem in
model selection, since a number of different models
(i.e., clustering solutions) are created using different
values of k, and the one that best fits the observed
data is selected based on a criterion function. This
is reminiscent of earlier work on sequential model
selection for creating models of word sense disam-
biguation (e.g., (O?Hara et al, 2000)), where it was
found that forward sequential search strategies were
most effective. These methods start with simpler
models and then add to them in a stepwise fash-
ion until no further improvement in model fit is ob-
served. This is in fact very similar to what we have
done here, where we start with solutions based on
one cluster, and steadily increase the number of clus-
ters until we find the best fitting solution.
SenseClusters supports four cluster stopping mea-
sures, each of which is based on interpreting a clus-
tering criterion function in some way. The first three
measures (PK1, PK2, PK3) look at the successive
values of the criterion functions as k increases, and
try to identify the point at which the criterion func-
tion stops improving significantly. We have also cre-
ated an adaptation of the Gap Statistic (Tibshirani
et al, 2001), which compares the criterion function
from the clustering of the observed data with the
clustering of a null reference distribution and selects
the value of k for which the difference between them
is greatest.
In order to evaluate our results, we sometimes
conduct experiments with words that have been
manually sense tagged. We also create name con-
276
flations where some number of names of persons,
places, or organizations are replaced with a single
name to create pseudo or false ambiguities. For ex-
ample, in this paper we refer to an example where
we have replaced all mentions of Sonia Gandhi and
Leonid Kuchma with a single ambiguous name.
Clustering methods are typically either partitional
or agglomerative. The main difference is that ag-
glomerative methods start with 1 or N clusters and
then iteratively arrive at a pre?specified number (k)
of clusters, while partitional methods start by ran-
domly dividing the contexts into k clusters and then
iteratively rearranging the members of the k clusters
until the selected criterion function is maximized. In
this work we have used K-means clustering, which
is a partitional method, and the H2 criterion func-
tion, which is the ratio of within?cluster similarity
(I2) to between?cluster similarity (E1).
2 Methodology
In word sense or name discrimination, the num-
ber of contexts (N) to cluster is usually very large,
and considering all possible values of k from 1...N
would be inefficient. As the value of k increases,
the criterion function will reach a plateau, indicat-
ing that dividing the contexts into more and more
clusters does not improve the quality of the solution.
Thus, we identify an upper bound to k that we refer
to as deltaK by finding the point at which the cri-
terion function only changes to a small degree as k
increases.
According to the H2 criterion function, the higher
its ratio of within?cluster similarity to between?
cluster similarity, the better the clustering. A large
value indicates that the clusters have high internal
similarity, and are clearly separated from each other.
Intuitively then, one solution to selecting k might
be to examine the trend of H2 scores, and look for
the smallest k that results in a nearly maximum H2
value.
However, a graph of H2 values for a clustering
of the 2 sense name conflation Sonia Gandhi and
Leonid Kuchma as shown in Figure 1 (top) reveals
the difficulties of such an approach. There is a grad-
ual curve in this graph and there is no obvious knee
point (i.e., sharp increase) that indicates the appro-
priate value of k.
0.0045
0.0050
0.0055
0.0060
0.0065
0.0070
0.0075
0.0080
0 2 4 6 8 10 12 14 16
H2 vs k
r
r
r
r
r
r
r
r
r
r
r
r
r r r
r
-1.5000
-1.0000
-0.5000
0.0000
0.5000
1.0000
0 2 4 6 8 10 12 14 16
PK1 vs k
r
r
r
r
r
r
r
r
r
r
r
r r r
r
0.9500
1.0000
1.0500
1.1000
1.1500
1.2000
1.2500
1.3000
2 4 6 8 10 12 14 16
PK2 vs kr
r
r
r
r
r
r r r r
r
r r r
0.9900
1.0000
1.0100
1.0200
1.0300
1.0400
1.0500
1.0600
2 4 6 8 10 12 14
PK3 vs k
r
r
r
r r r r
r r
r
r
r
r
Figure 1: H2 (top) and PK1, PK2, and PK3 for
the name conflate pair Sonia Gandhi and Leonid
Kuchma. The predicted number of senses is 2 for
all the measures.
277
2.1 PK1
The PK1 measure is based on (Mojena, 1977),
which finds clustering solutions for all values of k
from 1..N , and then determines the mean and stan-
dard deviation of the criterion function. Then, a
score is computed for each value of k by subtracting
the mean from the criterion function, and dividing
by the standard deviation. We adapt this technique
by using the H2 criterion function, and limit k from
1...deltaK:
PK1(k) = H2(k) ? mean(H2[1...deltaK])std(H2[1...deltaK])
(1)
To select a value of k, a threshold must be set.
Then, as soon as PK1(k) exceeds this threshold,
k-1 is selected as the appropriate number of clus-
ters. Mojena suggests values of 2.75 to 3.50, but also
states they would need to be adjusted for different
data sets. We have arrived at an empirically deter-
mined value of -0.70, which coincides with the point
in the standard normal distribution where 75% of the
probability mass is associated with values greater
than this.
We observe that the distribution of PK1 scores
tends to change with different data sets, making it
hard to apply a single threshold. The graph of the
PK1 scores shown in Figure 1 illustrates the diffi-
culty : the slope of these scores is nearly linear, and
as such any threshold is a somewhat arbitrary cutoff.
2.2 PK2
PK2 is similar to (Hartigan, 1975), in that both take
the ratio of a criterion function at k and k-1, in order
to assess the relative improvement when increasing
the number of clusters.
PK2(k) = H2(k)H2(k ? 1) (2)
When this ratio approaches 1, the clustering has
reached a plateau, and increasing k will have no
benefit. If PK2 is greater than 1, then we should
increase k. We compute the standard deviation of
PK2 and use that to establish a boundary as to what
it means to be ?close enough? to 1 to consider that
we have reached a plateau. Thus, PK2 will select k
where PK2(k) is the closest to (but not less than) 1
+ standard deviation(PK2[1...deltaK]).
The graph of PK2 in Figure 1 shows an elbow
that is near the actual number of senses. The critical
region defined by the standard deviation is shaded,
and note that PK2 selected the value of k that was
outside of (but closest to) that region. This is inter-
preted as being the last value of k that resulted in a
significant improvement in clustering quality. Note
that here PK2 predicts 2 senses, which corresponds
to the number of underlying entities.
2.3 PK3
PK3 utilizes three k values, in an attempt to find a
point at which the criterion function increases and
then suddenly decreases. Thus, for a given value of
k we compare its criterion function to the preceding
and following value of k:
PK3(k) = 2 ? H2(k)H2(k ? 1) + H2(k + 1) (3)
The form of this measure is identical to that of the
Dice Coefficient, although in set theoretic or prob-
abilistic applications Dice tends to be used to com-
pare two variables or sets with each other.
PK3 is close to 1 if the H2 values form a line,
meaning that they are either ascending, or they are
on the plateau. However, our use of deltaK elimi-
nates the plateau, so in our case values of 1 show that
k is resulting in consistent improvements to clus-
tering quality, and that we should continue. When
PK3 rises significantly above 1, we know that k+1
is not climbing as quickly, and we have reached a
point where additional clustering may not be help-
ful. To select k we select the largest value of
PK3(k) that is closest to (but still greater than) the
critical region defined by the standard deviation of
PK3.
PK3 is similar in spirit to (Salvador and Chan,
2004), which introduces the L measure. This tries to
find the point of maximum curvature in the criterion
function graph, by fitting a pair of lines to the curve
(where the intersection of these lines represents the
selected k).
278
2.4 The Gap Statistic
SenseClusters includes an adaptation of the Gap
Statistic (Tibshirani et al, 2001). It is distinct from
the measures PK1, PK2, and PK3 since it does not
attempt to directly find a knee point in the graph of
a criterion function. Rather, it creates a sample of
reference data that represents the observed data as
if it had no meaningful clusters in it and was sim-
ply made up of noise. The criterion function of the
reference data is then compared to that of the ob-
served data, in order to identify the value of k in the
observed data that is least like noise, and therefore
represents the best clustering of the data.
To do this, it generates a null reference distri-
bution by sampling from a distribution where the
marginal totals are fixed to the observed marginal
values. Then some number of replicates of the ref-
erence distribution are created by sampling from it
with replacement, and each of these replicates is
clustered just like the observed data (for successive
values of k using a given criterion function).
The criterion function scores for the observed and
reference data are compared, and the point at which
the distance between them is greatest is taken to pro-
vide the appropriate value of k. An example of this
is seen in Figure 2. The reference distribution repre-
sents the noise in the observed data, so the value of
k where the distance between the reference and ob-
served data is greatest represents the most effective
clustering of the data.
Our adaption of the Gap Statistic allows us to
use any clustering criterion function to make the
comparison of the observed and reference data,
whereas the original formulation is based on using
the within?cluster dispersion.
3 Acknowledgments
This research is supported by a National Science
Foundation Faculty Early CAREER Development
Award (#0092784).
References
J. Hartigan. 1975. Clustering Algorithms. Wiley, New
York.
R. Mojena. 1977. Hierarchical grouping methods and
40
60
80
100
120
140
160
180
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
I2(obs) vs k
r
r
r
r
r
r
r
r
r r
r
r
r r
r
r r
r
r r
r
r r
r r
r r
r r
r
r
I2(ref) vs k
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r r
r
r
r
r
r
r r
r r
r r
r r
r
10
15
20
25
30
35
40
45
50
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Gap vs k
r
r
r
r
r
r
r
r
r
r r r r
r
r r
r
r r
r r r
r r
r
r
r r r r
Figure 2: I2 for observed and reference data (top)
and the Gap between them (bottom) for the name
conflate pair Sonia Gandhi and Leonid Kuchma. The
predicted number of senses is 3.
stopping rules: An evaluation. The Computer Journal,
20(4):359?363.
T. O?Hara, J. Wiebe, and R. Bruce. 2000. Selecting
decomposable models for word-sense disambiguation:
The grling-sdm system. Computers and the Humani-
ties, 34(1?2):159?164.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
S. Salvador and P. Chan. 2004. Determining the
number of clusters/segments in hierarchical cluster-
ing/segmentation algorithms. In Proceedings of the
16th IEEE International Conference on Tools with AI,
pages 576?584.
R. Tibshirani, G. Walther, and T. Hastie. 2001. Esti-
mating the number of clusters in a dataset via the Gap
statistic. Journal of the Royal Statistics Society (Series
B), pages 411?423.
279
Proceedings of the ACL Student Research Workshop, pages 145?150,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Unsupervised Discrimination and Labeling
of Ambiguous Names
Anagha K. Kulkarni
Department of Computer Science
University Of Minnesota
Duluth, MN 55812
kulka020@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
This paper describes adaptations of unsu-
pervised word sense discrimination tech-
niques to the problem of name discrimina-
tion. These methods cluster the contexts
containing an ambiguous name, such that
each cluster refers to a unique underlying
person or place. We also present new tech-
niques to assign meaningful labels to the
discovered clusters.
1 Introduction
A name assigned to an entity is often thought to be
a unique identifier. However this is not always true.
We frequently come across multiple people sharing
the same name, or cities and towns that have iden-
tical names. For example, the top ten results for
a Google search of John Gilbert return six differ-
ent individuals: A famous actor from the silent film
era, a British painter, a professor of Computer Sci-
ence, etc. Name ambiguity is relatively common,
and makes searching for people, places, or organiza-
tions potentially very confusing.
However, in many cases a human can distinguish
between the underlying entities associated with an
ambiguous name with the help of surrounding con-
text. For example, a human can easily recognize that
a document that mentions Silent Era, Silver Screen,
and The Big Parade refers to John Gilbert the ac-
tor, and not the professor. Thus the neighborhood of
the ambiguous name reveals distinguishing features
about the underlying entity.
Our approach is based on unsupervised learning
from raw text, adapting methods originally proposed
by (Purandare and Pedersen, 2004). We do not
utilize any manually created examples, knowledge
bases, dictionaries, or ontologies in formulating our
solution. Our goal is to discriminate among multi-
ple contexts that mention a particular name strictly
on the basis of the surrounding contents, and assign
meaningful labels to the resulting clusters that iden-
tify the underlying entity.
This paper is organized as follows. First, we re-
view related work in name discrimination and clus-
ter labeling. Next we describe our methodology
step-by-step and then review our experimental data
and results. We conclude with a discussion of our
results and outline our plans for future work.
2 Related Work
A number of previous approaches to name discrim-
ination have employed ideas related to context vec-
tors. (Bagga and Baldwin, 1998) proposed a method
using the vector space model to disambiguate ref-
erences to a person, place, or event across mul-
tiple documents. Their approach starts by using
the CAMP system to find related references within
a single document. For example, it might deter-
mine that he and the President refers to Bill Clin-
ton. CAMP creates co-reference chains for each en-
tity in a single document, which are then extracted
and represented in the vector space model. This
model is used to find the similarity among referents,
and thereby identify the same referent that occurs in
multiple documents.
(Mann and Yarowsky, 2003) take an approach to
145
name discrimination that incorporates information
from the World Wide Web. They propose to use
various contextual characteristics that are typically
found near and within an ambiguous proper-noun
for the purpose of disambiguation. They utilize cat-
egorical features (e.g., age, date of birth), familial
relationships (e.g., wife, son, daughter) and associ-
ations that the entity frequently shows (e.g. coun-
try, company, organization). Such biographical in-
formation about the entities to be disambiguated is
mined from the Web using a bootstrapping method.
The Web pages containing the ambiguous name are
assigned a vector depending upon the extracted fea-
tures and then these vectors are grouped using ag-
glomerative clustering.
(Pantel and Ravichandran, 2004) have proposed
an algorithm for labeling semantic classes, which
can be viewed as a form of cluster. For example, a
semantic class may be formed by the words: grapes,
mango, pineapple, orange and peach. Ideally this
cluster would be labeled as the semantic class of
fruit. Each word of the semantic class is represented
by a feature vector. Each feature consists of syn-
tactic patterns (like verb-object) in which the word
occurs. The similarity between a few features from
each cluster is found using point-wise mutual infor-
mation (PMI) and their average is used to group and
rank the clusters to form a grammatical template or
signature for the class. Then syntactic relationships
such as Noun like Noun or Noun such as Noun are
searched for in the templates to give the cluster an
appropriate name label. The output is in the form
of a ranked list of concept names for each semantic
class.
3 Feature Identification
We start by identifying features from a corpus of
text which we refer to as the feature selection data.
This data can be the test data, i.e., the contexts to be
clustered (each of which contain an occurrence of
the ambiguous name) or it may be a separate cor-
pus. The identified features are used to translate
each context in the test data to a vector form.
We are exploring the use of bigrams as our fea-
ture type. These are lexical features that consist of
an ordered pair of words which may occur next to
each other, or have one intervening word. We are
interested in bigrams since they tend to be less am-
biguous and more specific than individual unigrams.
In order to reduce the amount of noise in the feature
set, we discard all bigrams that occur only once, or
that have a log-likelihood ratio of less than 3.841.
The latter criteria indicates that the words in the bi-
gram are not independent (i.e., are associated) with
95% certainty. In addition, bigrams in which either
word is a stop word are filtered out.
4 Context Representation
We employ both first and second order representa-
tions of the contexts to be clustered. The first order
representation is a vector that indicates which of the
features identified during the feature selection pro-
cess occur in this context.
The second order context representation is
adapted from (Schu?tze, 1998). First a co-occurrence
matrix is constructed from the features identified in
the earlier stage, where the rows represent the first
word in the bigram, and the columns represent the
second word. Each cell contains the value of the
log-likelihood ratio for its respective row and col-
umn word-pair.
This matrix is both large and sparse, so we use
Singular Value Decomposition (SVD) to reduce the
dimensionality and smooth the sparsity. SVD has
the effect of compressing similar columns together,
and then reorganizing the matrix so that the most
significant of these columns come first in the ma-
trix. This allows the matrix to be represented more
compactly by a smaller number of these compressed
columns.
The matrix is reduced by a factor equal to the min-
imum of 10% of the original columns, or 300. If
the original number of columns is less than 3,000
then the matrix is reduced to 10% of the number
of columns. If the matrix has greater than 3,000
columns, then it is reduced to 300.
Each row in the resulting matrix is a vector for the
word the row represents. For the second order repre-
sentation, each context in the test data is represented
by a vector which is created by averaging the word
vectors for all the words in the context.
The philosophy behind the second order repre-
sentation is that it captures indirect relationships
between bigrams which cannot be done using the
146
first order representation. For example if the word
ergonomics occurs along with science, and work-
place occurs with science, but not with ergonomics,
then workplace and ergonomics are second order
co-occurrences by virtue of their respective co-
occurrences with science.
Once the context is represented by either a first
order or a second order vector, then clustering can
follow. A hybrid method known as Repeated Bisec-
tions is employed, which tries to balance the quality
of agglomerative clustering with the speed of parti-
tional methods. In our current approach the number
of clusters to be discovered must be specified. Mak-
ing it possible to automatically identify the number
of clusters is one of our high priorities for future
work.
5 Labeling
Once the clusters are created, we assign each cluster
a descriptive and discriminating label. A label is a
list of bigrams that act as a simple summary of the
contents of the cluster.
Our current approach for descriptive labels is to
select the top N bigrams from contexts grouped in a
cluster. We use similar techniques as we use for fea-
ture identification, except now we apply them on the
clustered contexts. In particular, we select the top 5
or 10 bigrams as ranked by the log-likelihood ratio.
We discard bigrams if either of the words is a stop-
word, or if the bigram occurs only one time. For dis-
criminating labels we pick the top 5 or 10 bigrams
which are unique to the cluster and thus capture the
contents that separates one cluster from another.
6 Experimental Data
Our experimental data consists of two or more un-
ambiguous names whose occurrences in a corpus
have been conflated in order to create ambiguity.
These conflated forms are sometimes known as
pseudo words. For example, we take all occurrences
of Tony Blair and Bill Clinton and conflate them into
a single name that we then attempt to discriminate.
Further, we believe that the use of artificial pseudo
words is suitable for the problem of name discrim-
ination, perhaps more so than is the case in word
sense disambiguation in general. For words there is
always a debate as to what constitutes a word sense,
and how finely drawn a sense distinction should be
made. However, when given an ambiguous name
there are distinct underlying entities associated with
that name, so evaluation relative to such true cate-
gories is realistic.
Our source of data is the New York Times (Jan-
uary 2000 to June 2002) corpus that is included as a
part of the English GigaWord corpus.
In creating the contexts that include our conflated
names, we retain 25 words of text to the left and also
to the right of the ambiguous conflated name. We
also preserve the original names in a separate tag for
the evaluation stage.
We have created three levels of ambiguity: 2-way,
3-way, and 4-way. In each of the three categories we
have 3-4 examples that represent a variety of differ-
ent degrees of ambiguity. We have created several
examples of intra-category disambiguation, includ-
ing Bill Clinton and Tony Blair (political leaders),
and Mexico and India (countries). We also have
inter-category disambiguation such as Bayer, Bank
of America, and John Grisham (two companies and
an author).
The 3-way examples have been chosen by adding
one more dimension to the 2-way examples. For ex-
ample, Ehud Barak is added to Bill Clinton and Tony
Blair, and the 4-way examples are selected on simi-
lar lines.
7 Experimental Results
Table 1 summarizes the results of our experiments in
terms of the F-Measure, which is the harmonic mean
of precision and recall. Precision is the percentage
of contexts clustered correctly out of those that were
attempted. Recall is the percentage of contexts clus-
tered correctly out of the total number of contexts
given.
The variable M in Table 1 shows the number of
contexts of that target name in the input data. Note
that we divide the total input data into equal-sized
test and feature selection files, so the number of fea-
ture selection and test contexts is half of what is
shown, with approximately the same distribution of
names. (N) specifies the total number of contexts in
the input data. MAJ. represents the percentage of
the majority name in the data as a whole, and can be
viewed as a baseline measure of performance that
147
Table 1: Experimental Results (F-measure)
MAJ. K Order 1 Order 2
Target Word(M);+ (N) FSD TST FSD FSD/S TST TST/S
BAYER(1271); 60.0 2 67.2 68.6 71.0 51.3 69.2 53.2
BOAMERICA(846) (2117) 6 37.4 33.9 47.2 53.3 42.8 49.6
BCLINTON(1900); 50.0 2 82.2 87.6 81.1 81.2 81.2 70.3
TBLAIR(1900) (3800) 6 58.5 61.6 61.8 71.4 61.5 72.3
MEXICO(1500); 50.0 2 42.3 52.4 52.7 54.5 52.6 54.5
INDIA(1500) (3000) 6 28.4 36.6 37.5 49.0 37.9 52.4
THANKS(817); 55.6 2 61.2 65.3 61.4 56.7 61.4 56.7
RCROWE(652) (1469) 6 36.3 41.2 38.5 52.0 39.9 47.8
BAYER(1271);BOAMERICA(846); 43.2 3 69.7 73.7 57.1 54.7 55.1 54.7
JGRISHAM(828); (2945) 6 31.5 38.4 32.7 53.1 32.8 52.8
BCLINTON(1900);TBLAIR(1900); 33.3 3 51.4 56.4 47.7 44.8 47.7 44.9
EBARAK(1900); (5700) 6 58.0 54.1 43.8 48.1 43.7 48.1
MEXICO(1500);INDIA(1500); 33.3 3 40.4 41.7 38.1 36.5 38.2 37.4
CALIFORNIA(1500) (4500) 6 31.5 38.4 32.7 36.2 32.8 36.2
THANKS(817);RCROWE(652); 35.4 4 42.7 61.5 42.9 38.5 42.7 37.6
BAYER(1271);BOAMERICA(846) (3586) 6 47.0 53.0 43.9 34.0 43.5 34.6
BCLINTON(1900);TBLAIR(1900); 25.0 4 48.4 52.3 44.2 50.1 44.7 51.4
EBARAK(1900);VPUTIN(1900) (7600) 6 51.8 47.8 43.4 49.3 44.4 50.6
MEXICO(1500);INDIA(1500); 25.0 4 34.4 35.7 29.2 27.4 29.2 27.1
CALIFORNIA(1500);PERU(1500) (6000) 6 31.3 32.0 27.3 27.2 27.2 27.2
Table 2: Sense Assignment Matrix (2-way)
TBlair BClinton
C0 784 50 834
C1 139 845 984
923 895 1818
would be achieved if all the contexts to be clustered
were placed in a single cluster.
K is the number of clusters that the method will
attempt to classify the contexts into. FSD are the
experiments where a separate set of data is used as
the feature selection data. TST are the experiments
where the features are extracted from the test data.
For FSD and TST experiments, the complete context
was used to create the context vector to be clustered,
whereas for FSD/S and TST/S in the order 2 experi-
ments, only the five words on either side of the target
name are averaged to form the context-vector.
For each name conflated sample we evaluate our
Table 3: Sense Assignment Matrix (3-way)
BClinton TBlair EBarak
C0 617 57 30 704
C1 65 613 558 1236
C2 215 262 356 833
897 932 944 2773
methods by setting K to the exact number of clus-
ters, and then for 6 clusters. The motivation for the
higher value is to see how well the method performs
when the exact number of clusters is unknown. Our
belief is that with an artificially- high number spec-
ified, some of the resulting clusters will be nearly
empty, and the overall results will still be reason-
able. In addition, we have found that the precision
of the clusters associated with the known names re-
mains high, while the overall recall is reduced due to
the clusters that can not be associated with a name.
To evaluate the performance of the clustering,
148
Table 4: Labels for Name Discrimination Clusters (found in Table 1)
Original Name Type Created Labels
CLUSTER 0: Desc. Britain, British Prime, Camp David, Middle East, Minister, New York,
TONY Prime, Prime Minister, U S, Yasser Arafat
BLAIR Disc. Britain, British Prime, Middle East, Minister, Prime, Prime Minister
CLUSTER 1: Desc. Al Gore, Ariel Sharon, Camp David, George W, New York, U S, W Bush,
BILL White House, prime minister
CLINTON Disc. Al Gore, Ariel Sharon, George W, W Bush
CLUSTER 2: Desc. Bill Clinton, Camp David, New York, President, U S, White House,
EHUD Yasser Arafat, York Times, minister, prime minister
BARAK Disc. Bill Clinton, President, York Times, minister
a contingency matrix (e.g., Table 2 or 3) is con-
structed. The columns are re-arranged to maximize
the sum of the cells along the main diagonal. This
re-arranged matrix decides the sense that gets as-
signed to the cluster.
8 Discussion
The order 2 experiments show that limiting the
scope in the test contexts (and thereby creating an
averaged vector from a subset of the context) is more
effective than using the entire context. This corre-
sponds to the findings of (Pedersen et.al., 2005). The
words closest to the target name are most likely to
contain identifying information, whereas those that
are further away may be more likely to introduce
noise.
As the amount and the number of contexts to be
clustered (and to be used for feature identification)
increases, the order 1 context representation per-
forms better. This is because in the larger samples of
data it is more likely to find an exact match for a fea-
ture and thereby achieve overall better results. We
believe that this is why the order 1 results are gener-
ally better for the 3-way and 4-way distinctions, as
opposed to the 2-way distinctions. This observation
is consistent with earlier findings by Purandare and
Pedersen for general English text.
An example of a 2-way clustering is shown in Ta-
ble 2, where Cluster 0 is assigned to Tony Blair, and
Cluster 1 is for Bill Clinton. In this case the preci-
sion is 89.60 ((1629/1818)*100), whereas the recall
is 85.69 ((1629/1818+83)*100). This suggests that
there were 83 contexts that the clustering algorithm
was unable to assign, and so they were not clustered
and removed from the results.
Table 3 shows the contingency matrix for a 3-
way ambiguity. The distribution of contexts in clus-
ter 0 show that the single predominant sense in the
cluster is Bill Clinton, but for cluster 1 though the
number of contexts indicate clear demarcation be-
tween BClinton and TBlair, this distinction gets less
clear between TBlair and EBarak. This suggests that
perhaps the level of details in the New York Times
regarding Bill Clinton and his activities may have
been greater than that for the two non-US leaders,
although we will continue to analyze results of this
nature.
We can see from the labeling results shown in Ta-
ble 4 that clustering performance affects the quality
of cluster labels. Thus the quality of labels for clus-
ter assigned to BClinton and TBlair are more sug-
gestive of the underlying entity than are the labels
for EBarak clusters.
9 Future Work
We wish to supplement our cluster labeling tech-
nique by using World Wide Web (WWW) based
methods (like Google-Sets) for finding words related
to the target name and other significant words in the
context. This would open up a venue for large and
multi-dimensional data. We are cautious though that
we would have to deal with the problems of noisy
data that WWW brings along with the good data.
Another means of improving the clustering labeling
will be using WordNet::Similarity to find the relat-
edness amongst the words from the cluster using the
knowledge of WordNet as is also proposed by (Mc-
Carthy et.al., 2004).
149
Currently the number of clusters that the con-
texts should be grouped into has to be specified by
the user. We wish to automate this process such
that the clustering algorithm will automatically de-
termine the optimal number of clusters. We are ex-
ploring a number of options, including the use of
GAP statistic (Tibshirani et.al., 2000).
For the order 2 representation of the contexts there
is considerable noise induced in the resulting con-
text vector because of the averaging of all the word-
vectors. Currently we reduce the noise in the av-
eraged vector by limiting the word vectors to those
associated with words that are located near the tar-
get name. We also plan to develop methods that se-
lect the words to be included in the averaged vec-
tor more carefully, with an emphasis on locating the
most content rich words in the context.
Thus far we have tested our methods for one-
to-many discrimination. This resolves cases where
the same name is used by multiple different peo-
ple. However, we will also test our techniques for
the many-to-one kind ambiguity that occurs when
the same person is referred by multiple names, e.g.,
President Bush, George Bush, Mr. Bush, and Presi-
dent George W. Bush.
Finally, we will also evaluate our method on real
data. In particular, we will use the John Smith Cor-
pus as compiled by Bagga and Baldwin, and the
name data generated by Mann and Yarowsky for
their experiments.
10 Conclusions
We have shown that word sense discrimination tech-
niques can be extended to address the problem of
name discrimination. The experiments with second
order context representation work better with limited
or localized scope. As the dimensionality of the am-
biguity increases first order context representation
out-performs second order representation. The la-
beling of clusters using the simple technique of sig-
nificant bigram selection also shows encouraging re-
sults which highly depends on the performance of
the clustering of contexts.
11 Acknowledgments
I would like to thank my advisor Dr. Ted Pedersen
for his continual guidance and support.
I would also like to thank Dr. James Riehl, Dean
of the College of Science and Engineering, and Dr.
Carolyn Crouch, Director of Graduate Studies in
Computer Science, for awarding funds to partially
cover the expenses to attend the Student Research
Workshop at ACL 2005.
I am also thankful to Dr. Regina Barzilay and
the ACL Student Research Workshop organizers for
awarding the travel grant.
This research has been supported by a National
Science Foundation Faculty Early CAREER Devel-
opment Award (#0092784) during the 2004-2005
academic year.
References
Amruta Purandare and Ted Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. The Proceedings of the Conference
on Computational Natural Language Learning, pages
41-48. Boston, MA.
Gideon Mann and David Yarowsky. 2003. Unsupervised
personal name disambiguation. The Proceedings of
the Conference on Computational Natural Language
Learning, pages 33-40. Edmonton, Canada.
Amit Bagga and Breck Baldwin. 1998. Entity?based
cross?document co?referencing using the vector space
model. The Proceedings of the 17th international
conference on Computational linguistics, pages 79-85.
Montreal, Quebec, Canada.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. The Proceed-
ings of HLT-NAACL, pages 321-328. Boston, MA.
Diana McCarthy, Rob Koeling, Julie Weeds and
John Carroll. 2004. Finding Predominant Word
Senses in Untagged Text. The Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, pages 279-286. Barcelona, Spain.
Robert Tibshirani, Guenther Walther and Trevor Hastie.
2000. Estimating the number of clusters in a dataset
via the Gap statistic. Journal of the Royal Statistics
Society (Series B), 2000.
Ted Pedersen, Amruta Purandare and Anagha Kulkarni
2005. Name Discrimination by Clustering Similar
Contexts. The Proceedings of the Sixth International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 226-237. Mexico City,
Mexico.
Schu?tze H. 1998. Automatic Word Sense Discrimination
Computational Linguistics, 24(1):97-124.
150
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 105?108, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
SenseClusters: Unsupervised Clustering and Labeling of Similar Contexts
Anagha Kulkarni and Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
{kulka020,tpederse}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
SenseClusters is a freely available system
that identifies similar contexts in text. It
relies on lexical features to build first and
second order representations of contexts,
which are then clustered using unsuper-
vised methods. It was originally devel-
oped to discriminate among contexts cen-
tered around a given target word, but can
now be applied more generally. It also
supports methods that create descriptive
and discriminating labels for the discov-
ered clusters.
1 Introduction
SenseClusters seeks to group together units of text
(referred to as contexts) that are similar to each other
using lexical features and unsupervised clustering.
Our initial work (Purandare and Pedersen, 2004)
focused on word sense discrimination, which takes
as input contexts that each contain a given target
word, and produces as output clusters that are pre-
sumed to correspond to the different senses of the
word. This follows the hypothesis of (Miller and
Charles, 1991) that words that occur in similar con-
texts will have similar meanings.
We have shown that these methods can be ex-
tended to proper name discrimination (Pedersen et
al., 2005). People, places, or companies often share
the same name, and this can cause a considerable
amount of confusion when carrying out Web search
or other information retrieval applications. Name
discrimination seeks to group together the contexts
that refer to a unique underlying individual, and al-
low the user to recognize that the same name is being
used to refer to multiple entities.
We have also extended SenseClusters to clus-
ter contexts that are not centered around any tar-
get word, which we refer to as headless clustering.
Automatic email categorization is an example of a
headless clustering task, since each message can be
considered a context. SenseClusters will group to-
gether messages if they are similar in content, with-
out requiring that they share any particular target
word between them.
We are also addressing a well known limitation to
unsupervised clustering approaches. After cluster-
ing contexts, it is often difficult to determine what
underlying concepts or entities each cluster repre-
sents without manually inspecting their contents.
Therefore, we are developing methods that automat-
ically assign descriptive and discriminating labels to
each discovered cluster that provide a characteriza-
tion of the contents of the clusters that a human can
easily understand.
2 Clustering Methodology
We begin with the collection of contexts to be clus-
tered, referred to as the test data. These may all in-
clude a given target word, or they may be headless
contexts. We can select the lexical features from the
test data, or from a separate source of data. In either
case, the methodology proceeds in exactly the same
way.
SenseClusters is based on lexical features, in par-
ticular unigrams, bigrams, co?occurrences, and tar-
105
get co?occurrences. Unigrams are single words that
occur more than five times, bigrams are ordered
pairs of words that may have intervening words be-
tween them, while co-occurrences are simply un-
ordered bigrams. Target co-occurrences are those
co?occurrences that include the given target word.
We select bigrams and co?occurrences that occur
more than five times, and that have a log?likelihood
ratio of more than 3.841, which signifies a 95% level
of certainty that the two words are not independent.
We do not allow unigrams to be stop words, and we
eliminate any bigram or co?occurrence feature that
includes one or more stop words.
Previous work in word sense discrimination has
shown that contexts of an ambiguous word can be ef-
fectively represented using first order (Pedersen and
Bruce, 1997) or second order (Schu?tze, 1998) rep-
resentations. SenseClusters provides extensive sup-
port for both, and allows for them to be applied in a
wider range of problems.
In the first order case, we create a context (rows)
by lexical features (columns) matrix, where the fea-
tures may be any of the above mentioned types. The
cell values in this matrix record the frequencies of
each feature occurring in the context represented by
a given row. Since most lexical features only occur a
small number of times (if at all) in each context, the
resulting matrix tends to be very sparse and nearly
binary. Each row in this matrix forms a vector that
represents a context. We can (optionally) use Sin-
gular Value Decomposition (SVD) to reduce the di-
mensionality of this matrix. SVD has the effect of
compressing a sparse matrix by combining redun-
dant columns and eliminating noisy ones. This al-
lows the rows to be represented with a smaller num-
ber of hopefully more informative columns.
In the second order context representation we start
with creating a word by word co-occurrence ma-
trix where each row represent the first word and the
columns represent the second word of either bigram
or co?occurrence features previously identified. If
the features are bigrams then the word matrix is
asymmetric whereas for co-occurrences it is sym-
metric and the rows and columns do not suggest any
ordering. In either case, the cell values indicate how
often the two words occur together, or contains their
log?likelihood score of associativity. This matrix is
large and sparse, since most words do not co?occur
with each other. We may optionally apply SVD to
this co-occurrence matrix to reduce its dimension-
ality. Each row of this matrix is a vector that repre-
sents the given word at the row via its co?occurrence
characteristics. We create a second order represen-
tation of a context by replacing each word in that
context with its associated vector, and then averag-
ing together all these word vectors. This results in a
single vector that represents the overall context.
For contexts with target words we can restrict the
number of words around the target word that are av-
eraged for the creation of the context vector. In our
name discrimination experiments we limit this scope
to five words on either side of the target word which
is based on the theory that words nearer to the tar-
get word are more related to it than the ones that are
farther away.
The goal of the second order context represen-
tation is to capture indirect relationships between
words. For example, if the word Dictionary occurs
with Words but not with Meanings, and Words oc-
curs with Meanings, then the words Dictionary and
Meanings are second order co-occurrences via the
first order co-occurrence of Words.
In either the first or second order case, once we
have each context represented as a vector we pro-
ceed with clustering. We employ the hybrid clus-
tering method known as Repeated Bisections, which
offers nearly the quality of agglomerative clustering
at the speed of partitional clustering.
3 Labeling Methodology
For each discovered cluster, we create a descriptive
and a discriminating label, each of which is made
up of some number of bigram features. These are
identified by treating the contexts in each cluster as
a separate corpora, and applying our bigram feature
selection methods as described previously on each
of them.
Descriptive labels are the top N bigrams accord-
ing to the log?likelihood ratio. Our goal is that these
labels will provide clues as to the general nature of
the contents of a cluster. The discriminating labels
are any descriptive labels for a cluster that are not
descriptive labels of another cluster. Thus, the dis-
criminating label may capture the content that sep-
arates one cluster from another and provide a more
106
Table 1: Name Discrimination (F-measure)
MAJ. O1 O2
2-Way Name(M);+ (N) k=2 k=2
AAIRLINES(1075); 50.0 66.6 58.8
TCRUISE(1075) (2150)
AAIRLINES(3966); 51.7 61.7 59.6
HPACKARD(3690) (7656)
BGATES(1981); 64.8 63.4 53.8
TCRUISE(1075) (3056)
BSPEARS(1380); 50.0 56.6 65.8
GBUSH(1380) (2760)
3-Way Name (M);+ k=3 k=3
AAIRLINES(2500); 33.3 41.4 45.1
HPACKARD(2500); (7500)
BMW(2500);
AAIRLINES(1300); 33.3 46.0 45.3
HPACKARD(1300); (3900)
BSPEARS(1300);
BGATES(1075); 33.3 53.7 53.6
TCRUISE(1075); (3225)
GBUSH(1075)
detailed level of information.
4 Experimental Data
We evaluate these methods on proper name discrim-
ination and email (newsgroup) categorization.
For name discrimination we use the 700 million
word New York Times portion of the English Giga-
Word corpus as the source of contexts. While there
are many ambiguous names in this data, it is difficult
to evaluate the results of our approach given the ab-
sence of a disambiguated version of the text. Thus,
we automatically create ambiguous names by con-
flating the occurrences associated with two or three
relatively unambiguous names into a single obfus-
cated name.
For example, we combine Britney Spears and
George Bush into an ambiguous name Britney Bush,
and then see how well SenseClusters is able to cre-
ate clusters that reflect the true underlying identity
of the conflated name.
Our email experiments are based on the 20-
NewsGroup Corpus of USENET articles. This is
a collection of approximately 20,000 articles that
Table 2: Email Categorization (F-measure)
MAJ. O1 O2
Newsgroup(M);+ (N) k=2 k=2
comp.graphics(389); 50.1 61.1 63.9
misc.forsale(390) (779)
comp.graphics(389); 50.8 73.6 54.8
talk.pol.mideast(376) (756)
rec.motorcycles(398); 50.13 83.1 60.5
sci.crypt(396) (794)
rec.sport.hockey(399); 50.1 77.6 58.5
soc.relig.christian(398) (797)
sci.electronics(393); 50.3 67.8 52.3
soc.relig.christian(398) (791)
have been taken from 20 different newsgroups. As
such they are already classified, but since our meth-
ods are unsupervised we ignore this information un-
til it is time to evaluate our approach. We present
results that make two way distinctions between se-
lected pairs of newsgroups.
5 Experimental Results and Discussion
Table 1 presents the experimental results for 2-way
and 3-way name discrimination experiments, and
Table 2 presents results for a 2-way email cate-
gorization experiment. The results are reported in
terms of the F-measure, which is the harmonic mean
of precision and recall.
The first column in both tables indicates the possi-
ble names or newgroups, and the number of contexts
associated with each. The next column indicates the
percentage of the majority class (MAJ.) and count
(N) of the total number of contexts for the names
or newsgroups. The majority percentage provides a
simple baseline for level of performance, as this is
the F?measure that would be achieved if every con-
text were simply placed in a single cluster. We refer
to this as the unsupervised majority classifier.
The next two columns show the F?measure asso-
ciated with the order 1 and order 2 representations
of context, with all other options being held con-
stant. These experiments used bigram features, SVD
was performed as appropriate for each representa-
tion, and the method of Repeated Bisections was
used for clustering.
107
Table 3: Cluster Labels (for Table 1)
True Name Created Labels
CLUSTER 0: Flight 11, Flight 587, Sept 11,
AMERICAN Trade Center, World Trade,
AIRLINES Los Angeles, New York
CLUSTER 1: Jerry Maguire,
TOM Mission Impossible,
CRUISE Minority Report, Tom Cruise,
Penelope Cruz, Nicole Kidman,
United Airlines, Vanilla Sky,
Los Angeles, New York
CLUSTER 0: George Bush , George W,
GEORGE Persian Gulf, President, U S,
BUSH W Bush, former President,
lifting feeling, White House
CLUSTER 1: Chairman , Microsoft ,
BILL Microsoft Chairman,
GATES co founder, News Service,
operating system,
chief executive, White House
CLUSTER 2: Jerry Maguire,
TOM Mission Impossible,
CRUISE Minority Report, Al Gore,
New York , Nicole Kidman,
Penelope Cruz, Vanilla Sky,
Ronald Reagan, White House
Finally, note that the number of clusters to be dis-
covered must be provided by the user. In these ex-
periments we have taken the best case approach and
asked for a number of clusters equal to that which
actually exists. We are currently working to develop
methods that will automatically stop at an optimal
number of clusters, to avoid setting this value man-
ually.
In general all of our results significantly improve
upon the majority classifier, which suggests that the
clustering of contexts is successfully discriminating
among ambiguous names and uncategorized email.
Table 3 shows the descriptive and discriminating
labels assigned to the 2?way experimental case of
American Airlines and Tom Cruise, as well as the
3?way case of George Bush, Bill Gates and Tom
Cruise. The bold face labels are those that serve
as both descriptive and discriminating labels. The
fact that most labels serve both roles suggests that
the highest ranked bigrams in each cluster were also
unique to that cluster. The normal font indicates
labels that are only descriptive, and are shared be-
tween multiple clusters. There are only a few such
cases, for example White House happens to be a sig-
nificant bigram in all three of the clusters in the 3?
way case. There were no labels that were exclu-
sively discriminating in these experiments, suggest-
ing that the clusters are fairly clearly distinguished.
Please note that some labels include unigrams
(e.g., President for George Bush). These are created
from bigrams where the other word is the conflated
form, which is not included in the labels since it is
by definition ambiguous.
6 Acknowledgements
This research is partially supported by a National
Science Foundation Faculty Early CAREER Devel-
opment Award (#0092784).
References
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Lan-
guage Processing, pages 197?207, Providence, RI,
August.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In Pro-
ceedings of the Sixth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, pages 220?231, Mexico City, February.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
108
Improving Name Discrimination: A Language Salad Approach
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
Roxana Angheluta
Attentio SA
B-1030 Brussels, Belgium
roxana@attentio.com
Zornitsa Kozareva
Dept. de Lenguajes y Sistemas Informa?ticos
University of Alicante
03080 Alicante, Spain
zkozareva@dlsi.ua.es
Thamar Solorio
Department of Computer Science
University of Texas at El Paso
El Paso, TX 79902 USA
tsolorio@utep.edu
Abstract
This paper describes a method of discrim-
inating ambiguous names that relies upon
features found in corpora of a more abun-
dant language. In particular, we discrim-
inate ambiguous names in Bulgarian, Ro-
manian, and Spanish corpora using infor-
mation derived from much larger quan-
tities of English data. We also mix to-
gether occurrences of the ambiguous name
found in English with the occurrences of
the name in the language in which we are
trying to discriminate. We refer to this as
a language salad, and find that it often re-
sults in even better performance than when
only using English or the language itself
as the source of information for discrimi-
nation.
1 Introduction
Name ambiguity is a problem that is increasing
in complexity and scope as online information
sources grow and expand their coverage. Like
words, names are often ambiguous and can refer
to multiple underlying entities or concepts. Web
searches for names can often return results asso-
ciated with multiple people or organizations in a
disorganized and unclear fashion. For example,
the top 10 results of a Google search for George
Miller includes a mixture of entries for two dif-
ferent entities, one a psychology professor from
Princeton University and the other the director of
the film Mad Max.1
Name discrimination takes some number of
contexts that include an ambiguous name, and di-
vides them into groups or clusters, where the con-
1Search conducted January 4, 2006.
texts in each cluster should ideally refer to the
same underlying entity (and each cluster should
refer to a different entity). Thus, if we are given
10,000 contexts that include the name John Smith,
we would want to divide those contexts into clus-
ters corresponding to each of the different under-
lying entities that share that name.
We have developed an unsupervised method of
name discrimination (Pedersen et al, 2005). We
have shown the method to be language indepen-
dent (Pedersen et al, 2006), which is to say we
can apply it to English contexts as easily as we
can apply it to Romanian or French. However,
we have observed that there are situations where
the number of contexts in which an ambiguous
name occurs is relatively small, perhaps because
the name itself is unusual, or because the quantity
of data available for language is limited in general.
These problems of scarcity can make it difficult to
apply these methods and discriminate ambiguous
names, especially in languages with fewer online
resources.
This paper presents a method of name discrim-
ination is based on using a larger number of con-
texts in English that include an ambiguous name,
and applying information derived from these con-
texts to the discrimination of that name in another
language, where there are many fewer contexts.
We also show that mixing English contexts with
the contexts to be discriminated can result in a
performance improvement over only using the En-
glish or the original contexts alone.
2 Discrimination by Clustering Contexts
Our method of name discrimination is described in
more detail in (Pedersen et al, 2005), but in gen-
eral is based on an unsupervised approach to word
sense discrimination introduced by (Purandare and
25
Pedersen, 2004), which builds upon earlier work
in word sense discrimination, including (Schu?tze,
1998) and (Pedersen and Bruce, 1997).
Our method treats each occurrence of an am-
biguous name as a context that is to be clustered
with other contexts that also include the same
name. In this paper, each context consists of about
50 words, where the ambiguous name is generally
in the middle of the context. The goal is to cluster
similar contexts together, based on the presump-
tion that the occurrences of a name that appear
in similar contexts will refer to the same underly-
ing entity. This approach is motivated by both the
distributional hypothesis (Harris, 1968) and the
strong contextual hypothesis (Miller and Charles,
1991).
2.1 Feature Selection
The contexts to be clustered are represented by
lexical features which may be selected from either
the contexts being clustered, or from a separate
corpus. In this paper we use both approaches. We
cluster the contexts based on features identified in
those very same contexts, and we also cluster the
contexts based on features identified in a separate
set of data (in this case English). We explore the
use of a mixed feature selection strategy where we
identify features both from the data to be clustered
and the separate corpus of English text. Thus, our
feature selection data may come from one of three
sources: the contexts to be clustered (which we
will refer to as the evaluation contexts), English
contexts which include the same name but are not
to be clustered, and the combination of these two
(our so-called Language Salad or Mix).
The lexical features we employ are bigrams,
that is consecutive words that occur together in the
corpora from which we are identifying features. In
this work we identify bigram features using Point-
wise Mutual Information (PMI). This is defined as
the log of the ratio of the observed frequency with
which the two words occur together in the feature
selection data, to the expected number of times
the two words would occur together in a corpus if
they were independent. This expected value is es-
timated simply by taking the product of the num-
ber of times the two words occur individually, and
dividing this by the total number of bigrams in the
feature selection data. Thus, larger values of PMI
indicate that the observed frequency of the bigram
is greater than would be expected if the two words
were independent.
In these experiments we take the top 500 ranked
bigrams that occur five or more times in the feature
selection data. We also exclude any bigram from
consideration that is made up of one or two stop
words, which are high frequency function words
that have been specified in a manually created list.
Note that with smaller numbers of contexts (usu-
ally 200 or fewer), we lower the frequency thresh-
old to two or more.
In general PMI is known to have a bias towards
pairs of words (bigrams) that occur a small num-
ber of times and only with each other. In this work
that is a desirable quality, since that will tend to
identify pairs of words that are very strongly as-
sociated with each other and also provide unique
discriminating information.
2.2 Context Representation
Once the bigram features have been identified,
then the contexts to be clustered are represented
using second order co-occurrences that are de-
rived from those bigrams. In general a second
order co-occurrence is a pair of words that may
not occur with each other, but that both occur fre-
quently with a third word. For example, garden
and fire may not occur together often, but both
commonly occur with hose. Thus, garden hose
and fire hose represent first order co?occurrences,
and garden and fire represent a second order co?
occurrence.
The process of creating the second order repre-
sentation has several steps. First, the bigram fea-
tures identified by PMI (the top ranked 500 bi-
grams that have occurred 5 or more times in the
feature selection data) are used to create a word
by word co?occurrence matrix. The first word in
each bigram represents a row in the matrix, and the
second word in each bigram represents a column.
The cells in the matrix contain the PMI scores.
Note that this matrix is not symmetric, and that
there are many words that only occur in either a
row or a column (and not both) because they tend
to occur as the first or second word in a bigram.
For example, President might tend to be a first
word in a bigram (e.g., President Clinton, Presi-
dent Putin), whereas last names will tend to be the
second word.
Once the co?occurrence matrix is created, then
the contexts to be clustered can be represented.
Each word in the context is checked to see if it
26
has a corresponding row (i.e., vector) in the co?
occurrence matrix. If it does, that word is replaced
in the context by the row from the matrix, so that
the word in the context is now represented by the
vector of words with which it occurred in the fea-
ture selection data. If a word does not have a corre-
sponding entry in the co?occurrence matrix, then
it is simply removed from the context. After all
the words in the context are checked, then all of
the vectors that are selected are averaged together
to create a vector representation of the context.
Then these contexts are clustered into a pre?
specified number of clusters using the k?means
algorithm. Note that we are currently develop-
ing methods to automatically select the number of
clusters in the data (e.g., (Pedersen and Kulkarni,
2006)), although we have not yet applied them to
this particular work.
3 The Language Salad
In this paper, we explore the creation of a second
order representation for a set of evaluation con-
texts using three different sets of feature selection
data. The co?occurrence matrix may be derived
from the evaluation contexts themselves, or from
a separate set of contexts in a different language,
or from the combination of these two (the Salad or
Mix).
For example, suppose we have 100 Romanian
evaluation contexts that include an ambiguous
name, and that same name also occurs 10,000
times in an English language corpus.2 Our goal
is to cluster the 100 Romanian contexts, which
contain all the information that we have about the
name in Romanian. While we could derive a sec-
ond order representation of the contexts, the re-
sulting co?occurrence matrix would likely be very
small and sparse, and insufficient for making good
discrimination decisions. We could instead rely
on first order features, that is look for frequent
words or bigrams that occur in the evaluation con-
texts, and try and find evaluation contexts that
share some of the same words or phrases, and clus-
ter them based on this type of information. How-
ever, again, the small number of contexts available
would likely result in very sparse representations
for the contexts, and unreliable clustering results.
Thus, our method is to derive a co?occurrence
matrix from a language for which we have many
2We assume that the names either have the same spelling
in both languages, or that translations are readily available.
occurrences of the ambiguous name, and then use
that co?occurrence matrix to represent the evalua-
tion contexts. This relies on the fact that the eval-
uation contexts will contain at least a few names
or words that are also used in the larger corpus (in
this case English). In general, we have found that
while this is not always true, it is often the case.
We have also experimented with combining the
English contexts with the evaluation contexts, and
building a co?occurrence matrix based on this
combined or mixed collection of contexts. This
is the language salad that we refer to, a mixture of
contexts in two different languages that are used to
derive a representation of the evaluation contexts.
4 Experimental Data
We use data in four languages in these experi-
ments, Bulgarian, English, Romanian, and Span-
ish.
4.1 Raw Corpora
The Romanian data comes from the 2004 archives
of the newspaper Adevarul (The Truth)3. This is a
daily newspaper that is among the most popular in
Romania. While Romanian normally has diacrit-
ical markings, this particular newspaper does not
include those in their online edition, so the alpha-
bet used was the same as English.
The Bulgarian data is from the Sega 2002 news
corpus, which was originally prepared for the
CLEF competition.4 This is a corpus of news arti-
cles from the Newspaper Sega5, which is based in
Sofia, Bulgaria. The Bulgarian text was translit-
erated (phonetically) from Cyrillic to the Roman
alphabet. Thus, the alphabet used was the same
as English, although the phonetic transliteration
leads to fewer cognates and borrowed English
words that are spelled exactly the same as in En-
glish text.
The Spanish corpora comes from the Spanish
news agency EFE from the year 1994 and 1995.
This collection was used in the Question Answer-
ing Track at CLEF-2003, and also for CLEF-2005.
This text is represented in Latin-1, and includes
the usual accents that appear in Spanish.
The English data comes from the GigaWord
corpus (2nd edition) that is distributed by the Lin-
guistic Data Consortium. This consists of more
3http://www.adevarulonline.ro/arhiva
4http://www.clef-campaign.org
5http://www.segabg.com
27
than 2 billion words of newspaper text that comes
from five different news sources between the years
1994 and 2004. In fact, we subdivide the English
data into three different corpora, where one is from
2004, another from 2002, and the third from 1994-
95, so that for each of the evaluation languages
(Bulgarian, Spanish, and Romanian) we have an
English corpus from the same time period.
4.2 Evaluation Contexts
Our experimental data consists of evaluation con-
texts derived from the Bulgarian, Romanian, and
Spanish corpora mentioned above. We also have
English corpora that includes the same ambiguous
names as found in the evaluation contexts.
In order to quickly generate a large volume of
experimental data, we created evaluation contexts
from the corpora for each of our four languages
by conflating together pairs of well known names
or places, and that are generally not highly am-
biguous (although some might be rather general).
For example, one of the pairs of names we con-
flate is George Bush and Tony Blair. To do that,
every occurrence of both of these names is con-
verted to an ambiguous form (GB TB, for exam-
ple), and the discrimination task is to cluster these
contexts such that their original and correct name
is re?discovered. We retain a record of the orig-
inal name for each occurrence, so as to evaluate
the results of our method. Of course we do not use
this information anywhere in the process outside
of evaluation.
The following pairs of names were conflated in
all four of the languages: George Bush-Tony Blair,
Mexico-India, USA-Paris, Ronaldo-David Beck-
ham (2002 and 2004), Diego Maradona-Roberto
Baggio (1994-95 only), and NATO-USA. Note
that some of these names have different spellings
in some of our languages, so we look for and con-
flate the native spelling of the names in the differ-
ent language corpora. These pairs were selected
because they occur in all four of our languages,
and they represent name distinctions that are com-
monly of interest, that is they represent ambiguity
in names of people and places. With these pairs
we are also following (Nakov and Hearst, 2003)
who suggest that if one is introducing ambiguity
by creating pseudo?words or conflating names,
then these words should be related in some way
(in order to avoid the creation of very sharp or ob-
vious sense distinctions).
4.3 Discussion
For each of the three evaluation languages (Bul-
garian, Romanian, and Spanish) we have contexts
for five different name conflate pairs that we wish
to discriminate. We have corresponding English
contexts for each evaluation language, where the
dates of both are approximately the same. This
temporal consistency between the evaluation lan-
guage and English is important because the con-
texts in which a name is used may change over
time. In 1994, for example, Tony Blair was not
yet Prime Minister of England (he became PM in
1997), and references to George Bush most likely
refer to the US President who served from 1988
until 1992, rather than the current US President
(who began his term in office in 2001). In 1994
the current (as of 2006) US President had just been
elected governor of Texas, and was not yet a na-
tional figure. This points out that George Bush is
an example of an ambiguous name, but our ob-
servation has been that in the 2002 and 2004 data
(Romanian and Bulgarian) nearly all occurrences
are associated with the current president, and that
most of the occurrences in 1994-95 (Spanish) re-
fer to the former US President. This illustrates
an important point: it is necessary to consider the
perspective represented by the different corpora.
There is little reason to expect that news articles
from Spain in 1994 and 1995 would focus much
attention on the newly elected governor of Texas
in the United States.
Tables 1, 2, and 3 show the number of contexts
that have been collected for each name conflate
pair. For example, in Table 1 we see that there are
746 Bulgarian contexts that refer to either Mex-
ico or India, and that of these 51.47% truly re-
fer to Mexico, and 48.53% to India. There are
149,432 English contexts that mention Mexico or
India, and the Mix value shown is simply the sum
of the number of Bulgarian and English contexts.
In general these tables show that the English
contexts are much larger in number, however,
there are a few exceptions with the Spanish data.
This is because the EFE corpus is relatively large
as compared to the Bulgarian and Romanian cor-
pora, and provides frequency counts that are in
some cases comparable to those in the English cor-
pus.
28
5 Experimental Methodology
For each of the three evaluation languages (Bul-
garian, Romanian, Spanish) there are five name
conflate pairs. The same name conflate pairs
are used for all three languages, except for
Diego Maradona-Roberto Baggio which is only
used with Spanish, and Ronaldo-David Beckham,
which is only used with Bulgarian and Romanian.
This is due to the fact that in 1994-95 (the era
of the Spanish data) neither Ronaldo nor David
Beckham were as famous as they later became, so
they were mentioned somewhat less often than in
the 2002 and 2004 corpora. The other four name
conflate pairs are used in all of the languages.
For each name conflate pair we create a second
order representation using three different sources
of features selection data: the evaluation contexts
themselves, the corresponding English contexts,
and then the mix of the evaluation contexts and the
English contexts (the Mix). The objective of these
experiments is to determine which of these sources
of feature selection data results in the highest F-
Measure, which is the harmonic mean of the pre-
cision and recall of an experiment.
The precision of each experiment is the num-
ber of evaluation contexts clustered correctly, di-
vided by the number of contexts that are clustered.
The clustering algorithm may choose not to assign
every context to a cluster, which is why that de-
nominator may not be the same as the number of
evaluation contexts. The recall of each experiment
is the the number of correctly clustered evaluation
contexts divided by the total number of evaluation
contexts. Note that for each of the three variations
for each name conflate pair experiment exactly the
same evaluation language contexts are being dis-
criminated, all that is changing in each experiment
is the source of the feature selection data. Thus the
F-measures for a name conflate pair in a particular
language can be compared directly. Note however
that the F-measures across languages are harder to
compare directly, since different evaluation con-
texts are used, and different English contexts are
used as well.
There is a simple baseline that can be used as a
point of comparison, and that is to place all of the
contexts for each name conflate pair into one clus-
ter, and say that there is no ambiguity. If that is
done, then the resulting F-Measure will be equal
to the majority percentage of the true underlying
entity as shown in Tables 1, 2, and 3. For exam-
ple, for Bulgarian, if the 746 Bulgarian contexts
for Mexico and India are all put into the same clus-
ter, the resulting F-Measure would be 51.47%, be-
cause we would simply assign all the contexts in
the cluster to the more common of the two entities,
which is Mexico in this case.
6 Experimental Results
Tables 1, 2, and 3 show the results for our exper-
iments, language by language. Each table shows
the results for the 15 experiments done for each
language: five name conflate pairs, each with
three different sources of feature selection data.
The row labeled with the name of the evalua-
tion language reports the F-Measure for the eval-
uation contexts (whose number of occurrences is
shown in the far right column) when the fea-
ture selection data is the evaluation contexts them-
selves. The rows labeled English and Mix report
the F-Measures obtained for the evaluation con-
texts when the feature selection data is the English
contexts, or the Mix of the English and evaluation
contexts.
6.1 Bulgarian Results
The Bulgarian results are shown in Table 1. Note
that the number of contexts for English is consid-
erably larger than for Bulgarian for all five name
conflate pairs. The Bulgarian and English data
came from 2002 news reports.
The Mix of feature selection data results in the
best performance for three of the five name con-
flate pairs: George Bush - Tony Blair, Ronaldo -
David Beckham, and NATO - USA. For remain-
ing two name conflate pairs, just using the Bul-
garian evaluation contexts results in the highest F-
Measure (Mexico-India, USA-Paris).
We believe that this may be partially due to the
fact that the two cases where Bulgarian leads to the
best results are for very general or generic underly-
ing entities: Mexico and India, and then the USA
and Paris. In both cases, contexts that mention
these entities could be discussing a wide range of
topics, and the larger volumes of English data may
simply overwhelm the process with a huge num-
ber of second order features. In addition, it may
be that the English and Bulgarian corpora contain
different content that reflects the different interests
of the original readership of this text. For example,
news that is reported about India might be rather
different in the United States (the source of most
29
Table 1: Bulgarian Results (2002): Feature Selec-
tion Data, F-Measure, and Number of Contexts
George Bush (73.43) - Tony Blair (26.57)
Mix 68.37 11,570
Bulgarian 55.78 651
English 36.15 10,919
Mexico (51.47) - India (48.53)
Bulgarian 70.97 746
Mix 55.01 150,178
English 48.15 149,432
USA (79.53) - Paris (20.47)
Bulgarian 58.67 3,283
Mix 51.68 56,044
English 49.66 52,761
Ronaldo (61.25) - David Beckham (38.75)
Mix 64.88 8,649
Bulgarian 52.75 320
English 48.11 8,329
NATO (87.37) - USA (12.63)
Mix 75.44 54,193
Bulgarian 65.92 3,770
English 60.44 50,423
of the English data) than in Bulgaria. Thus, the
use of the English corpora might not have been
as helpful in those cases where the names to be
discriminated are more global figures. For exam-
ple, Tony Blair and George Bush are probably in
the news in the USA and Bulgaria for many of the
same reasons, thus the underlying content is more
comparable than that of the more general entities
(like Mexico and India) that might have much dif-
ferent content associated with them.
We observed that Bulgarian tends to have fewer
cognates or shared names with English than do
Romanian and English. This is due to the fact
that the Bulgarian text is transliterated. This may
account for the fact that the English-only results
for Bulgarian are very poor, and it is only in com-
bination with the Bulgarian contexts that the En-
glish contexts show any positive effect. This sug-
gests that there are only a few words in the Bulgar-
ian contexts that also occur in English, but those
that do have a positive impact on clustering per-
formance.
6.2 Romanian Results
The Romanian results are shown in Table 2. The
Romanian and English contexts come from 2004.
Table 2: Romanian Results (2004): Feature Selec-
tion Data, F-Measure, and Number of Contexts
Tony Blair (72.00) - George Bush (28.00)
English 64.23 11,616
Mix 54.31 11,816
Romanian 50.75 200
India (53.66) - Mexico (46.34)
Romanian 50.93 82
English 47.30 88,247
Mix 42.55 88,329
USA (60.29) - Paris (39.71)
English 59.05 45,346
Romanian 58.76 700
Mix 57.91 46,046
David Beckham (55.56) - Ronaldo (44.44)
Mix 81.00 4,365
English 70.85 4,203
Romanian 52.47 162
NATO (58.05) - USA (41.95)
Mix 60.48 43,508
Romanian 51.20 1,168
English 38.91 42,340
The Mix of Romanian and English contexts for
feature selection results in improvements for two
of the five pairs (David Beckham - Ronaldo, and
NATO - USA). The use of English contexts only
provides the best results for two other pairs (Tony
Blair - George Bush, and USA - Paris, although in
the latter case the difference in the F-Measures that
result from the three sources of data is minimal).
There is one case (Mexico-India) where using the
Romanian contexts as feature selection data re-
sults in a slightly better F-measure than when us-
ing English contexts.
The improvement that the Mix shows for David
Beckham-Ronaldo is significant, and is perhaps
due to fact that in both English and Romanian text,
the content about Beckham and Ronaldo is simi-
lar, making it more likely that the mix of English
and Romanian contexts will be helpful. However,
it is also true that the Mix results in a significant
improvement for NATO-USA, and it seems likely
that the local perspective in Romania and the USA
would be somewhat different on these two entities.
However, NATO-USA has a relatively large num-
ber of contexts in Romanian as well as English, so
perhaps the difference in perspective had less of
an impact in those cases where the number of Ro-
30
Table 3: Spanish Results (1994-95): Feature Se-
lection Data, F-Measure, and Number of Contexts
George Bush (75.58) - Tony Blair (24.42)
Mix 78.59 2,353
Spanish 64.45 1,163
English 54.29 1,190
D. Maradona (51.55) - R. Baggio (48.45)
English 67.65 1,588
Mix 61.35 3,594
Spanish 60.70 2,006
India (92.34) - Mexico (7.66)
English 72.76 19,540
Spanish 66.57 2,377
Mix 61.54 21,917
USA (62.30) - Paris (37.70)
Spanish 69.31 1,000
English 64.30 17,344
Mix 59.40 18,344
NATO (63.86) - USA (36.14)
Spanish 62.04 2,172
Mix 58.47 27,426
English 56.00 25,254
manian contexts is much smaller (as is the case for
Beckham and Ronaldo).
6.3 Spanish Results
The Spanish results are shown in Table 3. The
Spanish and English contexts come from 1994-
1995, which puts them in a slightly different his-
torical era than the Bulgarian and Romanian cor-
pora.
Due to this temporal difference, we used Diego
Maradona and Roberto Baggio as a conflated pair,
rather than David Beckham and Ronaldo, who
were much younger and somewhat less famous at
that time. Also, Ronaldo is a highly ambiguous
name in Spanish, as it is a very common first name.
This is true in English text as well, although casual
inspection of the English text from 2002 and 2004
(where the Ronaldo-Beckham pair was included
experimentally) reveals that Ronaldo the soccer
player tends to occur more so than any other single
entity named Ronaldo, so while there is a bit more
noise for Ronaldo, there is not really a significant
ambiguity.
For the Spanish results we only note one pair
(George Bush - Tony Blair) where the Mix of En-
glish and Spanish results in the best performance.
This again suggests that the perspective of the
Spanish and English corpora were similar with re-
spect to these entities, and their combination was
helpful. In two other cases (Maradona-Baggio,
India-Mexico) English only contexts achieve the
highest F-Measure, and then in the two remaining
cases (USA-Paris, NATO-USA) the Spanish con-
texts are the best source of features.
Note that for Spanish we have reasonably large
numbers of contexts (as compared to Bulgarian
and Romanian). Given that, it is especially inter-
esting that English-only contexts are the most ef-
fective in two of five cases. This suggests that this
approach may have merit even when the evalua-
tion language does not suffer from problems of ex-
treme scarcity. It may simply be that the informa-
tion in the English corpora provides more discrim-
inating information than does the Spanish, and that
it is somewhat different in content than the Span-
ish, otherwise we would expect the Mix of English
and Spanish contexts to do better than being most
accurate for just one of five cases.
7 Discussion
Of the 15 name conflate experiments (five pairs,
three languages), in only five cases did the use of
the evaluation contexts as a source of feature se-
lection data result in better F-Measure scores than
did either using the English contexts alone or as a
Mix with the evaluation language contexts. Thus,
we conclude that there is a clear benefit to using
feature selection data that comes from a different
language than the one for which discrimination is
being performed.
We believe that this is due to the volume of
the English data, as well as to the nature of the
name discrimination task. For example, a per-
son is often best described or identified by observ-
ing the people he or she tends to associate with,
or the places he or she visits, or the companies
with which he or she does business. If we ob-
serve that George Miller and Mel Gibson occur
together, then it seems we can safely infer that
George Miller the movie director is being referred
to, rather than George Miller the psychologist and
father of WordNet.
This argument might suggest that first order
co?occurrences would be sufficient to discrimi-
nate among the names. That is, simply group the
evaluation contexts based on the features that oc-
cur within them, and essentially cluster evaluation
31
contexts based on the number of features they have
in common with other evaluation contexts. In fact,
results on word sense discrimination (Purandare
and Pedersen, 2004) suggest that first order rep-
resentations are more effective with larger number
of context than second order methods. However,
we see examples in these results that suggests this
may not always be the case. In the Bulgarian re-
sults, the largest number of Bulgarian contexts are
for NATO-USA, but the Mix performs quite a bit
better than Bulgarian only. In the case of Roma-
nian, again NATO-USA has the largest number of
contexts, but the Mix still does better than Roma-
nian only. And in Spanish, Mexico-India has the
largest number of contexts and English-only does
better. Thus, even in cases where we have an abun-
dant number of evaluation contexts, the indirect
nature of the second order representation provides
some added benefit.
We believe that the perspective of the news or-
ganizations providing the corpora certainly has an
impact on the results. For example, in Romanian,
the news about David Beckham and Ronaldo is
probably much the same as in the United States.
These are international figures that are both ex-
ternal to countries where the news originates, and
there is no reason to suppose there would be a
unique local perspective represented by any of the
news sources. The only difference among them
might be in the number of contexts available. In
this situation, the addition of the English contexts
may provide enough additional information to im-
prove discrimination performance in another lan-
guage.
For example, in the 162 Romanian contexts
for Ronaldo-Beckham, there is one occurrence of
Posh, which was the stage name of Beckham?s
wife Victoria. This is below our frequency cut-
off threshold for feature selection, so it would be
discarded when using Romanian?only contexts.
However, in the English contexts Posh is men-
tioned 6 times, and is included as a feature. Thus,
the one occurrence of Posh in the Romanian cor-
pus can be well represented by information found
in the English contexts, thus allowing that Roma-
nian context to be correctly discriminated.
8 Conclusions
This paper shows that a method of name discrim-
ination based on second order context representa-
tions can take advantage of English contexts, and
the mix of English and evaluation contexts, in or-
der to perform more accurate name discrimination.
9 Acknowledgments
This research is supported by a National Sci-
ence Foundation Faculty Early CAREER Devel-
opment Award (#0092784). All of the experiments
in this paper were carried out with version 0.71
SenseClusters package, which is freely available
from http://senseclusters.sourceforge.net.
References
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
P. Nakov and M. Hearst. 2003. Category-based pseu-
dowords. In Companion Volume to the Proceedings
of HLT-NAACL 2003 - Short Papers, pages 67?69,
Edmonton, Alberta, Canada, May 27 - June 1.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 197?207, Providence,
RI, August.
T. Pedersen and A. Kulkarni. 2006. Selecting the
r?ightn?umber of senses based on clustering criterion
functions. In Proceedings of the Posters and Demo
Program of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Trento, Italy, April.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005.
Name discrimination by clustering similar contexts.
In Proceedings of the Sixth International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 220?231, Mexico City, February.
T. Pedersen, A. Kulkarni, R. Angheluta, Z. Kozareva,
and T. Solorio. 2006. An unsupervised language in-
dependent method of name discrimination using sec-
ond order co-occurrence features. In Proceedings
of the Seventh International Conference on Intelli-
gent Text Processing and Computational Linguistics,
pages 208?222, Mexico City, February.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41?48, Boston, MA.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
32
