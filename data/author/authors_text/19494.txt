Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 48?57, Dublin, Ireland, August 23-29 2014.
Capturing Cultural Differences in Expressions of Intentions
Marc T. Tomlinson David B. Bracewell
Language Computer
Richardson TX 75080
marc, david, wayne@languagecomputer.com
Wayne Krug
Abstract
The intersection of psychology and computational linguistics is capable of providing novel au-
tomated insight into the language of everyday cognition through analysis of micro-blogs. While
Twitter is often seen as banal or focused only on the who, what, when or where tweets can ac-
tually serve as a source for learning about the language people use to express complex cogntive
states and their cultural identity. In this contribution we introduce a novel model which cap-
tures latent cultural dimensions through an individual?s expressions of intentionality. We then
show how these latent cultures can be used to create a culturally-sensitive model which provides
enahnced detection of signals of intentionality in tweets. Finally, we demonstrate how these
models reveal interesting cross-cultural differences in the goals and motivations of individuals
from different cultures.
1 Introduction
Social media platforms have enabled new forms of discourse and have also provided enormous quantities of data
on these communications. For instance, the popular microblogging service Twitter provides an exceptionally use-
ful source of user-generated content which has attracted considerable interest from researchers in computational
linguistics (Ritter et al., 2009; Gimpel et al., 2011). Most of the language processing on tweets has involved
the identification of sentiment (Davidov et al., 2010), summarization (Sharifi et al., 2010), conversational mod-
els of Dialogue acts (Ritter et al., 2009), or lexical and semantic processing. In this effort we expand on these
previous approaches and show how individuals express their cultural identity through expressions revealing their
intentionality towards events and provide a way of capturing this information.
We define intentionality as the amount of effort an individual is willing to expend to achieve a goal(Ajzen, 1991).
Goals represent future states or events which an individual wishes to happen. Accordingly, intentions are goals
for which an individuals is willing to expend at least some minimal amount of effort to bring about. While people
express goals throughout the day, intentions are the goals that they are willing to follow through with. Identifying
when a goal is actually an intention requires the successful recognition of many distinct cognitive factors that can
be revealed through the individual?s use of language.
There is a long history of studies that have worked towards identifying a set of factors that underly an individual?s
intentions (Ajzen and Fishbein, 1977; Ajzen, 1991; Malle and Knobe, 1997; Sloman et al., 2012) of which, the
setting of goals is one important factor. These studies have concentrated on identifying the factors that affect an
individual?s motivation. The studies have also identified a set of factors that people use to gauge the intentionality
of other individuals. However, these factors have always been manually identified by an expert from an individual?s
speech or writing. It is not clear that these features can actually be detected automatically in language.
Intentions have also been considered in computational linguistics. In their seminal work entitled, ?Attention,
Intentions, and the Structure of Discourse? (Grosz and Sidner, 1986), Grosz and Sidner point out the fundamental
role of intentions and their effect on the theory and processing of discourse structure. They even define a set of in-
tentions that can be held by individuals that are relevant to discourse theory. In contrast, we focus on understanding
intentions outside of the discourse. In addition, we work with a more general definition of intentions taken from
psychology, defining intentionality as the amount of effort an individual is willing to expend to achieve a goal.
Culture refers to the set of beliefs, norms, and customs shared by a group of people. Beliefs and culture are
inseparably tied to intentions and language (Ajzen and Fishbein, 1977; Tomasello et al., 2005). Culture affects an
author?s proclivity to have a particular intention, for example Hofstede?s dimension of power distance (Hofstede,
1980) would suggest that individuals from high power-distance cultures have a lower likelihood of performing
This work is licenced under a Creative Commons Attribution 4.0 International license. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
48
actions with the intention of overriding the actions of an individual of higher status. Culture can also affect the
way in which individuals reason about other agents? intentions and the set of actions that are used to realize an
individual?s intentions. While considerable work has looked at the link between cultures and intentions, here we
show how a latent representation of an individual?s culture derived from their intentions can be utilized to explore
the intersection between culture and intentions using the vast amount of written expressions present on Twitter.
In this contribution, instead of focusing on the discourse meaning of intentions, we look at how personal in-
tentions can be understood through Twitter posts by focusing on the language of those posts contain. We briefly
discuss previous work showing how it is possible to capture language that reveal cognitive factors of intentionality
which could be used to capture broader intentions. Critically, we then augment the models of the cognitive factors
of intentionality by accounting for the culture of the authors on Twitter. Twitter contains an immense number of
authors covering a variety of different cultures definable at different levels, for example women, college-students,
or fitness buffs.
We have evaluated the models on a very large set of over 7.5 million tweets which cover a sampling of Twitter
from early 2011 to the middle of 2013. Our sample includes just over 900,000 authors. We found very promising
results for identifying the factors of intentionality, but by considering culture we were able to provide a significant
improvement of those results. We have shown that cognitive factors of intentionality, including goals, control and
skill, and rewards can be recognized through the use of simple language models. Similarly, our cultural models
were based on traditional techniques for latent variable modeling through principal component analysis enabling
an understanding of the cultural distribution of intentions.
The remainder of the paper is organized as follows. We first present the cognitive factors of intentionality that
we have used for this contribution. We then present a new cultural model of authors on Twitter and compare it to
existing approaches in the literature. We then present a series of models which capture the cultural variation of the
cognitive factors of intentionality. Finally, we present a look at some of the cultural differences identified through
our approach.
2 Factors of Intentionality
While there are numerous factors that affect an individual?s intentionality (Ajzen, 1991; Malle and Knobe, 1997;
Sloman et al., 2012), in this contribution we focus on investigating the most historically central factors: goals,
perceptions of control, and rewards. Below we provide brief examples of the three factors before detailing our
approach for identification of latent cultures.
2.1 Factor 1: Goals
The first factor that we consider is evidence that an individual has a goal. Goals are expressions of a desire for
a change of state or rewards which could require an action on the part of the individual. The setting of goals for
both action and inaction have been linked to many different motivational and long-term outcomes (Albarracin et
al., 2011; Locke, 1968). Examples of goals are
(1) I want to finish my paper
(2) I want to be famous
The first example of a goal expresses an intention to perform an action which could result in a positive reward
for the individual, however it doees not mention the reward. In contrast, the second example expresses a clear
expectation for a reward (fame), but does not describe the actions that will lead to that reward. Does the individual
want to be President or the next Kardashian? Additionally, in contrast to explicit goals stated by an individual,
goals can also be inferred by other people based on an analysis of actions (perceived intended events) carried out
by the individual. For example, it is presumed that an individual has a goal to win the lottery when they buy a
lottery ticket, or that the occupants of a car full of beach toys is headed to or from a beach. Goals represent the
factor that has seen the most recent attention in terms of the creation of automatic methods for their recognition
(Chen et al., 2013; Banerjee et al., 2012).
2.2 Factor 2: Perception of Control
Intentions are revealed not just through goals, but also through words expressing skill or a level of control. Individ-
uals that feel that they have more control over a situation will expend more effort on their actions (Ajzen, 1991).
Individuals are also perceived by others as having greater intentionality for actions that they have control over or
exhibit skill at. We considered multiple ways in which an individual can express their perceived control over an
event, subdividing this factor into three sub-factors. The first sub-factor captures expressions which indicate skill.
(3) Just helped some guy push his gas-less car to the garage #iamwoman #hearmeroar
49
Table 1: Example Hash Tags and Tweets.
Cognitive Factor Sample Tags Sample Tweets
F1: Goal #goalinlife, #mywish ?3 more days of studying?
F2: Control #dowhatisay, #kissmyfeet ?I defy the law of gravity?
F2: Skill #madskillz, #iamapro ?you are flat out amazing to watch?
F2: Lack of Control #oops, #cantstop ?cannot believe I said that?
F3: Negative Reward Self #fml, #crap ?I just locked the keys in my car?
F3: Negative Reward Other #worstdriverever, #awkward ?It does make me cringe?
F3: Positive Reward Self #whyismile, #victoryismine ?my cats make me smile?
F3: Positive Reward Other #ff, #thatsbadass ?Solar panels on the white house?
The second sub-factor captures expressions of control.
(4) I?m in control here!
The third sub-factor captures expressions of lack-of-control.
(5) i?m a little nervous for tomorrow
While several linguistic theories exist that could be utilized to create systems detecting control, such as agency
(Dowty, 1991), there is no prominent work on automatically identifying control directly in an individual?s expres-
sions.
2.3 Factor 3: Reception of Rewards
Intentions can also be inferred when an individual receives a reward.
(6) I?m so proud of what I did
(7) Your work sucks!
Rewards can be positive (increasing the likelihood of the action being repeated, Example 6) or negative (decreasing
the likelihood of the action in the future, Example 7). In addition, rewards can come from the individual (self-
directed rewards, Example 6) or from other individuals (other-directed rewards, Example 7). This establishes
four sub-factors for rewards. Knowing that an individual received a reward increases the likelihood that they
had effortful participation in the event. In addition, evidence of negative rewards are strongly inferential for
intentionality (Knobe, 2003). Interpretations of rewards are very culturally sensitive. For example, a comment
such as ?That is disgusting? would have a good chance of being interpreted as a positive reward when it was made
as a comment to a user-generated contribution on the website DeviantArt.com. Additionally, the effect of rewards
on motivation is not always clear-cut. Experts seek out and are actually motivated by criticism (Finkelstein and
Fishbach, 2012).
2.4 Linking Hashtags and Factors of Intentionality
The factors and sub-factors described above capture expressions which can be used to infer an individual?s inten-
tionality towards a future action. In Tomlinson et al. (2014) we showed that it is possible to link particular hashtags
used by people on Twitter to these cognitive factors. Our approach utilized two annotators. The first annotator,
through trial and error, identified a large number of potential candidate tags for each sub-factor. The annotator then
rated each hashtag for how well tweets containing that hashtag exhibited each sub-factor (on a scale of 1-5). The
second annotator then separately rated each tag which scored a 4 or 5. The two annotators had an agreement rate
of 87%. 178 tags in all were agreed to be a 4 or 5 by both annotators and considered representative of the particular
sub-factor. Examples of the hashtags utilized and tweets with those tags are shown in Table 1. The tweets have
been modified slightly to preserve anonymity.
3 Identification of Latent Cultures
In the preceding section we discussed examples of goals, control, and rewards, and discussed how hashtags are
used on Twitter to mark a tweet expressing one of these factors. Some of these examples require cultural knowledge
in order to correctly interpret. In this section we present the latent model of culture that is used for learning the
cultural specific expressions of the factors of intentionality.
50
3.1 SVD-Model of Culture
A considerable amount of work has demonstrated how particular social characteristics of individuals can be iden-
tified on Twitter, such as gender, age, and political orientation (Zamal et al., 2012; Pennacchiotti and Popescu,
2011). While superb results can be obtained for identifying these characteristics of authors using a complex set of
features, this approach does not neccesarily allow for generalization to other data sets. Therefore we settled on an
approach utilizing a specially trained latent variable model. Instead of utilizing Latent-Dirichlet Allocation (LDA,
Blei, Ng, and Jordan, 2003 ) as Pennacchiotti and Popescu we utilized a spectral analysis based on singular-value
decomposition (SVD). This approach has been shown to be generally superior to LDA on the domain of topic
modeling (Chen et al., 2011), but has not been tested for cultural modeling.
3.1.1 Data
We randomly sampled 1.6 million tweets from a Twitter dataset that had been generated by retrieving tweets that
carried at least one of the hashtags linked to a cognitive factor of intentionality (and other posts by that author). In
addition, we restricted the set to authors for which we had at least 20 posts in our dataset. For this dataset, all of
the markup was left in the tweet (e.g. hashtags, urls, etc.).
3.1.2 Model
From our dataset we created a set of documents, D = {a
1
, a
2
, . . . , a
A
}. Where each a
i
represents the entire col-
lection of tweets for a single author that contain mentions of goals, skill/control, or rewards. This set of documents
contains N words and hashtags. We then create a matrix, X ? <
N,A
, where each author represents a row in the
matrix and the columns are the number of times that the corresponding word or hashtag was used by that author.
Then we perform a singular value decomposition of the matrix to solve
X = V SC
T
(1)
Where S is a k x k matrix whose off-diagonal entries equal 0 and the on-diagonal entries are the k singular values
for the matrix X . For our approach we set k equal to 100. V represents a mapping of the words into our reduced
space <
n,k
, and C <
i,k
contains a weighting for each author with respect to the k
th
latent cultural dimension. The
cultural model can be used to identify the culture of an unseen author through the creation of a projection matrix,
P .
P = V S
?1
(2)
This matrix projects the tweets that make up the author into our latent cultural space C. This allows us to map each
author in our complete data set into our latent space which can then be used for training and testing. The latent
cultural space can be used to characterize the culture of an author as a distribution over the dimensions. Below we
evaluate our latent cultures on the shared dataset provided by Zamal et al. 2012.
3.2 Evaluating the Latent Cultures
Culture is a system of shared beliefs and actions. Culture is often shared between individuals based on social
similarity, this can be within a language, nation, gender, age-group or other social distinction. Thus, being able to
identify an individual?s culture should facilitate detection of socio-demographic information. To test this we looked
at using the latent cultural dimensions to predict socio-demographics on Twitter. We looked at the systems ability
to identify gender (male vs. female), age (young vs. old), and political orientation (Democrat vs. Republican) of
individuals based on their exhibition of particular latent cultural dimensions. In this model we first represented
an individual?s tweets as a distribution over the latent dimensions. We then utilized two different statistical ap-
proaches to find associations between particular dimensions and the relevant socio-demographic information. For
a comparison, we tested our SVD-culture model against a similarly trained LDA model and a model based on
n-grams.
3.2.1 Data
We utilized the publicly available dataset from Zamal, Liu, and Ruths (2012). The dataset consisted of Twitter
user names and associated meta-data identifying their gender (Male or Female), age (two classes, young and old),
and political orientation (Liberal or Conservative). Unfortunately, many of the identified tweets were no longer
available from the Twitter API, but we successfully retrieved 2.6 million tweets from authors identified in the
dataset with 310 users identified for gender, 320 identified for their age, and 380 for their political affiliation. The
tweets in our dataset are substantially different from the original dataset because of the time over which they were
collected. Zamal, et al.?s tweets were from 2012 and before, whereas our tweets covered much of 2013. This
suggests that comparisons of the raw numbers should be made with caution, particularly in the political area.
51
Table 2: Results for identifying user demographics based on latent cultural dimensions compared to linguistic style
and an ensemble method utilized by Zamal et al (2012).
Zamal et al. N-Grams LDA SVD
N F F F F
Gender 310 .80 .57 .71 .70
Age 320 .75 .63 .66 .67
Political Orientation 380 .89 .73 .66 .68
3.2.2 Modeling & Results
To provide a comprehensive view of the strengths and weakness of our approach we compared several models for
their ability to correctly predict the cultural demographics of individuals on Twitter. We first established a base-line
model which was an n-gram language model created from the language used by each individual in their tweets.
This model learned to identify the cultural demographics based on the frequency with which individual?s in that
demographic used sequences of words, called n-grams. This approach is consistently ranked as one of the single
best approaches to authorship identification and performs well on a large variety of datasets.
We also tested the SVD-Culture model introduced above on this dataset. For this experiment, we trained a
logistic-regression based classifier to identify the demographic information of an author based on the vector created
by projecting that author into our latent space.
Finally, to look for a difference in the performance between an SVD-based latent representation and one based
on LDA (Pennacchiotti and Popescu, 2011), we also trained and tested an LDA-based Culture model. The model
was trained on the same data as the SVD-based model and utilized the same number of dimensions.
All of the models were tested and trained utilizing 10-fold cross validation. It is very important to point out
that the data sets used to generate the underlying latent representational models did not include any of the tweets
from the data used for the 10-fold cross validation. That data was only utilized for the supervision of the logistic
regression.
The results of the base model, the SVD model, the LDA model, and the original results presented by Zamal
et al. (2012) are shown in Table 2. The latent models are clearly superior to the language model, on average
outperforming it by a significant margin of 4%. As expected, the SVD-based model does outperform the LDA
model on average, though it is only by 1%, on average.
The strength of this approach is in its simplicity. The latent cultural dimensions have been learned on a wholly
different dataset than that used for testing, this supports good generalization performance. While the latent SVD-
cultural model does not reach the performance of the system created by Zamal et al. (2012). Zamal et al.?s results
were obtained using a plethora of different feature types, which were specifically trained to solve each individual
problem. As pointed out in Cohen and Ruths (2013) this causes some issues on transfer to a novel dataset, because
the selected features were not representative of differences between liberals and conservatives in the second dataset.
In contrast, we suggest that the latent cultural model learns a more general representation utilizing only the set of
features provided by the underlying latent cultural models, which were not trained on any of the data in the test
set. Additionally, the latent SVD model is easy to implement and train.
Importantly, these results indicate that the latent cultural dimensions capture similarities in the ways in which
individuals of similar socio-demographics express themselves on Twitter. The model is able to easily identify the
gender, age, and political affiliation of individuals based on their tweets. In the next section we show how we can
utilize these latent cultural dimensions to facilitate learning of expressions conveying factors of intentionality.
4 Cultural Sensitive Identification of Cognitive Factors of Intentionality in Language
Recognizing language that expresses factors of intentionality is complicated because of the wide variety of ways
in which they can be expressed as shown in the examples in the previous section. While some work has explored
automatic goal recognition, most recently by (Chen et al., 2013) and (Banerjee et al., 2012), little work has been
done automatically characterizing the other factors, though work in detecting social implicatures in language is
similar (Bracewell et al., 2012b). We first present a general framework for learning to model the content of tweets
that express a given factor from our cognitive model, we then show how this approach can be enhanced with the
addition of latent cultural dimensions.
4.1 Culture Agnostic Model
Here we introduce the General Model that serves as the basis for the culture specific models. It is so named
because it applies to all cultures. We utilized an n-gram based language model to identify the factors in tweets.
52
We first constructed a vocabulary of all n-grams between 2 and 4 words in length. Each tweet, j, which is labeled
with a hashtag linked to a sub-factor f , is represented as a vector, X
j
. Entries in X
j
correspond to the number
of occurrences in the tweet of the i
th
n-gram from the vocabulary. We examined two different mathematical
approaches to modeling the cognitive factors to gain a better understanding of the problem.
The first approach utilized a Naive-Bayes based classifier (NB) where
p(F = f |X) =
p(F = f) ? p(X|F = f)
p(X)
(3)
The second approach utilized an L2-loss logistic regression model (L2):
p(F = f |X,W ) =
1
1 + exp(w
0
+
?
i
w
i
X
i
)
(4)
In which the weights, W , are learned by maximizing Equation (4)
m
f
?
j
log p(y
j
|X
j
;W )? ?||W ||
2
2
(5)
where m
f
represents a balanced training set created by randomly sampling the training tweets that are tied to
sub-factor f and an equal number of tweets that express one of the other factors. For solving the maximization
problem we utilized the LibLinear package (Fan et al., 2008).
4.2 Culture Sensitive Models
We compared two different methods for integrating the culture information from the SVD-based culture model
into the models for identifying the cognitive factors of intentionality. Both models assume that the authors have
been partitioned into a set of cultures, L, but differ in their modeling of the link between language and cognitive
factors.
In order to identify the cultures of the authors we utilize a clustering of the latent dimensions produced by the
SVD model, a spectral clustering (Kannan et al., 2004). We utilized a simple hierarchical clustering that capitalizes
on the y largest singular values. We create a set of hierarchical clusters based on a median split of each of the first
y columns in our latent space. When y = 1 we have two clusters where the authors have been split based on the
median value of the first latent dimension, with y = 2 each cluster is then independently split by the author?s value
along the second latent dimension, giving four clusters, and so on.
4.2.1 Culture-Specific Model
Our first method, which we call the culture specific model uses a separate model of each factor for each latent
culture, l ? L. We first identify a tweet x, as belonging to a given culture, l. We then determine whether or not the
language it contains expresses a particular cognitive factor based on
p(F = f |x
l
, L
l
) (6)
To learn the function we utilize a linear classifier, Logistic-Regression with an L2 regularization term, and limit
the training data to authors that belong to the particular culture.
4.2.2 Joint-Culture Model
Our second model, which we call the joint culture model utilizes an ensemble based approach. For each tweet, x
l
,
we calculate both a culture specific view of the language in the tween p(F = f |x
l
, L
l
) and a culture agnostic view
p(F = f |x
l
), taking the classification is that is most confident. This joint approach utilizes the culture-agnostic
model to smooth deficiencies caused by insufficient culture-specific data.
4.2.3 Number of Cultures
We explored settings of y = {2, 3, 4, 5} latent dimensions which equates to {2, 4, 8, 16, 32} latent cultures. Au-
thors are first split according into their cultural group and then tweets from each culture are broken into a training
and testing set. Because of the amount of data we utilized only a 5-fold cross validation procedure. In addition,
we also tested a random culture model that randomly assigned authors to cultures instead of utilizing the spectral
clustering. When creating these random cultures we balanced the number of authors in each random culture with
the corresponding spectral cultures.
53
Table 3: Accuracies for modeling each sub-factor of intentionality. L2 represents results obtained using an L2-
regularized linear regression, NB represents naive-Bayes, #Cultures signifies the number of latent cultural dimen-
sions used for clustering.
General L2 - Culture Specific L2 -Joint Culture
#Cultures NB-0 L2-0 2 3 4 5 2 3 4 5 NB-5
F1:Goals 79.8 80.9 81.1 80.9 79.8 78.8 82.1 82.2 82.1 82.0 79.1
F2:Control 70.1 75.5 75.5 75.3 74.4 73.8 76.4 76.4 76.4 76.4 72.3
F2:Lack of Control 69.1 73.7 75.2 74.9 74.1 72.9 75.9 75.7 75.8 75.6 71.6
F2:Skill 73.2 76.2 77.6 77.0 76.4 75.5 78.2 78.2 78.1 78.1 75.3
F3:Positive Other 78.3 82.9 84.3 84.1 83.7 83.4 84.5 84.4 84.4 84.5 81.9
F3:Positive Self 66.0 69.1 70.6 70.3 69.4 68.7 71.3 71.3 71.3 71.4 68.4
F3:Negative Other 68.7 72.3 73.6 73.4 72.5 71.6 74.1 74.0 74.1 74.0 70.8
F3:Negative Self 69.3 72.4 73.6 73.3 71.9 71.3 74.4 74.3 73.9 73.7 71.1
4.3 Data
Testing was done on a large number of tweets (7.5 million) that contained tweets from individuals that used any
of the representative hashtags. In our collection hashtags exhibiting the sub-factor of control contained the largest
number with approximately 575,000 tweets, while we only collected 110,000 tweets which were marked with a
hashtags indicating positive rewards for the actions of other individuals. For training and testing purposes we
removed all URLs, hashtags, and @users from the tweets. We then discarded tweets that were less than two words
long. This approach is conservative, because we removed the classifier?s ability to directly learn co-occurring
hashtags, however we wanted to ensure that we would minimize deficient solutions and maximize the ability of
the models to transfer from Twitter to other genres of text.
4.4 Results and Discussion
The accuracy of the classifiers for identifying each sub-factor are shown in Table 3. The accuracies reflect the
classifiers ability to separate tweets that have a hashtag representing the given sub-factor from those that do not.
The results suggest that all of the models are adequately capturing the differences between the cognitive factors.
On average, the logistic regression based classifier achieves a 3.5 percent advantage in accuracy over the Naive-
Bayes model, showing a clear advantage for the improved feature selection of the L2-loss logistic regression. Both
models required a similar amount of time to train and test.
To conserve space Table 3 shows only the results for the 5-dimension Joint Culture Naive-Bayes model. The
results for the Naive-Bayes model match the pattern exhibited by the logistic-regression Joint Culture model,
except that the Naive-Bayes Joint-Culture model increases steadily as more groups are added with a maximum
performance with 5 latent dimensions. With 5 latent dimension the gap between the two ML approaches shrinks
to 2.8 percent (73.2 to 76.0).
On average the Joint Culture model shows a 1.8 percent improvement (74.3 to 76.1) over the culture neutral
model for the L2-Logistic Regression, while it is a larger 2.2 percent for the Naive Bayes based approach (71.2 to
73.4). A comparison of the error reduction shows that the cultural integration is very promising. While the L2-loss
logistic regression provides an 11 percent error reduction over the Naive-Bayes, the joint culture model achieves a
comparable 7-9 percent reduction in error over the L2-loss regression and the Naive-Bayes model.
The improvements are strongest for positive self directed reward factor, skill factor, and lack of control factor.
Interestingly, the models also exhibited considerable variation in accuracies across the different cultures, for exam-
ple utilizing 3 dimensions positive rewards for others in one culture is recognized at 92 percent (this group contains
43,556 tweets), while for another culture of approximately the same size it is only recognized at 77 percent. Un-
fortunately, when moving to 4 dimension our clustering algorithm splits the group at 92 percent into two groups
where the factor can only be recognized with an average of 88 percent accuracy. This suggests that more complex
clusterings strategies within the latent space would be beneficial.
While not shown in the table for space reasons, we also tested the joint culture model utilizing a random assign-
ment of authors to cultures, instead of relying on the assignment produced by the SVD-model. As expected the
random model performed, on average, at approximately the same level as the general model, 74.6% compared to
74.5% respectively. Though the random culture model exhibited considerable variation in relation to the real joint
culture model across the different factors. This evidence reinforces the idea that the latent cultures are coherent
and that individuals within those cultures express the factors of intentionality in similar ways.
54
Table 4: Example cultures and the tags that are commonly associated with that factor.
Cultural Label Cognitive Factor Common Tags
Alterantive Medicine Health Positive Rewards Self #almond, #radish, #curd
Geek Interest Positive Rewards Self #theobroma, #freefiction, #nanotech
Teenagers Positive Rewards Self #bored, #me, #cute
Urban Hip/Hop Positive Rewards Self #bosslife, #teamfastfollow, #indiecharts
Martial Fitness Goals #healthynews, #fitnessimages, #fitso
Hip/Hop Goals #soundcloud, #support, #dl
General Religion Goals #singer, #jesus, #judas
Inspections of tweets where the cognitive factors have been discovered suggest that many times the hashtags
are used sarcastically. Anecdotally, we also examined a list of the top hashtags associated with instances labeled
by our approach and found good generalization to novel hashtags. We looked at a list of the hashtags based on
the average confidence of the labels being applied to the tweets containing those tags, we found many reasonable
candidate tags. For example, tweets containing the hashtags #day1 and #day2 were among the most likely to be
labeled as exhibiting a goal even though neither were identified by our annotators initially. These two tags are used
by individuals on the first and second day of pursuing a goal.
The results presented in this section suggest that breaking down the authors by culture before learning models
linking the hashtags marking expressions of the cognitive factors of intentionality to language provides a significant
benefit. It also hints at some interesting differences between the groups. In the next section we briefly explore some
of those differences.
5 Investigating Cultural Differences in the Language of Intentionality
We investigated the cultural discriminations made by the model by looking at the hashtags that were the most
popular for each culture. Two annotators provided labels for each of the cultures based on the most frequent
hashtags for that culture. We found that some of the cultures could easily be labeled based on their differential
use of topical hashtags. Many of the latent cultures reflected notions of distinctions between cultural (or sub-
cultural) groups, such as along political orientation or socio-demographics (urban, hipster, university students,
single mothers, and political activist). In addition to the latent cultures that weighed on group identity, some of the
other clusters captured more topical information, such as being fitness oriented or discussions focused around sex.
The cultural distinctions allowed us to quantify the differences in the event and intentionality associations across
the cultures and differences in expressions indicating cognitive factors of intentionality. For instance, activists and
urban individuals were most likely to produce tweets expressing control over situations. There were also groups,
such as the camaraderie group where individuals typically set goals that will benefit a group in some way as well as
the individual. In most of these cases, the author is the member of a team or some other group that will be engaging
in a cooperative or competitive activity. Some authors from this cultural group express goals of providing direct
or moral support to specific teams or groups of which they are not members. Others have goals of attending group
events or gatherings with no particular membership. In most cases, goals in this culture are associated with positive
rewards or defeating an opponent.
Table 4 shows the most probable tags by cognitive factor for some of the more interesting groups. These lists
were generated by first eliminating all tags from the culture that were not predictive of the culture. To do this, we
generated an estimate of the mean and variance for each hashtag in our dataset across all of the different cultures.
We then eliminated all tags where the probability of the tag given the culture was not significantly different than
its estimate given the general population. This has the effect of removing that hashtags that signaled the cognitive
factors because they had a fairly general distribution across the cultures.
6 Conclusion
In this paper we presented a novel approach for identifying factors of intentionality in tweets. Further, we showed
how a latent cultural model could be used to enhance those identifications through an improved understanding of
how these factors are expressed across the various cultures. The latent cultural dimensions identified by the model
correspond well with real cultural demographic information.
This work presents several exciting possibilities, while Twitter is notoriously difficult for traditional natural
language processing work because it doesn?t follow established syntactic and semantic conventions, models learned
over Twitter data are able to transfer to other types of social media, such as user-generated content sites (Tomlinson
et al., 2014a). Hashtags provide a very interesting form of distant annotation that could reduce the amount of time
and effort required to create models which capture a nuanced understanding of social or psychological pragmatics,
55
such as social acts (Bender et al., 2011; Bracewell et al., 2012a), thus making the exploration of a richer language
understanding more tractable.
Lastly, we have also shown that the models provide an ability to look at differences between cultures in the how
and when of their expressions of factors relating to intentionality. People express lots of goals, but what affects
when they actually intent them. These models should be able to provide a novel view on the pulse of a city (Rios
and Lin, 2013) or citizens? cognitive responses to events (Dodds et al., 2011). We can use these techniques to
identify what events make people establish new goals or instill feelings of a loss of control?
Acknowledgment
This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of
Defense US Army Research Laboratory contract number W911NF-12-C-0063. The U.S. Government is authorized
to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as
necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL,
or the U.S. Government.
References
Icek Ajzen and Martin Fishbein. 1977. Attitude-behavior relations: A theoretical analysis and review of empirical
research. Psychological Bulletin, 84(5):888?918.
Icek Ajzen. 1991. The Theory of Planned Behavior. Organizational Behavior and Human Decision Processes,
50:179?211.
D. Albarracin, J. Hepler, and M. Tannenbaum. 2011. General Action and Inaction Goals: Their Behavioral,
Cognitive, and Affective Origins and Influences. Current Directions in Psychological Science, 20(2):119?123,
April.
Nilanjan Banerjee, Dipanjan Chakraborty, Anupam Joshi, Sumit Mittal, Angshu Rai, and B. Ravindran. 2012.
Towards Analyzing Micro-Blogs for Detection and Classification of Real-Time Intentions. ICWSM.
E.M. Bender, J.T. Morgan, Meghan Oxley, Mark Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and Mari
Ostendorf. 2011. Annotating social acts: Authority claims and alignment moves in wikipedia talk pages. ACL
HLT 2011, (June):48.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022.
David B Bracewell, Marc T Tomlinson, Mary Brunson, Jesse Plymale, Jiajun Bracewell, and Daniel Boerger.
2012a. Annotation of Adversarial and Collegial Social Actions in Discourse. In 6th Linguistic Annotation
Workshop, number July, pages 184?192.
David B Bracewell, Marc T. Tomlinson, and Hui Wang. 2012b. Identification of Social Acts in Dialogue. In
COLING, number December 2012, pages 375?390.
Xi Chen, Bing Bai, Qihang Lin, and Jaime G Carbonell. 2011. Sparse Latent Semantic Analysis. In SDM.
Zhiyuan Chen, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013. Identifying Intention
Posts in Discussion Forums. In NAACL-HLT, number June, pages 1041?1050.
Raviv Cohen and Derek Ruths. 2013. Classifying Political Orientation on Twitter : It s Not Easy ! In ICWSM-
2013, pages 91?99.
Dmitry Davidov, Oren Tsur, and Ari Rappaport. 2010. Enhanced Sentiment Learning Using Twitter Hashtags and
Smileys. In Coling, number August, pages 241?249.
PS Dodds, KD Harris, and IM Kloumann. 2011. Temporal patterns of happiness and information in a global social
network: Hedonometrics and Twitter. PloS one, 6.
David Dowty. 1991. Thematic Proto-Roles and Argument Selection. Linguistic Society of America, 67(3):547?
619.
RE Fan, KW Chang, CJ Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear
classification. Journal of Machine Learning Research, 9(2008):1871?1874.
56
Stacey R. Finkelstein and Ayelet Fishbach. 2012. Tell Me What I Did Wrong: Experts Seek and Respond to
Negative Feedback. Journal of Consumer Research, 39(1):22?38, June.
Kevin Gimpel, Nathan Schneider, Brendan O Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-Speech Tagging for Twitter :
Annotation , Features , and Experiments. In Proceedings of the Association for Computational Linguistics,
number 2.
B Grosz and C Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics,
12(3).
Geert Hofstede. 1980. Culture?s consequences: International differences in work-related values. Sage Publica-
tions, Inc.
R. Kannan, S. Vempala, and A. Vetta. 2004. On clusterings: Good, bad and spectral. Journal of the ACM (JACM),
51(3):497?515.
J. Knobe. 2003. Intentional action and side effects in ordinary language. Analysis, 63(3):190?194, July.
E. A. Locke. 1968. Toward a theory of task motivation and incentives. Organizational behavior and human
performance, 3(2).
Bertram Malle and J. Knobe. 1997. The Folk Concept of Intentionality. Journal of Experimental Psychology,
33(2):101?121.
Marco Pennacchiotti and Ana-maria Popescu. 2011. to Twitter User Classification. In ICWSM?11, pages 281?288.
Miguel Rios and Jimmy Lin. 2013. Visualizing the? Pulse? of World Cities on Twitter. Seventh International
AAAI Conference on Weblogs . . . , pages 717?720.
Alan Ritter, Colin Cherry, and Bill Dolan. 2009. Unsupervised Modeling of Twitter Conversations. In HTL-
NAACL.
Beaux Sharifi, Mark-anthony Hutton, and Jugal Kalita. 2010. Summarizing Microblogs Automatically. In ACL-
HLT, number June, pages 685?688.
Steven a. Sloman, Philip M. Fernbach, and Scott Ewing. 2012. A Causal Model of Intentionality Judgment. Mind
& Language, 27(2):154?180, April.
Michael Tomasello, Malinda Carpenter, Josep Call, Tanya Behne, and Henrike Moll. 2005. Understanding and
sharing intentions: the origins of cultural cognition. The Behavioral and brain sciences, 28(5):675?91; discus-
sion 691?735, October.
Marc T Tomlinson, David Bracewell, Wayne Krug, and David Hinote. 2014a. # impressme : The Language of
Motivation in User Generated Content. In CICLING.
Marc T Tomlinson, David Bracewell, Wayne Krug, David Hinote, and Mary Draper. 2014b. # mygoal : Finding
Motivations on Twitter. In LREC - 2014. ELRA.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012. Homophily and Latent Attribute Inference : Inferring
Latent Attributes of Twitter Users from Neighbors. In ICWSM-2012.
57
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1752?1763, Dublin, Ireland, August 23-29 2014.
A Novel Distributional Approach to Multilingual Conceptual Metaphor
Recognition
Michael Mohler and Bryan Rink and David Bracewell and Marc Tomlinson
Language Computer Corp.
Richardson, Texas, USA
{michael,bryan,david,marc}@languagecomputer.com
Abstract
We present a novel approach to the problem of multilingual conceptual metaphor recognition.
Our approach extends recent work in conceptual metaphor discovery by combining a complex
methodology for facet-based concept induction with a distributional vector space model of lin-
guistic and conceptual metaphor. In the evaluation of our system in English, Spanish, Russian,
and Farsi, we experiment with several state-of-the-art vector space models and demonstrate a
clear benefit to the fine-grained concept representation that forms the basis of our methodology
for conceptual metaphor recognition.
1 Introduction
The role of metaphor in language has been defined by Lakoff et al. (1980; 1993) as a cognitive phe-
nomenon which operates at the level of mental processes, whereby one concept or domain is viewed
systematically in terms of another. For example, the phrase ?to cure poverty? is a metaphor which subtly
conveys a wide variety of information to the listener. In order to mentally process this phrase, we must
first recognize that a metaphor is being used and that ?cure? (as a medical term) is being used figura-
tively. Then, we assume some relationship between ?poverty? and ?things that can be medically cured?
which leads to the conceptual mapping ?POVERTY as DISEASE.? This conceptual mapping enables the
listener to transfer a variety of properties and associations between the two concepts, such as their as-
sociation with a feeling of helplessness, the existence of sustained efforts to end them, the potential for
them to spread, and their mutual relationship with ill-health and death. Therefore, by identifying the con-
ceptual domains associated with this linguistic metaphor, we are able to reason about the target domain
(POVERTY) using concepts and terms associated with the source domain (DISEASE).
Any natural language processing system capable of processing metaphor in text with human-level
competence must, therefore, overcome three problems in sequence:
1. the identification of metaphorical expressions (also known as linguistic metaphors (LMs))
2. the discovery of a conceptual domain mapping or conceptual metaphor (CM) which consists of
(a) the conceptual domain of the metaphor target (e.g., POVERTY); and
(b) the conceptual domain of the metaphor source (e.g., DISEASE)
3. the real-world interpretation of the metaphorical text which uses the conceptual metaphor frame-
work to transfer knowledge between the source and target domains.
While a significant amount of recent work has presented interesting and promising methodologies for
multilingual LM identification (Shutova and Sun, 2013; Wilks et al., 2013; Strzalkowski et al., 2013),
the work presented in this paper is focused on (2), the problem of multilingual CM recognition, which
will be made to serve as the foundation for a more fine-grained interpretation of metaphor.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1752
We cast the CM recognition process as a two-part methodology which (a) selects the target domain
associated with a particular LM that has been detected; and (b) determines the source domain to which
it should be mapped in order to produce a satisfactory interpretation. In this work, we assume that the
target domains are known and belong to one of the following conceptual spaces: POVERTY, WEALTH, or
TAXATION. Pragmatically speaking, research in CM recognition presupposes some methodology for LM
identification, and to this end, we have employed an existing state-of-the-art LM identification system
which has been developed to detect linguistic metaphors in four languages: English, Spanish, Russian,
and Farsi (Bracewell et al., 2014).
In order to generate a CM which can serve as the basis for an interpretation of an LM, we have
developed an approach that is based on the following hypotheses:
CONCEPTUAL HYPOTHESIS: When an LM has been identified as a pair of lexical items that
represent the source (e.g., ?cure?) and the target (e.g., ?poverty?), we can generate a conceptual
mapping by selecting the conceptual domains that are, a priori, the most likely for the source and
target lexemes.
1
DISTRIBUTIONAL HYPOTHESIS: It is possible to decide which conceptual space better repre-
sents a given lexeme by
1. expanding the lexical space with additional terms (which we call ?grammatical co-occurrents?)
that are strongly associated with the lexeme through grammatical relations such as AGENT,
PATIENT, INSTRUMENT, and ATTRIBUTE;
2. using these lexical expansions to produce distributional vectors; and
3. uncovering the selectional constraints of particular domain facets by clustering the distribu-
tional vectors within a semantic space.
DOMAIN HYPOTHESIS: The grammatical co-occurrents of the LM are themselves very likely to
belong to the same conceptual domain as the lexeme (e.g., ?cure patient?, ?cured of AIDS?, and
?doctor cured?).
MAPPING HYPOTHESIS: The semantic space representations of both the LM source and its gram-
matically associated terms can be used to produce mappings into a high dimensional space in which
source domains are known to exist.
While other computational linguistics research in metaphor has made use of the CONCEPTUAL and
DISTRIBUTIONAL hypotheses, to our knowledge the DOMAIN and MAPPING hypotheses have not
yet been explored in combination with a distributional approach.
The remainder of this work is organized as follows. In Section 2, we discuss related work in the field
of metaphor interpretation and unsupervised concept induction. In Section 3, we introduce the overall
architecture of our CM recognition system. In Section 4, we describe our method for representing lexical
items and conceptual metaphors in a distributional vector space. Then, in Section 5, we explain our
methodology for creating and ranking clusters of LM co-occurrents which are then mapped to conceptual
metaphors within our vector space. In Section 6, we describe our experimental setup and provide the
results of our experiments. Finally, in Section 7 we present our conclusions.
2 Related Work
Research in metaphor processing can broadly be divided into two categories: metaphor identification and
metaphor interpretation. Although some recent work on metaphor interpretation has skirted the issue of
conceptual metaphor entirely by casting the problem of metaphor interpretation as an instance of lexical
paraphrase (Shutova, 2010; Bollegala and Shutova, 2013) or textual entailment (Mohler et al., 2013), the
mapping and modeling of conceptual metaphors has historically served as an important foundation for
1
If the target domains are pre-selected, this hypotheses is reduced to selecting only the most likely source domain.
1753
more robust interpretation of metaphor. Indeed, a significant amount of research in metaphor interpreta-
tion has been concentrated on the development of highly-structured, manually curated representations of
both the CM source and CM target domains. Notable in this regard are the KARMA system (Feldman
and Narayanan, 2004) which was designed to simulate neurological modeling of verbs ? both abstract
and metaphorical ? and the ISOMETA system (Beust et al., 2003) which made use of differential tables
of CM domain lexical items to drive their metaphor interpretation process. The CorMet system (Ma-
son, 2004) sought to model conceptual metaphors by detecting individual source-target mappings that
provide evidence for a known CM by quantifying the overlap between clusters of terms with a strong
selectional preference to the most representative verbs within the source and target domains. After a man-
ual inspection of the source/target cluster pairs across domains, the directionality and the systematicity
of these underlying conceptual mappings were quantified in order to produce an overall confidence in
the mapping. As part of their development of the Hamburg Metaphor Database (HMD), Reining and
L?onneker-Rodman (2007) performed a a manual categorization of lexical items into conceptual source
domains with a facet-level granularity and enriched their domains using a WordNet-based lexical expan-
sion. In the same vein, Chung et al. (2005) chose to model source domains by expanding their lexical
items by exploiting the links between WordNet glosses and the SUMO ontology.
In recent years, however, research has focused on automating the modeling and classification of con-
ceptual metaphors as much as possible in order to encourage the scaling up of metaphor research in
general. Veale and Hao (2008), as part of the Talking Points system, developed what they refer to as a
Slipnet which defines linked chains of meaning that connect a source to a target through shared (or re-
lated) attributes and actions. As a step in this process, they combined WordNet relations with pragmatic
relations extracted from text and clustered nouns according to their relation (and attribute) similarity in
order to define a weak conceptual mapping within the clusters. In a similar way, Shutova et al. (2010),
beginning with a seed set of noun/verb linguistic metaphor pairs, performed spectral clustering on a large
set of nouns and verbs in order to predict metaphors which participate in the same conceptual metaphor
mapping. In particular, she modeled verbs according to their subcategorization frames parameterized by
a model of their selectional preferences, while nouns were modeled according to the verbs with which
they frequently co-occurred in a dependency relation.
More recently, Gandy et al. (2013) approached the CM discovery problem as a set covering problem.
For a given nominal target lexeme, they began by finding all facets (i.e., verbs/adjectives) that share
a positive PMI with the target. Then, they would find the set of nouns that also have a positive PMI
with those facets, compute their confidence in each association, and heuristically select pairs of concepts
(defined as rooted WordNet synset trees) which subsume a large percentage of those nouns and cover a
large portion of the overlapping facets. Similarly, Shutova and Sun (2013) detect conceptual mappings by
performing hierarchical graph factorization clustering on a graph in which the vertices are defined to be
nouns (i.e., concepts) and the edges are weighted using Jensen-Shannon Divergence. For a given input
LM source, its likely conceptual metaphors are then discovered by determining its non-literal cluster
membership. Finally, Strzalkowski et al. (2013) discovered terms (literal and metaphoric) which often
co-occur with an LM source in a corpus and clustered those terms using WordNet and corpus statistics
to form ?ProtoSources? which could be further inspected to define CM source concepts.
Two vector-based approaches to concept representation are of particular interest in understanding the
present work. In the first of these, Sch?utze (1998) described an approach to word sense identification
using second-order co-occurrence vectors which were used to cluster first-order vectors of the in-context
terms into senses.
2
Lin (1998), in developing a methodology for evaluating the quality of thesauri,
defined a word vector space that moved beyond simple co-occurrence by integrating information about
the relations between the word and its co-occurrents. In particular, a word?s vector was defined by the
number of times that word occurred within a set of (word, relation, word) tuples. Our DepVec space
represents an extension to Lin?s space insofar as we incorporate additional information about relational
(i.e., selectional) preference.
2
While context is critical in word sense disambiguation, we hasten to point out that one mark of metaphoricity is its discon-
nect from the surrounding literal context.
1754
Figure 1: The architecture of our conceptual metaphor recognition system. This system takes a linguistic
metaphor as input, induces potential concepts using vector-space clustering, and maps these clusters onto
a conceptual metaphor domain.
3 A New Methodology for Conceptual Metaphor Recognition
Figure 1 shows the overall flow of our metaphor processing architecture. We begin with a set of doc-
uments gathered from a variety of online news-wire sources. These documents are fed to our state-
of-the-art LM detection system which employs a binary logistic regression classifier using a variety of
feature modules including imageability and concreteness estimation, topicality modeling, pattern match-
ing, semantic categorization, selectional preference violation, and source/target vector space similarity.
The methodology used in this system is beyond the scope of this work, but it is described in detail by
Bracewell et al. (2014). The LMs provided by the detection system are validated by a group of native-
language experts before being sent for CM recognition system for concept-level interpretation.
Once the LMs have been collected and validated, the CM recognition system begins by extracting,
weighting, and clustering the common grammatical contexts of the LM source term. By grammatical
context, we refer to the syntactic relations (along with their arguments) which have been found to fre-
quently co-occur with the LM source term in open text. In order to model this grammatical context, we
have syntactically parsed a wide collection of documents in each of our focus languages: English, Span-
ish, Russian, and Farsi. From these parsed documents, we have extracted the most common grammatical
co-occurrents of each word in the corpus along with the relation that connects them and the number
of times they are connected by that relation. For a given word, we refer to the set of its grammatical
co-occurrents as the ?concept candidates? associated with that word, as they represent potential concepts
within the same conceptual domain as the given word (the DOMAIN HYPOTHESIS). For example,
grammatical co-occurrents of the noun ?battle? would include many WAR concepts such as ?fought?,
?died in?, ?waged?, ?naval?, and ?losing?.
Since a conceptual domain is made up of several interacting concepts, we perform a clustering over
the grammatical co-occurrents to produce groups of terms which are likely to represent individual con-
cepts within a domain. The clustering is performed within a high-dimensional, distributional vector
space which we describe in Section 4. The clusters are then merged and aligned with a set of 51 pre-
defined source concept domains (see Table 1) that have been found to occur frequently in conceptual
metaphors about POVERTY, WEALTH, or TAXATION. For each of these known conceptual domains, we
have amassed a collection of lexical items for the purpose of modeling the domains and aligning them
to our automatically discovered domains. The collection of lexical items associated with each domain
have been further partitioned into three to five facets which provide a more fine-grained representation of
the domain. For instance, the conceptual domain of ABYSS as been subdivided into facets representing
1755
Full Source Concept List
A GOD COMPETITION ENSLAVEMENT LIGHT NATURAL PHYSICAL FORCE PORTAL
A RIGHT CONFINEMENT FOOD LOW POINT OBESITY RESOURCE
ABYSS CRIME FORCEFUL EXTRACTION MACHINE PARASITE SCHISM
ACCIDENT CROP GAME MAZE PATHWAY STRUGGLE
ADDICTION DARKNESS GEOGRAPHIC FEATURE MEDICINE PHYSICAL BURDEN VERTICAL SCALE
ANIMAL DESTROYER GOAL DIRECTED MONSTER PHYSICAL HARM VISION
BLOOD SYSTEM DISEASE HIGH POINT MORAL DUTY PHYSICAL LOCATION
BODY OF WATER ENABLER HUMAN BODY MOVEMENT PHYSICAL OBJECT
BUILDING ENERGY IMPURITY MOVEMENT ON A VERTICAL SCALE PLANT
Sample Lexical Items
ANIMAL bite, bark, claw, bird, beaver MEDICINE dosage, prescription, heal
ENSLAVEMENT servant, oppression, ruler STRUGGLE enemy, fight, combat, attack
Table 1: The 51 source conceptual domains along with some sample English lexical items for a subset
of them.
DEPTH (e.g., ?deep?, ?bottomless?), ENTRANCE (e.g., ?plunged into?, ?falling into?), and EXIT (e.g.,
?climb out of?).
3.1 Motivating Example
Table 2 shows a sample of the concept candidates associated with the word ?cure? along with the relation
that connects them. Our methodology for extracting these terms is discussed in Section 5.1.
nsubj
NIH, WHO, therapist, doctor, vaccine,
prep of
cancer, AIDS, HIV, malaria, influenza,
drug, medicine, chef, butcher seizures, allergies
dobj
cancer, polio, Goji Berries, man,
prep by
bone marrow transplant, spleen cells,
genetic defects, aging, infant, woman, acupuncture, smoking, salting,
depression, meat, fish, garlic doxycycline, drying, burying, dipping
prep without
surgery, operation, suppuration, salt
prep to
?1
need, project, brine, mineral,
chemotherapy, injections coalition, run, walk, salt, nitrite
prep in
mice, children, baby, spices, salt,
prep for
grinding, smoking, voyages, lox,
monkeys, drug trial, breakthrough, transportation, preservation, jerky,
brine, smokehouse, basement, fridge sausages, bacon, sale
Table 2: Terms that are frequently a part of the grammatical context of ?cure? along with their associated
relations
It is clear from the concept candidates shown that there are at least two coarse-grained senses of
?cure? present ? corresponding to the domains of MEDICINE and FOOD. Table 3 shows a sample
result of clustering these concept candidates. These clusters are organized according to their domain
with MEDICINE-related clusters in the left grouping, FOOD-related clusters in the top-right grouping,
and clusters not strongly related to either domain in the bottom-right grouping. Each row of the table
represents a single cluster. In addition, it can be observed that these clusters correspond to particular
semantic facets of the conceptual domain. For instance, there is a cluster that defines ?procedures which
result in medical cures? (?acupuncture?, ?surgery?, ?operation?, etc.), one that defines ?individuals who
cure food products? (?chef?, ?butcher?), and one that defines ?diseases that can (potentially) be cured?
(?cancer?, ?polio?, ?AIDS?, etc.). Our methodology for automatically inducing such clusters is described
in Section 5.2.
Once the clusters have been identified, they can be used to define a mapping from the original LM
(?cure?) onto a pre-defined set of CM source domains (the MAPPING HYPOTHESIS). In particular,
individual concept candidates are mapped to CM domains by calculating the distance between the can-
didate and one or more vectors representing each domain in a high-dimensional distributional vector
space.
4 Distributional Representations
Our method for identifying conceptual metaphor domains relies on determining when multiple words
should be grouped as belonging to the same conceptual class (the DISTRIBUTIONAL HYPOTHESIS).
Previous work in semantic similarity has shown two types of approaches to work well: (a) hand-coded
knowledge such as WordNet or SUMO, and (b) distributional approaches which rely on statistics of
1756
NIH, WHO, therapist, doctor chef, butcher
vaccine, drug, medicine, doxycycline project, coalition
spleen cells, bone marrow transplant meat, fish, sausages, jerky, bacon, lox
acupuncture, surgery, operation garlic, Goji Berries
chemotherapy, injections, suppuration smoking, salting, drying, dipping
HIV, malaria, influenza burying
cancer, polio, AIDS salt, brine, spices, nitrite, mineral
genetic defects, aging, depression smokehouse, basement, fridge
seizures, allergies run, walk
drug trial, breakthrough voyages, transportation
infant, man, woman, children, baby mice, monkeys
Table 3: Terms from Table 2 grouped into conceptual clusters ? one per line. These clusters are organized
according to their domain association: MEDICINE (left), FOOD (top-right), unclear (bottom-right).
word usage in corpora. We adopt the distributional approach in order to facilitate research in languages
(such as Farsi) for which coverage of existing knowledge bases is limited. The only requirements for our
approach are a corpus with documents written in that language and a syntactic parser for the language.
We use the Malt dependency parser to obtain syntactic parses for web documents in each language.
Table 2 of Section 3.1 shows some of the words which participate regularly with the word ?cure?
in a dependency relation. These syntactic contexts of the word ?cure? form the basis for one semantic
representation we use to find other similar words, which we will call DepVec. All of the dependency
relations for a word are used to form a vector-based distributional representation for that word. This
representation projects words which are semantically similar to one another onto vectors which are near
to each other in the vector space. In the following subsection, we describe DepVec along with LSA and
word2vec which are alternative vector space models of word meaning. These vector spaces are then used
to calculate similarities between words in order to cluster them and to align them with lexicons which
model our existing conceptual spaces.
4.1 Dependency Vectors (DepVec) space
In our DepVec vector space model, each word is represented by a vector whose elements correspond
to syntactic contexts of the word. Each element of the vector for word w corresponds to the fre-
quency of a unique dependency relation (w, r, w
?
) seen in the corpus. For example, if the relation
(whale, nsubj
?1
, swim) is extracted once, then the vector for ?whale? contains a 1 in the element
for (nsubj
?1
, swim) , and the vector for ?swim? contains a 1 for the element (nsubj, whale). This
representation corresponds that proposed by Lin (1998).
However, the use of raw frequency counts in these vectors leads to a situation in which words that
are more frequent in the corpus (e.g., ?of?, ?the?, ?one?) will have higher frequencies in the vectors by
chance alone, and so a high co-occurrence count for those words is not indicative of a significant relation
to the word. We overcome this limitation by replacing the raw frequency counts in each vector with their
corresponding G-test scores. The G-test is a measure of statistical significance for proportions, similar to
the Chi-square test, which measures the degree to which a particular triple (w, r, w
?
) was found to occur
more frequently than expected given all relations (w
??
, r, w
?
). If w
?
occurs far more often with w than
it does with other words, then it will receive a high G-test score for w. In particular, the G-test score is
computed according to the following equation:
G = 2
?
i
O
i
? ln(O
i
/E
i
)
where the index i ranges over the four cells of a 2x2 contingency table, O
i
is the observed count in cell
i, and E
i
is the expected count in the same cell.
1757
Language Source # Documents Language Source # Documents
English ClueWeb 13,361,743 Spanish ClueWeb 3,682,478
Russian ruWac 1,173,590 Farsi Online news sites 835,588
Table 4: Statistics of the corpora used to construct the vector space models
4.2 Latent Semantic Analysis (LSA)
While the DepVec model provides information about the immediate contexts a word can be expected
to occur in, it does not directly capture information about the broader contexts typical of that word,
such as topical information. Latent Semantic Analysis (LSA) is a well-studied model (Landauer and
Dumais, 1997) which does capture such topical information. The LSA model utilizes a singular value
decomposition of a TF-IDF weighted matrix representation of the term-document co-occurrences. Terms
and documents are then represented in a reduced dimensionality space using only the information from
the eigenvectors with the k largest eigenvalues.
4.3 Continuous skip-gram model (W2V)
Mikolov et al. (2013) recently presented a new method for determining distributional word representa-
tions based on a shallow neural network model. The values of the latent vector for each word are trained
to optimize prediction of the words within a 10 token window. This prediction is performed using the
term?s latent vector as the input to a series of log-linear classifiers with outputs which correspond to
probability distributions over the tokens within the context window. Each position in the context window
is assigned its own classifier weights, so that the model used for making predictions about words imme-
diately following the input term is different than the model which makes predictions about the words two
tokens after the term, and so on. Because these latent vector representations are in a low dimensionality
space (300 dimensions in our case), the training process will tend to move the representations for similar
words closer together in this space in order to maximize the predictive accuracy of their contexts.
One benefit of the continuous skip-gram model is that it creates representations which capture some
local context as in the DepVec model, which is required to make predictions about the previous and next
tokens. However, it must also encode some topical knowledge in order to make accurate predictions
about the words seven tokens away. Therefore, using the latent term representations from the continuous
skip-gram model as a vector space puts it in a convenient position in between the two others we presented.
4.4 Corpus Processing
The vector models described above were developed using web-scale corpora collected from a combina-
tion of frequently used NLP corpora and web crawls on news websites. Table 4 indicates the number of
documents used for each language along with their source. These corpora were part-of-speech tagged
with in-house POS taggers for English and Spanish, TreeTagger
3
for Russian, and hunpos
4
for Farsi. The
open-source MaltParser was used to produce dependency parses for all four languages (Nivre, 2003). De-
pendency counts for all words occurring fewer than 40 times and for triples occurring fewer than three
times were discarded to minimize noise.
5 Concept Induction and CM Recognition
In Section 4, we described our DepVec representation of terms as vectors in a high-dimensional dis-
tributional space. These vector representations encode both the dominant grammatical contexts of a
term as well as the selectional preference information associated with it in the form of G-test scores. In
this section, we describe our methodology for inducing conceptual domains for a linguistic metaphor
by adapting techniques for unsupervised word-sense induction (Erk and Pad?o, 2008; Korkontzelos and
Manandhar, 2010; Hope and Keller, 2013). In particular, we induce conceptual domains in an uncon-
strained manner by extracting the grammatical co-occurrents of an LM source term (i.e., the ?concept
candidates?) and clustering them into semantically-related concept clusters. Both the clusters and our
3
http://www.cis.uni-muenchen.de/
?
schmid/tools/TreeTagger/
4
http://code.google.com/p/hunpos/
1758
given source domains are then mapped into a distributional vector space, allowing us to compute cluster-
to-domain scores. Finally, each source domain is assigned a score based on its affinity to each individual
cluster with these affinity scores weighted according to cluster quality. This results in an overall weighted
ranking of the given source conceptual domains for the linguistic metaphor.
5.1 Extracting Concept Candidates
Given a linguistic metaphor which consists of a metaphor source, s (e.g., ?cure?), and a metaphor target,
t (e.g., ?poverty?), our system extracts a set of terms (i.e., ?concept candidates?) from the typical gram-
matical contexts of s as found in the web-scale corpus described in Section 4.4. In order to extract these
candidates, we first determine the syntactic relation, r, which exists between s and t. This relation is the
key point of interaction between the domains of the source and the target for the given LM and, as such,
it provides an indication of which terms will contribute the most to our understanding of the underly-
ing conceptual mapping. In addition, we make use of a predefined set of relations that are semantically
meaningful ? specifically the subjects and objects of verbs (i.e., ?nsubj?, ?nsubjpass?, and ?dobj?),
5
at-
tributes and verbs associated with nouns (i.e., ?amod?, ?dobj
?1
?, ?nsubj
?1
?, and ?nsubjpass
?1
?), the
terms modified by adjectives or adverbs (i.e., ?advmod
?1
? and ?amod
?1
?), and prepositional relations
(e.g., ?prep by?, ?prep of?, ?prep for?). Using this set of relations, R, we extract the set of candidate
terms, X , that have been found to co-occur with the term s within some relation r
i
? R in the prepro-
cessed, web-scale corpus described in Section 4.4 such that X = {x|(s, r
i
, x)exists in the corpus}.
To improve the quality of our extracted candidates, we apply three criteria to isolate those that best
exemplify the underlying non-metaphorical senses of s. First, we anticipate that any term in X which
does not co-occur with s at least k times will not be informative,
6
and so we remove such terms from
further processing. Next, we predict that poorly imageable terms (i.e., highly abstract terms) are likely to
represent metaphorical usages of s and so are unlikely to be integral to a given literal source domain, so
these are filtered out as well.
7
Finally, to improve our ability to map these candidates into a conceptual
domain, we remove terms that are not significantly related to any of our provided source domains (i.e.,
those that are off-topic) along with terms that are strongly related to multiple source domains (i.e., those
that are ambiguous) as these provide little evidence to distinguish the most appropriate concept for the
given LM.
8
We determine the relatedness of a term to a source domain by measuring the similarity of
the term and domain vectors in our distributional space as described in Section 5.3.
5.2 Clustering Concept Candidates
Once the candidates have been extracted, they are clustered using a hierarchical agglomerative clustering
algorithm with the distance metric defined as the cosine distance between the vectors within one of our
distributional vector spaces. Each cluster is then assigned a quality score based on its size (to prefer large
clusters with a large amount of semantic evidence), average internal distance (to prefer tighter clusters),
and co-occurrence frequency with the LM source (to prefer more closely related terms). Formally, we
define the weight associated with a given cluster using the following equation:
w(C) = (1? IDIST (C)) ? (S2(C) + FREQ(C) ? (1 + S2(C)))
S2(C) =
max(SIZE(C)
2
, k)
k
where IDIST (C) represents the average vector distance between all pairs of terms in cluster C,
FREQ(C) represents the total co-occurrence frequency of the terms in C with the original LM,
5
These dependency relation types come from the MaltParser.
6
We empirically set k to 3.
7
We estimate candidate imageability by combining the scores of the candidate?s most distributionally similar words for
which an imageability score is available in the MRC psycholinguistics database (Coltheart, 1981) using the ranked weighting
methodology described in Mohler et al. (2014).
8
Note that filtering by conceptual domain relatedness is only necessary when mapping the induced concepts to a predefined
set of source concepts.
1759
SIZE(C) represents the number of unique terms in C, and k is a tuning parameter meant to favor
large clusters.
9
Singleton clusters are discarded.
5.3 Assigning Domain Scores to Concept Candidates
We propose two methods for calculating domain scores for candidates ? one which attempts to compare
candidate vectors to a source domain directly, and and another which attempts to compare them to indi-
vidual facets of the domain. These two methods rely on representing sources [CentS], or facets [CentF],
as centroids which take the average of the vectors of each the lexemes assigned to that source (or facet).
Our three vector spaces ? DepVec, W2V, and LSA ? along with our two methods for mapping terms to
domains ? CentS and CentF ? correspond to six approaches to modeling a CM domain in some vector
space.
In each case, the result for a given candidate is a distribution over all source domain scores. This
distribution is then normalized by subtracting the mean score between the candidate vector and any of
the source concepts. Formally, we define the normalized distribution for concept candidate x as:
S(x,D
y
) = (1?DIST (x,D
y
))?
?
D
k
?D
(1?DIST (x,D
k
))
|D|
where D is defined as the set of all known source domains and DIST (x, d) is the cosine distance from
x to a CM domain d in one of our vector spaces.
5.3.1 Assigning Domain Scores to Clusters
Within a given cluster (found as described in Section 5.2), the individual concept domain scores can then
be combined to produce cluster-level domain scores. For a given cluster C
x
, the score associated with a
particular source domain D
y
is defined as follows:
S(C
x
, D
y
) =
N
?
i=1
S(C
xi
, D
y
)
?
i
where N represents the number of concepts in C
x
with a positive score for the domain D
y
, C
xi
is the
i-th highest score associated with any candidate in the cluster, and ? is a tuning parameter which bounds
the growth of the cluster-level score.
10
Any cluster with a maximum domain score that does not exceed
a threshold is discarded as being weakly related to any CM source domain.
5.3.2 Assigning Domain Scores to the Linguistic Metaphor
We then sum the cluster-level source domain scores, scaling each by its associated cluster quality weight
w(c) as computed in Section 5.2. By scaling cluster domain scores in this way, we ensure that the most
pure and discriminating clusters contribute the most to the overall LM domain scores. The final result
measuring the association between the given LM and the source domain D
y
is then defined as:
S(D
y
) =
?
C
x
?C
w(C
x
) ? S(C
x
, D
y
)
Applied across all known domains, we therefore produce a ranked and scored list of CM source do-
mains (i.e., a mapping) that are associated with the given linguistic metaphor and can be used to drive
more robust interpretation of the metaphor.
6 Evaluation
We evaluate two aspects of our end-to-end CM recognition system. First, we analyze the impact of our
choice of vector space. Specifically, we compare the use of our DepVec space to link concept candidates
9
In our experiments, k is set to 5.
10
We have used a value of ? = 2 which ensures that the result remains within the bounds [0.0,1.0].
1760
with source domains against two off-the-shelf vector space models ? the continuous skip-gram model
[W2V] (Mikolov et al., 2013)
11
and latent semantic analysis [LSA] (Landauer and Dumais, 1997). Both
alternative models were trained over the same corpus as in our DepVec space using a predefined number
of dimensions (300 for W2V; 400 for LSA). Second, we have experimented with two different metrics for
calculating the distance between a vector and a source concept ? the cosine distance to the source-level
centroid (CentS) and the cosine distance to the facet-level centroid (CentF).
Our evaluation dataset consists of a held out, unseen set of documents taken from a variety of news
articles, opinion pages, and blogs on the open web. These documents consist of 3 to 5 sentences each
and cover four of our focus languages.
12
They were then annotated by two native-proficiency speakers in
the following way. For each LM, they were instructed to choose the most closely related source concept
from our list of 51 provided. Any source concepts selected by at least one annotator were considered
correct. Since our CM recognition system produces a ranked list of source concepts, we report both the
accuracy associated with our top-ranked concept and the accuracy of the system when allowed to select
two.
Cluster Linking
English Spanish Russian Farsi
Vector Space Distance Acc@1 Acc@2 Acc@1 Acc@2 Acc@1 Acc@2 Acc@1 Acc@2
DepVec CentS 28.0% 44.1% 33.3% 43.4% 24.4% 32.6% 16.5% 27.5%
CentF 25.8% 40.9% 33.3% 49.4% 25.6% 34.9% 26.4% 40.7%
LSA CentS 34.4% 45.2% 31.0% 41.4% 27.9% 41.9% 22.0% 27.5%
CentF 38.7% 54.9% 27.6% 46.0% 29.1% 47.7% 31.9% 44.0%
W2V CentS 24.7% 36.6% 42.5% 55.2% 31.4% 43.0% 25.3% 34.1%
CentF 28.0% 44.1% 46.0% 58.6% 34.9% 48.8% 35.2% 48.4%
Table 5: The accuracy of our conceptual interpretation system. We experiment with three vector spaces
(LSA, W2V, and DepVec) and two source concept centroid representations ? source-level (CentS) and
facet-level (CentF).
These results indicate that the continuous skip-gram vector space [W2V] is well suited to the task of
cluster-level concept mapping, consistently and significantly outperforming both the LSA space and the
DepVec space in every language but English. We believe that this is a result of its probabilistic represen-
tation of local context which implicitly collects many of the same relations as the DepVec model while
incorporating the advantages associated with dimensionality reduction which has not been incorporated
into our DepVec model.
13
We further observe an unmistakable dominance of the facet-level centroid
representation over the source-level representation. Based on these results, we believe that we have suc-
cessfully demonstrated the contribution of our system?s vector-space clustering component which groups
concept candidates at a facet-level granularity.
7 Conclusion
In this paper, we have presented a novel approach to the problem of multilingual conceptual metaphor
recognition which combines facet-based concept induction with a distributional vector space represen-
tation of metaphor. We have experimentally demonstrated the advantage of our fine-grained concept
induction approach within a variety of vector space models, including our novel DepVec space. Taken
together, we hypothesize that a facet-level conceptual model represented in a relational context vec-
tor space will serve as a reliable foundation enabling high-quality metaphoric interpretation in future
metaphor research. Future work includes expanding the set of concept candidates through higher-order
dependency contexts, improved clustering techniques, and evaluating the induced clusters directly.
11
We make use of the implementation included as part of the gensim python package: http://radimrehurek.com/
gensim/
12
This dataset consists of the following counts of documents: English (92), Spanish (86), Russian (85), Farsi (90).
13
During our pilot experiments, we applied singular value decomposition (SVD) to the DepVec space without any significant
improvement to system performance.
1761
Acknowledgments
This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via De-
partment of Defense US Army Research Laboratory contract number W911NF-12-C-0025. The U.S.
Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those
of the authors and should not be interpreted as necessarily representing the official policies or endorse-
ments, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.
References
Pierre Beust, St?ephane Ferrari, Vincent Perlerin, et al. 2003. NLP model and tools for detecting and interpreting
metaphors in domain-specific corpora. In Proceedings of the Corpus Linguistics 2003 conference, volume 16,
pages 114?123. Citeseer.
Danushka Bollegala and Ekaterina Shutova. 2013. Metaphor interpretation using paraphrases extracted from the
web. PloS one, 8(9):e74304.
D. Bracewell, M. Tomlinson, M. Mohler, and B. Rink. 2014. A tiered approach to the recognition of metaphor. In
Computational Linguistics and Intelligent Text Processing.
Siaw-Fong Chung, Kathleen Ahrens, and Chu-Ren Huang. 2005. Source domains as concept domains
in metaphorical expressions. International Journal of Computational Linguistics and Chinese Language
Processing, 10(4):553?570.
Max Coltheart. 1981. The MRC psycholinguistic database. The Quarterly Journal of Experimental Psychology,
33(4):497?505.
Katrin Erk and Sebastian Pad?o. 2008. A structured vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897?906. As-
sociation for Computational Linguistics.
J. Feldman and S. Narayanan. 2004. Embodied meaning in a neural theory of language. Brain and language,
89(2):385?392.
Lisa Gandy, Nadji Allan, Mark Atallah, Ophir Frieder, Newton Howard, Sergey Kanareykin, Moshe Koppel, Mark
Last, Yair Neuman, and Shlomo Argamon. 2013. Automatic identification of conceptual metaphors with limited
knowledge. In Twenty-Seventh AAAI Conference on Artificial Intelligence.
David Hope and Bill Keller. 2013. MaxMax: a graph-based soft clustering algorithm applied to word sense
induction. In Computational Linguistics and Intelligent Text Processing, pages 368?381. Springer.
Ioannis Korkontzelos and Suresh Manandhar. 2010. UoY: Graphs of unambiguous vertices for word sense in-
duction and disambiguation. In Proceedings of the 5th international workshop on semantic evaluation, pages
355?358. Association for Computational Linguistics.
G. Lakoff and M. Johnson. 1980. Metaphors we live by, volume 111. Chicago London.
G. Lakoff. 1993. The contemporary theory of metaphor. Metaphor and thought, 2:202?251.
T.K. Landauer and S.T. Dumais. 1997. A solution to Plato?s problem: The latent semantic analysis theory of acqui-
sition, induction, and representation of knowledge. Psychological Review; Psychological Review, 104(2):211.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
Z.J. Mason. 2004. CorMet: A computational, corpus-based conventional metaphor extraction system.
Computational Linguistics, 30(1):23?44.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in
vector space. arXiv preprint arXiv:1301.3781.
Michael Mohler, Marc Tomlinson, and David Bracewell. 2013. Applying textual entailment to the interpretation
of metaphor. In IEEE Seventh International Conference on Semantic Computing (ICSC), pages 118?125. IEEE.
1762
Michael Mohler, Marc Tomlinson, David Bracewell, and Bryan Rink. 2014. Semi-supervised methods for expand-
ing psycholinguistics norms by integrating distributional similarity with the structure of WordNet. Language
Resources and Evaluation Conference 2014.
Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th
International Workshop on Parsing Technologies (IWPT. Citeseer.
Astrid Reining and Birte L?onneker-Rodman. 2007. Corpus-driven metaphor harvesting. In Proceedings of the
Workshop on Computational Approaches to Figurative Language, pages 5?12. Association for Computational
Linguistics.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised metaphor identification using hierarchical graph factorization
clustering. In Proceedings of NAACL-HLT, pages 978?988.
E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor identification using verb and noun clustering. In
Proceedings of the 23rd International Conference on Computational Linguistics, pages 1002?1010. Associa-
tion for Computational Linguistics.
Ekaterina Shutova. 2010. Automatic metaphor interpretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the North American Chapter of the Association for
Computational Linguistics, pages 1029?1037. Association for Computational Linguistics.
Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh,
Ting Liu, Kit Cho, Umit Boz, Ignacio Cases, et al. 2013. Robust extraction of metaphors from novel data.
Meta4NLP 2013, page 67.
T. Veale and Y. Hao. 2008. A fluid knowledge representation for understanding and generating creative metaphors.
In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 945?952.
Association for Computational Linguistics.
Yorick Wilks, Lucian Galescu, James Allen, and Adam Dalton. 2013. Automatic metaphor detection using large-
scale lexical resources and conventional metaphor extraction. Meta4NLP 2013, page 36.
1763
Proceedings of the First Workshop on Metaphor in NLP, pages 27?35,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Semantic Signatures for Example-Based Linguistic Metaphor Detection
Michael Mohler and David Bracewell and David Hinote and Marc Tomlinson
Language Computer Corp.
Richardson, Texas, USA
{michael,david,dhinote,marc}@languagecomputer.com
Abstract
Metaphor is a pervasive feature of human lan-
guage that enables us to conceptualize and
communicate abstract concepts using more
concrete terminology. Unfortunately, it is
also a feature that serves to confound a com-
puter?s ability to comprehend natural human
language. We present a method to detect
linguistic metaphors by inducing a domain-
aware semantic signature for a given text and
compare this signature against a large index
of known metaphors. By training a suite of
binary classifiers using the results of several
semantic signature-based rankings of the in-
dex, we are able to detect linguistic metaphors
in unstructured text at a significantly higher
precision as compared to several baseline ap-
proaches.
1 Introduction
Metaphor is a widely-used literary mechanism
which allows for the comparison of seemingly un-
related concepts. It has been thoroughly studied in
both the linguistics literature (Ahrens et al, 2003;
Lakoff and Johnson, 1980; Tourangeau and Stern-
berg, 1982; Wilks, 1978) and more recently within
the field of computational linguistics.1 Although
there have been many influential theories regarding
the cognitive basis of metaphor, the most promi-
nent among them is Lakoff?s Contemporary The-
ory of Metaphor (Lakoff and Johnson, 1980; Lakoff,
1993), which popularized the idea of a conceptual
1For a broad survey of the relevant literature, see Shutova
(2010).
metaphor mapping. Within the cognitive framework
of a given conceptual mapping, terms pertaining to
one concept or domain (the source) can be used fig-
uratively to express some aspect of another concept
or domain (the target). For example, the conceptual
metaphor ?Life is a Journey? indicates a medium
within which the target concept ?life? may be more
easily discussed and understood. This particular
mapping allows us to speak of one being stuck in a
?dead-end? job, a crucial decision as being a ?fork in
the road?, or someone?s life ?taking a wrong turn?.
By allowing us to discuss an abstract target con-
cept using the vocabulary and world knowledge
associated with a more familiar source concept,
metaphor serves as a vehicle for human communica-
tion and understanding, and as such, has been found
to be extremely prevalent in natural language, oc-
curring as often as every third sentence (Shutova et
al., 2010). As a consequence of this ubiquity, it is
crucial that any system tasked with the understand-
ing of natural language be capable of detecting the
presence of metaphor in text and of modeling the
intended semantic content of the metaphoric expres-
sion. In this work, we first induce a domain-sensitive
semantic signature which we define as a set of highly
related and interlinked WordNet (Fellbaum, 1998)
senses drawn and augmented from a text that may
be used to place the text within the semantic space
of a metaphoric concept. We then employ a suite
of binary classifiers to detect metaphoricity within
a text by comparing its semantic signature to a set
of known metaphors. If the semantic signature of
the text closely matches the signature of a known
metaphor, we propose that it is likely to represent
27
Example Metaphor
Obama heard a bomb ticking in his left ear. No
Obama heard another political bomb ticking, this time in his left ear. Yes
Table 1: The top sentence describes a literal bomb ticking, while the bottom sentence uses metaphoric language to
describe an impending political disaster.
an instance of the same conceptual metaphor. To fa-
cilitate this work, we have built an index of known
metaphors within a particular target domain. We
have selected the domain of Governance which we
define broadly to include electoral politics, the set-
ting and enactment of economic policy, and the
creation, application, and enforcement of rules and
laws.
The problem of metaphor as it relates to computer
understanding is illustrated in the example sentences
of Table 1. A strictly literal reading suggests that the
two sentences are describing something very similar.
At the very least, the semantics of the phrases ?bomb
ticking? and ?in his left ear? are indistinguishable
without the added knowledge that the second sen-
tence is using metaphor to convey information about
something altogether different from explosives and
body parts. From the context of the full sentences,
it is clear that while the first sentence is straight-
forwardly describing Obama and his perception of
a literal bomb, the second is describing an impend-
ing political crisis as though it were a bomb. Rather
than a literal ?ear? this sentence uses the phrase ?in
his left ear? to suggest that the source of the crisis in
on the political ?left?. In order for an automated sys-
tem to correctly understand the intended meaning of
these sentences, it must first be aware that the text
under consideration is not to be taken literally, and
given this knowledge, it must employ all available
knowledge of the underlying conceptual mapping to
appropriately interpret the text in context.
The remainder of this work is organized as fol-
lows. In Section 2, we survey related work in se-
mantic representation and linguistic metaphor iden-
tification. Section 3 describes in detail our approach
to metaphor identification through the use of seman-
tic signatures. In Section 4, we discuss the setup of
our experiment which includes the creation of our
metaphor index as well as the extraction and anno-
tation of our training and testing data sets. Finally,
we show the results of our experiments in Section 5
and share our conclusions in Section 6.
2 Related Work
The phenomenon of metaphor has been studied
by researchers across multiple disciplines, includ-
ing psychology, linguistics, sociology, anthropol-
ogy, and computational linguistics. A number of
theories of metaphor have been proposed, includ-
ing the Contemporary Theory of Metaphor (Lakoff,
1993), the Conceptual Mapping Model (Ahrens et
al., 2003), the Structure Mapping Model (Wolff and
Gentner, 2000), and the Attribute Categorization
Hypothesis (McGlone, 1996). Based on these the-
ories, large collections of metaphors have been as-
sembled and published for use by researchers. The
Master Metaphor List (MML) (Lakoff, 1994) groups
linguistic metaphors together according to their
conceptual mapping, and the Hamburg Metaphor
Database (HMD) (Eilts and Lo?nneker, 2002) for
French and German fuses EuroWordNet synsets
with the MML source and target domains for a ro-
bust source of metaphoric semantics in those lan-
guages.
In recent years, the computational linguistics
community has seen substantial activity in the de-
tection of figurative language (Bogdanova, 2010;
Li and Sporleder, 2010; Peters and Wilks, 2003;
Shutova, 2011) one aspect of which is the iden-
tification of metaphoric expressions in text (Fass,
1991; Shutova et al, 2010; Mason, 2004). Much of
the early work on the identification of metaphor re-
lied upon hand-crafted world knowledge. The met*
(Fass, 1991) system sought to determine whether an
expression was literal or figurative by detecting the
violation of selectional preferences. Figurative ex-
pressions were then classified as either metonymic,
using hand-crafted patterns, or metaphoric, us-
ing a manually constructed database of analogies.
The CorMet (Mason, 2004) system determined the
28
source and target concepts of a metaphoric expres-
sion using domain-specific selectional preferences
mined from Internet resources. More recent work
has examined noun-verb clustering (Shutova et al,
2010) which starts from a small seed set of one-
word metaphors and results in clusters that rep-
resent source and target concepts connected via a
metaphoric relation. These clusters are then used to
annotate the metaphoricity of text.
Similar to our work, the Metaphor Interpreta-
tion, Denotation, and Acquisition System (MIDAS)
(Martin, 1990) employed a database of conventional
metaphors that could be searched to find a match
for a metaphor discovered in text. If no match
was found, the metaphoric text was replaced with a
more abstract equivalent (e.g. a hypernym) and the
database was searched again. If a match was found,
an interpretation mapping was activated, and the
novel metaphor would be added to the database for
use in future encounters. Unfortunately, this tech-
nique was limited to interpreting known metaphors
(and descendants of known metaphors) and was un-
able to detect truly novel usages. By expanding the
metaphors using a more robust semantic signature,
we attempt to transcend this limitation thereby pro-
ducing a more durable system for metaphoric exam-
ple linking.
An additional vein of metaphor research has
sought to model the human processing of metaphor
as a semantic space within which source and tar-
get concepts can be placed such that the similar-
ity between their representations within this space
(i.e. semantic vectors) can be sensibly quantified
(Katz, 1992; Utsumi, 2011). One computational
example of this approach (Kintsch, 2000) has em-
ployed latent semantic analysis (LSA) (Landauer
and Dumais, 1997) to represent the semantic space
of the metaphors in a reduced dimensionality (i.e.
using singular value decomposition). In their ap-
proach, metaphors were represented as a set of terms
found using a spreading activation algorithm in-
formed by the terms? independent vector related-
ness to the source and target concepts within some
LSA space. By contrast, we have chosen to rep-
resent the metaphoric space using WordNet senses
which have been shown in previous work(Lo?nneker,
2003) to represent a viable representation language
for metaphor. We believe that the ontological knowl-
edge encoded in the semantic relationships of Word-
Net represents an improvement over the distribu-
tional relatedness encoded within an LSA vector.
Also of relevance to the construction and use of
semantic signatures is current research on the induc-
tion of topic signatures. A topic signature is a set of
related words with associated weights which define
and indicate the distinct topics within a text. In their
work on automated summarization, Lin and Hovy
(2000) developed a method for the construction of
topic signatures which were mined from a large cor-
pus. Similarly, Harabagiu and Lacatusu (2005) ex-
plored the use of topic signatures and enhanced topic
signatures for their work on multi-document sum-
marization. By contrast, we explore the use of se-
mantic signatures which serve to enrich the seman-
tics of the source and target frame concepts being
expressed in a text for the purpose of detecting the
presence of metaphor.
3 Methodology
In this work, we approach the task of linguis-
tic metaphor detection as a classification problem.
Starting from a known target domain (i.e. Gover-
nance), we first produce a target domain signature
which represents the target-specific dimensions of
the full conceptual space. Using this domain sig-
nature, we are able to separate the individual terms
of a sentence into source frame elements and tar-
get frame elements and to independently perform a
semantic expansion for each set of elements using
WordNet and Wikipedia as described in our earlier
work (Bracewell et al, 2013). Taken together, the
semantic expansions of a text?s source frame ele-
ments and target frame elements make up the full se-
mantic signature of the text which can then be com-
pared to an index of semantic signatures generated
for a collection of manually detected metaphors. We
use as features for our classifiers a set of metrics that
are able to quantify the similarity between the given
semantic signature and the signatures of metaphors
found within the index.
3.1 Constructing a Target Domain Signature
In order to produce a semantic representation of the
text, we first build a target domain signature, which
we define as a set of highly related and interlinked
29
Figure 1: Focused crawling of Wikipedia articles pertaining to the target concept using intra-wiki links
Figure 2: Constructing the domain signature of the target concept from Wikipedia articles pertaining to the target
concept
WordNet senses that correspond to our particular
target domain with statistical reliability. For ex-
ample, in the domain of Governance the concepts
of ?law?, ?government?, and ?administrator?, along
with their associated senses in WordNet, are present
in the domain signature. We generate this signa-
ture using semantic knowledge encoded in the fol-
lowing resources: (1) the semantic network encoded
in WordNet; (2) the semantic structure implicit in
Wikipedia; and (3) collocation statistics taken from
the statistical analysis of a large corpora. In par-
ticular, we use Wikipedia as an important source
of world knowledge which is capable of provid-
ing information about concepts, such as named en-
tities, that are not found in WordNet as shown in
several recent studies (Toral et al, 2009; Niemann
and Gurevych, 2011). For example, the organi-
zation ?Bilderberg Group? is not present in Word-
Net, but can easily be found in Wikipedia where
it is listed under such categories as ?Global trade
and professional organizations?, ?International busi-
ness?, and ?International non-governmental orga-
nizations?. From these categories we can deter-
mine that the ?Bilderberg Group? is highly related
to WordNet senses such as ?professional organiza-
tion?, ?business?, ?international?, and ?nongovern-
mental organization?.
We begin our construction of the domain signa-
ture by utilizing the semantic markup in Wikipedia
to collect articles that are highly related to the tar-
get concept by searching for the target concept (and
optionally content words making up the definition
of the target concept) in the Wikipedia article titles
and redirects. These articles then serve as a ?seed
set? for a Wikipedia crawl over the intra-wiki links
present in the articles. By initiating the crawl on
these links, it becomes focused on the particular do-
main expressed in the seed articles. The crawling
process continues until either no new articles are
found or a predefined crawl depth (from the set of
seed articles) has been reached. The process is illus-
trated in Figure 1. The result of the crawl is a set
of Wikipedia articles whose domain is related to the
target concept. From this set of articles, the domain
signature can be built by exploiting the semantic in-
formation provided by WordNet.
The process of going from a set of target concept
articles to a domain signature is illustrated in Fig-
ure 2 and begins by associating the terms contained
in the gathered Wikipedia articles with all of their
possible WordNet senses (i.e. no word sense disam-
biguation is performed). The word senses are then
expanded using the lexical (e.g. derivationally re-
lated forms) and semantic relations (e.g. hypernym
and hyponym) available in WordNet. These senses
are then clustered to eliminate irrelevant senses us-
ing the graph-based Chinese Whispers algorithm
(Biemann, 2006). We transform our collection of
word senses into a graph by treating each word sense
as a vertex of an undirected, fully-connected graph
where edge weights are taken to be the product of
the Hirst and St-Onge (1998) WordNet similarity be-
30
tween the two word senses and the first-order cor-
pus cooccurrence of the two terms. In particular, we
use the normalized pointwise mutual information as
computed using a web-scale corpus.
The clusters resulting from the Chinese Whispers
algorithm contain semantically and topically similar
word senses such that the size of a cluster is directly
proportional to the centrality of the concepts within
the cluster as they pertain to the target domain. After
removing stopwords from the clusters, any clusters
below a predefined size are removed. Any cluster
with a low2 average normalized pointwise mutual
information (npmi) score between the word senses
in the cluster and the word senses in the set of terms
related to the target are likewise removed. This set
of target-related terms used in calculating the npmi
are constructed from the gathered Wikipedia articles
using TF-IDF (term frequency inverse document fre-
quency), where TF is calculated within the gathered
articles and IDF is calculated using the entire textual
content of Wikipedia. After pruning clusters based
on size and score, the set of word senses that remain
are taken to be the set of concepts that make up the
target domain signature.
3.2 Building Semantic Signatures for
Unstructured Text
After constructing a signature that defines the do-
main of the target concept, it is possible to use this
signature to map a given text (e.g. a sentence) into
a multidimensional conceptual space which allows
us to compare two texts directly based on their con-
ceptual similarity. This process begins by mapping
the words of the text into WordNet and extracting
the four most frequent senses for each term. In or-
der to improve coverage and to capture entities and
terms not found in WordNet, we also map terms
to Wikipedia articles based on a statistical measure
which considers both the text of the article and the
intra-wiki links. The Wikipedia articles are then
mapped back to WordNet senses using the text of
the categories associated with the article.
In the next step, source and target frame ele-
ments of a given text are separated using the Word-
Net senses contained in the target domain signature.
2We define low as being below an empirically defined
threshold, ? .
Terms in the text which have some WordNet sense
that is included in the domain signature are clas-
sified as target frame elements while those that do
not are considered source frame elements. Figure 3
shows an overview of the process for determining
the source and target concepts within a text. The
remainder of the signature induction process is per-
formed separately for the source and target frame el-
ements. In both cases, the senses are expanded using
the lexical and semantic relations encoded in Word-
Net, including hypernymy, domain categories, and
pertainymy. Additionally, source frame elements
are expanded using the content words found in the
glosses associated with each of the noun and verb
senses. Taken together, these concepts represent the
dimensions of a full conceptual space which can be
separately expressed as the source concept dimen-
sions and target concept dimensions of the space.
Figure 3: Example of a generated conceptual space for a
given text. In this work, only one iteration of the sense
expansion is performed.
In order to determine the correct senses for in-
clusion in the semantic signature of a text, cluster-
ing is performed using the same methodology as
in the construction of the domain signature. First,
a graph is built from the senses with edge weights
assigned based on WordNet similarity and cooccur-
rence. Then, the Chinese Whispers algorithm is used
to cluster the graph which serves to disambiguate the
senses and to prioritize which senses are examined
and incorporated into the source concept dimensions
of the conceptual space. Word senses are prioritized
by ranking the clusters based on their size and on the
highest scoring word sense contained in the cluster
using:
rank(c) = size(c) ?
(?
s score(s)
|c|
)
(1)
where c is the cluster, s is a word sense in the clus-
31
ter, and |c| is the total number of word senses in the
cluster. The senses are scored using: (1) the degree
distribution of the sense in the graph (more central
word senses are given a higher weight); and (2) the
length of the shortest path to the terms appearing in
the given text with concepts closer to the surface
form given a higher weight. Formally, score(s) is
calculated as:
score(s) =
degree(s) + dijkstra(s,R)
2
(2)
where degree(s) is degree distribution of s and
dijkstra(s,R) is the length of the shortest path in
the graph between s and some term in the original
text, R.
Clusters containing only one word sense or with
a score less than the average cluster score (?c) are
ignored. The remaining clusters and senses are
then examined for incorporation into the concep-
tual space with senses contained in higher ranked
clusters examined first. Senses are added as con-
cepts within the conceptual space when their score is
greater than the average word sense score (?s). To
decrease redundancy in the dimensions of the con-
ceptual space, neighbors of the added word sense in
the graph are excluded from future processing.
3.3 Classification
Given a semantic signature representing the place-
ment of a text within our conceptual space, it is pos-
sible to measure the conceptual distance to other sig-
natures within the same space. By mapping a set
of known metaphors into this space (using the pro-
cess described in Section 3.2), we can estimate the
likelihood that a given text contains some metaphor
(within the same target domain) by using the seman-
tic signature of the text to find the metaphors with
the most similar signatures and to measure their sim-
ilarity with the original signature.
We quantify this similarity using five related mea-
sures which are described in Table 2. Each of these
features involves producing a score that ranks ev-
ery metaphor in the index based upon the seman-
tic signature of the given text in a process similar to
that of traditional information retrieval. In particu-
lar, we use the signature of the text to build a query
against which the metaphors can be scored. For each
word sense included in the semantic signature, we
add a clause to the query which combines the vector
space model with the Boolean model so as to prefer
a high overlap of senses without requiring an identi-
cal match between the signatures.3
Three of the features simply take the score of
the highest ranked metaphor as returned by a query.
Most simply, the feature labeled Max Score (na??ve)
uses the full semantic signature for the text which
should serve to detect matches that are very simi-
lar in both the source concept dimensions and the
target concept dimensions. The features Max Score
(source) and Max Score (target) produce the query
using only the source concept dimensions of the
signature and the target concept dimensions respec-
tively.
The remaining two features score the metaphors
within the source dimensions and the target dimen-
sions separately before combining the results into a
joint score. The feature Max Score (joint) calculates
the product of the scores for each metaphor using the
source- and target-specific queries described above
and selects the maximum value among these prod-
ucts. The final feature, Joint Count, represents the
total number of metaphors with a score for both the
source and the target dimensions above some thresh-
old (?j). Unlike the more na??ve features for which a
very good score in one set of dimensions may incor-
rectly lead to a high overall score, these joint similar-
ity features explicitly require metaphors to match the
semantic signature of the text within both the source
and target dimensions simultaneously.
Altogether, these five features are used to train
a suite of binary classifiers to make a decision on
whether a given text is or is not a metaphor.
4 Experimental Setup
One crucial component of our linguistic metaphor
detection system is the index of metaphors (in the
domain of Governance) against which we com-
pare our candidate texts. As a part of this project,
we have produced an ever-growing, metaphor-rich
dataset taken from political speeches, political web-
sites (e.g. Communist Party USA, Tea Party sites,
3This functionality comes standard with the search function-
ality of Apache Lucene which we employ for the production of
our index.
32
Measure Description
Max Score (na??ve) Find the score of the metaphor that best matches the full semantic signature
Max Score (source) Find the score of the metaphor that best matches the source side of the semantic signature
Max Score (target) Find the score of the metaphor that best matches the target side of the semantic signature
Max Score (joint)
Independently score the metaphors by the target side and by the source side.
Find the metaphor with the highest product of the scores.
Joint Count
Independently score the metaphors by the target side and by the source side.
Count the number of metaphors that receive a positive score for both.
Table 2: The five features used by our metaphoricity classifiers.
etc.), and political commentary in web-zines and on-
line newspapers. Three annotators have analyzed
the raw texts and manually selected snippets of text
(with context) whenever some element in the text
seemed to have been used figuratively to describe
or stand in for another element not represented in
the text.4 Each of these metaphors is projected into
a conceptual space using the process described in
Section 3.2 and assembled into a searchable index.
For evaluation purposes, we have selected a sub-
set of our overall repository which consists of
500 raw documents that have been inspected for
metaphoricity by our annotators. We allocate 80%
of these documents for the training of our classi-
fiers and evaluate using the remaining 20%. In total,
our training data consists of 400 documents contain-
ing 1,028 positive examples of metaphor and around
16,000 negative examples. Our test set consists of
100 documents containing 4,041 sentences with 241
positive examples of metaphor and 3,800 negative
examples. For each sentence in each document, our
system attempts to determine whether the sentence
does or does not contain a metaphor within the do-
main of Governance.
We have experimented with several flavors of ma-
chine learning classification. In addition to an in-
house implementation of a binary maximum en-
tropy (MaxEnt) classifier, we have evaluated our re-
sults using four separate classifiers from the popu-
lar Weka machine learning toolkit.5 These include
an unpruned decision tree classifier (J48), a support
vector machine (SMO) approach using a quadratic
4Generally speaking, each annotator operated within a re-
gion of high precision and low recall, and the overlap between
individual annotators was low. As such, we have selected the
union of all metaphors detected by the annotators.
5http://www.cs.waikato.ac.nz/ml/weka/
kernel with parameters tuned via grid search, a rule-
based approach (JRIP), and a random forest clas-
sifier (RF). In addition, we have combined all five
classifiers into an ensemble classifier which uses a
uniformly-weighted voting methodology to arrive at
a final decision.
5 Results
We have evaluated our methodology in two ways.
First, we have performed an evaluation which high-
lights the discriminatory capabilities of our features
by testing on a balanced subset of our test data.
Next, we performed an evaluation which shows the
utility of each of our classifiers as they are applied
to real world data with a natural skew towards literal
usages.6 In both cases, we train on a balanced sub-
set of our training data using all 1,028 positive ex-
amples and a set of negative examples selected ran-
domly such that each document under consideration
contains the same number of positive and negative
examples. In an initial experiment, we trained our
classifiers on the full (skewed) training data, but the
results suggested that an error-minimizing strategy
would lead to all sentences being classified as ?lit-
eral?.
As shown in Table 3, the choice of classifier ap-
pears significant. Several of the classifiers (J48,
JRIP, and MaxEnt) maintain a high recall suggest-
ing the ability of the tree- and rule-based classifiers
to reliably ?filter out? non-metaphors. On the other
hand, other classifiers (SMO and ENSEMBLE) op-
erate in a mode of high precision suggesting that a
high confidence can be associated with their positive
classifications. In all cases, performance is signifi-
6Note that metaphors that are not related to the domain of
Governance are classified as ?literal?.
33
Classifier Precision Recall F-Measure
J48 56.1% 93.0% 70.0%
JRIP 57.7% 79.3% 66.8%
MaxEnt 59.9% 72.6% 65.7%
ENSEMBLE 72.0% 42.7% 53.7%
RF 55.8% 47.7% 51.5%
SMO 75.0% 33.6% 46.4%
All metaphor 50.0% 100.0% 66.7%
Random baseline 50.0% 50.0% 50.0%
Table 3: The results of our experiments using several ma-
chine learning classifiers while evaluating on a dataset
with 241 positive examples and 241 negative examples.
cantly better than chance as reported by our random
baseline.7
Table 4 shows the result of evaluating the same
models on an unbalanced dataset with a natural
skew towards ?literal? sentences which reflects a
more realistic use case in the context of linguistic
metaphor detection. The results suggest that, once
again, the decision tree classification accepts the
vast majority of all metaphors (93%), but also pro-
duces a significant number of false positives mak-
ing it difficult to usefully employ this classifier as
a complete metaphor detection system despite its
top-performing F-measure on the balanced dataset.
More useful is the SMO approach, which shows a
precision over twice that of the random baseline. Put
another way, a positive result from this classifier is
more than 110% more likely to be correct than a
random classification. From the standpoint of util-
ity, joining these classifiers in an ensemble config-
uration seems to combine the high precision of the
SMO classifier with the improved recall of the other
classifiers making the ensemble configuration a vi-
able choice in a real world scenario.
6 Conclusions
We have shown in this work the potential utility
of our example-based approach to detect metaphor
within a domain by comparing the semantic signa-
ture of a text with a set of known metaphors. Al-
though this technique is necessarily limited by the
coverage of the metaphors in the index, we believe
that it is a viable technique for metaphor detection
7According to Fisher?s exact test (one-tailed): RF (p <
0.02); all others (p < 0.0001).
Classifier Precision Recall F-Measure
SMO 12.7% 33.6% 18.4%
ENSEMBLE 11.2% 42.7% 17.8%
MaxEnt 8.7% 72.6% 15.6%
JRIP 8.1% 79.3% 14.8%
J48 7.6% 93.0% 14.0%
RF 7.4% 47.7% 12.7%
All metaphor 6.0% 100.0% 11.3%
Random baseline 6.0% 50.0% 10.7%
Table 4: The results of our experiments using several ma-
chine learning classifiers while evaluating on naturally
skewed dataset with 241 positive examples and 3,800
negative examples.
as more and more examples become available. In
future work, we hope to supplement our existing fea-
tures with such information as term imageability, the
transmission of affect, and selectional preference vi-
olation we believe will result in a robust system for
linguistic metaphor detection to further aid in the
computer understanding of natural language.
Acknowledgments
This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Defense US Army Research Labora-
tory contract number W911NF-12-C-0025. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Dis-
claimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoD/ARL, or the U.S. Govern-
ment. We would also like to thank our annotators
whose efforts have made this work possible.
References
K. Ahrens, S.F. Chung, and C. Huang. 2003. Concep-
tual metaphors: Ontology-based representation and
corpora driven mapping principles. In Proceedings
of the ACL 2003 workshop on Lexicon and figura-
tive language-Volume 14, pages 36?42. Association
for Computational Linguistics.
C. Biemann. 2006. Chinese whispers: an efficient graph
clustering algorithm and its application to natural lan-
34
guage processing problems. In Proceedings of the
First Workshop on Graph Based Methods for Natural
Language Processing, pages 73?80. Association for
Computational Linguistics.
D. Bogdanova. 2010. A framework for figurative lan-
guage detection based on sense differentiation. In Pro-
ceedings of the ACL 2010 Student Research Workshop,
pages 67?72. Association for Computational Linguis-
tics.
D. Bracewell, M. Tomlinson, and M. Mohler. 2013. De-
termining the conceptual space of metaphoric expres-
sions. In Computational Linguistics and Intelligent
Text Processing, pages 487?500. Springer.
C. Eilts and B. Lo?nneker. 2002. The Hamburg Metaphor
Database.
D. Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.
S. Harabagiu and F. Lacatusu. 2005. Topic themes for
multi-document summarization. In Proceedings of the
28th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 202?209. ACM.
G. Hirst and D. St-Onge. 1998. Lexical chains as rep-
resentations of context for the detection and correction
of malapropism. In Christiane Fellbaum, editor, Word-
Net: An Electronic Lexical Database, pages 305?332.
MIT Press.
A.N. Katz. 1992. Psychological studies in metaphor pro-
cessing: extensions to the placement of terms in se-
mantic space. Poetics Today, pages 607?632.
W. Kintsch. 2000. Metaphor comprehension: A com-
putational theory. Psychonomic Bulletin & Review,
7(2):257?266.
G. Lakoff and M. Johnson. 1980. Metaphors we live by,
volume 111. Chicago London.
G. Lakoff. 1993. The contemporary theory of metaphor.
Metaphor and thought, 2:202?251.
G. Lakoff. 1994. Master metaphor list. University of
California.
T.K. Landauer and S.T. Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review; Psychological Review,
104(2):211.
L. Li and C. Sporleder. 2010. Using gaussian mixture
models to detect figurative language in context. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 297?300.
Association for Computational Linguistics.
C. Lin and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics-Volume 1, pages 495?501. Association for
Computational Linguistics.
B. Lo?nneker. 2003. Is there a way to represent metaphors
in WordNets?: insights from the Hamburg Metaphor
Database. In Proceedings of the ACL 2003 workshop
on Lexicon and figurative language-Volume 14, pages
18?27. Association for Computational Linguistics.
J.H. Martin. 1990. A computational model of metaphor
interpretation. Academic Press Professional, Inc.
Z.J. Mason. 2004. CorMet: A computational, corpus-
based conventional metaphor extraction system. Com-
putational Linguistics, 30(1):23?44.
M.S. McGlone. 1996. Conceptual metaphors and figura-
tive language interpretation: Food for thought? Jour-
nal of memory and language, 35(4):544?565.
E. Niemann and I. Gurevych. 2011. The people?s web
meets linguistic knowledge: Automatic sense align-
ment of Wikipedia and WordNet. In Proceedings of
the 9th International Conference on Computational
Semantics (IWCS), pages 205?214. Citeseer.
W. Peters and Y. Wilks. 2003. Data-driven detection
of figurative language use in electronic language re-
sources. Metaphor and Symbol, 18(3):161?173.
E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor
identification using verb and noun clustering. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 1002?1010. Associ-
ation for Computational Linguistics.
E. Shutova. 2010. Models of metaphor in NLP. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 688?697. Asso-
ciation for Computational Linguistics.
E.V. Shutova. 2011. Computational approaches to fig-
urative language. Ph.D. thesis, University of Cam-
bridge.
S.L. Toral, M.R. Mart??nez-Torres, F. Barrero, and
F. Corte?s. 2009. An empirical study of the driving
forces behind online communities. Internet Research,
19(4):378?392.
R. Tourangeau and R.J. Sternberg. 1982. Understanding
and appreciating metaphors. Cognition, 11(3):203?
244.
A. Utsumi. 2011. Computational exploration of
metaphor comprehension processes using a semantic
space model. Cognitive science, 35(2):251?296.
Y. Wilks. 1978. Making preferences more active. Artifi-
cial Intelligence, 11(3):197?223.
P. Wolff and D. Gentner. 2000. Evidence for role-neutral
initial processing of metaphors. Journal of Experi-
mental Psychology: Learning, Memory, and Cogni-
tion, 26(2):529.
35
