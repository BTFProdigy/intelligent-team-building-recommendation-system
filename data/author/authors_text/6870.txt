Extraposition: A case study in German sentence realization 
Michael GAMON?, Eric RINGGER?, Zhu ZHANG?, 
Robert MOORE?, Simon CORSTON-OLIVER? 
 
?Microsoft Research 
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, bobmoore, 
simonco}@microsoft.com 
 
 
?University of Michigan 
Ann Arbor, MI 48109 
zhuzhang@umich.edu 
 
Abstract 
We profile the occurrence of clausal 
extraposition in corpora from different 
domains and demonstrate that extraposition 
is a pervasive phenomenon in German that 
must be addressed in German sentence 
realization. We present two different 
approaches to the modeling of extraposition, 
both based on machine learned decision tree 
classifiers. The two approaches differ in their 
view of the movement operation: one 
approach models multi-step movement 
through intermediate nodes to the ultimate 
target node, while the other approach models 
one-step movement to the target node. We 
compare the resulting models, trained on data 
from two domains and discuss the 
differences between the two types of models 
and between the results obtained in the 
different domains. 
Introduction 
Sentence realization, the last stage in natural 
language generation, derives a surface string 
from a more abstract representation. Numerous 
complex operations are necessary to produce 
fluent output, including syntactic aggregation, 
constituent ordering, word inflection, etc. We 
argue that for fluent output from German 
sentence realization, clausal extraposition needs 
to be included. We show how to accomplish this 
task by applying machine learning techniques. 
A comparison between English and German 
illustrates that it is possible in both languages to 
extrapose clausal material to the right periphery 
of a clause, as the following examples show: 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave 
the country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumor has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Unlike obligatory movement phenomena such as 
Wh-movement, extraposition is subject to 
pragmatic variability. A widely-cited factor 
influencing extraposition is clausal heaviness; in 
general, extraposition of heavy clauses is 
preferred over leaving them in place. Consider 
the following example from the technical 
domain: 
German: Es werden Datenstrukturen 
verwendet, die f?r die Benutzer nicht 
sichtbar sind. 
English: Data structures are used which 
are not visible to the user. 
This perfectly fluent sentence contains an 
extraposed relative clause. If the relative clause is 
left in place, as in the following example, the 
result is less fluent, though still grammatical: 
? Es werden Datenstrukturen, die f?r die 
Benutzer nicht sichtbar sind, verwendet. 
Data structures which are not visible to 
the users are used. 
Table 1 presents a quantitative analysis of the 
frequency of extraposition in different corpora in 
both English and German. This analysis is based 
on automatic data profiling using the NLPWin 
system (Heidorn 2000). The technical manual 
corpus consists of 100,000 aligned 
English-German sentence pairs from Microsoft 
technical manuals. The Encarta corpora consist 
of 100,000 randomly selected sentences from the 
Encarta encyclopedia in both English and 
German. The output of the parser was 
post-processed to identify relative clauses 
(RELCL), infinitival clauses (INFCL), and 
complement clauses (COMPCL) that have been 
moved from a position adjacent to the term they  
modify. According to this data profile, 
approximately one third of German relative 
clauses are extraposed in technical writing, while 
only 0.22% of English relative clauses are 
extraposed in the corresponding sentence set. The 
high number of extraposed relative clauses in 
German is corroborated by numbers from the 
German hand-annotated NEGRA corpus. In 
NEGRA, 26.75% of relative clauses are 
extraposed. Uszkoreit et al (1998) report 24% of 
relative clauses being extraposed in NEGRA, but 
their number is based on an earlier version of 
NEGRA, which is about half the size of the 
current NEGRA corpus. 
We also used the NEGRA corpus to verify the 
accuracy of our data profiling with NLPWin. 
These results are presented in Table 2. We only 
took into account sentences that received a 
complete parse in NLPWin. Of the 20,602 
sentences in NEGRA, 17,756 (86.19%) fell into 
that category. The results indicate that NLPWin 
is sufficiently reliable for the identification of 
relative clauses to make our conclusions 
noteworthy and to make learning from 
NLPWin-parsed data compelling. 
Extraposition is so rare in English that a sentence 
realization module may safely ignore it and still 
yield fluent output. The fluency of sentence 
realization for German, however, will suffer from 
the lack of a good extraposition mechanism.
 
 
German  
technical 
manuals 
English  
technical 
manuals 
German  
Encarta 
English  
Encarta 
RELCL 34.97% 0.22% 18.97% 0.30% 
INFCL 3.2% 0.53% 2.77% 0.33% 
COMPCL 1.50% 0.00% 2.54% 0.15% 
Table 1: Percentage of extraposed clauses in English and German corpora 
Relative clause 
identification overall 
Identification of 
extraposed relative clauses 
Identification of non-
extraposed relative clauses 
Recall Precision Recall Precision Recall Precision 
94.55 93.40 74.50 90.02 94.64 87.76 
Table 2: NLPWin recall and precision for relative clauses on the NEGRA corpus 
 
This evidence makes it clear that any serious 
sentence realization component for German 
needs to be able to produce extraposed relative 
clauses in order to achieve reasonable fluency. In 
the German sentence realization module, 
code-named Amalgam (Gamon et al 2002, 
Corston-Oliver et al 2002), we have successfully 
implemented both extraposition models as 
described here. 
1 Two strategies for modeling 
extraposition 
The linguistic and pragmatic factors involved in 
clause extraposition are inherently complex. We 
use machine learning techniques to leverage large 
amounts of data for discovering the relevant 
conditioning features for extraposition. As a 
machine learning technique for the problem at 
hand, we chose decision tree learning, a practical 
approach to inductive inference in widespread 
use. We employ decision tree learning to 
approximate discrete-valued functions from large 
feature sets that are robust to noisy data. Decision 
trees provide an easily accessible inventory of the 
selected features and some indication of their 
relative importance in predicting the target 
features in question. The particular tool we used 
to build our decision trees is the WinMine toolkit 
(Chickering et al, 1997, n.d.). Decision trees 
built by WinMine predict a probability 
distribution over all possible target values. 
We consider two different strategies for the 
machine-learned modeling of extraposition. The 
two strategies are a series of movements versus a 
single reattachment. 
1.1 Multi-step movement 
In the multi-step movement approach, the 
question to model for each potential attachment 
site of an extraposable clause is whether the 
clause should move up to its grandparent (a ?yes? 
answer) or remain attached to its current parent (a 
?no? answer). In other words, we have cast the 
problem as a staged classification task. At 
generation runtime, for a given extraposable 
clause, the movement question is posed, and if 
the DT classifier answers ?yes?, then the clause is 
reattached one level up, and the question is posed 
again. The final attachment site is reached when 
the answer to the classification task is ?no?, and 
hence further movement is barred. Figure 1 
illustrates the multi-step movement of a clause 
(lower triangle) through two steps to a new 
landing site (the reattached clause is the upper 
triangle). Note that in both Figure 1 and Figure 2 
linear order is ignored; only the hierarchical 
aspects of extraposition are represented. 
 
Figure 1: Multi-step movement 
1.2 One-step movement 
Modeling extraposition as a one-step movement 
involves a classification decision for each node in 
the parent chain of an extraposable clause. The 
classification task can be formulated as ?should 
the extraposable clause move up to this target 
from its base position??. Figure 2 shows the 
one-step movement approach to extraposition in 
the same structural configuration as in Figure 1. 
In this example, out of the three potential landing 
sites, only one qualifies. At generation runtime, if 
more than one node in the parent chain qualifies 
as a target for extraposition movement, the node 
with the highest probability of being a target is 
chosen. In the event of equally likely target nodes, 
the target node highest in the tree is chosen. 
 
Figure 2: One-step movement 
2 Data and features 
We employed two different sets of data to build 
the models for German: the 100,000 sentence 
technical manual corpus, and the 100,000 
sentence Encarta corpus. The data were split 
70/30 for training and parameter tuning purposes, 
respectively. We extracted features for each data 
point, using the syntactic and semantic analysis 
provided by the Microsoft NLPWin system (see 
Gamon et al 2002 for more details). We only 
considered sentences for feature extraction which 
received a complete spanning parse in NLPwin. 
85.14% of the sentences in the technical domain, 
and 88.37% of the sentences in the Encarta 
corpus qualified. The following features were 
extracted: 
? syntactic label of the node under 
consideration (i.e., the starting node for a 
single-step movement), its parent and 
grandparent, and the extraposable clause 
? semantic relation to the parent node of 
the node under consideration, the parent 
and the grandparent, and the 
extraposable clause 
? status of the head of the node under 
consideration as a separable prefix verb, 
the same for the parent and the 
grandparent 
? verb position information (verb-second 
versus verb-final) for the node under 
consideration, the parent and grandparent 
? all available analysis features and 
attributes in NLPWin (see Gamon et al 
2002 for a complete list of the currently 
used features and attributes) on the node 
under consideration, the parent and 
grandparent, and on the extraposable 
clause and its parent and grandparent 
? two features indicating whether the 
extraposable node has any verbal 
ancestor node with verb-final or 
verb-second properties 
? ?heaviness? of extraposable clause as 
measured in both number of words and 
number of characters 
? ?heaviness? of the whole sentence as 
measured in both number of words and 
number of characters 
A total of 1397 features were extracted for the 
multi-step movement model. For the single-step 
movement model, we extracted an additional 21 
features. Those features indicate for each of the 
21 labels for non-terminal nodes whether a node 
with that label intervenes between the parent of 
the extraposable clause and the putative landing 
site. 
Another linguistic feature commonly cited as 
influencing extraposition is the length and 
complexity of the part of the structure between 
the original position and the extraposed clause. 
Since in the Amalgam generation module 
extraposition is applied before word and 
constituent order is established, length of 
intervening strings is not accessible as a feature.  
For each training set, we built decision trees at 
varying levels of granularity (by manipulating the 
prior probability of tree structures to favor 
simpler structures) and selected the model with 
maximal accuracy on the corresponding 
parameter tuning data set. 
Since the syntactic label of the extraposable 
clause is one of the extracted features, we decided 
to build one general extraposition model, instead 
of building separate models for each of the three 
extraposable clause types (complement clause 
COMPCL, infinitival clause INFCL, and relative 
clause RELCL). If different conditions apply to 
the three types of extraposition, the decision tree 
model is expected to pick up on the syntactic 
label of the extraposable clause as a predictive 
feature. If, on the other hand, conditions for 
extraposition tend to be neutral with respect to the 
type of extraposable clause, the modeling of 
INFCL and COMPCL extraposition can greatly 
benefit from the much larger set of data points in 
relative clause extraposition. 
3 Comparison  
To compare the one-step and multi-step models, 
we processed a new blind test set of 10,000 
sentences from each domain, Microsoft technical 
manuals and Encarta, respectively. These 
sentences were extracted randomly from data in 
these domains that were neither included in the 
training nor in the parameter tuning set. For each 
extraposable clause, three different outputs were 
computed: the observed behavior, the prediction 
obtained by iteratively applying the multi-step 
model as described in Section 1.1, and the 
prediction obtained by applying the one-step 
model. The values for these outputs were either 
?no extraposition? or a specific target node. If 
either the general extraposition prediction or the 
predicted specific target node did not match the 
observed behavior, this was counted as an error. 
3.1 One-step versus multi-step in the 
technical domain 
Accuracy data on a blind set of 10,000 sentences 
from the technical manuals domain are presented 
in Table 3. 
 One-step Multi-step Baseline 
RELCL 81.56% 83.87% 60.93% 
INFCL 93.70% 92.02% 93.70% 
COMPCL 98.10% 98.57% 94.29% 
Overall 84.42% 86.12% 67.58% 
Table 3: Accuracy numbers for the two models in 
the technical domain 
The baseline score is the accuracy for a system 
that never extraposes. Both models outperform 
the overall baseline by a large margin; the 
multi-step movement model achieves an 
accuracy 1.7% higher than the one-step model. 
The baselines in INFCL and COMPCL 
extraposition are very high. In the test set there 
were only 15 cases of extraposed INFCLs and 12 
cases of extraposed COMPCLs, making it 
impossible to draw definite conclusions. 
3.2 One-step versus multi-step in the 
Encarta domain 
Results from a blind test set of 10,000 sentences 
from the Encarta domain are presented in Table 
4. 
 One-step Multi-step Baseline 
RELCL 87.59% 88.45% 80.48% 
INFCL 97.73% 97.48% 95.72% 
COMPCL 97.32% 97.32% 95.97% 
Overall 89.99% 90.61% 84.15% 
Table 4: Accuracy numbers for the two models in 
the Encarta domain 
As in the technical domain, the multi-step model 
outperforms the one-step model, and both 
outperform the baseline significantly. Again, 
extraposed COMPCLs and INFCLs are rare in 
the dataset (there were only 17 and 6 instances, 
respectively), making the results on these types of 
clauses inconclusive. 
3.3 Domain-specificity of the models 
Since we have data from two very different 
domains we considered the extent to which the 
domain-specific models overlapped. This is a 
linguistically interesting question: from a 
linguistic perspective one would expect both 
universal properties of extraposition as well as 
domain specific generalizations to emerge from 
such a comparison. 
3.3.1 Feature selection in the technical domain 
versus Encarta 
Of the 1397 features that were extracted for the 
multi-step model, the best model for the technical 
domain was created by the WinMine tools by 
selecting 60 features. In the Encarta domain, 49 
features were selected. 27 features are shared by 
the two models. This overlap in selected features 
indicates that the models indeed capture 
linguistic generalizations that are valid across 
domains. The shared features fall into the 
following categories (where node refers to the 
starting node for multi-step movement): 
? features relating to verbal properties of 
the node 
o a separable prefix verb as 
ancestor node 
o tense and mood of ancestor 
nodes 
o presence of a verb-final or 
verb-second VP ancestor 
o presence of Modals attribute 
(indicating the presence of a 
modal verb) on ancestors 
o verb-position in the current node 
and ancestors 
? ?heaviness?-related features on the 
extraposable clause and the whole 
sentence: 
o sentence length in characters 
o number of words in the 
extraposable clause 
? syntactic labels 
? the presence of a prepositional relation 
? the presence of semantic subjects and 
objects on the node and ancestors 
? definiteness features 
? the presence of modifiers on the parent 
? person and number features 
? some basic subcategorization features 
(e.g., transitive versus intransitive) 
Interestingly, the features that are not shared (33 
in the model for the technical domain and 27 in 
the model for the Encarta domain) fall roughly 
into the same categories as the features that are 
shared. To give some examples: 
? The Encarta model refers to the presence 
of a possessor on the parent node, the 
technical domain model does not. 
? The technical domain model selects more 
person and number features on ancestors 
of the node and ancestors of the 
extraposable clause than the Encarta 
model. 
For the one-step model, 1418 total features were 
extracted. Of these features, the number of 
features selected as being predictive is 49 both in 
the Encarta and in the technical domain. 
Twenty-eight of the selected features are shared 
by the models in the two domains. Again, this 
overlap indicates that the models do pick up on 
linguistically relevant generalizations. 
The shared features between the one-step models 
fall into the same categories as the shared features 
between the multi-step models. 
The results from these experiments suggest that 
the categories of selected features are 
domain-independent, while the choice of 
individual features from a particular category 
depends on the domain. 
3.3.2 Model complexity 
In order to assess the complexity of the models, 
we use the simple metric of number of branching 
nodes in the decision tree. The complexity of the 
models clearly differs across domains. Table 5 
illustrates that for both multi-step and one-step 
movement the model size is considerably smaller 
in the Encarta domain versus the technical 
domain. 
 One-step Multi-step 
Encarta 68 82 
Technical 87 116 
Table 5: Number of branching nodes in the 
decision trees 
We hypothesize that this difference in model 
complexity may be attributable to the fact that 
NLPWin assigns a higher percentage of spanning 
parses to the Encarta data, indicating that in 
general, the Encarta data may yield more reliable 
parsing output. 
3.3.3 Cross-domain accuracy 
The results in Table 3 and Table 4 above show 
that the models based on the Encarta domain 
achieve a much higher overall accuracy (89.99% 
and 90.61%) than the models based on the 
technical domain (84.42% and 86.12%), but they 
are also based on a much higher baseline of 
non-extraposed clauses (84.15% versus 67.58% 
in the technical domain). To quantify the domain 
specificity of the models, we applied the models 
across domains; i.e., we measured the 
performance of the Encarta models on the 
technical domain and vice versa. The results 
contrasted with the in-domain overall accuracy 
from Table 3 and Table 4 are given in Table 6. 
Encarta Model Technical Model  
1-step Multi 1-step Multi 
On 
Enc. 
89.99% 90.61% 84.42% 86.12% 
On 
Tech. 
79.39% 83.03% 88.54% 89.20% 
Table 6: Cross-domain accuracy of the models 
The results show that for both one-step and 
multi-step models, the models trained on a given 
domain will outperform the models trained on a 
different domain. These results are not surprising; 
they confirm domain-specificity of the 
phenomenon. Viewed from a linguistic 
perspective, this indicates that the generalizations 
governing clausal extraposition cannot be 
formulated independently of the text domain. 
Conclusion 
We have shown that it is possible to model 
extraposition in German using decision tree 
classifiers trained on automatic linguistic 
analyses of corpora. This method is particularly 
effective for extraposed relative clauses, which 
are pervasive in German text in domains as 
disparate as news, technical manuals, and 
encyclopedic text. Both one-step and multi-step 
models very clearly outperform the baseline in 
the two domains in which we experimented. This 
in itself is a significant result, given the 
complexity of the linguistic phenomenon of 
clausal extraposition. The machine learning 
approach to extraposition has two clear 
advantages: it eliminates the need for 
hand-coding of complex conditioning 
environments for extraposition, and it is 
adaptable to new domains. The latter point is 
supported by the cross-domain accuracy 
experiment and the conclusion that extraposition 
is governed by domain-specific regularities. 
We have shown that across domains, the 
multi-step model outperforms the one-step model. 
In the German sentence realization system 
code-named Amalgam (Corston-Oliver et al 
2002, Gamon et al 2002), we have experimented 
with implementations of both the one-step and 
multi-step extraposition models, and based on the 
results reported here we have chosen the 
multi-step model for inclusion in the end-to-end 
system. 
As we have shown, extraposed relative clauses 
outnumber other extraposed clause types by a 
large margin. Still, the combined model for 
clausal extraposition outperforms the baseline 
even for infinitival clauses and complement 
clauses, although the conclusions here are not 
very firm, given the small number of relevant 
data points in the test corpus. Since the syntactic 
label of the extraposed clause is one of the 
features extracted from the training data, 
however, the setup that we have used will adapt 
easily once more training data (especially for 
infinitival and complement clauses) become 
available. The models will automatically pick up 
distinctions between the generalizations covering 
relative clauses versus infinitival/complement 
clauses when they become relevant, by selecting 
the syntactic label feature as predictive. 
Finally, evaluation of the types of features that 
were selected by the extraposition models show 
that besides the ?heaviness? of the extraposed 
clause, a number of other factors from the 
structural context enter the determination of 
likelihood of extraposition. This, in itself, is an 
interesting result: it shows how qualitative 
inspection of a machine learned model can yield 
empirically based linguistic insights. 
Acknowledgements 
Our thanks go to Max Chickering for his 
assistance with the WinMine toolkit and to the 
anonymous reviewers for helpful comments. 
References  
Chickering D. M., Heckerman D. and Meek C. (1997). 
A Bayesian approach to learning Bayesian 
networks with local structure. In "Uncertainty in 
Artificial Intelligence: Proceedings of the 
Thirteenth Conference", D. Geiger and P. Punadlik 
Shenoy, ed., Morgan Kaufman, San Francisco, 
California,  pp. 80-89. 
Chickering, D. Max. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/To
oldoc.htm 
Corston-Oliver S., Gamon M., Ringger E. and Moore 
R. (2002). An overview of Amalgam: a 
machine-learned generation module. To appear in 
Proceedings of the Second International Natural 
Language Generation Conference 2002, New York. 
Gamon M., Ringger E., Corston-Oliver S.. (2002). 
Amalgam: A machine-learned generation module. 
Microsoft Technical Report MSR-TR-2002-57. 
Heidorn, G. E. (2000): Intelligent Writing Assistance. 
In "A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text", R. Dale, H. Moisl, and H. 
Somers (ed.), Marcel Dekker, New York, pp. 
181-207. 
Uszkoreit, H., Brants T., Duchier D., Krenn B., 
Konieczny L., Oepen S. and Skut W. (1998). 
Aspekte der Relativsatzextraposition im Deutschen. 
Claus-Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
Linguistically Informed Statistical Models of Constituent Structure for 
Ordering in Sentence Realization 
Eric RINGGER1, Michael GAMON1, Robert C. MOORE1, 
David ROJAS2, Martine SMETS1, Simon CORSTON-OLIVER1 
 
1Microsoft Research 
One Microsoft Way 
Redmond, Washington 98052, USA 
{ringger, mgamon, bobmoore, msmets, 
simonco}@microsoft.com 
2Butler Hill Group, LLC 
& Indiana University Linguistics Dept. 
1021 East 3rd Street, MM 322 
Bloomington, Indiana 47405, USA 
drojas@indiana.edu 
 
 
Abstract 
We present several statistical models of syntactic 
constituent order for sentence realization. We 
compare several models, including simple joint 
models inspired by existing statistical parsing 
models, and several novel conditional models. The 
conditional models leverage a large set of linguistic 
features without manual feature selection. We apply 
and evaluate the models in sentence realization for 
French and German and find that a particular 
conditional model outperforms all others. We 
employ a version of that model in an evaluation on 
unordered trees from the Penn TreeBank. We offer 
this result on standard data as a reference-point for 
evaluations of ordering in sentence realization. 
1 Introduction 
Word and constituent order play a crucial role in 
establishing the fluency and intelligibility of a 
sentence. In some systems, establishing order 
during the sentence realization stage of natural 
language generation has been accomplished by 
hand-crafted generation grammars in the past (see 
for example, Aikawa et al (2001) and Reiter and 
Dale (2000)). In contrast, the Nitrogen (Langkilde 
and Knight, 1998a, 1998b) system employs a word 
n-gram language model to choose among a large 
set of word sequence candidates which vary in 
constituent order, word order, lexical choice, and 
morphological inflection. Nitrogen?s model does 
not take into consideration any non-surface 
linguistic features available during realization.  
The Fergus system (Bangalore and Rambow, 
2000) employs a statistical tree model to select 
probable trees and a word n-gram model to rank 
the string candidates generated from the best trees. 
Like Nitrogen, the HALogen system (Langkilde, 
2000; Langkilde-Geary, 2002a, 2002b) uses word 
n-grams, but it extracts the best-scoring surface 
realizations efficiently from a packed forest by 
constraining the search first within the scope of 
each constituent.  
Our research is carried out within the Amalgam 
broad coverage sentence realization system. 
Amalgam generates sentence strings from abstract 
predicate-argument structures (Figure 1), using a 
pipeline of stages, many of which employ 
machine-learned models to predict where to 
perform specific linguistic operations based on the 
linguistic context (Corston-Oliver et al, 2002; 
Gamon et al, 2002a, 2002b; Smets et al, 2003). 
Amalgam has an explicit ordering stage that 
determines the order of constituents and their 
daughters. The input for this stage is an unordered 
tree of constituents; the output is an ordered tree of 
constituents or a ranked list of such trees. For 
ordering, Amalgam leverages tree constituent 
structure and, importantly, features of those 
constituents and the surrounding context. By 
separately establishing order within constituents, 
Amalgam heavily constrains the possible 
alternatives in later stages of the realization 
process.  The design allows for interaction between 
ordering choices and other realization decisions, 
such as lexical choice (not considered in the 
present work), through score combinations from 
distinct Amalgam pipeline stages. 
Most previous research into the problem of 
establishing order for sentence realization has 
focused on English, a language with fairly strict 
word and constituent order. In the experiments 
described here we first focus on German and 
French which present novel challenges.1 We also 
describe an English experiment involving data 
from the Penn Treebank. Our ultimate goal is to 
develop a model that handles all ordering 
phenomena in a unified and elegant way across 
typologically diverse languages. In the present 
paper, we explore the space of possible models and 
examine some of these closely. 
                                                          
1
 For an overview of some of the issues in 
determining word and constituent order in German and 
French, see (Ringger et al, 2003).  
 Figure 1: Abstract predicate-argument structure (NLPWin logical form) for the German sentence: 
In der folgenden Tabelle werden die Optionen sowie deren Funktionen aufgelistet. 
(The options and their functions are listed in the following table.) 
2 Models of Constituent Order 
In order to develop a model of constituent 
structure that captures important order phenomena, 
we will consider the space of possible joint and 
conditional models in increasing complexity. For 
each of the models, we will survey the 
independence assumptions and the feature set used 
in the models. 
Our models differ from the previous statistical 
approaches in the range of input features. Like the 
knowledge-engineered approaches, the models 
presented here incorporate lexical features, parts-
of-speech, constituent-types, constituent 
boundaries, long-distance dependencies, and 
semantic relations between heads and their 
modifiers. 
Our experiments do not cover the entire space of 
possible models, but we have chosen significant 
points in the space for evaluation and comparison. 
2.1 Joint Models 
We begin by considering joint models of 
constituent structure of the form ( ),P ? ?  over 
ordered syntax trees ?  and unordered syntax trees 
? . An ordered tree ?  contains non-terminal 
constituents C, each of which is the parent of an 
ordered sequence of daughters ( 1,..., nD D ), one of 
which is the head constituent H.2 Given an ordered 
tree ? , the value of the function 
_ ( )unordered tree ?  is an unordered tree ?  
corresponding to ?  that contains a constituent B 
for each C in ? , such that 
( ) ( )
1
_ ( )
{ ,..., }n
unordered set daughters Cdaughters B
D D
=
=
 
again with iH D=  for some i in ( )1..n . The 
hierarchical structure of ?  is identical to that of 
? . 
We employ joint models for scoring alternative 
ordered trees as follows: given an unordered 
syntax tree ? , we want the ordered syntax tree ??  
that maximizes the joint probability: 
                                                          
2 All capital Latin letters denote constituents, and 
corresponding lower-case Latin letters denote their 
labels (syntactic categories). 
( ) ( )
: _ ( )
? arg max , arg max
unordered tree
P P
? ? ? ?
? ? ? ?
=
= =    (1) 
As equation (1) indicates, we can limit our search 
to those trees ?  which are alternative orderings of 
the given tree ? . 
Inter-dependencies among ordering decisions 
within different constituents (e.g., for achieving 
parallelism) make the global sentence ordering 
problem challenging and are certainly worth 
investigating in future work.  For the present, we 
constrain the possible model types considered here 
by assuming that the ordering of any constituent is 
independent of the ordering within other 
constituents in the tree, including its daughters; 
consequently, 
( ) ( )
( )C constits
P P C
?
?
?
= ?  
Given this independence assumption, the only 
possible ordered trees are trees built with non-
terminal constituents computed as follows: for 
each ( )B constits ?? , 
( )
: _ ( )
* arg max
C B unordered set C
C P C
=
=  
In fact, we can further constrain our search for the 
best ordering of each unordered constituent B, 
since C?s head must match B?s head: 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C
=
=
=  
Thus, we have reduced the problem to finding the 
best ordering of each constituent of the unordered 
tree. 
Now if we wish to condition on some feature ( )x f ?= , then we must first predict it as follows: 
( ) ( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P x P C x
=
=
=  
If x is truly a feature of ?  and does not depend on 
any particular ordering of any constituent in ? , 
then ( )P x  is constant, and we do not need to 
compute it in practice. In other words, 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C x
=
=
=       (2) 
Hence, even for a joint model ( )P C , we can 
condition on features that are fixed in the given 
unordered tree ?  without first predicting them. 
The joint models described here are of this form. 
For this reason, when we describe a distribution ( )P C x , unless we explicitly state otherwise, we 
are actually describing the part of the joint model 
that is of interest. As justified above, we do not 
need to compute ( )P x  and will simply present 
alternative forms of ( )P C x . 
We can factor a distribution ( )P C x  in many 
different ways using the chain rule. As our starting 
point we adopt the class of models called Markov 
grammars.3 We first consider a left-to-right 
Markov grammar of order j that expands C by 
predicting its daughters 1,..., nD D  from left-to-
right, one at a time, as shown in Figure 2: in the 
figure. iD  depends only on ( i jD ? , ?, 1iD ? ), and 
the parent category C ., according to the 
distribution in equation (3). 
 
i?
Figure 2: Left-to-right Markov grammar. 
( ) ( )1
1
,..., , ,
n
i i i j
i
P C h P d d d c h
? ?
=
= ?  (3) 
In order to condition on another feature of each 
ordered daughter iD , such as its semantic relation 
i? to the head constituent H, we also first predict 
it, according to the chain rule. The result is the 
semantic Markov grammar in equation (4):  
( ) ( )( )
1 1
1 1 1
, ,..., , , ,
, , ,..., , , ,
n i i i i j i j
i i i i i i j i j
P d d c h
P C h
P d d d c h
? ? ?
? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (4) 
Thus, the model predicts semantic relation i? and 
then the label id  in the context of that semantic 
relation. We will refer to this model as Type 1 
(T1). 
As an extension to model Type 1, we include 
features computed by the following functions on 
the set i?  of daughters of C already ordered (see 
Figure 2): 
? Number of daughters already ordered (size 
of i? ) 
? Number of daughters in i?  having a 
particular label for each of the possible 
constituent labels {NP, AUXP, VP, etc.} 
(24 for German, 23 for French) 
We denote that set of features in shorthand as ( )if ? . With this extension, a model of Markov 
                                                          
3
 A ?Markov grammar? is a model of constituent 
structure that starts at the root of the tree and assigns 
probability to the expansion of a non-terminal one 
daughter at a time, rather than as entire productions 
(Charniak, 1997 & 2000). 
order j can potentially have an actual Markov order 
greater than j. Equation (5) is the extended model, 
which we will refer to as Type 2 (T2): 
( ) ( )( )( )( )
1 1
1 1 1
, ,..., , , , ,
, , ,..., , , , ,
n i i i i j i j i
i i i i i i j i j i
P d d c h f
P C h
P d d d c h f
? ? ? ?
? ? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (5) 
As an alternative to a left-to-right expansion, we 
can also expand a constituent in a head-driven 
fashion. We refer the reader to (Ringger et al, 
2003) for details and evaluations of several head-
driven models (the missing ?T3?, ?T4?, and ?T6? 
in this discussion). 
2.2 Conditional Models 
We now consider more complex models that use 
additional features. We define a function ( )g X on 
constituents, where the value of ( )g X represents a 
set of many lexical, syntactic, and semantic 
features of X (see section 5.2 for more details). No 
discourse features are included for the present 
work. We condition on 
? ( )g B , where B is the unordered constituent 
being ordered 
? ( )g H , where H is the head of B 
? ( )Bg P , where BP  is the parent of B, and 
? ( )Bg G , where BG  is the grandparent of B. 
These features are fixed in the given unordered tree 
? , as in the discussion of equation (2), hence the 
resulting complex model is still a joint model.   
Up until this point, we have been describing joint 
generative models that describe how to generate an 
ordered tree from an unordered tree. These models 
require extra effort and capacity to accurately 
model the inter-relations among all features. Now 
we move on to truly conditional models by 
including features that are functions on the set i?  
of daughters of C yet to be ordered. In the 
conditional models we do not need to model the 
interdependencies among all features. We include 
the following: 
? Number of daughters remaining to be 
ordered (size of i? ) 
? Number of daughters in i?  having a 
particular label 
As before, we denote these feature sets in 
shorthand as ( )if ? . The resulting distribution is 
represented in equation (6), which we will refer to 
as Type 5 (T5): 
( )
( )
( )
1 1
1 1 1
( ), ( ), ( ), ( )
, ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
, , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
B B
i i i j i j
i
n B B i i
i i i i i j i j
i
B B i i
P C g H g B g P g G
d d c h
P
g H g B g P g G f f
d d c h
P d
g H g B g P g G f f
? ?
?
? ?
? ? ?
? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
=
? ?
? ?
? ?
?
? ?
? ?
? ?
? ?
? ?
? ?
?
   (6) 
All models in this paper are nominally Markov 
order 2, although those models incorporating the 
additional feature functions ( )if ?  and ( )if ?  
defined in Section 2.2 can be said to have higher 
order. 
2.3 Binary conditional model 
We introduce one more model type called the 
binary conditional model. It estimates a much 
simpler distribution over the binary variable ?  
called ?sort-next? with values in {yes, no} 
representing the event that an as-yet unordered 
member D of i?  (the set of as-yet unordered 
daughters of parent C, as defined above) should be 
?sorted? next, as illustrated in Figure 3. 
i?i?
?
 
Figure 3: Binary conditional model. 
The conditioning features are almost identical to 
those used in the left-to-right conditional models 
represented in equation (6) above, except that id  
and i?  (the semantic relation of D with head H) 
appear in the conditional context and need not first 
be predicted. In its simple form, the model 
estimates the following distribution: 
( )
1 1, , , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
i i i i i j i j
i
B B i i
d d d c h
P
g H g B g P g G f f
? ? ?
?
? ?
? ? ? ?
? ?
? ?
? ?
? ?
   (7) 
In our shorthand, we will call this Type 7 (T7). We 
describe how to apply this model directly in a left-
to-right ?sorting? search later in the section on 
search. 
3 Estimation 
We estimate a model?s distributions with 
probabilistic decision trees (DTs).4 We build 
decision trees using the WinMine toolkit 
(Chickering, 2002). WinMine-learned decision 
trees are not just classifiers; each leaf is a 
conditional probability distribution over the target 
random variable, given all features available in 
training; hence the tree as a whole is an estimate of 
the conditional distribution of interest. The primary 
advantage of using decision trees, is the automatic 
feature selection and induction from a large pool of 
features. 
We train four models for German and French 
each. One model is joint (T1); one is joint with 
additional features on the set of daughters already 
ordered (T2); one is conditional (T5). In addition, 
we employ one binary conditional DT model (T7), 
both with and without normalization (see equation 
(8)). 
                                                          
4
 Other approaches to feature selection, feature 
induction, and distribution estimation are certainly 
possible, but they are beyond the scope of this paper. 
One experiment using interpolated language modeling 
techniques is described in (Ringger et al, 2003) 
4 Search 
4.1 Exhaustive search 
Given an unordered tree ?  and a model of 
constituent structure O of any of the types already 
presented, we search for the best ordered tree ?  
that maximizes ( )OP ?  or ( )OP ? ? , as 
appropriate, with the context varying according to 
the complexity of the model. Each of our models 
(except the binary conditional model) estimates the 
probability of an ordering of any given constituent 
C in ? , independently of the ordering inside other 
constituents in ? . The complete search is a 
dynamic programming (DP) algorithm, either left-
to-right in the daughters of C (or head-driven, 
depending on the model type). The search can 
optionally maintain one non-statistical constraint 
we call Input-Output Coordination Consistency 
(IOCC), so that the order of coordinated 
constituents is preserved as they were specified in 
the given unordered tree. For these experiments, 
we employ the constraint. 
4.2 Greedy search for binary conditional 
model 
The binary conditional model can be applied in a 
left-to-right ?sorting? mode (Figure 3). At stage i, 
for each unordered daughter jD , in i? , the model 
is consulted for the probability of j yes? = , 
namely the probability that jD  should be placed to 
the right of the already ordered sister constituents 
i? . The daughter in i?  with the highest 
probability is removed from i?  to produce 1i? +  
and added to the right of i? to produce 1i? + . The 
search proceeds through the remaining unordered 
constituents until all constituents have been 
ordered in this greedy fashion. 
4.3 Exhaustive search for binary conditional 
model 
In order to apply the binary conditional model in 
the exhaustive DP search, we normalize the model 
at every stage of the search and thereby coerce it 
into a probability distribution over the remaining 
daughters in i? . We represent the distribution in 
?equation? (7) in short-hand as ( ), , iP d? ? ? , 
with i?  representing the contextual features for the 
given search hypothesis at search stage i. Thus, our 
normalized distribution for stage i is given by 
equation (8). Free variable j represents an index on 
unordered daughters in i? , as does k. 
( ) ( )
( )
1
, ,
, ,
, ,
i
j j j i
j j j i
k k k i
k
P yes d
P D d
P yes d
?
? ?
?
? ?
=
= ?
? =
= ?
?
 (8) 
This turns out to be the decision tree analogue of a 
Maximum Entropy Markov Model (MEMM) 
(McCallum et al, 2000), which we can refer to as a 
DTMM. 
5 Experiments 
5.1 Training 
We use a training set of 20,000 sentences, both 
for French and German. The data come from 
technical manuals in the computer domain. For a 
given sentence in our training set, we begin by 
analyzing the sentence as a surface syntax tree and 
an abstract predicate argument structure using the 
NLPWin system (Heidorn, 2000). By consulting 
these two linked structures, we produce a tree with 
all of the characteristics of trees seen by the 
Amalgam ordering stage at generation run-time 
with one exception: these training trees are 
properly ordered. The training trees include all 
features of interest, including the semantic 
relations among a syntactic head and its modifiers. 
We train our order models from the constituents of 
these trees. NLPWin parser output naturally 
contains errors; hence, the Amalgam training data 
is imperfect. 
5.2 Selected Features 
A wide range of linguistic features is extracted 
for the different decision tree models. The number 
of selected features for German reaches 280 (out of 
651 possible features) in the binary conditional 
model T7. For the French binary conditional 
model, the number of selected features is 218 (out 
of 550). The binary conditional models draw from 
the full set of available features, including: 
? lexical sub-categorization features such as 
transitivity and compatibility with clausal 
complements 
? lemmas (word-stems) 
? semantic features such as the semantic 
relation and the presence of 
quantificational operators 
? length of constituent in words 
? syntactic information such as the label and 
the presence of syntactic modifiers 
5.3 Evaluation 
To evaluate the constituent order models in 
isolation, we designed our experiments to be 
independent of the rest of the Amalgam sentence 
realization process. We use test sets of 1,000 
sentences, also from technical manuals, for each 
language. To isolate ordering, for a given test 
sentence, we process the sentence as in training to 
produce an ordered tree ?  (the reference for 
evaluation) and from it an unordered tree ? . 
Given ? , we then search for the best ordered tree 
hypothesis ??  using the model in question. 
We then compare ?  and ?? . Because we are 
only ordering constituents, we can compare ? and 
??  by comparing their respective constituents. For 
each C in ? , we measure the per-constituent edit 
distance D, between C and its counterpart C? in ??  
as  follows: 
1. Let d be the edit distance between the 
ordered set of daughters in each, with the 
only possible edit operators being insert and 
delete 
2. Let the number of moves / 2m d= , since 
insertions and deletions can be paired 
uniquely 
3. Divide by the total number of 
daughters: ( )/D m daughters C=  
This metric is like the ?Generation Tree Accuracy? 
metric of Bangalore & Rambow (2000), except 
that there is no need to consider cross-constituent 
moves. The total score for the hypothesis tree ??  is 
the mean of the per-constituent edit distances. 
For each of the models under consideration and 
each language, we report in Table 1 the average 
score across the test set for the given language. The 
first row is a baseline computed from randomly 
scrambling constituents (mean over four 
iterations). 
Model German French 
Baseline (random) 35.14 % 34.36 % 
T1: DT joint 5.948% 3.828% 
T2: DT joint 
with ( )if ?   5.852% 4.008% 
T5: DT conditional 6.053% 4.271% 
T7: DT binary cond., 
greedy search 3.516% 1.986% 
T7: DT normalized 
binary conditional, 
exhaustive search 
3.400% 1.810% 
Table 1: Mean per-constituent edit distances for 
German & French. 
5.4 Discussion 
For both German and French, the binary 
conditional DT model outperforms all other 
models. Normalizing the binary conditional model 
and applying it in an exhaustive search performs 
better than a greedy search. All score differences 
are statistically significant; moreover, manual 
inspection of the differences for the various models 
also substantiates the better quality of those models 
with lower scores. 
With regard to the question of conditional versus 
joint models, the joint models (T1, T2) outperform 
their conditional counterparts (T5). This may be 
due to a lack of sufficient training data for the 
conditional models. At this time, the training time 
of the conditional models is the limiting factor. 
There is a clear disparity between the 
performance of the German models and the 
performance of the French models. The best 
German model is twice as bad as the best French 
model.  (For a discussion of the impact of 
modeling German verb position, please consult 
(Ringger et al, 2003).) 
 Baseline 
(random) 
Greedy, 
IOCC Greedy 
DP,  
IOCC DP 
Total Sentences 2416 2416 2416 2416 2416 
Mean Tokens/Sentence 23.59 23.59 23.59 23.59 23.59 
Time/Input (sec.) n/a 0.01 0.01 0.39 0.43 
Exact Match 0.424% 33.14% 27.53% 33.53% 35.72% 
Coverage 100% 100% 100% 100% 100% 
Mean Per-Const. Edit Dist. 38.3% 6.02% 6.84% 5.25% 4.98% 
Mean NIST SSA -16.75 74.98 67.19 74.65 73.24 
Mean IBM Bleu Score 0.136 0.828 0.785 0.817 0.836 
Table 2: DSIF-Amalgam ordering performance on WSJ section 23. 
6 Evaluation on the Penn TreeBank 
Our goal in evaluating on Penn Tree Bank (PTB) 
data is two-fold: (1) to enable a comparison of 
Amalgam?s performance with other systems 
operating on similar input, and (2) to measure 
Amalgam?s capabilities on less domain-specific 
data than technical software manuals. We derive 
from the bracketed tree structures in the PTB using 
a deterministic procedure an abstract 
representation we refer to as a Dependency 
Structure Input Format (DSIF), which is only 
loosely related to NLPWin?s abstract predicate-
argument structures. 
The PTB to DSIF transformation pipeline 
includes the following stages, inspired by 
Langkilde-Geary?s (2002b) description: 
A. Deserialize the tree 
B. Label heads, according to Charniak?s head 
labeling rules (Charniak, 2000) 
C. Remove empty nodes and flatten any 
remaining empty non-terminals 
D. Relabel heads to conform more closely to the 
head conventions of NLPWin 
E. Label with logical roles, inferred from PTB 
functional roles 
F. Flatten to maximal projections of heads 
(MPH), except in the case of conjunctions 
G. Flatten non-branching non-terminals 
H. Perform dictionary look-up and 
morphological analysis 
I. Introduce structure for material between 
paired delimiters and for any coordination 
not already represented in the PTB 
J. Remove punctuation 
K. Remove function words 
L. Map the head of each maximal projection to 
a dependency node, and map the head?s 
modifiers to the first node?s dependents, 
thereby forming a complete dependency tree. 
To evaluate ordering performance alone, our data 
are obtained by performing all of the steps above 
except for (J) and (K). We employ only a binary 
conditional ordering model, found in the previous 
section to be the best of the models considered. To 
train the order models, we use a set of 10,000 
sentences drawn from the standard PTB training 
set, namely sections 02?21 from the Wall Street 
Journal portion of the PTB (the full set contains 
approx. 40,000 sentences). For development and 
parameter tuning we used a separate set of 2000 
sentences drawn from sections 02?21. 
Decision trees are trained for each of five 
constituent types characterized by their head 
labels: adjectival, nominal, verbal, conjunctions 
(coordinated material), and other constituents not 
already covered. The split DTs can be thought of 
as a single DT with a five-way split at the top 
node. 
Our DSIF test set consists of the blind test set 
(section 23) of the WSJ portion of the PTB. At 
run-time, for each converted tree in the test set, all 
daughters of a given constituent are first permuted 
randomly with one another (scrambled), with the 
option for coordinated constituents to remain 
unscrambled, according to the Input-Output 
Coordination Consistency (IOCC) option. For a 
given unordered (scrambled) constituent, the 
appropriate order model (noun-head, verb-head, 
etc.) is used in the ordering search to order the 
daughters. Note that for the greedy search, the 
input order can influence the final result; therefore, 
we repeat this process for multiple random 
scramblings and average the results. 
We use the evaluation metrics employed in 
published evaluations of HALogen, FUF/SURGE, 
and FERGUS (e.g., Calloway, 2003), although our 
results are for ordering only. Coverage, or the 
percentage of inputs for which a system can 
produce a corresponding output, is uninformative 
for the Amalgam system, since in all cases, it can 
generate an output for any given DSIF. In addition 
to processing time per input, we apply four other 
metrics: exact match, NIST simple string accuracy 
(the complement of the familiar word error rate), 
the IBM Bleu score (Papineni et al, 2001), and the 
intra-constituent edit distance metric introduced 
earlier. 
We evaluate against ideal trees, directly 
computed from PTB bracketed tree structures. The 
results in Table 2 show the effects of varying the 
IOCC parameter. For both trials involving a greedy 
search, the results were averaged across 25 
iterations. As should be expected, turning on the 
input-output faithfulness option (IOCC) improves 
the performance of the greedy search. Keeping 
coordinated material in the same relative order 
would only be called for in applications that plan 
discourse structure before or during generation. 
7 Conclusions and Future Work 
The experiments presented here provide 
conclusive reasons to favor the binary conditional 
model as a model of constituent order. The 
inclusion of linguistic features is of great value to 
the modeling of order, specifically in verbal 
constituents for both French and German. 
Unfortunately space did not permit a thorough 
discussion of the linguistic features used. Judging 
from the high number of features that were 
selected during training for participation in the 
conditional and binary conditional models, the 
availability of automatic feature selection is 
critical. 
Our conditional and binary conditional models 
are currently lexicalized only for function words; 
the joint models not at all. Experiments by Daum? 
et al(2002) and the parsing work of Charniak 
(2000) and others indicate that further 
lexicalization may yield some additional 
improvements for ordering. However, the parsing 
results of Klein & Manning (2003) involving 
unlexicalized grammars suggest that gains may be 
limited. 
For comparison, we encourage implementers of 
other sentence realization systems to conduct 
order-only evaluations using PTB data. 
Acknowledgements 
We wish to thank Irene Langkilde-Geary and 
members of the MSR NLP group for helpful 
discussions.  Thanks also go to the anonymous 
reviewers for helpful feedback. 
References  
Aikawa T., Melero M., Schwartz L. Wu A. 2001. 
Multilingual sentence generation. In Proc. of 8th 
European Workshop on NLG. pp. 57-63. 
Bangalore S. Rambow O. 2000. Exploiting a 
probabilistic hierarchical model for generation. 
In Proc. of COLING. pp. 42-48. 
Calloway, C. 2003. Evaluating Coverage for Large 
Symbolic NLG Grammars.  In Proc. of IJCAI 
2003. pp 811-817. 
Charniak E. 1997. Statistical Techniques for 
Natural Language Parsing, In AI Magazine. 
Charniak E. 2000. A Maximum-Entropy-Inspired 
Parser. In Proc. of ACL. pp.132-139. 
Chickering D. M. 2002. The WinMine Toolkit. 
Microsoft Technical Report 2002-103. 
Corston-Oliver S., Gamon M., Ringger E., Moore 
R. 2002. An overview of Amalgam: a machine-
learned generation module. In Proc. of INLG. 
pp.33-40. 
Daum? III H., Knight K., Langkilde-Geary I., 
Marcu D., Yamada K. 2002. The Importance of 
Lexicalized Syntax Models for Natural 
Language Generation Tasks. In Proc. of INLG. 
pp. 9-16. 
Gamon M., Ringger E., Corston-Oliver S. 2002a. 
Amalgam: A machine-learned generation 
module. Microsoft Technical Report 2002-57. 
Gamon M., Ringger E., Corston-Oliver S., Moore 
R. 2002b. Machine-learned contexts for 
linguistic operations in German sentence 
realization. In Proc. of ACL. pp. 25-32. 
Heidorn G. 2000. Intelligent Writing Assistance. In 
A Handbook of Natural Language Processing,, 
R. Dale, H. Moisl, H. Somers (eds.). Marcel 
Dekker, NY. 
Klein D., Manning C. 2003. "Accurate 
Unlexicalized Parsing." In Proceedings of ACL-
03. 
Langkilde I. 2000. Forest-Based Statistical 
Sentence generation. In Proc. of NAACL. pp. 
170-177. 
Langkilde-Geary I. 2002a. An Empirical 
Verification of Coverage and Correctness for a 
General-Purpose Sentence Generator. In Proc. of 
INLG. pp.17-24. 
Langkilde-Geary, I. 2002b. A Foundation for 
General-purpose Natural Language Generation: 
Sentence Realization Using Probabilistic Models 
of Language. PhD Thesis, University of 
Southern California. 
Langkilde I., Knight K. 1998a. The practical value 
of n-grams in generation. In Proc. of 9th 
International Workshop on NLG. pp. 248-255. 
Langkilde I., Knight K. 1998b. Generation that 
exploits corpus-based statistical knowledge. In 
Proc. of ACL and COLING. pp. 704-710. 
McCallum A., Freitag D., & Pereira F. 2000. 
?Maximum Entropy Markov Models for 
Information Extraction and Segmentation.? In 
Proc. Of ICML-2000. 
Papineni, K.A., Roukos, S., Ward, T., and Zhu, 
W.J. 2001. Bleu: a method for automatic 
evaluation of machine translation. IBM 
Technical Report RC22176 (W0109-022). 
Reiter E. and Dale R. 2000. Building natural 
language generation systems. Cambridge 
University Press, Cambridge. 
Ringger E., Gamon M., Smets M., Corston-Oliver 
S. and Moore R. 2003 Linguistically informed 
models of constituent structure for ordering in 
sentence realization. Microsoft Research 
technical report MSR-TR-2003-54. 
Smets M., Gamon M., Corston-Oliver S. and 
Ringger E. (2003) The adaptation of a machine-
learned sentence realization system to French. 
In Proceedings of EACL. 
323
324
325
326
327
328
329
330
Machine-learned contexts for linguistic operations 
in German sentence realization 
 
Michael GAMON, Eric RINGGER, Simon CORSTON-OLIVER, Robert MOORE 
Microsoft Research  
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, simonco, bobmoore}@microsoft.com 
 
Abstract 
We show that it is possible to learn the 
contexts for linguistic operations which 
map a semantic representation to a 
surface syntactic tree in sentence 
realization with high accuracy. We cast 
the problem of learning the contexts for 
the linguistic operations as 
classification tasks, and apply 
straightforward machine learning 
techniques, such as decision tree 
learning. The training data consist of 
linguistic features extracted from 
syntactic and semantic representations 
produced by a linguistic analysis 
system. The target features are extracted 
from links to surface syntax trees. Our 
evidence consists of four examples from 
the German sentence realization system 
code-named Amalgam: case 
assignment, assignment of verb position 
features, extraposition, and syntactic 
aggregation 
1 Introduction 
The last stage of natural language generation, 
sentence realization, creates the surface string 
from an abstract (typically semantic) 
representation. This mapping from abstract 
representation to surface string can be direct, or it 
can employ intermediate syntactic representations 
which significantly constrain the output. 
Furthermore, the mapping can be performed 
purely by rules, by application of statistical 
models, or by a combination of both techniques. 
Among the systems that use statistical or 
machine learned techniques in sentence 
realization, there are various degrees of 
intermediate syntactic structure. Nitrogen 
(Langkilde and Knight, 1998a, 1998b) produces a 
large set of alternative surface realizations of an 
input structure (which can vary in abstractness). 
This set of candidate surface strings, represented 
as a word lattice, is then rescored by a word-
bigram language model, to produce the best-
ranked output sentence. FERGUS (Bangalore and 
Rambow, 2000), on the other hand, employs a 
model of syntactic structure during sentence 
realization. In simple terms, it adds a tree-based 
stochastic model to the approach taken by the 
Nitrogen system. This tree-based model chooses a 
best-ranked XTAG representation for a given 
dependency structure. Possible linearizations of 
the XTAG representation are generated and then 
evaluated by a language model to pick the best 
possible linearization, as in Nitrogen. 
In contrast, the sentence realization system 
code-named Amalgam (A Machine Learned 
Generation Module) (Corston-Oliver et al, 2002; 
Gamon et al, 2002b) employs a series of 
linguistic operations which map a semantic 
representation to a surface syntactic tree via 
intermediate syntactic representations. The 
contexts for most of these operations in Amalgam 
are machine learned. The resulting syntactic tree 
contains all the necessary information on its leaf 
nodes from which a surface string can be read.  
The goal of this paper is to show that it is 
possible to learn accurately the contexts for 
linguistically complex operations in sentence 
realization. We propose that learning the contexts 
for the application of these linguistic operations 
can be viewed as per-operation classification 
problems. This approach combines advantages of 
a linguistically informed approach to sentence 
realization with the advantages of a machine 
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 25-32.
                         Proceedings of the 40th Annual Meeting of the Association for
learning approach. The linguistically informed 
approach allows us to deal with complex linguistic 
phenomena, while machine learning automates the 
discovery of contexts that are linguistically 
relevant and relevant for the domain of the data. 
The machine learning approach also facilitates 
adaptation of the system to a new domain or 
language. Furthermore, the quantitative nature of 
the machine learned models permits finer 
distinctions and ranking among possible solutions. 
To substantiate our claim, we provide four 
examples from Amalgam: assignment of case, 
assignment of verb position features, 
extraposition, and syntactic aggregation. 
2 Overview of Amalgam 
Amalgam takes as its input a sentence-level 
semantic graph representation with fixed lexical 
choices for content words (the logical form graph 
of the NLPWin system ? see (Heidorn, 2000)). 
This representation is first degraphed into a tree, 
and then gradually augmented by the insertion of 
function words, assignment of case and verb 
position features, syntactic labels, etc., and 
transformed into a syntactic surface tree. A 
generative statistical language model establishes 
linear order in the surface tree (Ringger et al, in 
preparation), and a surface string is generated 
from the leaf nodes. Amalgam consists of eight 
stages. We label these ML (machine-learned 
context) or RB (rule-based). 
Stage 1 Pre-processing (RB): 
? degraphing of the semantic representation 
? retrieval of lexical information 
Stage 2 Flesh-out (ML): 
? assignment of syntactic labels 
? insertion of function words 
? assignment of case and verb position 
features 
Stage 3 Conversion to syntactic tree (RB): 
? introduction of syntactic representation 
for coordination 
? splitting of separable prefix verbs based 
on both lexical information and 
previously assigned verb position features 
? reversal of heads (e.g., in quantitative 
expressions) (ML) 
Stage 4 Movement: 
? extraposition (ML) 
? raising, wh movement (RB) 
Stage 5 Ordering (ML): 
? ordering of constituents and leaf nodes in 
the tree 
Stage 6 Surface cleanup (ML): 
? lexical choice of determiners and relative 
pronouns 
? syntactic aggregation 
Stage 7 Punctuation (ML) 
Stage 8 Inflectional generation (RB) 
All machine learned components, with the 
exception of the generative language model for 
ordering of constituents (stage 5), are decision tree 
classifiers built with the WinMine toolkit 
(Chickering et al, 1997; Chickering, nd.). There 
are a total of eighteen decision tree classifiers in 
the system. The complexity of the decision trees 
varies with the complexity of the modeled task. 
The number of branching nodes in the decision 
tree models in Amalgam ranges from 3 to 447. 
3 Data and feature extraction 
The data for all of the models were drawn from a 
set of 100,000 sentences from technical software 
manuals and help files. The sentences are 
analyzed by the NLPWin system, which provides 
a syntactic and logical form analysis. Nodes in the 
logical form representation are linked to the 
corresponding syntactic nodes, allowing us to 
learn contexts for the mapping from the semantic 
representation to a surface syntax tree. The data is 
split 70/30 for training versus model parameter 
tuning. For each set of data we built decision trees 
at several different levels of granularity (by 
manipulating the prior probability of tree 
structures to favor simpler structures) and selected 
the model with the maximal accuracy as 
determined on the parameter tuning set. All 
models are then tested on data extracted from a 
separate blind set of 10,000 sentences from the 
same domain. For both training and test, we only 
extract features from sentences that have received 
a complete, spanning parse: 85.14% of the 
sentences in the training and parameter tuning set, 
and 84.59% in the blind test set fall into that 
category. Most sentences yield more than one 
training case. 
We attempt to standardize as much as possible 
the set of features to be extracted. We exploit the 
full set of features and attributes available in the 
analysis, instead of pre-determining a small set of 
potentially relevant features (Gamon et al, 
2002b). This allows us to share the majority of 
code between the individual feature extraction 
tasks. More importantly, it enables us to discover 
new linguistically interesting and/or domain-
specific generalizations from the data. Typically, 
we extract the full set of available analysis 
features of the node under investigation, its parent 
and its grandparent, with the only restriction being 
that these features need to be available at the stage 
where the model is consulted at generation run-
time. This provides us with a sufficiently large 
structural context for the operations. In addition, 
for some of the models we add a small set of 
features that we believe to be important for the 
task at hand, and that cannot easily be expressed 
as a combination of analysis features/attributes on 
constituents. Most features, such as lexical 
subcategorization features and semantic features 
such as [Definite] are binary. Other features, such 
as syntactic label or semantic relation, have as 
many as 25 values. Training time on a standard 
500MHz PC ranges from one hour to six hours. 
4 Assignment of case 
In German sentence realization, proper 
assignment of morphological case is essential for 
fluent and comprehensible output. German is a 
language with fairly free constituent order, and the 
identification of functional roles, such as subject 
versus object, is not determined by position in the 
sentence, as in English, but by morphological 
marking of one of the four cases: nominative, 
accusative, genitive or dative. In Amalgam, case 
assignment is one of the last steps in the Flesh-out 
stage (stage 2). Morphological realization of case 
can be ambiguous in German (for example, a 
feminine singular NP is ambiguous between 
accusative and nominative case). Since the 
morphological realization of case depends on the 
gender, number and morphological paradigm of a 
given NP, we chose to only consider NP nodes 
with unambiguous case as training data for the 
model1. As the target feature for this model is 
                                                     
1
 Ideally, we should train the case assignment model on 
a corpus that is hand-disambiguated for case. In the 
absence of such a corpus, though, we believe that our 
approach is linguistically justified. The case of an NP 
depends solely on the syntactic context it appears in. 
morphological case, it has four possible values for 
the four cases in German. 
4.1 Features in the case assignment 
model 
For each data point, a total of 712 features was 
extracted. Of the 712 features available to the 
decision tree building tools, 72 were selected as 
having predictive value in the model. The selected 
features fall into the following categories: 
? syntactic label of the node, its parent and 
grandparent 
? lemma (i.e., citation form) of the parent, 
and lemma of the governing preposition 
? subcategorization information, including 
case governing properties of governing 
preposition and parent 
? semantic relation of the node itself to its 
parent, of the parent to its grandparent, 
and of the grandparent to its great-
grandparent 
? number information on the parent and 
grandparent 
? tense and mood on the parent and 
grandparent 
? definiteness on the node, its parent and 
grandparent 
? the presence of various semantic 
dependents such as subject, direct and 
indirect objects, operators, attributive 
adjuncts and unspecified modifiers on the 
node and its parent and grandparent 
? quantification, negation, coordination on 
the node, the parent and grandparent 
? part of speech of the node, the parent and 
the grandparent 
? miscellaneous semantic features on the 
node itself and the parent 
4.2 The case assignment model 
The decision tree model for case assignment 
has 226 branching nodes, making it one of the 
most complex models in Amalgam. For each 
nominal node in the 10,000 sentence test set, we 
compared the prediction of the model to the 
                                                                                  
Since we want to learn the syntactically determining 
factors for case, using unambiguously case marked NPs 
for training seems justified. 
morphological case compatible with that node. 
The previously mentioned example of a singular 
feminine NP, for example, would yield a ?correct? 
if the model had predicted nominative or 
accusative case (because the NP is 
morphologically ambiguous between accusative 
and nominative), and it would yield an ?incorrect? 
if the model had predicted genitive or dative. This 
particular evaluation setup was a necessary 
compromise because of the absence of a hand-
annotated corpus with disambiguated case in our 
domain. The caveat here is that downstream 
models in the Amalgam pipeline that pick up on 
case as one of their features rely on the absolute 
accuracy of the assigned case, not the relative 
accuracy with respect to morphological 
ambiguity. Accuracy numbers for each of the four 
case assignments are given in Table 1. Note that it 
is impossible to give precision/recall numbers, 
without a hand-disambiguated test set. The 
baseline for this task is 0.7049 (accuracy if the 
most frequent case (nominative) had been 
assigned to all NPs). 
Table 1. Accuracy of the case assignment model. 
Value Accuracy 
Dat 0.8705 
Acc 0.9707 
Gen 0.9457 
Nom 0.9654 
overall 0.9352 
5 Assignment of verb position 
features 
One of the most striking properties of German is 
the distributional pattern of verbs in main and 
subordinate clauses. Most descriptive accounts of 
German syntax are based on a topology of the 
German sentence that treats the position of the 
verb as the fixed frame around which other 
syntactic constituents are organized in relatively 
free order (cf. Eisenberg, 1999; Engel, 1996). The 
position of the verb in German is non-negotiable; 
errors in the positioning of the verb result in 
gibberish, whereas most permutations of other 
constituents only result in less fluent output. 
Depending on the position of the finite verb, 
German sentences and verb phrases are classified 
as being ?verb-initial?, ?verb-second? or ?verb-
final?. In verb-initial clauses (e.g., in imperatives), 
the finite verb is in initial position. Verb-second 
sentences contain one constituent preceding the 
finite verb, in the so-called ?pre-field?. The finite 
verb is followed by any number of constituents in 
the ?middle-field?, and any non-finite verbs are 
positioned at the right periphery of the clause, 
possibly followed by extraposed material or 
complement clauses (the ?post-field?). Verb-final 
clauses contain no verbal element in the verb-
second position: all verbs are clustered at the right 
periphery, preceded by any number of constituents 
and followed only by complement clauses and 
extraposed material. 
During the Flesh-out stage in Amalgam, a 
decision tree classifier is consulted to make a 
classification decision among the four verb 
positions: ?verb-initial?, ?verb-second?, ?verb-
final?, and ?undefined?. The value ?undefined? 
for the target feature of verb position is extracted 
for those verbal constituents where the local 
syntactic context is too limited to make a clear 
distinction between initial, second, or final 
position of the verb. The number of ?undefined? 
verb positions is small compared to the number of 
clearly established verb positions: in the test set, 
there were only 690 observed cases of 
?undefined? verb position out of a total of 15,492 
data points. At runtime in Amalgam, verb position 
features are assigned based on the classification 
provided by the decision tree model. 
5.1 Features in the verb position model 
For each data point, 713 features were extracted. 
Of those features, 41 were selected by the decision 
tree algorithm. The selected features fall into the 
following categories: 
? syntactic label of the node and the parent 
? subcategorization features 
? semantic relations of the node to its parent 
and of the parent node to its parent 
? tense and mood features 
? presence of empty, uncontrolled subject 
? semantic features on the node and the 
parent 
5.2 The verb position model 
The decision tree model for verb position has 
115 branching nodes. Precision, recall and F-
measure for the model are given in Table 2. As a 
point of reference for the verb position classifier, 
assigning the most frequent value (second) of the 
target feature yields a baseline score of 0.4240. 
Table 2. Precision, recall, and F-measure for the verb 
position model. 
Value Precision Recall F-measure 
Initial 0.9650 0.9809 0.9729 
Second 0.9754 0.9740 0.9743 
Final 0.9420 0.9749 0.9581 
Undefined 0.5868 0.3869 0.4663 
Overall 
accuracy 
0.9491 
6 Extraposition 
In both German and English it is possible to 
extrapose clausal material to the right periphery of 
the sentence (extraposed clauses underlined in the 
examples below): 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave the 
country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumour has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Extraposition is not obligatory like other types 
of movement (such as Wh-movement). Both 
extraposed and non-extraposed versions of a 
sentence are acceptable, with varying degrees of 
fluency. 
The interesting difference between English and 
German is the frequency of this phenomenon. 
While it can easily be argued that English 
sentence realization may ignore extraposition and 
still result in very fluent output, the fluency of 
sentence realization for German will suffer much 
more from the lack of a good extraposition 
mechanism. We profiled data from various 
domains (Gamon et al 2002a) to substantiate this 
linguistic claim (see Uszkoreit et al 1998 for 
similar results). In the technical domain, more 
than one third of German relative clauses are 
extraposed, as compared to a meagre 0.22% of 
English relative clauses. In encyclopaedia text 
(Microsoft Encarta), approximately every fifth 
German relative clause is extraposed, compared to 
only 0.3% of English relative clauses. For 
complement clauses and infinitival clauses, the 
differences are not as striking, but still significant: 
in the technical and encyclopaedia domains, 
extraposition of infinitival and complement 
clauses in German ranges from 1.5% to 3.2%, 
whereas English only shows a range from 0% to 
0.53%. 
We chose to model extraposition as an iterative 
movement process from the original attachment 
site to the next higher node in the tree (for an 
alternative one-step solution and a comparison of 
the two approaches see (Gamon et al, 2002a)). 
The target feature of the model is the answer to 
the yes/no question ?Should the clause move from 
node X to the parent of node X??. 
6.1 Features in the extraposition model 
The tendency of a clause to be extraposed depends 
on properties of both the clause itself (e.g., some 
notion of ?heaviness?) and the current attachment 
site. Very coarse linguistic generalizations are that 
a relative clause tends to be extraposed if it is 
sufficiently ?heavy? and if it is followed by verbal 
material in the same clause. Feature extraction for 
this model reflects that fact by taking into 
consideration features on the extraposition 
candidate, the current attachment site, and 
potential next higher landing site. This results in a 
total of 1168 features. Each node in the parent 
chain of an extraposable clause, up to the actual 
attachment node, constitutes a single data point 
During the decision tree building process, 60 
features were selected as predictive. They can be 
classified as follows: 
General feature: 
? overall sentence length 
Features on the extraposable clause: 
? presence of verb-final and verb-second 
ancestor nodes 
? ?heaviness? both in number of characters 
and number of tokens 
? various linguistic features in the local 
context (parent node and grandparent 
node): number and person, definiteness, 
voice, mood, transitivity, presence of 
logical subject and object, presence of 
certain semantic attributes, coordination, 
prepositional relations 
? syntactic label 
? presence of modal verbs 
? prepositional relations 
? transitivity 
Features on the attachment site 
? presence of logical subject 
? status of the parent and grandparent as a 
separable prefix verb 
? voice and presence of modal verbs on the 
parent and grandparent 
? presence of arguments and transitivity 
features on the parent and grandparent 
? number, person and definiteness; the same 
on parent and grandparent 
? syntactic label; the same on the parent and 
grandparent 
? verb position; the same on the parent 
? prepositional relation on parent and 
grandparent 
? semantic relation that parent and 
grandparent have to their respective 
parent node 
6.2 The extraposition model 
During testing of the extraposition model, the 
model was consulted for each extraposable clause 
to find the highest node to which that clause could 
be extraposed. In other words, the target node for 
extraposition is the highest node in the parent 
chain for which the answer to the classification 
task ?Should the clause move from node X to the 
parent of node X?? is ?yes? with no interceding 
?no? answer. The prediction of the model was 
compared with the actual observed attachment site 
of the extraposable clause to yield the accuracy 
figures shown in Table 3. The model has 116 
branching nodes. The baseline for this task is 
calculated by applying the most frequent value for 
the target feature (?don't move?) to all nodes. The 
baseline for extraposition of infinitival and 
complement clauses is very high. The number of 
extraposed clauses of both types in the test set 
(fifteen extraposed infinitival clauses and twelve 
extraposed complement clauses) is very small, so 
it comes as no surprise that the model accuracy 
ranges around the baseline for these two types of 
extraposed clauses. 
Table 3. Accuracy of the extraposition model. 
Extraposable clause Accuracy Baseline 
RELCL 0.8387 0.6093 
INFCL 0.9202 0.9370 
COMPCL 0.9857 0.9429 
Overall 0.8612 0.6758 
7 Syntactic aggregation 
Any sentence realization component that 
generates from an abstract semantic representation 
and strives to produce fluent output beyond simple 
templates will have to deal with coordination and 
the problem of duplicated material in 
coordination. This is generally viewed as a sub-
area of aggregation in the generation literature 
(Wilkinson, 1995; Shaw, 1998; Reape and 
Mellish, 1999; Dalianis and Hovy, 1993). In 
Amalgam, the approach we take is strictly intra-
sentential, along the lines of what has been called 
conjunction reduction in the linguistic literature 
(McCawley, 1988). While this may seem a fairly 
straightforward task compared to inter-sentential, 
semantic and lexical aggregation, it should be 
noted that the cross-linguistic complexity of the 
phenomenon makes it much less trivial than a first 
glance at English would suggest. In German, for 
example, position of the verb in the coordinated 
VPs plays an important role in determining which 
duplicated constituent can be omitted. 
The target feature for the classification task is 
formulated as follows: ?In which coordinated 
constituent is the duplicated constituent to be 
realized??. There are three values for the target 
feature: ?first?, ?last?, and ?middle?. The third 
value (?middle?) is a default value for cases where 
neither the first, nor the last coordinated 
constituent can be identified as the location for the 
realization of duplicated constituents. At 
generation runtime, multiple realizations of a 
constituent in coordination are collected and the 
aggregation model is consulted to decide on the 
optimal position in which to realize that 
constituent. The constituent in that position is 
retained, while all other duplicates are removed 
from the tree. 
7.1 Features in the syntactic aggregation 
model 
A total of 714 features were extracted for the 
syntactic aggregation model. Each instance of 
coordination which exhibits duplicated material at 
the semantic level without corresponding 
duplication at the syntactic level constitutes a data 
point. 
Of these features, 15 were selected as 
predictive in the process of building the decision 
tree model: 
? syntactic label and syntactic label of the 
parent node 
? semantic relation to the parent of the 
duplicated node, its parent and grandparent 
? part of speech of the duplicated node 
? verb position across the coordinated node 
? position of the duplicated node in 
premodifiers or postmodifiers of the parent 
? coordination of the duplicated node and 
the grandparent of the duplicated node 
? status of parent and grandparent as a 
proposition 
? number feature on the parent 
? transitivity and presence of a direct object 
on the parent 
7.2 The syntactic aggregation model 
The syntactic aggregation model has 21 branching 
nodes. Precision, recall and F-measure for the 
model are given in Table 4. As was to be expected 
on the basis of linguistic intuition, the value 
?middle? for the target feature did not play any 
role. In the test set there were only 2 observed 
instances of that value. The baseline for this task 
is 0.8566 (assuming ?first? as the default value). 
Table 4. Precision, recall, and F-measure for the 
syntactic aggregation model. 
Value Precision Recall F-measure 
last 0.9191 0.9082 0.9136 
first 0.9837 0.9867 0.9851 
middle 0.0000 0.0000 0.0000 
overall 
accuracy 
0.9746 
8 Conclusion and future research 
We have demonstrated on the basis of four 
examples that it is possible to learn the contexts 
for complex linguistic operations in sentence 
realization with high accuracy. We proposed to 
standardize most of the feature extraction for the 
machine learning tasks to all available linguistic 
features on the node, and its parent and 
grandparent node. This generalized set of features 
allows us to rapidly train on new sets of data and 
to experiment with new machine learning tasks. 
Furthermore, it prevents us from focusing on a 
small set of hand-selected features for a given 
phenomenon; hence, it allows us to learn new (and 
unexpected) generalizations from new data. 
We have found decision trees to be useful for 
our classification problems, but other classifiers 
are certainly applicable. Decision trees provided 
an easily accessible inventory of the selected 
features and some indication of their relative 
importance in predicting the target features in 
question. Although our exposition has focused on 
the preferred value (the mode) predicted by the 
models, decision trees built by WinMine predict a 
probability distribution over all possible target 
values. For a system such as Amalgam, built as a 
pipeline of stages, this point is critical, since 
finding the best final hypothesis requires the 
consideration of multiple hypotheses and the 
concomitant combination of probabilities assigned 
by the various models in the pipeline to all 
possible target values. For example, our 
extraposition model presented above depends 
upon the value of the verb-position feature, which 
is predicted upstream in the pipeline. Currently, 
we greedily pursue the best hypothesis, which 
includes only the mode of the verb-position 
model?s prediction. However, work in progress 
involves a search that constructs multiple 
hypotheses incorporating each of the predictions 
of the verb-position model and their scores, and 
likewise for all other models. 
We have found the combination of knowledge-
engineered linguistic operations with machine-
learned contexts to be advantageous. The 
knowledge-engineered choice of linguistic 
operations, allows us to deal with complex 
linguistic phenomena. Machine learning, on the 
other hand, automates the discovery of general 
and domain-specific contexts. This facilitates 
adaptation of the system to a new domain or even 
to a new language. 
It should also be noted that none of the learned 
models can be easily replaced by a rule. While 
case assignment, for example, depends to a high 
degree on the lexical properties of the governing 
preposition or governing verb, other factors such 
as semantic relations, etc., play a significant role, 
so that any rule approaching the accuracy of the 
model would have to be quite complex.  
We are currently adapting Amalgam to the task 
of French sentence realization, as a test of the 
linguistic generality of the system. Initial results 
are encouraging. It appears that much of the 
feature extraction and many of the linguistic 
operations are reusable. 
Acknowledgements 
Our thanks go to Max Chickering for assistance 
with the WinMine decision tree tools and to Zhu 
Zhang who made significant contributions to the 
development of the extraposition models. 
References 
S. Bangalore and O. Rambow 2000. Exploiting a 
probabilistic hierarchical model for generation. 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING 2000). 
Saarbr?cken, Germany. 42-48. 
D. M. Chickering. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/Tool
doc.htm 
D. M. Chickering, D. Heckerman and C. Meek. 1997. 
A Bayesian approach to learning Bayesian networks 
with local structure. In ?Uncertainty in Artificial 
Intelligence: Proceedings of the Thirteenth 
Conference?, D. Geiger and P. Punadlik Shenoy, 
ed., Morgan Kaufman, San Francisco, California, 
pp. 80-89. 
S. Corston-Oliver, M. Gamon, E. Ringger, and R. 
Moore. 2002. An overview of Amalgam: A machine-
learned generation module. To be presented at 
INLG 2002. 
H. Dalianis and E. Hovy 1993 Aggregation in natural 
language generation. Proceedings of the 4th 
European Workshop on Natural Language 
Generation, Pisa, Italy. 
P. Eisenberg 1999. Grundriss der deutschen 
Grammatik. Band2: Der Satz. Metzler, 
Stuttgart/Weimar. 
U. Engel. 1996. Deutsche Grammatik. Groos, 
Heidelberg. 
M. Gamon, E. Ringger, Z. Zhang, R. Moore and S. 
Corston-Oliver. 2002a. Extraposition: A case study 
in German sentence realization. To be presented at 
the 19th International Conference on Computational 
Linguistics (COLING) 2002. 
M. Gamon, E. Ringger, S. Corston-Oliver. 2002b. 
Amalgam: A machine-learned generation module. 
Microsoft Research Technical Report, to appear. 
G. E. Heidorn. 2002. Intelligent Writing Assistance. In 
?A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text?, R. Dale, H. Moisl, and H. 
Somers (ed.), Marce Dekker, New York. 
I. Langkilde. and K. Knight. 1998a. The practical value 
of n-grams in generation. Proceedings of the 9th 
International Workshop on Natural Language 
Generation, Niagara-on-the-Lake, Canada. pp. 248-
255. 
I. Langkilde and K. Knight. 1998b. Generation that 
exploits corpus-based statistical knowledge. 
Proceedings of the 36th ACL and 17th COLING 
(COLING-ACL 1998). Montr?al, Qu?bec, Canada. 
704-710. 
J. D. McCawley. 1988 The Syntactic Phenomena of 
English. The University of Chicago Press, Chicago 
and London. 
M. Reape. and C. Mellish. 1999. Just what is 
aggregation anyway? Proceedings of the 7th 
European Workshop on Natural Language 
Generation, Toulouse, France. 
E. Ringger, R. Moore, M. Gamon, and S. Corston-
Oliver. In preparation. A Linguistically Informed 
Generative Language Model for Intra-Constituent 
Ordering during Sentence Realization. 
J. Shaw. 1998 Segregatory Coordination and Ellipsis in 
Text Generation. Proceedings of COLING-ACL, 
1998, pp 1220-1226. 
H. Uszkoreit, T. Brants, D. Duchier, B. Krenn, L. 
Konieczny, S. Oepen and W. Skut. 1998. Aspekte 
der Relativsatzextraposition im Deutschen. Claus-
Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
J. Wilkinson 1995 Aggregation in Natural Language 
Generation: Another Look. Co-op work term report, 
Department of Computer Science, University of 
Waterloo. 
        Task-focused Summarization of Email 
Simon Corston-Oliver, Eric Ringger, Michael Gamon and Richard Campbell 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{simonco, ringger, mgamon, richcamp}@microsoft.com 
 
 
 
 
Abstract 
 We describe SmartMail, a prototype system for 
automatically identifying action items (tasks) in 
email messages. SmartMail presents the user with 
a task-focused summary of a message. The 
summary consists of a list of action items extracted 
from the message. The user can add these action 
items to their ?to do? list. 
1 Introduction 
Email for many users has evolved from a mere 
communication system to a means of organizing 
workflow, storing information and tracking tasks 
(i.e. ?to do? items) (Bellotti et al, 2003; Cadiz et 
al., 2001). Tools available in email clients for 
managing this information are often cumbersome 
or even so difficult to discover that users are not 
aware that the functionality exists. For example, in 
one email client, Microsoft Outlook, a user must 
switch views and fill in a form in order to create a 
task corresponding to the current email message. 
By automatically identifying tasks that occur in the 
body of an email message, we hope to simplify the 
use of email as a tool for task creation and 
management. 
In this paper we describe SmartMail, a prototype 
system that automatically identifies tasks in email, 
reformulates them, and presents them to the user in 
a convenient interface to facilitate adding them to a 
?to do? list.  
SmartMail performs a superficial analysis of an 
email message to distinguish the header, message 
body (containing the new message content), and 
forwarded sections. 1  SmartMail breaks the 
                                                                 
1  This simple division into header, message body, and 
forwarded sections was sufficient for the corpus of email 
messages we considered. Messages containing original 
messages interleaved with new content were extremely 
message body into sentences, then determines 
the speech act of each sentence in the message 
body by consulting a machine-learned classifier. 
If the sentence is classified as a task, SmartMail 
performs additional linguistic processing to 
reformulate the sentence as a task description. 
This task description is then presented to the 
user. 
2 Data 
We collected a corpus of 15,741 email 
messages. The messages were divided into 
training, development test and blind test. The 
training set contained 106,700 sentences in 
message bodies from 14,535 messages. To 
avoid overtraining to individual writing styles, 
we limited the number of messages from a 
given sender to 50. To ensure that our 
evaluations are indicative of performance on 
messages from previously unencountered 
senders, we selected messages from 3,098 
senders, assigning all messages from a given 
sender to either the training or the test sets. 
Three human annotators labeled the message 
body sentences, selecting one tag from the 
following set: Salutation, Chit-chat (i.e., social 
discussion unrelated to the main purpose of the 
message), Task, Meeting (i.e., a proposal to 
meet), Promise, Farewell, various components 
of an email signature (Sig_Name, Sig_Title, 
Sig_Affiliation, Sig_Location, Sig_Phone, 
Sig_Email, Sig_URL, Sig_Other), and the 
default category ?None of the above?. The set of 
tags can be considered a set of application-
specific speech acts analogous to the rather 
particular tags used in the Verbmobil project, 
such as ?Suggest_exclude_date? and 
                                                                                                
uncommon in our corpus. Most senders were using 
Microsoft Outlook, which places the insertion point for 
new content at the top of the message. 
?Motivate_appointment? (Warnke et al, 1997; 
Mast et al, 1996) or the form-based tags of Stolcke 
et al (1998). 
All three annotators independently labeled 
sentences in a separate set of 146 messages not 
included in the training, development or blind test 
sets. We measured inter-annotator agreement for 
the assignment of tags to sentences in the message 
bodies using Cohen?s Kappa. Annotator 1 and 
annotator 2 measured 85.8%; annotator 1 and 
annotator 3 measured 82.6%; annotator 2 and 
annotator 3 measured 82.3%. We consider this 
level of inter-annotator agreement good for a novel 
set of application-specific tags. 
The development test and blind test sets of 
messages were tagged by all three annotators, and 
the majority tag for each sentence was taken. If any 
sentence did not have a majority tag, the entire 
message was discarded, leaving a total of 507 
messages in the development test set and 699 
messages in the blind test set. 
The set of tags was intended for a series of 
related experiments concerning linguistic 
processing of email. For example, greetings and 
chit-chat could be omitted from messages 
displayed on cell phones, or the components of an 
email signature could be extracted and stored in a 
contact database. In the current paper we focus 
exclusively on the identification of tasks. 
Annotators were instructed to mark a sentence 
as containing a task if it looked like an appropriate 
item to add to an on-going ?to do? list. By this 
criterion, simple factual questions would not 
usually be annotated as tasks; merely responding 
with an answer fulfills any obligation. Annotators 
were instructed to consider the context of an entire 
message when deciding whether formulaic endings 
to email such as Let me know if you have any 
questions were to be interpreted as mere social 
convention or as actual requests for review and 
comment. The following are examples of actual 
sentences annotated as tasks in our data: 
Since Max uses a pseudo-
random number generator, you 
could possibly generate the 
same sequence of numbers to 
select the same cases. 
 
Sorry, yes, you would have to 
retrain. 
 
An even fast [sic] thing 
would be to assign your own 
ID as a categorical feature. 
 
Michael, it?d be great if 
you could add some stuff re 
MSRDPS. 
 
Could you please remote 
desktop in and try running 
it on my machine. 
 
If CDDG has its own notion 
of what makes for good 
responses, then we should 
use that. 
 
3 Features 
Each sentence in the message body is described 
by a vector of approximately 53,000 features. 
The features are of three types: properties of the 
message (such as the number of addressees, the 
total size of the message, and the number of 
forwarded sections in the email thread), 
superficial features and linguistic features. 
The superficial features include word 
unigrams, bigrams and trigrams as well as 
counts of special punctuation symbols (e.g. @, 
/, #), whether the sentence contains words with 
so-called ?camel caps? (e.g., SmartMail), 
whether the sentence appears to contain the 
sender?s name or initials, and whether the 
sentence contains one of the addressees? names. 
The linguistic features were obtained by 
analyzing the given sentence using the NLPWin 
system (Heidorn 2000). The linguistic features 
include abstract lexical features, such as part-of-
speech bigrams and trigrams, and structural 
features that characterize the constituent 
structure in the form of context-free phrase 
structure rewrites (e.g., DECL:NP-VERB-NP; 
i.e., a declarative sentence consisting of a noun 
phrase followed by a verb and another noun 
phrase). Deeper linguistic analysis yielded 
features that describe part-of-speech 
information coupled with grammatical relations 
(e.g., Verb-Subject-Noun indicating a nominal 
subject of a verb) and features of the logical 
form analysis such as transitivity, tense and 
mood. 
 
4 Results 
We trained support vector machines (SVMs) 
(Vapnik, 1995) using an implementation of the 
sequential minimal optimization algorithm 
(Platt, 1999). We trained linear SVMs, which 
have proven effective in text categorization with 
large feature vectors (Joachims, 1998; Dumais et 
al., 1998).  
Figure 1 illustrates the precision-recall curve for 
the SVM classifier trained to distinguish tasks vs. 
non-tasks measured on the blind test set. 
We conducted feature ablation experiments on 
the development test set to assess the contribution 
of categories of features to overall classification 
performance. In particular we were interested in 
the role of linguistic analysis features compared to 
using only surface features. Within the linguistic 
features, we distinguished deep linguistic features 
(phrase structure features and semantic features) 
from POS n-gram features. We conducted 
experiments with three feature sets: 
1. all features (message level features + word 
unigram, bigram and trigram  
2. features + POS bigram and trigram 
features + linguistic analysis features) 
3. no deep linguistic features (no phrase 
structure or semantic features) 
4. no linguistic features at all (no deep 
linguistic features and no POS n-gram 
features) 
Based on these experiments on the development 
test set, we chose the feature set used for our run-
time applications.  
 
Figure 1 shows final results for these feature 
sets on the blind test set: for recall between 
approximately 0.2 and 0.4 and between 
approximately 0.5 and 0.6 the use of all features 
produces the best results. The distinction 
between the ?no linguistic features? and ?no 
deep linguistic features? scenarios is negligible; 
word n-grams appear to be highly predictive. 
Based on these results, we expect that for 
languages where we do not have an NLPWin 
parser, we can safely exclude the deeper 
linguistic features and still expect good 
classifier performance. 
 
 
Figure 2 illustrates the accuracy of 
distinguishing messages that contain tasks from 
those that do not, using all features. A message 
was marked as containing a task if it contained 
at least one sentence classified as a task. Since 
only one task has to be found in order for the 
entire message to be classified as containing a 
task, accuracy is substantially higher than on a 
per-sentence basis. In section 6, we discuss the 
scenarios motivating the distinction between 
sentence classification and message 
classification. 
 
 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
Pr
ec
is
io
n
All features
No deep linguistic features
No linguistic features
 
 
Figure 1: Precision-Recall curves for ablation experiments 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
P
re
ci
si
on
Per sentence
Per message
 
 
Figure 2: Precision-Recall curves comparing message classification and sentence classification 
 
 
5 Reformulation of Tasks 
SmartMail performs post-processing of sentences 
identified as containing a task to reformulate them 
as task-like imperatives. The process of 
reformulation involves four distinct knowledge-
engineered steps:  
1. Produce a logical form (LF) for the 
extracted sentence (Campbell and Suzuki, 
2001). The nodes of the LF correspond to 
syntactic constituents. Edges in the LF 
represent semantic and deep syntactic 
relations among nodes. Nodes bear 
semantic features such as tense, number 
and mood. 
2. Identify the clause in the logical form that 
contains the task; this may be the entire 
sentence or a subpart. We consider such 
linguistic properties as whether the clause 
is imperative, whether its subject is second 
person, and whether modality words such 
as please or a modal verb are used. All 
parts of the logical form not subsumed by 
the task clause are pruned. 
3. Transform the task portion of the LF to 
exclude extraneous words (e.g. please, 
must, could), extraneous subordinate 
clauses, adverbial modifiers, and vocative 
phrases. We replace certain deictic 
elements (i.e., words or phrases whose 
denotation varies according to the writer or 
the time and place of utterance) with non-
deictic expressions. For example, first 
person pronouns are replaced by either the 
name of the sender of the email or by a 
third person pronoun, if such a pronoun 
would unambiguously refer to the sender. 
Similarly, a temporal expression such as 
Thursday, which may refer to a different 
date depending on the week in which it is 
written, is replaced by an absolute date 
(e.g., 4/1/2004). 
4. Pass the transformed LF to a sentence 
realization module to yield a string 
(Aikawa et al, 2001). 
Below we illustrate the reformulation of tasks with 
some examples from our corpus. 
 
Example 1: 
On the H-1 visa issue, I am 
positive that you need to go 
to the Embassy in London to 
get your visa stamped into 
your passport. 
Reformulation: 
Go to the Embassy in London to 
get your visa stamped into 
your passport. 
 
In this example, the embedded sentential 
complement, that is, the part of the sentence 
following positive, is selected as the part of the 
sentence containing the task, because of the modal 
verb need and the second person subject; only that 
part of the sentence gets reformulated. The modal 
verb and the second person subject are deleted to 
form an imperative sentence. 
 
Example 2: 
Can you please send me the 
follow up information for the 
demo(s) listed in this Email 
ASAP. 
Reformulation: 
Send Kendall the follow up 
information for the demo 
listed in this Email ASAP. 
 
In this example, the whole sentence is selected 
as containing the task (modal verb, second person 
subject); modal elements including please are 
deleted along with the second person subject to 
form an imperative. In addition, the first person 
pronoun me is replaced by a reference to the 
sender, Kendall in this instance. 
 
Example 3: 
I've been Wednesday at the 
lecture on Amalgam you gave in 
the 113/1021 Room (which I 
really liked), and I've been 
wondering how feasible would 
it be to use Amalgam for 
learning requirements or code 
corpus structures and rules 
(and eventually rephrase them 
in some way). 
Reformulation: 
On June 5, 2002 Pablo wrote: 
?I've been Wednesday at the 
lecture on Amalgam you gave in 
the 113/1021 Room (which I 
really liked), and I've been 
wondering how feasible would 
it be to use Amalgam for 
learning requirements or code 
corpus structures and rules 
(and eventually rephrase them 
in some way).' 
 
This example illustrates what happens when 
NLPWin is unable to produce a spanning parse and 
hence a coherent LF; in this case NLPWin 
misanalyzed the clause following wondering as a 
main clause, instead of correctly analyzing it as a 
complement clause. SmartMail?s back-off strategy 
for non-spanning parses is to enclose the entire 
original sentence in quotes, prefixed with a matrix 
sentence indicating the date and the name of the 
sender. 
 
 
6 Task-Focused Summarization 
We have considered several scenarios for 
presenting the tasks that SmartMail identifies. 
Under the most radical scenario, SmartMail would 
automatically add extracted tasks to the user?s ?to 
do? list. This scenario has received a fairly 
negative reception when we have suggested it to 
potential users of a prototype. From an application 
perspective, this scenario is ?fail hard?; i.e., 
classification errors might result in garbage being 
added to the ?to do? list, with the result that the 
user would have to manually remove items. Since 
our goal is to reduce the workload on the user, this 
outcome would seem to violate the maxim ?First, 
do no harm?. 
 
Figure 3 and Figure 4 illustrate several ideas for 
presenting tasks to the user of Microsoft Outlook. 
Messages that contain tasks are flagged, using the 
existing flag icons in Outlook for proof of concept. 
Users can sort mail to see all messages containing 
tasks. This visualization amounts to summarizing 
the message down to one bit, i.e., +/- Task, and is 
conceptually equivalent to performing document 
classification. 
The right-hand pane in Figure 3 is magnified as 
Figure 4 and shows two more visualizations. At the 
top of the pane, the tasks that have been identified 
are presented in one place, with a check box beside 
them. Checking the box adds the task to the Tasks 
or ?to do? list, with a link back to the original 
message. This presentation is ?fail soft?: the user 
can ignore incorrectly classified tasks, or tasks that 
were correctly identified but which the user does 
not care to add to the ?to do? list. This list of tasks 
amounts to a task-focused summary of the 
document. This summary is intended to be read as 
a series of disconnected sentences, thus side-
stepping the issue of producing a coherent text 
from a series of extracted sentences. In the event 
that users prefer to view these extracted sentences 
as a coherent text, it may prove desirable to 
attempt to improve the textual cohesion by using 
anaphoric links, cue phrases and so on. 
Finally, Figure 3 also shows tasks highlighted in 
context in the message, allowing the user to skim 
the document and read the surrounding text. 
In the prototype we allow the user to vary the 
precision and recall of the classifier by adjusting a 
slider (not illustrated here) that sets the probability 
threshold on the probability of Task. 
 
Figure 3 and Figure 4 illustrate a convention that 
we observed in a handful of emails: proper names 
occur as section headings. These names have scope 
over the tasks enumerated beneath them, i.e. there 
is a list of tasks assigned to Matt, a list assigned to 
Eric or Mo, and a list assigned to Mo. SmartMail 
does not currently detect this explicit assignment 
of tasks to individuals. 
Important properties of tasks beyond the text of 
the message could also be automatically extracted. 
For example, the schema for tasks in Outlook 
includes a field that specifies the due date of the 
task. This field could be filled with date and time 
information extracted from the sentence containing 
the task. Similarly the content of the sentence 
containing the task or inferences about social 
relationships of the email interlocutors could be 
used to mark the priority of tasks as High, Low, or 
Normal in the existing schema. 
7 Conclusion 
In this paper we have presented aspects of 
SmartMail, which provides a task-oriented 
summary of email messages. This summary is 
produced by identifying the task-related sentences 
in the message and then reformulating each task-
related sentence as a brief (usually imperative) 
summation of the task. The set of tasks extracted 
and reformulated from a given email message is 
thus a task-focused summary of that message. 
We plan to conduct user studies by distributing 
the prototype as an Outlook add-in to volunteers 
who would use it to read and process their own 
mail over a period of several weeks. We intend to 
measure more than the precision and recall of our 
classifier by observing how many identified tasks 
users actually add to their ?to do? list and by 
administering qualitative surveys of user 
satisfaction. 
The ability to reformulate tasks is in principle 
separate from the identification of tasks. In our 
planned usability study we will distribute variants 
of the prototype to determine the effect of 
reformulation. Do users prefer to be presented with 
the extracted sentences with no additional 
processing, the tasks reformulated as described in 
Section 5, or an even more radical reformulation to 
a telegraphic form consisting of a verb plus object, 
such as Send information or Schedule subjects? 
 
 
 
 
 
 
 
Figure 3: Prototype system showing ways of visualizing tasks 
 
 
Figure 4: Magnified view of prototype system showing message with enumerated tasks 
 
 
 
8  Acknowledgements 
Many of the ideas presented here were formulated 
in discussion with Bob Atkinson, Dave Reed and 
Malcolm Pearson. Our thanks go to Jeff 
Stevenson, Margaret Salome and Kevin Gaughen 
for annotating the data. 
References 
Aikawa, Takako, Maite Melero, Lee Schwartz and 
Andi Wu. 2001. Multilingual natural language 
generation. EAMT. 
Bellotti, Victoria, Nicolas Ducheneaut, Mark 
Howard , Ian Smith. 2003. Taking email to 
task: the design and evaluation of a task 
management centered email tool. Proceedings 
of the conference on human factors in 
computing systems, pages 345-352. 
Cadiz, J. J., Dabbish, L., Gupta, A., & Venolia, G. 
D. 2001. Supporting email workflow. MSR-TR-
2001-88: Microsoft Research. 
Campbell, Richard and Hisami Suzuki. 2002. 
Language neutral representation of syntactic 
structure. Proceedings of SCANALU 2002. 
Dumais, Susan, John Platt, David Heckerman, 
Mehran Sahami 1998: Inductive learning 
algorithms and representations for text 
categorization. Proceedings of CIKM-98, pages 
148-155. 
Heidorn, George. 2000. Intelligent writing 
assistance. In R. Dale, H. Moisl and H. Somers, 
(eds.), Handbook of Natural Language 
Processing. Marcel Dekker. 
Joachims, Thorsten. 1998. Text categorization 
with support vector machines: Learning with 
many relevant features. Proceedings of ECML 
1998, pages 137-142. 
Mast, M., Kompe, R., Harbeck, S., Kiessling, A., 
Niemann, H., N?th, E., Schukat-Talamazzini, 
E. G. and Warnke., V. 1996. Dialog act 
classification with the help of prosody. ICSLP 
96. 
Platt, John. 1999. Fast training of SVMs using 
sequential minimal optimization. In B. 
Schoelkopf, C. Burges and A. Smola (eds.) 
Advances in Kernel Methods: Support Vector 
Learning, pages 185-208, MIT Press, 
Cambridge, MA.  
Stolcke, A., E. Shriberg, R. Bates, N. Coccaro, D. 
Jurafsky, R. Martin, M. Meteer, K. Ries, P. 
Taylor and C. Van Ess-Dykema. 1998. Dialog 
act modeling for conversational speech. 
Proceedings of the AAAI-98 Spring Symposium 
on Applying Machine Learning to Discourse 
Processing.  
Vapnik, V. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Warnke, V., R. Kompe, H. Niemann and E. N?th. 
1997. Integrated dialog act segmentation and 
classification using prosodic features and 
language models. Proc. European Conf. on 
Speech Communication and Technology, vol 1, 
pages 207?210. 
 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 160?167,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilingual Dependency Parsing using Bayes Point Machines
Simon Corston-Oliver
Microsoft Research
One Microsoft Way
Redmond, WA 98052
simonco@microsoft.com
Anthony Aue
Microsoft Research
One Microsoft Way
Redmond, WA 98052
anthaue@microsoft.com
Kevin Duh
Dept. of Electrical Eng.
Univ. of Washington
Seattle, WA 98195
duh@ee.washington.edu
Eric Ringger
Computer Science Dept.
Brigham Young Univ.
Provo, UT 84602
ringger@cs.byu.edu
Abstract
We develop dependency parsers for Ara-
bic, English, Chinese, and Czech using
Bayes Point Machines, a training algo-
rithm which is as easy to implement as
the perceptron yet competitive with large
margin methods. We achieve results com-
parable to state-of-the-art in English and
Czech, and report the first directed depen-
dency parsing accuracies for Arabic and
Chinese. Given the multilingual nature of
our experiments, we discuss some issues
regarding the comparison of dependency
parsers for different languages.
1 Introduction
Dependency parsing is an alternative to constituency
analysis with a venerable tradition going back at
least two millenia. The last century has seen at-
tempts to formalize dependency parsing, particu-
larly in the Prague School approach to linguistics
(Tesnie`re, 1959; Melc?uk, 1988).
In a dependency analysis of syntax, words directly
modify other words. Unlike constituency analysis,
there are no intervening non-lexical nodes. We use
the terms child and parent to denote the dependent
term and the governing term respectively.
Parsing has many potential applications, rang-
ing from question answering and information re-
trieval to grammar checking. Our intended ap-
plication is machine translation in the Microsoft
Research Treelet Translation System (Quirk et al,
2005; Menezes and Quirk, 2005). This system ex-
pects an analysis of the source language in which
words are related by directed, unlabeled dependen-
cies. For the purposes of developing machine trans-
lation for several language pairs, we are interested in
dependency analyses for multiple languages.
The contributions of this paper are two-fold: First,
we present a training algorithm called Bayes Point
Machines (Herbrich et al, 2001; Harrington et al,
2003), which is as easy to implement as the per-
ceptron, yet competitive with large margin meth-
ods. This algorithm has implications for anyone
interested in implementing discriminative training
methods for any application. Second, we develop
parsers for English, Chinese, Czech, and Arabic and
probe some linguistic questions regarding depen-
dency analyses in different languages. To the best of
our knowledge, the Arabic and Chinese results are
the first reported results to date for directed depen-
dencies. In the following, we first describe the data
(Section 2) and the basic parser architecture (Section
3). Section 4 introduces the Bayes Point Machine
while Section 5 describes the features for each lan-
guage. We conclude with experimental results and
discussions in Sections 6 and 7.
2 Data
We utilize publicly available resources in Arabic,
Chinese, Czech, and English for training our depen-
dency parsers.
For Czech we used the Prague Dependency Tree-
bank version 1.0 (LDC2001T10). This is a corpus
of approximately 1.6 million words. We divided
the data into the standard splits for training, devel-
160
opment test and blind test. The Prague Czech De-
pendency Treebank is provided with human-edited
and automatically-assigned morphological informa-
tion, including part-of-speech labels. Training and
evaluation was performed using the automatically-
assigned labels.
For Arabic we used the Prague Arabic De-
pendency Treebank version 1.0 (LDC2004T23).
Since there is no standard split of the data into
training and test sections, we made an approxi-
mate 70%/15%/15% split for training/development
test/blind test by sampling whole files. The Ara-
bic Dependency Treebank is considerably smaller
than that used for the other languages, with approx-
imately 117,000 tokens annotated for morphologi-
cal and syntactic relations. The relatively small size
of this corpus, combined with the morphological
complexity of Arabic and the heterogeneity of the
corpus (it is drawn from five different newspapers
across a three-year time period) is reflected in the
relatively low dependency accuracy reported below.
As with the Czech data, we trained and evaluated us-
ing the automatically-assigned part-of-speech labels
provided with the data.
Both the Czech and the Arabic corpora are anno-
tated in terms of syntactic dependencies. For En-
glish and Chinese, however, no corpus is available
that is annotated in terms of dependencies. We there-
fore applied head-finding rules to treebanks that
were annotated in terms of constituency.
For English, we used the Penn Treebank version
3.0 (Marcus et al, 1993) and extracted dependency
relations by applying the head-finding rules of (Ya-
mada and Matsumoto, 2003). These rules are a
simplification of the head-finding rules of (Collins,
1999). We trained on sections 02-21, used section
24 for development test and evaluated on section
23. The English Penn Treebank contains approxi-
mately one million tokens. Training and evaluation
against the development test set was performed us-
ing human-annotated part-of-speech labels. Evalu-
ation against the blind test set was performed us-
ing part-of-speech labels assigned by the tagger de-
scribed in (Toutanova et al, 2003).
For Chinese, we used the Chinese Treebank ver-
sion 5.0 (Xue et al, 2005). This corpus contains
approximately 500,000 tokens. We made an approx-
imate 70%/15%/15% split for training/development
test/blind test by sampling whole files. As with the
English Treebank, training and evaluation against
the development test set was performed using
human-annotated part-of-speech labels. For evalu-
ation against the blind test section, we used an im-
plementation of the tagger described in (Toutanova
et al, 2003). Trained on the same training section
as that used for training the parser and evaluated on
the development test set, this tagger achieved a to-
ken accuracy of 92.2% and a sentence accuracy of
63.8%.
The corpora used vary in homogeneity from the
extreme case of the English Penn Treebank (a large
corpus drawn from a single source, the Wall Street
Journal) to the case of Arabic (a relatively small
corpus?approximately 2,000 sentences?drawn from
multiple sources). Furthermore, each language
presents unique problems for computational analy-
sis. Direct comparison of the dependency parsing
results for one language to the results for another
language is therefore difficult, although we do at-
tempt in the discussion below to provide some basis
for a more direct comparison. A common question
when considering the deployment of a new language
for machine translation is whether the natural lan-
guage components available are of sufficient quality
to warrant the effort to integrate them into the ma-
chine translation system. It is not feasible in every
instance to do the integration work first and then to
evaluate the output.
Table 1 summarizes the data used to train the
parsers, giving the number of tokens (excluding
traces and other empty elements) and counts of sen-
tences.1
3 Parser Architecture
We take as our starting point a re-implementation
of McDonald?s state-of-the-art dependency parser
(McDonald et al, 2005a). Given a sentence x, the
goal of the parser is to find the highest-scoring parse
y? among all possible parses y ? Y :
y? = arg max
y?Y
s(x, y) (1)
1The files in each partition of the Chinese and Arabic data
are given at http://research.microsoft.com/?simonco/
HLTNAACL2006.
161
Language Total Training Development Blind
Tokens Sentences Sentences Sentences
Arabic 116,695 2,100 446 449
Chinese 527,242 14,735 1,961 2,080
Czech 1,595,247 73,088 7,319 7,507
English 1,083,159 39,832 1,346 2,416
Table 1: Summary of data used to train parsers.
For a given parse y, its score is the sum of the scores
of all its dependency links (i, j) ? y:
s(x, y) = ?
(i,j)?y
d(i, j) = ?
(i,j)?y
w ? f(i, j) (2)
where the link (i, j) indicates a head-child depen-
dency between the token at position i and the token
at position j. The score d(i, j) of each dependency
link (i, j) is further decomposed as the weighted
sum of its features f(i, j).
This parser architecture naturally consists of three
modules: (1) a decoder that enumerates all possi-
ble parses y and computes the argmax; (2) a train-
ing algorithm for adjusting the weights w given the
training data; and (3) a feature representation f(i, j).
Two decoders will be discussed here; the training al-
gorithm and feature representation are discussed in
the following sections.
A good decoder should satisfy several proper-
ties: ideally, it should be able to search through all
valid parses of a sentence and compute the parse
scores efficiently. Efficiency is a significant issue
since there are usually an exponential number of
parses for any given sentence, and the discrimina-
tive training methods we will describe later require
repeated decoding at each training iteration. We re-
implemented Eisner?s decoder (Eisner, 1996), which
searches among all projective parse trees, and the
Chu-Liu-Edmonds? decoder (Chu and Liu, 1965;
Edmonds, 1967), which searches in the space of
both projective and non-projective parses. (A pro-
jective tree is a parse with no crossing dependency
links.) For the English and Chinese data, the head-
finding rules for converting from Penn Treebank
analyses to dependency analyses creates trees that
are guaranteed to be projective, so Eisner?s algo-
rithm suffices. For the Czech and Arabic corpora,
a non-projective decoder is necessary. Both algo-
rithms are O(N3), where N is the number of words
in a sentence.2 Refer to (McDonald et al, 2005b)
for a detailed treatment of both algorithms.
4 Training: The Bayes Point Machine
In this section, we describe an online learning al-
gorithm for training the weights w. First, we ar-
gue why an online learner is more suitable than a
batch learner like a Support Vector Machine (SVM)
for this task. We then review some standard on-
line learners (e.g. perceptron) before presenting the
Bayes Point Machine (BPM) (Herbrich et al, 2001;
Harrington et al, 2003).
4.1 Online Learning
An online learner differs from a batch learner in that
it adjusts w incrementally as each input sample is
revealed. Although the training data for our pars-
ing problem exists as a batch (i.e. all input sam-
ples are available during training), we can apply
online learning by presenting the input samples in
some sequential order. For large training set sizes,
a batch learner may face computational difficulties
since there already exists an exponential number of
parses per input sentence. Online learning is more
tractable since it works with one input at a time.
A popular online learner is the perceptron. It ad-
justs w by updating it with the feature vector when-
ever a misclassification on the current input sample
occurs. It has been shown that such updates con-
verge in a finite number of iterations if the data is lin-
early separable. The averaged perceptron (Collins,
2002) is a variant which averages the w across all
iterations; it has demonstrated good generalization
especially with data that is not linearly separable,
as in many natural language processing problems.
2The Chu-Liu-Edmonds? decoder, which is based on a maxi-
mal spanning tree algorithm, can run in O(N2), but our simpler
implementation of O(N3) was sufficient.
162
Recently, the good generalization properties of Sup-
port Vector Machines have prompted researchers to
develop large margin methods for the online set-
ting. Examples include the margin perceptron (Duda
et al, 2001), ALMA (Gentile, 2001), and MIRA
(which is used to train the parser in (McDonald et al,
2005a)). Conceptually, all these methods attempt to
achieve a large margin and approximate the maxi-
mum margin solution of SVMs.
4.2 Bayes Point Machines
The Bayes Point Machine (BPM) achieves good
generalization similar to that of large margin meth-
ods, but is motivated by a very different philoso-
phy of Bayesian learning or model averaging. In
the Bayesian learning framework, we assume a prior
distribution over w. Observations of the training
data revise our belief of w and produce a poste-
rior distribution. The posterior distribution is used
to create the final wBPM for classification:
wBPM = Ep(w|D)[w] =
|V (D)|?
i=1
p(wi|D) wi (3)
where p(w|D) is the posterior distribution of the
weights given the data D and Ep(w|D) is the expec-
tation taken with respect to this distribution. The
term |V (D)| is the size of the version space V (D),
which is the set of weights wi that is consistent with
the training data (i.e. the set of wi that classifies the
training data with zero error). This solution achieves
the so-called Bayes Point, which is the best approx-
imation to the Bayes optimal solution given finite
training data.
In practice, the version space may be large, so we
approximate it with a finite sample of size I . Further,
assuming a uniform prior over weights, we get the
following equation:
wBPM = Ep(w|D)[w] ?
I?
i=1
1
I wi (4)
Equation 4 can be computed by a very simple al-
gorithm: (1) Train separate perceptrons on different
random shuffles of the entire training data, obtaining
a set of wi. (2) Take the average (arithmetic mean)
of the weights wi. It is well-known that perceptron
training results in different weight vector solutions
Input: Training set D = ((x1, y1), (x2, y2), . . . , (xT , yT ))
Output: wBPM
Initialize: wBPM = 0
for i = 1 to I; do
Randomly shuffle the sequential order of samples in D
Initialize: wi = 0
for t = 1 to T; do
y?t = wi ? xt
if (y?t != yt) thenwi = wi + ytxt
done
wBPM = wBPM + 1Iwidone
Figure 1: Bayes Point Machine pseudo-code.
if the data samples are presented sequentially in dif-
ferent orders. Therefore, random shuffles of the data
and training a perceptron on each shuffle is effec-
tively equivalent to sampling different models (wi)
in the version space. Note that this averaging op-
eration should not be confused with ensemble tech-
niques such as Bagging or Boosting?ensemble tech-
niques average the output hypotheses, whereas BPM
averages the weights (models).
The BPM pseudocode is given in Figure 1. The
inner loop is simply a perceptron algorithm, so the
BPM is very simple and fast to implement. The
outer loop is easily parallelizable, allowing speed-
ups in training the BPM. In our specific implemen-
tation for dependency parsing, the line of the pseu-
docode corresponding to [y?t = wi ? xt] is replaced
by Eq. 1 and updates are performed for each in-
correct dependency link. Also, we chose to average
each individual perceptron (Collins, 2002) prior to
Bayesian averaging.
Finally, it is important to note that the definition of
the version space can be extended to include weights
with non-zero training error, so the BPM can handle
data that is not linearly separable. Also, although we
only presented an algorithm for linear classifiers (pa-
rameterized by the weights), arbitrary kernels can be
applied to BPM to allow non-linear decision bound-
aries. Refer to (Herbrich et al, 2001) for a compre-
hensive treatment of BPMs.
5 Features
Dependency parsers for all four languages were
trained using the same set of feature types. The
feature types are essentially those described in (Mc-
Donald et al, 2005a). For a given pair of tokens,
163
where one is hypothesized to be the parent and the
other to be the child, we extract the word of the par-
ent token, the part of speech of the parent token, the
word of the child token, the part of speech of the
child token and the part of speech of certain adjacent
and intervening tokens. Some of these atomic fea-
tures are combined in feature conjunctions up to four
long, with the result that the linear classifiers de-
scribed below approximate polynomial kernels. For
example, in addition to the atomic features extracted
from the parent and child tokens, the feature [Par-
entWord, ParentPOS, ChildWord, ChildPOS] is also
added to the feature vector representing the depen-
dency between the two tokens. Additional features
are created by conjoining each of these features with
the direction of the dependency (i.e. is the parent to
the left or right of the child) and a quantized measure
of the distance between the two tokens. Every token
has exactly one parent. The root of the sentence has
a special synthetic token as its parent.
Like McDonald et al we add features that con-
sider the first five characters of words longer than
five characters. This truncated word crudely approx-
imates stemming. For Czech and English the addi-
tion of these features improves accuracy. For Chi-
nese and Arabic, however, it is clear that we need a
different backoff strategy.
For Chinese, we truncate words longer than a sin-
gle character to the first character.3 Experimental
results on the development test set suggested that an
alternative strategy, truncation of words longer than
two characters to the first two characters, yielded
slightly worse results.
The Arabic data is annotated with gold-standard
morphological information, including information
about stems. It is also annotated with the output
of an automatic morphological analyzer, so that re-
searchers can experiment with Arabic without first
needing to build these components. For Arabic, we
truncate words to the stem, using the value of the
lemma attribute.
All tokens are converted to lowercase, and num-
bers are normalized. In the case of English, Czech
and Arabic, all numbers are normalized to a sin-
3There is a near 1:1 correspondence between characters
and morphemes in contemporary Mandarin Chinese. However,
most content words consist of more than one morpheme, typi-
cally two.
gle token. In Chinese, months are normalized to a
MONTH token, dates to a DATE token, years to a
YEAR token. All other numbers are normalized to a
single NUMBER token.
The feature types were instantiated using all or-
acle combinations of child and parent tokens from
the training data. It should be noted that when the
feature types are instantiated, we have considerably
more features than McDonald et al For example,
for English we have 8,684,328 whereas they report
6,998,447 features. We suspect that this is mostly
due to differences in implementation of the features
that backoff to stems.
The averaged perceptrons were trained on the
one-best parse, updating the perceptron for every
edge and averaging the accumulated perceptrons af-
ter every sentence. Experiments in which we up-
dated the perceptron based on k-best parses tended
to produce worse results. The Chu-Liu-Edmonds al-
gorithm was used for Czech. Experiments with the
development test set suggested that the Eisner de-
coder gave better results for Arabic than the Chu-
Liu-Edmonds decoder. We therefore used the Eisner
decoder for Arabic, Chinese and English.
6 Results
Table 2 presents the accuracy of the dependency
parsers. Dependency accuracy indicates for how
many tokens we identified the correct head. Root ac-
curacy, i.e. for how many sentences did we identify
the correct root or roots, is reported as F1 measure,
since sentences in the Czech and Arabic corpora can
have multiple roots and since the parsing algorithms
can identify multiple roots. Complete match indi-
cates how many sentences were a complete match
with the oracle dependency parse.
A convention appears to have arisen when report-
ing dependency accuracy to give results for English
excluding punctuation (i.e., ignoring punctuation to-
kens in the output of the parser) and to report results
for Czech including punctuation. In order to facil-
itate comparison of the present results with previ-
ously published results, we present measures includ-
ing and excluding punctuation for all four languages.
We hope that by presenting both sets of measure-
ments, we also simplify one dimension along which
published results of parse accuracy differ. A direct
164
Including punctuation Excluding punctuation
Language Dependency Root Complete Dependency Root Complete
Accuracy Accuracy Match Accuracy Accuracy Match
Arabic 79.9 90.0 9.80 79.8 87.8 10.2
Chinese 71.2 66.2 17.5 73.3 66.2 18.2
Czech 84.0 88.8 30.9 84.3 76.2 32.2
English 90.0 93.7 35.1 90.8 93.7 37.6
Table 2: Bayes Point Machine accuracy measured on blind test set.
comparison of parse results across languages is still
difficult for reasons to do with the different nature
of the languages, the corpora and the differing stan-
dards of linguistic detail annotated, but a compar-
ison of parsers for two different languages where
both results include punctuation is at least preferable
to a comparison of results including punctuation to
results excluding punctuation.
The results reported here for English and Czech
are comparable to the previous best published num-
bers in (McDonald et al, 2005a), as Table 3 shows.
This table compares McDonald et al?s results for an
averaged perceptron trained for ten iterations with
no check for convergence (Ryan McDonald, pers.
comm.), MIRA, a large margin classifier, and the
current Bayes Point Machine results. To determine
statistical significance we used confidence intervals
for p=0.95. For the comparison of English depen-
dency accuracy excluding punctuation, MIRA and
BPM are both statistically significantly better than
the averaged perceptron result reported in (McDon-
ald et al, 2005a). MIRA is significantly better
than BPM when measuring dependency accuracy
and root accuracy, but BPM is significantly better
when measuring sentences that match completely.
From the fact that neither MIRA nor BPM clearly
outperforms the other, we conclude that we have
successfully replicated the results reported in (Mc-
Donald et al, 2005a) for English.
For Czech we also determined significance using
confidence intervals for p=0.95 and compared re-
sults including punctuation. For both dependency
accuracy and root accuracy, MIRA is statisticallty
significantly better than averaged perceptron, and
BPM is statistically significantly better than MIRA.
Measuring the number of sentences that match com-
pletely, BPM is statistically significantly better than
averaged perceptron, but MIRA is significantly bet-
ter than BPM. Again, since neither MIRA nor BPM
outperforms the other on all measures, we conclude
that the results constitute a valiation of the results
reported in (McDonald et al, 2005a).
For every language, the dependency accuracy of
the Bayes Point Machine was greater than the ac-
curacy of the best individual perceptron that con-
tributed to that Bayes Point Machine, as Table 4
shows. As previously noted, when measuring
against the development test set, we used human-
annotated part-of-speech labels for English and Chi-
nese.
Although the Prague Czech Dependency Tree-
bank is much larger than the English Penn Treebank,
all measurements are lower than the corresponding
measurements for English. This reflects the fact that
Czech has considerably more inflectional morphol-
ogy than English, leading to data sparsity for the lex-
ical features.
The results reported here for Arabic are, to our
knowledge, the first published numbers for depen-
dency parsing of Arabic. Similarly, the results for
Chinese are the first published results for the depen-
dency parsing of the Chinese Treebank 5.0.4 Since
the Arabic and Chinese numbers are well short of
the numbers for Czech and English, we attempted
to determine what impact the smaller corpora used
for training the Arabic and Chinese parsers might
have. We performed data reduction experiments,
training the parsers on five random samples at each
size smaller than the entire training set. Figure 2
shows the dependency accuracy measured on the
complete development test set when training with
samples of the data. The graph shows the average
4(Wang et al, 2005) report numbers for undirected depen-
dencies on the Chinese Treebank 3.0. We cannot meaningfully
compare those numbers to the numbers here.
165
Language Algorithm DA RA CM
English Avg. Perceptron 90.6 94.0 36.5
(exc punc) MIRA 90.9 94.2 37.5
Bayes Point Machine 90.8 93.7 37.6
Czech Avg. Perceptron 82.9 88.0 30.3
(inc punc) MIRA 83.3 88.6 31.3
Bayes Point Machine 84.0 88.8 30.9
Table 3: Comparison to previous best published results reported in (McDonald et al, 2005a).
Arabic Chinese Czech English
Bayes Point Machine 78.4 83.8 84.5 91.2
Best averaged perceptron 77.9 83.1 83.5 90.8
Worst averaged perceptron 77.4 82.6 83.3 90.5
Table 4: Bayes Point Machine accuracy vs. averaged perceptrons, measured on development test set, ex-
cluding punctuation.
dependency accuracy for five runs at each sample
size up to 5,000 sentences. English and Chinese
accuracies in this graph use oracle part-of-speech
tags. At all sample sizes, the dependency accu-
racy for English exceeds the dependency accuracy
of the other languages. This difference is perhaps
partly attributable to the use of oracle part-of-speech
tags. However, we suspect that the major contribu-
tor to this difference is the part-of-speech tag set.
The tags used in the English Penn Treebank encode
traditional lexical categories such as noun, prepo-
sition, and verb. They also encode morphological
information such as person (the VBZ tag for exam-
ple is used for verbs that are third person, present
tense?typically with the suffix -s), tense, number
and degree of comparison. The part-of-speech tag
sets used for the other languages encode lexical cat-
egories, but do not encode morphological informa-
tion.5 With small amounts of data, the perceptrons
do not encounter sufficient instances of each lexical
item to calculate reliable weights. The perceptrons
are therefore forced to rely on the part-of-speech in-
formation.
It is surprising that the results for Arabic and Chi-
nese should be so close as we vary the size of the
5For Czech and Arabic we followed the convention estab-
lished in previous parsing work on the Prague Czech Depen-
dency Treebank of using the major and minor part-of-speech
tags but ignoring other morphological information annotated on
each node.
training data (Figure 2) given that Arabic has rich
morphology and Chinese very little. One possible
explanation for the similarity in accuracy is that the
rather poor root accuracy in Chinese indicates parses
that have gone awry. Anecdotal inspection of parses
suggests that when the root is not correctly identi-
fied, there are usually cascading related errors.
Czech, a morphologically complex language in
which root identification is far from straightfor-
ward, exhibits the worst performance at small sam-
ple sizes. But (not shown) as the sample size in-
creases, the accuracy of Czech and Chinese con-
verge.
7 Conclusions
We have successfully replicated the state-of-the-art
results for dependency parsing (McDonald et al,
2005a) for both Czech and English, using Bayes
Point Machines. Bayes Point Machines have the ap-
pealing property of simplicity, yet are competitive
with online wide margin methods.
We have also presented first results for depen-
dency parsing of Arabic and Chinese, together with
some analysis of the performance on those lan-
guages.
In future work we intend to explore the discrim-
inative reranking of n-best lists produced by these
parsers and the incorporation of morphological fea-
tures.
166
60
65
70
75
80
85
90
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000Sample size
Depen
dency
 Accu
racy
EnglishChineseArabicCzech
Figure 2: Dependency accuracy at various sample
sizes. Graph shows average of five samples at each
size and measures accuracy against the development
test set.
Acknowledgements
We would like to thank Ryan McDonald, Otakar
Smrz? and Hiroyasu Yamada for help in various
stages of the project.
References
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Processing. Ph.D. the-
sis, University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of EMNLP.
R. O. Duda, P. E. Hart, and D. G. Stork. 2001. Pattern
Classification. John Wiley & Sons, Inc.: New York.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of COLING 1996, pages 340?345.
Claudio Gentile. 2001. A new approximate maximal
margin classification algorithm. Journal of Machine
Learning Research, 2:213?242.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proc. 7th Pacific-Asia
Conference on Knowledge Discovery and Data Min-
ing, pages 241?252.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes point machines. Journal of Machine
Learning Research, pages 245?278.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Assocation for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005b. Online large-margin training of dependency
parsers. Technical Report MS-CIS-05-11, Dept. of
Computer and Information Science, Univ. of Pennsyl-
vania.
Igor A. Melc?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Arul Menezes and Chris Quirk. 2005. Microsoft re-
search treelet translation system: IWSLT evaluation.
In Proceedings of the International Workshop on Spo-
ken Language Translation.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd annual meet-
ing of the Association for Computational Linguistics.
Lucien Tesnie`re. 1959. E?le?ments de syntaxe structurale.
Librairie C. Klincksieck.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2005.
Strictly lexical dependency parsing. In Proceedings
of the Ninth International Workshop on Parsing Tech-
nologies, pages 152?159.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2).
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
167
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 65?68,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Assessing the Costs of Sampling Methods in Active Learning for Annotation
Robbie Haertel, Eric Ringger, Kevin Seppi, James Carroll, Peter McClanahan
Department of Computer Science
Brigham Young University
Provo, UT 84602, USA
robbie haertel@byu.edu, ringger@cs.byu.edu, kseppi@cs.byu.edu,
jlcarroll@gmail.com, petermcclanahan@gmail.com
Abstract
Traditional Active Learning (AL) techniques
assume that the annotation of each datum costs
the same. This is not the case when anno-
tating sequences; some sequences will take
longer than others. We show that the AL tech-
nique which performs best depends on how
cost is measured. Applying an hourly cost
model based on the results of an annotation
user study, we approximate the amount of time
necessary to annotate a given sentence. This
model allows us to evaluate the effectiveness
of AL sampling methods in terms of time
spent in annotation. We acheive a 77% re-
duction in hours from a random baseline to
achieve 96.5% tag accuracy on the Penn Tree-
bank. More significantly, we make the case
for measuring cost in assessing AL methods.
1 Introduction
Obtaining human annotations for linguistic data is
labor intensive and typically the costliest part of the
acquisition of an annotated corpus. Hence, there is
strong motivation to reduce annotation costs, but not
at the expense of quality. Active learning (AL) can
be employed to reduce the costs of corpus annotation
(Engelson and Dagan, 1996; Ringger et al, 2007;
Tomanek et al, 2007). With the assistance of AL,
the role of the human oracle is either to label a da-
tum or simply to correct the label from an automatic
labeler. For the present work, we assume that cor-
rection is less costly than annotation from scratch;
testing this assumption is the subject of future work.
In AL, the learner leverages newly provided anno-
tations to select more informative sentences which
in turn can be used by the automatic labeler to pro-
vide more accurate annotations in future iterations.
Ideally, this process yields accurate labels with less
human effort.
Annotation cost is project dependent. For in-
stance, annotators may be paid for the number of an-
notations they produce or by the hour. In the context
of parse tree annotation, Hwa (2004) estimates cost
using the number of constituents needing labeling
and Osborne & Baldridge (2004) use a measure re-
lated to the number of possible parses. With few ex-
ceptions, previous work on AL has largely ignored
the question of actual labeling time. One excep-
tion is (Ngai and Yarowsky, 2000) (discussed later)
which compares the cost of manual rule writing with
AL-based annotation for noun phrase chunking. In
contrast, we focus on the performance of AL algo-
rithms using different estimates of cost (including
time) for part of speech (POS) tagging, although the
results are applicable to AL for sequential labeling
in general. We make the case for measuring cost in
assessing AL methods by showing that the choice of
a cost function significantly affects the choice of AL
algorithm.
2 Benefit and Cost in Active Learning
Every annotation task begins with a set of un-
annotated items U . The ordered set A ? U con-
sists of all annotated data after annotation is com-
plete or after available financial resources (or time)
have been exhausted. We expand the goal of AL
to produce the annotated set A? such that the benefit
gained is maximized and cost is minimized.
In the case of POS tagging, tag accuracy is usu-
65
ally used as the measure of benefit. Several heuristic
AL methods have been investigated for determining
which data will provide the most information and
hopefully the best accuracy. Perhaps the best known
are Query by Committee (QBC) (Seung et al, 1992)
and uncertainty sampling (or Query by Uncertainty,
QBU) (Thrun and Moeller, 1992). Unfortunately,
AL algorithms such as these ignore the cost term of
the maximization problem and thus assume a uni-
form cost of annotating each item. In this case, the
ordering of annotated dataAwill depend entirely on
the algorithm?s estimate of the expected benefit.
However, for AL in POS tagging, the cost term
may not be uniform. If annotators are required to
change only those automatically generated tags that
are incorrect, and depending on how annotators are
paid, the cost of tagging one sentence can depend
greatly on what is known from sentences already an-
notated. Thus, in POS tagging both the benefit (in-
crease in accuracy) and cost of annotating a sentence
depend not only on properties of the sentence but
also on the order in which the items are annotated.
Therefore, when evaluating the performance of an
AL technique, cost should be taken into account. To
illustrate this, consider some basic AL algorithms
evaluated using several simple cost metrics. The re-
sults are presented against a random baseline which
selects sentences at random; the learning curves rep-
resent the average of five runs starting from a ran-
dom initial sentence. If annotators are paid by the
sentence, Figure 1(a) presents a learning curve in-
dicating that the AL policy that selects the longest
sentence (LS) performs rather well. Figure 1(a) also
shows that given this cost model, QBU and QBC are
essentially tied, with QBU enjoying a slight advan-
tage. This indicates that if annotators are paid by
the sentence, QBU is the best solution, and LS is a
reasonable alternative. Next, Figure 1(b) illustrates
that the results differ substantially if annotators are
paid by the word. In this case, using LS as an AL
policy is worse than random selection. Furthermore,
QBC outperforms QBU. Finally, Figure 1(c) shows
what happens if annotators are paid by the number
of word labels corrected. Notice that in this case, the
random selector marginally outperforms the other
techniques. This is because QBU, QBC, and LS tend
to select data that require many corrections. Con-
sidered together, Figures 1(a)-Figure 1(c) show the
significant impact of choosing a cost model on the
relative performance of AL algorithms. This leads
us to conclude that AL techniques should be eval-
uated and compared with respect to a specific cost
function.
While not all of these cost functions are neces-
sarily used in real-life annotation, each can be re-
garded as an important component of a cost model
of payment by the hour. Since each of these func-
tions depends on factors having a significant effect
on the perceived performance of the various AL al-
gorithms, it is important to combine them in a way
that will accurately reflect the true performance of
the selection algorithms.
In prior work, we describe such a cost model for
POS annotation on the basis of the time required for
annotation (Ringger et al, 2008). We refer to this
model as the ?hourly cost model?. This model is
computed from data obtained from a user study in-
volving a POS annotation task. In the study, tim-
ing information was gathered from many subjects
who annotated both sentences and individual words.
This study included tests in which words were pre-
labeled with a candidate labeling obtained from an
automatic tagger (with a known error rate) as would
occur in the context of AL. Linear regression on the
study data yielded a model of POS annotation cost:
h = (3.795 ? l + 5.387 ? c + 12.57)/3600 (1)
where h is the time in hours spent on the sentence, l
is the number of tokens in the sentence, and c is the
number of words in the sentence needing correction.
For this model, the Relative Standard Error (RSE) is
89.5, and the adjusted correlation (R2) is 0.181. This
model reflects the abilities of the annotators in the
study and may not be representative of annotators in
other projects. However, the purpose of this paper is
to create a framework for accounting for cost in AL
algorithms. In contrast to the model presented by
Ngai and Yarowsky (2000), which predicts mone-
tary cost given time spent, this model estimates time
spent from characteristics of a sentence.
3 Evaluation Methodology and Results
Our test data consists of English prose from the
POS-tagged Wall Street Journal text in the Penn
Treebank (PTB) version 3. We use sections 2-21 as
66
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  500  1000  1500  2000
Ta
g 
Ac
cu
ra
cy
Annotated Sentences
Random
LS
QBU
QBC
(a)
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  20000  40000  60000  80000  100000
Ta
g 
Ac
cu
ra
cy
Annotated Words
Random
LS
QBU
QBC
(b)
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  2000  4000  6000  8000  10000
Ta
g 
Ac
cu
ra
cy
Cumulative Tags Corrected
Random
LS
QBU
QBC
(c)
Figure 1: QBU, LS, QBC, and the random baseline plotted in terms of accuracy versus various cost functions: (a)
number of sentences annotated; (b) number of words annotated; and (c) number of tags corrected.
initially unannotated data. We employ section 24 as
the development test set on which tag accuracy is
computed at the end of every iteration of AL.
For tagging, we employ an order two Maximum
EntropyMarkovModel (MEMM). For decoding, we
found that a beam of size five sped up the decoder
with almost no degradation in accuracy fromViterbi.
The features used in this work are typical for modern
MEMM POS tagging and are mostly based on work
by Toutanova and Manning (2000).
In our implementation, QBU employs a single
MEMM tagger. We approximate the entropy of the
per-sentence tag sequences by summing over per-
word entropy and have found that this approxima-
tion provides equivalent performance to the exact se-
quence entropy. We also consider another selection
algorithm introduced in (Ringger et al, 2007) that
eliminates the overhead of entropy computations al-
together by estimating per-sentence uncertainty with
1 ? P (t?), where t? is the Viterbi (best) tag sequence.
We label this scheme QBUOMM (OMM = ?One
Minus Max?).
Our implementation of QBC employs a commit-
tee of three MEMM taggers to balance computa-
tional cost and diversity, following Tomanek et al
(2007). Each committee member?s training set is a
random bootstrap sample of the available annotated
data, but is otherwise as described above for QBU.
We follow Engelson & Dagan (1996) in the imple-
mentation of vote entropy for sentence selection us-
ing these models.
When comparing the relative performance of AL
algorithms, learning curves can be challenging to in-
terpret. As curves proceed to the right, they can ap-
proach one another so closely that it may be difficult
to see the advantage of one curve over another. For
this reason, we introduce the ?cost reduction curve?.
In such a curve, the accuracy is the independent vari-
able. We then compute the percent reduction in cost
(e.g., number of words or hours) over the cost of the
random baseline for the same accuracy a:
redux(a) = (costrnd(a) ? cost(a))/costrnd(a)
Consequently, the random baseline represents the
trajectory redux(a) = 0.0. Algorithms less costly
than the baseline appear above the baseline. For a
specific accuracy value on a learning curve, the cor-
responding value of the cost on the random baseline
is estimated by interpolation between neighboring
points on the baseline. Using hourly cost, Figure 2
shows the cost reduction curves of several AL al-
gorithms, including those already considered in the
learning curves of Figure 1 (except LS). Restricting
the discussion to the random baseline, QBC, and
QBU: for low accuracies, random selection is the
cheapest according to hourly cost; QBU begins to
be cost-effective at around 91%; and QBC begins to
outperform the baseline and QBU around 80%.
4 Normalized Methods
One approach to convert existing AL algorithms into
cost-conscious algorithms is to normalize the results
of the original algorithm by the estimated cost. It
should be somewhat obvious that many selection al-
gorithms are inherently length-biased for sequence
labeling tasks. For instance, since QBU is the sum
67
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.86  0.88  0.9  0.92  0.94  0.96
R
ed
uc
tio
n 
in
 H
ou
rly
 C
os
t
Tag Accuracy
Random
QBUOMM/N
QBC/N
QBU/N
QBUOMM
QBC
QBU
Figure 2: Cost reduction curves for QBU, QBC,
QBUOMM, their normalized variants, and the random
baseline on the basis of hourly cost
of entropy over all words, longer sentences will tend
to have higher uncertainty. The easiest solution is
to normalize by sentence length, as has been done
previously (Engelson and Dagan, 1996; Tomanek et
al., 2007). This of course assumes that annotators
are paid by the word, which may or may not be true.
Nevertheless, this approach can be justified by the
hourly cost model. Replacing the number of words
needing correction, c, with the product of l (the sen-
tence length) and the accuracy p of the model, equa-
tion 1 can be re-written as the estimate:
h? = ((3.795 + 5.387p) ? l + 12.57)/3600
Within a single iteration of AL, p is constant, so the
cost is approximately proportional to the length of
the sentence. Figure 2 shows that normalized AL al-
gorithms (suffixed with ?/N?) generally outperform
the standard algorithms based on hourly cost (in
contrast to the cost models used in Figures 1(a) -
(c)). All algorithms shown have significant cost
savings over the random baseline for accuracy lev-
els above 92%. Furthermore, all algorithms except
QBU depict trends of further increasing the advan-
tage after 95%. According to the hourly cost model,
QBUOMM/N has an advantage over all other algo-
rithms for accuracies over 91%, achieving a signifi-
cant 77% reduction in cost at 96.5% accuracy.
5 Conclusions
We have shown that annotation cost affects the as-
sessment of AL algorithms used in POS annotation
and advocate the use of a cost estimate that best es-
timates the true cost. For this reason, we employed
an hourly cost model to evaluate AL algorithms for
POS annotation. We have also introduced the cost
reduction plot in order to assess the cost savings pro-
vided by AL. Furthermore, inspired by the notion
of cost, we evaluated normalized variants of well-
known AL algorithms and showed that these vari-
ants out-perform the standard versions with respect
to the proposed hourly cost measure. In future work
we will build better cost-conscious AL algorithms.
References
S. Engelson and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora. In
Proc. of ACL, pages 319?326.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30:253?276.
G. Ngai and D. Yarowsky. 2000. Rule writing or an-
notation: cost-efficient resource usage for base noun
phrase chunking. In Proc. of ACL, pages 117?125.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proc. of HLT-
NAACL, pages 89?96.
E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In Proc. of Linguistic
Annotation Workshop, pages 101?108.
E. Ringger, M. Carmen, R. Haertel, K. Seppi, D. Lond-
sale, P. McClanahan, J. Carroll, and N. Ellison. 2008.
Assessing the costs of machine-assisted corpus anno-
tation through a user study. In Proc. of LREC.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proc. of CoLT, pages 287?
294.
S. Thrun and K. Moeller. 1992. Active exploration in dy-
namic environments. In NIPS, volume 4, pages 531?
538.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts annota-
tion costs and maintains reusability of annotated data.
Proc. of EMNLP-CoNLL, pages 486?495.
K. Toutanova and C. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63?70.
68
Proceedings of the Linguistic Annotation Workshop, pages 101?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
Active Learning for Part-of-Speech Tagging: 
Accelerating Corpus Annotation 
 
Eric Ringger*, Peter McClanahan*, Robbie Haertel*, George Busby*, Marc Carmen**, 
James Carroll*, Kevin Seppi*, Deryle Lonsdale** 
*Computer Science Department; **Linguistics Department 
Brigham Young University 
Provo, Utah, USA 84602 
 
Abstract 
In the construction of a part-of-speech an-
notated corpus, we are constrained by a 
fixed budget. A fully annotated corpus is 
required, but we can afford to label only a 
subset. We train a Maximum Entropy Mar-
kov Model tagger from a labeled subset 
and automatically tag the remainder. This 
paper addresses the question of where to 
focus our manual tagging efforts in order to 
deliver an annotation of highest quality. In 
this context, we find that active learning is 
always helpful. We focus on Query by Un-
certainty (QBU) and Query by Committee 
(QBC) and report on experiments with sev-
eral baselines and new variations of QBC 
and QBU, inspired by weaknesses particu-
lar to their use in this application. Experi-
ments on English prose and poetry test 
these approaches and evaluate their robust-
ness. The results allow us to make recom-
mendations for both types of text and raise 
questions that will lead to further inquiry. 
1 Introduction 
We are operating (as many do) on a fixed budget 
and need annotated text in the context of a larger 
project. We need a fully annotated corpus but can 
afford to annotate only a subset. To address our 
budgetary constraint, we train a model from a ma-
nually annotated subset of the corpus and automat-
ically annotate the remainder. At issue is where to 
focus manual annotation efforts in order to produce 
a complete annotation of highest possible quality. 
A follow-up question is whether these techniques 
work equally well on different types of text. 
In particular, we require part-of-speech (POS) 
annotations. In this paper we employ a state-of-the-
art tagger on both prose and poetry, and we ex-
amine multiple known and novel active learning 
(or sampling) techniques in order to determine 
which work best in this context. We show that the 
results obtained by a state-of-the-art tagger trained 
on a small portion of the data selected through ac-
tive learning can approach the accuracy attained by 
human annotators and are on par with results from 
exhaustively trained automatic taggers. 
In a study based on English language data pre-
sented here, we identify several active learning 
techniques and make several recommendations that 
we hope will be portable for application to other 
text types and to other languages. In section 2 we 
briefly review the state of the art approach to POS 
tagging. In section 3, we survey the approaches to 
active learning employed in this study, including 
variations on commonly known techniques. Sec-
tion 4 introduces the experimental regime and 
presents results and their implications. Section 5 
draws conclusions and identifies opportunities for 
follow-up research. 
2 Part of Speech Tagging 
Labeling natural language data with part-of-speech 
tags can be a complicated task, requiring much 
effort and expense, even for trained annotators. 
Several efforts, notably the Alembic workbench 
(Day et al, 1997) and similar tools, have provided 
interfaces to aid annotators in the process.  
Automatic POS tagging of text using probabilis-
tic models is mostly a solved problem but requires 
supervised learning from substantial amounts of 
training data. Previous work demonstrates the sui-
tability of Hidden Markov Models for POS tagging 
(Kupiec, 1992; Brants, 2000). More recent work 
has achieved state-of-the-art results with Maxi-
101
mum entropy conditional Markov models (MaxEnt 
CMMs, or MEMMs for short) (Ratnaparkhi, 1996; 
Toutanova & Manning, 2000; Toutanova et al, 
2003). Part of the success of MEMMs can be attri-
buted to the absence of independence assumptions 
among predictive features and the resulting ease of 
feature engineering. To the best of our knowledge, 
the present work is the first to present results using 
MEMMs in an active learning framework.  
An MEMM is a probabilistic model for se-
quence labeling. It is a Conditional Markov Model 
(CMM as illustrated in Figure 1) in which a Max-
imum Entropy (MaxEnt) classifier is employed to 
estimate the probability distribution
1.. 1 1 2( | , ) ( | , , , )i i ME i i i i ip t w t p t w f t t? ? ?? over 
possible labels it  for each element in the se-
quence?in our case, for each word iw  in a sen-
tence w . The MaxEnt model is trained from la-
beled data and has access to any predefined 
attributes (represented here by the collection if ) of 
the entire word sequence and to the labels of pre-
vious words ( 1.. 1it ? ). Our implementation employs 
an order-two Markov assumption so the classifier 
has access only to the two previous tags 1 2,i it t? ? . 
We refer to the features 1 2( , , , )i i i iw f t t? ? from 
which the classifier predicts the distribution over 
tags as ?the local trigram context?. 
A Viterbi decoder is a dynamic programming 
algorithm that applies the MaxEnt classifier to 
score multiple competing tag-sequence hypotheses 
efficiently and to produce the best tag sequence, 
according to the model. We approximate Viterbi 
very closely using a fast beam search. Essentially, 
the decoding process involves sequential classifi-
cation, conditioned on the (uncertain) decisions of 
the previous local trigram context classifications. 
The chosen tag sequence t? is the tag sequence 
maximizing the following quantity: 
1 2
1..
? arg max ( | )
arg max ( | , , , )
t
t ME i i i i i
i n
t P t w
p t w f t t? ?
=
=
= ?  
The features used in this work are reasonably 
typical for modern MEMM feature-based POS 
tagging and consist of a combination of lexical, 
orthographic, contextual, and frequency-based in-
formation. In particular, for each word the follow-
ing features are defined: the textual form of the 
word itself, the POS tags of the preceding two 
words, and the textual form of the following word. 
Following Toutanova and Manning (2000) approx-
imately, more information is defined for words that 
are considered rare (which we define here as words 
that occur fewer than fifteen times). We consider 
the tagger to be near-state-of-the-art in terms of 
tagging accuracy. 
 
Figure 1. Simple Markov order 2 CMM, with focus on 
the i-th hidden label (or tag). 
3 Active Learning 
The objective of this research is to produce more 
high quality annotated data with less human anno-
tator time and effort. Active learning is an ap-
proach to machine learning in which a model is 
trained with the selective help of an oracle. The 
oracle provides labels on a sufficient number of 
?tough? cases, as identified by the model. Easy 
cases are assumed to be understood by the model 
and to require no additional annotation by the 
oracle. Many variations have been proposed in the 
broader active learning and decision theory litera-
ture under many names, including ?active sam-
pling? and ?optimal sampling.? 
In active learning for POS tagging, as in other 
applications, the oracle can be a human. For expe-
rimental purposes, a human oracle is simulated 
using pre-labeled data, where the labels are hidden 
until queried. To begin, the active learning process 
requires some small amount of training data to 
seed the model. The process proceeds by identify-
ing the data in the given corpus that should be 
tagged first for maximal impact. 
3.1 Active Learning in the Language Context 
When considering the role of active learning, we 
were initially drawn to the work in active learning 
for classification. In a simple configuration, each 
instance (document, image, etc.) to be labeled can 
be considered to be independent. However, for ac-
tive learning for the POS tagging problem we con-
sidered the nature of human input as an oracle for 
the task. As an approximation, people read sen-
tences as propositional atoms, gathering contextual 
cues from the sentence in order to assemble the 
102
meaning of the whole. Consequently, we thought it 
unreasonable to choose the word as the granularity 
for active learning. Instead, we begin with the as-
sumption that a human will usually require much 
of the sentence or at least local context from the 
sentence in order to label a single word with its 
POS label. While focusing on a single word, the 
human may as well label the entire sentence or at 
least correct the labels assigned by the tagger for 
the sentence. Consequently, the sentence is the 
granularity of annotation for this work. (Future 
work will question this assumption and investigate 
tagging a word or a subsequence of words at a 
time.) This distinguishes our work from active 
learning for classification since labels are not 
drawn from a fixed set of labels. Rather, every sen-
tence of length n can be labeled with a tag se-
quence drawn from a set of size nT , where T  is 
the size of the per-word tag set. Granted, many of 
the options have very low probability. 
To underscore our choice of annotating at the 
granularity of a sentence, we also note that a max-
imum entropy classifier for isolated word tagging 
that leverages attributes of neighboring words?
but is blind to all tags?will underperform an 
MEMM that includes the tags of neighboring 
words (usually on the left) among its features. Pre-
vious experiments demonstrate the usefulness of 
tags in context on the standard Wall Street Journal 
data from the Penn Treebank (Marcus et al, 1999). 
A MaxEnt isolated word tagger achieves 93.7% on 
words observed in the training set and 82.6% on 
words unseen in the training set. Toutanova and 
Manning (2000) achieves 96.9% (on seen) and 
86.9% (on unseen) with an MEMM. They sur-
passed their earlier work in 2003 with a ?cyclic 
dependency network tagger?, achieving 
97.2%/89.05% (seen/unseen) (Toutanova et al, 
2003). The generally agreed upon upper bound is 
around 98%, due to label inconsistencies in the 
Treebank. The main point is that effective use of 
contextual features is necessary to achieve state of 
the art performance in POS tagging. 
In active learning, we employ several sets of 
data that we refer to by the following names: 
? Initial Training: the small set of data used 
to train the original model before active 
learning starts 
? Training: data that has already been la-
beled by the oracle as of step i in the learn-
ing cycle 
? Unannotated: data not yet labeled by the 
oracle as of step i 
? Test (specifically Development Test): la-
beled data used to measure the accuracy of 
the model at each stage of the active learn-
ing process. Labels on this set are held in 
reserve for comparison with the labels 
chosen by the model. It is the accuracy on 
this set that we report in our experimental 
results in Section 4. 
Note that the Training set grows at the expense of 
the Unannotated set as active learning progresses. 
Active Learning for POS Tagging consists of the 
following steps: 
1. Train a model with Initial Training data 
2. Apply model to Unannotated data 
3. Compute potential informativeness of 
each sentence 
4. Remove top n sentences with most po-
tential informativeness from Unanno-
tated data and give to oracle 
5. Add n sentences annotated (or corrected) 
by the oracle to Training data 
6. Retrain model with Training data 
7. Return to step 2 until stopping condition 
is met. 
There are several possible stopping conditions, 
including reaching a quality bar based on accuracy 
on the Test set, the rate of oracle error corrections 
in the given cycle, or even the cumulative number 
of oracle error corrections. In practice, the exhaus-
tion of resources, such as time or money, may 
completely dominate all other desirable stopping 
conditions. 
Several methods are available for determining 
which sentences will provide the most information. 
Expected Value of Sample Information (EVSI) 
(Raiffa & Schlaiffer, 1967) would be the optimal 
approach from a decision theoretic point of view, 
but it is computationally prohibitive and is not con-
sidered here. We also do not consider the related 
notion of query-by-model-improvement or other 
methods (Anderson & Moore, 2005; Roy & 
McCallum, 2001a, 2001b). While worth exploring, 
they do not fit in the context of this current work 
and should be considered in future work. We focus 
here on the more widely used Query by Committee 
(QBC) and Query by Uncertainty (QBU), includ-
ing our new adaptations of these. 
Our implementation of maximum entropy train-
ing employs a convex optimization procedure 
known as LBFGS. Although this procedure is rela-
tively fast, training a model (or models in the case 
103
of QBC) from scratch on the training data during 
every round of the active learning loop would pro-
long our experiments unnecessarily. Instead we 
start each optimization search with a parameter set 
consisting of the model parameters from the pre-
vious iteration of active learning (we call this ?Fast 
MaxEnt?). In practice, this converges quickly and 
produces equivalent results. 
3.2 Query by Committee 
Query by Committee (QBC) was introduced by 
Seung, Opper, and Sompolinsky (1992). Freund, 
Seung, Shamir, and Tishby (1997) provided a care-
ful analysis of the approach. Engelson and Dagan 
(1996) experimented with QBC using HMMs for 
POS tagging and found that selective sampling of 
sentences can significantly reduce the number of 
samples required to achieve desirable tag accura-
cies. Unlike the present work, Engelson & Dagan 
were restricted by computational resources to se-
lection from small windows of the Unannotated set, 
not from the entire Unannotated set. Related work 
includes learning ensembles of POS taggers, as in 
the work of Brill and Wu (1998), where an ensem-
ble consisting of a unigram model, an N-gram 
model, a transformation-based model, and an 
MEMM for POS tagging achieves substantial re-
sults beyond the individual taggers. Their conclu-
sion relevant to this paper is that different taggers 
commit complementary errors, a useful fact to ex-
ploit in active learning. QBC employs a committee 
of N models, in which each model votes on the 
correct tagging of a sentence. The potential infor-
mativeness of a sentence is measured by the total 
number of tag sequence disagreements (compared 
pair-wise) among the committee members. Possi-
ble variants of QBC involve the number of com-
mittee members, how the training data is split 
among the committee members, and whether the 
training data is sampled with or without replace-
ment. 
A potential problem with QBC in this applica-
tion is that words occur with different frequencies 
in the corpus. Because of the potential for greater 
impact across the corpus, querying for the tag of a 
more frequent word may be more desirable than 
querying for the tag of a word that occurs less fre-
quently, even if there is greater disagreement on 
the tags for the less frequent word. We attempted 
to compensate for this by weighting the number of 
disagreements by the corpus frequency of the word 
in the full data set (Training and Unannotated). 
Unfortunately, this resulted in worse performance; 
solving this problem is an interesting avenue for 
future work. 
3.3 Query by Uncertainty 
The idea behind active sampling based on uncer-
tainty appears to originate with Thrun and Moeller 
(1992). QBU has received significant attention in 
general. Early experiments involving QBU were 
conducted by Lewis and Gale (1994) on text classi-
fication, where they demonstrated significant bene-
fits of the approach. Lewis and Catlett (1994) ex-
amined its application for non-probabilistic learn-
ers in conjunction with other probabilistic learners 
under the name ?uncertainty sampling.? Brigham 
Anderson (2005) explored QBU using HMMs and 
concluded that it is sometimes advantageous. We 
are not aware of any published work on the appli-
cation of QBU to POS tagging. In our implementa-
tion, QBU employs a single MEMM tagger. The 
MaxEnt model comprising the tagger can assess 
the probability distribution over tags for any word 
in its local trigram context, as illustrated in the ex-
ample in Figure 2. 
Figure 2. Distribution over tags for the word ?hurdle? in 
italics. The local trigram context is in boldface. 
In Query by Uncertainty (QBU), the informa-
tiveness of a sample is assumed to be the uncer-
tainty in the predicted distribution over tags for 
that sample, that is the entropy of 
1 2( | , , , )ME i i i i ip t w f t t? ? . To determine the poten-
tial informativeness of a word, we can measure the 
entropy in that distribution. Since we are selecting 
sentences, we must extend our measure of uncer-
tainty beyond the word. 
3.4 Adaptations of QBU 
There are several problems with the use of QBU in 
this context: 
? Some words are more important; i.e., they 
contain more information perhaps because 
they occur more frequently. 
   NN 0 .85 
   VB  0.13 
   ... 
RB    DT JJS CD  2.0E-7 
 
Perhaps     the biggest   hurdle ? 
104
? MaxEnt estimates per-word distributions 
over tags, not per-sentence distributions 
over tag sequences. 
? Entropy computations are relatively costly. 
We address the first issue in a new version of QBU 
which we call ?Weighted Query by Uncertainty? 
(WQBU). In WQBU, per-word uncertainty is 
weighted by the word's corpus frequency. 
To address the issue of estimating per-sentence 
uncertainty from distributions over tag sequences, 
we have considered several different approaches. 
The per-word (conditional) entropy is defined as 
follows: 
 
 
 
 
 
 
where iT  is the random variable for the tag it  on 
word iw , and the features of the context in which 
iw  occurs are denoted, as before, by the collection 
if  and the prior tags 1 2,i it t? ? . It is straightforward 
to calculate this entropy for each word in a sen-
tence from the Unannotated set, if we assume that 
previous tags 1 2,i it t? ?  are from the Viterbi (best) 
tag sequence (for the entire sentence) according to 
the model. 
For an entire sentence, we estimate the tag-
sequence entropy by summing over all possible tag 
sequences. However, computing this estimate ex-
actly on a 25-word sentence, where each word can 
be labeled with one of 35 tags, would require 3525 
= 3.99*1038 steps. Instead, we approximate the per-
sentence tag sequence distribution entropy by 
summing per-word entropy: 
 
 
This is the approach we refer to as QBU in the 
experimental results section. We have experi-
mented with a second approach that estimates the 
per-sentence entropy of the tag-sequence distribu-
tion by Monte Carlo decoding. Unfortunately, cur-
rent active learning results involving this MC POS 
tagging decoder are negative on small Training set 
sizes, so we do not present them here. Another al-
ternative approximation worth pursuing is compu-
ting the per-sentence entropy using the n-best POS 
tag sequences. Very recent work by Mann and 
McCallum (2007) proposes an approach in which 
exact sequence entropy can be calculated efficient-
ly. Further experimentation is required to compare 
our approximation to these alternatives. 
An alternative approach that eliminates the 
overhead of entropy computations entirely is to 
estimate per-sentence uncertainty with ?1 ( )P t? , 
where t? is the Viterbi (best) tag sequence. We call 
this scheme QBUV. In essence, it selects a sample 
consisting of the sentences having the highest 
probability that the Viterbi sequence is wrong. To 
our knowledge, this is a novel approach to active 
learning. 
4 Experimental Results 
In this section, we examine the experimental setup, 
the prose and poetry data sets, and the results from 
using the various active learning algorithms on 
these corpora. 
4.1 Setup 
The experiments focus on the annotation scenario 
posed earlier, in which budgetary constraints af-
ford only some number x of sentences to be anno-
tated. The x-axis in each graph captures the num-
ber of sentences. For most of the experiments, the 
graphs present accuracies on the (Development) 
Test set. Later in this section, we present results for 
an alternate metric, namely number of words cor-
rected by the oracle. 
In order to ascertain the usefulness of the active 
learning approaches explored here, the results are 
presented against a baseline in which sentences are 
selected randomly from the Unannotated set. We 
consider this baseline to represent the use of a 
state-of-the-art tagger trained on the same amount 
of data as the active learner. Due to randomization, 
the random baseline is actually distinct from expe-
riment to experiment without any surprising devia-
tions. Also, each result curve in each graph 
represents the average of three distinct runs. 
Worth noting is that most of the graphs include 
active learning curves that are run to completion; 
namely, the rightmost extent of all curves 
represents the exhaustion of the Unannotated data. 
At this extreme point, active learning and random 
sample selection all have the same Training set. In 
the scenarios we are targeting, this far right side is 
not of interest. Points representing smaller amounts 
of annotated data are our primary interest. 
In the experiments that follow, we address sev-
eral natural questions that arise in the course of 
applying active learning. We also compare the va-
1 2
1 2
1 2
( | , , , )
( | , , , )
log ( | , , , )
i
i i i i i
ME i i i i i
t Tagset
ME i i i i i
H T w f t t
p t w f t t
p t w f t t
? ?
? ?
?
? ?
= ?
?
?
1 2
? ( | ) ( | , , , )
i
i i i i i
w w
H T w H T w f t t? ?
?
? ??
105
riants of QBU and QBC. For QBC, committee 
members divide the training set (at each stage of 
the active learning process) evenly. All committee 
members and final models are MEMMs. Likewise, 
all variants of QBU employ MEMMs. 
4.2 Data Sets 
The experiments involve two data sets in search 
of conclusions that generalize over two very dif-
ferent kinds of English text. The first data set con-
sists of English prose from the POS-tagged one-
million-word Wall Street Journal text in the Penn 
Treebank (PTB) version 3. We use a random sam-
ple of the corpus constituting 25% of the tradition-
al training set (sections 2?21). Initial Training data 
consists of 1% of this set. We employ section 24 as 
the Development Test set. Average sentence length 
is approximately 25 words. 
Our second experimental set consists of English 
poetry from the British National Corpus (BNC) 
(Godbert & Ramsay, 1991; Hughes, 1982; Raine, 
1984). The text is also fully tagged with 91 parts of 
speech from a different tag set than the one used 
for the PTB. The BNC XML data was taken from 
the files B1C.xml, CBO.xml, and H8R.xml. This 
results in a set of 60,056 words and 8,917 sen-
tences. 
4.3 General Results 
To begin, each step in the active learning process 
adds a batch of 100 sentences from the Unanno-
tated set at a time. Figure 3 demonstrates (using 
QBU) that the size of a query batch is not signifi-
cant in these experiments.  
The primary question to address is whether ac-
tive learning helps or not. Figure 4 demonstrates 
that QBU, QBUV, and QBC all outperform the 
random baseline in terms of total, per-word accu-
racy on the Test set, given the same amount of 
Training data. Figure 5 is a close-up version of 
Figure 4, placing emphasis on points up to 1000 
annotated sentences. In these figures, QBU and 
QBUV vie for the best performing active learning 
algorithm. These results appear to give some useful 
advice captured in Table 1. The first column in the 
table contains the starting conditions. The remain-
ing columns indicate that for between 800-1600 
sentences of annotation, QBUV takes over from 
QBU as the best selection algorithm. 
The next question to address is how much initial 
training data should be used; i.e., when should we 
start using active learning? The experiment in Fig-
ure 6 demonstrates (using QBU) that one should 
use as little data as possible for Initial Training 
Data. There is always a significant advantage to 
starting early. In the experiment documented in  
 
Figure 3. Varying the size of the query batch in active 
learning yields identical results after the first query batch.  
 
Figure 4. The best representatives of each type of active 
learner beat the baseline. QBU and QBUV trade off the 
top position over QBC and the Baseline. 
Figure 5. Close-up of the low end of the graph from Figure 
4. QBUV and QBU are nearly tied for best performance. 
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
Batch Query Size of 10 Sentences
Batch Query Size of 100 Sentences
Batch Query Size of 500 Sentences
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
ur
ac
y 
(%
)
Number of Sentences in Training Set
QBUV 
QBU
QBC
Baseline
 76
 78
 80
 82
 84
 86
 88
 90
 92
 100  1000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBUV 
QBU
QBC
Baseline
106
this figure, a batch query size of one was employed 
in order to make the point as clearly as possible. 
Larger batch query sizes produce a graph with sim-
ilar trends as do experiments involving larger Un-
annotated sets and other active learners. 
 
 100 200 400 800 1600 3200 6400 
QBU 76.26 86.11 90.63 92.27 93.67 94.65 95.42 
QBUV 76.65 85.09 89.75 92.24 93.72 94.96 95.60 
QBC 76.19 85.77 89.37 91.78 93.49 94.62 95.36 
Base 76.57 82.13 86.68 90.12 92.49 94.02 95.19 
Table 1. The best models (on PTB WSJ data) with various 
amounts of annotation (columns). 
 
Figure 6. Start active learning as early as possible for a 
head start. 
4.4 QBC Results 
An important question to address for QBC is 
what number of committee members produces the 
best results? There was no significant difference in 
results from the QBC experiments when using be-
tween 3 and 7 committee members. For brevity we 
omit the graph. 
4.5 QBU Results 
For Query by Uncertainty, the experiment in Fig-
ure 7 demonstrates that QBU is superior to QBUV 
for low counts, but that QBUV slightly overtakes 
QBU beyond approximately 300 sentences. In fact, 
all QBU variants, including the weighted version, 
surpassed the baseline. WQBU has been omitted 
from the graph, as it was inferior to straight-
forward QBU. 
4.6 Results on the BNC 
Next we introduce results on poetry from the Brit-
ish National Corpus. Recall that the feature set 
employed by the MEMM tagger was optimized for 
performance on the Wall Street Journal. For the 
experiment presented in Figure 8, all data in the 
Training and Unannotated sets is from the BNC, 
but we employ the same feature set from the WSJ 
experiments. This result on the BNC data shows 
first of all that tagging poetry with this tagger 
leaves a final shortfall of approximately 8% from 
the WSJ results. Nonetheless and more importantly, 
the active learning trends observed on the WSJ still 
hold. QBC is better than the baseline, and QBU 
and QBUV trade off for first place. Furthermore, 
for low numbers of sentences, it is overwhelmingly 
to one?s advantage to employ active learning for 
annotation. 
 
 
Figure 7. QBUV is superior to QBU overall, but QBU is 
better for very low counts. Both are superior to the ran-
dom baseline and the Longest Sentence (LS) baseline. 
 
Figure 8. Active learning results on the BNC poetry data. 
Accuracy of QBUV, QBU, and QBC against the random 
baseline. QBU and QBUV are nearly indistinguishable. 
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 10  100
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
1%
5%
10%
25%
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBU
QBUV
LS
Baseline 
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBU
QBUV
Baseline
QBC
107
4.7 Another Perspective 
Next, briefly consider a different metric on the ver-
tical axis. In Figure 9, the metric is the total num-
ber of words changed (corrected) by the oracle. 
This quantity reflects the cumulative number of 
differences between the tagger?s hypothesis on a 
sentence (at the point in time when the oracle is 
queried) and the oracle?s answer (over the training 
set). It corresponds roughly to the amount of time 
that would be required for a human annotator to 
correct the tags suggested by the model. This fig-
ure reveals that QBUV makes significantly more 
changes than QBU, QBC, or LS (the Longest Sen-
tence baseline). Hence, the superiority of QBU 
over QBUV, as measured by this metric, appears to 
outweigh the small wins provided by QBUV when 
measured by accuracy alone. That said, the random 
baseline makes the fewest changes of all. If this 
metric (and not some combination with accuracy) 
were our only consideration, then active learning 
would appear not to serve our needs. 
This metric is also a measure of how well a par-
ticular query algorithm selects sentences that espe-
cially require assistance from the oracle. In this 
sense, QBUV appears most effective. 
 
Figure 9. Cumulative number of corrections made by the 
oracle for several competitive active learning algorithms. 
QBU requires fewer corrections than QBUV. 
5 Conclusions 
Active learning is a viable way to accelerate the 
efficiency of a human annotator and is most effec-
tive when done as early as possible. We have pre-
sented state-of-the-art tagging results using a frac-
tion of the labeled data. QBUV is a cheap approach 
to performing active learning, only to be surpassed 
by QBU when labeling small numbers of sentences. 
We are in the midst of conducting a user study to 
assess the true costs of annotating a sentence at a 
time or a word at a time. We plan to incorporate 
these specific costs into a model of cost measured 
in time (or money) that will supplant the metrics 
reported here, namely accuracy and number of 
words corrected. As noted earlier, future work will 
also evaluate active learning at the granularity of a 
word or a subsequence of words, to be evaluated 
by the cost metric. 
References 
Anderson, B., and Moore, A. (2005). ?Active Learning for HMM: 
Objective Functions and Algorithms.? ICML, Germany. 
Brants, T., (2000). ?TnT -- a statistical part-of-speech tagger.? ANLP, 
Seattle, WA. 
Brill, E., and Wu, J. (1998). ?Classifier combination for improved 
lexical disambiguation.? Coling/ACL, Montreal, Quebec, Canada. 
Pp. 191-195.  
Day, D., et al (1997). ?Mixed-Initiative Development of Language 
Processing Systems.? ANLP, Washington, D.C. 
Engelson, S. and Dagan, I. (1996). ?Minimizing manual annotation 
cost in supervised training from corpora.? ACL, Santa Cruz, Cali-
fornia. Pp. 319-326. 
Freund, Y., Seung, H., Shamir, E., and Tishby, N. (1997). ?Selective 
sampling using the query by committee algorithm.? Machine 
Learning, 28(2-3):133-168.  
Godbert, G. and Ramsay, J. (1991). ?For now.? In the British National 
Corpus file B1C.xml. London: The Diamond Press (pp. 1-108).  
Hughes, T. (1982). ?Selected Poems.? In the British National Corpus 
file H8R.xml. London: Faber & Faber Ltd. (pp. 35-235).  
Kupiec, J. (1992). ?Robust part-of-speech tagging using a hidden 
Markov model.? Computer Speech and Language 6, pp. 225-242. 
Lewis, D., and Catlett, J. (1994). ?Heterogeneous uncertainty sam-
pling for supervised learning.? ICML. 
Lewis, D., and Gale, W. (1995). ?A sequential algorithm for training 
text classifiers: Corrigendum and additional data.? SIGIR Forum, 
29 (2), 13--19. 
Mann, G., and McCallum, A. (2007). "Efficient Computation of En-
tropy Gradient for Semi-Supervised Conditional Random Fields". 
NAACL-HLT. 
Marcus, M. et al (1999). ?Treebank-3.? Linguistic Data Consortium, 
Philadelphia, PA. 
Raiffa, H. and Schlaiffer, R. (1967). Applied Statistical Decision 
Theory. New York: Wiley Interscience.  
Raine, C. (1984). ?Rich.? In the British National Corpus file CB0.xml. 
London: Faber & Faber Ltd. (pp. 13-101).  
Ratnaparkhi, A. (1996). ?A Maximum Entropy Model for Part-Of-
Speech Tagging.? EMNLP. 
Roy, N., and McCallum, A. (2001a). ?Toward optimal active learning 
through sampling estimation of error reduction.? ICML. 
Roy, N. and McCallum, A. (2001b). ?Toward Optimal Active Learn-
ing through Monte Carlo Estimation of Error Reduction.? ICML, 
Williamstown. 
Seung, H., Opper, M., and Sompolinsky, H. (1992). ?Query by com-
mittee?.  COLT. Pp. 287-294. 
Thrun S., and Moeller, K. (1992). ?Active exploration in dynamic 
environments.? NIPS.  
Toutanova, K., Klein, D., Manning, C., and Singer, Y. (2003). ?Fea-
ture-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-
work.? HLT-NAACL. Pp. 252-259. 
Toutanova, K. and Manning, C. (2000). ?Enriching the Knowledge 
Sources Used in a Maximum Entropy Part-of-Speech Tagger.? 
EMNLP, Hong Kong. Pp. 63-70. 
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 10000
 100  1000  10000
N
u
m
b
er
 o
f 
C
h
an
g
ed
 W
o
rd
s
Number of Sentences in Training Set
QBUV 
QBU 
QBC 
Baseline 
LS 
108
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 240?250,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Evaluating Models of Latent Document Semantics
in the Presence of OCR Errors
Daniel D. Walker, William B. Lund, and Eric K. Ringger
Brigham Young University
Provo, Utah, USA
danl4@cs.byu.edu, bill lund@byu.edu, ringger@cs.byu.edu
Abstract
Models of latent document semantics such as
the mixture of multinomials model and La-
tent Dirichlet Allocation have received sub-
stantial attention for their ability to discover
topical semantics in large collections of text.
In an effort to apply such models to noisy
optical character recognition (OCR) text out-
put, we endeavor to understand the effect
that character-level noise can have on unsu-
pervised topic modeling. We show the ef-
fects both with document-level topic analy-
sis (document clustering) and with word-level
topic analysis (LDA) on both synthetic and
real-world OCR data. As expected, experi-
mental results show that performance declines
as word error rates increase. Common tech-
niques for alleviating these problems, such as
filtering low-frequency words, are successful
in enhancing model quality, but exhibit fail-
ure trends similar to models trained on unpro-
cessed OCR output in the case of LDA. To our
knowledge, this study is the first of its kind.
1 Introduction
As text data becomes available in massive quanti-
ties, it becomes increasingly difficult for human cu-
rators to manually catalog and index modern docu-
ment collections. To aid in the automation of such
tasks, algorithms can be used to create models of
the latent semantics present in a given corpus. One
example of this type of analysis is document cluster-
ing, in which documents are grouped into clusters
by topic. Another type of topic analysis attempts
to discover finer-grained topics?labeling individual
words in a document as belonging to a particular
topic. This type of analysis has grown in popular-
ity recently as inference on models containing large
numbers of latent variables has become feasible.
The modern explosion of data includes vast
amounts of historical documents, made available
by means of Optical Character Recognition (OCR),
which can introduce significant numbers of er-
rors. Undertakings to produce such data include
the Google Books, Internet Archive, and HathiTrust
projects. In addition, researchers are having increas-
ing levels of success in digitizing hand-written man-
uscripts (Bunke, 2003), though error rates remain
much higher than for OCR. Due to their nature, these
collections often lack helpful meta-data or labels. In
the absence of such labels, unsupervised machine
learning methods can reveal patterns in the data.
Finding good estimates for the parameters of
models such as the mixture of multinomials docu-
ment model (Walker and Ringger, 2008) and the La-
tent Dirichlet Allocation (LDA) model (Blei et al,
2003) requires accurate counts of the occurrences
and co-occurrences of words. Depending on the
age of a document and the way in which it was
created, the OCR process results in text containing
many types of noise, including character-level er-
rors, which distort these counts. It is obvious, there-
fore, that model quality must suffer, especially since
unsupervised methods are typically much more sen-
sitive to noise than supervised methods. Good su-
pervised learning algorithms are substantially more
immune to spurious patterns in the data created by
noise for the following reason: under the mostly
reasonable assumption that the process contributing
the noise operates independently from the class la-
bels, the noise in the features will not correlate well
with the class labels, and the algorithm will learn
240
to ignore those features arising from noise. Unsu-
pervised models, in contrast, have no grounding in
labels to prevent them from confusing patterns that
emerge by chance in the noise with the ?true? pat-
terns of potential interest. For example, even on
clean data, LDA will often do poorly if the very sim-
ple feature selection step of removing stop-words is
not performed first. Though we expect model qual-
ity to decrease, it is not well understood how sensi-
tive these models are to OCR errors, or how quality
deteriorates as the level of OCR noise increases.
In this work we show how the performance of un-
supervised topic modeling algorithms degrades as
character-level noise is introduced. We demonstrate
the effect using both artificially corrupted data and
an existing real-world OCR corpus. The results are
promising, especially in the case of relatively low
word error rates (e.g. less than 20%). Though model
quality declines as errors increase, simple feature se-
lection techniques enable the learning of relatively
high quality models even as word error rates ap-
proach 50%. This result is particularly interesting
in that even humans find it difficult to make sense
of documents with error rates of that magnitude
(Munteanu et al, 2006).
Because of the difficulties in evaluating topic
models, even on clean data, these results should not
be interpreted as definitive answers, but they do offer
insight into prominent trends. For example, proper-
ties of the OCR data suggest measures that can be
taken to improve performance in future work. It is
our hope that this work will lead to an increase in
the usefulness of collections of OCRed texts, as doc-
ument clustering and topic modeling expose useful
patterns to historians and other interested parties.
The remainder of the paper is outlined as follows.
After an overview of related work in Section 2, Sec-
tion 3 introduces the data used in our experiments,
including an explanation of how the synthetic data
were created and of some of their properties. Sec-
tion 4 describes how the experiments were designed
and carried out, and gives an analysis of the results
both empirically and qualitatively. Finally, conclu-
sions and future work are presented in Section 5.
2 Related Work
Topic models have been used previously to process
documents digitized by OCR, including eighteenth-
century American newspapers (Newmann and
Block, 2006), OCRed editions of Science (Blei and
Lafferty, 2006), OCRed NIPS papers (Wang and
McCallum, 2006), and books digitized by the Open
Content Alliance (Mimno and Mccallum, 2007).
Most of this previous work ignores the presence of
OCR errors or attempts to remove corrupted tokens
with special pre-processing such as stop-word re-
moval and frequency cutoffs. Also, there are at least
two instances of using topic modeling to improve
the results of an OCR algorithm (Wick et al, 2007;
Farooq et al, 2009).
Similar evaluations to ours have been conducted
to assess the effect of OCR errors on supervised doc-
ument classification (Taghva et al, 2001; Agarwal et
al., 2007), information retrieval (Taghva et al, 1994;
Beitzel et al, 2003), and a more general set of natu-
ral language processing tasks (Lopresti, 2008). Re-
sults suggest that in these supervised tasks OCR er-
rors have a minimal impact on the performance of
the methods employed, though it has remained un-
clear how well these results transfer to unsupervised
methods.
3 Data
We conducted experiments on synthetic and real
OCR data. As a real-world dataset, we used a cor-
pus consisting of 604 of the Eisenhower World War
II communique?s (Jordan, 1945; Lund and Ringger,
2009). These documents relate the daily progress
of the Allied campaign from D-Day until the Ger-
man surrender. They were originally produced as
telegrams and were distributed as mimeographed
copies. The quality of the originals is often quite
poor, making them a challenging case for OCR en-
gines. The communique?s have been OCRed using
three popular OCR engines: ABBYY FineReader
(ABBYY, 2010), OmniPage Pro (Nuance Commu-
nications, Inc., 2010), and Tesseract (Google, Inc.,
2010). In addition, the curator of the collection has
created a ?gold standard? transcription, from which
it is possible to obtain accurate measures of average
document word error rates (WER) for each engine,
which are: 19.9%, 30.4%, and 50.1% respectively.
241
While the real-world data is attractive as an ex-
ample of just the sort of data that the questions ad-
dressed here apply to, it is limited in size and scope.
All of the documents in the Eisenhower corpus dis-
cuss the fairly narrow topic of troop movements and
battle developments taking place at the end of World
War II. Neither the subject matter nor the means of
conveyance allowed for a large or diverse vocabu-
lary of discourse.
In an attempt to generalize our results to larger
and more diverse data, we also ran experiments
using synthetic OCR data. This synthetic data
was created by corrupting ?clean? datasets, adding
character-level noise. The synthetic data was cre-
ated by building a noise model based on mistakes
made by the worst performing OCR engine on the
Eisenhower dataset, Tesseract.
To construct the noise model, a character-level
alignment between the human transcribed Eisen-
hower documents and the OCR output was first com-
puted. From this alignment, the contingency table
Md was generated such that Mdx,y was the count of
the instances in which a character x in the transcript
was aligned with a y in the OCR output. The rows
in this matrix were then normalized so that each rep-
resented the parameters of a categorical distribution,
conditioned on x. To parameterize the amount of
noise being generated, the Md matrix was interpo-
lated with an identity matrix I using a parameter ? so
that the final interpolated parameters Mi were cal-
culated with the formula Mi = ?Md + (1 ? ?)I.
So that at ? = 0, Mi = I and no errors were in-
troduced. At ? = 1.0, Mi = Md, and we would
expect to see characters corrupted at the same rate
as in the output of the OCR engine.
We then iterated over each document, choosing a
new (possibly the same) character yl for each orig-
inal character xl according to the probability distri-
bution p(yl = w?|xl = w) = M iw,w? . Our pro-
cess was a one-substitution algorithm, as we did not
include instances of insertions or deletions, conse-
quently words were changed but not split or deleted.
This allowed for a more straightforward calculation
of word error rate. Segmentation errors can still
occur in the learning stage, however, as the noise
model sometimes replaced alphabet characters with
punctuation characters, which were treated as delim-
iters by our tokenizer.
Dataset |D| K # Types # Tokens
20 News 19997 20 107211 2261805
Reuters 11367 81 29034 747458
Enron 4935 32 60495 2063667
Eisenhower 604 N/A 8039 76674
Table 1: Summary of test dataset characteristics. |D| is
the number of documents in the dataset. K is the number
of human-labeled classes provided with the dataset.
We chose three datasets to corrupt: 20 News-
groups (Lang, 1995), Reuters 21578 (Lewis, 1997),
and the LDC-annotated portion of the Enron e-mail
archive (Berry et al, 2007). Each of these datasets
were corrupted at values ? = i?0.01 for i ? (0, 13).
At this point, the word error rate of the corrupted
data was near 50% and, since this was approxi-
mately the WER observed for the worst OCR engine
on the real-world data, we chose to stop there. The
word error rate was calculated during the corruption
process. Here is an example sentence corrupted at
two ? values:
? = 0.000 I am also attaching the RFP itself.
? = 0.02 I am also attachEng the RFP itself.
? = 0.10 I Jm alAo attaching the RFP itself.
Table 3 shows some basic statistics for the
datasets. The values shown are for the ?clean? ver-
sions of the data. For an example of how noise
and pre-processing techniques affect these counts
see Section 4.1.
It is interesting to note that the word error rates
produced by the noise model appear to be signif-
icantly higher than first expected. One might as-
sume that the WER should increase fairly steadily
from 0% at ? = 0 to about 50% (the error rate
achieved by the Tesseract OCR engine on the Eisen-
hower dataset) at ? = 1. There are at least two
sources for the discrepancy. First, the vocabulary
of the Eisenhower dataset does not match well with
that of any of the source datasets from which the
synthetic data were generated. This means that the
word and character distributions are different and so
the error rates will be as well. Secondly, whereas our
technique gives the same probability of corruption to
all instances of a given character, errors in true OCR
output are bursty and more likely to be concentrated
in specific tokens, or regions, of a document. This
242
is because most sources of noise do not affect docu-
ment images uniformly. Also, modern OCR engines
do not operate at just the character level. They in-
corporate dictionaries and language models to pre-
vent them from positing words that are highly un-
likely. As a consequence, an OCR engine is much
more likely to either get a whole word correct, or to
miss it completely, concentrating multiple errors in
a single word. This is the difference between 10 er-
rors in a single word, which only contributes 1 to the
numerator of the WER formula and 10 errors spread
across 10 different words, which contributes 10 to
the numerator. Furthermore, because content bear-
ing words tend to be relatively rare, language mod-
els are poorer for them than for frequent function
words, meaning that the words most correlated with
semantics are also the most likely to be corrupted by
an OCR engine.
An example of this phenomenon is easy to find.
In the Enron corpus, there are 165,871 instances of
the word ?the? and 102 instances of the string ?thc?.
Since ?c? has a high rate of confusion with ?e?, we
would expect at least some instances of ?the? to be
corrupted to ?thc? by the error model. At ? = 0.03,
there are 156,663 instances of the word ?the? and
513 instances of ?thc?. So, the noise model converts
?the? to ?thc? roughly 0.3% of the time. In con-
trast, there are no instances of ?thc? in the Tesseract
OCR output even though there are 5186 instances
of ?the? in the transcription text, and so we would
expect approximately 16 occurrences of ?thc? if the
errors introduced by the noise model were truly rep-
resentative of the errors in the actual OCR output.
Another interesting property of the noise intro-
duced by actual OCR engines and our synthetic
noise model is the way in which this noise affects
words distributions. This is very important, since
word occurrence and co-occurrence counts are the
basis for model inference in both clustering and
topic modeling. As mentioned previously, one com-
mon way of lessening the impact of OCR noise
when training topic models over OCRed data is to
apply a frequency cutoff filter to cull words that oc-
cur fewer than a certain number of times. Figures 1
and 2 show the number of word types that are culled
from the synthetic 20 Newsgroups OCR data and the
Eisenhower OCR data, respectively, at various levels
of noise. Note that the cutoff filters use a strict ?less
0 10 20 30 40 50WER
0
200000
400000
600000
800000
1000000
Num
ber
 of 
Fea
ture
s C
ulle
d
2510
Figure 1: The number of word types culled with fre-
quency cutoff filters applied to the 20 Newsgroups data
with various levels of errors introduced.
than?, so a frequency cutoff of 2 eliminates only
words that occur once in the entire dataset. Also,
these series are additive, as the words culled with
a frequency cutoff of 2 are a subset of those culled
with a frequency cutoff of j > 2.
In both cases, it is apparent that by far the largest
impact that noise has is in the creation of single-
tons. It seems that the most common corruptions in
these scenarios is the creation of one-off word types
through a unique corruption of a (most likely rare)
word. This means that it is unlikely that enough evi-
dence will be available to associate, through similar
contexts, the original word and its corrupted forms.
Due to the fact that most clustering and topic
models ignore the forms of word tokens (the charac-
ters that make them up), and only take into account
word identities, we believe that the similarity in the
way in which real OCR engines and our synthetic
OCR noise model distort word distributions is suf-
ficient evidence to support the use of the synthetic
data until larger and better real-world OCR datasets
can be made available. Though the actual errors will
take a different form, the character-level details of
the errors are less relevant than the word distribution
alterations for the models in question.
4 Experimental Results
We ran experiments on both the real and synthetic
OCR data. In this section we explain our experi-
243
0 10 20 30 40 50 60WER
5000
10000
15000
20000
25000
30000
Num
ber
 of 
Wo
rds
 Cu
lled
2510
Figure 2: The number of word types culled with fre-
quency cutoff filters applied to the transcript and three
OCR engine outputs for the Eisenhower data.
mental methodology and present both empirical and
qualitative analyses of the results.
4.1 Methodology
For the synthetic OCR datasets, we ran clustering
experiments using EM on a mixture of multinomials
(c.f. (Walker and Ringger, 2008)). We specified
the number of clusters to be the same as the num-
ber of classes provided with the data. Clusters were
evaluated using several external cluster quality met-
rics which compare ?gold standard? labels to those
created through clustering. The metrics used were
Variation of Information (VI) (Meila?, 2007), and
the Adjusted Rand Index (ARI) (Hubert and Arabie,
1985). Other metrics were also calculated (e.g. the
V-Measure (Rosenberg and Hirschberg, 2007), and
Average Entropy (Liu et al, 2003)), but these results
were excluded due to space constraints and the fact
that their plots are similar to those shown. We did
not cluster the Eisenhower data because of the ab-
sence of the class labels necessary for evaluation.
For both the synthetic and non-synthetic data we
also trained LDA topic models (Blei et al, 2003) us-
ing Gibbs sampling. We used the implementation
found in the MALLET software package (McCal-
lum, 2002) with the option enabled to learn the pri-
ors during sampling as discussed by Wallach et al
(2009a). Each LDA model was trained on 90% of
the documents in each dataset. The trained model
was used to calculate an estimate of the marginal
log-likelihood of the remaining 10% of the docu-
ments using the left-to-right algorithm (Wallach et
al., 2009b). The number of topics used for each
dataset was adjusted a priori according to the num-
ber of documents it contained. We used 100 topics
for Enron and Eisenhower, 150 for Reuters, and 200
for 20 Newsgroups.
In addition to running experiments on the ?raw?
synthetic data, we also applied simple unsupervised
feature selectors before training in order to evalu-
ate the effectiveness of such measures in mitigat-
ing problems caused by OCR errors. For the topic
modeling (LDA) experiments three feature selectors
were used. The first method employed was a simple
term frequency cutoff filter (TFCF), with a cutoff
of 5 as in (Wang and McCallum, 2006). The next
method employed was Term Contribution (TC), a
feature selection algorithm developed for document
clustering (Liu et al, 2003). Term contribution is
parameterized by the number of word types that are
to remain after selection. We attempted three val-
ues for this parameter, 10,000, 20,000, and 50,000.
The final method we employed was a method called
Top-N per Document (TNPD) (Walker and Ring-
ger, 2010), which is a simple feature selection al-
gorithm that first assigns each type in every doc-
ument a document-specific score (e.g. its TF-IDF
weight), and then selects words to include in the fi-
nal vocabulary by choosing the N words with the
highest score from each document in the corpus. We
found that N = 1 gave the best results at the great-
est reduction in word types. After the vocabulary is
built, all words not in the vocabulary are culled from
the documents. This does not mean that all docu-
ments contain only one word after feature selection,
as the top word in one document may occur in many
other documents, even if it is not the top word in
those documents. Likewise, if two documents would
both contribute the same word, then the second doc-
ument makes no contribution to the vocabulary. This
process can result in vocabularies with thousands of
words, leaving sufficient words in each document
for analysis. For the clustering experiments, initial
tests showed little difference in the performance of
the feature selectors, so only the TNPD selector was
used. Figures 3(a) and 3(b) show how the various
pre-processing methods affect word type and token
244
0 5 10 15 20 25 30 35 40 45Word Error Rate0
10000
20000
30000
40000
50000
60000
Typ
es
tc.10000tc.20000tc.50000tfcf.5tnpd.1
(a) The number of word types remaining after pre-processing.
0 5 10 15 20 25 30 35 40 45Word Error Rate600000
800000
1000000
1200000
1400000
1600000
1800000
2000000
2200000
Tok
ens
tc.10000tc.20000tc.50000tfcf.5tnpd.1
(b) The number of word tokens remaining after pre-processing.
Figure 3: The effect of pre-processing on token and type counts for the 20 Newsgroups dataset at various error rates.
counts, respectively, for the 20 Newsgroups dataset.
In contrast, without pre-processing the number of
types scales from 107,211 to 892,983 and the num-
ber of tokens from 2,261,805 to 3,073,208.
Because all of these procedures alter the number
of words and tokens in the final data, log-likelihood
measured on a held-out set cannot be used to accu-
rately compare the quality of topic models trained
on pre-processed data, as the held-out data will con-
tain many unknown words. If the held-out data is
also pre-processed to only include known words,
then the likelihood will be greater for those proce-
dures that remove the most tokens, as the product
that dominates the calculation will have fewer terms.
If unknown words are allowed to remain, even a
smoothed model will assign them very little prob-
ability and so models will be heavily penalized.
We use an alternative method for evaluating the
topic models, discussed in (Griffiths et al, 2005), to
avoid the aforementioned problems with an evalu-
ation based on log-likelihood. Since the synthetic
data is derived from datasets that have topical docu-
ment labels, we are able to use the output from LDA
in a classification problem with the word vectors
for each document being replaced by the assigned
topic vectors. This is equivalent to using LDA as a
dimensionality reduction pre-process for document
classification. A naive Bayes learner is trained on a
portion of the topic vectors, labeled with the origi-
nal document label, and then the classification accu-
racy on a held-out portion of the data is computed.
Ten-fold cross-validation is used to control for sam-
pling issues. The rationale behind this evaluation is
that, even though the topics discovered by LDA will
not correspond directly to the labels, there should at
least be a high degree of correlation. Models that
discover topical semantics that correlate well with
the labels applied by humans will yield higher clas-
sification accuracies and be considered better mod-
els according to this metric.
To compensate for the randomness inherent in
the algorithms, each experiment was replicated ten
times. The results of these runs were averaged to
produce the values reported here.
4.2 Empirical Analysis
Both the mixture of multinomials document model
and LDA appear to be fairly resilient to character-
level noise. Figures 4 and 5 show the results of the
document clustering experiments with and without
feature selection, respectively. Memory issues pre-
vented the collection of results for the highest error
rates on the Enron and Reuters data without feature
selection.
With no pre-processing, the results are somewhat
mixed. The Enron dataset experiences almost no
quality degradation as the WER increases, yielding
remarkably constant results according to the metrics.
However, this may be an artifact of the relatively
poor starting performance for this dataset, a result
of the fact that the gold standard labels do not align
245
0 5 10 15 20 25 30 35 40 45Word Error Rate0.0
0.1
0.2
0.3
0.4
0.5
ari
20_newsgroupsreuters21578enron
(a) Adjusted Rand Index results
0 5 10 15 20 25 30 35 40 45Word Error Rate2.0
2.5
3.0
3.5
4.0
4.5
vi
20_newsgroupsreuters21578enron
(b) Variation of Information results (lower is better)
Figure 4: Results for the clustering experiments on the three synthetic datasets without feature selection.
well with automatically discovered patterns because
they correspond to external events. In contrast, the
Reuters data appears to experience drastic degrada-
tion in performance. Once feature selection occurs,
however, performance remains much more stable as
error rates increase.
Figure 6(a) shows the results of running LDA on
the transcript and digitized versions of the Eisen-
hower dataset. Log-likelihoods of the held-out set
are plotted with respect to the WER observed for
each OCR engine. The results support the find-
ing that the WER of the OCR engine that produced
the data has a significant negative correlation with
model quality. Unfortunately, it was not possible
to compare the performance of the pre-processing
methods on this dataset, due to a lack of document
topic labels and the deficiencies of log-likelihood
mentioned previously.
Figure 6(b) shows the results of the LDA topic-
modeling experiments on the three ?raw? synthetic
datasets. Similar trends are observed in this graph.
LDA experiences a marked degree of performance
degradation, with all of the trend lines indicating a
linear decrease in log-likelihood.
Figures 7(a) through 7(c) show the results of eval-
uating the various proposed pre-processing proce-
dures in the context of topic modeling. In the graph
?noop.0? represents no pre-processing, ?tc.N? are
the Term Contribution method parameterized to se-
lectN word types, ?tfcf.5? is the term frequency cut-
off filter with a cutoff of 5 and ?tnpd.1? is the Top
N per Document method with N = 1. The y-axis is
the average of the results for 10 distinct trials, where
the output for each trial is the average of the ten ac-
curacies achieved using ten-fold cross-validation as
described in Section 4.1.
Here, the cross-validation accuracy metric reveals
a slightly different story. These results show that
topic quality on both the raw and pre-processed
noisy data degrades at a rate relative to the amount
of errors in the data. That is, the difference in perfor-
mance between two relatively low word error rates
(e.g. 5% and 7% on the Reuters data) is small,
whereas the differences between two high error rates
(e.g. 30% and 32% on the Reuters data) can be rela-
tively quite large.
While pre-processing does improve model qual-
ity, in the case of LDA this improvement amounts
to a nearly constant boost; at high error rates quality
is improved the same amount as at low error rates.
Degradations in model quality, therefore, follow the
same trends, occurring at mostly the same rate in
pre-processed data as in the raw noisy data. This is
in contrast to the clustering experiments where pre-
processing virtually eliminates failure trends.
4.3 Qualitative Analysis
Higher values measured with automated metrics
such as log-likelihood on a held-out set and the
cross-validation classification metric discussed here
246
0 10 20 30 40 50Word Error Rate0.0
0.1
0.2
0.3
0.4
0.5
ari
20_newsgroupsreuters21578enron
(a) Adjusted Rand Index results
0 10 20 30 40 50Word Error Rate2.0
2.5
3.0
3.5
4.0
4.5
vi 20_newsgroupsreuters21578enron
(b) Variation of Information results (lower is better)
Figure 5: Results for the clustering experiments on the three synthetic OCR datasets with TNPD feature selection.
do not necessarily indicate superior topics according
to human judgement (Chang et al, 2009). In order
to provide a more thorough discussion of the relative
quality of the topic models induced on the OCR data
versus those induced on clean data, we sampled the
results of several of the runs of the LDA algorithm.
In Tables 2 and 3 we show the top words for the
five topics with the highest learned topic prior (? in
the LDA literature) learned during Gibbs sampling.
This information is shown for the Reuters data first
with no corruption and then at the highest error rate
for which we have results for that data of 45% WER.
In general, there appears to be a surprisingly good
correlation between the topics learned on the clean
data and those learned on the corrupted data, given
the high level of noise involved. The topics are
generally cohesive, containing mostly terms relat-
ing to financial market news. However, the topics
trained on the clean data, though all related to finan-
cial markets, are fairly distinctive. Topic 3, for ex-
ample seems to be about fluctuations in stock prices,
and Topics 106 and 34 about business acquisitions
and mergers. The topics trained on the noisy data
are fairly homogeneous and the differences between
them are more difficult to identify.
In addition, it appears as though the first topic
(topic 93) is not very coherent at all. This topic is
significantly larger, in terms of the number of tokens
assigned to it than the other topics shown in either
table. In addition, the most probable words listed for
the topic seem less cohesive than for the other top-
ics. It contains many two-letter words that are likely
a mixture of valid terms (e.g., stock exchange and
ticker symbols, and parts of place names like ?Rio
de Janeiro?) and corruptions of real words. For ex-
ample, even though there are no instances of ?ts? as
a distinct token in the clean Reuters data, it is in the
list of the top 19 words for topic 93. This is perhaps
due to the fact that ?is? can easily be converted to
?ts? by mistaking t for i.
It is also the case that, for most topics learned
on the corrupted data, the most probable words for
those topics tend to be shorter, on average, than for
topics learned on clean data. We believe this is due
to the fact that the processes used to add noise to the
data (both real OCR engines and our synthetic noise
model) are more likely to corrupt long words, es-
pecially in the case of the synthetic data which was
created using a character-level noise model.
Examination of the data tends to corroborate this
hypothesis, though even long words usually contain
only a few errors. For example, in the 20 News-
groups data there are 379 instances of the word ?yes-
terday?, a long word that is not close to other English
words in edit distance. When the data has been cor-
rupted to a WER of 47.9%, however, there are only
109 instances of ?yesterday? and 132 tokens that are
within an edit distance of 1 from ?yesterday?.
To some extent, we would expect to observe sim-
ilar trends in real-world data. However, most OCR
247
10 0 10 20 30 40 50 60Word Error Rate95000
90000
85000
80000
75000
70000
Log
-like
liho
od o
f He
ld-o
ut
abbyyomnipagetesseracttranscript
(a) Eisenhower Communique?s
0 10 20 30 40 50Word Error Rate7000000
6000000
5000000
4000000
3000000
2000000
1000000
0
Log
-like
liho
od o
f He
ld-o
ut
20_newsgroupsreuters21578enron
(b) Synthetic Data
Figure 6: Log-likelihood of heldout data for the LDA experiments.
0 5 10 15 20 25 30 35 40 45Word Error Rate52
54
56
58
60
62
64
66
68
70
Ave
rea
ge C
V A
ccu
racy
noop.0tc.10000tc.20000tc.50000tfcf.5tnpd.1
(a) 20 Newsgroups Data
0 5 10 15 20 25 30 35 40 45Word Error Rate67
68
69
70
71
72
73
Ave
rea
ge C
V A
ccu
racy
noop.0tc.10000tc.20000tc.50000tfcf.5tnpd.1
(b) Reuters Data
0 5 10 15 20 25 30 35 40 45Word Error Rate34
35
36
37
38
39
40
41
42
43
Ave
rea
ge C
V A
ccu
racy
noop.0tc.10000tc.20000tc.50000tfcf.5tnpd.1
(c) Enron Data
Figure 7: Average ten-fold cross-validation accuracy for the LDA pre-processing experiments on the synthetic OCR
data.
248
Topic Words Tokens
64 told, market, reuters, reuter, added, time,
year, major, years, president, make, made,
march, world, today, officials, industry,
government, move
67159
3 year, pct, prices, expected, rise, lower,
higher, demand, increase, due, fall, de-
cline, current, high, end, added, level,
drop, market
32907
106 reuter, corp, company, unit, sale, march,
dlrs, mln, sell, subsidiary, acquisition,
terms, group, april, purchase, acquired,
products, division, business
22167
34 shares, dlrs, company, mln, stock, pct,
share, common, reuter, corp, agreement,
march, shareholders, buy, cash, outstand-
ing, merger, acquire, acquisition
22668
7 mln, cts, net, shr, qtr, revs, reuter, avg, shrs,
march, mths, dlrs, sales, st, corp, oct, note,
year, april
18511
Table 2: Top words for the five topics with the highest ?
prior values found using MALLET for one run of LDA
on the uncorrupted Reuters data.
engines employ language models and dictionaries to
attempt to mitigate OCR errors. As a result, given
that a word recognition error has occurred in true
OCR output, it is more likely to be an error that lies
at an edit distance greater than one from the true
word, or else it would have been corrected inter-
nally. For example, there are 349 instances of the
word ?yesterday? in the Eisenhower transcripts, and
284 instances in the Tesserect OCR output and only
5 tokens within an edit distance of one, meaning that
60 corruptions of this word contained more than one
error, making up 90% of the errors for that word.
However, many of these errors still contain most of
the letters from the original word (e.g. ?yesterdj.?,
and ?yestjkday?). In all cases, the corrupted versions
of a given word are very rare, occurring usually only
once or twice in the noisy output, making them use-
less features for informing a model.
5 Conclusions and Future Work
The primary outcome of these experiments is an
understanding regarding when clustering and LDA
topic models can be expected to function well on
noisy OCR data. Our results imply that clustering
methods should perform almost as well on OCR data
as they do on clean data, provided that a reasonable
feature selection algorithm is employed. The LDA
topic model degraded less gracefully in performance
Topic Words Tokens
93 reuter, march, pct, year, april, ed, market,
er, told, es, st, end, ts, al, de, ng, id, sa,
added
258932
99 company, pct, corp, shares, stock, dlrs,
share, offer, group, reuter, mln, march,
unit, stake, buy, cash, bid, sale, board
50377
96 mln, cts, net, shr, qtr, dlrs, revs, reuter,
note, oper, avg, march, shrs, year, mths, st,
sales, corp, oct
54659
141 mln, dlrs, year, net, quarter, share, com-
pany, billion, tax, sales, earnings, dlr,
profit, march, income, ln, results, sale, corp
40475
53 pct, year, rose, rise, january, february, fell,
march, index, december, month, figures,
compared, reuter, rate, earlier, show, ago,
base
22556
Table 3: Top words for the five topics with the highest ?
prior values found using MALLET for one run of LDA
on the Reuters data corrupted with the data-derived noise
model to a WER of 45%.
with the addition of character level errors to its in-
put, with higher error rates impacting model quality
in a way that was apparent empirically in the log-
likelihood and ten-fold cross-validation metrics as
well as through human inspection of the produced
topics. Pre-processing the data also helps model
quality for LDA, yet still yields failure trends sim-
ilar to those observed on unprocessed data.
We found it to be the case that even in data with
high word error rates, corrupted words often share
many characters in common with their uncorrupted
form. This suggests an approach in which word sim-
ilarities are used to cluster the unique corrupted ver-
sions of a word in order to increase the evidence
available to the topic model during training time and
improve model quality. As the quality of models in-
creases on these noisy datasets, we anticipate a con-
sequent rise in their usefulness to researchers and
historians as browsing the data and mining it for use-
ful patterns becomes more efficient and profitable.
Acknowledgments
We would like to thank the Fulton Supercomput-
ing Center at BYU for providing the computing re-
sources required for experiments reported here.
References
ABBYY. 2010. ABBYY finereader.
http://finereader.abbyy.com.
249
S. Agarwal, S. Godbole, D. Punjani, and Shourya Roy. 2007.
How much noise is too much: A study in automatic text clas-
sification. In Proceedings of the Seventh IEEE Intl. Conf. on
Data Mining (ICDM 2007), pages 3?12.
Steven M. Beitzel, Eric C. Jensen, and David A. Grossman.
2003. A survey of retrieval strategies for ocr text collec-
tions. In In Proceedings of the Symposium on Document
Image Understanding Technologies.
Michael W. Berry, Murray Brown, and Ben Signer. 2007. 2001
topic annotated Enron email data set.
David M. Blei and John D. Lafferty. 2006. Dynamic topic
models. In Proceedings of the 23rd Intl. Conf. on Machine
Learning (ICML 2006).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Horst Bunke. 2003. Recognition of cursive roman
handwriting- past, present and future. In 7th International
Conference on Document Analysis and Recognition (ICDAR
2003), volume 1, pages 448?459.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong
Wang, and David Blei. 2009. Reading tea leaves: How
humans interpret topic models. In Advances in Neural Infor-
mation Processing Systems 22, pages 288?296.
Faisal Farooq, Anurag Bhardwaj, and Venu Govindaraju. 2009.
Using topic models for OCR correction. Intl. Journal
on Document Analysis and Recognition (IJDAR), 12(3),
September.
Google, Inc. 2010. Tesseract.
http://code.google.com/p/tesseract-ocr.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and syntax.
In In Advances in Neural Information Processing Systems
17, pages 537?544. MIT Press.
Lawrence Hubert and Phipps Arabie. 1985. Comparing parti-
tions. Journal of Classification, 2(1):193?218, December.
David Reed Jordan. 1945. Daily battle communiques, 1944-
1945. Harold B. Lee Library, L. Tom Perry Special Collec-
tions, MSS 2766.
Ken Lang. 1995. NewsWeeder: Learning to filter netnews.
In Proceedings of the Twelfth International Conference on
Machine Learning, pages 331?339.
D. Lewis. 1997. Reuters-21578 text categorization test collec-
tion. http://www.research.att.com/?lewis.
Tao Liu, Shengping Liu, Zheng Chen, and Wei-Ying Ma. 2003.
An evaluation on feature selection for text clustering. In Pro-
ceedings of the Twentieth Intl. Conf. on Machine Learning
(ICML 2003), August.
Daniel Lopresti. 2008. Optical character recognition errors and
their effects on natural language processing. In Proceedings
of the second workshop on Analytics for noisy unstructured
text data (AND 2008), pages 9?16.
William B. Lund and Eric. K Ringger. 2009. Improving op-
tical character recognition through efficient multiple system
alignment. In Proceedings of the Joint Conf. on Digital Li-
braries (JCDL?09), June.
Andrew Kachites McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Marina Meila?. 2007. Comparing clusterings?an informa-
tion based distance. Journal of Multivariate Analysis,
98(5):873?895.
David Mimno and Andrew Mccallum. 2007. Organizing the
OCA: learning faceted subjects from a library of digital
books. In Proceedings of the Joint Conf. on Digital Libraries
(JCDL?07), pages 376?385.
Cosmin Munteanu, Ronald Baecker, Gerald Penn, Elaine Toms,
and David James. 2006. The effect of speech recognition
accuracy rates on the usefulness and usability of webcast
archives. In Proceedings of the SIGCHI conference on Hu-
man Factors in computing systems, pages 493?502.
David J. Newmann and Sharon Block. 2006. Probabilistic topic
decomposition of an eighteenth-century american newspa-
per. J. Am. Soc. Inf. Sci. Technol., 57(6):753?767, February.
Nuance Communications, Inc. 2010. OmniPage Pro.
http://www.nuance.com/imaging/products/omnipage.asp.
Andrew Rosenberg and Julia Hirschberg. 2007. V-measure: A
conditional entropy-based external cluster evaluation mea-
sure. In Joint Conference on Empirical Methods in Natural
Language Processing and Computational Language Learn-
ing (EMNLP-CoNLL 2007).
Kazem Taghva, Julie Borsack, and Allen Condit. 1994. Results
of applying probabilistic ir to ocr text. In in Proc. 17th Intl.
ACM/SIGIR Conf. on Research and Development in Infor-
mation Retrieval, pages 202?211.
Kazem Taghva, Tom Nartker, Julie Borsack, Steve Lumos,
Allen Condit, and Ron Young. 2001. Evaluating text catego-
rization in the presence of ocr errors. In In Proc. IS&T/SPIE
2001 Intl. Symp. on Electronic Imaging Science and Tech-
nology, pages 68?74. SPIE.
Daniel Walker and Eric Ringger. 2008. Model-based document
clustering with a collapsed gibbs sampler. In Proceedings of
the 14th ACM SIGKDD Intl. Conf. on Knowledge Discovery
and Data Mining (KDD 2008).
Daniel Walker and Erik K. Ringger. 2010. Top N per docu-
ment: Fast and effective unsupervised feature selection for
document clustering. Technical Report 6, Brigham Young
University. http://nlp.cs.byu.edu/techreports/BYUNLP-
TR6.pdf.
Hanna Wallach, David Mimno, and Andrew McCallum. 2009a.
Rethinking LDA: Why priors matter. In Advances in Neural
Information Processing Systems 22, pages 1973?1981.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009b. Evaluation methods for topic models.
In Proceedings of the 26th Annual Intl. Conf. on Machine
Learning (ICML 2009), pages 1105?1112.
Xuerui Wang and Andrew McCallum. 2006. Topics over time:
A non-markov continuous-time model of topical trends. In
Proceedings of the 12th ACM SIGKDD Intl. Conf. on Knowl-
edge Discovery and Data Mining (KDD 2006).
Michael L. Wick, Michael G. Ross, and Erik G. Learned-
Miller. 2007. Context-sensitive error correction: Using
topic models to improve OCR. In Proceedings of the Ninth
Intl. Conf. on Document Analysis and Recognition (ICDAR
2007), pages 1168?1172.
250
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 810?820,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Probabilistic Morphological Analyzer for Syriac
Peter McClanahan, George Busby, Robbie Haertel, Kristian Heal ?,
Deryle Lonsdale?, Kevin Seppi, Eric Ringger
Department of Computer Science, ?Department of Linguistics,
?Center for the Preservation of Ancient Religious Texts (CPART)
Brigham Young University
Provo, Utah 84604 USA
http://nlp.cs.byu.edu/
Abstract
We define a probabilistic morphological ana-
lyzer using a data-driven approach for Syriac in
order to facilitate the creation of an annotated
corpus. Syriac is an under-resourced Semitic
language for which there are no available lan-
guage tools such as morphological analyzers.
We introduce novel probabilistic models for
segmentation, dictionary linkage, and morpho-
logical tagging and connect them in a pipeline
to create a probabilistic morphological analyzer
requiring only labeled data. We explore the per-
formance of models with varying amounts of
training data and find that with about 34,500
labeled tokens, we can outperform a reason-
able baseline trained on over 99,000 tokens and
achieve an accuracy of just over 80%. When
trained on all available training data, our joint
model achieves 86.47% accuracy, a 29.7% re-
duction in error rate over the baseline.
1 Introduction
Our objective is to facilitate the annotation of a large
corpus of classical Syriac (referred to simply as ?Syr-
iac? throughout the remainder of this work). Syr-
iac is an under-resourced Western Semitic language
of the Christian Near East and a dialect of Aramaic.
It is currently employed almost entirely as a liturgi-
cal language but was a true spoken language up un-
til the eighth century, during which time many pro-
lific authors wrote in Syriac. Even today there are
texts still being composed in or translated into Syr-
iac. By automatically annotating these texts with lin-
guistically useful information, we will facilitate sys-
tematic study by scholars of Syriac, the Near East,
and Eastern Christianity. Furthermore, languages
that are linguistically similar to Syriac (e.g., Arabic
and Hebrew) may benefit from the methodology pre-
sented here.
Our desired annotations include morphological
segmentation, links to dictionary entries, and mor-
phological attributes. Typically, annotations of this
kind are made with the assistance of language tools,
such as morphological analyzers, segmenters, or
part-of-speech (POS) taggers. Such tools do not
exist for Syriac, but some labeled data does exist:
Kiraz (1994) compiled an annotated version of the
Peshitta New Testament (1920) and a concordance
thereof. We aim to replicate this kind of annota-
tion on a much larger scale with more modern tools,
building up from the labeled New Testament data,
our only resource. Motivated by this state of affairs,
our learning and annotation framework requires only
labeled data.
We approach the problem of Syriac morphological
annotation by creating five probabilistic sub-models
that can be trained in a supervised fashion and com-
bined in a joint model of morphological annota-
tion. We introduce novel algorithms for segmenta-
tion, dictionary linkage, and morphological tagging.
We then combine these sub-models into a joint n-
best pipeline. This joint model outperforms a strong,
though na?ve, baseline for all amounts of training
data over about 9,900 word tokens.
1.1 Syriac Background
Since Syriac is an abjad, its writing system does
not require vowels. As a dialect of Aramaic, it
is an inflected language with a templatic (non-
concatenative) morphology, based on a system of
triliteral consonantal roots, with prefixes, suffixes,
infixes, and enclitic particles. Syriac is written from
810
right to left. For the purposes of this work, all Syr-
iac is transliterated according to the Kiraz (1994)
transliteration1 and is written left-to-right whenever
transliterated; the Syriac appearing in the Serto script
in this paper is shown right-to-left.
Since there is no standardized nomenclature for
the parts of a Syriac word, we define the following
terms to facilitate the definitions of segmentation,
dictionary linkage, and morphological tagging:
? word token - contiguous characters delimited by
whitespace and/or punctuation
? stem - an inflected form of the baseform and
the main part of the word to which prefixes and
suffixes can be attached; the affixes do not in-
flect the stem but include prepositions, object
suffixes, and enclitic pronouns
? baseform - the dictionary citation form; also
known as a lexeme or lemma
? root - the form from which the baseform is de-
rived
To clarify, we will use an example word token
???????, LMLCCON, which means ?to your (mas-
culine plural) king?. For this word, the stem is ???,
MLC; the baseform is ????, MLCA ?king?; and the
root is ???,MLC. To clarify, note that the word token
(including the stem) can be spoken and written with
vowels as diacritics; however, since the vowels are
not written in common practice and since most text
does not include them, this work omits any indica-
tion of vowels. Furthermore, the stem is an inflected
baseform and does not necessarily form a word on
its own. Also, the (unvocalized) stem and root are
not necessarily identical. In Syriac, the same root
???, MLC is the foundation for other words such as
promise, counsel, deliberate, reign, queen, kingdom,
and realm.
1.2 Sub-tasks
Segmentation, or tokenization as it is sometimes
called (e.g., Habash and Rambow, 2007), is the pro-
cess of dividing a word token into its prefix(es) (if
any), a stem, and a suffix (if any). For Syriac, each
1According to this transliteration all capital letters including
A (?, olaph) and O (?, waw) are consonants. Additionally, the
semi-colon (;), representing (?, yod), is also a consonant.
word token consists of exactly one stem, from zero
to three prefixes, and zero or one suffix. Each pre-
fix is exactly one character in length. Segmenta-
tion does not include the process of parsing the stem
for its inflectional morphology; that step is handled
separately in subsequent processes described below.
While segmenting a Syriac word, we can handle all
prefixes as a single unit. It is trivial to segment a
prefix cluster into its individual prefixes (one charac-
ter per prefix). Suffixes may be multiple characters
in length and encode the morphological attributes of
the suffix itself (not of the stem); the suffix usually
encodes the object of the stem and has its own gram-
matical attributes, which we list later. As an example
of token segmentation, for the word token ???????,
LMLCCON, the prefix is ?, L ?to?, the stem is ???,
MLC ?king?, and the suffix is ???, CON ?(masculine
plural) your?.
Dictionary linkage is the process of linking a stem
to its associated baseform and root. In most Syriac
dictionaries, all headwords are either baseforms or
roots, and for a given word these are the only rele-
vant entries in the dictionary. Each Syriac stem is
derived from a baseform, and each baseform is de-
rived from a root. There is ambiguity in this cor-
respondence which can be caused by, among other
things, homographic stems generated from different
roots or even from homographic roots. As such, link-
age may be thought of as two separate processes: (1)
baseform linkage, where the stem is mapped to its
most likely baseform; and (2) root linkage, where
the baseform is mapped to its most likely root. For
our example ???????, LMLCCON, baseform linkage
would map stem ???,MLC to baseform ????,MLCA,
and root linkage would map baseform ????,MLCA to
root ???, MLC.
Morphological tagging is the process of labeling
each word token with its morphological attributes.
Morphological tagging may be thought of as two
separate tagging tasks: (1) tagging the stem and (2)
tagging the suffix. For Syriac, scholars have defined
for this task a set of morphological attributes con-
sisting of twelve attributes for the stem and four at-
tributes for the suffix. The attributes for the stem
are as follows: grammatical category, verb conju-
gation, aspect, state, number, person, gender, pro-
noun type, demonstrative category, noun type, nu-
meral type, and participle type. The morphological
811
Attribute Value
Grammatical Category noun
Verb Conjugation N/A
Aspect N/A
State emphatic
Number singular
Person N/A
Gender masculine
Pronoun Type N/A
Demonstrative Category N/A
Noun Type common
Numeral Type N/A
Participle Type N/A
Table 1: The values for the morphological attributes of
the stem ???,MLC, ?king?.
Attribute Value
Gender masculine
Person second
Number plural
Contraction normal suffix
Table 2: The values for the morphological attributes of
the suffix ???, CON, ?(masculine plural) your?.
attributes for the suffix are gender, person, number,
and contraction. The suffix contraction attribute en-
codes whether the suffix is normal or contracted, a
phonological process involving the attachment of an
enclitic pronoun to a participle. These morphologi-
cal attributes were heavily influenced by those used
by Kiraz (1994), but were streamlined in order to fo-
cus directly on grammatical function. During mor-
phological tagging, each stem is labeled for each of
the stem attributes, and each suffix is labeled for each
of the suffix attributes. For a given grammatical cat-
egory (or POS), only a subset of the morphological
attributes is applicable. For those morphological at-
tributes (both of the stem and of the suffix) that do
not apply, the correct label is ?N/A? (not applicable).
Tables 1 and 2 show the correct stem and suffix tags
for the word ???????, LMLCCON.
The remainder of the paper will proceed as fol-
lows: Section 3 outlines our approach. In Section 4,
we describe our experimental setup; we present re-
sults in Section 5. Section 6 contrasts previous work
with our approach. Finally, in Section 7 we briefly
conclude and offer directions for future work.
2 The Syromorph Approach
Since lack language tools, we focus on automatically
annotating Syriac text in a data-driven fashion based
on the labeled data we have available. Since seg-
mentation, linkage, and morphological tagging are
not mutually independent tasks, we desire models
for the sub-tasks to influence each other. To accom-
modate these requirements, we use a joint pipeline
model (Finkel et al, 2006). In this section, we will
first discuss this joint pipeline model, which we call
syromorph. We then examine each of the individual
sub-models.
2.1 Joint Pipeline Model
Our approach is to create a joint pipeline model con-
sisting of a segmenter, a baseform linker, a root
linker, a suffix tagger, and a stem tagger. Figure 1
shows the dependencies among the sub-models in
the pipeline for a single word. Each sub-model
(oval) has access to the data and predictions (rect-
angles) indicated by the arrows. For example, for a
given word, the stem tagger has access to the previ-
ously predicted stem, baseform, root, and suffix tag.
The baseform linker has access to the segmentation,
most importantly the stem.
The training of syromorph is straightforward.
Each of the individual sub-models is trained sepa-
rately on the true labeled data. Features are extracted
from the local context in the sentence. The local con-
text consists first of predictions for the entire sen-
tence from earlier sub-tasks (those sub-tasks upon
which the sub-task in question depends). We cre-
ated the dependencies shown in Figure 1 taking into
account the difficulty of the tasks and natural depen-
dencies in the language. In addition to the predic-
tions for the entire sentence from previous sub-tasks,
the local context also includes the previous o tags of
the current sub-task, as the standard order o Markov
model does. For example, when the stem tagger is
being trained on a particular sentence, the local con-
text consists of the words in the sentence, the pre-
dicted segmentation, baseform, root, and suffix tags
for each word in the sentence, and additionally the
labels for the previous o stems. To further elaborate
812
Figure 1: The syromorph model. Each rectangle is an
input or output and each oval is a process employing a
sub-model.
on the example, since features are extracted from the
local context, for stem tagging we extract features
such as current stem, previous stem, current base-
form, previous baseform, current root, previous root,
current suffix tags, and previous suffix tags. (Here,
?previous? refers to labels on the immediately pre-
ceding word token.)
2.2 Segmentation
The syromorph segmentation model is a hybrid
word- and consonant-level model, based on the
model of Haertel et al (2010) for data-driven dia-
critization. Each of our probabilistic sequence mod-
els is a maximum entropy Markov model (MEMM).
Haertel et al (2010) showed that the distribution
over labels is different for known and words and rare
words. In this work, we only consider words not
seen in training (i.e., ?unknown?) to be rare. Follow-
ing Haertel et al?s (2010) model, a separate model
is trained for each word type seen in training with
the intent of choosing the best segmentation given
that word. This approach is closely related to the
idea of ambiguity classes mentioned in Haji? and
Hladk? (1998).
To handle unknown words, we back off to a
consonant-level model. Our consonant-level seg-
mentation model uses the notion of BI (Beginning
and Inside) tags, which have proven successful in
named-entity recognition. Since there are three
labels in which we are interested (prefix, stem, and
suffix), we apply the beginning and inside notion
to each of them to create six tags: BEGINNING-
PREFIX, INSIDE-PREFIX, BEGINNING-STEM,
INSIDE-STEM, BEGINNING-SUFFIX, and
INSIDE-SUFFIX. We train an MEMM to predict
one of these six tags for each consonant. Further-
more, we constrain the decoder to allow only legal
possible transitions given the current prediction,
so that prefixes must come before stems and stems
before suffixes. In order to capture the unknown
word distributions, we train the consonant-level
model on words occurring only once during training.
We call this word- and consonant-level segmenta-
tion model hybrid. As far as we are aware, this is a
novel approach to segmentation.
2.3 Dictionary Linkage
For dictionary linkage, we divide the problem into
two separate tasks: baseform linkage and root link-
age. For both of these tasks, we use a hybrid model
similar to that used for segmentation, consisting of
a collection of separate MEMMs for each word type
(either a stem or baseform, depending on the linker)
and amodel for unknown (or rare) words. For the un-
known words, we compare two distinct approaches.
The first approach for unknown words is based
on the work of Chrupa?a (2006), including the Mor-
fette system. Instead of predicting a baseform given
a stem, we predict what Chrupa?a calls a lemma-
class. A lemma-class is the transformation specified
by the minimum edit distance between the baseform
(which he calls a lemma) and the stem. The trans-
formation is a series of tuples, where each tuple in-
cludes (1) whether it was an insertion or deletion,
(2) the letter inserted or deleted, and (3) the position
of the insertion or deletion in the string (positions
begin at zero). All operations are assumed to oc-
cur sequentially, as in Morfette. For example, the
transformation of XE;N to XEA would proceed as
follows: delete ; from position 2, insert A into po-
sition 2, delete N from position 3.
813
In hybrid-morfette baseform linkage (respec-
tively, root linkage), we predict a lemma-class (i.e.,
transformation) for each baseform (respectively,
root). The predicted transformation is then applied
to the stem (respectively, baseform) in order to con-
struct the actual target baseform (respectively, root).
The advantage to this method is that common trans-
formations are grouped into a single class, thereby
allowing the model to generalize and adequately
predict baseforms (and roots) that have not been
seen during training, but whose transformations have
been seen. This model is trained on all words in or-
der to capture as many transformations as possible.
The second approach for unknown words, called
hybrid-maxent, uses an MEMM trained on all
words seen in training. Given a stem (respectively,
baseform), this approach predicts only baseforms
(respectively, roots) that were observed in training
data. Thus, this method has a distinct disadvan-
tage when it comes to predicting new forms. This
approach corresponds directly to the approach to
handling unknown -words by Toutanova and Man-
ning (2000) for POS tagging.
With regard to baseform and root linkage, we do
not use the dictionary to constrain possible base-
forms or roots, since we make no initial assumptions
about the completeness of a dictionary.
2.4 Morphological Tagging
For morphological tagging, we break the task into
two separate tasks: tagging the suffix and tagging
the stem. Since there are a number of values that
need to be predicted, we define two ways to ap-
proach the problem. We call the first approach the
monolithic approach, in which the label is the con-
catenation of all the morphological attribute values.
Table 3 illustrates the tagging of an example sen-
tence: the stem tag and suffix tag columns contain
the monolithic tags for stem tagging and suffix tag-
ging. We use an MEMM to predict a monolithic tag
for each stem or suffix and call this model maxent-
mono. No co-occurrence restrictions among related
or complementary morphological tags are directly
enforced. Co-occurrence patterns are observed in
the data, learned, and encoded in the models of the
tagging process. It is worth noting further that con-
straints provided by the baseforms ? predicted by
dictionary linkage ? on the morphological attributes
are likewise not directly enforced. Enforcement of
such constraints would require an infusion of expert
knowledge into the system.
The second approach is to assume that morpho-
logical attributes are independent of each other. We
call this the independent approach. Here, each tag
is predicted by a tagger for a single morphological
attribute. For example, the gender model is ignorant
of the other 11 sub-tags during stem tagging. Using
its local context (which does not include other stem
sub-tags), the model predicts the best gender for a
given word. The top prediction of each of these tag-
gers (12, for stem tagging) is then combined na?vely
with no notion of what combinations may be valid
or invalid. We use MEMMs for each of the single-
attribute taggers. This model is calledmaxent-ind.
2.5 Decoding
Our per-task decoders are beam decoders, with
beam-size b. In particular, we limit the number of
per-stage back-pointers to b due to the large size of
the tagset for some of our sub-models. Although
Viterbi decoding produces the most probable label
sequence given a sequence of unlabeled words, it is
potentially intractible on our hybrid models due to
the unbounded dependence on previous consonant-
level decisions. Our beam decoders produce a good
approximation when tuned properly.
Decoding in syromorph consists of extending the
per-task decoders to allow transitions from each sub-
model to the next sub-model in the pipe. For exam-
ple, in our pipeline, the first sub-model is segmen-
tation. We predict the top n segmentations for the
sentence (i.e., sequences of segmentations), where n
is the number of transitions tomaintain between each
sub-task. Then, we run the remaining sub-tasks with
each of the n sequences as a possible context. After
each sub-task is completed, we narrow the number
of possible contexts back to n.
We swept b and n for various values, and found
b = 5 and n = 5 to be good values that balanced
between accuracy and time; larger values saw only
minute gains in accuracy.
814
Word Transliteration Pre. Stem Suffix Baseform Root Suff. Tags Stem Tags
????? OEBDT O EBDT EBD EBD 0000 011012200000
???? ANON ANON HO HO 0000 300023222000
???? LALHN L ALH N ALHA ALH 1011 200310200200
?????? MLCOTA MLCOTA MLCOTA MLC 0000 200310300200
????? OCHNA O CHNA CHNA CHN 0000 200320200200
?????? OMLCA O MLCA MLCA MLC 0000 200320200200
Table 3: Part of a labeled Syriac sentence ?????? ????? ?????? ???? ???? ?????, ?And you have made them a kingdom and
priests and kings for our God.? (Revelation 5:10)
3 Experimental Setup
We are using the Syriac Peshitta New Testament in
the form compiled by Kiraz (1994).2 This data is
segmented, annotated with baseform and root, and
labeled with morphological attributes. Kiraz and
others in the Syriac community refined and corrected
the original annotation while preparing a digital and
print concordance of the New Testament. We aug-
mented Kiraz?s version of the data by segmenting
suffixes and by streamlining the tagset. The dataset
consists of 109,640 word tokens.
Table 3 shows part of a tagged Syriac sentence us-
ing this tagset. The suffix and stem tags consist of
indices representing morphological attributes. In the
example sentence, the suffix tag 1011 represents the
values ?masculine?, ?N/A?, ?plural?, ?normal suf-
fix? for the suffix attributes of gender, person, num-
ber, and contraction. Each value of 0 for each stem
and suffix attribute represents a value of ?N/A?, ex-
cept for that of grammatical category, which always
must have a value other than ?N/A?. Therefore, the
suffix tag 0000 means there is no suffix.
For the stem tags, the attribute order is the same
as that shown in Table 1 from top to bottom. The
following describes the interpretation of the stem
values represented in Table 3. Grammatical cate-
gory values 0, 2, and 3 represent ?verb?, ?noun?,
and ?pronoun?, respectively. (Grammatical cate-
gory has no ?N/A? value.) The verb conjugation
value 1 represents ?peal conjugation?. Aspect value
1 represents ?perfect?. State value 3 represents ?em-
phatic?. Number values 1 and 2 represent ?singular?
and ?plural?. Person values 2 and 3 represent ?sec-
2The Way International, a Biblical research ministry, anno-
tated this version of the New Testament by hand and required
15 years to do so.
ond? and ?third? person. Gender values 2 and 3 rep-
resent ?masculine? and ?feminine?. Pronoun type
value 2 represents ?demonstrative?. Demonstrative
category value 2 represents ?far?. Finally, noun type
2 represents ?common?. The last two columns of 0
represent ?N/A? for numeral type and particle type.
We implement five sub-tasks: segmentation, base-
form linkage, root linkage, suffix tagging, and stem
tagging. We compare each sub-task to a na?ve ap-
proach as a baseline. In addition to desiring good
sub-models, we also want a joint pipeline model that
significantly outperforms the na?ve joint approach,
which is formed by using each of the following base-
lines in the pipeline framework.
The baseline implementation of segmentation is to
choose the most-frequent label: for a given word,
the baseline predicts the segmentation with which
that word appeared most frequently during training.
For unknown words, it chooses the largest prefix
and largest suffix that is possible for that word from
the list of prefixes and suffixes seen during train-
ing. (This na?ve baseline for unknown words does
not take into account the fact that the stem is often at
least three characters in length.)
For dictionary linkage, the baseline is similar:
both baseform linkage and root linkage use the most-
frequent label approach. Given a stem, the baseline
baseform linker predicts the baseform with which
the stem was seen most frequently during training;
likewise, the baseline root linker predicts the root
from the baseform in a similar manner. For the un-
known stem case, the baseline baseform linker pre-
dicts the baseform to be identical to the stem. For
the unknown baseform case, the baseline root linker
predicts a root identical to the first three consonants
of the baseform, since for Syriac the root is exactly
815
three consonants in a large majority of the cases.
The baselines for stem and suffix tagging are the
most-frequent label approaches. These baselines
are similar to maxent-mono and maxent-ind, us-
ing the monolithic and independent approaches used
by maxent-mono and maxent-ind. The difference
is that instead of using maximum entropy, the na?ve
most-frequent approach is used in its place.
The joint baseline tagger uses each of the compo-
nent baselines in then-best joint pipeline framework.
Because this framework is modular, we can trivially
swap in and out different models for each of the sub-
tasks.
4 Experimental Results
Since we are focusing on under-resourced circum-
stances, we sweep the amount of training data and
produce learning curves to better understand how
our models perform in such circumstances. For each
point in our learning curves and for all other eval-
uations, we employ ten-fold cross-validation. The
learning curves use the chosen percentage of the data
for training and a fixed-size test set from each fold
and report the average accuracy.
The reported task accuracy requires the entire out-
put for that task to be correct in order to be counted as
correct. For example, during stem tagging, if one of
the sub-tags is incorrect, then the entire tag is said to
be incorrect. Furthermore, for syromorph, the out-
puts of every sub-task must be correct in order for
the word token to be counted as correct.
Moving beyond token-level metrics, in order to
understand performance of the system at the level
of individual decisions (including N/A decisions),
we compute decision-level accuracy: we call this
metric total-decisions. For the syromorph method
reported here, there are a total of 20 decisions: 2
for segmentation (prefix and suffix boundaries), 1
for baseform linkage, 1 for root linkage, 4 for suf-
fix tagging, and 12 for stem tagging. This accuracy
helps us to assess the number of decisions a human
annotator would need to correct, if data were pre-
annotated by a given model. Excluding N/A deci-
sions, we compute per-decision coverage and accu-
racy. These metrics are called applicable-coverage
and applicable-accuracy.
We show results on both the individual sub-tasks
and the entire joint task. Since previous sub-
tasks can adversely affect tasks further down in
the pipeline, we evaluate the sub-models by plac-
ing them in the pipeline with other (simulated) sub-
models that correctly predict every instance. For
example, when testing a root linker, we place the
root linker to be evaluated in the pipeline with a
segmenter, baseform linker, and taggers that return
the correct label for every prediction. This gives an
upper-bound for the individual model, removes the
possibility of error propagation, and shows how well
that model performs without the effects of the other
models in the pipeline.
For our results, unknown accuracy is the accuracy
of unknown instances, specific to the task, at training
time. In the case of baseform linkage, for example,
a stem is considered unknown if that stem was not
seen during training. It is therefore possible to have
a known word with an unknown stem and vice versa.
As in other NLP problems, unknown instances are a
manifestation of training data sparsity.
4.1 Baseline Results
Table 4 is grouped by sub-task and reports the results
of each of the baseline sub-tasks in the first row of
each group. Each of the baselines performs surpris-
ingly well. The accuracies of the baselines for most
of the tasks are high because the ambiguity of the
labels given the instance is quite low: the average
ambiguity across word types for segmentation, base-
form linkage, root linkage, suffix tagging, and stem
tagging are 1.01, 1.05, 1.02, 1.35, and 1.47, respec-
tively.
Preliminary experiments indicated that if we had
trained a baseline model using a single prediction (a
monolithic concatenation of the predictions for all
tasks) per token rather than separating the tasks, the
baseline tagging accuracy would have been lower.
Note that the unknown tagging accuracy for the
monolithic suffix tagger is not applicable, because
there were no test suffixes that were not seen during
training.
4.2 Individual Model Results
Table 4 also shows the results for the individual
models. In the table, SEG, BFL, RTL, SUFFIX,
and STEM represent segmentation, baseform link-
age, root linkage, suffix tagging, and stem tagging,
816
Model Total Known Unk
SE
G baseline 96.75 99.64 69.11
hybrid 98.87 99.70 90.83
BF
L baseline 95.64 98.45 22.28
hybrid-morfette 96.19 98.05 78.40
hybrid-maxent 96.19 99.15 67.86
RT
L baseline 98.84 99.56 80.20
hybrid-morfette 99.05 99.44 88.86
hybrid-maxent 98.34 99.45 69.30
SU
FF
IX
mono. baseline 98.75 98.75 N/A
ind. baseline 96.74 98.78 0.01
maxent-mono 98.90 98.90 N/A
maxent-ind 98.90 98.90 N/A
ST
EM
mono. baseline 83.08 86.26 0.01
ind. baseline 53.24 86.90 0.00
maxent-mono 89.48 92.87 57.04
maxent-ind 88.43 90.26 40.59
Table 4: Word-level accuracies for the individual sub-
models used in the syromorph approach.
respectively. Even though the baselines were high,
each individual model outperformed its respective
baseline, with the exception of the root linker. Two
of the most interesting results are the known ac-
curacy of the baseform linkers hybrid-maxent and
hybrid-morfette. As hybrid models, the difference
between them lies only in the treatment of unknown
words; however, the known accuracy of the mor-
fette model drops fairly significantly. This is due
to the unknown words altering the weights for fea-
tures in which those words occur. For instance, if
the previous word is unknown and a baseform that
was never seen was predicted, then the weights on
the next word for all features that contain that un-
known word will be quite different than if that pre-
vious word were a known word.
It is also worth noting that the stem tagger is by
far the worst model in this group of models, but it is
also the most difficult task. The largest gains in im-
proving the entire systemwould come from focusing
attention on that task.
4.3 Joint Model Results
Table 5 shows the accuracies for the joint mod-
els. The joint model incorporating ?maxent? vari-
ants performs best overall and on known cases. The
Model Total Known Unk
Baseline 80.76 85.74 28.07
Morfette Monolithic 85.96 89.85 44.86
Maxent Monolithic 86.47 90.77 40.93
Table 5: Word-level accuracies for various joint syro-
morph models.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  10  20  30  40  50  60  70  80  90  100
To
ta
l A
cc
ur
ac
y
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 2: The total accuracy of the joint model.
joint model incorporating the ?morfette? variants
performs best on unknown cases.
Decision-level metrics for the SEG:hybrid /
BFL and RTL:hybrid-maxent / SUFFIX and
STEM:maxent-mono model are as follows: for
total-decisions, the model achieves an accuracy
of 97.08%, compared to 95.50% accuracy for the
baseline, amounting to a 35.11% reduction in error
rate over the baseline; for applicable-coverage and
applicable-accuracy this model achieved 93.45%
and 93.81%, respectively, compared to the baseline?s
90.03% and 91.44%.
Figures 2, 3, and 4 show learning curves for to-
tal, known, and unknown accuracies for the joint
pipeline model. As can be seen in Figure 2, by the
time we reach 10% of the training data, syromorph
is significantly better than the baseline. In fact, at
35% of the training data, our joint pipeline model
outperforms the baseline trained with all available
training data.
Figure 3 shows the baseline performing quite well
on known words with very low amounts of data.
Since the x-axis varies the amount of training data,
the meaning of ?known? and ?unknown? evolves as
we move to the right of the graph; consequently, the
817
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0  10  20  30  40  50  60  70  80  90  100
Kn
ow
n 
Ac
cu
ra
cy
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 3: The accuracy of the joint model on known
words.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0  10  20  30  40  50  60  70  80  90  100
Un
kn
ow
n 
Ac
cu
ra
cy
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 4: The accuracy of the joint model on unknown
words.
left and right sides of the graph are incomparable.
When the percentage of training data is very low,
the percentage of unknown words is high, and the
number of known words is relatively low. On this
dataset, the more frequent words tend to be less am-
biguous, giving the most-frequent taggers an advan-
tage in a small random sample. For this reason, the
baseline performs very well on known accuracy with
lower amounts of training data.
Figure 4 clearly shows that hybrid-morfette link-
ers outperform hybrid-maxent linkers on unknown
words. However, Figures 2- 4 show that hybrid-
morfette?s advantage on unknown words is coun-
teracted by its lower performance on known words;
therefore, it has slightly lower overall accuracy than
hybrid-maxent.
5 Related Work
The most closely related work to our approach is
the Morfette tool for labeling inflectional morphol-
ogy (Chrupa?a et al, 2008). Chrupa?a et al cre-
ated a tool that labels Polish, Romanian, and Span-
ish with morphological information as well as base-
forms. It is a supervised learning approach that
requires data labeled with both morphological tags
and baseforms. This approach creates two separate
models (a morphological tagger and a lemmatizer)
and combines the decoding process in order to cre-
ate a joint model that predicts both morphological
tags and the baseform. Morfette uses MEMMs for
both models and has access to predicted labels in
the feature set. Reported accuracy rates are 96.08%,
93.83%, and 81.19% for joint accuracy on datasets
trained with fewer than 100,000 tokens for Roma-
nian, Spanish, and Polish, respectively. The major
difference between this work and ours is the degree
of morphological analysis required by the languages.
Chrupa?a et al neglect segmentation, a task not as
intuitive for their languages as it is for Syriac. These
languages also require only linkage to a baseform, as
no root exists.
Also closely related is the work of Daya, Roth, and
Wintner (2008) on Hebrew. The authors use the no-
tion of patterns into which root consonants are in-
jected to compose Semitic words. They employ lin-
guistic knowledge (specifically, lists of prefixes, suf-
fixes, and ?knowledge of word-formation processes?
combined with SNoW, a multi-class classifier that
has been shown to work well in other NLP tasks.
The major difference between this approach and the
method presented in this paper is that this method
does not require the extra knowledge required to en-
code word-formation processes. A further point of
difference is our use of hybrid word- and consonant-
level models, after Haertel et al (2010). Their work
builds on the work of Shacham and Wintner (2007),
which is also related to that of Habash and Rambow,
described below.
Work by Lee et al (2003) is themost relevant work
for segmentation, since they segment Arabic, closely
related to Syriac, with a data-driven approach. Lee
et al use an unsupervised algorithm bootstrapped
with manually segmented data to learn the segmen-
tation for Arabic without any additional language re-
818
sources. At the heart of the algorithm is a word-level
trigram language model, which captures the correct
weights for prefixes and suffixes. They report an ac-
curacy of 97%. We opted to use our own segmenter
because we felt we could achieve higher accuracy
with the hybrid segmenter.
Mohamed and K?bler (2010a, 2010b) report on
closely related work for morphological tagging.
They use a data-driven approach to find the POS tags
for Arabic, using both word tokens and segmented
words as inputs for their system. Although their seg-
mentation performance is high, they report that ac-
curacy is lower when first segmenting word tokens.
They employ TiMBL, a memory-based learner, as
their model and report an accuracy of 94.74%.
Habash and Rambow (2005) currently have the
most accurate approach for Arabic morphological
analysis using additional language tools. They focus
on morphological disambiguation (tagging), given
morphological segmentation in the output of the
morphological analyzer. For each word, they first
run it through the morphological analyzer to reduce
the number of possible outputs. They then train a
separate Support Vector Machine (SVM) for each
morphological attribute (ten in all). They look at dif-
ferent ways of combining these outputs to match an
output from the morphological analyzer. For their
best model, they report an overall tag accuracy of
97.6%.
Others have used morphological analyzers and
other language tools for morphological disambigua-
tion coupled with segmentation. The following
works exemplify this approach: Diab et al (2004)
use a POS tagger to jointly segment, POS tag, and
chunk base-phrases for Arabic with SVMs. Kudo
et al (2004) use SVMs to morphologically tag
Japanese. Smith et al (2005) use SVMs for seg-
mentation, lemmatization, and POS tagging for Ara-
bic, Korean, and Czech. Petkevi? (2001) use a mor-
phological analyzer and additional simple rules for
morphological disambiguation of Czech. Mansour
et al (2007) and Bar-haim et al (2008) both use hid-
denMarkov models to POS tag Hebrew, with the lat-
ter including segmentation as part of the task.
For Syriac, a morphological analyzer is not avail-
able. Kiraz (2000) created a Syriac morphological
analyzer using finite-state methods; however, it was
developed on outdated and now inaccessible equip-
ment and is no longer working or available to us.
6 Conclusions and Future Work
We have shown that we can effectively model seg-
mentation, linkage to headwords in a dictionary, and
morphological tagging using a joint model called sy-
romorph. We have introduced novel approaches for
segmentation, dictionary linkage, and morphologi-
cal tagging, and each of these approaches has out-
performed its corresponding na?ve baseline. Further-
more, we have shown that for Syriac, a data-driven
approach seems to be an appropriate way to solve
these problems in an under-resourced setting.
We hope to use this combined model for pre-
annotation in an active learning setting to aid anno-
tators in labeling a large Syriac corpus. This corpus
will contain data spanning multiple centuries and a
variety of authors and genres. Future work will re-
quire addressing issues encountered in this corpus.
In addition, there is much to do in getting the over-
all tag accuracy closer to the accuracy of individual
decisions. We leave further feature engineering for
the stem tagger and the exploration of possible new
morphological tagging techniques for future work.
Finally, future work includes the application of the
syromorph methodology to other under-resourced
Semitic languages.
Acknowledgments
We would like to thank David Taylor of the Oriental
Institute at Oxford University for collaboration on
the design of the simplified tagset. We also recog-
nize the assistance of Ben Hansen of BYU on a sub-
set of the experimental results. Finally, we would
like to thank the anonymous reviewers for helpful
guidance.
References
Roy Bar-haim, Khalil Sima?an, and Yoad Winter. 2008.
Part-of-speech tagging ofmodern hebrew text. Natural
Language Engineering, 14(2):223?251.
British and Foreign Bible Society, editors. 1920. The
New Testament in Syriac. Oxford: Frederick Hall.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08).
819
Grzegorz Chrupa?a. 2006. Simple data-driven
context-sensitive lemmatization. In Procesamiento del
Lenguaje Natural, volume 37, pages 121 ? 127.
Ezra Daya, Dan Roth, and Shuly Wintner. 2008.
Identifying Semitic roots: Machine learning with
linguistic constraints. Computational Linguistics,
34(3):429?448.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic tagging of Arabic text: From
raw text to base phrase chunks. In Proceedings of
the 5th Meeting of the North American Chapter of
the Association for Computational Linguistics/Human
Language Technologies Conference (HLT-NAACL04),
pages 149?152.
Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cascading
errors: Approximate Bayesian inference for linguistic
annotation pipelines. In EMNLP ?06: Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, pages 618?626. Association for
Computational Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 573?580. Asso-
ciation for Computational Linguistics.
Nizar Habash and Owen Rambow. 2007. Arabic diacriti-
zation through full morphological tagging. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 53?56. Association for Computational Lin-
guistics.
Robbie Haertel, Peter McClanahan, and Eric K. Ring-
ger. 2010. Automatic diacritization for low-resource
languages using a hybrid word and consonant cmm.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 519?527.
Association for Computational Linguistics.
Jan Haji? and Barbora Hladk?. 1998. Tagging inflective
languages: Prediction of morphological categories for
a rich, structured tagset. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics,
pages 483?490. Association for Computational Lin-
guistics.
George Kiraz. 1994. Automatic concordance generation
of Syriac texts. In R. Lavenant, editor, VI Symposium
Syriacum 1992, pages 461?.
George Anton Kiraz. 2000. Multitiered nonlinear mor-
phology using multitape finite automata: a case study
on Syriac and Arabic. Computational Linguistics,
26:77?105.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to japanese
morphological analysis. In Proceedings of EMNLP,
pages 230?237.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 399?406. Associ-
ation for Computational Linguistics.
Saib Mansour, Khalil Sima?an, and Yoad Winter. 2007.
Smoothing a lexicon-based pos tagger for Arabic and
Hebrew. In Semitic ?07: Proceedings of the 2007
Workshop on Computational Approaches to Semitic
Languages, pages 97?103. Association for Computa-
tional Linguistics.
Emad Mohamed and Sandra K?bler. 2010a. Arabic
part of speech tagging. In Proceedings of the Sev-
enth International Language Resources and Evalua-
tion (LREC?10).
Emad Mohamed and Sandra K?bler. 2010b. Is Arabic
part of speech tagging feasible without word segmen-
tation? In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
705?708. Association for Computational Linguistics.
Vladim?r Petkevi?. 2001. Grammatical agreement
and automatic morphological disambiguation of inflec-
tional languages. In TSD ?01: Proceedings of the
4th International Conference on Text, Speech and Dia-
logue, pages 47?53. Springer-Verlag.
Danny Shacham and Shuly Wintner. 2007. Morpholog-
ical disambiguation of Hebrew: a case study in clas-
sifier combination. In Proceedings of EMNLP-CoNLL
2007, the Conference on Empirical Methods in Natural
Language Processing and the Conference on Compu-
tational Natural Language Learning. Association for
Computational Linguistics.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 475?482. Association for Computational Lin-
guistics.
K. Toutanova and C. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proceedings of EMNLP, pages
63?70.
820
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 519?527,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Diacritization for Low-Resource Languages Using a Hybrid
Word and Consonant CMM
Robbie A. Haertel, Peter McClanahan, and Eric K. Ringger
Department of Computer Science
Brigham Young University
Provo, Utah 84602, USA
rah67@cs.byu.edu, petermcclanahan@gmail.com, ringger@cs.byu.edu
Abstract
We are interested in diacritizing Semitic lan-
guages, especially Syriac, using only dia-
critized texts. Previous methods have required
the use of tools such as part-of-speech taggers,
segmenters, morphological analyzers, and lin-
guistic rules to produce state-of-the-art results.
We present a low-resource, data-driven, and
language-independent approach that uses a
hybrid word- and consonant-level conditional
Markov model. Our approach rivals the best
previously published results in Arabic (15%
WER with case endings), without the use of
a morphological analyzer. In Syriac, we re-
duce the WER over a strong baseline by 30%
to achieve a WER of 10.5%. We also report
results for Hebrew and English.
1 Introduction
Abjad writing systems omit vowels and other di-
acritics. The ability to restore these diacritics is
useful for personal, industrial, and governmental
purposes?especially for Semitic languages. In its
own right, the ability to diacritize can aid language
learning and is necessary for speech-based assis-
tive technologies, including speech recognition and
text-to-speech. Diacritics are also useful for tasks
such as segmentation, morphological disambigua-
tion, and machine translation, making diacritization
important to Natural Language Processing (NLP)
systems and intelligence gathering. In alphabetic
writing systems, similar techniques have been used
to restore accents from plain text (Yarowsky, 1999)
and could be used to recover missing letters in the
compressed writing styles found in email, text, and
instant messages.
We are particularly interested in diacritizing Syr-
iac, a low-resource dialect of Aramaic, which pos-
sesses properties similar to Arabic and Hebrew. This
work employs conditional Markov models (CMMs)
(Klein and Manning, 2002) to diacritize Semitic
(and other) languages and requires only diacritized
texts for training. Such an approach is useful for
languages (like Syriac) in which annotated data and
linguistic tools such as part-of-speech (POS) tag-
gers, segmenters, and morphological analyzers are
not available. Our main contributions are as follows:
(1) we introduce a hybrid word and consonant CMM
that allows access to the diacritized form of the pre-
vious words; (2) we introduce new features avail-
able in the proposed model; and (3) we describe an
efficient, approximate decoder. Our models signifi-
cantly outperform existing low-resource approaches
across multiple related and unrelated languages and
even achieve near state-of-the-art results when com-
pared to resource-rich systems.
In the next section, we review previous work rel-
evant to our approach. Section 3 then motivates and
describes the models and features used in our frame-
work, including a description of the decoder. We
describe our data in Section 4 and detail our exper-
imental setup in Section 5. Section 6 presents our
results. Finally, Section 7 briefly discusses our con-
clusions and offers ideas for future work.
2 Previous Work
Diacritization has been receiving increased attention
due to the rising interest in Semitic languages, cou-
519
pled with the importance of diacritization to other
NLP-related tasks. The existing approaches can be
categorized based on the amount of resources they
require, their basic unit of analysis, and of course
the language they are targeting. Probabilistic sys-
tems can be further divided into generative and con-
ditional approaches.
Existing methodologies can be placed along a
continuum based on the quantity of resources they
require?a reflection of their cost. Examples of
resources used include morphological analyzers
(Habash and Rambow, 2007; Ananthakrishnan et al,
2005; Vergyri and Kirchhoff, 2004; El-Sadany and
Hashish, 1989), rules for grapheme-to-sound con-
version (El-Imam, 2008), transcribed speech (Ver-
gyri and Kirchhoff, 2004), POS tags (Zitouni et al,
2006; Ananthakrishnan et al, 2005), and a list of
prefixes and suffixes (Nelken and Shieber, 2005).
When such resources exist for a particular language,
they typically improve performance. For instance,
Habash and Rambow?s (2007) approach reduces the
error rate of Zitouni et al?s (2006) by as much as
30% through its use of a morphological analyzer. In
fact, such resources are not always available. Sev-
eral data-driven approaches exist that require only
diacritized texts (e.g., Ku?bler and Mohamed, 2008;
Zitouni et al, 2006; Gal, 2002) which are relatively
inexpensive to obtain: most literate speakers of the
target language could readily provide them.
Apart from the quantity of resources required, di-
acritization systems also differ in their basic unit of
analysis. A consonant-based approach treats each
consonant1 in a word as a potential host for one
or more (possibly null) diacritics; the goal is to
predict the correct diacritic(s) for each consonant
(e.g., Ku?bler and Mohamed, 2008). Zitouni et al
(2006) extend the problem to a sequence labeling
task wherein they seek the best sequence of diacrit-
ics for the consonants. Consequently, their approach
has access to previously chosen diacritics.
Alternatively, the basic unit of analysis can be the
full, undiacritized word. Since morphological ana-
lyzers produce analyses of undiacritized words, di-
acritization approaches that employ them typically
fall into this category (e.g., Habash and Rambow,
1We refer to all graphemes present in undiacritized texts as
consonants.
2007; Vergyri and Kirchoff, 2004). Word-based,
low-resource solutions tend to treat the problem as
word-level sequence labeling (e.g., Gal, 2002).
Unfortunately, word-based techniques face prob-
lems due to data sparsity: not all words in the
test set are seen during training. In contrast,
consonant-based approaches rarely face the anal-
ogous problem of previously unseen consonants.
Thus, one low-resource solution to data sparsity is to
use consonant-based techniques for unknown words
(Ananthakrishnan et al, 2005; Nelken and Shieber,
2005).
Many of the existing systems, especially recent
ones, are probabilistic or contain probabilistic com-
ponents. Zitouni et al (2006) show the superior-
ity of their conditional-based approaches over the
best-performing generative approaches. However,
the instance-based learning approach of Ku?bler and
Mohamed (2008) slightly outperforms Zitouni et
al. (2006). In the published literature for Arabic,
the latter two have the best low-resource solutions.
Habash and Rambow (2007) is the state-of-the-art,
high-resource solution for Arabic. To our knowl-
edge, no work has been done in this area for Syriac.
3 Models
In this work, we are concerned with diacritiza-
tion for Syriac for which a POS tagger, segmenter,
and other tools are not readily available, but for
which diacritized text is obtainable.2 Use of a sys-
tem dependent on a morphological analyzer such as
Habash and Rambow?s (2007) is therefore not cost-
effective. Furthermore, we seek a system that is ap-
plicable to a wide variety of languages. Although
Ku?bler and Mohamed?s (2008) approach is compet-
itive to Zitouni et al?s (2006), instance-based ap-
proaches tend to suffer with the addition of new fea-
tures (their own experiments demonstrate this). We
desire to add linguistically relevant features to im-
prove performance and thus choose to use a condi-
tional model. However, unlike Zitouni et al (2006),
we use a hybrid word- and consonant-level approach
based on the following observations (statistics taken
from the Syriac training and development sets ex-
plained in Section 4):
2Kiraz (2000) describes a morphological analyzer for Syriac
that is not publicly available and is costly to reproduce.
520
1. Many undiacritized words are unambiguous:
90.8% of the word types and 63.5% of the to-
kens have a single diacritized form.
2. Most undiacritized word types have only a few
possible diacritizations: the average number of
possible diacritizations is 1.11.
3. Low-frequency words have low ambiguity:
Undiacritized types occurring fewer than 5
times have an average of 1.05 possible diacriti-
zations.
4. Diacritized words not seen in the training data
occur infrequently at test time: 10.5% of the
diacritized test tokens were not seen in training.
5. The diacritics of previous words can provide
useful morphological information such as per-
son, number, and gender.
Contrary to observations 1 and 2, consonant-level
approaches dedicate modeling capacity to an expo-
nential (in the number of consonants) number of
possible diacritizations of a word. In contrast, a
word-level approach directly models the (few) dia-
critized forms seen in training. Furthermore, word-
based approaches naturally have access to the dia-
critics of previous words if used in a sequence la-
beler, as per observation 5. However, without a
?backoff? strategy, word-level models cannot pre-
dict a diacritized form not seen in the training data.
Also, low-frequency words by definition have less
information from which to estimate parameters. In
contrast, abundant information exists for each dia-
critic in a consonant-level system. To the degree
to which they hold, observations 3 and 4 mitigate
these latter two problems. Clearly a hybrid approach
would be advantageous.
To this end we employ a CMM in which we treat
the problem as an instance of sequence labeling at
the word level with less common words being han-
dled by a consonant-level CMM. Let u be the undi-
acriatized words in a sentence. Applying an order o
Markov assumption, the distribution over sequences
of diacritized words d is:
P (d|u) =
?d?
?
i=1
P (di|di?o...i?1,u;?,?, ?) (1)
in which the local conditional distribution of a di-
acritized word is an interpolation of a word-level
model (?ui) and a consonant-level model (?):
P (di|di?o...i?1,u;?,?, ?) =
?P (di|di?o...i?1,u;?ui) +
(1 ? ?)P (di|di?o...i?1,u;?)
We let the consonant-level model be a standard
CMM, similar to Zitouni et al (2006), but with ac-
cess to previously diacritized words. Note that the
order of this ?inner? CMM need not be the same as
that of the outer CMM.
The parameter ? reflects the degree to which we
trust the word-level model. In the most general case,
? can be a function of the undiacritized words and
the previous o diacritized words. Based on our ear-
lier enumerated observations, we use a simple delta
function for ?: we let ? be 0 when ui is rare and 1
otherwise. We leave discussion for what constitutes
a ?rare? undiacritized type for Section 5.2.
Figure 1b presents a graphical model of a sim-
ple example sentence in Syriac. The diacritiza-
tion for non-rare words is predicted for a whole
word, hence the random variable D for each such
word. These diacritized words Di depend on previ-
ous Di?1 as per equation (1) for an order-1 CMM
(note that the capitalized A, I, and O are in fact con-
sonants in this transliteration). Because ?NKTA?
and ?RGT? are rare, their diacritization is repre-
sented by a consonant-level CMM: one variable for
each possible diacritic in the word. Importantly,
these consonant-level models have access to the pre-
viously diacritized word (D4 and D6, respectively).
We use log-linear models for all local distribu-
tions in our CMMs, i.e., we use maximum entropy
(maxent) Markov models (McCallum et al, 2000;
Berger et al, 1996). Due to the phenomenon known
as d-separation (Pearl and Shafer, 1988), it is possi-
ble to independently learn parameters for each word
model ?ui by training only on those instances for
the corresponding word. Similarly, the consonant
model can be learned independent of the word mod-
els. We place a spherical normal prior centered at
zero with a standard deviation of 1 over the weights
of all models and use an L-BFGS minimizer to find
the MAP estimate of the weights for all the models
(words and consonant).
521
?C C C C C C 
CSIA AO
5,1 5,2 5,3 5,4 5,1 5,2
(a)
? DHBA
?CSIA
? AO ?
? LA
D1
CSIA
D2
AO
D3
DHBA
D4
AO NKTA
D6
LA RGT
C5,1 C5,2 C5,3 C5,4 C7,1 C7,2 C7,3
(b)
Figure 1: Graphical models of Acts 20:33 in Syriac, CSIA AO DHBA AO NKTA LA RGT ?silver or gold or
garment I have not coveted,? using Kiraz?s (1994) transliteration for (a) the initial portion of a consonant-
level-only model and (b) a combined word- and consonant-level model. For clarity, both models assume a
consonant-level Markov order of 1; (b) shows a word-level Markov order of 1. For simplicity, the figure
further assumes that additional features come only from the current (undiacritized) word.
Note that Zitouni et al?s (2006) model is a spe-
cial case of equation (1) where all words are rare, the
word-level Markov order (o) is 0, and the consonant-
level Markov order is 2. A simplified version of Zi-
touni?s model is presented in Figure 1a.
3.1 Features
Our features are based on those found in Zitouni et
al. (2006), although we have added a few of our own
which we consider to be one of the contributions of
this paper. Unlike their work, our consonant-level
model has access to previously diacritized words,
allowing us to exploit information noted in obser-
vation 5.
Each of the word-level models shares the same set
of features, defined by the following templates:
? The prefixes and suffixes (up to 4 characters) of
the previously diacritized words.
? The string of the actual diacritics, including the
null diacritic, from each of the previous o dia-
critized words and n-grams of these strings; a
similar set of features is extracted but without
the null diacritics.
? Every possible (overlapping) n-gram of all
sizes from n = 1 to n = 5 of undiacritized
words contained within the window defined by
2 words to the right and 2 to the left. These
templates yield 15 features for each token.
? The count of how far away the current token
is from the beginning/end of the sentence up
to the Markov order; also, their binary equiva-
lents.
The first two templates rely on diacritizations of pre-
vious words, in keeping with observation 5.
The consonant-level model has the following fea-
ture templates:
? The current consonant.
? Previous diacritics (individually, and n-grams
of diacritics ending in the diacritic prior to the
current consonant, where n is the consonant-
level Markov order).
? Conjunctions of the first two templates.
? Indicators as to whether this is the first or last
consonant.
? The first three templates independently con-
joined with the current consonant.
? Every possible (overlapping) n-gram of all
sizes from n = 1 to n = 11 consisting of con-
sonants contained within the window defined
by 5 words to the right and 5 to the left.
? Same as previous, but available diacritics are
included in the window.
? Prefixes and suffixes (of up to length 4) of pre-
viously diacritized words conjoined with previ-
ous diacritics in the current token, both individ-
ually and n-grams of such.
522
This last template is only possible because of our
model?s dependency on previous diacritized words.
3.2 Decoder
Given a sentence consisting of undiacritized words,
we seek the most probable sequence of diacritized
words, i.e., arg maxd P (d|u...). In sentences con-
taining no rare words, the well-known Viterbi algo-
rithm can be used to find the optimum.
However, as can be seen in Figure 1b, predictions
in the consonant-level model (e.g., C5,1...4) depend
on previously diacritized words (D4), and some dia-
critized words (e.g., D6) depend on diacritics in the
previous rare word (C5,1...4). These dependencies
introduce an exponential number of states (in the
length of the word) for rare words, making exact de-
coding intractable. Instead, we apply a non-standard
beam during decoding to limit the number of states
for rare words to the n-best (locally). This is ac-
complished by using an independent ?inner? n-best
decoder for the consonant-level CMM to produce
the n-best diacritizations for the rare word given the
previous diacritized words and other features. These
become the only states to and from which transitions
in the ?outer? word-level decoder can be made. We
note this is the same type of decoding that is done in
pipeline models that use n-best decoders (Finkel et
al., 2006). Additionally, we use a traditional beam-
search of width 5 to further reduce the search space
both in the outer and inner CMMs.
4 Data
Although our primary interest is in the Syriac lan-
guage, we also experimented with the Penn Arabic
Treebank (Maamouri et al, 2004) for the sake of
comparison with other approaches. We include He-
brew to provide results for yet another Semitic lan-
guage. We also apply the models to English to show
that our method and features work well outside of
the Semitic languages. A summary of the datasets,
including the number of diacritics, is found in Fig-
ure 2. The number of diacritics shown in the table
is less than the number of possible predictions since
we treat contiguous diacritics between consonants as
a single prediction.
For our experiments in Syriac, we use the New
Testament portion of the Peshitta (Kiraz, 1994) and
lang diacs train dev test
Syriac 9 87,874 10,747 11,021
Arabic 8 246,512 42,105 51,664
Hebrew 17 239,615 42,133 49,455
English 5 1,004,073 80,156 89,537
Figure 2: Number of diacritics and size (in tokens)
of each dataset
treat each verse as if it were a sentence. The diacrit-
ics we predict are the five short vowels, as well as
Se?ya?me?, Rukka?kha?, Qus?s?a?ya?, and linea ocultans.
For Arabic, we use the training/test split defined
by Zitouni et al (2006). We group all words having
the same P index value into a sentence. We build our
own development set by removing the last 15% of
the sentences of the training set. Like Zitouni, when
no solution exists in the treebank, we take the first
solution as the gold tag. Zitouni et al (2006) report
results on several different conditions, but we focus
on the most challenging of the conditions: we pre-
dict the standard three short vowels, three tanween,
sukuun, shadda, and all case endings. (Preliminary
experiments show that our models perform equally
favorably in the other scenarios as well.)
For Hebrew, we use the Hebrew Bible (Old Tes-
tament) in the Westminster Leningrad Codex (Zefa-
nia XML Project, 2009). As with Syriac, we treat
each verse as a sentence and remove the paragraph
markers (pe and samekh). There is a large number
of diacritics that could be predicted in Hebrew and
no apparent standardization in the literature. For
these reasons, we attempt to predict as many dia-
critics as possible. Specifically, we predict the di-
acritics whose unicode values are 05B0-B9, 05BB-
BD, 05BF, 05C1-C2, and 05C4. We treat the follow-
ing list of punctuation as consonants: maqaf, paseq,
sof pasuq, geresh, and gershayim. The cantillation
marks are removed entirely from the data.
Our English data comes from the Penn Treebank
(Marcus et al, 1994). We used sections 0?20 as
training data, 21?22 as development data, and 23?
24 as our test set. Unlike words in the Semitic lan-
guages, English words can begin with a vowel, re-
quiring us to prepend a prosthetic consonant to every
word; we also convert all English text to lowercase.
523
5 Experiments
For all feature engineering and tuning, we trained
and tested on training and development test sets, re-
spectively (as specified above). Final results are re-
ported by folding the development test set into the
training data and evaluating on the blind test set. We
retain only those features that occur more than once.
For each approach, we report the Word Error Rate
(WER) (i.e., the percentage of words that were in-
correctly diacritized), along with the Diacritic Er-
ror Rate (DER) (i.e., the percentage of diacritics, in-
cluding the null diacritic, that were incorrectly pre-
dicted). We also report both WER and DER for
only those words that were not seen during training
(UWER and UDER, respectively). We found that
precision, recall, and f-score were nearly perfectly
correlated with DER; hence, we omit this informa-
tion for brevity.
5.1 Models for Evaluation
In previous work, Ku?bler et al (2008) report the
lowest error rates of the low-resource models. Al-
though their results are not directly comparable to
Zitouni et al (2006), we have independently con-
firmed that the former slightly outperforms the latter
using the same diacritics and on the same dataset
(see Figure 4), thereby providing the strongest pub-
lished baseline for Arabic on a common dataset. We
denote this model as ku?bler and use it as a strong
baseline for all datasets.
For the Arabic results, we additionally include Zi-
touni et al?s (2006) lexical model (zitouni-lex)
and their model that uses a segmenter and POS
tagger (zitouni-all), which are not immediately
available to us for Syriac. For yet another point of
reference for Arabic, we provide the results from the
state-of-the-art (resource-rich) approach of Habash
and Rambow (2007) (habash). This model is at an
extreme advantage, having access to a full morpho-
logical analyzer. Note that for these three models
we simply report their published results and do not
attempt to reproduce them.
Since ku?bler is of a different model class than
ours, we consider an additional baseline that is a
consonant-level CMM with access to the same in-
formation, namely, only those consonants within a
window of 5 to either side (ccmm). This is equiva-
lent to a special case of our hybrid model wherein
both the word-level and the consonant-level Markov
order are 0. The features that we extract from this
window are the windowed n-gram features.
In order to assess the utility of previous diacritics
and how effectively our features leverage them, we
build a model based on the methodology from Sec-
tion 3 but specify that all words are rare, effectively
creating a consonant-only model that has access to
the diacritics of previous words. We call this model
cons-only. We note that the main difference be-
tween this model and zitouni-lex are features
that depend on previous diacritized words.
Finally, we present results using our full hybrid
model (hybrid). We use a Markov order of 2 at
the word and consonant level for both hybrid and
cons-only.
5.2 Consonant-Level Model and Rare Words
The hybrid nature of hybrid naturally raises the
question of whether or not the inner consonant
model should be trained only on rare words or on
all of the data. In other words, is the distribution
of diacritics different in rare words? If so, the con-
sonant model should be trained only on rare words.
To answer this question, we trained our consonant-
level model (cons-only) on words occurring fewer
than n times. We swept the value of the threshold n
and compared the results to the same model trained
on a random selection of words. As can be seen in
Figure 3, the performance on unknown words (both
UWER and UDER) using a model trained on rare
words can be much lower than using a model trained
on the same amount of randomly selected data. In
fact, training on rare words can lead to a lower error
rate on unknown words than training on all tokens
in the corpus. This suggests that the distribution of
diacritics in rare words is different from the distri-
bution of diacritics in general. This difference may
come from foreign words, especially in the Arabic
news corpus.
While this phenomenon is more pronounced in
some languages and with some models more than
others, it appears to hold in the cases we tried. We
found the WER for unknown words to be lowest for
a threshold of 8, 16, 32, and 32 for Syriac, Arabic,
Hebrew, and English, respectively.
524
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10000  20000  30000  40000  50000  60000  70000  80000  90000
e
rr
o
r 
ra
te
tokens
UWER (random)
UWER (rare)
UDER (random)
UDER (rare)
(a) Syriac
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50000  100000  150000  200000  250000
e
rr
o
r 
ra
te
tokens
UWER (random)
UWER (rare)
UDER (random)
UDER (rare)
(b) Arabic
Figure 3: Learning curves showing impact on consonant-level models when training on rare tokens for
Syriac and Arabic. Series marked ?rare? were trained with the least common tokens in the dataset.
Approach WER DER UWER UDER
Sy
ria
c
ku?bler 15.04 5.23 64.65 18.21
ccmm 13.99 4.82 54.54 15.18
cons-only 12.31 5.03 55.68 19.09
hybrid 10.54 4.29 55.16 18.86
A
ra
bi
c
zitouni-lex 25.1 8.2 NA NA
ku?bler 23.61 7.25 66.69 20.51
ccmm 22.63 6.61 57.71 16.10
cons-only 15.02 5.15 48.10 15.76
hybrid 17.87 5.67 47.85 15.63
zitouni-all 18.0 5.5 NA NA
habash 14.9 4.8 NA NA
H
eb
re
w
ku?bler 30.60 12.96 89.52 36.86
ccmm 29.67 12.05 80.02 29.39
cons-only 23.39 10.92 75.70 33.34
hybrid 22.18 10.71 74.38 32.40
En
gl
ish
ku?bler 10.54 4.38 54.96 16.31
ccmm 11.60 4.71 58.55 16.34
cons-only 8.71 3.87 58.93 17.85
hybrid 5.39 2.38 57.24 16.51
Figure 4: Results for all languages and approaches
6 Discussion of Results
Since Syriac is of primary interest to us, we begin
by examining the results from this dataset. Syriac
appears to be easier to diacritize than Arabic, con-
sidering it has a similar number of diacritics and
only one-third the amount of data. On this dataset,
hybrid has the lowest WER and DER, achieving
nearly 30% and 18% reduction in WER and DER,
respectively, over ku?bler; it reduces both error
rates over cons-only by more than 14%. These
results attest to the effectiveness of our model in ac-
counting for the observations made in Section 3.
A similar pattern holds for the Hebrew and En-
glish datasets, namely that hybrid reduces the
WER over ku?bler by 28% to upwards of 50%;
cons-only also consistently and significantly out-
performs ku?bler and ccmm. However, the reduc-
tion in error rate for our cons-only and hybrid
models tends to be lower for DER than WER in
all languages except for English. In the case of
hybrid, this is probably because it is inherently
word-based. Having access to entire previous dia-
critized words may be a contributing factor as well,
especially in cons-only.
When comparing model classes (ku?bler and
ccmm), it appears that performance is comparable
across all languages, with the maxent approach en-
joying a slight advantage except in English. Interest-
ingly, the maxent solution usually handles unknown
words better, although it does not specifically target
this case. Both models outperform zitouni-lex
in Arabic, despite the fact that they use a much
simpler feature set, most notably, the lack of pre-
vious diacritics. In the case of ccmm this may be at-
tributable in part to our use of an L-BFGS optimizer,
convergence criteria, feature selection, or other po-
tential differences not noted in Zitouni et al (2006).
We note that the maxent-based approaches are much
more time and memory intensive.
Using the Arabic data, we are able to com-
pare our methods to several other published results.
525
The cons-only model significantly outperforms
zitouni-all despite the additional resources to
which the latter has access. This is evidence sup-
porting our hypothesis that the diacritics from pre-
vious words in fact contain useful information for
prediction. This empirically suggests that the inde-
pendence assumptions in consonant-only models are
too strict.
Perhaps even more importantly, our low-resource
method approaches the performance of habash. We
note that the differences may not be statistically sig-
nificant, and also that Habash and Rambow (2007)
omit instances in the data that lack solutions. In fact,
cons-only has a lower WER than all but two of
the seven techniques used by Habash and Rambow
(2007), which use a morphological analyzer.
Interestingly, hybrid does worse than
cons-only on this dataset, although it is still
competitive with zitouni-all. We hypothesize
that the observations from Section 3 do not hold
as strongly for this dataset. For this reason, using
a smooth interpolation function (rather than the
abrupt one we employ) may be advantageous and is
an interesting avenue for future research.
One last observation is that the approaches that
use diacritics from previous words (i.e., cons-only
and hybrid) usually have lower sentence error rates
(not shown in Figure 4). This highlights an advan-
tage of observation 5: that dependencies on previ-
ously diacritized words can help ensure a consistent
tagging within a sentence.
7 Conclusions and Future Work
In this paper, we have presented a low-resource so-
lution for automatic diacritization. Our approach is
motivated by empirical observations of the ambigu-
ity and frequency of undiacritized and diacritized
words as well as by the hypothesis that diacrit-
ics from previous words provide useful informa-
tion. The main contributions of our work, based
on these observations, are (1) a hybrid word-level
CMM combined with a consonant-level model for
rare words, (2) a consonant-level model with depen-
dencies on previous diacritized words, (3) new fea-
tures that leverage these dependencies, and (4) an
efficient, approximate decoder for these models. As
expected, the efficacy of our approach varies across
languages, due to differences in the actual ambigu-
ity and frequency of words in these languages. Nev-
ertheless, our models consistently reduce WER by
15% to nearly 50% over the best performing low-
resource models in the literature. In Arabic, our
models approach state-of-the-art despite not using a
morphological analyzer. Arguably, our results have
brought diacritization very close to being useful for
practical application, especially when considering
that we evaluated our method on the most difficult
task in Arabic, which has been reported to have dou-
ble the WER (Zitouni et al, 2006).
The success of this low-resource solution natu-
rally suggests that where more resources are avail-
able (e.g., in Arabic), they could be used to further
reduce error rates. For instance, it may be fruitful to
incorporate a morphological analyzer or segmenta-
tion and part-of-speech tags.
In future work, we would like to consider using
CRFs in place of MEMMs. Also, other approximate
decoders used in pipeline approaches could be ex-
plored as alternatives to the one we used (e.g., Finkel
et al, 2006). Additionally, we wish to include our
model as a stage in a pipeline that segments, dia-
critizes, and labels morphemes. Since obtaining data
for these tasks is substantially more expensive, we
hope to use active learning to obtain more data.
Our framework is applicable for any sequence la-
beling task that can be done at either a word or a
sub-word (e.g., character) level. Segmentation and
lemmatization are particularly promising tasks to
which our approach could be applied.
Finally, for the sake of completeness, we note that
more recent work has been done based on our base-
line models that has emerged since the preparation
of the current work, particularly Zitouni et al (2009)
and Mohamed et al (2009). We wish to address any
improvements captured by this more recent work
such as the use of different data sets and addressing
problems with the hamza to decrease error rates.
Acknowledgments
We thank Imed Zitouni, Nizar Habash, Sandra
Ku?bler, and Emad Mohamed for their assistance in
reconstructing datasets, models, and features.
526
References
S. Ananthakrishnan, S. Narayanan, and S. Bangalore.
2005. Automatic diacritization of Arabic transcripts
for automatic speech recognition. In Proceedings of
the International Conference on Natural Language
Processing.
A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996.
Amaximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22:39?71.
Y. A. El-Imam. 2008. Synthesis of the intonation of neu-
trally spoken Modern Standard Arabic speech. Signal
Processing, 88(9):2206?2221.
T. A. El-Sadany and M. A. Hashish. 1989. An Ara-
bic morphological system. IBM Systems Journal,
28(4):600?612.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
Bayesian inference for linguistic annotation pipelines.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 618?
626.
Y. Gal. 2002. An HMM approach to vowel restoration
in Arabic and Hebrew. In Proceedings of the ACL-
02 Workshop on Computational Approaches to Semitic
Languages, pages 1?7.
N. Habash and O. Rambow. 2007. Arabic diacritiza-
tion through full morphological tagging. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 53?56.
G. Kiraz. 1994. Automatic concordance generation of
Syriac texts. In R. Lavenant, editor, VI Symposium
Syriacum 1992, pages 461?471, Rome, Italy.
G. A. Kiraz. 2000. Multitiered nonlinear morphology
using multitape finite automata: a case study on Syr-
iac and Arabic. Computational Linguistics, 26(1):77?
105.
D. Klein and C. D. Manning. 2002. Conditional structure
versus conditional estimation in NLP models. In Pro-
ceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing, pages 9?16.
S. Ku?bler and E. Mohamed. 2008. Memory-based vocal-
ization of Arabic. In Proceedings of the LREC Work-
shop on HLT and NLP within the Arabic World.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proceedings of the
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19:313?330.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proceedings of the 17th In-
ternational Conference on Machine Learning, pages
591?598.
E. Mohamed and S. Ku?bler. 2009. Diacritization for
real-world Arabic texts. In Proceedings of Recent Ad-
vances in Natural Language Processing 2009.
R. Nelken and S. M. Shieber. 2005. Arabic diacritiza-
tion using weighted finite-state transducers. In Pro-
ceedings of the ACL Workshop on Computational Ap-
proaches to Semitic Languages, pages 79?86.
J. Pearl and G. Shafer. 1988. Probabilistic reasoning
in intelligent systems: networks of plausible inference.
Morgan Kaufman, San Mateo, CA.
D. Vergyri and K. Kirchhoff. 2004. Automatic diacritiza-
tion of Arabic for acoustic modeling in speech recog-
nition. In Proceedings of the COLING 2004Workshop
on Computational Approaches to Arabic Script-based
Languages, pages 66?73.
D. Yarowsky. 1999. A comparison of corpus-based tech-
niques for restoring accents in Spanish and French
text. Natural language processing using very large
corpora, pages 99?120.
Zefania XML Project. 2009. Zefania XML bible:
Leningrad codex. http://sourceforge.
net/projects/zefania-sharp/files/
Zefania\%20XML\%20Bibles\%204\
%20hebraica/Leningrad\%20Codex/sf_
wcl.zip/download.
I. Zitouni and R. Sarikaya. 2009. Arabic diacritic
restoration approach based on maximum entropymod-
els. Computer Speech & Language, 23(3):257?276.
I. Zitouni, J. S. Sorensen, and R. Sarikaya. 2006. Max-
imum entropy based restoration of Arabic diacritics.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 577?584.
527
Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 33?41,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Parallel Active Learning: Eliminating Wait Time with Minimal Staleness
Robbie Haertel, Paul Felt, Eric Ringger, Kevin Seppi
Department of Computer Science
Brigham Young University
Provo, Utah 84602, USA
rah67@cs.byu.edu, pablofelt@gmail.com,
ringger@cs.byu.edu, kseppi@cs.byu.edu
http://nlp.cs.byu.edu/
Abstract
A practical concern for Active Learning (AL)
is the amount of time human experts must wait
for the next instance to label. We propose a
method for eliminating this wait time inde-
pendent of specific learning and scoring al-
gorithms by making scores always available
for all instances, using old (stale) scores when
necessary. The time during which the ex-
pert is annotating is used to train models and
score instances?in parallel?to maximize the
recency of the scores. Our method can be seen
as a parameterless, dynamic batch AL algo-
rithm. We analyze the amount of staleness
introduced by various AL schemes and then
examine the effect of the staleness on perfor-
mance on a part-of-speech tagging task on the
Wall Street Journal. Empirically, the parallel
AL algorithm effectively has a batch size of
one and a large candidate set size but elimi-
nates the time an annotator would have to wait
for a similarly parameterized batch scheme to
select instances. The exact performance of our
method on other tasks will depend on the rel-
ative ratios of time spent annotating, training,
and scoring, but in general we expect our pa-
rameterless method to perform favorably com-
pared to batch when accounting for wait time.
1 Introduction
Recent emphasis has been placed on evaluating the
effectiveness of active learning (AL) based on re-
alistic cost estimates (Haertel et al, 2008; Settles
et al, 2008; Arora et al, 2009). However, to our
knowledge, no previous work has included in the
cost measure the amount of time that an expert an-
notator must wait for the active learner to provide in-
stances. In fact, according to the standard approach
to cost measurement, there is no reason not to use the
theoretically optimal (w.r.t. a model, training proce-
dure, and utility function) (but intractable) approach
(see Haertel et al, 2008).
In order to more fairly compare complex and
time-consuming (but presumably superior) selec-
tion algorithms with simpler (but presumably in-
ferior) algorithms, we describe ?best-case? (mini-
mum, from the standpoint of the payer) and ?worst-
case? (maximum) cost scenarios for each algorithm.
In the best-case cost scenario, annotators are paid
only for the time they spend actively annotating. The
worst-case cost scenario additionally assumes that
annotators are always on-the-clock, either annotat-
ing or waiting for the AL framework to provide them
with instances. In reality, human annotators work on
a schedule and are not always annotating or waiting,
but in general they expect to be paid for the time
they spend waiting for the next instance. In some
cases, the annotator is not paid directly for wait-
ing, but there are always opportunity costs associ-
ated with time-consuming algorithms, such as time
to complete a project. In reality, the true cost usually
lies between the two extremes.
However, simply analyzing only the best-case
cost, as is the current practice, can be misleading,
as illustrated in Figure 1. When excluding waiting
time for a particular selection algorithm1 (?AL An-
notation Cost Only?), the performance is much bet-
1We use the ROI-based scoring algorithm (Haertel et al,
2008) and the zero-staleness technique, both described below.
33
 0.78
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06
Acc
ura
cy
Time in Seconds
AL Annotation Cost OnlyRandom Total CostAL Total Cost
Figure 1: Accuracy as a function of cost (time).
Side-by-side comparison of best-case and worst-
case cost measurement scenarios reveals that not ac-
counting for the time required by AL to select in-
stances affects the evaluation of an AL algorithm.
ter than the cost of random selection (?Random Total
Cost?), but once waiting time is accounted for (?AL
Total cost?), the AL approach can be worse than ran-
dom. Given only the best-case cost, this algorithm
would appear to be very desirable. Yet, practition-
ers would be much less inclined to adopt this al-
gorithm knowing that the worst-case cost is poten-
tially no better than random. In a sense, waiting time
serves as a natural penalty for expensive selection
algorithms. Therefore, conclusions about the use-
fulness of AL selection algorithms should take both
best-case and worst-case costs into consideration.
Although it is current practice to measure only
best-case costs, Tomanek et al (2007) mention as a
desideratum for practical AL algorithms the need for
what they call fast selection time cycles, i.e., algo-
rithms that minimize the amount of time annotators
wait for instances. They address this by employing
the batch selection technique of Engleson and Da-
gan (1996). In fact, most AL practitioners and re-
searchers implicitly acknowledge the importance of
wait time by employing batch selection.
However, batch selection is not a perfect solution.
First, using the tradtional implementation, a ?good?
batch size must be specified beforehand. In research,
it is easy to try multiple batch sizes, but in practice
where there is only one chance with live annotators,
specifying a batch size is a much more difficult prob-
lem; ideally, the batch size would be set during the
process of AL. Second, traditional methods use the
same batch size throughout the entire learning pro-
cess. However, in the beginning stages of AL, mod-
els have access to very little training data and re-
training is often much less costly (in terms of time)
than in the latter stages of AL in which models are
trained on large amounts of data. Intuitively, small
batch sizes are acceptable in the beginning stages,
whereas large batch sizes are desirable in the latter
stages in order to mitigate the time cost of training.
In fact, Haertel et al (2008) mention the use of an
increasing batch size to speed up their simulations,
but details are scant and the choice of parameters for
their approach is task- and dataset-dependent. Also,
the use of batch AL causes instances to be chosen
without the benefit of all of the most recently anno-
tated instances, a phenomenon we call staleness and
formally define in Section 2. Finally, in batch AL,
the computer is left idle while the annotator is work-
ing and vice-verse.
We present a parallel, parameterless solution that
can eliminate wait time irrespective of the scoring
alogrithm and training method. Our approach is
based on the observation that instances can always
be available for annotation if we are willing to serve
instances that may have been selected without the
benefit of the most recent annotations. By having
the computer learner do work while the annotator is
busy annotating, we are able to mitigate the effects
of using these older annotations.
The rest of this paper will proceed as follows:
Section 2 defines staleness and presents a progres-
sion of four AL algorithms that strike different bal-
ances between staleness and wait time, culminat-
ing in our parallelized algorithm. We explain our
methodology and experimental parameters in Sec-
tion 3 and then present experimental results and
compare the four AL algorithms in Section 4. Con-
clusions and future work are presented in Section 5.
2 From Zero Staleness to Zero Wait
We work within a pool- and score-based AL setting
in which the active learner selects the next instance
from an unlabeled pool of data U . A scoring func-
tion ? (aka scorer) assigns instances a score using
a model ? trained on the labeled data A; the scores
serve to rank the instances. Lastly, we assume that
34
Input: A seed set of annotated instances A, a set of
pairs of unannotated instances and their
initial scores S, scoring function ?, the
candidate set size N , and the batch size B
Result: A is updated with the instances chosen by
the AL process as annotated by the oracle
while S 6= ? do1
? ? TrainModel(A)2
stamp? |A|3
C ? ChooseCandidates(S,N)4
K ? {(c[inst], ?(c[inst], ?)) | c ? C}5
S ? S ? C ?K6
T ? pairs from K with c[score] in the top B7
scores
for t ? T do8
S ? S ? t9
staleness? |A| ? stamp ; // unused10
A ? A? Annotate(t)11
end12
end13
Algorithm 1: Pool- and score-based active learner.
an unerring oracle provides the annotations. These
concepts are demonstrated in Algorithm 1.
In this section, we explore the trade-off between
staleness and wait time. In order to do so, it is bene-
ficial to quantitatively define staleness, which we do
in the context of Algorithm 1. After each model ?
is trained, a stamp is associated with that ? that indi-
cates the number of annotated instances used to train
it (see line 3). The staleness of an item is defined
to be the difference between the current number of
items in the annotated set and the stamp of the scorer
that assigned the instance a score. This concept can
be applied to any instance, but it is particularly in-
formative to speak of the staleness of instances at
the time they are actually annotated (we will simply
refer to this as staleness, disambiguating when nec-
essary; see line 10). Intuitively, an AL scheme that
chooses instances having less stale scores will tend
to produce a more accurate ranking of instances.
2.1 Zero Staleness
There is a natural trade-off between staleness and
the amount of time an annotator must wait for an
instance. Consider Algorithm 1 when B = 1 and
N = ? (we refer to this parameterization as ze-
rostale). In line 8, a single instance is selected for
annotation (|T | = B = 1); the staleness of this in-
stance is zero since no other annotations were pro-
vided between the time it was scored and the time it
was removed. Therefore, this algorithm will never
select stale instances and is the only way to guaran-
tee that no selected instances are stale.
However, the zero staleness property comes with
a price. Between every instance served to the an-
notator, a new model must be trained and every in-
stance scored using this model, inducing potentially
large waiting periods. Therefore, the following op-
tions exist for reducing the wait time:
1. Optimize the learner and scoring function (in-
cluding possible parallelization)
2. Use a different learner or scoring function
3. Parallelize the scoring process
4. Allow for staleness
The first two options are specific to the learning and
scoring algorithms, whereas we are interested in re-
ducing wait time independent of these in the general
AL framework. We describe option 3 in section 2.4;
however, it is important to note that when train-
ing time dominates scoring, the reduction in waiting
time will be minimal with this option. This is typi-
cally the case in the latter stages of AL when models
are trained on larger amounts of data.
We therefore turn our attention to option 4: in this
context, there are at least three ways to decrease the
wait time: (A) train less often, (B) score fewer items,
or (C) allow old scores to be used when newer ones
are unavailable. Strategies A and B are the batch se-
lection scheme of Engelson and Dagan (1996); an
algorithm that allows for these is presented as Al-
gorithm 1, which we refer to as ?traditional? batch,
or simply batch. We address the traditional batch
strategy first and then address strategy C.
2.2 Traditional Batch
In order to train fewer models, Algorithm 1 can pro-
vide the annotator with several instances scored us-
ing the same scorer (controlled by parameter B);
consequently, staleness is introduced. The first item
annotated on line 11 has zero staleness, having been
scored using a scorer trained on all available anno-
tated instances. However, since a model is not re-
trained before the next item is sent to the annotator,
35
the next items have staleness 1, 2, ? ? ? , B?1. By in-
troducing this staleness, the time the annotator must
wait is amortized across allB instances in the batch,
reducing the wait time by approximately a factor of
B. The exact effect of staleness on the quality of
instances selected is scorer- and data-dependent.
The parameter N , which we call the candidate set
size, specifies the number of instances to score. Typ-
ically, candidates are chosen in round-robin fash-
ion or with uniform probability (without replace-
ment) from U . If scoring is expensive (e.g., if it
involves parsing, translating, summarizing, or some
other time-consuming task), then reducing the can-
didate set size will reduce the amount of time spent
scoring by the same factor. Interestingly, this param-
eter does not affect staleness; instead, it affects the
probability of choosing the same B items to include
in the batch when compared to scoring all items.
Intuitively, it affects the probability of choosing B
?good? items. As N approaches B, this probabil-
ity approaches uniform random and performance ap-
proaches that of random selection.
2.3 Allowing Old Scores
One interesting property of Algorithm 1 is that line 7
guarantees that the only items included in a batch are
those that have been scored in line 5. However, if the
candidate set size is small (because scoring is expen-
sive), we could compensate by reusing scores from
previous iterations when choosing the best items.
Specifically, we change line 7 to instead be:
T ? pairs from S with c[score] in the top B scores
We call this allowold, and to our knowledge, it is a
novel approach. Because selected items may have
been scored many ?batches? ago, the expected stale-
ness will never be less than in batch. However, if
scores do not change much from iteration to itera-
tion, then old scores will be good approximations
of the actual score and therefore not all items nec-
essarily need to be rescored every iteration. Con-
sequently, we would expect the quality of instances
selected to approach that of zerostale with less wait-
ing time. It is important to note that, unlike batch,
the candidate set size does directly affect staleness;
smaller N will increase the likelihood of selecting
an instance scored with an old model.
2.4 Eliminating Wait Time
There are portions of Algorithm 1 that are trivially
parallelizable. For instance, we could easily split the
candidate set into equal-sized portions across P pro-
cessors to be scored (see line 5). Furthermore, it is
not necessary to wait for the scorer to finish training
before selecting the candidates. And, as previously
mentioned, it is possible to use parallelized training
and/or scoring algorithms. Clearly, wait time will
decrease as the speed and number of processors in-
crease. However, we are interested in parallelization
that can guarantee zero wait time independent of the
training and scoring algorithms without precluding
these other forms of parallelization.
All other major operations of Algorithm 1 have
serial dependencies, namely, we cannot score until
we have trained the model and chosen the candi-
dates, we cannot select the instances for the batch
until the candidate set is scored, and we cannot start
annotating until the batch is prepared. These depen-
dencies ultimately lead to waiting.
The key to eliminating this wait time is to ensure
that all instances have scores at all times, as in al-
lowold. In this way, the instance that currently has
the highest score can be served to the annotator with-
out having to wait for any training or scoring. If
the scored instances are stored in a priority queue
with a constant time extract-max operation (e.g., a
sorted list), then the wait time will be negligible.
Even a heap (e.g., binary or Fibonacci) will often
provide negligible overhead. Of course, eliminating
wait time comes at the expense of added staleness as
explained in the context of allowold.
This additional staleness can be reduced by allow-
ing the computer to do work while the oracle is busy
annotating. If models can retrain and score most in-
stances in the amount of time it takes the oracle to
annotate an item, then there will be little staleness.2
Rather than waiting for training to complete be-
fore beginning to score instances, the old scorer can
be used until a new one is available. This allows
us to train models and score instances in parallel.
Fast training and scoring procedures result in more
instances having up-to-date scores. Hence, the stale-
2Since the annotator requests the next instance immediately
after annotating the current instance, the next instance is virtu-
ally guaranteed to have a staleness factor of at least 1.
36
ness (and therefore quality) of selected instances de-
pends on the relative time required to train and score
models, thereby encouraging efficient training and
scoring algorithms. In fact, the other forms of par-
allelization previously mentioned can be leveraged
to reduce staleness rather than attempting to directly
reduce wait time.
These principles lead to Algorithm 2, which we
call parallel (for clarity, we have omitted steps re-
lated to concurrency). AnnotateLoop represents
the tireless oracle who constantly requests instances.
The call to Annotate is a surrogate for the actual
annotation process and most importantly, the time
spent in this method is the time required to provide
annotations. Once an annotation is obtained, it is
placed on a shared buffer B where it becomes avail-
able for training. While the annotator is, in effect,
a producer of annotations, TrainLoop is the con-
sumer which simply retrains models as annotated in-
stances become available on the buffer. This buffer
is analagous to the batch used for training in Algo-
rithm 1. However, the size of the buffer changes
dynamically based on the relative amounts of time
spent annotating and training. Finally, ScoreLoop
endlessly scores instances, using new models as
soon as they are trained. The set of instances scored
with a given model is analagous to the candidate set
in Algorithm 1.
3 Experimental Design
Because the performance of the parallel algorithm
and the ?worst-case? cost analysis depend on wait
time, we hold computing resources constant, run-
ning all experiments on a cluster of Dell PowerEdge
M610 servers equipped with two 2.8 GHz quad-core
Intel Nehalem processors and 24 GB of memory.
All experiments were on English part of speech
(POS) tagging on the POS-tagged Wall Street Jour-
nal text in the Penn Treebank (PTB) version 3 (Mar-
cus et al, 1994). We use sections 2-21 as initially
unannotated data and randomly select 100 sentences
to seed the models. We employ section 24 as the set
on which tag accuracy is computed, but do not count
evaluation as part of the wait time. We simulate an-
notation costs using the cost model from Ringger et
al. (2008): cost(s) = (3.80 ? l + 5.39 ? c+ 12.57),
where l is the number of tokens in the sentence, and
Input: A seed set of annotated instances A, a set of
pairs of unannotated instances and their
initial scores S, and a scoring function ?
Result: A is updated with the instances chosen by
the AL process as annotated by the oracle
B ? ?, ? ? null
Start(AnnotateLoop)
Start(TrainLoop)
Start(ScoreLoop)
procedure AnnotateLoop()
while S 6= ? do
t? c from S having max c[score]
S ? S ? t
B ? B ? Annotate(t)
end
end
procedure TrainLoop()
while S 6= ? do
? ? TrainModel(A)
A ? A? B
B ? ?
end
end
procedure ScoreLoop()
while S 6= ? do
c? ChooseCandidate(S)
S ?
S ? {c} ? {(c[inst], ?(c[inst], ?))|c ? S}
end
end
Algorithm 2: parallel
c is the number of pre-annotated tags that need cor-
rection, which can be estimated using the current
model. We use the same model for pre-annotation
as for scoring.
We employ the return on investment (ROI) AL
framework introduced by Haertel et. al (2008).
This framework requires that one define both a cost
and benefit estimate and selects instances that max-
imize benefit(x)?cost(x)cost(x) . For simplicity, we esti-
mate cost as the length of a sentence. Our bene-
fit model estimates the utility of each sentence as
follows: benefit(s) = ? log (maxt p(t|s)) where
p(t|s) is the probability of a tagging given a sen-
tence. Thus, sentences having low average (in the
geometric mean sense) per-tag probability are fa-
vored. We use a maximum entropy Markov model
to estimate these probabilities, to pre-annotate in-
stances, and to evaluate accuracy.
37
Figure 2: Staleness of the allowold algorithm over
time for different candidate set sizes
4 Results
Two questions are pertinent regarding staleness:
how much staleness does an algorithm introduce?
and how detrimental is that staleness? For zerostale
and batch, the first question was answered analyti-
cally in a previous section. We proceed by address-
ing the answer empirically for allowold and parallel
after which we examine the second question.
Figure 2 shows the observed staleness of instances
selected for annotation over time and for varying
candidate set sizes for allowold. As expected, small
candidate sets induce more staleness, in this case in
very high amounts. Also, for any given candidate
set size, staleness decreases over time (after the be-
ginning stages), since the effective candidate set in-
cludes an increasingly larger percentage of the data.
Since parallel is based on the same allow-old-
scores principle, it too could potentially see highly
stale instances. However, we found the average per-
instance staleness of parallel to be very low: 1.10; it
was never greater than 4 in the range of data that we
were able to collect. This means that for our task and
hardware, the amount of time that the oracle takes to
annotate an instance is high enough to allow new
models to retrain quickly and score a high percent-
age of the data before the next instance is requested.
We now examine effect that staleness has on
AL performance, starting with batch. As we have
shown, higher batch sizes guarantee more staleness
so we compare the performance of several batch
sizes (with a candidate set size of the full data) to ze-
rostale and random. In order to tease out the effects
that the staleness has on performance from the ef-
fects that the batches have on wait time (an element
of performance), we purposely ignore wait time.
The results are shown in Figure 3. Not surprisingly,
zerostale is slightly superior to the batch methods,
and all are superior to random selection. Further-
more, batch is not affected much by the amount of
staleness introduced by reasonable batch sizes: for
B < 100 the increase in cost of attaining 95% accu-
racy compared to zerostale is 3% or less.
Recall that allowold introduces more staleness
than batch by maintaining old scores for each in-
stance. Figure 4 shows the effect of different
candidate set sizes on this approach while fixing
batch size at 1 (wait time is excluded as before).
Larger candidate set sizes have less staleness, so
not surprisingly performance approaches zerostale.
Smaller candidate set sizes, having more staleness,
perform similarly to random during the early stages
when the model is changing more drastically each
instance. In these circumstances, scores produced
from earlier models are not good approximations to
the actual scores so allowing old scores is detrimen-
tal. However, once models stabilize and old scores
become better approximations, performance begins
to approach that of zerostale.
Figure 6 compares the performance of allowold
for varying batch sizes for a fixed candidate set size
(5000; results are similiar for other settings). As
before, performance suffers primarily in the early
stages and for the same reasons. However, a batch
excerbates the problem since multiple instances with
poor scores are selected simultaneously. Neverthe-
less, the performance appears to mostly recover once
the scorers become more accurate. We note that
batch sizes of 5 and 10 increase the cost of acheiving
95% accuracy by 3% and 10%, respectively, com-
pared to zerostale. The implications for parallel
are that stalness may not be detrimental, especially
if batch sizes are small and candidate set sizes are
large in the beginning stages of AL.
Figure 5 compares the effect of staleness on all
four algorithms when excluding wait time (B = 20,
N = 5000 for the batch algorithms). After achiev-
ing around 85% accuracy, batch and parallel are
virtually indistinguishable from zerostale, implying
that the staleness in these algorithms is mostly ignor-
able. Interestingly, allowold costs around 5% more
than zerostale to acheive an accuracy of 95%. We
attribute this to increased levels of staleness which
parallel combats by avoiding idle time.
38
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06
Acc
ura
cy
Time in Seconds
Zero StalenessBatch size 5Batch size 50Batch size 500Batch size 5000Random
Figure 3: Effect of staleness due to batch size for
batch, N =?
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06
Acc
ura
cy
Time in Seconds
Zero StalenessCandidate Set Size 5000Candidate Set Size 500Candidate Set Size 100Candidate Set Size 50Random
Figure 4: Effect of staleness due to candidate set size
for allowold, B = 1
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06
Acc
ura
cy
Time in Seconds
Zero StalenessParallelTraditional BatchAllow Old ScoresRandom
Figure 5: Comparison of algorithms (not including
wait time)
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06
Acc
ura
cy
Time in Seconds
Zero StalenessBatch Size 5Batch Size 10Batch Size 50Batch Size 100Batch Size 500Random
Figure 6: Effect of staleness due to batch size for
allowold, N = 5000
 0
 50000
 100000
 150000
 200000
 250000
 0  2000  4000  6000  8000  10000  12000
Ins
tan
ces
 Sc
ore
d
Model Number
Figure 7: Effective candidate set size of parallel
over time
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 100000  200000  300000  400000  500000  600000  700000  800000  900000  1e+06
Acc
ura
cy
Time in Seconds
Zero StalenessParallelTraditional BatchAllow Old ScoresRandom
Figure 8: Comparison of algorithms (including wait
time)
39
Since the amount of data parallel uses to train
models and score instances depends on the amount
of time instances take to annotate, the ?effective?
candidate set sizes and batch sizes over time is of in-
terest. We found that the models were always trained
after receiving exactly one instance, within the data
we were able to collect. Figure 7 shows the number
of instances scored by each successive scorer, which
appears to be very large on average: over 75% of the
time the scorer was able to score the entire dataset.
For this task, the human annotation time is much
greater than the amount of time it takes to train new
models (at least, for the first 13,000 instances). The
net effect is that under these conditions, parallel is
parameterized similar to batch with B = 1 and N
very high, i.e., approaching zerostale, and therefore
has very low staleness, yet does so without incurring
the waiting cost.
Finally, we compare the performance of the four
algorithms using the same settings as before, but in-
clude wait time as part of the cost. The results are
in Figure 8. Importantly, parallel readily outper-
forms zerostale, costing 40% less to reach 95% ac-
curacy. parallel also appears to have a slight edge
over batch, reducing the cost to acheive 95% accu-
racy by a modest 2%; however, had the simulation
continued, we we may have seen greater gains given
the increasing training time that occurs later on. It
is important to recognize in this comparison that the
purpose of parallel is not necessarily to significantly
outperform a well-tuned batch algorithm. Instead,
we aim to eliminate wait time without requiring pa-
rameters, while hopefully maintaining performance.
These results suggest that our approach successfully
meets these criteria.
Taken as a whole, our results appear to indicate
that the net effect of staleness is to make selection
more random. Models trained on little data tend to
produce scores that are not reflective of the actual
utility of instances and essentially produce a ran-
dom ranking of instances. As more data is collected,
scores become more accurate and performance be-
gins to improve relative to random selection. How-
ever, stale scores are by definition produced using
models trained with less data than is currently avail-
able, hence more staleness leads to more random-
like behavior. This explains why batch selection
tends to perform well in practice for ?reasonable?
batch sizes: the amount of staleness introduced by
batch (B?12 on average for a batch of size B) intro-
duces relatively little randomness, yet cuts the wait
time by approximately a factor of B.
This also has implications for our parallel method
of AL. If a given learning algorithm and scoring
function outperform random selection when using
zerostale and excluding wait time, then any added
staleness should cause performance to more closely
resemble random selection. However, once wait-
ing time is accounted for, performance could ac-
tually degrade below that of random. In parallel,
more expensive training and scoring algorithms are
likely to introduce larger amounts of staleness, and
would cause performance to approach random selec-
tion. However, parallel has no wait time, and hence
our approach should always perform at least as well
as random in these circumstances. In contrast, poor
choices of parameters in batch could perform worse
than random selection.
5 Conclusions and Future Work
Minimizing the amount of time an annotator must
wait for the active learner to provide instances is an
important concern for practical AL. We presented a
method that can eliminate wait time by allowing in-
stances to be selected on the basis of the most re-
cently assigned score. We reduce the amount of
staleness this introduces by allowing training and
scoring to occur in parallel while the annotator is
busy annotating. We found that on PTB data us-
ing a MEMM and a ROI-based scorer that our pa-
rameterless method performed slightly better than a
hand-tuned traditional batch algorithm, without re-
quiring any parameters. Our approach?s parallel na-
ture, elimination of wait time, ability to dynamically
adapt the batch size, lack of parameters, and avoid-
ance of worse-than-random behavior, make it an at-
tractive alternative to batch for practical AL.
Since the performance of our approach depends
on the relative time spent annotating, training, and
scoring, we wish to apply our technique in future
work to more complex problems and models that
have differing ratios of time spent in these areas. Fu-
ture work could also draw on the continual compu-
tation framework (Horvitz, 2001) to utilize idle time
in other ways, e.g., to predict annotators? responses.
40
References
S. Arora, E. Nyberg, and C. P. Rose?. 2009. Estimating
annotation cost for active learning in a multi-annotator
environment. In Proceedings of the NAACL HLT 2009
Workshop on Active Learning for Natural Language
Processing, pages 18?26.
S. P. Engelson and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora. In
Proceedings of the 34th annual meeting on Associa-
tion for Computational Linguistics, pages 319?326.
R. A. Haertel, K. D. Seppi, E. K. Ringger, and J. L. Car-
roll. 2008. Return on investment for active learning.
In NIPS Workshop on Cost Sensitive Learning.
E. Horvitz. 2001. Principles and applications of con-
tinual computation. Artificial Intelligence Journal,
126:159?96.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19:313?330.
E. Ringger, M. Carmen, R. Haertel, K. Seppi, D. Lond-
sale, P. McClanahan, J. Carroll, and N. Ellison. 2008.
Assessing the costs of machine-assisted corpus anno-
tation through a user study. In Proc. of LREC.
B. Settles, M. Craven, and L. Friedland. 2008. Active
learning with real annotation costs. In Proceedings of
the NIPS Workshop on Cost-Sensitive Learning, pages
1069?1078.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts annota-
tion costs and maintains reusability of annotated data.
Proc. of EMNLP-CoNLL, pages 486?495.
41
