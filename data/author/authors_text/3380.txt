Proceedings of the Second Workshop on Statistical Machine Translation, pages 181?184,
Prague, June 2007. c?2007 Association for Computational Linguistics
Getting to know Moses:  
Initial experiments on German?English factored translation  
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg 
Department of Computer and Information Science 
Link?pings universitet, Sweden 
{marho,sarst,lah}@ida.liu.se 
 
 
 
Abstract 
We present results and experiences from 
our experiments with phrase-based statisti-
cal machine translation using Moses. The 
paper is based on the idea of using an off-
the-shelf parser to supply linguistic infor-
mation to a factored translation model and 
compare the results of German?English 
translation to the shared task baseline sys-
tem based on word form. We report partial 
results for this model and results for two 
simplified setups. Our best setup takes ad-
vantage of the parser?s lemmatization and 
decompounding. A qualitative analysis of 
compound translation shows that decom-
pounding improves translation quality. 
1 Introduction  
One of the stated goals for the shared task of this 
workshop is ?to offer newcomers a smooth start 
with hands-on experience in state-of-the-art statis-
tical machine translation methods?. As our previ-
ous research in machine translation has been 
mainly concerned with rule-based methods, we 
jumped at this offer. 
We chose to work on German-to-English trans-
lation for two reasons. Our primary practical inter-
est lies with translation between Swedish and Eng-
lish, and of the languages offered for the shared 
task, German is the one closest in structure to 
Swedish. While there are differences in word order 
and morphology between Swedish and German, 
there are also similarities, e.g., that both languages 
represent nominal compounds as single ortho-
graphic words. We chose the direction from Ger-
man to English because our knowledge of English 
is better than our knowledge of German, making it 
easier to judge the quality of translation output. 
Experiments were performed on the Europarl data. 
With factored statistical machine translation, 
different levels of linguistic information can be 
taken into account during training of a statistical 
translation system and decoding. In our experi-
ments we combined syntactic and morphological 
factors from an off-the-shelf parser with the fac-
tored translation framework in Moses (Moses, 
2007). We wanted to test the following hypotheses: 
? Translation models based on lemmas will im-
prove translation quality (Popovi? and Ney, 
2004) 
? Decompounding German nominal compounds 
will improve translation quality (Koehn and 
Knight, 2003) 
? Re-ordering models based on word forms and 
parts-of-speech will improve translation qual-
ity (Zens and Ney, 2006). 
2 The parser 
The parser, Machinese Syntax, is a commercially 
available dependency parser from Connexor Oy 1. 
It provides each word with lemma, part-of-speech, 
morphological features and dependency relations 
(see Figure 1). In addition, the lemmas of com-
pounds are marked by a ?#? separating the two 
parts of the compound. For the shared task we only 
used shallow linguistic information: lemma, part-
of-speech and morphology. The compound bound-
ary identification was used to split noun com-
                                                 
1 Connexor Oy, http://www.connexor.com. 
181
pounds to make the German input more similar to 
English text. 
 
1 Mit   mit   pm>2    @PREMARK PREP 
2 Blick blick advl>10 @NH N MSC SG DAT 
3 auf   auf   pm>5    @PREMARK PREP 
 
Figure 1. Example of parser output 
 
We used the parser?s tokenization as given. Some 
common multiword units, such as ?at all? and ?von 
heute?, are treated as single words by the parser 
(cf. Niessen and Ney, 2004). The German parser 
also splits contracted prepositions and determiners 
like ?zum? ? ?zu dem? (?to the?). 
3 System description 
For our experiments with Moses we basically fol-
lowed the shared task baseline system setup to 
train our factored translation models. After training 
a statistical model, minimum error-rate tuning was 
performed to tune the model parameters. All ex-
periments were performed on an AMD 64 Athlon 
4000+ processor with 4 Gb of RAM and 32 bit 
Linux (Ubuntu).  
Since time as well as computer resources were 
limited we designed a model that we hoped would 
make the best use of all available factors. This 
model turned out to be too complex for our ma-
chine and in later experiments we abandoned it for 
a simpler model.  
3.1 Pre-processing 
In the pre-processing step we used the standard 
pre-processing of the shared task baseline system, 
parsed the German and English texts and processed 
the output to obtain four factors: word form, 
lemma, part-of-speech and morphology. Missing 
values for lemma, part-of-speech and morphology 
were replaced with default values. 
Noun compounds are very frequent in German, 
2.9% of all tokens in the tuning corpus were identi-
fied by the parser as noun compounds. Compounds 
tend to lead to sparse data problems and splitting 
them has been shown to improve German-English 
translation (Koehn and Knight, 2003). Thus we 
decided to decompund German noun compounds 
identified as such by our parser.  
We used a simple strategy to remove fillers and 
to correct some obvious mistakes. We removed the 
filler ?-s? that appear before a marked split unless it 
was one of ?-ss?, ?-urs?, ?-eis? or ?-us?. This applied 
to 35% of the noun compounds in the tuning cor-
pus. The fillers were removed both in the word 
form and the lemma (see Figure 2). 
There were some mistakes made by the parser, 
for instance on compounds containing the word 
?nahmen? which was incorrectly split as ?stel-
lungn#ahmen? instead of ?stellung#nahmen? 
(?statement?). These splits were corrected by mov-
ing the ?n? to the right side of the split. 
We then split noun-lemmas on hyphens unless 
there were numbers on either side of it and on the 
places marked by ?#?. Word forms were split in the 
corresponding places as the lemmas. 
The part-of-speech and morphology of the last 
word in the compound is the same as for the whole 
compound. For the other parts we hypothesized 
that part-of-speech is Noun and the morphology is 
unknown, marked by the tag UNK. 
 
Parser output: 
unionsl?nder unions#land N NEU PL ACC 
 
Factored output: 
union|union|N|UNK 
l?nder|land|N|NEU_PL_ACC 
 
Figure 2. Compound splitting for ?unionsl?nder? 
(?countries in the union?) 
 
These strategies are quite crude and could be fur-
ther refined by studying the parser output thor-
oughly to pinpoint more problems.  
3.2 Training translation models with linguis-
tic factors 
After pre-processing, the German?English Eu-
roparl training data contains four factors: 0: word 
form, 1: lemma, 2: part-of-speech, 3: morphology. 
As a first step in training our translation models we 
performed word alignment on lemmas as this could 
potentially improve word alignment. 
3.2.1 First setup 
Factored translation requires a number of decoding 
steps, which are either mapping steps mapping a 
source factor to a target factor or generation steps 
generating a target factor from other target factors. 
Our first setup contained three mapping steps, T0?
T2, and one generation step, G0.  
 
 
182
T0: 0-0 (word ? word) 
T1: 1-1 (lemma ? lemma) 
T2: 2,3-2,3  (pos+morph ? pos+morph) 
G0:  1,2,3-0  (lemma+pos+morph ? word)  
 
With the generation step, word forms that did not 
appear in the training data may still get translated 
if the lemma, part-of-speech and morphology can 
be translated separately and the target word form 
can be generated from these factors. 
Word order varies a great deal between German 
and English. This is especially true for the place-
ment of verbs. To model word order changes we 
included part-of-speech information and created 
two reordering models, one based on word form 
(0), the other on part-of-speech (2): 
 
0-0.msd-bidirectional-fe 
2-2.msd-bidirectional-fe 
 
The decoding times for this setup turned out to be 
unmanageable. In the first iteration of parameter 
tuning, decoding times were approx. 6 
min/sentence. In the second iteration decoding 
time increased to approx. 30 min/sentence.  Re-
moving one of the reordering models did not result 
in a significant change in decoding time. Just trans-
lating the 2000 sentences of test data with untuned 
parameters would take several days. We inter-
rupted the tuning and abandoned this setup. 
3.2.2 Second setup 
Because of the excessive decoding times of the 
first factored setup we resorted to a simpler system 
that only used the word form factor for the transla-
tion and reordering models. This setup differs from 
the shared task baseline in the following ways: 
First, it uses the tokenization provided by the 
parser. Second, alignment was performed on the 
lemma factor. Third, German compounds were 
split using the method described above. To speed 
up tuning and decoding, we only used the first 200 
sentences of development data (dev2006) for tun-
ing and reduced stack size to 50.  
 
T0: 0-0 (word ? word) 
R:  0-0.msd-bidirectional-fe 
3.2.3 Third setup 
To test our hypothesis that word reordering would 
benefit from part-of-speech information we created 
another simpler model. This setup has two map-
ping steps, T0 and T1, and a reordering model 
based on part-of-speech.  
 
T0: 0-0 (word ? word) 
T1: 2,3-2,3 (pos+morph ? pos+morph) 
R: 2-2.msd-bidirectional-fe 
4 Results  
We compared our systems to a baseline system 
with the same setup as the WMT2007 shared task 
baseline system but tuned with our system?s sim-
plified tuning settings (200 instead of 2000 tuning 
sentences, stack size 50). Table 1 shows the Bleu 
improvement on the 200 sentences development 
data from the first and last iteration of tuning. 
 
Dev2006 (200) System 
1st iteration Last iteration 
Baseline 19.56 27.07 
First 21.68 - 
Second 20.43 27.16 
Third 20.72 24.72 
 Table 1. Bleu scores on 200 sentences of tuning 
data before and after tuning 
 
The final test of our systems was performed on the 
development test corpus (devtest2006) using stack 
size 50. The results are shown in Table 2. The low 
Bleu score for the third setup implies that reorder-
ing on part-of-speech is not enough on its own. 
The second setup performed best with a slightly 
higher Bleu score than the baseline. We used the 
second setup to translate test data for our submis-
sion to the shared task.  
 
System Devtest2006 (NIST/Bleu) 
Baseline 6.7415 / 25.94  
First - 
Second  6.8036 / 26.04 
Third 6.5504 / 24.57 
Table 2. NIST and Bleu scores on development 
test data 
4.1 Decompounding 
We have evaluated the decompounding strategy by 
analyzing how the first 75 identified noun com-
pounds of the devtest corpus were translated by our 
second setup compared to the baseline. The sample 
183
excluded doubles and compounds that had no clear 
translation in the reference corpus.  
Out of these 75 compounds 74 were nouns that 
were correctly split and 1 was an adjective that was 
split incorrectly: ?allumfass#ende?. Despite that it 
was incorrectly identified and split it was trans-
lated satisfyingly to ?comprehensive?. 
The translations were grouped into the catego-
ries shown in Table 3. The 75 compounds were 
classified into these categories for our second sys-
tem and the baseline system, as shown in Table 4. 
As can be seen the compounds were handled better 
by our system, which had 62 acceptable transla-
tions (C or V) compared to 48 for the baseline and 
did not leave any noun compounds untranslated.  
 
Table 3. Classification scheme with examples for 
compound translations 
 
Table 4. Classification of 75 compounds from our 
second system and the baseline system 
Decompounding of nouns reduced the number 
of untranslated words, but there were still some 
left. Among these were cases that can be handled 
such as separable prefix verbs like ?aufzeigten? 
(?pointed out?) (Niessen and Ney, 2000) or adjec-
tive compounds such as ?multidimensionale? 
(?multi dimensional?). There were also some noun 
compounds left which indicates that we might need 
a better decompounding strategy than the one used 
by the parser (see e.g. Koehn and Knight, 2003). 
4.2 Experiences and future plans  
With the computer equipment at our disposal, 
training of the models and tuning of the parameters 
turned out to be a very time-consuming task. For 
this reason, the number of system setups we could 
test was small, and much fewer than we had hoped 
for. Thus it is too early to draw any conclusions as 
regards our hypotheses, but we plan to perform 
more tests in the future, also on Swedish?English 
data. The parser's ability to identify compounds 
that can be split before training seems to give a 
definite improvement, however, and is a feature 
that can likely be exploited also for Swedish-to-
English translation with Moses. 
References 
Koehn, Philipp and Kevin Knight, 2003. Empirical 
methods for compound splitting. In Proceedings of 
EACL 2003, 187-194. Budapest, Hungary. 
Moses ? a factored phrase-based beam-search decoder 
for machine translation. 13 April 2007,  URL:  
http://www.statmt.org/moses/ . 
Niessen, Sonja and Hermann Ney, 2004. Statistical ma-
chine translation with scarce resources using mor-
pho-syntactic information. Computational Linguis-
tics, 181-204 . 
Niessen, Sonja and Hermann Ney, 2000. Improving 
SMT Quality with Morpho-syntactic Analysis. In 
Proceedings of Coling 2000. 1081-1085. Saar-
br?cken, Germany.  
Popovi?, Maja and Hermann Ney, 2004.  Improving 
Word Alignment Quality using Morpho-Syntactic In-
formation. In Proceedings of Coling 2004, 310-314, 
Geneva, Switzerland. 
Zens, Richard and Hermann Ney, 2006. Discriminative 
Reordering Models for Statistical Machine Transla-
tion. In HLT-NAACL: Proceedings of the Workshop 
on Statistical Machine Translation, 55-63, New York 
City, NY.  
Category Example 
C-correct Regelungsentwurf 
Draft regulation  
Ref: Draft regulation 
V-variant Schlachth?fen 
Abattoirs  
Ref: Slaughter houses  
P-partly correct Anpassungsdruck 
Pressure 
Ref: Pressure for adaption 
F-wrong form L?nderberichte 
Country report  
Ref: Country reports 
W-wrong Erbonkel 
Uncle dna  
Ref: Sugar daddy 
U-untranslated Schlussentwurf 
Schlussentwurf  
Ref: Final draft  
Baseline system 
 C V P W U F Tot 
C 36 1 3  3 1 44 
V 1 9 2 1 5  18 
P   3  2  5 
W    1 2  3 
U       0 
F 1     4 5 S
ec
on
d 
sy
st
em
 
Tot 38 10 8 2 12 5 75 
184
Proceedings of the Third Workshop on Statistical Machine Translation, pages 135?138,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Effects of Morphological Analysis in Translation between German and
English
Sara Stymne, Maria Holmqvist and Lars Ahrenberg
Department of Computer and Information Science
Linko?ping University, Sweden
{sarst,marho,lah}@ida.liu.se
Abstract
We describe the LIU systems for German-
English and English-German translation sub-
mitted to the Shared Task of the Third Work-
shop of Statistical Machine Translation. The
main features of the systems, as compared
with the baseline, is the use of morphologi-
cal pre- and post-processing, and a sequence
model for German using morphologically rich
parts-of-speech. It is shown that these addi-
tions lead to improved translations.
1 Introduction
Research in statistical machine translation (SMT)
increasingly makes use of linguistic analysis in order
to improve performance. By including abstract cat-
egories, such as lemmas and parts-of-speech (POS),
in the models, it is argued that systems can become
better at handling sentences for which training data
at the word level is sparse. Such categories can be
integrated in the statistical framework using factored
models (Koehn et al, 2007). Furthermore, by pars-
ing input sentences and restructuring based on the
result to narrow the structural difference between
source and target language, the current phrase-based
models can be used more effectively (Collins et al,
2005).
German differs structurally from English in sev-
eral respects (see e.g. Collins et al, 2005). In this
work we wanted to look at one particular aspect
of restructuring, namely splitting of German com-
pounds, and evaluate its effect in both translation di-
rections, thus extending the initial experiments re-
ported in Holmqvist et al (2007). In addition, since
German is much richer in morphology than English,
we wanted to test the effects of using a sequence
model for German based on morphologically sub-
categorized parts-of-speech. All systems have been
specified as extensions of the Moses system pro-
vided for the Shared Task.
2 Part-of-speech and Morphology
For both English and German we used the part-of-
speech tagger TreeTagger (Schmid, 1994) to obtain
POS-tags.
The German POS-tags from TreeTagger were re-
fined by adding morphological information from
a commercial dependency parser, including case,
number, gender, definiteness, and person for nouns,
pronouns, verbs, adjectives and determiners in the
cases where both tools agreed on the POS-tag. If
they did not agree, the POS-tag from TreeTagger
was chosen. This tag set seemed more suitable for
SMT, with tags for proper names and foreign words
which the commercial parser does not have.
3 Compound Analysis
Compounding is common in many languages, in-
cluding German. Since compounding is highly pro-
ductive it increases vocabulary size and leads to
sparse data problems.
Compounds in German are formed by joining
words, and in addition filler letters can be inserted
or letters can be removed from the end of all but the
last word of the compound (Langer, 1998). We have
chosen to allow simple additions of letter(s) (-s, -n,
-en, -nen, -es, -er, -ien) and simple truncations (-e,
135
-en, -n). Example of compounds with additions and
truncations can be seen in (1).
(1) a. Staatsfeind (Staat + Feind)
public enemy
b. Kirchhof (Kirche + Hof)
graveyard
3.1 Splitting compounds
Noun and adjective compounds are split by a mod-
ified version of the corpus-based method presented
by Koehn and Knight (2003). First the German lan-
guage model data is POS-tagged and used to calcu-
late frequencies of all nouns, verbs, adjectives, ad-
verbs and the negative particle. Then, for each noun
and adjective all splits into these known words from
the corpus, allowing filler additions and truncations,
are considered, choosing the splitting option with
the highest arithmetic mean1 of the frequencies of
its parts.
A length limit of each part was set to 4 charac-
ters. For adjectives we restrict the number of parts
to maximum two, since they do not tend to have
multiple parts as often as nouns. In addition we
added a stop list with 14 parts, often mistagged, that
gave rise to wrong adjective splits, such as arische
(?Aryan?) in konsularische (?consular?).
As Koehn and Knight (2003) points out, parts of
compounds do not always have the same meaning
as when they stand alone, e.g. Grundrechte (?basic
rights?), where the first part, Grund, usually trans-
lates as foundation, which is wrong in this com-
pound. To overcome this we marked all compound
parts but the last, with the symbol ?#?. Thus they are
handled as separate words. Parts of split words also
receive a special POS-tag, based on the POS of the
last word of the compound, and the last part receives
the same POS as the full word.
We also split words containing hyphens based on
the same algorithm. Their parts receive a different
POS-tag, and the hyphens are left at the end of all
but the last part.
1We choose the arithmetic mean over the geometric mean
used by Koehn and Knight (2003) in order to increase the num-
ber of splits.
3.2 Merging compounds
For translation into German, the translation output
contains split compounds, which need to be merged.
An algorithm for merging has been proposed by
Popovic? et al (2006) using lists of compounds and
their parts. This method cannot merge unseen com-
pounds, however, so instead we base merging on
POS. If a word has a compound-POS, and the fol-
lowing word has a matching POS, they are merged.
If the next POS does not match, a hyphen is added
to the word, allowing for coordinated compounds as
in (2).
(2) Wasser- und Bodenqualita?t
water and soil quality
4 System Descriptions
The main difference of our system in relation to the
baseline system of the Shared Task2 is the pre- and
post-processing described above, the use of a POS
factor, and an additional sequence model on POS.
We also modified the tuning to include compound
merging, and used a smaller corpus, 600 sentences
picked evenly from the dev2006 corpus, for tuning.
We use the Moses decoder (Koehn et al, 2007) and
SRILM language models (Stolcke, 2002).
4.1 German ? English
We used POS as an output factor, as can be seen in
Figure 1. Using additional factors only on the tar-
get side means that only the training data need to be
POS-tagged, not the tuning data or translation input.
However, POS-tagging is still performed for Ger-
man as input to the pre-processing step. As Figure 1
shows we have two sequence models. A 5-gram lan-
guage model based on surface form using Kneser-
Ney smoothing and in addition a 7-gram sequence
model based on POS using Witten-Bell3 smoothing.
The training corpus was filtered to sentences with
2?40 words, resulting in a total of 1054688 sen-
tences. Training was done purely on Europarl data,
but results were submitted both on Europarl and
2http://www.statmt.org/wmt08/baseline.
html
3Kneser-Ney smoothing can not be used for the POS se-
quence model, since there were counts-of-counts of zero. How-
ever, Witten-Bell smoothing gives good results when the vocab-
ulary is small.
136
7?gram
POSPOS
wordword
Source Target
5?gram
word
Factors Sequence
models
Figure 1: Architecture of the factored system
News data. The news data were submitted to see
how well a pure out-of-domain system could per-
form.
In the pre-processing step compounds were split.
This was done for training, tuning and translation.
In addition German contracted prepositions and de-
terminers, such as zum from zu dem (?to the?), when
identified as such by the tagger, were split.
4.2 English ? German
All features of the German to English system were
used, and in addition more fine-grained German
POS-tags that were sub-categorized for morpholog-
ical features. This was done for training, tuning
and sequence models. At translation time no pre-
processing was needed for the English input, but a
post-processing step for the German output is re-
quired, including the merging of compounds and
contracted prepositions and determiners. The latter
was done in connection with uppercasing, by train-
ing an instance of Moses on a lower cased corpus
with split contractions and an upper-cased corpus
with untouched contractions. The tuning step was
modified so that merging of compounds were done
as part of the tuning.
4.3 Baseline
For comparison, we constructed a baseline accord-
ing to the shared-task description, but with smaller
tuning corpus, and the same sentence filtering for the
translation model as in the submitted system, using
only sentences of length 2-40.
In addition we constructed a factored baseline
system, with POS as an output factor and a se-
quence model for POS. Here we only used the orig-
inal POS-tags from TreeTagger, no additional mor-
phology was added for German.
De-En En-De
Baseline 26.95 20.16
Factored baseline 27.43 20.27
Submitted system 27.63 20.46
Table 1: Bleu scores for Europarl (test2007)
De-En En-De
Baseline 19.54 14.31
Factored baseline 20.16 14.37
Submitted system 20.61 14.77
Table 2: Bleu scores for News Commentary (nc-test2007)
5 Results
Case-sensitive Bleu scores4 (Papineni et al, 2002)
for the Europarl devtest set (test2007) are shown in
table 1. We can see that the submitted system per-
forms best, and that the factored baseline is better
than the pure baseline, especially for translation into
English.
Bleu scores for News Commentary5 (nc-test2007)
are shown in Table 2. Here we can also see that the
submitted system is the best. As expected, Bleu is
much lower on out-of-domain news text than on the
Europarl development test set.
5.1 Compounds
The quality of compound translations were analysed
manually. The first 100 compounds that could be
found by the splitting algorithm were extracted from
the Europarl reference text, test2007, together with
their English translations6 .
System translations were compared to the an-
notated compounds and classified into seven cate-
gories: correct, alternative good translation, correct
but different form, part of the compound translated,
no direct equivalent, wrong and untranslated. Out
of these the first three categories can be considered
good translations.
We performed the error analysis for the submitted
and the baseline system. The result can be seen in
4The %Bleu notation is used in this report
5No development test set for News test were provided, so we
present result for the News commentary, which can be expected
to give similar results.
6The English translations need not be compounds. Com-
pounds without a clear English translation were skipped.
137
De ? En En ? De
Subm Base Subm Base
Correct 50 46 40 39
Alternative 36 26 32 29
Form 5 7 6 8
Part 2 5 10 15
No equivalent 6 2 8 5
Wrong 1 7 1 1
Untranslated ? 7 3 3
Table 3: Results of the error analysis of compound trans-
lations
Table 3. For translation into English the submitted
system handles compound translations considerably
better than the baseline with 91% good translations
compared to 79%. In the submitted system all com-
pounds have a translation, compared to the baseline
system which has 7% of the compounds untrans-
lated. In the other translation direction the difference
is smaller, the biggest difference is that the submit-
ted system has fewer cases of partial translation.
5.2 Agreement in German NPs
To study the effects of using fine-grained POS-tags
in the German sequence model, a similar close study
of German NPs was performed. 100 English NPs
having at least two dependents of the head noun
were selected from a randomly chosen subsection
of the development test set. Their translations in
the baseline and submitted system were then identi-
fied. Translations that were not NPs were discarded.
In about two thirds (62 out of 99) of the cases, the
translations were identical. For the remainder, 12
translations were of equal quality, the submitted sys-
tem had a better translation in 17 cases (46%), and a
worse one in 8 cases (22%). In the majority of cases
where the baseline was better, this was due to word
selection, not agreement.
6 Conclusions
Adding morphological processing improved trans-
lation results in both directions for both text types.
Splitting compounds gave a bigger effect for trans-
lation from German. Marking of compound parts
worked well, with no untranslated parts left in the
sample used for evaluation. The mini-evaluation
of German NPs in English-German translation in-
dicates that the morphologically rich POS-based se-
quence model for German also had a positive effect.
Acknowledgement
We would like to thank Joe Steinhauer for help with
the evaluation of German output.
References
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the ACL, pages
531?540, Ann Arbor, Michigan.
M. Holmqvist, S. Stymne, and L. Ahrenberg. 2007. Get-
ting to know Moses: Initial experiments on German-
English factored translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 181?184, Prague, Czech Republic. Association
for Computational Linguistics.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of the tenth con-
ference of EACL, pages 187?193, Budapest, Hungary.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL, demonstration ses-
sion, Prague, Czech Republic.
S. Langer. 1998. Zur Morphologie und Semantik von
Nominalkomposita. In Tagungsband der 4. Konferenz
zur Verarbeitung natu?rlicher Sprache (KONVENS),
pages 83?97.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 311?318, Philadelphia, Pennsyl-
vania.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical ma-
chine translation of German compound words. In Pro-
ceedings of FinTAL - 5th International Conference on
Natural Language Processing, pages 616?624, Turku,
Finland.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Preoceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP),
pages 901?904, Denver, Colorado.
138
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 120?124,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Improving alignment for SMT by reordering
and augmenting the training corpus
Maria Holmqvist, Sara Stymne, Jody Foo and Lars Ahrenberg
Department of Computer and Information Science
Link?ping University, Sweden
{marho,sarst,jodfo,lah}@ida.liu.se
Abstract
We describe the LIU systems for English-
German and German-English translation
in the WMT09 shared task. We focus on
two methods to improve the word align-
ment: (i) by applying Giza++ in a sec-
ond phase to a reordered training cor-
pus, where reordering is based on the
alignments from the first phase, and (ii)
by adding lexical data obtained as high-
precision alignments from a different word
aligner. These methods were studied in
the context of a system that uses com-
pound processing, a morphological se-
quence model for German, and a part-
of-speech sequence model for English.
Both methods gave some improvements to
translation quality as measured by Bleu
and Meteor scores, though not consis-
tently. All systems used both out-of-
domain and in-domain data as the mixed
corpus had better scores in the baseline
configuration.
1 Introduction
It is an open question whether improved word
alignment actually improves statistical MT. Fraser
and Marcu (2007) found that improved alignments
as measured by AER will not necessarily improve
translation quality, whereas Ganchev et al (2008)
did improve translation quality on several lan-
guage pairs by extending the alignment algorithm.
For this year?s shared task we therefore stud-
ied the effects of improving word alignment in the
context of our system for the WMT09 shared task.
Two methods were tried: (i) applying Giza++ in
a second phase to a reordered training corpus,
where reordering is based on the alignments from
the first phase, and (ii) adding lexical data ob-
tained as high-precision alignments from a differ-
ent word aligner. The submitted system includes
the first method in addition to the processing of
compounds and additional sequence models used
by Stymne et al (2008). Heuristics were used
to generate true-cased versions of the translations
that were submitted, as reported in section 6.
In this paper we report case-insensitive Bleu
scores (Papineni et al, 2002), unless otherwise
stated, calculated with the NIST tool, and case-
insensitive Meteor-ranking scores, without Word-
Net (Agarwal and Lavie, 2008).
2 Baseline system
Our baseline system uses compound split-
ting, compound merging and part-of-
speech/morphological sequence models (Stymne
et al, 2008). Except for these additions it is
similar to the baseline system of the workshop1.
The translation system is a factored phrase-
based translation system that uses the Moses
toolkit (Koehn et al, 2007) for decoding and train-
ing, GIZA++ for word alignment (Och and Ney,
2003), and SRILM (Stolcke, 2002) for language
models. Minimum error rate training was used to
tune the model feature weights (Och, 2003).
Tuning was performed on the news-dev2009a
set with 1025 sentences. All development test-
ing was performed on the news-dev2009b set with
1026 sentences.
2.1 Sequence model based on part-of-speech
and morphology
The translation models were factored with one ad-
ditional output factor. For English we used part-
of-speech tags obtained with TreeTagger (Schmid,
1994). For German we enriched the tags from
TreeTagger with morphological information, such
as case or tense, that we get from a commercial
1http://www.statmt.org/wmt09/baseline.
html
120
dependency parser2.
We used the extra factor in an additional se-
quence model which can improve agreement be-
tween words, and word order. For German this
factor was also used for compound merging.
2.2 Compound processing
Prior to training and translation, compound pro-
cessing was performed using an empirical method
based on (Koehn and Knight, 2003; Stymne,
2008). Words were split if they could be split
into parts that occur in a monolingual corpus. We
chose the split with the highest arithmetic mean
of the corpus frequencies of compound parts. We
split nouns, adjectives and verbs into parts that
were content words or particles. A part had to
be at least 3 characters in length and a stop list
was used to avoid parts that often lead to errors,
such as arische (Aryan) in konsularische (con-
sular). Compound parts sometimes have special
compound suffixes, which could be additions or
truncations of letters, or combinations of these.
We used the top 10 suffixes from a corpus study
of Langer (1998), and we also treated hyphens as
suffixes of compound parts. Compound parts were
given a special part-of-speech tag that matched the
head word.
For translation into German, compound parts
were merged to form compounds, both during test
and tuning. The merging is based on the spe-
cial part-of-speech tag used for compound parts
(Stymne, 2009). A token with this POS-tag is
merged with the next token, either if the POS-tags
match, or if it results in a known word.
3 Domain adaptation
This year three training corpora were available, a
small bilingual news commentary corpus, a rea-
sonably large Europarl corpus, and a very large
monolingual news corpus, see Table 1 for details.
The bilingual data was filtered to remove sen-
tences longer than 60 words. Because the German
news training corpus contained a number of En-
glish sentences, this corpus was cleaned by remov-
ing sentences containing a number of common En-
glish words.
Based on Koehn and Schroeder (2007) we
adapted our system from last year, which was fo-
cused on Europarl, to perform well on test data
2Machinese syntax, from Connexor Oy http://www.
connexor.eu
Corpus German English
news-commentary09 81,141
Europarl 1,331,262
news-train08 9,619,406 21,215,311
Table 1: Number of sentences in the corpora (after
filtering)
Corpus En?De De?En
Bleu Meteor Bleu Meteor
News com. 12.13 47.01 17.21 36.08
Europarl 12.92 47.27 18.53 37.65
Mixed 12.91 47.96 18.76 37.69
Mixed+ 14.62 49.48 19.92 38.18
Table 2: Results of domain adaptation
from the news domain. We used the possibility
to include several translation models in the Moses
decoder by using multiple alternative decoding
paths. We first trained systems on either bilingual
news data or Europarl. Then we trained a mixed
system, with two translation models one from each
corpus, a language model from the bilingual news
data, and a Europarl reordering model. The mixed
system was slightly better than the Europarl only
system. All sequence models used 5-grams for
surface form and 7-grams for part-of-speech. All
scores are shown in Table 2.
We wanted to train sequence models on the
large monolingual corpora, but due to limited
computer resources, we had to use a lower order
for this, than on the small corpus. Thus our se-
quence models on this data has lower order than
those trained on bilingual news or Europarl, with
4-grams for surface form and 6-grams for part-
of-speech. We also used the entropy-based prun-
ing included in the SRILM toolkit, with 10?8 as
a threshold. Using these sequence models in the
mixed model, called mixed+, improved the results
drastically, as shown in Table 2.
The other experiments reported in this paper are
based on the mixed+ system.
4 Improved alignment by reordering
Word alignment with Giza++ has been shown to
improve from making the source and target lan-
guage more similar, e.g., in terms of segmentation
(Ma et al, 2007) or word order.
We used the following simple procedure to im-
prove alignment of the training corpus by reorder-
ing the words in one of the texts according to the
121
Corpus En?De De?En
Bleu Meteor Bleu Meteor
Mixed+ 14.62 49.48 19.92 38.18
Re-Src 14.63 49.80 20.54 38.86
Re-Trg 14.51 48.62 20.48 38.73
Table 3: Results of reordering experiments
word order in the other language:
1. Word align the corpus with Giza++.
2. Reorder the German words according to the
order of the English words they are aligned
to. (This is a common step in approaches that
extract reordering rules for translation. How-
ever, this is not what we use it for here.)
3. Word align the reordered German and origi-
nal English corpus with Giza++.
4. Put the reordered German words back into
their original position and adjust the align-
ments so that the improved alignment is pre-
served.
After this step we will have a possibly improved
alignment compared to the original Giza++ align-
ment. A phrase table was extracted from the align-
ment and training was performed as usual. The re-
ordering procedure was carried out on both source
(Re-Src) and target data (Re-Trg) and the results
of translating devtest data using these alignments
are shown in Table 3.
Compared with our baseline (mixed+), Bleu
and Meteor increased for the translation direction
German?English. Both source reordering and tar-
get reordering resulted in a 0.6 increase in Bleu.
For translation into German, source reordering
resulted in a somewhat higher Meteor score, but
overall did not seem to improve translation. Tar-
get reordering in this direction resulted in lower
scores.
It is not clear why reordering improved trans-
lation for German?English and not for English?
German. In all experiments, the heuristic sym-
metrization of directed Giza++ alignments was
performed in the intended translation direction 3.
3Our experiments show that symmetrization in the wrong
translation direction will result in lower translation quality
scores.
5 Augmenting the corpus with an
extracted dictionary
Previous research (Callison-Burch et al, 2004;
Fraser and Marcu, 2006) has shown that includ-
ing word aligned data during training can improve
translation results. In our case we included a dic-
tionary extracted from the news-commentary cor-
pus during the word alignment.
Using a method originally developed for term
extraction (Merkel and Foo, 2007), the news-
commentary09 corpus was grammatically anno-
tated and aligned using a heuristic word aligner.
Candidate dictionary entries were extracted from
the alignments. In order to optimize the qual-
ity of the dictionary, dictionary entry candidates
were ranked according to their Q-value, a metric
specifically designed for aligned data (Merkel and
Foo, 2007). The Q-value is based on the following
statistics:
? Type Pair Frequencies (TPF), i.e. the number
of times where the source and target types are
aligned.
? Target types per Source type (TpS), i.e. the
number of target types a specific source type
has been aligned to.
? Source types per Target type (SpT), i.e. the
number of source types a specific target type
has been aligned to.
The Q-value is calculated as
Q?value= TPFTpS+SpT . A high Q-value indi-
cates a dictionary candidate pair with a relatively
low number of translation variations. The candi-
dates were filtered using a Q-value threshold of
0.333, resulting in a dictionary containing 67287
entries.
For the experiments, the extracted dictionary
was inserted 200 times into the corpus used dur-
ing word alignment. The added dictionary entries
were removed before phrase extraction. Experi-
ments using the extracted dictionary as an addi-
tional phrase table were also run, but did not result
in any improvement of translation quality.
The results can be seen in Table 4. There was
no evident pattern how the inclusion of the dictio-
nary during alignment (DictAl) affected the trans-
lation quality. The inclusion of the dictionary pro-
duced both higher and lower Bleu scores than the
122
Corpus En?De De?En
Bleu Meteor Bleu Meteor
Mixed+ 14.62 49.48 19.92 38.18
DictAl 14.73 49.39 18.93 37.71
Table 4: Results of domain adaptation
Corpus En?De De?En
Mixed+ 13.31 17.47
with OOV 13.74 17.96
Table 5: Case-sensitive Bleu scores
baseline system depending on the translation di-
rection. Meteor scores were however consistently
lower than the baseline system.
6 Post processing of out-of-vocabulary
words
In the standard systems all out-of-vocabulary
words are transferred as is from the translation in-
put to the translation output. Many of these words
are proper names, which do not get capitalized
properly, or numbers, which have different for-
matting in German and English. We used post-
processing to improve this.
For all unknown words we capitalized either the
first letter, or all letters, if they occur in that form
in the translation input. For unknown numbers
we switched between the German decimal comma
and the English decimal point for decimal num-
bers. For large numbers, English has a comma
to separate thousands, and German has a period.
These were also switched. This improved case-
sensitive Bleu scores in both translation directions,
see Table 5.
7 Submitted system
For both translation directions De-En and En-De
we submitted a system with two translation mod-
els trained on bilingual news and Europarl. The
alignment was improved by using the reordering
techniques described in section 4. The systems
also use all features described in this paper except
for the lexical augmentation (section 5) which did
not result in significant improvement. The results
of the submitted systems on devtest data are bold-
faced in Table 3.
Corpus En?De De?En
All 14.63 20.54
En-De orig. 19.93 26.82
Other set 11.66 16.17
Table 6: Bleu scores for the reordered systems on
two sections of development set news-dev2009b.
NIST scores show the same distribution.
8 Results on two sections of devtest data
Comparisons of translation output with reference
translations on devtest data showed some surpris-
ing differences, which could be attributed to cor-
responding differences between source and refer-
ence data. The differences were not evenly dis-
tributed but especially frequent in those sections
where the original language was something other
than English or German. To check the homogene-
ity of the devtest data we divided it into two sec-
tions, one for documents of English or German
origin, and the other for the remainder. It turned
out that scores were dramatically different for the
two sections, as shown in Table 6.
The reason for the difference is likely to be that
only the En-De set contains source texts and trans-
lations, while the other section contains parallel
translations from the same source. This suggests
that it would be interesting to study the effects of
splitting the training corpus in the same way be-
fore training.
9 Conclusion
The results of augmenting the training corpus with
an extracted lexicon were inconclusive. How-
ever, the alignment reordering improved transla-
tion quality, especially in the De?En direction.
The result of these reordering experiments indi-
cates that better word alignment quality will im-
prove SMT. The reordering method described in
this paper also has the advantage of only requir-
ing two runs of Giza++, no additional resources or
training is necessary to get an improved alignment.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, M-
BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio.
123
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In Pro-
ceedings of the 42nd Annual Meeting of ACL, pages
175?182, Barcelona, Spain.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of ACL, pages 769?776, Sydney, Australia.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Kuzman Ganchev, Jo?o de Almeida Varelas Gra?a, and
Ben Taskar. 2008. Better alignments = better trans-
lations? In Proceedings of the 46th Annual Meeting
of ACL, pages 986?993, Columbus, Ohio.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the tenth conference of EACL, pages 187?193, Bu-
dapest, Hungary.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
ACL, demonstration session, Prague, Czech Repub-
lic.
Stefan Langer. 1998. Zur Morphologie und Seman-
tik von Nominalkomposita. In Tagungsband der
4. Konferenz zur Verarbeitung nat?rlicher Sprache
(KONVENS), pages 83?97.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Boostrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of ACL,
pages 304?311, Prague, Czech Republic.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of the 16th Nordic Con-
ference of Computational Linguistics (NODALIDA-
2007), pages 349?354, Tartu, Estonia.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of ACL, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of ACL, pages 311?318,
Philadelphia, Pennsylvania.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing (ICSLP), pages 901?904, Denver, Col-
orado.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 135?138, Columbus, Ohio.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In Aarne Ranta and
Bengt Nordstr?m, editors, Proceedings of GoTAL,
6th International Conference on Natural Language
Processing, LNCS/LNAI Volume 5221, pages 464?
475.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of German compounds. In Pro-
ceedings of the EACL09 Student Research Work-
shop, Athens, Greece.
124
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474?482,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Dense Models of Query Similarity from User Click Logs
Fabio De Bona?
Friedrich Miescher Laboratory
of the Max Planck Society
Tu?bingen, Germany
fabio@tuebingen.mpg.de
Stefan Riezler
Google Research
Zu?rich, Switzerland
riezler@google.com
Keith Hall
Google Research
Zu?rich, Switzerland
kbhall@google.com
Massimiliano Ciaramita
Google Research
Zu?rich, Switzerland
massi@google.com
Amac? Herdag?delen?
University of Trento
Rovereto, Italy
amac@herdagdelen.com
Maria Holmqvist?
Linkopings University
Linkopings, Sweden
marho@ida.liu.se
Abstract
The goal of this work is to integrate query
similarity metrics as features into a dense
model that can be trained on large amounts
of query log data, in order to rank query
rewrites. We propose features that incorpo-
rate various notions of syntactic and semantic
similarity in a generalized edit distance frame-
work. We use the implicit feedback of user
clicks on search results as weak labels in train-
ing linear ranking models on large data sets.
We optimize different ranking objectives in a
stochastic gradient descent framework. Our
experiments show that a pairwise SVM ranker
trained on multipartite rank levels outperforms
other pairwise and listwise ranking methods
under a variety of evaluation metrics.
1 Introduction
Measures of query similarity are used for a wide
range of web search applications, including query
expansion, query suggestions, or listings of related
queries. Several recent approaches deploy user
query logs to learn query similarities. One set of ap-
proaches focuses on user reformulations of queries
that differ only in one phrase, e.g., Jones et al
(2006). Such phrases are then identified as candi-
date expansion terms, and filtered by various signals
such as co-occurrence in similar sessions, or log-
likelihood ratio of original and expansion phrase.
Other approaches focus on the relation of queries
and search results, either by clustering queries based
?The work presented in this paper was done while the au-
thors were visiting Google Research, Zu?rich.
on their search results, e.g., Beeferman and Berger
(2000), or by deploying the graph of queries and re-
sults to find related queries, e.g., Sahami and Heil-
man (2006).
The approach closest to ours is that of Jones et al
(2006). Similar to their approach, we create a train-
ing set of candidate query rewrites from user query
logs, and use it to train learners. While the dataset
used in Jones et al (2006) is in the order of a few
thousand query-rewrite pairs, our dataset comprises
around 1 billion query-rewrite pairs. Clearly, man-
ual labeling of rewrite quality is not feasible for our
dataset, and perhaps not even desirable. Instead, our
intent is to learn from large amounts of user query
log data. Such data permit to learn smooth mod-
els because of the effectiveness of large data sets to
capture even rare aspects of language, and they also
are available as in the wild, i.e., they reflect the ac-
tual input-output behaviour that we seek to automate
(Halevy et al, 2009). We propose a technique to au-
tomatically create weak labels from co-click infor-
mation in user query logs of search engines. The
central idea is that two queries are related if they
lead to user clicks on the same documents for a large
amount of documents. A manual evaluation of a
small subset showed that a determination of positive
versus negative rewrites by thresholding the number
of co-clicks correlates well with human judgements
of similarity, thus justifying our method of eliciting
labels from co-clicks.
Similar to Jones et al (2006), the features of our
models are not based on word identities, but instead
on general string similarity metrics. This leads to
dense rather than sparse feature spaces. The dif-
474
ference of our approach to Jones et al (2006) lies
in our particular choice of string similarity metrics.
While Jones et al (2006) deploy ?syntactic? fea-
tures such as Levenshtein distance, and ?semantic?
features such as log-likelihood ratio or mutual in-
formation, we combine syntactic and semantic as-
pects into generalized edit-distance features where
the cost of each edit operation is weighted by vari-
ous term probability models.
Lastly, the learners used in our approach are appli-
cable to very large datasets by an integration of lin-
ear ranking models into a stochastic gradient descent
framework for optimization. We compare several
linear ranking models, including a log-linear prob-
ability model for bipartite ranking, and pairwise and
listwise SVM rankers. We show in an experimen-
tal evaluation that a pairwise SVM ranker trained on
multipartite rank levels outperforms state-of-the-art
pairwise and listwise ranking methods under a vari-
ety of evaluation metrics.
2 Query Similarity Measures
2.1 Semantic measures
In several of the similarity measures we describe be-
low, we employ pointwise mutual information (PMI)
as a measure of the association between two terms or
queries. Let wi and wj be two strings that we want
to measure the amount of association between. Let
p(wi) and p(wj) be the probability of observing wi
and wj in a given model; e.g., relative frequencies
estimated from occurrence counts in a corpus. We
also define p(wi, wj) as the joint probability of wi
and wj ; i.e., the probability of the two strings occur-
ring together. We define PMI as follows:
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
. (1)
PMI has been introduced by Church and Hanks
(1990) as word assosiatio ratio, and since then
been used extensively to model semantic similar-
ity. Among several desirable properties, it correlates
well with human judgments (Recchia and Jones,
2009).
2.2 Taxonomic normalizations
As pointed out in earlier work, query transitions tend
to correlate with taxonomic relations such as gener-
alization and specialization (Lau and Horvitz, 1999;
Rieh and Xie, 2006). Boldi et al (2009) show how
knowledge of transition types can positively impact
query reformulation. We would like to exploit this
information as well. However, rather than building a
dedicated supervised classifier for this task we try to
capture it directly at the source. First, we notice how
string features; e.g., length, and edit distance already
model this phenomenon to some extent, and in fact
are part of the features used in Boldi et al (2009).
However, these measures are not always accurate
and it is easy to find counterexamples both at the
term level (e.g., ?camping? to ?outdoor activities? is
a generalization) and character level (?animal pic-
tures? to ?cat pictures? is a specialization). Sec-
ondly, we propose that by manipulating PMI we can
directly model taxonomic relations to some extent.
Rather than using raw PMI values we re-
normalize them. Notice that it is not obvious in our
context how to interpret the relation between strings
co-occurring less frequently than random. Such
noisy events will yield negative PMI values since
p(wi, wj) < p(wi)p(wj). We enforce zero PMI val-
ues for such cases. If PMI is thus constrained to
non-negative values, normalization will bound PMI
to the range between 0 and 1.
The first type of normalization, called joint nor-
malization, uses the negative log joint probability
and is defined as
PMI(J)(wi, wj) = PMI(wi, wj)/?log(p(wi, wj)).
The jointly normalized PMI(J) is a symmetric
measure between wi and wj in the sense that
PMI(J)(wi, wj) = PMI(J)(wj , wi). Intuitively it
is a measure of the amount of shared information
between the two strings relative to the sum of indi-
vidual strings information. The advantages of the
joint normalization of PMI have been noticed be-
fore (Bouma, 2009).
To capture asymmetries in the relation between
two strings, we introduce two non-symmetric nor-
malizations which also bound the measure between
0 and 1. The second normalization is called special-
ization normalization and is defined as
PMI(S)(wi, wj) = PMI(wi, wj)/? log(p(wi)).
The reason we call it specialization is that PMI(S)
favors pairs where the second string is a specializa-
475
tion of the first one. For instance, PMI(S) is at its
maximum when p(wi, wj) = p(wj) and that means
the conditional probability p(wi|wj) is 1 which is an
indication of a specialization relation.
The last normalization is called the generalization
normalization and is defined in the reverse direction
as
PMI(G)(wi, wj) = PMI(wi, wj)/? log(p(wj)).
Again, PMI(G) is a measure between 0 and 1 and is
at its maximum value when p(wj |wi) is 1.
The three normalizations provide a richer rep-
resentation of the association between two strings.
Furthermore, jointly, they model in an information-
theoretic sense the generalization-specialization di-
mension directly. As an example, for the query
transition ?apple? to ?mac os? PMI(G)=0.2917 and
PMI(S)=0.3686; i.e., there is more evidence for a
specialization. Conversely for the query transition
?ferrari models? to ?ferrari? we get PMI(G)=1 and
PMI(S)=0.5558; i.e., the target is a ?perfect? gener-
alization of the source1.
2.3 Syntactic measures
Let V be a finite vocabulary and ? be the null
symbol. An edit operation: insertion, deletion or
substitution, is a pair (a, b) ? {V ? {?} ? V ?
{?}} \ {(?, ?)}. An alignment between two se-
quences wi and wj is a sequence of edit oper-
ations ? = (a1, b1), ..., (an, bn). Given a non-
negative cost function c, the cost of an alignment is
c(?) =
?n
i=1 c(?i). The Levenshtein distance, or
edit distance, defined over V , dV (wi, wj) between
two sequences is the cost of the least expensive se-
quence of edit operations which transforms wi into
wj (Levenshtein, 1966). The distance computation
can be performed via dynamic programming in time
O(|wi||wj |). Similarity at the string, i.e., character
or term, level is an indicator of semantic similar-
ity. Edit distance captures the amount of overlap be-
tween the queries as sequences of symbols and has
been previously used in information retrieval (Boldi
et al, 2009; Jones et al, 2006).
We use two basic Levenshtein distance models.
The first, called Edit1 (E1), employs a unit cost func-
tion for each of the three operations. That is, given
1The values are computed from Web counts.
a finite vocabulary T containing all terms occurring
in queries:
?a, b ? T, cE1(a, b) = 1 if(a 6= b), 0 else.
The second, called Edit2 (E2), uses unit costs for
insertion and deletion, but computes the character-
based edit distance between two terms to decide on
the substitution cost. If two terms are very similar
at the character level, then the cost of substitution is
lower. Given a finite vocabulary T of terms and a
finite vocabulary A of characters, the cost function
is defined as:
?a, b ? T, cE2(a, b) = dA(a, b) ifa ? b 6= ?, 1 else.
where dA(a, b) is linearly scaled between 0 and 1
dividing by max(|a|, |b|).
We also investigate a variant of the edit distance
algorithm in which the terms in the input sequences
are sorted, alphabetically, before the distance com-
putation. The motivation behind this variant is the
observation that linear order in queries is not always
meaningful. For example, it seems reasonable to as-
sume that ?brooklyn pizza? and ?pizza brooklyn?
denote roughly the same user intent. However, the
pair has an edit distance of two (delete-insert), while
the distance between ?brooklyn pizza? and the less
relevant ?brooklyn college? is only one (substitute).
The sorted variant relaxes the ordering constraint.
2.4 Generalized measures
In this section we extend the edit distance frame-
work introduced in Section 2.3 with the semantic
similarity measures described in Section 2.1, using
the taxonomic normalizations defined in Section 2.2.
Extending the Levenshtein distance framework
to take into account semantic similarities between
terms is conceptually simple. As in the Edit2 model
above we use a modified cost function. We introduce
a cost matrix encoding individual costs for term sub-
stitution operations; the cost is defined in terms of
the normalized PMI measures of Section 2.2, recall
that these measures range between 0 and 1. Given a
normalized similarity measure f , an entry in a cost
matrix S for a term pair (wi, wj) is defined as:
s(wi, wj) = 2? 2f(wi, wj) + 
476
We call these models SEdit (SE), where S specifies
the cost matrix used. Given a finite term vocabulary
T and cost matrix S, the cost function is defined as:
?a, b ? T, cSE(a, b) = s(a, b) ifa ? b 6= ?, 1 else.
The cost function has the following properties.
Since insertion and deletion have unit cost, a term
is substituted only if a substitution is ?cheaper? than
deleting and inserting another term, namely, if the
similarity between the terms is not zero. The 
correction, coupled with unit insertion and deletion
cost, guarantees that for an unrelated term pair a
combination of insertion and deletion will always be
less costly then a substitution. Thus in the compu-
tation of the optimal alignment, each operation cost
ranges between 0 and 2.
As a remark on efficiency, we notice that here the
semantic similarities are computed between terms,
rather than full queries. At the term level, caching
techniques can be applied more effectively to speed
up feature computation. The cost function is imple-
mented as a pre-calculated matrix, in the next sec-
tion we describe how the matrix is estimated.
2.5 Cost matrix estimation
In our experiments we evaluated two different
sources to obtain the PMI-based cost matrices. In
both cases, we assumed that the cost of the substitu-
tion of a term with itself (i.e. identity substitution)
is always 0. The first technique uses a probabilis-
tic clustering model trained on queries and clicked
documents from user query logs. The second model
estimates cost matrices directly from user session
logs, consisting of approximately 1.3 billion U.S.
English queries. A session is defined as a sequence
of queries from the same user within a controlled
time interval. Let qs and qt be a query pair observed
in the session data where qt is issued immediately
after qs in the same session. Let q?s = qs \ qt and
q?t = qt \ qs, where \ is the set difference opera-
tor. The co-occurrence count of two terms wi and
wj from a query pair qs, qt is denoted by ni,j(qs, qt)
and is defined as:
ni,j(qs, qt) =
?
?
?
1 if wi = wj ? wi ? qs ? wj ? qt
1/(|q?s| |q
?
t|) if wi ? q
?
s ? wj ? q
?
t
0 else.
In other words, if a term occurs in both queries,
it has a co-occurrence count of 1. For all other term
pairs, a normalized co-occurrence count is computed
in order to make sure the sum of co-occurrence
counts for a term wi ? qs sums to 1 for a given
query pair. The normalization is an attempt to avoid
the under representation of terms occurring in both
queries.
The final co-occurrence count of two arbitrary
terms wi and wj is denoted by Ni,j and it is defined
as the sum over all query pairs in the session logs,
Ni,j =
?
qs,qt ni,j(qs, qt). Let N =
?
wi,wj
Ni,j be
the sum of co-occurrence counts over all term pairs.
Then we define a joint probability for a term pair as
p(wi, wj) =
Ni,j
N . Similarly, we define the single-
occurrence counts and probabilities of the terms
by computing the marginalized sums over all term
pairs. Namely, the probability of a termwi occurring
in the source query is p(i, ?) =
?
wj
Ni,j/N and
similarly the probability of a term wj occurring in
the target query is p(?, j) =
?
wi
Ni,j/N . Plugging
in these values in Eq. (1), we get the PMI(wi, wj)
for term pair wi and wj , which are further normal-
ized as described in Section 2.2.
More explanation and evaluation of the features
described in this section can be found in Ciaramita
et al (2010).
3 Learning to Rank from Co-Click Data
3.1 Extracting Weak Labels from Co-Clicks
Several studies have shown that implicit feedback
from clickstream data is a weaker signal than human
relevance judgements. Joachims (2002) or Agrawal
et al (2009) presented techniques to convert clicks
into labels that can be used for machine learning.
Our goal is not to elicit relevance judgments from
user clicks, but rather to relate queries by pivoting on
commonly clicked search results. The hypothesis is
that two queries are related if they lead to user clicks
on the same documents for a large amount of docu-
ments. This approach is similar to the method pro-
posed by Fitzpatrick and Dent (1997) who attempt
to measure the relatedness between two queries by
using the normalized intersection of the top 200 re-
trieval results. We add click information to this
setup, thus strengthening the preference for preci-
sion over recall in the extraction of related queries.
477
Table 1: Statistics of co-click data sets.
train dev test
number of queries 250,000 2,500 100
average number of
rewrites per query 4,500 4,500 30
percentage of rewrites
with ? 10 coclicks 0.2 0.2 43
In our experiments we created two ground-truth
ranking scenarios from the co-click signals. In a first
scenario, called bipartite ranking, we extract a set
of positive and a set of negative query-rewrite pairs
from the user logs data. We define positive pairs as
queries that have been co-clicked with at least 10 dif-
ferent results, and negative pairs as query pairs with
fewer than 10 co-clicks. In a second scenario, called
multipartite ranking, we define a hierarchy of levels
of ?goodness?, by combining rewrites with the same
number of co-clicks at the same level, with increas-
ing ranks for higher number of co-clicks. Statistics
on the co-click data prepared for our experiments are
given in Table 1.
For training and development, we collected
query-rewrite pairs from user query logs that con-
tained at least one positive rewrite. The training set
consists of about 1 billion of query-rewrite pairs; the
development set contains 10 million query-rewrite
pairs. The average number of rewrites per query is
around 4,500 for the training and development set,
with a very small amount of 0.2% positive rewrites
per query. In order to confirm the validity of our co-
click hypothesis, and for final evaluation, we held
out another sample of query-rewrite pairs for man-
ual evaluation. This dataset contains 100 queries for
each of which we sampled 30 rewrites in descending
order of co-clicks, resulting in a high percentage of
43% positive rewrites per query. The query-rewrite
pairs were annotated by 3 raters as follows: First the
raters were asked to rank the rewrites in descend-
ing order of relevance using a graphical user inter-
face. Second the raters assigned rank labels and bi-
nary relevance scores to the ranked list of rewrites.
This labeling strategy is similar to the labeling strat-
egy for synonymy judgements proposed by Ruben-
stein and Goodenough (1965). Inter-rater agree-
ments on binary relevance judgements, and agree-
ment between rounded averaged human relevance
scores and assignments of positive/negative labels
by the co-click threshold of 10 produced a Kappa
value of 0.65 (Siegel and Castellan, 1988).
3.2 Learning-to-Rank Query Rewrites
3.2.1 Notation
Let S = {(xq, yq)}nq=1 be a training sample
of queries, each represented by a set of rewrites
xq = {xq1, . . . , xq,n(q)}, and set of rank labels
yq = {yq1, . . . , yq,n(q)}, where n(q) is the num-
ber of rewrites for query q. For full rankings of
all rewrites for a query, a total order on rewrites is
assumed, with rank labels taking on values yqi ?
{1, . . . , n(q)}. Rewrites of equivalent rank can be
specified by assuming a partial order on rewrites,
where a multipartite ranking involves r < n(q) rele-
vance levels such that yqi ? {1, . . . , r} , and a bipar-
tite ranking involves two rank values yqi ? {1, 2}
with relevant rewrites at rank 1 and non-relevant
rewrites at rank 2.
Let the rewrites in xq be identified by the integers
{1, 2, . . . , n(q)}, and let a permutation piq on xq be
defined as a bijection from {1, 2, . . . , n(q)} onto it-
self. Let ?q denote the set of all possible permuta-
tions on xq, and let piqi denote the rank position of
xqi. Furthermore, let (i, j) denote a pair of rewrites
in xq and let Pq be the set of all pairs in xq.
We associate a feature function ?(xqi) with each
rewrite i = 1, . . . , n(q) for each query q. Further-
more, a partial-order feature map as used in Yue et
al. (2007) is created for each rewrite set as follows:
?(xq, piq) =
1
|Pq|
?
(i,j)?Pq
?(xqi)??(xqj)sgn(
1
piqi
?
1
piqj
).
The goal of learning a ranking over the rewrites
xq for a query q can be achieved either by sorting the
rewrites according to the rewrite-level ranking func-
tion f(xqi) = ?w, ?(xqi)?, or by finding the permu-
tation that scores highest according to a query-level
ranking function f(xq, piq) = ?w, ?(xq, piq)?.
In the following, we will describe a variety
of well-known ranking objectives, and extensions
thereof, that are used in our experiments. Optimiza-
tion is done in a stochastic gradient descent (SGD)
framework. We minimize an empirical loss objec-
tive
min
w
?
xq ,yq
`(w)
478
by stochastic updating
wt+1 = wt ? ?tgt
where ?t is a learning rate, and gt is the gradient
gt = ?`(w)
where
?`(w) =
?
?
?w1
`(w),
?
?w2
`(w), . . . ,
?
?wn
`(w)
?
.
3.2.2 Listwise Hinge Loss
Standard ranking evaluation metrics such as
(Mean) Average Precision (Manning et al, 2008)
are defined on permutations of whole lists and are
not decomposable over instances. Joachims (2005),
Yue et al (2007), or Chakrabarti et al (2008) have
proposed multivariate SVM models to optimize such
listwise evaluation metrics. The central idea is to
formalize the evaluation metric as a prediction loss
function L, and incorporate L via margin rescal-
ing into the hinge loss function, such that an up-
per bound on the prediction loss is achieved (see
Tsochantaridis et al (2004), Proposition 2).
The loss function is given by the following list-
wise hinge loss:
`lh(w) = (L(yq, pi
?
q )?
?
w, ?(xq, yq)? ?(xq, pi
?
q )
?
)+
where pi?q is the maximizer of the
maxpiq??q\yq L(yq, pi
?
q ) +
?
w, ?(xq, pi?q )
?
ex-
pression, (z)+ = max{0, z} and L(yq, piq) ? [0, 1]
denotes a prediction loss of a predicted ranking piq
compared to the ground-truth ranking yq.2
In this paper, we use Average Precision (AP) as
prediction loss function s.t.
LAP (yq, piq) = 1?AP (yq, piq)
where AP is defined as follows:
AP (yq, piq) =
?n(q)
j=1 Prec(j) ? (|yqj ? 2|)
?n(q)
j=1 (|yqj ? 2|)
,
P rec(j) =
?
k:piqk?piqj
(|yqk ? 2|)
piqj
.
2We slightly abuse the notation yq to denote the permutation
on xq that is induced by the rank labels. In case of full rankings,
the permutation piq corresponding to ranking yq is unique. For
multipartite and bipartite rankings, there is more than one pos-
sible permutation for a given ranking, so that we let piq denote
a permutation that is consistent with ranking yq .
Note that the ranking scenario is in this case bipartite
with yqi ? {1, 2}.
The derivatives for `lh are as follows:
?
?wk
`lh =
?
?
?
0 if
(?
w, ?(xq, yq)? ?(xq, pi?q )
?)
> L(yq, pi?q ),
?(?k(xq, yq)? ?k(xq, pi?q )) else.
SGD optimization involves computing pi?q for each
feature and each query, which can be done effi-
ciently using the greedy algorithm proposed by Yue
et al (2007). We will refer to this method as the
SVM-MAP model.
3.2.3 Pairwise Hinge Loss for Bipartite and
Multipartite Ranking
Joachims (2002) proposed an SVM method that
defines the ranking problem as a pairwise classifi-
cation problem. Cortes et al (2007) extended this
method to a magnitude-preserving version by penal-
izing a pairwise misranking by the magnitude of the
difference in preference labels. A position-sensitive
penalty for pairwise ranking SVMs was proposed
by Riezler and De Bona (2009) and Chapelle and
Keerthi (2010), and earlier for perceptrons by Shen
and Joshi (2005). In the latter approaches, the mag-
nitude of the difference in inverted ranks is accrued
for each misranked pair. The idea is to impose an
increased penalty for misrankings at the top of the
list, and for misrankings that involve a difference of
several rank levels.
Similar to the listwise case, we can view the
penalty as a prediction loss function, and incor-
porate it into the hinge loss function by rescaling
the margin by a pairwise prediction loss function
L(yqi, yqj). In our experiments we used a position-
sensitive prediction loss function
L(yqi, yqj) = |
1
yqi
?
1
yqj
|
defined on the difference of inverted ranks. The
margin-rescaled pairwise hinge loss is then defined
as follows:
`ph(w) =
?
(i,j)?Pq
(L(yqi, yqj)?
?w, ?(xqi)? ?(xqj)? sgn(
1
yqi
?
1
yqj
))+
479
Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,
SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.
MAP NDCG@10 AUC Prec@1 Prec@3 Prec@5
Random 51.8 48.7 50.4 45.6 45.6 46.6
Best-feature 71.9 70.2 74.5 70.2 68.1 68.7
SVM-bipart. 73.7 73.7 74.7 79.4 70.1 70.1
SVM-MAP 74.3 75.2 75.3 76.3 71.8 72.0
Log-linear 74.7 75.1 75.7 75.3 72.2 71.3
SVM-pos.-sens. 75.7 76.0 76.6 82.5 72.9 73.0
SVM-multipart. 76.5 77.3 77.2 83.5 74.2 73.6
The derivative of `ph is calculated as follows:
?
?wk
`lp =
?
????
????
0 if (?w, ?(xqi)? ?(xqj)?
sgn( 1yqi ?
1
yqj
)) > L(yqi, yqj),
?(?k(xqi)? ?k(xqj))sgn( 1yqi ?
1
yqj
)
else.
Note that the effect of inducing a position-
sensitive penalty on pairwise misrankings applies
only in case of full rankings on n(q) rank levels,
or in case of multipartite rankings involving 2 <
r < n(q) rank levels. Henceforth we will refer to
margin-rescaled pairwise hinge loss for multipartite
rankings as the SVM-pos.-sens. method.
Bipartite ranking is a special case where
L(yqi, yqj) is constant so that margin rescaling does
not have the effect of inducing position-sensitivity.
This method will be referred to as the SVM-bipartite
model.
Also note that for full ranking or multipartite
ranking, predicting a low ranking for an instance
that is ranked high in the ground truth has a domino
effect of accruing an additional penalty at each
rank level. This effect is independent of margin-
rescaling. The method of pairwise hinge loss
for multipartite ranking with constant margin will
henceforth be referred to as the SVM-multipartite
model.
Computation in SGD optimization is dominated
by the number of pairwise comparisons |Pq| for
each query. For full ranking, a comparison of
|Pq| =
(n(q)
2
)
pairs has to be done. In the case
of multipartite ranking at r rank levels, each in-
cluding |li| rewrites, pairwise comparisons between
rewrites at the same rank level can be ignored.
This reduces the number of comparisons to |Pq| =
?r?1
i=1
?r
j=i+1 |li||lj |. For bipartite ranking of p
positive and n negative instances, |Pq| = p ? n com-
parisons are necessary.
3.2.4 Log-linear Models for Bipartite Ranking
A probabilistic model for bipartite ranking can be
defined as the conditional probability of the set of
relevant rewrites, i.e., rewrites at rank level 1, given
all rewrites at rank levels 1 and 2. A formalization in
the family of log-linear models yields the following
logistic loss function `llm that was used for discrim-
inative estimation from sets of partially labeled data
in Riezler et al (2002):
`llm(w) = ? log
?
xqi?xq |yqi=1
e?w,?(xqi)?
?
xqi?xq
e?w,?(xqi)?
.
The gradient of `llm is calculated as a difference be-
tween two expectations:
?
?wk
`llm = ?pw [?k|xq; yqi = 1] + pw [?k|xq] .
The SGD computation for the log-linear model is
dominated by the computation of expectations for
each query. The logistic loss for bipartite ranking is
henceforth referred to as the log-linear model.
4 Experimental Results
In the experiments reported in this paper, we trained
linear ranking models on 1 billion query-rewrite
pairs using 60 dense features, combined of the build-
ing blocks of syntactic and semantic similarity met-
rics under different estimations of cost matrices. De-
velopment testing was done on a data set that was
held-out from the training set. Final testing was car-
ried out on the manually labeled dataset. Data statis-
tics for all sets are given in Table 1.
480
Table 3: P-values computed by approximate randomization test for 15 pairwise comparisons of result differences.
Best-feature SVM-bipart. SVM-MAP Log-linear SVM-pos.-sens. SVM-multipart.
Best-feature - < 0.005 < 0.005 < 0.005 < 0.005 < 0.005
SVM-bipart. - - 0.324 < 0.005 < 0.005 < 0.005
SVM-MAP - - - 0.374 < 0.005 < 0.005
Log-linear - - - - 0.053 < 0.005
SVM-pos.-sens. - - - - - < 0.005
SVM-multipart. - - - - - -
Model selection was performed by adjusting
meta-parameters on the development set. We
trained each model at constant learning rates ? ?
{1, 0.5, 0.1, 0.01, 0.001}, and evaluated each variant
after every fifth out of 100 passes over the training
set. The variant with the highest MAP score on the
development set was chosen and evaluated on the
test set. This early stopping routine also served for
regularization.
Evaluation results for the systems are reported in
Table 2. We evaluate all models according to the fol-
lowing evaluation metrics: Mean Average Precision
(MAP), Normalized Discounted Cumulative Gain
with a cutoff at rank 10 (NDCG@10), Area-under-
the-ROC-curve (AUC), Precision@n3. As baselines
we report a random permutation of rewrites (ran-
dom), and the single dense feature that performed
best on the development set (best-feature). The latter
is the log-probability assigned to the query-rewrite
pair by the probabilistic clustering model used for
cost matrix estimation (see Section 2.5). P-values
are reported in Table 3 for all pairwise compar-
isons of systems (except the random baseline) us-
ing an Approximate Randomization test where strat-
ified shuffling is applied to results on the query level
(see Noreen (1989)). The rows in Tables 2 and 3
are ranked according to MAP values of the systems.
SVM-multipartite outperforms all other ranking sys-
tems under all evaluation metrics at a significance
level ? 0.995. For all other pairwise comparisons
of result differences, we find result differences of
systems ranked next to each other to be not statis-
tically significant. All systems outperform the ran-
dom and best-feature baselines with statistically sig-
nificant result differences. The distinctive advantage
of the SVM-multipartite models lies in the possibil-
3For a definition of these metrics see Manning et al (2008)
ity to rank rewrites with very high co-click num-
bers even higher than rewrites with reasonable num-
bers of co-clicks. This preference for ranking the
top co-clicked rewrites high seems the best avenue
for transferring co-click information to the human
judgements encoded in the manually labeled test set.
Position-sensitive margin rescaling does not seem to
help, but rather seems to hurt.
5 Discussion
We presented an approach to learn rankings of query
rewrites from large amounts of user query log data.
We showed how to use the implicit co-click feed-
back about rewrite quality in user log data to train
ranking models that perform well on ranking query
rewrites according to human quality standards. We
presented large-scale experiments using SGD opti-
mization for linear ranking models. Our experimen-
tal results show that an SVM model for multipartite
ranking outperforms other linear ranking models un-
der several evaluation metrics. In future work, we
would like to extend our approach to other models,
e.g., sparse combinations of lexicalized features.
References
R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra,
and P. Tsaparas. 2009. Generating labels from clicks.
In Proceedings of the 2nd ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain.
Doug Beeferman and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD?00), Boston, MA.
P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009.
From ?Dango? to ?Japanese cakes?: Query reformula-
481
tion models and patterns. In Proceedings of Web Intel-
ligence. IEEE Cs Press.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Proceedings of
GSCL.
Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and
Chiru Bhattacharayya. 2008. Structured learning for
non-smooth ranking losses. In Proceedings of the 14th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), Las Vegas, NV.
Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Massimiliano Ciaramita, Amac? Herdag?delen, Daniel
Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler,
and Enrique Alfonseca. 2010. Generalized syntactic
and semantic models of query reformulation. In Pro-
ceedings of the 33rd ACM SIGIR Conference, Geneva,
Switzerland.
Corinna Cortes, Mehryar Mohri, and Asish Rastogi.
2007. Magnitude-preserving ranking algorithms. In
Proceedings of the 24th International Conference on
Machine Learning (ICML?07), Corvallis, OR.
Larry Fitzpatrick and Mei Dent. 1997. Automatic feed-
back using past queries: Social searching? In Pro-
ceedings of the 20th Annual International ACM SIGIR
Conference, Philadelphia, PA.
Alon Halevy, Peter Norvig, and Fernando Pereira. 2009.
The unreasonable effectiveness of data. IEEE Intelli-
gent Systems, 24:8?12.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), New York, NY.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd International Conference on Machine Learn-
ing (ICML?05), Bonn, Germany.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th International World Wide Web
conference (WWW?06), Edinburgh, Scotland.
T. Lau and E. Horvitz. 1999. Patterns of search: analyz-
ing and modeling web query refinement. In Proceed-
ings of the seventh international conference on User
modeling, pages 119?128. Springer-Verlag New York,
Inc.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
G. Recchia and M.N. Jones. 2009. More data trumps
smarter algorithms: comparing pointwise mutual in-
formation with latent semantic analysis. Behavioral
Research Methods, 41(3):647?656.
S.Y. Rieh and H. Xie. 2006. Analysis of multiple query
reformulations on the web: the interactive information
retrieval context. Inf. Process. Manage., 42(3):751?
768.
Stefan Riezler and Fabio De Bona. 2009. Simple risk
bounds for position-sensitive max-margin ranking al-
gorithms. In Proceedings of the Workshop on Ad-
vances in Ranking at the 23rd Annual Conference
on Neural Information Processing Systems (NIPS?09),
Whistler, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 10(3):627?633.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th Inter-
national World Wide Web conference (WWW?06), Ed-
inburgh, Scotland.
Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73?96.
Sidney Siegel and John Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. Second Edition.
MacGraw-Hill, Boston, MA.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning (ICML?04), Banff,
Canada.
Yisong Yue, Thomas Finley, Filip Radlinski, and
Thorsten Joachims. 2007. A support vector method
for optimizing average precision. In Proceedings of
the 30th Annual International ACM SIGIR Confer-
ence, Amsterdam, The Netherlands.
482
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 183?188,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Vs and OOVs: Two Problems for Translation between German and
English
Sara Stymne, Maria Holmqvist, Lars Ahrenberg
Linko?ping University
Sweden
{sarst,marho,lah}@ida.liu.se
Abstract
In this paper we report on experiments
with three preprocessing strategies for im-
proving translation output in a statistical
MT system. In training, two reordering
strategies were studied: (i) reorder on the
basis of the alignments from Giza++, and
(ii) reorder by moving all verbs to the
end of segments. In translation, out-of-
vocabulary words were preprocessed in a
knowledge-lite fashion to identify a likely
equivalent. All three strategies were im-
plemented for our English?German sys-
tem submitted to the WMT10 shared task.
Combining them lead to improvements in
both language directions.
1 Introduction
We present the Liu translation system for the con-
strained condition of the WMT10 shared transla-
tion task, between German and English in both di-
rections. The system is based on the 2009 Liu sub-
mission (Holmqvist et al, 2009), that used com-
pound processing, morphological sequence mod-
els, and improved alignment by reordering.
This year we have focused on two issues: trans-
lation of verbs, which is problematic for transla-
tion between English and German since the verb
placement is different with German verbs often be-
ing placed at the end of sentences; and OOVs, out-
of-vocabulary words, which are problematic for
machine translation in general. Verb translation
is targeted by trying to improve alignment, which
we believe is a crucial step for verb translation
since verbs that are far apart are often not aligned
at all. We do this mainly by moving verbs to the
end of sentences previous to alignment, which we
also combine with other alignments. We trans-
form OOVs into known words in a post-processing
step, based on casing, stemming, and splitting of
hyphenated compounds. In addition, we perform
general compound splitting for German both be-
fore training and translation, which also reduces
the OOV rate.
All results in this article are for the develop-
ment test set newstest2009, on truecased output.
We report Bleu scores (Papineni et al, 2002) and
Meteor ranking (without WordNet) scores (Agar-
wal and Lavie, 2008), using percent notation. We
also used other metrics, but as they gave similar
results they are not reported. For significance test-
ing we used approximate randomization (Riezler
and Maxwell, 2005), with p < 0.05.
2 Baseline System
The 2010 Liu system is based on the PBSMT base-
line system for the WMT shared translation task1.
We use the Moses toolkit (Koehn et al, 2007) for
decoding and to train translation models, Giza++
(Och and Ney, 2003) for word alignment, and the
SRILM toolkit (Stolcke, 2002) to train language
models. The main difference to the WMT base-
line is that the Liu system is trained on truecased
data, as in Koehn et al (2008), instead of lower-
cased data. This means that there is no need for a
full recasing step after translation, instead we only
need to uppercase the first word in each sentence.
2.1 Corpus
We participated in the constrained task, where we
only trained the Liu system on the news and Eu-
roparl corpora provided for the workshop. The
translation and reordering models were trained us-
ing the bilingual Europarl and news commentary
corpora, which we concatenated.
We used two sets of language models, one
where we first trained two models on Europarl
and news commentary, which we then interpolated
1http://www.statmt.org/wmt10/baseline.
html
183
with more weight given to the news commentary,
using weights from Koehn and Schroeder (2007).
The second set of language models were trained
on monolingual news data. For tuning we used
every second sentence, in total 1025 sentences, of
news-test2008.
2.2 Training with Limited Computational
Resources
One challenge for us was to train the transla-
tion sytem with limited computational resources.
We trained all systems on one Intel Core 2 CPU,
3.0Ghz, 16 Gb of RAM, 64 bit Linux (RedHat)
machine. This constrained the possibilities of us-
ing the data provided by the workshop to the full.
The main problem was training the language mod-
els, since the monolingual data was very large
compared to the bilingual data.
In order to train language models that were both
fast at runtime, and possible to train with the avail-
able memory, we chose to use the SRILM toolkit
(Stolcke, 2002), with entropy-based pruning, with
10?8 as a threshold. To reduce the model size we
also used lower order models for the large corpus;
4-grams instead of 5-grams for words and 6-grams
instead of 7-grams for the morphological models.
It was still impossible to train on the monolingual
English news corpus, with nearly 50 million sen-
tences, so we split that corpus into three equal size
parts, and trained three models, that were interpo-
lated with equal weights.
3 Morphological Processing
We added morphological processing to the base-
line system, by training additional sequence mod-
els on morphologically enriched part-of-speech
tags, and by compound processing for German.
We utilized the factored translation framework
in Moses, to enrich the baseline system with an
additional target sequence model. For English
we used part-of-speech tags obtained using Tree-
Tagger (Schmid, 1994), enriched with more fine-
grained tags for the number of determiners, in or-
der to target more agreement issues, since nouns
already have number in the tagset. For German
we used morphologically rich tags from RFTag-
ger (Schmid and Laws, 2008), that contains mor-
phological information such as case, number, and
gender for nouns and tense for verbs. We used
the extra factor in an additional sequence model
on the target side, which can improve word order
System Bleu Meteor
Baseline 13.42 48.83
+ morph 13.85 49.69
+ comp 14.24 49.41
Table 1: Results for morphological processing,
English?German
System Bleu Meteor
Baseline 18.34 38.13
+ morph 18.39 37.86
+ comp 18.50 38.47
Table 2: Results for morphological processing,
German?English
and agreement between words. For German the
factor was also used for compound merging.
Prior to training and translation, compound pro-
cessing was performed, using an empirical method
(Koehn and Knight, 2003; Stymne, 2008) that
splits words if they can be split into parts that oc-
cur in a monolingual corpus, choosing the split-
ting option with the highest arithmetic mean of its
part frequencies in the corpus. We split nouns,
adjectives and verbs, into parts that are content
words or particles. We imposed a length limit on
parts of 3 characters for translation from German
and of 6 characters for translation from English,
and we had a stop list of parts that often led to
errors, such as arische (Aryan) in konsularische
(consular). We allowed 10 common letter changes
(Langer, 1998) and hyphens at split points. Com-
pound parts were given a special part-of-speech
tag that matches the head word.
For translation into German, compound parts
were merged into full compounds using a method
described in Stymne and Holmqvist (2008), which
is based on matching of the special part-of-speech
tag for compound parts. A word with a compound
POS-tag were merged with the next word, if their
POS-tags were matching.
Tables 1 and 2 show the results of the addi-
tional morphological processing. Adding the se-
quence models on morphologically enriched part-
of-speech tags gave a significant improvement for
translation into German, but similar or worse re-
sults as the baseline for translation into English.
This is not surprising, since German morphology
is more complex than English morphology. The
addition of compound processing significantly im-
proved the results on Meteor for translation into
184
English, and it also reduced the number of OOVs
in the translation output by 20.8%. For translation
into German, compound processing gave a signif-
icant improvement on both metrics compared to
the baseline, and on Bleu compared to the system
with morphological sequence models. Overall, we
believe that both compound splitting and morphol-
ogy are useful; thus all experiments reported in the
sequel are based on the baseline system with mor-
phology models and compound splitting, which
we will call base.
4 Improved Alignment by Reordering
Previous work has shown that translation quality
can be improved by making the source language
more similar to the target language, for instance
in terms of word order (Wang et al, 2007; Xia
and McCord, 2004). In order to harmonize the
word order of the source and target sentence, they
applied hand-crafted or automatically induced re-
ordering rules to the source sentences of the train-
ing corpus. At decoding time, reordering rules
were again applied to input sentences before trans-
lation. The positive effects of such methods seem
to come from a combination of improved align-
ment and improved reordering during translation.
In contrast, we focus on improving the word
alignment by reordering the training corpus. The
training corpus is reordered prior to word align-
ment with Giza++ (Och and Ney, 2003) and then
the word links are re-adjusted back to the original
word positions. From the re-adjusted corpus, we
create phrase tables that allow translation of non-
reordered input text. Consequently, our reordering
only affects the word alignment and the phrase ta-
bles extracted from it.
We investigated two ways of reordering. The
first method is based on word alignments and the
other method is based on moving verbs to sim-
ilar positions in the source and target sentences.
We also investigated different combinations of re-
orderings and alignments. All results for the sys-
tems with improved reordering are shown in Ta-
bles 3 and 4.
4.1 Reordering Based on Alignments
The first reordering method does not require any
syntactic information or rules for reordering. We
simply used symmetrized Giza++ word align-
ments to reorder the words in the source sentences
to reflect the target word order and applied Giza++
System Bleu Meteor
base 14.24 49.41
reorder 14.32 49.58
verb 13.93 49.22
base+verb 14.38 49.72
base+verb+reorder 14.39 49.39
Table 3: Results for improved alignment,
English?German
System Bleu Meteor
base 18.50 38.47
reorder 18.77 38.53
verb 18.61 38.53
base+verb 18.66 38.61
base+verb+reorder 18.73 38.59
Table 4: Results for improved alignment,
German?English
again to the reordered training corpus. The follow-
ing steps were performed to produce the final word
alignment:
1. Word align the training corpus with Giza++.
2. Reorder the source words according to the or-
der of the target words they are aligned to
(store the original source word positions for
later).
3. Word align the reordered source and original
target corpus with Giza++.
4. Re-adjust the new word alignments so that
they align source and target words in the orig-
inal corpus.
The system built on this word alignment (re-
order) had a significant improvement in Bleu score
over the unreordered baseline (base) for transla-
tion into English, and small improvements other-
wise.
4.2 Verb movement
The positions of finite verbs are often very differ-
ent in English and German, where they are often
placed at the end of sentences. In several cases we
noted that finite verbs were misaligned by Giza++.
To improve the alignment of verbs, we moved all
verbs in both English and German to the end of the
sentences prior to word alignment. The reordered
sentences were word aligned with Giza++ and the
185
resulting word links were then re-adjusted to align
words in the original corpus.
The system created from this alignment (verb)
resulted in significantly lower scores than base for
translation into German, and similar scores as base
for translation into English.
4.3 Combination Systems
The alignment based on reordered verbs did not
produce a better alignment in terms of Bleu scores
of the resulting translations, which led us to the
conclusion that the alignment was noisy. How-
ever, it is possible that we did correctly align some
words that were misaligned in the baseline align-
ment. To investigate this issue we concatenated
first the baseline and verb alignments, and then all
three alignments, and extracted phrase tables from
the concatenated training sets.
All scores for both combined systems signifi-
cantly outperformed the unfactored baseline, and
were slightly better than base. For translation into
German it was best to use the combination of only
verb and base, which was significantly better than
base on Meteor. This shows that even though the
verb alignments were not good when used in a sin-
gle system, they still could contribute in a combi-
nation system.
5 Preprocessing of OOVs
Out-of-vocabulary words, words that have not
been seen in the training data, are a problem in
statistical machine translation, since no transla-
tions have been observed for them. The standard
strategy is to transfer them as is to the translation
output, which, naive as it sounds, actually works
well in some cases, since many OOVs are numbers
or proper names (Stymne and Holmqvist, 2008).
However, it still results in incomprehensible words
in the output in many cases. We have investi-
gated several ways of changing unknown words
into similar words that have been seen in the train-
ing data, in a preprocessing step.
We also considered another OOV problem,
number formatting, since it differs between En-
glish and German. To address this, we swapped
decimal points/commas, and other delimeters for
unknown numbers in a post-processing step.
In the preprocessing step, we applied a num-
ber of transformations to each OOV word, accept-
ing the first applicable transformation that led to a
known word:
Type German English
total OOVs 1833 1489
casing 124 26
stemming 270 72
hyphenated words 230 124
end hyphens 24 ?
Table 5: Number of affected words by OOV-
preprocessing
1. Change the word into a known cased ver-
sion (since we trained a truecased system,
this handles cased variations of words)
2. Stem the word, and if we know the stem,
choose the most common realisation of that
stem (using a Porter stemmer)
3. For hyphenated words, split at the hyphen (if
any of the resulting parts are OOVs, they are
recursively treated as well)
4. Remove hyphens at the end of German words
(that could result from compound splitting)
The first two steps were based on frequency lists
of truecased and stemmed words that we compiled
from the monolingual training corpora.
Inspection of the initial results showed that
proper names were often changed into other words
in English, so we excluded them from the prepro-
cessing by not applying it to words with an initial
capital letter. This happened to a lesser extent for
German, but here it was impossible to use the same
simple heuristic for proper names, since German
nouns also have an initial capital letter.
The number of affected words for the baseline
using the final transformations are shown in Table
5. Even though we managed to transform some
words, we still lack a transformation for the ma-
jority of OOVs. Despite this, there is a tendency of
small improvements on both metrics in the major-
ity of cases in both translation directions, as shown
in Tables 6 and 7.
Figure 1 shows an example of how OOV pro-
cessing affects one sentence for translation from
German to English. In this case splitting a hy-
phenated compound gives a better translation,
even though the word opening is chosen rather
than jack. There is also a stemming change,
where the adjective ausgereiftesten (the most well-
engineered), is changed form superlative to posi-
tive. This results in a more understandable trans-
186
DE original Die besten und technisch ausgereiftesten Telefone mit einer 3,5-mm-O?ffnung
fu?r normale Kopfho?rer kosten bis zu fu?nfzehntausend Kronen.
DE preprocessed die besten und technisch ausgereifte Telefone mit einer 3,5 mm O?ffnung fu?r
normale Kopf Ho?rer kosten bis zu fu?nfzehntausend Kronen .
base+verb+reorder The best and technically ausgereiftesten phones with a 3,5-mm-O?ffnung for
normal earphones cost up to fifteen thousand kronor.
base+verb+reorder
+OOV
The best and technologically advanced phones with a 3.5 mm opening for nor-
mal earphones cost up to fifteen thousand kronor.
EN reference The best and most technically well-equipped telephones, with a 3.5 mm jack
for ordinary headphones, cost up to fifteen thousand crowns.
Figure 1: Example of the effects of OOV processing for German?English
System Bleu Meteor
base 14.24 49.41
+ OOV 14.26 49.43
base+verb 14.38 49.72
+ OOV 14.42 49.75
+ MBR 14.41 49.77
Table 6: Results for OOV-processing and MBR,
English?German.
System Bleu Meteor
base 18.50 38.47
+ OOV 18.48 38.59
base+verb+reorder 18.73 38.59
+ OOV 18.81 38.70
+ MBR 18.84 38.75
Table 7: Results for OOV-processing and MBR,
German?English.
lation, which, however, is harmful to automatic
scores, since the preceding word, technically,
which is identical to the reference, is changed into
technologically.
This work is related to work by Arora et al
(2008), who transformed Hindi OOVs by us-
ing morphological analysers, before translation to
Japanese. Our work has the advantage that it is
more knowledge-lite, as it only needs a Porter
stemmer and a monolingual corpus. Mirkin et al
(2009) used WordNet to replace OOVs by syn-
onyms or hypernyms, and chose the best overall
translation partly based on scoring of the source
transformations. Our OOV handling could po-
tentially be used in combination with both these
strategies.
6 Final Submission
For the final Liu shared task submission we
used the base+verb+reorder+OOV system for
German?English and the base+verb+OOV sys-
tem for English?German, which had the best
overall scores considering all metrics. To these
systems we added minimum Bayes risk (MBR)
decoding (Kumar and Byrne, 2004). In standard
decoding, the top suggestion of the translation sys-
tem is chosen as the system output. In MBR de-
coding the risk is spread by choosing the trans-
lation that is most similar to the N highest scor-
ing translation suggestions from the system, with
N = 100, as suggested in Koehn et al (2008).
MBR decoding gave hardly any changes in auto-
matic scores, as shown in Tables 6 and 7. The final
system was significantly better than the baseline in
all cases, and significantly better than base on Me-
teor in both translation directions, and on Bleu for
translation into English.
7 Conclusions
As in Holmqvist et al (2009) reordering by us-
ing Giza++ in two phases had a small, but consis-
tent positive effect. Aligning verbs by co-locating
them at the end of sentences had a largely negative
effect. However, when output from this method
was concatenated with the baseline alignment be-
fore extracting the phrase table, there were con-
sistent improvements. Combining all three align-
ments, however, had mixed effects. Combining re-
ordering in training with a knowledge-lite method
for handling out-of-vocabulary words led to sig-
nificant improvements on Meteor scores for trans-
lation between German and English in both direc-
tions.
187
References
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, USA.
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of unknown words in phrase-
based statistical machine translation for languages
of rich morphology. In Proceedings of the 1st Inter-
national Workshop on Spoken Languages Technolo-
gies for Under-Resourced Languages, pages 70?75,
Hanoi, Vietnam.
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT
by reordering and augmenting the training corpus.
In Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 120?124, Athens,
Greece.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference of the EACL, pages 187?193,
Budapest, Hungary.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting
of the ACL, demonstration session, pages 177?180,
Prague, Czech Republic.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142, Columbus, Ohio, USA.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 2004 Human Language
Technology Conference of the NAACL, pages 169?
176, Boston, Massachusetts, USA.
Stefan Langer. 1998. Zur Morphologie und Seman-
tik von Nominalkomposita. In Tagungsband der
4. Konferenz zur Verarbeitung natu?rlicher Sprache
(KONVENS), pages 83?97, Bonn, Germany.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 791?
799, Suntec, Singapore.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the Workshop on In-
trinsic and Extrinsic Evaluation Measures for MT
and/or Summarization at the 43th Annual Meeting of
the ACL, pages 57?64, Ann Arbor, Michigan, USA.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an
application to fine-grained pos tagging. In Proceed-
ings of the 22th International Conference on Com-
putational Linguistics, pages 777?784, Manchester,
UK.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne and Maria Holmqvist. 2008. Process-
ing of Swedish compounds for phrase-based statis-
tical machine translation. In Proceedings of the
12th Annual Conference of the European Associa-
tion for Machine Translation, pages 180?189, Ham-
burg, Germany.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In Proceedings of
GoTAL ? 6th International Conference on Natural
Language Processing, pages 464?475, Gothenburg,
Sweden.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 737?745, Prague, Czech Republic.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 508?514, Geneva, Switzerland.
188
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 393?398,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Experiments with word alignment, normalization and clause reordering for
SMT between English and German
Maria Holmqvist, Sara Stymne and Lars Ahrenberg
Department of Computer and Information Science
Linko?ping University, Sweden
firstname.lastname@liu.se
Abstract
This paper presents the LIU system for the
WMT 2011 shared task for translation be-
tween German and English. For English?
German we attempted to improve the trans-
lation tables with a combination of standard
statistical word alignments and phrase-based
word alignments. For German?English trans-
lation we tried to make the German text more
similar to the English text by normalizing Ger-
man morphology and performing rule-based
clause reordering of the German text. This re-
sulted in small improvements for both transla-
tion directions.
1 Introduction
In this paper we present the LIU system for the
WMT11 shared task, for translation between En-
glish and German in both directions. We added a
number of features that address problems for trans-
lation between German and English such as word or-
der differences, incorrect alignment of certain words
such as verbs, and the morphological complexity
of German compared to English, as well as dealing
with previously unseen words.
In both translation directions our systems in-
clude compound processing, morphological se-
quence models, and a hierarchical reordering model.
For German?English translation we also added mor-
phological normalization, source side reordering,
and processing of out-of-vocabulary words (OOVs).
For English?German translation, we extracted word
alignments with a supervised method and combined
these alignments with Giza++ alignments in various
ways to improve the phrase table. We experimented
with different ways of combining the two alignments
such as using heuristic symmetrization and interpo-
lating phrase tables.
Results are reported on three metrics, BLEU (Pa-
pineni et al, 2002), NIST (Doddington, 2002) and
Meteor ranking scores (Agarwal and Lavie, 2008)
based on truecased output.
2 Baseline System
This years improvements were added to the LIU
baseline system (Stymne et al, 2010). Our base-
line is a factored phrase based SMT system that uses
the Moses toolkit (Koehn et al, 2007) for transla-
tion model training and decoding, GIZA++ (Och
and Ney, 2003) for word alignment, SRILM (Stol-
cke, 2002) an KenLM (Heafield, 2011) for language
modelling and minimum error rate training (Och,
2003) to tune model feature weights. In addition,
the LIU baseline contains:
? Compound processing, including compound
splitting and for translation into German also
compound merging
? Part-of-speech and morphological sequence
models
All models were trained on truecased data. Trans-
lation and reordering models were trained using the
bilingual Europarl and News Commentary corpora
that were concatenated before training. We created
two language models. The first model is a 5-gram
model that we created by interpolating two language
393
models from bilingual News Commentary and Eu-
roparl with more weight on the News Commentary
model. The second model is a 4-gram model trained
on monolingual News only. All models were cre-
ated using entropy-based pruning with 10?8 as the
threshold.
Due to time constraints, all tuning and evaluation
were performed on half of the provided shared task
data. Systems were tuned on 1262 sentences from
newstest2009 and all results reported in Tables 1 and
2 are based on a devtest set of 1244 sentences from
newstest2010.
2.1 Sequence models with part-of-speech and
morphology
To improve target word order and agreement in the
translation output, we added an extra output factor in
our translation models consisting of tags with POS
and morphological features. For English we used
tags that were obtained by enriching POS tags from
TreeTagger (Schmid, 1994) with additional morpho-
logical features such as number for determiners. For
German, the POS and morphological tags were ob-
tained from RFTagger (Schmid and Laws, 2008)
which provides morphological information such as
case, number and gender for nouns and tense for
verbs. We trained two sequence models for each
system over this output factor and added them as
features in our baseline system. The first sequence
model is a 7-gram model interpolated from models
of bilingual Europarl and News Commentary. The
second model is a 6-gram model trained on mono-
lingual News only.
2.2 Compound processing
In both translation directions we split compounds,
using a modified version of the corpus-based split-
ting method of Koehn and Knight (2003). We split
nouns, verb, and adjective compounds into known
parts that were content words or cardinal numbers,
based on the arithmetic mean of the frequency of
the parts in the training corpus. We allowed 10 com-
mon letter changes (Langer, 1998) and hyphens at
split points. Compound parts were kept in their sur-
face form and compound modifiers received a part-
of-speech tag based on that of the tag of the full com-
pound.
For translation into German, compounds were
merged using the POS-merging strategy of Stymne
(2009). A compound part in the translation output,
identified by the special part-of-speech tags, was
merged with the next word if that word had a match-
ing part-of-speech tag. If the compound part was
followed by the conjunction und (and), we added a
hyphen to the part, to account for coordinated com-
pounds.
2.3 Hierarchical reordering
In our baseline system we experimented with two
lexicalized reordering models. The standard model
in Moses (Koehn et al, 2005), and the hierarchi-
cal model of Galley and Manning (2008). In both
models the placement of a phrase is compared to
that of the previous and/or next phrase. In the stan-
dard model up to three reorderings are distinguished,
monotone, swap, and discontinuous. In the hier-
archical model the discontinuous class can be fur-
ther subdivided into two classes, left and right dis-
continuous. The hierarchical model further differs
from the standard model in that it compares the or-
der of the phrase with the next or previous block of
phrases, not only with the next or previous single
phrase.
We investigated one configuration of each
model. For the standard model we used the msd-
bidirectional-fe setting, which uses three orienta-
tions, is conditioned on both the source and target
language, and considers both the previous and next
phrase. For the hierarchical model we used all four
orientations, and again it is conditioned on both the
source and target language, and considers both the
previous and next phrase.
The result of replacing the standard reordering
model with an hierarchical model is shown in Table
1 and 2. For translation into German adding the hi-
erarchical model led to small improvements as mea-
sured by NIST and Meteor. For translation in the
other direction, the differences on automatic metrics
were very small. Still, we decided to use the hierar-
chical model in all our systems.
3 German?English
For translation from German into English we fo-
cused on making the German source text more sim-
ilar to English by removing redundant morphology
394
and changing word order before training translation
models.
3.1 Normalization
We performed normalization of German words to re-
move distinctions that do not exist in English, such
as case distinctions on nouns. This strategy is sim-
ilar to that of El-Kahlout and Yvon (2010), but we
used a slightly different set of transformations, that
we thought better mirrored the English structure.
For morphological tags we used RFTagger and for
lemmas we used TreeTagger. The morphological
transformations we performed were the following:
? Nouns:
? Replace with lemma+s if plural number
? Replace with lemma otherwise
? Verbs:
? Replace with lemma if present tense, not
third person singular
? Replace with lemma+p if past tense
? Adjectives:
? Replace with lemma+c if comparative
? Replace with lemma+sup if superlative
? Replace with lemma otherwise
? Articles:
? Definite articles:
? Replace with des if genitive
? Replace with der otherwise
? Indefinite articles:
? Replace with eines if genitive
? Replace with ein otherwise
? Pronouns:
? Replace with RELPRO if relative
? Replace with lemma if indefinite, interrog-
ative, or possessive pronouns
? Add +g to all pronouns which are geni-
tive, unless they are possessive
For all word types that are not mentioned in the
list, surface forms were kept.
BLEU NIST Meteor
Baseline 21.01 6.2742 41.32
+hier reo 20.94 6.2800 41.24
+normalization 20.85 6.2370 41.04
+source reordering 21.06 6.3082 41.40
+ OOV proc. 21.22 6.3692 41.51
Table 1: German?English translation results. Results are
cumulative.
We also performed those tokenization and
spelling normalizations suggested by El-Kahlout
and Yvon (2010), that we judged could safely be
done for translation from German without collect-
ing corpus statistics. We split words with numbers
and letters, such as 40-ja?hrigen or 40ja?hrigen (40
year-old), unless the suffix indicates that it is a ordi-
nal, such as 70sten (70th). We also did some spelling
normalization by exchanging ? with ss and replacing
tripled consonants with doubled consonants. These
changes would have been harmful for translation
into German, since they change the language into a
normalized variant, but for translation from German
we considered them safe.
3.2 Source side reordering
To make the word order of German input sen-
tences more English-like a version of the rules of
(Collins et al, 2005) were partially implemented us-
ing tagged output from the RFTagger. Basically,
beginnings of subordinate clauses, their subjects (if
present) and final verb clusters were identified based
on tag sequences, and the clusters were moved to
the beginning of the clause, and reordered so that
the finite verb ended up in the second clause posi-
tion. Also, some common adverbs were moved with
the verb cluster and placed between finite and non-
finite verbs. After testing, we decided to apply these
rules only to subordinate clauses at the end of sen-
tences, since these were the only ones that could be
identified with good precision. Still, some 750,000
clauses were reordered.
3.3 OOV Processing
We also added limited processing of OOVs. In a pre-
processing step we replaced unknown words with
known cased variants if available, removed markup
from normalized words if that resulted in an un-
395
known token, and split hyphened words. We also
split suspected names in cases where we had a pat-
tern with a single upper-case letter in the middle of a
word, such as ConocoPhillips into Conoco Phillips.
In a post-processing step we changed the number
formatting of unknown numbers by changing dec-
imal points and thousand separators, to agree with
English orthography. This processing only affects
a small number of words, and cannot be expected
to make a large impact on the final results. Out
of 884 OOVs in the devtest, 39 had known cased
options, 126 hyphened words were split, 147 cases
had markup from the normalization removed, and 13
suspected names were split.
3.4 Results
The results of these experiments can be seen in Table
1 where each new addition is added to the previous
system. When we compare the new additions with
the baseline with hierarchical reordering, we see that
while the normalization did not seem to have a posi-
tive effect on any metric, both source reordering and
OOV processing led to small increases on all scores.
4 English?German
For translation from English into German we at-
tempted to improve the quality of the phrase table by
adding new word alignments to the standard Giza++
alignments.
4.1 Phrase-based word alignment
We experimented with different ways of com-
bining word alignments from Giza++ with align-
ments created using phrase-based word alignment
(PAL) which previously has been shown to improve
alignment quality for English?Swedish (Holmqvist,
2010). The idea of phrase-based word alignment is
to use word and part-of-speech sequence patterns
from manual word alignments to align new texts.
First, parallel phrases containing a source segment,
a target segment and links between source and target
words are extracted from word aligned texts (Figure
1). In the second step, these phrases are matched
against new parallel text and if a matching phrase
is found, word links from the phrase are added to
the corresponding words in the new text. In order
to increase the number of matching phrases and im-
prove word alignment recall, words in the parallel
En: a typical example
De: ein typisches Beispiel
Links: 0-0 1-1 2-2
En: a JJ example
De: ein ADJA Beispiel
Links: 0-0 1-1 2-2
En: DT JJ NN
De: ART ADJA N
Links: 0-0 1-1 2-2
Figure 1: Examples of parallel phrases used in word
alignment.
BLEU NIST Meteor
Baseline 16.16 6.2742 50.89
+hier reo 16.06 6.2800 51.25
+pal-gdfa 16.14 5.6527 51.10
+pal-dual 15.71 5.5735 50.43
+pal-inter 15.92 5.6230 50.73
Table 2: English?German translation results, results
are cumulative except for the three alternative PAL-
configurations.
segments were replaced by POS/morphological tags
from RFTagger.
Alignment patterns were extracted from 1000 sen-
tences in the manually word aligned sample of
English?German Europarl texts from Pado and Lap-
ata (2006). All parallel phrases were extracted from
the word aligned texts, as when extracting a trans-
lation model. Parallel phrases that contain at least
3 words were generalized with POS tags to form
word/POS patterns for alignment. A subset of these
patterns, with high alignment precision (> 0.80) on
the 1000 sentences, were used to align the entire
training corpus.
We combined the new word alignments with
the Giza++ alignments in two ways. In the first
method, we used a symmetrization heuristic similar
to grow-diag-final-and to combine three word align-
ments into one, the phrase-based alignment and two
Giza++ alignments in different directions. In the
second method we extracted a separate phrase ta-
ble from the sparser phrase-based alignment using
a constrained method of phrase extraction that lim-
ited the number of unaligned words in each phrase
pair. The reason for constraining the phrase table
396
extraction was that the standard extraction method
does not work well for the sparse word alignments
that PAL produces, but we think it could still be
useful for extracting highly reliable phrases. After
some experimentation we decided to allow an unlim-
ited number of internal unaligned words, that is un-
aligned words that are surrounded by aligned words,
but limit the number of external unaligned words,
i.e., unaligned words at the beginning or end of the
phrase, to either one each in the source and target
phrase, or to zero.
We used two ways to include the sparse phrase-
table into the translation process:
? Have two separate phrase-tables, the sparse ta-
ble, and the standard GIZA++ based phrase-
table, and use Moses? dual decoding paths.
? Interpolate the sparse phrase-table with the
standard phrase-table, using the mixture model
formulation of Ueffing et al (2007), with equal
weights, in order to boost the probabilities of
highly reliable phrases.
4.2 Results
We evaluated our systems on devtest data and found
that the added phrase-based alignments did not pro-
duce large differences in translation quality com-
pared to the baseline system with hierarchical re-
ordering as shown in Table 2. The system created
with a heuristic combination of PAL and Giza++
(pal-gdfa) had a small increase in BLEU, but no im-
provement on the other metrics. Systems using a
phrase table extracted from the sparse alignments
did not produce better results than baseline. The sys-
tem using dual decoding paths (pal-dual) produced
worse results than the system using an interpolated
phrase table (pal-inter).
5 Submitted systems
The LIU system participated in German?English
and English?German translation in the WMT 2011
shared task. The new additions were a combina-
tion of unsupervised and supervised word align-
ments, spelling normalization, clause reordering and
OOV processing. Our submitted systems contain
all additions described in this paper. For English-
German we used the best performing method of
BLEU
System Devtest Test
en-de
baseline +hier 16.1 14.5
submitted 16.1 14.8
de-en
baseline +hier 20.9 19.3
submitted 21.2 19.9
Table 3: Summary of devtest results and shared task test
results for submitted systems and LIU baseline with hier-
archical reordering.
word alignment combination which was the method
that uses heuristic combination similar to grow-diag-
final-and.
The results of our submitted systems are shown
in Table 3 where we compare them to the LIU base-
line system with hierarchical reordering models. We
report modest improvements on the devtest set for
both translation directions. We also found small im-
provements of our submitted systems in the official
shared task evaluation on the test set newstest2011.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio.
Michael Collins, Philipp Koehn, and Ivona Kucerova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 531?540, Ann Arbor, Michigan.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurence
statistics. In Proceedings of the Second International
Conference on Human Language Technology, pages
228?231, San Diego, California.
I?lknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English statisti-
cal machine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation,
pages 251?258, Paris, France.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
848?856, Honolulu, Hawaii.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the Sixth
397
Workshop on Statistical Machine Translation, Edin-
burgh, UK.
Maria Holmqvist. 2010. Heuristic word alignment
with parallel phrases. In Proceedings of the Seventh
Conference on International Language Resources and
Evaluation, pages 744-748, Valletta, Malta.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
Tenth Conference of EACL, pages 187?193, Budapest,
Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation, Pittsburgh, Pennsylvania.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL, Demon-
stration Session, 177?180, Prague, Czech Republic.
Stefan Langer. 1998. Zur Morphologie und Semantik
von Nominalkomposita. In Tagungsband der 4. Kon-
ferenz zur Verarbeitung natu?rlicher Sprache, pages
83?97, Bonn, Germany.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167, Sap-
poro, Japan.
Sebastian Pado and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 1161?1168, Sydney, Aus-
tralia.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of ACL, pages 311?318,
Philadelphia, Pennsylvania.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics, pages 777?784, Manchester, UK.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, UK.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 189?194, Uppsala,
Sweden.
Sara Stymne. 2009. A comparison of merging strategies
for translation of German compounds. In Proceedings
of the EACL Student Research Workshop, pages 61?
69, Athens, Greece.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Semi-supervised model adaptation for statistical
machine translation. Machine Translation, 21(2):77?
94.
398
