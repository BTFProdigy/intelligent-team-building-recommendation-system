Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 394?402,
Beijing, August 2010
Better Arabic Parsing: Baselines, Evaluations, and Analysis
Spence Green and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,manning}@stanford.edu
Abstract
In this paper, we offer broad insight
into the underperformance of Arabic con-
stituency parsing by analyzing the inter-
play of linguistic phenomena, annotation
choices, and model design. First, we iden-
tify sources of syntactic ambiguity under-
studied in the existing parsing literature.
Second, we show that although the Penn
Arabic Treebank is similar to other tree-
banks in gross statistical terms, annotation
consistency remains problematic. Third,
we develop a human interpretable gram-
mar that is competitive with a latent vari-
able PCFG. Fourth, we show how to build
better models for three different parsers.
Finally, we show that in application set-
tings, the absence of gold segmentation
lowers parsing performance by 2?5% F1.
1 Introduction
It is well-known that constituency parsing mod-
els designed for English often do not generalize
easily to other languages and treebanks.1 Expla-
nations for this phenomenon have included the
relative informativeness of lexicalization (Dubey
and Keller, 2003; Arun and Keller, 2005), insensi-
tivity to morphology (Cowan and Collins, 2005;
Tsarfaty and Sima?an, 2008), and the effect of
variable word order (Collins et al, 1999). Cer-
tainly these linguistic factors increase the diffi-
culty of syntactic disambiguation. Less frequently
studied is the interplay among language, annota-
tion choices, and parsing model design (Levy and
Manning, 2003; Ku?bler, 2005).
1The apparent difficulty of adapting constituency mod-
els to non-configurational languages has been one motivation
for dependency representations (Hajic? and Zema?nek, 2004;
Habash and Roth, 2009).
To investigate the influence of these factors,
we analyze Modern Standard Arabic (henceforth
MSA, or simply ?Arabic?) because of the unusual
opportunity it presents for comparison to English
parsing results. The Penn Arabic Treebank (ATB)
syntactic guidelines (Maamouri et al, 2004) were
purposefully borrowed without major modifica-
tion from English (Marcus et al, 1993). Further,
Maamouri and Bies (2004) argued that the English
guidelines generalize well to other languages. But
Arabic contains a variety of linguistic phenom-
ena unseen in English. Crucially, the conventional
orthographic form of MSA text is unvocalized, a
property that results in a deficient graphical rep-
resentation. For humans, this characteristic can
impede the acquisition of literacy. How do addi-
tional ambiguities caused by devocalization affect
statistical learning? How should the absence of
vowels and syntactic markers influence annotation
choices and grammar development? Motivated by
these questions, we significantly raise baselines
for three existing parsing models through better
grammar engineering.
Our analysis begins with a description of syn-
tactic ambiguity in unvocalized MSA text (?2).
Next we show that the ATB is similar to other tree-
banks in gross statistical terms, but that annotation
consistency remains low relative to English (?3).
We then use linguistic and annotation insights to
develop a manually annotated grammar for Arabic
(?4). To facilitate comparison with previous work,
we exhaustively evaluate this grammar and two
other parsing models when gold segmentation is
assumed (?5). Finally, we provide a realistic eval-
uation in which segmentation is performed both
in a pipeline and jointly with parsing (?6). We
quantify error categories in both evaluation set-
tings. To our knowledge, ours is the first analysis
of this kind for Arabic parsing.
394
2 Syntactic Ambiguity in Arabic
Arabic is a morphologically rich language with a
root-and-pattern system similar to other Semitic
languages. The basic word order is VSO, but
SVO, VOS, and VO configurations are also pos-
sible.2 Nouns and verbs are created by selecting
a consonantal root (usually triliteral or quadrilit-
eral), which bears the semantic core, and adding
affixes and diacritics. Particles are uninflected.
Diacritics can also be used to specify grammatical
relations such as case and gender. But diacritics
are not present in unvocalized text, which is the
standard form of, e.g., news media documents.3
Let us consider an example of ambiguity caused
by devocalization. Table 1 shows four words
whose unvocalized surface forms ? an are indis-
tinguishable. Whereas Arabic linguistic theory as-
signs (1) and (2) to the class of pseudo verbs ?
Ahw	
? inna and her sisters since they can be
inflected, the ATB conventions treat (2) as a com-
plementizer, which means that it must be the head
of SBAR. Because these two words have identical
complements, syntax rules are typically unhelp-
ful for distinguishing between them. This is es-
pecially true in the case of quotations?which are
common in the ATB?where (1) will follow a verb
like (2) (Figure 1).
Even with vocalization, there are linguistic cat-
egories that are difficult to identify without se-
mantic clues. Two common cases are the attribu-
tive adjective and the process nominal CdOm?
maSdar, which can have a verbal reading.4 At-
tributive adjectives are hard because they are or-
thographically identical to nominals; they are in-
flected for gender, number, case, and definiteness.
Moreover, they are used as substantives much
2Unlike machine translation, constituency parsing is not
significantly affected by variable word order. However, when
grammatical relations like subject and object are evaluated,
parsing performance drops considerably (Green et al, 2009).
In particular, the decision to represent arguments in verb-
initial clauses as VP internal makes VSO and VOS configu-
rations difficult to distinguish. Topicalization of NP subjects
in SVO configurations causes confusion with VO (pro-drop).
3Techniques for automatic vocalization have been studied
(Zitouni et al, 2006; Habash and Rambow, 2007). However,
the data sparsity induced by vocalization makes it difficult to
train statistical models on corpora of the size of the ATB, so
vocalizing and then parsing may well not help performance.
4Traditional Arabic linguistic theory treats both of these
types as subcategories of noun ?F?.
Word Head Of Complement POS
1 ?? inna ?Indeed, truly? VP Noun VBP
2 ??
 anna ?That? SBAR Noun IN
3 ? in ?If? SBAR Verb IN
4 ?
 an ?to? SBAR Verb IN
Table 1: Diacritized particles and pseudo-verbs that, after
orthographic normalization, have the equivalent surface form
? an. The distinctions in the ATB are linguistically justified,
but complicate parsing. Table 8a shows that the best model
recovers SBAR at only 71.0% F1.
VP
VBD
?AR
she added
S
VP
PUNC
?
VBP
?
Indeed
NP
NN
?d}
Saddam
. . .
(a) Reference
VP
VBD
?AR
she added
SBAR
PUNC
?
IN
?
Indeed
NP
NN
?d}
Saddam
. . .
(b) Stanford
Figure 1: The Stanford parser (Klein and Manning, 2002)
is unable to recover the verbal reading of the unvocalized
surface form ? an (Table 1).
more frequently than is done in English.
Process nominals name the action of the tran-
sitive or ditransitive verb from which they derive.
The verbal reading arises when the maSdar has an
NP argument which, in vocalized text, is marked
in the accusative case. When the maSdar lacks
a determiner, the constituent as a whole resem-
bles the ubiquitous annexation construct T?AR?
iDafa. Gabbard and Kulick (2008) show that
there is significant attachment ambiguity associ-
ated with iDafa, which occurs in 84.3% of the
trees in our development set. Figure 4 shows
a constituent headed by a process nominal with
an embedded adjective phrase. All three mod-
els evaluated in this paper incorrectly analyze the
constituent as iDafa; none of the models attach the
attributive adjectives properly.
For parsing, the most challenging form of am-
biguity occurs at the discourse level. A defining
characteristic of MSA is the prevalence of dis-
course markers to connect and subordinate words
and phrases (Ryding, 2005). Instead of offsetting
new topics with punctuation, writers of MSA in-
sert connectives such as ? wa and ? fa to link
new elements to both preceding clauses and the
text as a whole. As a result, Arabic sentences are
usually long relative to English, especially after
395
Length English (WSJ) Arabic (ATB)
? 20 41.9% 33.7%
? 40 92.4% 73.2%
? 63 99.7% 92.6%
? 70 99.9% 94.9%
Table 2: Frequency distribution for sentence lengths in the
WSJ (sections 2?23) and the ATB (p1?3). English parsing
evaluations usually report results on sentences up to length
40. Arabic sentences of up to length 63 would need to be
evaluated to account for the same fraction of the data. We
propose a limit of 70 words for Arabic parsing evaluations.
Part of Speech Tag Freq.
? wa
?and?
conjunction CC 4256
preposition IN 6
abbreviation NN 6
? fa
?so, then?
conjunction CC 160
connective particle RP 67
abbreviation NN 22
response conditioning particle RP 11
subordinating conjunction IN 3
Table 3: Dev set frequencies for the two most significant dis-
course markers in Arabic are skewed toward analysis as a
conjunction.
segmentation (Table 2). The ATB gives several
different analyses to these words to indicate dif-
ferent types of coordination. But it conflates the
coordinating and discourse separator functions of
wa (?W`? ??) into one analysis: conjunction
(Table 3). A better approach would be to distin-
guish between these cases, possibly by drawing
on the vast linguistic work on Arabic connectives
(Al-Batal, 1990). We show that noun-noun vs.
discourse-level coordination ambiguity in Arabic
is a significant source of parsing errors (Table 8c).
3 Treebank Comparison
3.1 Gross Statistics
Linguistic intuitions like those in the previous sec-
tion inform language-specific annotation choices.
The resulting structural differences between tree-
banks can account for relative differences in pars-
ing performance. We compared the ATB5 to tree-
banks for Chinese (CTB6), German (Negra), and
English (WSJ) (Table 4). The ATB is disadvan-
taged by having fewer trees with longer average
5LDC A-E catalog numbers: LDC2008E61 (ATBp1v4),
LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).
We map the ATB morphological analyses to the shortened
?Bies? tags for all experiments.
ATB CTB6 Negra WSJ
Trees 23449 28278 20602 43948
Word Typess 40972 45245 51272 46348
Tokens 738654 782541 355096 1046829
Tags 32 34 499 45
Phrasal Cats 22 26 325 27
Test OOV 16.8% 22.2% 30.5% 13.2%
Per Sentence
Depth (? / ?2) 3.87 / 0.74 5.01 / 1.44 3.58 / 0.89 4.18 / 0.74
Breadth (? / ?2) 14.6 / 7.31 10.2 / 4.44 7.50 / 4.56 12.1 / 4.65
Length (? / ?2) 31.5 / 22.0 27.7 / 18.9 17.2 / 10.9 23.8 / 11.2
Constituents (?) 32.8 32.5 8.29 19.6
? Const. / ? Length 1.04 1.18 0.482 0.820
Table 4: Gross statistics for several different treebanks. Test
set OOV rate is computed using the following splits: ATB
(Chiang et al, 2006); CTB6 (Huang and Harper, 2009); Ne-
gra (Dubey and Keller, 2003); English, sections 2-21 (train)
and section 23 (test).
yields.6 But to its great advantage, it has a high
ratio of non-terminals/terminals (? Constituents /
? Length). Evalb, the standard parsing metric, is
biased toward such corpora (Sampson and Babar-
czy, 2003). Also surprising is the low test set OOV
rate given the possibility of morphological varia-
tion in Arabic. In general, several gross corpus
statistics favor the ATB, so other factors must con-
tribute to parsing underperformance.
3.2 Inter-annotator Agreement
Annotation consistency is important in any super-
vised learning task. In the initial release of the
ATB, inter-annotator agreement was inferior to
other LDC treebanks (Maamouri et al, 2008). To
improve agreement during the revision process,
a dual-blind evaluation was performed in which
10% of the data was annotated by independent
teams. Maamouri et al (2008) reported agree-
ment between the teams (measured with Evalb) at
93.8% F1, the level of the CTB. But Rehbein and
van Genabith (2007) showed that Evalb should
not be used as an indication of real difference?
or similarity?between treebanks.
Instead, we extend the variation n-gram
method of Dickinson (2005) to compare annota-
tion error rates in the WSJ and ATB. For a corpus
C, let M be the set of tuples ?n, l?, where n is an
n-gram with bracketing label l. If any n appears
6Generative parsing performance is known to deteriorate
with sentence length. As a result, Habash et al (2006) devel-
oped a technique for splitting and chunking long sentences.
In application settings, this may be a profitable strategy.
396
Corpus Sample Error %
Trees Nuclei n-grams Type n-gram
WSJ 2?23 43948 25041 746 12.0% 2.10%
ATB 23449 20292 2100 37.0% 1.76%
Table 5: Evaluation of 100 randomly sampled variation nu-
clei types. The samples from each corpus were indepen-
dently evaluated. The ATB has a much higher fraction of
nuclei per tree, and a higher type-level error rate.
in a corpus position without a bracketing label,
then we also add ?n,NIL? to M. We call the set
of unique n-grams with multiple labels in M the
variation nuclei of C.
Bracketing variation can result from either an-
notation errors or linguistic ambiguity. Human
evaluation is one way to distinguish between the
two cases. Following Dickinson (2005), we ran-
domly sampled 100 variation nuclei from each
corpus and evaluated each sample for the presence
of an annotation error. The human evaluators were
a non-native, fluent Arabic speaker (the first au-
thor) for the ATB and a native English speaker for
the WSJ.7
Table 5 shows type- and token-level error rates
for each corpus. The 95% confidence intervals for
type-level errors are (5580, 9440) for the ATB and
(1400, 4610) for the WSJ. The results clearly in-
dicate increased variation in the ATB relative to
the WSJ, but care should be taken in assessing the
magnitude of the difference. On the one hand,
the type-level error rate is not calibrated for the
number of n-grams in the sample. At the same
time, the n-gram error rate is sensitive to samples
with extreme n-gram counts. For example, one of
the ATB samples was the determiner??? dhalik
?that.? The sample occurred in 1507 corpus po-
sitions, and we found that the annotations were
consistent. If we remove this sample from the
evaluation, then the ATB type-level error rises to
only 37.4% while the n-gram error rate increases
to 6.24%. The number of ATB n-grams also falls
below the WSJ sample size as the largest WSJ
sample appeared in only 162 corpus positions.
7Unlike Dickinson (2005), we strip traces and only con-
sider POS tags when pre-terminals are the only intervening
nodes between the nucleus and its bracketing (e.g., unaries,
base NPs). Since our objective is to compare distributions of
bracketing discrepancies, we do not use heuristics to prune
the set of nuclei.
NP
NN
Tm?
summit
NP
NNP
?rJ
Sharm
DTNNP
%yK?
Al-Sheikh
(a)
NP
NN
Tm?
summit
NP
NNP
?rJ
Sharm
NP
DTNNP
%yK?
Al-Sheikh
(b)
Figure 2: An ATB sample from the human evaluation. The
ATB annotation guidelines specify that proper nouns should
be specified with a flat NP (a). But the city name Sharm Al-
Sheikh is also iDafa, hence the possibility for the incorrect
annotation in (b).
4 Grammar Development
We can use the preceding linguistic and annota-
tion insights to build a manually annotated Ara-
bic grammar in the manner of Klein and Manning
(2003). Manual annotation results in human in-
terpretable grammars that can inform future tree-
bank annotation decisions. A simple lexicalized
PCFG with second order Markovization gives rel-
atively poor performance: 75.95% F1 on the test
set.8 But this figure is surprisingly competitive
with a recent state-of-the-art baseline (Table 7).
In our grammar, features are realized as annota-
tions to basic category labels. We start with noun
features since written Arabic contains a very high
proportion of NPs. genitiveMark indicates recur-
sive NPs with a indefinite nominal left daughter
and an NP right daughter. This is the form of re-
cursive levels in iDafa constructs. We also add an
annotation for one-level iDafa (oneLevelIdafa)
constructs since they make up more than 75% of
the iDafa NPs in the ATB (Gabbard and Kulick,
2008). For all other recursive NPs, we add a
common annotation to the POS tag of the head
(recursiveNPHead).
Base NPs are the other significant category of
nominal phrases. markBaseNP indicates these
non-recursive nominal phrases. This feature in-
cludes named entities, which the ATB marks with
a flat NP node dominating an arbitrary number of
NNP pre-terminal daughters (Figure 2).
For verbs we add two features. First we mark
any node that dominates (at any level) a verb
8We use head-finding rules specified by a native speaker
of Arabic. This PCFG is incorporated into the Stanford
Parser, a factored model that chooses a 1-best parse from the
product of constituency and dependency parses.
397
Feature States Tags F1 Indiv. ?F1
? 3208 33 76.86 ?
recursiveNPHead 3287 38 77.46 +0.60
genitiveMark 3471 38 77.88 +0.42
splitPUNC 4221 47 77.98 +0.10
markContainsVerb 5766 47 79.16 +1.18
markBaseNP 6586 47 79.5 +0.34
markOneLevelIdafa 7202 47 79.83 +0.33
splitIN 7595 94 80.48 +0.65
containsSVO 9188 94 80.66 +0.18
splitCC 9492 124 80.87 +0.21
markFem 10049 141 80.95 +0.08
Table 6: Incremental dev set results for the manually anno-
tated grammar (sentences of length ? 70).
phrase (markContainsVerb). This feature has a
linguistic justification. Historically, Arabic gram-
mar has identified two sentences types: those that
begin with a nominal (TymF? Tlm)?), and those
that begin with a verb (Tyl`f? Tlm)?). But for-
eign learners are often surprised by the verbless
predications that are frequently used in Arabic.
Although these are technically nominal, they have
become known as ?equational? sentences. mark-
ContainsVerb is especially effective for distin-
guishing root S nodes of equational sentences. We
also mark all nodes that dominate an SVO con-
figuration (containsSVO). In MSA, SVO usually
appears in non-matrix clauses.
Lexicalizing several POS tags improves perfor-
mance. splitIN captures the verb/preposition id-
ioms that are widespread in Arabic. Although
this feature helps, we encounter one consequence
of variable word order. Unlike the WSJ corpus
which has a high frequency of rules like VP ?
VB PP, Arabic verb phrases usually have lexi-
calized intervening nodes (e.g., NP subjects and
direct objects). For example, we might have
VP?VB NP PP, where the NP is the subject.
This annotation choice weakens splitIN.
The ATB gives all punctuation a single tag. For
parsing, this is a mistake, especially in the case
of interrogatives. splitPUNC restores the conven-
tion of the WSJ. We also mark all tags that dom-
inate a word with the feminine ending ? taa mar-
buuTa (markFeminine).
To differentiate between the coordinating and
discourse separator functions of conjunctions (Ta-
ble 3), we mark each CC with the label of its
right sister (splitCC). The intuition here is that
the role of a discourse marker can usually be de-
termined by the category of the word that follows
it. Because conjunctions are elevated in the parse
trees when they separate recursive constituents,
we choose the right sister instead of the category
of the next word. We create equivalence classes
for verb, noun, and adjective POS categories.
5 Standard Parsing Experiments
We compare the manually annotated grammar,
which we incorporate into the Stanford parser, to
both the Berkeley (Petrov et al, 2006) and Bikel
(Bikel, 2004) parsers. All experiments use ATB
parts 1?3 divided according to the canonical split
suggested by Chiang et al (2006). Preprocessing
the raw trees improves parsing performance con-
siderably.9 We first discard all trees dominated by
X, which indicates errors and non-linguistic text.
At the phrasal level, we remove all function tags
and traces. We also collapse unary chains with
identical basic categories like NP ? NP. The pre-
terminal morphological analyses are mapped to
the shortened ?Bies? tags provided with the tree-
bank. Finally, we add ?DT? to the tags for definite
nouns and adjectives (Kulick et al, 2006).
The orthographic normalization strategy we use
is simple.10 In addition to removing all diacrit-
ics, we strip instances of taTweel ??wW, col-
lapse variants of alif  to bare alif,11 and map Ara-
bic punctuation characters to their Latin equiva-
lents. We retain segmentation markers?which
are consistent only in the vocalized section of the
treebank?to differentiate between e.g. ?? ?they?
and ??+ ?their.? Because we use the vocalized
section, we must remove null pronoun markers.
In Table 7 we give results for several evalua-
tion metrics. Evalb is a Java re-implementation
of the standard labeled precision/recall metric.12
9Both the corpus split and pre-processing code are avail-
able at http://nlp.stanford.edu/projects/arabic.shtml.
10Other orthographic normalization schemes have been
suggested for Arabic (Habash and Sadat, 2006), but we ob-
serve negligible parsing performance differences between
these and the simple scheme used in this evaluation.
11taTweel (?) is an elongation character used in Arabic
script to justify text. It has no syntactic function. Variants
of alif are inconsistently used in Arabic texts. For alif with
hamza, normalization can be seen as another level of devo-
calization.
12For English, our Evalb implementation is identical to the
most recent reference (EVALB20080701). For Arabic we
398
Leaf Ancestor Evalb Tag
Model System Length Corpus Sent Exact LP LR F1 %
Stanford (v1.6.3)
Baseline 70 0.791 0.825 358 80.37 79.36 79.86 95.58
all 0.773 0.818 358 78.92 77.72 78.32 95.49
GoldPOS 70 0.802 0.836 452 81.07 80.27 80.67 99.95
Bikel (v1.2)
Baseline (Self-tag) 70 0.770 0.801 278 77.92 76.00 76.95 94.64
all 0.752 0.794 278 76.96 75.01 75.97 94.63
Baseline (Pre-tag) 70 0.771 0.804 295 78.35 76.72 77.52 95.68
all 0.752 0.796 295 77.31 75.64 76.47 95.68
GoldPOS 70 0.775 0.808 309 78.83 77.18 77.99 96.60
Berkeley (Sep. 09)
(Petrov, 2009) all ? ? ? 76.40 75.30 75.85 ?
Baseline 70 0.809 0.839 335 82.32 81.63 81.97 95.07
all 0.796 0.834 336 81.43 80.73 81.08 95.02
GoldPOS 70 0.831 0.859 496 84.37 84.21 84.29 99.87
Table 7: Test set results. Maamouri et al (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev
set results with gold POS tags for sentences of length ? 40. The Bikel GoldPOS configuration only supplies the gold POS
tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser.
75
80
85
5000 10000 15000
Berkeley
Stanford
Bikel
training trees
F1
Figure 3: Dev set learning curves for sentence lengths ? 70.
All three curves remain steep at the maximum training set
size of 18818 trees.
The Leaf Ancestor metric measures the cost of
transforming guess trees to the reference (Samp-
son and Babarczy, 2003). It was developed in re-
sponse to the non-terminal/terminal bias of Evalb,
but Clegg and Shepherd (2005) showed that it is
also a valuable diagnostic tool for trees with com-
plex deep structures such as those found in the
ATB. For each terminal, the Leaf Ancestor metric
extracts the shortest path to the root. It then com-
putes a normalized Levenshtein edit distance be-
tween the extracted chain and the reference. The
range of the score is between 0 and 1 (higher is
better). We report micro-averaged (whole corpus)
and macro-averaged (per sentence) scores along
add a constraint on the removal of punctuation, which has a
single tag (PUNC) in the ATB. Tokens tagged as PUNC are
not discarded unless they consist entirely of punctuation.
with the number of exactly matching guess trees.
5.1 Parsing Models
The Stanford parser includes both the manually
annotated grammar (?4) and an Arabic unknown
word model with the following lexical features:
1. Presence of the determiner ? Al
2. Contains digits
3. Ends with the feminine affix ? p
4. Various verbal (e.g., ?, 1) and adjectival
suffixes (e.g., T?)
Other notable parameters are second order vertical
Markovization and marking of unary rules.
Modifying the Berkeley parser for Arabic is
straightforward. After adding a ROOT node to
all trees, we train a grammar using six split-and-
merge cycles and no Markovization. We use the
default inference parameters.
Because the Bikel parser has been parameter-
ized for Arabic by the LDC, we do not change the
default model settings. However, when we pre-
tag the input?as is recommended for English?
we notice a 0.57% F1 improvement. We use the
log-linear tagger of Toutanova et al (2003), which
gives 96.8% accuracy on the test set.
5.2 Discussion
The Berkeley parser gives state-of-the-art perfor-
mance for all metrics. Our baseline for all sen-
tence lengths is 5.23% F1 higher than the best pre-
vious result. The difference is due to more careful
399
S-NOM
VP
VBG
?2A`tF
restoring
NP
NP
NN
C?2
role
NP
PRP
?
its
ADJP
DTJJ
?Anb?
constructive
DTJJ
??Af?
effective
(a) Reference
NP
NN
?2A`tF
NP
NP
NN
C?2
NP
PRP
?
ADJP
DTJJ
?Anb?
ADJP
DTJJ
??Af?
(b) Stanford
NP
NP
NN
?2A`tF
NP
NP
NN
C?2
NP
PRP
?
ADJP
DTJJ
?Anb?
ADJP
DTJJ
??Af?
(c) Berkeley
NP
NN
?2A`tF
NP
NP
NP
NN
C?2
NP
PRP
?
ADJP
DTJJ
?Anb?
ADJP
DTJJ
??Af?
(d) Bikel
Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmen-
tation). The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs,
maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking
determiners and heading iDafa (Fassi Fehri, 1993). In the ATB, ?2A`tF asta?adah is tagged 48 times as a noun and 9 times
as verbal noun. Consequently, all three parsers prefer the nominal reading. Table 8b shows that verbal nouns are the hardest
pre-terminal categories to identify. None of the models attach the attributive adjectives correctly.
pre-processing. However, the learning curves in
Figure 3 show that the Berkeley parser does not
exceed our manual grammar by as wide a mar-
gin as has been shown for other languages (Petrov,
2009). Moreover, the Stanford parser achieves the
most exact Leaf Ancestor matches and tagging ac-
curacy that is only 0.1% below the Bikel model,
which uses pre-tagged input.
In Figure 4 we show an example of variation
between the parsing models. We include a list
of per-category results for selected phrasal labels,
POS tags, and dependencies in Table 8. The er-
rors shown are from the Berkeley parser output,
but they are representative of the other two pars-
ing models.
6 Joint Segmentation and Parsing
Although the segmentation requirements for Ara-
bic are not as extreme as those for Chinese, Ara-
bic is written with certain cliticized prepositions,
pronouns, and connectives connected to adjacent
words. Since these are distinct syntactic units,
they are typically segmented. The ATB segmen-
tation scheme is one of many alternatives. Until
now, all evaluations of Arabic parsing?including
the experiments in the previous section?have as-
sumed gold segmentation. But gold segmentation
is not available in application settings, so a seg-
menter and parser are arranged in a pipeline. Seg-
mentation errors cascade into the parsing phase,
placing an artificial limit on parsing performance.
Lattice parsing (Chappelier et al, 1999) is an
alternative to a pipeline that prevents cascading
errors by placing all segmentation options into
the parse chart. Recently, lattices have been used
successfully in the parsing of Hebrew (Tsarfaty,
2006; Cohen and Smith, 2007), a Semitic lan-
guage with similar properties to Arabic. We ex-
tend the Stanford parser to accept pre-generated
lattices, where each word is represented as a finite
state automaton. To combat the proliferation of
parsing edges, we prune the lattices according to
a hand-constructed lexicon of 31 clitics listed in
the ATB annotation guidelines (Maamouri et al,
2009a). Formally, for a lexicon L and segments
I ? L, O /? L, each word automaton accepts the
language I?(O+ I)I?. Aside from adding a simple
rule to correct alif deletion caused by the prepo-
sition ?, no other language-specific processing is
performed.
Our evaluation includes both weighted and un-
weighted lattices. We weight edges using a
unigram language model estimated with Good-
Turing smoothing. Despite their simplicity, uni-
gram weights have been shown as an effective fea-
ture in segmentation models (Dyer, 2009).13 The
joint parser/segmenter is compared to a pipeline
that uses MADA (v3.0), a state-of-the-art Arabic
segmenter, configured to replicate ATB segmen-
tation (Habash and Rambow, 2005). MADA uses
an ensemble of SVMs to first re-rank the output of
a deterministic morphological analyzer. For each
13Of course, this weighting makes the PCFG an improper
distribution. However, in practice, unknown word models
also make the distribution improper.
400
Label # gold F1
ADJP 1216 59.45
SBAR 2918 69.81
FRAG 254 72.87
VP 5507 78.83
S 6579 78.91
PP 7516 80.93
NP 34025 84.95
ADVP 1093 90.64
WHNP 787 96.00
(a) Major phrasal
categories
Tag # gold % Tag # gold %
VBG 182 48.84 JJR 134 92.83
VN 163 60.37 DTNNS 1069 94.29
VBN 352 72.42 DTJJ 3361 95.07
DTNNP 932 83.48 NNP 4152 95.09
JJ 1516 86.09 NN 10336 95.23
ADJ NUM 277 88.93 DTNN 6736 95.78
VBP 2139 89.94 NOUN QUANT 352 98.16
RP 818 91.23 PRP 1366 98.24
NNS 907 91.75 CC 4076 98.92
DTJJR 78 92.41 IN 8676 99.07
VBD 2580 92.42 DT 525 99.81
(b) Major POS categories
Parent Head Modifer Dir # gold F1
NP NP TAG R 946 0.54
S S S R 708 0.57
NP NP ADJP R 803 0.64
NP NP NP R 2907 0.66
NP NP SBAR R 1035 0.67
NP NP PP R 2713 0.67
VP TAG PP R 3230 0.80
NP NP TAG L 805 0.85
VP TAG SBAR R 772 0.86
S VP NP L 961 0.87
(c) Ten lowest scoring (Collins,
2003)-style dependencies occur-
ring more than 700 times
Table 8: Per category performance of the Berkeley parser on sentence lengths ? 70 (dev set, gold segmentation). (a) Of
the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in ?2 that lexical ambiguity
explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN)
and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007),
but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by
e.g., ?S S S R? and ?NP NP NP R?. ?NP NP PP R? and ?NP NP ADJP R? are both iDafa attachment.
input token, the segmentation is then performed
deterministically given the 1-best analysis.
Since guess and gold trees may now have dif-
ferent yields, the question of evaluation is com-
plex. Cohen and Smith (2007) chose a metric like
SParseval (Roark et al, 2006) that first aligns the
trees and then penalizes segmentation errors with
an edit-distance metric. But we follow the more
direct adaptation of Evalb suggested by Tsarfaty
(2006), who viewed exact segmentation as the ul-
timate goal. Therefore, we only score guess/gold
pairs with identical character yields, a condition
that allows us to measure parsing, tagging, and
segmentation accuracy by ignoring whitespace.
Table 9 shows that MADA produces a high
quality segmentation, and that the effect of cas-
cading segmentation errors on parsing is only
1.92% F1. However, MADA is language-specific
and relies on manually constructed dictionaries.
Conversely, the lattice parser requires no linguis-
tic resources and produces segmentations of com-
parable quality. Nonetheless, parse quality is
much lower in the joint model because a lattice
is effectively a long sentence. A cell in the bottom
row of the parse chart is required for each poten-
tial whitespace boundary. As we have said, parse
quality decreases with sentence length. Finally,
we note that simple weighting gives nearly a 2%
F1 improvement, whereas Goldberg and Tsarfaty
(2008) found that unweighted lattices were more
effective for Hebrew.
LP LR F1 Seg F1 Tag F1 Coverage
STANFORD (Gold) 81.64 80.55 81.09 100.0 95.81 100.0%
MADA ? ? ? 97.67 ? 96.42%
MADA+STANFORD 79.44 78.90 79.17 97.67 94.27 96.42%
STANFORDJOINT 76.13 72.61 74.33 94.12 90.13 94.73%
STANFORDJOINT+UNI 77.09 74.97 76.01 96.26 92.23 95.87%
Table 9: Dev set results for sentences of length ? 70. Cov-
erage indicates the fraction of hypotheses in which the char-
acter yield exactly matched the reference. Each model was
able to produce hypotheses for all input sentences. In these
experiments, the input lacks segmentation markers, hence the
slightly different dev set baseline than in Table 6.
7 Conclusion
By establishing significantly higher parsing base-
lines, we have shown that Arabic parsing perfor-
mance is not as poor as previously thought, but
remains much lower than English. We have de-
scribed grammar state splits that significantly im-
prove parsing performance, catalogued parsing er-
rors, and quantified the effect of segmentation er-
rors. With a human evaluation we also showed
that ATB inter-annotator agreement remains low
relative to the WSJ corpus. Our results suggest
that current parsing models would benefit from
better annotation consistency and enriched anno-
tation in certain syntactic configurations.
Acknowledgments We thank Steven Bethard, Evan Rosen,
and Karen Shiells for material contributions to this work. We
are also grateful to Markus Dickinson, Ali Farghaly, Nizar
Habash, Seth Kulick, David McCloskey, Claude Reichard,
Ryan Roth, and Reut Tsarfaty for constructive discussions.
The first author is supported by a National Defense Science
and Engineering Graduate (NDSEG) fellowship. This paper
is based on work supported in part by DARPA through IBM.
The content does not necessarily reflect the views of the U.S.
Government, and no official endorsement should be inferred.
401
References
Al-Batal, M. 1990. Connectives as cohesive elements in a
modern expository Arabic text. In Eid, Mushira and John
McCarthy, editors, Perspectives on Arabic Linguistics II.
John Benjamins.
Arun, A and F Keller. 2005. Lexicalization in crosslinguistic
probabilistic parsing: The case of French. In ACL.
Bikel, D M. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30:479?511.
Chappelier, J-C, M Rajman, R Arages, and A Rozenknop.
1999. Lattice parsing for speech recognition. In TALN.
Chiang, D, M Diab, N Habash, O Rambow, and S Shareef.
2006. Parsing Arabic dialects. In EACL.
Clegg, A and A Shepherd. 2005. Evaluating and integrating
treebank parsers on a biomedical corpus. In ACL Work-
shop on Software.
Cohen, S and N A Smith. 2007. Joint morphological and
syntactic disambiguation. In EMNLP.
Collins, M, J Hajic, L Ramshaw, and C Tillmann. 1999. A
statistical parser for Czech. In ACL.
Collins, M. 2003. Head-Driven statistical models for natural
language parsing. Computational Linguistics, 29(4):589?
637.
Cowan, B and M Collins. 2005. Morphology and reranking
for the statistical parsing of Spanish. In NAACL.
Diab, M. 2007. Towards an optimal POS tag set for Modern
Standard Arabic processing. In RANLP.
Dickinson, M. 2005. Error Detection and Correction in An-
notated Corpora. Ph.D. thesis, The Ohio State University.
Dubey, A and F Keller. 2003. Probabilistic parsing for Ger-
man using sister-head dependencies. In ACL.
Dyer, C. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In NAACL.
Fassi Fehri, A. 1993. Issues in the structure of Arabic clauses
and words. Kluwer Academic Publishers.
Gabbard, R and S Kulick. 2008. Construct state modification
in the Arabic treebank. In ACL.
Goldberg, Y and R Tsarfaty. 2008. A single generative model
for joint morphological segmentation and syntactic pars-
ing. In ACL.
Green, S, C Sathi, and C D Manning. 2009. NP subject
detection in verb-initial Arabic clauses. In Proc. of the
Third Workshop on Computational Approaches to Arabic
Script-based Languages (CAASL3).
Habash, N and O Rambow. 2005. Arabic tokenization, part-
of-speech tagging and morphological disambiguation in
one fell swoop. In ACL.
Habash, N and O Rambow. 2007. Arabic diacritization
through full morphological tagging. In NAACL.
Habash, N and R Roth. 2009. CATiB: The Columbia Arabic
Treebank. In ACL, Short Papers.
Habash, N and F Sadat. 2006. Arabic preprocessing schemes
for statistical machine translation. In NAACL.
Habash, N, B Dorr, and C Monz. 2006. Challenges in build-
ing an Arabic-English GHMT system with SMT compo-
nents. In EAMT.
Hajic?, J and P Zema?nek. 2004. Prague Arabic dependency
treebank: Development in data and tools. In NEMLAR.
Huang, Z and M Harper. 2009. Self-training PCFG
grammars with latent annotations across languages. In
EMNLP.
Klein, D and C D Manning. 2002. Fast exact inference with
a factored model for natural language parsing. In NIPS.
Klein, D and C D Manning. 2003. Accurate unlexicalized
parsing. In ACL.
Kulick, S, R Gabbard, and M Marcus. 2006. Parsing the
Arabic Treebank: Analysis and improvements. In TLT.
Ku?bler, S. 2005. How do treebank annotation schemes influ-
ence parsing results? Or how not to compare apples and
oranges. In RANLP.
Levy, R and C D Manning. 2003. Is it harder to parse Chi-
nese, or the Chinese treebank? In ACL.
Maamouri, M and A Bies. 2004. Developing an Arabic
Treebank: Methods, guidelines, procedures, and tools. In
Proc. of the Workshop on Computational Approaches to
Arabic Script-based Languages (CAASL1).
Maamouri, M, A Bies, T Buckwalter, and W Mekki. 2004.
The Penn Arabic Treebank: Building a large-scale anno-
tated Arabic corpus. In NEMLAR.
Maamouri, M, A Bies, and S Kulick. 2008. Enhancing the
Arabic Treebank: A collaborative effort toward new an-
notation guidelines. In LREC.
Maamouri, M, A Bies, S Krouna, F Gaddeche, and
B Bouziri. 2009a. Penn Arabic Treebank guidelines
v4.92. Technical report, Linguistic Data Consortium, Uni-
versity of Pennsylvania, August 5.
Maamouri, M, A Bies, and S Kulick. 2009b. Creating a
methodology for large-scale correction of treebank anno-
tation: The case of the Arabic Treebank. In MEDAR.
Marcus, M, M A Marcinkiewicz, and B Santorini. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313?330.
Petrov, S, L Barrett, R Thibaux, and D Klein. 2006. Learning
accurate, compact, and interpretable tree annotation. In
ACL.
Petrov, S. 2009. Coarse-to-Fine Natural Language Process-
ing. Ph.D. thesis, University of California-Berkeley.
Rehbein, I and J van Genabith. 2007. Treebank annotation
schemes and parser evaluation for German. In EMNLP-
CoNLL.
Roark, B, M Harper, E Charniak, B Dorr, M Johnson, J G
Kahne, Y Liuf, Mari Ostendorf, J Hale, A Krasnyanskaya,
M Lease, I Shafran, M Snover, R Stewart, and L Yung.
2006. SParseval: Evaluation metrics for parsing speech.
In LREC.
Ryding, K. 2005. A Reference Grammar of Modern Standard
Arabic. Cambridge University Press.
Sampson, G and A Babarczy. 2003. A test of the leaf-
ancestor metric for parse accuracy. Natural Language En-
gineering, 9:365?380.
Toutanova, K, D Klein, C D Manning, and Y Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In NAACL.
Tsarfaty, R and K Sima?an. 2008. Relational-realizational
parsing. In COLING.
Tsarfaty, R. 2006. Integrated morphological and syntactic
disambiguation for Modern Hebrew. In ACL.
Zitouni, I, J S Sorensen, and R Sarikaya. 2006. Maximum
entropy based restoration of Arabic diacritics. In ACL.
402
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 725?735,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Multiword Expression Identification with Tree Substitution Grammars:
A Parsing tour de force with French
Spence Green*, Marie-Catherine de Marneffe?, John Bauer*, and Christopher D. Manning*?
*Computer Science Department, Stanford University
?Linguistics Department, Stanford University
{spenceg,mcdm,horatio,manning}@stanford.edu
Abstract
Multiword expressions (MWE), a known nui-
sance for both linguistics and NLP, blur the
lines between syntax and semantics. Previous
work onMWE identification has relied primar-
ily on surface statistics, which perform poorly
for longer MWEs and cannot model discontin-
uous expressions. To address these problems,
we show that even the simplest parsing mod-
els can effectively identify MWEs of arbitrary
length, and that Tree Substitution Grammars
achieve the best results. Our experiments show
a 36.4% F1 absolute improvement for French
over an n-gram surface statistics baseline, cur-
rently the predominant method for MWE iden-
tification. Our models are useful for several
NLP tasks in which MWE pre-grouping has
improved accuracy.
1 Introduction
Multiword expressions (MWE) have long been a
challenge for linguistic theory and NLP. There is
no universally accepted definition of the term, but
MWEs can be characterized as ?idiosyncratic inter-
pretations that cross word boundaries (or spaces)?
(Sag et al, 2002) such as traffic light, or as ?fre-
quently occurring phrasal units which are subject
to a certain level of semantic opaqueness, or non-
compositionality? (Rayson et al, 2010).
MWEs are often opaque fixed expressions, al-
though the degree to which they are fixed can vary.
Some MWEs do not allow morphosyntactic varia-
tion or internal modification (e.g., in short, but *in
shorter or *in very short). Other MWEs are ?semi-
fixed,? meaning that they can be inflected or undergo
internal modification. The type of modification is of-
ten limited, but not predictable, so it is not possible
to enumerate all variants (Table 1).
French English
? terme in the near term
? court terme in the short term
? tr?s court terme in the very short term
? moyen terme in the mediumterm
? long terme in the long term
? tr?s long terme in the very long term
Table 1: Semi-fixed MWEs in French and English. The
French adverb ? terme ?in the end? can be modified by
a small set of adjectives, and in turn some of these ad-
jectives can be modified by an adverb such as tr?s ?very?.
Similar restrictions appear in English.
Merging known MWEs into single tokens has
been shown to improve accuracy for a variety of
NLP tasks: dependency parsing (Nivre and Nilsson,
2004), constituency parsing (Arun andKeller, 2005),
sentence generation (Hogan et al, 2007), and ma-
chine translation (Carpuat andDiab, 2010). Most ex-
periments use gold MWE pre-grouping or language-
specific resources like WordNet. For unlabeled text,
the best MWE identification methods, which are
based on surface statistics (Pecina, 2010), suffer
from sparsity induced by longer n-grams (Ramisch
et al, 2010). A dilemma thus exists: MWE knowl-
edge is useful, but MWEs are hard to identify.
In this paper, we show the effectiveness of statis-
tical parsers for MWE identification. Specifically,
Tree Substitution Grammars (TSG) can achieve a
36.4% F1 absolute improvement over a state-of-the-
art surface statistics method. We choose French,
which has pervasive MWEs, for our experiments.
Parsing models naturally accommodate discontinu-
ous MWEs like phrasal verbs, and provide syntac-
tic subcategorization. By contrast, surface statistics
methods are usually limited to binary judgements for
contiguous n-grams or dependency bigrams.
725
FTB (train) WSJ (train)
Sentences 13,449 39,832
Tokens 398,248 950,028
#Word Types 28,842 44,389
#Tag Types 30 45
#Phrasal Types 24 27
Per Sentence
Depth (?/?2) 4.03 / 0.360 4.18 / 0.730
Breadth (?/?2) 13.5 / 6.79 10.7 / 4.59
Length (?/?2) 29.6 / 17.3 23.9 / 11.2
Constituents (?) 20.3 19.6
? Constituents / ? Length 0.686 0.820
Table 2: Gross corpus statistics for the pre-processed FTB
(training set) andWSJ (sec. 2-21). The FTB sentences are
longer with broader syntactic trees. The FTB POS tag set
has 33% fewer types than theWSJ. The FTB dev set OOV
rate is 17.77% vs. 12.78% for the WSJ.
Type #Total #Single %Single %Total
MWN noun 9,680 2,737 28.3 49.7
MWADV adverb 3,852 449 11.7 19.8
MWP prep. 3,526 342 9.70 18.1
MWC conj. 814 73 8.97 4.18
MWV verb. 585 243 41.5 3.01
MWD det. 328 69 21.0 1.69
MWA adj. 324 126 38.9 1.66
MWPRO pron. 266 33 12.4 1.37
MWCL clitic 59 1 1.69 0.30
MWET foreign 24 18 0.75 0.12
MWI interj. 4 2 0.50 0.02
19,462 4,093 21.0% 100.0%
Table 3: Frequency distribution of the 11 MWE subcate-
gories in the FTB (training set). MWEs account for 7.08%
of the bracketings and 13.0% of the tokens in the treebank.
Only 21% of the MWEs occur once (?single?).
We first introduce a new instantiation of the
French Treebank that, unlike previous work, does not
use gold MWE pre-grouping. Consequently, our ex-
perimental results also provide a better baseline for
parsing raw French text.
2 French Treebank Setup
The corpus used in our experiments is the French
Treebank (Abeill? et al (2003), version from June
2010, hereafter FTB). In French, there is a linguis-
tic tradition of lexicography which compiles lists
of MWEs occurring in the language. For exam-
ple, Gross (1986) shows that dictionaries contain
about 1,500 single-word adverbs but that French con-
tains over 5,000 multiword adverbs. MWEs occur
in every part-of-speech (POS) category (e.g., noun
trousse de secours ?first-aid kit?; verb faire main-
basse [do hand-low] ?seize?; adverb comme dans du
beurre [as in butter] ?easily?; adjective ?? part en-
ti?re? ?wholly?).
The FTB explicitly annotates MWEs (also called
compounds in prior work). We used the subset of
the corpus with functional annotations, not for those
annotations but because this subset is known to be
more consistently annotated. POS tags for MWEs
are given not only at the MWE level, but also inter-
nally: most tokens that constitute an MWE also have
a POS tag. Table 2 compares this part of the FTB to
the WSJ portion of the Penn Treebank.
2.1 Preprocessing
The FTB requires significant pre-processing prior to
parsing.
Tokenization We changed the default tokenization
for numbers by fusing adjacent digit tokens. For ex-
ample, 500 000 is tagged as an MWE composed of
two words 500 and 000. We made this 500000 and
retained the MWE POS, although we did not mark
the new token as an MWE. For consistency, we used
one token for punctuated numbers like ?17,9?.
MWE Tagging We marked MWEs with a flat
bracketing in which the phrasal label is the MWE-
level POS tag with an ?MW? prefix, and the preter-
minals are the internal POS tags for each terminal.
The resulting POS sequences are not always unique
to MWEs: they appear in abundance elsewhere in
the corpus. However, some MWEs contain normally
ungrammatical POS sequences (e.g., adverb ? la va
vite ?in a hurry?: PDVADV [at the goes quick]), and
some words appear only as part of an MWE, such as
insu in ? l?insu de ?to the ignorance of?.
Labels We augmented the basic FTB label set?
which contains 14 POS tags and 19 phrasal tags?in
two ways. First, we added 16 finer-grained POS tags
for punctuation.1 Second, we added the 11 MWE
1Punctuation tag clusters?as used in the WSJ?did not im-
prove accuracy. Enriched tag sets like that of Crabb? and Can-
dito (2008) could also be investigated and compared to our re-
sults since Evalb is insensitive to POS tags.
726
labels shown in Table 3, resulting in 24 total phrasal
categories.
Corrections Historically, the FTB suffered from
annotation errors such as missing POS and phrasal
tags (Arun and Keller, 2005). We found that this
problem has been largely resolved in the current re-
lease. However, 1,949 tokens and 36 MWE spans
still lacked tags. We restored the labels by first as-
signing each token its most frequent POS tag else-
where in the treebank, and then assigning the most
frequent MWE phrasal category for the resulting
POS sequence.2
Split We used the 80/10/10 split described by
Crabb? and Candito (2008). However, they used a
previous release of the treebank with 12,531 trees.
3,391 trees have been added to the present version.
We appended these extra trees to the training set, thus
retaining the same development and test sets.
2.2 Comparison to Prior FTB Representations
Our pre-processing approach is simple and auto-
matic3 unlike the three major instantiations of the
FTB that have been used in previous work:
Arun-Cont and Arun-Exp (Arun and Keller,
2005): Two instantiations of the full 20,000 sentence
treebank that differed principally in their treatment of
MWEs: (1) Cont, in which the tokens of eachMWE
were concatenated into a single token (en moyenne
? en_moyenne); (2)Exp, in which theyweremarked
with a flat structure. For both representations, they
also gave results in which coordinated phrase struc-
tures were flattened. In the published experiments,
they mistakenly removed half of the corpus, believ-
ing that the multi-terminal (per POS tag) annotations
of MWEs were XML errors (Schluter and Genabith,
2007).
MFT (Schluter andGenabith, 2007): Manual revi-
sion to 3,800 sentences. Major changes included co-
ordination raising, an expanded POS tag set, and the
273 of the unlabeled word types did not appear elsewhere
in the treebank. All but 11 of these were nouns. We manually
assigned the correct tags, but we would not expect a negative
effect by deterministically labeling all of them as nouns.
3We automate tree manipulation with Tregex/Tsurgeon
(Levy and Andrew, 2006). Our pre-processing package is avail-
able at http://nlp.stanford.edu/software/lex-parser.shtml.
correction of annotation errors. Like Arun-Cont,
MFT contains concatenated MWEs.
FTB-UC (Candito and Crabb?, 2009): An in-
stantiation of the functionally annotated section that
makes a distinction between MWEs that are ?syn-
tactically regular? and those that are not. Syntacti-
cally regular MWEs were given internal structure,
while all other MWEs were concatenated into sin-
gle tokens. For example, nouns followed by ad-
jectives, such as loi agraire ?land law? or Union
mon?taire et ?conomique ?monetary and economic
Union? were considered syntactically regular. They
are MWEs because the choice of adjective is arbi-
trary (loi agraire and not *loi agricole, similarly to
?coal black? but not *?crow black? for example), but
their syntactic structure is not intrinsic to MWEs.
In such cases, FTB-UC gives the MWE a conven-
tional analysis of an NP with internal structure. Such
analysis is indeed sufficient to recover the mean-
ing of these semantically compositional MWEs that
are extremely productive. On the other hand, the
FTB-UC loses information about MWEs with non-
compositional semantics.
Almost all work on the FTB has followed Arun-
Cont and used goldMWEpre-grouping. As a result,
most results for French parsing are analogous to early
results for Chinese, which used gold word segmen-
tation, and Arabic, which used gold clitic segmenta-
tion. Candito et al (2010) were the first to acknowl-
edge and address this issue, but they still used FTB-
UC (with some pre-grouped MWEs). Since the syn-
tax and definition of MWEs is a contentious issue,
we take a more agnostic view?which is consistent
with that of the FTB annotators?and leave them to-
kenized. This permits a data-oriented approach to
MWE identification that is more robust to changes
to the status of specific MWE instances.
To set a baseline prior to grammar development,
we trained the Stanford parser (Klein and Manning,
2003) with no grammar features, achieving 74.2%
labeled F1 on the development set (sentences ? 40
words). This is lower than the most recent results ob-
tained by Seddah (2010). However, the results are
not comparable: the data split was different, they
made use of morphological information, and more
importantly they concatenated MWEs. The focus of
727
our work is on models and data representations that
enable MWE identification.
3 MWEs in Lexicon-Grammar
The MWE representation in the FTB is close to
the one proposed in the Lexicon-Grammar (Gross,
1986). In the Lexicon-Grammar, MWEs are classi-
fied according to their global POS tags (noun, verb,
adverb, adjective), and described in terms of the se-
quence of the POS tags of the words that constitute
the MWE (e.g., ?N de N? garde d?enfant [guard of
child] ?daycare?, pied de guerre [foot of war] ?at the
ready?). In other words, MWEs are represented by a
flat structure. The Lexicon-Grammar distinguishes
between units that are fixed and have to appear as is
(en tout et pour tout [in all and for all] ?in total?) and
units that accept some syntactic variation such as ad-
mitting the insertion of an adverb or adjective, or the
variation of one of the words in the expression (e.g.,
a possessive as in ?from the top of one?s hat?). It also
notes whether the MWE displays some selectional
preferences (e.g., it has to be preceded by a verb or
by an adjective).
Our FTB instantiation is largely consistent with
the Lexicon-Grammar. Recall that we defined differ-
ent MWE categories based on the global POS. We
now detail three of the categories.
MWN The MWN category consists of proper
nouns (1a), foreign common nouns (1b), as well as
common nouns. The common nouns appear in sev-
eral syntactically regular sequences of POS tags (2).
Multiword nouns allow inflection (singular vs. plu-
ral) but no insertion.
(1) a. London Sunday Times, Los Angeles
b. week - end, mea culpa, joint - venture
(2) a. NA: corpsm?dical ?medical staff?, dette
publique ?public debt?
b. N PN:mode d?emploi ?instruction man-
ual?
c. N N: num?ro deux ?number two?, mai-
sonm?re [housemother] ?headquarters?,
gr?ve surprise ?sudden strike?
d. N P D N: imp?t sur le revenu ?income
tax?, ministre de l??conomie ?finance
minister?
MWA Multiword adjectives appear with different
POS sequences (3). They include numbers such as
vingt et uni?me ?21st?. Some items in (3b) allow in-
ternal variation: some adverbs or adjectives can be
added to both examples given (? tr?s haut risque, de
toute derni?re minute).
(3) a. P N: d?antan [from before] ?old?, en
question ?under discussion?
b. P A N: ? haut risque ?high-risk?, de
derni?re minute [from the last minute]
?at the eleventh hour?
c. A C A: pur et simple [pure and simple]
?straightforward?, noir et blanc ?black
and white?
MWV Multiword verbs also appear in several POS
sequences (4). All verbs allow number and tense in-
flections. Some MWVs containing a noun or an ad-
jective allow the insertion of a modifier (e.g., don-
ner grande satisfication ?give great satisfaction?),
whereas others do not. When an adverb intervenes
between the main verb and its complement, the FTB
marks the two parts of the MWV discontinuously
(e.g., [MWV [V prennent]] [ADV d?j?] [MWV [P en] [N
cause]] ?already take into account?).
(4) a. V N: avoir lieu ?take place?, donner sat-
isfaction ?give satisfaction?
b. V P N: mettre en place ?put in place?,
entrer en vigueur ?to come into effect?
c. V P ADV: mettre ? mal [put at bad]
?harm?, ?tre ? m?me [be at same] ?be
able?
d. V D N P N: tirer la sonnette d?alarme
?ring the alarm bell?, avoir le vent en
poupe ?to have the wind astern?
4 Parsing Models
We develop two parsers for French with the goal
of improving MWE identification. The first is a
manually-annotated grammar that we incorporate
into the Stanford parser. Manual annotation results in
human interpretable grammars that can inform future
treebank annotation decisions. Moreover, the gram-
mar can be used as the base distribution in our sec-
ond model, a Probabilistic Tree Substitution Gram-
mar (PTSG) parser. PTSGs learn parameters for tree
728
Feature States Tags F1 ?F1
? 4325 31 74.21
tagPA 4509 215 76.94 +2.73
markInf 4510 216 77.42 +0.48
markPart 4511 217 77.73 +0.31
markVN 5986 217 78.32 +0.59
markCoord 7361 217 78.45 +0.13
markDe 7521 233 79.11 +0.66
markP 7523 235 79.34 +0.23
markMWE 7867 235 79.23 ?0.11
Table 4: Effects on grammar size and labeled F1 for each
of the manual state splits (development set, sentences ?
40 words). markMWE decreases overall accuracy, but
increases both the number of correctly parsed trees (by
0.30%) and per category MWE accuracy.
fragments larger than basic CFG rules. PTSG rules
may also be lexicalized. This means that commonly
observed collocations?some of which areMWEs?
can be stored in the grammar.
4.1 Stanford Parser
We configure the Stanford parser with settings that
are effective for other languages: selective parent an-
notation, lexicon smoothing, and factored parsing.
We use the head-finding rules of Dybro-Johansen
(2004), which we find to yield an approximately
1.0% F1 development set improvement over those of
Arun (2004). Finally, we include a simple unknown
word model consisting entirely of surface features:
- Nominal, adjectival, verbal, adverbial, and plu-
ral suffixes
- Contains a digit or punctuation
- Is capitalized (except the first word in a sen-
tence)
- Consists entirely of capital letters
- If none of the above, add a one- or two-character
suffix
Combined with the grammar features, this unknown
word model yields 97.3% tagging accuracy on the
development set.
4.1.1 Grammar Development
Table 4 lists the symbol refinements used in our
grammar. Most of the features are POS splits as
many phrasal tag splits did not lead to any improve-
ment. Parent annotation of POS tags (tagPA) cap-
tures information about the external context. mark-
Inf and markPart accomplish a finite/nonfinite dis-
tinction: they respectively specify whether the verb
is an infinitive or a participle based on the type of
the grandparent node. markVN captures the notion
of verbal distance as in Klein and Manning (2003).
We opted to keep the COORD phrasal tag, and
to capture parallelism in coordination, we mark CO-
ORD with the type of its child (NP, AP, VPinf, etc.).
markDe identifies the preposition de and its variants
(du, des, d?) which is very frequent and appears in
several different contexts. markP identifies preposi-
tions which introduce PPs modifying a noun. Mark-
ing other kinds of prepositional modifiers (e.g., verb)
did not help. markMWE adds an annotation to sev-
eral MWE categories for frequently occuring POS
sequences. For example, we mark MWNs that occur
more than 600 times (e.g., ?N P N? and ?N N?).
4.2 DP-TSG Parser
A shortcoming of CFG-based grammars is that they
do not explicitly capture idiomatic usage. For exam-
ple, consider the two utterances:
(5) a. He [MWV kicked the bucket] .
b. He [VP kicked [NP the pail]] .
The examples in (5) may be equally probable and re-
ceive the same analysis under a PCFG; words are
generated independently. However, recall that in
our representation, (5a) should receive a flat analysis
as MWV, whereas (5b) should have a conventional
analysis of the verb kicked and its two arguments.
An alternate view of parsing is one in which new
utterances are built from previously observed frag-
ments. This is the original motivation for data ori-
ented parsing (DOP) (Bod, 1992), in which ?id-
iomaticity is the rule rather than the exception?
(Scha, 1990). If we have seen the collocation kicked
the bucket several times before, we should store that
whole fragment for later use.
We consider a variant of the non-parametric PTSG
model of Cohn et al (2009) in which tree fragments
are drawn from a Dirichlet process (DP) prior.4
The DP-TSG can be viewed as a DOP model with
Bayesian parameter estimation. A PTSG is a 5-tuple
?V,?, R,?,?? where c ? V are non-terminals;
4Similar models were developed independently by
O?Donnell et al (2009) and Post and Gildea (2009).
729
?c DP concentration parameter for each c ? V
P0(e|c) CFG base distribution
x Set of non-terminal nodes in the treebank
S Set of sampling sites (one for each x ? x)
S A block of sampling sites, where S ? S
b = {bs}s?S Binary variables to be sampled (bs = 1 ?
frontier node)
z Latent state of the segmented treebank
m Number of sites s ? S s.t. bS = 1
n = {nc,e} Sufficient statistics of z
?nS:m Change in counts by setting m sites in S
Table 5: DP-TSG model notation. For consistency, we
largely follow the notation of Liang et al (2010). Note
that z = (b,x), and as such z = ?c, e?.
t ? ? are terminals; e ? R are elementary trees;5
? ? V is a unique start symbol; and ?c,e ? ? are
parameters for each tree fragment. A PTSG deriva-
tion is created by successively applying the substitu-
tion operator to the leftmost frontier node (denoted
by c+). All other nodes are internal (denoted by c?).
In the supervised setting, DP-TSG grammar ex-
traction reduces to a segmentation problem. We have
a treebank T that we segment into the set R, a pro-
cess that we model with Bayes? rule:
p(R | T ) ? p(T | R) p(R) (1)
Since the tree fragments completely specify each
tree, p(T | R) is either 0 or 1, so all work is per-
formed by the prior over the set of elementary trees.
The DP-TSG contains a DP prior for each c ? V
(Table 5 defines further notation). We generate ?c, e?
tuples as follows:
?c|c, ?c, P0(?|c) ? DP (?c, P0)
e|?c ? ?c
The data likelihood is given by the latent state z and
the parameters ?: p(z|?) =?z?z ?
nc,e(z)
c,e . Integrat-
ing out the parameters, we have:
p(z) =
?
c?V
?
e(?cP0(e|c))nc,e(z)
?nc,?(z)c
(2)
where xn = x(x + 1) . . . (x + n ? 1) is the rising
factorial. (?A.1 contains ancillary details.)
Base Distribution The base distribution P0 is the
same maximum likelihood PCFG used in the Stan-
5We use the terms tree fragment and elementary tree inter-
changeably.
NP+
PUNC-(1)
?
N+
Jacques
N-
Chirac
PUNC+(2)
?
Figure 1: Example of two conflicting sites of the same
type. Define the type of a site t(z, s) def= (?ns:0,?ns:1).
Sites (1) and (2) above have the same type since t(z, s1) =
t(z, s2). However, the two sites conflict since the prob-
abilities of setting bs1 and bs2 both depend on counts for
the tree fragment rooted at NP. Consequently, sites (1) and
(2) are not exchangeable: the probabilities of their assign-
ments depend on the order in which they are sampled.
ford parser.6,7 After applying the manual state splits,
we perform simple right binarization, collapse unary
rules, and replace rare words with their signatures
(Petrov et al, 2006).
For each non-terminal type c, we learn a stop prob-
ability sc ? Beta(1, 1). Under P0, the probability of
generating a rule A+ ? B? C+ composed of non-
terminals is
P0(A+ ? B? C+) = pMLE(A ? B C)sB(1?sC)
(3)
For lexical insertion rules, we add a penalty propor-
tional to the frequency of the lexical item:
P0(c ? t) = pMLE(c ? t)p(t) (4)
where p(t) is equal to the MLE unigram probabil-
ity of t in the treebank. Lexicalizing a rule makes it
very specific, so we generally want to avoid lexical-
ization with rare words. Empirically, we found that
this penalty reduces overfitting.
Type-based Inference Algorithm To learn the pa-
rameters ? we use the collapsed, block Gibbs sam-
pler of Liang et al (2010). We sample binary vari-
ables bs associated with each non-terminal node/site
in the treebank. The key idea is to select a block
of exchangeable sites S of the same type that do not
conflict (Figure 1). Since the sites in S are exchange-
able, we can set bS randomly so long as we know m,
the number of sites with bs = 1. Because this algo-
rithm is a not a contribution of this paper, we refer
the reader to Liang et al (2010).
6The Stanford parser is a product model, so the results in ?5.1
include the contribution of a dependency parser.
7Bansal and Klein (2010) also experimented with symbol re-
finement in an all-fragments (parametric) TSG for English.
730
After each Gibbs iteration, we sample each sc di-
rectly using binomial-Beta conjugacy. We re-sample
the DP concentration parameters ?c with the auxil-
iary variable procedure of West (1995).
Decoding We compute the rule score of each tree
fragment from a single grammar sample as follows:
?c,e =
nc,e(z) + ?cP0(e|c)
nc,?(z) + ?c
(5)
To make the grammar more robust, we also include
all CFG rules in P0 with zero counts inn. Scores for
these rules follow from (5) with nc,e(z) = 0.
For decoding, we note that the derivations of a
TSG are a CFGparse forest (Vijay-Shanker andWeir,
1993). As such, we can use a Synchronous Context
Free Grammar (SCFG) to translate the 1-best parse
to its derivation. Consider a unique tree fragment ei
rooted at X with frontier ?, which is a sequence of
terminals and non-terminals. We encode this frag-
ment as an SCFG rule of the form
[X ? ? , X ? i, Y1, . . . , Yn] (6)
where Y1, . . . , Yn is the sequence of non-terminal
nodes in ?.8 During decoding, the input is re-
written as a sequence of tree fragment (rule) indices
{i, j, k, . . . }. Because the TSG substitution operator
always applies to the leftmost frontier node, we can
deterministically recover the monolingual parse with
top-down re-writes of ?.
The SCFG formulation has a practical benefit: we
can take advantage of the heavily-optimized SCFG
decoders for machine translation. We use cdec
(Dyer et al, 2010) to recover the Viterbi derivation
under a DP-TSG grammar sample.
5 Experiments
5.1 Standard Parsing Experiments
We evaluate parsing accuracy of the Stanford and
DP-TSG models (Table 6). For comparison, we also
include the Berkeley parser (Petrov et al, 2006).9
For the DP-TSG, we initialized all bs with fair coin
tosses and ran for 400 iterations, after which likeli-
hood stopped improving.
8This formulation is due to Chris Dyer.
9Training settings: right binarization, no parent annotation,
six split-merge cycles, and random initialization.
Leaf Ancestor Evalb
Corpus Sent LP LR F1 EX%
PA-PCFG 0.793 0.812 68.1 67.0 67.6 10.5
DP-TSG 0.823 0.842 75.6 76.0 75.8 15.1
Stanford 0.843 0.861 77.8 79.0 78.4 17.5
Berkeley 0.880 0.891 82.4 82.0 82.2 21.4
Table 6: Standard parsing experiments (test set, sentences
? 40 words). All parsers exceed 96% tagging accuracy.
Berkeley and DP-TSG results are the average of three in-
dependent runs.
We report two different parsing metrics. Evalb
is the standard labeled precision/recall metric.10
Leaf Ancestor measures the cost of transforming
guess trees to the reference (Sampson and Babar-
czy, 2003). It was developed in response to the non-
terminal/terminal ratio bias of Evalb, which penal-
izes flat treebanks like the FTB. The range of the
score is between 0 and 1 (higher is better). We report
micro-averaged (whole corpus) and macro-averaged
(per sentence) scores.
In terms of parsing accuracy, the Berkeley parser
exceeds both Stanford and DP-TSG. This is consis-
tent with previous experiments for French by Sed-
dah et al (2009), who show that the Berkeley parser
outperforms other models. It also matches the or-
dering for English (Cohn et al, 2010; Liang et al,
2010). However, the standard baseline for TSGmod-
els is a simple parent-annotated PCFG (PA-PCFG).
For English, Liang et al (2010) showed that a similar
DP-TSG improved over PA-PCFG by 4.2% F1. For
French, our gain is a more substantial 8.2% F1.
5.2 MWE Identification Experiments
Table 7 lists overall and per-category MWE identifi-
cation results for the parsing models. Although DP-
TSG is less accurate as a general parsing model, it is
more effective at identifying MWEs.
The predominant approach to MWE identification
is the combination of lexical association measures
(surface statistics) with a binary classifier (Pecina,
2010). A state-of-the-art, language independent
package that implements this approach for higher
order n-grams is mwetoolkit (Ramisch et al,
2010).11 In Table 8 we compare DP-TSG to both
10Available at http://nlp.cs.nyu.edu/evalb/ (v.20080701).
11Available at http://multiword.sourceforge.net/. See ?A.2 for
731
#gold Stanford DP-TSG Berkeley
MWET 3 0.0 0.0 0.0
MWV 26 64.0 57.7 50.7
MWA 8 26.1 32.2 29.8
MWN 456 64.1 67.6 67.1
MWD 15 70.3 65.5 70.1
MWPRO 17 73.7 78.0 76.2
MWADV 220 74.6 72.7 70.4
MWP 162 81.3 80.5 77.7
MWC 47 83.5 83.5 80.8
954 70.1 71.1 69.6
Table 7: MWE identification per category and overall re-
sults (test set, sentences ? 40 words). MWI and MWCL
do not occur in the test set.
Model F1
mwetoolkit All 15.4
PA-PCFG 32.6
mwetoolkit Filter 34.7
PA-PCFG+Features 63.1
DP-TSG 71.1
Table 8: MWE identification F1 of the best parsing model
vs. the mwetoolkit baseline (test set, sentences ? 40
words). PA-PCFG+Features includes the grammar fea-
tures in Table 4, which is the CFG from which the TSG is
extracted. For mwetoolkit, All indicates the inclusion
of all n-grams in the training corpus. Filter indicates pre-
filtering of the training corpus by removing rare n-grams
(see ?A.2 for details).
mwetoolkit and the CFG from the which the TSG
is extracted. The TSG-based parsing model outper-
forms mwetoolkit by 36.4% F1 while providing
syntactic subcategory information.
6 Discussion
Automatic learning methods run the risk of produc-
ing uninterpretable models. However, the DP-TSG
model learns useful generalizations over MWEs. A
sample of the rules is given in Table 9. Some spe-
cific sequences like ?[MWN [coup de N]]? are part of
the grammar: such rules can indeed generate quite
a few MWEs, e.g., coup de pied ?kick?, coup de
coeur, coup de foudre ?love at first sight?, coup de
main ?help?, coup d??tat, coup de gr?ce (note that
only some of these MWEs are seen in the training
configuration details.
MWN MWV MWP
soci?t?s de N sous - V de l?ordre de
prix de N faire N y compris
coup de N V les moyens au N de
N d??tat V de N en N de
N de N V en N ADV de
N ? N
Table 9: Sample of the TSG rules learned.
MWN
N
tour
P
de
N
passe
-
-
N
passe
(a) Reference
NP
N
tour
PP
P
de
NP
MWN
N
passe
-
-
N
passe
(b) DP-TSG
Figure 2: Example of an MWE error for tour de passe-
passe ?magic trick?. (dev set)
data). For MWV, ?V de N? as in avoir de cesse ?give
no peace?, perdre de vue [lose from sight] ?forget?,
prendre de vitesse [take from speed] ?outpace?), is
learned. For prepositions, the grammar stores full
subtrees of MWPs, but can also generalize the struc-
ture of very frequent sequences: ?en N de? occurs in
manymultiword prepositions (e.g., en compagnie de,
en face de, en mati?re de, en terme de, en cours de,
en faveur de, en raison de, en fonction de). The TSG
grammar thus provides a categorization of MWEs
consistent with the Lexicon-Grammar. It also learns
verbal phrases which contain discontinuous MWVs
due to the insertion of an adverb or negation such as
?[VN [MWV va] [MWADV d?ailleurs] [MWV bon train]]?
[go indeed well], ?[VN [MWV a] [ADV jamais] [MWV
?t? question d?]]? [has never been in question].
A significant fraction of errors for MWNs occur
with adjectives that are not recognized as part of the
MWE. For example, since ?tablissements priv?s ?pri-
vate corporation? is unseen in the training data, it is
not found. Sometimes the parser did not recognize
the whole structure of an MWE. Figure 2 shows an
example where the parser only found a subpart of the
MWN tour de passe-passe ?magic trick?.
Other DP-TSG errors are due to inconsistencies in
the FTB annotation. For example, sous pr?texte que
732
MWC
P
sous
N
pr?texte
C
que
(a) Reference
PP
P
sous
NP
N
pr?texte
Ssub
C
que
(b) Reference
Figure 3: Example of an inconsistent FTB annotation for
sous pr?texte que ?on the pretext of?.
?on the pretext of? is tagged as both MWC and as a
regular PP structure (Figure 3). However, the parser
always assigns a MWC structure, which is a better
analysis than the gold annotation. We expect that
more consistent annotation would help the DP-TSG
more than the CFG-based parsers.
The DP-TSG is not immune to false positives: in
Le march? national, fait-on remarquer, est enfin en
r?gression . . . ?The national economy, people at last
note, is going down? the parser tags march? national
asMWN. As noted, the boundary of what should and
should not count as an MWE can be fuzzy, and it is
therefore hard to assess whether or not this should be
an MWE. The FTB does not mark it as one.
There are multiple examples were the DP-TSG
found the MWE whereas Stanford (its base distribu-
tion) did not, such as in Figure 4. Note that the ?N
P N? structure is quite frequent for MWNs, but the
TSG correctly identifies the MWADV in emplois ?
domicile [jobs at home] ?homeworking?.
7 Related Work
There is a voluminous literature on MWE identi-
fication. Here we review closely related syntax-
based methods.12 The linguistic and computa-
tional attractiveness of lexicalized grammars for
modeling idiosyncratic constructions in French was
identified by Abeill? (1988) and Abeill? and Sch-
abes (1989). They manually developed a small
Tree Adjoining Grammar (TAG) of 1,200 elemen-
tary trees and 4,000 lexical items that included
MWEs. The classic statistical approach to MWE
identification, Xtract (Smadja, 1993), used an in-
12See Seretan (2011) for a comprehensive survey of syntax-
based methods for MWE identification. For an overview of n-
gram methods like mwetoolkit, see Pecina (2010).
MWN
N
campagne
P
de
N
promotion
(a) DP-TSG
NP
N
campagne
PP
P
de
NP
N
promotion
(b) Stanford
NP
N
emplois
MWADV
P
?
N
domicile
(c) DP-TSG
NP
N
emplois
PP
P
?
NP
N
domicile
(d) Stanford
Figure 4: Correct analyses by DP-TSG. (dev set)
cremental parser in the third stage of its pipeline
to identify predicate-argument relationships. Lin
(1999) applied information-theoretic measures to
automatically-extracted dependency relations to find
MWEs. To our knowledge, Wehrli (2000) was the
first to use syntactically annotated corpora to im-
prove a parser for MWE identification. He pro-
posed to rank analyses of a symbolic parser based
on the presence of collocations, although details of
the ranking function were not provided.
The most similar work to ours is that of Nivre
and Nilsson (2004), who converted a Swedish cor-
pus into two versions: one in which MWEs were
left as tokens, and one in which they were merged.
On the first version, they showed that a deterministic
dependency parser could identify MWEs at 71.1%
F1, albeit without subcategory information. On
the second version?which simulated perfect MWE
identification?they showed that labeled attachment
improved by about 1%.
Recent statistical parsing work on French has in-
cluded Stochastic Tree Insertion Grammars (STIGs),
which are related to TAGs, but with a restricted ad-
junction operation.13 Seddah et al (2009) and Sed-
dah (2010) showed that STIGs underperform CFG-
based parsers on the FTB. In their experiments,
MWEs were concatenated.
13TSGs differ from TAGs and STIGs in that they do not in-
clude an adjunction operator.
733
8 Conclusion
The main result of this paper is that an existing sta-
tistical parser can achieve a 36.4% F1 absolute im-
provement for MWE identification over a state-of-
the-art n-gram surface statistics package. Parsers
also provide syntactic subcategorization, and do not
require pre-filtering of the training data. We have
also demonstrated that TSGs can capture idiomatic
usage better than a PCFG.While the DP-TSG, which
is a relatively new parsing model, still lags state-of-
the-art parsers in terms of overall labeling accuracy,
we have shown that it is already very effective for
other tasks like MWE identification. We plan to im-
prove the DP-TSG by experimenting with alternate
parsing objectives (Cohn et al, 2010), lexical rep-
resentations, and parameterizations of the base dis-
tribution. A particularly promising base distribution
is the latent variable PCFG learned by the Berkeley
parser. However, initial experiments with this distri-
bution were negative, so we leave further develop-
ment to future work.
We chose French for these experiments due to the
pervasiveness ofMWEs and the availability of an an-
notated corpus. However, MWE lists and syntactic
treebanks exist for many of the world?s major lan-
guages. We will investigate automatic conversion of
these treebanks (by flattening MWE bracketings) for
MWE identification.
A Appendix
A.1 Notes on the Rising Factorial
The rising factorial?also known as the ascending
factorial or Pochhammer symbol?arises in the con-
text of samples from a Dirichlet process (see Prop.
3 of Antoniak (1974) for details). For a positive in-
teger n and a complex number x, the rising factorial
xn is defined14 by
xn = x(x + 1) . . . (x + n? 1)
=
n?
j=1
(x + j ? 1) (7)
The rising factorial can be generalized to a com-
plex number ? with the gamma function:
x? = ?(x + ?)?(x) (8)
14We adopt the notation of Knuth (1992).
where x0 ? 1.
In our type-based sampler, we computed (7) di-
rectly in a dynamic program. We found that (8) was
prohibitively slow for sampling.
A.2 mwetoolkit Configuration
We configured mwetoolkit15 with the four stan-
dard lexical features: the maximum likelihood esti-
mator, Dice?s coefficient, pointwise mutual informa-
tion (PMI), and Student?s t-score. We added the POS
sequence for each n-gram as a single feature. We re-
moved the web counts features to make the experi-
ments comparable. To compensate for the absence
of web counts, we computed the lexical features us-
ing the gold lemmas from the FTB instead of using
an automatic lemmatizer.
Since MWE n-grams only account for a small
fraction of the n-grams in the corpus, we filtered the
training and test sets by removing all n-grams that
occurred once. To further balance the proportion of
MWEs, we trained on all valid MWEs plus 10x ran-
domly selected non-MWE n-grams. This proportion
matches the fraction of MWE/non-MWE tokens in
the FTB. Since we generated a random training set,
we reported the average of three independent runs.
We created feature vectors for the training n-
grams and trained a binary Support Vector Machine
(SVM) classifier with Weka (Hall et al, 2009). Al-
though mwetoolkit defaults to a linear kernel,
we achieved higher accuracy on the development set
with an RBF kernel.
The FTB is sufficiently large for the corpus-based
methods implemented in mwetoolkit. Ramisch
et al (2010)?s experiments were on Genia, which
contains 18k sentences and 490k tokens, similar to
the FTB. Their test set had 895 sentences, smaller
than ours. They reported 30.6% F1 for their task
against an Xtract baseline, which only obtained 7.3%
F1. These results are comparable inmagnitude to our
FTB results.
Acknowledgments We thank Marie Candito, Chris Dyer,
Dan Flickinger, Percy Liang, Carlos Ramisch, Djam?
Seddah, and Val Spitkovsky for their helpful comments.
The first author is supported by a National Defense Sci-
ence and Engineering Graduate (NDSEG) fellowship.
15We re-implemented mwetoolkit in Java for compatibil-
ity with Weka and our pre-processing routines.
734
References
A. Abeill? and Y. Schabes. 1989. Parsing idioms in lexicalized
TAGs. In EACL.
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building a tree-
bank for French, chapter 10. Kluwer.
A. Abeill?. 1988. Parsing Frenchwith TreeAdjoiningGrammar:
some linguistic accounts. In COLING.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152?1174.
A. Arun and F. Keller. 2005. Lexicalization in crosslinguistic
probabilistic parsing: The case of French. In ACL.
A. Arun. 2004. Statistical parsing of the French treebank. Tech-
nical report, University of Edinburgh.
M. Bansal and D. Klein. 2010. Simple, accurate parsing with
an all-fragments grammar. In ACL.
R. Bod. 1992. A computation model of language performance:
Data-Oriented Parsing. In COLING.
M. Candito and B. Crabb?. 2009. Improving generative statisti-
cal parsing with semi-supervised word clustering. In IWPT.
M. Candito, B. Crabb?, and P. Denis. 2010. Statistical French
dependency parsing: treebank conversion and first results. In
LREC.
M. Carpuat and M. Diab. 2010. Task-based evaluation of mul-
tiword expressions: a pilot study in statistical machine trans-
lation. In HLT-NAACL.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing compact
but accurate tree-substitution grammars. In HLT-NAACL.
T. Cohn, P. Blunsom, and S. Goldwater. 2010. Inducing tree-
substitution grammars. JMLR, 11:3053?3096, Nov.
B. Crabb? and M. Candito. 2008. Exp?riences d?analyse syn-
taxique statistique du fran?ais. In TALN.
A. Dybro-Johansen. 2004. Extraction automatique de gram-
maires ? partir d?un corpus fran?ais. Master?s thesis, Univer-
sit? Paris 7.
C. Dyer, A. Lopez, J. Ganitkevitch, J.Weese, F. Ture, et al 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL Sys-
tem Demonstrations.
M. Gross. 1986. Lexicon-Grammar: the representation of com-
pound words. In COLING.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten. 2009. The WEKA data mining software: an
update. SIGKDD Explorations Newsletter, 11:10?18.
D. Hogan, C. Cafferkey, A. Cahill, and J. van Genabith. 2007.
Exploiting multi-word units in history-based probabilistic
generation. In EMNLP-CoNLL.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. E. Knuth. 1992. Two notes on notation. American Mathe-
matical Monthly, 99:403?422, May.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon: tools for
querying and manipulating tree data structures. In LREC.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based MCMC.
In HLT-NAACL.
D. Lin. 1999. Automatic identification of non-compositional
phrases. In ACL.
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic pars-
ing. In Methodologies and Evaluation of Multiword Units in
Real-World Applications (MEMURA).
T. J. O?Donnell, J. B. Tenenbaum, and N. D. Goodman. 2009.
Fragment grammars: Exploring computation and reuse in
language. Technical report, MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series, MIT-
CSAIL-TR-2009-013.
P. Pecina. 2010. Lexical association measures and collocation
extraction. Language Resources and Evaluation, 44:137?
158.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning
accurate, compact, and interpretable tree annotation. In ACL.
M. Post and D. Gildea. 2009. Bayesian learning of a tree sub-
stitution grammar. In ACL-IJCNLP, Short Papers.
C. Ramisch, A. Villavicencio, and C. Boitet. 2010. mwe-
toolkit: a framework for multiword expression identifi-
cation. In LREC.
P. Rayson, S. Piao, S. Sharoff, S. Evert, and B. Moir?n. 2010.
Multiword expressions: hard going or plain sailing? Lan-
guage Resources and Evaluation, 44:1?5.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In CICLing.
G. Sampson and A. Babarczy. 2003. A test of the leaf-ancestor
metric for parse accuracy. Natural Language Engineering,
9:365?380.
R. Scha, 1990. Taaltheorie en taaltechnologie: competence en
performance, pages 7?22. Landelijke Vereniging van Neer-
landici (LVVNjaarboek).
N. Schluter and J. Genabith. 2007. Preparing, restructuring,
and augmenting a French treebank: Lexicalised parsers or
coherent treebanks? In Pacling.
D. Seddah, M. Candito, and B. Crabb?. 2009. Cross parser
evaluation and tagset variation: a French treebank study. In
IWPT.
D. Seddah. 2010. Exploring the Spinal-STIG model for parsing
French. In LREC.
V. Seretan. 2011. Syntax-Based Collocation Extraction, vol-
ume 44 of Text, Speech, and Language Technology. Springer.
F. Smadja. 1993. Retrieving collocations from text: Xtract.
Computational Linguistics, 19:143?177.
K. Vijay-Shanker and D. J. Weir. 1993. The use of shared forests
in tree adjoining grammar parsing. In EACL.
E. Wehrli. 2000. Parsing and collocations. In Natural Lan-
guage Processing?NLP 2000, volume 1835 of Lecture Notes
in Computer Science, pages 272?282. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
735
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1225?1236,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Human Effort and Machine Learnability in Computer Aided Translation
Spence Green, Sida Wang, Jason Chuang,
*
Jeffrey Heer,
*
Sebastian Schuster,
and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,sidaw,sebschu,manning}@stanford.edu
*
Computer Science Department, University of Washington
{jcchuang,jheer}@uw.edu
Abstract
Analyses of computer aided translation typi-
cally focus on either frontend interfaces and
human effort, or backend translation and
machine learnability of corrections. How-
ever, this distinction is artificial in prac-
tice since the frontend and backend must
work in concert. We present the first holis-
tic, quantitative evaluation of these issues
by contrasting two assistive modes: post-
editing and interactive machine translation
(MT). We describe a new translator inter-
face, extensive modifications to a phrase-
based MT system, and a novel objective
function for re-tuning to human correc-
tions. Evaluation with professional bilin-
gual translators shows that post-edit is faster
than interactive at the cost of translation
quality for French-English and English-
German. However, re-tuning the MT sys-
tem to interactive output leads to larger, sta-
tistically significant reductions in HTER
versus re-tuning to post-edit. Analysis
shows that tuning directly to HTER results
in fine-grained corrections to subsequent
machine output.
1 Introduction
The goal of machine translation has always been to
reduce human effort, whether by partial assistance
or by outright replacement. However, preoccupa-
tion with the latter?fully automatic translation?at
the exclusion of the former has been a feature of
the research community since its first nascent steps
in the 1950s. Pessimistic about progress during
that decade and future prospects, Bar-Hillel (1960,
p.3) argued that more attention should be paid to a
?machine-post-editor partnership,? whose decisive
problem is ?the region of optimality in the contin-
uum of possible divisions of labor.? Today, with
human-quality, fully automatic machine translation
(MT) elusive still, that decades-old recommenda-
tion remains current.
This paper is the first to look at both sides of
the partnership in a single user study. We compare
two common flavors of machine-assisted transla-
tion: post-editing and interactive MT. We analyze
professional, bilingual translators working in both
modes, looking first at user productivity. Does the
additional machine assistance available in the inter-
active mode affect translation time and/or quality?
Then we turn to the machine side of the part-
nership. The user study results in corrections to
the baseline MT output. Do these corrections help
the MT system, and can it learn from them quickly
enough to help the user? We perform a re-tuning
experiment in which we directly optimize human
Translation Edit Rate (HTER), which correlates
highly with human judgments of fluency and ade-
quacy (Snover et al., 2006). It is also an intuitive
measure of human effort, making fine distinctions
between 0 (no editing) and 1 (complete rewrite).
We designed a new user interface (UI) for the
experiment. The interface places demands on the
MT backend?not the other way around. The most
significant new MT system features are prefix de-
coding, for translation completion based on a user
prefix; and dynamic phrase table augmentation, to
handle target out-of-vocabulary (OOV) words. Dis-
criminative re-tuning is accomplished with a novel
cross-entropy objective function.
We report three main findings: (1) post-editing
is faster than interactive MT, corroborating Koehn
(2009a); (2) interactive MT yields higher quality
translation when baseline MT quality is high; and
(3) re-tuning to interactive feedback leads to larger
held-out HTER gains relative to post-edit. Together
these results show that a human-centered approach
to computer aided translation (CAT) may involve
tradeoffs between human effort and machine
learnability. For example, if speed is the top
priority, then a design geared toward post-editing
1225
AB
C
D E
Figure 1: Main translation interface. The user sees the full document context, with French source inputs
(A) interleaved with suggested English translations (B). The sentence in focus is indicated by the blue
rectangle, which can be moved via two hot keys. Source coverage (C) of the user prefix?shaded in
blue?updates as the user works, as do autocomplete suggestions (D) and a full completion (E).
is appropriate. However, if reductions in HTER
ultimately correspond to lower human effort, then
investing slightly more time in the interactive mode,
which results in more learnable output, may be op-
timal. Mixed UI designs may offer a compromise.
Code and data from our experiments are available at:
http://nlp.stanford.edu/software/phrasal/
A holistic comparison with human subjects nec-
essarily involves many moving parts. Section 2
briefly describes the interface, focusing on NLP
components. Section 3 describes changes to the
backend MT system. Section 4 explains the user
study, and reports human translation time and qual-
ity results. Section 5 describes the MT re-tuning
experiment. Analysis (section 6) and related work
(section 7) round out the paper.
2 New Translator User Interface
Figure 1 shows the translator interface, which is
designed for expert, bilingual translators. Previ-
ous studies have shown that expert translators work
and type quickly (Carl, 2010), so the interface is
designed to be very responsive, and to be primar-
ily operated by the keyboard. Most aids can be
accessed via either typing or four hot keys. The
current design focuses on the point of text entry
and does not include conventional translator work-
bench features such as workflow management, spell
checking, and text formatting tools.
In the trivial post-edit mode, the interactive aids
are disabled and a 1-best translation pre-populates
the text entry box.
We have described the HCI-specific motivations
for and contributions of this new interface in Green
et al. (2014c). This section focuses on interface
elements built on NLP components.
2.1 UI Overview and Walkthrough
We categorized interactions into three groups:
source comprehension: word lookups, source cov-
erage highlighting; target gisting: 1-best transla-
tion, real-time target completion; target genera-
tion: real-time autocomplete, target reordering, in-
sert complete translation. The interaction designs
are novel; those in italic have, to our knowledge,
never appeared in a translation workbench.
Source word lookup When the user hovers over
a source word, a menu of up to four ranked trans-
lation suggestions appears (Figure 2). The menu
is populated by a phrase-table query of the word
plus one token of left context. This query usually
returns in under 50ms. The width of the horizontal
bars indicates confidence, with the most confident
suggestion ?regularly? placed at the bottom, near-
est to the cursor. The user can insert a translation
suggestion by clicking.
Source coverage highlighting The source cover-
age feature (Figure 1C) helps the user quickly find
untranslated words in the source. The interaction is
1226
Figure 2: Source word lookup and target autocom-
plete menus. The menus show different suggestions.
The word lookupmenu (top) is not dependent on the
target context Teachers, whereas the autocomplete
dropdown (bottom) is.
based on the word alignments between source and
target generated by the MT system. We found that
the raw alignments are too noisy to show users, so
the UI filters them with phrase-level heuristics.
1-best translation The most common use of MT
output is gisting (Koehn, 2010, p.21). The gray text
below each black source input shows the best MT
system output (Figure 1B).
Real-time target completion When the user ex-
tends the black prefix, the gray text will update to
the most probable completion (Figure 1E). This up-
date comes from decoding under the full translation
model. All previous systems performed inference
in a word lattice.
Real-time autocomplete The autocomplete
dropdown at the point of text entry is the main
translation aid (Figures 1D and 2). Each real-time
update actually contains a distinct 10-best list for
the full source input. The UI builds up a trie from
these 10-best lists. Up to four distinct suggestions
are then shown at the point of translation. The
suggestion length is based on a syntactic parse of
the fixed source input. As an offline, pre-processing
step, we parse each source input with Stanford
CoreNLP (Manning et al., 2014). The UI combines
those parses with word alignments from the full
translation suggestions to project syntactic con-
stituents to each item on the n-best list. Syntactic
projection is a very old idea that underlies many
MT systems (see: Hwa et al. (2002)). Here we
make novel use of it for suggestion prediction
filtering.
1
Presently, we project noun phrases,
verb phrases (minus the verbal arguments), and
prepositional phrases. Crucially, these units are
natural to humans, unlike statistical target phrases.
Target Reordering Carl (2010) showed that ex-
pert translators tend to adopt local planning: they
read a few words ahead and then translate in a
roughly online fashion. However, word order differ-
ences between languages will necessarily require
longer range planning and movement. To that end,
the UI supports keyboard-based reordering. Sup-
pose that the user wants to move a span in gray
text to the insertion position for editing. Typing
the prefix of this string will update the autocom-
plete dropdown with matching strings from the gray
text. Consequently, sometimes the autocomplete
dropdown will contain suggestions from several
positions in the full suggested translation.
Insert complete translation The user can insert
the full completion via a hot key. Notice that if
the user presses this hot key immediately, all gray
text becomes black, and the interface effectively
switches to post-edit mode. This feature greatly ac-
celerates translation when the MT is mostly correct,
and the user only wants to make a few changes.
2.2 User Activity Logging
A web application serves the Javascript-based in-
terface, relays translation requests to the MT sys-
tem, and logs user records to a database. Each user
record is a tuple of the form (f, e?, h, u), where f
is the source sequence, e? is the latest 1-best ma-
chine translation of f , h is the correction of e?, and
u is the log of interaction events during the transla-
tion session. Our evaluation corpora also include
independently generated references e for each f .
3 Interactive MT Backend
Now we describe modifications to Phrasal (Green
et al., 2014b), the phrase-based MT system that sup-
ports the interface. Phrasal follows the log-linear
approach to phrase-based translation (Och and Ney,
2004) in which the decision rule has the familiar
linear form
e? = arg max
e
w
>
?(e, f) (1)
1
The classic TransType system included a probabilistic
prediction length component (Foster et al., 2002), but we find
that the simpler projection technique works well in practice.
1227
where w ? R
d
is the model weight vector and
?(?) ? R
d
is a feature map.
3.1 Decoding
The default Phrasal search algorithm is cube prun-
ing (Huang and Chiang, 2007). In the post-edit con-
dition, search is executed as usual for each source
input, and the 1-best output is inserted into the tar-
get textbox. However, in interactive mode, the full
search algorithm is executed each time the user
modifies the partial translation. Machine sugges-
tions e? must match user prefix h. Define indicator
function pref(e?, h) to return true if e? begins with
h, and false otherwise. Eq. 1 becomes:
e? = arg max
e s.t.pref(e,h)
w
>
?(e, f) (2)
Cube pruning can be straightforwardly modified to
satisfy this constraint by simple string matching of
candidate translations. Also, the pop limit must be
suspended until at least one legal candidate appears
on each beam, or the priority queue of candidates is
exhausted. We call this technique prefix decoding.
2
There is another problem. Human translators are
likely to insert unknown target words, including
new vocabulary, misspellings, and typographical
errors. They might also reorder source text so as to
violate the phrase-based distortion limit. To solve
these problems, we perform dynamic phrase table
augmentation, adding new synthetic rules specific
to each search. Rules allowing any source word to
align with any unseen or ungeneratable (due to the
distortion limit) target word are created.
3
These
synthetic rules are given rule scores lower than any
other rules in the set of queried rules for that source
input f . Then candidates are allowed to compete
on the beam. Candidates with spurious alignments
will likely be pruned in favor of those that only turn
to synthetic rules as a last resort.
3.2 Tuning
We choose BLEU (Papineni et al., 2002) for base-
line tuning to independent references, and HTER
for re-tuning to human corrections. Our rationale
is as follows: Cer et al. (2010) showed that BLEU-
tuned systems score well across automatic metrics
and also correlate with human judgment better than
2
Och et al. (2003) describe a similar algorithm for word
graphs.
3
Ortiz-Mart?nez et al. (2009) describe a related technique
in which all source and target words can align, with scores set
by smoothing.
systems tuned to other metrics. Conversely, sys-
tems tuned to edit-distance-based metrics like TER
tend to produce short translations that are heavily
penalized by other metrics.
When human corrections become available, we
switch to HTER, which correlates with human judg-
ment and is an interpretable measure of editing
effort. Whereas TER is computed as TER(e, e?),
HTER is HTER(h, e?). HBLEU is an alternative,
but since BLEU is invariant to some permutations
(Callison-Burch et al., 2006), it is less interpretable.
We find that it also does not work as well in practice.
We previously proposed a fast, online tuning al-
gorithm (Green et al., 2013b) based on AdaGrad
(Duchi et al., 2011). The default loss function is
expected error (EE) (Och, 2003; Cherry and Foster,
2012). Expected BLEU is an example of EE, which
we found to be unstable when switching metrics.
This may result from direct incorporation of the
error metric into the gradient computation.
To solve this problem, we propose a cross-
entropy loss which, to our knowledge, is new in
MT. Let
?
E = {e?
i
}
n
i=1
be an n-best list ranked
by a gold metric G(e, e?) ? 0. Assume we
have a preference of a higher G (e.g., BLEU or
1?HTER). Define the model distribution over
?
E
as q(e?|f) ? exp[w
>
?(e?, f)] normalized so that
?
e??
?
E
q(e?|f) = 1; q indicates howmuch the model
prefers each translation. Similarly, define p(e?|f)
based on any function of the gold metric so that
?
e??
?
E
p(e?|f) = 1; p indicates how much the met-
ric prefers each translation. We choose a DCG-
style
4
parameterization that skews the p distribu-
tion toward higher-ranked items on the n-best list:
p(e?
i
|f) ? G(e, e?
i
)/ log(1 + i) for the ith ranked
item. The cross-entropy (CE) loss function is:
`
CE
(w;E) = E
p(e?|f)
[? log(q(e?|f)] (3)
It turns out that if p is simply the posterior distribu-
tion of the metric, then this loss is related to the log
of the standard EE loss:
5
`
EE
(w;E) = ? log[E
p(e?|f)
[q(e?|f)]] (4)
We can show that `
CE
? `
EE
by applying Jensen?s
inequality to the function ? log(?). So minimizing
`
CE
also minimizes a convex upper bound of the
log expected error. This convexity given the n-
4
Discounted cumulative gain (DCG) is widely used in infor-
mation retrieval learning-to-rank settings. n-best MT learning
is standardly formulated as a ranking task.
5
For expected error, p(e?
i
) = G(e, e?
i
) is not usually nor-
malized. Normalizing p adds a negligible constant.
1228
best list does not mean that the overall MT tuning
loss is convex, since the n-best list contents and
order depend on the parameters w. However, all
regret bounds and other guarantees of online con-
vex optimization would now apply in the CE case
since `
CE,t
(w
t?1
;E
t
) is convex for each t. This
is attractive compared to expected error, which is
non-convex even given the n-best list. We empiri-
cally observed that CE converges faster and is less
sensitive to hyperparameters than EE.
Faster decoding trick We found that online tun-
ing also permits a trick that speeds up decoding
during deployment. Whereas the Phrasal default
beam size is 1,200, we were able to reduce the beam
size to 800 and run the tuner longer to achieve the
same level of translation quality. For example, at
the default beam size for French-English, the algo-
rithm converges after 12 iterations, whereas at the
lower beam size it achieves that level after 20 itera-
tions. In our experience, batch tuning algorithms
seem to be more sensitive to the beam size.
3.3 Feature Templates
The baseline system contains 19 dense feature tem-
plates: the nine Moses (Koehn et al., 2007) baseline
features, the eight-feature hierarchical lexicalized
re-ordering model of Galley and Manning (2008),
the (log) count of each rule in the bitext, and an
indicator for unique rules. We found that sparse
features, while improving translation quality, came
at the cost of slower decoding due to feature extrac-
tion and inner products with a higher dimensional
feature map ?. During prototyping, we observed
that users found the system to be sluggish unless
it responded in approximately 300ms or less. This
budget restricted us to dense features.
When re-tuning to corrections, we extract fea-
tures from the user logs u and add them to the
baseline dense model. For each tuning input f ,
the MT system produces candidate derivations d =
(f, e?, a), where a is a word alignment. The user log
u also contains the last MT derivation
6
accepted
by the user d
u
= (f, e?
u
, a
u
). We extract features
by comparing d and d
u
. The heuristic we take is
intersection: ?(d)? ?(d) ? ?(d
u
).
Lexicalized and class-based alignments Con-
sider the alignment in Figure 3. We find that
user derivations often contain many unigram rules,
6
Extracting features from intermediate user editing actions
is an interesting direction for future work.
tarceva
parvient
ainsi
?
stopper
la
croissance
t
a
r
c
e
v
a
w
a
s
t
h
u
s
a
b
l
e
t
o
h
a
l
t
t
h
e
g
r
o
w
t
h
Figure 3: User translation word alignment obtained
via prefix decoding and dynamic phrase table aug-
mentation.
which are less powerful than larger phrases, but
nonetheless provide high-precision lexical choice
information. We fire indicators for both unigram
links and multiword cliques. We also fire class-
based versions of this feature.
Source OOV blanket Source OOVs are usually
more frequent when adapting to a new domain. In
the case of European languages?our experimental
setting?many of the words simply transfer to the
target, so the issue is where to position them. In Fig-
ure 3, the proper noun tarceva is unknown, so the de-
coder OOV model generates an identity translation
rule. We add features in which the source word is
concatenated with the left, right, and left/right con-
texts in the target, e.g., {<s>-tarceva, tarceva-
was, <s>-tarceva-was}. We also add versions
with target words mapped to classes.
3.4 Differences from Previous Work
Our backend innovations support the UI and enable
feature-based learning from human corrections. In
contrast, most previous work on incremental MT
learning has focused on extracting new translation
rules, language model updating, and modifying
translation model probabilities (see: Denkowski
et al. (2014a)). We regard these features as ad-
ditive to our own work: certainly extracting new,
unseen rules should help translation in a new do-
main. Moreover, to our knowledge, all previous
work on updating the weight vector w has consid-
ered simulated post-editing, in which the indepen-
dent references e are substituted for corrections h.
Here we extract features from and re-tune to actual
corrections to the baseline MT output.
1229
4 Translation User Study
We conducted a human translation experiment with
a 2 (translation conditions) ? n (source sentences)
mixed design, where n depended on the language
pair. Translation conditions (post-edit and interac-
tive) and source sentences were the independent
variables (factors). Experimental subjects saw all
factor levels, but not all combinations, since one
exposure to a sentence would influence another.
Subjects completed the experiment remotely on
their own hardware. They received personalized
login credentials for the translation interface, which
administered the experiment. Subjects first com-
pleted a demographic questionnaire about prior ex-
perience with CAT and language proficiency. Next,
they completed a training module that included a
4-minute tutorial video and a practice ?sandbox? for
developing proficiency with the UI. Then subjects
completed the translation experiment. Finally, they
completed an exit questionnaire.
Unlike the experiment of Koehn (2009a), sub-
jects were under time pressure. An idle timer pre-
vented subjects from pausing for more than three
minutes while the translator interface was open.
This constraint eliminates a source of confound in
the timing analysis.
We randomized the order of translation condi-
tions and the assignment of sentences to conditions.
At most five sentences appeared per screen, and
those sentences appeared in the source document
order. Subjects could move among sentences within
a screen, but could not revise previous screens. Sub-
jects received untimed breaks both between trans-
lation conditions and after about every five screens
within a translation condition.
4.1 Linguistic Materials
We chose two language pairs: French-English (Fr-
En) and English-German (En-De). Anecdotally,
French-English is an easy language pair for MT,
whereas English-German is very hard due to re-
ordering and complex German morphology.
We chose three text genres: software, medical,
and informal news. The software text came from
the graphical interfaces of Autodesk AutoCAD and
Adobe Photoshop. The medical text was a drug re-
view from the European Medicines Agency. These
two data sets came from TAUS
7
and included inde-
pendent reference translations. The informal news
text came from the WMT 2013 shared task test set
7http://www.tausdata.org/
(Bojar et al., 2013). The evaluation corpus was con-
structed from equal proportions of the three genres.
The Fr-En dataset contained 3,003 source tokens
(150 segments); the En-De dataset contained 3,002
(173 segments). As a rule of thumb, a human trans-
lator averages about 2,700 source tokens per day
(Ray, 2013, p.36), so the experiment was designed
to replicate a slightly demanding work day.
4.2 Selection of Subjects
For each language pair, we recruited 16 profes-
sional, freelance translators on Proz, which is the
largest online translation community.
8
We posted
ads for both language pairs at a fixed rate of $0.085
per source word, an average rate in the industry. In
addition, we paid $10 to each translator for complet-
ing the training module. All subjects had significant
prior experience with a CAT workbench.
4.3 Results
We analyze the translation conditions in terms of
two response variables: time and quality. We ex-
cluded one Fr-En subject and two En-De subjects
from the models. One subject misunderstood the in-
structions of the experiment and proceeded without
clarification; another skipped the training module
entirely. The third subject had a technical problem
that prevented logging. Finally, we also filtered
segment-level sessions for which the log of transla-
tion time was greater than 2.5 standard deviations
from the mean.
4.3.1 Translation Time
We analyze time with a linear mixed effects model
(LMEM) estimated with the lme4 (Bates, 2007) R
package. When experimental factors are sampled
from larger populations?e.g., humans, sentences,
words?LMEMs are more robust to type II errors
(see: Baayen et al. (2008)). The log-transformed
time is the response variable and translation condi-
tion is the main independent variable. The maximal
random effects structure (Barr et al., 2013) contains
intercepts for subject, sentence id, and text genre,
each with random slopes for translation condition.
We found significant main effects for translation
condition (Fr-En, p < 0.05; En-De, p < 0.01).
The orientation of the coefficients indicates that
interactive is slower for both language pairs. For Fr-
En, the LMEM predicts a mean time (intercept) of
46.0 sec/sentence in post-edit vs. 54.6 sec/sentence
8http://www.proz.com
1230
Fr-En En-De
TER HTER TER HTER
post-edit 47.32 23.51 56.16 37.15
interactive 47.05 24.14 55.89 39.55
Table 1: Automatic assessment of translation qual-
ity. Here we change the definitions of TER and
HTER slightly. TER is the human translations com-
pared to the independent references. HTER is the
baseline MT compared to the human corrections.
in interactive, or 18.7% slower. For En-De, the
mean is 51.8 sec/sentence vs. 63.3 sec/sentence in
interactive, or 22.1% slower.
We found other predictive covariates that reveal
more about translator behavior. When subjects did
not edit the MT suggestion, they were significantly
faster. When token edit distance fromMT or source
input length increased, they were slower. Subjects
were usually faster as the experiment progressed, a
result that may indicate increased proficiency with
practice. Note that all subjects reported profes-
sional familiarity with post-edit, whereas the in-
teractive mode was entirely new to them. In the
exit survey many translators suggested that with
more practice, they could have been as fast in the
interactive mode.
9
4.3.2 Translation Quality
We evaluated translation quality with both auto-
matic and manual measures. Table 1 shows that
in the interactive mode, TER is lower and HTER
is higher: subjects created translations closer to
the references (lower TER), but performed more
editing (higher HTER). This result suggests better
translations in the interactive mode.
To confirm that intuition, we elicited judgments
from professional human raters. The setup followed
the manual quality evaluation of the WMT 2014
shared task (Bojar et al., 2014). We hired six raters?
three for each language pair?who were paid be-
tween $15?20 per hour. The raters logged into Ap-
praise (Federmann, 2010) and for each source seg-
ment, ranked five randomly selected translations.
From these 5-way rankings we extracted pairwise
judgments pi = {<,=}, where u
1
< u
2
indicates
that subject u
1
provided a better translation than
subject u
2
for a given source input (Table 2).
9
See (Green et al., 2014c) for significance levels of the
other covariates along with analysis of subject learning rates,
subject behavior, and qualitative feedback.
Fr-En En-De
#pairwise 14,211 15,001
#ties (=) 5,528 2,964
IAA 0.419 (0.357) 0.407 (0.427)
EW (inter.) 0.512 0.491
Table 2: Pairwise judgments for the manual qual-
ity assessment. Inter-annotator agreement (IAA)
? scores are measured with the official WMT14
script. For comparison, the WMT14 IAA scores
are given in parentheses. EW (inter.) is expected
wins of interactive according to Eq. (6).
Fr-En En-De
sign p sign p
ui (interactive) + ? ?
log edit distance ? ??? + ???
gender (female) ? + ?
log session order ? + ?
Table 3: LMEM manual translation quality results
for each fixed effect with contrast conditions for
binary predictors in (). The signs of the coefficients
can be interpreted as in ordinary regression. edit
distance is token-level edit distance from baseline
MT. session order is the order in which the subject
translated the sentence during the experiment. Sta-
tistical significance was computed with a likelihood
ratio test: ??? p < 0.001; ? p < 0.05.
In WMT the objective is to rank individual sys-
tems; here we need only compare interface condi-
tions. However, we should control for translator
variability. Therefore, we build a binomial LMEM
for quality. The model is motivated by the simple
and intuitive expected wins (EW) measure used at
WMT. Let S be the set of pairwise judgments and
wins(u
1
, u
2
) = |{(u
1
, u
2
, pi) ? S | pi = <}|. The
standard EW measure is:
e(u
1
) =
1
|S|
?
u
1
6=u
2
wins(u
1
, u
2
)
wins(u
1
, u
2
) + wins(u
2
, u
1
)
(5)
Sakaguchi et al. (2014) showed that, despite its sim-
plicity, Eq. (5) is nearly as effective as model-based
methods given sufficient high-quality judgments.
Since we care only about the two translation condi-
tions, we reinterpret the u
i
as interface conditions,
i.e., u
1
= int and u
2
= pe. We can then disregard
1231
the normalizing term to obtain:
e(u
1
) =
wins(u
1
, u
2
)
wins(u
1
, u
2
) + wins(u
2
, u
1
)
(6)
which is the expected value of a Bernoulli distribu-
tion (so e(u
2
) = 1 ? e(u
1
)). The intercept-term
of the binomial LMEM will be approximately this
value subject to other fixed and random effects.
To estimate the model, we convert each pairwise
judgment u
1
< u
2
to two examples where the re-
sponse is 1 for u
1
and 0 for u
2
. We add the fixed
effects shown in Table 3, where the numeric effects
are centered and scaled by their standard deviations.
The maximal random effects structure contains in-
tercepts for sentence id nested within subject along
with random slopes for interface condition.
Table 3 shows the p-values and coefficient orien-
tations. The models yield probabilities that can be
interpreted like Eq. (6) but with all fixed predictors
set to 0. For Fr-En, the value for post-edit is 0.472
vs. 0.527 for interactive. For En-De, post-edit is
0.474 vs. 0.467 for interactive. The difference is
statistically significant for Fr-En, but not for En-De.
When MT quality was anecdotally high (Fr-En),
high token-level edit distance from the initial sug-
gestion decreased quality. When MT was poor (En-
De), significant editing improved quality. Female
En-De translators were better than males, possibly
due to imbalance in the subject pool (12 females vs.
4 males). En-De translators seemed to improve with
practice (positive coefficient for session order).
The Fr-En results are the first showing an inter-
active UI that improves over post-edit.
5 MT Re-tuning Experiment
The human translators corrected the output of the
BLEU-tuned, baseline MT system. No updating of
the MT system occurred during the experiment to
eliminate a confound in the time and quality analy-
ses. Now we investigate re-tuning the MT system
to the corrections by simply re-starting the online
learning algorithm from the baseline weight vector
w, this time scoring with HTER instead of BLEU.
Conventional incremental MT learning experi-
ments typically resemble domain adaptation: small-
scale baselines are trained and tuned on mostly out-
of-domain data, and then re-tuned incrementally
on in-domain data. In contrast, we start with large-
scale systems. This is more consistent with a pro-
fessional translation environment where translators
receive suggestions from state-of-the-art systems
like Google Translate.
Bilingual Monolingual
#Segments #Tokens #Tokens
En-De 4.54M 224M 1.7B
Fr-En 14.8M 842M 2.24B
Table 4: Gross statistics of MT training corpora.
En-De Fr-En
baseline-tune 9,469 8,931
baseline-dev 9,012 9,030
int-tune 680 589
int-test 457 368
pe-tune 764 709
pe-test 492 447
Table 5: Tuning, development, and test corpora
(#segments). tune and dev were used for baseline
system preparation. Re-tuning was performed on
int-tune and pe-tune, respectively. We report held-
out results on the two test data sets. All sets are
supplied with independent references.
5.1 Datasets
Table 4 shows the monolingual and parallel train-
ing corpora. Most of the data come from the con-
strained track of the WMT 2013 shared task (Bojar
et al., 2013). We also added 61k parallel segments
of TAUS data to the En-De bitext, and 26k TAUS
segments to the Fr-En bitext. We aligned the par-
allel data with the Berkeley Aligner (Liang et al.,
2006) and symmetrized the alignments with the
grow-diag heuristic. For each target language we
used lmplz (Heafield et al., 2013) to estimate unfil-
tered, 5-gram Kneser-Ney LMs from the concate-
nation of the target side of the bitext and the mono-
lingual data. For the class-based features, we esti-
mated 512-class source and target mappings with
the algorithm of Green et al. (2014a).
The upper part of Table 5 shows the baseline
tuning and development sets, which also contained
1/3 TAUS medical text, 1/3 TAUS software text,
and 1/3 WMT newswire text (see section 4).
The lower part of Table 5 shows the organization
of the human corrections for re-tuning and testing.
Recall that for each unique source input, eight hu-
man translators produced a correction in each con-
dition. First, we filtered all corrections for which a
log u was not recorded (due to technical problems).
Second, we de-duplicated the corrections so that
each h was unique. Finally, we split the unique
(f, h) tuples according to a natural division in the
1232
System tune BLEU? TER? HTER
baseline bleu 23.12 60.29 44.05
re-tune hter 22.18 60.85 43.99
re-tune+feat hter 21.73 59.71 42.35
(a) En-De int-test results.
System tune BLEU? TER? HTER
baseline bleu 39.33 45.29 28.28
re-tune hter 39.99 45.73 26.96
re-tune+feat hter 40.30 45.28 26.40
(b) Fr-En int-test results.
Table 6: Main re-tuning results for interactive
data. baseline is the BLEU-tuned system used
in the translation user study. re-tune is the base-
line feature set re-tuned to HTER on int-tune. re-
tune+feat adds the human feature templates de-
scribed in section 3.3. bold indicates statistical
significance relative to the baseline at p < 0.001;
italic at p < 0.05 by the permutation test of Riezler
and Maxwell (2005).
data. There were five source segments per docu-
ment, and each document was rendered as a single
screen during the translation experiment. Segment
order was not randomized, so we could split the
data as follows: assign the first three segments of
each screen to tune, and the last two to test. This is
a clean split with no overlap.
This tune/test split has two attractive properties.
First, if we can quickly re-tune on the first few sen-
tences on a screen and provide better translations
for the last few, then presumably the user experience
improves. Second, source inputs f are repeated?
eight translators translated each input in each condi-
tion. This means that a reduction in HTER means
better average suggestions for multiple human trans-
lators. Contrast this experimental design with tun-
ing to the corrections of a single human translator.
There the system might overfit to one human style,
and may not generalize to other human translators.
5.2 Results
Table 6 contains the main results for re-tuning to in-
teractive MT corrections. For both language pairs,
we observe large statistically significant reductions
inHTER.However, the results for BLEU and TER?
which are computed with respect to the independent
references?are mixed. The lower En-De BLEU
score is explained by a higher brevity penalty for
the re-tuned output (0.918 vs. 0.862). However, the
re-tuned 4-gram and 3-gram precisions are signif-
System HTER? System HTER?
int pe
baseline 44.05 baseline 41.05
re-tune (int) 43.99 re-tune (pe) 40.34
re-tune+feat 42.35 ? ?
? ?1.80 ?0.71
Table 7: En-De test results for re-tuning to post-edit
(pe) vs. interactive (int). Features cannot be ex-
tracted from the post-edit data, so the re-tune+feat
system cannot be learned. The Fr-En results are
similar but are omitted due to space.
icantly higher. The unchanged Fr-En TER value
can be explained by the observation that no human
translators produced TER scores higher than the
baselineMT. This odd result has also been observed
for BLEU (Culy and Riehemann, 2003), although
here we do observe a slight BLEU improvement.
The additional features (854 for Fr-En; 847 for
En-De) help significantly and do not slow down
decoding. We used the same L
1
regularization
strength as the baseline, but feature growth could
be further constrained by increasing this parame-
ter. Tuning is very fast at about six minutes for the
whole dataset, so tuning during a live user session
is already practical.
Table 7 compares re-tuning to interactive vs.
post-edit corrections. Recall that the int-test and
pe-test datasets are different and contain different
references. The post-edit baseline is lower because
humans performed less editing in the baseline con-
dition (see Table 1). Features account for the great-
est reduction in HTER. Of course, the features are
based mostly on word alignments, which could be
obtained for the post-edit data by running an online
word alignment tool (see: Farajian et al. (2014)).
However, the interactive logs contain much richer
user state information that we could not exploit due
to data sparsity. We also hypothesize that the fi-
nal interactive corrections might be more useful
since suggestions prime translators (Green et al.,
2013a), and the MT system was able to refine its
suggestions.
6 Re-tuning Analysis
Tables 6 and 7 raise two natural questions: what
accounts for the reduction in HTER, and why are
the TER/BLEU results mixed? Comparison of the
BLEU-tuned baseline to the HTER re-tuned sys-
tems gives some insight. For both questions, fine-
1233
grained corrections appear to make the difference.
Consider this French test example (with gloss):
(1) une
one
ligne
line
de
of
chimioth?rapie
chemotherapy
ant?rieure
previous
The independent reference for une ligne de chimio-
th?rapie is ?previous chemotherapy treatment?, and
the baseline produces ?previous chemotherapy line.?
The source sentence appears seven times with the
following user translations: ?one line or more
of chemotherapy?, ?one prior line of chemother-
apy?, ?one previous line of chemotherapy? (2), ?one
line of chemotherapy before? (2), ?one protocol of
chemotherapy?. The re-tuned, feature-based sys-
tem produces ?one line of chemotherapy before?,
matching two of the humans exactly, and six of the
humans in terms of idiomatic medical jargon (?line
of chemotherapy? vs. ?chemotherapy treatment?).
However, the baseline output would have received
better BLEU and TER scores.
Sometimes re-tuning improves the translations
with respect to both the reference and the human
corrections. This English phrase appears in the
En-De test set:
(2) depending
abh?ngig
on
von
the
der
file
datei
The baseline produces exactly the gloss shown in Ex.
(2). The human translators produced: ?je nach datei?
(6), ?das dokument?, and ?abh?ngig von der datei?.
The re-tuned system rendered the phrase ?je nach
dokument?, which is closer to both the independent
reference ?je nach datei? and the human corrections.
This change improves TER, BLEU, and HTER.
7 Related Work
The process study most similar to ours is that of
Koehn (2009a), who compared scratch, post-edit,
and simple interactive modes. However, he used un-
dergraduate, non-professional subjects, and did not
consider re-tuning. Our experimental design with
professional bilingual translators follows our previ-
ous work Green et al. (2013a) comparing scratch
translation to post-edit.
Many research translation UIs have been pro-
posed including TransType (Langlais et al., 2000),
Caitra (Koehn, 2009b), Thot (Ortiz-Mart?nez and
Casacuberta, 2014), TransCenter (Denkowski et
al., 2014b), and CasmaCat (Alabau et al., 2013).
However, to our knowledge, none of these inter-
faces were explicitly designed according to mixed-
initiative principles from the HCI literature.
Incremental MT learning has been investigated
several times, usually starting from no data (Bar-
rachina et al., 2009; Ortiz-Mart?nez et al., 2010),
via simulated post-editing (Mart?nez-G?mez et al.,
2012; Denkowski et al., 2014a), or via re-ranking
(W?schle et al., 2013). No previous experiments
combined large-scale baselines, full re-tuning of
the model weights, and HTER optimization.
HTER tuning can be simulated by re-
parameterizing an existing metric. Snover et
al. (2009) tuned TERp to correlate with HTER,
while Denkowski and Lavie (2010) did the same
for METEOR. Zaidan and Callison-Burch (2010)
showed how to solicit MT corrections for HTER
from Amazon Mechanical Turk.
Our learning approach is related to coactive learn-
ing (Shivaswamy and Joachims, 2012). Their basic
preference perceptron updates toward a correction,
whereas we use the correction for metric scoring
and feature extraction.
8 Conclusion
We presented a new CAT interface that supports
post-edit and interactive modes. Evaluation with
professional, bilingual translators showed post-edit
to be faster, but prior subject familiarity with post-
edit may have mattered. For French-English, the
interactive mode enabled higher quality translation.
Re-tuning the MT system to interactive corrections
also yielded large HTER gains. Technical contri-
butions that make re-tuning possible are a cross-
entropy objective, prefix decoding, and dynamic
phrase table augmentation. Larger quantities of cor-
rections should yield further gains, but our current
experiments already establish the feasibility of Bar-
Hillel?s virtuous ?machine-post-editor partnership?
which benefits both humans and machines.
Acknowledgements
We thank TAUS for access to their data reposi-
tory. We also thank John DeNero, Chris Dyer,
Alon Lavie, and Matt Post for helpful conversa-
tions. The first author is supported by a National
Science Foundation Graduate Research Fellowship.
This work was also supported by the Defense Ad-
vanced Research Projects Agency (DARPA) Broad
Operational Language Translation (BOLT) program
through IBM. Any opinions, findings, and conclu-
sions or recommendations expressed are those of
the author(s) and do not necessarily reflect the view
of either DARPA or the US government.
1234
References
V. Alabau, R. Bonk, C. Buck, M. Carl, F. Casacuberta,
M. Garc?a-Mart?nez, et al. 2013. Advanced com-
puter aided translation with a web-based workbench.
In 2nd Workshop on Post-Editing Technologies and
Practice.
R.H. Baayen, D.J. Davidson, and D.M. Bates. 2008.
Mixed-effects modeling with crossed random effects
for subjects and items. Journal of Memory and Lan-
guage, 59(4):390?412.
Y. Bar-Hillel. 1960. The present status of automatic
translation of languages. Advances in Computers,
1:91?163.
D. J. Barr, R. Levy, C. Scheepers, and H. J. Tily. 2013.
Random effects structure for confirmatory hypothe-
sis testing: Keep it maximal. Journal of Memory
and Language, 68(3):255?278.
S. Barrachina, O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, et al. 2009. Statistical ap-
proaches to computer-assisted translation. Compu-
tational Linguistics, 35(1):3?28.
D. M. Bates. 2007. lme4: Linear mixed-
effects models using S4 classes. Technical re-
port, R package version 1.1-5, http://cran.r-
project.org/package=lme4.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, et al. 2013. Findings of the
2013 Workshop on Statistical Machine Translation.
In WMT.
O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn,
J. Leveling, et al. 2014. Findings of the 2014 Work-
shop on Statistical Machine Translation. In WMT.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine transla-
tion research. In EACL.
M. Carl. 2010. A computational framework for a cogni-
tive model of human translation processes. In Aslib
Translating and the Computer Conference.
D. Cer, C. D.Manning, and D. Jurafsky. 2010. The best
lexical metric for phrase-based statistical MT system
optimization. In NAACL.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In NAACL.
C. Culy and S. Z. Riehemann. 2003. The limits of n-
gram translation evaluation metrics. In MT Summit
IX.
M. Denkowski and A. Lavie. 2010. Extending the ME-
TEOR machine translation evaluation metric to the
phrase level. In NAACL.
M. Denkowski, C. Dyer, and A. Lavie. 2014a. Learn-
ing from post-editing: Online model adaptation for
statistical machine translation. In EACL.
M. Denkowski, A. Lavie, I. Lacruz, and C. Dyer.
2014b. Real time adaptive machine translation for
post-editing with cdec and TransCenter. In Work-
shop on Humans and Computer-assisted Translation.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
M. A. Farajian, N. Bertoldi, and M. Federico. 2014.
Online word alignment for online adaptive machine
translation. InWorkshop on Humans and Computer-
assisted Translation.
C. Federmann. 2010. Appraise: An open-source
toolkit for manual phrase-based evaluation of trans-
lations. In LREC.
G. Foster, P. Langlais, and G. Lapalme. 2002. User-
friendly text prediction for translators. In EMNLP.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, J. Heer, and C. D. Manning. 2013a. The effi-
cacy of human post-editing for language translation.
In CHI.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
S. Green, D. Cer, and C. D. Manning. 2014a. An em-
pirical comparison of features and tuning for phrase-
based machine translation. In WMT.
S. Green, D. Cer, and C. D. Manning. 2014b. Phrasal:
A toolkit for new directions in statistical machine
translation. In WMT.
S. Green, J. Chuang, J. Heer, andC.D.Manning. 2014c.
Predictive Translation Memory: A mixed-initiative
system for human language translation. In UIST.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
ACL.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Koehn. 2009a. A process study of computer-aided
translation. Machine Translation, 23:241?263.
P. Koehn. 2009b. A web-based interactive computer
aided translation tool. In ACL-IJCNLP, Software
Demonstrations.
1235
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Langlais, G. Foster, and G. Lapalme. 2000.
TransType: a computer-aided translation typing sys-
tem. In Workshop on Embedded Machine Transla-
tion Systems.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C. Manning, M. Surdeanu, J. Bauer, J. Finkel,
S. Bethard, and D. McClosky. 2014. The Stanford
CoreNLP natural language processing toolkit. In
ACL, System Demonstrations.
P. Mart?nez-G?mez, G. Sanchis-Trilles, and F. Casacu-
berta. 2012. Online adaptation strategies for sta-
tistical machine translation in post-editing scenarios.
Pattern Recognition, 45(9):3193?3203.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och, R. Zens, and H. Ney. 2003. Efficient
search for interactive statistical machine translation.
In EACL.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
D. Ortiz-Mart?nez and F. Casacuberta. 2014. The new
Thot toolkit for fully automatic and interactive statis-
tical machine translation. In EACL, System Demon-
strations.
D. Ortiz-Mart?nez, I. Garc?a-Varea, and F. Casacuberta.
2009. Interactive machine translation based on par-
tial statistical phrase-based alignments. In RANLP.
D. Ortiz-Mart?nez, I. Garc?a-Varea, and F. Casacuberta.
2010. Online learning for interactive statistical ma-
chine translation. In NAACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
R. Ray. 2013. Ten essential research findings for 2013.
In 2013 Resource Directory & Index. Multilingual.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
K. Sakaguchi, M. Post, and B. Van Durme. 2014. Effi-
cient elicitation of annotations for human evaluation
of machine translation. In WMT.
P. Shivaswamy and T. Joachims. 2012. Online struc-
tured prediction via coactive learning. In ICML.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric.
In WMT.
K. W?schle, P. Simianer, N. Bertoldi, S. Riezler, and
M. Federico. 2013. Generative and discriminative
methods for online adaptation in SMT. In MT Sum-
mit XIV.
O. F. Zaidan and C. Callison-Burch. 2010. Predicting
human-targeted translation edit rate via untrained hu-
man annotators. In NAACL.
1236
Parsing Models for Identifying
Multiword Expressions
Spence Green?
Stanford University
Marie-Catherine de Marneffe??
Stanford University
Christopher D. Manning?
Stanford University
Multiword expressions lie at the syntax/semantics interface and have motivated alternative
theories of syntax like Construction Grammar. Until now, however, syntactic analysis and
multiword expression identification have been modeled separately in natural language process-
ing. We develop two structured prediction models for joint parsing and multiword expression
identification. The first is based on context-free grammars and the second uses tree substitution
grammars, a formalism that can store larger syntactic fragments. Our experiments show that
both models can identify multiword expressions with much higher accuracy than a state-of-the-
art system based on word co-occurrence statistics.
We experiment with Arabic and French, which both have pervasive multiword expres-
sions. Relative to English, they also have richer morphology, which induces lexical sparsity
in finite corpora. To combat this sparsity, we develop a simple factored lexical representation
for the context-free parsing model. Morphological analyses are automatically transformed into
rich feature tags that are scored jointly with lexical items. This technique, which we call
a factored lexicon, improves both standard parsing and multiword expression identification
accuracy.
1. Introduction
Multiword expressions are groups of words which, taken together, can have un-
predictable semantics. For example, the expression part of speech refers not to some
aspect of speaking, but to the syntactic category of a word. If the expression is
altered in some ways?part of speeches, part of speaking, type of speech?then the
idiomatic meaning is lost. Other modifications, however, are permitted, as in the plural
parts of speech. These characteristics make multiword expressions (MWEs) difficult to
identify and classify. But if they can be identified, then the incorporation of MWE
knowledge has been shown to improve task accuracy for a range of NLP applications
? Department of Computer Science. E-mail: spenceg@stanford.edu.
?? Department of Linguistics. E-mail: mcdm@stanford.edu.
? Departments of Computer Science and Linguistics. E-mail: manning@stanford.edu.
Submission received: October 1, 2011; revised submission received: June 9, 2012; accepted for publication:
August 3, 2012.
No rights reserved. This work was authored as part of the Contributor?s official duties as an Employee of
the United States Government and is therefore a work of the United States Government. In accordance with
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
Computational Linguistics Volume 39, Number 1
including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and
Baldwin 2006), sentence generation (Hogan et al 2007), machine translation (Carpuat
and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010).
The standard approach to MWE identification is n-gram classification. This tech-
nique is simple. Given a corpus, all n-grams are extracted, filtered using heuristics,
and assigned feature vectors. Each coordinate in the feature vector is a real-valued
quantity such as log likelihood or pointwise mutual information. A binary classifier
is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE
Shared Task (Evert 2008) utilized variants of this technique.
Broadly speaking, n-gram classification methods measure word co-occurrence. Sup-
pose that a corpus contains more occurrences of part of speech than parts of speech. Surface
statistics may erroneously predict that only the former is an MWE and the latter is not.
More worrisome is that the statistics for the two n-grams are separate, thus missing an
obvious generalization.
In this article, we show that statistical parsing models generalize more effectively
over arbitrary-length multiword expressions. This approach has not been previously
demonstrated. To show its effectiveness, we build two parsing models for MWE iden-
tification. The first model is based on a context-free grammar (CFG) with manual
rule refinements (Klein and Manning 2003). This parser also includes a novel lexical
model?the factored lexicon?that incorporates morphological features. The second
model is based on tree substitution grammar (TSG), a formalism with greater strong
generative capacity that can store larger structural tree fragments, some of which are
lexicalized.
We apply the models to Modern Standard Arabic (henceforth MSA, or simply
?Arabic?) and French, two morphologically rich languages (MRLs). The lexical sparsity
(in finite corpora) induced by rich morphology poses a particular challenge for n-gram
classification. Relative to English, French has a richer array of morphological features?
such as grammatical gender and verbal conjugation for aspect and voice. Arabic also
has richer morphology including gender and dual number. It has pervasive verb-
initial matrix clauses, although preposed subjects are also possible. For languages like
these it is well known that constituency parsing models designed for English often do
not generalize well. Therefore, we focus on the interplay among language, annotation
choices, and parsing model design for each language (Levy and Manning 2003; K?bler
2005, inter alia), although our methods are ultimately very general.
Our modeling strategy for MWEs is simple: We mark them with flat bracketings
in phrase structure trees. This representation implicitly assumes a locality constraint
on idioms, an assumption with a precedent in linguistics (Marantz 1997, inter alia).
Of course, it is easy to find non-local idioms that do not correspond to surface con-
stituents or even contiguous strings (O?Grady 1998). Utterances such as All hell seemed
to break loose and The cat got Mary?s tongue are clearly idiomatic, yet the idiomatic
elements are discontiguous. Our models cannot identify these MWEs, but then again,
neither can n-gram classification. Nonetheless, many common MWE types like nominal
compounds are contiguous and often correspond to constituent boundaries.
Consider again the phrasal compound part of speech,1 which is non-compositional:
The idiomatic meaning ?syntactic category? does not derive from any of the component
1 It is common to hyphenate some nominal compounds, e.g., part-of-speech. This practice invites a
words-with-spaces treatment of idioms. However, hyphens are inconsistently used in English.
Hyphenation is more common in French, but totally absent in Arabic.
196
Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions
words. This non-compositionality affects the syntactic environment of the compound as
shown by the addition of an attributive adjective:
(1) a. Noun is a part of speech.
b. *Noun is a big part of speech.
c. *Noun is a big part.
(2) a. Liquidity is a part of growth.
b. Liquidity is a big part of growth.
c. Liquidity is a big part.
In Example (1a) the copula predicate part of speech as a whole describes Noun. In
Examples (1b) and (1c) big clearly modifies only part and the idiomatic meaning is
lost. The attributive adjective cannot probe arbitrarily into the non-compositional com-
pound. In contrast, Example (2) contains parallel data without idiomatic semantics.
The conventional syntactic analysis of Example (2a) is identical to that of Example (1a)
except for the lexical items, yet part of growth is not idiomatic. Consequently, many pre-
modifiers are appropriate for part, which is semantically vacuous. In Example (2b), big
clearly modifies part, and of growth is just an optional PP complement, as shown by
Example (2c), which is still grammatical.
This article proposes different phrase structures for examples such as (1a) and
(2a). Figure 1a shows a Penn Treebank (PTB) (Marcus, Marcinkiewicz, and Santorini
1993) parse of Example (1a), and Figure 1b shows the parse of a paraphrase. The
phrasal compound part of speech functions syntactically like a single-word nominal
like category, and indeed Noun is a big category is grammatical. Single-word para-
phrasability is a common, though not mandatory, characteristic of MWEs (Baldwin
and Kim 2010). Starting from the paraphrase parse, we create a representation like
Figure (1c). The MWE is indicated by a label in the predicted structure, which is
flat. This representation explicitly models the idiomatic semantics of the compound
and is context-free, so we can build efficient parsers for it. Crucially, MWE identifi-
cation becomes a by-product of parsing as we can trivially extract MWE spans from
full parses.
We convert existing Arabic and French syntactic treebanks to the new MWE
representation. With this representation, the TSG model yields the best MWE iden-
tification results for Arabic (81.9% F1) and competitive results for French (71.3%),
even though its parsing results lag state-of-the-art probabilistic CFG (PCFG)-based
parsers. The TSG model also learns human-interpretable MWE rules. The fac-
tored lexicon model with gold morphological annotations achieves the best MWE
results for French (87.3% F1) and competitive results for Arabic (78.2% F1). For both
languages the factored lexicon model also approaches state-of-the-art basic parsing
accuracy.
The remainder of this article begins with linguistic background on common MWE
types in Arabic and French (Section 2). We then describe two constituency parsing
models that are tuned for MWE identification (Sections 3 and 4). These models are
supervised and can be trained on existing linguistic resources (Section 5). We evaluate
the models for both basic parsing and MWE identification (Section 6). Finally, we
compare our results with a state-of-the-art n-gram classification system (Section 7) and
to prior work (Section 8).
197
Computational Linguistics Volume 39, Number 1
2. Multiword Expressions in Arabic and French
In this section we provide a general definition and taxonomy of MWEs. Then we discuss
types of MWEs in Arabic and French.
2.1 Definition of Multiword Expressions
MWEs, a known nuisance for both linguistics and NLP, blur the lines between syntax
and semantics. Jackendoff (1997, page 156) comments that MWEs ?are hardly a marginal
part of our use of language,? and estimates that a native speaker knows at least as many
MWEs as single words. A linguistically adequate representation for MWEs remains an
active area of research, however. Baldwin and Kim (2010) define MWEs as follows:
Definition 1
Multiword expressions are lexical items that: (a) can be decomposed into multi-
ple lexemes; and (b) display lexical, syntactic, semantic, pragmatic, and/or statistical
idiomaticity.
S
NP
NN
Noun
VP
VBZ
is
NP
DT
a
NN
part
PP
IN
of
NP
NN
speech
S
NP
NN
Noun
VP
VBZ
is
NP
DT
a
NN
category
(a) Standard analysis of Example (1a) (b) Standard analysis of a paraphrase
S
NP
NN
Noun
VP
VBZ
is
NP
DT
a
MWN
NN
part
IN
of
NN
speech
(c) MWE analysis used in this article
Figure 1
(a) A standard PTB parse of Example (1a). (b) The MWE part of speech functions syntactically
like the ordinary nominal category, as shown by this paraphrase. (c) We incorporate the
presence of the MWE into the syntactic analysis by flattening the tree dominating part of speech
and introducing a new non-terminal label multiword noun (MWN) for the resulting span. The
new representation classifies an MWE according to a global syntactic type and assigns a POS
to each of the internal tokens. It makes no commitment to the internal syntactic structure of
the MWE, however.
198
Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions
Table 1
Semi-fixed MWEs in French and English. The French adverb ? terme (?in the end?) can be
modified by a small set of adjectives, and in turn some of these adjectives can be modified
by an adverb such as tr?s (?very?). Similar restrictions appear in English.
French English
? terme in the near term
? court terme in the short term
? tr?s court terme in the very short term
? moyen terme in the medium term
? long terme in the long term
? tr?s long terme in the very long term
MWEs fall into four broad categories (Sag et al 2002):
1. Fixed?do not allow morphosyntactic variation or internal modification (in short,
by and large).
2. Semi-fixed?can be inflected or undergo internal modification (Table 1).
3. Syntactically flexible?undergo syntactic variation such as inflection (e.g.,
phrasal verbs such as look up and write down).
4. Institutionalized phrases?fully compositional phrases that are statistically
idiosyncratic (traffic light, Secretary of State).
Statistical parsers are well-suited for coping with lexical, syntactic, and statistical
idiomaticity across all four MWE classes. However, to our knowledge, we are the first
to explicitly tune parsers for MWE identification.
2.2 Arabic MWEs
The most recent and most relevant work on Arabic MWEs was by Ashraf (2012), who
analyzed an 83-million-word Arabic corpus. He developed an empirical taxonomy of
six MWE types, which correspond to syntactic classes. The syntactic class is defined
by the projection of the purported syntactic head of the MWE. MWEs are further
subcategorized by observed POS sequences. For some of these classes, the syntactic
distinctions are debatable. For example, in the verb-object idiom 





	




 




Daraba cSfuurayn bi-Hajar (?he killed two birds with one stone?)2 the composition of

	




 (?two birds?) with 

 (?stone?) is at least as important as composition with the
verb 



 (?he killed?), yet Ashraf (2012) classifies the phrase as a verbal idiom.
The corpus in our experiments only marks three of the six Arabic MWE classes:
Nominal idioms (MWN) consist of proper nouns (Example 3a), noun compounds
(Example 3b), and construct NPs (Example 3c). MWNs typically correspond to NP
bracketings:
(3) a. N N: 




 


 abuu Dabii (?Abu Dhabi?)
2 For each Arabic example in this work, we provide native script, transliterations in italics according to the
phonetic scheme in Ryding (2005), and English translations in single quotes.
199
Computational Linguistics Volume 39, Number 1
b. D+N D+N: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 867?875,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improved Models of Distortion Cost for Statistical Machine Translation
Spence Green, Michel Galley, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{spenceg,mgalley,manning}@stanford.edu
Abstract
The distortion cost function used in Moses-
style machine translation systems has two
flaws. First, it does not estimate the future
cost of known required moves, thus increas-
ing search errors. Second, all distortion is
penalized linearly, even when appropriate re-
orderings are performed. Because the cost
function does not effectively constrain search,
translation quality decreases at higher dis-
tortion limits, which are often needed when
translating between languages of different ty-
pologies such as Arabic and English. To ad-
dress these problems, we introduce a method
for estimating future linear distortion cost, and
a new discriminative distortion model that pre-
dicts word movement during translation. In
combination, these extensions give a statis-
tically significant improvement over a base-
line distortion parameterization. When we
triple the distortion limit, our model achieves
a +2.32 BLEU average gain over Moses.
1 Introduction
It is well-known that translation performance in
Moses-style (Koehn et al, 2007) machine transla-
tion (MT) systems deteriorates when high distortion
is allowed. The linear distortion cost model used in
these systems is partly at fault. It includes no es-
timate of future distortion cost, thereby increasing
the risk of search errors. Linear distortion also pe-
nalizes all re-orderings equally, even when appro-
priate re-orderings are performed. Because linear
distortion, which is a soft constraint, does not effec-
tively constrain search, a distortion limit is imposed
on the translation model. But hard constraints are
ultimately undesirable since they prune the search
space. For languages with very different word or-
Followers of all of the Christian and Islamic sects
engaged

Verb

NP-OBJ
	

PP

2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 60?69,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Entity Clustering Across Languages
Spence Green*, Nicholas Andrews?, Matthew R. Gormley?,
Mark Dredze?, and Christopher D. Manning*
*Computer Science Department, Stanford University
{spenceg,manning}@stanford.edu
?Human Language Technology Center of Excellence, Johns Hopkins University
{noa,mrg,mdredze}@cs.jhu.edu
Abstract
Standard entity clustering systems commonly
rely on mention (string) matching, syntactic
features, and linguistic resources like English
WordNet. When co-referent text mentions ap-
pear in different languages, these techniques
cannot be easily applied. Consequently, we
develop new methods for clustering text men-
tions across documents and languages simulta-
neously, producing cross-lingual entity clusters.
Our approach extends standard clustering algo-
rithms with cross-lingual mention and context
similarity measures. Crucially, we do not as-
sume a pre-existing entity list (knowledge base),
so entity characteristics are unknown. On an
Arabic-English corpus that contains seven dif-
ferent text genres, our best model yields a 24.3%
F1 gain over the baseline.
1 Introduction
This paper introduces techniques for clustering co-
referent text mentions across documents and lan-
guages. On the web today, a breaking news item
may instantly result in mentions to a real-world entity
in multiple text formats: news articles, blog posts,
tweets, etc. Much NLP work has focused on model
adaptation to these diverse text genres. However, the
diversity of languages in which the mentions appear
is a more significant challenge. This was particularly
evident during the 2011 popular uprisings in the Arab
world, in which electronic media played a prominent
role. A key issue for the outside world was the aggre-
gation of information that appeared simultaneously
in English, French, and various Arabic dialects.
To our knowledge, we are the first to consider clus-
tering entity mentions across languages without a pri-
ori knowledge of the quantity or types of real-world
entities (a knowledge base). The cross-lingual set-
ting introduces several challenges. First, we cannot
assume a prototypical name format. For example,
the Anglo-centric first/middle/last prototype used in
previous name modeling work (cf. (Charniak, 2001))
does not apply to Arabic names like Abdullah ibn
Abd Al-Aziz Al-Saud or Chinese names like Hu Jin-
tao (referred to as Mr. Hu, not Mr. Jintao). Sec-
ond, organization names often require both translit-
eration and translation. For example, the Arabic
	PP?

K?? ?Q
	
g.

??Q?? ?General Motors Corp? contains
transliterations of 	PP?K?? ?Q
	
g. ?General Motors?,
but a translation of

??Q?? ?Corporation?.
Our models are organized as a pipeline. First, for
each document, we perform standard mention detec-
tion and coreference resolution. Then, we use pair-
wise cross-lingual similarity models to measure both
mention and context similarity. Finally, we cluster
the mentions based on similarity.
Our work makes the following contributions: (1)
introduction of the task, (2) novel models for cross-
lingual entity clustering of person and organization en-
tities, (3) cross-lingual annotation of the NIST Auto-
matic Content Extraction (ACE) 2008 Arabic-English
evaluation set, and (4) experimental results using both
gold and automatic within-document processing. We
will release our software and annotations to support
future research.
1.1 Task Description via a Simple Example
Consider the toy corpus in Fig. 1. The English docu-
ments contain mentions of two people: Steven Paul
Jobs and Mark Elliot Zuckerberg. Of course, the sur-
face realization of Mr. Jobs? last name in English is
also an ordinary nominal, hence the ambiguous men-
tion string (absent context) in the second document.
The Arabic document introduces an organization en-
tity (Apple Inc.) along with proper and pronominal
references to Mr. Jobs. Finally, the French document
refers to Mr. Jobs by the honorific ?Monsieur,? and to
60
Jobs program details delayed
Steve Jobs admired Mark Zuckerberg
M. Jobs, le fondateur d'Apple, est mort
	
		
?
=
? E1
E2
E3
=
=
doc1:
doc2:
doc3:
doc4:
Figure 1: Clustering entity mentions across languages and documents. The toy corpus contains English (doc1 and
doc2), Arabic (doc3), and French (doc4). Together, the documents make reference to three real-world entities, the
identification of which is the primary objective of this work. We use a separately-trained system for within-document
mention detection and coreference (indicated by the text boxes and intra-document links, respectively). Our experimental
results are for Arabic-English only.
Apple without its corporate designation.
Our goal is to automatically produce the cross-
lingual entity clusters E1 (Mark Elliot Zuckerberg),
E2 (Apple Inc.), and E3 (Steven Paul Jobs). Both the
true number and characteristics of these entities are
unobserved. Our models require two pre-processing
steps: mention detection and within-document coref-
erence/anaphora resolution, shown in Fig. 1 by the
text boxes and intra-document links, respectively. For
example, in doc3, a within-document coreference sys-
tem would pre-link 	QK. ?k. joobz ?Jobs? with the mascu-
line pronoun ? h ?his?. In addition, the mention detec-
tor determines that the surface form ?Jobs? in doc2
is not an entity reference. For this within-document
pre-processing we use Serif (Ramshaw et al, 2011).1
Our models measure cross-lingual similarity of the
coreference chains to make clustering decisions (?
in Fig. 1). The similarity models (indicated by the
= and 6= operators in Fig. 1) consider both mention
string and context similarity (?2). We use the men-
tion similarities as hard constraints, and the context
similarities as soft constraints. In this work, we inves-
tigate two standard constrained clustering algorithms
(?3). Our methods can be used to extend existing sys-
tems for mono-lingual entity clustering (also known
as ?cross-document coreference resolution?) to the
cross-lingual setting.
1Serif is a commercial system that assumes each document
contains only one language. Currently, there are no publicly avail-
able within-document coreference systems for Arabic and many
other languages. To remedy this problem, the CoNNL-2012
shared task aims to develop multilingual coreference systems.
2 Mention and Context Similarity
Our goal is to create cross-lingual sets of co-referent
mentions to real-world entities (people, places, orga-
nizations, etc.). In this paper, we adopt the following
notation. LetM be a set of distinct text mentions in a
collection of documents;C is a partitioning ofM into
document-level sets of co-referent mentions (called
coreference chains); E is a partitioning of C into sets
of co-referent chains (called entities). Let i, j be non-
negative integers less than or equal to |M | and a, b be
non-negative integers less than or equal to |C|. Our
experiments use a separate within-document corefer-
ence system to createC, which is fixed. We will learn
E, which has size no greater than |C| since the set of
mono-lingual chains is the largest valid partitioning.
We define accessor functions to access properties
of mentions and chains. For any mentionmi, define
the following functions: lang(mi) is the language;
doc(mi) is the document containingmi; type(mi) is
the semantic type, which is assigned by the within-
document coreference system. We also extract a set
of mention contexts S, which are the sentences con-
taining each mention (i.e., |S| = |M |).
We learn the partition E by considering mention
and context similarity, which are measured with sep-
arate component models.
2.1 Mention Similarity
We use separate methods for within- and cross-
language mention similarity. The pairwise similarity
61
Arabic Rules
H. ? b
H? t H? th h. ? j
h? h p? kh X? d
	
X? th
P? r 	P? z ?? s ?? sh
?? s 	?? d ?? t 	?? th
?? a
	
?? g
	
?? f

?? q
?? k ?? l ?? m 	?? n
?? h @? a ?? w ?? a

?? ah ?


? ? Z? ?
English Rules
k? c p? b x? ks e,i,o,u? ?
Table 1: English-Arabic mapping rules to a common or-
thographic representation. ??? indicates a null mapping.
For English, we also lowercase and remove determiners
and punctuation. For Arabic, we remove the determiner
?@ Al ?the? and the elongation character tatwil ??.
of any two mentionsmi andmj is:
sim(mi,mj) =
{
jaro-winkler(mi,mj) if lang(mi) = lang(mj)
maxent(mi,mj) otherwise
Jaro-Winkler Distance (within-language) If
lang(mi) = lang(mj), we use the Jaro-Winkler edit
distance (Porter and Winkler, 1997). Jaro-Winkler
rewards matching prefixes, the empirical justification
being that less variation typically occurs at the
beginning of names.2 The metric produces a score in
the range [0,1], where 0 indicates equality.
Maxent model (cross-language) When lang(mi)
6= lang(mj), then the two mentions might be in dif-
ferent writing systems. Edit distance calculations
no longer apply directly. One solution would be
full-blown transliteration (Knight and Graehl, 1998),
followed by application of Jaro-Winkler. However,
transliteration systems are complex and require sig-
nificant training resources. We find that a simpler,
low-resource approach works well in practice.
First, we deterministically map both languages to a
common phonetic representation (Tbl. 1).3 Next, we
align the mention pairs with the Hungarian algorithm,
2For multi-token names, we sort the tokens prior to computing
the score, as suggested by Christen (2006).
3This idea is reminiscent of Soundex, which Freeman et al
(2006) used for cross-lingual name matching.
Overlap Active for each bigram in
cbigrams(mi,u)
?
cbigrams(mj,v)
Bigram-Diff-mi Active for each bigram in
cbigrams(mi)? cbigrams(mj)
Bigram-Diff-mj Active for each bigram in
cbigrams(mj)? cbigrams(mi)
Bigram-Len-Diff Value of abs(size(cbigrams(mi)?
cbigrams(mj)))
Big-Edit-Dist Count of token pairs with
Lev(mi,u,mj,v) > 3.0
Total-Edit-Dist Sum of aligned token edit distances
Length Active for one of:
len(mi) > len(mj) or
len(mi) < len(mj) or
len(mi) = len(mj)
Length-Diff abs(len(mi)? len(mj))
Singleton Active if len(mi) = 1
Singleton-Pair Active if len(mi) = len(mj) = 1
Table 2: Cross-language Maxent feature templates for a
whitespace-tokenized mention pair ?mi,mj? with align-
ment Ami,mj . Let (u, v) ? Ami,mj indicate aligned to-
ken indices. Define the following functions for strings:
cbigrams(?) returns the set of character bigrams; len(?) is
the token length; Lev(?, ?) is the Levenshtein edit distance
between two strings. Prior to feature extraction, we add
unique start and end symbols to the mention strings.
which produces a word-to-word alignment Ami,mj .
4
Finally, we build a simple binary Maxent classifier
p(y|mi,mj ;?) that extracts features from the aligned
mentions (Tbl. 2). We learn the parameters ? using a
quasi-Newton procedure with L1 (lasso) regulariza-
tion (Andrew and Gao, 2007).
2.2 Context Mapping and Similarity
Mention strings alone are not always sufficient for
disambiguation. Consider again the simple exam-
ple in Fig. 1. Both doc3 and doc4 reference ?Steve
Jobs? and ?Apple? in the same contexts. Context co-
occurence and/or similarity can thus disambiguate
these two entities from other entities with similar ref-
erences (e.g., ?Steve Jones? or ?Apple Corps?). As
with the mention strings, the contexts may originate
in different writing systems. We consider both high-
and low-resource approaches for mapping contexts to
a common representation.
4The Hungarian algorithm finds an optimal minimum-cost
alignment. For pairwise costs between tokens, we used the Lev-
enshtein edit distance
62
Machine Translation (MT) For the high-resource
setting, if lang(mi) 6=English, then we translate both
mi and its context si to English with an MT system.
We use Phrasal (Cer et al, 2010), a phrase-based
system which, like most public MT systems, lacks a
transliteration module. We believe that this approach
yields the most accurate context mapping for high-
resource language pairs (like English-Arabic).
Polylingual Topic Model (PLTM) The polylin-
gual topic model (PLTM) (Mimno et al, 2009) is
a generative process in which document tuples?
groups of topically-similar documents?share a topic
distribution. The tuples need not be sentence-aligned,
so training data is easier to obtain. For example, one
document tuple might be the set of Wikipedia articles
(in all languages) for Steve Jobs.
Let D be a set of document tuples, where
there is one document in each tuple for each
of L languages. Each language has vocabu-
lary Vl and each document dlt has N
l
t tokens.
We specify a fixed-size set of topics K. The
PLTM generates the document tuples as follows:
Polylingual Topic Model
?t ? Dir(?K) [cross-lingual tuple-topic prior]
?lk ? Dir(?
Vl) [word-topic prior]
for each token wlt,n with n = {1, . . . , N
l
t}:
zt,n ? Mult(?t)
wlt,n ? Mult(?
l
zt,n)
For cross-lingual context mapping, we infer the 1-
best topic assignments for each token in all S mention
contexts. This technique reduces Vl = k for all l.
Moreover, all languages have a common vocabulary:
the set of K topic indices. Since the PLTM is not
a contribution of this paper, we refer the interested
reader to (Mimno et al, 2009) for more details.
After mapping each mention context to a common
representation, we measure context similarity based
on the choice of clustering algorithm.
3 Clustering Algorithms
We incorporate the mention and context similarity
measures into a clustering framework. We consider
two algorithms. The first is hierarchical agglomera-
tive clustering (HAC), with which we assume basic
familiarity (Manning et al, 2008). A shortcoming of
HAC is that a stop threshold must be tuned. To avoid
this requirement, we also consider non-parametric
probabilistic clustering in the form of a Dirichlet pro-
cess mixture model (DPMM) (Antoniak, 1974) .
Both clustering algorithms can be modified to ac-
commodate pairwise constraints. We have observed
better results by encoding mention similarity as a
hard constraint. Context similarity is thus the cluster
distance measure.5
To turn the Jaro-Winkler distance into a hard
boolean constraint, we tuned a threshold ? on held-out
data, i.e., jaro-winkler(mi,mj) ? ? ? mi = mj .
Likewise, the Maxent model is a binary classifier, so
p(y = 1|mi,mj ;?) > 0.5? mi = mj .
In both clustering algorithms, any two chains Ca
and Cb cannot share the same cluster assignment if:
1. Document origin: doc(Ca) = doc(Cb)
2. Semantic type: type(Ca) 6= type(Cb)
3. Mention Match: sim(mi,mj) = false,
wheremi = repr(Ca) andmj = repr(Cb).
The deterministic accessor function repr(Ca) returns
the representative mention of a chain. The heuristic
we used was ?first mention?: the function returns the
earliest mention that appears in the associated docu-
ment. In many languages, the first mention is typi-
cally more complete than later mentions. This heuris-
tic also makes our system less sensitive to within-
document coreference errors.6 The representative
mention only has special status for mention similar-
ity: context similarity considers all mention contexts.
3.1 Constrained Hierarchical Clustering
HAC iteratively merges the ?nearest? clusters accord-
ing to context similarity. In our system, each cluster
context is a bag of wordsW formed from the contexts
of all coreference chains in that cluster. For each word
inW we estimate a unigram Entity Language Model
(ELM) (Raghavan et al, 2004):
P (w) =
countW (w) + ?PV (w)
?
w? countW (w
?) + ?
PV (w) is the unigram probability in all contexts in
the corpus7 and ? is a smoothing parameter. For any
5Specification of a combined similarity measure is an inter-
esting direction for future work.
6These constraints are similar to the pair-filters of Mayfield
et al (2009).
7Recall that after context mapping, all languages have a com-
mon vocabulary V .
63
two entity clusters Ea and Eb, the distance between
PEa and PEb is given by a metric based on the Jensen-
Shannon Divergence (JSD) (Endres and Schindelin,
2003):
dist(PEa , PEb) =
?
2 ? JSD(PEa ||PEb)
=
?
KL(PEa ||M) +KL(M ||PEb)
where KL(PEa ||M) is the Kullback-Leibler diver-
gence andM = 12(PEa + PEb).
We initialize HAC to E = C, i.e., the initial clus-
tering solution is just the set of all coreference chains.
Thenwe remove all links in the HAC proximitymatrix
that violate pairwise cannot-link constraints. During
clustering, we do not merge Ea and Eb if any pair of
chains violates a cannot-link constraint. This proce-
dure propagates the cannot-link constraints (Klein et
al., 2002). To output E, we stop clustering when the
minimum JSD exceeds a stop threshold ?, which is
tuned on a development set.
3.2 Constrained Dirichlet Process Mixture
Model (DPMM)
Instead of tuning a parameter like ?, it would be prefer-
able to let the data dictate the number of entity clus-
ters. We thus consider a non-parametric Bayesian
mixture model where the mixtures are multinomial
distributions over the entity contexts S. Specifically,
we consider a DPMM, which automatically infers
the number of mixtures. Each Ca has an associated
mixture ?a:
Ca|?a ? Mult(?a)
?a|G ? G
G|?,G0 ? DP(?,G0)
? ? Gamma(1, 1)
where ? is the concentration parameter of the DP
prior and G0 is the base distribution with support V .
For our experiments, we set G0 = Dir(pi1, . . . , piV ),
where pii = PV (wi).
For inference, we use the Gibbs sampler of Vla-
chos et al (2009), which can incorporate pairwise
constraints. The sampler is identical to a standard col-
lapsed, token-based sampler, except the conditional
probability p(Ea = E|E?a, Ca) = 0 if Ca cannot
be merged with the chains in clusterE. This property
makes the model non-exchangeable, but in practice
non-exchangeable models are sometimes useful (Blei
and Frazier, 2010). During sampling, we also learn ?
using the auxiliary variable procedure of West (1995),
so the only fixed parameters are those of the vague
Gamma prior. However, we found that these hyper-
parameters were not sensitive.
4 Training Data and Procedures
We trained our system for Arabic-English cross-
lingual entity clustering.8
Maxent Mention Similarity The Maxent mention
similarity model requires a parallel name list for train-
ing. Name pair lists can be obtained from the LDC
(e.g., LDC2005T34 contains nearly 450,000 parallel
Chinese-English names) or Wikipedia (Irvine et al,
2010). We extracted 12,860 name pairs from the par-
allel Arabic-English translation treebanks,9 although
our experiments show that the model achieves high
accuracy with significantly fewer training examples.
We generated a uniform distribution of training ex-
amples by running a Bernoulli trial for each aligned
name pair in the corpus. If the coin was heads, we
replaced the English name with another English name
chosen randomly from the corpus.
MT Context Mapping For the MT context map-
ping method, we trained Phrasal with all data permit-
ted under the NIST OpenMT Ar-En 2009 constrained
track evaluation. We built a 5-gram language model
from the Xinhua and AFP sections of the Gigaword
corpus (LDC2007T07), in addition to all of the target
side training data. In addition to the baseline Phrasal
feature set, we used the lexicalized re-ordering model
of Galley and Manning (2008).
PLTM Context Mapping For PLTM training, we
formed a corpus of 19,139 English-Arabic topically-
aligned Wikipedia articles. Cross-lingual links in
Wikipedia are abundant: as of February 2010, there
were 77.07M cross-lingual links among Wikipedia?s
272 language editions (de Melo and Weikum, 2010).
To increase vocabulary coverage for our ACE2008
evaluation corpus, we added 20,000 document sin-
gletons from the ACE2008 training corpus. The
8We tokenized all English documents with packages from
the Stanford parser (Klein and Manning, 2003). For Arabic
documents, we used Mada (Habash and Rambow, 2005) for
orthographic normalization and clitic segmentation.
9LDC Catalog numbers LDC2009E82 and LDC2009E88.
64
topically-aligned tuples served as ?glue? to share top-
ics between languages, while the ACE documents
distribute those topics over in-domain vocabulary.10
We used the PLTM implementation in Mallet (Mc-
Callum, 2002). We ran the sampler for 10,000 itera-
tions and set the number of topicsK = 512.
5 Task Evaluation Framework
Our experimental design is a cross-lingual extension
of the standard cross-document coreference resolu-
tion task, which appeared in ACE2008 (Strassel et
al., 2008; NIST, 2008). We evaluate name (NAM)
mentions for cross-lingual person (PER) and organi-
zation (ORG) entities. Neither the number nor the
attributes of the entities are known (i.e., the task does
not include a knowledge base). We report results for
both gold and automatic within-document mention
detection and coreference resolution.
Evaluation Metrics We use entity-level evaluation
metrics, i.e., we evaluate the E entity clusters rather
than the mentions. For the gold setting, we report:
? B3 (Bagga and Baldwin, 1998a): Precision and
recall are computed from the intersection of the
hypothesis and reference clusters.
? CEAF (Luo, 2005): Precision and recall are
computed from a maximum bipartite matching
between hypothesis and reference clusters.
? NVI (Reichart and Rappoport, 2009):
Information-theoretic measure that uti-
lizes the entropy of the clusters and their mutual
information. Unlike the commonly-used Varia-
tion of Information (VI) metric, normalized VI
(NVI) is not sensitive to the size of the data set.
For the automatic setting, we must apply a different
metric since the number of system chains may differ
from the reference. We use B3sys (Cai and Strube,
2010), a variant of B3 that was shown to penalize
both twinless reference chains and spurious system
chains more fairly.
Evaluation Corpus The automatic evaluation of
cross-lingual coreference systems requires annotated
10Mimno et al (2009) showed that so long as the proportion
of topically-aligned to non-aligned documents exceeded 0.25,
the topic distributions (as measured by mean Jensen-Shannon
Divergence between distributions) did not degrade significantly.
Docs Tokens Entities Chains Mentions
Arabic 412 178,269 2,594 4,216 9,222
English 414 246,309 2,278 3,950 9,140
Table 3: ACE2008 evaluation corpus PER and ORG entity
statistics. Singleton chains account for 51.4% of the Arabic
data and 46.2% of the English data. Just 216 entities appear
in both languages.
multilingual corpora. Cross-document annotation
is expensive (Strassel et al, 2008), so we chose the
ACE2008 Arabic-English evaluation corpus as a start-
ing point for cross-lingual annotation. The corpus
consists of seven genres sampled from independent
sources over the course of a decade (Tbl. 3). The
corpus provides gold mono-lingual cross-document
coreference annotations for both PER and ORG enti-
ties. Using these annotations as a starting point, we
found and annotated 216 cross-lingual entities.11
Because a similar corpus did not exist for develop-
ment, we split the evaluation corpus into development
and test sections. However, the usual method of split-
ting by document would not confine all mentions of
each entity to one side of the split. We thus split the
corpus by global entity id. We assigned one-third of
the entities to development, and the remaining two-
thirds to test.
6 Comparison to Related Tasks and Work
Our modeling techniques and task formulation can be
viewed as cross-lingual extensions to cross-document
coreference resolution. The classic work on this task
was by Bagga and Baldwin (1998b), who adapted
the Vector Space Model (VSM) (Salton et al, 1975).
Gooi and Allan (2004) found effective algorithmic
extensions like agglomerative clustering. Successful
feature extensions to the VSM for cross-document
coreference have included biographical information
(Mann and Yarowsky, 2003) and syntactic context
(Chen and Martin, 2007). However, neither of these
feature sets generalize easily to the cross-lingual set-
ting with multiple entity types. Fleischman and Hovy
(2004) added a discriminative pairwise mention clas-
sifier to a VSM-like model, much as we do. More
11The annotators were the first author and another fluent
speaker of Arabic. The annotations, corrections, and corpus
split are available at http://www.spencegreen.com/research/.
65
recent work has considered new models for web-scale
corpora (Rao et al, 2010; Singh et al, 2011).
Cross-document work on languages other than En-
glish is scarce. Wang (2005) used a combination of
the VSM and heuristic feature selection strategies to
cluster transliterated Chinese personal names. For
Arabic, Magdy et al (2007) started with the output of
the mention detection and within-document corefer-
ence system of Florian et al (2004). They clustered
the entities incrementally using a binary classifier.
Baron and Freedman (2008) used complete-link ag-
glomerative clustering, wheremerging decisions were
based on a variety of features such as document topic
and name uniqueness. Finally, Sayeed et al (2009)
translated Arabic name mentions to English and then
formed clusters greedily using pairwise matching.
To our knowledge, the cross-lingual entity cluster-
ing task is novel. However, there is significant prior
work on similar tasks:
? Multilingual coreference resolution: Adapt
English within-document coreference models to
other languages (Harabagiu andMaiorano, 2000;
Florian et al, 2004; Luo and Zitouni, 2005).
? Named entity translation: For a non-English
document, produce an inventory of entities in
English. An ACE2007 pilot task (Song and
Strassel, 2008).
? Named entity clustering: Assign semantic
types to text mentions (Collins and Singer, 1999;
Elsner et al, 2009).
? Cross-language name search / entity linking:
Match a single query name against a list of
known multilingual names (knowledge base). A
track in the 2011NIST Text Analysis Conference
(TAC-KBP) evaluation (Aktolga et al, 2008;
McCarley, 2009; Udupa and Khapra, 2010; Mc-
Namee et al, 2011).
Our work incorporates elements of the first three tasks.
Most importantly, we avoid the key element of entity
linking: a knowledge base.
7 Experiments
We performed intrinsic evaluations for both mention
and context similarity. For context similarity, we
analyzed mono-lingual entity clustering, which also
facilitated comparison to prior work on the ACE2008
Genre #Train #Test Accuracy(%)
wb 125 16 87.5
bn 2,720 340 95.6
nw 7,443 930 96.6
all 10,288 1,286 97.1 (+7.55)
Table 4: Cross-lingual mention matching accuracy [%].
The training data contains names from three genres: broad-
cast news (bn), newswire (nw), and weblog (wb). We used
the full training corpus (all) for the cross-lingual clustering
experiments, but the model achieved high accuracy with
significantly fewer training examples (e.g., bn).
CEAF? NVI? B3 ?
#hyp P R F1
Mono-lingual Arabic (#gold=1,721)
HAC 87.2 0.052 1,669 89.8 89.8 89.8
Mono-lingual English (#gold=1,529)
HAC 88.5 0.042 1,536 93.7 89.0 91.4
Table 5: Mono-lingual entity clustering evaluation (test
set, gold within-document processing). Higher scores (?)
are better for CEAF and B3, whereas lower (?) is better
for NVI. #gold indicates the number of reference entities,
whereas #hyp is the size of E.
evaluation set. Our main results are for the new task:
cross-lingual entity clustering.
7.1 Intrinsic Evaluations
Cross-lingual Mention Matching We created a
random 80/10/10 (train, development, test) split of
the Maxent training corpus and evaluated binary clas-
sification accuracy (Tbl. 4). Of the mis-classified
examples, we observed three major error types. First,
the model learns that high edit distance is predictive
of a mismatch. However, singleton strings that do not
match often have a lower edit distance than longer
strings that do match. As a result, singletons often
cause false positives. Second, names that originate in
a third language tend to violate the phonemic corre-
spondences. For example, the model gives a false neg-
ative for a German football team: 	?QK???P 	Q
? ?

??
	
?@
(phonetic mapping: af s kazrslawtrn) versus ?FC
Kaiserslautern.? Finally, names that require trans-
lation are problematic. For example, the classifier
produces a false negative for ?God, gd?
?
= ? ?

<?

@, allh?.
66
#gold = 3,057 CEAF? NVI? B3 ? B3target ? (#gold = 146)
#hyp P R F1 #hyp P R F1
Singleton 64.9 0.165 5,453 100.0 56.1 71.8 1,587 100.0 9.20 16.9
No-context 57.4 0.136 2,216 65.6 75.2 70.1 517 78.3 41.8 54.5
HAC+MT 79.8 0.070 2,783 84.4 86.4 85.4 310 91.7 69.1 78.8
DPMM+MT 74.3 0.122 3,649 89.3 64.1 74.6 634 93.3 24.3 38.6
HAC+PLTM 72.1 0.110 2,746 76.9 77.6 77.3 506 84.4 44.6 58.4
DPMM+PLTM 57.2 0.180 2,609 64.0 62.8 63.4 715 73.9 22.2 34.1
Table 6: Cross-lingual entity clustering (test set, gold within-document processing). B3target is the standard B
3 metric
applied to the subset of target cross-lingual entities in the test set. For CEAF and B3, Singleton is the stronger baseline
due to the high proportion of singleton entities in the corpus. Of course, cross-lingual entities have at least two chains,
so No-context is a better baseline for cross-lingual clustering.
Mono-lingual Entity Clustering For comparison,
we also evaluated our system on a standard mono-
lingual cross-document coreference task (Arabic and
English) (Tbl. 5). We configured the system with
HAC clustering and Jaro-Winkler (within-language)
mention similarity. We built mono-lingual ELMs for
context similarity.
We used two baselines:
? Singleton: E = C, i.e., the cross-lingual clus-
tering solution is just the set of mono-lingual
coreference chains. This is a common baseline
for mono-lingual entity clustering (Baron and
Freedman, 2008).
? No-context: We run HAC with ? =?. There-
fore, E is the set of fully-connected components
in C subject to the pairwise constraints.
For HAC, we manually tuned the stop threshold ?,
the Jaro-Winkler threshold ?, and the ELM smoothing
parameter ? on the development set. For the DPMM,
no development tuning was necessary, and we evalu-
ated a single sample of E taken after 3,000 iterations.
To our knowledge, Baron and Freedman (2008)
reported the only previous results on the ACE2008
data set. However, they only gave gold results for
English, and clustered the entire evaluation corpus
(test+development). To control for the effect of
within-document errors, we considered their gold in-
put (mention detection and within-document coref-
erence resolution) results. They reported B3 for the
two entity types separately: ORG (91.5% F1) and
PER (94.3% F1). The different experimental designs
preclude a precise comparison, but the accuracy of
#gold = 3,057 B3sys ?
#hyp P R F1
Singleton 7,655 100.0 57.1 72.7
No-context 2,918 63.3 71.1 67.0
HAC+MT 3,804 75.6 77.8 76.7
DPMM+MT 4,491 77.1 62.5 69.0
HAC+PLTM 6,353 94.1 62.8 75.3
DPMM+PLTM 3,522 64.6 62.0 63.3
Table 7: Cross-lingual entity clustering (test set, automatic
(Serif) within-document processing). For HAC, we used
the same parameters as the gold setting.
the two systems are at least in the same range.
7.2 Cross-lingual Entity Clustering
We evaluated four system configurations on the new
task: HAC+MT, HAC+PLTM, DPMM+MT, and
DPMM+PLTM. First, we established an upper bound
by assuming gold within-document mention detection
and coreference resolution (Tbl. 6). This setting iso-
lated the new cross-lingual clustering methods from
within-document processing errors. Then we evalu-
ated with Serif (automatic) within-document process-
ing (Tbl. 7). This second experiment replicated an
application setting. We used the same baselines and
tuning procedures as in the mono-lingual clustering
experiment.
Results In the gold setting, HAC+MTproduces the
best results, as expected. The dimensionality reduc-
tion of the vocabulary imposed by PLTM significantly
reduces accuracy, but HAC+PLTM still exceeds the
67
baseline. We tried increasing the number of PLTM
topics k, but did not observe an improvement in task
accuracy. For both context-mapping methods, the
DPMM suffers from low-recall. Upon inspection, the
clustering solution of DPMM+MT contains a high
proportion of singleton hypotheses, suggesting that
the model finds lower similarity in the presence of a
larger vocabulary. When the context vocabulary con-
sists of PLTM topics, larger clusters are discovered
(DPMM+PLTM).
The effect of dimensionality reduction is also appar-
ent in the clustering solutions of the PLTM models.
For example, for the Serif output, DPMM+PLTM
produces a cluster consisting of ?White House?, ?Sen-
ate?, ?House of Representatives?, and ?Parliament?.
Arabic mentions of the latter three entities pass the
pairwise mention similarity constraints due to the
word ??m.? ?council?, which appears in text mentions
for all three legislative bodies. A cross-language
matching error resulted in the linking of ?White
House?, and the reduced granularity of the contexts
precluded further disambiguation. Of course, these
entities probably appear in similar contexts.
The caveat with the Serif results in Tbl. 7 is that
3,251 of the 7,655 automatic coreference chains are
not in the reference. Consequently, the evaluation is
dominated by the penalty for spurious system coref-
erence chains. Nonetheless, all models except for
DPMM+PLTM exceed the baselines, and the rela-
tionships between models depicted in the gold exper-
iments hold for the this setting.
8 Conclusion
Cross-lingual entity clustering is a natural step to-
ward more robust natural language understanding.
We proposed pipeline models that make clustering
decisions based on cross-lingual similarity. We inves-
tigated two methods for mapping documents in differ-
ent languages to a common representation: MT and
the PLTM. Although MT may achieve more accurate
results for some language pairs, the PLTM training
resources (e.g., Wikipedia) are readily available for
many languages. As for the clustering algorithms,
HAC appears to perform better than the DPMM on
our dataset, but this may be due to the small corpus
size. The instance-level constraints represent tenden-
cies that could be learned from larger amounts of data.
With more data, we might be able to relax the con-
straints and use an exchangeable DPMM,whichmight
be more effective. Finally, we have shown that sig-
nificant quantities of within-document errors cascade
into the cross-lingual clustering phase. As a result,
we plan a model that clusters the mentions directly,
thus removing the dependence on within-document
coreference resolution.
In this paper, we have set baselines and proposed
models that significantly exceeded those baselines.
The best model improved upon the cross-lingual en-
tity baseline by 24.3% F1. This result was achieved
without a knowledge base, which is required by previ-
ous approaches to cross-lingual entity linking. More
importantly, our techniques can be used to extend
existing cross-document entity clustering systems for
the increasingly multilingual web.
AcknowledgmentsWe thank Jason Eisner, David Mimno,
Scott Miller, Jim Mayfield, and Paul McNamee for helpful
discussions. This work was started during the SCALE
2010 summer workshop at Johns Hopkins. The first author
is supported by a National Science Foundation Graduate
Fellowship.
References
E. Aktolga, M. Cartright, and J. Allan. 2008. Cross-document
cross-lingual coreference retrieval. In CIKM.
G. Andrew and J. Gao. 2007. Scalable training of L1-regularized
log-linear models. In ICML.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152?1174.
A. Bagga and B. Baldwin. 1998a. Algorithms for scoring coref-
erence chains. In LREC.
A. Bagga and B. Baldwin. 1998b. Entity-based cross-document
coreferencing using the vector space model. In COLING-ACL.
A. Baron and M. Freedman. 2008. Who is Who and What
is What: Experiments in cross-document co-reference. In
EMNLP.
D. Blei and P. Frazier. 2010. Distance dependent Chinese restau-
rant processes. In ICML.
J. Cai and M. Strube. 2010. Evaluation metrics for end-to-
end coreference resolution systems. In Proceedings of the
SIGDIAL 2010 Conference.
D. Cer, M. Galley, D. Jurafsky, and C. D.Manning. 2010. Phrasal:
A statistical machine translation toolkit for exploring new
model features. In HLT-NAACL, Demonstration Session.
E. Charniak. 2001. Unsupervised learning of name structure
from coreference data. In NAACL.
Y. Chen and J. Martin. 2007. Towards robust unsupervised
personal name disambiguation. In EMNLP-CoNLL.
68
P. Christen. 2006. A comparison of personal name matching:
Techniques and practical issues. Technical Report TR-CS-06-
02, Australian National University.
M. Collins and Y. Singer. 1999. Unsupervised models for named
entity classification. In EMNLP.
G. de Melo and G. Weikum. 2010. Untangling the cross-lingual
link structure of Wikipedia. In ACL.
M. Elsner, E. Charniak, and M. Johnson. 2009. Structured
generative models for unsupervised named-entity clustering.
In HLT-NAACL.
D. M. Endres and J. E. Schindelin. 2003. A new metric for
probability distributions. IEEE Transactions on Information
Theory, 49(7):1858 ? 1860.
M. Fleischman and E. Hovy. 2004. Multi-document person name
resolution. In ACL Workshop on Reference Resolution and its
Applications.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, et al
2004. A statistical model for multilingual entity detection and
tracking. In HLT-NAACL.
A. T. Freeman, S. L. Condon, and C. M. Ackerman. 2006. Cross
linguistic name matching in English and Arabic: a one to
many mapping extension of the Levenshtein edit distance
algorithm. In HLT-NAACL.
M. Galley and C. D. Manning. 2008. A simple and effective
hierarchical phrase reordering model. In EMNLP.
C. H. Gooi and J. Allan. 2004. Cross-document coreference on
a large scale corpus. In HLT-NAACL.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-of-
speech tagging and morphological disambiguation in one fell
swoop. In ACL.
S. M. Harabagiu and S. J. Maiorano. 2000. Multilingual corefer-
ence resolution. In ANLP.
A. Irvine, C. Callison-Burch, and A. Klementiev. 2010. Translit-
erating from all languages. In AMTA.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. Klein, S. D. Kamvar, and C. D.Manning. 2002. From instance-
level constraints to space-level constraints: Making the most
of prior knowledge in data clustering. In ICML.
K. Knight and J. Graehl. 1998. Machine transliteration. Compu-
tational Linguistics, 24:599?612.
X. Luo and I. Zitouni. 2005. Multi-lingual coreference resolution
with syntactic features. In HLT-EMNLP.
X. Luo. 2005. On coreference resolution performance metrics.
In HLT-EMNLP.
W. Magdy, K. Darwish, O. Emam, and H. Hassan. 2007. Arabic
cross-document person name normalization. In Workshop on
Computational Approaches to Semitic Languages.
G. S. Mann and D. Yarowsky. 2003. Unsupervised personal
name disambiguation. In NAACL.
C. D. Manning, P. Raghavan, and H. Sch?tze. 2008. Introduction
to Information Retrieval. Cambridge University Press.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed, et al
2009. Cross-document coreference resolution: A key technol-
ogy for learning by reading. In AAAI Spring Symposium on
Learning by Reading and Learning to Read.
A. K. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
J. S. McCarley. 2009. Cross language name matching. In SIGIR.
P. McNamee, J. Mayfield, D. Lawrie, D.W. Oard, and D. Doer-
mann. 2011. Cross-language entity linking. In IJCNLP.
D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and
A. McCallum. 2009. Polylingual topic models. In EMNLP.
NIST. 2008. Automatic Content Extraction 2008 evaluation
plan (ACE2008): Assessment of detection and recognition
of entities and relations within and across documents. Tech-
nical Report rev. 1.2d, National Institute of Standards and
Technology (NIST), 8 August.
E. H. Porter and W. E. Winkler, 1997. Approximate String Com-
parison and its Effect on an Advanced Record Linkage System,
chapter 6, pages 190?199. U.S. Bureau of the Census.
H. Raghavan, J. Allan, and A. McCallum. 2004. An explo-
ration of entity models, collective classification and relation
description. In KDD Workshop on Link Analysis and Group
Detection.
L. Ramshaw, E. Boschee, M. Freedman, J. MacBride,
R. Weischedel, and A. Zamanian. 2011. SERIF language
processing?effective trainable language understanding. In
J. Olive et al, editors,Handbook of Natural Language Process-
ing and Machine Translation: DARPA Global Autonomous
Language Exploitation, pages 636?644. Springer.
D. Rao, P. McNamee, and M. Dredze. 2010. Streaming cross
document entity coreference resolution. In COLING.
R. Reichart and A. Rappoport. 2009. The NVI clustering evalu-
ation measure. In CoNLL.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model
for automatic indexing. CACM, 18:613?620, November.
A. Sayeed, T. Elsayed, N. Garera, D. Alexander, T. Xu, et al
2009. Arabic cross-document coreference detection. In ACL-
IJCNLP, Short Papers.
S. Singh, A. Subramanya, F. Pereira, and A. McCallum. 2011.
Large-scale cross-document coreference using distributed in-
ference and hierarchical models. In ACL.
Z. Song and S. Strassel. 2008. Entity translation and alignment
in the ACE-07 ET task. In LREC.
S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda.
2008. Linguistic resources and evaluation techniques for
evaluation of cross-document automatic content extraction.
In LREC.
R. Udupa and M. M. Khapra. 2010. Improving the multilin-
gual user experience of Wikipedia using cross-language name
search. In HLT-NAACL.
A. Vlachos, A. Korhonen, and Z. Ghahramani. 2009. Unsuper-
vised and constrained Dirichlet process mixture models for
verb clustering. In Proc. of the Workshop on Geometrical
Models of Natural Language Semantics.
H. Wang. 2005. Cross-document transliterated personal name
coreference resolution. In L. Wang and Y. Jin, editors, Fuzzy
Systems and Knowledge Discovery, volume 3614 of Lecture
Notes in Computer Science, pages 11?20. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
69
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 146?155,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Class-Based Agreement Model for
Generating Accurately Inflected Translations
Spence Green
Computer Science Department, Stanford University
spenceg@stanford.edu
John DeNero
Google
denero@google.com
Abstract
When automatically translating from a weakly
inflected source language like English to a tar-
get language with richer grammatical features
such as gender and dual number, the output
commonly contains morpho-syntactic agree-
ment errors. To address this issue, we present
a target-side, class-based agreement model.
Agreement is promoted by scoring a sequence
of fine-grained morpho-syntactic classes that
are predicted during decoding for each transla-
tion hypothesis. For English-to-Arabic transla-
tion, our model yields a +1.04 BLEU average
improvement over a state-of-the-art baseline.
The model does not require bitext or phrase ta-
ble annotations and can be easily implemented
as a feature in many phrase-based decoders.
1 Introduction
Languages vary in the degree to which surface forms
reflect grammatical relations. English is a weakly in-
flected language: it has a narrow verbal paradigm, re-
stricted nominal inflection (plurals), and only the ves-
tiges of a case system. Consequently, translation into
English?which accounts for much of the machine
translation (MT) literature (Lopez, 2008)?often in-
volves some amount of morpho-syntactic dimension-
ality reduction. Less attention has been paid to what
happens during translation from English: richer gram-
matical features such as gender, dual number, and
overt case are effectively latent variables that must
be inferred during decoding. Consider the output of
Google Translate for the simple English sentence in
Fig. 1. The correct translation is a monotone mapping
of the input. However, in Arabic, SVO word order
requires both gender and number agreement between
the subject

?PAJ
??@ ?the car? and verb I. ?
	
YK
 ?go?. The
MT system selects the correct verb stem, but with
masculine inflection. Although the translation has
(1)

?PAJ
??@
the-carsg.def.fem
I. ?
	
YK

gosg.masc

??Q??.
with-speedsg.fem
The car goes quickly
Figure 1: Ungrammatical Arabic output of Google Trans-
late for the English input The car goes quickly. The subject
should agree with the verb in both gender and number, but
the verb has masculine inflection. For clarity, the Arabic
tokens are arranged left-to-right.
the correct semantics, it is ultimately ungrammatical.
This paper addresses the problem of generating text
that conforms to morpho-syntactic agreement rules.
Agreement relations that cross statistical phrase
boundaries are not explicitly modeled in most phrase-
based MT systems (Avramidis and Koehn, 2008).
We address this shortcoming with an agreement
model that scores sequences of fine-grained morpho-
syntactic classes. First, bound morphemes in transla-
tion hypotheses are segmented. Next, the segments
are labeled with classes that encode both syntactic
category information (i.e., parts of speech) and gram-
matical features such as number and gender. Finally,
agreement is promoted by scoring the predicted class
sequences with a generative Markov model.
Our model scores hypotheses during decoding. Un-
like previous models for scoring syntactic relations,
our model does not require bitext annotations, phrase
table features, or decoder modifications. The model
can be implemented using the feature APIs of popular
phrase-based decoders such as Moses (Koehn et al,
2007) and Phrasal (Cer et al, 2010).
Intuition might suggest that the standard n-gram
language model (LM) is sufficient to handle agree-
ment phenomena. However, LM statistics are sparse,
and they are made sparser by morphological varia-
tion. For English-to-Arabic translation, we achieve
a +1.04 BLEU average improvement by tiling our
model on top of a large LM.
146
It has also been suggested that this setting requires
morphological generation because the bitext may not
contain all inflected variants (Minkov et al, 2007;
Toutanova et al, 2008; Fraser et al, 2012). However,
using lexical coverage experiments, we show that
there is ample room for translation quality improve-
ments through better selection of forms that already
exist in the translation model.
2 A Class-based Model of Agreement
2.1 Morpho-syntactic Agreement
Morpho-syntactic agreement refers to a relationship
between two sentence elements a and b that must
have at least one matching grammatical feature.1
Agreement relations tend to be defined for partic-
ular syntactic configurations such as verb-subject,
noun-adjective, and pronoun-antecedent. In some
languages, agreement affects the surface forms of the
words. For example, from the perspective of gener-
ative grammatical theory, the lexicon entry for the
Arabic nominal

?PAJ
??@ ?the car? contains a feminine
gender feature. When this nominal appears in the sub-
ject argument position, the verb-subject agreement
relationship triggers feminine inflection of the verb.
Our model treats agreement as a sequence of
scored, pairwise relations between adjacent words.
Of course, this assumption excludes some agreement
phenomena, but it is sufficient for many common
cases. We focus on English-Arabic translation as
an example of a translation direction that expresses
substantially more morphological information in the
target. These relations are best captured in a target-
side model because they are mostly unobserved (from
lexical clues) in the English source.
The agreement model scores sequences of morpho-
syntactic word classes, which express grammatical
features relevant to agreement. The model has three
components: a segmenter, a tagger, and a scorer.
2.2 Morphological Segmentation
Segmentation is a procedure for converting raw sur-
face forms to component morphemes. In some lan-
guages, agreement relations exist between bound
morphemes, which are syntactically independent yet
phonologically dependent morphemes. For example,
1We use morpho-syntactic and grammatical agreement inter-
changeably, as is common in the literature.
	



Pron+Fem+Sg Verb+Masc+3+Pl Prt Conj
andwillthey writeit
Figure 2: Segmentation and tagging of the Arabic token
A?
	
E?J.

J?J
?? ?and they will write it?. This token has four seg-
ments with conflicting grammatical features. For example,
the number feature is singular for the pronominal object
and plural for the verb. Our model segments the raw to-
ken, tags each segment with a morpho-syntactic class (e.g.,
?Pron+Fem+Sg?), and then scores the class sequences.
the single raw token in Fig. 2 contains at least four
grammatically independent morphemes. Because the
morphemes bear conflicting grammatical features and
basic parts of speech (POS), we need to segment the
token before we can evaluate agreement relations.2
Segmentation is typically applied as a bitext pre-
processing step, and there is a rich literature on the
effect of different segmentation schemata on transla-
tion quality (Koehn and Knight, 2003; Habash and
Sadat, 2006; El Kholy and Habash, 2012). Unlike pre-
vious work, we segment each translation hypothesis
as it is generated (i.e., during decoding). This permits
greater modeling flexibility. For example, it may be
useful to count tokens with bound morphemes as a
unit during phrase extraction, but to score segmented
morphemes separately for agreement.
We treat segmentation as a character-level se-
quence modeling problem and train a linear-chain
conditional random field (CRF) model (Lafferty et
al., 2001). As a pre-processing step, we group con-
tiguous non-native characters (e.g., Latin characters
in Arabic text). The model assigns four labels:
? I: Continuation of a morpheme
? O: Outside morpheme (whitespace)
? B: Beginning of a morpheme
? F: Non-native character(s)
2Segmentation also improves translation of compounding
languages such as German (Dyer, 2009) and Finnish (Macherey
et al, 2011).
147
Translation Model
e Target sequence of I words
f Source sequence of J words
a Sequence ofK phrase alignments for ?e, f?
? Permutation of the alignments for target word order e
h Sequence ofM feature functions
? Sequence of learned weights for theM features
H A priority queue of hypotheses
Class-based Agreement Model
t ? T Set of morpho-syntactic classes
s ? S Set of all word segments
?seg Learned weights for the CRF-based segmenter
?tag Learned weights for the CRF-based tagger
?o, ?t CRF potential functions (emission and transition)
? Sequence of I target-side predicted classes
pi T dimensional (log) prior distribution over classes
s? Sequence of l word segments
? Model state: a tagged segment ?s, t?
Figure 3: Notation used in this paper. The convention eIi
indicates a subsequence of a length I sequence.
The features are indicators for (character, position,
label) triples for a five character window and bigram
label transition indicators.
This formulation is inspired by the classic ?IOB?
text chunking model (Ramshaw and Marcus, 1995),
which has been previously applied to Chinese seg-
mentation (Peng et al, 2004). It can be learned from
gold-segmented data, generally applies to languages
with bound morphemes, and does not require a hand-
compiled lexicon.3 Moreover, it has only four labels,
so Viterbi decoding is very fast. We learn the param-
eters ?seg using a quasi-Newton (QN) procedure with
l1 (lasso) regularization (Andrew and Gao, 2007).
2.3 Morpho-syntactic Tagging
After segmentation, we tag each segment with a fine-
grained morpho-syntactic class. For this task we also
train a standard CRF model on full sentences with
gold classes and segmentation. We use the same QN
procedure as before to obtain ?tag.
A translation derivation is a tuple ?e, f, a? where
e is the target, f is the source, and a is an alignment
between the two. The CRF tagging model predicts a
target-side class sequence ??
?? = arg max
?
I?
i=1
?tag ? {?o(?i, i, e) + ?t(?i, ?i?1)}
where further notation is defined in Fig. 3.
3Mada, the standard tool for Arabic segmentation (Habash
and Rambow, 2005), relies on a manually compiled lexicon.
Set of Classes The tagger assignsmorpho-syntactic
classes, which are coarse POS categories refined with
grammatical features such as gender and definiteness.
The coarse categories are the universal POS tag set
described by Petrov et al (2012). More than 25 tree-
banks (in 22 languages) can be automatically mapped
to this tag set, which includes ?Noun? (nominals),
?Verb? (verbs), ?Adj? (adjectives), and ?ADP? (pre-
and post-positions). Many of these treebanks also
contain per-token morphological annotations. It is
easy to combine the coarse categories with selected
grammatical annotations.
For Arabic, we used the coarse POS tags plus
definiteness and the so-called phi features (gender,
number, and person).4 For example,

?PAJ
??@ ?the
car? would be tagged ?Noun+Def+Sg+Fem?. We
restricted the set of classes to observed combinations
in the training data, so the model implicitly disallows
incoherent classes like ?Verb+Def?.
Features The tagging CRF includes emission fea-
tures ?o that indicate a class ?i appearing with various
orthographic characteristics of the word sequence
being tagged. In typical CRF inference, the entire
observation sequence is available throughout infer-
ence, so these features can be scored on observed
words in an arbitrary neighborhood around the cur-
rent position i. However, we conduct CRF inference
in tandem with the translation decoding procedure
(?3), creating an environment in which subsequent
words of the observation are not available; the MT
system has yet to generate the rest of the translation
when the tagging features for a position are scored.
Therefore, we only define emission features on the
observed words at the current and previous positions
of a class: ?o(?i, ei, ei?1).
The emission features are word types, prefixes and
suffixes of up to three characters, and indicators for
digits and punctuation. None of these features are
language specific.
Bigram transition features ?t encode local agree-
ment relations. For example, the model learns that the
Arabic class ?Noun+Fem? is followed by ?Adj+Fem?
and not ?Adj+Masc? (noun-adjective gender agree-
ment).
4Case is also relevant to agreement in Arabic, but it is mostly
indicated by diacritics, which are absent in unvocalized text.
148
2.4 Word Class Sequence Scoring
The CRF tagger model defines a conditional distribu-
tion p(? |e; ?tag) for a class sequence ? given a sen-
tence e and model parameters ?tag. That is, the sam-
ple space is over class?not word?sequences. How-
ever, in MT, we seek a measure of sentence quality
q(e) that is comparable across different hypotheses
on the beam (much like the n-gram language model
score). Discriminative model scores have been used
as MT features (Galley and Manning, 2009), but we
obtained better results by scoring the 1-best class se-
quences with a generative model. We trained a simple
add-1 smoothed bigram language model over gold
class sequences in the same treebank training data:
q(e) = p(?) =
I?
i=1
p(?i|?i?1)
We chose a bigram model due to the aggressive
recombination strategy in our phrase-based decoder.
For contexts in which the LM is guaranteed to back
off (for instance, after an unseen bigram), our decoder
maintains only theminimal state needed (perhaps only
a single word). In less restrictive decoders, higher
order scoring models could be used to score longer-
distance agreement relations.
We integrate the segmentation, tagging, and scor-
ing models into a self-contained component in the
translation decoder.
3 Inference during Translation Decoding
Scoring the agreement model as part of translation
decoding requires a novel inference procedure. Cru-
cially, the inference procedure does not measurably
affect total MT decoding time.
3.1 Phrase-based Translation Decoding
We consider the standard phrase-based approach to
MT (Och and Ney, 2004). The distribution p(e|f) is
modeled directly using a log-linear model, yielding
the following decision rule:
e? = arg max
e,a,?
{
M?
m=1
?mhm(e, f, a,?)
}
(1)
This decoding problem is NP-hard, thus a beam search
is often used (Fig. 4). The beam search relies on three
operations, two of which affect the agreement model:
Input: implicitly defined search space
generate initial hypotheses and add toH
setHfinal to ?
whileH is not empty:
setHext to ?
for each hypothesis ? inH:
if ? is a goal hypothesis:
add ? toHfinal
else Extend ? and add toHext IScore agreement
Recombine and PruneHext
setH toHext
Output: argmax ofHfinal
Figure 4: Breadth-first beam search algorithm of Och and
Ney (2004). Typically, a hypothesis stackH is maintained
for each unique source coverage set.
Input: (eI1, n, is_goal)
run segmenter on attachment eIn+1 to get s?
L
1
get model state ? = ?s, t? for translation prefix en1
initialize pi to ??
set pi(t) = 0
compute ?? from parameters ?s, s?L1 , pi, is_goal?
compute q(eIn+1) = p(?
?) under the generative LM
set model state ?new = ?s?L, ??L? for prefix e
I
1
Output: q(eIn+1)
Figure 5: Procedure for scoring agreement for each hy-
pothesis generated during the search algorithm of Fig. 4.
In the extended hypothesis eI1, the index n+ 1 indicates
the start of the new attachment.
? Extend a hypothesis with a new phrase pair
? Recombine hypotheses with identical states
We assume familiarity with these operations, which
are described in detail in (Och and Ney, 2004).
3.2 Agreement Model Inference
The class-based agreement model is implemented as
a feature function hm in Eq. (1). Specifically, when
Extend generates a new hypothesis, we run the algo-
rithm shown in Fig. 5. The inputs are a translation
hypothesis eI1, an index n distinguishing the prefix
from the attachment, and a flag indicating if their
concatenation is a goal hypothesis.
The beam search maintains state for each deriva-
tion, the score of which is a linear combination of
the feature values. States in this program depend on
some amount of lexical history. With a trigram lan-
guage model, the state might be the last two words
of the translation prefix. Recombine can be applied
to any two hypotheses with equivalent states. As a
149
result, two hypotheses with different full prefixes?
and thus potentially different sequences of agreement
relations?can be recombined.
Incremental Greedy Decoding Decoding with
the CRF-based tagger model in this setting requires
some slight modifications to the Viterbi algorithm.
We make a greedy approximation that permits recom-
bination and works well in practice. The agreement
model state is the last tagged segment ?s, t? of the
concatenated hypothesis. We tag a new attachment by
assuming a prior distribution pi over the starting posi-
tion such that pi(t) = 0 and ?? for all other classes,
a deterministic distribution in the tropical semiring.
This forces the Viterbi path to go through t. We only
tag the final boundary symbol for goal hypotheses.
To accelerate tagger decoding in our experiments,
we also used tagging dictionaries for frequently ob-
served word types. For each word type observed more
than 100 times in the training data, we restricted the
set of possible classes to the set of observed classes.
3.3 Translation Model Features
The agreement model score is one decoder feature
function. The output of the procedure in Fig. 5 is the
log probability of the class sequence of each attach-
ment. Summed over all attachments, this gives the
log probability of the whole class sequence.
We also add a new length penalty feature. To dis-
criminate between hypotheses that might have the
same number of raw tokens, but different underlying
segmentations, we add a penalty equal to the length
difference between the segmented and unsegmented
attachments |s?L1 | ? |e
I
n+1|.
4 Related Work
We compare our class-based model to previous ap-
proaches to scoring syntactic relations in MT.
Unification-based Formalisms Agreement rules
impose syntactic and semantic constraints on the
structure of sentences. A principled way to model
these constraints is with a unification-based gram-
mar (UBG). Johnson (2003) presented algorithms for
learning and parsing with stochastic UBGs. However,
training data for these formalisms remains extremely
limited, and it is unclear how to learn such knowledge-
rich representations from unlabeled data. One partial
solution is to manually extract unification rules from
phrase-structure trees. Williams and Koehn (2011)
annotated German trees, and extracted translation
rules from them. They then specified manual unifi-
cation rules, and applied a penalty according to the
number of unification failures in a hypothesis. In
contrast, our class-based model does not require any
manual rules and scores similar agreement phenom-
ena as probabilistic sequences.
Factored Translation Models Factored transla-
tion models (Koehn and Hoang, 2007) facilitate a
more data-oriented approach to agreement modeling.
Words are represented as a vector of features such as
lemma and POS. The bitext is annotated with separate
models, and the annotations are saved during phrase
extraction. Hassan et al (2007) noticed that the target-
side POS sequences could be scored, much as we do
in this work. They used a target-side LM over Combi-
natorial Categorial Grammar (CCG) supertags, along
with a penalty for the number of operator violations,
and also modified the phrase probabilities based on
the tags. However, Birch et al (2007) showed that
this approach captures the same re-ordering phenom-
ena as lexicalized re-ordering models, which were
not included in the baseline. Birch et al (2007) then
investigated source-side CCG supertag features, but
did not show an improvement for Dutch-English.
Subotin (2011) recently extended factored transla-
tion models to hierarchical phrase-based translation
and developed a discriminative model for predicting
target-side morphology in English-Czech. His model
benefited from gold morphological annotations on
the target-side of the 8M sentence bitext.
In contrast to these methods, our model does not af-
fect phrase extraction and does not require annotated
translation rules.
Class-based LMs Class-based LMs (Brown et al,
1992) reduce lexical sparsity by placing words in
equivalence classes. They have been widely used
for speech recognition, but not for MT. Och (1999)
showed a method for inducing bilingual word classes
that placed each phrase pair into a two-dimensional
equivalence class. To our knowledge, Uszkoreit and
Brants (2008) are the only recent authors to show an
improvement in a state-of-the-art MT system using
class-based LMs. They used a classical exchange al-
gorithm for clustering, and learned 512 classes from
150
a large monolingual corpus. Then they mixed the
classes into a word-based LM. However, both Och
(1999) and Uszkoreit and Brants (2008) relied on
automatically induced classes. It is unclear if their
classes captured agreement information.
Monz (2011) recently investigated parameter es-
timation for POS-based language models, but his
classes did not include inflectional features.
Target-Side Syntactic LMs Our agreement model
is a form of syntactic LM, of which there is a long
history of research, especially in speech processing.5
Syntactic LMs have traditionally been too slow for
scoring during MT decoding. One exception was
the quadratic-time dependency language model pre-
sented by Galley and Manning (2009). They applied
a quadratic time dependency parser to every hypothe-
sis during decoding. However, to achieve quadratic
running time, they permitted ill-formed trees (e.g.,
parses with multiple roots). More recently, Schwartz
et al (2011) integrated a right-corner, incremental
parser into Moses. They showed a large improve-
ment for Urdu-English, but decoding slowed by three
orders of magnitude.6 In contrast, our class-based
model encodes shallow syntactic information without
a noticeable effect on decoding time.
Our model can be viewed as a way to score local
syntactic relations without extensive decoder modifi-
cations. For long-distance relations, Shen et al (2010)
proposed a new decoder that generates target-side
dependency trees. The target-side structure enables
scoring hypotheses with a trigram dependency LM.
5 Experiments
We first evaluate the Arabic segmenter and tagger
components independently, then provide English-
Arabic translation quality results.
5.1 Intrinsic Evaluation of Components
Experimental Setup All experiments use the Penn
Arabic Treebank (ATB) (Maamouri et al, 2004) parts
1?3 divided into training/dev/test sections according
to the canonical split (Rambow et al, 2005).7
5See (Zhang, 2009) for a comprehensive survey.
6In principle, their parser should run in linear time. An imple-
mentation issue may account for the decoding slowdown. (p.c.)
7LDC catalog numbers: LDC2008E61 (ATBp1v4),
LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).
Full (%) Incremental (%)
Segmenter 98.6 ?
Tagger 96.3 96.2
Table 1: Intrinsic evaluation accuracy [%] (development
set) for Arabic segmentation and tagging.
The ATB contains clitic-segmented text with per-
segment morphological analyses (in addition to
phrase-structure trees, which we discard). For train-
ing the segmenter, we used markers in the vocalized
section to construct the IOB character sequences. For
training the tagger, we automatically converted the
ATB morphological analyses to the fine-grained class
set. This procedure resulted in 89 classes.
For the segmentation evaluation, we report per-
character labeling accuracy.8 For the tagger, we re-
port per-token accuracy.
Results Tbl. 1 shows development set accuracy for
two settings. Full is a standard evaluation in which
features may be defined over the whole sentence. This
includes next-character segmenter features and next-
word tagger features. Incremental emulates the MT
setting in which the models are restricted to current
and previous observation features. Since the seg-
menter operates at the character level, we can use
the same feature set. However, next-observation fea-
tures must be removed from the tagger. Nonetheless,
tagging accuracy only decreases by 0.1%.
5.2 Translation Quality
Experimental Setup Our decoder is based on the
phrase-based approach to translation (Och and Ney,
2004) and contains various feature functions includ-
ing phrase relative frequency, word-level alignment
statistics, and lexicalized re-ordering models (Till-
mann, 2004; Och et al, 2004). We tuned the feature
weights on a development set using lattice-based min-
imum error rate training (MERT) (Macherey et al,
The data was pre-processed with packages from the Stanford
Arabic parser (Green and Manning, 2010). The corpus split is
available at http://nlp.stanford.edu/projects/arabic.shtml.
8We ignore orthographic re-normalization performed by the
annotators. For example, they converted the contraction ???? ll
back to ??@'?? l Al. As a result, we can report accuracy since
the guess and gold segmentations have equal numbers of non-
whitespace characters.
151
MT04 (tune) MT02 MT03 MT05 Avg
Baseline 18.14 23.87 18.88 22.60
+POS 18.11 ?0.03 23.65 ?0.22 18.99 +0.11 22.29 ?0.31 ?0.17
+POS+Agr 18.86 +0.72 24.84 +0.97 20.26 +1.38 23.48 +0.88 +1.04
genres nw nw nw nw
#sentences 1353 728 663 1056 2447
Table 2: Translation quality results (BLEU-4 [%]) for newswire (nw) sets. Avg is the weighted averaged (by number of
sentences) of the individual test set gains. All improvements are statistically significant at p ? 0.01.
MT06 MT08 Avg
Baseline 14.68 14.30
+POS 14.57 ?0.11 14.30 +0.0 ?0.06
+POS+Agr 15.04 +0.36 14.49 +0.19 +0.29
genres nw,bn,ng nw,ng,wb
#sentences 1797 1360 3157
Table 3: Mixed genre test set results (BLEU-4 [%]). The
MT06 result is statistically significant at p ? 0.01; MT08
is significant at p ? 0.02. The genres are: nw, broadcast
news (bn), newsgroups (ng), and weblog (wb).
2008). For each set of results, we initialized MERT
with uniform feature weights.
We trained the translation model on 502 million
words of parallel text collected from a variety of
sources, including theWeb. Word alignments were in-
duced using a hidden Markov model based alignment
model (Vogel et al, 1996) initialized with bilexical
parameters from IBM Model 1 (Brown et al, 1993).
Both alignment models were trained using two itera-
tions of the expectation maximization algorithm. Our
distributed 4-gram language model was trained on
600 million words of Arabic text, also collected from
many sources including the Web (Brants et al, 2007).
For development and evaluation, we used the NIST
Arabic-English data sets, each of which contains one
set of Arabic sentences and multiple English refer-
ences. To reverse the translation direction for each
data set, we chose the first English reference as the
source and the Arabic as the reference.
The NIST sets come in two varieties: newswire
(MT02-05) and mixed genre (MT06,08). Newswire
contains primarily Modern Standard Arabic (MSA),
while the mixed genre data sets also contain tran-
scribed speech and web text. Since the ATB contains
MSA, and significant lexical and syntactic differences
may exist between MSA and the mixed genres, we
achieved best results by tuning on MT04, the largest
newswire set.
We evaluated translation quality with BLEU-4 (Pa-
pineni et al, 2002) and computed statistical signifi-
cance with the approximate randomization method
of Riezler and Maxwell (2005).9
6 Discussion of Translation Results
Tbl. 2 shows translation quality results on newswire,
while Tbl. 3 contains results for mixed genres. The
baseline is our standard system feature set. For
comparison, +POS indicates our class-based model
trained on the 11 coarse POS tags only (e.g., ?Noun?).
Finally, +POS+Agr shows the class-based model
with the fine-grained classes (e.g., ?Noun+Fem+Sg?).
The best result?a +1.04 BLEU average gain?
was achieved when the class-based model training
data, MT tuning set, and MT evaluation set contained
the same genre. We realized smaller, yet statistically
significant, gains on the mixed genre data sets. We
tried tuning on both MT06 and MT08, but obtained
insignificant gains. In the next section, we investigate
this issue further.
Tuning with a Treebank-Trained Feature The
class-based model is trained on the ATB, which is pre-
dominantly MSA text. This data set is syntactically
regular, meaning that it does not have highly dialectal
content, foreign scripts, disfluencies, etc. Conversely,
the mixed genre data sets contain more irregulari-
ties. For example, 57.4% of MT06 comes from non-
newswire genres. Of the 764 newsgroup sentences,
112 contain some Latin script tokens, while others
contain very little morphology:
9With the implementation of Clark et al (2011), available at:
http://github.com/jhclark/multeval.
152
(2) ?


??
	
g@
mix
1/2
1/2
H. ??
cup
?
	
g
vinegar
hA
	
?

K
apple
Mix 1/2 cup apple vinegar
(3)

@YK.
start
l .
?A
	
KQK.
program
? 	P?J
?
miozik
?

A?
maatsh
MusicMatch
MusicMatch
Start the program music match (MusicMatch)
In these imperatives, there are no lexically marked
agreement relations to score. Ex. (2) is an excerpt
from a recipe that appears in full in MT06. Ex. (3)
is part of usage instructions for the MusicMatch soft-
ware. The ATB contains few examples like these, so
our class-based model probably does not effectively
discriminate between alternative hypotheses for these
types of sentences.
Phrase Table Coverage In a standard phrase-
based system, effective translation into a highly in-
flected target language requires that the phrase table
contain the inflected word forms necessary to con-
struct an output with correct agreement. If the requi-
site words are not present in the search space of the
decoder, then no feature function would be sufficient
to enforce morpho-syntactic agreement.
During development, we observed that the phrase
table of our large-scale English-Arabic system did
often contain the inflected forms that we desired the
system to select. In fact, correctly agreeing alterna-
tives often appeared in n-best translation lists. To
verify this observation, we computed the lexical cov-
erage of the MT05 reference sentences in the decoder
search space. The statistics below report the token-
level recall of reference unigrams:10
? Baseline system translation output: 44.6%
? Phrase pairs matching source n-grams: 67.8%
The bottom category includes all lexical items that
the decoder could produce in a translation of the
source. This large gap between the unigram recall
of the actual translation output (top) and the lexical
coverage of the phrase-based model (bottom) indi-
cates that translation performance can be improved
dramatically by altering the translation model through
features such as ours, without expanding the search
space of the decoder.
10To focus on possibly inflected word forms, we excluded
numbers and punctuation from this analysis.
Human Evaluation We also manually evaluated
the MT05 output for improvements in agreement.11
Our system produced different output from the base-
line for 785 (74.3%) sentences. We randomly sam-
pled 100 of these sentences and counted agreement
errors of all types. The baseline contained 78 errors,
while our system produced 66 errors, a statistically
significant 15.4% error reduction at p ? 0.01 accord-
ing to a paired t-test.
In our output, a frequent source of remaining errors
was the case of so-called ?deflected agreement?: inan-
imate plural nouns require feminine singular agree-
ment with modifiers. On the other hand, animate
plural nouns require the sound plural, which is indi-
cated by an appropriate masculine or feminine suffix.
For example, the inanimate plural HAK
B??@ ?states? re-
quires the singular feminine adjective

?Yj

J?? @ ?united?,
not the sound plural H@YjJ?? @. The ATB does not con-
tain animacy annotations, so our agreement model
cannot discriminate between these two cases. How-
ever, Alkuhlani and Habash (2011) have recently
started annotating the ATB for animacy, and our
model could benefit as more data is released.
7 Conclusion and Outlook
Our class-based agreement model improves transla-
tion quality by promoting local agreement, but with
a minimal increase in decoding time and no addi-
tional storage requirements for the phrase table. The
model can be implemented with a standard CRF pack-
age, trained on existing treebanks for many languages,
and integrated easily with many MT feature APIs.
We achieved best results when the model training
data, MT tuning set, and MT evaluation set con-
tained roughly the same genre. Nevertheless, we also
showed an improvement, albeit less significant, on
mixed genre evaluation sets.
In principle, our class-based model should be more
robust to unseen word types and other phenomena that
make non-newswire genres challenging. However,
our analysis has shown that for Arabic, these genres
typically contain more Latin script and transliterated
words, and thus there is less morphology to score.
One potential avenue of future work would be to adapt
our component models to new genres by self-training
them on the target side of a large bitext.
11The annotator was the first author.
153
AcknowledgmentsWe thank Zhifei Li and ChrisManning
for helpful discussions, and Klaus Macherey, Wolfgang
Macherey, Daisy Stanton, and Richard Zens for engineer-
ing support. This work was conducted while the first au-
thor was an intern at Google. At Stanford, the first author
is supported by a National Science Foundation Graduate
Research Fellowship.
References
S. Alkuhlani and N. Habash. 2011. A corpus for modeling
morpho-syntactic agreement in Arabic: Gender, number and
rationality. In ACL-HLT.
G. Andrew and J. Gao. 2007. Scalable training of l1-regularized
log-linear models. In ICML.
E. Avramidis and P. Koehn. 2008. Enriching morphologically
poor languages for statistical machine translation. In ACL.
A. Birch, M. Osborne, and P. Koehn. 2007. CCG supertags in
factored statistical machine translation. In WMT.
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. 2007. Large
language models in machine translation. In EMNLP-CoNLL.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra,
and J. C. Lai. 1992. Class-based n-gram models of natural
language. Computational Linguistics, 18:467?479.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993. The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics, 19(2):263?
313.
D. Cer, M. Galley, D. Jurafsky, and C. D.Manning. 2010. Phrasal:
A statistical machine translation toolkit for exploring new
model features. In HLT-NAACL, Demonstration Session.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Better hy-
pothesis testing for statistical machine translation: Controlling
for optimizer instability. In ACL.
C. Dyer. 2009. Using a maximum entropy model to build seg-
mentation lattices for MT. In NAACL.
A. El Kholy and N. Habash. 2012. Orthographic and mor-
phological processing for English-Arabic statistical machine
translation. Machine Translation, 26(1-2):25?45.
A. Fraser, M. Weller, A. Cahill, and F. Cap. 2012. Modeling
inflection and word-formation in SMT. In EACL.
M. Galley and C. D. Manning. 2009. Quadratic-time dependency
parsing for machine translation. In ACL-IJCNLP.
S. Green and C. D. Manning. 2010. Better Arabic parsing:
baselines, evaluations, and analysis. In COLING.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-of-
speech tagging and morphological disambiguation in one fell
swoop. In ACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing schemes
for statistical machine translation. In NAACL.
H. Hassan, K. Sima?an, and A. Way. 2007. Supertagged phrase-
based statistical machine translation. In ACL.
M. Johnson. 2003. Learning and parsing stochastic unification-
based grammars. In COLT.
P. Koehn and H. Hoang. 2007. Factored translation models. In
EMNLP-CoNLL.
P. Koehn and K. Knight. 2003. Empirical methods for compound
splitting. In EACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico,
N. Bertoldi, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In ACL, Demonstration Session.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional ran-
dom fields: Probablistic models for segmenting and labeling
sequence data. In ICML.
A. Lopez. 2008. Statistical machine translation. ACMComputing
Surveys, 40(8):1?49.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki. 2004.
The Penn Arabic Treebank: Building a large-scale annotated
Arabic corpus. In NEMLAR.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-
basedminimum error rate training for statistical machine trans-
lation. In EMNLP.
K. Macherey, A. Dai, D. Talbot, A. Popat, and F. Och. 2011.
Language-independent compound splitting with morphologi-
cal operations. In ACL.
E. Minkov, K. Toutanova, and H. Suzuki. 2007. Generating
complex morphology for machine translation. In ACL.
C. Monz. 2011. Statistical machine translation with local lan-
guage models. In EMNLP.
F. J. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguistics,
30(4):417?449.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, et al 2004. A smorgasbord of features for sta-
tistical machine translation. In HLT-NAACL.
F. J. Och. 1999. An efficient method for determining bilingual
word classes. In EACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In
ACL.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmentation
and new word detection using conditional random fields. In
COLING.
S. Petrov, D. Das, and R. McDonald. 2012. A universal part-of-
speech tagset. In LREC.
O. Rambow, D. Chiang, M. Diab, N. Habash, R. Hwa, et al 2005.
Parsing Arabic dialects. Technical report, Johns Hopkins
University.
L. A. Ramshaw and M. Marcus. 1995. Text chunking using
transformation-based learning. In Proc. of the ThirdWorkshop
on Very Large Corpora.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in auto-
matic evaluation and significance testing in MT. In ACL-05
Workshop on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization (MTSE).
L. Schwartz, C. Callison-Burch, W. Schuler, and S. Wu. 2011.
Incremental syntactic language models for phrase-based trans-
lation. In ACL-HLT.
L. Shen, J. Xu, and R. Weischedel. 2010. String-to-dependency
statistical machine translation. Computational Linguistics,
36(4):649?671.
154
M. Subotin. 2011. An exponential translation model for target
language morphology. In ACL-HLT.
C. Tillmann. 2004. A unigram orientation model for statistical
machine translation. In NAACL.
K. Toutanova, H. Suzuki, and A. Ruopp. 2008. Applying mor-
phology generation models to machine translation. In ACL-
HLT.
J. Uszkoreit and T. Brants. 2008. Distributed word clustering
for large scale class-based language modeling in machine
translation. In ACL-HLT.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In COLING.
P. Williams and P. Koehn. 2011. Agreement constraints for
statistical machine translation into German. In WMT.
Y. Zhang. 2009. Structured Language Models for Statistical Ma-
chine Translation. Ph.D. thesis, Carnegie Mellon University.
155
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311?321,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fast and Adaptive Online Training of Feature-Rich Translation Models
Spence Green, Sida Wang, Daniel Cer, and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,sidaw,danielcer,manning}@stanford.edu
Abstract
We present a fast and scalable online
method for tuning statistical machine trans-
lation models with large feature sets. The
standard tuning algorithm?MERT?only
scales to tens of features. Recent discrimi-
native algorithms that accommodate sparse
features have produced smaller than ex-
pected translation quality gains in large
systems. Our method, which is based on
stochastic gradient descent with an adaptive
learning rate, scales to millions of features
and tuning sets with tens of thousands of
sentences, while still converging after only
a few epochs. Large-scale experiments on
Arabic-English and Chinese-English show
that our method produces significant trans-
lation quality gains by exploiting sparse fea-
tures. Equally important is our analysis,
which suggests techniques for mitigating
overfitting and domain mismatch, and ap-
plies to other recent discriminative methods
for machine translation.
1 Introduction
Sparse, overlapping features such as words and n-
gram contexts improve many NLP systems such as
parsers and taggers. Adaptation of discriminative
learning methods for these types of features to sta-
tistical machine translation (MT) systems, which
have historically used idiosyncratic learning tech-
niques for a few dense features, has been an active
research area for the past half-decade. However, de-
spite some research successes, feature-rich models
are rarely used in annual MT evaluations. For exam-
ple, among all submissions to theWMT and IWSLT
2012 shared tasks, just one participant tuned more
than 30 features (Hasler et al, 2012a). Slow uptake
of these methods may be due to implementation
complexities, or to practical difficulties of configur-
ing them for specific translation tasks (Gimpel and
Smith, 2012; Simianer et al, 2012, inter alia).
We introduce a new method for training feature-
rich MT systems that is effective yet comparatively
easy to implement. The algorithm scales to millions
of features and large tuning sets. It optimizes a lo-
gistic objective identical to that of PRO (Hopkins
and May, 2011) with stochastic gradient descent, al-
though other objectives are possible. The learning
rate is set adaptively using AdaGrad (Duchi et al,
2011), which is particularly effective for the mixture
of dense and sparse features present in MT models.
Finally, feature selection is implemented as efficient
L1 regularization in the forward-backward splitting
(FOBOS) framework (Duchi and Singer, 2009). Ex-
periments show that our algorithm converges faster
than batch alternatives.
To learn good weights for the sparse features,
most algorithms?including ours?benefit from
more tuning data, and the natural source is the train-
ing bitext. However, the bitext presents two prob-
lems. First, it has a single reference, sometimes of
lower quality than the multiple references in tun-
ing sets from MT competitions. Second, large bi-
texts often comprise many text genres (Haddow and
Koehn, 2012), a virtue for classical dense MT mod-
els but a curse for high dimensional models: bitext
tuning can lead to a significant domain adaptation
problem when evaluating on standard test sets. Our
analysis separates and quantifies these two issues.
We conduct large-scale translation quality exper-
iments on Arabic-English and Chinese-English. As
baselines we use MERT (Och, 2003), PRO, and
the Moses (Koehn et al, 2007) implementation
of k-best MIRA, which Cherry and Foster (2012)
recently showed to work as well as online MIRA
(Chiang, 2012) for feature-rich models. The first
experiment uses standard tuning and test sets from
the NIST OpenMT competitions. The second ex-
periment uses tuning and test sets sampled from the
large bitexts. The new method yields significant
improvements in both experiments. Our code is
included in the Phrasal (Cer et al, 2010) toolkit,
which is freely available.
311
2 Adaptive Online Algorithms
Machine translation is an unusual machine learning
setting because multiple correct translations exist
and decoding is comparatively expensive. When we
have a large feature set and therefore want to tune
on a large data set, batch methods are infeasible.
Online methods can converge faster, and in practice
they often find better solutions (Liang and Klein,
2009; Bottou and Bousquet, 2011, inter alia).
Recall that stochastic gradient descent (SGD),
a fundamental online method, updates weights w
according to
wt = wt?1 ? ??`t(wt?1) (1)
with loss function1 `t(w) of the tth example,
(sub)gradient of the loss with respect to the param-
eters ?`t(wt?1), and learning rate ?.
SGD is sensitive to the learning rate ?, which is
difficult to set in an MT system that mixes frequent
?dense? features (like the language model) with
sparse features (e.g., for translation rules). Further-
more, ? applies to each coordinate in the gradient,
an undesirable property in MT where good sparse
features may fire very infrequently. We would in-
stead like to take larger steps for sparse features and
smaller steps for dense features.
2.1 AdaGrad
AdaGrad is a method for setting an adaptive learn-
ing rate that comes with good theoretical guaran-
tees. The theoretical improvement over SGD is
most significant for high-dimensional, sparse fea-
tures. AdaGrad makes the following update:
wt = wt?1 ? ??1/2t ?`t(wt?1) (2)
??1t = ??1t?1 +?`t(wt?1)?`t(wt?1)>
=
t?
i=1
?`i(wi?1)?`i(wi?1)> (3)
A diagonal approximation to ? can be used for a
high-dimensional vector wt. In this case, AdaGrad
is simple to implement and computationally cheap.
Consider a single dimension j, and let scalars vt =
wt,j , gt = ?j`t(wt?1), Gt = ?ti=1 g2i , then theupdate rule is
vt = vt?1 ? ? G?1/2t gt (4)
Gt = Gt?1 + g2t (5)
Compared to SGD, we just need to storeGt = ??1t,jjfor each dimension j.
1We specify the loss function for MT in section 3.1.
2.2 Prior Online Algorithms in MT
AdaGrad is related to two previous online learning
methods for MT.
MIRA Chiang et al (2008) described an adaption
of MIRA (Crammer et al, 2006) to MT. MIRA
makes the following update:
wt = arg min
w
1
2??w ? wt?1?
2
2 + `t(w) (6)
The first term expresses conservativity: the weight
should change as little as possible based on a sin-
gle example, ensuring that it is never beneficial to
overshoot the minimum.
The relationship to SGD can be seen by lineariz-
ing the loss function `t(w) ? `t(wt?1) + (w ?
wt?1)>?`t(wt?1) and taking the derivative of (6).
The result is exactly (1).
AROW Chiang (2012) adapted AROW (Cram-
mer et al, 2009) to MT. AROW models the current
weight as a Gaussian centered at wt?1 with covari-
ance ?t?1, and does the following update upon
seeing training example xt:
wt,?t =
arg min
w,?
1
?DKL(N (w,?)||N (wt?1,?t?1))
+ `t(w) +
1
2?x
>
t ?xt (7)
The KL-divergence term expresses a more general,
directionally sensitive conservativity. Ignoring the
third term, the ? that minimizes the KL is actu-
ally ?t?1. As a result, the first two terms of (7)
generalize MIRA so that we may be more conser-
vative in some directions specified by ?. To see
this, we can write out the KL-divergence between
two Gaussians in closed form, and observe that the
terms involving w do not interact with the terms
involving ?:
wt = arg min
w
1
2? (w ? wt?1)
>??1t?1(w ? wt?1)
+ `t(w) (8)
?t = arg min
?
1
2? log
( |?t?1|
|?|
)
+ 12?Tr
(
??1t?1?
)
+ 12?x
>
t ?xt (9)
The third term in (7), called the confidence term,
gives us adaptivity, the notion that we should have
smaller variance in the direction v as more data xt
312
is seen in direction v. For example, if ? is diagonal
and xt are indicator features, the confidence term
then says that the weight for a rarer feature should
have more variance and vice-versa. Recall that for
generalized linear models?`t(w) ? xt; if we sub-
stitute xt = ?t?`t(w) into (9), differentiate and
solve, we get:
??1t = ??1t?1 + xtx>t
= ??10 +
t?
i=1
?2i?`i(wi?1)?`i(wi?1)>
(10)
The precision ??1t generally grows as more datais seen. Frequently updated features receive an espe-
cially high precision, whereas the model maintains
large variance for rarely seen features.
If we substitute (10) into (8), linearize the loss
`t(w) as before, and solve, then we have the lin-
earized AROW update
wt = wt?1 ? ??t?`t(wt?1) (11)
which is also an adaptive update with per-coordinate
learning rates specified by ?t (as opposed to ?1/2tin AdaGrad).
2.3 Comparing AdaGrad, MIRA, AROW
Compare (3) to (10) and observe that if we set
??10 = 0 and ?t = 1, then the only differencebetween the AROW update (11) and the AdaGrad
update (2) is a square root. Under a constant gradi-
ent, AROW decays the step size more aggressively
(1/t) compared to AdaGrad (1/?t), and it is sensi-
tive to the specification of ??10 .Informally, SGD can be improved in the conser-
vativity direction using MIRA so the updates do
not overshoot. Second, SGD can be improved in
the adaptivity direction using AdaGrad where the
decaying stepsize is more robust and the adaptive
stepsize allows better weight updates to features
differing in sparsity and scale. Finally, AROW com-
bines both adaptivity and conservativity. For MT,
adaptivity allows us to deal withmixed dense/sparse
features effectively without specific normalization.
Why do we choose AdaGrad over AROW?
MIRA/AROW requires selecting the loss function
`(w) so that wt can be solved in closed-form, by
a quadratic program (QP), or in some other way
that is better than linearizing. This usually means
choosing a hinge loss. On the other hand, Ada-
Grad/linearized AROW only requires that the gradi-
ent of the loss function can be computed efficiently.
Algorithm 1 Adaptive online tuning for MT.
Require: Tuning set {fi, e1:ki }i=1:M1: Set w0 = 02: Set t = 1
3: repeat
4: for i in 1 . . .M in random order do
5: Decode n-best list Ni for fi6: Sample pairs {dj,+, dj,?}j=1:s from Ni7: Compute Dt = {?(dj,+)? ?(dj,?)}j=1:s8: Set gt = ?`(Dt; wt?1)}
9: Set ??1t = ??1t?1 + gtg>t . Eq. (3)
10: Update wt = wt?1 ? ??1/2t gt . Eq. (2)11: Regularize wt . Eq. (15)12: Set t = t+ 1
13: end for
14: until convergence
Linearized AROW, however, is less robust than Ada-
Grad empirically2 and lacks known theoretical guar-
antees. Finally, by using AdaGrad, we separate
adaptivity from conservativity. Our experiments
suggest that adaptivity is actually more important.
3 Adaptive Online MT
Algorithm 1 shows the full algorithm introduced in
this paper. AdaGrad (lines 9?10) is a crucial piece,
but the loss function, regularization technique, and
parallelization strategy described in this section are
equally important in the MT setting.
3.1 Pairwise Logistic Loss Function
Algorithm 1 lines 5?8 describe the gradient com-
putation. We cast MT tuning as pairwise ranking
(Herbrich et al, 1999, inter alia), which Hopkins
and May (2011) applied to MT. The pairwise ap-
proach results in simple, convex loss functions suit-
able for online learning. The idea is that for any
two derivations, the ranking predicted by the model
should be consistent with the ranking predicted by
a gold sentence-level metric G like BLEU+1 (Lin
and Och, 2004).
Consider a single source sentence f with asso-
ciated references e1:k. Let d be a derivation in an
n-best list of f that has the target e = e(d) and the
feature map ?(d). Let M(d) = w ? ?(d) be the
model score. For any derivation d+ that is better
than d? under G, we desire pairwise agreement
such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
2According to experiments not reported in this paper.
313
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011).
We compute difference vectors Dt = {x1:s+ } (Al-gorithm 1 line 7) from s pairs (d+, d?) for source
sentence ft. We use the familiar logistic loss:
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(12)
Choosing the hinge loss instead of the logistic
loss results in the 1-class SVM problem. The 1-
class separation problem is equivalent to the binary
classification problem with x+ = ?(d+)? ?(d?)
as positive data and x? = ?x+ as negative data,
which may be plugged into an existing logistic re-
gression solver.
We find that Algorithm 1 works best with mini-
batches instead of single examples. In line 4 we
simply partition the tuning set so that i becomes a
mini-batch of examples.
3.2 Updating and Regularization
Algorithm 1 lines 9?11 compute the adaptive learn-
ing rate, update the weights, and apply regulariza-
tion. Section 2.1 explained the AdaGrad learn-
ing rate computation. To update and regularize
the weights we apply the Forward-Backward Split-
ting (FOBOS) (Duchi and Singer, 2009) framework,
which separates the two operations. The two-step
FOBOS update is
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (13)
wt = arg min
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(14)
where (13) is just an unregularized gradient descent
step and (14) balances the regularization term r(w)
with staying close to the gradient step.
Equation (14) permits efficient L1 regulariza-
tion, which is well-suited for selecting good features
from exponentially many irrelevant features (Ng,
2004). It is well-known that feature selection is very
important for feature-rich MT. For example, sim-
ple indicator features like lexicalized re-ordering
classes are potentially useful yet bloat the the fea-
ture set and, in the worst case, can negatively impact
Algorithm 2 ?Stale gradient? parallelization
method for Algorithm 1.
Require: Tuning set {fi, e1:ki }i=1:M1: Initialize threadpool p1, . . . , pj2: Set t = 1
3: repeat
4: for i in 1 . . .M in random order do
5: Wait until any thread p is idle
6: Send (fi, e1:ki , t) to p . Alg. 1 lines 5?87: while ? p? done with gradient gt? do . t? ? t8: Update wt = wt?1 ? ?gt? . Alg. 1 lines 9?119: Set t = t+ 1
10: end while
11: end for
12: until convergence
search. Some of the features generalize, but many
do not. This was well understood in previous work,
so heuristic filtering was usually applied (Chiang
et al, 2009, inter alia). In contrast, we need only
select an appropriate regularization strength ?.
Specifically, when r(w) = ??w?1, the closed-
form solution to (14) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(15)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls
below the threshold ?t?1?. It is straightforward to
adapt this to AdaGrad with diagonal ? by setting
each dimension of ?t?1,j = ??
1
2
t,jj and by takingelement-wise products.
We find that?`t?1(wt?1) only involves several
hundred active features for the current example
(or mini-batch). However, naively following the
FOBOS framework requires updating millions of
weights. But a practical benefit of FOBOS is that
we can do lazy updates on just the active dimensions
without any approximations.
3.3 Parallelization
Algorithm 1 is inherently sequential like standard
online learning. This is undesirable in MT where
decoding is costly. We therefore parallelize the algo-
rithm with the ?stale gradient? method of Langford
et al (2009) (Algorithm 2). A fixed threadpool of
workers computes gradients in parallel and sends
them to a master thread, which updates a central
weight vector. Crucially, the weight updates need
not be applied in order, so synchronization is unnec-
essary; the workers only idle at the end of an epoch.
The consequence is that the update in line 8 of Al-
gorithm 2 is with respect to gradient gt? with t? ? t.
Langford et al (2009) gave convergence results for
314
stale updating, but the bounds do not apply to our
setting since we use L1 regularization. Neverthe-
less, Gimpel et al (2010) applied this framework
to other non-convex objectives and obtained good
empirical results.
Our asynchronous, stochastic method has practi-
cal appeal for MT. During a tuning run, the online
method decodes the tuning set under many more
weight vectors than a MERT-style batch method.
This characteristic may result in broader exploration
of the search space, and make the learner more ro-
bust to local optima local optima (Liang and Klein,
2009; Bottou and Bousquet, 2011, inter alia). The
adaptive algorithm identifies appropriate learning
rates for the mixture of dense and sparse features.
Finally, large data structures such as the language
model (LM) and phrase table exist in shared mem-
ory, obviating the need for remote queries.
4 Experiments
We built Arabic-English and Chinese-English MT
systems with Phrasal (Cer et al, 2010), a phrase-
based system based on alignment templates (Och
and Ney, 2004). The corpora3 in our experiments
(Table 1) derive from several LDC sources from
2012 and earlier. We de-duplicated each bitext ac-
cording to exact string match, and ensured that no
overlap existed with the test sets. We produced
alignments with the Berkeley aligner (Liang et al,
2006b) with standard settings and symmetrized via
the grow-diag heuristic.
For each language we used SRILM (Stolcke,
2002) to estimate 5-gram LMs with modified
Kneser-Ney smoothing. We included the monolin-
gual English data and the respective target bitexts.
4.1 Feature Templates
The baseline ?dense? model contains 19 features:
the nine Moses baseline features, the hierarchical
lexicalized re-ordering model of Galley and Man-
ning (2008), the (log) count of each rule, and an
indicator for unique rules.
To the dense features we add three high di-
mensional ?sparse? feature sets. Discrimina-
3We tokenized the English with packages from the Stan-
ford Parser (Klein and Manning, 2003) according to the Penn
Treebank standard (Marcus et al, 1993), the Arabic with the
Stanford Arabic segmenter (Green and DeNero, 2012) accord-
ing to the Penn Arabic Treebank standard (Maamouri et al,
2008), and the Chinese with the Stanford Chinese segmenter
(Chang et al, 2008) according to the Penn Chinese Treebank
standard (Xue et al, 2005).
Bilingual Monolingual
Sentences Tokens Tokens
Ar-En 6.6M 375M 990MZh-En 9.3M 538M
Table 1: Bilingual and monolingual corpora used
in these experiments. The monolingual English
data comes from the AFP and Xinhua sections of
English Gigaword 4 (LDC2009T13).
tive phrase table (PT): indicators for each rule
in the phrase table. Alignments (AL): indica-
tors for phrase-internal alignments and deleted
(unaligned) source words. Discriminative re-
ordering (LO): indicators for eight lexicalized re-
ordering classes, including the six standard mono-
tone/swap/discontinuous classes plus the two sim-
pler Moses monotone/non-monotone classes.
4.2 Tuning Algorithms
The primary baseline is the dense feature set tuned
with MERT (Och, 2003). The Phrasal implemen-
tation uses the line search algorithm of Cer et al
(2008), uniform initialization, and 20 random start-
ing points.4 We tuned according to BLEU-4 (Pap-
ineni et al, 2002).
We built high dimensional baselines with two dif-
ferent algorithms. First, we tuned with batch PRO
using the default settings in Phrasal (L2 regulariza-
tion with ?=0.1). Second, we ran the k-best batch
MIRA (kb-MIRA) (Cherry and Foster, 2012) imple-
mentation in Moses. We did implement an online
version of MIRA, and in small-scale experiments
found that the batch variant worked just as well.
Cherry and Foster (2012) reported the same result,
and their implementation is available in Moses. We
ran their code with standard settings.
Moses5 also contains the discriminative phrase
table implementation of (Hasler et al, 2012b),
which is identical to our implementation using
Phrasal. Moses and Phrasal accept the same phrase
table and LM formats, so we kept those data struc-
tures in common. The two decoders also use the
same multi-stack beam search (Och and Ney, 2004).
For our method, we used uniform initialization,
16 threads, and a mini-batch size of 20. We found
that ?=0.02 and ?=0.1 worked well on development
sets for both languages. To compute the gradients
4Other system settings for all experiments: distortion limit
of 5, a maximum phrase length of 7, and an n-best size of 200.
5v1.0 (28 January 2013)
315
Model #features Algorithm Tuning Set MT02 MT03 MT04 MT09
Dense 19 MERT MT06 45.08 51.32 52.26 51.42 48.44
Dense 19 This paper MT06 44.19 51.42 52.52 50.16 48.13
+PT 151k kb-MIRA MT06 42.08 47.25 48.98 47.08 45.64
+PT 23k PRO MT06 44.31 51.06 52.18 50.23 47.52
+PT 50k This paper MT06 50.61 51.71 52.89 50.42 48.74
+PT+AL+LO 109k PRO MT06 44.87 51.25 52.43 50.05 47.76
+PT+AL+LO 242k This paper MT06 57.84 52.45 53.18 51.38 49.37
Dense 19 MERT MT05/6/8 49.63 51.60 52.29 51.73 48.68
+PT+AL+LO 390k This paper MT05/6/8 58.20 53.61 54.99 52.79 49.94
(Chiang, 2012)* 10-20k MIRA MT04/6 ? ? ? ? 45.90
(Chiang, 2012)* 10-20k AROW MT04/6 ? ? ? ? 47.60
#sentences 728 663 1,075 1,313
Table 2: Ar-En results [BLEU-4 % uncased] for the NIST tuning experiment. The tuning and test sets
each have four references. MT06 has 1,717 sentences, while the concatenated MT05/6/8 set has 4,213
sentences. Bold indicates statistical significance relative to the best baseline in each block at p < 0.001;
bold-italic at p < 0.05. We assessed significance with the permutation test of Riezler and Maxwell (2005).
(*) Chiang (2012) used a similar-sized bitext, but two LMs trained on twice as much monolingual data.
Model #features Algorithm Tuning Set MT02 MT03 MT04
Dense 19 MERT MT06 33.90 35.72 33.71 34.26
Dense 19 This paper MT06 32.60 36.23 35.14 34.78
+PT 105k kb-MIRA MT06 29.46 30.67 28.96 30.05
+PT 26k PRO MT06 33.70 36.87 34.62 34.80
+PT 66k This paper MT06 33.90 36.09 34.86 34.73
+PT+AL+LO 148k PRO MT06 34.81 36.31 33.81 34.41
+PT+AL+LO 344k This paper MT06 38.99 36.40 35.07 34.84
Dense 19 MERT MT05/6/8 32.36 35.69 33.83 34.33
+PT+AL+LO 487k This paper MT05/6/8 37.64 37.81 36.26 36.15
#sentences 878 919 1,597
Table 3: Zh-En results [BLEU-4 % uncased] for the NIST tuning experiment. MT05/6/8 has 4,103
sentences. OpenMT 2009 did not include Zh-En, hence the asymmetry with Table 2.
we sampled 15 derivation pairs for each tuning ex-
ample and scored them with BLEU+1.
4.3 NIST OpenMT Experiment
The first experiment evaluates our algorithm when
tuning and testing on standard test sets, each with
four references. When we add features, our algo-
rithm tends to overfit to a standard-sized tuning set
like MT06. We thus concatenated MT05, MT06,
and MT08 to create a larger tuning set.
Table 2 shows the Ar-En results. Our algorithm
is competitive with MERT in the low dimensional
?dense? setting, and compares favorably to PRO
with the PT feature set. PRO does not benefit
from additional features, whereas our algorithm im-
proves with both additional features and data. The
underperformance of kb-MIRA may result from
a difference between Moses and Phrasal: Moses
MERT achieves only 45.62 on MT09. Moses PRO
with the PT feature set is slightly worse, e.g., 44.52
on MT09. Nevertheless, kb-MIRA does not im-
prove significantly over MERT, and also selects an
unnecessarily large model.
The full feature set PT+AL+LO does help. With
the PT feature set alne, our algorithm tuned on
MT05/6/8 scores well below the best model, e.g.
316
Model #features Algorithm Tuning Set #refs bitext5k-test MT04
Dense 19 MERT MT06 45.08 4 39.28 51.42
+PT 72k This paper MT05/6/8 51.29 4 39.50 50.60
+PT 79k This paper bitext5k 44.79 1 43.85 45.73
+PT+AL+LO 647k This paper bitext15k 45.68 1 43.93 45.24
Table 4: Ar-En results [BLEU-4 % uncased] for the bitext tuning experiment. Statistical significance is
relative to the Dense baseline. We include MT04 for comparison to the NIST genre.
Model #features Algorithm Tuning Set #refs bitext5k-test MT04
Dense 19 MERT MT06 33.90 4 33.44 34.26
+PT 97k This paper MT05/6/8 34.45 4 35.08 35.19
+PT 67k This paper bitext5k 36.26 1 36.01 33.76
+PT+AL+LO 536k This paper bitext15k 37.57 1 36.30 34.05
Table 5: Zh-En results [BLEU-4 % uncased] for the bitext tuning experiment.
48.56 BLEU on MT09. For Ar-En, our algorithm
thus has the desirable property of benefiting from
more and better features, and more data.
Table 3 shows Zh-En results. Somewhat sur-
prisingly our algorithm improves over MERT in
the dense setting. When we add the discrimina-
tive phrase table, our algorithm improves over kb-
MIRA, and over batch PRO on two evaluation sets.
With all features and the MT05/6/8 tuning set, we
improve significantly over all other models. PRO
learns a smaller model with the PT+AL+LO fea-
ture set which is surprising given that it applies L2
regularization (AdaGrad uses L1). We speculate
that this may be an consequence of stochastic learn-
ing. Our algorithm decodes each example with
a new weight vector, thus exploring more of the
search space for the same tuning set.
4.4 Bitext Tuning Experiment
Tables 2 and 3 show that adding tuning examples
improves translation quality. Nevertheless, even
the larger tuning set is small relative to the bitext
from which rules were extracted. He and Deng
(2012) and Simianer et al (2012) showed significant
translation quality gains by tuning on the bitext.
However, their bitexts matched the genre of their
test sets. Our bitexts, like those of most large-scale
systems, do not. Domain mismatch matters for the
dense feature set (Haddow and Koehn, 2012). We
show that it also matters for feature-rich MT.
Before aligning each bitext, we randomly sam-
pled and sequestered 5k and 15k sentence tuning
sets, and a 5k test set. We prevented overlap be-
DA DB |A| |B| |A ?B|
MT04 MT06 70k 72k 5.9k
MT04 MT568 70k 96k 7.6k
MT04 bitext5k 70k 67k 4.4k
MT04 bitext15k 70k 310k 10.5k
5ktest bitext5k 82k 67k 5.6k
5ktest bitext15k 82k 310k 14k
Table 6: Number of overlapping phrase table (+PT)
features on various Zh-En dataset pairs.
tween the tuning sets and the test set. We then
tuned a dense model with MERT on MT06, and
feature-rich models on both MT05/6/8 and the bi-
text tuning set. Table 4 shows the Ar-En results.
When tuned on bitext5k the translation quality gains
are significant for bitext5k-test relative to tuning on
MT05/6/8, which has multiple references. However,
the bitext5k models do not generalize as well to the
NIST evaluation sets as represented by the MT04
result. Table 5 shows similar trends for Zh-En.
5 Analysis
5.1 Feature Overlap Analysis
How many sparse features appear in both the tun-
ing and test sets? In Table 6, A is the set of phrase
table features that received a non-zero weight when
tuned on datasetDA (same forB). ColumnDA lists
several Zh-En test sets used and column DB lists
tuning sets. Our experiments showed that tuning
on MT06 generalizes better to MT04 than tuning
317
on bitext5k, whereas tuning on bitext5k general-
izes better to bitext5k-test than tuning on MT06.
These trends are consistent with the level of fea-
ture overlap. Phrase table features in A ? B are
overwhelmingly short, simple, and correct phrases,
suggesting L1 regularization is effective for feature
selection. It is also important to balance the number
of features with how well weights can be learned
for those features, as tuning on bitext15k produced
higher coverage for MT04 but worse generalization
than tuning on MT06.
5.2 Domain Adaptation Analysis
To understand the domain adaptation issue we com-
pared the non-zero weights in the discriminative
phrase table (PT) for Ar-En models tuned on bi-
text5k and MT05/6/8. Table 7 illustrates a statisti-
cal idiosyncrasy in the data for the American and
British spellings of program/programme. The mass
is concentrated along the diagonal, probably be-
cause MT05/6/8 was prepared by NIST, an Amer-
ican agency, while the bitext was collected from
many sources including Agence France Presse.
Of course, this discrepancy is consequential for
both dense and feature-rich models. However, we
observe that the feature-rich models fit the tuning
data more closely. For example, the MT05/6/8
model learns rules like l .?A 	KQK. 	?? 	?JK
 ? program
includes, l .?A 	KQK. ? program of, and l .?A 	KQ. ? @ ? 	Y 	?A 	K?
program window. Crucially, it does not learn the
basic rule l .?A 	KQK. ? program.
In contrast, the bitext5k model contains ba-
sic rules such l .?A 	KQK. ? programme, l .?A 	KQ. ? @ @ 	Y?
? this programme, and l .?A 	KQ. ? @ ?? 	X ? that pro-
gramme. It also contains more elaborate rules such
as l .?A 	KQ. ? @ HA? 	? 	K I	KA? ? programme expenses
were and ????A?? @ ?J
KA 	? 	?? @ HCgQ?@ l .?@QK.?manned
space flight programmes. We observed similar
trends for ?defense/defence?, ?analyze/analyse?, etc.
This particular genre problem could be addressed
with language-specific pre-processing, but our sys-
tem solves it in a data-driven manner.
5.3 Re-ordering Analysis
We also analyzed re-ordering differences. Arabic
matrix clauses tend to be verb-initial, meaning that
the subject and verb must be swapped when translat-
ing to English. To assess re-ordering differences?
if any?between the dense and feature-rich models,
we selected all MT09 segments that began with one
# bitext5k # MT05/6/8
programme 185 0
program 19 449
PT rules w/ programme 353 79
PT rules w/ program 9 31
Table 7: Top: comparison of token counts in two
Ar-En tuning sets for programme and program. Bot-
tom: rule counts in the discriminative phrase table
(PT) for models tuned on the two tuning sets. Both
spellings correspond to the Arabic l .?A 	KQK. .
of seven common verbs: ?A? qaal ?said?, hQ?? SrH
?declared?, PA ?@ ashaar ?indicated?, 	?A? kaan ?was?,
Q?
	
X dhkr ?commented?, 	?A 	?@ aDaaf ?added?, 	???@
acln ?announced?. We compared the output of the
MERT Dense model to our method with the full
feature set, both tuned on MT06. Of the 208 source
segments, 32 of the translation pairs contained dif-
ferent word order in the matrix clause. Our feature-
rich model was correct 18 times (56.3%), Dense
was correct 4 times (12.5%), and neither method
was correct 10 times (31.3%).
(1) ref: lebanese prime minister , fuad siniora ,
announced
a. and lebanese prime minister fuad siniora
that
b. the lebanese prime minister fouad siniora
announced
(2) ref: the newspaper and television reported
a. she said the newspaper and television
b. television and newspaper said
In (1) the dense model (1a) drops the verb while the
feature-rich model correctly re-orders and inserts
it after the subject (1b). The coordinated subject
in (2) becomes an embedded subject in the dense
output (2a). The feature-rich model (2b) performs
the correct re-ordering.
5.4 Runtime Comparison
Table 8 compares our method to standard implemen-
tations of the other algorithms. MERT parallelizes
easily but runtime increases quadratically with n-
best list size. PRO runs (single-threaded) L-BFGS
to convergence on every epoch, a potentially slow
procedure for the larger feature set. Moreover, both
318
epochs min.
MERT Dense 22 180
PRO +PT 25 35
kb-MIRA* +PT 26 25
This paper +PT 10 10
PRO +PT+AL+LO 13 150
This paper +PT+AL+LO 5 15
Table 8: Epochs to convergence (?epochs?) and
approximate runtime per epoch in minutes (?min.?)
for selected Zh-En experiments tuned on MT06.
All runs executed on the same dedicated system
with the same number of threads. (*) Moses and
kb-MIRA are written in C++, while all other rows
refer to Java implementations in Phrasal.
the Phrasal and Moses PRO implementations use
L2 regularization, which regularizes every weight
on every update. kb-MIRA makes multiple passes
through the n-best lists during each epoch. The
Moses implementation parallelizes decoding but
weight updating is sequential.
The core of our method is an inner product be-
tween the adaptive learning rate vector and the gra-
dient. This is easy to implement and is very fast
even for large feature sets. Since we applied lazy
regularization, this inner product usually involves
hundred-dimensional vectors. Finally, our method
does not need to accumulate n-best lists, a practice
that slows down the other algorithms.
6 Related Work
Our work relates most closely to that of Hasler et al
(2012b), who tuned models containing both sparse
and dense features with Moses. A discriminative
phrase table helped them improve slightly over a
dense, online MIRA baseline, but their best results
required initialization with MERT-tuned weights
and re-tuning a single, shared weight for the dis-
criminative phrase table with MERT. In contrast,
our algorithm learned good high dimensional mod-
els from a uniform starting point.
Chiang (2012) adapted AROW to MT and ex-
tended previous work on online MIRA (Chiang et
al., 2008; Watanabe et al, 2007). It was not clear if
his improvements came from the novel Hope/Fear
search, the conservativity gain from MIRA/AROW
by solving the QP exactly, adaptivity, or sophis-
ticated parallelization. In contrast, we show that
AdaGrad, which ignores conservativity and only
capturing adaptivity, is sufficient.
Simianer et al (2012) investigated SGD with a
pairwise perceptron objective. Their best algorithm
used iterative parameter mixing (McDonald et al,
2010), which we found to be slower than the stale
gradient method in section 3.3. They regularized
once at the end of each epoch, whereas we regular-
ized each weight update. An empirical comparison
of these two strategies would be an interesting fu-
ture contribution.
Watanabe (2012) investigated SGD and even ran-
domly selected pairwise samples as we did. He
considered both softmax and hinge losses, observ-
ing better results with the latter, which solves a QP.
Their parallelization strategy required a line search
at the end of each epoch.
Many other discriminative techniques have been
proposed based on: ramp loss (Gimpel, 2012);
hinge loss (Cherry and Foster, 2012; Haddow et
al., 2011; Arun and Koehn, 2007); maximum en-
tropy (Xiang and Ittycheriah, 2011; Ittycheriah and
Roukos, 2007; Och and Ney, 2002); perceptron
(Liang et al, 2006a); and structured SVM (Till-
mann and Zhang, 2006). These works use radically
different experimental setups, and to our knowl-
edge only (Cherry and Foster, 2012) and this work
compare to at least two high dimensional baselines.
Broader comparisons, though time-intensive, could
help differentiate these methods.
7 Conclusion and Outlook
We introduced a new online method for tuning
feature-rich translation models. The method is
faster per epoch than MERT, scales to millions of
features, and converges quickly. We used efficient
L1 regularization for feature selection, obviating
the need for the feature scaling and heuristic filter-
ing common in prior work. Those comfortable with
implementing vanilla SGD should find our method
easy to implement. Even basic discriminative fea-
tures were effective, so we believe that our work
enables fresh approaches to more sophisticated MT
feature engineering.
Acknowledgments We thank John DeNero for helpful com-
ments on an earlier draft. The first author is supported by a
National Science Foundation Graduate Research Fellowship.
We also acknowledge the support of the Defense Advanced
Research Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM. Any opin-
ions, findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily
reflect the view of the DARPA or the US government.
319
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In MT Summit XI.
L. Bottou and O. Bousquet. 2011. The tradeoffs of
large scale learning. In Optimization for Machine
Learning, pages 351?368. MIT Press.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regu-
larization and search for minimum error rate training.
In WMT.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
P-C. Chang, M. Galley, and C. D. Manning. 2008.
Optimizing Chinese word segmentation for machine
translation performance. In WMT.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
HLT-NAACL.
D. Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13:1159?1187.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In HLT-
NAACL.
K. Gimpel, D. Das, and N. A. Smith. 2010. Distributed
asynchronous online learning for natural language
processing. In CoNLL.
K. Gimpel. 2012. Discriminative Feature-Rich Mod-
eling for Syntax-Based Machine Translation. Ph.D.
thesis, Language Technologies Institute, Carnegie
Mellon University.
S. Green and J. DeNero. 2012. A class-based agree-
ment model for generating accurately inflected trans-
lations. In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect
of out-of-domain data on SMT systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleR-
ank training for phrase-basedmachine translation. In
WMT.
E. Hasler, P. Bell, A. Ghoshal, B. Haddow, P. Koehn,
F. McInnes, et al 2012a. The UEDIN systems for
the IWSLT 2012 evaluation. In IWSLT.
E. Hasler, B. Haddow, and P. Koehn. 2012b. Sparse
lexicalised features and topic adaptation for SMT. In
IWSLT.
X. He and L. Deng. 2012. Maximum expected BLEU
training of phrase and lexicon translation models. In
ACL.
R. Herbrich, T. Graepel, and K. Obermayer. 1999.
Support vector learning for ordinal regression. In
ICANN.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In HLT-NAACL.
D. Klein and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
J. Langford, A. J. Smola, and M. Zinkevich. 2009.
Slow learners are fast. In NIPS.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In HLT-NAACL.
P. Liang, A. Bouchard-C?t?, D. Klein, and B. Taskar.
2006a. An end-to-end discriminative approach to
machine translation. In ACL.
P. Liang, B. Taskar, and D. Klein. 2006b. Alignment
by agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-
ing the Arabic Treebank: A collaborative effort to-
ward new annotation guidelines. In LREC.
320
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
R.McDonald, K. Hall, andG.Mann. 2010. Distributed
training strategies for the structured perceptron. In
NAACL-HLT.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization (MTSE).
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-
scale discriminative training in SMT. In ACL.
A Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In ICSLP.
C. Tillmann and T. Zhang. 2006. A discriminative
global training algorithm for statistical MT. In ACL-
COLING.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In EMNLP-CoNLL.
T. Watanabe. 2012. Optimized online rank learning
for machine translation. In HLT-NAACL. Associa-
tion for Computational Linguistics.
B. Xiang and A. Ittycheriah. 2011. Discriminative
feature-tied mixture modeling for statistical machine
translation. In ACL-HLT.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207?238.
321
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 206?211,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Word Segmentation of Informal Arabic with Domain Adaptation
Will Monroe, Spence Green, and Christopher D. Manning
Computer Science Department, Stanford University
{wmonroe4,spenceg,manning}@stanford.edu
Abstract
Segmentation of clitics has been shown to
improve accuracy on a variety of Arabic
NLP tasks. However, state-of-the-art Ara-
bic word segmenters are either limited to
formal Modern Standard Arabic, perform-
ing poorly on Arabic text featuring dialectal
vocabulary and grammar, or rely on lin-
guistic knowledge that is hand-tuned for
each dialect. We extend an existing MSA
segmenter with a simple domain adapta-
tion technique and new features in order
to segment informal and dialectal Arabic
text. Experiments show that our system
outperforms existing systems on newswire,
broadcast news and Egyptian dialect, im-
proving segmentation F
1
score on a recently
released Egyptian Arabic corpus to 95.1%,
compared to 90.8% for another segmenter
designed specifically for Egyptian Arabic.
1 Introduction
Segmentation of words, clitics, and affixes is essen-
tial for a number of natural language processing
(NLP) applications, including machine translation,
parsing, and speech recognition (Chang et al, 2008;
Tsarfaty, 2006; Kurimo et al, 2006). Segmentation
is a common practice in Arabic NLP due to the lan-
guage?s morphological richness. Specifically, clitic
separation has been shown to improve performance
on Arabic parsing (Green and Manning, 2010) and
Arabic-English machine translation (Habash and
Sadat, 2006). However, the variety of Arabic di-
alects presents challenges in Arabic NLP. Dialectal
Arabic contains non-standard orthography, vocab-
ulary, morphology, and syntax. Tools that depend
on corpora or grammatical properties that only con-
sider formal Modern Standard Arabic (MSA) do
not perform well when confronted with these differ-
ences. The creation of annotated corpora in dialec-
tal Arabic (Maamouri et al, 2006) has promoted
the development of new systems that support di-
alectal Arabic, but these systems tend to be tailored
to specific dialects and require separate efforts for
Egyptian Arabic, Levantine Arabic, Maghrebi Ara-
bic, etc.
We present a single clitic segmentation model
that is accurate on both MSA and informal Arabic.
The model is an extension of the character-level
conditional random field (CRF) model of Green
and DeNero (2012). Our work goes beyond theirs
in three aspects. First, we handle two Arabic ortho-
graphic normalization rules that commonly require
rewriting of tokens after segmentation. Second,
we add new features that improve segmentation ac-
curacy. Third, we show that dialectal data can be
handled in the framework of domain adaptation.
Specifically, we show that even simple feature space
augmentation (Daum?, 2007) yields significant im-
provements in task accuracy.
We compare our work to the original Green and
DeNero model and two other Arabic segmenta-
tion systems: the MADA+TOKAN toolkit v. 3.1
(Habash et al, 2009) and its Egyptian dialect vari-
ant, MADA-ARZ v. 0.4 (Habash et al, 2013). We
demonstrate that our system achieves better perfor-
mance across the board, beating all three systems
on MSA newswire, informal broadcast news, and
Egyptian dialect. Our segmenter achieves a 95.1%
F
1
segmentation score evaluated against a gold stan-
dard on Egyptian dialect data, compared to 90.8%
for MADA-ARZ and 92.9% for Green and DeN-
ero. In addition, our model decodes input an order
of magnitude faster than either version of MADA.
Like the Green and DeNero system, but unlike
MADA and MADA-ARZ, our system does not rely
on a morphological analyzer, and can be applied
directly to any dialect for which segmented training
data is available. The source code is available in
the latest public release of the Stanford Word Seg-
menter (http://nlp.stanford.edu/software/
segmenter.shtml).
206
2 Arabic Word Segmentation Model
A CRF model (Lafferty et al, 2001) defines a distri-
bution p(Y|X; ?), whereX = {x
1
, . . . , x
N
} is the
observed input sequence andY = {y
1
, . . . , y
N
} is
the sequence of labels we seek to predict. Green
and DeNero use a linear-chain model with X as
the sequence of input characters, and Y
?
chosen
according to the decision rule
Y
?
= argmax
Y
N
?
i=1
?
>
?(X, y
i
, . . . , y
i?3
, i) .
where ? is the feature map defined in Section 2.1.
Their model classifies each y
i
as one of I (contin-
uation of a segment), O (whitespace outside any
segment), B (beginning of a segment), or F (pre-
grouped foreign characters).
Our segmenter expands this label space in order
to handle two Arabic-specific orthographic rules.
In our model, each y
i
can take on one of the six
values {I,O,B,F,RewAl,RewTa}:
? RewAl indicates that the current character,
which is always the Arabic letter ? l, starts a
new segment and should additionally be trans-
formed into the definite article ? @ al- when
segmented. This type of transformation occurs
after the prefix ? li- ?to?.
? RewTa indicates that the current character,
which is always the Arabic letter

H t, is a
continuation but should be transformed into
the letter

? h when segmented. Arabic orthog-
raphy rules restrict the occurrence of

? h to
the word-final position, writing it instead as

H t whenever it is followed by a suffix.
2.1 Features
The model of Green and DeNero is a third-order
(i.e., 4-gram) Markov CRF, employing the follow-
ing indicator features:
? a five-character window around the current
character: for each ?2 ? ? ? 2 and 1 ? i ?
N , the triple (x
i+?
, ?, y
i
)
? n-grams consisting of the current character
and up to three preceding characters: for
each 2 ? n ? 4 and n ? i ? N ,
the character-sequence/label-sequence pair
(x
i?n+1
. . . x
i
, y
i?n+1
. . . y
i
)
? whether the current character is punctuation
? whether the current character is a digit
? the Unicode block of the current character
? the Unicode character class of the current char-
acter
In addition to these, we include two other types of
features motivated by specific errors the original
system made on Egyptian dialect development data:
? Word length and position within a word: for
each 1 ? i ? N , the pairs (`, y
i
), (a, y
i
), and
(b, y
i
), where `, a, and b are the total length
of the word containing x
i
, the number of char-
acters after x
i
in the word, and the number of
characters before x
i
in the word, respectively.
Some incorrect segmentations produced by
the original system could be ruled out with the
knowledge of these statistics.
? First and last two characters of the current
word, separately influencing the first two
labels and the last two labels: for each
word consisting of characters x
s
. . . x
t
, the tu-
ples (x
s
x
s+1
, x
t?1
x
t
, y
s
y
s+1
, ?begin?) and
(x
s
x
s+1
, x
t?1
x
t
, y
t?1
y
t
, ?end?). This set of
features addresses a particular dialectal Arabic
construction, the negation A? m
?
a- + [verb] +

? -sh, which requires a matching prefix and
suffix to be segmented simultaneously. This
feature set alo allows themodel to take into ac-
count other interactions between the beginning
and end of a word, particularly those involving
the definite article ? @ al-.
A notable property of this feature set is that it re-
mains highly dialect-agnostic, even though our ad-
ditional features were chosen in response to errors
made on text in Egyptian dialect. In particular,
it does not depend on the existence of a dialect-
specific lexicon or morphological analyzer. As a
result, we expect this model to perform similarly
well when applied to other Arabic dialects.
2.2 Domain adaptation
In this work, we train our model to segment Arabic
text drawn from three domains: newswire, which
consists of formal text in MSA; broadcast news,
which contains scripted, formal MSA as well as
extemporaneous dialogue in a mix of MSA and di-
alect; and discussion forum posts written primarily
in Egyptian dialect.
207
F1
(%) TEDEval (%)
Model Training Data ATB BN ARZ ATB BN ARZ
GD ATB 97.60 94.87 79.92 98.22 96.81 87.30
GD +BN+ARZ 97.28 96.37 92.90 98.05 97.45 95.01
+Rew ATB 97.55 94.95 79.95 98.72 97.45 87.54
+Rew +BN 97.58 96.60 82.94 98.75 98.18 89.43
+Rew +BN+ARZ 97.30 96.09 92.64 98.59 97.91 95.03
+Rew+DA +BN+ARZ 97.71 96.57 93.87 98.79 98.14 95.86
+Rew+DA+Feat +BN+ARZ 98.36 97.35 95.06 99.14 98.57 96.67
Table 1: Development set results. GD is the model of Green and DeNero (2012). Rew is support for
orthographic rewrites with the RewAl and RewTa labels. The fifth row shows the strongest baseline,
which is the GD+Rew model trained on the concatenated training sets from all three treebanks. DA is
domain adaptation via feature space augmentation. Feat adds the additional feature templates described
in section 2.1. ATB is the newswire ATB; BN is the Broadcast News treebank; ARZ is the Egyptian
treebank. Best results (bold) are statistically significant (p < 0.001) relative to the strongest baseline.
The approach to domain adaptation we use is
that of feature space augmentation (Daum?, 2007).
Each indicator feature from the model described
in Section 2.1 is replaced by N + 1 features in
the augmented model, where N is the number of
domains from which the data is drawn (here, N =
3). These N + 1 features consist of the original
feature and N ?domain-specific? features, one for
each of theN domains, each of which is active only
when both the original feature is present and the
current text comes from its assigned domain.
3 Experiments
We train and evaluate on three corpora: parts 1?3 of
the newswire Arabic Treebank (ATB),
1
the Broad-
cast News Arabic Treebank (BN),
2
and parts 1?8
of the BOLT Phase 1 Egyptian Arabic Treebank
(ARZ).
3
These correspond respectively to the do-
mains in section 2.2. We target the segmentation
scheme used by these corpora (leaving morphologi-
cal affixes and the definite article attached). For the
ATB, we use the same split as Chiang et al (2006).
For each of the other two corpora, we split the data
into 80% training, 10% development, and 10% test
in chronological order by document.
4
We train the
Green and DeNero model and our improvements
using L-BFGS with L
2
regularization.
1
LDC2010T13, LDC2011T09, LDC2010T08
2
LDC2012T07
3
LDC2012E{93,98,89,99,107,125}, LDC2013E{12,21}
4
These splits are publicly available at
http://nlp.stanford.edu/software/parser-
arabic-data-splits.shtml.
3.1 Evaluation metrics
We use two evaluation metrics in our experiments.
The first is an F
1
precision-recall measure, ignoring
orthographic rewrites. F
1
scores provide a more
informative assessment of performance than word-
level or character-level accuracy scores, as over 80%
of tokens in the development sets consist of only
one segment, with an average of one segmentation
every 4.7 tokens (or one every 20.4 characters).
The second metric we use is the TEDEval met-
ric (Tsarfaty et al, 2012). TEDEval was devel-
oped to evaluate joint segmentation and parsing
5
in Hebrew, which requires a greater variety of or-
thographic rewrites than those possible in Arabic.
Its edit distance-based scoring algorithm is robust
enough to handle the rewrites produced by both
MADA and our segmenter.
We measure the statistical significance of differ-
ences in these metrics with an approximate ran-
domization test (Yeh, 2000; Pad?, 2006), with
R = 10,000 samples.
3.2 Results
Table 1 contains results on the development set
for the model of Green and DeNero and our im-
provements. Using domain adaptation alone helps
performance on two of the three datasets (with a sta-
tistically insignificant decrease on broadcast news),
and that our additional features further improve
5
In order to evaluate segmentation in isolation, we convert
each segmented sentence from both the model output and
the gold standard to a flat tree with all segments descending
directly from the root.
208
F1
(%) TEDEval (%)
ATB BN ARZ ATB BN ARZ
MADA 97.36 94.54 78.35 97.62 96.96 86.78
MADA-ARZ 92.83 91.89 90.76 91.26 91.10 90.39
GD+Rew+DA+Feat 98.30 97.17 95.13 99.10 98.42 96.75
Table 2: Test set results. Our final model (last row) is trained on all available data (ATB+BN+ARZ). Best
results (bold) are statistically significant (p < 0.001) relative to each MADA version.
ATB BN ARZ
MADA 705.6 ? 5.1 472.0 ? 0.8 767.8 ? 1.9
MADA-ARZ 784.7 ? 1.6 492.1 ? 4.2 779.0 ? 2.7
GD+Rew+DA+Feat 90.0 ? 1.0 59.5 ? 0.3 72.7 ? 0.2
Table 3: Wallclock time (in seconds) for MADA, MADA-ARZ, and our model for decoding each of
the three development datasets. Means and standard deviations were computed for 10 independent runs.
MADA and MADA-ARZ are single-threaded. Our segmenter supports multithreaded execution, but the
times reported here are for single-threaded runs.
segmentation on all datasets. Table 2 shows the
segmentation scores our model achieves when eval-
uated on the three test sets, as well as the results for
MADA and MADA-ARZ. Our segmenter achieves
higher scores than MADA and MADA-ARZ on all
datasets under both evaluation metrics. In addi-
tion, our segmenter is faster than MADA. Table 3
compares the running times of the three systems.
Our segmenter achieves a 7x or more speedup over
MADA and MADA-ARZ on all datasets.
4 Error Analysis
We sampled 100 errors randomly from all errors
made by our final model (trained on all three
datasets with domain adaptation and additional fea-
tures) on the ARZ development set; see Table 4.
These errors fall into three general categories:
? typographical errors and annotation inconsis-
tencies in the gold data;
? errors that can be fixed with a fuller analysis
of just the problematic token, and therefore
represent a deficiency in the feature set; and
? errors that would require additional context or
sophisticated semantic awareness to fix.
4.1 Typographical errors and annotation
inconsistencies
Of the 100 errors we sampled, 33 are due to typo-
graphical errors or inconsistencies in the gold data.
We classify 7 as typos and 26 as annotation incon-
sistencies, although the distinction between the two
is murky: typos are intentionally preserved in the
treebank data, but segmentation of typos varies de-
pending on how well they can be reconciled with
standard Arabic orthography. Four of the seven
typos are the result of a missing space, such as:
? ?


?AJ



??AK
.
Q??


yashar-bi-?l-lay
?
al
?
? ?staysawakeat-
night? (Q?D?


yashar + K
.
bi- + ?


?AJ



?? @ al-lay
?
al
?
?)
?
	
?

@ A
	
J

J??
?
?amilatn
?
a-?an ?madeus? (

I??
?
?amilat + A
	
J -n
?
a +
	
?

@ ?an)
The first example is segmented in the Egyptian tree-
bank but is left unsegmented by our system; the
second is left as a single token in the treebank but is
split into the above three segments by our system.
Of the annotation inconsistencies that do not in-
volve typographical errors, a handful are segmen-
tation mistakes; however, in the majority of these
cases, the annotator chose not to segment a word
for justifiable but arbitrary reasons. In particular, a
few colloquial ?filler? expressions are sometimes
not segmented, despite being compound Arabic
words that are segmented elsewhere in the data.
These include A
	
J

K
.
P rabbin
?
a ?[our] Lord? (oath);
A?Y
	
J? ?indam
?
a ?when?/?while?; and ?J



?
	
g khall
?
?-
k ?keep?/?stay?. Also, tokens containing foreign
words are sometimes not segmented, despite car-
rying Arabic affixes. An example of this is
Q

???
209
Category # of errors
Abnormal gold data 33
Typographical error 7
Annotation inconsistency 26
Need full-token features 36
Need more context 31
B? wl
?
a 5
A
	
J -n
?
a: verb/pron 7
?


 -y: nisba/pron 4
other 15
Table 4: Counts of error categories (out of 100
randomly sampled ARZ development set errors).
wamistur ?andMister [English]?, which could be
segmented as ? wa- +
Q

?? mistur.
4.2 Features too local
In 36 of the 100 sampled errors, we conjecture that
the presence of the error indicates a shortcoming
of the feature set, resulting in segmentations that
make sense locally but are not plausible given the
full token. Two examples of these are:
?

?

?K


Q?
	
?? wafit
.
ar
?
?qah ?and in the way? seg-
mented as ? wa- +

?

?K


Q?
	
? fit
.
ar
?
?qah (correct
analysis is ? wa- + 
	
? fi- +

?

?K


Q? t
.
ar
?
?qah).
Q?
	
? ft
.
r ?break?/?breakfast? is a common Ara-
bic root, but the presence of

? q should indi-
cate that Q?
	
? ft
.
r is not the root in this case.
? ??

??E


B? wal
?
ayuhimmhum ?and it?s not im-
portant to them? segmented as ? wa- + ?
li- +

??E


A -ayuhimm + ?? -hum (correct
analysis is ? wa- + B l
?
a +

??E


yuhimm +
?? -hum). The 4-character window ?K


B l
?
ayh
occurs commonly with a segment boundary
after the ? l, but the segment

??E


A -ayuhimm
is not a well-formed Arabic word.
4.3 Context-sensitive segmentations and
multiple word senses
In the remaining 31 of 100 errors, external context
is needed. In many of these, it is not clear how to
address the error without sophisticated semantic
reasoning about the surrounding sentence.
One token accounts for five of these errors: B?
wl
?
a, which in Egyptian dialect can be analyzed as
? wa- + B l
?
a ?and [do/does] not? or as B

? wall
?
a
?or?. In a few cases, either is syntactically correct,
and the meaning must be inferred from context.
Two other ambiguities are a frequent cause of
error and seem to require sophisticated disambigua-
tion. The first is A
	
J -n
?
a, which is both a first person
plural object pronoun and a first person plural past
tense ending. The former is segmented, while the
latter is not. An example of this is the pair A
	
J???
?ilmun
?
a ?our knowledge? (??
?
?ilmu + A
	
J -n
?
a) ver-
sus A
	
J??? ?alimn
?
a ?we knew? (one segment). The
other is ?


 -y, which is both a first person singular
possessive pronoun and the nisba adjective ending
(which turns a noun into an adjective meaning ?of
or related to?); only the former is segmented. One
example of this distinction that appeared in the de-
velopment set is the pair ?


??
	
??? mawd
.
?
u?
?
? ?my
topic? (??
	
??? mawd
.
?
u?+ ?


 -y) versus

?


??
	
???
mawd
.
?
u?
?
?y ?topical?, ?objective?.
5 Conclusion
In this paper we demonstrate substantial gains on
Arabic clitic segmentation for both formal and
dialectal text using a single model with dialect-
independent features and a simple domain adap-
tation strategy. We present a new Arabic segmenter
which performs better than tools employing sophis-
ticated linguistic analysis, while also giving im-
pressive speed improvements. We evaluated our
segmenter on broadcast news and Egyptian Arabic
due to the current availability of annotated data in
these domains. However, as data for other Arabic di-
alects and genres becomes available, we expect that
the model?s simplicity and the domain adaptation
method we use will allow the system to be applied
to these dialects with minimal effort and without a
loss of performance in the original domains.
Acknowledgments
We thank the three anonymous reviewers, and
Reut Tsarfaty for valuable correspondence regard-
ing TEDEval. The second author is supported
by a National Science Foundation Graduate Re-
search Fellowship. This work was supported by
the Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the view of DARPA
or the US government.
210
References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. InWMT.
David Chiang, Mona T. Diab, Nizar Habash, Owen
Rambow, and Safiullah Shareef. 2006. Parsing Ara-
bic dialects. In EACL.
Hal Daum?, III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In ACL.
Spence Green and Christopher D. Manning. 2010. Bet-
ter Arabic parsing: Baselines, evaluations, and anal-
ysis. In COLING.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In NAACL, Short Papers.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In MEDAR.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological anal-
ysis and disambiguation for dialectal Arabic. InHLT-
NAACL.
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Si-
ivola, Teemu Hirsim?ki, Janne Pylkk?nen, Tanel
Alum?e, and Murat Saraclar. 2006. Unlimited
vocabulary speech recognition for agglutinative lan-
guages. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
Mona Diab, Nizar Habash, Owen Rambow, and
Dalila Tabessi. 2006. Developing and using a pilot
dialectal Arabic treebank. In LREC.
Sebastian Pad?, 2006. User?s guide to sigf:
Significance testing by approximate randomisa-
tion. http://www.nlpado.de/~sebastian/
software/sigf.shtml.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Joint evaluation of morphological segmenta-
tion and syntactic parsing. In ACL, Short Papers.
Reut Tsarfaty. 2006. Integrated morphological and
syntactic disambiguation for Modern Hebrew. In
COLING-ACL.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In COLING.
211
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 148?153,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Feature-Rich Phrase-based Translation: Stanford University?s Submissionto the WMT 2013 Translation Task
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt*, John Bauer
Sida Wang, Natalia Silveira?, Julia Neidert and Christopher D. Manning
Computer Science Department, Stanford University
*Center for East Asian Studies, Stanford University
?Department of Linguistics, Stanford University
{spenceg,cerd,kreschke,robvoigt,horatio,sidaw,natalias,jneid,manning}@stanford.edu
Abstract
We describe the Stanford University NLP
Group submission to the 2013 Workshop
on Statistical Machine Translation Shared
Task. We demonstrate the effectiveness of a
new adaptive, online tuning algorithm that
scales to large feature and tuning sets. For
both English-French and English-German,
the algorithm produces feature-rich mod-
els that improve over a dense baseline and
compare favorably to models tuned with
established methods.
1 Introduction
Green et al (2013b) describe an online, adaptive
tuning algorithm for feature-rich translation mod-
els. They showed considerable translation quality
improvements over MERT (Och, 2003) and PRO
(Hopkins and May, 2011) for two languages in a
research setting. The purpose of our submission to
the 2013 Workshop on Statistical Machine Trans-
lation (WMT) Shared Task is to compare the algo-
rithm to more established methods in an evaluation.
We submitted English-French (En-Fr) and English-
German (En-De) systems, each with over 100k fea-
tures tuned on 10k sentences. This paper describes
the systems and also includes new feature sets and
practical extensions to the original algorithm.
2 Translation Model
Our machine translation (MT) system is Phrasal
(Cer et al, 2010), a phrase-based system based on
alignment templates (Och and Ney, 2004). Like
many MT systems, Phrasal models the predictive
translation distribution p(e|f ;w) directly as
p(e|f ;w) = 1Z(f) exp
[
w>?(e, f)
]
(1)
where e is the target sequence, f is the source se-
quence, w is the vector of model parameters, ?(?)
is a feature map, and Z(f) is an appropriate nor-
malizing constant. For many years the dimension
of the feature map ?(?) has been limited by MERT,
which does not scale past tens of features.
Our submission explores real-world translation
quality for high-dimensional feature maps and as-
sociated weight vectors. That case requires a more
scalable tuning algorithm.
2.1 Online, Adaptive Tuning Algorithm
FollowingHopkins andMay (2011) we castMT tun-
ing as pairwise ranking. Consider a single source
sentence f with associated references e1:k. Let d
be a derivation in an n-best list of f that has the
target e = e(d) and the feature map ?(d). Define
the linear model scoreM(d) = w ? ?(d). For any
derivation d+ that is better than d? under a gold
metric G, we desire pairwise agreement such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011). Suppose that we sample
s pairs for source sentence ft to compute a set of
difference vectors Dt = {x1:s+ }. Then we optimize
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(2)
which is the familiar logistic loss. Hopkins and
May (2011) optimize (2) in a batch algorithm
that alternates between candidate generation (i.e.,
n-best list or lattice decoding) and optimization
(e.g., L-BFGS). We instead use AdaGrad (Duchi
148
et al, 2011), a variant of stochastic gradient de-
scent (SGD) in which the learning rate is adapted
to the data. Informally, AdaGrad scales the weight
updates according to the geometry of the data ob-
served in earlier iterations. Consider a particu-
lar dimension j of w, and let scalars vt = wt,j ,
gt = ?j`t(wt?1), and Gt = ?ti=1 g2i . The Ada-Grad update rule is
vt = vt?1 ? ? G?1/2t gt (3)
Gt = Gt?1 + g2t (4)
In practice,Gt is a diagonal approximation. IfGt =
I , observe that (3) is vanilla SGD.
In MT systems, the feature map may generate
exponentially many irrelevant features, so we need
to regularize (3). The L1 norm of the weight vec-
tor is known to be an effective regularizer in such
a setting (Ng, 2004). An efficient way to apply
L1 regularization is the Forward-Backward split-
ting (FOBOS) framework (Duchi and Singer, 2009),
which has the following two-step update:
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (5)
wt = argmin
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(6)
where (5) is just an unregularized gradient descent
step and (6) balances the regularization term r(w)
with staying close to the gradient step.
For L1 regularization we have r(w) = ?||w||1
and the closed-form solution to (6) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(7)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls below
the threshold ?t?1?.
Online algorithms are inherently sequential; this
algorithm is no exception. If we want to scale the
algorithm to large tuning sets, then we need to par-
allelize the weight updates. Green et al (2013b)
describe the parallelization technique that is imple-
mented in Phrasal.
2.2 Extensions to (Green et al, 2013b)
Sentence-Level Metric We previously used the
gold metric BLEU+1 (Lin and Och, 2004), which
smoothes bigram precisions and above. This metric
worked well with multiple references, but we found
that it is less effective in a single-reference setting
like WMT. To make the metric more robust, Nakov
et al (2012) extended BLEU+1 by smoothing both
the unigram precision and the reference length. We
found that this extension yielded a consistent +0.2
BLEU improvement at test time for both languages.
Subsequent experiments on the data sets of Green
et al (2013b) showed that standard BLEU+1 works
best for multiple references.
Custom regularization parameters Green et al
(2013b) showed that large feature-rich models over-
fit the tuning sets. We discovered that certain fea-
tures caused greater overfitting than others. Custom
regularization strengths for each feature set are one
solution to this problem. We found that technique
largely fixed the overfitting problem as shown by
the learning curves presented in section 5.1.
Convergence criteria Standard MERT imple-
mentations approximate tuning BLEU by re-
ranking the previous n-best lists with the updated
weight vector. This approximation becomes infeasi-
ble for large tuning sets, and is less accurate for algo-
rithms like ours that do not accumulate n-best lists.
We approximate tuning BLEU by maintaining the
1-best hypothesis for each tuning segment. At the
end of each epoch, we compute corpus-level BLEU
from this hypothesis set. We flush the set of stored
hypotheses before the next epoch begins. Although
memory-efficient, we find that this approximation
is less dependable as a convergence criterion than
the conventional method. Whereas we previously
stopped the algorithm after four iterations, we now
select the model according to held-out accuracy.
3 Feature Sets
3.1 Dense Features
The baseline ?dense? model has 19 features: the
nine Moses (Koehn et al, 2007) baseline features, a
hierarchical lexicalized re-ordering model (Galley
and Manning, 2008), the (log) bitext count of each
translation rule, and an indicator for unique rules.
The final dense feature sets for each language
differ slightly. The En-Fr system incorporates a
second language model. The En-De system adds a
future cost component to the linear distortion model
(Green et al, 2010).The future cost estimate allows
the distortion limit to be raised without a decrease
in translation quality.
149
3.2 Sparse Features
Sparse features do not necessarily fire on each hy-
pothesis extension. Unlike prior work on sparseMT
features, our feature extractors do not filter features
based on tuning set counts. We instead rely on the
regularizer to select informative features.
Several of the feature extractors depend on
source-side part of speech (POS) sequences and
dependency parses. We created those annotations
with the Stanford CoreNLP pipeline.
Discriminative Phrase Table A lexicalized in-
dicator feature for each rule in a derivation. The
feature weights can be interpreted as adjustments
to the associated dense phrase table features.
Discriminative Alignments A lexicalized indi-
cator feature for the phrase-internal alignments in
each rule in a derivation. For one-to-many, many-to-
one, and many-to-many alignments we extract the
clique of aligned tokens, perform a lexical sort, and
concatenate the tokens to form the feature string.
Discriminative Re-ordering A lexicalized indi-
cator feature for each rule in a derivation that ap-
pears in the following orientations: monotone-with-
next, monotone-with-previous, non-monotone-
with-next, non-monotone-with-previous. Green
et al (2013b) included the richer non-monotone
classes swap and discontinuous. However, we found
that these classes yielded no significant improve-
ment over the simpler non-monotone classes. The
feature weights can be interpreted as adjustments
to the generative lexicalized re-ordering model.
Source Content-Word Deletion Count-based
features for source content words that are ?deleted?
in the target. Content words are nouns, adjectives,
verbs, and adverbs. A deleted source word is ei-
ther unaligned or aligned to one of the 100 most
frequent target words in the target bitext. For each
deleted word we increment both the feature for the
particular source POS and an aggregate feature for
all parts of speech. We add similar but separate
features for head content words that are either un-
aligned or aligned to frequent target words.
Inverse Document Frequency Numeric fea-
tures that compare source and target word frequen-
cies. Let idf(?) return the inverse document fre-
quency of a token in the training bitext. Suppose
a derivation d = {r1, r2, . . . , rn} is composed of
n translation rules, where e(r) is the target side of
the rule and f(r) is the source side. For each rule
Bilingual Monolingual
Sentences Tokens Tokens
En-Fr 5.0M 289M 1.51B
En-De 4.4M 223M 1.03B
Table 1: Gross corpus statistics after data selection
and pre-processing. The En-Fr monolingual counts
include French Gigaword 3 (LDC2011T10).
r that translates j source tokens to i target tokens
we compute
q =
?
i
idf(e(r)i)?
?
j
idf(f(r)j) (8)
We add two numeric features, one for the source and
another for the target. When q > 0 we increment
the target feature by q; when q < 0 we increment
the target feature by |q|. Together these features
penalize asymmetric rules that map rare words to
frequent words and vice versa.
POS-based Re-ordering The lexicalized dis-
criminative re-ordering model is very sparse, so we
added re-ordering features based on source parts of
speech. When a rule is applied in a derivation, we
extract the associated source POS sequence along
with the POS sequences from the previous and next
rules. We add a ?with-previous? indicator feature
that is the conjunction of the current and previous
POS sequences; the ?with-next? indicator feature is
created analogously. This feature worked well for
En-Fr, but not for En-De.
4 Data Preparation
Table 1 describes the pre-processed corpora from
which our systems are built.
4.1 Data Selection
We used all of the monolingual and parallel En-
De data allowed in the constrained condition. We
incorporated all of the French monolingual data,
but sampled a 5M-sentence bitext from the approx-
imately 40M available En-Fr parallel sentences.
To select the sentences we first created a ?target?
corpus by concatenating the tuning and test sets
(newstest2008?2013). Then we ran the feature
decay algorithm (FDA) (Bi?ici and Yuret, 2011),
which samples sentences that most closely resem-
ble the target corpus. FDA is a principled method
for reducing the phrase table size by excluding less
relevant training examples.
150
4.2 Tokenization
We tokenized the English (source) data according
to the Penn Treebank standard (Marcus et al, 1993)
with Stanford CoreNLP. The French data was to-
kenized with packages from the Stanford French
Parser (Green et al, 2013a), which implements a
scheme similar to that used in the French Treebank
(Abeill? et al, 2003).
German is more complicated due to pervasive
compounding. We first tokenized the data with the
same English tokenizer. Then we split compounds
with the lattice-based model (Dyer, 2009) in cdec
(Dyer et al, 2010). To simplify post-processing we
added segmentation markers to split tokens, e.g.,
?berschritt? ?ber #schritt.
4.3 Alignment
We aligned both bitexts with the Berkeley Aligner
(Liang et al, 2006) configured with standard set-
tings. We symmetrized the alignments according
to the grow-diag heuristic.
4.4 Language Modeling
We estimated unfiltered 5-gram language models
using lmplz (Heafield et al, 2013) and loaded them
with KenLM (Heafield, 2011). For memory effi-
ciency and faster loading we also used KenLM to
convert the LMs to a trie-based, binary format. The
German LM included all of the monolingual data
plus the target side of the En-De bitext. We built
an analogous model for French. In addition, we
estimated a separate French LM from the Gigaword
data.1
4.5 French Agreement Correction
In French verbs must agree in number and person
with their subjects, and adjectives (and some past
participles) must agree in number and gender with
the nouns they modify. On their own, phrasal align-
ment and target side language modeling yield cor-
rect agreement inflection most of the time. For
verbs, we find that the inflections are often accurate:
number is encoded in the English verb and subject,
and 3rd person is generally correct in the absence
of a 1st or 2nd person pronoun. However, since En-
glish does not generally encode gender, adjective
inflection must rely on language modeling, which
is often insufficient.
1The MT system learns significantly different weights for
the two LMs: 0.086 for the primary LM and 0.044 for the
Gigaword LM.
To address this problem we apply an automatic
inflection correction post-processing step. First, we
generate dependency parses of our system?s out-
put using BONSAI (Candito and Crabb?, 2009),
a French-specific extension to the Berkeley Parser
(Petrov et al, 2006). Based on these dependencies,
we match adjectives with the nouns they modify
and past participles with their subjects. Then we
use Lefff (Sagot, 2010), a machine-readable French
lexicon, to determine the gender and number of the
noun and to choose the correct inflection for the
adjective or participle.
Applied to our 3,000 sentence development set,
this correction scheme produced 200 corrections
with perfect accuracy. It produces a slight (?0.014)
drop in BLEU score. This arises from cases where
the reference translation uses a synonymous but
differently gendered noun, and consequently has
different adjective inflection.
4.6 German De-compounding
Split German compounds must be merged after
translation. This process often requires inserting
affixes (e.g., s, en) between adjacent tokens in the
compound. Since the German compounding rules
are complex and exception-laden, we rely on a dic-
tionary lookup procedure with backoffs. The dic-
tionary was constructed during pre-processing. To
compound the final translations, we first lookup
the compound sequence?which is indicated by
segmentation markers?in the dictionary. If it is
present, then we use the dictionary entry. If the com-
pound is novel, then for each pair of words to be
compounded, we insert the suffix most commonly
appended in compounds to the first word of the pair.
If the first word itself is unknown in our dictionary,
we insert the suffix most commonly appended after
the last three characters. For example, words end-
ing with ung most commonly have an s appended
when they are used in compounds.
4.7 Recasing
Phrasal includes an LM-based recaser (Lita et al,
2003), which we trained on the target side of the
bitext for each language. On the newstest2012 de-
velopment data, the German recaser was 96.8% ac-
curate and the French recaser was 97.9% accurate.
5 Translation Quality Experiments
During system development we tuned on
newstest2008?2011 (10,570 sentences) and tested
151
#iterations #features tune newstest2012 newstest2013?
Dense 10 20 30.26 31.12 ?
Feature-rich 11 207k 32.29 31.51 29.00
Table 2: En-Fr BLEU-4 [% uncased] results. The tuning set is newstest2008?2011. (?) newstest2013 is
the cased score computed by the WMT organizers.
#iterations #features tune newstest2012 newstest2013?
Dense 10 19 16.83 18.45 ?
Feature-rich 13 167k 17.66 18.70 18.50
Table 3: En-De BLEU-4 [% uncased] results.
on newstest2012 (3,003 sentences). We compare
the feature-rich model to the ?dense? baseline.
The En-De system parameters were: 200-best
lists, a maximum phrase length of 8, and a distortion
limit of 6 with future cost estimation. The En-Fr
system parameters were: 200-best lists, a maximum
phrase length of 8, and a distortion limit of 5.
The online tuning algorithm used a default learn-
ing rate ? = 0.03 and a mini-batch size of 20. We
set the regularization strength ? to 10.0 for the dis-
criminative re-ordering model, 0.0 for the dense
features, and 0.1 otherwise.
5.1 Results
Tables 2 and 3 show En-Fr and En-De results, re-
spectively. The ?Feature-rich? model, which con-
tains the full complement of dense and sparse fea-
tures, offers ameager improvement over the ?Dense?
baseline. This result contrasts with the results
of Green et al (2013b), who showed significant
translation quality improvements over the same
dense baseline for Arabic-English and Chinese-
English. However, they had multiple target refer-
ences, whereas the WMT data sets have just one.
We speculate that this difference is significant. For
example, consider a translation rule that rewrites
to a 4-gram in the reference. This event can in-
crease the sentence-level score, thus encouraging
the model to upweight the rule indicator feature.
More evidence of overfitting can be seen in Fig-
ure 1, which shows learning curves on the devel-
opment set for both language pairs. Whereas the
dense model converges after just a few iterations,
the feature-rich model continues to creep higher.
Separate experiments on a held-out set showed that
generalization did not improve after about eight
iterations.
6 Conclusion
We submitted a feature-rich MT system to WMT
2013. While sparse features did offer a measur-
able improvement over a baseline dense feature set,
the gains were not as significant as those shown
by Green et al (2013b). One important difference
between the two sets of results is the number of ref-
erences. Their NIST tuning and test sets had four
references; the WMT data sets have just one. We
speculate that sparse features tend to overfit more
in this setting. Individual features can greatly in-
fluence the sentence-level metric and thus become
large components of the gradient. To combat this
phenomenon we experimented with custom reg-
ularization strengths and a more robust sentence-
level metric. While these two improvements greatly
reduced the model size relative to (Green et al,
2013b), a generalization problem remained. Nev-
ertheless, we showed that feature-rich models are
now competitive with the state-of-the-art.
Acknowledgments This work was supported by the Defense
Advanced Research Projects Agency (DARPA) Broad Opera-
tional Language Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of DARPA or the US government.
References
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building
a treebank for French, chapter 10. Kluwer.
E. Bi?ici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
M. Candito and B. Crabb?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT.
152
ll l l l l
l l l l
l
l
l l
l l
l l
l l
29
30
31
32
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(a) En-Fr tuning
l
l
l l l l l l
l l
l
l
l l l
l l
l l l
7.5
10.0
12.5
15.0
17.5
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(b) En-De tuning
Figure 1: BLEU-4 [% uncased] Learning curves on newstest2008?2011 with loess trend lines.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In NAACL.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In HLT-NAACL.
S. Green, M-C. de Marneffe, and C. D. Manning.
2013a. Parsing models for identifying multiword
expressions. Computational Linguistics, 39(1):195?
227.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL.
B. Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In LREC.
153
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 114?121,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Phrasal: A Toolkit for New Directions in Statistical Machine Translation
Spence Green, Daniel Cer, and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,danielcer,manning}@stanford.edu
Abstract
We present a new version of Phrasal, an
open-source toolkit for statistical phrase-
based machine translation. This revision
includes features that support emerging re-
search trends such as (a) tuning with large
feature sets, (b) tuning on large datasets like
the bitext, and (c) web-based interactive ma-
chine translation. A direct comparison with
Moses shows favorable results in terms of
decoding speed and tuning time.
1 Introduction
In the early part of the last decade, phrase-based ma-
chine translation (MT) (Koehn et al., 2003) emerged
as the preeminent design of statistical MT systems.
However, most systems were proprietary or closed-
source, so progress was initially constrained by
the high engineering barrier to entry into the field.
Then Moses (Koehn et al., 2007) was released.
What followed was a flowering of work on all as-
pects of the translation problem, from rule extrac-
tion to deployment issues. Other toolkits appeared
including Joshua (Post et al., 2013), Jane (Wuebker
et al., 2012), cdec (Dyer et al., 2010) and the first
version of our package, Phrasal (Cer et al., 2010), a
Java-based, open source package.
This paper presents a completely re-designed
release of Phrasal that lowers the barrier to entry
into several exciting areas of MT research. First,
Phrasal exposes a simple yet flexible feature API for
building large-scale, feature-rich systems. Second,
Phrasal provides multi-threaded decoding and on-
line tuning for learning feature-rich models on very
large datasets, including the bitext. Third, Phrasal
supplies the key ingredients for web-based, inter-
active MT: an asynchronous RESTful JSON web
service implemented as a J2EE servlet, integrated
pre- and post-processing, and fast search.
Revisions to Phrasal were guided by several de-
sign choices. First, we optimized the system for
multi-core architectures, eschewing distributed in-
frastructure like Hadoop and MapReduce. While
?scaling-out? with distributed infrastructure is the
conventional industry and academic choice, we find
that ?scaling-up? on a single large-node is an at-
tractive yet overlooked alternative (Appuswamy et
al., 2013). A single ?scale-up? node is usually
competitive in terms of cost and performance, and
multi-core code has fewer dependencies in terms
of software and expertise. Second, Phrasal makes
extensive use of Java interfaces and reflection. This
is especially helpful in the feature API. A feature
function can be added to the system by simply im-
plementing an interface and specifying the class
name on the decoder command line. There is no
need to modify or recompile anything other than
the new feature function.
This paper presents a direct comparison of
Phrasal and Moses that shows favorable results
in terms of decoding speed and tuning time. An
indirect comparison via the WMT2014 shared
task (Neidert et al., 2014) showed that Phrasal
compares favorably to Moses in an evaluation
setting. The source code is freely available at:
http://nlp.stanford.edu/software/phrasal/
2 Standard System Pipeline
This section describes the steps required to build
a phrase-based MT system from raw text. Each
step is implemented as a stand-alone executable.
For convenience, the Phrasal distribution includes
a script that coordinates the steps.
2.1 Prerequisites
Phrasal assumes offline preparation of word align-
ments and at least one target-side language model.
Word Alignment The rule extractor can accom-
modate either unsymmetrized or symmetrized
alignments. Unsymmetrized alignments can be
produced with either GIZA++ or the Berkeley
Aligner (Liang et al., 2006). Phrasal then applies
symmetrization on-the-fly using heuristics such as
grow-diag or grow-diag-final. If the alignments are
symmetrized separately, then Phrasal accepts align-
114
ments in the i-j Pharaoh format, which indicates
that source token i is aligned to target token j.
Language Modeling Phrasal can load any n-
gram language model saved in the ARPA format.
There are two LM loaders. The Java-based loader is
used by default and is appropriate for small-scale ex-
periments and pure-Java environments. The C++
KenLM (Heafield, 2011) loader
1
is best for large-
scale LMs such as the unfiltered models produced
by lmplz (Heafield et al., 2013). Profiling shows
that LM queries often account for more than 50% of
the CPU time in a Phrasal decoding run, so we de-
signed the Phrasal KenLM loader to execute queries
mostly in C++ for efficiency. The KenLM bind-
ing efficiently passes full strings to C++ via JNI.
KenLM then iterates over the string, returning a
score and a state length. Phrasal can load multiple
language models, and includes native support for
the class-based language models that have become
popular in recent evaluations (Wuebker et al., 2012;
Ammar et al., 2013; Durrani et al., 2013).
2.2 Rule Extraction
The next step in the pipeline is extraction of a phrase
table. Phrasal includes a multi-threaded version
of the rule extraction algorithm of Och and Ney
(2004). Phrase tables can be filtered to a specific
data set?as is common in research environments.
When filtering, the rule extractor lowers memory
utilization by splitting the data into arbitrary-sized
chunks and extracting rules from each chunk.
The rule extractor includes a feature API that is
independent of the decoder feature API. This al-
lows for storage of static rule feature values in the
phrase table. Static rule features are useful in two
cases. First, if a feature value depends on bitext
statistics, which are not accessible during tuning
or decoding, then that feature should be stored in
the phrase table. Examples are the standard phrase
translation probabilities, and the dense rule count
and rule uniqueness indicators described by Green
et al. (2013). Second, if a feature depends only
on the rule and is unlikely to change, then it may
be more efficient to store that feature value in the
phrase table. An example is a feature template that
indicates inclusion in a specific data domain (Dur-
rani et al., 2013). Rule extractor feature templates
must implement the FeatureExtractor inter-
face and are loaded via reflection.
1
Invoked by prefixing the LM path with the ?kenlm:?.
The rule extractor can also create lexicalized re-
ordering tables. The standard phrase orientation
model (Tillmann, 2004) and the hierarchical model
of Galley and Manning (2008) are available.
2.3 Tuning
Once a language model has been estimated and a
phrase table has been extracted, the next step is to
estimate model weights. Phrasal supports tuning
over n-best lists, which permits rapid experimenta-
tion with different error metrics and loss functions.
Lattice-based tuning, while in principle more pow-
erful, requires metrics and losses that factor over
lattices, and in practice works no better than n-best
tuning (Cherry and Foster, 2012).
Tuning requires a parallel set {(f
t
, e
t
)}
T
t=1
of
source sentences f
t
and target references e
t
.
2
Phrasal follows the log-linear approach to phrase-
based translation (Och and Ney, 2004) in which
the predictive translation distribution p(e|f ;w) is
modeled directly as
p(e|f ;w) =
1
Z(f)
exp
[
w
>
?(e, f)
]
(1)
where w ? R
d
is the vector of model parameters,
?(?) ? R
d
is a feature map, and Z(f) is an appro-
priate normalizing constant.
MT differs from other machine learning settings
in that it is not common to tune to log-likelihood
under (1). Instead, a gold error metric G(e
?
, e) is
chosen that specifies the similarity between a hy-
pothesis e
?
and a reference e, and that error is min-
imized over the tuning set. Phrasal includes Java
implementations of BLEU (Papineni et al., 2002),
NIST, and WER, and bindings for TER (Snover et
al., 2006) and METEOR (Denkowski and Lavie,
2011). The error metric is incorporated into a loss
function ` that returns the loss at either the sentence-
or corpus- level.
For conventional corpus-level (batch) tuning,
Phrasal includes multi-threaded implementations
of MERT (Och, 2003) and PRO (Hopkins and
May, 2011). The MERT implementation uses the
line search of Cer et al. (2008) to directly min-
imize corpus-level error. The PRO implementa-
tion uses a pairwise logistic loss to minimize the
number of inversions in the ranked n-best lists.
These batch implementations accumulate n-best
lists across epochs.
2
For simplicity, we assume one reference, but the multi-
reference case is analogous.
115
Online tuning is faster and more scalable than
batch tuning, and sometimes leads to better solu-
tions for non-convex settings like MT (Bottou and
Bousquet, 2011). Weight updates are performed
after each tuning example is decoded, and n-best
lists are not accumulated. Consequently, online tun-
ing is preferable for large tuning sets, or for rapid
iteration during development. Phrasal includes the
AdaGrad-based (Duchi et al., 2011) tuner of Green
et al. (2013). The regularization options are L
2
,
efficient L
1
for feature selection (Duchi and Singer,
2009), or L
1
+ L
2
(elastic net). There are two on-
line loss functions: a pairwise (PRO) objective and
a listwise minimum expected error objective (Och,
2003). These online loss functions require sentence-
level error metrics, several of which are available in
the toolkit: BLEU+1 (Lin and Och, 2004), Nakov
BLEU (Nakov et al., 2012), and TER.
2.4 Decoding
The Phrasal decoder can be invoked either program-
matically as a Java object or as a standalone appli-
cation. In both cases the decoder is configured via
options that specify the language model, phrase
table, weight vector w, etc. The decoder is multi-
threaded, with one decoding instance per thread.
Each decoding instance has its own weight vector,
so in the programmatic case, it is possible to decode
simultaneously under different weight vectors.
Two search procedures are included. The default
is the phrase-based variant of cube pruning (Huang
and Chiang, 2007). The standard multi-stack beam
search (Och and Ney, 2004) is also an option. Ei-
ther procedure can be configured in one of several
recombination modes. The ?Pharaoh? mode only
considers linear distortion, source coverage, and
target LM history. The ?Exact? mode considers
these states in addition to any feature that declares
recombination state (see section 3.3).
The decoder includes several options for deploy-
ment environments such as an unknown word API,
pre-/post-processing APIs, and both full and prefix-
based force decoding.
2.5 Evaluation and Post-processing
All of the error metrics available for tuning can
also be invoked for evaluation. For significance
testing, the toolkit includes an implementation of
the permutation test of Riezler and Maxwell (2005),
which was shown to be less susceptible to Type-I
error than bootstrap re-sampling (Koehn, 2004).
r : s(r,w)
r ? R axiom
d : w(d) r : s(r,w)
d
?
: s(d
?
,w)
r /? cov(d) item
|cov(d)| = |s| goal
Table 1: Phrase-based MT as deductive inference.
This notation can be read as follows: if the an-
tecedents on the top are true, then the consequent
on the bottom is true subject to the conditions on
the right. The new item d
?
is creating by appending
r to the ordered sequence of rules that define d.
Phrasal also includes two truecasing packages.
The LM-based truecaser (Lita et al., 2003) requires
an LM estimated from cased, tokenized text. A
subsequent detokenization step is thus necessary. A
more convenient alternative is the CRF-based post-
processor that can be trained to invert an arbitrary
pre-processor. This post-processor can perform
truecasing and detokenization in one pass.
3 Feature API
Phrasal supports dynamic feature extraction dur-
ing tuning and decoding. In the API, feature tem-
plates are called featurizers. There are two types
with associated interfaces: RuleFeaturizer
and DerivationFeaturizer. One way to il-
lustrate these two featurizers is to consider phrase-
based decoding as a deductive system. Let r =
?f, e? be a rule in a set R, which is conventionally
called the phrase table. Let d = {r
i
}
N
i=1
be an
ordered sequence of derivation N rules called a
derivation, which specifies a translation for some
source input sequence s (which, by some abuse of
notation, is equivalent to f in Eq. (1)). Finally,
define functions cov(d) as the source coverage set
of d as a bit vector and s(?, w) as the score of a rule
or derivation under w.
3
The expression r /? cov(d)
means that r maps to an empty/uncovered span in
cov(d). Table 1 shows the deductive system.
3.1 Dynamic Rule Features
RuleFeaturizers are invoked when scoring axioms,
which do not require any derivation context. The
static rule features described in section 2.2 also
contribute to axiom scoring, and differ only from
RuleFeaturizers in that they are stored permanently
in the phrase table. In contrast, RuleFeaturizers
3
Note that s(d,w) = w
>
?(d) in the log-linear formulation
of MT (see Eq. (1)).
116
Listing 1: A RuleFeaturizer, which depends
only on a translation rule.
public class WordPenaltyFeaturizer
implements RuleFeaturizer {
@Override
public List<FeatureValue>
ruleFeaturize(Featurizable f) {
List<FeatureValue> features =
Generics.newLinkedList();
// Extract single feature
features.add(new FeatureValue(
"WordPenalty", f.targetPhrase.size()));
return features;
}
}
are extracted during decoding. An example feature
template is the word penalty, which is simply the
dimension of the target side of r (Listing 1).
Featurizable wraps decoder state from
which features can be extracted. RuleFeaturizers
are extracted during each phrase table query and
cached, so they can be simply efficiently retrieved
during decoding.
Once the feature is compiled, it is simply speci-
fied on the command-line when the decoder is exe-
cuted. No other configuration is required.
3.2 Derivation Features
DerivationFeaturizers are invoked when scoring
items, and thus depend on some derivation context.
An example is the LM, which requires the n-gram
context from d to score r when creating the new
hypothesis d
?
(Listing 2).
The LM featurizer first looks up the recombi-
nation state of the derivation, which contains the
n-gram context. Then it queries the LM by passing
the rule and context, and sets the new state as the
result of the LM query. Finally, it returns a feature
?LM? with the value of the LM query.
3.3 Recombination State
Listing 2 shows a state lookup during feature ex-
traction. Phrase-based MT feature design differs
significantly from that of convex classifiers in terms
of the interaction with inference. For example, in
a maximum entropy classifier inference is exact,
so a good optimizer can simply nullify bad fea-
tures to retain baseline accuracy. In contrast, MT
feature templates affect search through both future
cost heuristics and recombination state. Bad fea-
tures can introduce search errors and thus decrease
Listing 2: A DerivationFeaturizer, which
must lookup and save recombination state for ex-
traction.
public class NGramLanguageModelFeaturizer
extends DerivationFeaturizer {
@Override
public List<FeatureValue> featurize(
Featurizable f) {
// Get recombination state
LMState priorState = f.prior.getState(this);
// LM query
LMState state = lm.score(f.targetPhrase, priorState);
List<FeatureValue> features =
Generics.newLinkedList();
// Extract single feature
features.add(
new FeatureValue("LM", state.getScore()));
// Set new recombination state
f.setState(this, state);
return features;
}
}
accuracy, sometimes catastrophically.
The feature API allows DerivationFeaturizers
to explicitly declare recombination state via the
FeaturizerState interface.4 The interface re-
quires a state equality operator and a hash code
function. Then the search procedure will only re-
combine derivations with equal states. For example,
the state of the n-gram LM DerivationFeaturizer
(Listing 2) is the n-1 gram context, and the hash-
code is a hash of that context string. Only deriva-
tions for which the equality operator of LMState
returns true can be recombined.
4 Web Service
Machine translation output is increasingly uti-
lized in computer-assisted translation (CAT) work-
benches. To support deployment, Phrasal includes
a lightweight J2EE servlet that exposes a REST-
ful JSON API for querying a trained system. The
toolkit includes a standalone servlet container, but
the servlet may also be incorporated into a J2EE
server. The servlet requires just one input param-
eter: the Phrasal configuration file, which is also
used for tuning and decoding. Consequently, after
running the standard pipeline, the trained system
can be deployed with one command.
4
To control future cost estimation, the designer would need
to write a new heuristic that considers perhaps a subset of
the full feature map. There is a separate API for future cost
heuristics.
117
4.1 Standard Web Service
The standard web service supports two types of
requests. The first is TranslationRequest,
which performs full decoding on a source input.
The JSON message structure is:
Listing 3: TranslationRequest message.
TranslationRequest {
srcLang :(string),
tgtLang :(string),
srcText :(string),
tgtText :(string),
limit :(integer),
properties :(object)
}
The srcLang and tgtLang fields are ignored by
the servlet, but can be used by a middleware proxy
to route requests to Phrasal servlet instances, one
per language pair. The srcText field is the source
input, and properties is a Javascript associa-
tive array that can contain key/value pairs to pass
to the feature API. For example, we often use the
properties field to pass domain information
with each request.
Phrasal will perform full decoding and respond
with the message:
Listing 4: TranslationReply message,
which is returned upon successful processing of
TranslationRequest.
TranslationReply {
resultList :[
{tgtText :(string),
align :(string),
score :(float)
},...]
}
resultList is a ranked n-best list of transla-
tions, each with target tokens, word alignments,
and a score.
The second request type is RuleRequest,
which enables phrase table queries. These requests
are processed very quickly since decoding is not
required. The JSON message structure is:
Listing 5: RuleRequest message, which
prompts a direct lookup into the phrase table.
RuleRequest {
srcLang :(string),
tgtLang :(string),
srcText :(string),
limit :(integer),
properties :(object)
}
limit is the maximum number of translations to
return. The response message is analogous to that
for TranslationRequest, so we omit it.
4.2 Interactive Machine Translation
Interactive machine translation (Bisbey and Kay,
1972) pairs human and machine translators in hopes
of increasing the throughput of high quality trans-
lation. It is an old idea that is again in focus. One
challenge is to present relevant machine suggestions
to humans. To that end, Phrasal supports context-
sensitive translation queries via prefix decod-
ing. Consider again the TranslationRequest
message. When the tgtText field is empty, the
source input is decoded from scratch. But when
this field contains a prefix, Phrasal returns transla-
tions that begin with the prefix. The search proce-
dure force decodes the prefix, and then completes
the translation via conventional decoding. Conse-
quently, if the user has typed a partial translation,
Phrasal can suggest completions conditioned on
that prefix. The longer the prefix, the faster the de-
coding, since the user prefix constrains the search
space. This feature allows Phrasal to produce in-
creasingly precise suggestions as the user works.
5 Experiments
We compare Phrasal and Moses by restricting an
existing large-scale system to a set of common fea-
tures. We start with the Arabic?English system of
Green et al. (2014), which is built from 6.6M paral-
lel segments. The system includes a 5-gram English
LM estimated from the target-side of the bitext and
990M English monolingual tokens. The feature set
is their dense baseline, but without lexicalized re-
ordering and the two extended phrase table features.
This leaves the nine baseline features also imple-
mented by Moses. We use the same phrase table,
phrase table query limit (20), and distortion limit
(5) for both decoders. The tuning set (mt023568)
contains 5,604 segments, and the development set
(mt04) contains 1,075 segments.
We ran all experiments on a dedicated server with
16 physical cores and 128GB of memory.
Figure 1 shows single-threaded decoding time
of the dev set as a function of the cube pruning
pop limit. At very low limits Moses is faster than
Phrasal, but then slows sharply. In contrast, Phrasal
scales linearly and is thus faster at higher pop limits.
Figure 2 shows multi-threaded decoding time of
the dev set with the cube pruning pop limit fixed
at 1,200. Here Phrasal is initially faster, but Moses
becomes more efficient at four threads. There are
two possible explanations. First, profiling shows
that LM queries account for approximately 75%
118
l
l
l
l l
l
l l
l
l
l l
l l
l l
l
l
l
l100200 500 1000 1500 2000Pop LimitTime (seconds) Systeml PhrasalMoses
Figure 1: Development set decoding time as a
function of the cube pruning pop limit.
of the Phrasal CPU-time. KenLM is written in
C++, and Phrasal queries it via JNI. It appears
as though multi-threading across this boundary is
a source of inefficiency. Second, we observe that
the Java parallel garbage collector (GC) runs up to
seven threads, which become increasingly active
as the number of decoder threads increases. These
and other Java overhead threads must be scheduled,
limiting gains as the number of decoding threads
approaches the number of physical cores.
Finally, Figure 3 shows tuning BLEU as a func-
tion of wallclock time. For Moses we chose the
batch MIRA implementation of Cherry and Fos-
ter (2012), which is popular for tuning feature-rich
systems. Phrasal uses the online tuner with the ex-
pected BLEU objective (Green et al., 2014). Moses
achieves a maximum BLEU score of 47.63 after
143 minutes of tuning, while Phrasal reaches this
level after just 17 minutes, later reaching a maxi-
mum BLEU of 47.75 after 42 minutes. Much of
the speedup can be attributed to phrase table and
LM loading time: the Phrasal tuner loads these data
structures just once, while the Moses tuner loads
them every epoch. Of course, this loading time be-
comes more significant with larger-scale systems.
6 Conclusion
We presented a revised version of Phrasal, an open-
source, phrase-based MT toolkit. The revisions
support new directions in MT research including
feature-rich models, large-scale tuning, and web-
l
l
l
l
l
l l l l l l l l l l l
50100150200 4 8 12 16ThreadsTime (seconds) Systeml PhrasalMoses
Figure 2: Development set decoding time as a
function of the threadpool size.
l
l
llll
lllllllllllllllllll45464748 0 100 200Time (minutes)Approx. BLEU?4 Systeml PhrasalMoses
Figure 3: Approximate BLEU-4 during tuning
as a function of time over 25 tuning epochs. The
horizontal axis is accumulated time, while each
point indicates BLEU at the end of an epoch.
based interactive MT. A direct comparison with
Moses showed favorable performance on a large-
scale translation system.
Acknowledgments We thank Michel Galley for previous con-
tributions to Phrasal. The first author is supported by aNational
Science Foundation Graduate Research Fellowship. This work
was supported by the Defense Advanced Research Projects
Agency (DARPA) Broad Operational Language Translation
(BOLT) program through IBM. Any opinions, findings, and
conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the view
of DARPA or the US government.
119
References
W. Ammar, V. Chahuneau, M. Denkowski, G. Hanne-
man, W. Ling, A. Matthews, et al. 2013. The CMU
machine translation systems at WMT 2013: Syntax,
synthetic translation options, and pseudo-references.
In WMT.
R. Appuswamy, C. Gkantsidis, D. Narayanan, O. Hod-
son, and A. Rowstron. 2013. Nobody ever got fired
for buying a cluster. Technical report, Microsoft Cor-
poration, MSR-TR-2013-2.
R. Bisbey and Kay. 1972. The MIND translation sys-
tem: a study in man-machine collaboration. Techni-
cal Report P-4786, Rand Corp., March.
L. Bottou and O. Bousquet. 2011. The tradeoffs of
large scale learning. In Optimization for Machine
Learning, pages 351?368. MIT Press.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regu-
larization and search for minimum error rate training.
In WMT.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
M. Denkowski and A. Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation
of machine translation systems. In WMT.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
N. Durrani, B. Haddow, K. Heafield, and P. Koehn.
2013. Edinburgh?s machine translation systems for
European language pairs. In WMT.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al. 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
S. Green, D. Cer, and C. D. Manning. 2014. An em-
pirical comparison of features and tuning for phrase-
based machine translation. In WMT.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In EMNLP.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
J. Neidert, S. Schuster, S. Green, K. Heafield, and C. D.
Manning. 2014. Stanford University?s submissions
to the WMT 2014 translation task. In WMT.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
M. Post, J. Ganitkevitch, L. Orland, J. Weese, Y. Cao,
and C. Callison-Burch. 2013. Joshua 5.0: Sparser,
better, faster, server. In WMT.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
120
C. Tillmann. 2004. A unigram orientation model for
statistical machine translation. In NAACL.
J. Wuebker, M. Huck, S. Peitz, M. Nuhn, M. Freitag,
J. T. Peter, S. Mansour, and H. Ney. 2012. Jane 2:
Open source phrase-based and hierarchical statisti-
cal machine translation. InCOLING:Demonstration
Papers.
121
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150?156,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Stanford University?s Submissions to the WMT 2014 Translation Task
Julia Neidert
?
, Sebastian Schuster
?
, Spence Green,
Kenneth Heafield, and Christopher D. Manning
Computer Science Department, Stanford University
{jneid,sebschu,spenceg,heafield,manning}@cs.stanford.edu
Abstract
We describe Stanford?s participation in
the French-English and English-German
tracks of the 2014 Workshop on Statisti-
cal Machine Translation (WMT). Our sys-
tems used large feature sets, word classes,
and an optional unconstrained language
model. Among constrained systems, ours
performed the best according to uncased
BLEU: 36.0% for French-English and
20.9% for English-German.
1 Introduction
Phrasal (Green et al., 2014b) is a phrase-based ma-
chine translation system (Och and Ney, 2004) with
an online, adaptive tuning algorithm (Green et al.,
2013c) which allows efficient tuning of feature-
rich translation models. We improved upon the
basic Phrasal system with sparse features over word
classes, class-based language models, and a web-
scale language model.
We submitted one constrained French-English
(Fr-En) system, one unconstrained English-German
(En-De) system with a huge language model, and
one constrained English-German system without it.
Each system was built using over 100,000 features
and was tuned on over 10,000 sentences. This paper
describes our submitted systems and discusses how
the improvements affect translation quality.
2 Data Preparation & Post-Processing
We used all relevant data allowed by the con-
strained condition, with the exception of HindEn-
Corp and Wiki Headlines, which we deemed too
noisy. Specifically, our parallel data consists of the
Europarl version 7 (Koehn, 2005), parallel Com-
monCrawl (Smith et al., 2013), French-English UN,
Giga-FrEn, and News Commentary corpora pro-
vided by the evaluation. For monolingual data, we
?
These authors contributed equally.
Sentences Tokens
En-De 4.5M 222M
Fr-En 36.3M 2.1B
Table 1: Gross parallel corpus statistics after pre-
processing.
Constrained LM Unconstrained LM
German 1.7B 38.9 B
English 7.2B -
Table 2: Number of tokens in pre-processed mono-
lingual corpora used to estimate the language mod-
els. We split the constrained English data into two
models: 3.7 billion tokens from Gigaword and 3.5
billion tokens from all other sources.
used the provided news crawl data from all years,
English Gigaword version 5 (Parker et al., 2011),
and target sides of the parallel data. This includes
English from the Yandex, CzEng, and parallel Com-
monCrawl corpora. For parallel CommonCrawl,
we concatenated the English halves for various lan-
guage pairs and then deduplicated at the sentence
level.
In addition, our unconstrained English-German
system used German text extracted from the en-
tire 2012, 2013, and winter 2013 CommonCrawl
1
corpora by Buck et al. (2014).
Tables 1 and 2 show the sizes of the pre-
processed corpora of parallel text and monolingual
text from which our systems were built.
2.1 Pre-Processing
We used Stanford CoreNLP to tokenize the English
and German data according to the Penn Treebank
standard (Marcus et al., 1993). The French source
data was tokenized similarly to the French Treebank
1http://commoncrawl.org
150
(Abeill? et al., 2003) using the Stanford French
tokenizer (Green et al., 2013b).
We also lowercased the data and removed any
control characters. Further, we filtered out all lines
that consisted mainly of punctuation marks, re-
moved characters that are frequently used as bullet
points and standardized white spaces and newlines.
We additionally filtered out sentences longer than
100 tokens from the parallel corpora in order to
speed up model learning.
2.2 Alignment
For both systems, we used the Berkeley Aligner
(Liang et al., 2006) with default settings to align
the parallel data. We symmetrized the alignments
using the grow-diag heuristic.
2.3 Language Models
Our systems used up to three language models.
2.3.1 Constrained Language Models
For En-De, we used lmplz (Heafield et al., 2013)
to estimate a 5-gram language model on all WMT
German monolingual data and the German side of
the parallel Common Crawl corpus. To query the
model, we used KenLM (Heafield, 2011).
For the Fr-En system, we also estimated a 5-gram
language model from all the monolingual English
data and the English side of the parallel Common
Crawl, UN, Giga-FrEn, CzEng and Yandex corpora
using the same procedure as above. Additionally,
we estimated a second language model from the
English Gigaword corpus.
All of these language models used interpolated
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998).
2.3.2 Unconstrained Language Model
Our unconstrained En-De submission used an ad-
ditional language model trained on German web
text gathered by the Common Crawl Foundation
and processed by Buck et al. (2014). This cor-
pus was formed from the 2012, 2013, and winter
2013 CommonCrawl releases, which consist of web
pages converted to UTF-8 encoding with HTML
stripped. Applying the Compact Language Detec-
tor 2,
2
2.89% of the data was identified as German,
amounting to 1 TB of uncompressed text. After
splitting sentences with the Europarl sentence split-
ter (Koehn, 2005), the text was deduplicated at the
sentence level to reduce the impact of boilerplate
2https://code.google.com/p/cld2/
Order 1 2 3 4 5
Count 226 1,916 6,883 13,292 17,576
Table 3: Number of unique n-grams, in millions,
appearing in the Common Crawl German language
model.
and pages that appeared in multiple crawls, discard-
ing 78% of the data. We treated the resulting data
as normal text, pre-processing it as described in
Section 2.1 to yield 38.9 billion tokens. We built
an unpruned interpolated modified Kneser-Ney lan-
guage model with this corpus (Table 3) and added
it as an additional feature alongside the constrained
language models. At 38.9 billion tokens after dedu-
plication, this monolingual data is almost 23 times
as large as the rest of the German monolingual cor-
pus. Since the test data was also collected from the
web, we cannot be sure that the test sentences were
not in the language model. However, substantial
portions of the test set are translations from other
languages, which were not posted online until after
2013.
2.3.3 Word-Class Language Model
We also built a word-class language model for the
En-De system. We trained 512 word classes on
the constrained German data using the predictive
one-sided class model of Whittaker and Woodland
(2001) with the parallelized clustering algorithm of
Uszkoreit and Brants (2008) by Green et al. (2014a).
All tokens were mapped to their word class; infre-
quent tokens appearing fewer than 5 times were
mapped to a special cluster for unknown tokens.
Finally, we estimated a 7-gram language model on
the mapped corpus with SRILM (Stolcke, 2002)
using Witten-Bell smoothing (Bell et al., 1990).
2.4 Tuning and Test Data
For development, we tuned our systems on all
13,573 sentences contained in the newstest2008-
2012 data sets and tested on the 3,000 sentences of
the newstest2013 data set. The final system weights
were chosen among all tuning iterations using per-
formance on the newstest2013 data set.
2.5 Post-Processing
Our post-processor recases and detokenizes sys-
tem output. For the English-German system, we
combined both tasks by using a Conditional Ran-
dom Field (CRF) model (Lafferty et al., 2001) to
151
learn transformations between the raw output char-
acters and the post-processed versions. For each
test dataset, we trained a separate model on 500,000
sentences selected using the Feature Decay Algo-
rithm for bitext selection (Bi?ici and Yuret, 2011).
Features used include the character type of the cur-
rent and surrounding characters, the token type of
the current and surrounding tokens, and the position
of the character within its token.
The English output was recased using a language
model based recaser (Lita et al., 2003). The lan-
guage model was trained on the English side of the
Fr-En parallel data using lmplz.
3 Translation System
We built our translation systems using Phrasal.
3.1 Features
Our translation model has 19 dense features that
were computed for all translation hypotheses: the
nine Moses (Koehn et al., 2007) baseline features,
the eight hierarchical lexicalized reordering model
features by Galley and Manning (2008), the log
count of each rule, and an indicator for unique rules.
On top of that, the model uses the following addi-
tional features of Green et al. (2014a).
Rule indicator features: An indicator feature for
each translation rule. To combat overfitting, this
feature fires only for rules that occur more than
50 times in the parallel data. Additional indicator
features were constructed by mapping the words in
each rule to their corresponding word classes.
Target unigram class: An indicator feature for
the class of each target word.
Alignments: An indicator feature for each align-
ment in a translation rule, including multi-word
alignments. Again, class-based translation rules
were used to extract additional indicator features.
Source class deletion: An indicator feature for
the class of each unaligned source word in a trans-
lation rule.
Punctuation count ratio: The ratio of target
punctuation tokens to source punctuation tokens
for each derivation.
Functionword ratio: The ratio of target function
words to source functionwords. The functionwords
for each language are the 35 most frequent words
on each side of the parallel data. Numbers and
punctuation marks are not included in this list.
Target-class bigram boundary: An indicator
feature for the concatenation of the word class of
the rightmost word in the left rule and the word
class of the leftmost word in the right rule in each
adjacent rule pair in a derivation.
Length features: Indicator features for the length
of the source side and for the length of the target
side of the translation rule and an indicator feature
for the concatenation of the two lengths.
Rule orientation features: An indicator feature
for each translation rule combined with its orienta-
tion class (monotone, swap, or discontinuous). This
feature also fires only for rules that occur more than
50 times in the parallel data. Again, class-based
translation rules were used to extract additional fea-
tures.
Signed linear distortion: The signed linear dis-
tortion ? for two rules a and b is ? = r(a)?l(b)+1,
where r(x) is the rightmost source index of rule x
and l(x) is the leftmost source index of rule x. Each
adjacent rule pair in a derivation has an indicator
feature for the signed linear distortion of this pair.
Many of these features consider word classes
instead of the actual tokens. For the target side, we
used the same word classes as we used to train the
class-based language model. For the source side,
we trained word classes on all available data using
the same method.
3.2 Tuning
We used an online, adaptive tuning algorithm
(Green et al., 2013c) to learn the feature weights.
The loss function is an online variant of expected
BLEU (Green et al., 2014a). As a sentence-level
metric, we used the extended BLEU+1 metric that
smooths the unigram precision as well as the refer-
ence length (Nakov et al., 2012). For feature selec-
tion, we used L
1
regularization. Each tuning epoch
produces a different set of weights; we tried all of
them on newstest2013, which was held out from the
tuning set, then picked the weights that produced
the best uncased BLEU score.
3.3 System Parameters
We started off with the parameters of our systems
for the WMT 2013 Translation Task (Green et
al., 2013a) and optimized the L
1
-regularization
strength. Both systems used the following tuning
parameters: a 200-best list, a learning rate of 0.02
and a mini-batch size of 20. The En-De system
152
Track Stanford Best Rank
En-De constrained 19.9 20.1 3
En-De unconstrained 20.0 20.6 5
Fr-En constrained 34.5 35.0 3
(a) cased BLEU (%)
Track Stanford Best Rank
En-De constrained 20.7 20.7 1
En-De unconstrained 20.9 21.0 3
Fr-En constrained 36.0 36.0 1
(b) uncased BLEU (%)
Table 4: Official results in terms of cased and uncased BLEU of our submitted systems compared to the
best systems for each track. The ranks for the unconstrained system are calculated relative to all primary
submissions for the language pair, whereas the ranks for the constrained systems are relative to only the
constrained systems submitted.
used a phrase length limit of 8, a distortion limit of
6 and a L
1
-regularization strength of 0.0002. The
Fr-En system used a phrase length limit of 9, a dis-
tortion limit of 5 and a L
1
-regularization strength
of 0.0001.
During tuning, we set the stack size for cube prun-
ing to Phrasal?s default value of 1200. To decode
the test set, we increased the stack size to 3000.
4 Results
Table 4 shows the official results of our systems
compared to other submissions to the WMT shared
task. Both our En-De and Fr-En systems achieved
the highest uncased BLEU scores among all con-
strained submissions. However, our recaser evi-
dently performed quite poorly compared to other
systems, so our constrained systems ranked third by
cased BLEU score. Our unconstrained En-De sub-
mission ranked third among all systems by uncased
BLEU and fifth by cased BLEU.
To demonstrate the effectiveness of the individ-
ual improvements, we show results for four differ-
ent En-De systems: (1) A baseline that contains
only the 19 dense features, (2) a feature-rich trans-
lation system with the additional rich features, (3)
a feature-rich translation system with an additional
word class LM, and (4) a feature-rich translation
system with an additional wordclass LM and a huge
language model. For Fr-En we only built systems
(1)-(3). Results for all systems can be seen in Table
5 and Table 6. From these results, we can see that
both language pairs benefitted from adding rich fea-
tures (+0.4 BLEU for En-De and +0.5 BLEU for
Fr-En). However, we only see improvements from
the class-based language model in the case of the
En-De system (+0.4 BLEU). For this reason our Fr-
En submission did not use a class-based language
model. Using additional data in the form of a huge
language model further improved our En-De sys-
tem by almost 1% BLEU on the newstest2013 data
set. However, we only saw 0.2 BLEU improvement
on the newstest2014 data set.
4.1 Analysis
Gains from rich features are in line with the gains
we saw in the WMT 2013 translation task (Green
et al., 2013a). We suspect that rich features would
improve the translation quality a lot more if we had
several reference translations to tune on.
The word class language model seemed to im-
prove only translations in our En-De system while
it had no effect on BLEU in our Fr-En system. One
of the main reasons seems to be that the 7-gram
word class language model helped particularly with
long range reordering, which happens far more fre-
quently in the En-De language pair compared to the
Fr-En pair. For example, in the following transla-
tion, we can see that the system with the class-based
language model successfully translated the verb in
the second clause (set in italic) while the system
without the class-based language model did not
translate the verb.
Source: It became clear to me that this is my path.
Feature-rich: Es wurde mir klar, dass das mein
Weg.
Word class LM: Es wurde mir klar, dass das mein
Weg ist.
We can also see that the long range of the word
class language model improved grammaticality as
shown in the following example:
Source: Meanwhile, more than 40 percent of the
population are HIV positive.
Feature-rich: Inzwischen sind mehr als 40
Prozent der Bev?lkerung sind HIV positiv.
153
#iterations tune 2013 2013 cased 2014 2014 cased
Dense 8 16.9 19.6 18.7 20.0 19.2
Feature-rich 10 20.1 20.0 19.0 20.0 19.2
+ Word class LM 15 21.1 20.4 19.5 20.7 19.9
+ Huge LM 9 21.0 21.3 20.3 20.9 20.1
Table 5: En-De BLEU results. The tuning set is newstest2008?2012. Scores on newstest2014 were
computed after the system submission deadline using the released references.
#iterations tune 2013 2013 cased 2014 2014 cased
Dense 1 29.1 32.0 30.4 35.6 34.0
Feature-rich 12 37.2 32.5 30.9 36.0 34.5
+ Word class LM 14 35.7 32.3 30.7 ? ?
Table 6: Fr-En BLEU results. The tuning set is newstest2008?2012. Scores on newstest2014 were
computed after the system submission deadline using the released references.
Word class LM: Unterdessen mehr als 40 Prozent
der Bev?lkerung sind HIV positiv.
In this example, the system without the class-
based language model translated the verb twice. In
the second translation, the class-based language
model prevented this long range disagreement. An
analysis of the differences in the translation output
of our Fr-En systems showed that the word class
languagemodelmainly led to different word choices
but does not seem to help grammatically.
4.2 Casing
Our system performed comparatively poorly at cas-
ing, as shown in Table 4. In analysis after the eval-
uation, we found many of these errors related to
words with internal capitals, such as ?McCaskill?,
because the limited recaser we used, which is based
on a language model, considered only all lowercase,
an initial capital, or all uppercase words. We ad-
dressed this issue by allowing any casing seen in the
monolingual data. Some words were not seen at all
in the monolingual data but, since the target side of
the parallel data was included in monolingual data,
these words must have come from the source sen-
tence. In such situations, we preserved the word?s
original case. Table 7 shows the results with the re-
vised casing model. We gained about 0.24% BLEU
for German recasing and 0.15% BLEU for English
recasing over our submitted systems. In future work,
we plan to compare with a truecased system.
En-De Fr-En
Uncased Oracle 20.71 36.05
Conditional Random Field 19.85 ?
Limited Recaser 19.82 34.51
Revised Recaser 20.09 34.66
Table 7: Casing results on newstest2014 performed
after the evaluation. The oracle scores are uncased
BLEU (%) while all other scores are cased. Sub-
mitted systems are shown in italic.
5 Negative Results
We experimented with several additions that did not
make it into the final submissions.
5.1 Preordering
One of the key challenges when translating from
English to German is the long-range reordering of
verbs. For this reason, we implemented a depen-
dency tree based reordering system (Lerner and
Petrov, 2013). We parsed all source side sentences
using the Stanford Dependency Parser (De Marn-
effe et al., 2006) and trained the preordering system
on the entire bitext. Then we preordered the source
side of the bitext and the tuning and development
data sets using our preordering system, realigned
the bitext and tuned a machine translation system
using the preordered data. While preordering im-
proved verb reordering in many cases, many other
parts of the sentences were often also reordered
which led to an overall decrease in translation qual-
154
ity. Therefore, we concluded that this systemwill re-
quire further development before it is useful within
our translation system.
5.2 Minimum Bayes Risk Decoding
We further attempted to improve our output by re-
ordering the best 1000 translations for each sentence
using Minimum Bayes Risk decoding (Kumar and
Byrne, 2004) with BLEU as the distance measure.
This in effect increases the score of candidates that
are ?closer? to the other likely translations, where
?closeness? is measured by the BLEU score for the
candidate when the other translations are used as the
reference. Choosing the best translation following
this reordering improved overall performance when
tuned on the first half of the newstest2013 test set by
only 0.03 BLEU points for the English-German sys-
tem and 0.005 BLEU points for the French-English
system, so we abandoned this approach.
6 Conclusion
We submitted three systems: one constrained Fr-En
system, one constrained En-De system, and one un-
constrained En-De system. Among all constrained
systems, ours performed the best according to un-
cased BLEU. The key differentiating components
of our systems are class-based features, word class
language models, and a huge web-scale language
model. In ongoing work, we are investigating pre-
ordering for En-De translation as well as improved
recasing.
Acknowledgements
We thank Michael Kayser and Thang Luong for
help with experiments. This work was supported
by the Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Translation
(BOLT) program through IBM. This work used the
Extreme Science and Engineering Discovery Envi-
ronment (XSEDE), which is supported by National
Science Foundation grant number OCI-1053575.
The authors acknowledge the Texas Advanced Com-
puting Center (TACC) at The University of Texas
at Austin for providing HPC resources that have
contributed to the research results reported within
this paper. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of DARPA or the US government.
References
Anne Abeill?, Lionel Cl?ment, and Alexandra Kinyon,
2003. Building a treebank for French, chapter 10.
Kluwer.
Timothy C. Bell, John G. Cleary, and Ian H. Witten.
1990. Text compression. Prentice-Hall.
Ergun Bi?ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In WMT.
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram counts and language models from the
common crawl. In LREC.
Stanley Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard Univer-
sity, August.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In LREC.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt,
John Bauer, Sida Wang, et al. 2013a. Feature-rich
phrase-based translation: Stanford University?s sub-
mission to the WMT 2013 translation task. In WMT.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2013b. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D. Manning. 2013c. Fast and adaptive online
training of feature-rich translation models. In ACL.
Spence Green, Daniel Cer, and Christopher D. Man-
ning. 2014a. An empirical comparison of features
and tuning for phrase-based machine translation. In
WMT.
Spence Green, Daniel Cer, and Christopher D. Man-
ning. 2014b. Phrasal: A toolkit for new directions
in statistical machine translation. In WMT.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
Kneser-Ney language model estimation. In ACL.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In WMT.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
ICASSP.
155
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
et al. 2007. Moses: Open source toolkit for statisti-
cal machine translation. In ACL, Demonstration Ses-
sion.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. InProceedings ofMT
Summit.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML.
Uri Lerner and Slav Petrov. 2013. Source-side classi-
fier preordering for machine translation. In EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
Nanda Kambhatla. 2003. tRuEcasIng. In ACL.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Preslav Nakov, Francisco Guzman, and Stephan Vogel.
2012. Optimizing for sentence-level BLEU+1 yields
short translations. In COLING.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Robert Parker, David Graff, Junbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword
fifth edition, june. Linguistic Data Consortium,
LDC2011T07.
Jason Smith, Herv? Saint-Amand, Magdalena Plamada,
Philipp Koehn, Chris Callison-Burch, and Adam
Lopez. 2013. Dirt cheap web-scale parallel text
from the common crawl. In ACL. Association for
Computational Linguistics, August.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In ICLSP.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In ACL.
Ed W. D. Whittaker and Philip C. Woodland. 2001. Ef-
ficient class-based language modelling for very large
vocabularies. In ICASSP.
156
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 466?476,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
An Empirical Comparison of Features and Tuning
for Phrase-based Machine Translation
Spence Green, Daniel Cer, and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,danielcer,manning}@stanford.edu
Abstract
Scalable discriminative training methods
are now broadly available for estimating
phrase-based, feature-rich translation mod-
els. However, the sparse feature sets typi-
cally appearing in research evaluations are
less attractive than standard dense features
such as language and translation model
probabilities: they often overfit, do not gen-
eralize, or require complex and slow fea-
ture extractors. This paper introduces ex-
tended features, which are more specific
than dense features yet more general than
lexicalized sparse features. Large-scale ex-
periments show that extended features yield
robust BLEU gains for both Arabic-English
(+1.05) and Chinese-English (+0.67) rel-
ative to a strong feature-rich baseline. We
also specialize the feature set to specific
data domains, identify an objective function
that is less prone to overfitting, and release
fast, scalable, and language-independent
tools for implementing the features.
1 Introduction
Scalable discriminative algorithm design for ma-
chine translation (MT) has lately been a booming
enterprise. There are now algorithms for every taste:
probabilistic and distribution-free, online and batch,
regularized and unregularized. Technical differ-
ences aside, the papers that apply these algorithms
to phrase-based translation often share a curious
empirical characteristic: the algorithms support ex-
tra features, but the features do not significantly
improve translation. For example, Hopkins and
May (2011) showed that PRO with some simple ad
hoc features only exceeds the baseline on one of
three language pairs. Gimpel and Smith (2012b)
observed a similar result for both PRO and their
ramp-loss algorithm. Cherry and Foster (2012)
found that, at least in the batch case, many algo-
rithms produce similar results, and features only
significantly increased quality for one of three lan-
guage pairs. Only recently did Cherry (2013) and
Green et al. (2013b) identify certain features that
consistently reduce error.
These empirical results suggest that feature de-
sign and model fitting, the subjects of this paper,
warrant a closer look. We introduce an effective
extended feature set for phrase-based MT and iden-
tify a loss function that is less prone to overfitting.
Extended features share three attractive characteris-
tics with the standard Moses dense features (Koehn
et al., 2007): ease of implementation, language in-
dependence, and independence from ancillary cor-
pora like treebanks. In our experiments, they do
not overfit and can be extracted efficiently during
decoding. Because all feature weights are tuned
on the development set, the new feature templates
are amenable to feature augmentation (Daum? III,
2007), a simple domain adaptation technique that
we show works surprisingly well for MT.
Extended features are designed according to a
principle rather than a rule: they should fire less
than standard dense features, which are general, but
more than so-called sparse features, which are very
specific?they are usually lexicalized?and thus
prone to overfitting. This principle is motivated
by analysis, which shows how expressive models
can be a mixed blessing in the translation setting.
It is obvious that features allow the model to fit
the tuning data more tightly. For example, sparse
lexicalized features could reduce tuning error by
learning that the references prefer U.S. over United
States, a minor lexical distinction. Reference choice
should matter more than in the dense case, an issue
that we quantify. We also show that frequency cut-
offs, which are a crude but common form of feature
selection, are unnecessary and even detrimental
when features follow this principle.
We report large-scale translation quality experi-
ments relative to both dense and feature-rich base-
lines. Our best feature set, which includes domain
adaptation features, yields an average+1.05 BLEU
improvement for Arabic-English and +0.67 for
466
Chinese-English. In addition to the extended fea-
ture set, we show that an online variant of expected
error (Och, 2003) is significantly faster to compute,
less prone to overfitting, and nearly as effective as a
pairwise loss. We release all software?feature ex-
tractors, and fast word clustering and data selection
packages?used in our experiments.
1
2 Phrase-based Models and Learning
The log-linear approach to phrase-based translation
(Och and Ney, 2004) directly models the predictive
translation distribution
p(e|f ;w) =
1
Z(f)
exp
[
w
>
?(e, f)
]
(1)
where e is the target string, f is the source string,
w ? R
d
is the vector of model parameters, ?(?) ?
R
d
is a feature map, and Z(f) is an appropriate
normalizing constant. Assume that there is also a
function ?(e, f) ? R
d
that produces a recombina-
tion map for the features. That is, each coordinate
in ? represents the state of the corresponding co-
ordinate in ?. For example, suppose that ?
j
is the
log probability produced by the n-gram language
model (LM). Then ?
j
would be the appropriate LM
history. Recall that recombination collapses deriva-
tions with equivalent recombination maps during
search and thus affects learning. This issue signifi-
cantly influences feature design.
To learn w, we follow the online procedure of
Green et al. (2013b), who calculate gradient steps
with AdaGrad (Duchi et al., 2011) and perform fea-
ture selection via L
1
regularization in the FOBOS
(Duchi and Singer, 2009) framework. This proce-
dure accommodates any loss function for which a
subgradient can be computed. Green et al. (2013b)
used a PRO objective (Hopkins and May, 2011)
with a logistic (surrogate) loss function. However,
later results showed overfitting (Green et al., 2013a),
and we found that their online variant of PRO tends
to produce short translations like its batch counter-
part (Nakov et al., 2013). Moreover, PRO requires
sampling, making it slow to compute.
To address these shortcomings, we explore an
online variant of expected error (Och, 2003, Eq.7).
Let E
t
= {e
i
}
n
i=1
be a scored n-best list of trans-
lations at time step t for source input f
t
. Let G(e)
be a gold error metric that evaluates each candi-
date translation with respect to a set of one or more
1http://nlp.stanford.edu/software/phrasal
references. The smooth loss function is
`
t
(w
t?1
) = E
p(e|f
t
;w
t?1
)
[G(e)]
=
1
Z
?
e
?
?E
t
exp
(
w
>
?(e
?
, f)
)
?G(e
?
)
(2)
with normalization constant Z =
?
e
?
?E
t
exp
(
w
>
?(e
?
, f)
)
. The gradient g
t
for coordinate j is:
g
t
= E[G(e)?
j
(e, f
t
)]?
E[G(e)]E[?
j
(e, f
t
)] (3)
To our knowledge, we are the first to experiment
with the online version of this loss.
2
When G(e) is
sentence-level BLEU+1 (Lin and Och, 2004)?the
setting in our experiments?this loss is also known
as expected BLEU (Cherry and Foster, 2012). How-
ever, other metrics are possible.
3 Extended Phrase-based Features
We divide our feature templates into five categories,
which are well-known sources of error in phrase-
based translation. The features are defined over
derivations d = {r
i
}
D
i=1
, which are ordered se-
quences of rules r from the translation model. De-
fine functions f(?) to be the source string of a rule
or derivation and e(?) to be the target string. Local
features can be extracted from individual rules and
do not declare any state in the recombination map,
thus for all local features i we have ?
i
= 0. Non-
local features are defined over partial derivations
and declare some state, either a real-valued param-
eter or an index indicating a categorical value like
an n-gram context.
For each language, the extended feature tem-
plates require unigram counts and a word-to-class
mapping ? : w 7? c for word w ? V and class
c ? C. These can be extracted from any monolin-
gual data; our experiments simply use both sides of
the unaligned parallel training data.
The features are language-independent, but we
will use Arabic-English as a running example.
3.1 Lexical Choice
Lexical choice features make more specific distinc-
tions between target words than the dense transla-
tion model features (Koehn et al., 2003).
2
Gao and He (2013) used stochastic gradient descent and
expected BLEU to learn phrase table feature weights, but not
the full translation model w.
467
Lexicalized rule indicator (Liang et al., 2006a)
Some rules occur frequently enough that we can
learn rule-specific weights that augment the dense
translation model features. For example, our model
learns the following rule indicator features and
weights:
H
.
AJ
.
?

@? reasons -0.022
H
.
AJ
.
?

@? reasons for 0.002
H
.
AJ
.
?

@? the reasons for 0.016
These translations are all correct depending on con-
text. When the plural noun H
.
AJ
.
?

@ ?reasons? appears
in a construct state (iDafa) the preposition for is
unrealized. Moreover, depending on the context,
the English translation might also require the deter-
miner the, which is also unrealized. The weights
reflect that H
.
AJ
.
?

@ ?reasons? often appears in con-
struct and boost insertion of necessary target terms.
To prevent overfitting, this template only fires an
indicator for rules that occur more than 50 times
in the parallel training data (this is different from
frequency filtering on the tuning data; see section
6.1). The feature is local.
Class-based rule indicator Word classes ab-
stract over lexical items. For each rule r, a pro-
totype that abstracts over many rules can be built
by concatenating {?(w) : w ? f(r)} with
{?(w) : w ? e(r)}. For example, suppose
that Arabic class 492 consists primarily of Arabic
present tense verbs and class 59 contains English
auxiliaries. Then the model might penalize a rule
prototype like 492>59_59, which drops the verb.
This template fires an indicator for each rule proto-
type and is local.
Target unigram class (Ammar et al., 2013) Tar-
get lexical items with similar syntactic and semantic
properties may have very different frequencies in
the training data. These frequencies will influence
the dense features. For example, in one of our En-
glish class mappings the following words map to
the same class:
word class freq.
surface-to-surface 0 269
air-to-air 0 98
ground-to-air 0 63
The classes capture common linguistic attributes of
these words, which is the motivation for a full class-
based LM. Learning unigram weights directly is
surprisingly effective and does not require building
another LM. This template fires a separate indicator
for each class {?(w) : w ? e(r)} and is local.
3.2 Word Alignments
Word alignment features allow the model to recog-
nize fine-grained phrase-internal information that
is largely opaque in the dense model.
Lexicalized alignments (Liang et al., 2006a)
Consider the internal alignments of the rule:
sunday ,
??K


1
YgB@ 2
Alignment 1 ???K


?day?? ,? is incorrect and align-
ment 2 is correct. The dense translation model
features might assign this rule high probability if
alignment 1 is a common alignment error. Lexical-
ized alignment features allow the model to compen-
sate for these events. This feature fires an indicator
for each alignment in a rule?including multiword
cliques?and is local.
Class-based alignments Like the class-based
rule indicator, this feature template replaces each
lexical itemwith its word class, resulting in an align-
ment prototype. This feature fires an indicator for
each alignment in a rule after mapping lexical items
to classes. It is local.
Source class deletion Phrase extraction algo-
rithms often use a ?grow? symmetrization step (Och
and Ney, 2003) to add alignment points. Sometimes
this procedure can produce a rule that deletes im-
portant source content words. This feature template
allows the model to penalize these rules by firing
an indicator for the class of each unaligned source
word. The feature is local.
Punctuation ratio Languages use different types
and ratios of punctuation (Salton, 1958). For ex-
ample, quotation marks are not commonly used in
Arabic, but they are conventional in English. Fur-
thermore, spurious alignments often contain punc-
tuation. To control these two phenomena, this fea-
ture template returns the ratio of target punctuation
tokens to source punctuation tokens for each deriva-
tion. Since the denominator is constant, this feature
can be computed incrementally as a derivation is
constructed. It is local.
Function word ratio Words can also be spuri-
ously aligned to non-punctuation, non-digit func-
tion words such as determiners and particles. Fur-
thermore, linguistic differences may account for
468
differences in function word occurrences. For ex-
ample, English has a broad array of modal verbs
and auxiliaries not found in Arabic. This feature
template takes the 25 most frequent words in each
language (according to the unigram counts), and
computes the ratio between target and source func-
tion words for each derivation. As before the de-
nominator is constant, so the feature can be com-
puted efficiently. It is local.
3.3 Phrase Boundaries
The LM and hierarchical reordering model are the
only dense features that cross phrase boundaries.
Target-class bigramboundary Wehave already
added target class unigrams. We find that both lexi-
calized and class-based bigrams cause overfitting,
therefore we restrict to bigrams that straddle phrase
boundaries. The feature template fires an indicator
for the concatenation of the word classes on either
side of each boundary. This feature is non-local
and its recombination state ? is the word class at
the right edge of the partial derivation.
3.4 Derivation Quality
To satisfy strong features like the LM, or hard con-
straints like the distortion limit, the phrase-based
model can build derivations from poor translation
rules. For example, a derivation consisting mostly
of unigram rules may miss idiomatic usage that
larger rules can capture. All of these feature tem-
plates are local.
Source dimension (Hopkins and May, 2011) An
indicator feature for the source dimension of the
rule: |f(r)|.
Target dimension (Hopkins and May, 2011) An
indicator for the target dimension: |e(r)|.
Rule shape (Hopkins and May, 2011) The
conjunction of source and target dimension:
|f(r)|_|e(r)|.
3.5 Reordering
Lexicalized reordering models score the orientation
of a rule in an alignment grid. We use the same
baseline feature extractor as Moses, which has three
classes: monotone, swap, and discontinuous. We
also add the non-monotone class, which is a con-
junction of swap and discontinuous, for a total of
eight orientations.
3
3
Each class has ?with-previous? and ?with-next? special-
izations.
Algorithm (implementation) #threads Time
Brown (wcluster) 1 1023.39
Clark (cluster_neyessen) 1 890.11
Och (mkcls) 1 199.04
PredictiveFull (this paper) 8 3.27
Predictive (this paper) 8 2.42
Table 1: Wallclock time (min.sec) to generate a
mapping from a vocabulary of 63k English words
(3.7M tokens) to 512 classes. All experiments were
run on the same server, which had eight physical
cores. Our Java implementation is multi-threaded;
the C++ baselines are single-threaded.
Lexicalized rule orientation (Liang et al.,
2006a) For each rule, the template fires an indi-
cator for the concatenation of the orientation class,
each element in f(r), and each element in e(r). To
prevent overfitting, this template only fires for rules
that occur more than 50 times in the training data.
The feature is non-local and its recombination state
? is the rule orientation.
Class-based rule orientation For each rule, the
template fires an indicator for the concatenation
of the orientation class, each element in {?(w) :
w ? f(r)}, and each element in {?(w) : w ?
e(r)}. The feature is non-local and its recombina-
tion state ? is the rule orientation.
Signed linear distortion The dense feature set
includes a simple reordering cost model. Assume
that [r] returns the index of the leftmost source index
in f(d) and [[r]] returns the rightmost index. Then
the linear distortion is:
? = [r
1
] +
D
?
i=2
|[[r
i?1
]] + 1? [r
i
]| (4)
This score does not distinguish between left and
right distortion. To correct this issue, this feature
template fires an indicator for each signed com-
ponent in the sum, for each positive and negative
component. The feature is non-local and its recom-
bination state ? is the signed distortion.
3.6 Feature Dependencies
While unigram counts are trivial to compute, the
same is not necessarily true of the word-to-class
mapping ?. Standard algorithms run in O(n
2
),
where n = |V |. Table 1 shows an evaluation of
standard implementations of several popular algo-
rithms: Brown et al. (1992) implemented by Liang
469
(2005); Clark (2003) without the morphological
prior, which increases training time dramatically;
and the implementation of Och (1999) that comes
with the GIZA++ word aligner. The latter has
been used recently for MT features (Ammar et al.,
2013; Cherry, 2013; Yu et al., 2013). In a broad
survey, Christodoulopoulos et al. (2010) found that
for several downstream tasks, most word clustering
algorithms?including Brown and Clark?result in
similar task accuracy. For our large-scale setting,
the primary issue is then the time to estimate ?.
For large corpora the existing implementations
may require days or weeks, making our feature set
less practical than the traditional dense MT features.
Consequently, we re-implemented the predictive
one-sided class model of Whittaker and Woodland
(2001) with the parallelized clustering algorithm of
Uszkoreit and Brants (2008) (Predictive), which
was originally developed for very large scale lan-
guage modeling. Our implementation uses multiple
threads on a single processor instead ofMapReduce.
We also added two extensions that are useful for
translation features. First, we map all digits to 0.
This reduces sparsity while retaining useful patterns
such as 0000 (e.g., years) and 0th (e.g., ordinals).
Second, we mapped all words occurring fewer than
? times to an <unk> token. In our experiment,
these two changes reduce the vocabulary size by
71.1%. They also make the mapping ? more ro-
bust to unseen events during translation decoding.
For a conservative comparison to the other three
algorithms, we include results without these two
extensions (PredictiveFull).
4
4 Domain Adaptation Features
Feature augmentation is a simple yet effective do-
main adaptation technique (Daum? III, 2007). Sup-
pose that the source data comes fromM domains.
Then for each original feature ?
i
, we addM addi-
tional features, one for each domain. The original
feature ?
i
can be interpreted as a prior over theM
domains (Finkel and Manning, 2009, fn.2).
Most of the extended features are defined over
rules, so the critical issue is how to identify in-
domain rules. The trick is to know which training
sentence pairs are in-domain. Then we can annotate
all rules extracted from these instances with domain
4
For the baselines the training settings are the suggested
defaults: Brown, default; Clark, 10 iterations, frequency cutoff
? = 5; Och, 10 iterations. Our implementation: PredictiveFull,
30 iterations, ? = 0; Predictive, 30 iterations, ? = 5.
labels. The in-domain rule sets need not be disjoint
since some rules might be useful across domains.
This paper explores the following approach: we
choose one of theM domains as the default. Next,
we collect some source sentences for each of the
M ? 1 remaining domains. Using these examples
we then identify in-domain sentence pairs in the bi-
text via data selection, in our case the feature decay
algorithm (Bi?ici and Yuret, 2011). Finally, our rule
extractor adds domain labels to all rules extracted
from each selected sentence pair. Crucially, these
labels do not influence which rules are extracted
or how they are scored. The resulting phrase table
contains the same rules, but with a few additional
annotations.
Our method assumes domain labels for each
source input to be decoded. Our experiments utilize
gold, document-level labels, but accurate sentence-
level domain classifiers exist (Wang et al., 2012).
4.1 Augmentation of Extended Features
Irvine et al. (2013) showed that lexical selection is
the most quantifiable and perhaps most common
source of error in phrase-based domain adaptation.
Our development experiments seemed to confirm
this hypothesis as augmentation of the class-based
and non-lexical (e.g., Rule shape) features did not
reduce error. Therefore, we only augment the lex-
icalized features: rule indicators and orientations,
and word alignments.
4.2 Domain-Specific Feature Templates
In-domain Rule Indicator (Durrani et al., 2013)
An indicator for each rule that matches the input do-
main. This template fires a generic in-domain indi-
cator and a domain-specific indicator (e.g., the fea-
tures might be indomain and indomain-nw).
The feature is local.
Adjacent Rule Indicator Indicators for adjacent
in-domain rules. This template also fires both
generic and domain-specific features. The feature
is non-local and the state is a boolean indicating if
the last rule in a partial derivation is in-domain.
5 Experiments
We evaluate and analyze our feature set under a vari-
ety of large-scale experimental conditions including
multiple domains and references. To our knowl-
edge, the only language pairs with sufficient re-
search resources to support this protocol are Arabic-
English (Ar-En) and Chinese-English (Zh-En). The
470
Bilingual Monolingual
#Seg. #Tok. #Tok.
Ar-En 6.6M 375M
990M
Zh-En 9.3M 538M
Table 2: Bilingual and monolingual training cor-
pora. The monolingual English data comes from
the AFP and Xinhua sections of English Gigaword
4 (LDC2009T13).
training corpora
5
come from several Linguistic
Data Consortium (LDC) sources from 2012 and
earlier (Table 2). The test, development, and tuning
corpora
6
come from the NIST OpenMT andMetric-
sMATR evaluations (Table 3). Extended features
benefit from more tuning data, so we concatenated
five NIST data sets to build one large tuning set.
Observe that all test data come from later epochs
than the tuning and development data.
From these data we built phrase-based MT sys-
tems with Phrasal (Green et al., 2014).
7
We aligned
the parallel corpora with the Berkeley aligner
(Liang et al., 2006b) with standard settings and
symmetrized via the grow-diag heuristic. We cre-
ated separate English LMs for each language pair by
concatenating the monolingual Gigaword data with
the target-side of the respective bitexts. For each
corpus we estimated unfiltered 5-gram language
models with lmplz (Heafield et al., 2013).
For each condition we ran the learning algorithm
for 25 epochs
8
and selected the model according
to the maximum uncased, corpus-level BLEU-4
(Papineni et al., 2002) score on the dev set.
5.1 Results
We evaluate the new feature set relative to two base-
lines. Dense is the same baseline as Green et al.
5
We tokenized the English with Stanford CoreNLP ac-
cording to the Penn Treebank standard (Marcus et al., 1993),
the Arabic with the Stanford Arabic segmenter (Monroe et
al., 2014) according to the Penn Arabic Treebank standard
(Maamouri et al., 2008), and the Chinese with the Stanford
Chinese segmenter (Chang et al., 2008) according to the Penn
Chinese Treebank standard (Xue et al., 2005).
6
Data sources: tune, MT023568; dev, MT04; dev-dom,
domain adaptation dev set is MT04 and all wb and bn data
from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En);
test2, Progress0809 which was revealed in the OpenMT 2012
evaluation; test3, MetricsMATR08-10.
7
System settings: distortion limit of 5, cube pruning beam
size of 1200, maximum phrase length of 7.
8
Other learning settings: 16 threads, mini-batch size of 20;
L
1
regularization strength ? = 0.001; learning rate ?
0
= 0.02;
initialization of LM to 0.5, word penalty to -1.0, and all other
dense features to 0.2; initialization of extended features to 0.0.
#Seg. #Ref. Domains
Ar-En Zh-En
tune 5,604 5,900 4 nw,wb,bn
dev 1,075 1,597 4 nw
dev-dom 2,203 2,317 1 nw,wb,bn
test1 1,313 820 4 nw,wb
test2 1,378 1,370 4 nw,wb
test3 628 613 1 nw,wb,bn
Table 3: Development, test, and tuning data. Do-
main abbreviations: broadcast news (bn), newswire
(nw), and web (wb).
(2013b); these dense features are included in all of
the models that follow. Sparse is their best feature-
rich model, which adds lexicalized rule indicators,
alignments, orientations, and source deletions with-
out bitext frequency filtering.
We do not perform a full ablation study. Both
the approximate search and the randomization of
the order of tuning instances make the contribu-
tions of each individual template differ from run to
run. Resource constraints prohibit multiple large-
scale runs for each incremental feature. Instead,
we divide the extended feature set into two parts,
and report large-scale results. Ext includes all ex-
tended features except for the the filtered lexicalized
feature templates. Ext+Filt adds those filtered
lexicalized templates: rule indicators and orienta-
tions, and word alignments (section 3).
Table 4 shows translation quality results. The
new feature set significantly exceeds the baseline
Dense model for both language pairs. An interest-
ing result is that the new extended features alone
match the strong Sparse baseline. The class-based
features, which are more general, should clearly
be preferred to the sparse features when decoding
out-of-domain data (so long as word mappings are
trained for that data). The increased runtime per
iteration comes not from feature extraction but from
larger inner products as the model size increases.
Next, we add the domain features from section
4.2. We marked in-domain sentence pairs by con-
catenating the tuning data with additional bn and
wb monolingual in-domain data from several LDC
sources.
9
The FDA selection size was set to 20
times the number of in-domain examples for each
genre. Newswire was selected as the default domain
since most of the bitext comes from that domain.
The bottom rows of Tables 4a and 4b compare
9
Catalog: LDC2007T24, LDC2008T08, LDC2008T18,
LDC2012T16, LDC2013T01, LDC2013T05, LDC2013T14.
471
Model #features Epochs Min. / Epoch tune dev test1 test2 test3
Dense (D) 18 24 3 49.52 50.25 47.98 43.41 27.56
D+Sparse 48,597 24 8 56.51 52.98 49.55 45.40 29.02
D+Ext 62,931 16 11 57.83 54.33 49.66 45.66 29.15
D+Ext+Filt 94,606 17 14 59.13 55.35 50.02 46.24 29.59
D+Ext+Filt+Dom 123,353 22 18 59.97 29.20
?
50.45 46.24 30.84
(a) Ar-En.
Model #features Epochs Min. / Epoch tune dev test1 test2 test3
Dense (D) 18 17 3 32.82 34.96 26.61 26.72 10.19
D+Sparse 55,024 17 8 38.91 36.68 27.86 28.41 10.98
D+Ext 67,936 16 13 40.96 37.19 28.27 28.40 10.72
D+Ext+Filt 100,275 17 14 41.38 37.36 28.68 28.90 11.24
D+Ext+Filt+Dom 126,014 17 14 41.70 17.20
?
28.71 28.96 11.67
(b) Zh-En.
Table 4: Translation quality results (uncased BLEU-4%). Per-epoch times are in minutes (Min.). Statistical
significance relative to D+Sparse, the strongest baseline: bold (p < 0.001) and bold-italic (p < 0.05).
Significance is computed by the permutation test of Riezler and Maxwell (2005).
?
The dev score of
Ext+Filt+Dom is the dev-dom data set from Table 3, so it is not comparable with the other rows.
Ext+Filt+Dom to the baselines and other feature
sets. The gains relative to Sparse are statistically
significant for all six test sets.
A crucial result is that with domain features accu-
racy relative to Ext+Filt never decreases: a single
domain-adapted system is effective across domains.
Irvine et al. (2013) showed that when models from
multiple domains are interpolated, scoring errors
affecting lexical selection?the model could have
generated the correct target lexical item but did
not?increase significantly. We do not observe that
behavior, at least from the perspective of BLEU.
Table 5 separates out per-domain results. The
web data appears to be the hardest domain. That is
sensible given that broadcast news transcripts are
more similar to newswire, the default domain, than
web data. Moreover, inspection of the bitext sources
revealed very little web data, so our automatic data
selection is probably less effective. Accuracy on
newswire actually increases slightly.
6 Analysis
6.1 Learning
Loss Function In a now classic empirical com-
parison of batch tuning algorithms, Cherry and Fos-
ter (2012) showed that PRO and expected BLEU
Ar-En test1 test2 test3
nw wb nw wb bn nw wb
EF 59.78 39.55 51.69 38.80 30.39 37.59 20.58
EFD 60.21 40.38 51.76 38.77 31.63 38.18 22.37
Zh-En
EF 34.56 21.94 17.38 12.07 3.04 17.42 12.83
EFD 34.87 21.82 17.96 12.66 3.01 17.74 13.80
Table 5: Per-domain results (uncased BLEU-4 %).
Here bold simply indicates the maximum in each
column. Model abbreviations: EF is Ext+Filt and
EFD is Ext+Filt+Dom.
yielded similar translation quality results. In con-
trast, Table 6a shows significant differences be-
tween these loss functions. First, expected BLEU
can be computed faster since it is linear in the n-
best list size, whereas exact computation of the PRO
objective is O(n
2
) (thus sampling is often used). It
also converges faster. Second, PRO tends to select
larger models.
10
Finally, PRO seems to overfit on
the tuning set, since there are no gains on test1.
Feature Selection A common yet crude method
of feature selection is frequency cutoffs on the
10
PRO L
1
regularization strength of ? = 0.01, above which
model size decreases but translation quality degrades.
472
Loss #epochs Min./Epoch #feat. tune test1
EB 17 14 94,606 59.13 50.02
PRO 14 25 181,542 61.20 50.09
(a) PRO vs. expected BLEU (EB) for Ext+Filt.
Feature Selection #features tune test1
L
1
94,606 59.13 50.02
Freq. cutoffs 23,617 56.84 49.79
(b) Feature selection for Ext+Filt.
Model #refs tune test1
Dense 4 49.52 47.98
Dense 1 49.34 47.78
Ext+Filt 4 59.13 50.02
Ext+Filt 1 55.39 48.88
(c) Single- vs. multiple-reference tuning.
Table 6: Ar-En learning comparisons.
tuning data. Only features that fire more than
some threshold are admitted into the feature set.
Table 6b shows that for our new feature set, L
1
regularization?which simply requires setting a reg-
ularization strength parameter?is more effective
than frequency cutoffs.
References FewMT data sets supply multiple ref-
erences. Even when they do, those references are
but a sample from a larger pool of possible trans-
lations. This observation has motivated attempts
at generating lattices of translations for evaluation
(Dreyer and Marcu, 2012; Bojar et al., 2013). But
evaluation is only part of the problem. Table 6c
shows that the Dense model, which has only a
few features to describe the data, is little affected
by the elimination of references. In contrast, the
feature-rich model degrades significantly. This may
account for the underperformance of features in
single-reference settings like WMT (Durrani et al.,
2013; Green et al., 2013a). The next section ex-
plores the impact of references further.
6.2 Reference Variance
We took the Dense Ar-En output for the dev
data, which has four references, and computed the
sentence-level BLEU+1 with respect to each refer-
ence. Figure 1a shows a point for each of the 1,075
translations. The horizontal axis is the minimum
score with respect to any reference and the verti-
cal axis is the maximum (BLEU has a maximum
value of 1.0). Ideally, from the perspective of learn-
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l ll ll
l
l
l
l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l l
ll
l
l l
l l
l
l
l
ll
l
ll
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
lll l
l
ll
l
l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
ll l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
lll
l
l
ll l
l
l
lll l
l
l
l l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l ll
ll
l l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l l
l
l
l
l l
ll
l
l
l l
l
l
l
l
l
l
l l
l
ll
l
l l
l l
l
l
l
l
l
l l
l
l
l
l
l
l
ll l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
ll l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
ll
l
l
ll
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l
l l
ll
l
l
ll
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll l
ll
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
ll
l
l
l
l
ll
ll
l
l0255075100 0 25 50 75 100MinimumMaximum
(a) Maximum vs. minimum BLEU+1 (%)
l l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll
ll
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l l
l
l
l
ll
l
l
l
l
l
l
l ll
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l ll l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
lll
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l ll
l
l
l l
l
l
l
l
l
l
l
ll
l
l
l
l
l l
l
l
l
l
l
ll
l
l
ll
l
l
l
l
ll
ll
l
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
lll
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l l
l
l
l
l l
l
l
l
lll
l
l
l ll
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l ll
l
l
l
ll l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
ll
l
l
l
l
l0255075100 0 25 50 75 100MaximumAll References
(b) BLEU+1 (%) according to all four references vs.
maximum
Figure 1: Reference choice analysis for Ar-En
Dense output on the dev set.
ing, the scores should cluster around the diagonal:
the references should yield similar scores. This is
hardly the case. The mean difference isM = 18.1
BLEU, with a standard deviation SD = 11.5.
Figure 1b shows the same data set, but with the
maximum on the horizontal axis and the multiple-
reference score on the vertical axis. Assuming
a constant brevity penalty, the maximum lower-
bounds themultiple-reference score since BLEU ag-
gregates n-grams across references. The multiple-
reference score is an ?easier? target since the model
has more opportunities to match n-grams.
Consider again the single-reference condition
and one of the pathological cases at the top of Fig-
ure 1a. Suppose that the low-scoring reference is
observed in the single-reference condition. The
more expressive feature-rich model has a greater
capacity to fit that reference when, under another
473
reference, it would have matched the translation
exactly and incurred a low loss.
Nakov et al. (2012) suggested extensions to
BLEU+1 that were subsequently found to improve
accuracy in the single-reference condition (Gimpel
and Smith, 2012a). Repeating the min/max calcula-
tions with the most effective extensions (according
to Gimpel and Smith (2012a)) we observe lower
variance (M = 17.32, SD = 10.68). These exten-
sions are very simple, so a more sophisticated noise
model is a promising future direction.
7 Related Work
We review work on phrase-based discriminative fea-
ture sets that influence decoder search, and domain
adaptation with features.
11
7.1 Feature Sets
Variants of some extended features are scattered
throughout previous work: unfiltered lexicalized
rule indicators and alignments (Liang et al., 2006a);
rule shape (Hopkins and May, 2011); rule orien-
tation (Liang et al., 2006b; Cherry, 2013); target
unigram class (Ammar et al., 2013). We found
that other prior features did not improve translation:
higher-order target lexical n-grams (Liang et al.,
2006a; Watanabe et al., 2007; Gimpel and Smith,
2012b), higher-order target class n-grams (Ammar
et al., 2013), target word insertion (Watanabe et al.,
2007; Chiang et al., 2009), and many other unpub-
lished ideas transmitted through received wisdom.
To our knowledge, Yu et al. (2013) were the first
to experiment with non-local (derivation) features
for phrase-based MT. They added discriminative
rule features conditioned on target context. This is
a good idea that we plan to explore. However, they
do not mention if their non-local features declare
recombination state. Our empirical experience is
that non-local features are less effective when they
do not influence recombination.
Liang et al. (2006a) proposed replacing lexical
items with supervised part-of-speech (POS) tags to
reduce sparsity. This is a natural idea that lay dor-
mant until recently. Ammar et al. (2013) incorpo-
rated unigram and bigram target class features. Yu
et al. (2013) used word classes as backoff features to
reduce overfitting. Wuebker et al. (2013) replaced
all lexical items in the bitext and monolingual data
with classes, and estimated the dense feature set.
11
Space limitations preclude discussion of re-ranking fea-
tures.
Then they added these dense class-based features
to the baseline lexicalized system. Finally, Cherry
(2013) experimented with class-based hierarchical
reordering features. However, his features used a
bespoke representation rather than the simple full
rule string that we use.
7.2 Domain Adaptation with Features
Both Clark et al. (2012) and Wang et al. (2012) aug-
mented the baseline dense feature set with domain
labels. They each showed modest improvements
for several language pairs. However, neither incor-
porated a notion of a default prior domain.
Liu et al. (2012) investigated local adaption of
the log-linear scores by selecting comparable bitext
examples for a given source input. After selecting
a small local corpus, their algorithm then performs
several online update steps?starting from a glob-
ally tuned weight vector?prior to decoding the
input. The resulting model is effectively a locally
weighted, domain-adapted classifier.
Su et al. (2012) proposed domain adaptation
via monolingual source resources much as we use
in-domain monolingual corpora for data selection.
They labeled each bitext sentence with a topic using
a Hidden Topic Markov Model (HTMM) Gruber
et al. (2007). Source topic information was then
mixed into the translation model dense feature cal-
culations. This work follows Chiang et al. (2011),
who present a similar technique but using the same
gold NIST labels that we use. Hasler et al. (2012)
extended these ideas to a discriminative sparse fea-
ture set by augmenting both rule and unigram align-
ment features with HTMM topic information.
8 Conclusion
This paper makes four major contributions. First,
we introduced extended features for phrase-based
MT that exceeded both dense and feature-rich base-
lines. Second, we specialized the features to source
domains, further extending the gains. Third, we
showed that online expected BLEU is faster and
more stable than online PRO for extended fea-
tures. Finally, we released fast, scalable, language-
independent tools for implementing the feature set.
Our work should help practitioners quickly estab-
lish higher baselines on the way to more targeted
linguistic features. However, our analysis showed
that reference choice may restrain otherwise justifi-
able enthusiasm for feature-rich MT.
474
AcknowledgmentsWe thank John DeNero for comments on
an earlier version of this work. The first author is supported by
a National Science Foundation Graduate Research Fellowship.
This work was supported by the Defense Advanced Research
Projects Agency (DARPA) Broad Operational Language Trans-
lation (BOLT) program through IBM. Any opinions, findings,
and conclusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily reflect the
view of DARPA or the US government.
References
W. Ammar, V. Chahuneau, M. Denkowski, G. Hanne-
man, W. Ling, A. Matthews, et al. 2013. The CMU
machine translation systems at WMT 2013: Syntax,
synthetic translation options, and pseudo-references.
In WMT.
E. Bi?ici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
O. Bojar, M. Mach??ek, A. Tamchyna, and D. Zeman.
2013. Scratching the surface of possible translations.
In I. Habernal and V.Matou?ek, editors, Text, Speech,
and Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 465?474. Springer Berlin Hei-
delberg.
P-C. Chang, M. Galley, and C. D. Manning. 2008.
Optimizing Chinese word segmentation for machine
translation performance. In WMT.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
C. Cherry. 2013. Improved reordering for phrase-based
translation using sparse features. In HLT-NAACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
HLT-NAACL.
D. Chiang, S. DeNeefe, and M. Pust. 2011. Two easy
improvements to lexical weighting. In ACL.
C. Christodoulopoulos, S. Goldwater, andM. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In EMNLP.
J. H. Clark, A. Lavie, and C. Dyer. 2012. One system,
many domains: Open-domain statistical machine
translation via feature augmentation. In AMTA.
H. Daum? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
M. Dreyer and D. Marcu. 2012. HyTER: Meaning-
equivalent semantics for translation evaluation. In
NAACL.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
N. Durrani, B. Haddow, K. Heafield, and P. Koehn.
2013. Edinburgh?s machine translation systems for
European language pairs. In WMT.
J. R. Finkel and C. D. Manning. 2009. Hierarchical
bayesian domain adaptation. In HLT-NAACL.
J. Gao and X. He. 2013. Training MRF-based phrase
translation models using gradient ascent. In NAACL.
K. Gimpel and N. A. Smith. 2012a. Addendum to
structured ramp loss minimization for machine trans-
lation. Technical report, Language Technologies In-
stitute, Carnegie Mellon University.
K. Gimpel and N. A. Smith. 2012b. Structured ramp
loss minimization for machine translation. In HLT-
NAACL.
S. Green, D. Cer, K. Reschke, R. Voigt, J. Bauer,
S. Wang, and others. 2013a. Feature-rich phrase-
based translation: Stanford University?s submission
to the WMT 2013 translation task. In WMT.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
S. Green, D. Cer, and C. D. Manning. 2014. Phrasal: A
toolkit for new directions in statistical machine trans-
lation. In WMT.
A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden
topic markov models. In AISTATS.
E. Hasler, B. Haddow, and P. Koehn. 2012. Sparse
lexicalised features and topic adaptation for SMT. In
IWSLT.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
A. Irvine, J. Morgan, M. Carpuat, H. Daum? III, and
D. Munteanu. 2013. Measuring machine translation
errors in new domains. TACL, 1.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, A. Bouchard-C?t?, D. Klein, and B. Taskar.
2006a. An end-to-end discriminative approach to
machine translation. In ACL.
P. Liang, B. Taskar, and D. Klein. 2006b. Alignment
by agreement. In NAACL.
475
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. Liu, H. Cao, T. Watanabe, T. Zhao, M. Yu, and
C. Zhu. 2012. Locally training the log-linear model
for SMT. In EMNLP-CoNLL.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-
ing the Arabic Treebank: A collaborative effort to-
ward new annotation guidelines. In LREC.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
W. Monroe, S. Green, and C. D. Manning. 2014. Word
segmentation of informal Arabic with domain adap-
tation. In ACL, Short Papers.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
P. Nakov, F. Guzm?n, and S. Vogel. 2013. A tale about
PRO and monsters. In ACL, Short Papers.
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. Compu-
tational Linguistics, 29(1):19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
G. Salton. 1958. The use of punctuation patterns in ma-
chine translation. Mechanical Translation, 5(1):16?
24, July.
J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong, and
Q. Liu. 2012. Translation model adaptation for sta-
tistical machine translation with monolingual topic
information. In ACL.
J. Uszkoreit and T. Brants. 2008. Distributed word clus-
tering for large scale class-based language modeling
in machine translation. In ACL-HLT.
W. Wang, K. Macherey, W. Macherey, F. J. Och, and
P. Xu. 2012. Improved domain adaptation for statis-
tical machine translation. In AMTA.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In EMNLP-CoNLL.
E. W. D. Whittaker and P. C. Woodland. 2001. Effi-
cient class-based language modelling for very large
vocabularies. In ICASSP.
J. Wuebker, S. Peitz, F. Rietig, and H. Ney. 2013.
Improving statistical machine translation with word
class models. In EMNLP.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207?238.
H. Yu, L. Huang, H. Mi, and K. Zhao. 2013. Max-
violation perceptron and forced decoding for scalable
MT training. In EMNLP.
476
