Proceedings of the Workshop on BioNLP: Shared Task, pages 137?140,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring ways beyond the simple supervised learning approach for
biological event extraction
Gyo?rgy Mo?ra1, Richa?rd Farkas1, Gyo?rgy Szarvas2?, Zsolt Molna?r3
gymora@gmail.com, rfarkas@inf.u-szeged.hu,
szarvas@tk.informatik.tu-darmstadt.de, zsolt@acheuron.hu
1 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
Aradi ve?rtanuk tere 1., H-6720 Szeged, Hungary
2 Ubiquitous Knowledge Processing Lab, Technische Universita?t Darmstadt
Hochschulstra?e 10., D-64289 Darmstadt, Germany
3 Acheuron Hungary Ltd., Chemo-, and Bioinformatics group,
Tiszavira?g u. 11., H-6726 Szeged, Hungary
Abstract
Our paper presents the comparison of a
machine-learnt and a manually constructed
expert-rule-based biological event extraction
system and some preliminary experiments to
apply a negation and speculation detection
system to further classify the extracted events.
We report results on the BioNLP?09 Shared
Task on Event Extraction evaluation datasets,
and also on an external dataset for negation
and speculation detection.
1 Introduction
When we consider the sizes of publicly available
biomedical scientific literature databases for re-
searchers, valuable biological knowledge is acces-
sible today in enormous amounts. The efficient pro-
cessing of these large text collections is becoming
an increasingly important issue in Natural Language
Processing. For a survey on techniques used in bio-
logical Information Extraction, see (Tuncbag et al,
2009).
The BioNLP?09 Shared Task (Kim et al, 2009)
involved the recognition of bio-molecular events in
scientific abstracts. In this paper we describe our
systems submitted to the event detection and charac-
terization (Task1) and the recognition of negations
and speculations (Task3) subtasks. Our experiments
can be regarded as case studies on i) how to define
a framework for a hybrid human-machine biological
information extraction system, ii) how the linguis-
tic scopes of negation/speculation keywords relate
to biological event annotations.
?On leave from RGAI of Hungarian Acad. Sci.
2 Event detection
We formulated the event extraction task as a classifi-
cation problem for each event-trigger-word/protein
pair. A domain expert collected 140 keywords
which he found meaningful and reliable by manual
inspection of the corpus. This set of high-precision
keywords covered 69.8% of the event annotations in
the training data.
We analysed each occurrence of these keywords
in two different approaches. We used C4.5 deci-
sion tree classifier to predict one of the event types
considered in the shared task or the keyword/protein
pair being unrelated; and we also developed a hand-
crafted expert system with a biological expert. We
observed that the two systems extract markedly dif-
ferent sets of true positive events. Our final submis-
sion was thus the union of the events extracted by
the expert-rule-based and the statistical systems (we
call this hybrid system later on).
2.1 The statistical event classifier
The preprocessing of the data was performed us-
ing the UltraCompare (Kano et al, 2008) repository
provided by the organizers of the challenge: Genia
sentence splitter, Genia tagger for POS coding and
NER.
The statistical system classified each key-
word/protein pair into 9 event and 2 non-event
classes. A pair was either labeled according to
the predicted event type (the keyword as an event
trigger and the protein name as the theme of the
event), non-event (keyword not an event trigger)
or wrong-protein (the theme of the event is a
different protein). We chose to use two non-event
137
classes to make the decision tree more human read-
able (the negative cases being separated). This made
the comparison of the statistical model and the rule-
based system easier.
The features we used were the following: 1) the
words and POS codes in a window (? 3 tokens)
around the keyword, preserving position informa-
tion relative to the keyword; 2) the distances be-
tween the keyword and the two nearest annotated
proteins (left and right) and the theme candidate as
numeric features1. The protein annotations were re-
placed by the term $protein, Genia tagger anno-
tations by $genia-protein (mainly complexes),
to enable the classifier to learn the difference be-
tween events involved in the shared task, and events
out of the scope of the task. Events with protein
complexes and families often had the same linguistic
structure as events with annotated proteins. As com-
plexes did not form events in the shared task, they
sometimes misled our local-context-based classifier.
For example ?the binding of ISGF3? was not anno-
tated as an event because the theme is not a ?protein?
(as defined by the shared task guidelines), while ?the
binding of TRAF2? was (TRAF2 being a protein,
and not a complex as in the former example).
We trained a C4.5 decision tree classifier using
Weka (Witten and Frank, 2005). The human read-
able models and fast training time motivated our
selection of a learning algorithm which allowed a
straightforward comparison with the expert system.
2.2 Expert-rule-based system
The expert system was constructed by a biologist
who had over 4 years of experience in similar tasks.
The main idea was to define rules ? which have a
very high precision ? in order to compare them with
the learnt decision trees and to increase the cover-
age of the final system by adding these annotations
to the output of the statistical system. We only man-
aged to prepare expert rules for the Phosphorylation
and Gene expression classes due to time constraints
(a total of 46 patterns). The expert was asked to
construct high-precision rules (they were tested on
the train set to keep the false positive rate near zero)
in order to gain insight into the structure of reliable
1More information on the features
and parameters used can be found at
www.inf.u-szeged.hu/rgai/BioEventExtraction
rules.
Here each rule is bound to a specific keyword. Ev-
ery rule is a sequence of ?word patterns? (with or
without a suffix). A word pattern can match a pro-
tein, an arbitrary word, an exact word or the key-
word. Every pattern can have a Regular Expression
style suffix:
Table 1: Word pattern types and suffixes
<keyword> matching the keyword of the event
"word" matching regular words
matching any token
$protein matching any annotated protein
? zero or one of the word pattern
* zero or more of the word pattern
+ one or more of the word pattern
{a,b} definite number of word patterns
For example the ?<expression> ? "of"
? $protein? pattern recognizes an event with
the keyword expression, followed by an arbitrary
word and then the word of, or immediately by of and
then a protein (or immediately by the protein name).
An obvious drawback of this system is that nega-
tion is not allowed, so the expert was unable to de-
fine a word pattern like !"of" to match any to-
ken besides of. This extension would have been a
straightforward way of improving the system.
2.3 Experimental results
We expected the recall of the hybrid system to be
near the sum of the recalls of the individual systems,
meaning that they had recognized different events,
as the pattern matching was mainly based on the
order of the tokens, while the statistical classifier
learned position-oriented contextual clues. Thanks
to the high precision of the rule-based system, the
overall precision also increased. The two event
classes which were included in the expert system
had a significantly better precision score. The cov-
erage of the Phosphorylation class was lower than
that for the Gene expression class because its pat-
terns were still incomplete2.
2A discussion on comparing the contribution of the
two approaches and individual rules can be found at
www.inf.u-szeged.hu/rgai/BioEventExtraction
138
Table 2: Results of rule based-system compared to the
statistical and combined systems (R/P/fscore)
All Event Gene exp. Phosph.
stat. 16 / 31 / 21 36 / 41 / 38 73 / 37 / 49
rule 5 / 80 / 10 20 / 85 / 33 17 / 58 / 26
hybrid 22 / 37 / 27 56 / 51 / 54 81 / 40 / 53
3 Recognition of negations and
speculations
For negation and speculation detection, we applied
a model trained on a different dataset (Vincze et al,
2008) of scientific abstracts, which had been spe-
cially annotated for negative and uncertain keywords
and their linguistic scope. Due to time constraints
we used our model to produce annotations for Task3
without any sort of fine tuning to the shared task gold
standard annotations.
The only exception here was a subclass of specu-
lative annotations that were not triggered by a word
used to express uncertainty, but were judged to be
speculative because the sentence itself reported on
some experiments performed, the focus of the in-
vestigations described in the article, etc. That is,
it was not the meaning of the text that was uncer-
tain, but ? as saying that something has been exam-
ined does not mean it actually exists ? the sentence
implicitly contained uncertain information. Since
such sentences were not covered by our corpus, for
these cases we collected the most reliable text cues
from the shared task training data and applied a
dictionary-lookup-based approach. We did this so
as to get a comprehensive model for the Genia nega-
tion and speculation task.
As for the explicit uncertain and negative state-
ments, we applied a more sophisticated approach
that exploited the annotations of the BioScope cor-
pus (Vincze et al, 2008). For each frequent and am-
biguous keyword found in the approximately 1200
abstracts annotated in BioScope, we trained a sepa-
rate classifier to discriminate keyword/non-keyword
uses of each term, using local contextual patterns
(neighbouring lemmas, their POS codes, etc.) as
features. In others words, for the most common
uncertain and negative keywords, we attempted a
context-based disambiguation, instead of a simple
keyword lookup. Having the keywords, we pre-
dicted their scope using simple heuristics (?to the
end of the sentence?, ?to the next punctation mark
in both directions?, etc.). In the shared task we ex-
amined each extracted event and they were said to
be negated or hedged when some of their arguments
(trigger word, theme or clause) were within a lin-
guistic scope.
3.1 Experimental results
First we evaluated our negation and speculation
keyword/non-keyword classification models on the
BioScope corpus by 5-fold cross-validation. We
trained models for 15 negation and 41 speculative
keywords. We considered different word forms of
the same lemma to be different keywords because
they may be used in a different meaning/context.
For instance, different keyword/non-keyword deci-
sion rules must be used for appear, appears and ap-
peared. We trained a C4.5 decision tree using word
uni- and bigram features and POS codes to discrim-
inate keyword/non-keyword uses and compared the
results with the most frequent class (MFC) baseline.
Overall, our context-based classification method
outperformed the baseline algorithm by 3.7% (giv-
ing an error reduction of 46%) and 3.1% (giving an
error reduction of 27%) on the negation and specula-
tion keywords, respectively. The learnt models were
typically very small decision trees i.e. they repre-
sented very simple rules indicating collocations (like
?hypothesis is a keyword if and only if followed by
that, etc.). More complex rules (e.g. ?clear is a key-
word if and only if not is in ?3 environment?) were
learnt just in a few cases.
Our second set of experiments focused on Task3
of the shared task (Kim et al, 2009). As the offi-
cial evaluation process of Task3 was built upon the
detected events of Task1, it did not provide any use-
ful feedback about our negation and speculation de-
tection approach. Thus instead of our Task1 out-
put, we evaluated our model on the gold standard
Task1 annotation of the training and the develop-
ment datasets. The statistical parts of the system
were learnt on the BioScope corpus, thus the train
set was kept blind as well. Table 3 summarises the
results obtained by the explicit negation, speculation
and by the full speculation (both explicit and implicit
keywords) detection methods.
Analysing the errors of the system, we found that
139
Table 3: Negation and speculation detection results
Train (R/P/F) Dev. (R/P/F)
negation 46.9 / 61.3 / 52.8 42.8 / 57.9 / 49.2
exp. spec. 15.4 / 39.5 / 23.6 15.4 / 32.6 / 20.1
full spec. 25.5 / 71.1 / 37.5 27.9 / 65.3 / 39.1
most of the false positives came from the different
approaches of the BioScope and the Genia annota-
tions (see below for a detailed discussion). Most of
the false negative predictions were a consequence of
the incompleteness of our keyword list.
3.2 Discussion
We applied this negation and speculation detection
model more as a case study to assess the usability
of the BioScope corpus. This means that we did not
fine-tune the system to the Genia annotations. Our
experiments revealed some fundamental and inter-
esting differences between the Genia-interpretation
of negation and speculation, and the corpus used by
us. The chief difference is that the BioScope corpus
was constructed following more linguistic-oriented
principles than the Genia negation and speculation
annotation did, which sought to extract biological
information. These differences taken together ex-
plain the relatively poor results we got for the shared
task.
There are significant differences in the interpreta-
tion of both at the keyword level (i.e. what triggers
negation/uncertainty and what does not) and in the
definition of the scope of keywords. For example,
in a sentence like ?have NO effect on the inducibil-
ity of the IL-2 promoter?, Genia annotation just con-
siders the effect to be negated. This means that the
inducibility of IL-2 is regarded as an assertive event
here. In BioScope, the complements of effect are
also placed within the scope of no, thus it would also
be annotated as a negative one. We argue here that
the above example is not a regular sentence to ex-
press the fact: IL-2 is inducible. We rather think
that if the paper has some result (evidence) regard-
ing this event, it should be stated elsewhere in the
text, and we should not retrieve this information as a
fact just based on the above sentence. Thus we argue
that more sophisticated guidelines are needed for the
consistent annotation and efficient handling of nega-
tion and uncertainty in biomedical text mining.
4 Conclusions
We described preliminary experiments on two dif-
ferent approaches which take us beyond the ?take-
goldstandard-data, extract-some-features, train-a-
classifier? approach for biomedical event extraction
from scientific texts (incorporating rule-based sys-
tems and linguistic negation/uncertainty detection).
The systems introduced here participated in the Ge-
nia Event annotation shared task. They achieved rel-
atively poor results on this dataset, mainly due to
1) the special annotation guidelines of the shared
task (like disregarding events with protein complex
or family arguments, and treating subevents as as-
sertive information) and 2) the limited resources we
had to allocate for the task during the challenge
timeline. We consider that the lessons learnt here
are still useful and we also plan to improve our sys-
tem in the near future.
5 Acknowledgements
The authors would like to thank the organizers of
the shared task for their efforts. This work was sup-
ported in part by the NKTH grant of the Hungarian
government (codename BELAMI).
References
Y. Kano, N. Nguyen, R. Saetre, K. Yoshida, Y. Miyao,
Y. Tsuruoka, Y. Matsubayashi, S. Ananiadou, and
J. Tsujii. 2008. Filling the gaps between tools and
users: a tool comparator, using protein-protein inter-
action as an example. Pac Symp Biocomput.
J-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of bionlp?09 shared task on event
extraction. In Proceedings of Natural Language Pro-
cessing in Biomedicine (BioNLP) NAACL 2009 Work-
shop. To appear.
N. Tuncbag, G. Kar, O. Keskin, A. Gursoy, and R. Nussi-
nov. 2009. A survey of available tools and web servers
for analysis of protein-protein interactions and inter-
faces. Briefings in Bioinformatics.
V. Vincze, Gy. Szarvas, R. Farkas, Gy. Mo?ra, and
J. Csirik. 2008. The bioscope corpus: biomedi-
cal texts annotated for uncertainty, negation and their
scopes. BMC Bioinformatics, 9(Suppl 11):S9.
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann.
140
Proceedings of ACL-08: HLT, pages 281?289,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Hedge classification in biomedical texts with a weakly supervised selection of
keywords
Gyo?rgy Szarvas
Research Group on Artificial Intelligence
Hungarian Academy of Sciences / University of Szeged
HU-6720 Szeged, Hungary
szarvas@inf.u-szeged.hu
Abstract
Since facts or statements in a hedge or negated
context typically appear as false positives, the
proper handling of these language phenomena
is of great importance in biomedical text min-
ing. In this paper we demonstrate the impor-
tance of hedge classification experimentally
in two real life scenarios, namely the ICD-
9-CM coding of radiology reports and gene
name Entity Extraction from scientific texts.
We analysed the major differences of specu-
lative language in these tasks and developed
a maxent-based solution for both the free text
and scientific text processing tasks. Based on
our results, we draw conclusions on the pos-
sible ways of tackling speculative language in
biomedical texts.
1 Introduction
The highly accurate identification of several regu-
larly occurring language phenomena like the specu-
lative use of language, negation and past tense (tem-
poral resolution) is a prerequisite for the efficient
processing of biomedical texts. In various natural
language processing tasks, relevant statements ap-
pearing in a speculative context are treated as false
positives. Hedge detection seeks to perform a kind
of semantic filtering of texts, that is it tries to sep-
arate factual statements from speculative/uncertain
ones.
1.1 Hedging in biomedical NLP
To demonstrate the detrimental effects of specula-
tive language on biomedical NLP tasks, we will con-
sider two inherently different sample tasks, namely
the ICD-9-CM coding of radiology records and gene
information extraction from biomedical scientific
texts. The general features of texts used in these
tasks differ significantly from each other, but both
tasks require the exclusion of uncertain (or specula-
tive) items from processing.
1.1.1 Gene Name and interaction extraction
from scientific texts
The test set of the hedge classification dataset 1
(Medlock and Briscoe, 2007) has also been anno-
tated for gene names2.
Examples of speculative assertions:
Thus, the D-mib wing phenotype may result from de-
fective N inductive signaling at the D-V boundary.
A similar role of Croquemort has not yet been tested,
but seems likely since the crq mutant used in this
study (crqKG01679) is lethal in pupae.
After an automatic parallelisation of the 2 annota-
tions (sentence matching) we found that a significant
part of the gene names mentioned (638 occurences
out of a total of 1968) appears in a speculative sen-
tence. This means that approximately 1 in every 3
genes should be excluded from the interaction detec-
tion process. These results suggest that a major por-
tion of system false positives could be due to hedg-
ing if hedge detection had been neglected by a gene
interaction extraction system.
1.1.2 ICD-9-CM coding of radiology records
Automating the assignment of ICD-9-CM codes
for radiology records was the subject of a shared task
1http://www.cl.cam.ac.uk/?bwm23/
2http://www.cl.cam.ac.uk/?nk304/
281
challenge organised in Spring 2007. The detailed
description of the task, and the challenge itself can
be found in (Pestian et al, 2007) and online3. ICD-
9-CM codes that are assigned to each report after
the patient?s clinical treatment are used for the reim-
bursement process by insurance companies. There
are official guidelines for coding radiology reports
(Moisio, 2006). These guidelines strictly state that
an uncertain diagnosis should never be coded, hence
identifying reports with a diagnosis in a specula-
tive context is an inevitable step in the development
of automated ICD-9-CM coding systems. The fol-
lowing examples illustrate a typical non-speculative
context where a given code should be added, and
a speculative context where the same code should
never be assigned to the report:
non-speculative: Subsegmental atelectasis in the
left lower lobe, otherwise normal exam.
speculative: Findings suggesting viral or reactive
airway disease with right lower lobe atelectasis or
pneumonia. In an ICD-9 coding system developed
for the challenge, the inclusion of a hedge classi-
fier module (a simple keyword-based lookup method
with 38 keywords) improved the overall system per-
formance from 79.7% to 89.3%.
1.2 Related work
Although a fair amount of literature on hedging in
scientific texts has been produced since the 1990s
(e.g. (Hyland, 1994)), speculative language from a
Natural Language Processing perspective has only
been studied in the past few years. This phe-
nomenon, together with others used to express forms
of authorial opinion, is often classified under the no-
tion of subjectivity (Wiebe et al, 2004), (Shana-
han et al, 2005). Previous studies (Light et al,
2004) showed that the detection of hedging can be
solved effectively by looking for specific keywords
which imply that the content of a sentence is spec-
ulative and constructing simple expert rules that de-
scribe the circumstances of where and how a key-
word should appear. Another possibility is to treat
the problem as a classification task and train a sta-
tistical model to discriminate speculative and non-
speculative assertions. This approach requires the
availability of labeled instances to train the models
3
http://www.computationalmedicine.org/challenge/index.php
on. Riloff et al (Riloff et al, 2003) applied boot-
strapping to recognise subjective noun keywords
and classify sentences as subjective or objective in
newswire texts. Medlock and Briscoe (Medlock and
Briscoe, 2007) proposed a weakly supervised setting
for hedge classification in scientific texts where the
aim is to minimise human supervision needed to ob-
tain an adequate amount of training data.
Here we follow (Medlock and Briscoe, 2007) and
treat the identification of speculative language as the
classification of sentences for either speculative or
non-speculative assertions, and extend their method-
ology in several ways. Thus given labeled sets Sspec
and Snspec the task is to train a model that, for each
sentence s, is capable of deciding whether a previ-
ously unseen s is speculative or not.
The contributions of this paper are the following:
? The construction of a complex feature selection
procedure which successfully reduces the num-
ber of keyword candidates without excluding
helpful keywords.
? We demonstrate that with a very limited
amount of expert supervision in finalising the
feature representation, it is possible to build ac-
curate hedge classifiers from (semi-) automati-
cally collected training data.
? The extension of the feature representation
used by previous works with bigrams and tri-
grams and an evaluation of the benefit of using
longer keywords in hedge classification.
? We annotated a small test corpora of biomed-
ical scientific papers from a different source
to demonstrate that hedge keywords are highly
task-specific and thus constructing models that
generalise well from one task to another is not
feasible without a noticeable loss in accuracy.
2 Methods
2.1 Feature space representation
Hedge classification can essentially be handled by
acquiring task specific keywords that trigger specu-
lative assertions more or less independently of each
other. As regards the nature of this task, a vector
space model (VSM) is a straightforward and suit-
able representation for statistical learning. As VSM
282
is inadequate for capturing the (possibly relevant) re-
lations between subsequent tokens, we decided to
extend the representation with bi- and trigrams of
words. We chose not to add any weighting of fea-
tures (by frequency or importance) and for the Max-
imum Entropy Model classifier we included binary
data about whether single features occurred in the
given context or not.
2.2 Probabilistic training data acquisition
To build our classifier models, we used the dataset
gathered and made available by (Medlock and
Briscoe, 2007). They commenced with the seed set
Sspec gathered automatically (all sentences contain-
ing suggest or likely ? two very good speculative
keywords), and Snspec that consisted of randomly
selected sentences from which the most probable
speculative instances were filtered out by a pattern
matching and manual supervision procedure. With
these seed sets they then performed the following
iterative method to enlarge the initial training sets,
adding examples to both classes from an unlabelled
pool of sentences called U :
1. Generate seed training data: Sspec and Snspec
2. Initialise: Tspec ? Sspec and Tnspec ? Snspec
3. Iterate:
? Train classifier using Tspec and Tnspec
? Order U by P (spec) values assigned by
the classifier
? Tspec ? most probable batch
? Tnspec ? least probable batch
What makes this iterative method efficient is that,
as we said earlier, hedging is expressed via key-
words in natural language texts; and often several
keywords are present in a single sentence. The
seed set Sspec contained either suggest or likely,
and due to the fact that other keywords cooccur
with these two in many sentences, they appeared
in Sspec with reasonable frequency. For example,
P (spec|may) = 0.9985 on the seed sets created
by (Medlock and Briscoe, 2007). The iterative ex-
tension of the training sets for each class further
boosted this effect, and skewed the distribution of
speculative indicators as sentences containing them
were likely to be added to the extended training set
for the speculative class, and unlikely to fall into the
non-speculative set.
We should add here that the very same feature has
an inevitable, but very important side effect that is
detrimental to the classification accuracy of mod-
els trained on a dataset which has been obtained
this way. This side effect is that other words (often
common words or stopwords) that tend to cooccur
with hedge cues will also be subject to the same it-
erative distortion of their distribution in speculative
and non-speculative uses. Perhaps the best exam-
ple of this is the word it. Being a stopword in our
case, and having no relevance at all to speculative
assertions, it has a class conditional probability of
P (spec|it) = 74.67% on the seed sets. This is due
to the use of phrases like it suggests that, it is likely,
and so on. After the iterative extension of training
sets, the class-conditional probability of it dramati-
cally increased, to P (spec|it) = 94.32%. This is a
consequence of the frequent co-occurence of it with
meaningful hedge cues and the probabilistic model
used and happens with many other irrelevant terms
(not just stopwords). The automatic elimination of
these irrelevant candidates is one of our main goals
(to limit the number of candidates for manual con-
sideration and thus to reduce the human effort re-
quired to select meaningful hedge cues).
This shows that, in addition to the desired ef-
fect of introducing further speculative keywords and
biasing their distribution towards the speculative
class, this iterative process also introduces signifi-
cant noise into the dataset. This observation led us
to the conclusion that in order to build efficient clas-
sifiers based on this kind of dataset, we should fil-
ter out noise. In the next part we will present our
feature selection procedure (evaluated in the Results
section) which is capable of underranking irrelevant
keywords in the majority of cases.
2.3 Feature (or keyword) selection
To handle the inherent noise in the training dataset
that originates from its weakly supervised construc-
tion, we applied the following feature selection pro-
cedure. The main idea behind it is that it is unlikely
that more than two keywords are present in the text,
which are useful for deciding whether an instance is
speculative. Here we performed the following steps:
283
1. We ranked the features x by frequency and
their class conditional probability P (spec|x).
We then selected those features that had
P (spec|x) > 0.94 (this threshold was cho-
sen arbitrarily) and appeared in the training
dataset with reasonable frequency (frequency
above 10?5). This set constituted the 2407 can-
didates which we used in the second analysis
phase.
2. For trigrams, bigrams and unigrams ? pro-
cessed separately ? we calculated a new class-
conditional probability for each feature x, dis-
carding those observations of x in speculative
instances where x was not among the two high-
est ranked candidate. Negative credit was given
for all occurrences in non-speculative contexts.
We discarded any feature that became unreli-
able (i.e. any whose frequency dropped be-
low the threshold or the strict class-conditional
probability dropped below 0.94). We did this
separately for the uni-, bi- and trigrams to avoid
filtering out longer phrases because more fre-
quent, shorter candidates took the credit for all
their occurrences. In this step we filtered out
85% of all the keyword candidates and kept 362
uni-, bi-, and trigrams altogether.
3. In the next step we re-evaluated all 362 candi-
dates together and filtered out all phrases that
had a shorter and thus more frequent substring
of themselves among the features, with a sim-
ilar class-conditional probability on the specu-
lative class (worse by 2% at most). Here we
discarded a further 30% of the candidates and
kept 253 uni-, bi-, and trigrams altogether.
This efficient way of reranking and selecting po-
tentially relevant features (we managed to discard
89.5% of all the initial candidates automatically)
made it easier for us to manually validate the re-
maining keywords. This allowed us to incorporate
supervision into the learning model in the feature
representation stage, but keep the weakly supervised
modelling (with only 5 minutes of expert supervi-
sion required).
2.4 Maximum Entropy Classifier
Maximum Entropy Models (Berger et al, 1996)
seek to maximise the conditional probability of
classes, given certain observations (features). This
is performed by weighting features to maximise the
likelihood of data and, for each instance, decisions
are made based on features present at that point, thus
maxent classification is quite suitable for our pur-
poses. As feature weights are mutually estimated,
the maxent classifier is capable of taking feature de-
pendence into account. This is useful in cases like
the feature it being dependent on others when ob-
served in a speculative context. By downweighting
such features, maxent is capable of modelling to a
certain extent the special characteristics which arise
from the automatic or weakly supervised training
data acquisition procedure. We used the OpenNLP
maxent package, which is freely available4 .
3 Results
In this section we will present our results for hedge
classification as a standalone task. In experiments
we made use of the hedge classification dataset of
scientific texts provided by (Medlock and Briscoe,
2007) and used a labeled dataset generated automat-
ically based on false positive predictions of an ICD-
9-CM coding system.
3.1 Results for hedge classification in
biomedical texts
As regards the degree of human intervention needed,
our classification and feature selection model falls
within the category of weakly supervised machine
learning. In the following sections we will evalu-
ate our above-mentioned contributions one by one,
describing their effects on feature space size (effi-
ciency in feature and noise filtering) and classifi-
cation accuracy. In order to compare our results
with Medlock and Briscoe?s results (Medlock and
Briscoe, 2007), we will always give the BEP (spec)
that they used ? the break-even-point of precision
and recall5. We will also present F?=1(spec) values
4http://maxent.sourceforge.net/
5It is the point on the precision-recall curve of spec class
where P = R. If an exact P = R cannot be realised due to
the equal ranking of many instances, we use the point closest
to P = R and set BEP (spec) = (P + R)/2. BEP is an
284
which show how good the models are at recognising
speculative assertions.
3.1.1 The effects of automatic feature selection
The method we proposed seems especially effec-
tive in the sense that we successfully reduced the
number of keyword candidates from an initial 2407
words having P (spec|x) > 0.94 to 253, which
is a reduction of almost 90%. During the pro-
cess, very few useful keywords were eliminated and
this indicated that our feature selection procedure
was capable of distinguishing useful keywords from
noise (i.e. keywords having a very high specula-
tive class-conditional probability due to the skewed
characteristics of the automatically gathered train-
ing dataset). The 2407-keyword model achieved a
BEP (spec) os 76.05% and F?=1(spec) of 73.61%,
while the model after feature selection performed
better, achieving a BEP (spec) score of 78.68%
and F?=1(spec) score of 78.09%. Simplifying the
model to predict a spec label each time a keyword
was present (by discarding those 29 features that
were too weak to predict spec alone) slightly in-
creased both the BEP (spec) and F?=1(spec) val-
ues to 78.95% and 78.25%. This shows that the
Maximum Entropy Model in this situation could
not learn any meaningful hypothesis from the cooc-
curence of individually weak keywords.
3.1.2 Improvements by manual feature
selection
After a dimension reduction via a strict reranking
of features, the resulting number of keyword candi-
dates allowed us to sort the retained phrases manu-
ally and discard clearly irrelevant ones. We judged
a phrase irrelevant if we could consider no situation
in which the phrase could be used to express hedg-
ing. Here 63 out of the 253 keywords retained by
the automatic selection were found to be potentially
relevant in hedge classification. All these features
were sufficient for predicting the spec class alone,
thus we again found that the learnt model reduced
to a single keyword-based decision.6 These 63 key-
interesting metric as it demonstrates how well we can trade-off
precision for recall.
6We kept the test set blind during the selection of relevant
keywords. This meant that some of them eventually proved to
be irrelevant, or even lowered the classification accuracy. Ex-
amples of such keywords were will, these data and hypothesis.
words yielded a classifier with a BEP (spec) score
of 82.02% and F?=1(spec) of 80.88%.
3.1.3 Results obtained adding external
dictionaries
In our final model we added the keywords used in
(Light et al, 2004) and those gathered for our ICD-
9-CM hedge detection module. Here we decided not
to check whether these keywords made sense in sci-
entific texts or not, but instead left this task to the
maximum entropy classifier, and added only those
keywords that were found reliable enough to predict
spec label alone by the maxent model trained on the
training dataset. These experiments confirmed that
hedge cues are indeed task specific ? several cues
that were reliable in radiology reports proved to be
of no use for scientific texts. We managed to in-
crease the number of our features from 63 to 71 us-
ing these two external dictionaries.
These additional keywords helped us to increase
the overall coverage of the model. Our final hedge
classifier yielded a BEP (spec) score of 85.29%
and F?=1(spec) score of 85.08% (89.53% Preci-
sion, 81.05% Recall) for the speculative class. This
meant an overall classification accuracy of 92.97%.
Using this system as a pre-processing module for
a hypothetical gene interaction extraction system,
we found that our classifier successfully excluded
gene names mentioned in a speculative sentence (it
removed 81.66% of all speculative mentions) and
this filtering was performed with a respectable pre-
cision of 93.71% (F?=1(spec) = 87.27%).
Articles 4
Sentences 1087
Spec sentences 190
Nspec sentences 897
Table 1: Characteristics of the BMC hedge dataset.
3.1.4 Evaluation on scientific texts from a
different source
Following the annotation standards of Medlock
and Briscoe (Medlock and Briscoe, 2007), we man-
ually annotated 4 full articles downloaded from the
We assumed that these might suggest a speculative assertion.
285
BMC Bioinformatics website to evaluate our final
model on documents from an external source. The
chief characteristics of this dataset (which is avail-
able at7) is shown in Table 1. Surprisingly, the model
learnt on FlyBase articles seemed to generalise to
these texts only to a limited extent. Our hedge clas-
sifier model yielded a BEP (spec) = 75.88% and
F?=1(spec) = 74.93% (mainly due to a drop in pre-
cision), which is unexpectedly low compared to the
previous results.
Analysis of errors revealed that some keywords
which proved to be very reliable hedge cues in Fly-
Base articles were also used in non-speculative con-
texts in the BMC articles. Over 50% (24 out of
47) of our false positive predictions were due to
the different use of 2 keywords, possible and likely.
These keywords were many times used in a mathe-
matical context (referring to probabilities) and thus
expressed no speculative meaning, while such uses
were not represented in the FlyBase articles (other-
wise bigram or trigram features could have captured
these non-speculative uses).
3.1.5 The effect of using 2-3 word-long phrases
as hedge cues
Our experiments demonstrated that it is indeed a
good idea to include longer phrases in the vector
space model representation of sentences. One third
of the features used by our advanced model were ei-
ther bigrams or trigrams. About half of these were
the kind of phrases that had no unigram components
of themselves in the feature set, so these could be re-
garded as meaningful standalone features. Examples
of such speculative markers in the fruit fly dataset
were: results support, these observations, indicate
that, not clear, does not appear, . . . The majority of
these phrases were found to be reliable enough for
our maximum entropy model to predict a specula-
tive class based on that single feature.
Our model using just unigram features achieved
a BEP (spec) score of 78.68% and F?=1(spec)
score of 80.23%, which means that using bigram
and trigram hedge cues here significantly improved
the performance (the difference in BEP (spec) and
F?=1(spec) scores were 5.23% and 4.97%, respec-
tively).
7
http://www.inf.u-szeged.hu/?szarvas/homepage/hedge.html
3.2 Results for hedge classification in radiology
reports
In this section we present results using the above-
mentioned methods for the automatic detection of
speculative assertions in radiology reports. Here we
generated training data by an automated procedure.
Since hedge cues cause systems to predict false pos-
itive labels, our idea here was to train Maximum
Entropy Models for the false positive classifications
of our ICD-9-CM coding system using the vector
space representation of radiology reports. That is,
we classified every sentence that contained a medi-
cal term (disease or symptom name) and caused the
automated ICD-9 coder8 to predict a false positive
code was treated as a speculative sentence and all
the rest were treated as non-speculative sentences.
Here a significant part of the false positive predic-
tions of an ICD-9-CM coding system that did not
handle hedging originated from speculative asser-
tions, which led us to expect that we would have
the most hedge cues among the top ranked keywords
which implied false positive labels.
Taking the above points into account, we used
the training set of the publicly available ICD-9-CM
dataset to build our model and then evaluated each
single token by this model to measure their predic-
tivity for a false positive code. Not surprisingly,
some of the best hedge cues appeared among the
highest ranked features, while some did not (they
did not occur frequently enough in the training data
to be captured by statistical methods).
For this task, we set the initial P (spec|x) thresh-
old for filtering to 0.7 since the dataset was gener-
ated by a different process and we expected hedge
cues to have lower class-conditional probabilities
without the effect of the probabilistic data acqui-
sition method that had been applied for scientific
texts. Using all 167 terms as keywords that had
P (spec|x) > 0.7 resulted in a hedge classifier with
an F?=1(spec) score of 64.04%
After the feature selection process 54 keywords
were retained. This 54-keyword maxent classifier
got an F?=1(spec) score of 79.73%. Plugging this
model (without manual filtering) into the ICD-9 cod-
ing system as a hedge module, the ICD-9 coder
8Here the ICD-9 coding system did not handle the hedging
task.
286
yielded an F measure of 88.64%, which is much bet-
ter than one without a hedge module (79.7%).
Our experiments revealed that in radiology re-
ports, which mainly concentrate on listing the iden-
tified diseases and symptoms (facts) and the physi-
cian?s impressions (speculative parts), detecting
hedge instances can be performed accurately using
unigram features. All bi- and trigrams retained by
our feature selection process had unigram equiva-
lents that were eliminated due to the noise present
in the automatically generated training data.
We manually examined all keywords that had a
P (spec) > 0.5 given as a standalone instance for
our maxent model, and constructed a dictionary of
hedge cues from the promising candidates. Here we
judged 34 out of 54 candidates to be potentially use-
ful for hedging. Using these 34 keywords we got an
F?=1(spec) performance of 81.96% due to the im-
proved precision score.
Extending the dictionary with the keywords we
gathered from the fruit fly dataset increased the
F?=1(spec) score to 82.07% with only one out-
domain keyword accepted by the maxent classifier.
Biomedical papers Medical reports
BEP (spec) F?=1(spec) F?=1(spec)
Baseline 1 60.00 ? 48.99
Baseline 2 76.30 ? ?
All features 76.05 73.61 64.04
Feature selection 78.68 78.09 79.73
Manual feat. sel. 82.02 80.88 81.96
Outer dictionary 85.29 85.08 82.07
Table 2: Summary of results.
4 Conclusions
The overall results of our study are summarised in
a concise way in Table 2. We list BEP (spec)
and F?=1(spec) values for the scientific text dataset,
and F?=1(spec) for the clinical free text dataset.
Baseline 1 denotes the substring matching system of
Light et al (Light et al, 2004) and Baseline 2 de-
notes the system of Medlock and Briscoe (Medlock
and Briscoe, 2007). For clinical free texts, Baseline
1 is an out-domain model since the keywords were
collected for scientific texts by (Light et al, 2004).
The third row corresponds to a model using all key-
words P (spec|x) above the threshold and the fourth
row a model after automatic noise filtering, while the
fifth row shows the performance after the manual fil-
tering of automatically selected keywords. The last
row shows the benefit gained by adding reliable key-
words from an external hedge keyword dictionary.
Our results presented above confirm our hypothe-
sis that speculative language plays an important role
in the biomedical domain, and it should be han-
dled in various NLP applications. We experimen-
tally compared the general features of this task in
texts from two different domains, namely medical
free texts (radiology reports), and scientific articles
on the fruit fly from FlyBase.
The radiology reports had mainly unambiguous
single-term hedge cues. On the other hand, it proved
to be useful to consider bi- and trigrams as hedge
cues in scientific texts. This, and the fact that many
hedge cues were found to be ambiguous (they ap-
peared in both speculative and non-speculative as-
sertions) can be attributed to the literary style of the
articles. Next, as the learnt maximum entropy mod-
els show, the hedge classification task reduces to a
lookup for single keywords or phrases and to the
evaluation of the text based on the most relevant cue
alone. Removing those features that were insuffi-
cient to classify an instance as a hedge individually
did not produce any difference in the F?=1(spec)
scores. This latter fact justified a view of ours,
namely that during the construction of a statistical
hedge detection module for a given application the
main issue is to find the task-specific keywords.
Our findings based on the two datasets employed
show that automatic or weakly supervised data ac-
quisition, combined with automatic and manual fea-
ture selection to eliminate the skewed nature of the
data obtained, is a good way of building hedge clas-
sifier modules with an acceptable performance.
The analysis of errors indicate that more com-
plex features like dependency structure and clausal
phrase information could only help in allocating the
scope of hedge cues detected in a sentence, not the
detection of any itself. Our finding that token uni-
gram features are capable of solving the task accu-
rately agrees with the the results of previous works
on hedge classification ((Light et al, 2004), (Med-
287
lock and Briscoe, 2007)), and we argue that 2-3
word-long phrases also play an important role as
hedge cues and as non-speculative uses of an oth-
erwise speculative keyword as well (i.e. to resolve
an ambiguity). In contrast to the findings of Wiebe
et al ((Wiebe et al, 2004)), who addressed the
broader task of subjectivity learning and found that
the density of other potentially subjective cues in
the context benefits classification accuracy, we ob-
served that the co-occurence of speculative cues in
a sentence does not help in classifying a term as
speculative or not. Realising that our learnt mod-
els never predicted speculative labels based on the
presence of two or more individually weak cues and
discarding such terms that were not reliable enough
to predict a speculative label (using that term alone
as a single feature) slightly improved performance,
we came to the conclusion that even though specu-
lative keywords tend to cooccur, and two keywords
are present in many sentences; hedge cues have a
speculative meaning (or not) on their own without
the other term having much impact on this.
The main issue thus lies in the selection of key-
words, for which we proposed a procedure that is
capable of reducing the number of candidates to an
acceptable level for human evaluation ? even in data
collected automatically and thus having some unde-
sirable properties.
The worse results on biomedical scientific papers
from a different source also corroborates our find-
ing that hedge cues can be highly ambiguous. In
our experiments two keywords that are practically
never used in a non-speculative context in the Fly-
Base articles we used for training were responsi-
ble for 50% of false positives in BMC texts since
they were used in a different meaning. In our case,
the keywords possible and likely are apparently al-
ways used as speculative terms in the FlyBase arti-
cles used, while the articles from BMC Bioinformat-
ics frequently used such cliche phrases as all possi-
ble combinations or less likely / more likely . . . (re-
ferring to probabilities shown in the figures). This
shows that the portability of hedge classifiers is lim-
ited, and cannot really be done without the examina-
tion of the specific features of target texts or a more
heterogenous corpus is required for training. The
construction of hedge classifiers for each separate
target application in a weakly supervised way seems
feasible though. Collecting bi- and trigrams which
cover non-speculative usages of otherwise common
hedge cues is a promising solution for addressing the
false positives in hedge classifiers and for improving
the portability of hedge modules.
4.1 Resolving the scope of hedge keywords
In this paper we focused on the recognition of hedge
cues in texts. Another important issue would be to
determine the scope of hedge cues in order to lo-
cate uncertain sentence parts. This can be solved ef-
fectively using a parser adapted for biomedical pa-
pers. We manually evaluated the parse trees gen-
erated by (Miyao and Tsujii, 2005) and came to the
conclusion that for each keyword it is possible to de-
fine the scope of the keyword using subtrees linked
to the keyword in the predicate-argument syntac-
tic structure or by the immediate subsequent phrase
(e.g. prepositional phrase). Naturally, parse errors
result in (slightly) mislocated scopes but we had
the general impression that state-of-the-art parsers
could be used efficiently for this issue. On the other
hand, this approach requires a human expert to de-
fine the scope for each keyword separately using the
predicate-argument relations, or to determine key-
words that act similarly and their scope can be lo-
cated with the same rules. Another possibility is
simply to define the scope to be each token up to
the end of the sentence (and optionally to the previ-
ous punctuation mark). The latter solution has been
implemented by us and works accurately for clinical
free texts. This simple algorithm is similar to NegEx
(Chapman et al, 2001) as we use a list of phrases
and their context, but we look for punctuation marks
to determine the scopes of keywords instead of ap-
plying a fixed window size.
Acknowledgments
This work was supported in part by the NKTH grant
of Jedlik ?Anyos R&D Programme 2007 of the Hun-
garian government (codename TUDORKA7). The
author wishes to thank the anonymous reviewers for
valuable comments and Veronika Vincze for valu-
able comments in linguistic issues and for help with
the annotation work.
288
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. Journal of Biomedi-
cal Informatics, 5:301?310.
Ken Hyland. 1994. Hedging in academic writing and eap
textbooks. English for Specific Purposes, 13(3):239?
256.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, spec-
ulations, and statements in between. In Lynette
Hirschman and James Pustejovsky, editors, HLT-
NAACL 2004 Workshop: BioLINK 2004, Linking Bi-
ological Literature, Ontologies and Databases, pages
17?24, Boston, Massachusetts, USA, May 6. Associa-
tion for Computational Linguistics.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 992?999,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 83?90, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Marie A. Moisio. 2006. A Guide to Health Insurance
Billing. Thomson Delmar Learning.
John P. Pestian, Chris Brew, Pawel Matykiewicz,
DJ Hovermale, Neil Johnson, K. Bretonnel Cohen, and
Wlodzislaw Duch. 2007. A shared task involving
multi-label classification of clinical free text. In Bi-
ological, translational, and clinical language process-
ing, pages 97?104, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the Seventh Com-
putational Natural Language Learning Conference,
pages 25?32, Edmonton, Canada, May-June. Associa-
tion for Computational Linguistics.
James G. Shanahan, Yan Qu, and Janyce Wiebe. 2005.
Computing Attitude and Affect in Text: Theory
and Applications (The Information Retrieval Series).
Springer-Verlag New York, Inc., Secaucus, NJ, USA.
Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308.
289
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 161?164,
Prague, June 2007. c?2007 Association for Computational Linguistics
GYDER: maxent metonymy resolution
Richa?rd Farkas
University of Szeged
Department of Informatics
H-6720 Szeged, A?rpa?d te?r 2.
rfarkas@inf.u-szeged.hu
Eszter Simon
Budapest U. of Technology
Dept. of Cognitive Science
H-1111 Budapest, Stoczek u 2.
esimon@cogsci.bme.hu
Gyo?rgy Szarvas
University of Szeged
Department of Informatics
H-6720 Szeged, A?rpa?d te?r 2.
szarvas@inf.u-szeged.hu
Da?niel Varga
Budapest U. of Technology
MOKK Media Research
H-1111 Budapest, Stoczek u 2.
daniel@mokk.bme.hu
Abstract
Though the GYDER system has achieved
the highest accuracy scores for the
metonymy resolution shared task at
SemEval-2007 in all six subtasks, we don?t
consider the results (72.80% accuracy for
org, 84.36% for loc) particularly impres-
sive, and argue that metonymy resolution
needs more features.
1 Introduction
In linguistics metonymy means using one term, or
one specific sense of a term, to refer to another,
related term or sense. For example, in ?the pen
is mightier than the sword? pen refers to writing,
the force of ideas, while sword refers to military
force. Named Entity Recognition (NER) is of
key importance in numerous natural language pro-
cessing applications ranging from information ex-
traction to machine translation. Metonymic usage
of named entities is frequent in natural language.
On the basic NER categories person, place,
organisation state-of-the-art systems generally
perform in the mid to the high nineties. These sys-
tems typically do not distinguish between literal or
metonymic usage of entity names, even though this
would be helpful for most applications. Resolving
metonymic usage of proper names would therefore
directly benefit NER and indirectly all NLP tasks
(such as anaphor resolution) that require NER.
Markert and Nissim (2002) outlined a corpus-
based approach to proper name metonymy as a se-
mantic classification problem that forms the basis
of the 2007 SemEval metonymy resolution task.
Instances like ?He was shocked by Vietnam? or
?Schengen boosted tourism? were assigned to broad
categories like place-for-event, sometimes
ignoring narrower distinctions, such as the fact that
it wasn?t the signing of the treaty at Schengen but
rather its actual implementation (which didn?t take
place at Schengen) that boosted tourism. But the
corpus makes clear that even with these (sometimes
coarse) class distinctions, several metonymy types
seem to appear extremely rarely in actual texts.
The shared task focused on two broad named en-
tity classes as metonymic sources, location and
org, each having several target classes. For more
details on the data sets, see the task description pa-
per Markert and Nissim (2007).
Several categories (e.g. place-for-event,
organisation-for-index) did not contain a
sufficient number of examples for machine learn-
ing, and we decided early on to accept the fact that
these categories will not be learned and to concen-
trate on those classes where learning seemed feasi-
ble. The shared task itself consisted of 3 subtasks
of different granularity for both organisation and lo-
cation names. The fine-grained evaluation aimed
at distinguishing between all categories, while the
medium-grained evaluation grouped different types
of metonymic usage together and addressed literal /
mixed / metonymic usage. The coarse-grained sub-
task was in fact a literal / nonliteral two-class classi-
fication task.
Though GYDER has obtained the highest accu-
racy for the metonymy shared task at SemEval-2007
in all six subtasks, we don?t consider the results
161
(72.80% accuracy for org, 84.36% for loc) par-
ticularly impressive. In Section 3 we describe the
feature engineering lessons learned fromworking on
the task. In Section 5 we offer some speculative re-
marks on what it would take to improve the results.
2 Learning
GYDER (the acronym was formed from the initials
of the author? first names) is a maximum entropy
learner. It uses Zhang Le?s 1 maximum entropy
toolkit, setting the Gaussian prior to 1. We used ran-
dom 5-fold cross-validation to determine the useful-
ness of a particular feature. Due to the small num-
ber of instances and features, the learning algorithm
always converged before 30 iterations, so the cross-
validation process took only seconds.
We also tested the classic C4.5 decision tree learn-
ing algorithm Quinlan (1993), but our early exper-
iments showed that the maximum entropy learner
was consistently superior to the decision tree clas-
sifier for this task, yielding about 2-5% higher accu-
racy scores on average on both tasks (on the training
set, using cross-validation).
3 Feature Engineering
We tested several features describing orthographic,
syntactic, or semantic characteristics of the Possibly
Metonymic Words (PMWs). Here we follow Nissim
and Markert (2005), who reported three classes of
features to be the most relevant for metonymy res-
olution: the grammatical annotations provided for
the corpus examples by the task organizers, the de-
terminer, and the grammatical number of the PMW.
We also report on some features that didn?t work.
3.1 Grammatical annotations
We used the grammatical annotations provided for
each PMW in several ways. First, we used as a
feature the type of the grammatical relation and the
word form of the related word. (If there was more
than one related word, each became a feature.) To
overcome data sparseness, it is useful to general-
ize from individual headwords Markert and Nissim
(2003). We used three different methods to achieve
this:
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
First, we used Levin?s (1993) verb classification
index to generalize the headwords of the most rele-
vant grammatical relations (subject and object). The
added feature was simply the class assigned to the
verb by Levin.
We also used WordNet (Fellbaum 1998) to gen-
eralize headwords. First we gathered the hypernym
path from WordNet for each headword?s sense#1 in
the train corpus. Based on these paths we collected
synsets whose tree frequently indicated metonymic
sense. We indicated with a feature if the headword
in question was in one of such collected subtrees.
Third, we have manually built a very small verb
classification ?Trigger? table for specific cases. E.g.
announce, say, declare all trigger the same feature.
This table is the only resource in our final system
that was manually built by us, so we note that on the
test corpus, disabling this ?Trigger? feature does not
alter org accuracy, and decreases loc accuracy by
0.44%.
3.2 Determiners
Following Nissim and Markert (2005), we distin-
guished between definite, indefinite, demonstrative,
possessive, wh and other determiners. We also
marked if the PMW was sentence-initial, and thus
necessarily determinerless. This feature was useful
for the resolution of organisation PMWs so we used
it only for the org tasks. It was not straightforward,
however, to assign determiners to the PMWswithout
proper syntactic analysis. After some experiments,
we linked the nearest determiner and the PMW to-
gether if we found only adjectives (or nothing) be-
tween them.
3.3 Number
This feature was particularly useful to separate
metonymies of the org-for-product class. We
assumed that only PMWs ending with letter s might
be in plural form, and for themwe compared the web
search result numbers obtained by the Google API.
We ran two queries for each PMWs, one for the full
name, and one for the name without its last charac-
ter. If we observed a significant increase in the num-
ber of hits returned by Google for the shorter phrase,
we set this feature for plural.
162
3.4 PMW word form
We included the surface form of the PMW as a fea-
ture, but only for the org domain. Cross-validation
on the training corpus showed that the use of this
feature causes an 1.5% accuracy improvement for
organisations, and a slight degradation for locations.
The improvement perfectly generalized to the test
corpora. Some company names are indeed more
likely to be used in a metonymic way, so we be-
lieve that this feature does more than just exploit-
ing some specificity of the shared task corpora. We
note that the ranking of our system would have been
unaffected even if we didn?t use this feature.
3.5 Unsuccessful features
Here we discuss those features where cross-
validation didn?t show improvements (and thus were
not included in the submitted system).
Trigger words were automatically collected lists of
word forms and phrases that more frequently
appeared near metonymic PMWs.
Expert triggers were similar trigger words or
phrases, but suggested by a linguist expert to
be potentially indicative for metonymic usage.
We experimented with sample-level, sentence-
level and vicinity trigger phrases.
Named entity labels given by a state-of-the-art
named entity recognizer (Szarvas et al 2006).
POS tags around PMWs.
Ortographical features such as capitalisation and
and other surface characteristics for the PMW
and nearby words.
Individual tokens of the potentially metonymic
phrase.
Main category of Levin?s hierarchical classification.
Inflectional category of the verb nearest to the PMW
in the sentence.
4 Results
Table 1. shows the accuracy scores of our submitted
system on fine classification granularity. As a base-
line, we also evalute the system without the Word-
Net, Levin, Trigger and PMW word form features.
This baseline system is quite similar to the one de-
scribed by Nissim and Markert (2005). We also pub-
lish the majority baseline scores.
run majority baseline submitted
org train 5-fold 63.30 77.51 80.92
org test 61.76 70.55 72.80
loc train 5-fold 79.68 85.58 88.36
loc test 79.41 83.59 84.36
Table 1: Accuracy of the submitted system
We could not exploit the hierarchical structure of
the fine-grained tag set, and ended up treating it as
totally unstructured even for the mixed class, unlike
Nissim and Markert, who apply complicated heuris-
tics to exploit the special semantics of this class.
For the coarse and medium subtasks of the loc
domain, we simply coarsened the fine-grained re-
sults. For the coarse and medium subtasks of
the org domain, we coarsened the train corpus to
medium coarseness before training. This idea was
based on observations on training data, but was
proven to be unjustified: it slightly decreased the
system?s accuracy on the medium subtask.
coarse medium fine
location 85.24 84.80 84.36
organisation 76.72 73.28 72.80
Table 2: Accuracy of the GYDER system for each
domain / granularity
In general, the coarser grained evaluation did not
show a significantly higher accuracy (see Table 2.),
proving that the main difficulty is to distinguish be-
tween literal and metonymic usage, rather than sepa-
rating metonymy classes from each other (since dif-
ferent classes represent significantly different usage
/ context). Because of this, data sparseness remained
a problem for coarse-grained classification as well.
Per-class results of the submitted system for
both domains are shown on Table 3. Note
that our system never predicted loc values from
the four small classes place-for-event and
product, object-for-name and other as
these had only 26 instances altogether. Since
we never had significant results for the mixed
category, in effect the loc task ended up a bi-
nary classification task between literal and
place-for-people.
163
loc class # prec rec f
literal 721 86.83 95.98 91.17
place-for-people 141 68.22 51.77 58.87
mixed 20 25.00 5.00 8.33
othermet 11 - 0.0 -
place-for-event 10 - 0.0 -
object-for-name 4 - 0.0 -
place-for-product 1 - 0.0 -
org class # prec rec f
literal 520 75.76 90.77 82.59
org-for-members 161 65.99 60.25 62.99
org-for-product 67 82.76 35.82 50.00
mixed 60 43.59 28.33 34.34
org-for-facility 16 100.0 12.50 22.22
othermet 8 - 0.0 -
object-for-name 6 50.00 16.67 25.00
org-for-index 3 - 0.0 -
org-for-event 1 - 0.0 -
Table 3: Per-class accuracies for both domains
While in the org set the system also ig-
nores the smallest categories othermet,
org-for-index and event (a total of 11
instances), the six major categories literal,
org-for-members, org-for-product,
org-for-facility, object-for-name,
mixed all receive meaningful hypotheses.
5 Conclusions, Further Directions
The features we eventually selected performed well
enough to actually achieve the best scores in all six
subtasks of the shared task, and we think they are
useful in general. But it is worth emphasizing that
many of these features are based on the grammatical
annotation provided by the task organizers, and as
such, would require a better dependency parser than
we currently have at our disposal to create a fully
automatic system.
That said, there is clearly a great deal of merit to
provide this level of annotation, and we would like
to speculate what would happen if even more de-
tailed annotation, not just grammatical, but also se-
mantical, were provided manually. We hypothesize
that the metonymy task would break down into the
task of identifying several journalistic cliches such
as ?location for sports team?, ?capital city for gov-
ernment?, and so on, which are not yet alays dis-
tinguished by the depth of the annotation.
It would be a true challenge to create a data set
of non-cliche metonymy cases, or a corpus large
enough to represent rare metonymy types and chal-
lenging non-cliche metonymies better.
We feel that at least regarding the corpus used for
the shared task, the potential of the grammatical an-
notation for PMWs was more or less well exploited.
Future systems should exploit more semantic knowl-
edge, or the power of a larger data set, or preferably
both.
Acknowledgement
We wish to thank Andra?s Kornai for help and
encouragement, and the anonymous reviewers for
valuable comments.
References
Christiane Fellbaum ed. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. A Preliminary Investigation. The University of
Chicago Press.
Katja Markert and Malvina Nissim. 2002. Metonymy
resolution as a classification task. Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002). Philadelphia, USA.
Katja Markert and Malvina Nissim. 2003. Syntactic Fea-
tures and Word Similarity for Supervised Metonymy
Resolution. Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL2003). Sapporo, Japan.
Malvina Nissim and Katja Markert. 2005. Learning
to buy a Renault and talk to BMW: A supervised
approach to conventional metonymy. International
Workshop on Computational Semantics (IWCS2005).
Tilburg, Netherlands.
Katja Markert and Malvina Nissim. 2007. SemEval-
2007 Task 08: Metonymy Resolution at SemEval-
2007. In Proceedings of SemEval-2007.
Ross Quinlan. 1993. C4.5: Programs for machine learn-
ing. Morgan Kaufmann.
Gyo?rgy Szarvas, Richa?rd Farkas and Andra?s Kocsor.
2006. Multilingual Named Entity Recognition Sys-
tem Using Boosting and C4.5 Decision Tree Learning
Algorithms. Proceedings of Discovery Science 2006,
DS2006, LNAI 4265 pp. 267-278. Springer-Verlag.
164
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 38?45,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The BioScope corpus: annotation for negation, uncertainty and their 
scope in biomedical texts 
 
 
Gy?rgy Szarvas1, Veronika Vincze1, Rich?rd Farkas2 and J?nos Csirik2 
1Department of Informatics 2Research Group on Artificial Intelligence 
University of Szeged Hungarian Academy of Science 
H-6720, Szeged, ?rp?d t?r 2. H-6720, Szeged, Aradi v?rtan?k tere 1. 
{szarvas, vinczev, rfarkas, csirik}@inf.u-szeged.hu 
 
Abstract 
This article reports on a corpus annotation 
project that has produced a freely available re-
source for research on handling negation and 
uncertainty in biomedical texts (we call this 
corpus the BioScope corpus). The corpus con-
sists of three parts, namely medical free texts, 
biological full papers and biological scientific 
abstracts. The dataset contains annotations at 
the token level for negative and speculative 
keywords and at the sentence level for their 
linguistic scope. The annotation process was 
carried out by two independent linguist anno-
tators and a chief annotator ? also responsible 
for setting up the annotation guidelines ? who 
resolved cases where the annotators disagreed. 
We will report our statistics on corpus size, 
ambiguity levels and the consistency of anno-
tations. 
1 Introduction 
Detecting uncertain and negative assertions is es-
sential in most Text Mining tasks where in general, 
the aim is to derive factual knowledge from textual 
data. This is especially so for many tasks in the 
biomedical (medical and biological) domain, 
where these language forms are used extensively in 
textual documents and are intended to express im-
pressions, hypothesised explanations of experi-
mental results or negative findings. Take, for 
example, the clinical coding of medical reports, 
where the coding of a negative or uncertain disease 
diagnosis may result in an over-coding financial 
penalty. Another example from the biological do-
main is interaction extraction, where the aim is to 
mine text evidence for biological entities with cer-
tain relations between them. Here, while an uncer-
tain relation or the non-existence of a relation 
might be of some interest for an end-user as well, 
such information must not be confused with real 
textual evidence (reliable information). A general 
conclusion is that for text mining, extracted infor-
mation that is within the scope of some negative / 
speculative (hedge or soft negation) keyword 
should either be discarded or presented separately 
from factual information.  
Even though many successful text processing 
systems (Friedman et al, 1994, Chapman et al 
2001, Elkin et al 2005) handle the above-
mentioned phenomena, most of them exploit hand-
crafted rule-based negation/uncertainty detection 
modules. To the best of our knowledge, there are 
no publicly available standard corpora of reason-
able size that are usable for evaluating the auto-
matic detection and scope resolution of these 
language phenomena. The availability of such a 
resource would undoubtedly facilitate the devel-
opment of corpus-based statistical systems for ne-
gation/hedge detection and resolution.  
Our study seeks to fill this gap by presenting the 
BioScope corpus, which consists of medical and 
biological texts annotated for negation, speculation 
and their linguistic scope. This was done to permit 
a comparison between and to facilitate the devel-
opment of systems for negation/hedge detection 
and scope resolution. The corpus described in this 
paper has been made publicly available for re-
search purposes and it is freely downloadable1. 
                                                          
1 www.inf.u-szeged.hu/rgai/bioscope  
38
1.1 Related work 
Chapman et al (2001) created a simple regular 
expression algorithm called NegEx that can detect 
phrases indicating negation and identify medical 
terms falling within the negative scope. With this 
process, a large part of negatives can be identified 
in discharge summaries. 
Mutalik et al (2001) earlier developed 
Negfinder in order to recognise negated patterns in 
medical texts. Their lexer uses regular expressions 
to identify words indicating negation and then it 
passes them as special tokens to the parser, which 
makes use of the single-token look-ahead strategy. 
Thus, without appealing to the syntactic structure 
of the sentence, Negfinder can reliably identify 
negated concepts in medical narrative when they 
are located near the negation markers. 
Huang and Lowe (2007) implemented a hybrid 
approach to automated negation detection. They 
combined regular expression matching with 
grammatical parsing: negations are classified on 
the basis of syntactic categories and they are 
located in parse trees. Their hybrid approach is 
able to identify negated concepts in radiology 
reports even when they are located at some 
distance from the negative term. 
The Medical Language Extraction and Encoding 
(MedLEE) system was developed as a general 
natural language processor in order to encode 
clinical documents in a structured form (Friedman 
et al, 1994). Negated concepts and certainty 
modifiers are also encoded within the system, thus 
it enables them to make a distinction between 
negated/uncertain concepts and factual information 
which is crucial in information retrieval. 
Elkin et al (2005) use a list of negation words 
and a list of negation scope-ending words in order 
to identify negated statements and their scope. 
Although a fair amount of literature on 
uncertainty (or hedging) in scientific texts has been 
produced since the 1990s (e.g. Hyland, 1994), 
speculative language from a Natural Language 
Processing perspective has only been studied in the 
past few years. Previous studies (Light et al, 2004) 
showed that the detection of hedging can be solved 
effectively by looking for specific keywords which 
imply speculative content. 
Another possibility is to treat the problem as a 
classification task and train a statistical  model to 
discriminate speculative and non-speculative 
assertions. This approach requires the availability 
of labeled instances to train the models on. 
Medlock and Briscoe (2007) proposed a weakly 
supervised setting for hedge classification in 
scientific texts where the aim is to minimise human 
supervision needed to obtain an adequate amount 
of training data. Their system focuses on locating 
hedge cues in text and thus they do not determine 
the scopes (in other words in a text they define the 
scope to be a whole sentence). 
1.2 Related resources 
Even though the problems of negation (mainly in 
the medical domain) and hedging (mainly in the 
scientific domain) have received much interest in 
the past few years, open access annotated resources 
for training, testing and comparison are rare and 
relatively small in size. Our corpus is the first one 
with an annotation of negative/speculative 
keywords and their scope. The authors are only 
aware of the following related corpora: 
 
? The Hedge classification corpus (Medlock 
and Briscoe, 2007), which has been 
annotated for hedge cues (at the sentence 
level) and consists of five full biological 
research papers (1537 sentences). No scope 
annotation is given in the original corpus. 
We included this publicly available corpus 
in ours, enriching the data with annotation 
for negation cues and linguistic scope for 
both hedging and negation. 
? The Genia Event corpus (Kim et al, 2008), 
which annotates biological events with 
negation and three levels of uncertainty 
(1000 abstracts). 
? The BioInfer corpus (Pyysalo et al, 2007), 
where biological relations are annotated for 
negation (1100 sentences in size).  
In the two latter corpora biological terms 
(relations and events) have been annotated for both 
negation and hedging, but linguistic cues (i.e. 
which keyword modifies the semantics of the 
statement) have not been annotated. We annotated 
keywords and their linguistic scope, which is very 
useful for machine learning or rule-based negation 
and hedge detection systems. 
39
2 Annotation guidelines 
This section describes the basic principles on the 
annotation of speculative and negative scopes in 
biomedical texts. Some basic definitions and tech-
nical details are given in Section 2.1, then the gen-
eral guidelines are discussed in Section 2.2 and the 
most typical keywords and their scopes are illus-
trated with examples in Section 2.3. Some special 
cases and exceptions are listed in Section 2.4, then 
the annotation process of the corpus is described 
and discussed in Section 2.5. The complete annota-
tion guidelines document is available from the cor-
pus homepage. 
2.1 Basic issues 
In a text, just sentences with some instance of 
speculative or negative language are considered for 
annotation. The annotation is based on linguistic 
principles, i.e. parts of sentences which do not con-
tain any biomedical term are also annotated if they 
assert the non-existence/uncertainty of something.  
As for speculative annotation, if a sentence is a 
statement, that is, it does not include any specula-
tive element that suggests uncertainty, it is disre-
garded. Questions inherently suggest uncertainty ? 
which is why they are asked ?, but they will be 
neglected and not annotated unless they contain 
speculative language. 
Sentences containing any kind of negation are 
examined for negative annotation. Negation is un-
derstood as the implication of the non-existence of 
something. However, the presence of a word with 
negative content does not imply that the sentence 
should be annotated as negative, since there are 
sentences that include grammatically negative 
words but have a speculative meaning or are actu-
ally regular assertions (see the examples below). 
In the corpus, instances of speculative and nega-
tive language ? that is, keywords and their scope ? 
are annotated. Speculative elements are marked by 
angled brackets: <or>, <suggests> etc., while 
negative keywords are marked by square brackets: 
[no], [without] etc. The scope of both negative and 
speculative keywords is denoted by parentheses. 
Also, the speculative or negative cue is always in-
cluded within its scope: 
This result (<suggests> that the valency of Bi in 
the material is smaller than + 3). 
Stable appearance the right kidney ([without] hy-
dronephrosis). 
In the following, the general guidelines for specu-
lative and negative annotation are presented. 
2.2 General guidelines 
During the annotation process, we followed a min-
max strategy for the marking of keywords and their 
scope. When marking the keywords, a minimalist 
strategy was followed: the minimal unit that ex-
pressed hedging or negation was marked as a key-
word. However, there are some cases when hedge 
or negation can be expressed via a phrase rather 
than a single word. Complex keywords are phrases 
that express uncertainty or negation together, but 
they cannot do this on their own (the meaning or 
the semantics of its subcomponents are signifi-
cantly different from the semantics of the whole 
phrase). An instance of a complex keyword can be 
seen in the following sentence: 
Mild bladder wall thickening (<raises the question 
of> cystitis). 
On the other hand, a sequence of words cannot be 
marked as a complex keyword if it is only one of 
those words that express speculative or negative 
content (even without the other word). Thus prepo-
sitions, determiners, adverbs and so on are not an-
notated as parts of the complex keyword if the 
keyword can have a speculative or negative con-
tent on its own: 
The picture most (<likely> reflects airways dis-
ease). 
Complex keywords are not to be confused with the 
sequence of two or more keywords because they 
can express hedge or negation on their own, that is, 
without the other keyword as well. In this case, 
each keyword is annotated separately, as is shown 
in the following example: 
Slightly increased perihilar lung markings (<may> 
(<indicate> early reactive airways disease)). 
2.3 Scope marking 
When marking the scopes of negative and specula-
tive keywords, we extended the scope to the big-
gest syntactic unit possible (in contrast to other 
corpora like the one described in (Mutalik et al, 
2001)). Thus, annotated scopes always have the 
40
maximal length ? as opposed to the strategy for 
annotating keywords, where we marked the mini-
mal unit possible. Our decision was supported by 
two facts. First, since scopes must contain their 
keywords, it seemed better to include every ele-
ment in between the keyword and the target word 
in order to avoid ?empty? scopes, that is, scopes 
without a keyword. In the next example, however 
is not affected by the hedge cue but it should be 
included within the scope, otherwise the keyword 
and its target phrase would be separated: 
(Atelectasis in the right mid zone is, however, 
<possible>). 
Second, the status of modifiers is occasionally 
vague: it is sometimes not clear whether the modi-
fier of the target word belongs to its scope as well. 
The following sentence can describe two different 
situations: 
There is [no] primary impairment of glucocorti-
coid metabolism in the asthmatics. 
First, the glucocorticoid metabolism is impaired in 
the asthmatics but not primarily, that is, the scope 
of no extends to primary. Second, the scope of no 
extends to impairment (and its modifiers and com-
plements as well), thus there is no impairment of 
the glucocorticoid metabolism at all. Another ex-
ample is shown here: 
Mild viral <or> reactive airways disease is de-
tected. 
The syntactic structure of the above sentence is 
ambiguous. First, the airways disease is surely 
mild, but it is not known whether it is viral or reac-
tive; or second, the airways disease is either mild 
and viral or reactive and not mild. Most of the sen-
tences with similar problems cannot be disambigu-
ated on the basis of contextual information, hence 
the proper treatment of such sentences remains 
problematic. However, we chose to mark the wid-
est scope available: in other words, we preferred to 
include every possible element within the scope 
rather than exclude elements that should probably 
be included. 
 The scope of a keyword can be determined on 
the basis of syntax. The scope of verbs, auxiliaries, 
adjectives and adverbs usually extends to the right 
of the keyword. In the case of verbal elements, i.e. 
verbs and auxiliaries, it ends at the end of the 
clause (if the verbal element is within a relative 
clause or a coordinated clause) or the sentence, 
hence all complements and adjuncts are included, 
in accordance with the principle of maximal scope 
size. Take the following examples: 
The presence of urothelial thickening and mild 
dilatation of the left ureter (<suggest> that the 
patient may have continued vesicoureteral reflux). 
These findings that (<may> be from an acute 
pneumonia) include minimal bronchiectasis as 
well. 
These findings (<might> be chronic) and (<may> 
represent reactive airways disease). 
The scope of attributive adjectives generally ex-
tends to the following noun phrase, whereas the 
scope of predicative adjectives includes the whole 
sentence. For example, in the following two state-
ments: 
This is a 3 month old patient who had (<possible> 
pyelonephritis) with elevated fever. 
(The demonstration of hormone receptor proteins 
in cells from malignant effusions is <possible>). 
Sentential adverbs have a scope over the entire 
sentence, while the scope of other adverbs usually 
ends at the end of the clause or sentence. For in-
stance, 
(The chimaeric oncoprotein <probably> affects 
cell survival rather than cell growth). 
Right upper lobe volume loss and (<probably> 
pneumonia). 
The scope of conjunctions extends to all members 
of the coordination. That is, it usually extends to 
the both left and right: 
Symptoms may include (fever, cough <or> itches). 
Complex keywords such as either ? or have one 
scope: 
Mild perihilar bronchial wall thickening may rep-
resent (<either> viral infection <or> reactive 
airways disease). 
Prepositions have a scope over the following 
(noun) phrase: 
Mildly hyperinflated lungs ([without] focal opac-
ity). 
41
When the subject of the sentence contains the 
negative determiners no or neither, its scope ex-
tends to the entire sentence: 
Surprisingly, however, ([neither] of these proteins 
bound in vitro to EBS1 or EBS2). 
The main exception that changes the original scope 
of the keyword is the passive voice. The subject of 
the passive sentence was originally the object of 
the verb, that is, it should be within its scope. This 
is why the subject must also be marked within the 
scope of the verb or auxiliary. For instance, 
(A small amount of adenopathy <cannot be> com-
pletely <excluded>). 
Another example of scope change is the case of 
raising verbs (seem, appear, be expected, be likely 
etc.). These can have two different syntactic pat-
terns, as the following examples suggest:  
It seems that the treatment is successful. 
The treatment seems to be successful. 
In the first case, the scope of seems starts right 
with the verb. If this was the case in the second 
pattern, the treatment would not be included in the 
scope, but it should be like that shown in the first 
pattern. Hence in the second sentence, the scope 
must be extended to the subject as well: 
It (<seems> that the treatment is successful). 
(The treatment <seems> to be successful). 
Sometimes a negative keyword is present in the 
text apparently without a scope: negative obviously 
expresses negation, but the negated fact ? what 
medical problem the radiograph is negative for ? is 
not part of the sentence. In such cases, the keyword 
is marked and the scope contains just the keyword: 
([Negative]) chest radiograph. 
In the case of elliptic sentences, the same strategy 
is followed: the keyword is marked and its scope 
includes only the keyword since the verbal phrase, 
that is, the scope of not, is not repeated in the sen-
tence. 
This decrease was seen in patients who responded 
to the therapy as well as in those who did ([not]). 
Generally, punctuation marks or conjunctions 
function as scope boundary markers in the corpus, 
in contrast to the corpus described in (Mutalik et 
al., 2001) where certain lexical items are treated as 
negation-termination tokens. Since in our corpus 
the scope of negation or speculation is mostly ex-
tended to the entire clause in the case of verbal 
elements, it is clear that markers of a sentence or 
clause boundary determine the end of their scope. 
2.4 Special cases 
It seems unequivocal that whenever there is a 
speculative or negative cue in the sentence, the 
sentence expresses hedge or negation. However, 
we have come across several cases where the pres-
ence of a speculative/negative keyword does not 
imply a hedge/negation. That is, some of the cues 
do not denote speculation or negation in all their 
occurrences, in other words, they are ambiguous. 
For instance, the following sentence is a state-
ment and it is the degree of probability that is pre-
cisely determined, but it is not an instance of 
hedging although it contains the cue probable: 
The planar amide groups in which is still digging 
nylon splay around 30 less probable event. 
As for negative cues, sentences including a nega-
tive keyword are not necessarily to be annotated 
for negation. They can, however, have a specula-
tive content as well. The following sentence con-
tains cannot, which is a negative keyword on its 
own, but not in this case: 
(A small amount of adenopathy <cannot be> com-
pletely <excluded>). 
Some other sentences containing a negative key-
word are not to be annotated either for speculation 
or for negation. In the following example, the 
negative keyword is accompanied by an adverb 
and their meaning is neither speculative nor nega-
tive. The sequence of the negative keyword and the 
adverb can be easily substituted by another adverb 
or adjective having the same (or a similar) mean-
ing, which is by no means negative ? as shown in 
the example. In this way, the sentence below can 
be viewed as a positive assertion (not a statement 
of the non-existence of something). 
Thus, signaling in NK3.3 cells is not always 
(=sometimes) identical with that in primary NK 
cells. 
As can be seen from the above examples, hedging 
or negation is determined not just by the presence 
42
of an apparent cue: it is rather an issue of the key-
word, the context and the syntactic structure of the 
sentence taken together. 
2.5 Annotation process 
Our BioScope corpus was annotated by two inde-
pendent linguists following the guidelines written 
by our linguist expert before the annotation of the 
corpus was initiated. These guidelines were devel-
oped throughout the annotation process as annota-
tors were often confronted with problematic issues. 
The annotators were not allowed to communicate 
with each other as far as the annotation process 
was concerned, but they could turn to the expert 
when needed and regular meetings were also held 
between the annotators and the linguist expert in 
order to discuss recurring and/or frequent problem-
atic issues. When the two annotations for one sub-
corpus were finalised, differences between the two 
were resolved by the linguist expert, yielding the 
gold standard labeling of the subcorpus. 
3 Corpus details 
In this section we will discuss in detail the overall 
characteristics of the corpus we developed, includ-
ing a brief description of the texts that constitute 
the BioScope corpus and some general statistics 
concerning the size of each part, distribution of 
negation/hedge cues, ambiguity levels and finally 
we will present statistics on the final results of the 
annotation work. 
3.1 Corpus texts 
The corpus consists of texts taken from 4 different 
sources and 3 different types in order to ensure that 
it captures the heterogenity of language use in the 
biomedical domain. We decided to add clinical 
free-texts (radiology reports), biological full papers 
and biological paper abstracts (texts from Genia). 
Table 1 summarises the chief characteristics of 
the three subcorpora. The 3rd and 5th rows of the 
table show the ratio of sentences which contain 
negated or uncertain statements. The 4rd and 6th 
rows show the number of negation and hedge cue 
occurrences in the given corpus.  
A major part of the corpus consists of clinical 
free-texts. We chose to add medical texts to the 
corpus in order to facilitate research on nega-
tion/hedge detection in the clinical domain. The 
radiology report corpus that was used for the clini-
cal coding challenge (Pestian et al, 2007) organ-
ised by the Computational Medicine Center in 
Cincinatti, Ohio in 2007 was annotated for nega-
tions and uncertainty along with the scopes of each 
phenomenon. This part contains 1954 documents, 
each having a clinical history and an impression 
part, the latter being denser in negated and specula-
tive parts. 
Another part of the corpus consists of full sci-
entific articles. 5 articles from FlyBase (the same 
data were used by Medlock and Briscoe (2007) for 
evaluating sentence-level hedge classifiers) and 4 
articles from the open access BMC Bioinformatics 
website were downloaded and annotated for nega-
tions, uncertainty and their scopes. Full papers are 
particularly useful for evaluating negation/hedge 
classifiers as different parts of an article display 
different properties in the use of speculative or ne-
gated phrases. Take, for instance, the Conclusions 
section of scientific papers that tends to contain 
significantly more uncertain or negative findings 
than the description of Experimental settings and 
methods. 
Scientific abstracts are the main targets for 
various Text Mining applications like protein-
protein interaction mining due to their public ac-
cessibility (e.g. through PubMed). We therefore 
decided to include quite a lot of texts from the ab-
stracts of scientific papers. This is why we in-
cluded the abstracts of the Genia corpus (Collier et 
al., 1999). This decision was straightforward for 
two reasons. First, the Genia corpus contains syn-
tax tree annotation, which allows a comparison 
between scope annotation and syntactic structure. 
Being syntactic in nature, scopes should align with 
the bracket structure of syntax trees, while scope 
resolution algorithms that exploit treebank data can 
be used as a theoretical upper bound for the 
evaluation of parsers for resolving negative/hedge 
scopes. The other reason was that scope annotation 
can mutually benefit from the rich annotations of 
the Genia corpus, such as term annotation (evalua-
tion) and event annotation (comparison with the 
biologist uncertainty labeling of events). 
The corpus consists of more than 20.000 anno-
tated sentences altogether. We consider this size to 
be sufficiently large to serve as a standard evalua-
tion corpus for negation/hedge detection in the 
biomedical domain. 
 
43
 Clinical Full Paper Abstract 
#Documents 1954 9 1273 
#Sentences 6383 2624 11872 
Negation  
sentences 6.6% 13.76% 13.45% 
#Negation cues 871 404 1757 
Hedge sentences 13.4% 22.29% 17.69% 
#Hedge cues 1137 783 2691 
Table 1: Statistics of the three subcorpora. 
3.2 Agreement analysis 
We measured the consistency level of the annota-
tion using inter-annotator agreement analysis. The 
inter-annotator agreement rate is defined as the 
F?=1 measure of one annotation, treating the second 
one as the gold standard. We calculated agreement 
rates for all three subcorpora between the two in-
dependent annotators and between the two annota-
tors and the gold standard labeling. The gold 
standard labeling was prepared by the creator of 
the annotation guide, who resolved all cases where 
the two annotators disagreed on a keyword or its 
scope annotation. 
We measured the agreement rate of annotating 
negative and hedge keywords, and the agreement 
rate of annotating the linguistic scope for each 
phenomenon. We distinguished left-scope, right-
scope and full scope agreement that required both 
left and right scope boundaries to match exactly to 
be considered as coinciding annotations. A detailed 
analysis of the consistency levels for the three sub-
corpora and the ambiguity levels for each negative 
and hedge keyword (that is, the ratio of a keyword 
being annotated as a negative/speculative cue and 
the number of all the occurrences of the same 
keyword in the corpus) can be found at the corpus 
homepage. 
 
3.3 BioScope corpus availability 
The corpus is available free of charge for research 
purposes and can be obtained for a modest price 
for business use. For more details, see the Bio-
Scope homepage: 
www.inf.u-szeged.hu/rgai/bioscope. 
4 Conclusions 
In this paper we reported on the construction of a 
corpus annotated for negations, speculations and 
their linguistic scopes. The corpus is accessible for 
academic purposes and is free of charge. Apart 
from the intended goal of serving as a common 
resource for the training, testing and comparison of 
biomedical Natural Language Processing systems, 
the corpus is also a good resource for the linguistic 
analysis of scientific and clinical texts. 
The most obvious conclusions here are that the 
usual language of clinical documents makes it 
much easier to detect negation and uncertainty 
cues than in scientific texts because of the very 
high ratio of the actual cue words (i.e. low ambigu-
ity level), which explains the high accuracy scores 
reported in the literature. In scientific texts ? which 
are nowadays becoming a popular target for Text 
Mining (for literature-based knowledge discovery) 
? the detection and scope resolution of negation 
and uncertainty is, on the other hand, a problem of 
great complexity, with the percentage of non-
hedge occurrences being as high as 90% for some 
hedge cue candidates in biological paper abstracts. 
Take for example the keyword or which is labeled 
as a speculative keyword in only 11.32% of the 
cases in scientific abstracts, while it was labeled as 
speculative in 97.86% of the cases in clinical texts. 
Identifying the scope is also more difficult in sci-
entific texts where the average sentence length is 
much longer than in clinical data, and the style of 
the texts is also more literary in the former case. 
In our study we found that hedge detection is a 
more difficult problem than identifying negations 
because the number of possible cue words is higher 
and the ratio of real cues is significantly lower in 
the case of speculation (higher keyword/non-
keyword ambiguity). The annotator-agreement ta-
ble also confirms this opinion: the detection of 
hedging is more complicated than negation even 
for humans. 
Our corpus statistics also prove the importance 
of negation and hedge detection. The ratio of ne-
gated and hedge sentences in the corpus varies in 
the subcorpora, but we can say that over 20% of 
the sentences contains a modifier that radically 
influences the semantic content of the sentence. 
One of the chief construction principles of the 
BioScope corpus was to facilitate the train-
ing/development of automatic negation and hedge 
detection systems. Such systems have to solve two 
sub-problems: they have to identify real cue words 
(note that the probability of any word being a key-
word can be different for various domains) and 
44
then they have to determine the linguistic scope of 
actual keywords. 
These automatic hedge and negation detection 
methods can be utilised in a variety of ways in a 
(biomedical) Text Mining system. They can be 
used as a preprocessing tool, i.e. each word in a 
detected scope can be removed from the docu-
ments if we seek to extract true assertions. This can 
significantly reduce the level of noise for process-
ing in such cases where only a document-level la-
beling is provided (like that for the ICD-9 coding 
dataset) and just clear textual evidence for certain 
things should be extracted. On the other hand, 
similar systems can classify previously extracted 
statements according to their certainty or uncer-
tainty, which is generally an important issue in the 
automatic processing of scientific texts. 
Acknowledgments 
This work was supported in part by the NKTH 
grant of the Jedlik ?nyos R&D Programme 2007 
(project codename TUDORKA7) of the Hungarian 
government. The authors wish to thank the anony-
mous reviewers for their useful suggestions and 
comments. The authors also wish to thank the crea-
tors of the ICD-9 coding dataset and the Genia 
corpus for making the texts that were used here 
publicly available. The authors thank Jin-Dong 
Kim as well for the useful comments and sugges-
tions on the annotation guide and Orsolya Vincze 
and Mih?ly Mink? (the two annotators) for their 
work. 
References  
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34(5):301?310. 
N. Collier, H. S. Park, N. Ogata, Y. Tateishi, C. Nobata, 
T. Ohta, T. Sekimizu, H. Imai, K. Ibushi, and J. Tsu-
jii. 1999. The GENIA project: corpus-based knowl-
edge acquisition and information extraction from 
genome research papers. Proceedings of EACL-99. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey 
S. Husser, William Carruth, Larry R. Bergstrom and 
Dietlind L. Wahner-Roedler. 2005. A controlled trial 
of automated classification of negation from clinical 
notes. BMC Medical Informatics and Decision Mak-
ing 5:13 doi:10.1186/1472-6947-5-13. 
C. Friedman, P.O. Alderson, J.H. Austin, J.J. Cimino, 
and S.B. Johnson. 1994. A general natural-language 
text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 
1(2):161?174. 
Yang Huang and Henry J. Lowe. 2007. A Novel Hybrid 
Approach to Automated Negation Detection in Clini-
cal Radiology Reports. Journal of the American 
Medical Informatics Association, 14(3):304?311. 
Ken Hyland. 1994. Hedging in academic writing and 
EAP textbooks. English for Specific Purposes, 
13(3):239?256. 
Jin-Dong Kim, Tomoko Ohta, and Jun'ichi Tsujii. 2008. 
Corpus annotation for mining biomedical events 
from literature. BMC Bioinformatics 2008, 9:10. 
Marc Light, Xin Ting Qui, and Padmini Srinivasan. 
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings of 
BioLink 2004 Workshop on Linking Biological Lit-
erature, Ontologies and Databases: Tools for Users. 
Boston, Massachusetts, Association for Computa-
tional Linguistics, 17?24. 
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific 
literature. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics. Pra-
gue, Association for Computational Linguistics, 992?
999. 
Pradeep G. Mutalik, Aniruddha Deshpande, and 
Prakash M. Nadkarni. 2001. Use of General-purpose 
Negation Detection to Augment Concept Indexing of 
Medical Documents: A Quantitative Study Using the 
UMLS. Journal of the American Medical Informatics 
Association, 8(6):598?609. 
John P. Pestian, Chris Brew, Pawel Matykiewicz, DJ 
Hovermale, Neil Johnson, K. Bretonnel Cohen, and 
Wlodzislaw Duch. 2007. A shared task involving 
multi-label classification of clinical free text. In Bio-
logical, translational, and clinical language process-
ing. Prague, Association for Computational 
Linguistics, 97?104. 
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen, and Tapio 
Salakoski. 2007. BioInfer: a corpus for information 
extraction in the biomedical domain. BMC Bioinfor-
matics 2007, 8:50 doi:10.1186/1471-2105-8-50. 
45
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1926?1932,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning to rank lexical substitutions
Gyo?rgy Szarvas1
Amazon Inc.
szarvasg@amazon.com
Ro?bert Busa-Fekete2 Eyke Hu?llermeier
University of Marburg
Hans-Meerwein-Str., 35032 Marburg, Germany
busarobi@mathematik.uni-marburg.de
eyke@mathematik.uni-marburg.de
Abstract
The problem to replace a word with a syn-
onym that fits well in its sentential context
is known as the lexical substitution task. In
this paper, we tackle this task as a supervised
ranking problem. Given a dataset of target
words, their sentential contexts and the poten-
tial substitutions for the target words, the goal
is to train a model that accurately ranks the
candidate substitutions based on their contex-
tual fitness. As a key contribution, we cus-
tomize and evaluate several learning-to-rank
models to the lexical substitution task, includ-
ing classification-based and regression-based
approaches. On two datasets widely used for
lexical substitution, our best models signifi-
cantly advance the state-of-the-art.
1 Introduction
The task to generate lexical substitutions in context
(McCarthy and Navigli, 2007), i.e., to replace words
in a sentence without changing its meaning, has be-
come an increasingly popular research topic. This
task is used, e.g. to evaluate semantic models with
regard to their accuracy in modeling word meaning
in context (Erk and Pado?, 2010). Moreover, it pro-
vides a basis of NLP applications in many fields,
including linguistic steganography (Topkara et al,
2006; Chang and Clark, 2010), semantic text simi-
larity (Agirre et al, 2012) and plagiarism detection
(Gipp et al, 2011). While closely related to WSD,
1Work was done while working at RGAI of the Hungarian
Acad. Sci. and University of Szeged.
2 R. Busa-Fekete is on leave from the Research Group on
Artificial Intelligence of the Hungarian Academy of Sciences
and University of Szeged.
lexical substitution does not rely on explicitly de-
fined sense inventories (Dagan et al, 2006): the pos-
sible substitutions reflect all conceivable senses of
the word, and the correct sense has to be ascertained
to provide an accurate substitution.
While a few lexical sample datasets (McCarthy
and Navigli, 2007; Biemann, 2012) with human-
provided substitutions exist and can be used to
evaluate different lexical paraphrasing approaches,
a practically useful system must also be able to
rephrase unseen words, i.e., any word for which
a list of synonyms is provided. Correspondingly,
unsupervised and knowledge-based approaches that
are not directly dependent on any training material,
prevailed in the SemEval 2007 shared task on En-
glish Lexical Substitution and dominated follow-up
work. The only supervised approach is limited to
the combination of several knowledge-based lexi-
cal substitution models based on different underly-
ing lexicons (Sinha and Mihalcea, 2009).3
A recent work by Szarvas et al (2013) de-
scribes a tailor-made supervised system based on
delexicalized features that ? unlike earlier super-
vised approaches, and similar to unsupervised and
knowledge-based methods proposed for this task ?
is able to generalize to an open vocabulary. For
each target word to paraphrase, they first compute
a set of substitution candidates using WordNet: all
synonyms from all of the target word?s WordNet
synsets, together with the words from synsets in
similar to, entailment and also see relation to these
synsets are considered as potential substitutions.
Each candidate then constitutes a training (or test)
3Another notable example for supervised lexical substitu-
tion is Biemann (2012), but this is a lexical sample system ap-
plicable only to the target words of the training datasets.
1926
example, and these instances are characterized using
non-lexical features from heterogeneous evidence
such as lexical-semantic resources and distributional
similarity, n-gram counts and shallow syntactic fea-
tures computed on large, unannotated background
corpora. The goal is then i) to predict how well
a particular candidate fits in the original context,
and ii) given these predictions for each of the can-
didates, to correctly order the elements of the candi-
date set according to their contextual fitness. That is,
a model is successful if it prioritizes plausible substi-
tutions ahead of less likely synonyms (given the con-
text). This model is able to generate paraphrases for
target words not contained in the training material.
This favorable property is achieved using only such
features (e.g. local n-gram frequencies in context)
that are meaningfully comparable across the differ-
ent target words and candidate substitutions they are
computed from. More importantly, their model also
provides superior ranking results compared to state
of the art unsupervised and knowledge based ap-
proaches and therefore it defines the current state of
the art for open vocabulary lexical substitution.
Motivated by the findings of Szarvas et al (2013),
we address lexical substitution as a supervised learn-
ing problem, and go beyond their approach from
a methodological point of view. Our experiments
show that the performance on the lexical substitution
task is strongly influenced by the way in which this
task is formalized as a machine learning problem
(i.e., as binary or multi-class classification or regres-
sion) and by the learning method used to solve this
problem. As a result, we are able to report the best
performances on this task for two standard datasets.
2 Related work
Previous approaches to lexical substitution often
seek to automatically generate a set of candidate
substitutions for each target word first, and to rank
the elements of this set of candidates afterward (Has-
san et al, 2007; Giuliano et al, 2007; Martinez et
al., 2007; Yuret, 2007). Alternatively, the candidate
set can be defined by all human-suggested substi-
tutions for the given target word in all of its con-
texts; then, the focus is just on the ranking problem
(Erk and Pado?, 2010; Thater et al, 2010; Dinu and
Lapata, 2010; Thater et al, 2011). While only the
former approach qualifies as a full-fledged substitu-
tion system for arbitrary, previously unseen target
words, the latter simplifies the comparison of se-
mantic ranking models, as the ranking step is not
burdened with the shortcomings of automatically
generated substitution candidates.
As mentioned before, Szarvas et al (2013) re-
cently formalized the lexical substitution problem as
a supervised learning task, using delexicalized fea-
tures. This non-lexical feature representation makes
different target word/substitution pairs in different
contexts4 directly comparable. Thus, it becomes
possible to learn an all-words system that is appli-
cable to unseen words, using supervised methods,
which provides superior ranking accuracy to unsu-
pervised and knowledge based models.
In this work, we build on the problem formu-
lation and the features proposed by Szarvas et
al. (2013) while largely extending their machine
learning methodology. We customize and experi-
ment with several different learning-to-rank models,
which are better tailored for this task. As our experi-
ments show, this contribution leads to further signif-
icant improvements in modeling the semantics of a
text and in end-system accuracy.
3 Datasets and experimental setup
Here we introduce the datasets, experimental setup
and evaluation measures used in our experiments.
Since space restrictions prohibit a comprehensive
exposition, we only provide the most essential in-
formation and refer to Szarvas et al (2013), whose
experimental setup we adopted, for further details.
Datasets. We use two prominent datasets for lex-
ical substitution. The LexSub dataset introduced
in the Lexical Substitution task at Semeval 2007
(McCarthy and Navigli, 2007)5 contains 2002 sen-
tences for a total of 201 target words (from all
parts of speech), and lexical substitutions assigned
(to each target word and sentence pair) by 5 na-
tive speaker annotators. The second dataset, TWSI
(Biemann, 2012)6, consists of 24,647 sentences for
a total of 1,012 target nouns, and lexical substitu-
4E.g., bright substituted with intelligent in ?He was bright
and independent and proud? and side for part in ?Find someone
who can compose the biblical side?.
5http://nlp.cs.swarthmore.edu/
semeval/tasks/task10/data.shtml
6http://www.ukp.tu-darmstadt.de/data/
lexical-resources/twsi-lexical-substitutions/
1927
tions for each target word in context resulting from
a crowdsourced annotation process.
For each sentence in each dataset, the annotators
provided as many substitutions for the target word
as they found appropriate in the context. Each sub-
stitution is then labeled by the number of annotators
who listed that word as a good lexical substitution.
Experimental setup and Evaluation. On both
datasets, we conduct experiments using a 10-fold
cross validation process, and evaluate all learning al-
gorithms on the same train/test splits. The datasets
are randomly split into 10 equal-sized folds on the
target word level, such that all examples for a par-
ticular target word fall into either the training or
the test set, but never both. This way, we make
sure to evaluate the models on target words not seen
during training, thereby mimicking an open vocab-
ulary paraphrasing system: at testing time, para-
phrases are ranked for unseen target words, simi-
larly as the models would rank paraphrases for any
words (not necessarily contained in the dataset). For
algorithms with tunable parameters, we further di-
vide the training sets into a training and a validation
part to find the best parameter settings. For evalua-
tion, we use Generalized Average Precision (GAP)
(Kishida, 2005) and Precision at 1 (P@1), i.e., the
percentage of correct paraphrases at rank 1.
Features. In all experiments, we used the features
described in Szarvas et al (2013), implemented pre-
cisely as proposed by the original work.
Each (sentence, targetword, substitution)
triplet represents an instance, and the feature values
are computed from the sentence context, the target
word and the substitution word. The features used
fall into four major categories.
The most important features describe the syntag-
matic coherence of the substitution in context, mea-
sured as local n-gram frequencies obtained from
web data. The frequency for a 1-5gram context with
the substitution word is computed and normalized
with respect to either 1) the frequency of the origi-
nal context (with the target word) or 2) the sum of
frequencies observed for all possible substitutions.
A third feature computes similar frequencies for the
substitution and the target word observed in the lo-
cal context (as part of a conjunctive phrase).
A second group of features describe the (non-
positional, i.e. non-local) distributional similarity of
the target and its candidate substitution in terms
of sentence level co-occurrence statistics collected
from newspaper texts: 1) How many words from the
sentence appear in the top 1000 salient words listed
for the candidate substitution in a distributional the-
saurus, 2) how similar the top K salient words lists
are for the candidate and the target word, 3) how
similar the 2nd order distributional profiles are for
candidate and target, etc. All these features are care-
fully normalized so that values compare well accross
different words and contexts.
Another set of features capture the properties of
the target and candidate word in WordNet, such as
their 1) number of senses, 2) how frequent senses
are synonymous and 3) the lowest common ancestor
(and all synsets up) for the candidate and target word
in the WordNet hierarchy (represented as a nominal
feature, by the ID of these synsets).
Lastly a group of features capture shallow syntac-
tic patterns of the target word and its local context in
the form of 1) part of speech patterns (trigrams) in
a sliding window around the target word using main
POS categories, i.e. only the first letter of the Penn
Treebank codes, and 2) the detailed POS code of the
candidate word assigned by a POS tagger.
We omit a mathematically precise description of
these features for space reasons and refer the reader
to Szarvas et al (2013) for a more formal and
detailed description of the feature functions. Im-
portantly, these delexicalized features are numeri-
cally comparable across the different target words
and candidate substitutions they are computed from.
This property enables the models to generalize over
the words in the datasets and thus enables a super-
vised, all-words lexical substitution system.
4 Learning-to-Rank methods
Machine learning methods for ranking are tradition-
ally classified into three categories. In the point-
wise approach, a model is trained that maps in-
stances (in this case candidate substitutions in a con-
text) to scores indicating their relevance or fitness;
to this end, one typically applies standard regression
techniques, which essentially look at individual in-
stances in isolation (i.e., independent of any other
instances in the training or test set). To predict a
ranking of a set of query instances, these are sim-
ply sorted by their predicted scores (Li et al, 2007).
1928
The pairwise approach trains models that are able
to compare pairs of instances. By marking such a
pair as positive if the first instance is preferred to the
second one, and as negative otherwise, the problem
can formally be reduced to a binary classification
task (Freund et al, 2003). Finally, in the listwise ap-
proach, tailor-made learning methods are used that
directly optimize the ranking performance with re-
spect to a global evaluation metric, i.e., a measure
that evaluates the ranking of a complete set of query
instances (Valizadegan et al, 2009).
Below we give a brief overview of the methods in-
cluded in our experiments. We used the implementa-
tions provided by the MultiBoost (Benbouzid et al,
2012), RankSVM and RankLib packages.7 For a de-
tailed description, we refer to the original literature.
4.1 MAXENT
The ranking model proposed by Szarvas et al (2013)
was used as a baseline. This is a pointwise approach
based on a maximum entropy classifier, in which
the ranking task is cast as a binary classification
problem, namely to discriminate good (label > 0)
from bad substitutions. The actual label values for
good substitutions were used for weighting the train-
ing examples. The underlying MaxEnt model was
trained until convergence, i.e., there was no hyper-
parameter to be tuned. For a new target/substitution
pair, the classifier delivers an estimation of the pos-
terior probability for being a good substitution. The
ranking is then produced by sorting the candidates
in decreasing order according to this probability.
4.2 EXPENS
EXPENS (Busa-Fekete et al, 2013) is a point-
wise method with listwise meta-learning step
that exploits an ensemble of multi-class classi-
fiers. It consists of three steps. First, AD-
ABOOST.MH (Schapire and Singer, 1999) classi-
fiers with several different weak learners (Busa-
Fekete et al, 2011; Ke?gl and Busa-Fekete, 2009)
are trained to predict the level of relevance (qual-
ity) of a substitution (i.e., the number of annotators
who proposed the candidate for that particular con-
text). Second, the classifiers are calibrated to obtain
7RankLib is available at http://people.cs.umass.
edu/?vdang/ranklib.html. We extended the imple-
mentation of the LAMBDAMART algorithm in this package to
compute the gradients of and optimize for the GAP measure.
an accurate posterior distribution; to this end, several
calibration techniques, such as Platt scaling (Platt,
2000), are used to obtain a diverse pool of calibrated
classifiers. Note that this step takes advantage of
the ordinal structure of the underlying scale of rel-
evance levels, which is an important difference to
MAXENT. Third, the posteriors of these calibrated
classifiers are additively combined, with the weight
of each model being exponentially proportional to
its GAP score (on the validation set). This method
has two hyperparameters: the number of boosting it-
erations T and the scaling factor in the exponential
weighting scheme c. We select T and c from the in-
tervals [100, 2000] and [0, 100], with step sizes 100
and 10, respectively.
4.3 RANKBOOST
RANKBOOST (Freund et al, 2003) is a pairwise
boosting approach. The objective function is the
rank loss (as opposed to ADABOOST, which opti-
mizes the exponential loss). In each boosting it-
eration, the weak classifier is chosen by maximiz-
ing the weighted rank loss. For the weak learner,
we used the decision stump described in (Freund et
al., 2003), which is able to optimize the rank loss
in an efficient way. The only hyperparameter of
RANKBOOST to be tuned is the number of iterations
that we selected from the interval [1, 1000].
4.4 RANKSVM
RANKSVM (Joachims, 2006) is a pairwise method
based on support vector machines, which formulates
the ranking task as binary classification of pairs of
instances. We used a linear kernel, because the opti-
mization using non-linear kernels cannot be done in
a reasonable time. The tolerance level of the op-
timization was set to 0.001 and the regularization
parameter was validated in the interval [10?6, 104]
with a logarithmically increasing step size.
4.5 LAMBDAMART
LAMBDAMART (Wu et al, 2010) is a listwise
method based on the gradient boosted regression
trees by Friedman (1999). The ordinal labels are
learned directly by the boosted regression trees
whose parameters are tuned by using a gradient-
based optimization method. The gradient of parame-
ters is calculated based on the evaluation metric used
(in this case GAP). We tuned the number of boosting
1929
Database LexSub TWSI
Candidates WN Gold WN Gold
GAP
MaxEnt 43.8 52.4 36.6 47.2
ExpEns 44.3 53.5 37.8 49.7
RankBoost 44.0 51.4 37.0 47.8
RankSVM 43.3 51.8 35.5 45.2
LambdaMART 45.5 55.0 37.8 50.1
P@1
MaxEnt 40.2 57.7 32.4 49.5
ExpEns 39.8 58.5 33.8 53.2
RankBoost 40.7 55.2 33.1 50.8
RankSVM 40.3 51.7 33.2 45.1
LambdaMART 40.8 60.2 33.1 53.6
Table 1: GAP and p@1 values, with significant improve-
ments over the performance of MaxEnt marked in bold.
System GAP
Erk and Pado? (2010) 38.6
Dinu and Lapata (2010) 42.9
Thater et al (2010) 46.0
Thater et al (2011) 51.7
Szarvas et al (2013) 52.4
EXPENS 53.5
LAMBDAMART 55.0
Table 2: Comparison to previous studies (dataset LexSub,
candidates Gold).
iterations in the interval [10, 1000] and the number
of tree leaves in {8, 16, 32}.
5 Results and discussion
Our results using the above learning methods are
summarized in Table 1. As can be seen, the two
methods that exploit the cardinal structure of the
label set (relevance degrees), namely EXPENS and
LAMBDAMART, consistently outperform the base-
line taken from Szarvas et al (2013) ? the only ex-
ception is the p@1 score for EXPENS on the Semeval
Lexical Substitution dataset and the candidate sub-
stitutions extracted from WordNet. The improve-
ments are significant (using paired t-test, p < 0.01)
for 3 out of 4 settings for EXPENS and in all settings
for LAMBDAMART. In particular, the results of
LAMBDAMART are so far the best scores that have
been reported for the best studied setting, i.e. the
LexSub dataset using substitution candidates taken
from the gold standard (see Table 2).
We suppose that the relatively good results
achieved by the LAMBDAMART and EXPENS
methods are due to that, first, it seems crucial to
properly model and exploit the ordinal nature of
the annotations (number of annotators who sug-
gested a given word as a good paraphrase) pro-
vided by the datasets. Second, the RANKBOOST and
RANKSVM are less complex methods than the EX-
PENS and LAMBDAMART. The RANKSVM is the
least complex method from the pool of learning-to-
rank methods we applied, since it is a simple lin-
ear model. The RANKBOOST is a boosted decision
stump where, in each boosting iteration, the stump
is found by maximizing the weighted exponential
rank loss. On the other hand, both the EXPENS
and LAMBDAMART make use of tree learners in the
ensemble classifier they produce. We believe that
overfitting is not an issue in a learning task like the
LexSub task: most features are relatively weak pre-
dictors on their own, and we can learn from a large
number of data points (2000 sentences with an av-
erage set size of 20, about 40K data points for the
smallest dataset and setting). Rather, as our results
show, less complex models tend to underfit the data.
Therefore we believe that more complex models can
achieve a better performance, of course with an in-
creased computational cost.
6 Conclusion and future work
In this paper, we customized and applied some rela-
tively novel algorithms from the field of learning-to-
rank for ranking lexical substitutions in context. In
turn, we achieved significant improvements on the
two prominent datasets for lexical substitution.
Our results indicate that an exploitation of the or-
dinal structure of the labels in the datasets can lead to
considerable gains in terms of both ranking quality
(GAP) and precision at 1 (p@1). This observation
is supported both for the theoretically simpler point-
wise learning approach and for the most powerful
listwise approach. On the other hand, the pairwise
methods that cannot naturally exploit this property,
did not provide a consistent improvement over the
baseline. In the future, we plan to investigate this
finding in the context of other, similar ranking prob-
lems in Natural Language Processing.
Acknowledgment
This work was supported by the German Research
Foundation (DFG) as part of the Priority Programme
1527.
1930
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada.
D. Benbouzid, R. Busa-Fekete, N. Casagrande, F.-D.
Collin, and B. Ke?gl. 2012. MultiBoost: a multi-
purpose boosting package. Journal of Machine Learn-
ing Research, 13:549?553.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
R. Busa-Fekete, B. Ke?gl, T. E?lteto?, and Gy. Szarvas.
2011. Ranking by calibrated AdaBoost. In (JMLR
W&CP), volume 14, pages 37?48.
R. Busa-Fekete, B. Ke?gl, T. E?lteto?, and Gy. Szarvas.
2013. Tune and mix: learning to rank using ensembles
of calibrated multi-class classifiers. Machine Learn-
ing, 93(2?3):261?292.
Ching-Yun Chang and Stephen Clark. 2010. Practi-
cal linguistic steganography using contextual synonym
substitution and vertex colour coding. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1194?1203, Cam-
bridge, MA.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 449?456, Sydney, Australia.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92?
97, Uppsala, Sweden.
Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 2003.
An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4:933?
969.
J. Friedman. 1999. Greedy function approximation: a
gradient boosting machine. Technical report, Dept. of
Statistics, Stanford University.
Bela Gipp, Norman Meuschke, and Joeran Beel. 2011.
Comparative Evaluation of Text- and Citation-based
Plagiarism Detection Approaches using GuttenPlag.
In Proceedings of 11th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries (JCDL?11), pages 255?258,
Ottawa, Canada. ACM New York, NY, USA. Avail-
able at http://sciplore.org/pub/.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. FBK-irst: Lexical substitution task exploit-
ing domain and syntagmatic coherence. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 145?148, Prague,
Czech Republic.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. UNT: SubFinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 410?413, Prague, Czech Republic.
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining (KDD).
B. Ke?gl and R. Busa-Fekete. 2009. Boosting products of
base classifiers. In International Conference on Ma-
chine Learning, volume 26, pages 497?504, Montreal,
Canada.
Kazuaki Kishida. 2005. Property of Average Precision
and Its Generalization: An Examination of Evaluation
Indicator for Information Retrieval Experiments. NII
technical report. National Institute of Informatics.
P. Li, C. Burges, and Q. Wu. 2007. McRank: Learning to
rank using multiple classification and gradient boost-
ing. In Advances in Neural Information Processing
Systems, volume 19, pages 897?904. The MIT Press.
David Martinez, Su Nam Kim, and Timothy Bald-
win. 2007. MELB-MKB: Lexical substitution system
based on relatives in context. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 237?240, Prague, Czech
Republic.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 48?53,
Prague, Czech Republic.
J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. In A.J. Smola, P. Bartlett, B. Schoelkopf,
and D. Schuurmans, editors, Advances in Large Mar-
gin Classifiers, pages 61?74. MIT Press.
1931
R. E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):297?336.
Ravi Sinha and Rada Mihalcea. 2009. Combining lex-
ical resources for contextual synonym expansion. In
Proceedings of the International Conference RANLP-
2009, pages 404?410, Borovets, Bulgaria.
Gyo?rgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013. Supervised all-words lexical substitution us-
ing delexicalized features. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT 2013), June.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948?957, Uppsala,
Sweden.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of the Fifth Interna-
tional Joint Conference on Natural Language Process-
ing : IJCNLP 2011, pages 1134?1143, Chiang Mai,
Thailand. MP, ISSN 978-974-466-564-5.
Umut Topkara, Mercan Topkara, and Mikhail J. Atal-
lah. 2006. The hiding virtues of ambiguity: quan-
tifiably resilient watermarking of natural language text
through synonym substitutions. In Proceedings of the
8th workshop on Multimedia and security, pages 164?
174, New York, NY, USA. ACM.
H. Valizadegan, R. Jin, R. Zhang, and J. Mao. 2009.
Learning to rank by optimizing NDCG measure. In
Advances in Neural Information Processing Systems
22, pages 1883?1891.
Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. 2010.
Adapting boosting for information retrieval measures.
Inf. Retr., 13(3):254?270.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 207?214, Prague, Czech Republic, June.
Association for Computational Linguistics.
1932
Cross-Genre and Cross-Domain Detection of
Semantic Uncertainty
Gyo?rgy Szarvas?
Technische Universita?t Darmstadt
Veronika Vincze??
Hungarian Academy of Sciences
Richa?rd Farkas?
Universita?t Stuttgart
Gyo?rgy Mo?ra?
University of Szeged
Iryna Gurevych?
Technische Universita?t Darmstadt
Uncertainty is an important linguistic phenomenon that is relevant in various Natural
Language Processing applications, in diverse genres from medical to community generated,
newswire or scientific discourse, and domains from science to humanities. The semantic un-
certainty of a proposition can be identified in most cases by using a finite dictionary (i.e., lexical
cues) and the key steps of uncertainty detection in an application include the steps of locating
the (genre- and domain-specific) lexical cues, disambiguating them, and linking them with the
units of interest for the particular application (e.g., identified events in information extraction).
In this study, we focus on the genre and domain differences of the context-dependent semantic
uncertainty cue recognition task.
We introduce a unified subcategorization of semantic uncertainty as different domain appli-
cations can apply different uncertainty categories. Based on this categorization, we normalized
the annotation of three corpora and present results with a state-of-the-art uncertainty cue
recognition model for four fine-grained categories of semantic uncertainty.
? Technische Universita?t Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt - FB 20
Hochschulstrasse 10, D-64289 Darmstadt, Germany. E-mail: {szarvas,gurevych}@tk.informatik
.tu-darmstadt.de.
?? Hungarian Academy of Sciences, Research Group on Artificial Intelligence, Tisza Lajos krt. 103,
6720 Szeged, Hungary. E-mail: vinczev@inf.u-szeged.hu.
? Universita?t Stuttgart, Institut fu?r Maschinelle Sprachverarbeitung. Azenbergstrasse 12, D-70174 Stuttgart,
Germany. E-mail: farkas@ims.uni-stuttgart.de.
? University of Szeged, Department of Informatics, A?rpa?d te?r 2, 6720 Szeged, Hungary.
E-mail: gymora@inf.u-szeged.hu.
Submission received: 6 April 2011; revised submission received: 1 October 2011; accepted for publication:
30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
Our results reveal the domain and genre dependence of the problem; nevertheless, we
also show that even a distant source domain data set can contribute to the recognition
and disambiguation of uncertainty cues, efficiently reducing the annotation costs needed to
cover a new domain. Thus, the unified subcategorization and domain adaptation for training
the models offer an efficient solution for cross-domain and cross-genre semantic uncertainty
recognition.
1. Introduction
In computational linguistics, especially in information extraction and retrieval, it is
of the utmost importance to distinguish between uncertain statements and factual
information. In most cases, what the user needs is factual information, hence uncertain
propositions should be treated in a special way: Depending on the exact task, the
system should either ignore such texts or separate them from factual information. In
machine translation, it is also necessary to identify linguistic cues of uncertainty because
the source and the target language may differ in their toolkit to express uncertainty
(one language uses an auxiliary, the other uses just a morpheme). To cite another
example, in clinical document classification, medical reports can be grouped according
to whether the patient definitely suffers, probably suffers, or does not suffer from an
illness.
There are several linguistic phenomena that are referred to as uncertainty in the
literature. We consider propositions to which no truth value can be attributed, given
the speaker?s mental state, as instances of semantic uncertainty. In contrast, uncertainty
may also arise at the discourse level, when the speaker intentionally omits some infor-
mation from the statement, making it vague, ambiguous, or misleading. Determining
whether a given proposition is uncertain or not may involve using a finite dictionary of
linguistic devices (i.e., cues). Lexical cues (such as modal verbs or adverbs) are respon-
sible for semantic uncertainty whereas discourse-level uncertainty may be expressed by
lexical cues and syntactic cues (such as passive constructions) as well. We focus on four
types of semantic uncertainty in this study and henceforth the term cuewill be taken to
mean lexical cue.
The key steps of recognizing semantically uncertain propositions in a natural
language processing (NLP) application include the steps of locating lexical cues for
uncertainty, disambiguating them (as not all occurrences of the cues indicate uncer-
tainty), and finally linking them with the textual representation of the propositions
in question. The linking of a cue to the textual representation of the proposition can
be performed on the basis of syntactic rules that depend on the word class of the
lexical cue, but they are independent of the actual application domain or text type
where the cue is observed. The set of cues used and the frequency of their certain
and uncertain usages are domain and genre dependent, however, and this has to be
addressed if we seek to craft automatic uncertainty detectors. Here we interpret genre
as the basic style and formal characteristics of the writing that is independent of its topic
(e.g., scientific papers, newswire texts, or business letters), and domain as a particular
field of knowledge and is related to the topic of the text (e.g., medicine, archeology, or
politics).
Uncertainty cue candidates do not display uncertainty in all of their occurrences.
For instance, the mathematical sense of probable is dominant in mathematical texts
whereas its ratio can be relatively low in papers in the humanities. The frequency of
the two distinct meanings of the verb evaluate (which can be a synonym of judge [an
336
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
uncertain meaning] and calculate) is also different in the bioinformatics and cell biology
domains. Compare:
(1) To evaluateCUE the PML/RARalpha role in myelopoiesis, transgenic mice
expressing PML/RARalpha were engineered.
(2) Our method was evaluated on the Lindahl benchmark for fold recognition.
In this article we focus on the domain-dependent aspects of uncertainty detection
and we examine the recognition of uncertainty cues in context. We do not address the
problem of linking cues to propositions in detail (see, e.g., Chapman, Chu, and Dowling
[2007] and Kilicoglu and Bergler [2009] for the information extraction case).
For the empirical investigation of the domain dependent aspects, data sets are
required from various domains. To date, several corpora annotated for uncertainty have
been constructed for different genres and domains (BioScope, FactBank, WikiWeasel,
and MPQA, to name but a few). These corpora cover different aspects of uncertainty,
however, being grounded on different linguistic models, which makes it hard to exploit
cross-domain knowledge in applications. These differences in part stem from the varied
application needs across application domains. Different types of uncertainty and classes
of linguistic expressions are relevant for different domains. Although hypotheses and
investigations form a crucial part of the relevant cases in scientific applications, they
are less prominent in newswire texts, where beliefs and rumors play a major role. This
finding motivates a more fine-grained treatment of uncertainty. In order to bridge the
existing gaps between application goals, these typical cases need to be differentiated. A
fine-grained categorization enables the individual treatment of each subclass, which
is less dependent on domain differences than using one coarse-grained uncertainty
class. Moreover, this approach enables each particular application to identify and select
from a pool of models only those aspects of uncertainty that are relevant in the specific
domain.
As one of the main contributions of this study, we propose a uniform subcategoriza-
tion of semantic uncertainty in which all the previous corpus annotation works can be
placed, and which reveals the fundamental differences between the currently existing
resources. In addition, we manually harmonized the annotations of three corpora and
performed the fine-grained labeling according to the suggested subcategorization so as
to be able to perform cross-domain experiments.
An important factor in training robust cross-domain models is to focus on shallow
features that can be reliably obtained for many different domains and text types, and
to craft models that exploit the shared knowledge from different sources as much
as possible, making the adaptation to new domains efficient. The study of learning
efficient models across different domains is the subject of transfer learning and domain
adaptation research (cf. Daume? III and Marcu 2006; Pan and Yang 2010). The domain
adaptation setting assumes a target domain (for which an accurate model should be
learned with a limited amount of labeled training data), a source domain (with charac-
teristics different from the target and for which a substantial amount of labeled data is
available), and an arbitrary supervised learning model that exploits both the target and
source domain data in order to learn an improved target domain model.
The success of domain adaptation mainly depends on two factors: (i) the similarity
of the target and source domains (the two domains should be sufficiently similar to
allow knowledge transfer); and (ii) the application of an efficient domain adaptation
337
Computational Linguistics Volume 38, Number 2
method (which permits the learning algorithm to exploit the commonalities of the
domains while preserving the special characteristics of the target domain).
As our second main contribution, we study the impact of domain differences on
uncertainty detection, how this impact depends on the distance between target and
source domains concerning their domains and genres, and how these differences can
be reduced to produce accurate target domain models with limited annotation effort.
Because previously existing resources exhibited fundamental differences that made
domain adaptation difficult,1 to our knowledge this is the first study to analyze domain
differences and adaptability in the context of uncertainty detection in depth, and also
the first study to report consistently positive results in cross-training.
The main contributions of the current paper can be summarized as follows:
 We provide a uniform subcategorization of semantic uncertainty (with
definitions, examples, and test batteries for annotation) and classify all
major previous studies on uncertainty corpus annotation into the proposed
categorization system, in order to reveal and analyze the differences.
 We provide a harmonized, fine-grained reannotation of three corpora,
according to the suggested subcategorization, to allow an in-depth
analysis of the domain dependent aspects of uncertainty detection.
 We compare the two state-of-the-art approaches to uncertainty cue
detection (i.e., the one based on token classification and the one on
sequence labeling models), using a shared feature set, in the context of the
CoNLL-2010 shared task, to understand their strengths and weaknesses.2
 We train an accurate semantic uncertainty detector that distinguishes four
fine-grained categories of semantic uncertainty (epistemic, doxastic,
investigation, and condition types) and thus is better for future
applications in various domains than previous models. Our experiments
reveal that, similar to the best model of the CoNLL-2010 shared task for
biological texts but in a fine-grained context, shallow features provide
good results in recognizing semantic uncertainty. We also show that this
representation is less suited to detecting discourse-level uncertainty
(which was part of the CoNLL task for Wikipedia texts).
 We examine in detail the differences between domains and genres as
regards the language used to express semantic uncertainty, and learn how
the domain or genre distance affects uncertainty recognition in texts with
unseen characteristics.
 We apply domain adaptation techniques to fully exploit out-of-domain
data and minimize annotation costs to adapt to a new domain, and we
report successful results for various text domains and genres.
The rest of the paper is structured as follows. In Section 2, our classification of
uncertainty phenomena is presented in detail and it is compared with the concept of
1 Only 3 out of the more than 20 participants of the related CoNLL-2010 shared task (Farkas et al 2010)
managed to exploit out-of-domain data to improve their results, and only by a negligible margin.
2 The most successful CoNLL systems were based on these approaches, but different feature
representations make direct comparisons difficult.
338
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
uncertainty used in existing corpora. A framework for detecting semantic uncertainty
is then presented in Section 3. Relatedwork on cue detection is summarized in Section 4,
which is followed by a description of our cue recognition system and a presentation of
our experimental set-up using various source and target genre and domain pairs for
cross-domain learning and domain adaptation in Section 5. Our results are elaborated
on in Section 6 with a focus on the effect of domain similarities and on the annotation
effort needed to cover a new domain. We then conclude with a summary of our results
and make some suggestions for future research.
2. The Phenomenon Uncertainty
In order to be able to introduce and discuss our data sets, experiments, and findings,
we have to clarify our understanding of the term uncertainty. Uncertainty?in its most
general sense?can be interpreted as lack of information: The receiver of the information
(i.e., the hearer or the reader) cannot be certain about some pieces of information. In this
respect, uncertainty differs from both factuality and negation; as regards the former,
the hearer/reader is sure that the information is true and as for the latter, he is sure
that the information is not true. From the viewpoint of computer science, uncertainty
emerges due to partial observability, nondeterminism, or both (Russell and Norvig
2010). Linguistic theories usually associate the notion of modality with uncertainty:
Epistemic modality encodes how much certainty or evidence a speaker has for the
proposition expressed by his utterance (Palmer 1986) or it refers to a possible state of
the world in which the given proposition holds (Kiefer 2005). The common point in
these approaches is that in the case of uncertainty, the truth value/reliability of the
proposition cannot be decided because some other piece of information is missing.
Thus, uncertain propositions are those in our understanding whose truth value or
reliability cannot be determined due to lack of information.
In the following, we focus on semantic uncertainty and we suggest a tentative
classification of several types of semantic uncertainty. Our classification is grounded on
the knowledge of existing corpora and uncertainty recognition tools and our chief goal
here is to provide a computational linguistics-oriented classification. With this in mind,
our subclasses are intended to be well-defined and easily identifiable by automatic
tools. Moreover, this classification allows different applications to choose the subset of
phenomena to be recognized in accordance with their main task (i.e., we tried to avoid
an overly coarse or fine-grained categorization).
2.1 Classification of Uncertainty Types
Several corpora annotated for uncertainty have been published in different domains
such as biology (Medlock and Briscoe 2007; Kim, Ohta, and Tsujii 2008; Settles, Craven,
and Friedland 2008; Shatkay et al 2008; Vincze et al 2008; Nawaz, Thompson, and
Ananiadou 2010), medicine (Uzuner, Zhang, and Sibanda 2009), news media (Rubin,
Liddy, and Kando 2005; Wilson 2008; Saur?? and Pustejovsky 2009; Rubin 2010), and
encyclopedia (Farkas et al 2010). As can be seen from publicly available annotation
guidelines, there are many overlaps but differences as well in the understanding of
uncertainty, which is sometimes connected to domain- and genre-specific features of
the texts. Here we introduce a domain- and genre-independent classification of several
types of semantic uncertainty, which was inspired by both theoretical and computa-
tional linguistic considerations.
339
Computational Linguistics Volume 38, Number 2
Figure 1
Types of uncertainty. FB = FactBank; Genia = Genia Event; Rubin = the data set described
in Rubin, Liddy and Noriko (2005); META = the data set described in Nawaz, Thompson
and Ananiadou (2010); Medlock = the data set described in Medlock and Briscoe (2007);
Shatkay = the data set described in Shatkay et al (2008); Settles = the data set described in
Settles et al (2008).
2.1.1 A Tentative Classification. Based on corpus data and annotation principles, the
expression uncertainty can be used as an umbrella term for covering phenomena at the
semantic and discourse levels.3 Our classification of semantic uncertainty is assumed
to be language-independent, but our examples presented here come from the English
language, to keep matters simple.
Semantically uncertain propositions can be defined in terms of truth conditional
semantics. They cannot be assigned a truth value (i.e., it cannot be stated for sure
whether they are true or false) given the speaker?s current mental state.
Semantic level uncertainty can be subcategorized into epistemic and hypothetical
(see Figure 1). The main difference between epistemic and hypothetical uncertainty is
that whereas instances of hypothetical uncertainty can be true, false or uncertain, epis-
temically uncertain propositions are definitely uncertain?in terms of possible worlds,
hypothetical propositions allow that the proposition can be false in the actual world,
but in the case of epistemic uncertainty the factuality of the proposition is not known.
In the case of epistemic uncertainty, it is known that the proposition is neither true
nor false: It describes a possible world where the proposition holds but this possible
world does not coincide with the speaker?s actual world. In other words, it is certain
that the proposition is uncertain. Epistemic uncertainty is related to epistemic modality:
a sentence is epistemically uncertain if on the basis of our world knowledge we cannot
decide at the moment whether it is true or false (hence the name) (Kiefer 2005). The
source of an epistemically uncertain proposition cannot claim the uncertain proposition
and be sure about its opposite at the same time.
(3) EPISTEMIC: Itmay be raining.
3 The entire typology of semantic uncertainty phenomena and a test battery for their classification are
described in a supplementary file. Together with the corpora and the experimental software, they are
available at http://www.inf.u-szeged.hu/rgai/uncertainty.
340
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
As for hypothetical uncertainty, the truth value of the propositions cannot be
determined either and nothing can be said about the probability of their happening.
Propositions under investigation are an example of such statements: Until further anal-
ysis, the truth value of the proposition under question cannot be stated. Conditionals
can also be classified as instances of hypotheses. It is also common in these two types of
uncertain propositions that the speaker can utter them while it is certain (for others or
even for him) that its opposite holds hence they can be called instances of paradoxical
uncertainty.
Hypothetical uncertainty is connectedwith non-epistemic types ofmodality aswell.
Doxastic modality expresses the speaker?s beliefs?which may be known to be true or
false by others in the current state of the world. Necessity (duties, obligation, orders)
is the main objective of deontic modality; dispositional modality is determined by the
dispositions (i.e., physical abilities) of the person involved; and circumstantial modality
is defined by external circumstances. Buletic modality is related to wishes, intentions,
plans, and desires. An umbrella term for deontic, dispositional, circumstantial, and
buletic modality is dynamic modality (Kiefer 2005).
HYPOTHETICAL:
(4) DYNAMIC: I have to go.
(5) DOXASTIC: He believes that the Earth is flat.
(6) INVESTIGATION: We examined the role of NF-kappa B in protein activation.
(7) CONDITION: If it rains, we?ll stay in.
Conditions and instances of dynamic modality are related to the future: In the
future, they may happen but at the moment it is not clear whether they will take place
or not / whether they are true, false, or uncertain.
2.1.2 Comparison with other Classifications. The feasibility of the classification proposed in
this study can be justified by mapping the annotation schemes used in other existing
corpora to our subcategorizations of uncertainty. This systematic comparison also high-
lights the major differences between existing works and partly explains why examples
for successful cross-domain application of existing resources and models are hard to
find in the literature.
Most of the annotations found in biomedical corpora (Medlock and Briscoe 2007;
Settles, Craven, and Friedland 2008; Shatkay et al 2008; Thompson et al 2008; Nawaz,
Thompson, and Ananiadou 2010) fall into the epistemic uncertainty class. BioScope
(Vincze et al 2008) annotations mostly belong to the epistemic uncertainty category,
with the exception of clausal hypotheses (i.e., hypotheses that are expressed by a clause
headed by if or whether), which are instances of the investigation class. The probable
class of Genia Event (Kim, Ohta, and Tsujii 2008) is of the epistemically uncertain type
and the doubtful class belongs to the investigation class. Rubin, Liddy, and Kando (2005)
consider uncertainty as a phenomenon belonging to epistemicmodality: The high, mod-
erate, and low levels of certainty coincide with our epistemic uncertainty category. The
speculation annotations of the MPQA corpus also belong to the epistemic uncertainty
class, with four levels (Wilson 2008). The probable and possible classes found in FactBank
(Saur?? and Pustejovsky 2009) are of the epistemically uncertain type, events with a
generic source belong to discourse-level uncertainty, whereas underspecified events are
341
Computational Linguistics Volume 38, Number 2
classified as hypothetical uncertainty in our system as, by definition, their truth value
cannot be determined. WikiWeasel (Farkas et al 2010) contains annotation for epistemic
uncertainty, but discourse-level uncertainty is also annotated in the corpus (see Figure 1
for an overview). The categories used for themachine reading task described inMorante
and Daelemans (2011) also overlap with our fine-grained classes: Uncertain events in
their system fall into our epistemic uncertainty class. Their modal events expressing
purpose, need, obligation, or desire are instances of dynamic modality, whereas their
conditions are understood in a similar way to our condition class. The modality types
listed in Baker et al (2010) can be classified as types of dynamic modality, except for
their belief category. Instances of the latter category are either certain (It is certain that he
met the president) or epistemic or doxastic modality in our system.
2.2 Types of Semantic Uncertainty Cues
We assume that the nature of the lexical unit determines the type of uncertainty it
represents, that is, semantic uncertainty is highly lexical in nature. The part of speech of
the uncertainty cue candidates serves as the basis for categorization, similar to the ones
found in Hyland (1994, 1996, 1998) and Rizomilioti (2006). In English, modality is often
associated with modal auxiliaries (Palmer 1979), but, as Table 1 shows, there are many
other parts of speech that can express uncertainty. It should be added that there are
cues where it depends on the context, rather than the given lexical item, what subclass
of uncertainty the cue refers to, for example, may can denote epistemic modality (It may
rain. . . ) or dynamic modality (Now you may open the door). These categories are listed in
Table 1.
3. A Framework for Detecting Semantic Uncertainty
In our model, uncertainty detection is a standalone task that is largely independent of
the underlying application. In this section, we briefly discuss how uncertainty detection
Table 1
Uncertainty cues.
Adjectives / adverbs
probable, likely, possible, unsure, possibly, perhaps, etc. epistemic
Auxiliaries
may, might, can, would, should, could, etc. semantic
Verbs
speculative: suggest, question, seem, appear, favor, etc. epistemic
psych: think, believe, etc. doxastic
analytic: investigate, analyze, examine, etc. investigation
prospective: plan, want, order, allow, etc. dynamic
Conjunctions
if, whether, etc. investigation
Nouns
nouns derived speculation, proposal, consideration, etc. same as the verb
from uncertain verb:
other rumor, idea, etc. doxastic
uncertain nouns:
342
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
can be incorporated into an information extraction task, which is probably the most
relevant application area (see Kim et al [2009] for more details). In the information
extraction context, the key steps of recognizing uncertain propositions are locating the
cues, disambiguating them (as not all occurrences of the cues indicate uncertainty; recall
the example of evaluate), and finally linking them with the textual representation of
the propositions in question. We note here that marking the textual representations of
important propositions (often referred to as events in information extraction) is actually
the main goal of an information extraction system, hence we will not focus on their
identification and just assume that they are already marked in texts.
The following is an example that demonstrates the process of uncertainty detection:
(8) In this study we hypothesizedCUE that the phosphorylation of TRAF2
inhibitsEVENT binding to the CD40 cytoplasmic domain.
Here the EVENT mark-up is produced by the information extraction system, and
uncertainty detection consists of i) the recognition of the cue word hypothesized, and
determining whether it denotes uncertainty in this specific case (producing the CUE
mark-up) and ii) determining whether the cue hypothesizedmodifies the event triggered
by inhibits or not (positive example in this case).
3.1 Uncertainty Cue Detection and Disambiguation
The cue detection and disambiguation problem can be essentially regarded as a token
labeling problem. Here the task is to assign a label to each of the tokens of a sentence
in question according to whether it is the starting token of an uncertainty cue (B-
CUE TYPE), an inside token of a cue (I-CUE TYPE), or it is not part of any cue (O). Most
previous studies assume a binary classification task, namely, each token is either part of
an uncertainty cue, or it is not a cue. For fine-grained uncertainty detection, a different
label has to be used for each uncertainty type to be distinguished. This way, the label
sequence of a sentence naturally identifies all uncertainty cues (with their types) in the
sentence, and disambiguation is solved jointly with recognition.
Because the uncertainty cue vocabulary and the distribution of certain and uncer-
tain senses of cues vary in different domains and genres, uncertainty cue detection and
disambiguation are the main focus of the current study.
3.2 Linking Uncertainty Cues to Propositions
The task of linking the detected uncertainty cues to propositions can be formulated
as a binary classification task over uncertainty cue and event marker pairs. The relation
holds and is considered true if the cuemodifies the truth value (confidence) of the event;
it does not hold and is considered false if the cue does not have any impact on the
interpretation of the event. That is, the pair (hypothesized, inhibits) in Example (8) is an
instance of positive relation.
The linking of uncertainty cues and event markers can be established by using de-
pendency grammar rules (i.e., the problem is mainly syntax driven). As the grammatical
properties of the language are similar in various domains and genres, this task is largely
domain-independent, as opposed to the recognition and disambiguation task. Because
of this, we sketch the most important matching patterns, but do not address the linking
task in great detail here.
343
Computational Linguistics Volume 38, Number 2
The following are the characteristic rules that can be used to link uncertainty cues
to event markers. For practical implementations of heuristic cue/event matching, see
Chapman, Chu, and Dowling (2007) and Kilicoglu and Bergler (2009).
 If the event clue has an uncertain verb, noun, preposition, or auxiliary as a
(not necessarily direct) parent in the dependency graph of the sentence,
the event is regarded as uncertain.
 If the event clue has an uncertain adverb or adjective as its child, it is
treated as uncertain.
4. Related Work on Uncertainty Cue Detection
Herewe review the publishedworks related to uncertainty cue detection. Earlier studies
focused either on in-domain cue recognition for a single domain or on cue lexicon
extraction from large corpora. The latter approach is applicable tomultiple domains, but
does not address the disambiguation of uncertain and other meanings of the extracted
cue words. We are also aware of several studies that discussed the differences of cue
distributions in various domains, without developing a cue detector. To the best of
our knowledge, our study is the first to address the genre- and domain-adaptability of
uncertainty cue recognition systems and thus uncertainty detection in a general context.
We should add that there are plenty of studies on end-application oriented uncer-
tainty detection, that is, how to utilize the recognized cues (see, for instance, Kilicoglu
and Bergler [2008], Uzuner, Zhang, and Sibanda [2009] and Saur?? [2008] for information
extraction or Farkas and Szarvas [2008] for document labeling applications), and a
recent pilot task sought to exploit negation and hedge cue detectors in machine reading
(Morante and Daelemans 2011). As the focus of our paper is cue recognition, however,
we omit their detailed description here.
4.1 In-Domain Cue Detection
In-domain uncertainty detectors have been developed since the mid 1990s. Most of
these systems use hand-crafted lexicons for cue recognition and they treat each oc-
currence of the lexicon items as a cue?that is, they do not address the problem of
disambiguating cues (Friedman et al 1994; Light, Qiu, and Srinivasan 2004; Farkas and
Szarvas 2008; Saur?? 2008; Conway, Doan, and Collier 2009; Van Landeghem et al 2009).
ConText (Chapman, Chu, and Dowling 2007) uses regular expressions to define cues
and ?pseudo-triggers?. A pseudo-trigger is a superstring of a cue and it is basically
used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be
regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and
Baldwin (2009) introduced a system which also used non-consecutive tokens as cues
(like not+as+yet).
Utilizing manually labeled corpora, machine learning?based uncertainty cue de-
tectors have also been developed (to the best of our knowledge each of them uses an
in-domain training data set). They use token classification (Morante and Daelemans
2009; Clausen 2010; Fernandes, Crestana, and Milidiu? 2010; Sa?nchez, Li, and Vogel
2010) or sequence labeling approaches (Li et al 2010; Rei and Briscoe 2010; Tang et al
2010; Zhang et al 2010). In both cases the tokens are labeled according to whether
they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of
344
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
tokens) thus it naturally deals with the context of a particular word. On the other hand,
context information for a token is built into the feature space of the token classification
approaches. O?zgu?r and Radev (2009) and Velldal (2010) match cues from a lexicon then
apply a binary classifier based on features describing the context of the cue candidate.
Each of these approaches uses a rich feature representation for tokens, which usu-
ally includes surface-level, part-of-speech, and chunk-level features. A few systems
have also used dependency relation types originating at the cue (Rei and Briscoe 2010;
Sa?nchez, Li, and Vogel 2010; Velldal, ?vrelid, and Oepen 2010; Zhang et al 2010); the
CoNLL-2010 Shared Task final ranking suggests that it has only a limited impact on the
performance of an entire system (Farkas et al 2010), however. O?zgu?r and Radev (2009)
further extended the feature set with the other cues that occur in the same sentence as
the cue, and positional features such as the section header of the article in which the cue
occurs (the latter is only defined for scientific publications). Velldal (2010) argues that
the dimensionality of the uncertainty cue detection feature space is too high and reports
improvements by using the sparse random indexing technique.
Ganter and Strube (2009) proposed a rather different approach for (weasel) cue
detection?exploiting weasel tags4 in Wikipedia articles given by editors. They used
syntax-based patterns to recognize the internal structure of the cues, which has proved
useful as discourse-level uncertainty cues are usually long and have a complex internal
structure (as opposed to semantic uncertainty cues).
As can be seen, uncertainty cue detectors have mostly been developed in the bio-
logical and medical domains. All of these studies, however, focus on only one domain,
namely, in-domain cue detection is carried out, which assumes the availability of a
training data set of sufficient size. The only exceptionwe are aware of is the CoNLL-2010
Shared Task (Farkas et al 2010), where participants had the chance to use Wikipedia
data on biomedical domain and vice versa. Probably due to the differences in the
annotated uncertainty types and the stylistic and topical characteristics of the texts,
very few participants performed cross-domain experiments and reported only limited
success (see Section 5.3.2 for more on this).
Overall, the findings of these studies indicate that disambiguating cue candidates is
an important aspect of uncertainty detection and that the domain specificity of disam-
biguation models and domain adaptation in general are largely unexplored problems
in uncertainty detection.
4.2 Weakly Supervised Extraction of Cue Lexicon
Similar to our approach, several studies have addressed the problem of developing
an uncertainty detector for a new domain using as little annotation effort as possible.
The aim of these studies is to identify uncertain sentences; this is carried out by semi-
automatic construction of cue lexicons. The weakly supervised approaches start with
very small seed sets of annotated certain and uncertain sentences, and use bootstrap-
ping to induce a suitable training corpus in an automatic way. Such approaches collect
potentially certain and uncertain sentences from a large unlabeled pool based on their
similarity to the instances in the seed sets (Medlock and Briscoe 2007), or based on the
known errors of an information extraction system that is itself sensitive to uncertain
texts (Szarvas 2008). Further instances are then collected (in an iterative fashion) on
the basis of their similarity to the current training instances. Based on the observation
4 See http://en.wikipedia.org/wiki/Wikipedia:Embrace_weasel_words.
345
Computational Linguistics Volume 38, Number 2
that uncertain sentences tend to contain more than one uncertainty cue, these models
successfully extend the seed sets with automatically labeled sentences, and can produce
an uncertainty classifier with a sentence-level F-score of 60?80% for the uncertain class,
given that the texts of the seed examples, the unlabeled pool, and the actual evaluation
data share very similar properties.
Szarvas (2008) showed that these models essentially learn the uncertainty lexicon
(set of cues) of the given domain, but are otherwise unable to disambiguate the potential
cue words?that is, to distinguish between the uncertain and certain uses of the previ-
ously seen cues. This deficiency of the derived models is inherent to the bootstrapping
process, which considers all occurrences of the cue candidates as good candidates for
positive examples (as opposed to unlabeled sentences without any previously seen cue
words).
Kilicoglu and Bergler (2008) proposed a semi-automatic method to expand a seed
cue lexicon. Their linguistically motivated approach is also based on the weakly super-
vised induction of a corpus of uncertain sentences. It exploits the syntactic patterns of
uncertain sentences to identify new cue candidates.
The previous studies on weakly supervised approaches to uncertainty detection
did not tackle the problem of disambiguating the certain and uncertain uses of cue
candidates, which is a major drawback from a practical point of view.
4.3 Cue Distribution Analyses
Besides automatic uncertainty recognition, several studies investigated the distribution
of hedge cues in scientific papers from different domains (Hyland 1998; Falahati 2006;
Rizomilioti 2006). The effect of different domains on the frequency of uncertain expres-
sions was examined in Rizomilioti (2006). Based on a previously defined dictionary of
hedge cues, she analyzed the linguistic tools expressing epistemic modality in research
papers from three domains, namely, archeology, literary criticism, and biology. Her
results indicated that archaeological papers tend to contain the most uncertainty cues
(which she calls downtoners) and the fewest uncertainty cues can be found in literary
criticism papers. Different academic disciplines were contrasted in Hyland (1998) from
the viewpoint of hedging: Papers belonging to the humanities contain significantly
more hedging devices than papers in sciences. It is interesting to note, however, that
in both studies, biological papers are situated in the middle as far as the percentage rate
of uncertainty cues is concerned. Falahati (2006) examined hedges in research articles in
medicine, chemistry, and psychology and concluded that it is psychology articles that
contain the most hedges.
Overall, these studies demonstrate that there are substantial differences in the way
different technical/scientific domains and different genres express uncertainty in gen-
eral, and in the use of semantic uncertainty in particular. Differences are found not just
in the use of different vocabulary for expressing uncertainty, but also in the frequency
of certain and uncertain usage of particular uncertainty cues. These findings underpin
the practical importance of domain portability and domain adaptation of uncertainty
detectors.
5. Uncertainty Cue Recognition
In this section, we present our uncertainty cue detector and the results of the cross-genre
and -domain experiments carried out by us. Before describing ourmodel and discussing
the results of the experiments, a short overview of the texts used as training and test
346
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
data sets will be given along with an empirical analysis of the sense distributions of the
most frequent cues.
5.1 Data Sets
In our investigations, we selected three corpora (i.e., BioScope, WikiWeasel, and Fact-
Bank) from different domains (biomedical, encyclopedia, and newswire, respectively).
Genres also vary in the corpora (in the scientific genre, there are papers and abstracts
whereas the other corpora contain pieces of news and encyclopedia articles). We pre-
ferred corpora on which earlier experiments had been carried out because this allowed
us to compare our results with those of previous studies. This selectionmakes it possible
to investigate domain and genre differences because each domain has its characteristic
language use (which might result in differences in cue distribution) and different genres
also require different writing strategies (e.g., in abstracts, implications of experimental
results are often emphasized, which usually involves the use of uncertain language).
The BioScope corpus (Vincze et al 2008) contains clinical texts as well as biological
texts from full papers and scientific abstracts; the texts were manually annotated for
hedge cues and their scopes. In our experiments, 15 other papers annotated for the
CoNLL-2010 Shared Task (Farkas et al 2010) were also added to the set of BioScope
papers. The WikiWeasel corpus (Farkas et al 2010) was also used in the CoNLL-2010
Shared Task and it was manually annotated for weasel cues and semantic uncertainty
in randomly selected paragraphs taken from Wikipedia articles. The FactBank corpus
contains texts from the newswire domain (Saur?? and Pustejovsky 2009). Events are
annotated in the data set and they are evaluated on the basis of their factuality from
the viewpoint of their sources.
Table 2 provides statistical data on the three corpora. Because in our experimental
set-up, texts belonging to different genres also play an important role, data on abstracts
and papers are included separately.
5.1.1 Genres and Domains. Texts found in the three corpora to be investigated can be
categorized into three genres, which can be further divided to subgenres at a finer level
of distinction. Figure 2 depicts this classification.
The majority of BioScope texts (papers and abstracts) belong to the scientific dis-
course genre. FactBank texts can be divided into broadcast and written news, and
Wikipedia texts belong to the encyclopedia genre.
As for the domain of the texts, there are three broad domains, namely, biology, news,
and encyclopedia. Once again, these domains can be further divided into narrower
Table 2
Data on the corpora. sent. = sentence; epist. = epistemic cue; dox. = doxastic cue;
inv. = investigation cue; cond. = condition cue.
Data Set #sent. #epist. #dox. #inv. #cond. Total
BioScope papers 7676 1373 220 295 187 2075
BioScope abstracts 11797 2478 200 784 24 3486
BioScope total 19473 3851 420 1079 211 5561
WikiWeasel 20756 1171 909 94 491 3265
FactBank 3123 305 201 36 178 720
Total 43352 5927 1530 1209 880 9546
347
Computational Linguistics Volume 38, Number 2
Figure 2
Genres of texts.
Figure 3
Domains of texts.
topics at a fine-grained level, which is shown in Figure 3. All abstracts and five papers
in BioScope are related to the MeSH terms human, blood cell, and transcription factor (hbc
in Figure 3). Nine BMC Bioinformatics papers come from the bioinformatics domain
(bmc in Figure 3), and ten papers describe some experimental results on the Drosophila
species (fly). FactBank news can be classified as stock news, political news, and
criminal news. Encyclopedia articles cover a broad range of topics, hence no detailed
classification is given here.
5.1.2 The Normalization of the Corpora. In order to uniformly evaluate our methods in
each domain and genre (and each corpus), the evaluation data sets were normalized.
This meant that cues had to be annotated in each data set and differentiated for types
of semantic uncertainty. This resulted in the reannotation of BioScope, WikiWeasel, and
FactBank.5 In BioScope, the originally annotated cues were separated into epistemic
cues and subtypes of hypothetical cues and instances of hypothetical uncertainty not
yet marked were also annotated. In FactBank, epistemic and hypothetical cues were
annotated: Uncertain events were matched with their uncertainty cues and instances
of hypothetical uncertainty that were originally not annotated were also marked in
the corpus. In the case of WikiWeasel, these two types of cues were separated from
discourse-level cues.
One class of hypothetical uncertainty (i.e., dynamic modality) was not annotated
in any of the corpora. Although dynamic modality seems to play a role in the news
domain, it is less important and less represented in the other two domains we investi-
gated here. The other subclasses are more of general interest for the applications. For
example, one of our training corpora comes from the scientific domain, where it is more
important to distinguish facts from hypotheses and propositions under investigation
5 The corpora are available at http://www.inf.u-szeged.hu/rgai/uncertainty.
348
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
(which can be later confirmed or rejected, compare the meta-knowledge annotation
scheme developed for biological events [Nawaz, Thompson, andAnaniadou 2010]), and
from propositions that depend on each other (conditions).
5.1.3 Uncertainty Cues in the Corpora. An analysis of the cue distributions reveals some
interesting trends that can be exploited in uncertainty detection across domains and
genres. The most frequent cue stems in the (sub)corpora used in our study can be seen
in Table 3 and they are responsible for about 74% of epistemic cue occurrences, 55% of
doxastic cue occurrences, 70% of investigation cue occurrences, and 91% of condition
cue occurrences.
As can be seen, one of the most frequent epistemic cues in each corpus is may. If,
possible, might, and suggest also occur frequently in our data set.
The distribution of the uncertainty cues was also analyzed from the perspective of
uncertainty classes in each corpus, which is presented in Figure 4. Inmost of the corpora,
epistemic cues are the most frequent (except for FactBank) and they vary the most:
Out of the 300 cue stems occurring in the corpora, 206 are epistemic cues. Comparing
the domains, it can readily be seen that in biological texts, doxastic uncertainty is not
frequent, which is especially true for abstracts, whereas in FactBank and WikiWeasel
they cover about 27% of the data. The most frequent doxastic keywords exhibit some
domain-specific differences, however: In BioScope, the most frequent ones include puta-
tive and hypothesis, which rarely occur in FactBank and WikiWeasel. Nevertheless, cues
belonging to the investigation class can be found almost exclusively in scientific texts
(89% of them are in BioScope), which can be expected because the aim of scientific pub-
lications is to examine whether a hypothesized phenomenon occurs. Among the most
Table 3
The most frequent cues in the corpora. epist. = epistemic cue; dox. = doxastic cue; inv. =
investigation cue; cond. = condition cue.
Global Abstracts Full papers BioScope FactBank WikiWeasel
Epist. may 1508 suggest 616 may 228 suggest 810 may 43 may 721
suggest 928 may 516 suggest 194 may 744 could 29 probable 112
indicate 421 indicate 301 indicate 103 indicate 404 possible 26 suggest 108
possible 304 appear 143 possible 84 appear 213 likely 24 possible 93
appear 260 or 119 might 83 or 197 might 23 likely 80
might 256 possible 101 or 78 possible 185 appear 15 might 78
likely 221 might 72 can 73 might 155 seem 11 seem 67
or 198 potential 72 appear 70 can 117 potential 10 could 55
could 196 likely 60 likely 57 likely 117 probable 10 perhaps 51
probable 157 could 56 could 56 could 112 suggest 10 appear 32
Dox. consider 276 putative 43 putative 37 putative 80 expect 75 consider 250
believe 222 think 43 hypothesis 33 hypothesis 77 believe 25 believe 173
expect 136 hypothesis 43 assume 24 think 66 think 24 allege 81
think 131 believe 14 think 24 assume 32 allege 8 think 61
putative 83 consider 10 expect 22 predict 26 accuse 7 regard 58
Invest. whether 247 investigate 177 whether 73 investigate 221 whether 26 whether 52
investigate 222 examine 160 investigate 44 examine 183 if 3 if 20
examine 183 whether 96 test 25 whether 169 remain to be seen 2 whether or not 7
study 102 study 88 examine 23 study 101 question 1 assess 3
determine 90 determine 67 determine 20 determine 87 determine 1 evaluate 3
Cond. if 418 if 14 if 85 if 99 if 65 if 254
would 238 would 6 would 46 would 52 would 50 would 136
will 80 until 2 will 20 will 20 will 21 will 39
until 40 could 1 should 11 should 11 until 16 until 15
could 30 unless 1 could 9 could 10 could 9 unless 14
349
Computational Linguistics Volume 38, Number 2
Figure 4
Cue type distributions in the corpora.
frequent cues, investigate, examine, and study belong to this group. These data reveal
that the frequency of doxastic and investigation cues is strongly domain-dependent,
and this explains the fact that the investigation vocabulary is very limited in Factbank
and WikiWeasel. Only about 10 cue stems belong to this uncertainty class in these cor-
pora. The set of condition cue stems, however, is very small in each corpus; altogether
18 condition cue stems can be found in the data, although if and would are responsible
for almost 75% of condition cue occurrences. It should also be mentioned that the
percentage of condition cues is higher in FactBank than in the other corpora.
Another interesting trend was observed when word forms were considered instead
of stemmed forms: Certain verbs in third person singular (e.g., expects or believes) occur
mostly in FactBank and WikiWeasel. The reason for this may be that when speaking
about someone else?s opinion in scientific discourse, the source of the opinion is usually
provided in the form of references or citations?usually at the end of the sentence?and
due to this, the verb is often used in the passive form, as in Example (9).
(9) It is currently believed that both RAG1 and RAG2 proteins were originally
encoded by the same transposon recruited in a common ancestor of jawed
vertebrates [3,12,13,16].
In contrast, impersonal constructions are hardly used in news media, where the ob-
jective is to inform listeners about the source of the news presented as well in order
to enable them to judge the reliability of a piece of news. Here, a clause including the
source and a communication verb is usually attached to the proposition.
A genre-related difference between scientific abstracts and full papers is that con-
dition cues can rarely be found in abstracts, although they occur more frequently in
papers (with the non-cue usage still being much more frequent). Another difference is
the percentage of cues of the investigation type, which may be related to the structure
350
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
of abstracts. Biological abstracts usually present the problem they examine and describe
methods they use. This entails the application of predicates belonging to the investiga-
tion class of uncertainty. It can be argued, however, that scientific papers also have these
characteristics but abstracts are much shorter than papers (generally, they contain about
10?12 sentences). Hence, investigation cues are responsible for a greater percentage of
cues.
There are some lexical differences among the corpora that are related to domain or
genre specificity. For instance, due to their semantics, the words charge, accuse, allege,
fear, worry, and rumor are highly unlikely to occur in scientific publications, but they
occur relatively often in news texts and in Wikipedia articles. As for lexical divergences
between abstracts and papers, many of them are related to verbs of investigation and
their different usage. In the corpora, verbs of investigations were marked only if it was
not clear whether the event/phenomenon would take place or not. If it has already
happened (The police are investigating the crime) or the existence of the thing under
investigation can be stated with certainty, independently of the investigation (The top
ten organisms were examined), then they are not instances of hypotheses, so they were
not annotated. As the data sets make clear, there were some candidates of investigation
verbs that occurred in the investigation sense mostly in abstracts but in another sense in
papers, especially in the bmc data set (e.g. assess or examine). Evaluate also had a special
mathematical sense in bmc papers, which did not occur in abstracts.
It can also be seen that some of the very frequent cues in papers do not occur (or
only relatively rarely) in abstracts. This is especially true for the bmc data set, where can,
if, would, could, and will are among the 15 most frequent cues and represent 23.21% of
cue occurrences, but only 3.85% in abstracts. It is also apparent that the rate of epistemic
cues is lower in bmc papers than in abstracts or other types of papers.
Genre-dependent characteristics can be analyzed if BioScope abstracts and hbc
papers are compared because their fine-grained domain is the same. Thus, it may be
assumed that differences between their cues are related to the genre. The sets of cues
used are similar, but the sense distributions may differ for certain ambiguous cues. For
instance, indicate mostly appears in the ?suggest? sense in abstracts, whereas in papers
it is used in the ?signal? sense. Another difference is that the percentage rate of doxastic
cues is almost twice as high in papers as in abstracts (10.6% and 5.7%, respectively).
Besides these differences, the two data sets are quite similar.
Domain-related differences can be analyzed when the three subdomains of biolog-
ical papers are contrasted. As stressed earlier, bmc papers contain fewer instances of
epistemic uncertainty, but condition cues occur more frequently in them. Nevertheless,
fly and hbc papers are rather similar in these respects but hbc papers contain more
investigation cues than the other two subcorpora. As regards lexical issues, the non-cue
usage of possible in comparative constructions is more frequent in the bmc data set than
in the other papers and many occurrences of if in bmc are related to definitions, which
were not annotated as uncertain. On the basis of this information, the fly and the hbc
domains seem to be more similar to each other than to the BMC data set from a linguistic
point of view.
From the perspective of genre and domain adaptation, the following points should
be highlighted concerning the distribution of uncertainty cues across corpora. Doxastic
uncertainty is of primary importance in the news and encyclopedia domains whereas
the investigation class is characteristic of the biological domain. Within the latter, there
is a genre-related difference as well: It is the epistemic and investigation classes that
are mainly present in abstracts whereas in papers cues belonging to other uncertainty
classes can also be found. Thus, when applying techniques developed for biological
351
Computational Linguistics Volume 38, Number 2
texts or abstracts to news texts, for example, doxastic uncertainty cues deserve special
attention as it might well be the case that there are insufficient training examples for this
class of uncertainty cues. The adaptation of an uncertainty cue detector constructed for
encyclopedia texts requires the special treatment of investigation cues, however, if, for
instance, scientific discourse is the target genre since they are underrepresented in the
source genre.
5.2 Evaluation Metrics
As evaluation metrics, we used cue-level and sentence-level F?=1 scores for the uncer-
tain class (the standard evaluation metrics of Task 1 of the CoNLL-2010 shared task)
and denote them by Fcue and Fsent, respectively. We report cue-level F?=1 scores on the
individual subcategories of uncertainty and the unlabeled (binary) F?=1 scores as well.
A sentence is treated as uncertain (in the gold standard and prediction) iff it contains at
least one cue. Note that the cue-level metric is quite strict as it is based on recognized
phrases?that is, only cues with perfect boundary matches are true positives. For the
sentence-level evaluation we simply labeled those sentences as uncertain that contained
at least one recognized cue.
5.3 Cross-Domain Cue Recognition Model
In order to minimize the development cost of a labeled corpus and an uncertainty
detector for a new genre/domain, we need to induce an accurate model from a minimal
amount of labeled data, or take advantage of existing corpora for different genres
and/or domains and use a domain adaptation approach. Experiments investigating the
value and sufficiency of existing corpora (which are usually out-of-domain) and simple
domain adaptation methods were carried out. For this purpose, we implemented a cue
recognition model, which is described in this section.
To train our models, we applied surface level (e.g., capitalization) and shallow
syntactic features (part-of-speech tags and chunks) and avoided the use of lexicon-based
features listing potential cue words, in order to reduce the domain dependence of the
learned models. Now we will introduce our model, which is competitive with the state-
of-the-art systems and focus on its domain adaptability. We will also describe the im-
plementation details of the learning model and the features employed. We should add
that the optimization of a cue detector was not the main focus of our study, however.
5.3.1 Feature Set.We extracted two types of features for each token to describe the token
itself, together with its local context in a window of limited size (1, 2, or no window,
depending on the feature).
The first group consists of features describing the surface form of the tokens. Here
we provide the list of the surface features with the corresponding window sizes:
 Stems of the tokens by the Porter stemmer in a window of size 2 (current
token and two tokens to the left and right).
 Surface pattern of the tokens in a window of size one (current token
and 1 token to the left and right). These patterns are similar to the word
shape feature described in Sun et al (2007). This feature can describe the
capitalization and other orthographic features as well. Patterns represent
352
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
character sequences of the same type with one single character for a
given word. There are six different pattern types denoting capitalized and
lowercased character sequences with the characters ?A? and ?a?, number
sequences with ?0?, Greek letter sequences with ?G? and ?g?, Roman
numerals with ?R? and ?r?, and non-alphanumerical characters with ?!?.
 Prefixes and suffixes of word forms from three to five characters long.
The second group of features describes the syntactic properties of the token and its
local context. The list of the syntactic features with the corresponding window sizes is
the following:
 Part-of-speech (POS) tags of the tokens by the C&C POS-tagger in a
window of size 2.
 Syntactic chunk of the tokens, as given by the C&C chunker,6 and the
chunk code of the tokens in a window of size 2.
 Concatenated stem, POS, and chunk labels similar to the features used
by Tang et al (2010). These feature strings were a combination of the stem
and the chunk code of the current token, the stem of the current token
combined with the POS-codes of the token left and right, and the chunk
code of the current token with the stems of the neighboring tokens.
5.3.2 CoNLL-2010 Experiments. The CoNLL-2010 shared task Learning to detect hedges and
their scope in natural language text focused on uncertainty detection. Two subtasks were
defined at the shared task: The first task sought to recognize sentences that contain some
uncertain language in two different domains and the second task sought to recognize
lexical cues together with their linguistic scope in biological texts (i.e., the text span in
terms of constituency grammar that covers the part of the sentence that is modified
by the cue). The lexical cue recognition subproblem of the second task7 is identical
to the problem setting used in this study, with the only major difference being the
types of uncertainty addressed: In the CoNLL-2010 task biological texts contained only
epistemic, doxastic, and investigation types of uncertainty. Apart from these differences,
the CoNLL-2010 shared task offers an excellent testbed for comparing our uncertainty
detection model with other state-of-the-art approaches for uncertainty detection and to
compare different classification approaches. Here we present our detailed experiments
using the CoNLL data sets, analyze the performance of our models, and select the most
suitable models for further experiments.
CoNLL systems. The uncertainty detection systems that were submitted to the CoNLL
shared task can be classified into three major types. The first set of systems treats the
problem as a sentence classification task, that is, one to decide whether a sentence
contains any uncertain element or not. These models operate at the sentence level and
are unsuitable for cue detection. The second group handles the problem as a token
6 POS-tagging and chunking were performed on all corpora using the C&C Tools (Curran, Clark, and
Bos 2007).
7 As an intermediate level, participants of the first task could submit the lexical cues found in sentences
for evaluation, without their scope, which gave some insight into the nature of cue detection on the
Wikipedia corpus (where scope annotation does not exist) as well.
353
Computational Linguistics Volume 38, Number 2
Table 4
Results on the original CoNLL-2010 data sets. The first three rows correspond to our baseline,
token-based, and sequence labeling models. The BEST/SEQ row shows the results of the best
sequence labeling approach of the CoNLL shared task (for both domains), the BEST/TOK rows
show the best token-based models, and the BEST/SENT rows show the best sentence-level
classifiers (these models did not produce cue-level results).
BIOLOGICAL WIKIPEDIA
Fcue Fsent Fcue Fsent
BASELINE 74.5 81.4 19.5 58.6
TOKEN/MAXENT 79.7 85.8 22.3 58.1
SEQUENCE/CRF 81.4 87.0 32.7 47.0
BEST/SEQ (Tang et al 2010) 81.3 86.4 36.5 55.0
BEST/TOK BIO (Velldal, ?vrelid, and Oepen 2010) 78.7 85.2 ? ?
BEST/TOKWIKI (Morante, Van Asch, and Daelemans 2010) 76.7 81.7 11.3 57.3
BEST/SENT BIO (Ta?ckstro?m et al 2010) ? 85.2 ? 55.4
BEST/SENTWIKI (Georgescul 2010) ? 78.5 ? 60.2
classification task, and classifies each token independently as uncertain (or not).
Contextual information is only included in the form of feature functions. The third
group of systems handled the task as a sequential token labeling problem, that is, de-
termined the most likely label sequence of a sentence in one step, taking the informa-
tion about neighboring labels into account. Sequence labeling and token classification
approaches performed best for biological texts and sentence-level models and token
classification approaches gave the best results for Wikipedia texts (see Table 6 in Farkas
et al [2010]). Here we compare a state-of-the-art token classification and sequence
labeling approach using a shared feature representation to decide which model to use
in further experiments.
Classifier models. We used a first-order linear chain conditional random fields (CRF)
model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as a
token classifier, implemented in the Mallet (McCallum 2002) package for training the
uncertainty cue detectors. This choicewasmotivated by the fact that thesewere themost
popular classification approaches among the CoNLL-2010 participants, and that CRF
models are known to provide high accuracy for the detection of phrases with accurate
boundaries (e.g., in named entity recognition). We trained the CRF and Maxent models
with their default settings in Mallet for 200 iterations or until convergence (CRF), and
also until convergence (Maxent) in each experimental set-up.
As a baseline model, we applied a simple dictionary-based approach which clas-
sifies every uni- and bigram as uncertain that is tagged as uncertain in over 50% of
the cases in the training data. Hence, it is a similar system to that presented by Tjong
Kim Sang (2010), without tuning the decision threshold for predicting uncertainty.
CoNLL results. An overview of the results achieved on the CoNLL-2010 data sets can
be found in Table 4. A comparison of our models with the CoNLL systems reveals that
our uncertainty detection model is very competitive when applied on the biological
data set. Our CRF model trained on the official training data set of the shared task
achieved a cue-level F-score of 81.4 and sentence-level F-score of 87.0 on the biological
354
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
evaluation data set. These results would have come first in the shared task, with a
marginal difference compared to the top performing participant. In contrast, our model
is less competitive on the Wikipedia data set: The Maxent model achieved a cue-level
F-score of 22.3 and sentence-level F-score of 58.1 on the Wikipedia evaluation data
set, whereas our CRF model was not competitive with the best participating systems.
The observation that sequence-labeling models perform worse than token-based
approaches on Wikipedia, especially for sentence-level evaluation measures, coincides
with the findings of the shared task: The discourse-level uncertainty cues in the
Wikipedia data set are rather long and heterogeneous and sequence labeling models
often revert to not annotating any token in a sentence when the phrase boundaries are
hard to detect. Still, sequence labeling models have an advantage in terms of cue-level
accuracy. This is not surprising because CRF is a state-of-the-art model for chunking /
sequence labeling tasks.
We conclude from Table 4 that our model is competitive with the state-of-the-art
systems for detecting semantic uncertainty (which is closer to the biological subtask),
but it is less suited to recognizing discourse-level uncertainty. In the subsequent exper-
iments we used our CRF model, which performed best in detecting uncertainty cues in
natural language sentences.
5.3.3 Domain Adaptation Model. In supervised machine learning, the task is to learn how
to make predictions on previously unseen, new examples based on a statistical model
learned from a collection of labeled training examples (i.e., a set of examples coupled
with the desired output for them). The classification setting assumes a set of labels L, a
set of features X, and a probability distribution p(X) describing the examples in terms of
their features. Then the training examples are assumed to be given in the form of {xi, li}
pairs and the goal of classification is to estimate the label distribution p(L|X), which can
be used later on to predict the labels for unseen examples.
Domain adaptation focuses on the problem where the same (or a closely related)
learning task has to be solved in multiple domains which have different characteristics
in terms of their features: The set of features X may be different or the probability dis-
tributions p(X) describing the inputs may be different. When the target tasks are treated
as different (but related), the label distribution p(L|X) is dependent on the domain. That
is, given a domain d, the problem can be formalized as modeling p(L|X)d based on Xd,
p(X)d and a set of examples: {xi,d, li}.8 In the context of domain adaptation, there is a
target domain t and a source domain s, with labeled data available for both, and the
goal is to induce a more accurate target domain model p(L|X)t from {xi,t, li} ? {xi,s, li}
than the one learned from {xi,t, li} only. In practical scenarios, the goal is to exploit the
source data to acquire an accurate model from just limited target data which are alone
insufficient to train an accurate in-domain model, and thus to port the model to a new
domain with moderate annotation costs. The problem is difficult because it is nontrivial
for a learning method to account for the different data (and label) distributions between
target and source, which causes a remarkable drop in model accuracy when it is applied
to classifying examples taken from the target domain.
In our experimental context, both topic- and genre-related differences of texts pose
an adaptation problem as these factors have an impact on both the vocabulary (p(X))
and the sense distributions of the cues (p(L|X)) found in different texts. There is some
8 The literature also describes the case when the set of labels depends on the domain, but we omit this case
to simplify our notation and discussion. For details, see Pan and Yang (2010).
355
Computational Linguistics Volume 38, Number 2
confusion in the literature regarding the terminology describing the various domain
mismatches in the learning problem. For example, Daume? III (2007) describes a domain
adaptation method where he assumes that the label distribution is unchanged (we note
here that this assumption is not exploited in the method, and that the label distribution
changes in our problem), whereas Pan and Yang (2010) uses the term inductive transfer
learning to refer to our scenario (in their paper, domain adaptation refers to a different
setting).9 In this study we always use the term domain adaptation to refer to our problem
setting, that is, where both p(X) and p(L|X) are assumed to change.
In our experiments, we used various data sets taken from multiple genres and
domains (see Section 5.1.1 for an overview) and applied a simple but effective do-
main adaptation model (Daume? III 2007) for training our classifiers. In this model,
domain adaptation is carried out by defining each feature over the target and source
data sets twice?just once for target domain instances, and once for both the tar-
get and source domain instances. Formally, having a target domain t and a source
domain s and n features {f1, f2, . . . fn}, for each fi we have a target-only version fi,t
and a shared version fi,t+s. Each target domain example is described by 2n features:
{ f1,t, f2,t, . . . fn,t, f1,t+s, f2,t+s, . . . fn,t+s} and source domain examples are described by only
the n shared features: { f1,t+s, f2,t+s, . . . fn,t+s}. Using the union of the source and target
training data sets {xi,t, li} ? {xi,s, li} and this feature representation, any standard super-
vised machine learning technique can be used and it becomes possible for the algorithm
to learn target-dependent and shared patterns at the same time and handle the changes
in the underlying distributions. This easy domain adaptation technique has been found
to work well in many NLP-oriented tasks. We used the CRF models introduced herein
and in this way, we were able to exploit feature?label correspondences across domains
(for features that behave consistently across domains) and also to learn patterns specific
to the target domain.
5.4 Cross-Domain and Genre Experiments
We defined several settings (target and source pairs) with varied domain and genre
distances and target data set sizes. These experiments allowed us to study the po-
tential of transferring knowledge across existing corpora for the accurate detection of
uncertain language in a wide variety of text types. In our experiments, we used all the
combinations of genres and domains that we found plausible. News texts (and their
subdomains) were not used as source data because FactBank is significantly smaller
than the other corpora (WikiWeasel or scientific texts). As the source data set is typically
larger than the target data set in practical scenarios, news texts can only be used as target
data. Abstracts were only used as source data because information extraction typically
addresses full texts whereas abstracts just provide annotated data for development pur-
poses. Besides these restrictions, we experimented with all possible target and source
pairs.
We used four different machine-learning settings for each target?source pair in our
investigations. In the purely cross-domain (CROSS) setting, the model was trained on
the source domain and evaluated on the target (i.e., no labeled target domain data
sets were used for training). In the purely in-domain setting (TARGET), we performed
9 More on this can be found in Pan and Yang (2010) and at http://nlpers.blogspot.com/2007/11/
domain-adaptation-vs-transfer-learning.html.
356
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Table 5
Experimental results on different target and source domain pairs. The third column contains the
ratio of the target train and source data sets? sizes in terms of sentences. DIST shows the distance
of the source and target domain/genre (?-? same; ?+? fine-grade difference; ?++? coarse-grade
difference; bio = biological; enc = encyclopedia; sci paper = scientific paper; sci abs = scientific
abstract; sci paper hbc = scientific papers on human blood cell experiments; sci paper fly =
scientific papers on Drosophila; sci paper bmc = scientific papers on bioinformatics).
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fcue Fsent Fcue Fsent Fcue Fsent Fcue Fsent
enc sci paper+ abs 0.9 ++/++ 68.0 74.2 82.4 87.4 82.6 87.6 82.6 87.6
news sci paper+ abs 6.2 ++/++ 64.4 70.5 68.7 77.1 72.7 79.5 73.8 81.0
news enc 6.6 ++/++ 68.2 74.8 68.7 77.1 73.7 81.2 73.1 80.0
sci paper enc 2.7 ++/++ 67.8 75.1 78.8 84.4 80.0 85.9 79.8 85.4
sci paper bmc sci abs hbc 4.3 +/+ 58.2 70.5 64.0 74.5 68.1 76.7 69.3 77.8
sci paper fly sci abs hbc 3.4 +/+ 70.5 79.1 80.0 85.1 83.3 88.2 82.9 87.8
sci paper hbc sci abs hbc 8.2 ?/+ 76.5 82.9 74.2 80.2 84.2 88.6 83.0 88.9
sci paper bmc sci paper fly+ hbc 1.8 +/? 69.8 77.6 64.0 74.5 70.0 78.2 69.4 78.1
sci paper fly sci paper bmc+ hbc 1.2 +/? 78.4 83.5 80.0 85.1 82.6 87.0 82.9 87.0
sci paper hbc sci paper bmc+ fly 4.4 +/? 81.7 85.9 74.2 80.2 80.7 86.9 80.7 85.9
AVERAGE: 70.4 77.4 73.5 80.6 77.8 84.0 77.8 84.0
10-fold cross-validation on the target data (i.e., no source domain data were used). In
the two domain adaptation settings, we again performed 10-fold cross-validation on
the target data but exploited the source data set (as described in Section 5.3). Here, we
either used each sentence of the source data set (DA/ALL) or only those sentences that
contained a cue observed in the target train data set (DA/CUE).
Table 5 lists the results obtained on various target and source domains in various
machine learning settings and Table 6 contains the absolute differences between a
particular result and the in-domain (TARGET) results.
Fine-grained semantic uncertainty classification results are summarized in Tables 7
and 8. Table 7 contrasts the coarse-grained Fcue with the unlabeled/binary Fcue of fine-
grained experiments, therefore it quantifies the difference in accuracy due to the more
difficult classification setting and the increased sparseness of the task. Table 8 shows
the per class Fcue scores, namely, how accurately our model recognizes the individual
uncertainty types.
Table 6
The absolute difference between the F-scores of Table 5 relative to the baseline TARGET setting.
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fcue Fsent Fcue Fsent Fcue Fsent Fcue Fsent
enc sci paper+ abs 0.9 ++/++ ?14.4 ?13.2 82.4 87.4 0.2 0.2 0.2 0.2
news sci paper+ abs 6.2 ++/++ ?4.3 ?6.6 68.7 77.1 4.0 2.4 5.1 3.9
news enc 6.6 ++/++ ?0.5 ?2.3 68.7 77.1 5.0 4.1 4.4 2.9
sci paper enc 2.7 ++/++ ?11.0 ?9.3 78.8 84.4 1.2 1.5 1.0 1.0
sci paper bmc sci abs hbc 4.3 +/+ ?5.8 ?4.0 64.0 74.5 4.1 2.2 5.3 3.3
sci paper fly sci abs hbc 3.4 +/+ ?9.5 ?6.0 80.0 85.1 3.3 3.1 2.9 2.7
sci paper hbc sci abs hbc 8.2 ?/+ 2.3 2.7 74.2 80.2 10.0 8.4 8.8 8.7
sci paper bmc sci paper fly+ hbc 1.8 +/? 5.8 3.1 64.0 74.5 6.0 3.7 5.4 3.6
sci paper fly sci paper bmc+ hbc 1.2 +/? ?1.6 ?1.6 80.0 85.1 2.6 1.9 2.9 1.9
sci paper hbc sci paper bmc+ fly 4.4 +/? 7.5 5.7 74.2 80.2 6.5 6.7 6.5 5.7
AVERAGE: ?3.1 ?3.2 73.5 80.6 4.3 3.4 4.3 3.4
357
Computational Linguistics Volume 38, Number 2
Table 7
Comparison of cue-level binary (Fbin) and unlabeled F-scores (Funl). Binary F-score corresponds
to coarse-grained classification (uncertain vs. certain), and unlabeled F-score is the fine-grained
classification converted to binary (disregarding the fine-grained category labels).
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fbin Funl Fbin Funl Fbin Funl Fbin Funl
enc sci paper+ abs 0.9 ++/++ 68.0 67.4 82.4 82.4 82.6 81.9 82.6 81.7
news sci paper+ abs 6.2 ++/++ 64.4 59.9 68.7 66.4 72.7 71.5 73.8 71.8
news enc 6.6 ++/++ 68.2 67.0 68.7 66.4 73.7 73.6 73.1 73.4
sci paper enc 2.7 ++/++ 67.8 67.2 78.8 78.3 80.0 80.2 79.8 79.5
sci paper bmc sci abs hbc 4.3 +/+ 58.2 66.3 64.0 61.9 68.1 68.5 69.3 67.9
sci paper fly sci abs hbc 3.4 +/+ 70.5 78.7 80.0 79.2 83.3 83.4 82.9 83.2
sci paper hbc sci abs hbc 8.2 ?/+ 76.5 83.6 74.2 69.3 84.2 83.1 83.0 83.4
sci paper bmc sci paper fly+ hbc 1.8 +/? 69.8 69.7 64.0 61.9 70.0 69.5 69.4 65.9
sci paper fly sci paper bmc+ hbc 1.2 +/? 78.4 77.7 80.0 79.2 82.6 82.1 82.9 82.5
sci paper hbc sci paper bmc+ fly 4.4 +/? 81.7 81.9 74.2 69.3 80.7 81.3 80.7 81.2
AVERAGE: 70.4 71.9 73.5 71.4 77.8 77.5 77.8 77.0
Table 8
The per class cue-level F-scores in fine-grained classification. Fcrs, Ftgt, and Fda correspond to the
CROSS, TARGET, and DA/CUE settings, respectively (same as previous). The DA/ALL setting
is not shown for space reasons and due to its similarity to the DA/CUE results.
EPISTEMIC INVESTIGATION DOXASTIC CONDITION
TARGET SOURCE Fcrs Ftgt Fda Fcrs Ftgt Fda Fcrs Ftgt Fda Fcrs Ftgt Fda
enc sci paper+ abs 75.9 83.4 82.8 67.3 67.5 70.4 48.8 89.2 88.1 54.4 62.6 61.2
news sci paper+ abs 70.9 65.4 75.2 79.5 75.9 83.1 39.1 68.9 71.3 47.2 57.1 57.5
news enc 65.4 65.4 74.5 74.6 75.9 87.5 76.3 68.9 78.0 50.6 57.1 56.7
sci paper enc 72.9 81.2 81.9 36.5 72.9 72.4 63.6 74.9 79.8 57.0 58.9 59.7
sci paper bmc sci abs hbc 71.5 68.3 72.6 56.1 37.7 58.1 68.1 61.9 69.4 45.5 45.0 49.5
sci paper fly sci abs hbc 82.9 82.1 85.3 69.0 68.6 76.6 75.1 71.7 75.4 28.6 63.4 64.1
sci paper hbc sci abs hbc 87.5 77.7 86.4 76.5 53.5 77.5 80.6 39.0 76.7 26.1 10.0 33.3
sci paper bmc sci paper fly+ hbc 74.4 68.3 69.2 55.9 37.7 57.4 63.7 61.9 64.7 57.3 45.0 50.7
sci paper fly sci paper bmc+ hbc 80.3 82.1 84.3 66.7 68.6 75.8 77.7 71.7 77.3 53.5 63.4 68.0
sci paper hbc sci paper bmc+ fly 85.2 77.7 86.0 74.0 53.5 70.3 75.9 39.0 70.2 58.1 10.0 41.4
AVERAGE: 76.7 75.2 79.8 65.6 61.2 72.9 66.9 64.7 75.1 47.8 47.3 54.2
The size of the target training data sets proved to be an important factor in these
investigations. Hence, we performed experiments with different target data set sizes.
We utilized the DA/ALL model (which is more robust for extremely small target data
sizes [e.g., 100-400 sentences]) and performed the same 10-fold cross validation on the
target data set as in Tables 5-8. For each fold of the cross-validation here, however, we
just used n sentences (x axis of the figures) from the target training data set and a fixed
set of 4,000 source sentences to alleviate the effect of varying data set sizes. Figure 5
depicts the learning curves for two target/source data set pairs.
6. Discussion
As Table 5 shows, incorporating labeled data from different genres and/or domains
consistently improves the performance. The successful applicability of domain adap-
tation tells us that the problem of detecting uncertainty has similar characteristics
across genres and domains. The uncertainty cue lexicons of different domains and
358
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Figure 5
Learning curves: Results achieved with different target train sizes. The left and right figures
show two selected source/target pairs. The upper figures depict coarse-grained classification
results (Fcue); DA, CROSS, and TARGET with the same settings as in Table 5. The lower figures
show the per class Fcue of the DA/ALL model in the fine-grained classification.
genres indeed share a core vocabulary and despite the differences in sense distributions,
labeled data from a different source improves uncertainty classification in a new genre
and domain if the different data sets are annotated consistently. This justifies our aim
to create a consistent representation of uncertainty that can be applied to multiple
domains.
6.1 Domain Adaptation Results
The size of the target and source data sets largely influences to what extent external
data can improve results. The only case where domain adaptation had only a negligible
effect (an F-score gain less than 1%) is where the target data set is itself very large.
This is expected as the more target data one has, the less crucial it is to incorporate
additional data with some undesirable characteristics (difference in style, domain,
certain/uncertain sense distribution, etc.).
The performance scores for the CROSS setting clearly indicate the domain/genre
distance of the data sets: Themore distant the domain and genre of the source and target
data sets are, the more the CROSS performance (where no labeled target data is used)
degrades, compared with the TARGET model. In general, when the distance between
both the domain and the genre of texts is substantial (++/++ and +/+ rows in Tables 5
359
Computational Linguistics Volume 38, Number 2
and 6), this accounts for a 6?10% decrease in both the sentence and cue-level F-scores.
An exception is the case of encyclopedic source and news target domains. Here the
performance is very close to the target domain performance. This indicates that these
settings are not so different from each other as it might seem at the first glance. The
encyclopedic and news genres share quite a lot of commonalities (compare cue distribu-
tions in Figure 4, for instance). We verified this observation by using a knowledge-poor
quantitative estimator of similarity between domains (Van Asch and Daelemans 2010):
Using cosine as the similarity measure, the newswire and encyclopedia texts are found
to be the second most similar domain pair in our experiments, with a score comparable
to those obtained for the pairs of scientific article types bmc, hbc, and fly.
When there is a domain or genre match between source and target (?/+ and +/?
rows in Tables 5 and 6), however, and the distance regarding the other is just moderate,
the cross-training performance is close to or even better than the target-only results.
That is, the larger amount of source training data balances the differences between the
domains. These results indicate that the learned uncertainty classifiers can be directly
applied to slightly different data sets. This suitability is due to the learned disambigua-
tion models, which generalize well in similar settings. This is contrary to the findings
of earlier studies, which built the uncertainty detectors using seed examples and boot-
strapping. These models were not designed to learn any disambiguation models for
the cue words found, and their performance degraded even for slightly different data
(Szarvas 2008).
Comparing the two domain adaptation procedures DA/CUE and DA/ALL, adap-
tation via transferring only source sentences that contain a target domain cue is, on
average, comparable to transferring all the data from the source domain. In other words,
when we have a small but sufficient amount of target data available, it is enough to
account for source data corresponding to the uncertainty cues we saw in the limited
target data set. This observation has several consequences, namely:
 The source-only cues, or to be more precise, their disambiguation models,
are not helpful for the target domains as they cannot be adapted. This is
due to the differences in the source and target disambiguation models.
 Similarly, domain adaptation improves the disambiguation models for the
observed target cues, rather than introducing new vocabulary into the
target domain. This mechanism coincides with our initial goal of using
domain adaptation to learn better semantic models. This effect is the
opposite of how bootstrapping-based weakly supervised approaches
improve the performance in an underresourced domain. This observation
suggests a promising future direction of combining the two approaches to
maximize the gains while minimizing the annotation costs.
 In a general context, we can effectively extend the data for a given domain
if we have robust knowledge of the potential uncertainty vocabulary for
that domain. Given the wide variety of the domains and genres of our data
sets, it is reasonable to suppose that they represent uncertain language in
general quite well, and the joint vocabularies provide a good starting point
for a targeted data development for further domains.
As regards the fine-grained classification results, Table 7 demonstrates that the
fine-grained distinction results in only a small, or no, loss in performance. The coarse-
grained model is slightly more accurate than the fine-grained model (counting correctly
360
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
recognized but misclassified cues as true positives) in most settings. The most signif-
icant difference is observed for the target-only settings, where no out-of-domain data
are used for the training and thus the data sets are accordingly smaller. A noticeable
exception is when scientific abstracts are used for cross training: In those settings the
coarse-grained model performs poorly, due to its lower recall, which we attribute to
overfitting the special characteristics of abstracts. The fact that in fine-grained classi-
fication the CROSS results consistently outperform the TARGET models (see Table 8)
even for distant domain pairs, also underlines that the increased sparseness caused by
the differentiation of the various subtypes of uncertainty is an important factor only for
smaller data sets. The improvement by domain adaptation is clearly more prominent
in fine-grained than in coarse-grained classification, however: The individual cue types
benefit by 5?10% points in terms of the F-score from out-of-domain data and domain
adaptation. Moreover, as Table 8 shows, for the domain pairs and fine-grained classes
where a nice amount of positive examples are at hand, the per class Fcue scores are
also around 80% and above. This means that it is possible to accurately identify the
individual subtypes of semantic uncertainty, and thus it also proves the feasibility of
the subcategorization and annotation scheme proposed in this study (Section 2). Other
important observations here are that domain adaptation is even more significant in the
more difficult fine-grained classification setting, and that the condition class represents
a challenge for our model. The performance for the condition class is lower than that
for the other classes, which can only in part be attributed to the fact that this is the
least represented subtype in our data sets: as opposed to other cue types, condition cues
are typically used in many different contexts and they may belong to other uncertainty
classes as well.
6.2 The Required Amount of Annotation
Based on our experiments, we may conclude that a manually annotated training data
set consisting of 3,000?5,000 sentences is sufficient for training an accurate cue detector
for a new genre/domain. The results of our learning curve experiments (Figure 5)
illustrate the situations where only a limited amount of annotated data (fewer than 3,000
sentences) is available for the target domain. The feasibility of decreasing annotation
efforts and the real added value of domain adaptation are more prominent in this range.
It is easy to see that the TARGET results approach to DA results with more target data.
Figure 5 shows that the size of the target training data set where the supervised
TARGET setting outperforms the CROSS model (trained on 4,000 source sentences) is
around 1,000 sentences. Aswementioned earlier, even distant domain data can improve
the cue recognition model in the absence of a sufficient target data set. Figure 5 justifies
this observation, as the CROSS and DA settings outperform the TARGET setting on
each source?target data set pair. It can also be observed that the doxastic type is more
domain-dependent than the others and its results consistently improve by increasing
the size of the target domain annotation (which coincides with the cue frequency
investigations of Section 5.1.3). In the news target domain, however, the investigation
and epistemic classes benefit a lot from a small amount of annotated target data but their
performance scores increase just slightly after that. This indicates that most of the im-
portant domain-dependent (probably lexical) knowledge could be gathered from 100?
400 sentences. In the biological experiments, we may conclude that the investigation
class is already covered by the source domain (intuitively, the investigation cues are well
represented in the abstracts) and its results are not improved significantly by usingmore
361
Computational Linguistics Volume 38, Number 2
target data. The condition class is underrepresented in both the source and target data
sets and hence no reliable observations can bemade regarding this subclass (see Table 2).
Overall, if we would like to have an uncertainty cue detector for a new
genre/domain: (i) We can achieve performance around 60?70% by using cross training
depending on the difference between the domains (i.e., without any annotation effort);
(ii) By annotating around 3,000 sentences, we can have a performance of 70?80%,
depending on the level of difficulty of the texts; (iii) We can get the same 70?80% results
with annotating just 1,000 sentences and using domain adaptation.
6.3 Interesting Examples and Error Analysis
As might be expected, most of the erroneous cue predictions were due to vocabulary
differences, for example, fear or accuse occurred only in news texts, which is why they
were not recognized by models trained on biological or encyclopedia texts. Another
example is the case of or, which is a frequent cue in biological texts. Still, it is rarely
used as a cue in other domains but without domain adaptation, the model trained on
biological texts marks quite a few occurrences of or as cues in the news or encyclope-
dia domains. Many of these anomalies were eliminated by the application of domain
adaptation techniques, however.
Many errors were related to multi-class cues. These cues are especially hard to
disambiguate because not only can they refer to several classes of uncertainty, but
they typically have non-cue usage as well. For instance, the case of would is rather
complicated because it can fulfill several functions:
(10) EPISTEMIC USAGE (?IT IS HIGHLY PROBABLE?): Further biochemical studies
on the mechanism of action of purified kinesin-5 from multiple systems
would obviously be fruitful. (Corpus: fly)
(11) CONDITIONAL: ?If religion was a thing that money could buy,/The rich
would live and the poor would die.? (Corpus: WikiWeasel)
(12) FUTURE IN THE PAST: This Aarup can trace its history back to 1500, but it
would be 1860?s before it would become a town. (Corpus: WikiWeasel)
(13) REPEATED ACTION IN THE PAST (?USED TO?): ?Becker? was the next T.V.
Series for Paramount that Farrell would co-star in. (Corpus: WikiWeasel)
(14) DYNAMIC MODALITY: Individuals would first have a small lesion at the
site of the insect bite, which would eventually leave a small scar. (Corpus:
WikiWeasel)
(15) PRAGMATIC USAGE: Although some would dispute the fact, the joke
related to a peculiar smell that follows his person. (Corpus: WikiWeasel)
The epistemic uses of would are annotated as epistemic cues whereas its occurrences in
conditionals are marked as hypothetical cues. The habitual past meaning is not related
to uncertainty, hence it is not annotated. The future in the past meaning (i.e., past tense
of will), however, denotes an event of which it is known that happened later, so it is
certain. The dynamically modal would is similar to the future will (which is an instance
of dynamic modality as well), but it is not annotated in the corpora. The pragmatic
362
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
use of would does not refer to semantic uncertainty (the semantic value of the sentence
would be exactly the same without it or if it is replaced with may, might, will, etc., that
is, some will/may/might/? dispute the fact mean the same). It is rather a stylistic issue
to further express uncertainty at the discourse level (i.e., there are some unidentified
people who dispute the fact, hence the opinion cannot be associated with any definite
source).
The last two uses of would are not typically described in grammars of English
and seem to be characteristic primarily of the news and encyclopedia domains. Thus
it is advisable to explore such cases and treat them with special consideration when
adapting an algorithm trained and tested in a specific domain to another domain.
Another interesting example is may in its non-cue usage. Being (one of) the most
frequent cues in each subcorpus, its non-cue usage is rather limited but can be found
occasionally in FactBank and WikiWeasel. The following instance of may in FactBank
was correctly marked as non-cue by the cue detector when trained on Wikipedia texts.
On the other hand, it was marked as a cue when trained on biological texts since in this
case, there were insufficient training examples of may not being a cue:
(16) ?Wellmay we say ?God save the Queen,? for nothing will save the
republic,? outraged monarchist delegate David Mitchell said. (Corpus:
FactBank)
A final example to be discussed is concern. This word also has several uses:
(17) NOUN MEANING ?COMPANY?: The insurance concern said all conversion
rights on the stock will terminate on Nov. 30. (Corpus: FactBank)
(18) NOUN MEANING ?WORRY?: Concern about declines in other markets,
especially New York, caused selling pressure. (Corpus: FactBank)
(19) PREPOSITION: The company also said it continues to explore all options
concerning the possible sale of National Aluminum?s 54.5% stake in an
aluminum smelter in Hawesville, Ky. (Corpus: FactBank)
(20) VERB: Many of the predictions in these two data sets concern protein pairs
and proteins that are not present in other data sets. (Corpus: bmc)
Among these examples, only the second one should be annotated as uncertain. POS-
tagging seems to provide enough information for excluding the verbal and preposi-
tional uses of the word but in the case of nominal usage, additional information is also
required to enable the system to decide whether it is an uncertainty cue or not (in this
case, the noun in the ?company? sense cannot have an argument while in the ?worry?
sense, it can have [about declines]). Again, the frequency of the two senses depends
heavily on the domain of the texts, which should also be considered when adapting
the cue detector to a different domain. We should mention that the role of POS-tagging
is essential in cue detection because many ambiguities can be resolved on the basis of
POS-tags. Hence, POS-tagging errors can lead to a serious decline in performance.
We think that an analysis of similar examples can further support domain adapta-
tion and cue detection across genres and domains.
363
Computational Linguistics Volume 38, Number 2
7. Conclusions and Future Work
In this article, we introduced an uncertainty cue detection model that can perform well
across different domains and genres. Even though several types of uncertainty exist,
available corpora and resources focus only on some of the possible types and thereby
only cover particular aspects of the phenomenon. This means that uncertainty models
found in the literature are heterogeneous, and the results of experiments on different
corpora are hardly comparable. These facts motivated us to offer a unified model of
semantic uncertainty enhanced by linguistic and computer science considerations. In
accordance with this classification, we reannotated three corpora from several domains
and genres using our uniform annotation guidelines.
Our results suggest that simple cross training can be employed and it achieves a
reasonable performance (60?70% cue-level F-score) when no annotated data is at hand
for a new domain. When some annotated data is available (here somemeans fewer than
3,000 annotated sentences for the target domain), domain adaptation techniques are
the best choice: (i) they lead to a significant improvement compared to simple cross
training, and (ii) they can provide a reasonable performance with significantly less
annotation. In our experiments, the annotation of 3,000 sentences and training only on
these is roughly equivalent to the annotation of 1,000 sentences using external data and
domain adaptation. If the size of the training data set is sufficiently large (larger than
5,000 sentences) the effect of incorporating additional data?having some undesirable
characteristics?is not crucial.
Comparing different domain adaptation techniques, we found that similar results
could be attained when the source domain was filtered for sentences that contained
cues in the target domain. This tells us that models learn to better disambiguate the
cues seen in the target domain instead of finding new, unseen cues. In this sense, this
approach can be regarded as a complementarymethod toweakly supervised techniques
for lexicon extraction. A promising way to further minimize annotation costs while
maximizing performance would be the integration of the two approaches, which we
plan to investigate in the near future.
In our study, we did not pay attention to dynamic modality (due to the lack of an-
notated resources), but the detection of such phenomena is also desirable. For instance,
dynamically modal events cannot be treated as certain?that is, the event of buying
cannot be assigned the same truth value in They agreed to buy the company and They
bought the company. Whereas the second sentence expresses a fact, the first one informs
us about the intention of buying the company, which will be certainly carried out in a
world where moral or business laws are observed but at the moment it cannot be stated
whether the transaction takes place (i.e., that it is certain). Hence, in the future, we also
intend to integrate the identification of dynamically modal cues into our uncertainty
cue detector.
Acknowledgments
This work was supported in part by
the NIH grants (project codenames
MASZEKER and BELAMI) of the
Hungarian government, by the German
Ministry of Education and Research
under grant SiDiM (grant no. 01IS10054G),
and by the Volkswagen Foundation as
part of the Lichtenberg-Professorship
Program (grant no. I/82806). Richa?rd
Farkas was funded by the Deutsche
Forschungsgemeinschaft grant SFB 732.
References
Baker, Kathy, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Ed Hovy, Lori Levin,
Marjorie McShane, Teruko Mitamura,
364
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Sergei Nirenburg, Christine Piatko,
Owen Rambow, and Gramm Richardson.
2010. Modality Annotation Guidelines.
Technical Report 4, Human Language
Technology Center of Excellence,
Baltimore, MD.
Chapman, Wendy W., David Chu, and
John N. Dowling. 2007. ConText: An
algorithm for identifying contextual
features from clinical text. In Proceedings
of the ACL Workshop on BioNLP 2007,
pages 81?88, Prague, Czech Republic.
Clausen, David. 2010. HedgeHunter:
A system for hedge detection and
uncertainty classification. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010):
Shared Task, pages 120?125, Uppsala.
Conway, Mike, Son Doan, and Nigel Collier.
2009. Using hedges to enhance a disease
outbreak report text mining system. In
Proceedings of the BioNLP 2009 Workshop,
pages 142?143, Boulder, CO.
Curran, James, Stephen Clark, and Johan
Bos. 2007. Linguistically motivated
large-scale NLP with C&C and Boxer. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
Companion Volume Proceedings of the Demo
and Poster Sessions, pages 33?36, Prague.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 256?263,
Prague.
Daume? III, Hal and Daniel Marcu. 2006.
Domain adaptation for statistical
classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
Falahati, Reza. 2006. The use of hedging
across different disciplines and rhetorical
sections of research articles. In Proceedings
of the 22nd NorthWest Linguistics Conference
(NWLC22), pages 99?112, Burnaby.
Farkas, Richa?rd and Gyo?rgy Szarvas. 2008.
Automatic construction of rule-based
ICD-9-CM coding systems. BMC
Bioinformatics, 9:1?9.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 Shared Task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010):
Shared Task, pages 1?12, Uppsala.
Fernandes, Eraldo R., Carlos E. M. Crestana,
and Ruy L. Milidiu?. 2010. Hedge detection
using the RelHunter approach. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 64?69,
Uppsala.
Friedman, Carol, Philip O. Alderson,
John H. M. Austin, James J. Cimino, and
Stephen B. Johnson. 1994. A General
natural-language text processor for clinical
radiology. Journal of the American Medical
Informatics Association, 1(2):161?174.
Ganter, Viola and Michael Strube. 2009.
Finding hedges by chasing weasels: Hedge
detection using Wikipedia tags and
shallow linguistic features. In Proceedings
of the ACL-IJCNLP 2009 Conference Short
Papers, pages 173?176, Suntec.
Georgescul, Maria. 2010. A hedgehop over a
max-margin framework using hedge cues.
In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 26?31,
Uppsala.
Hyland, Ken. 1994. Hedging in academic
writing and EAP textbooks. English for
Specific Purposes, 13(3):239?256.
Hyland, Ken. 1996. Writing without
conviction? Hedging in scientific
research articles. Applied Linguistics,
17(4):433?454.
Hyland, Ken. 1998. Boosters, hedging and
the negotiation of academic knowledge.
Text, 18(3):349?382.
Kiefer, Ferenc. 2005. Leheto?se?g e?s
szu?kse?gszeru?se?g [Possibility and
necessity]. Tinta Kiado?, Budapest.
Kilicoglu, Halil and Sabine Bergler.
2008. Recognizing speculative
language in biomedical research articles:
A linguistically motivated perspective.
In Proceedings of the Workshop on Current
Trends in Biomedical Natural Language
Processing, pages 46?53, Columbus, OH.
Kilicoglu, Halil and Sabine Bergler. 2009.
Syntactic dependency based heuristics for
biological event extraction. In Proceedings
of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 119?127,
Boulder, CO.
Kim, Jin-Dong, Tomoko Ohta, Sampo
Pyysalo, Yoshinobu Kano, and Jun?ichi
Tsujii. 2009. Overview of BioNLP?09
Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task,
pages 1?9, Boulder, OH.
Kim, Jin-Dong, Tomoko Ohta, and Jun?ichi
Tsujii. 2008. Corpus annotation for mining
biomedical events from literature. BMC
Bioinformatics, 9(Suppl 10).
365
Computational Linguistics Volume 38, Number 2
Li, Xinxin, Jianping Shen, Xiang Gao, and
Xuan Wang. 2010. Exploiting rich features
for detecting hedges and their scope. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 78?83,
Uppsala.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings
of the HLT-NAACL 2004 Workshop:
Biolink 2004, Linking Biological Literature,
Ontologies and Databases, pages 17?24,
Boston, Massachusetts, USA.
MacKinlay, Andrew, David Martinez, and
Timothy Baldwin. 2009. Biomedical event
annotation with CRFs and precision
grammars. In Proceedings of the Workshop
on Current Trends in Biomedical Natural
Language Processing: Shared Task,
BioNLP ?09, pages 77?85, Uppsala.
McCallum, Andrew Kachites. 2002.
MALLET: A Machine Learning for
Language Toolkit. Available at
http://mallet.cs.umass.edu.
Medlock, Ben and Ted Briscoe. 2007.
Weakly supervised learning for hedge
classification in scientific literature. In
Proceedings of the ACL, pages 992?999,
Prague.
Morante, Roser and Walter Daelemans.
2009. Learning the scope of hedge cues
in biomedical texts. In Proceedings of the
BioNLP 2009 Workshop, pages 28?36,
Boulder, CO.
Morante, Roser and Walter Daelemans. 2011.
Annotating modality and negation for a
machine reading evaluation. In Proceedings
of CLEF 2011, Amsterdam, Netherlands.
Morante, Roser, Vincent Van Asch, and
Walter Daelemans. 2010. Memory-based
resolution of in-sentence scopes of hedge
cues. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010): Shared
Task, pages 40?47, Uppsala, Sweden.
Nawaz, Raheel, Paul Thompson, and
Sophia Ananiadou. 2010. Evaluating a
meta-knowledge annotation scheme for
bio-events. In Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, pages 69?77, Uppsala.
O?zgu?r, Arzucan and Dragomir R. Radev.
2009. Detecting speculations and their
scopes in scientific text. In Proceedings
of the 2009 Conference on Empirical
Methods in Natural Language Processing,
pages 1398?1407, Singapore.
Palmer, Frank Robert. 1979.Modality and
the English Modals. Longman, London.
Palmer, Frank Robert. 1986.Mood and
Modality. Cambridge University Press,
Cambridge.
Pan, Sinno Jialin and Qiang Yang. 2010.
A survey on transfer learning. IEEE
Transactions on Knowledge and Data
Engineering, 22(10):1345?1359.
Rei, Marek and Ted Briscoe. 2010. Combining
manual rules and supervised learning
for hedge cue and scope detection. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 56?63,
Uppsala.
Rizomilioti, Vassiliki. 2006. Exploring
epistemic modality in academic discourse
using corpora. In Elisabet Arno? Macia,
Antonia Soler Cervera, and Carmen Rueda
Ramos, editors, Information Technology in
Languages for Specific Purposes, volume 7
of Educational Linguistics. Springer US,
New York, pages 53?71.
Rubin, Victoria L. 2010. Epistemic modality:
From uncertainty to certainty in the
context of information seeking as
interactions with texts. Information
Processing & Management, 46(5):533?540.
Rubin, Victoria L., Elizabeth D. Liddy,
and Noriko Kando. 2005. Certainty
identification in texts: Categorization
model and manual tagging results. In
James G. Shanahan, Yan Qu, and Janyce
Wiebe, editors, Computing Attitude and
Affect in Text: Theory and Applications (the
Information Retrieval Series), Springer
Verlag, New York, pages 61?76.
Russell, Stuart J. and Peter Norvig. 2010.
Artificial Intelligence?AModern Approach
(3rd international edition). Upper Saddle
River, NJ: Pearson Education.
Sa?nchez, Liliana Mamani, Baoli Li, and
Carl Vogel. 2010. Exploiting CCG
structures with tree kernels for speculation
detection. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010): Shared
Task, pages 126?131, Uppsala.
Saur??, Roser. 2008. A Factuality Profiler
for Eventualities in Text. Ph.D. thesis,
Brandeis University, Waltham, MA.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources and
Evaluation, 43:227?268.
Settles, Burr, Mark Craven, and Lewis
Friedland. 2008. Active learning with
real annotation costs. In Proceedings of
366
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
the NIPS Workshop on Cost-Sensitive
Learning, pages 1?10, Vancouver, Canada.
Shatkay, Hagit, Fengxia Pan, Andrey
Rzhetsky, and W. John Wilbur. 2008.
Multi-dimensional classification of
biomedical text: Toward automated,
practical provision of high-utility
text to diverse users. Bioinformatics,
24(18):2086?2093.
Sun, Chengjie, Lei Lin, Xiaolong Wang,
and Yi Guan. 2007. Using maximum
entropy model to extract protein-protein
interaction information from biomedical
literature. In De-Shuang Huang, Donald C.
Wunsch, Daniel S. Levine, and Kang-Hyun
Jo, editors, Advanced Intelligent Computing
Theories and Applications. With Aspects
of Theoretical and Methodological Issues.
Springer Verlag, Heidelberg,
pages 730?737.
Szarvas, Gyo?rgy. 2008. Hedge classification
in biomedical texts with a weakly
supervised selection of keywords.
In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, OH.
Ta?ckstro?m, Oscar, Sumithra Velupillai,
Martin Hassel, Gunnar Eriksson,
Hercules Dalianis, and Jussi Karlgren.
2010. Uncertainty detection as
approximate max-margin sequence
labelling. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010):
Shared Task, pages 84?91, Uppsala.
Tang, Buzhou, Xiaolong Wang, Xuan Wang,
Bo Yuan, and Shixi Fan. 2010. A cascade
method for detecting hedges and their
scope in natural language text. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 13?17,
Uppsala.
Thompson, Paul, Giulia Venturi, John
McNaught, Simonetta Montemagni, and
Sophia Ananiadou. 2008. Categorising
modality in biomedical texts. In Proceedings
of the LREC 2008 Workshop on Building and
Evaluating Resources for Biomedical Text
Mining, pages 27?34, Marrakech, Morocco.
Tjong Kim Sang, Erik. 2010. A baseline
approach for detecting sentences
containing uncertainty. In Proceedings
of the Fourteenth Conference on
Computational Natural Language
Learning (CoNLL-2010): Shared Task,
pages 148?150, Uppsala.
Uzuner, O?zlem, Xiaoran Zhang, and
Tawanda Sibanda. 2009. Machine
learning and rule-based approaches to
assertion classification. Journal of the
American Medical Informatics Association,
16(1):109?115.
Van Asch, Vincent and Walter Daelemans.
2010. Using domain similarity for
performance estimation. In Proceedings of
the 2010 Workshop on Domain Adaptation for
Natural Language Processing, pages 31?36,
Uppsala.
Van Landeghem, Sofie, Yvan Saeys,
Bernard De Baets, and Yves Van de Peer.
2009. Analyzing text in search of
bio-molecular events: A high-precision
machine learning framework. In
Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task,
pages 128?136, Boulder, CO.
Velldal, Erik. 2010. Detecting uncertainty
in biomedical literature: A simple
disambiguation approach using sparse
random indexing. In Proceedings of SMBM
2010, pages 75?83, Cambridge.
Velldal, Erik, Lilja ?vrelid, and Stephan
Oepen. 2010. Resolving speculation:
MaxEnt cue classification and
dependency-based scope rules.
In Proceedings of the Fourteenth Conference
on Computational Natural Language
Learning (CoNLL-2010): Shared Task,
pages 48?55, Uppsala.
Vincze, Veronika, Gyo?rgy Szarvas, Richa?rd
Farkas, Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008. The BioScope Corpus: Biomedical
texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics,
9(Suppl 11):S9.
Wilson, Theresa Ann. 2008. Fine-grained
Subjectivity and Sentiment Analysis:
Recognizing the Intensity, Polarity, and
Attitudes of Private States. Ph.D. thesis,
University of Pittsburgh, PA.
Zhang, Shaodian, Hai Zhao, Guodong Zhou,
and Bao-Liang Lu. 2010. Hedge detection
and scope finding by sequence labeling
with normalized feature selection. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 92?99,
Uppsala.
367
Proceedings of NAACL-HLT 2013, pages 1131?1141,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supervised All-Words Lexical Substitution using Delexicalized Features
Gyo?rgy Szarvas1 Chris Biemann2 Iryna Gurevych3,4
(1) Nuance Communications Deutschland GmbH
Kackertstrasse 10, D-52072 Aachen, Germany
(2) FG Language Technology
Department of Computer Science, Technische Universita?t Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
(4) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.nuance.com , http://www.ukp.tu-darmstadt.de
Abstract
We propose a supervised lexical substitu-
tion system that does not use separate clas-
sifiers per word and is therefore applicable
to any word in the vocabulary. Instead of
learning word-specific substitution patterns, a
global model for lexical substitution is trained
on delexicalized (i.e., non lexical) features,
which allows to exploit the power of super-
vised methods while being able to general-
ize beyond target words in the training set.
This way, our approach remains technically
straightforward, provides better performance
and similar coverage in comparison to unsu-
pervised approaches. Using features from lex-
ical resources, as well as a variety of features
computed from large corpora (n-gram counts,
distributional similarity) and a ranking method
based on the posterior probabilities obtained
from a Maximum Entropy classifier, we im-
prove over the state of the art in the LexSub
Best-Precision metric and the Generalized Av-
erage Precision measure. Robustness of our
approach is demonstrated by evaluating it suc-
cessfully on two different datasets.
1 Introduction
In recent years, the task of automatically providing
lexical substitutions in context (McCarthy and Nav-
igli, 2007) received much attention. The premise
to be able to replace words in a sentence with-
out changing its meaning gave rise to applications
like linguistic steganography (Topkara et al, 2006;
Chang and Clark, 2010), semantic text similarity
(Agirre et al, 2012), and plagiarism detection (Gipp
et al, 2011).
Lexical substitution, a special form of contex-
tual paraphrasing where only a single word is re-
placed, is closely related to word sense disambigua-
tion (WSD): polysemous words have possible sub-
stitutions reflecting several senses, and the correct
sense has to be picked to avoid spurious system be-
havior. However, no explicit word sense inventory is
required for lexical substitution (Dagan et al, 2006).
The prominent tasks in a lexical substitution sys-
tem are generation and ranking, i.e. to generate a set
of possible substitutions for the target word and then
to rank this set of possible substitutions according to
their contextual fitness. The task to generate a high
quality set of possible substitutions is challenging in
itself, for two reasons. First, the available lexical
resources are seldom complete in listing synonyms.
Second, manually annotated substitutions show that
not all synonyms of a word are appropriate in a given
context, and many good substitutions have other lex-
ical relation than synonymy to the original word.
In this work, we present a supervised lexical sub-
stitution system that, unlike the usual lexical sam-
ple supervised approaches, can produce substitu-
tions for targets that are not contained in the train-
ing material. We reach this by using non-lexical
features from heterogeneous evidence, including
lexical-semantic resources and distributional simi-
larity, n-gram and shallow syntactic features based
on large, unannotated background corpora. In light
of the existence of lexical resources such as Word-
Net (Fellbaum, 1998) or machine readable dictio-
naries that can serve as the source for lexical infor-
mation, and with the ever-increasing availability of
large unannotated corpora for many languages and
1131
domains, our proposal enables us to leverage the
quality gain of supervised machine learning while
generalizing over a large vocabulary through the
avoidance of lexicalized features. Using a single
classifier for all substitution targets in this way re-
sults in an all-words substitution system. As our re-
sults demonstrate, our model improves over the state
of the art in lexical substitution with practically no
open parameters that have to be optimized and se-
lected carefully according to the dataset at hand.
2 Related Work
Previous works in lexical substitution either ad-
dress both the generation and the ranking tasks, and
are therefore applicable to any word without pre-
labeled data (c.f. the Semeval 2007 task (McCarthy
and Navigli, 2007) and related work) or focus on
the more challenging ranking step only (c.f. Erk
and Pado? (2008) and related work). The latter ap-
proaches take the list of possible substitutions di-
rectly from the testing data as a workaround to gen-
erating the possible substitutions, and merely evalu-
ate the ranking capabilities of these methods.
The most accurate lexical substitution systems
use supervised machine learning to train (and test)
a separate classifier per target word, using lexical
and shallow syntactic features. These systems rely
on the existence of a large number of annotated
examples (i.e. sentences together with the con-
textually valid substitutions) for each word. Bie-
mann (2012) describes a supervised lexical sub-
stitution system for frequent nouns. Exploiting a
large amount of sense tagged examples and (sense-
specific) data annotated with substitutions, an ac-
curate coarse-grained WSD model is trained and
then the most frequent substitutions of the predicted
sense are assigned to the new occurrences of the tar-
get words. The results demonstrate that lexical sub-
stitution of noun targets can be attained with very
high precision (over 90%) if sufficient training ma-
terial is available. However, due to high annotation
costs, methods that do not require labeled training
data per target scale better to a large vocabulary.
Knowledge-based systems like e.g. by Hassan et
al. (2007), who use a number of knowledge-based
and unsupervised methods and combine these clues
using a voting scheme, do not need training data per
target. The combination of different signals, how-
ever, has to be done manually. Unsupervised sys-
tems that rely on distributional similarity (Thater et
al., 2011) or topic models (Li et al, 2010) are single
signals in this sense, and their development is guided
by the performance and observations on standard
datasets. Such signals, however, can also be kept
simple avoiding any task-specific optimization and
can be integrated in a single model for all words us-
ing a limited amount of training data and delexical-
ized features, as in Senselearner (Mihalcea and Cso-
mai, 2005) for weakly supervised all-words disam-
biguation. This way, task specific development can
be replaced by a machine learning component and
the resulting model applies also to unseen words,
similar to the knowledge-based approaches.
2.1 Full Lexical Substitution Systems
Related works that address the lexical substitution
problem according to the settings established by the
English Lexical Substitution Task (McCarthy and
Navigli, 2007) at Semeval 2007 (LexSub) typically
employ a simple ranking strategy based on local
n-gram frequencies and focus on finding an opti-
mal source of possible substitutions, as the selec-
tion of lexical resources has largest impact on the
overall system performance: Sinha and Mihalcea
(2009) systematically explored the benefits of mul-
tiple lexical resources and found that a supervised
combination of several resources lead to statisti-
cally significant improvements in accuracy (about
3.5% points over the best single resource, WordNet).
They tested LSA (Deerwester et al, 1990), ESA
(Gabrilovich and Markovitch, 2007) and n-gram fre-
quencies for contextualization and found n-gram fre-
quencies to be more effective than dimensionality
reduction techniques by a large margin. Their im-
provements were obtained by supervised learning on
the combination of several lexical resources. Our
work, on the other hand, is concerned with using
more advanced features and we obtain significant
improvements based on a diverse set of features and
a different learning setup: we train a model for con-
textualization, rather than to combine substitutions
from several different resources.
A recent work by Sinha and Mihalcea (2011) used
an approach based on graph centrality to rank the
candidates and achieved comparable performance
1132
to n-gram-frequency-based ranking. To summarize,
the use of n-gram frequencies for ranking and Word-
Net as the (most appropriate single) source of syn-
onyms is competitive to more complex solutions and
provides a simple and strong lexical substitution sys-
tem. This motivated the follow-up work by Chang
and Clark (2010) to use WordNet and n-grams in a
linguistic steganography application and this moti-
vates us to use this method as our baseline.
2.2 Ranking Word Meaning in Context
Another prominent line of related work focused
solely on the accurate ranking of a pre-given set of
possible synonyms, according to their plausibility as
a substitution in a given context. Typically, lexi-
cal substitution data is used for evaluation purposes,
taking the candidate substitutions directly from the
test data. This choice is motivated by the assump-
tion that better semantic models should rank near-
synonyms more accurately according to how they fit
in the original word?s context.
Erk and Pado? (2008) proposed the use of multiple
vector representations of words, where the basic rep-
resentation corresponds to a standard co-occurrence
vector, while further vectors are used to characterize
words according to their inverse selectional prefer-
ence statistics for typical dependency relations. The
representation of a word in its context is computed
via combining the basic representation of a word
with the inverse selectional preference vectors of its
related words from the context. Ranking is done by
comparing vectors of possible substitutions with the
substitution target. Thater et al (2010) took a sim-
ilar approach but used second order co-occurrence
vectors and report improved performance.
An exemplar-based approach is presented by Erk
and Pado? (2010) and Reisinger and Mooney (2010b)
to model word meaning with respect to its context:
instead of representing the word and the context as
separate vectors and combining them, a set of word
occurrences in similar contexts is picked first, and
then only these exemplars are used to represent the
word in context. While this approach provides good
results with relatively simple and transparent mod-
els, each occurrence of a word has a unique repre-
sentation (that can only be computed at testing time),
and it is computationally expensive to scale these
models to a large number of examples.
Dinu and Lapata (2010) used a bag of words la-
tent variable model to characterize the meaning of a
word as a distribution over a set of latent variables
(that is, probabilistic senses). Contextualized repre-
sentation of word meaning is then attained by con-
ditioning the model on the context words in which
the target word occurs. A similar approach has
been evaluated for word similarity (Reisinger and
Mooney, 2010a) and word sense disambiguation (Li
et al, 2010).
Although our main goal here is to develop a full-
fledged lexical substitution system, we mainly fo-
cus on the construction of better ranking models
based on supervised machine learning and delexi-
calized features that scale well for unseen words.
This approach has similar properties (applicability
to all words without word-specific training data) to
the knowledge-based and unsupervised models de-
scribed above, so we will also refer to these systems
for comparison.
3 Datasets
In our work, we use two major freely available
datasets that contain human-annotated substitutions
for single words in their full-sentence context.
3.1 LexSub dataset
This dataset was introduced in the Lexical Substi-
tution task at Semeval 20071. It consists of 2002
sentences for a total of 201 words (10 sentences
per word, but 8 sentences does not have gold stan-
dard labels). Each sentence was assigned to 5 na-
tive speaker annotators, who entered as many para-
phrases or substitutions as they found appropriate
for the word in context. Paraphrases are assigned a
weight (or frequency) that denotes how many anno-
tators suggested that particular word as a substitute.
3.2 TWSI
A similar, but larger dataset is the Turk Bootstrap
Word Sense Inventory (TWSI2, (Biemann, 2012)).
The data was collected through a three-step crowd-
sourcing process and comprises 24,647 sentences
1download at http://nlp.cs.swarthmore.edu/
semeval/tasks/task10/data.shtml
2http://www.ukp.tu-darmstadt.de/data/
lexical-resources/twsi-lexical-substitutions/
1133
for a total of 1,012 target nouns, where crowdwork-
ers have provided substitutions for a target word in
context. We did not use the roughly 150,000 sense-
labeled contexts and the sense inventory of this re-
source, i.e. this dataset ? as used in this study ? is
transparent to the LexSub data. For the majority of
the data, responses from 3 annotators were collected
per context, and there are on average 24 sentences
per target word in the dataset. Due to this, the aver-
age weight of good substitutions is somewhat lower
than in the LexSub dataset (1.27 vs. 1.58 in Lex-
Sub), but the average number of unique substitutions
per target word is slightly higher in TWSI (average
of 22 words / target vs. 17 in LexSub).
3.3 Source of Possible Substitutions
In our lexical substitution system, we used WordNet
as the source for candidate synonyms. For each sub-
stitution target, we took all synonyms from all of the
word?s WordNet synsets as candidates, together with
the words from synsets in similar to, entailment and
also see relation to these synsets3. In order to evalu-
ate and compare our ranking methodology in a trans-
parent way with those studies that focused just on
the candidate ranking task, we also performed exper-
iments where we pooled the set of candidates from
the gold standard dataset. This setting ensures that
each set contains a positive candidate, and that all
human-suggested paraphrases are available as posi-
tive examples for a given sentence.
The main characteristics of the datasets (with both
WordNet or the gold standard as the source of candi-
date substitutions) are summarized in Table 1. The
rows in the table indicate the source of possible sub-
stitutions, number of target words, instances with at
least one non-multiword possible substitution, aver-
age size of candidate sets, and number of instances
with no good candidate and frequency of different
labels. The labels denote how many annotators pro-
posed a particular word as substitution in the given
context and can be interpreted as a measure of good-
ness: the higher the value, the better the candidate
fits in the context. Similarly, the label 0 denotes the
total number of negative examples in our datasets,
i.e. bad substitutions ? words that belong to the can-
3This candidate set was found best for WordNet by Martinez
et al (2007).
LexSub TWSI
source WN Gold St. WN Gold St.
# words 201 201 908 1007
#inst 2002 2002 22543 24643
avg. set 21 17 7.5 22
# empty 508 17 11165 620
#0 39465 27300 151538 443993
#1 1302 4698 10678 77417
#2 582 1251 4171 17585
#3 308 571 2069 5629
#4 212 319 74 325
#5+ 129 179 121 411
Table 1: Details of the datasets: WN=WordNet
didate set for a particular target word, but are not
listed as good substitutions in the given context in
the dataset.
4 Methodology
4.1 Experimental Setup and Evaluation
We follow previous works in lexical substitution and
evaluate our models using the Generalized Average
Precision (GAP) (Kishida, 2005) measure which as-
sesses the quality of the entire ranked list. In addi-
tion, we also provide the precision of our system at
the first rank (P@1), i.e. the percentage of correct
paraphrases at rank 1. This is a realistic evaluation
criterion for many applications, such as paraphras-
ing for linguistic steganography: it is the highest-
ranked candidate that can be used to replace the orig-
inal word (the manipulated text should preserve the
original meaning) and there is no straightforward
way to exploit multiple correct answers. In addition,
we also provide the Semeval 2007 best precision4
metric (McCarthy and Navigli, 2007) for the Lex-
Sub dataset for comparison to Semeval 2007 partic-
ipants. This metric also evaluates the first guess of
a system (per context), but gives less credit to easier
contexts, where several good options exist. This fact
motivates us to use P@1 rather than the best preci-
sion metric in all other experiments.
4Since our system always provides an answer, the Semeval
2007 best recall equals best precision.
1134
4.2 Machine Learning on Delexicalized
Features
After the list of potential substitutions is obtained,
lexical substitution is cast as a ranking task where
the goal is to prefer contextually plausible substitu-
tions over implausible ones. The goal of this study
is to learn a ranking model that is applicable to any
word, for which a list of synonyms is available. A
supervised model can generalize over the example
target words in the datasets, if aggregate features
can be defined that have the same semantics regard-
less of the actual context, target word or candidate
substitution they are computed from. Having such a
representation, one can expect to learn patterns that
generalize over the words/contexts seen in the train-
ing dataset, and thus the setup constitutes a super-
vised all-word system.
To simulate an all-word scenario, we perform a
10-fold cross validation in our experiments, splitting
the dataset into equal-sized folds randomly on the
target word level. That is, all sentences for a particu-
lar target word fall into the same fold and thus either
the training or the test set (but never both). This way
we always train and test the model on disjoint sets of
words and as such, the learnt models cannot exploit
word-specific properties. This makes our results re-
alistic estimates of an open vocabulary paraphrasing
system, where we would apply the models (mostly)
to words that were not in the training material.
4.2.1 Machine Learning Model
In our experiments, we used a Maximum Entropy
(MaxEnt) classifier model implemented in the Mal-
let (McCallum, 2002) package and trained a binary
classifier to predict if a given substitution is valid in
a particular context or not.
We chose to use Maximum Entropy models for
two main reasons: MaxEnt is not sensitive to param-
eter settings and handles correlated features well,
which is crucial in our situation where many features
are highly correlated.
Due to the low number of positive examples in the
datasets (see Table 1, labels 1-5+) and to emphasize
better paraphrases suggested by several annotators,
we assigned a weight to positive instances during the
training process equal to their score (the number of
annotators suggesting that paraphrase; the weight of
negative instances was set to 1).
The output of the MaxEnt classifier is a posterior
probability distribution for each target/substitution
pair, denoting the probabilities of the instance to
be a good or a bad substitution, given the feature
values that describe both the words and their con-
text. The ranking over a set of candidates can be
naturally induced based on their posterior scores for
the positive class, i.e. a number that denotes ?how
good the candidate is, given the context?. That is,
the best substitution candidate s (characterized by a
set of features F) from a set of candidates S is ob-
tained as argmaxs?S[P (good|F)], the next best as
the argmax of the remaining elements, and so on.
This pointwise approach to subset ranking (Cos-
sock and Zhang, 2008) is arguably simplistic, but
several studies (c.f. Li et al (2007; Busa-Fekete
et al (2011)) found this approach to perform rea-
sonably well given that the model provides accurate
probability estimates, which is the case for MaxEnt.
4.3 Delexicalized Features
We use heterogeneous sources of information to de-
scribe each target word/candidate substitution pair
in its context. The most important features describe
the syntagmatic coherence of the substitute in con-
text, measured as local n-gram frequencies obtained
from web data, in a sliding window around the tar-
get word. In addition we use features to describe the
(non-positional, i.e. non-local) distributional simi-
larity of the target and its candidate substitution in
terms of sentence level co-occurrence statistics col-
lected from newspaper texts. A further set of fea-
tures captures the properties of the target and can-
didate word in a lexical resource (WordNet), such
as their number of senses, how frequent senses are
synonymous, etc. Lastly, we use part of speech pat-
terns to describe the target word in context. This
way, unlike many other methods suggested in previ-
ous works (Thater et al, 2011; Erk and Pado?, 2008),
our model does not require deep syntactic analysis
of the test sentences in order to rank the candidates.
Even though we make intensive use of WordNet to
compute some of our feature functions, this is not
a severe restriction for a practical paraphrasing sys-
tem: one has to have a decent lexical resource in or-
der to mine a reasonable set of candidate synonyms
and such a resource can also serve as a source for
features in the classifier. The rest of the feature func-
1135
tions exploit only large unannotated corpora and a
POS tagger at application time.
For a target word t, and candidate substitution si
from a set of candidates S, we used the features be-
low. Each numeric feature is used both in the form
given below, and set-wise scaled to [0, 1] (we leave
it to the classifier to pick the more useful form of
information). For the LexSub dataset, each feature
is defined once for all instances, and once specific
to the four POS categories in the dataset. That is
each instance would have the described features de-
fined twice, once the general form (defined for every
instance) and once the form according to the pre-
dicted POS category of the target word. This allows
the model to learn general and also POS-specific
patterns based on the information described below
(i.e. frequency thresholds, distributional properties
etc. for nouns or verbs etc. in particular). We denote
the left and right contexts around t and all words in
the sentence except t with cl, cr and c, respectively)
4.3.1 Lexical Resource Features
We used Wordnet 3.0 as the source for substi-
tution candidates and as a source for delexicalized
features. We found the measure of ambiguity and
the sense number to provide useful information in
a more general context: it is informative how many
senses a word has, and it is informative from which
sense number of the substitution target the substitu-
tion candidate came from, since they are ordered by
corpus frequency. In addition, we used the synsets
IDs of the words? hypernyms as features, which can
capture more general semantics (the word to replace
is ?animate?, ?abstract?, etc.). The following features
were extracted from WordNet:
? number of senses of t and si in WordNet
? the sense numbers of t and si which are syn-
onymous (in case they are direct synonyms, c.f.
WN sense numbers encode sense frequencies)
? binary features for synset IDs of the hypernyms
of the synset containing t and si (this feature
type did not significantly improve results)
4.3.2 Corpus-based Features
In order to create a Distributional Thesaurus (DT)
similar to Lin (1998), we parsed a source corpus
of 120M sentence English newspaper texts from
the LCC5 (Richter et al, 2006) with the Stanford
parser (de Marneffe et al, 2006) and used depen-
dencies to extract features for words: each depen-
dency triple (w1, r, w2) denoting a dependency of
type r between words w1 and w2 results in a fea-
ture (r, w2) characterizing w1, and a feature (w1, r)
characterizing w26. After counting the frequency
of each feature for each word, we apply a signifi-
cance measure (log-likelihood test (LL), (Dunning,
1993)), rank features per word according to their
significance, and prune the data, keeping only the
1000 most salient features (Fw) per word7. The sim-
ilarity of two words is then given by the number
of their common features. Our distributional the-
saurus provides a list of the 1000 most salient fea-
tures and a ranked list of up to 200 similar words
(simw, based on the number of shared features) for
all words above a certain frequency in the source
corpus. We compute the following features to char-
acterize a target word / substitution pair:
? To what extent the context c characterizes si:?
c?Fsi
LL(Fsi (c))
?
sj?S
?
c?Fsj
LL(Fsj (c))
? percentage of shared words among
the top k similar words to t and
to si:
|simt|k?|simsi |k
max(|simt|k,|simsi |k)
, for k =
1, 5, 10, 20, 50, 100, 2008
? percentage of shared salient features among the
top k features of t and si, globally and re-
stricted to the words from the target sentence:
|Ft|k?|Fsi |k
max(|Ft|k,|Fsi |k)
and
|Ft|k?|Fsi |k?|c|
|c| , for k =
1, 5, 10, 20, 50, 100, 1000
? boolean feature indicating whether si ? simt
or not (in top 100 similar words)
5http://corpora.informatik.uni-leipzig.de/
6open source implementation and data available at
http://sourceforge.net/p/jobimtext
7The pruning operation greatly reduces runtime at the-
saurus collection, rendering memory reduction techniques like
(Charikar et al, 2004) as unnecessary.
8The various values for k trade off the salience of this fea-
ture for coverage: only very few substitutions have overlap in
the top 1-5 similar words set, but if this happens, it is a very
strong indicator of contextual fitness, whereas overlap within
the top 100-200 similar words is present for much more tar-
get/substitution pairs, but it is a weaker indicator of fitness.
1136
4.3.3 Local n-gram Features (from Web 1T)
Syntagmatic coherence, measured as the n-gram
frequency of the context with the candidate substi-
tution serves as the basis of ranking in the best Se-
meval 2007 system (Giuliano et al, 2007), which is
also our baseline method here. We use the same n-
grams as features in our supervised model:
? 1-5-gram frequencies in a sliding window
around t: freq(clsicr)/freq(cltcr), normal-
ized w.r.t t
? 1-5-gram frequencies in a sliding window
around t: freq(clsicr)/
?
freq(clScr), nor-
malized w.r.t. S
? for each of x in {?and?, ?or?, ?,?}, 3-5-
gram frequencies in a sliding window around
t: freq(cltxsicr)/freq(cltcr) (how frequently
the target and candidate are part of a list or con-
junctive phrase)
4.3.4 Shallow Syntactic Features
We also use part of speech information (from
TreeTagger (Schmid, 1994)) as features, in order
to enable the model to learn POS-specific patterns.
This is especially important for the LexSub dataset,
which contains examples from all major parts of
speech (the TWSI dataset contains only noun tar-
gets). Specifically, we use:
? 1-3-grams of main POS categories in a window
around t, e.g. NVV for a noun, verb, verb con-
text
? Penn Treebank POS code of t
4.3.5 Example
For clarity, we exemplify our delexicalized fea-
tures briefly. Using WordNet as a source for the
word bright, we considered the 11 words brilliant,
vivid, smart, burnished, lustrous, shining, shiny,
undimmed, brilliant, hopeful, promising from the
synsets of bright, and 64 further words from its re-
lated synsets (e.g. intelligent, glimmery, polished,
happy, ...) as potential paraphrases. That is, for
the sentence ?He was bright and independent and
proud.?, where the human annotators listed intelli-
gent, clever as suitable paraphrases, our system had
1 correct (intelligent) and 74 incorrect substituions
in the candidate set (that is, clever is not found in
WordNet in the above described way). The substitu-
tion intelligent in this context is characterized by a
total of 178 active features. Of those, 112 features
are based on local n-gram features (Sect. 4.3.3),
where the large number stems from different n in
n-gram, as well as the different variants of normal-
ization and copies for the particular POS (here: JJ)
and for all POS. For instance, ?bright? and ?intelli-
gent? are frequently occurring in comma-separated
enumerations, and ?intelligent? fits well in the target
context based on n-gram probabilities. The second
largest block of features is constituted by 48 active
distributional similarity features (Sect. 4.3.2), which
are also available per POS and for different normal-
izations. Here is e.g. captured that the candidate
has a high distributional similarity to the target with
respect to our background corpus. The 12 shallow
syntatic features (Sect 4.3.4) capture various present
POS patterns around the target, and the 6 resource-
based features (Sect. 4.3.1) e.g. inform about the
number of senses of the target (10) and the candi-
date (4).
4.4 Results
Now, we describe our results in detail. First we com-
pare our system on two datasets with a competitive
baseline, which uses the same candidate set as our
ML-based model, and the simple and effective rank-
ing function based on Google n-grams described by
Giuliano et al (2007). Later on we analyze how the
four major feature groups contribute to the results in
a feature ablation experiment, and then we provide
a detailed and thorough comparison to earlier works
that are similar to the model presented here and used
the same dataset (LexSub) for evaluation.
4.4.1 Semeval 2007 Lexical Substitution
In Table 2 we report results on the LexSub dataset.
As can be seen, our model outperforms the baseline
by a significant margin (p < 0.01 for all measures,
using a paired t-test for significance). Both the over-
all rankings and the P@1 scores are of higher quality
than the rankings based only on n-grams.
4.4.2 Turk Bootstrap Word Sense Inventory
The results on the TWSI dataset are provided in
Table 3. Our model outperforms the baseline in all
1137
cand. from WN from Gold St.
GAP P@1 GAP P@1
Baseline 36.8 31.1 46.9 49.5
Our model 43.8 40.2 52.4 57.7
Table 2: Comparison to the baseline on LexSub 2007.
cand. from WN from Gold St.
GAP P@1 GAP P@1
Baseline 33.8 28.2 44.4 44.5
Our model 36.6 32.4 47.2 49.5
Table 3: Comparison to the baseline on the TWSI dataset.
the comparisons similar to the LexSub dataset. The
differences are not so pronounced but still highly
significant (p < 0.01). This is consistent with the
observation by several Semeval 2007 participants
and with a per-POS analysis of our results on Lex-
Sub: the ranking task seems to be more challenging
for nouns than for other parts of speech. When us-
ing WordNet, for about half (11165/22543) of the
instances, individual scores are 0 (cf. Table 1). For
the other half, avg. P@1 score is around 0.7, which
results in 0.324 overall. Note that the task of ranking
in avg. 7.5 items is considerably easier than rank-
ing in avg. 22 items, which explains the high P@1
scores for cases where good candidates exist ? also,
a random ranker would score higher in this case.
These results demonstrate that the proposed
delexicalized approach is superior to a competitive
baseline across two datasets.
4.5 Feature Exploration
We explored the contribution of our various fea-
ture types on the LexSub dataset with candidate set
from the gold standard. Our MaxEnt model rely-
ing only on local n-gram frequency features, i.e. the
GAP P@1
w/o n-gram features 47.3 48.9
w/o distr. thesaurus 49.8 55.0
w/o POS features 51.6 56.3
w/o WN features 51.7 57.0
Our model (all) 52.4 57.7
Table 4: Feature ablation experiment (on LexSub dataset,
with candidates from Gold Standard).
same information as the baseline model, achieved a
GAP score of 48.3 and P@1 of 52.1, respectively.
This result is significantly better than the baseline
(p < 0.01), i.e. the machine learnt ranking model
is better than a state-of-the-art handcrafted ranker
based on the same data. All single feature groups,
when combined with n-grams, lead to significant im-
provements (p < 0.01), which proves the usefulness
of each feature group. In order to assess the contri-
bution of each group to the overall system perfor-
mance, we performed a feature ablation experiment.
That is, we trained the MaxEnt model with using
all feature groups (this equals the model in Table 2)
and then with leaving each of the feature groups out
once. As can be seen, all feature groups improve the
overall results in a noticeable way, i.e. their contri-
bution is complementary.
4.5.1 Comparison to Previous Works
In Table 5 we compare our method with previous
works in the field, using the LexSub dataset.
candidates from WN from Gold Standard
Best-P GAP
Pado?Erk10 38.6
Giuliano 12.93 DinuLapata 42.9
Martinez 12.68 Thater10 46.0
Sinha 13.60 Thater11 51.7
Baseline 11.75 Baseline 46.9
Our model 15.94 Our model 52.4
Table 5: Comparison to previous works (LexSub dataset).
In the left column of Table 5, we compare the per-
formance of our system to representative Semeval
2007 participants, namely Martinez et al (2007) and
Giuliano et al (2007). In order to make a fair com-
parison, we report scores for the official test data
of Semeval 2007, using a 10-fold cross-validation
scheme. Martinez et al (2007) developed their sys-
tem based on WordNet and we use the same can-
didate set here that they proposed in their system
description. Our reimplementation of (Giuliano et
al., 2007) performs below the original scores, due
to the more restricted source of substitution can-
didates (they use more lexical resources), yet uses
the same ranking methodology based on Google n-
grams that we adopted here as our baseline. We also
report the best previous result for this task, which
1138
was achieved via the (supervised) combination of
lexical resources to improve the performance (Sinha
and Mihalcea, 2009). Our model outperforms this
result by a large margin for the best-precision eval-
uation (mode-P, precision measured on those exam-
ples where there is a clear best substitution provided
by humans was 26.3%, compared to 21.3% reported
by Sinha and Mihalcea (2009). This is especially
promising in light of the fact that we use only a sin-
gle source (WordNet) for synonyms and achieve our
improvements through more advanced delexicalized
features in an improved ranking model. Sinha and
Mihalcea (2009), on the other hand, used compara-
bly simple features for contextualization, of which
n-gram features were deemed most successful. As
Sinha and Mihalcea (2009) showed improvements
through utilizing several synonym sources, a combi-
nation of their approach with ours should allow for
further improvements in the future.
In the right column of Table 5, we compare our
model to previous works that addressed only the
ranking task, and report performance on the whole
dataset (i.e. trial and test). As can be seen, the
methodology proposed here outperforms previous
ranking models, without the need to develop a high-
quality ranking model by hand, and without the need
to parse the test sentences. Our delexicalized super-
vised model only requires the development of fea-
tures, and achieves excellent results without major
task-specific tuning or customization: we omitted
the optimization of the feature set and the parame-
ters of the learning model. This fact makes us as-
sume that the proposed model can be applied more
quickly and easily than previous models that have
several important design aspects to choose from.
5 Conclusion and Future Work
In this study, we presented a supervised approach to
all-words lexical substitution based on delexicalized
features, which enables us to fully exploit the power
of supervised models while ensuring applicability to
a large, open vocabulary.
Results demonstrate the feasibility of this method:
our MaxEnt-based ranking approach improved over
the baseline in all settings and yielded ? to our
knowledge ? the best scores for lexical substitu-
tion with automatically gathered synonyms on the
Semeval 2007 LexSub dataset. Also, it performed
slightly better than the state of the art for candidates
pooled from the gold standard without any parame-
ter tuning or empirical design choices.
In this study, we established transparency be-
tween Semeval-style and ranking-only studies in
lexical substitution ? two lines of work that were dif-
ficult to compare in the past. Further, we observe
similar improvements on two different datasets,
showing the robustness of the approach.
While previous works showed the potential of
more/improved lexical resources for lexical substi-
tution, we improved over the best Semeval-style per-
formance just by exploiting an improved ranking
model over a standard WordNet-based candidate set.
These results indicate that improvements from lexi-
cal resources and better ranking models are additive,
thus we want to add more lexical resources in our
system in the future.
Of course there are several other ways to improve
further the work described here. First of all, simi-
lar to the best ranking approaches (e.g. Thater et al
(2011)), one could use contextualized feature func-
tions to make global information from the distri-
butional thesaurus more accurate. Instead of using
globally calculated similarities, information from
the distributional thesaurus could be contextualized
via constraining the statistics with words from the
context.
Other natural ways to improve the model de-
scribed here are to make heavier use of parser infor-
mation or to employ pair-wise or list-wise machine
learning models (Cao et al, 2007), which are specif-
ically designed for subset ranking. Lastly, while in-
trinsic evaluation of lexical substitution is important,
we would like to show its practicability in tasks such
as steganography or information retrieval.
Acknowledgements
This work has been supported by the Hes-
sian research excellence program Landes-Offensive
zur Entwicklung Wissenschaftlich-o?konomischer
Exzellenz (LOEWE) as part of the research center
Digital Humanities, and by the German Ministry of
Education and Research under grant SiDiM (grant
no. 01IS10054G).
1139
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
Ro?bert Busa-Fekete, Bala?zs Ke?gl, E?lteto? Yama?s, and
Gyo?rgi Szarvas. 2011. A robust ranking method-
ology based on diverse calibration of adaboost. In
European Conference on Machine Learning, volume
LNCS, 6911, pages 263?279.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24rd International Conference on Machine Learning,
pages 129?136.
Ching-Yun Chang and Stephen Clark. 2010. Practi-
cal linguistic steganography using contextual synonym
substitution and vertex colour coding. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1194?1203, Cam-
bridge, MA.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312(1):3?15.
D. Cossock and T. Zhang. 2008. Statistical analysis of
Bayes optimal subset ranking. IEEE Transactions on
Information Theory, 54(11):5140?5154.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 449?456, Sydney, Australia.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006, Genova, Italy.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?906,
Honolulu, Hawaii.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92?
97, Uppsala, Sweden.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611.
Bela Gipp, Norman Meuschke, and Joeran Beel. 2011.
Comparative Evaluation of Text- and Citation-based
Plagiarism Detection Approaches using GuttenPlag.
In Proceedings of 11th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries (JCDL?11), pages 255?258,
Ottawa, Canada. ACM New York, NY, USA. Avail-
able at http://sciplore.org/pub/.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. FBK-irst: Lexical substitution task exploit-
ing domain and syntagmatic coherence. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 145?148, Prague,
Czech Republic.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. UNT: SubFinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 410?413, Prague, Czech Republic.
Kazuaki Kishida. 2005. Property of Average Precision
and Its Generalization: An Examination of Evaluation
Indicator for Information Retrieval Experiments. NII
technical report. National Institute of Informatics.
Ping Li, Christopher J.C. Burges, and Qiang Wu. 2007.
McRank: Learning to rank using multiple classifica-
tion and gradient boosting. In Advances in Neural In-
formation Processing Systems, volume 19, pages 897?
904. The MIT Press.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
1140
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, pages 1138?1147.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and the 17th International Conference on Compu-
tational Linguistics, volume 2 of ACL ?98, pages 768?
774, Montreal, Quebec, Canada.
David Martinez, Su Nam Kim, and Timothy Bald-
win. 2007. MELB-MKB: Lexical substitution system
based on relatives in context. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 237?240, Prague, Czech
Republic.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 48?53,
Prague, Czech Republic.
Rada Mihalcea and Andras Csomai. 2005. Senselearner:
word sense disambiguation for all words in unre-
stricted text. In Proceedings of the ACL 2005 on Inter-
active poster and demonstration sessions, ACLdemo
?05, pages 53?56.
Joseph Reisinger and Raymond Mooney. 2010a. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173?
1182, Cambridge, MA.
Joseph Reisinger and Raymond J. Mooney. 2010b.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
109?117, Los Angeles, California.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proceesings of the IS-LTC 2006. Ljubljana, Slove-
nia.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Ravi Sinha and Rada Mihalcea. 2009. Combining lex-
ical resources for contextual synonym expansion. In
Proceedings of the International Conference RANLP-
2009, pages 404?410, Borovets, Bulgaria.
Ravi Som Sinha and Rada Flavia Mihalcea. 2011. Using
centrality algorithms on directed graphs for synonym
expansion. In R. Charles Murray and Philip M. Mc-
Carthy, editors, FLAIRS Conference. AAAI Press.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948?957, Uppsala,
Sweden.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of the Fifth Interna-
tional Joint Conference on Natural Language Process-
ing : IJCNLP 2011, pages 1134?1143, Chiang Mai,
Thailand. MP, ISSN 978-974-466-564-5.
Umut Topkara, Mercan Topkara, and Mikhail J. Atal-
lah. 2006. The hiding virtues of ambiguity: quan-
tifiably resilient watermarking of natural language text
through synonym substitutions. In Proceedings of the
8th workshop on Multimedia and security, pages 164?
174, New York, NY, USA. ACM.
1141
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 210?213,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
TUD: semantic relatedness for relation classification
Gy
?
orgy Szarvas
?
and Iryna Gurevych
Ubiquitous Knowledge Processing (UKP) Lab
Computer Science Department
Technische Universit?at Darmstadt
Hochschulstra?e 10., D-64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de/
Abstract
In this paper, we describe the system sub-
mitted by the team TUD to Task 8 at
SemEval 2010. The challenge focused on
the identification of semantic relations be-
tween pairs of nominals in sentences col-
lected from the web. We applied max-
imum entropy classification using both
lexical and syntactic features to describe
the nominals and their context. In addi-
tion, we experimented with features de-
scribing the semantic relatedness (SR) be-
tween the target nominals and a set of clue
words characteristic to the relations. Our
best submission with SR features achieved
69.23% macro-averaged F-measure, pro-
viding 8.73% improvement over our base-
line system. Thus, we think SR can serve
as a natural way to incorporate external
knowledge to relation classification.
1 Introduction
Automatic extraction of typed semantic relations
between sentence constituents is an important step
towards deep semantic analysis and understand-
ing the semantic content of natural language texts.
Identification of relations between a nominal and
the main verb, and between pairs of nominals are
important steps for the extraction of structured se-
mantic information from text, and can benefit vari-
ous applications ranging from Information Extrac-
tion and Information Retrieval to Machine Trans-
lation or Question Answering.
The Multi-Way Classification of Semantic Re-
lations Between Pairs of Nominals challenge
(Hendrickx et al, 2010) focused on the identi-
fication of specific relation types between nomi-
nals (nouns or base noun phrases) in natural lan-
guage sentences collected from the web. The main
?
On leave from the Research Group on Artificial Intelli-
gence of the Hungarian Academy of Sciences
task of the challenge was to identify and clas-
sify instances of 9 abstract semantic relations be-
tween noun phrases, i.e. Cause-Effect, Instrument-
Agency, Product-Producer, Content-Container,
Entity-Origin, Entity-Destination, Component-
Whole, Member-Collection, Message-Topic. That
is, given two nominals (e1 and e2) in a sentence,
systems had to decide whether relation(e1,e2), re-
lation(e2,e1) holds for one of the relation types or
the nominals? relation is other (falls to a category
not listed above or they are unrelated). In this
sense, the challenge was an important pilot task
towards large scale semantic processing of text.
In this paper, we describe the system we sub-
mitted to Semeval 2010, Task 8. We applied max-
imum entropy classification to the problem using
both lexical and contextual features to describe
the nominals themselves and their context (i.e.
the sentence). In addition, we experimented with
features exploiting the strength of association be-
tween the target nominals and a predefined set of
clue words characteristic to the nine relation types.
In order to measure the semantic relatedness (SR)
of targets and clues, we used the Explicit Seman-
tic Analysis (Gabrilovich and Markovitch, 2007)
SR measure (based on Wikipedia, Wiktionary and
WordNet). Our best submission, benefiting from
SR features, achieved 69.23% macro-averaged F-
measure for the 9 relation types used. Providing
8.73% improvement over our baseline system, we
found the SR-based features to be beneficial for
the classification of semantic relations.
2 Experimental setup
2.1 Feature set and selection
Feature set In our system, we used both lexical
(1-3) and contextual features (4-8) to describe the
nominals and their context (i.e. the sentence). Ad-
ditionally, we experimented with a set of features
(9) that exploit the co-occurrence statistics of the
210
nominals and a set of clue words chosen manu-
ally, examining the relation definitions and exam-
ples provided by the organizers. The clues char-
acterize the relations addressed in the task (e.g.
cargo, goods, content, box, bottle characterize the
Content-Container relation)
1
. Each feature type
was distinguished from the others using a prefix.
All but the semantic relatedness features we used
were binary, denoting whether a specific word,
lemma, POS tag, etc. is found in the example sen-
tence, or not. SR features were real valued, scaled
to [0, 1] for each clue word separately (on train,
and the same scaling factores were applied on the
test data). The feature types used:
1. Token: word unigrams in the sentence in their
inflected form. 2. Lemma: word uni- and bigrams
in the sentence in their lemmatized form. 3. Tar-
get Nouns: the syntactic head words of the target
nouns. 4. POS: the part of speech uni- and bi-
and trigrams in the sentence. 5. Between POS: the
part of speech sequence between the target nouns.
6. Dependency Path: the dependency path (syn-
tactic relations and directions) between the target
nouns. The whole path constituted a single fea-
ture. 7. Target Distance: the distance between
the target nouns (in tokens). 8. Sentence Length:
the length of the sentence (in tokens). 9. Seman-
tic Relatedness: the semantic relatedness scores
measuring the strength of association between the
target nominals and the set of clue words we col-
lected. In order to measure the semantic related-
ness (SR) of targets and clues, we used the Explicit
Semantic Analysis (ESA) SR measure.
Feature selection In order to discard uninforma-
tive features automatically, we performed feature
selection on the binary features. We kept features
that satisfied the following three conditions:
freq(x) > 3 (1)
p = argmax
y
P (y|x) > t
1
(2)
p
5
? freq(x) > t
2
(3)
where freq(x) denotes the frequency of feature x
observed in the training dataset, y denotes a class
label, p denotes the highest posterior probability
(for feature x) over the nine relations (undirected)
and the other class. Finally, t
1
, t
2
are filtering
thresholds chosen arbitrarily. We used t
1
= 0.25
for all features but the dependency path, where we
1
The clue list is available at:
http://www.ukp.tu-darmstadt.de/research/data/
relation-classification/
relation type size c4.5 SMO maxent
cause-effect 1003 75.2% 78.9% 78.2%
component-whole 941 46.7% 53.0% 54.7%
content-container 540 72.9% 78.1% 75.1%
entity-destination 845 77.6% 82.3% 82.0%
entity-origin 716 61.8% 65.0% 68.7%
instrument-agency 534 40.7% 42.7% 47.6%
member-collection 690 68.2% 72.1% 75.3%
message-topic 634 41.3% 47.3% 56.4%
product-producer 717 43.8% 50.3% 53.4%
macro AVG F1 6590 58.7% 63.3% 65.7%
Table 1: Performance of different learning meth-
ods on train (10-fold).
used t
1
= 0.2. We set the thresold t
2
to 1.9 for
lexical features (i.e. token and lemma features),
to 0.3 for dependency path features and to 0.9 for
all other features. All parameters for the feature
selection process were chosen manually (cross-
validating the parameters was omitted due to lack
of time during the challenge development period).
The higher t
2
value for lexical features was moti-
vated by the aim to avoid overfitting, and the lower
thresholds for dependency-based features by the
hypothesis that these can be most efficient to deter-
mine the direction of relationships (c.f. we disre-
garded direction during feature selection). As the
numeric SR features were all bound to clue words
selected specifically for the task, we did not per-
form any feature selection for that feature type.
2.2 Learning models
We compared three learning algorithms, using
the baseline feature types (1-8), namely a C4.5
decision tree learner, a support vector classifier
(SMO), and a maximum entropy (logistic regres-
sion) classifier, all implemented in the Weka pack-
age (Hall et al, 2009). We trained the SMO model
with polynomial kernel of degree 2, fitting logistic
models to the output to get valid probability esti-
mates and the C4.5 model with pruning confidence
factor set to 0.33. All other parameters were set to
their default values as defined in Weka. We found
the maxent model to perform best in 10-fold cross
validation on the training set (see Table 1). Thus,
we used maxent in our submissions.
3 Results
We submitted 4 runs to the challenge. Table
2 shows the per-class and the macro average F-
measures of the 9 relation classes and the accu-
racy over all classes including other, on the train
(10-fold) and the test sets (official evaluation):
211
Train Test
relation type Base WP cSR cSR-t Base WP cSR cSR-t
cause-effect 78.17% 78.25% 79.42% 79.10% 80.69% 81.90% 83.76% 83.38%
component-whole 54.68% 58.71% 60.18% 60.79% 50.52% 57.90% 61.67% 62.15%
content-container 75.09% 77.55% 78.26% 78.11% 75.27% 78.96% 78.33% 78.87%
entity-destination 81.99% 82.97% 83.12% 82.90% 77.59% 82.86% 81.54% 81.12%
entity-origin 68.74% 70.39% 71.14% 71.18% 67.08% 72.05% 71.03% 70.36%
instrument-agency 47.59% 56.71% 59.60% 59.80% 31.09% 44.06% 46.78% 46.91%
member-collection 75.27% 79.43% 80.71% 80.89% 66.37% 71.24% 72.65% 72.65%
message-topic 56.40% 62.68% 64.77% 65.15% 49.88% 65.06% 68.15% 69.83%
product-producer 53.36% 57.98% 59.97% 60.70% 46.04% 57.94% 56.00% 57.85%
macro AVG F1 65.70% 69.40% 70.80% 70.96% 60.50% 68.00% 68.88% 69.23%
accuracy (incl. other) 62.10% 65.42% 66.83% 67.12% 56.13% 63.49% 64.63% 65.37%
Table 2: Performance of 4 submissions on train (10-fold) and test.
Baseline (Base) As our baseline system, we used
the information extracted from the sentence itself
(i.e. lexical and contextual features, types 1-8).
Wikipedia (WP) As a first extension, we added
SR features (9) exploiting term co-occurrence in-
formation, using the ESA model with Wikipedia.
Combined Semantic Relatedness (cSR) Second,
we replaced the ESA measure with a combined
measure developed by us, exploiting term co-
occurrence not only in Wikipedia, but also in
WordNet and Wiktionary glosses. We found this
measure to perform better than the Wikipedia-
based ESA in earlier experiments.
cSR threshold (cSR-t) We submitted the predic-
tions of the cSR system, with less emphasis on the
other class: we predicted other label only when
the following held for the posteriors predicted by
cSR:
argmax
y
P (y|x)
p(other)
< 0.7. The threshold 0.7 was
chosen based on the training dataset.
First, the SR features improved the performance
of our system by a wide marging (see Table
2). The difference in performance is even more
prominent on the Test dataset, which suggests that
these features efficiently incorporated useful ex-
ternal evidence on the relation between the nomi-
nals and this not just improved the accuracy of the
system, but also helped to avoid overfitting. Thus
we conclude that the SR features with the encoded
external knowledge helped the maxent model to
learn a hypothesis that clearly generalized better.
Second, we notice that the combined SR mea-
sure proved to be more useful than the standard
ESA measure (Gabrilovich and Markovitch, 2007)
improving the performance by approximately 1
percent over ESA, both in terms of macro aver-
aged F-measure and overall accuracy. This con-
firms our hypothesis that the combined measure is
more robust than ESA with just Wikipedia.
prediction category cSR cSR-t
true positive relation (TP) 1555 1612
true positive other (TN) 201 164
wrong relation type (FP & FN) 291 341
wrong relation direction (FP & FN) 50 58
relation classified as other (FN) 367 252
other classified as relation (FP) 253 290
total 2717 2717
Table 3: Prediction error statistics.
3.1 Error Analysis
Table 3 shows the breakdown of system predic-
tions to different categories, and their contribution
to the official ranking as true/false positives and
negatives. The submission that manipulated the
decision threshold for the other class improved
the overall performance by a small margin. This
fact, and Table 3 confirm that our approach had
major difficulties in correctly discriminating the 9
relation categories from other. Since this class is
an umbrella class for unrelated nominals and the
numerous semantic relations not considered in the
challenge, it proved to be extremely difficult to ac-
curately characterize this class. On the other hand,
the confusion of the 9 specified relations (between
each other) and directionality were less prominent
error types. The most frequent cross-relation
confusion types were the misclassification
of Component-Whole as Instrument-Agency
and Member-Collection; Content-Container
as Component-Whole; Instrument-Agency as
Product-Producer and vice versa. Interestingly,
Component-Whole and Cause-Effect relations
were the most typical sources for wrong direction
errors. Lowering the decision threshold for other
in our system naturally resulted in more true
positive relation classifications, but unfortunately
not only raised the number of other instances
falsely classified as being one of the valuable re-
212
lations, but also introduced several wrong relation
classification errors (see Table 3). That is why
this step resulted only in marginal improvement.
4 Conclusions & Future Work
In this paper, we presented our system submitted
to the Multi-Way Classification of Semantic Re-
lations Between Pairs of Nominals challenge at
SemEval 2010. We submitted 4 different system
runs. Our first submission was a baseline system
(Base) exploiting lexical and contextual informa-
tion collected solely from the sentence to be classi-
fied. A second run (WP) complemented this base-
line configuration with a set of features that used
Explicit Semantic Analysis (Wikipedia) to model
the SR of the nominals to be classified and a set
of clue words characteristic of the relations used
in the challenge. Our third run (cSR) used a com-
bined semantic relatedness measure that exploits
multiple lexical semantic resources (Wikipedia,
Wiktionary and WordNet) to provide more reliable
relatedness estimates. Our final run (cSR-t) ex-
ploited that our system in general was inaccurate
in predicting instances of the other class. Thus,
it used the same predictions as cSR, but favored
the prediction of one of the 9 specified classes in-
stead of other, when a comparably high posterior
for such a class was predicted by the system.
Our approach is fairly simple, in the sense that it
used mostly just local information collected from
the sentence. It is clear though that encoding as
much general world knowledge to the representa-
tion as possible is crucial for efficient classifica-
tion of semantic relations. In the light of the above
fact, the results we obtained are reasonable.
As the main goal of our study, we attempted
to use semantic relatedness features that exploit
texts in an external knowledge source (Wikipedia,
Wiktionary or WordNet in our case) to incorpo-
rate some world knowledge in the form of term co-
occurrence scores. We found that our SR features
significantly contribute to system performance.
Thus, we think this kind of information is useful
in general for relation classification. The experi-
mental results showed that our combined SR mea-
sure performed better than the standard ESA using
Wikipedia. This confirms our hypothesis that ex-
ploiting multiple resources for modeling term re-
latedness is beneficial in general.
Obviously, our system leaves much space for
improvement ? the feature selection parameters
and the clue word set for the SR features were
chosen manually, without any cross-validation (on
the training set), due to lack of time. One of the
participating teams used an SVM-based system
and gained a lot from manipulating the decision
thresholds. Thus, despite our preliminary results,
it is also an interesting option to use SVMs.
In general, we think that more features are
needed to achieve significantly better performance
than we reported here. Top performing systems
in the challenge typically exploited web frequency
information (n-gram data) and manually encoded
relations from an ontology (mainly WordNet).
Thus, future work is to incorporate such features.
We demonstrated that SR features are helpful to
move away from lexicalized systems using token-
or lemma-based features. Probably the same holds
for web-based and ontology-based features exten-
sively used by top performing systems. This sug-
gests that experimenting with all these to see if
their value is complementary is an especially in-
teresting piece of future work.
Acknowledgments
This work was supported by the German Ministry
of Education and Research (BMBF) under grant
?Semantics- and Emotion-Based Conversation
Management in Customer Support (SIGMUND)?,
No. 01ISO8042D, and by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship
Program under the grant No. I/82806.
References
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of The 20th International Joint Conference on Arti-
ficial Intelligence, pages 1606?1611.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1):10?18.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid
?
O S?eaghdha, Sebastian
Pad?o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation.
213
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1?12,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text
Richa?rd Farkas1,2, Veronika Vincze1, Gyo?rgy Mo?ra1, Ja?nos Csirik1,2, Gyo?rgy Szarvas3
1 University of Szeged, Department of Informatics
2 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
3 Technische Universita?t Darmstadt, Ubiquitous Knowledge Processing Lab
{rfarkas,vinczev,gymora,csirik}@inf.u-szeged.hu,
szarvas@tk.informatik.tu-darmstadt.de
Abstract
The CoNLL-2010 Shared Task was dedi-
cated to the detection of uncertainty cues
and their linguistic scope in natural lan-
guage texts. The motivation behind this
task was that distinguishing factual and
uncertain information in texts is of essen-
tial importance in information extraction.
This paper provides a general overview
of the shared task, including the annota-
tion protocols of the training and evalua-
tion datasets, the exact task definitions, the
evaluation metrics employed and the over-
all results. The paper concludes with an
analysis of the prominent approaches and
an overview of the systems submitted to
the shared task.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
provides a competitive shared task for the Com-
putational Linguistics community. After a five-
year period of multi-language semantic role label-
ing and syntactic dependency parsing tasks, a new
task was introduced in 2010, namely the detection
of uncertainty and its linguistic scope in natural
language sentences.
In natural language processing (NLP) ? and in
particular, in information extraction (IE) ? many
applications seek to extract factual information
from text. In order to distinguish facts from unre-
liable or uncertain information, linguistic devices
such as hedges (indicating that authors do not
or cannot back up their opinions/statements with
facts) have to be identified. Applications should
handle detected speculative parts in a different
manner. A typical example is protein-protein in-
teraction extraction from biological texts, where
the aim is to mine text evidence for biological enti-
ties that are in a particular relation with each other.
Here, while an uncertain relation might be of some
interest for an end-user as well, such information
must not be confused with factual textual evidence
(reliable information).
Uncertainty detection has two levels. Auto-
matic hedge detectors might attempt to identify
sentences which contain uncertain information
and handle whole sentences in a different man-
ner or they might attempt to recognize in-sentence
spans which are speculative. In-sentence uncer-
tainty detection is a more complicated task com-
pared to the sentence-level one, but it has bene-
fits for NLP applications as there may be spans
containing useful factual information in a sentence
that otherwise contains uncertain parts. For ex-
ample, in the following sentence the subordinated
clause starting with although contains factual in-
formation while uncertain information is included
in the main clause and the embedded question.
Although IL-1 has been reported to con-
tribute to Th17 differentiation in mouse
and man, it remains to be determined
{whether therapeutic targeting of IL-1
will substantially affect IL-17 in RA}.
Both tasks were addressed in the CoNLL-2010
Shared Task, in order to provide uniform manu-
ally annotated benchmark datasets for both and to
compare their difficulties and state-of-the-art so-
lutions for them. The uncertainty detection prob-
lem consists of two stages. First, keywords/cues
indicating uncertainty should be recognized then
either a sentence-level decision is made or the lin-
guistic scope of the cue words has to be identified.
The latter task falls within the scope of semantic
analysis of sentences exploiting syntactic patterns,
as hedge spans can usually be determined on the
basis of syntactic patterns dependent on the key-
word.
1
2 Related Work
The term hedging was originally introduced by
Lakoff (1972). However, hedge detection has re-
ceived considerable interest just recently in the
NLP community. Light et al (2004) used a hand-
crafted list of hedge cues to identify specula-
tive sentences in MEDLINE abstracts and several
biomedical NLP applications incorporate rules for
identifying the certainty of extracted information
(Friedman et al, 1994; Chapman et al, 2007; Ara-
maki et al, 2009; Conway et al, 2009).
The most recent approaches to uncertainty de-
tection exploit machine learning models that uti-
lize manually labeled corpora. Medlock and
Briscoe (2007) used single words as input features
in order to classify sentences from biological ar-
ticles (FlyBase) as speculative or non-speculative
based on semi-automatically collected training ex-
amples. Szarvas (2008) extended the methodology
of Medlock and Briscoe (2007) to use n-gram fea-
tures and a semi-supervised selection of the key-
word features. Kilicoglu and Bergler (2008) pro-
posed a linguistically motivated approach based
on syntactic information to semi-automatically re-
fine a list of hedge cues. Ganter and Strube (2009)
proposed an approach for the automatic detec-
tion of sentences containing uncertainty based on
Wikipedia weasel tags and syntactic patterns.
The BioScope corpus (Vincze et al, 2008) is
manually annotated with negation and specula-
tion cues and their linguistic scope. It consists
of clinical free-texts, biological texts from full pa-
pers and scientific abstracts. Using BioScope for
training and evaluation, Morante and Daelemans
(2009) developed a scope detector following a su-
pervised sequence labeling approach while O?zgu?r
and Radev (2009) developed a rule-based system
that exploits syntactic patterns.
Several related works have also been published
within the framework of The BioNLP?09 Shared
Task on Event Extraction (Kim et al, 2009), where
a separate subtask was dedicated to predicting
whether the recognized biological events are un-
der negation or speculation, based on the GENIA
event corpus annotations (Kilicoglu and Bergler,
2009; Van Landeghem et al, 2009).
3 Uncertainty Annotation Guidelines
The shared task addressed the detection of uncer-
tainty in two domains. As uncertainty detection
is extremely important for biomedical information
extraction and most existing approaches have tar-
geted such applications, participants were asked
to develop systems for hedge detection in bio-
logical scientific articles. Uncertainty detection
is also important, e.g. in encyclopedias, where
the goal is to collect reliable world knowledge
about real-world concepts and topics. For exam-
ple, Wikipedia explicitly declares that statements
reflecting author opinions or those not backed up
by facts (e.g. references) should be avoided (see
3.2 for details). Thus, the community-edited en-
cyclopedia, Wikipedia became one of the subjects
of the shared task as well.
3.1 Hedges in Biological Scientific Articles
In the biomedical domain, sentences were manu-
ally annotated for both hedge cues and their lin-
guistic scope. Hedging is typically expressed by
using specific linguistic devices (which we refer to
as cues in this article) that modify the meaning or
reflect the author?s attitude towards the content of
the text. Typical hedge cues fall into the following
categories:
? auxiliaries: may, might, can, would, should,
could, etc.
? verbs of hedging or verbs with speculative
content: suggest, question, presume, suspect,
indicate, suppose, seem, appear, favor, etc.
? adjectives or adverbs: probable, likely, possi-
ble, unsure, etc.
? conjunctions: or, and/or, either . . . or, etc.
However, there are some cases where a hedge is
expressed via a phrase rather than a single word.
Complex keywords are phrases that express un-
certainty together, but not on their own (either the
semantic interpretation or the hedging strength of
its subcomponents are significantly different from
those of the whole phrase). An instance of a com-
plex keyword can be seen in the following sen-
tence:
Mild bladder wall thickening {raises
the question of cystitis}.
The expression raises the question of may be sub-
stituted by suggests and neither the verb raises nor
the noun question convey speculative meaning on
their own. However, the whole phrase is specula-
tive therefore it is marked as a hedge cue.
2
During the annotation process, a min-max strat-
egy for the marking of keywords (min) and their
scope (max) was followed. On the one hand, when
marking the keywords, the minimal unit that ex-
presses hedging and determines the actual strength
of hedging was marked as a keyword. On the other
hand, when marking the scopes of speculative key-
words, the scope was extended to the largest syn-
tactic unit possible. That is, all constituents that
fell within the uncertain interpretation were in-
cluded in the scope. Our motivation here was that
in this way, if we simply disregard the marked text
span, the rest of the sentence can usually be used
for extracting factual information (if there is any).
For instance, in the example above, we can be sure
that the symptom mild bladder wall thickening is
exhibited by the patient but a diagnosis of cystitis
would be questionable.
The scope of a speculative element can be de-
termined on the basis of syntax. The scopes of
the BioScope corpus are regarded as consecutive
text spans and their annotation was based on con-
stituency grammar. The scope of verbs, auxil-
iaries, adjectives and adverbs usually starts right
with the keyword. In the case of verbal elements,
i.e. verbs and auxiliaries, it ends at the end of the
clause or sentence, thus all complements and ad-
juncts are included. The scope of attributive ad-
jectives generally extends to the following noun
phrase, whereas the scope of predicative adjec-
tives includes the whole sentence. Sentential ad-
verbs have a scope over the entire sentence, while
the scope of other adverbs usually ends at the end
of the clause or sentence. Conjunctions generally
have a scope over the syntactic unit whose mem-
bers they coordinate. Some linguistic phenomena
(e.g. passive voice or raising) can change scope
boundaries in the sentence, thus they were given
special attention during the annotation phase.
3.2 Wikipedia Weasels
The chief editors of Wikipedia have drawn the at-
tention of the public to uncertainty issues they call
weasel1. A word is considered to be a weasel
word if it creates an impression that something im-
portant has been said, but what is really commu-
nicated is vague, misleading, evasive or ambigu-
ous. Weasel words do not give a neutral account
of facts, rather, they offer an opinion without any
1http://en.wikipedia.org/wiki/Weasel_
word
backup or source. The following sentence does
not specify the source of information, it is just the
vague term some people that refers to the holder of
this opinion:
Some people claim that this results in a
better taste than that of other diet colas
(most of which are sweetened with as-
partame alone).
Statements with weasel words usually evoke ques-
tions such as Who says that?, Whose opinion is
this? and How many people think so?.
Typical instances of weasels can be grouped in
the following way (we offer some examples as
well):
? Adjectives and adverbs
? elements referring to uncertainty: prob-
able, likely, possible, unsure, often, pos-
sibly, allegedly, apparently, perhaps,
etc.
? elements denoting generalization:
widely, traditionally, generally, broadly-
accepted, widespread, etc.
? qualifiers and superlatives: global, su-
perior, excellent, immensely, legendary,
best, (one of the) largest, most promi-
nent, etc.
? elements expressing obviousness:
clearly, obviously, arguably, etc.
? Auxiliaries
? may, might, would, should, etc.
? Verbs
? verbs with speculative content and their
passive forms: suggest, question, pre-
sume, suspect, indicate, suppose, seem,
appear, favor, etc.
? passive forms with dummy subjects: It
is claimed that . . . It has been men-
tioned . . . It is known . . .
? there is / there are constructions: There
is evidence/concern/indication that. . .
? Numerically vague expressions / quantifiers
? certain, numerous, many, most, some,
much, everyone, few, various, one group
of, etc. Experts say . . . Some people
think . . .More than 60% percent . . .
3
? Nouns
? speculation, proposal, consideration,
etc. Rumour has it that . . . Common
sense insists that . . .
However, the use of the above words or grammat-
ical devices does not necessarily entail their being
a weasel cue since their use may be justifiable in
their contexts.
As the main application goal of weasel detec-
tion is to highlight articles which should be im-
proved (by reformulating or adding factual is-
sues), we decided to annotate only weasel cues
in Wikipedia articles, but we did not mark their
scopes.
During the manual annotation process, the fol-
lowing cue marking principles were employed.
Complex verb phrases were annotated as weasel
cues since in some cases, both the passive con-
struction and the verb itself are responsible for the
weasel. In passive forms with dummy subjects and
there is / there are constructions, the weasel cue
included the grammatical subject (i.e. it and there)
as well. As for numerically vague expressions, the
noun phrase containing a quantifier was marked
as a weasel cue. If there was no quantifier (in the
case of a bare plural), the noun was annotated as
a weasel cue. Comparatives and superlatives were
annotated together with their article. Anaphoric
pronouns referring to a weasel word were also an-
notated as weasel cues.
4 Task Definitions
Two uncertainty detection tasks (sentence clas-
sification and in-sentence hedge scope detec-
tion) in two domains (biological publications and
Wikipedia articles) with three types of submis-
sions (closed, cross and open) were given to the
participants of the CoNLL-2010 Shared Task.
4.1 Detection of Uncertain Sentences
The aim of Task1 was to develop automatic proce-
dures for identifying sentences in texts which con-
tain unreliable or uncertain information. In par-
ticular, this task is a binary classification problem,
i.e. factual and uncertain sentences have to be dis-
tinguished.
As training and evaluation data
? Task1B: biological abstracts and full articles
(evaluation data contained only full articles)
from the BioScope corpus and
? Task1W: paragraphs from Wikipedia possi-
bly containing weasel information
were provided. The annotation of weasel/hedge
cues was carried out on the phrase level, and sen-
tences containing at least one cue were considered
as uncertain, while sentences with no cues were
considered as factual. The participating systems
had to submit a binary classification (certain vs.
uncertain) of the test sentences while marking cues
in the submissions was voluntary (but participants
were encouraged to do this).
4.2 In-sentence Hedge Scope Resolution
For Task2, in-sentence scope resolvers had to be
developed. The training and evaluation data con-
sisted of biological scientific texts, in which in-
stances of speculative spans ? that is, keywords
and their linguistic scope ? were annotated manu-
ally. Submissions to Task2 were expected to auto-
matically annotate the cue phrases and the left and
right boundaries of their scopes (exactly one scope
must be assigned to a cue phrase).
4.3 Evaluation Metrics
The evaluation for Task1 was carried out at the
sentence level, i.e. the cue annotations in the sen-
tence were not taken into account. The F?=1 mea-
sure (the harmonic mean of precision and recall)
of the uncertain class was employed as the chief
evaluation metric.
The Task2 systems were expected to mark cue-
and corresponding scope begin/end tags linked to-
gether by using some unique IDs. A scope-level
F?=1 measure was used as the chief evaluation
metric where true positives were scopes which ex-
actly matched the gold standard cue phrases and
gold standard scope boundaries assigned to the cue
word. That is, correct scope boundaries with in-
correct cue annotation and correct cue words with
bad scope boundaries were both treated as errors.
This scope-level metric is very strict. For in-
stance, the requirement of the precise match of the
cue phrase is questionable as ? from an application
point of view ? the goal is to find uncertain text
spans and the evidence for this is not so impor-
tant. However, the annotation of cues in datasets
is essential for training scope detectors since lo-
cating the cues usually precedes the identification
of their scope. Hence we decided to incorporate
cue matches into the evaluation metric.
4
Another questionable issue is the strict bound-
ary matching requirement. For example, includ-
ing or excluding punctuations, citations or some
bracketed expressions, like (see Figure 1) from
a scope is not crucial for an otherwise accurate
scope detector. On the other hand, the list of
such ignorable phenomena is arguable, especially
across domains. Thus, we considered the strict
boundary matching to be a straightforward and un-
ambiguous evaluation criterion. Minor issues like
those mentioned above could be handled by sim-
ple post-processing rules. In conclusion we think
that the uncertainty detection community may find
more flexible evaluation criteria in the future but
the strict scope-level metric is definitely a good
starting point for evaluation.
4.4 Closed and Open Challenges
Participants were invited to submit results in dif-
ferent configurations, where systems were allowed
to exploit different kinds of annotated resources.
The three possible submission categories were:
? Closed, where only the labeled and unla-
beled data provided for the shared task were
allowed, separately for each domain (i.e.
biomedical train data for biomedical test set
and Wikipedia train data for Wikipedia test
set). No further manually crafted resources
of uncertainty information (i.e. lists, anno-
tated data, etc.) could be used in any domain.
On the other hand, tools exploiting the man-
ual annotation of linguistic phenomena not
related to uncertainty (such as POS taggers
and parsers trained on labeled corpora) were
allowed.
? Cross-domain was the same as the closed one
but all data provided for the shared task were
allowed for both domains (i.e. Wikipedia
train data for the biomedical test set, the
biomedical train data for Wikipedia test set
or a union of Wikipedia and biomedical train
data for both test sets).
? Open, where any data and/or any additional
manually created information and resource
(which may be related to uncertainty) were
allowed for both domains.
The motivation behind the cross-domain and the
open challenges was that in this way, we could
assess whether adding extra (i.e. not domain-
specific) information to the systems can contribute
to the overall performance.
5 Datasets
Training and evaluation corpora were annotated
manually for hedge/weasel cues and their scope
by two independent linguist annotators. Any dif-
ferences between the two annotations were later
resolved by the chief annotator, who was also re-
sponsible for creating the annotation guidelines
and training the two annotators. The datasets
are freely available2 for further benchmark experi-
ments at http://www.inf.u-szeged.hu/
rgai/conll2010st.
Since uncertainty cues play an important role
in detecting sentences containing uncertainty, they
are tagged in the Task1 datasets as well to enhance
training and evaluation of systems.
5.1 Biological Publications
The biological training dataset consisted of the bi-
ological part of the BioScope corpus (Vincze et al,
2008), hence it included abstracts from the GE-
NIA corpus, 5 full articles from the functional ge-
nomics literature (related to the fruit fly) and 4 ar-
ticles from the open access BMC Bioinformatics
website. The automatic segmentation of the doc-
uments was corrected manually and the sentences
(14541 in number) were annotated manually for
hedge cues and their scopes.
The evaluation dataset was based on 15 biomed-
ical articles downloaded from the publicly avail-
able PubMedCentral database, including 5 ran-
dom articles taken from the BMC Bioinformat-
ics journal in October 2009, 5 random articles to
which the drosophila MeSH term was assigned
and 5 random articles having the MeSH terms
human, blood cells and transcription factor (the
same terms which were used to create the Genia
corpus). These latter ten articles were also pub-
lished in 2009. The aim of this article selection
procedure was to have a theme that was close to
the training corpus. The evaluation set contained
5003 sentences, out of which 790 were uncertain.
These texts were manually annotated for hedge
cues and their scope. To annotate the training and
the evaluation datasets, the same annotation prin-
ciples were applied.
2under the Creative Commons Attribute Share Alike li-
cense
5
For both Task1 and Task2, the same dataset was
provided, the difference being that for Task1, only
hedge cues and sentence-level uncertainty were
given, however, for Task2, hedge cues and their
scope were marked in the text.
5.2 Wikipedia Datasets
2186 paragraphs collected from Wikipedia
archives were also offered as Task1 training
data (11111 sentences containing 2484 uncertain
ones). The evaluation dataset contained 2346
Wikipedia paragraphs with 9634 sentences, out of
which 2234 were uncertain.
For the selection of the Wikipedia paragraphs
used to construct the training and evaluation
datasets, we exploited the weasel tags added by
the editors of the encyclopedia (marking unsup-
ported opinions or expressions of a non-neutral
point of view). Each paragraph containing weasel
tags (5874 different ones) was extracted from the
history dump of EnglishWikipedia. First, 438 ran-
domly selected paragraphs were manually anno-
tated from this pool then the most frequent cue
phrases were collected. Later on, two other sets
of Wikipedia paragraphs were gathered on the ba-
sis of whether they contained such cue phrases or
not. The aim of this sampling procedure was to
provide large enough training and evaluation sam-
ples containing weasel words and also occurrences
of typical weasel words in non-weasel contexts.
Each sentence was annotated manually for
weasel cues. Sentences were treated as uncer-
tain if they contained at least one weasel cue, i.e.
the scope of weasel words was the entire sentence
(which is supposed to be rewritten by Wikipedia
editors).
5.3 Unlabeled Data
Unannotated but pre-processed full biological arti-
cles (150 articles from the publicly available Pub-
MedCentral database) and 1 million paragraphs
from Wikipedia were offered to the participants as
well. These datasets did not contain any manual
annotation for uncertainty, but their usage permit-
ted data sampling from a large pool of in-domain
texts without time-wasting pre-processing tasks
(cleaning and sentence splitting).
5.4 Data Format
Both training and evaluation data were released
in a custom XML format. For each task, a sep-
arate XML file was made available containing the
whole document set for the given task. Evaluation
datasets were available in the same format as train-
ing data without any sentence-level certainty, cue
or scope annotations.
The XML format enabled us to provide more
detailed information about the documents such as
segment boundaries and types (e.g. section titles,
figure captions) and it is the straightforward for-
mat to represent nested scopes. Nested scopes
have overlapping text spans which may contain
cues for multiple scopes (there were 1058 occur-
rences in the training and evaluation datasets to-
gether). The XML format utilizes id-references
to determine the scope of a given cue. Nested
constructions are rather complicated to represent
in the standard IOB format, moreover, we did not
want to enforce a uniform tokenization.
To support the processing of the data files,
reader and writer software modules were devel-
oped and offered to the participants for the uCom-
pare (Kano et al, 2009) framework. uCompare
provides a universal interface (UIMA) and several
text mining and natural language processing tools
(tokenizers, POS taggers, syntactic parsers, etc.)
for general and biological domains. In this way
participants could configure and execute a flexible
chain of analyzing tools even with a graphical UI.
6 Submissions and Results
Participants uploaded their results through the
shared task website, and the official evaluation was
performed centrally. After the evaluation period,
the results were published for the participants on
the Web. A total of 23 teams participated in the
shared task. 22, 16 and 13 teams submitted output
for Task1B, Task1W and Task2, respectively.
6.1 Results
Tables 1, 2 and 3 contain the results of the submit-
ted systems for Task1 and Task2. The last name
of the first author of the system description pa-
per (published in these proceedings) is used here
as a system name3. The last column contains the
type of submission. The system of Kilicoglu and
Bergler (2010) is the only open submission. They
adapted their system introduced in Kilicoglu and
Bergler (2008) to the datasets of the shared task.
Regarding cross submissions, Zhao et al (2010)
and Ji et al (2010) managed to achieve a no-
ticeable improvement by exploiting cross-domain
3O?zgu?r did not publish a description of her system.
6
Name P / R / F type
Georgescul 72.0 / 51.7 / 60.2 C
Ji 62.7 / 55.3 / 58.7 X
Chen 68.0 / 49.7 / 57.4 C
Morante 80.6 / 44.5 / 57.3 C
Zhang 76.6 / 44.4 / 56.2 C
Zheng 76.3 / 43.6 / 55.5 C
Ta?ckstro?m 78.3 / 42.8 / 55.4 C
Mamani Sa?nchez 68.3 / 46.2 / 55.1 C
Tang 82.3 / 41.4 / 55.0 C
Kilicoglu 67.9 / 46.0 / 54.9 O
Tjong Kim Sang 74.0 / 43.0 / 54.4 C
Clausen 75.1 / 42.0 / 53.9 C
O?zgu?r 59.4 / 47.9 / 53.1 C
Zhou 85.3 / 36.5 / 51.1 C
Li 88.4 / 31.9 / 46.9 C
Prabhakaran 88.0 / 28.4 / 43.0 C
Ji 94.2 / 6.6 / 12.3 C
Table 1: Task1 Wikipedia results (type ?
{Closed(C), Cross(X), Open(O)}).
data. Zhao et al (2010) extended the biological
cue word dictionary of their system ? using it as
a feature for classification ? by the frequent cues
of the Wikipedia dataset, while Ji et al (2010)
used the union of the two datasets for training
(they have reported an improvement from 47.0 to
58.7 on the Wikipedia evaluation set after a post-
challenge bugfix).
Name P / R / F type
Morante 59.6 / 55.2 / 57.3 C
Rei 56.7 / 54.6 / 55.6 C
Velldal 56.7 / 54.0 / 55.3 C
Kilicoglu 62.5 / 49.5 / 55.2 O
Li 57.4 / 47.9 / 52.2 C
Zhou 45.6 / 43.9 / 44.7 O
Zhou 45.3 / 43.6 / 44.4 C
Zhang 46.0 / 42.9 / 44.4 C
Fernandes 46.0 / 38.0 / 41.6 C
Vlachos 41.2 / 35.9 / 38.4 C
Zhao 34.8 / 41.0 / 37.7 C
Tang 34.5 / 31.8 / 33.1 C
Ji 21.9 / 17.2 / 19.3 C
Ta?ckstro?m 2.3 / 2.0 / 2.1 C
Table 2: Task2 results (type ? {Closed(C),
Open(O)}).
Each Task2 and Task1W system achieved a
Name P / R / F type
Tang 85.0 / 87.7 / 86.4 C
Zhou 86.5 / 85.1 / 85.8 C
Li 90.4 / 81.0 / 85.4 C
Velldal 85.5 / 84.9 / 85.2 C
Vlachos 85.5 / 84.9 / 85.2 C
Ta?ckstro?m 87.1 / 83.4 / 85.2 C
Shimizu 88.1 / 82.3 / 85.1 C
Zhao 83.4 / 84.8 / 84.1 X
O?zgu?r 77.8 / 91.3 / 84.0 C
Rei 83.8 / 84.2 / 84.0 C
Zhang 82.6 / 84.7 / 83.6 C
Kilicoglu 92.1 / 74.9 / 82.6 O
Morante 80.5 / 83.3 / 81.9 X
Morante 81.1 / 82.3 / 81.7 C
Zheng 73.3 / 90.8 / 81.1 C
Tjong Kim Sang 74.3 / 87.1 / 80.2 C
Clausen 79.3 / 80.6 / 80.0 C
Szidarovszky 70.3 / 91.0 / 79.3 C
Georgescul 69.1 / 91.0 / 78.5 C
Zhao 71.0 / 86.6 / 78.0 C
Ji 79.4 / 76.3 / 77.9 C
Chen 74.9 / 79.1 / 76.9 C
Fernandes 70.1 / 71.1 / 70.6 C
Prabhakaran 67.5 / 19.5 / 30.3 X
Table 3: Task1 biological results (type ?
{Closed(C), Cross(X), Open(O)}).
higher precision than recall. There may be two
reasons for this. The systems may have applied
only reliable patterns, or patterns occurring in the
evaluation set may be imperfectly covered by the
training datasets. The most intense participation
was on Task1B. Here, participants applied vari-
ous precision/recall trade-off strategies. For in-
stance, Tang et al (2010) achieved a balanced pre-
cision/recall configuration, while Li et al (2010)
achieved third place thanks to their superior preci-
sion.
Tables 4 and 5 show the cue-level performances,
i.e. the F-measure of cue phrase matching where
true positives were strict matches. Note that it was
optional to submit cue annotations for Task1 (if
participants submitted systems for both Task2 and
Task1B with cue tagging, only the better score of
the two was considered).
It is interesting to see that Morante et al (2010)
who obtained the best results on Task2 achieved
a medium-ranked F-measure on the cue-level (e.g.
their result on the cue-level is lower by 4% com-
7
pared to Zhou et al (2010), while on the scope-
level the difference is 13% in the reverse direc-
tion), which indicates that the real strength of the
system of Morante et al (2010) is the accurate de-
tection of scope boundaries.
Name P / R / F
Tang 63.0 / 25.7 / 36.5
Li 76.1 / 21.6 / 33.7
O?zgu?r 28.9 / 14.7 / 19.5
Morante 24.6 / 7.3 / 11.3
Table 4: Wikipedia cue-level results.
Name P / R / F type
Tang 81.7 / 81.0 / 81.3 C
Zhou 83.1 / 78.8 / 80.9 C
Li 87.4 / 73.4 / 79.8 C
Rei 81.4 / 77.4 / 79.3 C
Velldal 81.2 / 76.3 / 78.7 C
Zhang 82.1 / 75.3 / 78.5 C
Ji 78.7 / 76.2 / 77.4 C
Morante 78.8 / 74.7 / 76.7 C
Kilicoglu 86.5 / 67.7 / 76.0 O
Vlachos 82.0 / 70.6 / 75.9 C
Zhao 76.7 / 73.9 / 75.3 X
Fernandes 79.2 / 64.7 / 71.2 C
Zhao 63.7 / 74.1 / 68.5 C
Ta?ckstro?m 66.9 / 58.6 / 62.5 C
O?zgu?r 49.1 / 57.8 / 53.1 C
Table 5: Biological cue-level results (type ?
{Closed(C), Cross(X), Open(O)}).
6.2 Approaches
The approaches to Task1 fall into two major cat-
egories. There were six systems which handled
the task as a classical sentence classification prob-
lem and employed essentially a bag-of-words fea-
ture representation (they are marked as BoW in
Table 6). The remaining teams focused on the
cue phrases and sought to classify every token if
it was a part of a cue phrase, then a sentence was
predicted as uncertain if it contained at least one
recognized cue phrase. Five systems followed a
pure token classification approach (TC) for cue de-
tection while others used sequential labeling tech-
niques (usually Conditional Random Fields) to
identify cue phrases in sentences (SL).
The feature set employed in Task1 systems typ-
ically consisted of the wordform, its lemma or
stem, POS and chunk codes and about the half of
the participants constructed features from the de-
pendency and/or constituent parse tree of the sen-
tences as well (see Table 6 for details).
It is interesting to see that the top ranked sys-
tems of Task1B followed a sequence labeling ap-
proach, while the best systems on Task1W applied
a bag-of-words sentence classification. This may
be due to the fact that biological sentences have
relatively simple patterns. Thus the context of the
cue words (token classification-based approaches
used features derived from a window of the token
in question, thus, they exploited the relationship
among the tokens and their contexts) can be uti-
lized while Wikipedia weasels have a diverse na-
ture. Another observation is that the top systems
in both Task1B and Task1W are the ones which
did not derive features from syntactic parsing.
Each Task2 system was built upon a Task1 sys-
tem, i.e. they attempted to recognize the scopes
for the predicted cue phrases (however, Zhang et
al. (2010) have argued that the objective functions
of Task1 and Task2 cue detection problems are
different because of sentences containing multiple
hedge spans).
Most systems regarded multiple cues in a sen-
tence to be independent from each other and
formed different classification instances from
them. There were three systems which incor-
porated information about other hedge cues (e.g.
their distance) of the sentence into the feature
space and Zhang et al (2010) constructed a cas-
cade system which utilized directly the predicted
scopes (it processes cue phrases from left to right)
during predicting other scopes in the same sen-
tence.
The identification of the scope for a certain cue
was typically carried out by classifying each to-
ken in the sentence. Task2 systems differ in the
number of class labels used as target and in the
machine learning approaches applied. Most sys-
tems ? following Morante and Daelemans (2009)
? used three class labels (F)IRST, (L)AST and
NONE. Two participants used four classes by
adding (I)NSIDE, while three systems followed
a binary classification approach (SCOPE versus
NONSCOPE). The systems typically included a
post-processing procedure to force scopes to be
continuous and to include the cue phrase in ques-
tion. The machine learning methods applied can
be again categorized into sequence labeling (SL)
8
NA
ME
ap
pro
ach
ma
ch
ine
fea
tur
e
fea
tur
es
em
plo
ye
d
lea
rne
r
sel
ect
ion
dic
t
ort
ho
lem
ma
/st
em
PO
S
ch
un
k
de
p
do
cp
art
oth
er
Cl
au
sen
Bo
W
Ma
xE
nt
+
+
he
dg
ec
ue
dis
tan
ce
Ch
en
Bo
W
Ma
xE
nt
sta
tis
tic
al
+
+
+
+
sen
ten
cel
en
gth
Fe
rna
nd
es
SL
ET
L
+
+
+
Ge
org
esc
ul
Bo
W
SV
M+
pa
ram
tun
ing
+
Ji
TC
Mo
dA
vg
Pe
rce
ptr
on
+
Ki
lic
og
lu
TC
ma
nu
al
+
+
+
ex
ter
na
ld
ict
Li
SL
CR
F+
po
stp
roc
gre
ed
yf
wd
+
+
+
Ma
ma
ni
Sa?
nc
he
z
Bo
W
SV
MT
ree
Ke
rne
l
+
+
+
+
+
Mo
ran
te
(w
iki
)
TC
SV
M+
po
stp
roc
sta
tis
tic
al
+
+
+
+
+
Mo
ran
te
(bi
o)
SL
KN
N
sta
tis
tic
al
+
+
+
+
+
+
Pra
bh
ak
ara
n
SL
CR
F
gre
ed
yf
wd
+
+
+
+
Le
vin
Cl
ass
Re
i
SL
CR
F
+
+
+
+
Sh
im
izu
SL
Ba
ye
sP
oin
tM
ach
ine
s
GA
+
+
+
+
NE
s,u
nla
be
led
da
ta
Sz
ida
rov
szk
y
SL
CR
F
ex
ha
ust
ive
+
+
+
Ta?
ck
str
o?m
Bo
W
SV
M
gre
ed
yf
wd
+
+
+
+
+
sen
ten
cel
en
gth
Ta
ng
SL
CR
F,S
VM
HM
M
sta
tis
tic
al
+
+
+
+
+
Tjo
ng
Ki
m
Sa
ng
TC
Na
ive
Ba
ye
s
Ve
lld
al
TC
Ma
xE
nt
ma
nu
al
+
+
+
+
Vl
ach
os
TC
Ba
ye
sia
nL
og
Re
g
ma
nu
al
+
+
+
+
Zh
an
g
SL
CR
F+
fea
tur
ec
om
bin
ati
on
gre
ed
yf
wd
+
+
+
+
+
NE
s
Zh
ao
SL
CR
F
sta
tis
tic
al
+
+
+
+
Zh
en
g
SL
CR
F,M
ax
En
t
ma
nu
al
+
+
+
+
Co
nst
itu
en
tP
ars
ing
Zh
ou
SL
CR
F
sta
tis
tic
al
+
+
+
+
Wo
rdN
et
Ta
ble
6:
Sy
ste
m
arc
hit
ect
ure
so
ve
rvi
ew
for
Ta
sk1
.A
pp
roa
ch
es:
seq
ue
nc
el
ab
eli
ng
(SL
),t
ok
en
cla
ssi
fic
ati
on
(T
C)
,b
ag
-of
-w
ord
sm
od
el
(B
oW
);M
ach
ine
lea
rne
rs:
En
tro
py
Gu
ide
dT
ran
sfo
rm
ati
on
Le
arn
ing
(E
TL
),A
ve
rag
ed
Pe
rce
ptr
on
(A
P),
k-n
ear
est
ne
igh
bo
ur
(K
NN
);F
eat
ure
sel
ect
ion
:g
ath
eri
ng
ph
ras
es
fro
m
the
tra
ini
ng
co
rpu
su
sin
gs
tat
ist
ica
lth
res
ho
lds
(st
ati
sti
cal
);F
eat
ure
s:
ort
ho
gra
ph
ica
lin
for
ma
tio
na
bo
ut
the
tok
en
(or
tho
),l
em
ma
or
ste
m
of
the
tok
en
(st
em
),P
art
-of
-Sp
eec
h
co
de
s(
PO
S),
syn
tac
tic
ch
un
ki
nfo
rm
ati
on
(ch
un
k),
de
pe
nd
en
cy
pa
rsi
ng
(de
p),
po
sit
ion
ins
ide
the
do
cu
me
nt
or
sec
tio
ni
nfo
rm
ati
on
(do
cp
os)
an
dt
ok
en
cla
ssi
fic
ati
on
(T
C)
ap
pro
ach
es
(se
eT
ab
le
7).
Th
ef
eat
ure
set
su
sed
he
re
are
the
sam
ea
sf
or
Ta
sk1
,e
xte
nd
ed
by
sev
era
lf
eat
ure
sd
esc
rib
ing
the
rel
ati
on
shi
pb
etw
een
the
cu
ep
hra
se
an
dt
he
tok
en
in
qu
est
ion
mo
stl
yb
yd
esc
rib
ing
the
de
pe
nd
en
cy
pa
th
be
tw
een
the
m.
9
NAME approach scope ML postproc tree dep multihedge
Fernandes TC FL ETL
Ji TC I AP +
Kilicoglu HC manual + + +
Li SL FL CRF, SVMHMM + + +
Morante TC FL KNN + +
Rei SL FIL manual+CRF + +
Ta?ckstro?m TC FI SVM +
Tang SL FL CRF + + +
Velldal HC manual +
Vlachos TC I Bayesian MaxEnt + +
Zhang SL FIL CRF + +
Zhao SL FL CRF +
Zhou SL FL CRF + +
Table 7: System architectures overview for Task2. Approaches: sequence labeling (SL), token clas-
sification (TC), hand-crafted rules (HC); Machine learners: Entropy Guided Transformation Learning
(ETL), Averaged Perceptron (AP), k-nearest neighbour (KNN); The way of identifying scopes: predict-
ing first/last tokens (FL), first/inside/last tokens (FIL), just inside tokens (I); Multiple Hedges: the system
applied a mechanism for handling multiple hedges inside a sentence
and token classification (TC) approaches (see Ta-
ble 7). The feature sets used here are the same
as for Task1, extended by several features describ-
ing the relationship between the cue phrase and the
token in question mostly by describing the depen-
dency path between them.
7 Conclusions
The CoNLL-2010 Shared Task introduced the
novel task of uncertainty detection. The challenge
consisted of a sentence identification task on un-
certainty (Task1) and an in-sentence hedge scope
detection task (Task2). In the latter task the goal
of automatic systems was to recognize speculative
text spans inside sentences.
The relatively high number of participants in-
dicates that the problem is rather interesting for
the Natural Language Processing community. We
think that this is due to the practical importance
of the task for (principally biomedical) applica-
tions and because it addresses several open re-
search questions. Although several approaches
were introduced by the participants of the shared
task and we believe that the ideas described in
this proceedings can serve as an excellent starting
point for the development of an uncertainty de-
tector, there is a lot of room for improving such
systems. The manually annotated datasets and
software tools developed for the shared task may
act as benchmarks for these future experiments
(they are freely available at http://www.inf.
u-szeged.hu/rgai/conll2010st).
Acknowledgements
The authors would like to thank Joakim Nivre
and Llu??s Ma?rquez for their useful suggestions,
comments and help during the organisation of the
shared task.
This work was supported in part by the
National Office for Research and Technol-
ogy (NKTH, http://www.nkth.gov.hu/)
of the Hungarian government within the frame-
work of the projects TEXTREND, BELAMI and
MASZEKER.
References
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. TEXT2TABLE: Medical Text Summa-
rization System Based on Named Entity Recogni-
tion and Modality Identification. In Proceedings of
the BioNLP 2009 Workshop, pages 185?192, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Wendy W. Chapman, David Chu, and John N. Dowl-
ing. 2007. ConText: An Algorithm for Identifying
Contextual Features from Clinical Text. In Proceed-
ings of the ACL Workshop on BioNLP 2007, pages
81?88.
Mike Conway, Son Doan, and Nigel Collier. 2009. Us-
ing Hedges to Enhance a Disease Outbreak Report
10
Text Mining System. In Proceedings of the BioNLP
2009 Workshop, pages 142?143, Boulder, Colorado,
June. Association for Computational Linguistics.
Carol Friedman, Philip O. Alderson, John H. M.
Austin, James J. Cimino, and Stephen B. Johnson.
1994. A General Natural-language Text Processor
for Clinical Radiology. Journal of the American
Medical Informatics Association, 1(2):161?174.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ingWikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173?176, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Feng Ji, Xipeng Qiu, and Xuanjing Huang. 2010. De-
tecting Hedge Cues and their Scopes with Average
Perceptron. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 139?146,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Yoshinobu Kano, William A. Baumgartner, Luke
McCrohon, Sophia Ananiadou, Kevin B. Cohen,
Lawrence Hunter, and Jun?ichi Tsujii. 2009. U-
Compare: Share and Compare Text Mining Tools
with UIMA. Bioinformatics, 25(15):1997?1998,
August.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing Speculative Language in Biomedical Research
Articles: A Linguistically Motivated Perspective.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
46?53, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2009. Syn-
tactic Dependency Based Heuristics for Biological
Event Extraction. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 119?127, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2010. A High-
Precision Approach to Detecting Hedges and Their
Scopes. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 103?110, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
George Lakoff. 1972. Linguistics and natural logic.
In The Semantics of Natural Language, pages 545?
665, Dordrecht. Reidel.
Xinxin Li, Jianping Shen, Xiang Gao, and Xuan
Wang. 2010. Exploiting Rich Features for Detect-
ing Hedges and Their Scope. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning (CoNLL-2010): Shared Task,
pages 36?41, Uppsala, Sweden, July. Association
for Computational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Proceed-
ings of the HLT-NAACL 2004 Workshop: Biolink
2004, Linking Biological Literature, Ontologies and
Databases, pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the ACL, pages 992?
999, Prague, Czech Republic, June.
Roser Morante andWalter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based Resolution of In-
sentence Scopes of Hedge Cues. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 48?55, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. De-
tecting Speculations and their Scopes in Scientific
Text. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1398?1407, Singapore, August. Associ-
ation for Computational Linguistics.
Gyo?rgy Szarvas. 2008. Hedge Classification in
Biomedical Texts with a Weakly Supervised Selec-
tion of Keywords. In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, Ohio, June. Association
for Computational Linguistics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A Cascade Method for De-
tecting Hedges and their Scope in Natural Language
Text. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 25?29, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,
and Yves Van de Peer. 2009. Analyzing Text in
Search of Bio-molecular Events: A High-precision
Machine Learning Framework. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
11
Shared Task, pages 128?136, Boulder, Colorado,
June. Association for Computational Linguistics.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
Shaodian Zhang, Hai Zhao, Guodong Zhou, and Bao-
liang Lu. 2010. Hedge Detection and Scope Find-
ing by Sequence Labeling with Procedural Feature
Selection. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 70?77, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong
Cheng. 2010. Learning to Detect Hedges and their
Scope Using CRF. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 64?
69, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Huiwei Zhou, Xiaoyan Li, Degen Huang, Zezhong Li,
and Yuansheng Yang. 2010. Exploiting Multi-
Features to Detect Hedges and Their Scope in
Biomedical Texts. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 56?
63, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
12
