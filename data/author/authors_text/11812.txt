Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 105?108,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Query-Focused Summaries or Query-Biased Summaries ?
Rahul Katragadda
Language Technologies Research Center
IIIT Hyderabad
rahul k@research.iiit.ac.in
Vasudeva Varma
Language Technologies Research Center
IIIT Hyderabad
vv@iiit.ac.in
Abstract
In the context of the Document Understand-
ing Conferences, the task of Query-Focused
Multi-Document Summarization is intended to
improve agreement in content among human-
generated model summaries. Query-focus also
aids the automated summarizers in directing
the summary at specific topics, which may re-
sult in better agreement with these model sum-
maries. However, while query focus corre-
lates with performance, we show that high-
performing automatic systems produce sum-
maries with disproportionally higher query
term density than human summarizers do. Ex-
perimental evidence suggests that automatic
systems heavily rely on query term occurrence
and repetition to achieve good performance.
1 Introduction
The problem of automatically summarizing text doc-
uments has received a lot of attention since the early
work by Luhn (Luhn, 1958). Most of the current auto-
matic summarization systems rely on a sentence extrac-
tive paradigm, where key sentences in the original text
are selected to form the summary based on the clues (or
heuristics), or learning based approaches.
Common approaches for identifying key sentences
include: training a binary classifier (Kupiec et al,
1995), training a Markov model or CRF (Conroy et al,
2004; Shen et al, 2007) or directly assigning weights
to sentences based on a variety of features and heuris-
tically determined feature weights (Toutanova et al,
2007). But, the question of which components and fea-
tures of automatic summarizers contribute most to their
performance has largely remained unanswered (Marcu
and Gerber, 2001), until Nenkova et al (Nenkova et
al., 2006) explored the contribution of frequency based
measures. In this paper, we examine the role a query
plays in automated multi-document summarization of
newswire.
One of the issues studied since the inception of auto-
matic summarization is that of human agreement: dif-
ferent people choose different content for their sum-
maries (Rath et al, 1961; van Halteren and Teufel,
2003; Nenkova et al, 2007). Later, it was as-
sumed (Dang, 2005) that having a question/query to
provide focus would improve agreement between any
two human-generated model summaries, as well as be-
tween a model summary and an automated summary.
Starting in 2005 until 2007, a query-focused multi-
document summarization task was conducted as part of
the annual Document Understanding Conference. This
task models a real-world complex question answering
scenario, where systems need to synthesize from a set
of 25 documents, a brief (250 words), well organized
fluent answer to an information need.
Query-focused summarization is a topic of ongoing
importance within the summarization and question an-
swering communities. Most of the work in this area
has been conducted under the guise of ?query-focused
multi-document summarization?, ?descriptive question
answering?, or even ?complex question answering?.
In this paper, based on structured empirical evalu-
ations, we show that most of the systems participat-
ing in DUC?s Query-Focused Multi-Document Sum-
marization (QF-MDS) task have been query-biased in
building extractive summaries. Throughout our discus-
sion, the term ?query-bias?, with respect to a sentence,
is precisely defined to mean that the sentence has at
least one query term within it. The term ?query-focus?
is less precisely defined, but is related to the cognitive
task of focusing a summary on the query, which we as-
sume humans do naturally. In other words, the human
generated model summaries are assumed to be query-
focused.
Here we first discuss query-biased content in Sum-
mary Content Units (SCUs) in Section 2 and then in
Section 3 by building formal models on query-bias we
discuss why/how automated systems are query-biased
rather than being query-focused.
2 Query-biased content in
Summary Content Units (SCUs)
Summary content units, referred as SCUs hereafter, are
semantically motivated subsentential units that are vari-
able in length but not bigger than a sentential clause.
SCUs are constructed from annotation of a collection
of human summaries on a given document collection.
They are identified by noting information that is re-
peated across summaries. The repetition is as small
as a modifier of a noun phrase or as large as a clause.
The evaluation method that is based on overlapping
SCUs in human and automatic summaries is called the
105
Figure 1: SCU annotation of a source document.
pyramid method (Nenkova et al, 2007).
The University of Ottawa has organized the pyramid
annotation data such that for some of the sentences in
the original document collection, a list of correspond-
ing content units is known (Copeck et al, 2006). A
sample of an SCU mapping from topic D0701A of
the DUC 2007 QF-MDS corpus is shown in Figure 1.
Three sentences are seen in the figure among which
two have been annotated with system IDs and SCU
weights wherever applicable. The first sentence has not
been picked by any of the summarizers participating in
Pyramid Evaluations, hence it is unknown if the sen-
tence would have contributed to any SCU. The second
sentence was picked by 8 summarizers and that sen-
tence contributed to an SCU of weight 3. The third
sentence in the example was picked by one summa-
rizer, however, it did not contribute to any SCU. This
example shows all the three types of sentences avail-
able in the corpus: unknown samples, positive samples
and negative samples.
We extracted the positive and negative samples in the
source documents from these annotations; types of sec-
ond and third sentences shown in Figure 1. A total
of 14.8% sentences were annotated to be either posi-
tive or negative. When we analyzed the positive set,
we found that 84.63% sentences in this set were query-
biased. Also, on the negative sample set, we found that
69.12% sentences were query-biased. That is, on an
average, 76.67% of the sentences picked by any au-
tomated summarizer are query-biased. On the other
hand, for human summaries only 58% sentences were
query-biased. All the above numbers are based on the
DUC 2007 dataset shown in boldface in Table 1
1
.
There is one caveat: The annotated sentences come
only from the summaries of systems that participated in
the pyramid evaluations. Since only 13 among a total
32 participating systems were evaluated using pyramid
evaluations, the dataset is limited. However, despite
this small issue, it is very clear that at least those sys-
tems that participated in pyramid evaluations have been
biased towards query-terms, or at least, they have been
better at correctly identifying important sentences from
the query-biased sentences than from query-unbiased
sentences.
1
We used DUC 2007 dataset for all experiments reported.
3 Formalizing query-bias
Our search for a formal method to capture the relation
between occurrence of query-biased sentences in the
input and in summaries resulted in building binomial
and multinomial model distributions. The distributions
estimated were then used to obtain the likelihood of a
query-biased sentence being emitted into a summary by
each system.
For the DUC 2007 data, there were 45 summaries
for each of the 32 systems (labeled 1-32) among which
2 were baselines (labeled 1 and 2), and 18 summaries
from each of 10 human summarizers (labeled A-J). We
computed the log-likelihood, log(L[summary;p(C
i
)]),
of all human and machine summaries from DUC?07
query focused multi-document summarization task,
based on both distributions described below (see Sec-
tions 3.1, 3.2).
3.1 The binomial model
We represent the set of sentences as a binomial distribu-
tion over type of sentences. Let C
0
and C
1
denote the
sets of sentences without and with query-bias respec-
tively. Let p(C
i
) be the probability of emitting a sen-
tence from a specified set. It is also obvious that query-
biased sentences will be assigned lower emission prob-
abilities, because the occurrence of query-biased sen-
tences in the input is less likely. On average each topic
has 549 sentences, among which 196 contain a query
term; which means only 35.6% sentences in the input
were query-biased. Hence, the likelihood function here
denotes the likelihood of a summary to contain non
query-biased sentences. Humans? and systems? sum-
maries must now constitute low likelihood to show that
they rely on query-bias.
The likelihood of a summary then is :
L[summary; p(C
i
)] =
N !
n
0
!n
1
!
p(C
0
)
n
0
p(C
1
)
n
1
(1)
Where N is the number of sentences in the sum-
mary, and n
0
+ n
1
= N; n
0
and n
1
are the cardinali-
ties of C
0
and C
1
in the summary. Table 2 shows var-
ious systems with their ranks based on ROUGE-2 and
the average log-likelihood scores. The ROUGE (Lin,
2004) suite of metrics are n-gram overlap based met-
rics that have been shown to highly correlate with hu-
man evaluations on content responsiveness. ROUGE-2
and ROUGE-SU4 are the official ROUGE metrics for
evaluating query-focused multi-document summariza-
tion task since DUC 2005.
3.2 The multinomial model
In the previous section (Section 3.1), we described
the binomial model where we classified each sentence
as being query-biased or not. However, if we were
to quantify the amount of query-bias in a sentence,
we associate each sentence to one among k possible
classes leading to a multinomial distribution. Let C
i
?
106
Dataset total positive biased positive negative biased negative % bias in positive % bias in negative
DUC 2005 24831 1480 1127 1912 1063 76.15 55.60
DUC 2006 14747 1047 902 1407 908 86.15 71.64
DUC 2007 12832 924 782 975 674 84.63 69.12
Table 1: Statistical information on counts of query-biased sentences.
ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2
1 31 -1.9842 0.06039 J -3.9465 0.13904 24 4 -5.8451 0.11793
C -2.1387 0.15055 E -3.9485 0.13850 9 12 -5.9049 0.10370
16 32 -2.2906 0.03813 10 28 -4.0723 0.07908 14 14 -5.9860 0.10277
27 30 -2.4012 0.06238 21 22 -4.2460 0.08989 5 23 -6.0464 0.08784
6 29 -2.5536 0.07135 G -4.3143 0.13390 4 3 -6.2347 0.11887
12 25 -2.9415 0.08505 25 27 -4.4542 0.08039 20 6 -6.3923 0.10879
I -3.0196 0.13621 B -4.4655 0.13992 29 2 -6.4076 0.12028
11 24 -3.0495 0.08678 19 26 -4.6785 0.08453 3 9 -7.1720 0.10660
28 16 -3.1932 0.09858 26 21 -4.7658 0.08989 8 11 -7.4125 0.10408
2 18 -3.2058 0.09382 23 7 -5.3418 0.10810 17 15 -7.4458 0.10212
D -3.2357 0.17528 30 10 -5.4039 0.10614 13 5 -7.7504 0.11172
H -3.4494 0.13001 7 8 -5.6291 0.10795 32 17 -8.0117 0.09750
A -3.6481 0.13254 18 19 -5.6397 0.09170 22 13 -8.9843 0.10329
F -3.8316 0.13395 15 1 -5.7938 0.12448 31 20 -9.0806 0.09126
Table 2: Rank, Averaged log-likelihood score based on binomial model, true ROUGE-2 score for the summaries
of various systems in DUC?07 query-focused multi-document summarization task.
ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2
1 31 -4.6770 0.06039 10 28 -8.5004 0.07908 5 23 -14.3259 0.08784
16 32 -4.7390 0.03813 G -9.5593 0.13390 9 12 -14.4732 0.10370
6 29 -5.4809 0.07135 E -9.6831 0.13850 22 13 -14.8557 0.10329
27 30 -5.5110 0.06238 26 21 -9.7163 0.08989 4 3 -14.9307 0.11887
I -6.7662 0.13621 J -9.8386 0.13904 18 19 -15.0114 0.09170
12 25 -6.8631 0.08505 19 26 -10.3226 0.08453 14 14 -15.4863 0.10277
2 18 -6.9363 0.09382 B -10.4152 0.13992 20 6 -15.8697 0.10879
C -7.2497 0.15055 25 27 -10.7693 0.08039 32 17 -15.9318 0.09750
H -7.6657 0.13001 29 2 -12.7595 0.12028 7 8 -15.9927 0.10795
11 24 -7.8048 0.08678 21 22 -13.1686 0.08989 17 15 -17.3737 0.10212
A -7.8690 0.13254 24 4 -13.2842 0.11793 8 11 -17.4454 0.10408
D -8.0266 0.17528 30 10 -13.3632 0.10614 31 20 -17.5615 0.09126
28 16 -8.0307 0.09858 23 7 -13.7781 0.10810 3 9 -19.0495 0.10660
F -8.2633 0.13395 15 1 -14.2832 0.12448 13 5 -19.3089 0.11172
Table 3: Rank, Averaged log-likelihood score based on multinomial model, true ROUGE-2 score for the sum-
maries of various systems in DUC?07 query-focused multi-document summarization task.
{C
0
, C
1
, C
2
, . . . , C
k
} denote the k levels of query-
bias. C
i
is the set of sentences, each having i query
terms.
The number of sentences participating in each class
varies highly, with C
0
bagging a high percentage of
sentences (64.4%) and the rest {C
1
, C
2
, . . . , C
k
} dis-
tributing among themselves the rest 35.6% sentences.
Since the distribution is highly-skewed, distinguish-
ing systems based on log-likelihood scores using this
model is easier and perhaps more accurate. Like be-
fore, Humans? and systems? summaries must now con-
stitute low likelihood to show that they rely on query-
bias.
The likelihood of a summary then is :
L[summary; p(C
i
)] =
N !
n
0
!n
1
! ? ? ?n
k
!
p(C
0
)
n
0
p(C
1
)
n
1
? ? ? p(C
k
)
n
k
(2)
Where N is the number of sentences in the sum-
mary, and n
0
+ n
1
+ ? ? ? + n
k
= N; n
0
, n
1
,? ? ? ,n
k
are respectively the cardinalities of C
0
, C
1
, ? ? ? ,C
k
,
in the summary. Table 3 shows various systems with
their ranks based on ROUGE-2 and the average log-
likelihood scores.
3.3 Correlation of ROUGE and log-likelihood
scores
Tables 2 and 3 display log-likelihood scores of vari-
ous systems in the descending order of log-likelihood
scores along with their respective ROUGE-2 scores.
We computed the pearson correlation coefficient (?) of
?ROUGE-2 and log-likelihood? and ?ROUGE-SU4 and
log-likelihood?. This was computed for systems (ID: 1-
32) (r1) and for humans (ID: A-J) (r2) separately, and
for both distributions.
For the binomial model, r1 = -0.66 and r2 = 0.39 was
obtained. This clearly indicates that there is a strong
negative correlation between likelihood of occurrence
of a non-query-term and ROUGE-2 score. That is, a
strong positive correlation between likelihood of occur-
107
rence of a query-term and ROUGE-2 score. Similarly,
for human summarizers there is a weak negative cor-
relation between likelihood of occurrence of a query-
term and ROUGE-2 score. The same correlation anal-
ysis applies to ROUGE-SU4 scores: r1 = -0.66 and r2
= 0.38.
Similar analysis with the multinomial model have
been reported in Tables 4 and 5. Tables 4 and 5 show
the correlation among ROUGE-2 and log-likelihood
scores for systems
2
and humans
3
.
? ROUGE-2 ROUGE-SU4
binomial -0.66 -0.66
multinomial -0.73 -0.73
Table 4: Correlation of ROUGE measures with log-
likelihood scores for automated systems
? ROUGE-2 ROUGE-SU4
binomial 0.39 0.38
multinomial 0.15 0.09
Table 5: Correlation of ROUGE measures with log-
likelihood scores for humans
4 Conclusions and Discussion
Our results underscore the differences between human
and machine generated summaries. Based on Sum-
mary Content Unit (SCU) level analysis of query-bias
we argue that most systems are better at finding impor-
tant sentences only from query-biased sentences. More
importantly, we show that on an average, 76.67% of
the sentences picked by any automated summarizer are
query-biased. When asked to produce query-focused
summaries, humans do not rely to the same extent on
the repetition of query terms.
We further confirm based on the likelihood of emit-
ting non query-biased sentence, that there is a strong
(negative) correlation among systems? likelihood score
and ROUGE score, which suggests that systems are
trying to improve performance based on ROUGE met-
rics by being biased towards the query terms. On the
other hand, humans do not rely on query-bias, though
we do not have statistically significant evidence to sug-
gest it. We have also speculated that the multinomial
model helps in better capturing the variance across the
systems since it distinguishes among query-biased sen-
tences by quantifying the amount of query-bias.
From our point of view, most of the extractive sum-
marization algorithms are formalized based on a bag-
of-words query model. The innovation with individ-
ual approaches has been in formulating the actual algo-
rithm on top of the query model. We speculate that
2
All the results in Table 4 are statistically significant with
p-value (p < 0.00004, N=32)
3
None of the results in Table 5 are statistically significant
with p-value (p > 0.265, N=10)
the real difference in human summarizers and auto-
mated summarizers could be in the way a query (or rel-
evance) is represented. Traditional query models from
IR literature have been used in summarization research
thus far, and though some previous work (Amini and
Usunier, 2007) tries to address this issue using con-
textual query expansion, new models to represent the
query is perhaps one way to induce topic-focus on the
summary. IR-like query models, which are designed
to handle ?short keyword queries?, are perhaps not ca-
pable of handling ?an elaborate query? in case of sum-
marization. Since the notion of query-focus is appar-
ently missing in any or all of the algorithms, the future
summarization algorithms must try to incorporate this
while designing new algorithms.
Acknowledgements
We thank Dr Charles L A Clarke at the University of
Waterloo for his deep reviews and discussions on ear-
lier versions of the paper. We are also grateful to all the
anonymous reviewers for their valuable comments.
References
Massih R. Amini and Nicolas Usunier. 2007. A contextual query expansion
approach by term clustering for robust text summarization. In the proceed-
ings of Document Understanding Conference.
John M. Conroy, Judith D. Schlesinger, Jade Goldstein, and Dianne P. O?leary.
2004. Left-brain/right-brain multi-document summarization. In the pro-
ceedings of Document Understanding Conference (DUC) 2004.
Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy, D Kipp, Vivi Nastase,
and Stan Szpakowicz. 2006. Leveraging duc. In proceedings of DUC
2006.
Hoa Trang Dang. 2005. Overview of duc 2005. In proceedings of Document
Understanding Conference.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document
summarizer. In the proceedings of ACM SIGIR?95, pages 68?73. ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of sum-
maries. In the proceedings of ACL Workshop on Text Summarization
Branches Out. ACL.
H.P. Luhn. 1958. The automatic creation of literature abstracts. In IBM Jour-
nal of Research and Development, Vol. 2, No. 2, pp. 159-165, April 1958.
Daniel Marcu and Laurie Gerber. 2001. An inquiry into the nature of mul-
tidocument abstracts, extracts, and their evaluation. In Proceedings of the
NAACL-2001 Workshop on Automatic Summarization.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compo-
sitional context sensitive multi-document summarizer: exploring the fac-
tors that influence summarization. In SIGIR ?06: Proceedings of the 29th
annual international ACM SIGIR conference on Research and development
in information retrieval, pages 573?580, New York, NY, USA. ACM.
Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The
pyramid method: Incorporating human content selection variation in sum-
marization evaluation. In ACM Trans. Speech Lang. Process., volume 4,
New York, NY, USA. ACM.
G.J. Rath, A. Resnick, and R. Savage. 1961. The formation of abstracts by the
selection of sentences: Part 1: Sentence selection by man and machines. In
Journal of American Documentation., pages 139?208.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Doc-
ument summarization using conditional random fields. In the proceedings
of IJCAI ?07., pages 2862?2867. IJCAI.
Kristina Toutanova, Chris Brockett, Michael Gamon, Jagadeesh Jagarlamundi,
Hisami Suzuki, and Lucy Vanderwende. 2007. The pythy summarization
system: Microsoft research at duc 2007. In the proceedings of Document
Understanding Conference.
Hans van Halteren and Simone Teufel. 2003. Examining the consensus be-
tween human summaries: initial experiments with factoid analysis. In
HLT-NAACL 03 Text summarization workshop, pages 57?64, Morristown,
NJ, USA. Association for Computational Linguistics.
108
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 46?52,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Sentence Position revisited:
A robust light-weight Update Summarization ?baseline? Algorithm
Rahul Katragadda
rahul k@research.iiit.ac.in
Prasad Pingali
pvvpr@iiit.ac.in
Language Technologies Research Center
IIIT Hyderabad
Vasudeva Varma
vv@iiit.ac.in
Abstract
In this paper, we describe a sentence po-
sition based summarizer that is built based
on a sentence position policy, created from
the evaluation testbed of recent summariza-
tion tasks at Document Understanding Con-
ferences (DUC). We show that the summa-
rizer thus built is able to outperform most sys-
tems participating in task focused summariza-
tion evaluations at Text Analysis Conferences
(TAC) 2008. Our experiments also show that
such a method would perform better at pro-
ducing short summaries (upto 100 words) than
longer summaries. Further, we discuss the
baselines traditionally used for summarization
evaluation and suggest the revival of an old
baseline to suit the current summarization task
at TAC: the Update Summarization task.
1 Introduction
Document summarization received a lot of atten-
tion since an early work by Luhn (1958). Statis-
tical information derived from word frequency and
distribution was used by the machine to compute
a relative measure of significance, first for individ-
ual words and then for sentences. Later, Edmund-
son (1969) introduced four clues for identifying sig-
nificant words (topics) in a text. Among them title
and location are related to position methods, while
the other two are presence of cue words and high
frequency content words. Edmundson assigned pos-
itive weights to sentences according to their ordinal
position in the text, giving more weight to the first
sentence in the first paragraph and last sentence in
the last paragraph.
Position of a sentence in a document or the po-
sition of a word in a sentence give good clues to-
wards importance of the sentence or word respec-
tively. Such features are called locational features,
and a sentence position feature deals with presence
of key sentences at specific locations in the text.
Sentence Position has been well studied in summa-
rization research since its inception, early in Ed-
mundson?s work (1969). Earlier, Baxendale (1958)
investigated a sample of 200 paragraphs to deter-
mine where the important words are most likely to
be found. He concluded that in 85% of the para-
graphs, the first sentence was a topic sentence and in
7% of the paragraphs, the final one.
Recent advances in machine learning have been
adapted to summarization problem through the years
and locational features have been consistently used
to identify salience of a sentence. Some represen-
tative work in ?learning? sentence extraction would
include training a binary classifier (Kupiec et al,
1995), training a Markov model (Conroy et al,
2004), training a CRF (Shen et al, 2007), and learn-
ing pairwise-ranking of sentences (Toutanova et al,
2007).
In recent years, at the Document Understand-
ing Conferences (DUC1), Text Summarization re-
search evolved through task focused evaluations
ranging from ?generic single-document summariza-
tion? to ?query-focused multi-document summariza-
tion (QFMDS)?. The QFMDS task models the real-
world complex question answering task wherein,
given a topic and a set of 25 relevant documents, the
1http://duc.nist.gov/
46
task is to synthesize a fluent, well-organized 250-
word summary of the documents that answers the
question(s) in the topic statement. Recent focus
in the community has been towards query-focused
update-summarization task at DUC and the Text
Analysis Conference (TAC2). The update task was to
produce short (~100 words) multi-document update
summaries of newswire articles under the assump-
tion that the user has already read a set of earlier
articles. The purpose of each update summary will
be to inform the reader of new information about a
particular topic.
The rest of the paper is organized as follows. In
Section 2, we describe a Sub-optimal Position Pol-
icy (SPP) based on Pyramid Annotated Data, then
we derive a simple algorithm for summarization
based on the SPP in Section 3, and show evaluation
results. Next, in Section 4, we explain the current
baselines and evaluation for Multi-Document Sum-
marization and finally in Section 5, we discuss the
need for an older baseline in the current context of
the short summary task of update summarization.
2 Sub-Optimal Sentence Position Policy
Given a large text collection and a way to approxi-
mate the relevance for a reasonably large subset of
sentences, we could identify significant positional
attributes for the genre of the collection. Our ex-
periments are based on the work described in (Lin
and Hovy, 1997), whose experiments using the Ziff-
Davis corpus gave great insights on the selective
power of the position method.
2.1 Sentence Position Yield and Optimal
Position Policy (OPP)
Lin and Hovy (1997) provide an empirical validation
for the position hypothesis. They describe a method
of deriving an Optimal Position Policy for a collec-
tion of texts within a genre, as long as a small set
of topic keywords is defined for each text. They de-
fined sentence yield (strength of relevance) of a sen-
tence based on the mention of topic keywords in the
sentence.
The positional yield is defined as the average sen-
tence yield for that position in the document. They
2http://www.nist.gov/tac/
computed the yield of each sentence position in each
document by counting the number of different key-
words contained in the respective sentence in each
document, and averaging over all documents. An
Optimal Position Policy (OPP) is derived based on
the decreasing values of positional yield.
Their experiments grounded on the assumption
that abstract is an ideal representation of central
topic(s) of a text. For their evaluations, they used
the abstract to compare whether the sentences found
based on their Optimal Position Policy are indeed a
good selection. They used precision-recall measures
to establish those findings.
At our disposal we had data from pyramid eval-
uations that provided sentences and their mapping
to any content units in the gold standard summaries.
The annotations in the data provide a unique prop-
erty that each sentence can derive for itself a score
for relevance.
2.2 Documents
There are a wide variety of document types across
genre. In our case of newswire collection we have
identified two primary types of documents: small
document and large document. This distinction is
made based on the total sentences in the document.
All documents that have the number of sentences
above a threshold should be considered large. We
experimented on thresholds varying from 10 to 35
sentences and figured out that documents? distribu-
tion into the two categories was acceptable when
threshold-ed at 20 sentences. This decision is also
well supported by the fact that the last sentences of
a document were more important than the others in
the middle (Baxendale, 1958).
Sentence Position Yield (SPY) is obtained sep-
arately for both types of documents. For a small
document, sentence positions have values from 1
through 20. Meanwhile, for a large document we
compute SPY for position 1 through 20, then the last
15 sentences labeled 136 through 150 and ?any other
sentence? is labeled 100. It can be seen in figure 3
that sentences that do not come from leading or trail-
ing part of large documents do not contribute much
content to the summaries.
47
Figure 1: A sample mapping of SCU annotation to source document sentences. An excerpt from mapping of topic
D0701A of DUC 2007 QF-MDS task.
Figure 2: Sentence Position Yield for small documents.
2.3 Pyramid Data
Summary content units, referred as SCUs hereafter,
are semantically motivated, sub-sentential units that
are variable in length but not bigger than a sentential
clause. SCUs emerge from annotation of a collec-
tion of human summaries for the same input. They
are identified by noting information that is repeated
across summaries, whether the repetition is as small
as a modifier of a noun phrase or as large as a clause.
The weight an SCU obtains is directly proportional
to the number of reference summaries that support
that piece of information. The evaluation method
that is based on overlapping SCUs in human and
automatic summaries is described in the Pyramid
method (Nenkova et al, 2007).
The University of Ottawa has organized the pyra-
mid annotation data such that for some of the sen-
tences in the original document collection (those
that were picked by systems participating in pyra-
mid evaluation), a list of corresponding content units
is known (Copeck et al, 2006). We used this data to
identify locations in a document from where most
sentences were being picked, and which of those lo-
cations were being most content responsive to the
query.
A sample of SCU mapping is shown in figure 1.
Three sentences are seen in the figure among which
two have been annotated with system IDs and SCU
weights wherever applicable. The first sentence has
not been picked by any of the summarizers partici-
pating in Pyramid Evaluations, hence it is unknown
if the sentence would have contributed to any SCU.
The second sentence was picked by 8 summarizers
and that sentence contributed to an SCU of weight
3. The third sentence in the example was picked
by one summarizer, however, it did not contribute
to any SCU. This example shows all the three types
of sentences available in the corpus: unknown sam-
ples, positive samples and negative samples.
For each SCU, a weight is associated in pyramid
annotations. Thus a sentential score could be de-
fined as sum of weights of all the contributing SCUs
of the sentence. For an unknown sample and a neg-
ative sample, sentential score is 0. For example, in
the second sentence in figure 1 the score is 3, con-
tributed by a single SCU.While the same for the first
and third sentences is 0.
For each sentence position the sentential score is
averaged over all documents, which we call Sen-
tence Position Yield. SPY for small and large doc-
uments is shown in figures 2 and 3. Based on these
values for various positions, a simple Position Pol-
48
Figure 3: Sentence Position Yield for large documents
icy was framed as shown below. A position policy is
an ordered set consisting of elements in the order of
most importance. Within a subset, each sub-element
is equally important and treated likewise.
{s1, S1, {s2, S2, s3} , {S3, s4, s5, s6, s7, s8, s20} ,
{S4, s9} . . . }
In the above position policy, sentences from small
documents and large documents are represented by
si and Sj respectively.
The position policy described above provides an
ordering of ranked sentence positions based on a
very accurate ?relevance? annotations on sentences.
However, there is a large subset of sentences that are
not annotated with either positive or negative rele-
vance judgment. Hence, the policy derived is based
on a high-precision low-recall corpus3 for sentence
relevance. If all the sentences were annotated with
such judgements, the policy could have been differ-
ent. For this reason we call the above derived policy,
a Sub-optimal Position Policy (SPP).
3 SPP as an algorithm
The goal of creating a position policy was to identify
its effectiveness as a summarization algorithm. The
3DUC 2005 and 2006 data has been used for learning the
SPP. In further experiments in section 3, DUC 2007 and TAC
2008 data have been used as test data.
above simple heuristic was easily incorporated as an
algorithm based on simple scoring for each distinct
set in the policy. For instance, based on the policy
above, all s1 get the highest weight followed by next
best weight to all S1 and so on.
As it can be observed, only the first sentence of
each document could end up comprising the sum-
mary. This is okay, till we don?t get redundant infor-
mation in the summary. Hence we also used a sim-
ple unigram match based redundancy measure that
doesn?t allow a sentence if it matches any of the al-
ready selected sentences in at least 40% of content
words in it. We also dis-allow sentences greater than
25 content words.
We applied the above algorithm to generate multi-
document summaries for various tasks. We have ap-
plied it to Query-Focused Multi-Document Summa-
rization (QF-MDS) task of DUC 2007 and Query-
Focused Update Summarization task of TAC 2008.
3.1 Query-Focused Multi-Document
Summarization
The query-focused multi-document summarization
task at DUC models the real world complex ques-
tion answering task. Given a topic and a set of 25
relevant documents, this task is to synthesize a flu-
ent, well-organized 250 word summary of the docu-
ments that answers the question(s) in the topic state-
49
ment/narration.
The summaries from the above algorithm for the
QF-MDS were evaluated based on ROUGE met-
rics (Lin, 2004). The average4 recall scores are re-
ported for ROUGE-2 and ROUGE-SU4 in Table 1.
Also reported are the performance of the top per-
forming system and the official baseline(s). This al-
gorithm performed worse than most systems partic-
ipating in the task that year and performed better5
than only the ?first x words? baseline and 3 other sys-
tems.
system ROUGE-2 ROUGE-SU4
?first x words? baseline 0.06039 0.10507
?generic? baseline 0.09382 0.14641
SPP algorithm 0.06913 0.12492
system 15 (top system) 0.12448 0.17711
Table 1: ROUGE 2, SU4 Recall scores for two base-
lines, the SPP algorithm and a top performing system
at Query-Focused Multi-Document Summarization task,
DUC 2007.
3.2 Update Summarization Task
The update summarization task is to produce short
(~100 words) multi-document update summaries of
newswire articles under the assumption that the user
has already read a set of earlier articles. The initial
document set is called cluster A and the next set of
articles are called cluster B. For cluster A, a query-
focused multi-document summary is expected. The
purpose of each ?update summary? (summary of
cluster B) will be to inform the reader of new in-
formation about a particular topic. Summaries from
the above algorithm for the Query Focused Up-
date Summarization task were evaluated based on
ROUGE metrics. This algorithm performed surpris-
ingly better at this task when compared to QF-MDS.
The rouge scores suggest that this algorithm is well
above the median for cluster A and among the top 5
systems for cluster B.
It must be noted that consistent performance
across clusters (both A and B) shows the robustness
of the ?SPP algorithm? at the update summarization
task. Also, it is evident that such an algorithm is
computationally simple and light-weight.
4Averaged over all the 45 topics of DUC 2007 dataset.
5Better in a statistical sense, based on 95% confidence inter-
vals of the two systems? evaluation based on ROUGE-2.
These surprisingly high scores on ROUGE met-
rics prompted us to evaluate the summaries based on
Pyramid Evaluation (Nenkova et al, 2007). Pyramid
evaluation provides a more semantic approach to
evaluation of content based on SCUs as discussed in
Section 2.3. The average6 modified pyramid scores
of cluster A and cluster B summaries is shown in
Table 2, along with the average recall scores for
ROUGE-2, ROUGE-SU4 scores. The pyramid eval-
uation7 suggests that this algorithm performs better
than all other automated systems at TAC 2008. Ta-
ble 3 shows the average performance (across clus-
ters) of ?first x words? baseline, SPP algorithm and
two top performing systems (System ID=43 and
ID=11). System 43 was adjudged best system based
on ROUGE metrics, and system 11 was top per-
former based on pyramid evaluations at TAC 2008.
ROUGE-2 ROUGE-SU4 pyramid
cluster A 0.08987 0.1213 0.3432
cluster B 0.09319 0.1283 0.3576
Table 2: Cluster wise ROUGE 2, SU4 Recall scores and
modified Pyramid Scores for SPP algorithm at the Update
Summarization task.
3.3 Discussion
It is interesting to observe that the algorithm that
performs very poorly at QF-MDS, does very well
in the Update Summarization task. A possible ex-
planation for such behavior could be based on sum-
mary length. For a 250 word summary in the QF-
MDS task, human summaries might provide a de-
scriptive answer to the query that includes informa-
tion nuggets accompanied by background informa-
tion. Indeed, it has been earlier reported that humans
appreciate receiving more information than just the
answer to the query, whenever possible (Lin et al,
2003; Bosma, 2005).
Whereas, in the case of Update Summarization
task the summary length is only 100 words. In such
a short length humans need to trade-off between an-
swer sentences and supporting sentences, and usu-
ally answers are preferred. And since our method
6Averaged over all the 48 topics of TAC 2008 dataset.
7Pyramid Annotation were done by a volunteer who also
volunteered for annotations during DUC 2007.
50
system ROUGE-2 ROUGE-SU4 pyramid
?first x words? baseline 0.05896 0.09327 0.166
SPP algorithm 0.09153 0.1245 0.3504
System 43 (top in ROUGE) 0.10395 0.13646 0.289
System 11 (top in pyramid) 0.08858 0.12484 0.336
Table 3: Average ROUGE 2, SU4 Recall scores and modified Pyramid Scores for baseline, SPP algorithm and two top
performing systems at TAC 2008.
identifies sentences that are known to be contribut-
ing towards the needed answers, it performs better
at the shorter version of the task.
Another possible explanation is that as a shorter
summary length is required, the task of choosing the
most important information becomes more difficult
and no approach works well consistently. Also, it
has often been noted that this baseline is indeed quite
strong for this genre, due to the journalistic conven-
tion for putting the most important part of an article
in the initial paragraphs.
4 Baselines in Summarization Tasks
Over the years, as summarization research followed
trends from generic single-document summariza-
tion, to generic multi-document summarization, to
focused multi-document summarization there were
two major baselines that stayed throughout the eval-
uations. Those two baselines are:
1. First N words of the document (or of the most re-
cent document).
2. First sentence from each document in chronological
order until the length requirement is reached.
The first baseline was in place ever since the first
evaluation of generic single document summariza-
tion took place in DUC 2001. For multi-document
summarization, first N words of the most recent
document (chronologically) was chosen as the base-
line 1. In the recent summarization evaluations at
Text Analysis Conference (TAC 2008), where up-
date summarization was evaluated; baseline 1 still
persists. This baseline performs pretty poorly at con-
tent evaluations based on all manual and automatic
metrics. However, since it doesn?t disturb the orig-
inal flow and ordering of a document, linguistically
these summaries are the best. Indeed it outperforms
all the automated systems based on linguistic quality
evaluations.
The second baseline had been used occasionally
with multi-document summarization from 2001 to
2004 with both generic multi-document summariza-
tion and focused multi-document summarization. In
2001 only one system significantly outperformed the
baseline 2 (Nenkova, 2005). In 2003 QF-MDS how-
ever, only one system outperformed the baseline 2
above, while in 2004 at the same task, no system
significantly outperforms the baseline. This baseline
as can be seen, over the years has been pretty much
untouched by systems based on content evaluation.
However, the linguistic aspects of summary quality
would be compromised in such a summary.
Currently, for the Update Summarization task at
TAC 2008, NIST?s baseline is the baseline 1 (?first x
words? baseline). And all systems (except one) per-
form better than the baseline in all forms of content
evaluation. Since the task is to generate 100 word
summaries (short summaries), based on past experi-
ences, there is no doubt that baseline 2 would per-
form well.
It is interesting to observe that baseline 2 is a close
approximation to the ?SPP algorithm? described in
this paper. There are two main differences that we
draw between ?baseline 2? and SPP algorithm. First,
?baseline 2? picks only the first sentence in each
document, while ?SPP algorithm? could pick other
sentences in an order described by the position pol-
icy. Second, ?baseline 2? puts no restriction on re-
dundancy, thus due to journalistic conventions entire
summary might be comprised of the same ?informa-
tion nuggets?, wasting the minimal real-estate avail-
able (~100 words). On the other hand, in our ?SPP
algorithm? we consider a simple unigram-overlap
measure to identify redundant information in sen-
tence pairs that avoids redundant nuggets in the final
summary.
51
5 Discussion and Conclusion
Baselines 1 and 2 mentioned above, could together
act as a balancing mechanism to compare for lin-
guistic quality and responsive content in the sum-
mary. The availability of a stronger content respon-
sive summary as a baseline would enable steady
progress in the field. While all the linguistically
motivated systems would compare themselves with
baseline 1, the summary content motivated systems
would compare with the stronger baseline 2 and get
better than it.
Over the years to come, the usage of ?baseline 1?
doesn?t help in understanding whether there has
been significant improvement in the field. This is be-
cause almost every simple algorithm beats the base-
line performance. Having a better baseline, like the
one based on the position hypothesis, would raise
the bar for systems participating in coming years,
and tracking progress of the field over the years is
easier.In this paper, we derived a method to identify a
?sub-optimal position policy? based on pyramid an-
notation data, that were previously unavailable. We
also distinguish small and large documents to obtain
the position policy. We described the Sub-optimal
Sentence Position Policy (SPP) based on pyramid
annotation data and implemented the SPP as an al-
gorithm to show that a position policy thus formed
is a good representative of the genre and thus per-
forms way above median performance. We further
describe the baselines used in summarization evalu-
ation and discuss the need to bring back baseline 2
(or the ?SPP algorithm?) as an official baseline for
update summarization task.
Ultimately, as Lin and Hovy (1997) suggest, the
position method can only take us certain distance. It
has a limited power of resolution (the sentence) and
its limited method of identification (the position in a
text). Which is why we intend to use it as a baseline.
Currently, as we can see the algorithm generates a
generic summary, it doesn?t consider the topic or
query to generate a query-focused summary. In fu-
ture we plan to extend the SPP algorithm with some
basic method for bringing in relevance.
References
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature ? an experiment. IBM Journal of Re-
search and Development, 2(Non-topical Issue).
Wauter Bosma. 2005. Extending answers using dis-
course structures. In Horacio Saggion and J. L. Minel,
editors, RANLP workshop on Crossing Barriers in Text
summarization Research, pages 2?9. Incoma Ltd.
John M. Conroy, Judith D. Schlesinger, Jade Goldstein,
and Dianne P. O?leary. 2004. Left-brain/right-brain
multi-document summarization. In the proceedings of
Document Understanding Conference (DUC) 2004.
Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy,
D Kipp, Vivi Nastase, and Stan Szpakowicz. 2006.
Leveraging duc. In proceedings of DUC 2006.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. In Journal of the ACM, volume 16, pages
264?285. ACM.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In the proceedings
of ACM SIGIR?95, pages 68?73. ACM.
Chin-Yew Lin and Eduard Hovy. 1997. Identifying top-
ics by position. In Proceedings of the fifth conference
on Applied natural language processing, pages 283?
290. ACL.
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,
David Huynh, Boris Katz, and David R. Karger. 2003.
The role of context in question answering systems. In
the proceedings of CHI?04. ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In the proceedings of ACL
Workshop on Text Summarization Branches Out. ACL.
H.P. Luhn. 1958. The automatic creation of literature ab-
stracts. In IBM Journal of Research and Development,
Vol. 2, No. 2, pp. 159-165, April 1958.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. In ACM Trans. Speech Lang. Process., vol-
ume 4, New York, NY, USA. ACM.
Ani Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document under-
standing conference. In Manuela M. Veloso and Sub-
barao Kambhampati, editors, AAAI, pages 1436?1441.
AAAI Press / The MIT Press.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng
Chen. 2007. Document summarization using condi-
tional random fields. In the proceedings of IJCAI ?07.,
pages 2862?2867. IJCAI.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamundi, Hisami Suzuki, and Lucy Van-
derwende. 2007. The pythy summarization system:
Microsoft research at duc 2007. In the proceedings of
Document Understanding Conference 2007.
52
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 7?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
On Automated Evaluation of Readability of Summaries:
Capturing Grammaticality, Focus, Structure and Coherence
Ravikiran Vadlapudi
Language Technologies Research Center
IIIT Hyderabad
ravikiranv@research.iiit.ac.in
Rahul Katragadda
Language Technologies Research Center
IIIT Hyderabad
rahul k@research.iiit.ac.in
Abstract
Readability of a summary is usually graded
manually on five aspects of readability: gram-
maticality, coherence and structure, focus,
referential clarity and non-redundancy. In the
context of automated metrics for evaluation
of summary quality, content evaluations have
been presented through the last decade and
continue to evolve, however a careful exami-
nation of readability aspects of summary qual-
ity has not been as exhaustive. In this paper
we explore alternative evaluation metrics for
?grammaticality? and ?coherence and struc-
ture? that are able to strongly correlate with
manual ratings. Our results establish that our
methods are able to perform pair-wise rank-
ing of summaries based on grammaticality, as
strongly as ROUGE is able to distinguish for
content evaluations. We observed that none
of the five aspects of readability are indepen-
dent of each other, and hence by addressing
the individual criterion of evaluation we aim to
achieve automated appreciation of readability
of summaries.
1 Introduction
Automated text summarization deals with both the
problem of identifying relevant snippets of informa-
tion and presenting it in a pertinent format. Auto-
mated evaluation is crucial to automatic text sum-
marization to be used both to rank multiple partic-
ipant systems in shared tasks1, and to developers
whose goal is to improve the summarization sys-
tems. Summarization evaluations help in the cre-
ation of reusable resources and infrastructure; it sets
up the stage for comparison and replication of re-
sults by introducing an element of competition to
produce better results (Hirschman and Mani, 2001).
1The summarization tracks at Text Analysis Conference
(TAC) 2009, 2008 and its predecessors at Document Under-
standing Conferences (DUC).
Readability or Fluency of a summary is categor-
ically measured based on a set of linguistic qual-
ity questions that manual assessors answer for each
summary. The linguistic quality markers are: gram-
maticality, Non-Redundancy, Referential Clarity,
Focus and Structure and Coherence. Hence read-
ability assessment is a manual method where ex-
pert assessors give a rating for each summary on
the Likert Scale for each of the linguistic quality
markers. Manual evaluation being time-consuming
and expensive doesn?t help system developers ?
who appreciate fast, reliable and most importantly
automated evaluation metric. So despite having a
sound manual evaluation methodology for readabil-
ity, there is an need for reliable automatic metrics.
All the early approaches like Flesch Reading Ease
(Flesch, 1948) were developed for general texts and
none of these techniques have tried to character-
ize themselves as approximations to grammatical-
ity or structure or coherence. In assessing read-
ability of summaries, there hasn?t been much of
dedicated analysis with text summaries, except in
(Barzilay and Lapata, 2005) where local coherence
was modeled for text summaries and in (Vadlapudi
and Katragadda, 2010) where grammaticality of text
summaries were explored. In a marginally related
work in Natural Language Generation, (Mutton et
al., 2007) addresses sentence level fluency regard-
less of content, while recent work in (Chae and
Nenkova, 2009) gives a systematic study on how
syntactic features were able to distinguish machine
generated translations from human translations. In
another related work, (Pitler and Nenkova, 2008)
investigated the impact of certain surface linguistic
features, syntactic, entity coherence and discourse
features on the readability of Wall Street Journal
(WSJ) Corpus. We use the syntactic features used
in (Pitler and Nenkova, 2008) as baselines for our
experiments on grammaticality in this paper.
7
While studying the coherence patterns in student
essays, (Higgins et al, 2004) identified that gram-
matical errors affect the overall expressive quality
of the essays. In this paper, due to the lack of an ap-
propriate baseline and due to the interesting-ness of
the above observation we use metrics for grammat-
icality as a baseline measure for structure and co-
herence. Focus of a summary, is the only aspect of
readability that relies to a larger extent on the content
of the summary. In this paper, we use Recall Ori-
ented Understudy of Gisting Evaluation (ROUGE)
(Lin, 2004) based metrics as one of the baselines to
capture focus in a summary.
2 Summary Grammaticality
Grammaticality of summaries, in this paper, is de-
fined based on the grammaticality of its sentences,
since it is more a sentence level syntactic property.
A sentence can either be grammatically correct or
grammatically incorrect. The problem of grammati-
cal incorrectness should not occur in summaries be-
ing evaluated because they are generated mostly by
extract based summarization systems.
But as the distribution of grammaticality scores
in Table 1 shows, there are a lot of summaries that
obtain very low scores. Hence, We model the prob-
lem of grammaticality as ?how suitable or accept-
able are the sentence structures to be a part of a
summary??.
The acceptance or non acceptance of sentence
structures varies across reviewers because of vari-
ous factors like usage, style and dialects. Hence, we
define a degree to which a sentence structure is ac-
ceptable to the reviewers, this is called the degree of
acceptance throughout this paper.
Grammaticality Score 1 2 3 4 5
Percentage Distribution (in %) 10 13 15 37 25
Table 1: Percentage distribution of grammaticality scores
in system summaries
In this paper, the degree of acceptance of sen-
tence structures is estimated using language mod-
els trained on a corpus of human written sum-
maries. Considering the sentence structures in ref-
erence summaries as the best accepted ones (with
highest degree of acceptance), we estimate the de-
gree of acceptance of sentences in system gener-
ated summaries by quantifying the amount of sim-
ilarity/digression from the references using the lan-
guage models.
The structure of the sentences can be represented
by sequences of parts-of-speech (POS) tags and
chunk tags. Our previous observations (Vadlapudi
and Katragadda, 2010) show that the tagset size
plays an important role in determining the degree
of acceptance. In this paper, we combine the two
features of a sentence ? the POS-tag sequence and
chunk-tag sequence ? to generate the POS-Chunk-
tag training corpus.
Some aspects of grammatical structure are well
identifiable at the level of POS tags, while some
other aspects (such as distinguishing between appos-
itives and lists for eg.) need the power of chunk tags,
the combination of these two tag-sequences provides
the power of both.
Hence, the following approaches use probabilis-
tic models, learned on POS tag corpus and POS-
Chunk tag corpus, in 3 different ways to determine
the grammaticality of a sentence.
2.1 Enhanced Ngram model
As described in our previous work, the Ngram
model estimates the probability of a sentence to be
grammatically acceptable with respect to the corpus
using language models. Sentences constructed us-
ing frequent grammar rules would have higher prob-
ability and are said to have a well accepted sentence
structure. The grammaticality of a summary is com-
puted as
G(Sum) = AV G(P (Seqi)) ; P (Seqi) = log( n
?
?
?
?
n?
j=1
P (Kj))
P (Kj) = P (tj?2tj?1tj)
P (t1t2t3) = ?1 ? P (t3|t1t2) + ?2 ? P (t3|t2) + ?3 ? P (t3)
where G(Sum) is grammaticality score of a sum-
mary Sum and G(Si) is grammaticality of sentence
Si which is estimated by the probability (P (Seqi))
of its POS-tag sequence (Seqi). P (Kj) is proba-
bility of POS-tag trigram Kj which is tj?2tj?1tj
and ?tj , tj ? POS tags. The additional tags
t?1, t0 and tn+1 are the beginning-of-sequence and
end-of-sequence markers. The average AV G of
the grammaticality scores of sentences P (Seqi) in
a summary gives the final grammaticality score of
the summary. In the prior work, arithmetic mean
was used as the averaging technique, which per-
forms consistently well. However, here two other
averaging techniques namely geometric mean and
8
harmonic mean are experimented and based on our
experiments, we found geometric mean perform-
ing better than the other two averaging techniques.
All the results reported in this paper are based on
geometric mean. The above procedure estimates
grammaticality of sentence using its POS tags and
we call this run ?Ngram (POS)?. A similar proce-
dure is followed to estimate grammaticality using its
POS-Chunk tags (language models trained on POS-
chunk-tag training corpus). The corresponding run
is called ?Ngram (POS-Chunk)? in the results.
2.2 Multi-Level Class model
In this model, we view the task of scoring grammati-
cality as a n-level classification problem. Grammat-
icality of summaries is manually scored on a scale
of 1 to 5, which means the summaries are classi-
fied into 5 classes. We assume that each sentence of
the summary is also rated on a similar scale which
cumulatively decides to which class the summary
must belong. In our approach, sentences are classi-
fied into 5 classes on the basis of frequencies of un-
derlying grammar rules (trigram) by defining class
boundaries on frequencies. Hence, the cumulative
score of the rules estimate the score of grammatical-
ity of a sentence and inturn the summary.
Similar to (Vadlapudi and Katragadda, 2010), tri-
grams are classified into 5 classes C1, C2, C3, C4
and C5 and each class is assigned a score on a sim-
ilar scale (?jscore(Cj) = j) and class boundaries
are estimated using the frequencies of trigrams in
the training corpus. The most frequent trigram, for
example, would fall into class C5. POS-Class se-
quences are generated from POS-tag sequences us-
ing class boundaries as shown in Figure 1. This is
the first level of classification.
Figure 1: Two-level class model
Like the first level of classification, a series of
classifications are performed upto ?k? levels. At each
level we apply the scoring method described below
to evaluate the grammaticality of summaries. We
observed that from 3rd level onwards the structural
dissimilarity disappears and the ability to distinguish
different structures is lost. Hence, we report on the
second level of classification, that captures the gram-
matical acceptability of summaries very well, and
Figure 1 explains the two level classification.
G(Si) = AV G(H(Cw1),H(Cw2), ....,H(Cwn)) (1)
AVG is the average of H(Cwi), where w1, w2,
. . . wn are class trigrams, Cwi is the class into which
class trigram wi falls into and H(Cwi) is score as-
signed to the class Cwi. The AV G is computed us-
ing geometric mean and this run is referred as ?Class
(POS 2 level)? in the results.
Similar to above approach, the grammaticality
of a sentence can also be estimated using POS-
Chunk tag sequence and POS-Chunk Class training
data, and the corresponding run is referred as ?Class
(POS-Chunk 2 level)?.
2.3 Hybrid Model
As would be later seen in Table 2, the Ngram (POS)
and Class (POS 2 level) runs are able to distin-
guish various systems based on grammaticality. We
also note that these runs are able to very finely
distinguish the degree of grammaticality at sum-
mary level. This is a very positive result, one that
shows the applicability of applying these methods
to any test summaries in this genre. To fully uti-
lize these methods we combine the two methods
by a linear combination of their scores to form a
?hybrid model?. As seen with earlier approaches,
both the POS-tag sequences and POS-Chunk-tag se-
quences could be used to estimate the grammatical-
ity of a sentence, and hence the summary. These two
runs are called ?Hybrid (POS)? and ?Hybrid (POS-
Chunk)?, respectively.
3 Structure and Coherence
Most automated systems generate summaries from
multiple documents by extracting relevant sentences
and concatenating them. For these summaries to be
comprehensible they must also be cohesive and co-
herent, apart from being content bearing and gram-
matical. Lexical cohesion is a type of cohesion that
arises from links between words in a text (Halliday
and Hasan, 1976).A Lexical chain is a sequence of
9
such related words spanning a unit of text. Lexi-
cal cohesion along with presuppositions and impli-
cations with world knowledge achieves coherence in
texts. Hence, coherence is what makes text semanti-
cally meaningful, and in this paper, we also attempt
to automate the evaluation of the ?structure and co-
herence? of summaries.
We capture the structure or lexical cohesion of a
summary by constructing a lexical chain that spans
the summary. The relation between entities (noun
phrases) in adjacent sentences could be of type
center-reference (pronoun reference or reiteration),
or based on semantic relatedness (Morris and Hirst,
1991). A center-reference relation exists if an en-
tity in a sentence is a reference to center in adjacent
sentence. Identifying centers of reference expres-
sions can be done using a co-reference resolution
tool. Performance of co-reference resolution tools in
summaries, being evaluated, is not as good as their
performance on generic texts. Semantic relatedness
relation cannot be captured by using tools likeWord-
net because they are not very exhaustive and hence
are not effective. We use a much richer knowledge
base to define this relation ? Wikipedia.
Coherence of a summary is modelled by its struc-
ture and content together. Structure is captured by
lexical chains which also give information about fo-
cus of each sentence which inturn contribute to the
topic focus of the summary. Content presented in
the summary must be semantically relevant to the
topic focus of the summary. If the content presented
by each sentence is semantically relevant to the fo-
cus of the sentence, then it would be semantically
relevant to the topic focus of the summary. As the
foci of sentences are closely related, a prerequisite
for being a part of a lexical chain, the summary is
said to be coherent. In this paper, the semantic relat-
edness of topic focus and content is captured using
Wikipedia as elaborated in Section 3.1 of this paper.
3.1 Construction of lexical chains
In this approach, we identify the strongest lexical
chain possible which would capture the structure of
the summary. We define this problem of finding the
strongest possible lexical chain as that of finding the
best possible parts-of-speech tag sequence for a sen-
tence using the Viterbi algorithm shown in (Brants,
2000). The entities (noun phrases) of each sentence
are the nodes and transition probabilities are defined
as relatedness score (Figure 2). The strongest lex-
ical chain would have the highest score than other
possible lexical chains obtained.
Consider sentence Sk with entity set (e11, e12,
e13, . . . e1n) and sentence Sk+1 with entity set (e21,
e22, e23, . . . e2m). Sentences Sk and Sk+1 are said
to be strongly connected if there exists entities e1i ?
Sk and e2j ? Sk+1 that are closely related. e1i and
e2j are considered closely related if
? e2j is a pronoun reference of the center e1i
? e2j is a reiteration of e1i
? e2j and e1i are semantically related
Pronoun reference In this approach, we resolve
the reference automatically by finding more than
one possible center for the reference expression us-
ing Wikipedia. Since the summaries are generated
from news articles, we make a fair assumption that
related articles are present in Wikipedia. We en-
sure that the correct center is one among the pos-
sible centers through which Sk+1 and Sk+2 might
be strongly connected. Entities with query hits ra-
tio ? ? are considered as possible centers and entity
e2j is replaced by entities that act as the possible
centers. Since the chain with the identified correct
center is likely to have the highest score, our final
lexical chain would contain the correct center.
Query hit ratio =
Query hits(e1i and e2j)
Query hits(e1i)
Reiteration Generally, an entity with a determiner
can be treated as reiteration expression but not vice
versa. Therefore, we check whether e2j is actually
a reiteration expression or not, using query hits on
Wikipedia. If Query hits (e2j) ? ? then we con-
sider it to be a reiteration expression. A reiterating
expression of a named entity is generally a common
noun that occurs in many documents. After identify-
ing a reiteration expression we estimate relatedness
using semantic relatedness approach.
Figure 2: Viterbi trace for identifying lexical chain
10
Semantic relatedness By using query hits over
Wikipedia we estimate the semantic relatedness of
two entities. Such an approach has been previously
attempted in (Strube and Ponzetto, 2006). Based on
our experiments on grammaticality 2.2, classifying
into 5 classes is better suited for evaluation tasks,
hence we follow suit and classify semantic related-
ness into 5 classes. These classes indicate how se-
mantically related the entities are. Each class is as-
signed a value that is given to the hits which fall into
the class. For example, if query hits lie in the range
(?1, ?2) or if query hit ratio is ? ? then it falls into
class k and is assigned a score equal to k.
Now that we have computed semantic connect-
edness between adjacent sentences using the meth-
ods explained above, we identify the output node
with maximum score (node V2 in Figure 2). This
node with best score is selected and by backtack-
ing the Viterbi path we generate the lexical chain for
the summary. The constants ?, ?1, ?2and? are deter-
mined based on empirical tuning.
3.2 Coherence
We estimate coherence of the summary by estimat-
ing how the sentences stick together and the seman-
tic relevance of their collocation. In a sentence, the
semantic relatedness of entities with the focus esti-
mates score for the meaningfulness of the sentence,
and the average score of all the sentences estimates
the coherence of the summary.
C(Summary) =
?Ni=1G(si)
N
G(si) =
?k?1j=1H(Q(F and eij))
k
Where C(Summary) is the coherence of sum-
mary Summary, and G(si) is the semantic relat-
edness of a sentence si in Summary, while Q(q)
denotes the number of query hits of query q. F is
the focus of si and eij is an entity in si, and H(Q)
is the score of class into which query falls.
4 Evaluation
This paper deals with methods that imitate manual
evaluation metric for grammaticality and structure
and coherence by producing a score for each sum-
mary. An evaluation of these new summarization
evaluation metrics is based on how well the system
rankings produced by them correlate with manual
evaluations. We use 3 types of correlation evalu-
ations ? Spearman?s Rank Correlation, Pearson?s
Correlation and Kendall?s Tau ? each describing
some aspect of ordering problems.
We used reference summaries from TAC 2008,
2009 for the reference corpus and the experiments
described were tested on DUC 2007 query-focused
multi-document summarization datasets which have
45 topics and 32 system summaries for each topic
apart from 4 human reference summaries.
Table 2 shows the system level correlations of
our approaches to grammaticality assessment with
that of human ratings. We have used four base-
line approaches: AverageNPs, AverageVPs, Aver-
ageSBARs and AverageParseTreeHeight. Our ap-
proaches constitute of the following runs: Ngram
(POS), Ngram (POS-Chunk), Class (POS 2 level),
Class (POS-Chunk 2 level), Hybrid (POS), Hybrid
(POS-Chunk).
RUN Spearman?s ? Pearson?s r Kendall?s ?
Baselines
AverageNPs 0.1971 0.2378 0.1577
AverageSBARs 0.2923 0.4167 0.2138
AverageVPs 0.3118 0.3267 0.2225
ParseTreeHeight 0.2483 0.3759 0.1922
Our experiments
Ngram (POS) 0.7366 0.7411 0.5464
Ngram (POS+Chunk) 0.7247 0.6903 0.5421
Class (POS 2 level) 0.7168 0.7592 0.5464
Class (POS+Chunk 2 level) 0.7061 0.7409 0.5290
Hybrid (POS) 0.7273 0.7845 0.5205
Hybrid (POS+Chunk) 0.7733 0.7485 0.5810
Table 2: System level correlations of automated and man-
ual metrics for grammaticality.
RUN Spearman?s ? Pearson?s r Kendall?s ?
Experiments
Ngram (POS) 0.4319 0.4171 0.3165
Ngram (POS+Chunk) 0.4132 0.4086 0.3124
Class (POS 2 level) 0.3022 0.3036 0.2275
Class (POS+Chunk 2 level) 0.2698 0.2650 0.2015
Hybrid (POS) 0.3652 0.3483 0.2747
Hybrid (POS+Chunk) 0.3351 0.3083 0.2498
Table 3: Summary level correlations of automated and manual met-
rics for grammaticality .
RUN Spearman?s ? Pearson?s r Kendall?s ?
Baselines
Human Grammaticality rating 0.5546 0.6034 0.4152
Ngram(POS) 0.3236 0.4765 0.2229
Experiments
Our coherence model 0.7133 0.5379 0.5173
Table 4: System level correlations of automated and manual metrics
for coherence .
Table 4 shows the system level correlations of
our approach to structure and coherence assessment
with that of human ratings. As mentioned earlier in
Section 1, human ratings for grammaticality and our
11
RUN Spearman?s ? Pearson?s r Kendall?s ?
Baselines
Human Grammaticality rating 0.5979 0.6463 0.4360
Human Coherence rating 0.9400 0.9108 0.8196
Ngram(POS) 0.4336 0.6578 0.3175
Our coherence model 0.5900 0.5331 0.4125
ROUGE-2 0.3574 0.4237 0.2681
Table 5: System level correlations of automated and manual metrics
for focus
best performing system for grammaticality are used
as baselines for structure and coherence assessment.
Again, like we previously mentioned, focus can be
easily characterized using structure and coherence,
and to an extent the grammatical well-formedness.
Also the focus of a summary is also dependent on
content of the summary. Hence, we use ROUGE-
2, manual rating for grammaticality, manual rating
for coherence, and our approaches to both grammat-
icality and structure and coherence as baselines as
shown in Table 5.
5 Discussion and Conclusion
In this paper, we addressed the problem of identi-
fying the degree of acceptance of grammatical for-
mations at sentence level using surface features like
Ngrams probabilities (in Section 2.1), and trigrams
based class Ngrams (in Section 2.2) and a hybrid
model using both Ngram and Class model (in Sec-
tion 2.3), on the POS-tag sequences and POS-chunk-
tag sequences which have produced impressive re-
sults improving upon our previous work.
Our approaches have produced high correlations
to human judgment on grammaticality. Results in
Table 2 show that the Hybrid approach on the POS-
Chunk tag sequences outperforms all the other ap-
proaches. Our approaches to grammaticality assess-
ment have performed decently at pair-wise ranking
of summaries, shown by correlations of the order of
0.4 for many runs. This correlation is of the same
order as that of similar figure for content evaluations
using ROUGE and Basic Elements.
Table 4 shows that our approach to the ?structure
and coherence? assessment outperforms the base-
lines set and has an impressive correlation with man-
ual ratings. From Table 5 we found that grammati-
cality is a good indicator of focus while we also ob-
serve that structure and coherence forms a strong
alternative to focus.
The focus of this paper was on providing a com-
plete picture on capturing the grammaticality as-
pects of readability of a summary using relatively
shallow features as POS-tags and POS-Chunk-tags.
We used lexical chains to capture structure and co-
herence of summaries, whose performance also cor-
related with focus of summaries. None of the five
aspects of readability are completely independent of
each other, and by addressing the individual criteria
for evaluation we aim to achieve overall appreciation
of readability of summary.
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In ACL.
Thorsten Brants. 2000. Tnt: a statistical part-of-speech
tagger. In Proceedings of the sixth conference on
Applied natural language processing, pages 224?231,
Morristown, NJ, USA. Association for Computational
Linguistics.
Jieun Chae and Ani Nenkova. 2009. Predicting the
fluency of text with shallow structural features: Case
studies of machine translation and human-written text.
In EACL, pages 139?147. The Association for Com-
puter Linguistics.
Rudolf Flesch. 1948. A new readability yardstick. Jour-
nal of Applied Psychology, 32:221?233.
M.A.K Halliday and Ruqayia Hasan. 1976. Longman
publishers.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In HLT-NAACL 2004: Main
Proceedings, pages 185?192, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
Lynette Hirschman and Inderjeet Mani. 2001. Evalua-
tion.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In the proceedings of ACL
Workshop on Text Summarization Branches Out. ACL.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Comput. Linguist., 17(1):21?48.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In ACL. The Association for Computer
Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In EMNLP, pages 186?195. ACL.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In 21. AAAI / 18. IAAI 2006. AAAI Press,
july.
Ravikiran Vadlapudi and Rahul Katragadda. 2010.
Quantitative evaluation of grammaticality of sum-
maries. In CICLing.
12
