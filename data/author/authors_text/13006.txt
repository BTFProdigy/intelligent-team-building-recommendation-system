Proceedings of NAACL HLT 2007, Companion Volume, pages 5?8,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic acquisition of grammatical types for nouns 
 
N?ria Bel Sergio Espeja Montserrat Marimon 
IULA 
Universitat Pompeu Fabra 
P. de la Merc?, 10-12 
ES-08002 ? Barcelona 
{nuria.bel,sergio.espeja,montserrat.marimon}@upf.edu 
 
 
 
 
Abstract 
The work1 we present here is concerned 
with the acquisition of deep grammati-
cal information for nouns in Spanish. 
The aim is to build a learner that can 
handle noise, but, more interestingly, 
that is able to overcome the problem of 
sparse data, especially important in the 
case of nouns. We have based our work 
on two main points. Firstly, we have 
used distributional evidences as fea-
tures. Secondly, we made the learner 
deal with all occurrences of a word as a 
single complex unit. The obtained re-
sults show that grammatical features of 
nouns is a level of generalization that 
can be successfully approached with a 
Decision Tree learner. 
1 Introduction 
Our work aims to the acquisition of deep gram-
matical information for nouns, because having in-
formation such as countability and complementa-
tion is necessary for different applications, espe-
cially for deep analysis grammars, but also for 
question answering, topic detection and tracking, 
etc.  
Most successful systems of deep lexical acquisi-
tion are based on the idea that distributional fea-
tures (i.e. the contexts where words occur) are as-
sociated to concrete lexical  types. The difficulties 
                                                          
1 This research was supported by the Spanish Ministerio de Educaci?n y Cien-
cia: project AAILE, HUM2004-05111-C02-01/FILO, Ram?n y Cajal, Juan de la 
Cierva Programs and PTA-CTE/1370/2003 with Fondo Social Europeo,. 
are, on the one hand, that some filtering must be 
applied to get rid of noise, that is, contexts wrongly 
assessed as cues of a given type and, on the other 
hand, that for a pretty large number of words, their  
occurrences in a corpus of any length are very few, 
making statistical treatment very difficult. 
The phenomenon of noise is related to the fact 
that one particular context can be a cue of different 
lexical types. The problem of sparse data is pre-
dicted by the Zipfian distribution of words in texts: 
there is a large number of words likely to occur a 
very reduced number of times in any corpus. Both 
of these typical problems are maximized in the 
case of nouns.  
The aim of the work we present here is to build 
a learner that can handle noise, but, more interest-
ingly, that is able to overcome the problem of 
sparse data. The learner must predict the correct 
type both when there is a large number of occur-
rences as well as when there are only few occur-
rences, by learning on features that maximize gen-
eralization capacities of the learner while control-
ling overfitting phenomena.  
We have based our work on two main points. 
Firstly, we have used morphosyntactic information 
as features. Secondly, we made the learner deal 
with all occurrences of a word as a complex unit. 
In our system, linguistic cues of every occurrence 
are collected in the signature of the word (more 
technically a pair lema + part of speech) in a par-
ticular corpus. In the next sections we give further 
details about the features used, as well as about the 
use of signatures. 
The rest of the paper is as follows. Section 2 
presents an overview of the state of the art in deep 
lexical acquisition. In section 3, we introduce de-
tails about our selection of linguistically motivated 
5
cues to be used as features for training a Decision 
Tree (DT). Section 4 shortly introduces the meth-
odology and data used in the experiments whose 
results are presented in section 5. And in section 6 
we conclude by comparing with the published re-
sults for similar tasks and we sketch future re-
search.  
2 State of the art 
Most of the work on deep lexical information 
acquisition has been devoted to verbs. The existing 
acquisition systems learn very specialized linguis-
tic information such as verb subcategorization 
frame2. The results for verb subcategorization are 
mostly around the 0.8 of precision. Briscoe & Car-
roll (1997) reported a type precision of 0,76 and a 
type recall of 0.43. Their results were improved by 
the work of Korhonen (2002) with a type precision 
of 0.87 and a recall of 0.68 using external re-
sources to filter noise. Shulte im Walde (2002) re-
ports a precision of 0.65 and a recall of 0.58. 
Chesley & Salmon-Alt (2006) report a precision of 
0.86 and a recall of 0.54 for verb subcategorization 
acquisition for French.  
Lexical acquisition for nouns has been con-
cerned mainly with ontological classes and has 
mainly worked on measuring semantic similarity 
on the basis of occurrence contexts. As for gram-
matical information, the work of Baldwin and 
Bond (2003) in acquisition of countability features 
for English nouns also tackles the very important 
problem of feature selection. Other work like Car-
roll and Fang?s (2004) and Baldwin?s (2005) have 
focused on grammatical information acquisition 
for HPSG based computational grammars. The 
latter is the most similar exercises to our work. 
Baldwin (2005) reports his better results in terms 
of type accuracy has been obtained by using syn-
tactic information in a chunked and parsed corpus. 
The type F-scores for the different tested catego-
ries for English were: for verbs 0.47, for nouns 0.6  
and for adjectives 0.832.  
3 Feature selection  
One of the most important tasks in developing 
machine learning applications is the selection of 
                                                          
2 Given the argument-adjunct distinction, subcategorization 
concerns the specification for a predicate of the number and 
type of arguments which it requires for well-formedness. 
the features that leads to the smallest classification 
error. For our system, we have looked at distribu-
tional motivated features that can help in discrimi-
nating the different types that we ultimately use to 
classify words.  
The lexical types used in deep analysis gram-
mars are linguistic generalizations drawn from the 
distributional characteristics of particular sets of 
words. For the research we present here, we have 
taken the lexicon of a HPSG-based grammars de-
veloped in the LKB platform (Copestake, 2002) for 
Spanish, similarly to the work of Baldwin (2005). 
In the LKB grammatical framework, lexical types 
are defined as a combination of features. Lexical 
typology of nouns for Spanish, for instance, can be 
seen as a cross-classification of noun countability 
vs. mass distinctions, and subcategorization frame 
or valence, including prepositional selection.  For 
example nouns as ?temor? (?fear?) and ?adicci?n? 
(?adiction) belong to the type 
n_ppde_pcomp_a_count as they take two com-
plements: one with de and the other with a bound 
preposition a, as in ?El temor de la ni?a a los fan-
tasmas? (?The girl?s fear to ghosts?) vs. ?La adic-
ci?n a la coca?na? (?The addiction to cocaine?).  
We decided to carry out the classification for 
each of the grammatical features that conform the 
cross-classified types as a better level of generali-
zation than the type: mass and countable, on the 
one hand and, on the other hand, for subcategoriza-
tion information three further basic features: trans, 
for nouns with thematic complements introduced 
by the preposition de, intrans, when the noun can 
appear with no complements and pcomp for nouns 
having complements introduced by a bound prepo-
sition. The complete type can be recomposed with 
the assigned features. ?Temor? and ?adicci?n? will 
be examples of trans and pcomp_a. They both 
have also to be assigned the feature countable. The 
combination of features assigned corresponds to 
the final type which is a definition of the complete 
behaviour of the noun with respect, for instance, 
optional complements.  
We have used 23 linguistic cues, that is, the pat-
terns of contexts that can be indicative of a particu-
lar feature. The most frequent cue that can be re-
lated to countable is for the noun to be found with 
plural morphology. A singular noun without de-
terminer after a verb or a preposition is a cue of the 
noun being mass: ?hay barro en el sal?n? (?there is 
mud in the living room?) vs. ?hay hombres en el 
6
sal?n? (?there are men in the living room?). A fur-
ther cue for mass is the presence of particular 
quantifiers, such as ?m?s? (?more?), ?menos? 
(?less?), etc. But these cues, based on a collection 
of lexical items, are less productive than other 
characteristics such as morphological number or 
presence of determiners, as they appear very 
scarcely in texts. Nevertheless, we should mention 
that most of mass nouns in Spanish can also appear 
in the contexts of countables, as in the case of 
?beer? when in constructions such as ?three beers, 
please?.   
More difficult was to find cues for identifying 
the transitive nature of a noun. After some empiri-
cal work, we found a tendency of argumental com-
plements to have a definite article: ?temor de la 
ni?a? (?fear of the girl?), while modifiers tend to 
appear without determiners: ?mesa de juegos? (?ta-
ble of games?). Besides, we have taken as a cue the 
morphological characteristics of deverbal nouns. 
Suffixes such as ?-ci?n?, ?-si?n?, and ?-miento?, 
are very much indicative of transitive nouns. Fi-
nally, to find the bound preposition of comple-
ments, we used a pattern for each possible preposi-
tion found after the noun in question. 
We used Regular Expressions to implement the 
linguistic motivated patterns that check for the in-
formation just mentioned in a part of speech tagged 
corpus. The various patterns determine whether the 
linguistic cues that we have related to syntactic 
features are found in each occurrence of a particu-
lar word in a corpus. The positive or negative re-
sults of the n pattern checking are stored as binary 
values of a n dimensional vector, one for each oc-
currence. All vectors produced, one per occurrence 
of the word in question, are stored then in a kind of 
vector of vectors that we have called its signature.  
The term signature wants to capture the notion that 
the data it embodies is truly representative of a par-
ticular item, and that shows the details of its typical 
behavior. Particularly, we wanted linguistic cues 
appearing in different occurrences of the same 
word to be observed as related information. We 
have not dealt with ambiguity at all, however. One 
of the reasons was our focus on low frequency 
nouns. 
4 Methodology and data 
We have worked with the Corpus T?cnic de 
l?IULA, a multilingual part of speech tagged corpus 
which consists of domain specific texts. The sec-
tion used for our evaluation was the Spanish with 
1,091,314 words in the domain of economy and 
4,301,096 for medicine. A dataset of 289 nouns, 
present in both subcorpora, was selected. It was 
important to compare the behavior of the same 
nouns in both corpus to check whether the learner 
was subject to unwanted overfitting.  
We used the data for building a C4.5 DT clas-
sifier3. DT?s are one well known and successful 
technique for this class of tasks when there is 
enough pre-annotated data available. DT?s have 
the additional benefit that the results can be in-
spected. The signatures of the words in the Gold-
Standard lists were extracted from the corpus of 
medicine and of the economy one. There was a 
further test set of 50 nouns with a single occur-
rence in the corpus of economy for testing pur-
poses. The DT was trained with the signatures of 
the economy corpus, and the medicine ones as well 
as the singles set were used for testing.  
5 Evaluation 
The purpose of the evaluation was to validate our 
system with respect to the two problems men-
tioned: noise filtering and generalization capacity 
by measuring type precision and type recall. We 
understand type precision as a measure of the noise 
filtering success, and recall as a measure of the 
generalization capacity.  
In the following tables we present the results of 
the different experiments. In Table 1, there is a 
view of the results of the experiment after training 
and testing with the signatures got in the smaller 
corpus. The results are for the assignment of the 
grammatical feature for the two values, yes and no. 
And the column named global refers to the total 
percentage of correctly classified instances. 
 
  yes no 
lt global prec. rec.  F prec. rec. F 
MASS 0.67 0.4 0.26 0.31 0.73 0.83 0.78
COUNT 0.96 0.97 0.99 0.98 0 0 0 
TRANS 0.85 0.73 0.45 0.55 0.86 0.95 0.91
INT 0.81 0.84 0.94 0.89 0.64 0.32 0.48
PCOMP 0.9 0.4 0.08 0.13 0.91 0.98 0.95
Table 1. DT results of economy signatures for 
training and test 
                                                          
3 We have used WEKA J48 decision tree classifier (Witten and Frank, 2005). 
7
 
The most difficult task for the learner is to iden-
tify nouns with bound prepositions. Note that there 
are only 20 nouns with prepositional complements 
of the 289 test nouns, and that the occurrence of 
the preposition is not mandatory, and hence the 
signatures are presented to the learner with very 
little information.  
Table 2 shows the results for 50 nouns with only 
one occurrence in the corpus. The performance 
does not change significantly, showing that the 
generalization capacity of the learner can cope 
with low frequency words, and that noise in larger 
signatures has been adequately filtered. 
 
  yes no 
lt global prec. rec.  F prec. rec. F 
MASS 0.71 0.5 0.16 0.25 0.73 0.93 0.82
COUNT 0.97 0.97 1 0.98 0 0 0 
TRANS 0.85 0.75 0.46 0.57 0.87 0.96 0.91
INT 0.83 0.85 0.95 0.89 0.70 0.41 0.52
PCOMP 0.91 0 0 0 0.91 1 0.95
Table 2. DT results for training with signatures of 
the economy corpus and testing 50 unseen nouns 
with a single occurrence as test 
 
Table 3 shows that there is little variation in the 
results of training with signatures of the economy 
corpus and testing with ones of the medicine cor-
pus. As expected, no variation due to domain is 
relevant as the information learnt should be valid 
in all domains.  
 
  yes no 
lt global prec. rec. F prec. rec. F 
MASS 0.65 0.44 0.53 0.48 0.77 0.70 0.73
COUNT 0.97 0.97 1 0.98 0 0 0 
TRANS 0.82 0.62 0.47 0.54 0.86 0.92 0.89
INT 0.78 0.82 0.92 0.86 0.58 0.35 0.43
PCOMP 0.81 0.31 0.28 0.29 0.92 0.93 0.93
Table 3. DT results for training with economy sig-
natures and testing with medicine signatures 
6  Conclusions 
The obtained results show that the learning of 
grammatical features of nouns are learned success-
fully when using distributional linguistic informa-
tion as learning features that allow the learner to 
generalize so as to maintain the performance in 
cases of nouns with just one occurrence.  
There are however issues that should be further 
investigated. Grammatical features with low preci-
sion and recall results (mass and pcomp) show that 
some more research should be carried out for find-
ing relevant linguistic cues to be used as learning 
features. In that respect, the local cues based on 
morphosyntactic tagging have proved to be useful, 
minimizing the text preprocessing requirements for 
getting usable results. 
Acknowledgements 
The authors would like to thank Jordi Porta, 
Daniel Chicharro and the anonymous reviewers for 
helpful comments and suggestions. 
References  
Baldwin, T. 2005. ?Bootstrapping Deep Lexical Re-
sources: Resources for Courses?, ACL-SIGLEX 2005. 
Workshop on Deep Lexical Acquisition. 
Baldwin, T. and F. Bond. 2003. ?Learning the Count-
ability of English Nouns from Corpus Data?. Pro-
ceedings of the 41st. Annual Meeting of the ACL.  
Briscoe, T. and J. Carroll. 1997.  ?Automatic extraction 
of subcategorization from corpora?. In Proceedings 
of the Fifth Conference on Applied Natural Process-
ing, Washington.  
Carroll, J. and A. Fang. 2004. ?The automatic acquisi-
tion of verb subcategorisations and their impact on 
the performance of an HPSG parser?. In Proceedings 
of the 1st International Joint Conference on Natural 
Language Processing (IJCNLP), Sanya City, China.  
Chesley, P and S. Salmon-Alt. 2006. ?Automatic extrac-
tion of subcategorization frames for French?. In 
Proc. of the LREC Conference, Genoa. 
Copestake, A.. 2002. Implementing Typed Feature 
Structure Grammars. CSLI Publications. 
Korhonen, A. 2002. ?Subcategorization acquisition?. As 
Technical Report UCAM-CL-TR-530, University of 
Cambridge, UK. 
Shulte im Walde, S. 2002. ?Evaluating verb subcate-
gorization frames learned by a German statistical 
grammar against manual definitions in the Duden 
Dictionary?. In Proceedings of the 10th EURALEX In-
ternational Congress, 187-197. 
Witten, Ian H. and Eibe Frank. 2005. Data Mining: 
Practical machine learning tools and techniques. 2nd 
Edition, Morgan Kaufmann, San Francisco. 
8
Corpus representativeness for syntactic information acquisition 
N?ria BEL 
IULA, Universitat Pompeu Fabra 
La Rambla 30-32 
08002 Barcelona  
Spain
nuria.bel@upf.edu 
Abstract
This paper refers to part of our research in the 
area of automatic acquisition of computational 
lexicon information from corpus. The present 
paper reports the ongoing research on corpus 
representativeness. For the task of inducing 
information out of text, we wanted to fix a 
certain degree of confidence on the size and 
composition of the collection of documents to 
be observed. The results show that it is 
possible to work with a relatively small corpus 
of texts if it is tuned to a particular domain. 
Even more, it seems that a small tuned corpus 
will be more informative for real parsing than 
a general corpus. 
1 Introduction 
The coverage of the computational lexicon used 
in deep Natural Language Processing (NLP) is 
crucial for parsing success. But rather frequently, 
the absence of particular entries or the fact that the 
information encoded for these does not cover very 
specific syntactic contexts --as those found in 
technical texts? make high informative grammars 
not suitable for real applications. Moreover, this 
poses a real problem when porting a particular 
application from domain to domain, as the lexicon 
has to be re-encoded in the light of the new 
domain. In fact, in order to minimize ambiguities 
and possible over-generation, application based 
lexicons tend to be tuned for every specific domain 
addressed by a particular application. Tuning of 
lexicons to different domains is really a delaying 
factor in the deployment of NLP applications, as it 
raises its costs, not only in terms of money, but 
also, and crucially, in terms of time.  
A desirable solution would be a ?plug and play? 
system that, given a collection of documents 
supplied by the customer, could induce a tuned 
lexicon. By ?tuned? we mean full coverage both in 
terms of: 1) entries: detecting new items and 
assigning them a syntactic behavior pattern; and 2) 
syntactic behavior pattern: adapting the encoding 
of entries to the observations of the corpus, so as to 
assign a class that accounts for the occurrences of 
this particular word in that particular corpus. The 
question we have addressed here is to define the 
size and composition of the corpus we would need 
in order to get necessary and sufficient information 
for Machine Learning techniques to induce that 
type of information. 
Representativeness of a corpus is a topic largely 
dealt with, especially in corpus linguistics. One of 
the standard references is Biber (1993) where the 
author offers guidelines for corpus design to 
characterize a language. The size and composition 
of the corpus to be observed has also been studied 
by general statistical NLP (Lauer 1995), and in 
relation with automatic acquisition methods 
(Zernick, 1991, Yang & Song 1999). But most of 
these studies focused in having a corpus that 
actually models the whole language. However, we 
will see in section 3 that for inducing information 
for parsing we might want to model just a 
particular subset of a language, the one that 
corresponds to the texts that a particular 
application is going to parse. Thus, the research we 
report about here refers to aspects related to the 
quantity and optimal composition of a corpus that 
will be used for inducing syntactic information. 
 In what follows, we first will briefly describe 
the observation corpus. In section 3, we introduce 
the phenomena observed and the way we got an 
objective measure. In Section 4, we report on 
experiments done in order to check the validity of 
this measure in relation with word frequency.  In 
section 5 we address the issue of corpus size and 
how it affects this measure.  
2 Experimental corpus description 
We have used a corpus of technical specialized 
texts, the CT. The CT is made of subcorpora 
belonging to 5 different areas or domains: 
Medicine, Computing, Law, Economy, 
Environmental sciences and what is called a 
General subcorpus made basically of news. The 
size of the subcorpora range between 1 and 3 
million words per domain. The CT corpus covers 3 
different languages although for the time being we 
have only worked on Spanish. For Spanish, the 
size of the subcorpora is stated in Table 1. All texts 
have been processed and are annotated with 
morphosyntactic information. 
The CT corpus has been compiled as a test-bed 
for studying linguistic differences between general 
language and specialized texts. Nevertheless, for 
our purposes, we only considered it as documents 
that represent the language used in particular 
knowledge domains. In fact, we use them to 
simulate the scenario where a user supplies a 
collection of documents with no specific sampling 
methodology behind.  
3 Measuring syntactic behavior: the case of 
adjectives
We shall first motivate the statement that 
parsing lexicons require tuning for a full coverage 
of  a particular domain. We use the term ?full 
coverage? to describe the ideal case where we 
would have correct information for all the words 
used in the (unknown a priori) set of texts we want 
a NLP application to handle. Note that full 
coverage implies two aspects. First, type coverage: 
all words that are used in a particular domain are in 
the lexicon. Second, that the information contained 
in the lexicon is the information needed by the 
grammar to parse every word occurrence as 
intended.
Full coverage is not guaranteed by working with 
?general language? dictionaries. Grammar 
developers know that the lexicon must be tuned to 
the application?s domain, because general language 
dictionaries either contain too much information, 
causing overgeneration, or do not cover every 
possible syntactic context, some of them because 
they are specific of a particular domain. The key 
point for us was to see whether texts belonging to a 
domain justify this practice. 
In order to obtain objective data about the 
differences among domains that motivate lexicon 
tuning, we have carried out an experiment to study 
the syntactic behavior (syntactic contexts) of a list 
of about 300 adjectives in technical texts of four 
different domains. We have chosen adjectives 
because their syntactic behavior is easy to be 
captured by bigrams, as we will see below. 
Nevertheless, the same methodology could have 
been applied to other open categories. 
The first part of the experiment consisted of 
computing different contexts for adjectives 
occurring in texts belonging to 4 different domains. 
We wanted to find out how significant could 
different uses be; that is, different syntactic 
contexts for the same word depending on the 
domain. We took different parameters to 
characterize what we call ?syntactic behavior?.  
For adjectives, we defined 5 different parameters 
that were considered to be directly related with 
syntactic patterns. These were the following 
contexts: 1) pre-nominal position, e.g. ?importante 
decisi?n? (important decision) 2) post-nominal 
position, e.g. ?decisi?n importante? 3) ?ser? copula1
predicative position, e.g.  ?la decisi?n es 
importante? (the decision is important) 4) ?estar? 
copula predicative position, e.g. ?la decisi?n est? 
interesante/*importante? (the decision is 
interesting/important) 5) modified by a quantity 
adverb, e.g. ?muy interesante? (very interesting).
Table 1 shows the data gathered for the adjective 
?paralelo? (parallel) in the 4 different domain 
subcorpora. Note the differences in the position 3 
(?ser? copula) when observed in texts on 
computing, versus the other domains. 
Corpora/n.of occurrences 1 2 3 4 5
general (3.1 M words) 1 61 29 3 0
computing (1.2 M words) 4 30 0 0 0
medecine (3.7 M words) 3 67 22 1 0
economy (1 M words) 0 28 6 0 0
Table 1: Computing syntactic contexts as 
behaviour
The observed occurrences (as in Table 1) were 
used as parameters for building a vector for every 
lemma for each subcorpus. We used cosine 
distance2 (CD) to measure differences among the 
occurrences in different subcorpora. The closer to 
0, the more significantly different, the closer to 1, 
the more similar in their syntactic behavior in a 
particular subcorpus with respect to the general 
subcorpus. Thus, the CD values for the case of 
?paralelo? seen in Table 1 are the following: 
Corpus Cosine Distance 
computing 0.7920 
economy 0.9782 
medecine 0.9791 
Table 2: CD for ?paralelo? compared to the 
general corpus 
                                                     
1 Copulative sentences are made of 2 different basic copulative verbs ?ser? 
and ?estar?. Most authors tend to express as ?lexical idyosincracy? preferences 
shown by particular adjectives as to go with one of them or even with both 
although with different meaning. 
2 Cosine distance shows divergences that have to do with  large differences in 
quantity between parameters in the same position, whether small quantities 
spread along the different parameters does not compute significantly. Cosine 
distance was also considered to be interesting because it computes relative 
weight of parameters within the vector. Thus we are not obliged to take into 
account relative frequency, which is actually different according to the different 
domains. 
What we were interested in was identifying
significant divergences, like, in this case, the
complete absence of predicative use of the
adjective ?paralelo? in the computing corpus. The 
CD measure has been sensible to the fact that no
predicative use has been observed in texts on 
computing, the CD going down to 0.7.  Cosine 
distance takes into account significant distances
among the proportionality of the quantities in the
different features of the vector. Hence we decided
to use CD to measure the divergence in syntactic
behavior of the observed adjectives. Figure 1 plots
CD for the 4 subcorpora (Medicine, Computing,
Economy) compared each one with the general 
subcorpus. It corresponds to the observations for
about 300 adjectives, which were present in all the 
corpora. More than a half for each corpus is in fact 
below the 0.9 of similarity. Recall also that this 
mark holds for the different corpora, independently
of the number of tokens (Economy is made of 1
million words and Medicine of 3). 
-0,2
0
0,2
0,4
0,6
0,8
1
1,2
1
2
5
4
9
7
3
9
7
1
2
1
1
4
5
1
6
9
1
9
3
2
1
7
2
4
1
2
6
5
2
8
9
3
1
3
The data of figure 1 would allow us to conclude 
that for lexicon tuning, the sample has to be rich in
domain dependent texts.
4 Frequency and CD measure 
For being sure that CD was a good measure, we 
checked to what extent what we called syntactic
behavior differences measured by a low CD could
be due to a different number of occurrences in each
of the observed subcorpora. It would have been
reasonable to think that when something is seen 
more times, more different contexts can be
observed, while when something is seen only a few
times, variations are not that significant. 
-500
0
500
1000
1500
2000
2500
0 0,2 0,4 0,6 0,8 1 1,2
Figure 2: Difference in n. of observations 
in 2 corpora and CD 
Figure 2 relates the obtained CD and the 
frequency for every adjective. For being able to do 
it, we took the difference of occurrences in two
subcorpora as the frequency measure, that is, the 
number resulting of subtracting the occurrences in 
the computing subcorpus from the number of 
occurrences in the general subcorpus. It clearly
shows that there is no regular relation between 
different number of occurrences in the two corpora 
and the observed divergence in syntactic behavior. 
Those elements that have a higher CD (0.9) range
over all ranking positions: those that are 100 times
more frequent in one than in other, etc.  Thus we 
can conclude that CD do capture syntactic
behavior differences that are not motivated by
frequency related issues. 
5 Corpus size and syntactic behavior 
We also wanted to see the minimum corpus size
for observing syntactic behavior differences
clearly. The idea behind was to measure when CD
gets stable, that is, independent of the number of 
occurrences observed. This measure would help us
in deciding the minimum corpus size we need to
have a reasonable representation for our induced 
lexicon. In fact our departure point was to check 
whether syntactic behavior could be compared
with the figures related to number of types
(lemmas) and number of tokens in a corpus. Biber 
1993, S?nchez and Cantos, 1998, demonstrate that 
the number of new types does not increase
proportionally to the number of words once a 
certain quantity of texts has been observed.
Figure 1: Cosine distance for the 4 
different subcorpus
In our experiment, we split the computing
corpus in 3 sets of 150K, 350K and 600K words in
order to compare the CD?s obtained. In Figure 3, 1
represents the whole computing corpus of 1,200K 
for the set of 300 adjectives we had worked with 
before.
00,2
0,4
0,6
0,8
1
1,2
1
4
1
8
1
1
2
1
1
6
1
2
0
1
2
4
1
2
8
1
105K
351K
603K
3M GEN
As shown in Figure 3, the results of this
comparison were conclusive: for the computing
corpus, with half of the corpus, that is around
600K, we already have a good representation of
the whole corpus. The CD being superior to 0.9 for
all adjectives (mean is 0.97 and 0.009 of standard 
deviation). Surprisingly, the CD of the general 
corpus, the one that is made of 3 million words of
news, is lower than the CD achieved for the
smallest computing subcorpus. Table 3 shows the 
mean and standard deviation for all de subcorpora
(CC is Computing Corpus). 
Corpus size mean st. deviation
CC 150K 0.81 0.04
CC   360K 0.93 0.01
CC 600K 0.97 0.009
CC 1.2 M 1 0
General 3M 0.75 0.03
Table 3: Comparing corpus size and CD 
What Table 3 suggests is that according to CD, 
measured as shown here, the corpus to be used for 
inducing information about syntactic behavior does 
not need to be very large, but made of texts
representative of a particular domain. It is part of 
our future work to confirm that Machine Learning 
Techniques can really induce syntactic information
from such a corpus.
References
Biber, D. 1993. Representativeness in corpus
design. Literary and Linguistic Computing 8:
243-257.
Lauer, M. 1995. ?How much is enough? Data 
requirements for Statistical NLP?. In 2nd.
Conference of the Pacific Association for 
Computational Linguistics. Brisbane, Australia.
S?nchez, A. & Cantos P., 1997, ?Predictability of
Word Forms (Types) and Lemmas in Linguistic 
Corpora, A Case Study Based on the Analysis of 
the CUMBRE Corpus: An 8-Million-Word 
Corpus of Contemporary Spanish,? In
International Journal of Corpus Linguistics Vol.
2, No. 2.
Schone, P & D. Jurafsky. 2001. Language-
Independent induction of part of speech class
labels using only language universals.
Proceedings IJCAI, 2001.
Figure 3: CD of 300 adjs. in different 
size subcorpora and general corpus 
Yang, D-H and M. Song. 1999. ?The Estimate of 
the Corpus Size for Solving Data Sparseness?.
Journal of KISS, 26(4): 568-583.
Zernik, U. Lexical Acquisition. 1991. Exploiting
On-Line Resources to Build a Lexicon. 
Lawrence Erlbaum Associates: 1-26. 
Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LEXICAL MARKUP FRAMEWORK (LMF)  
FOR NLP MULTILINGUAL RESOURCES 
Gil Francopoulo1, Nuria Bel2, Monte George3, Nicoletta Calzolari4, 
Monica Monachini5, Mandy Pet6, Claudia Soria7
 
1INRIA-Loria: gil.francopoulo@wanadoo.fr 
2UPF: nuria.bel@upf.edu 
3ANSI: dracalpha@earthlink.net 
4CNR-ILC: glottolo@ilc.cnr.it 
5CNR-ILC: monica.monachini@ilc.cnr.it 
6MITRE: mpet@mitre.org 
7CNR-ILC: claudia.soria@ilc.cnr.it 
 
Abstract 
Optimizing the production, maintenance 
and extension of lexical resources is one 
the crucial aspects impacting Natural 
Language Processing (NLP). A second 
aspect involves optimizing the process 
leading to their integration in applica-
tions. With this respect, we believe that 
the production of a consensual specifica-
tion on multilingual lexicons can be a 
useful aid for the various NLP actors. 
Within ISO, one purpose of LMF (ISO-
24613) is to define a standard for lexi-
cons that covers multilingual data. 
1 Introduction 
Lexical Markup Framework (LMF) is a model 
that provides a common standardized framework 
for the construction of Natural Language Proc-
essing (NLP) lexicons. The goals of LMF are to 
provide a common model for the creation and 
use of lexical resources, to manage the exchange 
of data between and among these resources, and 
to enable the merging of a large number of indi-
vidual electronic resources to form extensive 
global electronic resources. 
Types of individual instantiations of LMF can 
include monolingual, bilingual or multilingual 
lexical resources. The same specifications are to 
be used for both small and large lexicons. The 
descriptions range from morphology, syntax, 
semantic to translation information organized as 
different extensions of an obligatory core pack-
age. The model is being developed to cover all 
natural languages. The range of targeted NLP 
applications is not restricted. LMF is also used to 
model machine readable dictionaries (MRD), 
which are not within the scope of this paper. 
2 History and current context 
In the past, this subject has been studied and de-
veloped by a series of projects like GENELEX 
[Antoni-Lay], EAGLES, MULTEXT, PAROLE, 
SIMPLE, ISLE and MILE [Bertagna]. More re-
cently within ISO1 the standard for terminology 
management has been successfully elaborated by 
the sub-committee three of ISO-TC37 and pub-
lished under the name "Terminology Markup 
Framework" (TMF) with the ISO-16642 refer-
ence. Afterwards, the ISO-TC37 National dele-
gations decided to address standards dedicated to 
NLP. These standards are currently elaborated as 
high level specifications and deal with word 
segmentation (ISO 24614), annotations 
(ISO 24611, 24612 and 24615), feature struc-
tures (ISO 24610), and lexicons (ISO 24613) 
with this latest one being the focus of the current 
paper. These standards are based on low level 
specifications dedicated to constants, namely 
data categories (revision of ISO 12620), lan-
guage codes (ISO 639), script codes 
(ISO 15924), country codes (ISO 3166), dates 
(ISO 8601) and Unicode (ISO 10646). 
 
This work is in progress. The two level organiza-
tion will form a coherent family of standards 
with the following simple rules: 
1) the low level specifications provide standard-
ized constants; 
                                                 
1 www.iso.org 
1
2) the high level specifications provide struc-
tural elements that are adorned by the standard-
ized constants. 
3 Scope and challenges 
The task of designing a lexicon model that satis-
fies every user is not an easy task. But all the 
efforts are directed to elaborate a proposal that 
fits the major needs of most existing models. 
In order to summarise the objectives, let's see 
what is in the scope and what is not. 
 
LMF addresses the following difficult chal-
lenges: 
? Represent words in languages where 
multiple orthographies (native scripts or 
transliterations) are possible, e.g. some 
Asian languages. 
? Represent explicitly (i.e. in extension) 
the morphology of languages where a de-
scription of all inflected forms (from a list 
of lemmatised forms) is manageable, e.g. 
English. 
? Represent the morphology of languages 
where a description in extension of all in-
flected forms is not manageable (e.g. Hun-
garian). In this case, representation in in-
tension is the only manageable issue. 
? Easily associate written forms and spo-
ken forms for all languages. 
? Represent complex agglutinating com-
pound words like in German. 
? Represent fixed, semi-fixed and flexible 
multiword expressions. 
? Represent specific syntactic behaviors, 
as in the Eagles recommendations. 
? Allow complex argument mapping be-
tween syntax and semantic descriptions, as 
in the Eagles recommendations. 
? Allow a semantic organisation based on 
SynSets (like in WordNet) or on semantic 
predicates (like in FrameNet). 
? Represent large scale multilingual re-
sources based on interlingual pivots or on 
transfer linking. 
LMF does not address the following topics: 
? General sentence grammar of a language 
? World knowledge representation 
In other words, LMF is mainly focused on the 
linguistic representation of lexical information. 
4 Key standards used by LMF 
LMF utilizes Unicode in order to represent the 
orthographies used in lexical entries regardless of 
language. 
Linguistic constants, like /feminine/ or 
/transitive/, are not defined within LMF but are 
specified in the Data Category Registry (DCR) 
that is maintained as a global resource by 
ISO TC37 in compliance with ISO/IEC 11179-
3:2003. 
The LMF specification complies with the 
modeling principles of Unified Modeling Lan-
guage (UML) as defined by OMG2 [Rumbaugh 
2004]. A model is specified by a UML class dia-
gram within a UML package: the class name is 
not underlined in the diagrams. The various ex-
amples of word description are represented by 
UML instance diagrams: the class name is under-
lined.  
5 Structure and core package 
LMF is comprised of two components: 
1) The core package consists of a structural 
skeleton that describes the basic hierarchy of in-
formation in a lexical entry. 
2) Extensions to the core package are ex-
pressed in a framework that describes the reuse 
of the core components in conjunction with addi-
tional components required for the description of 
the contents of a specific lexical resource. 
In the core package, the class called Database 
represents the entire resource and is a container 
for one or more lexicons. The Lexicon class is 
the container for all the lexical entries of the 
same language within the database. The Lexicon 
Information class contains administrative infor-
mation and other general attributes. The Lexical 
Entry class is a container for managing the top 
level language components. As a consequence, 
the number of representatives of single words, 
multi-word expressions and affixes of the lexicon 
is equal to the number of lexical entries in a 
given lexicon. The Form and Sense classes are 
parts of the Lexical Entry. Form consists of a text 
string that represents the word. Sense specifies or 
identifies the meaning and context of the related 
form. Therefore, the Lexical Entry manages the 
relationship between sets of related forms and 
their senses. If there is more than one orthogra-
                                                 
2 www.omg.org 
2
phy for the word form (e.g. transliteration) the 
Form class may be associated with one to many 
Representation Frames, each of which contains a 
specific orthography and one to many data cate-
gories that describe the attributes of that orthog-
raphy. 
The core package classes are linked by the re-
lations as defined in the following UML class 
diagram: 
 
Representation Frame
Lexicon Information
Form Sense
Entry Relation
Sense Relation
Lexical Entry
Database
Lexicon
0..* 0..*
0..*1
0..* 0..*
0..*1
1
0..*
1
1
1
0..*
1
1..*
1
0..*
1
1..*
1..*
1
 
 
Form class can be sub-classed into Lemmatised 
Form and Inflected Form class as follows: 
 
Lemmatised Form Inflected Form
Form
 
 
A subset of the core package classes are ex-
tended to cover different kinds of linguistic data. 
All extensions conform to the LMF core package 
and cannot be used to represent lexical data in-
dependently of the core package. From the point 
of view of UML, an extension is a UML pack-
age. Current extensions for NLP dictionaries are: 
NLP Morphology3, NLP inflectional paradigm, 
NLP Multiword Expression pattern, NLP Syntax, 
NLP Semantic and Multilingual notations, which 
is the focus of this paper. 
6 NLP Multilingual Extension 
The NLP multilingual notation extension is 
dedicated to the description of the mapping be-
tween two or more languages in a LMF database. 
The model is based on the notion of Axis that 
links Senses, Syntactic Behavior and examples 
pertaining to different languages. "Axis" is a 
                                                 
3 Morphology, Syntax and Semantic packages are 
described in [Francopoulo]. 
3
term taken from the Papillon4 project [S?rasset 
2001] 5 . Axis can be organized at the lexicon 
manager convenience in order to link directly or 
indirectly objects of different languages.  
 
6.1 Considerations for standardizing multi-
lingual data  
The simplest configuration of multilingual 
data is a bilingual lexicon where a single link is 
used to represent the translation of a given 
form/sense pair from one language into another. 
But a survey of actual practices clearly reveals 
other requirements that make the model more 
complex. Consequently, LMF has focused on the 
following ones: 
 
(i) Cases where the relation 1-to-1 is impos-
sible because of lexical differences among lan-
guages. An example is the case of English word 
?river? that relates to French words ?rivi?re? and 
?fleuve?, where the latter is used for specifying 
that the referent is a river that flows into the sea. 
The bilingual lexicon should specify how these 
units relate. 
 
(ii) The bilingual lexicon approach should 
be optimized to allow the easiest management of 
large databases for real multilingual scenarios. In 
order to reduce the explosion of links in a multi-
bilingual scenario, translation equivalence can be 
managed through an intermediate "Axis". This 
object can be shared in order to contain the num-
ber of links in manageable proportions. 
 
(iii) The model should cover both transfer 
and pivot approaches to translation, taking also 
into account hybrid approaches. In LMF, the 
pivot approach is implemented by a ?Sense 
Axis?. The transfer approach is implemented by 
a ?Transfer Axis?. 
 
(iv) A situation that is not very easy to deal 
with is how to represent translations to languages 
that are similar or variants. The problem arises, 
for instance, when the task is to represent transla-
tions from English to both European Portuguese 
and Brazilian Portuguese. It is difficult to con-
                                                 
4 www.papillon-dictionary.org  
5 To be more precise, Papillon uses the term "axie" 
from "axis" and "lexie". In the beginning of the LMF 
project, we used the term "axie" but after some bad 
comments about using a non-English term in a stan-
dard, we decided to use the term "axis". 
sider them as two separate languages. In fact, one 
is a variant of the other. The differences are mi-
nor: a certain number of words are different and 
some limited phenomena in syntax are different. 
Instead of managing two distinct copies, it is 
more effective to manage one lexicon with some 
objects that are marked with a dialectal attribute. 
Concerning the translation from English to Por-
tuguese: a limited number of specific Axis in-
stances record this variation and the vast major-
ity of Axis instances is shared. 
 
(v) The model should allow for representing 
the information that restricts or conditions the 
translations. The representation of tests that 
combine logical operations upon syntactic and 
semantic features must be covered. 
6.2 Structure 
The model is based on the notion of Axis that 
link Senses, Syntactic Behavior and examples 
pertaining to different languages. Axis can be 
organized at the lexicon manager convenience in 
order to link directly or indirectly objects of dif-
ferent languages. A direct link is implemented by 
a single axis. An indirect link is implemented by 
several axis and one or several relations. 
The model is based on three main classes: 
Sense Axis, Transfer Axis, Example Axis. 
6.3 Sense Axis 
Sense Axis is used to link closely related 
senses in different languages, under the same 
assumptions of the interlingual pivot approach, 
and, optionally, it can also be used to refer to one 
or several external knowledge representation sys-
tems.  
The use of the Sense Axis facilitates the repre-
sentation of the translation of words that do not 
necessarily have the same valence or morpho-
logical form in one language than in another. For 
example, in a language, we can have a single 
word that will be translated by a compound word 
into another language: English ?wheelchair? to 
Spanish ?silla de ruedas?. Sense Axis may have 
the following attributes: a label, the name of an 
external descriptive system, a reference to a spe-
cific node inside an external description. 
6.4 Sense Axis Relation 
Sense Axis Relation permits to describe the 
linking between two different Sense Axis in-
stances. The element may have attributes like 
label, view, etc. 
4
6.6 Transfer Axis Relation 
Transfer Axis Relation links two Transfer Axis 
instances. The element may have attributes like: 
label, variation. 
The label enables the coding of simple inter-
lingual relations like the specialization of 
?fleuve? compared to ?rivi?re? and ?river?. It is 
not, however, the goal of this strategy to code a 
complex system for knowledge representation, 
which ideally should be structured as a complete 
coherent system designed specifically for that 
purpose. 
6.7 Source Test and Target Test 
Source Test permits to express a condition on 
the translation on the source language side while 
Target Test does it on the target language side. 
Both elements may have attributes like: text and 
comment. 
6.5 Transfer Axis 
Transfer Axis is designed to represent multi-
lingual transfer approach. Here, linkage refers to 
information contained in syntax. For example, 
this approach enables the representation of syn-
tactic actants involving inversion, such as (1): 
6.8 Example Axis  
Example Axis supplies documentation for 
sample translations. The purpose is not to record 
large scale multilingual corpora. The goal is to 
link a Lexical Entry with a typical example of 
translation. The element may have attributes like: 
comment, source. 
 
(1) fra:?elle me manque? => 
eng:?I miss her? 
 
Due to the fact that a lexical entry can be a 
support verb, it is possible to represent transla-
tions that start from a plain verb to a support verb 
like (2) that means "Mary dreams": 
6.9 Class Model Diagram 
The UML class model is an UML package. The 
diagram for multilingual notations is as follows:  
(2)  fra:?Marie r?ve? =>  
  jpn:"Marie wa yume wo miru"  
Transfer Axis Relation
Sense Axis Relation
Syntactic Behavior
SenseExample
Transfer Axis
Example Axis
Source Test
Sense Axis
Target Test
SynSet
Sense
0..*
0..*
0..*
0..*
1
0..*
0..* 0..*
0..*
0..*
1
0..*
1
0..1
1
0..*
0..1
1
1
0..*
1
0..*
1
0..*
5
7 Three examples 
7.1 First example 
The first example is about the interlingual ap-
proach with two axis instances to represent a 
near match between "fleuve" in French and 
"river" in English. In the diagram, French is lo-
cated on the left side and English on the right 
side. The axis on the top is not linked directly to 
any English sense because this notion does not 
exist in English.  
: Sense Axis Relation
comment = flows into the sea
label = more precise
: Sense
label = eng:riverlabel = fra:rivi?re
: Sense
: Sense
label = fra:fleuve
: Sense Axis
: Sense Axis
 
 
7.2 Second example 
Let's see now an example about the transfer 
approach about slight variations between vari-
ants. The example is about English on one side 
and European Portuguese and Brazilian on the 
other side. Due to the fact that these two last 
variants have a very similar syntax, but with 
some local exceptions, the goal is to avoid a full 
and dummy duplication. For instance, the nomi-
native forms of the third person clitics are largely 
preferred in Brazilian rather than the oblique 
form as in European Portuguese. The transfer 
axis relations hold a label to distinguish which 
axis to use depending on the target object. 
 
: Transfer Axis Relation
label = European Portuguese
: Transfer Axis Relation
label = Brazilian
: Syntactic Behavior
label = let me see
: Syntactic Behavior
label = Deixa eu ver
: Syntactic Behavior
label = Deixa-me ver
: Transfer Axis
: Transfer Axis
: Transfer Axis
 
7.3 Third example 
A third example shows how to use the Trans-
fer Axis relation to relate different information in 
a multilingual transfer lexicon. It represents the 
translation of the English ?develop? into Italian 
and Spanish. Recall that the more general sense 
links ?eng:develop? and ?esp:desarrollar?. Both, 
Spanish and Italian, have restrictions that should 
6
be tested in the source language: if the second 
argument of the construction refers to certain 
elements (picture, mentalCreation, building) it 
should be translated into specific verbs.  
 
: Source Test
semanticRestriction = eng:mentalCreation
syntacticArgument = 2
: Source Test
semanticRestriction = eng:picture
syntacticArgument = 2
: Source Test
semanticRestriction = eng:building
syntacticArgument = 2
: Transfer Axis Relation
: Transfer Axis Relation
: Transfer Axis Relation
: Syntactic Behavior
label = esp:revelar
: Syntactic Behavior
label = ita:sviluppare
: Syntactic Behavior
label = ita:costruire
: Syntactic Behavior
label = eng:develop
: Syntactic Behavior
label = esp:construir
: Syntactic Behavior
label = esp:desarrollar
: Transfer Axis
: Transfer Axis
: Transfer Axis
: Transfer Axis
 
8 LMF in XML  
During the last three years, the ISO group fo-
cused on the UML specification. In the last ver-
sion of the LMF document [LMF 2006] a DTD 
has been provided as an informative annex. The 
following conventions are adopted: 
? each UML attribute is transcoded as a 
DC (for Data Category) element 
? each UML class is transcoded as an 
XML element 
? UML aggregations are transcoded as 
content inclusion 
? UML shared associations (i.e. associa-
tions that are not aggregations) are 
transcoded as IDREF(S) 
The first example (i.e. "river") can be represented 
with the following XML tags: 
 
 
<Database> 
<!?   French section ? 
<Lexicon> 
<LexiconInformation 
<DC att="name" val=?French Extract?/> 
<DC att="language" val="fra"/> 
</LexiconInformation> 
<LexicalEntry > 
<DC att="partOfSpeech" val=?noun?/> 
<LemmatisedForm> 
<DC att="writtenForm" val=?fleuve?/> 
</LemmatisedForm> 
<Sense id=?fra.fleuve1?> 
 <SemanticDefinition> 
                  <DC att="text" 
val=?Grande rivi?re lorsqu'elle aboutit ? la mer?/> 
<DC att="source" val=?Le Petit Robert 2003?/> 
</SemanticDefinition> 
</Sense> 
</LexicalEntry> 
<LexicalEntry> 
<DC att="partOfSpeech" val=?noun?/> 
<LemmatisedForm> 
  <DC att="writtenForm" val=?rivi?re?/> 
</LemmatisedForm> 
<Sense id=?fra.riviere1?> 
 <SemanticDefinition> 
<DC att="text"  
val=?Cours d'eau naturel de moyenne importance?/> 
<DC att="source" val=?Le Petit Robert 2003?/> 
</SemanticDefinition> 
</Sense> 
</LexicalEntry> 
</Lexicon> 
<!?                                                 Multilingual section ? 
<SenseAxis id=?A1? senses="fra.fleuve1"> 
7
<SenseAxisRelation targets="A2"> 
 <DC att="comment" val="flows into the sea"/> 
 <DC att="label" val="more precise"/> 
</SenseAxisRelation> 
</SenseAxis> 
<SenseAxis id=?A2? senses="fra.riviere1 eng.river1"/> 
<!?                                                English section ? 
<Lexicon> 
<LexiconInformation> 
<DC att="name" val=?English Extract?/> 
<DC att="language" val="eng"/> 
</LexiconInformation> 
<LexicalEntry> 
<DC att="partOfSpeech" val=?noun?/> 
<LemmatisedForm> 
<DC att="writtenForm" val=?river?/> 
</LemmatisedForm> 
<Sense id=?eng.river1?> 
 <SemanticDefinition> 
<DC att="text" 
val=?A natural and continuous flow of water in a long 
line across a country into the sea?/> 
<DC att="source" val=?Longman DCE 2005?/> 
</SemanticDefinition> 
</Sense> 
</LexicalEntry> 
</Lexicon> 
</Database> 
 
 
9 Comparison 
A serious comparison with previously existing 
models is not possible in this current paper due 
to the lack of space. We advice the interested 
colleague to consult the technical report "Ex-
tended examples of lexicons using LMF" located 
at:  "http://lirics.loria.fr" in the document area. 
The report explains how to use LMF in order to 
represent OLIF-2, Parole/Clips, LC-Star, Word-
Net, FrameNet and BD?f. 
10 Conclusion 
In this paper we presented the results of the 
ongoing research activity of the LMF ISO stan-
dard. The design of a common and standardized 
framework for multilingual lexical databases will 
contribute to the optimization of the use of lexi-
cal resources, specially their reusability for dif-
ferent applications and tasks. Interoperability is 
the condition of a effective deployment of usable 
lexical resources. 
In order to reach a consensus, the work done 
has paid attention to the similarities and differ-
ences of existing lexicons and the models behind 
them. 
Acknowledgements 
The work presented here is partially funded by 
the EU eContent-22236 LIRICS project 6 , par-
tially by the French TECHNOLANGUE 7 + 
OUTILEX8 programs. 
References 
Antoni-Lay M-H., Francopoulo G., Zaysser L. 1994 
A generic model for reusable lexicons: the 
GENELEX project. Literary and linguistic comput-
ing 9(1) 47-54 
Bertagna F., Lenci A., Monachini M., Calzolari N. 
2004 Content interoperability of lexical resources, 
open issues and MILE perspectives LREC Lisbon 
Francopoulo G., George M., Calzolari N., Monachini 
M., Bel N., Pet M., Soria C. 2006 Lexical Markup 
Framework (LMF) LREC Genoa. 
LMF 2006 Lexical Markup Framework ISO-
CD24613-revision-9, ISO Geneva 
Rumbaugh J., Jacobson I.,Booch G. 2004 The unified 
modeling language reference manual, second edi-
tion, Addison Wesley 
S?rasset G., Mangeot-Lerebours M. 2001 Papillon 
Lexical Database project: monolingual dictionaries 
& interlingual links NLPRS Tokyo 
                                                 
6 http://lirics.loria.fr 
7 www.technolangue.net 
8 www.at-lci.com/outilex/outilex.html 
8
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 105?111,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Spanish Resource Grammar: pre-processing strategy and lexical acquisi-
tion 
Montserrat Marimon, N?ria Bel, Sergio Espeja, Natalia Seghezzi 
IULA - Universitat Pompeu Fabra 
Pl. de la Merc?, 10-12 
 08002-Barcelona 
{montserrat.marimon,nuria.bel,sergio.espeja,natalia.seghezzi}@upf.edu
 
 
Abstract 
This paper describes work on the develop-
ment of an open-source HPSG grammar for 
Spanish implemented within the LKB sys-
tem. Following a brief description of the 
main features of the grammar, we present 
our approach for pre-processing and on-
going research on automatic lexical acqui-
sition.1   
1 Introduction 
In this paper we describe the development of the 
Spanish Resource Grammar (SRG), an open-
source 2  medium-coverage grammar for Spanish. 
The grammar is grounded in the theoretical 
framework of HPSG (Head-driven Phrase Struc-
ture Grammar; Pollard and Sag, 1994) and uses 
Minimal Recursion Semantics (MRS) for the se-
mantic representation (Copestake et al 2006). The 
SRG is implemented within the Linguistic Knowl-
edge Building (LKB) system (Copestake, 2002), 
based on the basic components of the grammar 
Matrix, an open?source starter-kit for the devel-
opment of HPSG grammars developed as part of 
the LinGO consortium?s multilingual grammar 
engineering (Bender et al, 2002).  
The SRG is part of the DELPH-IN open-source 
repository of linguistic resources and tools for 
writing (the LKB system), testing (The [incr 
tsbd()]; Oepen and Carroll, 2000) and efficiently 
                                                 
                                                
1 This research was supported by the Spanish Ministerio de 
Educaci?n y Ciencia: Project AAILE (HUM2004-05111-C02-
01), Ramon y Cajal, Juan de la Cierva programmes and PTA-
CTE/1370/2003 with Fondo Social Europeo. 
2 The Spanish Resource Grammar may be downloaded from: 
http://www.upf.edu/pdi/iula/montserrat.marimon/. 
processing HPSG grammars (the PET system; 
Callmeier, 2000). Further linguistic resources that 
are available in the DELPH-IN repository include 
broad-coverage grammars for English, German and 
Japanese as well as  smaller grammars for French, 
Korean, Modern Greek, Norwegian and 
Portuguese .3   
The SRG has a full coverage of closed word 
classes and it contains about 50,000 lexical entries 
for open classes (roughtly: 6,600 verbs, 28,000 
nouns, 11,200 adjectives and 4,000 adverbs).  
These lexical entries are organized into a type 
hierachy of about 400 leaf types (defined by a type 
hierarchy of  around 5,500 types). The grammar 
also has 40 lexical rules to perform valence 
changing operations on lexical items and 84 
structure rules to combine words and phrases into 
larger constituents and to compositionally build up 
the semantic representation.  
We have been developing the SRG since 
January 2005. The range of linguistic phenomena 
that the grammar handles includes almost all types 
of subcategorization structures, valence 
alternations,  subordinate clauses, raising and 
control, determination,  null-subjects and 
impersonal constructions, compound tenses, 
modification,  passive constructions, comparatives 
and superlatives, cliticization, relative and 
interrogative clauses and sentential adjuncts, 
among others. 
Together with the linguistic resources (grammar 
and lexicon) we provide a set of controlled hand-
constructed test suites. The construction of the test 
suites plays a major role in the development of the 
SRG, since test suites provide a fine-grained diag-
 
3 See . http://www.delph-in.net/
 
105
nosis of grammar performance and they allow us to 
compare the SRG with other DELPH-IN gram-
mars. In building the test suites we aimed at (a) 
testing specific phenomena in isolation or in con-
trolled interaction, (b) providing test cases which 
show systematic and exhaustive variations over 
each phenomenon, including infrequent phenom-
ena and variations, (c) avoiding irrelevant variation 
(i.e. different instances of the same lexical type), (d) 
avoiding ambiguity, and (e) including negative or 
ungrammatical data. We have about 500 test cases 
which are distributed by linguistic phenomena (we 
have 17 files). Each test case includes a short lin-
guistic annotation describing the phenomenon and 
the number of expected results when more than 
one analysis cannot be avoided (e.g. testing op-
tionality). 
Test suites are not the only source of data we 
have used for testing the SRG. Hand-constructed 
sentences were complemented by real corpus cases 
from: (a) the Spanish questions from the Question 
Answering track at CLEF (CLEF-2003, CLEF-
2004, CLEF-2005 and CLEF-2006), and (b) the 
general sub-corpus of the Corpus T?cnic de 
l?IULA (IULA?s Technical Corpus; Cabr? and 
Bach, 2004); this sub-corpus includes newspaper 
articles and it has been set up for contrastive 
studies. CLEF cases include short queries showing 
little interaction of phenomena and an average of 
9.2 words; newspaper articles show a high level of 
syntactic complexity and interaction of phenomena, 
sentences are a bit longer, ranging up to 35 words. 
We are currently shifting to much more varied 
corpus data of the Corpus T?cnic de l?IULA, which 
includes specialized corpus of written text in the 
areas of computer science, environment, law, 
medicine and economics, collected from several 
sources, such as legal texts, textbooks, research 
reports, user manuals, ? In these texts sentence 
length may range up to 70 words.  
The rest of the paper describes the pre-
processing strategy we have adopted and on our 
on-going research on lexical acquisition. 
2 Pre-processing in the SRG 
Following previous experiments within the 
Advanced Linguistic Engineering Platform (ALEP) 
platform (Marimon, 2002), we have integrated a 
shallow processing tool, the FreeLing tool, as a 
pre-processing module of the grammar.  
The FreeLing tool is an open-source4 language 
analysis tool suite (Atserias et al, 2006) perfoming 
the following functionalities (though 
disambiguation, named entity classification and the 
last three functionalities have not been integrated):  
 
? Text tokenization (including MWU and 
contraction splitting). 
? Sentence splitting. 
? Morpho-syntactic analysis and 
disambiguation. 
n. 
                                                
? Named entity detection and classification. 
? Date/number/currency/ratios/physical 
magnitude (speed, weight, temperature, 
density, etc.) recognitio
? Chart-based shallow parsing. 
? WordNet-based sense annotation.  
? Dependency parsing.  
FreeLing also includes a guesser to deal with 
words which are not found in the lexicon by 
computing the probability of each possible PoS tag 
given the longest observed termination string for 
that word. Smoothing using probabilities of shorter 
termination strings is also performed. Details can 
be found in Brants (2000) and Samuelson (1993).  
Our system integrates the FreeLing tool by 
means of the LKB Simple PreProcessor Protocol 
(SPPP; http://wiki.delph-in.net/moin/LkbSppp), 
which assumes that a preprocessor runs as an 
external process to the LKB system, and uses the 
LKB inflectional rule component to convert the 
PoS tags delivered by the FreeLing tool into partial 
descriptions of feature structures. 
2.1 The integration of PoS tags 
The integration of the morpho-syntactic analysis in 
the LKB system using the SPPP protocol means 
defining inflectional rules that propagate the mor-
pho-syntactic information associated to full-forms, 
in the form of PoS tags, to the morpho-syntactic 
features of the lexical items. (1) shows the rule 
propagating the tag AQMS (adjective qualitative 
masculine singular) delivered by FreeLing. Note 
 
4 The FreeLing tool may be downloaded from 
http://www.garraf.epsevg.upc.es/freeling/. 
106
that we use the tag as the rule identifier (i.e. the 
name of the inflectional rule in the LKB).  
(1) aqms :=  
 %suffix () 
 [SYNSEM.LOCAL[CAT adj, 
               AGR.PNG[PN 3sg, 
                     GEN masc]]] 
 
In Spanish, when the verb is in non-finite form, 
such as infinitive or gerund, or it is in the impera-
tive, clitics5 take the form of enclitics. That is, they 
are attached to the verb forming a unique word, 
e.g. hacerlo (hacer+lo; to do it), gustarle (gus-
tar+le; to like to him). FreeLing does not split 
verbs and pronouns, but uses complex tags that 
append the tags of each word. Thus, the form ha-
cerlo gets the tag VMN+PP3MSA (verb main in-
finitive + personal pronoun 3rd masculine singular 
accusative). In order to deal with these complex 
tags, the SRG includes a series of rules that build 
up the same type of linguistic structure as that one 
built up with the structure rules attaching affixes to 
the left of verbal heads. Since the application of 
these rules is based on the tag delivered by FreeL-
ing, they are included in the set of inflectional rules 
and they are applied after the set of rules dealing 
with complement cliticization.   
 Apart from avoiding the implementation of in-
flectional rules for such a highly inflected lan-
guage, the integration of the morpho-syntactic 
analysis tags will allow us to implement default 
lexical entries (i.e. lexical entry templates that are 
activated when the system cannot find a particular 
lexical entry to apply) on the basis of the category 
encoded to the lexical tag delivered by FreeLing, 
for virtually unlimited lexical coverage. 6
2.2 The integration of multiword expressions 
All multiword expressions in FreeLing are stored 
in a file. The format of the file is one multiword 
per line, having three fields each: form, lemma and 
PoS.7 (2) shows two examples of multiword fixed 
                                                 
                                                
5 Actually, Spanish weak pronouns are considered pronominal 
affixes rather than pronominal clitics. 
6 The use of underspecified default lexical entries in a 
highly lexicalized grammar, however, may increase 
ambiguity and overgeneration (Marimon and Bel, 
2004). 
7 FreeLing only handles continuous multiword expres-
sions. 
expressions; i.e. the ones that are fully lexicalized 
and never show morpho-syntactic variation, a 
trav?s de (through) and a buenas horas (finally). 
 
(2) a_trav?s_de a_trav?s_de SPS00 
   a_buenas_horas a_buenas_horas RG 
 
The multiword form field may admit lemmas in 
angle brackets, meaning that any form with that 
lemma will be a valid component for the multi-
word. Tags are specified directly or as a reference 
to the tag of some of the multiword components. 
(3) builds a multiword with both singular and plu-
ral forms  (apartado(s) de correos (P.O Box)). The 
tag of the multiform is that of its first form ($1) 
which starts with NC and takes the values for 
number depending on whether the form is singular 
or plural.  
 
(3) <apartado>_de_correos apar-
tado_de _correos \$1:NC 
 
Both fixed expressions and semi-fixed expres-
sions are integrated by means of the inflectional 
rules that we have described in the previous sub-
section and they are treated in the grammar as 
word complex with a single part of speech.  
2.3 The integration of messy details and 
named entities 
FreeLing identifies, classifies and, when appropri-
ate, normalizes special text constructions that may 
be considered peripheral to the lexicon, such as 
dates, numbers, currencies, ratios, physical magni-
tudes, etc.  FreeLing also identifies and classifies 
named entities (i.e. proper names); however, we do 
not activate the classification functionality, since 
high performance of that functionality is only 
achieved with PoS disambiguated contexts.   
To integrate these messy details and named enti-
ties into the grammar, we require special inflec-
tional rules and lexical entry templates for each 
text construction tag delivered by FreeLing. Some 
of these tags are: W for dates, Z for numbers, Zm 
for currencies, ... In order to define one single en-
try for each text construct, we identify the tag and 
the STEM feature. (4) shows the lexical entry for 
dates.8
 
8 Each lexical entry in the SRG consists of a unique identifier, 
a lexical type, an orthography and a semantic relation. 
107
 
(4)  
date := date_le & 
[STEM <?w?>, 
SYNSEM.LKEY.KEYREL.PRED time_n_rel] 
 
The integration of these messy details allows us 
to release the analysis process from certain tasks 
that may be reliably dealt with by shallow external 
components.  
3 Automatic Lexical Acquisition 
We have investigated Machine Learning (ML) 
methods applied to the acquisition of the informa-
tion contained in the lexicon of the SRG. 
ML applied to lexical acquisition is a very active 
area of work linked to deep linguistic analysis due 
to the central role that lexical information has in 
lexicalized grammars and the costs of hand-
crafting them. Korhonen (2002), Carroll and Fang 
(2004), Baldwin (2005), Blunsom and Baldwin 
(2006), and Zhang and Kordoni (2006) are just a 
few examples of reported research work on deep 
lexical acquisition. 
The most successful systems of lexical acquisi-
tion are based on the linguistic idea that the con-
texts where words occur are associated to particu-
lar lexical types. Although the methods are differ-
ent, most of the systems work upon the syntactic 
information on words as collected from a corpus, 
and they develop different techniques to decide 
whether this information is relevant for type as-
signment or it is noise, especially when there are 
just a few examples. In the LKB grammatical 
framework, lexical types are defined as a combina-
tion of grammatical features. For our research, we 
have looked at these morpho-syntactically moti-
vated features that can help in discriminating the 
different types that we will ultimately use to clas-
sify words. Thus, words are assigned a number of 
grammatical features, the ones that define the lexi-
cal types. 
Table 1 and Table 2 show the syntactic features 
that we use to characterize 6 types of adjectives 
and 7 types of nouns in Spanish, respectively.9 As 
can be observed, adjectives are cross-classified 
according to their syntactic position within the NP, 
i.e. (preN(ominal)) vs  postN(ominal), the possibil-
ity of co-occurring in predicative constructions 
                                                 
9 The SRG has 35 types for nouns and 44 types for adjectives. 
(pred) and being modified by degree adverbs (G), 
and their subcategorization frame (pcomp); 
whereas lexical types for nouns are basically de-
fined on the basis of the mass/countable distinction 
and valence information. Thus, an adjective like 
bonito (nice), belonging to the type a_qual_intr, 
may be found both in pre-nominal and post-
nominal position or in predicative constructions, it 
may also be modified by degree adverbs, this type 
of adjectives, however, does not take comple-
ments. Nouns belonging to the type n_intr_count, 
like muchacha (girl), are countable intransitive 
nouns. 
 
TYPE/SF preN postN pred G pcomp 
a_adv_int yes no no no no 
a_adv_event yes yes no no no 
a_rel_nonpred no yes no no no 
a_rel_pred no yes yes no no 
a_qual_intr yes yes yes yes no 
a_qual_trans yes yes yes yes yes 
Table 1. Some adjectival types of the SRG 
 
TYPE/SF mass count intr trans pcomp 
n_intr_mass yes no yes no no 
n_intr_count no yes yes no no 
n_intr_cnt-
mss 
yes yes yes no no 
n_trans_mass yes no no yes no 
n_trans_count no yes no yes no 
n_ppde_pcom
p_count 
no yes no yes yes 
n_ppde_pcom
p_mss 
yes no no yes yes 
Table 2. Some nominal types of the SRG 
 
We have investigated two methods to automati-
cally acquire such linguistic information for Span-
ish nouns and adjectives: a Bayesian model and a 
decision tree. The aim of working with these two 
methods was to compare their performance taking 
into account that while the decision tree gets the 
information from previously annotated data, the 
Bayesian method learns it from the linguistic ty-
pology as defined by the grammar. These methods 
are described in the following subsections.  
3.1 A Bayesian model for lexical acquisition 
We have used a Bayesian model of inductive learn-
ing for assigning grammatical features to words 
occurring in a corpus. Given a hypothesis space 
(the linguistic features of words according to its 
lexical type) and one or more occurrences of the 
108
word to classify, the learner evaluates all hypothe-
ses for word features and values by computing 
their posterior probabilities, proportional to the 
product of prior probabilities and likelihood.  
In order to obtain the likelihood, grammatical 
features are related to the expected contexts where 
their instances might appear. The linguistic typol-
ogy provides likelihood information that is the 
learner?s expectation about which contexts are 
likely to be observed given a particular hypothesis 
of a word type. This likelihood is used as a substi-
tute of the computations made by observing di-
rectly the data, which is what a supervised machine 
learning method does. As said, our aim was to 
compare these two strategies.   
The decision on a particular word is determined 
by averaging the predictions of all hypothesis 
weighted by their posterior probabilities. More 
technically, for each syntactic feature {sf1, sf2, ..., 
sfn} of the set SF (Syntactic Features) represented 
in the lexical typology, we define the goal of our 
system to be the assignment of a value, {no, yes}, 
that maximizes the result of a function f: ?? SF, 
where ? is the collection of its occurrences (? = 
{v1, v2, ..., vz}), each being a n-dimensional vector. 
The decision on value assignment is achieved by 
considering every occurrence as a cumulative evi-
dence in favour or against of having each syntactic 
feature. Thus, our function Z?(SF, ?), shown in (5), 
will assess how much relevant information is got 
from all the vectors. A further function, shown in 
(8), will decide on the maximal value in order to 
assign sfi,x. 
(5)  ?= z
j j
vxisfPxisfZ )|,(),,(' ?
 
To assess P(sfi,x|vj), we use (6), which is the ap-
plication of Bayes Rule for solving the estimation 
of the probability of a vector conditioned to a par-
ticular feature and value.  
(6) 
?
=
k ki
sfPkisfjvP
xisfPxisfjvP
jvxisfP ),(),|(
),(),|(
)|,(
 
 
For solving (6), the prior P(sfi,x) is computed on 
the basis of a lexical typology too, assuming that 
what is more frequent in the typology will corre-
spondingly be more frequent in the data. For com-
puting the likelihood P(vj|sfi,x), as each vector is 
made of m components, that is, the linguistic cues 
vz = {lc1, lc2, ..., lcm}, we proceed as in (7) on the 
basis of P(lcl|sfi,x); i.e. the likelihood of finding the 
word in a particular context given a particular syn-
tactic feature. 
(7)  ?==
m
l xi
sfllcPxisfjvP 1
),|(),|(
 
Finally Z, as in (8), is the function that assigns 
the syntactic features to ? .10
 
(8)  
??
??
?
??
??
?
?=>=
?=>==
no
yesxi
sfZ
noxi
sfZ
yes
noxi
sfZ
yesxi
sfZ
Z
)|
,
(')|
,
('
)|
,
(')|
,
('
??
??
 
For computing the likelihood, we count on the 
conditional probabilities of the correlations be-
tween features as defined in the typology. We use 
these correlations to infer the expectation of ob-
serving the linguistic cues associated to particular 
syntactic features, and to make it to be conditional 
to a particular feature and value. However, linguis-
tic cues and syntactic features are in two different 
dimensions; syntactic features are properties of 
lexical items, while linguistic cues show the char-
acteristics of actual occurrences. As we assume 
that each syntactic feature must have at least one 
corresponding linguistic cue, we must tune the 
probability to acknowledge the factors that affect 
linguistic cues. For such a tuning, we have consid-
ered the following two issues: (i) to include in the 
assessments the known uncertainty of the linguistic 
cues that can be present in the occurrence or not; 
and (ii) to create a dummy variable to deal with the 
fact that, while syntactic features in the typology 
are independent from one another, evidences in 
text are not so. 
We have also observed that the information that 
can be gathered by looking at all word occurrences 
as a complex unit have a conclusive value. Take 
for instance the case of prepositions. The observa-
tion of a given prepositions in different occur-
rences of the same word is a conclusive evidence 
for considering it a bound preposition.  In order to 
take this into account, we have devised a function 
that acts as a dynamic weighting module. The 
function app_lc(sfi, ?) returns the number of con-
texts where the cue is found. In the case that in a 
                                                 
10 In the theoretical case of having the same probability 
for yes and for no, Z is undefined.  
109
particular signature there is no context with such a 
lc, it returns ?1?. Thus, app_lc is used to reinforce 
this conclusive evidence in (5), which is now (9). 
 
(9) 
 
),(_*)|,(),,(' ?? isflcapp
z
j j
vyesxisfPyesxisfZ ??
???
?? ===
 ? ===
z
j j
vnoxisfPnoxisfZ )|,(),,(' ?  
 
3.2 A Decision tree 
Linguistic motivated features have also been 
evaluated using a C4.5 Decision Tree (DT) classi-
fier (Quinlan, 1993) in the Weka implementation 
(Witten and Frank, 2005). These features corre-
spond to the expected contexts for the different 
nominal and adjectival lexical types. 
We have trained the DT with all the vectors of 
the word occurrences that we had in the different 
gold-standards, using their encoding for the super-
vised experiment in a 10-fold cross-validation test-
ing (Bel et al 2007).  
3.3 Evaluation and Results 
For the evaluation, we have applied both methods 
to the lexical acquisition of nouns and adjectives.  
We have worked with a PoS tagged corpus of 
1,091,314 words. Datasets of 496 adjectives and 
289 nouns were selected among the ones that had 
occurrences in the corpus. Some manual selection 
had to be done in order to have all possible types 
represented but still it roughly corresponds to the 
distribution of features in the existing lexicon. 
We evaluated by comparing with Gold-
standard files; i.e. the manually encoded lexicon of 
the SRG. The usual accuracy measures as type 
precision (percentage of feature values correctly 
assigned to all values assigned) and type recall 
(percentage of correct feature values found in the 
dictionary) have been used. F1 is the usual score 
combining precision and recall.  
Table 3 shows the results in terms of F1 score 
for the different methods and PoS for feature as-
signment. From these data, we concluded that the 
probabilistic information inferred from the lexical 
typology defined in our grammar is a good source 
of knowledge for lexical acquisition.  
 
 
PoS noun adj 
Z 0.88 0.87 
DT 0.89 0.9 
Table 3. F1 for different methods and PoS. 
 
Table 4 shows more details of the results compar-
ing between DT and Z for Spanish adjectives. 
 
 SF = no SF = yes 
 Z DT Z DT 
prep_a 0.98 0.97 0.72 0.44 
prep_en 0.98 0.99 0.27 0 
prep_con 0.99 0.99 0.60 0 
prep_para 0.98 0.99 0.51 0.53 
prep_de 0.88 0.97 0.34 0.42 
postN 0 0 0.99 0.99 
preN 0.75 0.83 0.44 0.80 
Pred 0.50 0.41 0.59 0.82 
G 0.85 0.80 0.75 0.72 
Sent 0.97 0.97 0.55 0.44 
Table 4. F1 for Spanish adjectival features. 
 
Finally, Table 5 shows the results for 50 Spanish 
nouns with only one occurrence in the corpus. 
These results show that grammatical features can 
be used for lexical acquisition of low frequency 
lexical items, providing a good hypothesis for en-
suring grammar robustness and adding no over-
generation to parsing results.  
 
 DT Z 
 prec. rec. F prec. rec. F 
MASS 0.50 0.16 0.25 0.66 0.25 0.36 
COUNT 0.97 1.00 0.98 1.00 0.96 0.98 
TRANS 0.75 0.46 0.57 0.68 0.73 0.71 
INTRANS 0.85 0.95 0.89 0.89 0.76 0.82 
PCOMP 0 0 0 0.14 0.20 0.16 
Table 5. Results of 50 unseen nouns with a sin-
gle occurrence. 
4 Future Work 
We have presented work on the development of an 
HPSG grammar for Spanish; in particular, we have 
described our approach for pre-processing and on-
going research on automatic lexical acquisition. 
Besides extending the coverage of the SRG and 
continuing research on lexical acquisition, the spe-
cific aims of our future work on the SRG are: 
? Treebank development. 
110
? To extend the shallow/deep architecture 
and integrate the structures generated by 
partial parsing, to provide robust techniques 
for infrequent structural constructions. The 
coverage of these linguistic structures by 
means of structure rules would increase both 
processing time and ambiguity.  
? To use ML methods for disambiguation; 
i.e. for ranking possible parsings according 
to relevant linguistic features, thus enabling 
the setting of a threshold to select the n-best 
analyses. 
? The development of error mining tech-
niques (van Noord, 2004) to identify errone-
ous and incomplete information in the lin-
guistic resources which cause the grammar 
to fail.  
References 
J. Atserias, B. Casas, E. Comelles, M. Gonz?lez, L. Pa-
dr? and M. Padr?. 2006. FreeLing 1.3: Syntactic and 
semantic services in an open-source NLP library. 5th 
International Conference on Language Resources 
and Evaluation. Genoa, Italy. 
T. Baldwin. 2005. Bootstrapping Deep Lexical Re-
sources: Resources for Courses, ACL-SIGLEX 2005. 
Workshop on Deep Lexical Acquisition. Ann Arbor, 
Michigan.  
N. Bel, S. Espeja, M. Marimon. 2007. Automatic Ac-
quisition of Grammatical Types for Nouns. Human 
Language Technologies: The Annual Conference of 
the North American Chapter of the Association for 
Computational Linguistics. Rochester, NY, USA, 
E.M. Bender, D. Flickinger and S. Oepen. 2002. The 
grammar Matrix. An open-source starter-kit for the 
rapid development of cress-linguistically consistent 
broad-coverage precision grammar. Workshop on 
Grammar Engineering and Evaluation, 19th Interna-
tional Conference on Computational Linguistics. 
Taipei, Taiwan.  
P. Blunsom and T. Baldwin. 2006. Multilingual Deep 
Lexical Acquisition for HPSGs via Supertagging. 
Conference on Empirical Methods in Natural Lan-
guage Processing. Sydney, Australia. 
T. Brants. 2000. TnT: A statistical part-of-speech tag-
ger. 6th Conference on Applied Natural Language 
Processing. Seattle, USA. 
T. Cabr? and C. Bach, 2004. El corpus t?cnic de 
l?IULA: corpus textual especializado pluriling?e. 
Panacea, V. 16, pages 173-176. 
U. Callmeier. 2000. Pet ? a platform for experimenta-
tion with efficient HPSG processing. Journal of 
Natural Language Engineering 6(1): Special Issue 
on Efficient Processing with HPSG: Methods, Sys-
tem, Evaluation, pages 99-108. 
A. Copestake, D. Flickinger, C. Pollard and I.A. Sag. 
2006. Minimal Recursion Semantics: An Introduc-
tion. Research on Language and Computation 
3.4:281-332. 
A. Copestake. 2002. Implementing Typed Features 
Structure Grammars. CSLI Publications.  
A. Korhonen. 2002. ?Subcategorization acquisition?. As 
Technical Report UCAM-CL-TR-530, University of 
Cambridge, UK. 
M. Marimon. 2002. Integrating Shallow Linguistic 
Processing into a Unification-based Spanish Gram-
mar. 9th International Conference on Computational 
Linguistics. Taipei, Taiwan.  
M. Marimon and N. Bel. 2004. Lexical Entry Templates 
for Robust Deep Parsing. 4th International Confer-
ence on Language Resources and Evaluation. Lis-
bon, Portugal. 
S. Oepen and J. Carroll. 2000. Performance Profiling for 
Parser Engineering. Journal of Natural Language 
Engineering 6(1): Special Issue on Efficient Process-
ing with HPSG: Methods, System, Evaluation, pages 
81-97. 
C.J. Pollard and I.A. Sag. 1994. Head-driven Phrase 
Structure Grammar. The University of Chicago 
Press, Chicago.  
R.J. Quinlan 1993. C4.5: Programs for Machine Learn-
ing. Series in Machine Learning. Morgan Kaufman, 
San Mateo, CA. 
C. Samuelson. 1993. Morphological tagging based en-
tirely on Bayesian inference. 9th Nordic Conference 
on Computational Linguistics. Stockholm, Sweden.  
I.H. Witten and E. Frank. 2005. Data Mining: Practical 
machine learning tools and techniques. Morgan 
Kaufmann, San Francisco. 
G. van Noord. 2004. Error mining for wide-coverage 
grammar engineering. 42th Annual Meeting of the 
ACL. Barcelona, Spain. 
Y. Zhang and V. Kordoni. 2006. Automated deep lexi-
cal acquisition for robust open text processing. 5th 
International Conference on Language Resources 
and Evaluation. Genoa, Italy. 
111
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 46?52,
Beijing, August 2010
Automatic Detection of on-deverbal Event ouns for 
Quick Lexicon Production 
?ria Bel 
IULA 
Universitat Pompeu Fabra 
nuria.bel@upf.edu 
Maria Coll 
IULA 
Universitat Pompeu Fabra 
maria.coll02@ 
campus.upf.edu 
Gabriela Resnik 
Universidad Nacional de 
General Sarmiento and Uni-
versitat Pompeu Fabra 
gresnik@ungs.edu.ar 
 
Abstract 
In this work we present the results of ex-
perimental work on the development of 
lexical class-based lexica by automatic 
means. Our purpose is to assess the use 
of linguistic lexical-class based informa-
tion as a feature selection methodology 
for the use of classifiers in quick lexical 
development. The results show that the 
approach can help reduce the human ef-
fort required in the development of lan-
guage resources significantly. 
1 Introduction 
Although language independent, many linguistic 
technologies are inherently tied to the availabili-
ty of particular language data (i.e. Language Re-
sources, LR). The nature of these data is very 
much dependent on particular technologies and 
the applications where are used. Currently, most 
systems are using LR collected by hand that still 
do not cover all languages, or all possible appli-
cation domains, or all possible information re-
quired by the many applications that are being 
proposed. Methods for the automatic and quick 
development of new LR have to be developed in 
order to guarantee a supply of the required data. 
Esuli and Sebastiani (2006) did a classification 
experiment for creating lexica for opinion min-
ing, for instance, and the importance of lexical 
information for event extraction in Biomedical 
texts has been addressed in Fillmore et al 
(2006). One way of producing such resources is 
to classify words into lexical classes via methods 
based on their morphosyntactic contexts of oc-
currence. 
In the next three sections we report on an ex-
periment on cue-based lexical classification for 
non-deverbal event nouns, that is, nouns such as 
?party? or ?conflict?, which refer to an event but 
cannot be identified by their morphology, as is 
the case with deverbal nouns such as ?construc-
tion?. The purpose of this experiment was, as 
already stated, to investigate methods for the 
rapid generation of an event nouns lexicon for 
two different languages, using a reduced quanti-
ty of available texts. Assuming that linguistic 
information can be provided by occurrence dis-
tribution, as is usually done in linguistic theory 
to motivate lexical classes (e.g. Grimshaw, 
1990), cue information has been gathered from 
texts and used to train and test a Decision Tree-
based classifier. We experimented with two dif-
ferent languages to test the potential coverage of 
the proposed technique in terms of its adaptation 
to different languages, and also used different 
types of corpora to test its adaptability to differ-
ent domains and sizes.  
2 Some properties of on-Deverbal 
Event ouns in Spanish and English. 
We based our experiment on the work by Resnik 
(2004) who proposes a specific lexical class for 
Spanish event nouns like accidente (?accident?) 
or guerra (?war?) which cannot be identified by 
suffixes such as ?-ci?n? (?-tion?) or ?miento? (?-
ment?), i.e. the morphological marks of deverbal 
derivation. Her proposal of creating a new class 
is motivated by the syntactic behaviour of these 
non-deverbal event nouns that differ significant-
ly both from deverbal nominalizations and from 
non event nouns. This proposal differs signifi-
cantly from work such as Grimshaw (1990).  
In Grimshaw (1990) a significant difference is 
shown to exist between process and result no-
minals, evident in certain ambiguous nouns such 
as building, which can have a process reading ?
46
in a sentence like The building of the access 
road took three weeks (= 'process of building')? 
and a non-eventive or result reading ?in a sen-
tence like The building collapsed (= 'edifice'). 
These two types of nominals differ in many lex-
ico-syntactic properties, such as the obligato-
ry/optional internal argument realization, the 
manner of external argument realization, the de-
terminer selection and their ability to control 
infinitival clauses. Simple event nouns such as 
trip share several syntactic properties with result 
nominals, although their lexical meaning is in-
deed similar to that of the process or complex 
event nouns. The main difference is the fact that 
result nominals and simple event nouns, contrary 
to complex event nominals, are not verb-like in 
the way they combine with their satellites 
(Grimshaw 1990). The similarity between result 
nominals and simple event nouns is accepted in 
Picallo's (1991, 1999) analysis of Catalan and 
Spanish nominalizations and in Alexiadou's 
(2001) work on nominalizations in Greek, Eng-
lish, Hebrew and other languages. 
Although the similarities between non-
deverbal event nouns like accidente and result 
nominals are undeniable, some evidence (Res-
nik, 2004 and 2009) has been found that non-
deverbal event nouns cannot be assimilated to 
either result nominals or simple non event nouns 
like tren (?train?), in spite of their shared proper-
ties. In the next sections, we briefly present evi-
dence that non-deverbal event nouns are a sepa-
rate lexical class and that this evidence can be 
used for identifying the members of this class 
automatically, both in Spanish and in English. 
Our hypothesis is that whenever there is a lexical 
class motivated by a particular distributional be-
haviour, a learner can be trained to identify the 
members of this class. However, there are two 
main problems to lexical classification: noise 
and silence, as we will see in section 4.  
Resnik (2004) shows that non-deverbal event 
nouns occur in a unique combination of syntac-
tic patterns: they are basically similar to result 
nouns (and simple non event nouns) regarding 
the realization of argument structure, yet they 
pattern along process nominals regarding event 
structure, given that they accept the same range 
of aspectual adjuncts and quantifiers as these 
nouns and are selected as subjects by the same 
?aspectual? verbs (empezar, ?to start?; durar, ?to 
last?, etc.) (cf. section 3.2). As to other nominal 
properties, such as the mass/count distinction, 
the contexts show that non-deverbal event nouns 
are not quite like either of the two kinds of no-
minalizations, and they behave like simple non 
event nouns. The table below summarizes the 
lexico-syntactic properties of the different nouns 
described by Grimshaw (1990) with the addition 
of Resnik?s proposed new one. 
 
 NDV E N 
(war) 
PR-N 
(construction 
=  
event) 
RES-N 
(construction 
= 
 result. obj.) 
NEN  
(map) 
Obligatory 
internal ar-
gument 
no yes no No 
External 
argument 
realization 
genitive 
DP 
PP_by genitive 
DP 
genitive 
DP 
Subject of 
aspectual 
verbs  
(begin, last..) 
yes yes no no 
Aspectual 
quantifier  
(a period of)  
yes yes no no 
Complement 
of during, ?  
yes yes no no 
Count/mass  
(determiners, 
plural forms) 
mass/count mass count mass/ 
count 
Table 1. Lexico-syntactic properties of Eng-
lish Non-Deverbal Event Nouns (NDV E N), 
Process Nouns (PR-N) and Result Nouns (RES-
N) and Non Event Nouns (NEN). 
3 Automatic Detection of on-deverbal 
Event ouns 
We have referred to the singularities of non-
deverbal event nouns as a lexical class in con-
trast with other event and non-event nouns. In 
our experiment, we have extracted the characte-
ristics of the contexts where we hypothesize that 
members of this class occur and we have used 
them as variables to train an automatic learner 
that can rely on these features to automatically 
classify words into those which are indeed non-
deverbal event nouns and those which are not. 
Because deverbal result nouns are easily identi-
fiable by the nominal suffix they bear (for in-
stance, ?-tion? for English and ?-ci?n? for Span-
ish), our experiment has been centered in sepa-
rating non-deverbal event nouns like guerra/war 
from non event nouns like tren/train.  
47
Some work related to our experiments can be 
found in the literature dealing with the identifi-
cation of new events for broadcast news and se-
mantic annotation of texts, which are two possi-
ble applications of automatic event detection 
(Allan et al 1998 and Saur? et al 2005, respec-
tively, for example). For these systems, howev-
er, it would be difficult to find non-deverbal 
event nouns because of the absence of morpho-
logical suffixes, and therefore they could benefit 
from our learner.   
3.1 Cue-based Lexical Information Acqui-
sition 
According to the linguistic tradition, words that 
can be inserted in the same contexts can be said 
to belong to the same class. Thus, lexical classes 
are linguistic generalizations drawn from the 
characteristics of the contexts where a number of 
words tend to appear. Consequently, one of the 
approaches to lexical acquisition proposes to 
classify words taking as input characteristics of 
the contexts where words of the same class oc-
cur. The idea behind this is that differences in 
the distribution of the contexts will separate 
words in different classes, e.g. the class of tran-
sitive verbs will show up in passive construc-
tions, while the intransitive verbs will not. Thus, 
the whole set of occurrences (tokens) of a word 
are taken as cues for defining its class (the class 
of the type), either because the word is observed 
in a number of particular contexts or because it 
is not. Selected references for this approach are: 
Brent, 1993; Merlo and Stevenson, 2001; Bald-
win and Bond, 2003; Baldwin, 2005; Joanis and 
Stevenson, 2003; Joanis et al 2007.  
Different supervised Machine Learning (ML) 
techniques have been applied to cue-based lexi-
cal acquisition. A learner is supplied with classi-
fied examples of words represented by numeri-
cal information about matched and not matched 
cues. The final exercise is to confirm that the 
data characterized by the linguistically moti-
vated cues support indeed the division into the 
proposed classes. This was the approach taken 
by Merlo and Stevenson (2001), who worked 
with a Decision Tree and selected linguistic cues 
to classify English verbs into three classes: un-
accusative, unergative and object-drop. Anima-
cy of the subject, for instance, is a significant 
cue for the class of object dropping verbs, in 
contrast with verbs in unergative and unaccusa-
tive classes. Baldwin and Bond (2003) used a 
number of linguistic cues (i.e. co-occurence with 
particular determiners, number, etc.) to learn the 
countability of English nouns. Bel et al (2007) 
proposed a number of cues for classifying nouns 
into different types according to a lexical typol-
ogy. The need for using more general cues has 
also been pointed out, such as the part of speech 
tags of neighboring words (Baldwin, 2005), or 
general linguistic information as in Joanis et al 
(2007), who used the frequency of filled syntac-
tic positions or slots, tense and voice features, 
etc., to describe the whole system of English 
verbal classes. 
3.2 Cues for the Detection of on-deverbal 
Event ouns in Spanish 
As we have seen in section 2, non-deverbal 
event nouns can be identified by their occur-
rence in particular syntactic and lexical contexts 
of co-occurrence.We have used 11 cues for sepa-
rating non-deverbal event nouns from non event 
nouns in Spanish. These cues are the following: 
Cues 1-3. Nouns occurring in PPs headed by 
prepositions such as durante (?during?), hasta el 
final de (?until the end of?), desde el principio de 
(?from the beginning of?), and similar expres-
sions are considered to be eventive. Thus, occur-
rence after one of such expressions will be in-
dicative of an event noun.   
Cues 4-8. Nouns occurring as external or in-
ternal arguments of verbs such as ocurrir (?oc-
cur?), producir (?produce? or ?occur?, in the case 
of ergative variant producirse), celebrar (?cele-
brate?), and others with similar meanings, are 
also events. Note that we identify as ?external 
arguments? the  nouns occurring immediately 
after the verb in particular constructions, as our 
pos- tagged text does not contain information 
about subjects (see below). In many cases it is 
the internal argument occurring in these con-
texts. These verbs tend to appear in ?presenta-
tive? constructions such as Se produjo un acci-
dente (?An accident occurred?), with the pronoun 
se signalling the lack of external argument. 
Verbs like ocurrir appear in participial absolute 
constructions or with participial adjectives, 
which means they are unaccusatives. 
Cue 9. The presence of temporal quantifying 
expressions such as dos semanas de (?two weeks 
48
of?) or similar would indicate the eventive cha-
racter of a noun occurring with it, as mentioned 
in section 2.  
Cue 10. Non-deverbal event nouns will not be 
in Prepositional Phrases headed by locative pre-
positions such as encima de (?on top of?) or de-
bajo de (?under?). These cues are used as nega-
tive evidence for non-event deverbal nouns. 
Cue 11. Non-deverbal event nouns do have an 
external argument that can also be realized as an 
adjective. The alternation of DP arguments with 
adjectives was then a good cue for detecting 
non-deverbal events, even when some other 
nouns may appear in this context as well. For 
instance: fiesta nacional (?national party?) vs. 
mapa nacional  (?national map?). 
3.3 Cues for the Detection of on-Deverbal 
Event ouns in English 
As for Spanish, cues for English were meant to 
separate the newly proposed class of non-
deverbal event nouns from non-event nouns if 
such a class exists as well. 
Cues 1-3. Process nominals and non-deverbal 
event nouns can be identified by appearing as 
complements of aspectual PPs headed by prepo-
sitions like during, after and before, and com-
plex prepositions such as at the end of and at the 
beginning of. 
Cues 4 and 5. Non-deverbal nouns may occur 
as external or internal arguments of aspectual as 
well as occurrence verbs such as initiate, take 
place, happen, begin, and occur. Those argu-
ments are identified either as subjects of active 
or passive sentences, depending on the verb, i.e. 
the therapy was initiated and the conflict took 
place. 
Cue 6. Likewise, nouns occurring in expres-
sions such as frequency of, occurrence of and 
period of would probably be event nouns, i.e. the 
frequency of droughts. 
Cue 7 and 8. Event nouns may as well appear 
as objects of aspectual and time-related verbs, 
such as in have begun a campaign or have car-
ried out a campaign. 
Cues 10 and 11. They are intended to register 
event nouns whose external argument, although 
optional, is realized as a genitive complement, 
e.g. enzyme?s loss, even though this cue is 
shared with other types of nouns. Following the 
characterization suggested for Spanish, we also 
tried external arguments realized as adjectives in 
cue 11, as in !apoleonic war, but we found em-
pirical evidence that it is not useful.  
Cues 12-16. Finally, as in the experiment for 
Spanish, we have also included evidence that is 
more common for non-event nouns, that is, we 
have used negative evidence to tackle the prob-
lem of sparse data or silence discussed in the 
next section. It is considered a negative cue for a 
noun to be preceded by an indefinite determiner, 
to be in a PP headed by a locative preposition, 
and to be followed by the prepositions by or of, 
as a PP headed by one these prepositions could 
be an external argument and, as it has been noted 
above, the external argument of event nouns 
tends to be realized as a genitive complement (as 
in John?s trip/party).  
In the selection of these cues, we have con-
centrated on those that separate the class of non-
deverbal event nouns from the class formed by 
simple non event nouns like train, where no par-
ticular deverbal suffix can assist their detection. 
If it is the case that these are really cues for de-
tecting non-deverbal event nouns, the learner 
should confirm it by classifying non-deverbal 
event nouns correctly, separating them from oth-
er types of nouns. 
4 Experiment and results 
For our experiments we have used Regular Ex-
pressions to implement the patterns just men-
tioned, which look for the intended cues in a 
part-of-speech tagged corpus. We have used a 
corpus of 21M tokens from two Spanish news-
papers (El Pa?s and La Vanguardia), and an 
English technical corpus made of texts dealing 
with varying subject matter (Economy, Medi-
cine, Computer science and Environmental is-
sues), of about 3.2M tokens. Both Spanish and 
English corpora are part of the Technical Corpus 
of IULA at the UPF (CT-IULA, Cabr? et al 
2006).  The positive or negative results of the n-
pattern checking in all the occurrences of a word 
are stored in an n-dimension vector. Thus, a sin-
gle vector summarizes all the occurrences of a 
word (the type) by encoding how many times 
each cue has been observed. Zero values, i.e. no 
matching, are also registered.  
We used a Decision Tree (DT) classifier in 
the Weka (Witten and Frank, 2005) implementa-
tion of pruned C4.5 decision tree (Quinlan, 
49
1993). The DT performs a general to specific 
search in a feature space, selecting the most in-
formative attributes for a tree structure as the 
search proceeds. The goal is to select the minim-
al set of attributes that efficiently partitions the 
feature space into classes of observations and 
assemble them into a tree. During the experi-
ment, we tuned the list of cues actually used in 
the classification task, because some of them 
turned out to be useless, as they did not show up 
even once in the corpus. This was especially true 
for the English corpus with cues 5, 11 and 12. 
Note that the English corpus is only 3.2 million 
words.  
In the experiment we used a 10-fold cross-
validation testing using manually annotated 
gold-standard files made of 99 non-event and 
100 non-deverbal event nouns  for Spanish and 
93 non event and 74 non-deverbal event nouns 
for English1. In this first experiment, we decided 
to use mostly non-deverbal non event nouns 
such as map, because detecting result nouns like 
construction is easy enough, due to the deverbal 
suffix. However, for the English experiment, and 
because of the scarcity of non-deverbal nouns 
occurrences, we had to randomly select some 
deverbals that were not recognized by the suffix.  
The results of our experiment gave a total ac-
curacy of 80% for Spanish and 79.6% for Eng-
lish, which leads to think that corpus size is not a 
                                                 
1Positive: accident, assembly, audience, battle, boycott, 
campaign, catastrophe, ceremony, cold, collapse, confe-
rence, conflict, course, crime, crisis, cycle, cyclone, change, 
choice, decline, disease, disaster, drought, earthquake, epi-
demic, event, excursion, fair, famine, feast, festival, fever, 
fight, fire, flight, flood, growth, holiday, hurricane, impact, 
incident, increase, injury, interview, journey, lecture, loss, 
meal, measurement, meiosis, marriage, mitosis, monsoon, 
period, process, program, quake, response, seminar, snows-
torm, speech, storm, strike, struggle, summit, symposium, 
therapy, tour, treaty, trial, trip, vacation, war. egative: 
agency, airport, animal, architecture, bag, battery, bird, 
bridge, bus, canal, circle, city, climate, community, compa-
ny, computer, constitution, country, creature, customer, 
chain, chair, channel, characteristic, child, defence, direc-
tor, drug, economy, ecosystem, energy, face, family, firm, 
folder, food, grade, grant, group, health, hope, hospital, 
house, illusion, information, intelligence, internet, island, 
malaria, mammal, map, market, mountain, nation, nature, 
ocean, office, organism, pencil, people, perspective, phone, 
pipe, plan, plant, profile, profit, reserve, river, role, satellite, 
school, sea, shape, source, space, star, statistics, store, tech-
nology, television, temperature, theme, theory, tree, medi-
cine, tube, university, visa, visitor, water, weather, window, 
world. 
determinant factor and that this method can be 
used for addressing different languages, pro-
vided a good characterization of the lexical class 
in terms of particular occurrence distributions is 
achieved. Yet, although the accuracy of both 
English and Spanish test sets is similar, we will 
see later on that the size of the corpus does in-
deed affect the results. 
An analysis of the errors shows that they can 
be classified in two groups: errors due to noise, 
and errors due to silence. 
 (i) Noise. In his seminal work, Brent (1993) 
already pointed out that ?the cues occur in con-
texts that were not aimed at?. Noise can be due 
to errors in processing the text, because we had 
only used low-level analysis tools. For instance, 
in ?during the first world war? our RE cannot 
detect that ?world? is not the head of the Noun 
Phrase. Brent?s hypothesis, followed by most 
authors afterwards, is that noise can be eliminat-
ed by statistical methods because of its low fre-
quency. However, the fact is that in our test set 
significant information is as sparse as noise, and 
the DT cannot correctly handle this. In our data 
sets, most of the false positives are due to noise. 
  (ii) Silence. Some nouns appear only once or 
twice in the corpus and do not show up in any of 
the sought contexts (for instance, terremoto, 
?earthquake?, in Spanish press). Moreover, this 
is independent of the size of the corpus, because 
the Zipfian distribution of tokens allows us to 
predict that there will always be low-frequency 
nouns. Low frequency words produce non in-
formative vectors, with only zero-valued cues, 
and our classifier tends to classify non-
informative vectors as non-event nouns, because 
most of the cues have been issued to identify 
event nouns. This was the main reason to intro-
duce negative contexts as well as positive ones, 
as we mentioned in section 3.  
However, these systematic sources of error 
can be taken as an advantage when assessing the 
usability of the resulting resources. Having 
about 80% of accuracy would not be enough to 
ensure the proper functioning of the application 
in which the resource is going to be used. So, in 
order to gain precision, we decided to separate 
the set of words that could be safely taken as 
correctly classified. Thus, we had used the con-
fidence, i.e. probability of the classification de-
50
cisions to assess which are below a reasonable 
level of confidence. 
In the Spanish test set, for instance, precision 
of the positive classification, i.e. the percentage 
of words correctly classified as event nouns, 
raises from 0.82 to 0.95 when only instances of 
classification with a confidence of more than 0.8 
are selected. In the figure below, we can see the 
precision curve for the Spanish test set.  
 
 
Figure 1: Precision curve 
for the Spanish test set. 
 
In general, precision is higher when confi-
dence is higher, except for complete confidence, 
1, as we will explain later with the English case. 
This general behavior could be interpreted as a 
guarantee that there is a significant number of 
classified nouns (87 out of 199 for the Spanish 
test set with a threshold of 0.8 confidence) that 
need not to be manually reviewed, i.e. a 43% of 
the automatically acquired lexica can safely be 
considered correct. From figure 1, we can also 
see that the classifier is consistently identifying 
the class of non-deverbal event nouns even with 
a lower threshold. However, the resulting non-
event noun set contains a significant number of 
errors. From the point of view of the usability, 
we could also say that only those words that are 
classified as non-event nouns must be revised.  
Figure 2 for English test set shows a different 
behavior, which can only be justified because of 
the difference in corpus size. A small corpus 
increases the significance of silence errors. Few-
er examples give less information to the classifi-
er, which still makes the right decisions but with 
less confidence in general. However, for the ex-
treme cases, for instance the case of 7 word vec-
tors with only zero-values, the confidence is 
very high, that is 1, but the decisions are wrong. 
These cases of mostly zero values are wrongly 
considered to be non-events. This is the reason 
for the low precision of very confident decisions 
in English, i.e. sparse data and its consequence, 
silence.  
 
 
Figure 2: Precision curve  
for the English test set. 
5 Conclusions 
In this paper we have proposed the use of lexical 
classification methods based on differences in 
the distributional behavior of word classes for 
the quick production of lexica containing the 
information required by particular applications. 
We have dealt with non-deverbal event nouns, 
which cannot be easily recognized by any suf-
fixes, and we have carried out a classification 
experiment, which consisted in training a DT 
with the information used in the linguistic litera-
ture to justify the existence of this class. The 
results of the classifier, close to 80% accuracy in 
two different languages and with different size 
and types of source corpora, show the validity of 
this very simple approach, which can be decisive 
in the production of lexica with the knowledge 
required by different technologies and applica-
tions in a time-efficient way. From the point of 
view of usability, this approach can be said to 
reduce the amount of work in more than a 40%.  
Acknowledgements 
We want to thank Muntsa Padr? for her valu-
able contribution. This work was partially sup-
ported by the PANACEA project (EU-7FP-ITC-
248064). 
 
51
References 
Alexiadou, A. (2001). Functional Structure in No-
minals: Nominalization and Ergativity. Amster-
dam/Philadelphia: John Benjamins Publishing 
Company. 
Allan, J.; Papka, R.; Lavrenko, V. (1998). On-line 
New Event Detection and Tracking. SIGIR98, 
Melbourne, Australia.  
Baldwin, T. and F. Bond. 2003. ?Learning the Coun-
tability of English Nouns from Corpus Data?. Pro-
ceedings of the 41st. Annual Meeting of the Associ-
ation for Computational Linguistics, Sapporo, Ja-
pan. 
Baldwin, T. 2005. General-Purpose Lexical Acquisi-
tion: Procedures, Questions and Results, In Pro-
ceedings of the Pacific Association for Computa-
tional Linguistics 2005, Tokyo, Japan 
 Bel, N.; Espeja, S.; Marimon, M. 2007. Automatic 
Acquisition of Grammatical Types for Nouns. In H 
LT 2007: The Conference of the !AACL. Compa-
nion Volume, Short Papers. Rochester, USA. 
Brent, M. R. 1993, ?From grammar to lexicon: unsu-
pervised learning of lexical syntax?. Computatio-
nal Linguistics 19: 243-262. 
Cabr?, M. T.; Bach, C.; Vivaldi, J. 2006. 10 anys del 
Corpus de l'IULA. Barcelona: Institut Universitari 
de Ling??stica Aplicada. Universitat Pompeu Fabra 
Esuli, A. and Sebastiani, F.. 2006. Determining term 
subjectivity and term orientation for opinion min-
ing. In Proceedings of EACL-06, 11th Conference 
of the European Chapter of the Association for 
Computational Linguistics, Trento, IT. 
Fillmore, Charles J.Srini Narayanan, and Collin F. 
Baker. 2006. What Can Linguistics Contribute to 
Event Extraction? Proceedings of the 2006 AAAI 
Workshop on Event Extraction and Synthesis, page 
18--23. 
Grimshaw, J. (1990). Argument Structure. Cam-
bridge: The MIT Press. 
Joanis, E; Stevenson, S; and James, D. 2007. A Gen-
eral Feature Space for Automatic Verb Classifica-
tion. !atural Language Engineering, 14. 
Korhonen, A. 2002. ?Subcategorization acquisition?. 
As Technical Report UCAM-CL-TR-530, Univer-
sity of Cambridge, UK. 
Merlo P. and Stevenson S. 2001. Automatic Verb 
Classification based on Statistical Distribution of 
Argument Structure, Computational Linguis-
tics, 27:3. 
Picallo, M. C. (1999). ?La estructura del sintagma 
nominal: Las nominalizaciones y otros sustantivos 
con complementos argumentales?, en Bosque, I. & 
V. Demonte (eds.) Gram?tica descriptiva de la 
lengua espa?ola. Madrid: Real Academia Espa?o-
la / Espasa Calpe. Vol. 1, Cap. 6, 363-394. 
Quinlan, R.J. 1993. C4.5: Programs for Machine 
Learning. Series in Machine Learning. Morgan 
Kaufman, San Mateo, CA. 
Resnik, G. (2004). Los nombres eventivos no dever-
bales en espa?ol. Proyecto de Tesis. Institut Uni-
versitari de Ling??stica Aplicada, Universitat 
Pompeu Fabra. 
Resnik, G. (2009) ?La determinaci?n de la eventivi-
dad nominal en espa?ol?. En G. Ciapuscio (ed.) 
De la palabra al texto: estudios ling??sticos del 
espa?ol. Buenos Aires: Eudeba. 
Saur?, R.; R. Knippen, M. Verhagen and J. Puste-
jovsky. 2005. Evita: A Robust Event Recognizer 
for QA Systems. Proceedings of HLT/EM!LP 
2005: 700-70 
Witten, I. H. and Frank E. 2005. Data Mining: Prac-
tical machine learning tools and techniques. 2nd 
Edition, Morgan Kaufmann, San Francisco. 
 
52
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?5,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Language Resources Factory: case study on the acquisition of
Translation Memories?
Marc Poch
UPF Barcelona, Spain
marc.pochriera@upf.edu
Antonio Toral
DCU Dublin, Ireland
atoral@computing.dcu.ie
Nu?ria Bel
UPF Barcelona, Spain
nuria.bel@upf.edu
Abstract
This paper demonstrates a novel distributed
architecture to facilitate the acquisition of
Language Resources. We build a factory
that automates the stages involved in the ac-
quisition, production, updating and mainte-
nance of these resources. The factory is de-
signed as a platform where functionalities
are deployed as web services, which can
be combined in complex acquisition chains
using workflows. We show a case study,
which acquires a Translation Memory for a
given pair of languages and a domain using
web services for crawling, sentence align-
ment and conversion to TMX.
1 Introduction
A fundamental issue for many tasks in the field of
Computational Linguistics and Language Tech-
nologies in general is the lack of Language Re-
sources (LRs) to tackle them successfully, espe-
cially for some languages and domains. It is the
so-called LRs bottleneck.
Our objective is to build a factory of LRs that
automates the stages involved in the acquisition,
production, updating and maintenance of LRs
required by Machine Translation (MT), and by
other applications based on Language Technolo-
gies. This automation will significantly cut down
the required cost, time and human effort. These
reductions are the only way to guarantee the con-
tinuous supply of LRs that Language Technolo-
gies demand in a multilingual world.
? We would like to thank the developers of Soaplab, Tav-
erna, myExperiment and Biocatalogue for solving our ques-
tions and attending our requests. This research has been
partially funded by the EU project PANACEA (7FP-ICT-
248064).
2 Web Services and Workflows
The factory is designed as a platform of web ser-
vices (WSs) where the users can create and use
these services directly or combine them in more
complex chains. These chains are called work-
flows and can represent different combinations of
tasks, e.g. ?extract the text from a PDF docu-
ment and obtain the Part of Speech (PoS) tagging?
or ?crawl this bilingual website and align its sen-
tence pairs?. Each task is carried out using NLP
tools deployed as WSs in the factory.
Web Service Providers (WSPs) are institutions
(universities, companies, etc.) who are willing
to offer services for some tasks. WSs are ser-
vices made available from a web server to re-
mote users or to other connected programs. WSs
are built upon protocols, server and program-
ming languages. Their massive adoption has con-
tributed to make this technology rather interoper-
able and open. In fact, WSs allow computer pro-
grams distributed in different locations to interact
with each other.
WSs introduce a completely new paradigm in
the way we use software tools. Before, every
researcher or laboratory had to install and main-
tain all the different tools that they needed for
their work, which has a considerable cost in both
human and computing resources. In addition, it
makes it more difficult to carry out experiments
that involve other tools because the researcher
might hesitate to spend time resources on in-
stalling new tools when there are other alterna-
tives already installed.
The paradigm changes considerably with WSs,
as in this case only the WSP needs to have a deep
knowledge of the installation and maintenance of
the tool, thus allowing all the other users to benefit
1
from this work. Consequently, researchers think
about tools from a high level and solely regard-
ing their functionalities, thus they can focus on
their work and be more productive as the time re-
sources that would have been spent to install soft-
ware are freed. The only tool that the users need
to install in order to design and run experiments is
a WS client or a Workflow editor.
3 Choosing the tools for the platform
During the design phase several technologies
were analyzed to study their features, ease of use,
installation, maintenance needs as well as the es-
timated learning curve required to use them. In-
teroperability between components and with other
technologies was also taken into account since
one of our goals is to reach as many providers and
users as possible. After some deliberation, a set of
technologies that have proved to be successful in
the Bioinformatics field were adopted to build the
platform. These tools are developed by the my-
Grid1 team. This group aims to develop a suite
of tools for researchers that work with e-Science.
These tools have been used in numerous projects
as well as in different research fields as diverse as
astronomy, biology and social science.
3.1 Web Services: Soaplab
Soaplab (Senger et al 2003)2 allows a WSP to
deploy a command line tool as a WS just by writ-
ing a metadata file that describes the parameters
of the tool. Soaplab takes care of the typical is-
sues regarding WSs automatically, including tem-
porary files, protocols, the WSDL file and its pa-
rameters, etc. Moreover, it creates a Web interface
(called Spinet) where WSs can be tested and used
with input forms. All these features make Soaplab
a suitable tool for our project. Moreover, its nu-
merous successful stories make it a safe choise;
e.g., it has been used by the European Bioinfor-
matics Institute3 to deploy their tools as WSs.
3.2 Registry: Biocatalogue
Once the WSs are deployed by WSPs, some
means to find them becomes necessary. Biocat-
alogue (Belhajjame et al 2008)4 is a registry
1http://www.mygrid.org.uk
2http://soaplab.sourceforge.net/
soaplab2/
3http://www.ebi.ac.uk
4http://www.biocatalogue.org/
where WSs can be shared, searched for, annotated
with tags, etc. It is used as the main registration
point for WSPs to share and annotate their WSs
and for users to find the tools they need. Bio-
catalogue is a user-friendly portal that monitors
the status of the WSs deployed and offers multi-
ple metadata fields to annotate WSs.
3.3 Workflows: Taverna
Now that users can find WSs and use them, the
next step is to combine them to create complex
chains. Taverna (Missier et al 2010)5 is an open
source application that allows the user to create
high-level workflows that integrate different re-
sources (mainly WSs in our case) into a single
experiment. Such experiments can be seen as
simulations which can be reproduced, tuned and
shared with other researchers.
An advantage of using workflows is that the
researcher does not need to have background
knowledge of the technical aspects involved in
the experiment. The researcher creates the work-
flow based on functionalities (each WS provides a
function) instead of dealing with technical aspects
of the software that provides the functionality.
3.4 Sharing workflows: myExperiment
MyExperiment (De Roure et al 2008)6 is a so-
cial network used by workflow designers to share
workflows. Users can create groups and share
their workflows within the group or make them
publically available. Workflows can be annotated
with several types of information such as descrip-
tion, attribution, license, etc. Users can easily find
examples that will help them during the design
phase, being able to reuse workflows (or parts of
them) and thus avoiding reinveinting the wheel.
4 Using the tools to work with NLP
All the aforementioned tools were installed, used
and adapted to work with NLP. In addition, sev-
eral tutorials and videos have been prepared7 to
help partners and other users to deploy and use
WSs and to create workflows.
Soaplab has been modified (a patch has been
developed and distributed)8 to limit the amount of
data being transfered inside the SOAP message in
5http://www.taverna.org.uk/
6http://www.myexperiment.org/
7http://panacea-lr.eu/en/tutorials/
8http://myexperiment.elda.org/files/5
2
order to optimize the network usage. Guidelines
that describe how to limit the amount of concur-
rent users of WSs as well as to limit the maximum
size of the input data have been prepared.9
Regarding Taverna, guidelines and workflow
examples have been shared among partners show-
ing the best way to create workflows for the
project. The examples show how to benefit from
useful features provided by this tool, such as
?retries? (to execute up to a certain number of
times a WS when it fails) and ?parallelisation? (to
run WSs in parallel, thus increasing trhoughput).
Users can view intermediate results and parame-
ters using the provenance capture option, a useful
feature while designing a workflow. In case of any
WS error in one of the inputs, Taverna will report
the error message produced by the WS or proces-
sor component that causes it. However, Taverna
will be able to continue processing the rest of the
input data if the workflow is robust (i.e. makes
use of retry and parallelisation) and the error is
confined to a WS (i.e. it does not affect the rest of
the workflow).
An instance of Biocatalogue and one of my-
Experiment have been deployed to be the Reg-
istry and the portal to share workflows and other
experiment-related data. Both have been adapted
by modifying relevant aspects of the interface
(layout, colours, names, logos, etc.). The cate-
gories that make up the classification system used
in the Registry have been adapted to the NLP
field. At the time of writing there are more than
100 WSs and 30 workflows registered.
5 Interoperability
Interoperability plays a crucial role in a platform
of distributed WSs. Soaplab deploys SOAP10
WSs and handles automatically most of the issues
involved in this process, while Taverna can com-
bine SOAP and REST11 WSs. Hence, we can say
that communication protocols are being handled
by the tools. However, parameters and data inter-
operability need to be addressed.
5.1 Common Interface
To facilitate interoperability between WSs and to
easily exchange WSs, a Common Interface (CI)
9http://myexperiment.elda.org/files/4
10http://www.w3.org/TR/soap/
11http://www.ics.uci.edu/?fielding/
pubs/dissertation/rest_arch_style.htm
has been designed for each type of tool (e.g. PoS-
taggers, aligners, etc.). The CI establishes that all
WSs that perform a given task must have the same
mandatory parameters. That said, each tool can
have different optional parameters. This system
eases the design of workflows as well as the ex-
change of tools that perform the same task inside
a workflow. The CI has been developed using an
XML schema.12
5.2 Travelling Object
A goal of the project is to facilitate the deploy-
ment of as many tools as possible in the form of
WSs. In many cases, tools performing the same
task use in-house formats. We have designed a
container, called ?Travelling Object? (TO), as the
data object that is being transfered between WSs.
Any tool that is deployed needs to be adapted to
the TO, this way we can interconnect the different
tools in the platform regardless of their original
input/output formats.
We have adopted for TO the XML Corpus En-
coding Standard (XCES) format (Ide et al 2000)
because it was the already existing format that re-
quired the minimum transduction effort from the
in-house formats. The XCES format has been
used successfully to build workflows for PoS tag-
ging and alignment.
Some WSs, e.g. dependency parsers, require a
more complex representation that cannot be han-
dled by the TO. Therefore, a more expressive for-
mat has been adopted for these. The Graph Anno-
tation Format (GrAF) (Ide and Suderman, 2007)
is a XML representation of a graph that allows
different levels of annotation using a ?feature?
value? paradigm. This system allows different
in-house formats to be easily encapsulated in this
container-based format. On the other hand, GrAF
can be used as a pivot format between other for-
mats (Ide and Bunt, 2010), e.g. there is software
to convert GrAF to UIMA and GATE formats (Ide
and Suderman, 2009) and it can be used to merge
data represented in a graph.
Both TO and GrAF address syntactic interop-
erability while semantic interoperability is still an
open topic.
12http://panacea-lr.eu/en/
info-for-professionals/documents/
3
6 Evaluation
The evaluation of the factory is based on its
features and usability requirements. A binary
scheme (yes/no) is used to check whether each re-
quirement is fulfilled or not. The quality of the
tools is not altered as they are deployed as WSs
without any modification. According to the eval-
uation of the current version of the platform, most
requirements are fulfilled (Aleksic? et al 2012).
Another aspect of the factory that is being eval-
uated is its performance and scalabilty. They do
not depend on the factory itself but on the design
of the workflows and WSs. WSPs with robust
WSs and powerful servers will provide a better
and faster service to users (considering that the
service is based on the same tool). This is analo-
gous to the user installing tools on a computer; if
the user develops a fragile script to chain the tools
the execution may fail, while if the computer does
not provide the required computational resources
the performance will be poor.
Following the example of the Bioinformatics
field where users can benefit of powerful WSPs,
the factory is used as a proof of concept that these
technologies can grow and scale to benefit many
users.
7 Case study
We introduce a case study in order to demonstrate
the capabilities of the platform. It regards the ac-
quisition of a Translation Memory (TM) for a lan-
guage pair and a specific domain. This is deemed
to be very useful for translators when they start
translating documents for a new domain. As at
that early stage they still do not have any content
in their TM, having the automatically acquired
TM can be helpful in order to get familiar with
the characteristic bilingual terminology and other
aspects of the domain. Another obvious potential
use of this data would be to use it to train a Statis-
tical MT system.
Three functionalities are needed to carry out
this process: acquisition of the data, its alignment
and its conversion into the desired format. These
are provided by WSs available in the registry.
First, we use a domain-focused bilingual
crawler13 in order to acquire the data. Given a pair
of languages, a set of web domains and a set of
seed terms that define the target domain for these
13http://registry.elda.org/services/127
languages, this tool will crawl the webpages in
the domains and gather pairs of web documents
in the target languages that belong to the target
domain. Second, we apply a sentence aligner.14
It takes as input the pairs of documents obtained
by the crawler and outputs pairs of equivalent sen-
tences.Finally, convert the aligned data into a TM
format. We have picked TMX15 as it is the most
common format for TMs. The export is done by
a service that receives as input sentence-aligned
text and converts it to TMX.16
The ?Bilingual Process, Sentence Alignment of
bilingual crawled data with Hunalign and export
into TMX?17 is a workflow built using Taverna
that combines the three WSs in order to provide
the functionality needed. The crawling part is
ommitted because data only needs to be crawled
once; crawled data can be processed with differ-
ent workflows but it would be very inefficient to
crawl the same data each time. A set of screen-
shots showing the WSs and the workflow, together
with sample input and output data is available.18
8 Demo and Requirements
The demo aims to show the web portals and tools
used during the development of the case study.
First, the Registry19 to find WSs, the Spinet Web
client to easily test them and Taverna to finally
build a workflow combining the different WSs.
For the live demo, the workflows will be already
designed because of the time constraints. How-
ever, there are videos on the web that illustrate
the whole process. It will be also interesting to
show the myExperiment portal,20 where all pub-
lic workflows can be found. Videos of workflow
executions will also be available.
Regarding the requirements, a decent internet
connection is critical for an acceptable perfor-
mance of the whole platform, specially for remote
WSs and workflows. We will use a laptop with
Taverna installed to run the workflow presented
in Section 7.
14http://registry.elda.org/services/92
15http://www.gala-global.org/
oscarStandards/tmx/tmx14b.html
16http://registry.elda.org/services/219
17http://myexperiment.elda.org/
workflows/37
18http://www.computing.dcu.ie/?atoral/
panacea/eacl12_demo/
19http://registry.elda.org
20http://myexperiment.elda.org
4
References
Vera Aleksic?, Olivier Hamon, Vassilis Papavassiliou,
Pavel Pecina, Marc Poch, Prokopis Prokopidis, Va-
leria Quochi, Christoph Schwarz, and Gregor Thur-
mair. 2012. Second evaluation report. Evalu-
ation of PANACEA v2 and produced resources
(PANACEA project Deliverable 7.3). Technical re-
port.
Khalid Belhajjame, Carole Goble, Franck Tanoh, Jiten
Bhagat, Katherine Wolstencroft, Robert Stevens,
Eric Nzuobontane, Hamish McWilliam, Thomas
Laurent, and Rodrigo Lopez. 2008. Biocatalogue:
A curated web service registry for the life science
community. In Microsoft eScience conference.
David De Roure, Carole Goble, and Robert Stevens.
2008. The design and realisation of the myexperi-
ment virtual research environment for social sharing
of workflows. Future Generation Computer Sys-
tems, 25:561?567, May.
Nancy Ide and Harry Bunt. 2010. Anatomy of anno-
tation schemes: mapping to graf. In Proceedings of
the Fourth Linguistic Annotation Workshop, LAW
IV ?10, pages 247?255, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Nancy Ide and Keith Suderman. 2007. GrAF: A
Graph-based Format for Linguistic Annotations. In
Proceedings of the Linguistic Annotation Workshop,
pages 1?8, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Nancy Ide and Keith Suderman. 2009. Bridging
the Gaps: Interoperability for GrAF, GATE, and
UIMA. In Proceedings of the Third Linguistic An-
notation Workshop, pages 27?34, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based encoding standard
for linguistic corpora. In Proceedings of the Second
International Language Resources and Evaluation
Conference. Paris: European Language Resources
Association.
Paolo Missier, Stian Soiland-Reyes, Stuart Owen,
Wei Tan, Aleksandra Nenadic, Ian Dunlop, Alan
Williams, Thomas Oinn, and Carole Goble. 2010.
Taverna, reloaded. In M. Gertz, T. Hey, and B. Lu-
daescher, editors, SSDBM 2010, Heidelberg, Ger-
many, June.
Martin Senger, Peter Rice, and Thomas Oinn. 2003.
Soaplab - a unified sesame door to analysis tools.
In All Hands Meeting, September.
5
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 725?730,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Annotation of regular polysemy and underspecification
He?ctor Mart??nez Alonso,
Bolette Sandford Pedersen
University of Copenhagen
Copenhagen (Denmark)
alonso@hum.ku.dk, bsp@hum.ku.dk
Nu?ria Bel
Universitat Pompeu Fabra
Barcelona (Spain)
nuria.bel@upf.edu
Abstract
We present the result of an annotation task
on regular polysemy for a series of seman-
tic classes or dot types in English, Dan-
ish and Spanish. This article describes
the annotation process, the results in terms
of inter-encoder agreement, and the sense
distributions obtained with two methods:
majority voting with a theory-compliant
backoff strategy, and MACE, an unsuper-
vised system to choose the most likely
sense from all the annotations.
1 Introduction
This article shows the annotation task of a corpus
in English, Danish and Spanish for regular poly-
semy. Regular polysemy (Apresjan, 1974; Puste-
jovsky, 1995; Briscoe et al, 1995; Nunberg, 1995)
has received a lot of attention in computational
linguistics (Boleda et al, 2012; Rumshisky et al,
2007; Shutova, 2009). The lack of available sense-
annotated gold standards with underspecification
is a limitation for NLP applications that rely on
dot types1 (Rumshisky et al, 2007; Poibeau, 2006;
Pustejovsky et al, 2009).
Our goal is to obtain human-annotated corpus
data to study regular polysemy and to detect it in
an automatic manner. We have collected a cor-
pus of annotated examples in English, Danish and
Spanish to study the alternation between senses
and the cases of underspecification, including a
contrastive study between languages. Here we de-
scribe the annotation process, its results in terms
of inter-encoder agreement, and the sense distri-
butions obtained with two methods: majority vot-
ing with a theory-compliant backoff strategy and,
MACE an unsupervised system to choose the most
likely sense from all the annotations.
1The corpus is freely available at
http://metashare.cst.dk/repository/search/?q=regular+polysemy
2 Regular polysemy
Very often a word that belongs to a semantic type,
like Location, can behave as a member of another
semantic type, like Organization, as shown by the
following examples from the American National
Corpus (Ide and Macleod, 2001) (ANC):
a) Manuel died in exile in 1932 in England.
b) England was being kept busy with other con-
cerns
c) England was, after all, an important wine
market
In case a), England refers to the English terri-
tory (Location), whereas in b) it refers arguably to
England as a political entity (Organization). The
third case refers to both. The ability of certain
words to switch between semantic types in a pre-
dictable manner is referred to as regular polysemy.
Unlike other forms of meaning variation caused
by metaphor or homonymy, regular polysemy is
considered to be caused by metonymy (Apresjan,
1974; Lapata and Lascarides, 2003). Regular pol-
ysemy is different from other forms of polysemy
in that both senses can be active at the same in a
predicate, which we refer to as underspecification.
Underspecified instances can be broken down in:
1. Contextually complex: England was, after
all, an important wine market
2. Zeugmatic, in which two mutually exclusive
readings are coordinated: England is conser-
vative and rainy
3. Vague, in which no contextual element en-
forces a reading: The case of England is sim-
ilar
3 Choice of semantic classes
The Generative Lexicon (GL) (Pustejovsky, 1995)
groups nouns with their most frequent metonymic
sense in a semantic class called a dot type. For
English, we annotate 5 dot types from the GL:
1. Animal/Meat: ?The chicken ran away? vs.
725
?the chicken was delicious?.
2. Artifact/Information : ?The book fell? vs.
?the book was boring?.
3. Container/Content: ?The box was red? vs.
?I hate the whole box?.
4. Location/Organization: ?England is far?
vs. ?England starts a tax reform?.
5. Process/Result: ?The building took months
to finish? vs. ?the building is sturdy?.
For Danish and Spanish, we have chosen Con-
tainer/Content and Location/Organization. We
chose the first one because we consider it the
most prototypical case of metonymy from the ones
listed in the GL. We chose the second one because
the metonymies in locations are a common con-
cern for Named-Entity Recognition (Johannessen
et al, 2005) and a previous area of research in
metonymy resolution (Markert and Nissim, 2009).
4 Annotation Scheme
For each of the nine (five for English, two for Dan-
ish, two for Spanish) dot types, we have randomly
selected 500 corpus examples. Each example con-
sists of a sentence with a selected headword be-
longing to the corresponding dot type. In spite of
a part of the annotation being made with a con-
trastive study in mind, no parallel text was used
to avoid using translated text. For English and
Danish we used freely available reference corpora
(Ide and Macleod, 2001; Andersen et al, 2002)
and, for Spanish, a corpus built from newswire and
technical text (Vivaldi, 2009).
For most of the English examples we used the
words in Rumshisky (2007), except for Loca-
tion/Organization. For Danish and Spanish we
translated the words from English. We expanded
the lists using each language?s wordnet (Pedersen
et al, 2009; Gonzalez-Agirre et al, 2012) as the-
saurus to make the total of occurrences reach 500
after we had removed homonyms and other forms
of semantic variation outside of the purview of
regular polysemy.
For Location/Organization we have used high-
frequency names of geopolitical locations from
each of the corpora. Many of them are corpus-
specific (e.g. Madrid is more frequent in the
Spanish corpus) but a set of words is shared:
Afghanistan, Africa, America, China, England,
Europe,Germany, London.
Every dot type has its particularities that we had
to deal with. For instance, English has lexical al-
ternatives for the meat of several common animals,
like venison or pork instead of deer and pig. This
lexical phenomenon does not impede metonymy
for the animal names, it just makes it less likely.
In order to assess this, we have included 20 ex-
amples of cow. The rest of the dataset consists of
animal names that do not participate in this lexical
alternation, like eel, duck, chicken, or sardine.
We call the first sense in the pair of metonyms
that make up the dot type the literal sense, and the
second sense the metonymic sense, e.g. Location
is the literal sense in Location/Organization.
Each block of 500 sentences belonging to a
dot type was an independent annotation subtask
with an isolated description. The annotator was
shown an example and had to determine whether
the headword in the example had the literal,
metonymic or the underspecified sense. Figure 1
shows an instance of the annotation process.
Figure 1: Screen capture for a Mechanical Turk
annotation instance or HIT
This annotation scheme is designed with the in-
tention of capturing literal, metonymic and under-
specified senses, and we use an inventory of three
possible answers, instead of using Markert and
Nissim?s (Markert and Nissim, 2002; Nissim and
Markert, 2005) approach with fine-grained sense
distinctions, which are potentially more difficult to
annotate and resolve automatically. Markert and
Nissim acknowledge a mixed sense they define as
being literal and metonymic at the same time.
For English we used Amazon Mechanical Turk
(AMT) with five annotations per example by turk-
ers certified as Classification Masters. Using AMT
provides annotations very quickly, possibly at the
expense of reliability, but it has been proven suit-
able for sense-disambiguation task (Snow et al,
2008). Moreover, it is not possible to obtain an-
notations for every language using AMT. Thus,
for Danish and Spanish, we obtained annotations
from volunteers, most of them native or very pro-
ficient non-natives. See Table 1 for a summary of
the annotation setup for each language.
After the annotation task we obtained the agree-
726
Language annotators type
Danish 3-4 volunteer
English 5 AMT
Spanish 6-7 volunteer
Table 1: Amount and type of annotators per in-
stance for each language.
ment values shown in Table 2. The table also pro-
vides the abbreviated names of the datasets.
Dot type Ao ? ? ?
eng:animeat 0.86 ? 0.24 0.69
eng:artinfo 0.48 ? 0.23 0.12
eng:contcont 0.65 ? 0.28 0.31
eng:locorg 0.72 ? 0.29 0.46
eng:procres 0.5 ? 0.24 0.10
da:contcont 0.32 ? 0.37 0.39
da:locorg 0.73 ? 0.37 0.47
spa:contcont 0.36 ? 0.3 0.42
spa:locorg 0.52 ?0.28 0.53
Table 2: Averaged observed agreement and its
standard deviation and alpha
Average observed agreement (Ao) is the mean
across examples for the proportion of matching
senses assigned by the annotators. Krippendorff?s
alpha is an aggregate measure that takes chance
disagreement in consideration and accounts for the
replicability of an annotation scheme. There are
large differences in ? across datasets.
The scheme can only provide reliable (Artstein
and Poesio, 2008) annotations (? > 0.6) for one
dot type2. This indicates that not all dot types are
equally easy to annotate, regardless of the kind of
annotator. In spite of the number and type of an-
notators, the Location/Organization dot type gives
fairly high agreement values for a semantic task,
and this behavior is consistent across languages.
5 Assigning sense by majority voting
Each example has more than one annotation and
we need to determine a single sense tag for each
example. However, if we assign senses by major-
ity voting, we need a backoff strategy in case of
ties.
The common practice of backing off to the most
frequent sense is not valid in this scenario, where
there can be a tie between the metonymic and
the underspecified sense. We use a backoff that
incorporates our assumption about the relations
2We have made the data freely available at
http://metashare.cst.dk/repository/search/?q=regular+polysemy
between senses, namely that the underspecified
sense sits between the literal and the metonymic
senses:
1. If there is a tie between the underspecified
and literal senses, the sense is literal.
2. If there is a tie between the underspec-
ified and metonymic sense, the sense is
metonymic.
3. If there is a tie between the literal and
metonymic sense or between all three senses,
the sense is underspecified.
Dot type L M U V B
eng:animeat 358 135 7 3 4
eng:artinfo 141 305 54 8 48
eng:contcont 354 120 25 0 25
eng:locorg 307 171 22 3 19
eng:procres 153 298 48 3 45
da:contcont 328 82 91 53 38
da:locorg 322 95 83 44 39
spa:contcont 291 140 69 54 15
soa:locorg 314 139 47 40 7
Table 3: Literal, Metonymic and Underspecified
sense distributions, and underspecified senses bro-
ken down in Voting and Backoff
The columns labelled L, M and U in Table 3
provide the sense distributions for each dot type.
The preference for the underspecified sense varies
greatly, from the very infrequent for English in
Animal/Meat to the two Danish datasets where the
underspecified sense evens with the metonymic
one. However, the Danish examples have mostly
three annotators, and chance disagreement is the
highest for this language in this setup, i.e., the
chance for an underspecified sense in Danish to
be assigned by our backoff strategy is the highest.
Columns V and B show respectively whether
the underspecified senses are a result of majority
voting or backoff. In contrast to volunteers, turk-
ers disprefer the underspecified option and most
of the English underspecified senses are assigned
by backoff. However, it cannot be argued that
turkers have overused clicking on the first option
(a common spamming behavior) because we can
see that two of the English dot types (eng:artinfo,
eng:procres) have majority of metonymic senses,
which are always second in the scheme (cf. Fig.
1). Looking at the amount of underspecified
senses that have been obtained by majority vot-
ing for Danish and Spanish, we suggest that the
level of abstraction required by this annotation is
too high for turkers to perform at a level compara-
727
ble to that of our volunteer annotators.
Figure 2: Proportion of non-literality in location
names across languages
Figure 2 shows the proportion of non-literal
(metonymic+underspecified) examples for the
Location/Organization words that are common
across languages. We can see that individual
words show sense skewdness. This skewdness is
a consequence of the kind of text in the corpus:
e.g. America has a high proportion of non-literal
senses in the ANC, where it usually means ?the
population or government of the US?. Similarly,
it is literal less than 50% of the times for the other
two languages. In contrast, Afghanistan is most
often used in its literal location sense.
6 Assigning senses with MACE
Besides using majority voting with backoff , we
use MACE (Hovy et al, 2013) to obtain the sense
tag for each example.
Dot type L M U D I
eng:animeat 340 146 14 .048 3
eng:artinfo 170 180 150 .296 46
eng:contcont 295 176 28 .174 0
eng:locorg 291 193 16 .084 3
eng:procres 155 210 134 .272 33
da:contcont 223 134 143 .242 79
da:locorg 251 144 105 .206 53
spa:contcont 270 155 75 .074 56
spa:locorg 302 146 52 .038 40
Table 4: Sense distributions calculated with
MACE, plus Difference and Intersection of under-
specified senses between methods
MACE is an unsupervised system that uses
Expectation-Maximization (EM) to estimate the
competence of annotators and recover the most
likely answer. MACE is designed as a Bayesian
network that treats the ?correct? labels as latent
variables. This EM method can also be understood
as a clustering that assigns the value of the closest
calculated latent variable (the sense tag) to each
data point (the distribution of annotations).
Datasets that show less variation between
senses calculated using majority voting and using
MACE will be more reliable. Along the sense dis-
tribution in the first three columns, Table 4 pro-
vides the proportion of the senses that is different
between majority voting and MACE (D), and the
size of the intersection (I) of the set of underspec-
ified examples by voting and by MACE, namely
the overlap of the U columns of Tables 3 and 4.
Table 4 shows a smoother distribution of senses
than Table 3, as majority classes are down-
weighted by MACE. It takes very different de-
cisions than majority voting for the two En-
glish datasets with lowest agreement (eng:artinfo,
eng:procres) and for the Danish datasets, which
have the fewest annotators. For these cases, the
diferences oscillate between 0.206 and 0.296.
Although MACE increases the frequency of
the underspecified senses for all datasets but one
(eng:locorg), the underspecified examples in Ta-
ble 3 are not subsumed by the MACE results. The
values in the I column show that none of the un-
derspecified senses of eng:contcont receive the un-
derspecified sense by MACE. All of these exam-
ples, however, were resolved by backoff, as well
as most of the other underspecified cases in the
other English datasets. In contrast to the voting
method, MACE does not operate with any theo-
retical assumption about the relation between the
three senses and treats them independently when
assigning the most likely sense tag to each distri-
bution of annotations.
7 Comparison between methods
The voting system and MACE provide different
sense tags. The following examples (three from
eng:contcont and four from eng:locorg) show dis-
agreement between the sense tag assigned by vot-
ing and by MACE:
d) To ship a crate of lettuce across the country,
a trucker needed permission from a federal
regulatory agency.
e) Controls were sent a package containing
stool collection vials and instructions for col-
lection and mailing of samples.
f) In fact, it was the social committee, and
our chief responsibilities were to arrange for
bands and kegs of beer .
728
g) The most unpopular PM in Canada?s mod-
ern history, he introduced the Goods and Ser-
vices Tax , a VAT-like national sales tax.
h) This is Boston?s commercial and financial
heart , but it s far from being an homogeneous
district [...]
i) California has the highest number of people
in poverty in the nation ? 6.4 million, includ-
ing nearly one in five children.
j) Under the Emperor Qianlong (Chien Lung),
Kangxi?s grandson, conflict arose between
Europe?s empires and the Middle Kingdom.
All of the previous examples were tagged as un-
derspecified by either the voting system or MACE,
but not by both. Table 5 breaks down the five an-
notations that each example received by turkers in
literal, metonymic and underspecified. The last
two columns show the sense tag provided by vot-
ing or MACE.
Example L M U VOTING MACE
d) 2 2 1 U L
e) 3 1 1 L U
f) 1 2 2 M U
g) 2 2 1 U M
h) 2 2 1 U M
i) 3 0 2 L U
j) 1 2 2 M U
Table 5: Annotation summary and sense tags for
the examples in this section
Just by looking at the table it is not immediate
which method is preferable to assign sense tags
in cases that are not clear-cut. In the case of i),
we consider the underspecified sense more ade-
quate than the literal one obtained by voting, just
like we are also more prone to prefer the under-
specified meaning in f), which has been assigned
by MACE. In the case of h), we consider that the
strictly metonymic sense assigned by MACE does
not capture both the organization- (?commercial
and financial?) and location-related (?district?) as-
pects of the meaning, and we would prefer the un-
derspecifed reading. However, MACE can also
overgenerate the underspecified sense, as the vials
mentioned in example e) are empty and have no
content yet, thereby being literal containers and
not their content.
Examples d), g) and h) have the same dis-
tribution of annotations?namely 2 literal, 2
metonymic and 1 underspecified?but d) has re-
ceived the literal sense from MACE, whereas the
other two are metonymic. This difference is a re-
sult of having trained MACE independently for
each dataset. The three examples receive the un-
derspecified sense from the voting scheme, since
neither the literal or metonymic sense is more
present in the annotations.
On the other hand, e) and i) are skewed towards
literality and receive the literal sense by plurality
without having to resort to any backoff, but they
are marked as underspecified by MACE.
8 Conclusions
We have described the annotation process of a
regular-polysemy corpus in English, Danish and
Spanish which deals with five different dot types.
After annotating the examples for their literal,
metonymic or underspecified reading, we have
determined that this scheme can provide reliable
(? over 0.60) annotations for one dot type. Not
all the dot types are equally easy to annotate.
The main source of variation in agreement, and
thus annotation reliability, is the dot type itself.
While eng:animeat and eng:locorg appear the eas-
iest, eng:artinfo and eng:procres obtain very low ?
scores.
9 Further work
After collecting annotated data, the natural next
step is to attempt class-based word-sense disam-
biguation (WSD) to predict the senses in Tables 3
and 4 using a state-of-the-art system like Nastase
et al (2012). We will consider a sense-assignment
method (voting or MACE) as more appropriate if
it provides the sense tags that are easiest to learn
by our WSD system.
However, learnability is only one possible pa-
rameter for quality, and we also want to develop
an expert-annotated gold standard to compare our
data against. We also consider the possibility of
developing a sense-assignment method that relies
both on the theoretical assumption behind the vot-
ing scheme and the latent-variable approach used
by MACE.
Acknowledgments
The research leading to these results has been
funded by the European Commission?s 7th Frame-
work Program under grant agreement 238405
(CLARA).
729
References
Mette Skovgaard Andersen, Helle Asmussen, and J?rg
Asmussen. 2002. The project of korpus 2000 go-
ing public. In The Tenth EURALEX International
Congress: EURALEX 2002.
J. D. Apresjan. 1974. Regular polysemy. Linguistics.
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555?596.
Gemma Boleda, Sabine Schulte im Walde, and Toni
Badia. 2012. Modeling regular polysemy: A study
on the semantic classification of catalan adjectives.
Computational Linguistics, 38(3):575?616.
Ted Briscoe, Ann Copestake, and Alex Lascarides.
1995. Blocking. In Computational Lexical Seman-
tics. Citeseer.
Aitor Gonzalez-Agirre, Egoitz Laparra, and German
Rigau. 2012. Multilingual central repository ver-
sion 3.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC 2012), pages 2525?2529.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with mace. In Proceedings of NAACL-HLT 2013.
N. Ide and C. Macleod. 2001. The american national
corpus: A standardized resource of american en-
glish. In Proceedings of Corpus Linguistics 2001,
pages 274?280. Citeseer.
J. B. Johannessen, K. Haagen, K. Haaland, A. B.
Jo?nsdottir, A. N?klestad, D. Kokkinakis, P. Meurer,
E. Bick, and D. Haltrup. 2005. Named entity recog-
nition for the mainland scandinavian languages. Lit-
erary and Linguistic Computing, 20(1):91.
M. Lapata and A. Lascarides. 2003. A probabilistic
account of logical metonymy. Computational Lin-
guistics, 29(2):261?315.
K. Markert and M. Nissim. 2002. Towards a cor-
pus annotated for metonymies: the case of location
names. In Proc. of LREC. Citeseer.
K. Markert and M. Nissim. 2009. Data and models
for metonymy resolution. Language Resources and
Evaluation, 43(2):123?138.
Vivi Nastase, Alex Judea, Katja Markert, and Michael
Strube. 2012. Local and global context for super-
vised and unsupervised metonymy resolution. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
183?193. Association for Computational Linguis-
tics.
Malvina Nissim and Katja Markert. 2005. Learning
to buy a renault and talk to bmw: A supervised ap-
proach to conventional metonymy. In Proceedings
of the 6th International Workshop on Computational
Semantics, Tilburg.
Geoffrey Nunberg. 1995. Transfers of meaning. Jour-
nal of semantics, 12(2):109?132.
B. S. Pedersen, S. Nimb, J. Asmussen, N. H. S?rensen,
L. Trap-Jensen, and H. Lorentzen. 2009. Dan-
net: the challenge of compiling a wordnet for danish
by reusing a monolingual dictionary. Language re-
sources and evaluation, 43(3):269?299.
Thierry Poibeau. 2006. Dealing with metonymic read-
ings of named entities. arXiv preprint cs/0607052.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. Glml: Annotating argument
selection and coercion. In IWCS-8: Eighth Interna-
tional Conference on Computational Semantics.
J. Pustejovsky. 1995. The generative lexicon: a theory
of computational lexical semantics.
A. Rumshisky, VA Grinberg, and J. Pustejovsky. 2007.
Detecting selectional behavior of complex types in
text. In Fourth International Workshop on Genera-
tive Approaches to the Lexicon, Paris, France. Cite-
seer.
Ekaterina Shutova. 2009. Sense-based interpretation
of logical metonymy using a statistical method. In
Proceedings of the ACL-IJCNLP 2009 Student Re-
search Workshop, pages 1?9. Association for Com-
putational Linguistics.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng.
2008. Cheap and fast?but is it good?: evalu-
ating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Jorge Vivaldi. 2009. Corpus and exploitation tool:
Iulact and bwananet. In I International Confer-
ence on Corpus Linguistics (CICL 2009), A survey
on corpus-based research, Universidad de Murcia,
pages 224?239.
730
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 29?31,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Web Services for Bayesian Learning 
 
 
Muntsa Padr? N?ria Bel 
Universitat Pompeu Fabra Universitat Pompeu Fabra 
Barcelona, Spain Barcelona, Spain 
muntsa.padro@upf.edu nuria.bel@upf.edu 
 
 
Abstract 
In this demonstration we present our web 
services to perform Bayesian learning for 
classification tasks.  
1 Introduction 
The Bayesian framework for probabilistic infe-
rence has been proposed (for instance, Griffiths 
et al, 2008 and a survey in Chater and Manning, 
2006 for language related topics) as a general 
approach to understanding how problems of in-
duction can be solved given only the sparse and 
noisy data that humans observe. In particular, 
how human acquire words if the available data 
severely limit the possibility of making infe-
rences. Bayesian framework has been proposed 
as way to introduce a priori knowledge to guide 
the inference process. In particular for Lexical 
Acquisition, Xu and Tenembaum (2007) pro-
posed that given a hypothesis space (all what a 
word can be, according to a set of existing 
classes) and one or more examples of a new 
word, the learner evaluates all hypotheses for 
candidate word classes by computing their post-
erior probabilities, proportional to the product of 
prior probabilities and likelihood. The prior 
probabilities are the learner?s beliefs about which 
hypotheses are more or less plausible. The like-
lihood reflects the learner?s expectations about 
which examples are likely to be observed given a 
particular hypothesis about a word class. And the 
decision on new words is determined by averag-
ing the predictions of all hypothesis weighted by 
their posterior probabilities. 
The hypothesis behind is that natural language 
characteristics, such as the Zipfian distribution of 
words (Zipf, 1935) and considerations as the 
classic argument on sparse data (Chomsky, 
1980), make it necessary to postulate that the 
learning of words must be guided by the know-
ledge of the lexical system itself, information 
about abstracted, not directly observable catego-
ries (Goldberg, 2006; Bybee, 1998). 
In order to test this hypothesis we developed a 
series of tools for the task of noun classification 
into lexical semantic classes (such as EVENT, 
HUMAN, LOCATION, etc.). The tools perform 
Bayesian parameter estimation where prior 
knowledge is included into the parameters as 
virtual evidence (following Griffiths et al 2008) 
and a Naive Bayes based classification. Our as-
sumption is that, if introducing prior knowledge 
improves the classification results, it may give 
some insights about the way humans learn lexical 
classes. 
The developed tools have been deployed as 
web services (following web-based architecture 
of the PANACEA project 1 ) in order to make 
them easily available to the community. They 
can be used in the task just mentioned but also in 
other tasks that may profit from a Bayesian ap-
proach. 
2 Web Services for Bayesian modeling 
In this demonstration, we present two web ser-
vices that can be used for Bayesian inference of 
parameters and classification with the aim that 
they may be useful to other researchers willing to 
use Bayesian methods in their research. 
2.1 Naive Bayes Classifier 
A first web service performs a traditional Naive 
Bayes classification. The input is the observed 
data from a given instance encoded as cue vec-
tors, this is, the number of times we have seen 
each cue in the context of the studied instance. 
Then, the web service computes how likely is 
that this instance belongs to a particular class. 
The input needed by the classifier is the set of 
probabilities of seeing each cue given each 
class??????|??. Those parameters should have 
                                                          
1
 http://panacea-lr.eu/ 
29
been previously induced (using Maximum Like-
lihood Estimation (MLE), a Bayesian approach, 
etc.). 
The classifier web service reads those prob-
abilities from a coma separated file and the cue 
vectors of the instances we want to classify in 
Weka format (Quinlan, 1993). In our implemen-
tation, we work with binary classification, i.e. we 
want to decide whether the noun belongs or does 
not belong to a given class. Thus, the service 
returns the most likely class for each instance 
given the parameters and a score for this classifi-
cation (i.e. how different was the probability of 
being and not being a member of the class). 
2.2 Bayesian Estimation of Probabilities 
A second web service performs parameter infer-
ence for the Naive Bayes classifier using Bayes-
ian methods. 
Bayesian methods (Griffiths et al, 2008; 
Mackay, 2003) are a formal framework to intro-
duce prior knowledge when estimating the pa-
rameters (probabilities) of a given system. The 
main difference between those methods and 
MLE is that the latter use only data to estimate 
parameters, while the former use both data and 
prior knowledge. 
An example of Bayesian learning is determin-
ing the probability of a coin producing heads in a 
short throw series. A MLE approach will deter-
mine this probability as ??????? ? ??????? . Thus, 
after observing a sequence of 5 heads in a row, 
MLE would assess that the probability of the 
coin producing heads is 1. Nevertheless, because 
of our knowledge, we would rather say that a tail 
is more than possible, and that the coin probabil-
ity can still be close to 0.5. Bayesian models 
allow us to formally introduce this knowledge 
when estimating the probabilities. 
In the case of Naive Bayes classification using 
cue vectors, we need to estimate ??????|??for 
each cue and k (for binary classification this 
would be k=1 for being a member of the class 
and k=0 for not being a member of the class). 
Bayesian modelling computes these parame-
ters approximating them by their Maximum a 
Posteriori (MAP) estimator. The canonical ap-
proach introduces the prior probabilities as a 
Beta distribution, and leads to the following 
MAP estimator (see Griffiths et al (2008) and 
Mackay  (2003) for details): 
??? ? ???????|?? ?
????? ??? ? ????? ???
????? ??? ? ????? ??? ? ???? ??? ? ???? ???
 
Where ????? ???  and ???? ???  are the observed oc-
currences in real data (????? ??? is the number of 
times we have seen cuei with class k and ???? ??? 
is the number of times we have not seen it, and 
????? ??? and ???? ??? represent what is called virtual 
data, this is, the data we expect to observe a pri-
ori. Thus, it can be seen from the MAP estimator 
that Bayesian inference allows us to add virtual 
data to actual evidence. 
The web service we want to show in this dem-
onstration implements the estimation of  
??????|?? combining the data and the priors sup-
plied by the user. The service reads labelled data 
in Weka format and the priors for each cue and 
class and computes ??????|??. The output of this 
web service can be directly used to classify new 
instances with the first one. 
3 Test case: Lexical Acquisition 
As a showcase, we will show our work in cue-
based noun classification. The aim is the auto-
matic acquisition of lexical semantic information 
by building classifiers for a number of lexical 
semantic classes.  
3.1 Demonstration Outline 
In our demonstration, we will show how we can 
use the web services to learn, tune and test 
Bayesian models for different lexical classes. We 
will compare our results with a Naive Bayes 
approach, which can also be learned with our 
system, using null virtual data. 
First of all, we will get noun occurrences from 
a corpus and encode these occurrences as cue 
vectors applying a set of regular expressions. 
This will be done with another web service that 
directly outputs a Weka file. This Weka file will 
be divided into train and test data. 
Secondly, the obtained training data will be 
used as input in the Bayesian learner web ser-
vice, obtaining the values for ??????|?? for each 
cue and class. We will perform two calls: one 
using prior knowledge and one without it (MLE 
approach). 
Finally, these two sets of parameters will be 
used to annotate the test data and we will com-
pare the performance of the Bayesian model with 
the performance of the MLE model. 
Acknowledgments 
This work was funded by the EU 7FP project 
248064 PANACEA. 
30
References 
J. Bybee. 1998. The emergent lexicon. CLS 34: The 
panels. Chicago Linguistics Society. 421-435.  
N. Chater,and C.D. Manning. 2006. Probabilistic 
models of language processing and acquisi-
tion.Trends in Cognitive Sciences, 10, 335-344. 
N. Chomsky.1980. Rules and representations. Oxford: 
Basil Blackwell. 
A. E. Goldberg. 2006. Constructions at work. Oxford 
University Press.  
T. L. Griffiths, C. Kemp, and J.B. Tenenbaum. 2008. 
Bayesian models of cognition. In Ron Sun (ed.), 
Cambridge Handbook of Computational Cognitive 
Modeling.Cambridge University Press. 
D. J. C. MacKay. 2003. Information Theory, Infe-
rence, and Learning Algorithms. Cambridge Uni-
versity Press, 2003. ISBN 0-521-64298-1 
R.J. Quinlan. 1993. C4.5: Programs for Machine 
Learning. Series in Machine Learning.Morgan 
Kaufman, San Mateo, CA. 
Xu, F. and Tenenbaum, J. B. (2007).Word learning as 
Bayesian inference.Psychological Review 114(2). 
G.K. Zipf. 1935. The Psycho-Biology of Language, 
Houghton Mifflin, Boston. 
 
31
