Automatic Term Extraction Based on Perplexity
of Compound Words
Minoru Yoshida1,2 and Hiroshi Nakagawa1,2
1 Information Technology Center, University of Tokyo,
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033
2 JST CREST, Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012
mino@r.dl.itc.u-tokyo.ac.jp, nakagawa@dl.itc.u-tokyo.ac.jp
Abstract. Many methods of term extraction have been discussed in
terms of their accuracy on huge corpora. However, when we try to apply
various methods that derive from frequency to a small corpus, we may
not be able to achieve sufficient accuracy because of the shortage of
statistical information on frequency. This paper reports a new way of
extracting terms that is tuned for a very small corpus. It focuses on the
structure of compound terms and calculates perplexity on the term unit?s
left-side and right-side. The results of our experiments revealed that the
accuracy with the proposed method was not that advantageous. However,
experimentation with the method combining perplexity and frequency
information obtained the highest average-precision in comparison with
other methods.
1 Introduction
Term extraction, which is the task of extracting terminology (or technical terms)
from a set of documents, is one of major topics in natural language processing. It
has a wide variety of applications including book indexing, dictionary generation,
and keyword extraction for information retrieval systems.
Most automatic term extraction systems make a sorted list of candidate terms
extracted from a given corpus according to the ?importance? scores of the terms,
so they require scores of ?importance? for the terms. Existing scores include
TF-IDF, C-Value [1], and FLR [9]. In this paper, we propose a new method
that involves revising the definition of the FLR method in a more sophisticated
way. One of the advantages of the FLR method is its size-robustness, i.e, it can
be applied to small corpus with less significant drop in performance than other
standard methods like TF and IDF, because it is defined using more fine-grained
features called term units. Our new method, called FPP, inherit this property
while exhibiting better performance than FLR.
At the same time, we also propose a new scheme for evaluating term ex-
traction systems. Our idea is to use summaries1 of articles as a gold standard.
This strategy is based on the assumption that summaries of documents can
1 In more detail, an article revised for display on mobile phones.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 269?279, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
270 M. Yoshida and H. Nakagawa
serve as collections of important terms because, in writing summaries, peo-
ple may make an original document shorter by dropping unnecessary parts
of original documents, while retaining essential fragments. Thus, we regard a
term in an original document to be important if it also appears in the
summary.
2 Term Extraction
Term extraction is the task of extracting important terms from a given corpus.
Typically, term extraction systems first extract term candidates, which are usu-
ally the noun phrases detected by handcrafted POS sequence patterns, from the
corpus. After that, term candidates are sorted according to some importance
score. Important terms, (i.e., terms that appear in the summary, in our problem
setting,) are desired to be ranked higher than others. In this paper we focus
on the second step, i.e., term candidate sorting by importance scores. We pro-
pose a new score of term importance by modifying an existing one in a more
sophisticated manner.
In the remainder of this paper, a term candidate is represented by W = w1w2
? ? ? wn where wi represents a term unit contained in W , and n is the number of term
units contained in W . Here, a term unit is the basic element comprising term can-
didates that is not further decomporsable without destruction of meaning. Term
units are used to calculate of the LR score that is explained in the next section.
3 Related Work
Many methods of term scoring have been proposed in the literature [7] [3] [4].
Methods that use corpus statistics have especially emerged over the past decade
due to the increasing number of machine-readable documents such as news arti-
cles and WWW documents. These methods can be mainly categorized into the
following three types according to what types of features are used to calculate
the scores.
? Measurement by frequencies
? Measurement by internal structures of term candidates
? Combination of the above
3.1 Score by Frequency: TF
Frequency is one of the most basic features of term extraction. Usually, a term
that appears frequently is assumed to be important. We introduce a score of this
type: tf(W ).
tf(W ) represents the TF(Term Frequency) of W . It is defined as the number
of occurrences of W in all documents. Note that tf(W ) is the result of the
brute force counting of W occurrences. This method, for example, counts the
Automatic Term Extraction Based on Perplexity of Compound Words 271
term natural even if it is merely part of another phrase such as natural language
processing.2
3.2 Score by Internal Structures in Term Candidates: LR
An LR method [9] is based on the intuition that some words are used as term
units more frequently than others, and a phrase that contains such ?good? term
units is likely to be important. The left score l(wi) of each term unit wi of a target
term is defined as the number (or the number of types) of term units connected
to the left of wi (i.e., appearing just in the left of wi in term candidates), and the
right score r(wi) is defined in the same manner.3 An LR score lr(wi) is defined
as the geometric mean of left and right scores:
lr(wi) =
?
l(wi)r(wi)
The total LR score of W is defined as a geometric mean of the scores of term
units as:
LR(W ) = (lr(w1)lr(w2) ? ? ? lr(wn))
1
n .
An example of LR score calculation is given in the next section.
3.3 Mixed Measures
C-Value. C-Value[1] is defined by the following two expressions:
t(W ): frequency of terms that contain W ,
c(W ): number of types of terms that contain W .
Note that t(W ) does not count W itself. Intuitively, t(W ) is the degree of being
part of another term, and c(W ) is the degree of being part of various types of
terms.
C-Value is defined by using these two expressions in the following way.
c-val(W ) = (n ? 1) ?
(
tf(W ) ? t(W )
c(W )
)
Note that the value is zero where n = 1. MC-Value [9] is a modified version
of C-Value adapted for use in term collections that include the term of length 1
(i.e., n = 1).
MC-val(W ) = n ?
(
tf(W ) ? t(W )
c(W )
)
We used MC-Value in the experiments because our task was to extract terms
regardless of whether each term is one-word term or not.
2 We can also use another frequency score F(Frequency), or f(W ), that is defined as
the number of independent occurrences of W in all documents. (Independent means
that W is not included in any larger term candidate.) However, we observed that
f(W ) (or the combination of f(W ) and another score) had no advantage over tf(W )
(or the combination of tf(W ) and another score) in the experiments,so in this paper
we omit scores that are the combination of f(W ) and other scores.
3 In addition, we apply the adding-one smoothing to both of them to avoid the score
being zero when wi has no connected terms.
272 M. Yoshida and H. Nakagawa
FLR. The LR method reflects the number of appearances of term units, but does
not reflect that of a whole term itself. For example, even if ?natural language? is
more frequent than ?language natural? and the former should be given a higher
score than the latter, LR cannot be used to do this.
An FLR method [9] was proposed to overcome this shortcoming of LR. It
reflects both the frequencies and inner structures of terms. FLR(W ) is defined
as the product of LR(W ) and tf(W ) as:
FLR(W ) = tf(W )LR(W ).
4 Our Method: Combining Types and Frequencies via
Entropy
4.1 Preliminaries: Token-LR and Type-LR
Figure 1 outlines example statistics for term unit connections. For example, the
term disaster information appeared three times in the corpus.
Disaster
3 times
Information
Ethics 2 times
System 1 times
Security 3 times












Fig. 1. An example of statistics for term unit connections
LR scores have two versions: Token-LR and Type-LR. Token-LR (and Type-
LR) are calculated by simply counting the frequency (and the types) of terms
connected to each term unit, respectively. In this case, a Type-LR score for the
term unit ?information? is
l(information) = 1 + 14, r(information) = 3 + 1, LR(information) =
?
8,
and a Token-LR score is
l(information) = 3 + 1, r(information) = 6 + 1, LR(information) =
?
28.
4 Note that the adding-one smoothing is applied.
Automatic Term Extraction Based on Perplexity of Compound Words 273
Type-LR cannot reflect frequencies which suggest whether there are spe-
cially important connecting terms or not. However, Token-LR cannot reflect the
number of types that suggest the variety of connections. To solve these short-
comings with LR measures, we propose a new kind that combines these two
through perplexity.
4.2 Term Extraction by Perplexity
Our method is based on the idea of perplexity [8]. The score of a term is defined
by the left perplexity and right perplexity of its term units. In this subsection we
first give a standard definition of the perplexity of language, from which our left
and right perplexity measures are derived. After that, we describe how to score
terms by using these perplexities.
Perplexity of language. Assume that language L is information source that
produces word lists of length n and each word list is produced independently
with probability P (wn1 ). Then, the entropy of language L is calculated as:
H0(L) = ?
?
wn1
P (wn1 ) log P (w
n
1 ).
The entropy per word is then calculated as:
H(L) = ? 1
n
?
wn1
P (wn1 ) log P (w
n
1 ).
This value indicates the number of bits needed to express each word generated
from L. Perplexity of language L is defined using H(L) as:
Perplexity = 2H(L).
Perplexity can be seen as the average number of types of words that follow each
preceding word. The larger the perplexity of L, the less predictable the word
connection in L.
Left and right perplexity. Assume that k types of unit words can connect to
the right of wi (see Figure 2).
Also assume that Ri is a random variable assigned to the i-th term unit which
represents its right connections and takes its value from the set {r1, r2, ? ? ? , rk}.
Then, entropy H(Ri) is calculated as:
H(Ri) = ?
k
?
j=1
P (rj) log2 P (rj)
Note that we define 0 log 0 = 0, according to the fact that x log x ? 0 where
x ? 0.
274 M. Yoshida and H. Nakagawa
ffifl
fi
wi
ffifl
fi
r1
ffifl
fi
r2
...
ffifl
fi
rk






















Fig. 2. Example of term unit and term units connected to its right
This entropy value can be thought of as a variety of terms that connect to
the right of wi, or, more precisely, the number of bits needed to describe words
that connect to the right of wi.
Then right perplexity ppr(wi) of term unit wi is defined as
ppr(wi) = 2H(R
i).
This value can be seen as the number of branches, in the sense of information
theory, of right-connection from wi. It naturally reflects both the frequency and
number of types of each connection between term units.
Random variable Li for the left connections is defined in the same manner.
The perplexity for left connections is thus defined as:
ppl(wi) = 2H(L
i).
Term Score by Perplexity. We define our measure by substituting l and r
in the definition of LR with ppl and ppr. First, a combination of left and right
perplexities is defined as the geometric mean of both:
pp(wi) = (ppl(wi) ? ppr(wi))
1
2 .
After that, perplexity score PP (W ) for W is defined as the geometric mean of
all pp(wi)s:
PP (W ) =
[
n
?
i=1
pp(wi)
]
1
n
.
Automatic Term Extraction Based on Perplexity of Compound Words 275
We used log PP (W ) instead of PP (W ) to make implementation easier. Notice
that log x is a monotonic (increasing) function of x.
PP (W ) =
[
n
?
i=1
{ppl(wi) ? ppr(wi)}
1
2
]
1
n
? log2 PP (W ) =
1
n
log2
(
n
?
i=1
{ppl(wi) ? ppr(wi)}
1
2
)
? log2 PP (W ) =
1
2n
n
?
i=1
(log2 ppl(wi) + log2 ppr(wi))
Using ppr(wi) = 2H(R
i) and ppl(wi) = 2H(l
i), we obtain
log2 PP (W ) =
1
2n
n
?
i=1
(
H(Ri) + H(Li)
)
.
The right side means the sum of the left and right entropies of all term units.
4.3 Term Extraction by Perplexity and TF
Perplexity itself serves as a good score for terms, but combining it with TF,
which is a measure from another point of view, can provide a still better score
that reflects both the inner structures of term candidates and their frequencies
which are regarded as global information about the whole corpus.
Our new score, FPP (W ), which is a combination of PP and TF, is defined
as their product:
FPP (W ) = tf(W )PP (W )
? log2 FPP (W ) = log2 tf(W ) + log2 PP (W )
? log2 FPP (W ) = log2 tf(W ) +
1
2n
n
?
i=1
(
H(Ri) + H(Li)
)
We avoided the problem of log2 tf(W ) being undefined with tf(W ) = 0
5
by applying the adding-one smoothing to tf(W ). Therefore, the above defi-
nition of log FPP (W ) changed as follows:
log2 FPP
?(W ) = log2(tf(W ) + 1) +
1
2n
n
?
i=1
(
H(Ri) + H(Li)
)
.
We used this log2 FPP
?(W ) measure for evaluation.
5 This situation occurs when we want to score a new term candidate from outside of
corpus.
276 M. Yoshida and H. Nakagawa
5 Experiments
5.1 Test Collection
We collected news articles and their summaries from the Mainichi Web News
from April, 2001 to March, 2002. The articles were categorized into four genres:
Economy, Society, World, and Politics. A shorter version of each article was
provided for browsing on mobile phones. Articles for mobile phones were written
manually from the original ones, which were shorter versions of the original
articles adapted to small displays. We regard them as summaries of the original
articles and used them to evaluate whether the extracted terms were correct
or not. If a term in the original article was also in the summary, the term was
correct, and incorrect if otherwise. Each article had a size of about 300 letters
and each summary had a size of about 50.
Table 1 lists the number of articles in each category.
Table 1. Number of articles in test collection
Economy Society World Politics
# of articles 4,177 5,952 6,153 4,428
5.2 Experimental Setup
We used test data on the various numbers of articles to investigate how the
performance of each measure changed according to corpus size. A corpus of each
size was generated by singly adding an article randomly selected from the corpus
of each genre. We generated test data consisting of 50 different sizes (from 1 to
50) for each genre. The average number of letters in the size 50 corpus was about
19,000, and the average number of term candidates was about 1,300. We used
five different seed numbers to randomly select articles. The performance of each
method was evaluated in terms of recall and precision, which were averaged over
the five trials.
5.3 Preprocessing: Term Candidate Extraction
Each article was preprocessed with a morphological analyzer, the Chasen 2.3.3.[2]
The output of Chasen was further modified according to heuristic rules as follows.
? Nouns and undefined words were extracted for further processes and other
words were discarded.
? Suffixes and prefixes were concatenated to their following and preceding
words, respectively.
The result was a set of term candidates to be evaluated with the term importance
scores described in the previous sections.
We applied the following methods to the term candidates: F, TF, DF
(Document Frequency) [8], LR, MC-Value, FLR, TF-IDF [8], PP, and FPP?.
Automatic Term Extraction Based on Perplexity of Compound Words 277
5.4 Evaluation Method
We used average precision [8] for the evaluation. Let D be a set of all the term
candidates and Dq ? D be a set of the correct ones among them. The extracted
term was correct if it appeared in the summary. Then, the average precision can
be calculated in the following manner.
Average-Precision =
1
|Dq|
?
1?k?|D|
?
?
?
rk ?
?
?
1
k
?
1?i?k
ri
?
?
?
?
?
where ri = 1 if the i-th term is correct, and ri = 0 if otherwise.
Note that the total number of correct answers was |Dq|. The next section
presents the experimental results obtained by average precision.
Table 2. Average precision on corpus of 1, 10, and 50 articles. Each cell contains
results for the Economy/World/Society/Politics genres.
Measure SIZE=1 SIZE=10 SIZE=50
F 0.275/0.274/0.246/0.406 0.337/0.350/0.325/0.378 0.401/0.415/0.393/0.425
TF 0.305/0.388/0.281/0.430 0.386/0.406/0.376/0.435 0.454/0.462/0.436/0.477
DF 0.150/0.173/0.076/0.256 0.237/0.253/0.234/0.294 0.337/0.357/0.332/0.378
LR 0.192/0.370/0.194/0.378 0.255/0.280/0.254/0.317 0.303/0.302/0.273/0.320
MC-Val 0.218/0.296/0.240/0.388 0.317/0.334/0.307/0.365 0.399/0.400/0.369/0.420
FLR 0.305/0.410/0.298/0.469 0.361/0.397/0.364/0.429 0.423/0.435/0.404/0.455
TF-IDF 0.150/0.173/0.076/0.256 0.388/0.407/0.376/0.437 0.457/0.465/0.438/0.479
PP 0.223/0.327/0.285/0.514 0.285/0.299/0.282/0.331 0.329/0.317/0.279/0.331
FPP? 0.320/0.457/0.380/0.561 0.407/0.444/0.409/0.471 0.487/0.480/0.448/0.493
6 Results and Discussion
Table 2 shows the results on the corpus of 1, 10, and 50 articles in all the gen-
res. Figure 3 plots the average precision for each corpus size (from 1 to 50) in
the economy category.6 In some cases, results on one article were better than
those on 10 and 50 articles. This was mainly caused by the fact that the av-
erage precision is tend to be high on articles of short length, and the average
length for one article was much shorter than that of ten articles in some genres.
PP outperformed LR in most cases. We think the reason was that PP could
provide more precious information about connections among term units. We ob-
served that PP depended less on the size of the corpus than frequency-based
methods like TF and MC-Val. FPP? had the best performance of all methods in
all genres.
6 We only show a graph in the economy genre, but the results in other genres were
similar to this.
278 M. Yoshida and H. Nakagawa
0.1
0.2
0.3
0.4
0.5
1 11 21 31 41
Corpus size (# of articles)
A
v
e
r
a
g
e
 
p
r
e
c
i
s
i
o
n
F TF DF
LR MC-Val FLR
TF-IDF PP FPP'
Fig. 3. Results in economy genre
0.3
0.4
0.5
0.6
50 150 250 350 450 550 650 750 850 950
Corpus size (# of articles)
A
v
e
r
a
g
e
 
p
r
e
c
i
s
i
o
n
F TF DF
LR MC-Val FLR
TF-IDF PP FPP'
Fig. 4. Results on 50 ? 1000 articles
Automatic Term Extraction Based on Perplexity of Compound Words 279
Figure 4 plots the results in the economy genre when the corpus size was
increased to 1,000 in increments of 50 articles. We observed that the perfor-
mance of PP and LR got close with the increase in corpus size, especially with
200 articles and more. FPP? once again outperformed all the other methods in
this experiment. The FPP? method exhibited the best performance regardless of
corpus size.
7 Conclusion and Future Work
We proposed a new method for extracting terms. It involved the combination of
two LR methods: Token-LR and Type-LR. We showed that these two could be
combined by using the idea of perplexity, and gave a definition for the combined
method. This new method was then combined with TF and experimental results
on the test corpus consisting of news articles and their summaries revealed that
the new method (FPP?) outperformed existing methods including TF, TF-IDF,
MC-Value, and FLR.
In future work, we would like to improve the performance of the method by,
for example, adding preprocessing rules, such as the appropriate treatment of
numerical characters, and developing more sophisticated methods for combin-
ing TF and PP. We also plan to extend our experiments to include other test
collections like TMREC [6].
References
1. Ananiadou, S.: A methodology for automatic term recognition. In Proceedings of
the 15th InternationalConference on Computational Linguistcs (COLING) (1994),
pp. 1034?1038.
2. Asahara, M., Matsumoto, Y.: Extended Models and Tools for High-performance
Part-of-Speech Tagger. Proceedings of COLING 2000. (2000).
3. COMPUTERM?98 First Workshop on Computational Terminology. (1998).
4. COMPUTERM?02 Second Workshop on Computational Terminology. (2002).
5. Frantzi, K. and Ananiadou, S.: The C-value/NC-value method for ATR. Journal of
NLP, Vol. 6, No. 3, (1999). pp.145?179.
6. Kageura, K.: TMREC Task: Overview and Evaluation. Proc. of the First NTCIR
Workshop on Research in Japanese Text Retrieval and Term Recognition, (1999).
pp. 411?440.
7. Kageura, K and Umino, B.: Methods of automatic term recognition: A review.
Terminology, Vol. 3, No. 2, (1996). pp. 259?289.
8. Manning, C.D., and Schutze, H..: Foundations of Statistical Natural Language Pro-
cessing. (1999). The MIT Press.
9. Nakagawa, H. and Mori, T.: Automatic Term Recognition based on Statistics of
Compound Nouns and their Components. Terminology, Vol. 9, No. 2, (2003). pp.
201?219.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 121?124, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Reformatting Web Documents via Header Trees
Minoru Yoshida and Hiroshi Nakagawa
Information Technology Center, University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
CREST, JST
mino@r.dl.itc.u-tokyo.ac.jp, nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
We propose a new method for reformat-
ting web documents by extracting seman-
tic structures from web pages. Our ap-
proach is to extract trees that describe hier-
archical relations in documents. We devel-
oped an algorithm for this task by employ-
ing the EM algorithm and clustering tech-
niques. Preliminary experiments showed
that our approach was more effective than
baseline methods.
1 Introduction
This paper proposes a novel method for reformat-
ting (i.e., changing visual representations,) of web
documents. Our final goal is to implement the sys-
tem that appropriately reformats layouts of web doc-
uments by separating semantic aspects (like XML)
from layout aspects (like CSS) of web documents,
and changing the layout aspects while retaining the
semantic aspects.
We propose a header tree, which is a reasonable
choice as a semantic representation of web docu-
ments for this goal. Header trees can be seen as vari-
ants of XML trees where each internal node is not an
XML tag, but a header which is a part of document
that can be regarded as tags annotated to other parts
of the document. Titles, headlines, and attributes are
examples of headers. The left part of Figure 1 shows
an example web document. In this document, the
headers are About Me, which is a title, and NAME
and AGE, which are attributes. (For example, NAME
can be seen as a tag annotated to John Smith.)
Figure 2 shows a header tree for the example docu-
ment. It should be noted that each node is labeled
with parts of HTML pages, not abstract categories
such as XML tags.
About Me
* NAME *
John Smith
* AGE *
25
Back to Home Page
...<h1>About Me</h1><center><br><br>
* NAME *<br>...
[ About,  Me,  NAME,  John,  Smith, ? ]
</h1><center><br><br>*
Web Page
HTML Source
List of Blocks
Separator
Figure 1: An Example Web Document and Conver-
sion from HTML Documents to Block Lists.
Therefore, the required task is to extract header
trees from given web documents. Web documents
can be reformatted by converting their header trees
into various forms including Powerpoint-like in-
dented lists, HTML tables1, and Tree-class objects
of Java. We implemented the system that produces
these representations by extracting header trees from
given web documents.
One application of such reformatting is a web
browser on small devices that shows extracted
header trees regardless of original HTML visual ren-
dering. Trees can be used as compact representa-
tions of web documents because they show internal
structures of web documents concisely, and they can
be further augmented with open/close operations on
each node for the purpose of closing unnecessary
nodes, or sentence summarization on leaf nodes con-
taining long sentences. Another application is a lay-
out changer, which change a layout (i.e., HTML tag
usage) of one web page to another, by aligning ex-
tracted header trees of two web documents. Other
applications include HTML to XML transformation
and audio-browsable web content (Mukherjee et al,
2003).
1For example, the first column represents the root, the sec-
ond column represents its children, etc.
121
About Me
NAME
John Smith
AGE
25
Back to Home Page
Figure 2: A Header Tree for the Example Web Doc-
ument
1.1 Related Work
Several studies have addressed the problem of ex-
tracting logical structures from general HTML doc-
uments without labeled training examples. One
of these studies used domain-specific knowledge to
extract information used to organize logical struc-
tures (Chung et al, 2002). However, their ap-
proach cannot be applied to domains for which
any knowledge is not provided. Another type of
study employed algorithms to detect repeated pat-
terns in a list of HTML tags and texts (Yang and
Zhang, 2001; Nanno et al, 2003), or more struc-
tured forms (Mukherjee et al, 2003; Crescenzi et
al., 2001; Chang and Lui, 2001) such as DOM
trees. This approach might be useful for certain
types of web documents, particularly those with
highly regular formats such as www.yahoo.com
and www.amazon.com. However, in many cases,
HTML tag usage does not have so much regularity,
and, there are even the case where headers do not
repeat at all. Therefore, this type of algorithm may
be inadequate for the task of header extraction from
arbitrary web documents.
The remainder of this paper is organized as fol-
lows. Section 2 defines the terms used in this paper.
Section 3 provides the details of our algorithm. Sec-
tion 4 lists the experimental results and Section 5
concludes this paper.
2 Definitions
2.1 Definition of Terms
Our system decomposes an HTML document into a
list of blocks. A block is defined as the part of a
web document that is separated by a separator. A
separator is a sequence of HTML tags and symbols.
Symbols are defined as characters in texts that are
neither numbers nor letters. Figure 1 shows an ex-
ample of the conversion of an HTML document to a
list of blocks.
[ [About Me, [NAME, John Smith], [AGE, 25] ], Back to Home
Page] ]
Figure 3: A List Representation of the Example Web
Document
A header is defined as a block that modifies sub-
sequent blocks. In other words, a block that can be
a tag annotated to subsequent blocks is defined as a
header. Some examples of headers are Titles (e.g.,
?About Me?), Headlines (e.g., ?Here is my pro-
file:?), Attributes (e.g., ?Name?, ?Age?, etc.), and
Dates.
2.2 Definition of the Task
The system produces header trees for given web
documents. A header tree can be seen as an indented
list of blocks where the level of each node?s indent
is equal to the depth of the node, as shown in Figure
2. Therefore, the main part of our task is to give a
depth to each block in a given web document. After
that, some heuristic rules are employed to construct
header trees from a list of depths. In the next sec-
tion, we discuss the task of assigning a depth to each
block. Therefore, an input to the system is a list of
blocks and the output is a list of depths.
The system also produces nested-list representa-
tion of header trees for the purpose of evaluation. In
nested-list representation, each node that has chil-
dren is represented by the list whose first element
represents the parent and remaining elements repre-
sent the children. Figure 3 shows list representation
of the tree in Figure 2.
3 Header Extraction Algorithm
In this section, we describe our algorithm that re-
ceives a list of blocks and returns a list of depths.
3.1 Basic Concepts
The algorithm proceeds in two steps: separator cat-
egorization and block clustering. The first step
estimates local block relations (i.e., relations be-
tween neighboring blocks) via probabilistic models
for characters and tags that appear around separa-
tors. The second step supplements the first by ex-
tracting the undetermined relations between blocks
by focusing on global features, i.e., regularities in
HTML tag sequences. We employed a clustering
framework to implement a flexible regularity detec-
tion system that is robust to noise.
3.2 STEP 1: Separator Categorization
The algorithm classifies each block relation into one
of three classes: NON-BOUNDARY, RELATING,
122
[ About,  Me,  NAME,  John,  Smith, AGE, ? ]
List of Blocks
NON-BOUNDARY RELATING UNRELATING
NON-BOUNDARYRELATING
Figure 4: An Example of Separator Categorization.
and UNRELATING. Both RELATING and UNRE-
LATING can be considered to be boundaries; how-
ever, blocks that sandwich RELATING separators
are regarded to consist of a header and its modified
block. Figure 4 shows an example of separator cate-
gorization for the list of blocks in Figure 1.
The left block of a RELATING separator must be
in the smaller depth than the right block. Figure 2
shows an example. In this tree, NAME is in a smaller
depth than John. On the other hand, both the left
and right blocks in a NON-BOUNDARY separator
must be in the same depth in a tree representation,
for example, John and Smith in Figure 2.
3.2.1 Local Model
We use a probabilistic model that assumes the lo-
cality of relations among separators and blocks. In
this model, each separator   and the strings around
it,  and , are modeled by means of the hidden vari-
able , which indicates the class in which   is cate-
gorized. We use the character zerogram, unigram, or
bigram (changed according to the number of appear-
ances2) for  and  to avoid data sparseness prob-
lems.
For example, let us consider the following part of
the example document:
NAME: John Smith.
In this case, : is a separator, ME is the left string and
Jo is the right string.
Assuming the locality of separator appearances,
the model for all separators in a given document set
is defined as       
 
      where   is a
vector of left strings,  is a vector of separators, and
 is a vector of right strings.
The joint probability of obtaining ,  , and  is
                    
assuming that  and  depend only on : a class of
relation between the blocks around  .34
2This generalization is performed by a heuristic algorithm.
The main idea is to use a bigram if its number of appearances is
over a threshold, and unigrams or zerograms otherwise.
3If the frequency for     is over a threthold,       is
used instead of        .
4If the frequency for  is under a threthold,  is replaced by
its longest prefix whose frequency is over the threthold.
Based on this model, each class of separators is
determined as follows:
  
 
             
The hidden parameters     ,    , and
   , are estimated by the EM algorithm (Demp-
ster et al, 1977). Starting with arbitrary initial pa-
rameters, the EM algorithm iterates E-STEPs and
M-STEPs in order to increase the (log-)likelihood
function 	
      

	
      .
To characterize each class of separators, we use a
set of typical symbols and HTML tags, called rep-
resentatives from each class. This constraint con-
tributes to give a structure to the parameter space.
3.3 STEP 2: Block Clustering
The purpose of block clustering is to take advantage
of the regularity in visual representations. For exam-
ple, we can observe regularity between NAME and
AGE in Figure 1 because both are sandwiched by the
character * and preceded by a null line. This visual
representation is described in the HTML source as,
for example,
... <br><br>* NAME *<br> ...
... <br><br>* AGE *<br> ...
Our idea is to define the similarities between (con-
text of) blocks based on the similarities between
their surrounding separators. Each separator is rep-
resented by the vector that consist of symbols and
HTML tags included in it, and the similarity be-
tween separators are calculated as cosine values.
The algorithm proceeds in a bottom-up manner by
examining a given block list from tail to head, find-
ing the block that is the most similar to the current
block, and collecting them into the same cluster. Af-
ter that, all blocks in the same cluster is assigned the
same depth.
4 Preliminary Experiments
We used a training data that consists of 1,418 web
documents5 of moderate file size6 that did not have
?src? or ?script? tags7. The former criteria is based
on the observation that too small or too large doc-
uments are hard to use for measuring performance
of algorithms, and the latter criteria is caused by the
fact our system currently has no module to handle
image files as blocks.
We randomly selected 20 documents as test doc-
uments. Each test document was bracketed by hand
5They are collected by retrieving all user pages on one server
of a Japanese ISP.
6from 1,000 to 10,000 bytes
7Src tags indicate inclusion of image files, java codes, etc
123
Algorithm Recall Precision F-measure
OUR ALGORITHM 0.477 0.266 0.329
NO-CL 0.178 0.119 0.139
NO-EM 0.389 0.211 0.265
PREV 0.144 0.615 0.202
Table 1: Macro-Averaged Recall, Precision, and F-
measure on Test Documents
to evaluate machine-made bracketings. The per-
formance of web-page structuring algorithms can
be evaluated via the nested-list form of tree by
bracketed recall and bracketed precision (Goodman,
1996). Recall is the rate that bracketing given by
hand are also given by machine, and precision is the
rate that bracketing given by machine are also given
by hand. F-measure is a harmonic mean of recall and
precision that is used as a combined measure. Recall
and precision were evaluated for each test document
and they were averaged across all test documents.
These averaged values are called macro-average re-
call, precision, and f-measure (Yang, 1999).
We implemented our algorithm and the following
three ones as baselines.
NO-CL does not perform block clustering.
NO-EM does not perform the EM-parameter-
estimation. Every boundary but representatives
is defined to be categorized as ?UNRELAT-
ING?.
PREV performs neither the EM-learning nor the
block clustering. Every boundary but represen-
tatives is defined to be categorized as ?NON-
BOUNDARY?8. It uses the heuristics that ?ev-
ery block depends on its previous block.?
Table 1 shows the result. We observed that use of
both the EM-learning and block clustering resulted
in the best performance. NO-EM performs the best
among the three baselines. It suggests that only rely-
ing on HTML tag information is not a so bad strat-
egy when the EM-training is not available because
of, for example, the lack of a sufficient number of
training examples.
Results on the documents that were rich in HTML
tags with highly coherent layouts were better than
those on the others like the documents with poor
separators such as only one space character or one
line feed. Some of the current results on the doc-
uments with such poor visual cues seemed difficult
for use in practical systems, which indicates our sys-
tem still leaves room for improvement.
8This strategy is based on the fact that it maximized the per-
formance in a preliminary investigation.
5 Conclusions and Future Work
This paper proposed a method for reformatting web
documents by extracting header trees that give hi-
erarchical structures of web documents. Prelimi-
nary experiments showed that the proposed algo-
rithm was effective compared with some baseline
methods. However, the performance of the algo-
rithm on some of the test documents was not suf-
ficient for practical use. We plan to improve the
performance by, for example, using larger amount
of training examples. Finding other reformatting
strategies in addition to the ones proposed in this pa-
per is also important future work.
References
Chia-Hui Chang and Shao-Chen Lui. 2001. IEPAD: In-
formation extraction based on pattern discovery. In
Proceedings of WWW2001, pages 681?688.
Christina Yip Chung, Michael Gertz, and Neel Sundare-
san. 2002. Reverse engineering for web data: From
visual to semantic structures. In ICDE.
Valter Crescenzi, Giansalvatore Mecca, and Paolo Meri-
aldo. 2001. ROADRUNNER: Towards automatic data
extraction from large web sites. In Proceedings of
VLDB ?01, pages 109?118.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of Royal Statistical Society: Series
B, 39:1?38.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In Proceedings of ACL96, pages 177?183.
Saikat Mukherjee, Guizhen Yang, Wenfang Tan, and I.V.
Ramakrishnan. 2003. Automatic discovery of seman-
tic structures in HTML documents. In Proceedings of
ICDAR 2003.
Tomoyuki Nanno, Suguru Saito, and Manabu Okumura.
2003. Structuring web pages based on repetition of
elements. In Proceedings of WDA2003.
Yudong Yang and Hongjiang Zhang. 2001. HTML page
analysis based on visual cues. In Proceedings of IC-
DAR01.
Yiming Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. INRT, 1:69?90.
124
