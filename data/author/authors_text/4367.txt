Proceedings of the Workshop on Computational Approaches to Figurative Language, pages 1?4,
Rochester, NY, April 26, 2007. c?2007 Association for Computational Linguistics
Lexical Influences on the Perception of Sarcasm   Roger J. Kreuz Gina M. Caucci Department of Psychology Department of Psychology The University of Memphis The University of Memphis Memphis, TN 38152 Memphis, TN 38152 rkreuz@memphis.edu gcaucci@memphis.edu     
Abstract Speakers and listeners make use of a vari-ety of pragmatic factors to produce and identify sarcastic statements. It is also possible that lexical factors play a role, al-though this possibility has not been inves-tigated previously. College students were asked to read excerpts from published works that originally contained the phrase said sarcastically, although the word sar-castically was deleted. The participants rated the characters? statements in these excerpts as more likely to be sarcastic than those from similar excerpts that did not originally contain the word sarcasti-cally. The use of interjections, such as gee or gosh, predicted a significant amount of the variance in the participants? ratings of sarcastic intent. This outcome suggests that sarcastic statements may be more formulaic than previously realized. It also suggests that computer software could be written to recognize such lexical factors, greatly increasing the likelihood that non-literal intent could be correctly interpreted by such programs, even if they are unable to identify the pragmatic components of nonliteral language. 
1 Introduction It has long been assumed that verbal irony, or sar-casm, is principally a pragmatic phenomenon, and many studies from psycholinguistics have demon-
strated the social, contextual, and interpersonal factors that affect its use and interpretation (for reviews, see Gibbs, 1994, 2003; Giora, 2003). An example of such a pragmatic factor is com-mon ground (Clark, 1996). The more familiar two people are with one other, the more likely it is that they will employ sarcasm (Kreuz, 1996). When interlocutors in a story share common ground, ex-perimental participants read sarcastic statements more quickly, and are more certain of the sarcastic intent, than when the interlocutors share little common ground (Kreuz and Link, 2002). These results can be explained in terms of a principle of inferability: speakers will only employ sarcasm if they are reasonably certain that their hearers will interpret it correctly (Kreuz, 1996). Such results have led to pessimistic forecasts concerning the likelihood that computer programs would ever be able to understand nonliteral lan-guage (e.g., Dews and Winner, 1997). If the use of such language relies solely on pragmatic factors, it would indeed be a considerable challenge to create software that could detect and interpret it. One difficulty with this conclusion is that most psycholinguistic studies of sarcasm have used ex-perimenter-generated materials instead of actual utterances. For example, sarcastic statements are often hyperbolic (Kreuz and Roberts, 1995), and so researchers have typically employed extreme con-structions, such as What perfectly lovely weather! as sarcastic commentary on a sudden downpour (Kreuz and Glucksberg, 1989). Such research, however, has unintentionally con-founded the pragmatic and the lexical aspects of sarcasm. It may be the case that particular words or collocations (e.g., perfectly lovely) serve as a cue 
1
for sarcasm by themselves. Previous research has not attempted to tease apart these lexical and pragmatic factors, even though the importance of lexical factors has been suggested previously. Kreuz and Roberts (1995) proposed that colloca-tions consisting of extreme adjectives and adverbs (e.g., simply amazing, or absolutely fantastic) may serve as a conventional way of signaling ironic intent. This idea has been expanded by Utsumi (2000), who suggested that such verbal cues pro-vide a way of implicitly displaying negative atti-tudes via sarcasm. Of course, interlocutors in face-to-face conversa-tions can rely upon both verbal and nonverbal cues to signal ironic intent (e.g., rolling of the eyes, heavy stress, or slow speaking rate). The authors of narratives must cue their readers without recourse to such conventions. The methods used by authors, therefore, might provide a way to assess the con-tribution of lexical factors to the perception of sar-casm. The goal of the present research was to deter-mine whether specific lexical factors (e.g., the use of certain parts of speech, or punctuation) reliably predict readers? perceptions of sarcasm. Unlike most previous research on sarcasm, the experimen-tal materials were drawn from published narra-tives. 2 Method Participants were asked to read excerpts from longer narratives, and then to rate how likely it was that the speaker was being sarcastic. 2.1 Materials Google Book Search was used to find instances of the phrase said sarcastically. This resource, for-merly known as Google Print, contains more than 100,000 published works that are either in the pub-lic domain, or have been provided by publishers. A wide variety of genres is represented (e.g., histori-cal novels, romance novels, and science fiction).  The phrase said sarcastically was found hun-dreds of times in the corpus, and 100 of these in-stances were randomly selected for the study. Fifteen control texts were also selected at random from the Google Book Search corpus. Five of the control items contained the phrase I said, five con-tained he said, and five contained she said. 
In order to create experimental materials, we ex-cerpted the entire paragraph that the key phrase appeared in, as well as the two paragraphs of con-text appearing above and below. The excerpts var-ied considerably in length, but the mean length for the 115 excerpts was 110 words (SD = 58). The phrase that the collocation said sarcasti-cally referred to was emphasized in bold-faced type. If the phrase appeared at the end of a sen-tence, only the words that occurred before it within quotation marks were made bold. If the sentence continued after the phrase said sarcastically, the following words in quotation marks were also made bold. Finally, the word sarcastically was re-moved, leaving just the phrase [speaker] said. The speakers? statements in the control excerpts were made bold using the same procedure, ensuring that the two sets of excerpts were identical in appear-ance. The mean length of the bold-faced phrases for all the excerpts was 6.45 words (SD = 8.05). Each excerpt was printed on a separate page, along with three questions. The first question asked How likely is it that the speaker was being sarcas-tic? A seven-point scale, with endpoints labeled not at all likely and very likely, appeared below the question. A second question asked Why do you think so? Two blank lines were provided for the participants? responses. Finally, the participants were asked How certain are you that the speaker was being sarcastic? A seven-point scale, with endpoints labeled not at all certain and very cer-tain, appeared below the question. Five different sets of sarcasm excerpts (20 per set) were created. Booklets were constructed by randomly interspersing the subset of sarcasm ex-cerpts with all of the control excerpts. The order of pages was randomized for each participant.  2.2 Coding  Two judges independently coded the excerpts on three dimensions: (1) Presence of adjectives and adverbs. Following Kreuz and Roberts (1995) and Utsumi (2000), the judges identified the use of adjectives or adverbs in the bold-faced segments of each ex-cerpt. The coding was binary: 0 for none, and 1 for one or more adjectives and adverbs. (2) Presence of interjections. Certain terms, such as gee or gosh, are used for the expression of emotion, and may also serve as a cue for nonliteral 
2
intent. The excerpts were again coded in a binary fashion. (3) Use of punctuation. Exclamation points in-dicate emphasis, which may be a signal of nonlit-eral intent. Question marks are used in tag questions (e.g., You really showed him, didn?t you?), which are often rhetorical and nonliteral (Kreuz et al, 1999). The use of either an exclama-tion point or question mark was coded in a binary fashion. The agreement between the judges? coding was 95% across all excerpts. The small number of dis-agreements was primarily the result of variability in how dictionaries define interjections. All dis-agreements were resolved through discussion. 2.3 Procedure Participants were 101 undergraduates at a large public university. They received course credit for their participation. The participants were tested in small groups, and asked to work through the book-lets at their own pace. Each participant read and answered questions for 35 excerpts: 20 sarcasm excerpts, and all 15 control excerpts (only a subset of the sarcasm materials was given to each partici-pant to offset fatigue effects). The term sarcasm was not defined for the par-ticipants, and they were asked to rely solely on their intuitive understanding of the term. (Previous research with the same population suggests that a fairly high level of agreement exists for the con-cept of sarcasm; see Kreuz, Dress, and Link, 2006). 3 Results  Only the responses from the first question (likeli-hood that the speaker is being sarcastic) will be discussed here. For each participant, a mean score for the 100 sarcasm and 15 control excerpts was computed. As expected, the sarcasm excerpts re-ceived higher scores (M = 4.85, SD = .67) than the control excerpts (M = 2.89, SD = .86), and the dif-ference was significant, t (100) = 19.35, p < .001. This means that the participants had sufficient con-text for determining sarcastic intent in the test ex-cerpts, and that the participants were able to distinguish between the two groups of excerpts. To determine the relative importance of the lexi-cal factors on the perception of sarcasm, a regres-
sion analysis was performed. The criterion variable was the mean sarcasm rating for each excerpt. Five predictor variables were employed: (1) the number of words in each excerpt, (2) the number of bold-faced words in each excerpt, (3) the presence of adjectives and adverbs, (4) the presence of interjec-tions, and (5) the use of exclamation points and question marks. Variables 3 to 5 were coded in a binary fashion, as described in section 2.2. Ratings for both the sarcastic and the control excerpts were entered. The number of words and number of bold-faced words are theoretically uninteresting variables, so they were forced into the equation first as one block. The three predictor variables of interest were entered in a second block using a stepwise method. The first block, containing the two length vari-ables, failed to account for a significant amount of the variance, F (2,112) = 1.37, n.s., R2 = .024. This was a desirable outcome, because it meant that par-ticipants were not influenced in their judgments by the lengths of the excerpts they were reading, with longer excerpts providing more contextual cues. For the second block with the three variables of interest, only the presence of interjections entered into the equation, F (1, 111) = 6.10, p = .015, R2 = .051. The presence of adjectives and adverbs, and the use of punctuation, failed to predict a signifi-cant amount of the variance in the participants? ratings of sarcastic intent.  4 Discussion  Previous theory and research has largely ignored the potential role of lexical factors in the delivery and detection of sarcasm. This bias has been rein-forced by the use of experimenter-generated mate-rials that may have obscured the contributions made by these factors. This study is the first to as-sess the importance of such lexical factors, using ecologically valid materials. On the one hand, the amount of variance ac-counted for by lexical factors was rather small: just 5%. On the other hand, it must be remembered that the excerpts themselves were taken from book-length works, so the participants only had a frac-tion of the original context with which to deter-mine the intent of the (potentially) sarcastic statement. Nevertheless, the participants were able to reliably differentiate between the sarcastic and 
3
control excerpts, which suggests that specific local factors were influencing their judgments. In addition, it must be remembered that only a small number of lexical factors was assessed, and in a fairly coarse way (i.e., with binary coding). Out of just three such factors, however, the use of interjections was a significant predictor of the par-ticipants? ratings. An inspection of the excerpts suggests that certain formulaic expressions (e.g., thanks a lot, good job), foreign terms (e.g., au con-traire), rhetorical statements (e.g., tell us what you really think), and repetitions (e.g., perfect, just per-fect) are also common in sarcastic statements. However, the set of excerpts was not large enough to allow an analysis of these expressions. A large online corpus would permit the identification of many such collocations, but determining whether the phrases were actually intended sarcastically would be more difficult than in the present study. One could argue that the use of the phrase said sarcastically reflects poorly on the authors them-selves. Ideally, a writer would not need to be so explicit about a character?s intentions: it should be clear from the context that the statement was in-tended nonliterally. However, an author is writing for an indeterminate audience that may exist in the present or in some future time. It should not be surprising, therefore, that authors occasionally feel the need to use such a phrase, and this reflects how difficult it is to communicate nonliteral intent clearly. It should also be noted, however, that some of the authors used the word sarcastically rather broadly, as a synonym for angrily or jokingly, even when the statement was intended literally. This suggests that the use of this term may be undergo-ing some change (see Nunberg, 2001 for a similar claim). Finally, these results have important implica-tions for software programs that attempt to ?under-stand? natural language. Nonliteral language presents formidable challenges for such programs, since a one-to-one mapping of words to meaning will not lead to a correct interpretation (e.g., Gee, I just love spending time waiting in line). However, the present results suggest that, in some contexts, the use of interjections, and perhaps other textual factors, may provide reliable cues for identifying sarcastic intent.  
References  Herbert H. Clark, 1996. Using Language. Cambridge: Cambridge University Press. Shelly Dews and Ellen Winner, 1997. Attributing mean-ing to deliberately false utterances: The case of irony. In C. Mandell and A. McCabe (Eds.), The Problem of Meaning: Behavioral and Cognitive Perspectives. Amsterdam: Elsevier. Raymond W. Gibbs, Jr., 1994. The Poetics of Mind: Figurative Thought, Language, and Understanding. Cambridge: Cambridge University Press. Raymond W. Gibbs, Jr., 2003. Nonliteral speech acts in text and discourse. In A. C. Graesser, M. A. Gerns-bacher, and S. R. Goldman (Eds.), Handbook of Dis-course Processes (pp. 357-393). Mahwah, NJ: Lawrence Erlbaum Associates. Rachel Giora, 2003. On our Mind: Salience, Context, and Figurative Language. New York: Oxford Uni-versity Press.  Roger J. Kreuz, 1996. The use of verbal irony: Cues and constraints. In J. S. Mio and A. N. Katz (Eds.), Meta-phor: Implications and Applications (pp. 23-38). Mahwah, NJ: Lawrence Erlbaum Associates Roger J. Kreuz, Megan L. Dress, and Kristen E. Link, 2006, July. Regional Differences in the Spontaneous Use of Sarcasm. Paper presented at the annual meet-ing of the Society for Text and Discourse, Minneapo-lis, MN. Roger J. Kreuz and Sam Glucksberg, 1989. How to be sarcastic: The echoic reminder theory of verbal irony. Journal of Experimental Psychology: General, 118:374-386. Roger J. Kreuz, Max A. Kassler, Lori Coppenrath, and Bonnie McLain Allen, 1999. Tag questions and common ground effects in the perception of verbal irony. Journal of Pragmatics, 31:1685-1700. Roger J. Kreuz and Kristen E. Link, 2002. Asymmetries in the use of verbal irony. Journal of Language and Social Psychology, 21:127-143. Roger J. Kreuz and Richard M. Roberts, 1995. Two cues for verbal irony: Hyperbole and the ironic tone of voice. Metaphor and Symbolic Activity, 10:21-31. Geoffrey Nunberg, 2001. The Way we Talk Now: Com-mentaries on Language and Culture. Boston: Hough-ton Mifflin Akira Utsumi, 2000. Verbal irony as implicit display of ironic environment: Distinguishing ironic utterances from nonirony. Journal of Pragmatics, 32:1777-1806. 
4
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 241?249, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Reinforcing a Ranking Algorithm with Sense 
Frequencies and Multidimensional Semantic Resources to solve 
Multilingual Word Sense Disambiguation 
 
Yoan Guti?rrez, Yenier 
Casta?eda, Andy Gonz?lez, 
Rainel Estrada, Dennys D. Piug, 
Jose I. Abreu, Roger P?rez 
Antonio Fern?ndez Orqu?n, 
Andr?s Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas DLSI, University of Alicante Independent Consultant 
Matanzas, Cuba Alicante, Spain USA 
{yoan.gutierrez, 
yenier.castaneda, 
rainel.estrada, 
dennys.puig, jose.abreu, 
roger.perez}@umcc.cu, 
andy.gonzalez@infonet.umcc
.cu 
antonybr@yahoo.com, 
{montoyo,rafael}@dlsi.ua.
es 
info@franccamara.c
om 
 
Abstract 
This work introduces a new unsupervised 
approach to multilingual word sense 
disambiguation. Its main purpose is to 
automatically choose the intended sense 
(meaning) of a word in a particular context for 
different languages. It does so by selecting the 
correct Babel synset for the word and the 
various Wiki Page titles that mention the 
word. BabelNet contains all the output 
information that our system needs, in its Babel 
synset. Through Babel synset, we find all the 
possible Synsets for the word in WordNet. 
Using these Synsets, we apply the 
disambiguation method Ppr+Freq to find what 
we need. To facilitate the work with WordNet, 
we use the ISR-WN which offers the 
integration of different resources to WordNet. 
Our system, recognized as the best in the 
competition, obtains results around 69% of 
Recall. 
1 Introduction 
Word Sense Disambiguation (WSD) focuses on 
resolving the semantic ambiguity of a given word.  
This is an important task in Natural Language 
Processing (NLP) because in many applications, 
such as Automatic Translation, it is essential to 
know the exact meaning of a word in a given 
context. In order to solve semantic ambiguity, 
different systems have been developed. However, 
we can categorize them in two main groups: 
supervised and unsupervised systems. The 
supervised ones need large quantity of hand-tagged 
data in order to gather enough information to build 
rules, train systems, and so on. Unsupervised 
systems, on the other hand, do not need such a 
large amount of hand-tagged datasets. This means 
that, when there aren?t enough corpora to train the 
systems, an unsupervised system is a good option. 
A sub-task of WSD is Multilingual Word Sense 
Disambiguation (MWSD) (Navigli et al, 2013) 
that aims at resolving ambiguities in different 
languages. 
In a language, there are words that have only one 
sense (or meaning), but in other languages, the 
same words can have different senses. For 
example, ?patient? is a word that in English can be 
either a noun or an adjective, but in German, it 
only has one sense - ?viz? (a person that needs 
treatment). This shows that the information 
obtained by combining two languages can be more 
useful for WSD because the word senses in each 
language can complement each other. For it to be 
useful, MWSD needs a multilingual resource that 
contains different languages, such as BabelNet 
(Navigli and Ponzetto, 2010; 2012) and 
EuroWordNet (Vossen, 1998). 
241
As the preferred disambiguation method, we 
decided to use the Ppr+Freq (Personalized Page 
Rank combined with Frequencies of senses)  
(Guti?rrez, 2012) method because, among 
unsupervised systems, graph-based methods have 
obtained more promising results.  
It is worth mentioning the relevant approaches 
used by the scientific community to achieve 
promising results. One approach used is structural 
interconnections, such as Structural Semantic 
Interconnections (SSI), which create structural 
specifications of the possible senses for each word 
in a context (Navigli and Velardi, 2005). The other 
approaches used are ?Exploring the integration of 
WordNet? (Miller et al, 1990), FrameNet (Laparra 
et al, 2010) and those using Page-Rank such as 
(Sinha and Mihalcea, 2007) and (Agirre and Soroa, 
2009). 
The aforementioned types of graph based 
approaches have achieved relevant results in both 
the SensEval-2 and SensEval-3 competitions (see 
Table 1). 
Algorithm Recall 
TexRank (Mihalcea, 2005)  54.2% 
(Sinha and Mihalcea, 2007) 56.4% 
(Tsatsaronis et al, 2007) 49.2% 
Ppr (Agirre and Soroa, 2009) 58.6% 
Table 1. Relevant WSD approaches. Recall measure is 
calculated recalls using SensEval-2 (English All Word 
task) guidelines over. 
Experiments using SensEval-2 and SensEval-3 
corpora suggest that Ppr+Freq (Guti?rrez, 2012) 
can lead to better results by obtaining over 64% of 
Recall. Therefore we selected Ppr+Freq as the 
WSD method for our system. 
The key proposal for this work is an 
unsupervised algorithm for MWSD, which uses an 
unsupervised method, Ppr+Freq, for semantic 
disambiguation with resources like BabelNet (as 
sense inventory only) (Navigli and Ponzetto, 2010) 
and ISR-WN (as knowledge base) (Guti?rrez et al, 
2011a; 2010a). 
ISR-WN was selected as the default knowledge 
base because of previous NLP research, which 
included: (Fern?ndez et al, 2012; Guti?rrez et al, 
2010b; Guti?rrez et al, 2012; 2011b; 2011c; 
2011d), which achieved relevant results using ISR-
WN as their knowledge base. 
2 System architecture  
By using one of BabelNet (BN) features, our 
technique begins by looking for all the Babel 
synsets (Bs) linked to the lemma of each word in 
the sentence that we need to disambiguate.  
Through the Bs offsets, we can get its 
corresponding WordNet Synset (WNS), which 
would be retrieved from WordNet (WN) using the 
ISR-WN resource. As a result, for each lemma, we 
have a WordNet Synset List (WNSL) from which 
our Word Sense Disambiguation method obtains 
one WNS as the correct meaning. 
Our WSD method consists of applying a 
modification of the Personalizing PageRank (Ppr) 
algorithm (Agirre and Soroa, 2009), which 
involves the senses frequency. More specifically, 
the key proposal is known as Ppr+Freq (see 
Section 2.3).  
Given a set of WNSLs of WNSL, as words 
window, we applied the Synsets ranking method, 
Ppr+Freq, which ranks in a descending order, the 
Synsets of each lemma according to a calculated 
factor of relevance. The first Synset (WNS) of 
each WNSL (the most relevant) is established as 
the correct one and its associated Babel synset (Bs) 
is also tagged as correct. To determine the Wiki 
Page Titles (WK), we examine the WIKI 
(Wikipedia pages) and WIKIRED (Wikipedia 
pages redirections) in the correct Babel synset 
obtained. 
Figure 1 shows a general description of our 
system that is made up of the following steps: 
I. Obtaining lemmas  
II. Obtaing WN Synset of selected lemmas  
III. Applying Ppr+Freq method  
IV. Assigning Synset, Babel synset and Wiki 
page title 
Note that ISR-WN contains WN as its nucleus. 
This allows linking both resources, BabelNet and 
ISR-WN.
242
 
Figure 1. General process description taking as instance a sentence provided by the trial dataset. 
 
2.1 Obtaining lemmas  
For each input sentence, we extract the labeled 
lemmas. As an example, for the sentence, ?The 
struggle against the drug lords in Colombia will be 
a near thing,? the selected lemmas are: ?struggle,? 
?drug_lord,? ?Colombia?, and ?near_thing.? 
 
Figure 2. Obtaining synset of lemmas. 
 
2.2 Obtaing WN Synset of selected lemmas  
For each lemma obtained in the previous section, 
we look through BabelNet to recover the Bs that 
contains the lemma among its labels. When BSs 
are mapped to WN, we use the ISR-WN resource 
to find the corresponding Synset. Since a lemma 
can appear in a different Bs, it can be mapped with 
several WNS. Thus, we get a Synset list for each 
lemma in the sentence. In case the lemma does not 
have an associated Bs, its list would be empty. An 
example of this step is shown on Figure 2. 
2.3 Applying Ppr+Freq method 
In the above case, Ppr+Freq modifies the ?classic? 
Page Rank approach instead of assigning the same 
weight for each sense of WN in the disambiguation 
graph (??). 
The PageRank (Brin and Page, 1998) 
adaptation, Ppr , which was popularized by (Agirre 
IV . Assigning Synset, Babel Synset and Wiki page title
? The struggle against the drug lords in Colombia will be a near thing .?
struggle drug_lord Colombia near_thing
Wikipedia WordNet BabelNet
ISR-WN
WordNet
(WN)
SUMO
WN-Domain
WN-Affect
SemanticClass eXtended WN3.0
eXtended WN1.7
struggle%1:04:01:: drug_lord%1:18:00:: colombia%1:15:00:: near_thing%1:04:00::
bn:00009079n bn:00028876n bn:00020697n bn:00057109n
-- Drug_Lord Colombia --
I. Obtaing lemmas
II. Obtaining Synset of selected lemmas
III. Applying Ppr+Freq method
WN key
BS
WK
struggle
drug_lord Colombia
near_thing
struggle
bn:00074762n wn:00587514n
bn:00009079n wn:00739796n
bn:00009080n wn:00901980n
drug_lord bn:00028876n wn:09394468n
colombia
bn:00020697n wn:08196765n
bn:02051949n
bn:02530766n
near_thing bn:00057109n wn:00193543n
Sentence lemmas 
Babel synset 
WordNet synset 
243
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main idea 
behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ? to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the more 
strength the voter would have. Thus, PageRank is 
generated by applying a random walkthrough from 
the internal interconnection of ?, where the final 
relevance of ??  represents the random walkthrough 
probability over ?, and ending on ??. 
Ppr+Freq includes the existent semantic and 
frequency patterns of each sense of the word to 
disambiguate while finding a way to connect each 
one of these words in a knowledge base. 
The new graph-based approach of WSD 
generates a graph of disambiguated words for each 
input sentence. For that reason, it is necessary to 
classify the word senses according to the other 
words that compose the context. The general 
method is shown in Figure 3. This method is 
divided into three steps: 
I. Creation of a disambiguation graph 
II. Application of Ppr+Freq in the generated 
graph 
III. Selection of the correct answer 
Creation of a disambiguation graph: In the first 
step, a disambiguation graph is built by means of a 
Breath First Search (BFS) over the ?super? graph 
composed by all the resources integrated into ISR-
WN. The components involved in this process are: 
WordNet, SUMO (Zouaq et al, 2009) WordNet 
Domains (Magnini and Cavaglia, 2000) WordNet 
Affects (Strapparava and Valitutti, 2004) Semantic 
Classes (Izquierdo et al, 2007) and eXtended 
WordNet (XWN) relations (Moldovan and Rus, 
2001). This search aims to recover all senses 
(nodes), domain labels (from WordNet Domain 
and WordNet Affects), SUMO categories, and 
Semantic Classes labels through the shortest path 
between every pair of senses in the WNSL set 
associated with the input sentence. Using ISR-WN 
as the KB, through experimentation, we obtained 
the shortest paths with a length of five edges. For a 
better understanding of this process, see (Guti?rrez, 
2012). 
Application of Ppr+Freq in the generated 
graph: In the second step, we use the weighted 
Personalized PageRank. Here, all the vertices from 
vector ? in ?? are initialized with the value  
1
?
 ; 
where ? is the number of nodes in ??. On the 
other hand, the vertices that represent word senses 
in the analyzed sentence are not initialized with 
this value. Instead, they are initialized with values 
in the range [0?1], which are associated to their 
occurrence frequency in SemCor1 (Corpus and 
sense frequencies knowledge). In the last step, 
after applying the Ppr+Freq algorithm over ??, we 
get a representative vector which contains ISR-WN 
nodes in ?? sorted in a descending order by a 
ranking score computed by this algorithm. For a 
better description, see (Guti?rrez, 2012). 
Selection of the correct answer: As the correct 
sense, we take the highest ranked sense of each 
target word involved in this vector. Note that 
domain labels, SUMO categories, semantic class 
labels, and affect labels are ranked too. They could 
be used in the future to determine relevant 
conceptualizations that would be useful for text 
classification and more. 
In our system, we assume the following 
configuration: dumping factor ? = 0.85 and like in 
(Agirre and Soroa, 2009) we used 30 iterations. A 
detailed explanation about PageRank algorithm 
can be found in (Agirre and Soroa, 2009). 
Table 2 shows an example that analyzes the 
Synset for each word in the sentence and also 
shows how the higher ranked Synsets of the target 
words are selected as the correct ones. For a 
detailed explanation of Ppr+Freq, see (Guti?rrez, 
2012). 
2.4 Assigning Synset, Babel synset and Wiki 
Pages 
In this step, English is handled differently from 
other languages because WordNet Synsets are 
available only for English. The following sections 
explain how we proceed in each case. Once the 
Synsets list is obtained for each lemma in section 
2.3, selecting the correct answer for the lemma is 
all that?s left to do. 
                                                     
1 http://www.cse.unt.edu/~rada/downloads.html 
244
 
Figure 3. General process of WSD with Ppr+Freq. 
2.4.1 English 
Given a lemma, we go through its Synset list from 
beginning to end looking for the first Synset that 
contains a key2 for the lemma. If such Synset 
exists, it is designated as the Synset for the lemma. 
Otherwise, no Synset is assigned. 
As already explained, each Synset in the list is 
connected to a Bs. Therefore, the lemma linked 
with the correct WNS selected in the previous step, 
is chosen as the correct lemma. In case no Synsets 
were designated as the correct ones, we take the 
first Bs in BN, which contains the lemma among 
its labels.  
To determine the Wiki pages titles (WK) we 
examine the WIKIRED and WIKI labels in the 
correct Bs selected in the preceding step. This 
search is restricted only to labels corresponding to 
the analyzed language and discriminating upper 
and lower case letters. Table 2 shows some sample 
results of the WSD process. 
Lemma struggle drug_lord 
WNS 00739796n 09394468n 
WN key struggle%1:04:01:: drug_lord%1:18:00:: 
Bs bn:00009079n bn:00028876n 
WK - Drug_Lord 
Lemma colombia near_thing 
WNS 08196765n 00193543n 
WN key colombia%1:15:00:: near_thing%1:04:00:: 
Bs bn:00020697n bn:00057109n 
WK Colombia - 
Table 2 : Example of English Language. 
                                                     
2A sense_key is the best way to represent a sense in 
semantic tagging or other systems that refer to WordNet 
senses. sense_key?s are independent of WordNet sense 
numbers and synset_offset?s, which vary between versions of 
the database. 
2.4.2 Other languages  
For this scenario, we introduce a change in the first 
step discussed in the previous section. The reason 
is that the Synsets do not contain any keys in any 
other language than English. Thus, the correct 
Synset for the lemma is the first in the Synset list 
for the lemma obtained, as described, in section 
2.3. 
3 Results 
We tested three versions (runs) of the proposed 
approach and evaluated them through a trial 
dataset provided by Task123 of Semeval-2013 
using babelnet-1.0.1. Table 3 shows the result for 
each run. Note that the table results were 
calculated with the traditional WSD recall 
measure, being this measure which has ranked 
WSD systems on mostly Semeval competitions. 
On the other hand, note that our precision and 
recall results are different because the coverage is 
not 100%. See Table 5. 
 English French 
Runs WNS Bs WK Bs WK 
Run1 0.70 0.71 0.77 0.59 0.85 
Run2 0.70 0.71 0.78 0.60 0.85 
Run3 0.69 0.70 0.77 - - 
Table 3 : Results of runs with trial recall values. 
As can be noticed on Table 3, results of different 
versions do not have big differences, but in 
general, Run2 achieves the best results; it?s better 
                                                     
3 http://www.cs.york.ac.uk/semeval-2013/task12 
ISR-WN
footballer#1 | cried#9 | winning#3
footballer | cry | winning
Lemmas
?The footballer cried when winning?
Disambiguation
Graph
(0,9)
Footballer#1
(0,3)
cry#7
(0,4)
cry#9
(0,2)
cry#10
(0,2)
cry#11
(0,2)
cry#12
(0,2)
winning#1
(0,3)
winning#3
Creating GD
Ppr+Freq
Selecting senses
245
than Run1 in the WK with a 78% in English and 
Bs with 60% in French. The best results are in the 
WK in French with a value of 85%. 
Since we can choose to include different 
resources into ISR-WN, it is important to analyze 
how doing so would affect the results. Table 4 
shows comparative results for Run 2 of a trial 
dataset with BabelNet version 1.1.1. 
As can be observed in Table 4, the result does not 
have a significant change even though we used the 
ISR-WN with all resources.  
A better analysis of Ppr+Freq in, as it relates to 
the influence of each resource involved in ISR-WN 
(similar to Table 4 description) assessing 
SensEval-2 and SensEval-3 dataset, is shown in 
(Guti?rrez, 2012). There are different resource 
combinations showing that only XWN1.7 and all 
ISR-WN resources obtain the highest performance. 
Other analysis found in (Guti?rrez, 2012) evaluates 
the influence of adding the sense frequency for 
Ppr+Freq.  
By excluding the Factotum Domain, we obtain 
the best result in Bs 54% for French (only 1% 
more than the version used in the competition). 
The other results are equal, with a 69% in WNS, 
66% in Bs, 64% in WK for English, and 69% in 
WK for French. 
        English French 
WN Domains Sumo Affect Factotum 
Domain 
SemanticClass XWN3.0 XWN1.7 WNS Bs WK Bs WK 
X X X X X X X X 0.69 0.66 0.64 0.53 0.69 
X X  X X X X X 0.69 0.66 0.64 0.53 0.69 
X    X X X X 0.68 0.65 0.64 0.52 0.69 
X X X X  X X X 0.69 0.66 0.64 0.54 0.69 
X X X X  X  X 0.68 0.65 0.65 0.53 0.69 
Table 4. Influence of different resources that integrate ISR-WN in our technique. 
    Wikipedia BabelNet WordNet 
System Language Precision Recall F-score Precision Recall F-score Precision Recall F-score 
MFS DE 0.836 0.827 0.831 0.676 0.673 0.686 - - - 
  EN 0.86 0.753 0.803 0.665 0.665 0.656 0.63 0.63 0.63 
  ES 0.83 0.819 0.824 0.645 0.645 0.644 - - - 
  FR 0.698 0.691 0.694 0.455 0.452 0.501 - - - 
  IT 0.833 0.813 0.823 0.576 0.574 0.572 - - - 
Run1 DE 0.758 0.46 0.572 0.619 0.617 0.618 - - - 
  EN 0.619 0.484 0.543 0.677 0.677 0.677 0.639 0.635 0.637 
  ES 0.773 0.493 0.602 0.708 0.703 0.705 - - - 
  FR 0.817 0.48 0.605 0.608 0.603 0.605 - - - 
  IT 0.785 0.458 0.578 0.659 0.656 0.657 - - - 
Run2 DE 0.769 0.467 0.581 0.622 0.62 0.621 - - - 
  EN 0.62 0.487 0.546 0.685 0.685 0.685 0.649 0.645 0.647 
  ES 0.778 0.502 0.61 0.713 0.708 0.71 - - - 
  FR 0.815 0.478 0.603 0.608 0.603 0.605 - - - 
  IT 0.787 0.463 0.583 0.659 0.657 0.658 - - - 
Run3 EN 0.622 0.489 0.548 0.68 0.68 0.68 0.642 0.639 0.64 
Table 5. Results of Runs for Task12 of semeval-2013 using the test dataset. 
 
246
3.1 Run1 
In this Run, WNSLs consist of all the target words 
involved in each sentence. This run is applied at 
the sentence level. The results for the competition 
are shown in Table 5. For this Run, the best result 
was obtained for Spanish with a 70.3% in Bs and 
49.3% in WK of Recall. As we can see, for Run1 
the precision is high for Wikipedia disambiguation, 
obtaining for French the best result of the ranking. The 
low Recall in Wikipedia is due to the exact mismatching 
of labels between our system output and the gold 
standard. This fact, affects the rest of our runs. 
3.2 Run2 
In this Run, WNSLs consist of all the target words 
involved in each domain. We can obtain the target 
words because the training and test dataset contain 
the sentences grouped by topics.  For instance, for 
English, 13 WNSLs are established. This Run is 
applied at the corpora level. The results for the 
competition are shown in Table 5. It is important to 
emphasize that our best results ranked our 
algorithm as first place among all proposed 
approaches for the MWSD task. 
For this run, the best Recall was obtained for 
Spanish with a 70.8% in Bs and 50.2% in WK. 
This Run also has the best result of the three runs. 
For the English competition, it ended up with a 
64.5% in WNS, 68.5% in Bs, and 48.7% in WK. 
This Run obtained promising results, which took 
first place in the competition. It also had better 
results than that of the First Sense (Most Frequent 
Sense) baseline in Bs results for all languages, 
except for German. In Bs, it only obtained lower 
results in German with a 62% of Recall for our 
system and 67.3% for the First Sense baseline. 
3.3 Run3 
In this run, WNSLs consist of all the words 
included in each sentence. This run uses target 
words and non-target words of each sentence, as 
they are applied to the sentence level. The results 
for the competition are shown in Table 5.  
As we can see, the behavior of this run is similar 
to the previous runs. 
4 Conclusions and Future work  
The above results suggest that our proposal is a 
promising approach. It is also important to notice 
that a richer knowledgebase can be built by 
combining different resources such as BabelNet 
and ISR-WN, which can lead to an improvement 
of the results. Notwithstanding, our system has 
been recognized as the best in the competition, 
obtaining results around 70% of Recall. 
According to the Task12 results4, only the 
baseline Most Frequent Sense (MFS) could 
improve our scores in order to achieve better WK 
and German (DE) disambiguation. Therefore, we 
plan to review this point to figure out why we 
obtained better results in other categories, but not 
for this one. At the same time, further work will 
use the internal Babel network to run the Ppr+Freq 
method in an attempt to find a way to enrich the 
semantic network obtained for each target sentence 
to disambiguate. On top of that, we plan to 
compare Ppr (Agirre and Soroa, 2009) with 
Ppr+Freq using the Task12 dataset. 
Availability of our Resource 
In case researchers would like to use our resource, 
it is available at the GPLSI5 home page or by 
contacting us via email. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), "An?lisis 
de Tendencias Mediante T?cnicas de Opini?n 
Sem?ntica" (TIN2012-38536-C03-03) and 
?T?cnicas de Deconstrucci?n en la Tecnolog?as del 
Lenguaje Humano? (TIN2012-31224); and by the 
Valencian Government through the project 
PROMETEO (PROMETEO/2009/199). 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 12th 
conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. 
                                                     
4 http://www.cs.york.ac.uk/semeval-
2013/task12/index.php?id=results 
5 http://gplsi.dlsi.ua.es/ 
247
Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez; A. 
Gonz?lez; R. Estrada; Y. Casta?eda; S. V?zquez; A. 
Montoyo and R. Mu?oz. UMCC_DLSI: 
Multidimensional Lexical-Semantic Textual 
Similarity. {*SEM 2012}: The First Joint Conference 
on Lexical and Computational Semantics -- Volume 
1: Proceedings of the main conference and the shared 
task, and Volume 2: Proceedings of the Sixth 
International Workshop on Semantic Evaluation 
{(SemEval 2012)}, Montreal, Canada, Association 
for Computational Linguistics, 2012. 608--616 p.  
Guti?rrez, Y. An?lisis Sem?ntico Multidimensional 
aplicado a la Desambiguaci?n del Lenguaje Natural. 
Departamento de Lenguajes y Sistemas Inform?ticos. 
Alicante, Alicante, 2012. 189. p. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based on 
WordNet. XXVI Congreso de la Sociedad Espa?ola 
para el Procesamiento del Lenguaje Natural, 
Universidad Polit?cnica de Valencia, Valencia, 
SEPLN 2010, 2010a. 161-168 p. 1135-5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011a, 47: 249-257. 
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Improving 
WSD using ISR-WN with Relevant Semantic Trees 
and SemCor Senses Frequency. Proceedings of the 
International Conference Recent Advances in Natural 
Language Processing 2011, Hissar, Bulgaria, RANLP 
2011 Organising Committee, 2011b. 233--239 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Sentiment 
Classification Using Semantic Features Extracted 
from WordNet-based Resources. Proceedings of the 
2nd Workshop on Computational Approaches to 
Subjectivity and Sentiment Analysis (WASSA 
2.011), Portland, Oregon., Association for 
Computational Linguistics, 2011c. 139--145 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Word Sense 
Disambiguation: A Graph-Based Approach Using N-
Cliques Partitioning Technique. en:  Natural 
Language Processing and Information Systems. 
MU?OZ, R.;MONTOYO, A.et al Springer Berlin / 
Heidelberg, 2011d. 6716: 112-124.p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. A graph-
Based Approach to WSD Using Relevant Semantic 
Trees and N-Cliques Model. CICLing 2012, New 
Delhi, India, 2012. 225-237 p.  
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Laparra, E.; G. Rigau and M. Cuadros. Exploring the 
integration of WordNet and FrameNet. Proceedings 
of the 5th Global WordNet Conference (GWC'10), 
Mumbai, India, 2010.  
Magnini, B. and G. Cavaglia. Integrating Subject Field 
Codes into WordNet. Proceedings of Third 
International Conference on Language Resources and 
Evaluation (LREC-2000), 2000. 1413--1418 p.  
Mihalcea, R. Unsupervised large-vocabulary word sense 
disambiguation with graph-based algorithms for 
sequence data labeling. Proceedings of HLT05, 
Morristown, NJ, USA., 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Moldovan, D. I. and V. Rus Explaining Answers with 
Extended WordNet ACL, 2001. 
Navigli, R.; D. Jurgens and D. Vannella. SemEval-2013 
Task 12: Multilingual Word Sense Disambiguation. . 
Proceedings of the 7th International Workshop on 
Semantic Evaluation (SemEval 2013), in conjunction 
with the Second Joint Conference on Lexical and 
Computational Semantics (*SEM 2013), Atlanta, 
Georgia, 2013.  
Navigli, R. and S. P. Ponzetto. BabelNet: Building a 
Very Large Multilingual Semantic Network. 
Proceedings of the 48th Annual Meeting of the 
Association for Computational Linguistics, Uppsala, 
Sweden, Association for Computational Linguistics, 
2010. 216--225 p.  
Navigli, R. and S. P. Ponzetto BabelNet: The automatic 
construction, evaluation and application of a wide-
coverage multilingual semantic network Artif. Intell., 
2012, 193: 217-250. 
Navigli, R. and P. Velardi Structural Semantic 
Interconnections: A Knowledge-Based Approach to 
Word Sense Disambiguation IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 2005, 
27(7): 1075-1086. 
Sinha, R. and R. Mihalcea. Unsupervised Graph-based 
Word Sense Disambiguation Using Measures of 
Word Semantic Similarity. Proceedings of the IEEE 
International Conference on Semantic Computing 
(ICSC 2007), Irvine, CA, 2007. 
248
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Tsatsaronis, G.; M. Vazirgiannis and I. 
Androutsopoulos. Word sense disambiguation with 
spreading activation networks generated from 
thesauri. IJCAI, 2007.  
Vossen, P. EuroWordNet: A Multilingual Database with 
Lexical Semantic Networks.  Dordrecht, Kluwer 
Academic Publishers, 1998.  
Zouaq, A.; M. Gagnon and B. Ozell. A SUMO-based 
Semantic Analysis for Knowledge Extraction. 
Proceedings of the 4th Language & Technology 
Conference, Pozna?, Poland, 2009.  
 
 
249
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 443?449, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(SA): Using a ranking algorithm and informal features 
to solve Sentiment Analysis in Twitter 
Yoan Guti?rrez, Andy Gonz?lez, 
Roger P?rez, Jos? I. Abreu 
University of Matanzas, Cuba 
{yoan.gutierrez, roger.perez 
,jose.abreu}@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, 
Alejandro Mosquera, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, 
{amosquera, montoyo, 
rafael}@dlsi.ua.es 
Franc Camara 
Independent Consultant 
USA 
info@franccamara
.com 
 
Abstract 
In this paper, we describe the development 
and performance of the supervised system 
UMCC_DLSI-(SA). This system uses corpora 
where phrases are annotated as Positive, 
Negative, Objective, and Neutral, to achieve 
new sentiment resources involving word 
dictionaries with their associated polarity. As 
a result, new sentiment inventories are 
obtained and applied in conjunction with 
detected informal patterns, to tackle the 
challenges posted in Task 2b of the Semeval-
2013 competition. Assessing the effectiveness 
of our application in sentiment classification, 
we obtained a 69% F-Measure for neutral and 
an average of 43% F-Measure for positive 
and negative using Tweets and SMS 
messages. 
1 Introduction 
Textual information has become one of the most 
important sources of data to extract useful and 
heterogeneous knowledge from. Texts can provide 
factual information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions, or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention.  
Many researchers, such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working on this and related areas. 
Related to assessment Sentiment Analysis (SA) 
systems, some international competitions have 
taken place. Some of those include: Semeval-2010 
(Task 18: Disambiguating Sentiment Ambiguous 
Adjectives 1 ) NTCIR (Multilingual Opinion 
Analysis Task (MOAT 2)) TASS 3  (Workshop on 
Sentiment Analysis at SEPLN workshop) and 
Semeval-2013 (Task 2 4  Sentiment Analysis in 
Twitter) (Kozareva et al, 2013). 
In this paper, we introduce a system for Task 2 
b) of the Semeval-2013 competition. 
1.1 Task 2 Description 
In participating in ?Task 2: Sentiment Analysis in 
Twitter? of Semeval-2013, the goal was to take a 
given message and its topic and classify whether it 
had a positive, negative, or neutral sentiment 
towards the topic. For messages conveying, both a 
positive and negative sentiment toward the topic, 
the stronger sentiment of the two would end up as 
the classification. Task 2 included two sub-tasks. 
Our team focused on Task 2 b), which provides 
two training corpora as described in Table 3, and 
two test corpora: 1) sms-test-input-B.tsv (with 
2094 SMS) and 2) twitter-test-input-B.tsv (with 
3813 Twit messages). 
The following section shows some background 
approaches. Subsequently, in section 3, we 
describe the UMCC_DLSI-(SA) system that was 
used in Task 2 b). Section 4 describes the 
assessment of the obtained resource from the 
Sentiment Classification task. Finally, the 
conclusion and future works are presented in 
section 5. 
2 Background 
The use of sentiment resources has proven to be a 
necessary step for training and evaluating  systems 
that implement sentiment analysis, which also 
                                                 
1 http://semeval2.fbk.eu/semeval2.php 
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
3 http://www.daedalus.es/TASS/ 
4http://www.cs.york.ac.uk/semeval-2013/task2/ 
443
include fine-grained opinion mining (Balahur, 
2011). 
In order to build sentiment resources, several 
studies have been conducted. One of the first is the 
relevant work by (Hu and Liu, 2004) using lexicon 
expansion techniques by adding synonymy and 
antonym relations provided by WordNet 
(Fellbaum, 1998; Miller et al, 1990) Another one 
is the research described by (Hu and Liu, 2004; 
Liu et al, 2005) which obtained an Opinion 
Lexicon compounded by a list of positive and 
negative opinion words or sentiment words for 
English (around 6800 words). 
A similar approach has been used for building 
WordNet-Affect (Strapparava and Valitutti, 2004) 
which expands six basic categories of emotion; 
thus, increasing the lexicon paths in WordNet. 
Nowadays, many sentiment and opinion 
messages are provided by Social Media. To deal 
with the informalities presented in these sources, it 
is necessary to have intermediary systems that 
improve the level of understanding of the 
messages. The following section offers a 
description of this phenomenon and a tool to track 
it. 
2.1 Text normalization 
Several informal features are present in opinions 
extracted from Social Media texts. Some research 
has been conducted in the field of lexical 
normalization for this kind of text. TENOR 
(Mosquera and Moreda, 2012) is a multilingual 
text normalization tool for Web 2.0 texts with an 
aim to transform noisy and informal words into 
their canonical form. That way, they can be easily 
processed by NLP tools and applications. TENOR 
works by identifying out-of-vocabulary (OOV) 
words such as slang, informal lexical variants, 
expressive lengthening, or contractions using a 
dictionary lookup and replacing them by matching 
formal candidates in a word lattice using phonetic 
and lexical edit distances. 
2.2 Construction of our own Sentiment 
Resource  
Having analyzed the examples of SA described in 
section 2, we proposed building our own sentiment 
resource (Guti?rrez et al, 2013) by adding lexical 
and informal patterns to obtain classifiers that can 
deal with Task 2b of Semeval-2013. We proposed 
the use of a method named RA-SR (using Ranking 
Algorithms to build Sentiment Resources) 
(Guti?rrez et al, 2013) to build sentiment word 
inventories based on senti-semantic evidence 
obtained after exploring text with annotated 
sentiment polarity information. Through this 
process, a graph-based algorithm is used to obtain 
auto-balanced values that characterize sentiment 
polarities, a well-known technique in Sentiment 
Analysis. This method consists of three key stages: 
(I) Building contextual word graphs; (II) Applying 
a ranking algorithm; and (III) Adjusting the 
sentiment polarity values. 
These stages are shown in the diagram in Figure 1, 
which the development of sentimental resources 
starts off by giving four corpora of annotated 
sentences (the first with neutral sentences, the 
second with objective sentences, the third with 
positive sentences, and the last with negative 
sentences). 
 
 
Figure 1. Resource walkthrough development 
process. 
2.3 Building contextual word graphs 
Initially, text preprocessing is performed by 
applying a Post-Tagging tool (using Freeling 
(Atserias et al, 2006) tool version 2.2 in this case) 
to convert all words to lemmas 5 . After that, all 
obtained lists of lemmas are sent to RA-SR, then 
divided into four groups: neutral, objective, 
positive, and negative candidates. As the first set 
                                                 
5 Lemma denotes canonic form of the words. 
Phr se 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
W ight =1
W ight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
W1 W2 W3
W4W5
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W1 W2 W4
W1
W5
Phrase 1 Neutral 
Phrases
W1 W6 W7 W8
W8 W7 W3
W6 W8 W7 W5
W5
W5 W2
Objective 
Phrases
Phrase 3
Phrase 2
Phrase 1
(II) 
W1 W2 W3 W4 W5 W6 W7 W8
(II) 
W1 W2 W3
W5
W6 W7
W8
Default Weight = 1/N
(I)(I)
Default Weight = 1/N
444
of results, four contextual graphs are 
obtained:  ????,   ???? , ????,  and ???? , where 
each graph includes the words/lemmas from the 
neutral, objective, positive and negative sentences 
respectively. These graphs are generated after 
connecting all words for each sentence into 
individual sets of annotated sentences in 
concordance with their annotations (??? , ??? , 
???, ??? ). 
Once the four graphs representing neutral, 
objective, positive and negative contexts are 
created, we proceed to assign weights to apply 
graph-based ranking techniques in order to auto-
balance the particular importance of each vertex ?? 
into ????, ????, ???? and ????. 
As the primary output of the graph-based ranking 
process, the positive, negative, neutral, and 
objective values are calculated using the PageRank 
algorithm and normalized with equation (1). For a 
better understanding of how the contextual graph 
was built see (Guti?rrez et al, 2013). 
2.4 Applying a ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ????, ????, ???? 
and ???? take the default of 1/N as their weight 
to define the weight of ? vector, which is used in 
our proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 4) as positive or negative, in relation to 
their respective graph, a weight value of 1 (in a 
range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
After that, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking algorithm 
is able to increase the significance of the words 
related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre and 
Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main 
idea behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
? i to ? j. As a result, the relevance of ? j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the 
more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
? , where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??.  
In our system, we apply the following 
configuration: dumping factor ? = 0.85 and, like 
in (Agirre and Soroa, 2009) we used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009)  
After applying PageRank, in order to obtain 
standardized values for both graphs, we normalize 
the rank values by applying the equation (1), 
where ???(??) obtains the maximum rank value 
of ?? vector (rankings? vector). 
??? = ???/???(??) (1) 
2.5 Adjusting the sentiment polarity values 
After applying the PageRank algorithm on????, 
???? , ????  and ???? , having normalized their 
ranks, we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. 
?? is represented by ???  lemmas, which would 
have, at that time, four assigned values: Neutral, 
Objective, Positive, and Negative, all of which 
correspond to a calculated rank obtained by the 
PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (2) 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (3) 
Where ???  is the Positive value and ???  the 
Negative value related to each lemma in ??. 
In order to standardize again the ???  and ??? 
values and making them more representative in a 
[0?1] scale, we proceed to apply a normalization 
process over the ??? and ??? values. 
From there, based on the objective features 
commented by (Baccianella et al, 2010), we 
assume the same premise to establish an 
alternative objective value of the lemmas. 
Equation (4) is used for that: 
?????? = 1 ? |??? ? ???| (4) 
Where ??????  represents the alternative 
objective value. 
445
As a result, each word obtained in the sentiment 
resource has an associated value of: positivity 
(??? , see equation (2)), negativity (??? , see 
equation (3)), objectivity(????_???,  obtained by 
PageRank over ????  and normalized with 
equation (1)), calculated-objectivity (??????, now 
cited as ???_???????? ) and neutrality (??? , 
obtained by PageRank over ???? and normalized 
with equation (1)). 
3  System Description 
The system takes annotated corpora as input from 
which two models are created. One model is 
created by using only the data provided at 
Semeval-2013 (Restricted Corpora, see Table 3), 
and the other by using extra data from other 
annotated corpora (Unrestricted Corpora, see 
Table 3). In all cases, the phrases are pre-
processed using Freeling 2.2 pos-tagger (Atserias 
et al, 2006) while a dataset copy is normalized 
using TENOR (described in section 2.1). 
The system starts by extracting two sets of 
features. The Core Features (see section 3.1) are 
the Sentiment Measures and are calculated for a 
standard and normalized phrase. The Support 
Features (see section 3.2) are based on regularities, 
observed in the training dataset, such as 
emoticons, uppercase words, and so on. 
The supervised models are created using Weka6 
and a Logistic classifier, both of which the system 
uses to predict the values of the test dataset. The 
selection of the classifier was made after analyzing 
several classifiers such as: Support Vector 
Machine, J48 and REPTree. Finally, the Logistic 
classifier proved to be the best by increasing the 
results around three perceptual points. 
The test data is preprocessed in the same way 
the previous corpora were. The same process of 
feature extraction is also applied. With the 
aforementioned features and the generated models, 
the system proceeds to classify the final values of 
Positivity, Negativity, and Neutrality.  
3.1 The Core Features 
The Core Features is a group of measures based on 
the resource created early (see section 2.2). The 
system takes a sentence preprocessed by Freeling 
2.2 and TENOR. For each lemma of the analyzed 
sentence, ??? , ??? , ???_???????? ,  ????_???, 
                                                 
6 http://www.cs.waikato.ac.nz/ 
and ???  are calculated by using the respective 
word values assigned in RA-SR. The obtained 
values correspond to the sum of the corresponding 
values for each intersecting word between the 
analyzed sentence (lemmas list) and the obtained 
resource by RA-SR. Lastly, the aforementioned 
attributes are normalized by dividing them by the 
number of words involved in this process. 
Other calculated attributes are: ???_????? , 
???_????? , ???_????????_????? , 
???_????_????? and ???_?????. These attributes 
count each involved iteration for each feature type 
( ??? , ??? , ????_??? , ??????  and ??? 
respectively, where the respective value may be 
greater than zero. 
Attributes ???  and cnn are calculated by 
counting the amount of lemmas in the phrases 
contained in the Sentiment Lexicons (Positive and 
Negative respectively).  
All of the 12 attributes described previously are 
computed for both, the original, and the 
normalized (using TENOR) phrase, totaling 24 
attributes. The Core features are described next.  
Feature Name Description 
??? 
Sum of respective value of each word. 
??? 
???_???????? 
????_??? 
??? 
???_????? 
Counts the words where its respective value 
is greater than zero 
???_????? 
???_????????_????? 
????_???_????? 
???_????? 
??? (to positive) Counts the words contained in the 
Sentiment Lexicons for their respective 
polarities. 
??? (to negative) 
Table 1. Core Features 
3.2 The Support Features 
The Support Features is a group of measures based 
on characteristics of the phrases, which may help 
with the definition on extreme cases. The emotPos 
and emotNeg values are the amount of Positive 
and Negative Emoticons found in the phrase. The 
exc and itr are the amount of exclamation and 
interrogation signs in the phrase. The following 
table shows the attributes that represent the 
support features: 
Feature Name Description 
??????? 
Counts the respective Emoticons 
??????? 
??? (exclamation marks (?!?)) 
Counts the respective marks 
??? (question marks (???)) 
?????_????? Counts the uppercase words 
?????_??? Sums the respective values of the 
Uppercase words ?????_??? 
?????_???_?????_??? (to Counts the Uppercase words 
446
positivity) contained in their respective 
Graph ?????_???_?????_???(to 
negativity) 
?????_???_?????_???? (to 
positivity) 
Counts the Uppercase words 
contained in the Sentiment 
Lexicons 7 for their respective 
polarity  
?????_???_?????_???? (to 
negativity) 
???????_????? Counts the words with repeated 
chars  
???????_??? Sums the respective values of the 
words with repeated chars ???????_??? 
???????_???_?????_???? (in 
negative lexical resource ) 
Counts the words with repeated 
chars contained in the respective 
lexical resource ???????_???_?????_???? (in 
positive lexical resource ) 
???????_???_?????_??? (in 
positive graph ) 
Counts the words with repeated 
chars contained in the respective 
graph ???????_???_?????_???  (in 
negative graph ) 
Table 2. The Support Features 
4 Evaluation 
In the construction of the sentiment resource, we 
used the annotated sentences provided by the 
corpora described in Table 3. The resources listed 
in Table 3 were selected to test the functionality of 
the words annotation proposal with subjectivity 
and objectivity. Note that the shadowed rows 
correspond to constrained runs corpora: tweeti-b-
sub.dist_out.tsv 8  (dist), b1_tweeti-objorneu-
b.dist_out.tsv 9  (objorneu), twitter-dev-input-
B.tsv10 (dev). 
The resources from Table 3 that include 
unconstrained runs corpora are: all the previously 
mentioned ones, Computational-intelligence11 (CI) 
and stno12 corpora. 
The used sentiment lexicons are from the 
WordNetAffect_Categories13 and opinion-words14 
files as shown in detail in Table 4. 
Some issues were taken into account throughout 
this process. For instance, after obtaining a 
contextual graph ?, factotum words are present in 
most of the involved sentences (i.e., verb ?to be?). 
This issue becomes very dangerous after applying 
the PageRank algorithm because the algorithm 
                                                 
7 Resources described in Table 4. 
8Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
9Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
10 http://www.cs.york.ac.uk/semeval-2013/task2/ 
11A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
12NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
13 http://wndomains.fbk.eu/wnaffect.html 
14 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
strengthens the nodes possessing many linked 
elements. For that reason, the subtractions ??? ?
??? and ??? ? ??? are applied, where the most 
frequent words in all contexts obtain high values. 
The subtraction becomes a dumping factor.  
As an example, when we take the verb ?to be?, 
before applying equation (1), the verb achieves the 
highest values in each subjective context graph 
(????  and ????)  namely, 9.94 and 18.67 rank 
values respectively. These values, once equation 
(1) is applied, are normalized obtaining both 
??? =  1 and ??? =  1 in a range [0...1]. At the 
end, when the following steps are executed 
(Equations (2) and (3)), the verb ?to be? 
achieves ??? = 0 , ??? = 0  and 
therefore  ?????? = 1 . Through this example, it 
seems as though we subjectively discarded words 
that appear frequently in both contexts (Positive 
and Negative). 
Corpus N P O Neu 
Obj 
or Neu 
Unk T 
C UC 
dist 176 368 110 34 - - 688 X X 
objorneu 828 1972 788 1114 1045 - 5747 X X 
dev 340 575 - 739 - - 1654 X X 
CI 6982 6172 - - - - 13154  X 
stno15 1286 660 - 384 - 10000 12330  X 
T 9272 9172 898 1532 1045 10000 31919   
Table 3. Corpora used to apply RA-SR. Positive (P), 
Negative (N), Objective (Obj/O), Unknow (Unk), Total 
(T), Constrained (C), Unconstrained (UC). 
Sources P N T 
WordNet-Affects_Categories 
 (Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words  
(Hu and Liu, 2004; Liu et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 4. Sentiment Lexicons. Positive (P), Negative 
(N) and Total (T). 
   Precision (%) Recall (%) Total (%) 
 C Inc P  N  Neu P N Neu Prec Rec F1 
Run1 8032 1631 80,7 83,8 89,9 90,9 69,5 86,4 84,8 82,3 82,9 
Run2 19101 4671 82,2 77,3 89,4 80,7 81,9 82,3 83,0 81,6 80,4 
Table 5. Training dataset evaluation using cross-
validation (Logistic classifier (using 10 folds)). 
Constrained (Run1), Unconstrained (Run2), Correct(C), 
Incorrect (Inc). 
4.1 The training evaluation 
In order to assess the effectiveness of our trained 
classifiers, we performed some evaluation tests.  
Table 5 shows relevant results obtained after 
applying our system to an environment (specific 
domain). The best results were obtained with the 
                                                 
15 NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
447
restricted corpus. The information used to increase 
the knowledge was not balanced or perhaps is of 
poor quality. 
4.2 The test evaluation 
The test dataset evaluation is shown in Table 6, 
where system results are compared with the best 
results in each case. We notice that the constrained 
run is better in almost every aspect. In the few 
cases where it was lower, there was a minimal 
difference. This suggests that the information used 
to increase our Sentiment Resource was 
unbalanced (high difference between quantity of 
tagged types of annotated phrases), or was of poor 
quality. By comparing these results with the ones 
obtained by our system on the test dataset, we 
notice that on the test dataset, the results fell in the 
middle of the effectiveness scores. After seeing 
these results (Table 5 and Table 6), we assumed 
that our system performance is better in a 
controlled environment (or specific domain). To 
make it more realistic, the system must be trained 
with a bigger and more balanced dataset. 
Table 6 shows the results obtained by our 
system while comparing them to the best results of 
Task 2b of Semeval-2013. In Table 5, we can see 
the difference between the best systems. They are 
the ones in bold and underlined as target results.  
These results have a difference of around 20 
percentage points. The grayed out ones correspond 
to our runs. 
      Precision (%) Recall (%) Total 
Runs C Inc P N Neu P N Neu Prec Rec F 1 
1_tw 2082 1731 60,9 46,5 52,8 49,8 41,4 64,1 53,4 51,8 49,3 
1_tw_cnd 2767 1046 81,4 69,7 67,7 66,7 60,4 82,6 72,9 69,9 69,0 
2_tw 2026 1787 58,0 42,2 42,2 52,2 43,9 57,4 47,4 51,2 49,0 
2_tw_ter 2565 1248 71,1 54,6 68,6 74,7 59,4 63,1 64,8 65,7 64,9 
1_sms 1232 862 43,9 46,1 69,5 55,9 31,7 68,9 53,2 52,2 43,4 
1_sms_cnd 1565 529 73,1 55,4 85,2 73,0 75,4 75,3 71,2 74,5 68,5 
2_sms 1023 1071 38,4 31,4 68,3 60,0 38,3 47,8 46,0 48,7 40,7 
2_sms_ava 1433 661 60,9 49,4 81,4 65,9 63,7 71,0 63,9 66,9 59,5 
Table 6. Test dataset evaluation using official scores. 
Corrects(C), Incorrect (Inc). 
Table 6 run descriptions are as follows:  
? UMCC_DLSI_(SA)-B-twitter-constrained 
(1_tw), 
? NRC-Canada-B-twitter-constrained 
(1_tw_cnd),  
? UMCC_DLSI_(SA)-B-twitter-unconstrained 
(2_tw), 
? teragram-B-twitter-unconstrained (2_tw_ter), 
? UMCC_DLSI_(SA)-B-SMS-constrained 
(1_sms), 
? NRC-Canada-B-SMS-constrained 
(1_sms_cnd), UMCC_DLSI_(SA)-B-SMS-
unconstrained (2_sms), 
? AVAYA-B-sms-unconstrained (2_sms_ava). 
As we can see in the training and testing 
evaluation tables, our training stage offered more 
relevant scores than the best scores in Task2b 
(Semaval-2013). This means that we need to 
identify the missed features between both datasets 
(training and testing). 
For that reason, we decided to check how many 
words our system (more concretely, our Sentiment 
Resource) missed. Table 7 shows that our system 
missed around 20% of the words present in the test 
dataset. 
 hits miss miss (%) 
twitter 23807 1591 6,26% 
sms 12416 2564 17,12% 
twitter nonrepeat   2426 863 26,24% 
sms norepeat 1269 322 20,24% 
Table 7. Quantity of words used by our system over 
the test dataset. 
5 Conclusion and further work 
Based on what we have presented, we can say that 
we could develop a system that would be able to 
solve the SA challenge with promising results. The 
presented system has demonstrated election 
performance on a specific domain (see Table 5) 
with results over 80%. Also, note that our system, 
through the SA process, automatically builds 
sentiment resources from annotated corpora.  
For future research, we plan to evaluate RA-SR 
on different corpora. On top of that, we also plan 
to deal with the number of neutral instances and 
finding more words to evaluate the obtained 
sentiment resource. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government through 
the project PROMETEO 
(PROMETEO/2009/199). 
448
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and Computing 
Systems. Alacant, Univeristy of Alacant, 2011. 299. 
p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. Martinez-
Barco. The OpAL System at NTCIR 8 MOAT. 
Proceedings of NTCIR-8 Workshop Meeting, 
Tokyo, Japan., 2010. 241-245 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer Networks 
and ISDN Systems, 1998, 30(1-7): 107-117. 
Fellbaum, C. WordNet. An Electronic Lexical 
Database.  University of Cambridge, 1998. p. The 
MIT Press.  
Guti?rrez, Y.; A. Gonz?lez; A. F. Orqu?n; A. Montoyo 
and R. Mu?oz. RA-SR: Using a ranking algorithm to 
automatically building resources for subjectivity 
analysis over annotated corpora. 4th Workshop on 
Computational Approaches to Subjectivity, 
Sentiment & Social Media Analysis (WASSA 2013), 
Atlanta, Georgia, 2013.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000.  
Hu, M. and B. Liu. Mining and Summarizing Customer 
Reviews. Proceedings of the ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD-2004), USA, 2004.  
Kim, S.-M. and E. Hovy. Extracting Opinions, Opinion 
Holders, and Topics Expressed in Online News 
Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the association 
for computational linguistics (COLING/ACL 2006), 
Sydney, Australia, 2006. 1-8 p.  
Kozareva, Z.; P. Nakov; A. Ritter; S. Rosenthal; V. 
Stoyonov and T. Wilson. Sentiment Analysis in 
Twitter. in:  Proceedings of the 7th International 
Workshop on Semantic Evaluation. Association for 
Computation Linguistics, 2013. 
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Mosquera, A. and P. Moreda. TENOR: A Lexical 
Normalisation Tool for Spanish Web 2.0 Texts. in:  
Text, Speech and Dialogue - 15th International 
Conference (TSD 2012). Springer, 2012. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in Language. 
Kluwer Academic Publishers, Netherlands, 2005.  
 
  
 
449
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 636?643, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Semantic and Lexical features for detection and 
classification Drugs in biomedical texts 
 
 
Armando Collazo, Alberto 
Ceballo, Dennys D. Puig, Yoan 
Guti?rrez, Jos? I. Abreu, Roger 
P?rez 
Antonio Fern?ndez 
Orqu?n, Andr?s 
Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba 
{armando.collazo, dennys.puig, 
yoan.gutierrez, jose.abreu, 
roger.perez}@umcc.cu, 
alberto.ceballo@infonet.umcc.cu 
DLSI, University of Alicante 
Carretera de San Vicente 
S/N Alicante, Spain 
antonybr@yahoo.com, 
{montoyo, 
rafael}@dlsi.ua.es 
Independent Consultant 
USA 
info@franccamara.com 
 
Abstract 
In this paper we describe UMCC_DLSI-
(DDI) system which attempts to detect and 
classify drug entities in biomedical texts. 
We discuss the use of semantic class and 
words relevant domain, extracted with ISR-
WN (Integration of Semantic Resources 
based on WordNet) resource to obtain our 
goal. Following this approach our system 
obtained an F-Measure of 27.5% in the 
DDIExtraction 2013 (SemEval 2013 task 
9). 
1. Introduction 
To understand biological processes, we must 
clarify how some substances interact with our 
body and one to each other. One of these 
important relations is the drug-drug interactions 
(DDIs). They occur when one drug interacts 
with another or when it affects the level, or 
activity of another drug. DDIs can change the 
way medications act in the body, they can cause 
powerful, dangerous and unexpected side 
effects, and also they can make the medications 
less effective. 
As suggested by (Segura-Bedmar et al, 2011), 
?...the detection of DDI is an important research 
area in patient safety since these interactions 
can become very dangerous and increase health 
care costs?. More recent studies (Percha and 
Altman, 2013) reports that ??Recent estimates 
indicate that DDIs cause nearly 74000 
emergency room visits and 195000 
hospitalizations each year in the USA?.  
But, on the other hand, there is an expansion in 
the volume of published biomedical research, 
and therefore the underlying biomedical 
knowledge base (Cohen and Hersh, 2005). 
Unfortunately, as often happens, this 
information is unstructured or in the best case 
scenario semi-structured. 
As we can see in (Tari et al, 2010), ?Clinical 
support tools often provide comprehensive lists 
of DDIs, but they usually lack the supporting 
scientific evidences and different tools can 
return inconsistent results?.  
Although, as mentioned (Segura-Bedmar et al, 
2011) ?there are different databases supporting 
healthcare professionals in the detection of DDI, 
these databases are rarely complete, since their 
update periods can reach up to three years?. In 
addition to these and other difficulties, the great 
amount of drug interactions are frequently 
reported in journals of clinical pharmacology 
and technical reports, due to this fact, medical 
literature becomes most effective source for 
detection of DDI. Thereby, the management of 
DDI is a critical issue due to the overwhelming 
amount of information available on them 
(Segura-Bedmar et al, 2011). 
636
1.1. Task Description 
With the aim of reducing the time the health care 
professionals invest on reviewing the literature, 
we present a feature-based system for drug 
detection and classification in biomedical texts. 
The DDIExtraction2013 task was divided into 
two subtasks: Recognition and classification of 
drug names (Task 9.1) and Extraction of drug-
drug interactions (Task 9.2). Our system was 
developed to be presented in the Task 9.1. In this 
case, participants were to detect and classify the 
drugs that were present in the test data set which 
was a set of sentences related to the biomedical 
domain obtained from a segmented corpus. The 
output consisted of a list mentioning all the 
detected drugs with information concerning the 
sentence it was detected from as well as its 
offset in that sentence (the position of the first 
and the last character of the drug in the sentence, 
0 being the first character of a sentence). Also 
the type of the drug should have been provided. 
As to the type, participants had to classify 
entities in one of these four groups1: 
? Drug: any chemical agent used for 
treatment, cure, prevention or diagnose of 
diseases, which have been approved for 
human usage. 
? Brand: any drug which firstly have been 
developed by a pharmaceutical company. 
? Group: any term in the text designating a 
relation among pharmaceutical substances. 
? No-Human: any chemical agent which 
affects the human organism. An active 
substance non-approved for human usage 
as medication. 
In the next section of the paper, we present 
related works (Section 2). In Section 3, we 
discuss the feature-based system we propose. 
Evaluation results are discussed in Section 4. 
Finally, we conclude and propose future work 
(Section 5). 
2. Related Work 
One of the most important workshops on the 
domain of Bioinformatics has been BioCreAtIve 
(Critical Assessment of Information Extraction 
                                                     
1 http://www.cs.york.ac.uk/semeval-2013/task9 
in Biology) (Hirschman et al, 2005). This 
workshop has improved greatly the Information 
Extraction techniques applied to the biological 
domain. The goal of the first BioCreAtIvE 
challenge was to provide a set of common 
evaluation tasks to assess the state-of-the-art for 
text mining applied to biological problems. The 
workshop was held in Granada, Spain on March 
28-31, 2004. 
According to Hirschman, the first 
BioCreAtIvE assessment achieved a high level 
of international participation (27 groups from 10 
countries). The best system results for a basic 
task (gene name finding and normalization), 
where a balanced 80% precision/recall or better, 
which potentially makes them suitable for real 
applications in biology. The results for the 
advanced task (functional annotation from free 
text) were significantly lower, demonstrating the 
current limitations of text-mining approaches. 
The greatest contribution of BioCreAtIve was 
the creation and release of training and test data 
sets for both tasks (Hirschman et al, 2005). 
One of the seminal works where the issue of 
drug detection was mentioned was (Gr?nroos et 
al., 1995). Authors argue the problem can be 
solved by using a computerized information 
system, which includes medication data of 
individual patients as well as information about 
non-therapeutic drug-effects. Also, they suggest 
a computerized information system to build 
decision support modules that, automatically 
give alarms or alerts of important drug effects 
other than therapeutic effects. If these warnings 
concern laboratory tests, they would be checked 
by a laboratory physician and only those with 
clinical significance would be sent to clinicians. 
Here, it is important to note the appearance of 
the knowledgebase DrugBank 2 . Since its first 
release in 2006 (Wishart et al, 2008) it has been 
widely used to facilitate in silico drug target 
discovery, drug design, drug docking or 
screening, drug metabolism prediction, drug 
interaction prediction and general 
pharmaceutical education. DrugBank has also 
significantly improved the power and simplicity 
of its structure query and text query searches. 
                                                     
2 http://redpoll.pharmacy.ualberta.ca/drugbank/ 
637
Later on, in 2010 Tari propose an approach 
that integrates text mining and automated 
reasoning to derive DDIs (Tari et al, 2010). 
Through the extraction of various facts of drug 
metabolism, they extract, not only the explicitly 
DDIs mentioned in text, but also the potential 
interactions that can be inferred by reasoning. 
This approach was able to find several potential 
DDIs that are not present in DrugBank. This 
analysis revealed that 81.3% of these 
interactions are determined to be correct. 
On the DDIExtraction 2011 (Segura-Bedmar et 
al., 2011) workshop (First Challenge Task on 
Drug-Drug Interaction Extraction) the best 
performance was achieved by the team WBI 
from Humboldt-Universitat, Berlin. This team 
combined several kernels and a case-based 
reasoning (CBR) system, using a voting 
approach. 
In this workshop relation extraction was 
frequently and successfully addressed by 
machine learning methods. Some of the more 
common used features were co-occurrences, 
character n-grams, Maximal Frequent 
Sequences, bag-of-words, keywords, etc. 
Another used technique is distant supervision. 
The first system evaluating distant supervision 
for drug-drug interaction was presented in 
(Bobi? et al, 2012), they have proposed a 
constraint to increase the quality of data used for 
training based on the assumption that no self-
interaction of real-world objects are described in 
sentences. In addition, they merge information 
from IntAct and the University of Kansas 
Proteomics Service (KUPS) database in order to 
detect frequent exceptions from the distant 
supervision assumption and make use of more 
data sources. 
Another important work related to Biomedical 
Natural Language Processing was BioNLP 
(Bj?rne et al, 2011) it is an application of 
natural language processing methods to analyze 
textual data on biology and medicine, often 
research articles. They argue that information 
extraction techniques can be used to mine large 
text datasets for relevant information, such as 
relations between specific types of entities. 
Inspired in the previews works the system we 
propose makes use of machine learning methods 
too, using some of the common features 
described above, such as the n-grams and 
keywords and co-occurrences, but we also add 
some semantic information to enrich those 
features. 
3. System Description  
As it has been mentioned before, the system was 
developed to detect and classify drugs in 
biomedical texts, so the process is performed in 
two main phases:  
? drug detection. 
? drug classification. 
Both phases are determined by the following 
stages, described in Figure 1: 
I. Preprocessing 
II. Feature extraction 
III. Classification 
 
 
 
 
 
 
 
Figure 1. Walkthrough system process. 
Given a biomedical sentence, the system 
obtains the lemmas and POS-tag of every token 
Classification 
 
Pre-Processing (using Freeling 2.2) 
Training set from Semeval 2013 
DDIExtraction2013 task 
Run(3) 
MFSC?s 
CSC MF 2-grams, 3-grams 
UC 
UCA 
MWord 
N 
 
   
I 
II 
III 
Tokenizing 
Run(1) Run(2) 
CSC - All 
EMFS
C 
DRD 
CD 
GC 
InRe 
WNum 
Feature Extraction 
Lemmatizing 
 
POS tagging 
 
 
   
 
   
638
of the sentence, by means of Freeling tool 3 . 
After that, it is able to generate candidates 
according to certain parameters (see section 3.3). 
Then, all the generated candidates are 
processed to extract the features needed for the 
learning methods, in order to determine which 
candidates are drugs. 
After the drugs are detected, the system 
generates a tagged corpus, following the 
provided training corpus structure, containing 
the detected entities, and then it proceeds to 
classify each one of them. To do so, another 
supervised learning algorithm was used (see 
section 3.3). 
3.1. Candidates generation 
Drugs and drug groups, as every entity in 
Natural Language, follow certain grammatical 
patterns. For instance, a drug is usually a noun 
or a set of nouns, or even a combination of verbs 
and nouns, especially verbs in the past participle 
tense and gerunds. But, one thing we noticed is 
that both drugs and drug groups end with a noun 
and as to drug groups that noun is often in the 
plural. 
Based on that idea, we decided to generate 
candidates starting from the end of each 
sentence and going forward. 
Generation starts with the search of a pivot 
word, which in this case is a noun. When the 
pivot is found, it is added to the candidates list, 
and then the algorithm takes the word before the 
pivot to see if it complies with one of the 
patterns i.e. if the word is a noun, an adjective, a 
gerund or past participle verb. If it does, then it 
and the pivot form another candidate.  
After that, the algorithm continues until it finds 
a word that does not comply with a pattern. In 
this case, it goes to the next pivot and stops 
when all the nouns in the sentence have been 
processed, or the first word of the sentence is 
reached. 
3.2. Feature Description 
For the DDIExtraction20134 task 9 three runs of 
the same system were performed with different 
                                                     
3 http://nlp.lsi.upc.edu/freeling/ 
features each time. The next sections describes 
the features we used. 
3.2.1. Most Frequent Semantic Classes 
(MFSC) 
Given a word, its semantic class label (Izquierdo 
et al, 2007) is obtained from WordNet using the 
ISR-WN resource (Guti?rrez et al, 2011; 2010). 
The semantic class is that associated to the most 
probable sense of the word. For each entity in 
the training set we take the words in the same 
sentence and for each word its semantic class is 
determined. This way, we identify the 4005 most 
frequent semantic classes associated to words 
surrounding the entities in the training set.  
For a candidate entity we use 400 features to 
encode information with regard to whether or 
not in its same sentence a word can be found 
belonging to one of the most frequent semantic 
classes. 
Each one of these features takes a value 
representing the distance (measured in words) a 
candidate is from the nearest word with same 
semantic class which represents the attribute.  
If the word is to the left of the candidate, the 
attribute takes a negative value, if it is to the 
right, the value is positive, and zero if no word 
with that semantic class is present in the 
sentence the candidate belongs to. 
To better understand that, consider A1 is the 
attribute which indicates if in the sentence of the 
candidate a word can be found belonging to the 
semantic class 1. Thus, the value of A1 is the 
distance the candidate is from the closest word 
with semantic class 1 in the sentence that is 
being analyzed. 
3.2.2. Candidate Semantic Class (CSC) 
The semantic class of candidates is also included 
in the feature set, if the candidate is a multi-
word, then the semantic class of the last word 
(the pivot word) is taken.  
 
                                                                               
4 http://www.cs.york.ac.uk/semeval-2013/task9/ 
5 This value was extracted from our previous experiment. 
639
3.2.3. Most Frequent Semantic Classes 
from Entities (EMFSC) 
In order to add more semantic information, we 
decided to find the most frequent semantic 
classes among all the entities that were tagged in 
the training data set. We included, in the feature 
set, all the semantic classes with a frequency of 
eight or more, because all the classes we wanted 
to identify were represented in that threshold. In 
total, they make 29 more features. The values of 
every one of them, is the sum of the number of 
times it appears in the candidate.  
3.2.4. Candidate Semantic Class All 
Words (CSC-All) 
This feature is similar to CSC, but in this case 
the candidate is a multi-word, we not only look 
for the semantic class of the pivot, but also the 
whole candidate as one. 
3.2.5. Drug-related domains (DRD) 
Another group of eight attributes describes how 
many times each one of the candidates belongs 
to one of the following drug-related domains 
(DRD) (medicine, anatomy, biology, chemistry, 
physiology, pharmacy, biochemistry, genetics).  
These domains where extracted from WordNet 
Domains. In order to determine the domain that 
a word belongs to, the proposal of DRelevant 
(V?zquez et al, 2007; V?zquez et al, 2004) was 
used. 
To illustrate how the DRD features take their 
values, consider the following sentence: 
??until the lipid response to Accutane is 
established.? 
One of the candidates the system generates 
would be ?lipid response?. It is a two-word 
candidate, so we take the first word and see if it 
belongs to one of the above domains. If it does, 
then we add one to that feature. If the word does 
not belong to any of the domains, then its value 
will be zero. We do the same with the other 
word. In the end, we have a collection where 
every value corresponds to each one of the 
domains. For the example in question the 
collection would be:  
 
 
medicine 1 
anatomy 0 
biology 0 
chemistry 0 
physiology 1 
pharmacy 0 
biochemistry 0 
genetics 0 
Table 1. DRD value assignment example. 
3.2.6. Candidate word number (WNum) 
Because there are candidates that are a multi-
word and others that are not, it may be the case 
that a candidate, which is a multi-word, has an 
EMFSC bigger than others which are not a 
multi-word, just because more than one of the 
words that conform it, have a frequent semantic 
class.  
We decided to add a feature, called WNum, 
which would help us normalize the values of the 
EMFSC. The value of the feature would be the 
number of words the candidate has. Same thing 
happens with DRD. 
3.2.7. Candidate Domain (CD) 
The value of this nominal feature is the domain 
associated to the candidate. If the candidate is a 
multi-word; we get the domain of all the words 
as a whole. In both cases the domain for a single 
word as well as for a multi-word is determined 
using the relevant domains obtained by 
(V?zquez et al, 2007; V?zquez et al, 2004). 
3.2.8. Maximum Frequent 2-grams, 3-
grams 
Drugs usually contain sequences of characters 
that are very frequent in biomedical domain 
texts. These character sequences are called n-
grams, where n is the number of characters in 
the sequence. Because of that, we decided to add 
the ten most frequent n-grams with n between 
two and three. The selected n-grams are the 
following: ?in? (frequency: 8170), ?ne? (4789), 
?ine? (3485), ?ti? (3234), ?id? (2768), ?an? 
(2704), ?ro? (2688), ?nt? (2593), ?et? (2423), 
?en? (2414). 
These features take a value of one if the 
candidate has the corresponding character 
sequence and zero if it does not. For instance: if 
640
we had the candidate ?panobinostat? it will 
generate the following collection:  
?in? 1 
?ne? 0 
?ine? 0 
?ti? 0 
?id? 0 
?an? 1 
?ro? 0 
?nt? 0 
?et? 0 
?en? 0 
Table 2. MF 2-gram, 3-gram. 
3.2.9. Uppercase (UC), Uppercase All 
(UCA). Multi-word (MWord) and 
Number (N) 
Other features say if the first letter of the 
candidate is an uppercase; if all of the letters are 
uppercase (UCA); if it is a multi-word (MWord) 
and also if it is in the singular or in the plural 
(N).  
3.2.10. L1, L2, L3 and R1, R2, R3 
The Part-of-Speech tags of the closest three 
surrounding words of the candidates are also 
included. We named those features L1, L2, and 
L3 for POS tags to the left of the candidate, and 
R1, R2, and R3 for those to the right. 
3.2.11. POS-tagging combination (GC) 
Different values are assigned to candidates, in 
order to identify its POS-tagging combination. 
For instance: to the following entity ?combined 
oral contraceptives? taken from DDI13-train-
TEES-analyses-130304.xml6 training file, which 
was provided for task 9.1, corresponds 5120. 
This number is the result of combining the four 
grammatical categories that really matter to us: 
R for adverb, V for verb, J for adjective, N for 
noun.  
A unique number was given to each 
combination of those four letters. We named this 
feature  GC. 
 
                                                     
6 http://www.cs.york.ac.uk/semeval-2013/task9 
3.2.12. In resource feature (InRe) 
A resource was created which contains all the 
drug entities that were annotated in the training 
corpus, so another attribute tells the system if the 
candidate is in the resource.  
Since all of the entities in the training data set 
were in the resource this attribute could take a 
value of one for all instances. Thus the classifier 
could classify correctly all instances in the 
training data set just looking to this attribute, 
which is not desirable. To avoid that problem, 
we randomly set its value to zero every 9/10 of 
the training instances. 
3.3. Classification 
All the features extracted in the previous stages 
are used in this stage to obtain the two models, 
one for drug detection phase, and the other for 
drug classification phase.  
We accomplished an extensive set of 
experiments in order to select the best classifier. 
All algorithms implemented in WEKA, except 
those that were designed specifically for a 
regression task, were tried. In each case we 
perform a 10-fold cross-validation. In all 
experiments the classifiers were settled with the 
default configuration. From those tests we select 
a decision tree, the C4.5 algorithm (Guti?rrez et 
al., 2011; 2010) implemented as the J48 
classifier in WEKA.  This classifier yields the 
better results for both drug detection and drug 
classification. 
The classifier was trained using a set of 463 
features, extracted from the corpus provided by 
SemEval 2013, the task 9 in question. 
As it was mentioned before, three runs were 
performed for the competition. Run (1) used the 
following features for drug detection: MFSC 
(only 200 frequent semantic classes), MF 2-
grams, 3-grams, UC, UCA, MWord, N, L1, L2, 
L3, R1, R2, R3, CSC, CD, WNum, GC and 
InRe.  
Drug classification in this run used the same 
features except for CD, WNum, and GC. Run 
(2) has all the above features, but we added the 
remaining 200 sematic classes that we left out in 
Run (1) to the detection and the classification 
models. In Run (3), we added EMFSC feature to 
the detection and the classification models. 
641
4. Results 
In the task, the results of the participants were 
compared to a gold-standard and evaluated 
according to various evaluation criteria:  
? Exact evaluation, which demands not only 
boundary match, but also the type of the 
detected drug has to be the same as that of 
the gold-standard. 
? Exact boundary matching (regardless of 
the type). 
? Partial boundary matching (regardless of 
the type) 
? Type matching. 
Precision and recall were calculated using the 
scoring categories proposed by MUC 7: 
? COR: the output of the system and the 
gold-standard annotation agree. 
? INC: the output of the system and the 
gold-standard annotation disagree. 
? PAR: the output of the system and the 
gold-standard annotation are not identical 
but has some overlapping text. 
? MIS: the number of gold-standard entities 
that were not identify by the system. 
? SPU: the number of entities labeled by the 
system that are not in the gold-standard. 
Table 3 , Table 4 and Table 5 show the system 
results in the DDIExtraction2013 competition 
for Run (1).  
Run (2) and Run (3) results are almost the 
same as Run (1). It is an interesting result since 
in those runs 200 additional features were 
supplied to the classifier.  In feature evaluation, 
using CfsSubsetEval and GeneticSearch with 
WEKA we found that all these new features 
were ranked as worthless for the classification. 
On the other hand, the following features were 
the ones that really influenced the classifiers: 
MFSC (215 features only), MF 2-grams, 3-
grams (?ne?, ?ine?, ?ti?, ?ro?, ?et?, ?en?), 
WNum, UC, UCA, L1, R1, CSC, CSC-All, CD, 
DRD (anatomy, physiology, pharmacy, 
biochemistry), InRe, GC and EMFS, specifically 
music.n.01, substance.n.01, herb.n.01, 
artifact.n.01, nutriment.n.01, nonsteroidal_anti-
inflammatory.n.01, causal_agent.n.01 have a 
                                                     
7http://www.itl.nist.gov/iaui/894.02/related_projects/muc/m
uc_sw/muc_sw_manual.html 
frequency of 8, 19, 35, 575, 52, 80, 63 
respectively.  
Measure Strict 
Exact 
Matching 
Partial 
Matching 
Type 
COR 319 354 354 388 
INC 180 145 0 111 
PAR 0 0 145 0 
MIS 187 187 187 187 
SPU 1137 1137 1137 1137 
Precision 0.19 0.22 0.22 0.24 
Recall 0.47 0.52 0.62 0.57 
Table 3. Run (1), all scores. 
Measure Drug Brand Group Drug_n 
COR 197 20 93 9 
INC 23 2 43 1 
PAR 0 0 0 0 
MIS 131 37 19 111 
SPU 754 47 433 14 
Precision 0.2 0.29 016 0.38 
Recall 0.56 0.34 0.6 0.07 
F1 0.3 0.31 0.26 0.12 
Table 4. Scores for entity types, exact matching in 
Run (1). 
 Precision Recall F1 
Macro average 0.26 0.39 0.31 
Strict matching 0.19 0.46 0.27 
Table 5. Macro average and Strict matching measures 
in Run (1). 
5. Conclusion and future works 
In this paper we show the description of 
UMCC_DLSI-(DDI) system, which is able to 
detect and classify drugs in biomedical texts 
with acceptable efficacy. It introduces in this 
thematic the use of semantic information such as 
semantic classes and the relevant domain of the 
words, extracted with ISR-WN resource. With 
this approach we obtained an F-Measure of 
27.5% in the Semeval DDI Extraction2013 task 
9.  
As further work we propose to eliminate some 
detected bugs (i.e. repeated instances, 
multiwords missed) and enrich our knowledge 
base (ISR-WN), using biomedical sources as 
UMLS8, SNOMED9 and OntoFis10. 
                                                     
8 http://www.nlm.nih.gov/research/umls 
9 http://www.ihtsdo.org/snomed-ct/ 
10 http://rua.ua.es/dspace/handle/10045/14216 
642
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
References 
Bj?rne, J.; A. Airola; T. Pahikkala and T. Salakoski 
Drug-Drug Interaction Extraction from 
Biomedical Texts with SVM and RLS Classifiers 
Proceedings of the 1st Challenge Task on Drug-
Drug Interaction Extraction, 2011, 761: 35-42. 
Bobi?, T.; R. Klinger; P. Thomas and M. Hofmann-
Apitius Improving Distantly Supervised Extraction 
of Drug-Drug and Protein-Protein Interactions 
EACL 2012, 2012: 35. 
Cohen, A. M. and W. R. Hersh A survey of current 
work in biomedical text mining Briefings in 
bioinformatics, 2005, 6(1): 57-71. 
Gr?nroos, P.; K. Irjala; J. Heiskanen; K. Torniainen 
and J. Forsstr?m Using computerized individual 
medication data to detect drug effects on clinical 
laboratory tests Scandinavian Journal of Clinical 
& Laboratory Investigation, 1995, 55(S222): 31-
36. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based 
on WordNet. XXVI Congreso de la Sociedad 
Espa?ola para el Procesamiento del Lenguaje 
Natural, Universidad Polit?cnica de Valencia, 
Valencia, SEPLN 2010, 2010. 161-168 p. 1135-
5948 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011, 47: 249-257. 
Hirschman, L.; A. Yeh; C. Blaschke and A. Valencia 
Overview of BioCreAtIvE: critical assessment of 
information extraction for biology BMC 
bioinformatics, 2005, 6(Suppl 1): S1. 
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Percha, B. and R. B. Altman Informatics confronts 
drug?drug interactions Trends in pharmacological 
sciences, 2013. 
Segura-Bedmar, I.; P. Mart?nez and D. S?nchez-
Cisneros The 1st DDIExtraction-2011 challenge 
task: Extraction of Drug-Drug Interactions from 
biomedical texts Challenge Task on Drug-Drug 
Interaction Extraction, 2011, 2011: 1-9. 
Tari, L.; S. Anwar; S. Liang; J. Cai and C. Baral 
Discovering drug?drug interactions: a text-mining 
and reasoning approach based on properties of 
drug metabolism Bioinformatics, 2010, 26(18): 
i547-i553. 
V?zquez, S.; A. Montoyo and Z. Kozareva. 
Extending Relevant Domains for Word Sense 
Disambiguation. IC-AI?07. Proceedings of the 
International Conference on Artificial Intelligence 
USA, 2007.  
V?zquez, S.; A. Montoyo and G. Rigau. Using 
Relevant Domains Resource for Word Sense 
Disambiguation. IC-AI?04. Proceedings of the 
International Conference on Artificial Intelligence, 
Ed: CSREA Press. Las Vegas, E.E.U.U., 2004. 
Wishart, D. S.; C. Knox; A. C. Guo; D. Cheng; S. 
Shrivastava; D. Tzur; B. Gautam and M. Hassanali 
DrugBank: a knowledgebase for drugs, drug 
actions and drug targets Nucleic acids research, 
2008, 36(suppl 1): D901-D906. 
643
