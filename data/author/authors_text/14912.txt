Sampling Tree Fragments from Forests
Tagyoung Chung?
University of Rochester
Licheng Fang??
University of Rochester
Daniel Gildea?
University of Rochester
Daniel S?tefankovic??
University of Rochester
We study the problem of sampling trees from forests, in the setting where probabilities for each
tree may be a function of arbitrarily large tree fragments. This setting extends recent work
for sampling to learn Tree Substitution Grammars to the case where the tree structure (TSG
derived tree) is not fixed. We develop a Markov chain Monte Carlo algorithm which corrects for
the bias introduced by unbalanced forests, and we present experiments using the algorithm to
learn Synchronous Context-Free Grammar rules for machine translation. In this application, the
forests being sampled represent the set of Hiero-style rules that are consistent with fixed input
word-level alignments. We demonstrate equivalent machine translation performance to standard
techniques but with much smaller grammars.
1. Introduction
Recent work on learning Tree Substitution Grammars (TSGs) has developed procedures
for sampling TSG rules from known derived trees (Cohn, Goldwater, and Blunsom
2009; Post and Gildea 2009). Here one samples binary variables at each node in the
tree, indicating whether the node is internal to a TSG rule or is a split point between
two rules. We consider the problem of learning TSGs in cases where the tree structure
is not known, but rather where possible tree structures are represented in a forest. For
example, we may wish to learn from text where treebank annotation is unavailable,
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: chung@cs.rochester.edu.
?? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: lfang@cs.rochester.edu.
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: gildea@cs.rochester.edu.
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: stefanko@cs.rochester.edu.
Submission received: 26 October 2012; revised version received: 14 March 2013; accepted for publication:
4 May 2013.
doi:10.1162/COLI a 00170
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
but a forest of likely parses can be produced automatically. Another application on
which we focus our attention in this article arises in machine translation, where we
want to learn translation rules from a forest representing the phrase decompositions that
are consistent with an automatically derived word alignment. Both these applications
involve sampling TSG trees from forests, rather than from fixed derived trees.
Chappelier and Rajman (2000) present a widely used algorithm for sampling trees
from forests: One first computes an inside probability for each node bottom?up, and
then chooses an incoming hyperedge for each node top?down, sampling according to
each hyperedge?s inside probability. Johnson, Griffiths, and Goldwater (2007) use this
sampling algorithm in a Markov chain Monte Carlo framework for grammar learning.
We can combine the representations used in this algorithm and in the TSG learning
algorithm discussed earlier, maintaining two variables at each node of the forest, one
for the identity of the incoming hyperedge, and another representing whether the node
is internal to a TSG rule or is a split point. However, computing an inside probability
for each node, as in the first phase of the algorithm of Johnson, Griffiths, and Goldwater
(2007), becomes difficult because of the exponential number of TSG rules that can apply
at any node in the forest. Not only is the number of possible TSG rules that can apply
given a fixed tree structure exponentially large in the size of the tree, but the number of
possible tree structures under a node is also exponentially large. This problem is par-
ticularly acute during grammar learning, as opposed to sampling according to a fixed
grammar, because any tree fragment is a valid potential rule. Cohn and Blunsom (2010)
address the large number of valid unseen rules by decomposing the prior over TSG
rules into an equivalent probabilistic context-free grammar; however, this technique
only applies to certain priors. In general, algorithms that match all possible rules are
likely to be prohibitively slow, as well as unwieldy to implement. In this article, we
design a sampling algorithm that avoids explicitly computing inside probabilities for
each node in the forest.
In Section 2, we derive a general algorithm for sampling tree fragments from forests.
We avoid computing inside probabilities, as in the TSG sampling algorithms of Cohn,
Goldwater, and Blunsom (2009) and Post and Gildea (2009), but we must correct for
the bias introduced by the forest structure, a complication that does not arise when the
tree structure is fixed. In order to simplify the presentation of the algorithm, we first set
aside the complication of large, TSG-style rules, and describe an algorithm for sampling
trees from forests while avoiding computation of inside probabilities. This algorithm is
then generalized to learn the composed rules of TSG in Section 2.3.
As an application of our technique, we present machine translation experiments in
the remainder of the article. We learn Hiero-style Synchronous Context-Free Grammar
(SCFG) rules (Chiang 2007) from bilingual sentences for which a forest of possible
minimal SCFG rules has been constructed fromfixedword alignments. The construction
of this forest and its properties are described in Section 3. We make the assumption
that the alignments produced by a word-level model are correct in order to simplify
the computation necessary for rule learning. This approach seems safe given that the
pipeline of alignment followed by rule extraction has generally remained the state of
the art despite attempts to learn joint models of alignment and rule decomposition
(DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn
2010a). We apply our sampling algorithm to learn the granularity of rule decomposition
in a Bayesian framework, comparing sampling algorithms in Section 4. The end-to-end
machine translation experiments of Section 5 show that our algorithm is able to achieve
performance equivalent to the standard technique of extracting all rules, but results in
a significantly smaller grammar.
204
Chung et al. Sampling Tree Fragments from Forests
2. Sampling Trees from Forests
As a motivating example, consider the small example forest of Figure 1. This forest
contains a total of five trees, one under the hyperedge labeled A, and four under the
hyperedge labeled B (the cross-product of the two options for deriving node 4 and the
two options for deriving node 5).
Let us suppose that we wish to sample trees from this forest according to a distribu-
tion Pt, and further suppose that this distribution is proportional to the product of the
weights of each tree?s hyperedges:
Pt(t) ?
?
h?t
w(h) (1)
To simplify the example, suppose that in Figure 1 each hyperedge has weight 1,
?h w(h) = 1
giving us a uniform distribution over trees:
?t Pt(t) = 15
A tree can be specified by attaching a variable zn to each node n in the forest
indicating which incoming hyperedge is to be used in the current tree. For example,
variable z1 can take values A and B in Figure 1, whereas variables z2 and z3 can only
take a single value. We use z to refer to the entire set of variables zn in a forest. Each
assignment to z specifies a unique tree, ?(z), which can be found by following the
incoming hyperedges specified by z from the goal node of each forest down to the
terminals.
A naive sampling strategy would be to resample each of these variables zn in
order, holding all others constant, as in standard Gibbs sampling. If we choose an
incoming hyperedge according to the probability Pt(?(z)) of the resulting tree, holding
all other variable assignments fixed, we see that, because Pt is uniform, we will choose
2 3 5
1
4
A
B
Figure 1
Example forest.
205
Computational Linguistics Volume 40, Number 1
with uniform probability of 1/m among the m incoming hyperedges at each node. In
particular, we will choose among the two incoming hyperedges at the root (node 1)
with equal probability, meaning that, over the long run, the sampler will spend half its
time in the state for the single tree corresponding to nodes 2 and 3, and only one eighth
of its time in each of the four other possible trees. Our naive algorithm has failed at its
goal of sampling among the five possible trees each with probability 1/5.
Thus, we cannot adopt the simple Gibbs sampling strategy, used for TSG induction
from fixed trees, of resampling one variable at a time according to the target distribution,
conditioned on all other variables. The intuitive reason for this, as illustrated by the
example, is the bias introduced by forests that are bushier (that is, havemore derivations
for each node) in some parts than in others. The algorithm derived in the remainder of
this section corrects for this bias, while avoiding the computation of inside probabilities
in the forest.
2.1 Choosing a Stationary Distribution
We will design our sampling algorithm by first choosing a distribution Pz over the set
of variables z defined earlier. We will show correctness of our algorithm by showing
that it is a Markov chain converging to Pz, and that Pz results in the desired distribution
Pt over trees.
The tree specified by an assignment to z will be denoted ?(z) (see Table 1). For a tree
t the vector containing the variables found at nodes in t will be denoted z[t]. This is a
subvector of z: for example, in Figure 1, if t chooses A at node 1 then z[t] = (z1, z2, z3),
and if t chooses B at node 1 then z[t] = (z1, z4, z5). We use z[?t] to denote the variables
not used in the tree t. The vectors z[t] and z[?t] differ according to t, but for any tree
t, the two vectors form a partition of z. There are many values of z that correspond to
the same tree, but each tree t corresponds to a unique subvector of variables z[t] and a
unique assignment to those specific variables?we will denote this unique assignment
of variables in z[t] by ?(t). (In terms of ? and ? one has for any z and t that ?(z) = t if
and only if z[t] = ?(t).)
Let Z be the random vector generated by our algorithm. As long as Pz(?(Z) = t) =
Pt(t) for all trees t, our algorithm will generate trees from the desired distribution.
Thus for any tree t the probability Pz(Z[t] = ?(t)) is fixed to Pt(t), but any distribu-
tion over the remaining variables not contained in t will still yield the desired dis-
tribution over trees. Thus, in designing the sampling algorithm, we may choose any
distribution Pz(Z[?t] | Z[t] = ?(t)). A simple and convenient choice is to make
Pz(Z[?t] | Z[t] = ?(t)) uniform. That is, each incoming hyperedge variable with m
alternatives assigns each hyperedge probability 1/m. (In fact, our algorithm can easily
Table 1
Notation
Pt desired distribution on trees
z vector of variables
Z random vector over z
?(z) tree corresponding to setting of z
Z[t] subset of random variables that occur in tree t
?(t) setting of variables in Z[t]
Z[?t] subset of random variables that do not occur in t
206
Chung et al. Sampling Tree Fragments from Forests
be adapted to other product distributions of Pz(Z[?t] | Z[t] = ?(t)).) This choice of
P(Z[?t] | Z[t] = ?(t)) determines a unique distribution for Pz:
Pz(Z = z) = Pt(?(z))Pz(Z[??(z)] = z[??(z)] | Z[?(z)] = z[?(z)])
= Pt(?(z))
?
v?z[??(z)]
1
deg(v)
(2)
where deg(v) is the number of possible values for v.
We proceed by designing a Gibbs sampler for this Pz. The sampler resamples vari-
ables from z one at a time, according to the joint probability Pz(z) for each alternative.
The set of possible values for zn at a node n having m incoming hyperedges consists of
the hyperedges ej, 1 ? j ? m. Let sj be the vector z with the value of zn changed to ej.
Note that the ?(sj)?s only differ at nodes below n.
Let z[in(n)] be the vector consisting of the variables at nodes below n (that is,
contained in subtrees rooted at n, or ?inside? n) in the forest, and let z[in(n)] be the
vector consisting of variables not under node n. Thus the vectors z[in(n)] and z[in(n)]
partition the complete vector z. We will use the notation z[t ? in(n)] to represent the
vector of variables from z that are both in a tree t and under a node n.
For Gibbs sampling, we need to compute the relative probabilities of sj?s. We now
consider the two terms of Equation (2) in this setting. Because of our requirement that Pz
correspond to the desired distribution Pt, the first term of Equation (2) can be computed,
up to a normalization constant, by evaluating our model over trees (Equation (1)).
The second term of Equation (2) is a product of uniform distributions that can be
decomposed into nodes below n and all other nodes:
Pz(Z[?t] = z[?t] | Z[t] = z[t]) =
?
v?z[?t? in(n)]
1
deg(v)
?
v?z[?t? in(n)]
1
deg(v)
(3)
where t = ?(z). Recall that ?(sj)?s only differ at vertices below n and hence
Pz(Z[?t] = z[?t] | Z[t] = z[t]) ?
?
v?z[?t? in(n)]
1
deg(v)
(4)
where we emphasize that ? refers to the relative probabilities of the sj?s, which corre-
spond to the options from which the Gibbs sampler chooses at a given step. We can
manipulate Equation (4) into a more computationally convenient form by multiplying
by the deg(v) term for each node v inside n. Because the terms for all nodes not included
in the current tree cancel each other out, we are left with:
Pz(Z[?t] = z[?t] | Z[t] = z[t]) ?
?
v?z[t? in(n)]
deg(v) (5)
Note that we only need to consider the nodes z[t] in the current tree, without needing
to examine the remainder of the forest at all.
207
Computational Linguistics Volume 40, Number 1
Substituting Equation (5) into Equation (2) gives a simple update rule for use in our
Gibbs sampler:
Pz(Z(i+1) = sj | Z(i) = z,n is updated) ? Pt(?(sj))
?
v?z[?(sj )?in(n)]
deg(v) (6)
To make a step of the Markov chain, we compute the right-hand side of Equation (6)
for every sj and then choose the next state of the chain Z(i+1) from the corresponding
distribution on the sj?s. The second term, which we refer to as the density factor, is
equal to the total number of trees in the forest under node n. This factor compensates
for the bias introduced by forests that are bushier in some places than in others, as in
the example of Figure 1. A related factor, defined on graphs rather than hypergraphs,
can be traced back as far as Knuth (1975), who wished to estimate the sum of values at
all nodes in a large tree by sampling a small number of the possible paths from the root
to the leaves. Knuth sampled paths uniformly and independently, rather than using a
continuously evolving Markov chain as in our Gibbs sampler.
2.2 Sampling Schedule
In a standard Gibbs sampler, updates are made iteratively to each variable z1, . . . , zN,
and this general strategy can be applied in our case. However, it may be wasteful to
continually update variables that are not used by the current tree and are unlikely to be
used by any tree. We propose an alternative sampling schedule consisting of sweeping
from the root of the current tree down to its leaves, resampling variables at each node
in the current tree as we go. If an update changes the structure of the current tree, the
sweep continues along the new tree structure. This strategy is shown in Algorithm 1,
where v(z, i) denotes the ith variable in a top?down ordering of the variables of the
current tree ?(z). The top?down ordering may be depth-first or breadth-first, among
other possibilities, as long as the variables at each node have lower indices than the
variables at the node?s descendants in the current tree.
To show that this sampling schedule will converge to the desired distribution over
trees, we will first show that Pz is a stationary distribution for the transition defined by
a single step of the sweep:
Lemma 1
For any setting of variables z, any top?down ordering v(z, i), and any i, updating
variable zv(z,i) according to Equation (6) is stationary with respect to the distribution
Pz defined by Equation (2).
Algorithm 1 Sampling algorithm
Require: A function v(z, i) returning the index of the ith variable of z in a top-down ordering of
the variables of the tree ?(z).
1: i ? 1
2: while i ? |Z[?(z)]| do  Until last node of current tree.
3: Resample zv(z,i) according to Equation (6)
4: i ? i + 1
5: end while
208
Chung et al. Sampling Tree Fragments from Forests
Proof
We will show that each step of the sweep is stationary for Pz by showing that it satisfies
detailed balance. Detailed balance is the condition that, on average, for each pair of
states z and z?, the number of transitions between the two states is the same in either
direction:
Pz(Z = z)P(Z(i+1) = z? | Z(i) = z) = Pz(Z = z?)P(Z(i+1) = z | Z(i) = z?) (7)
where P(Z(i+1) = z? | Z(i) = z) is the transition performed by one step of the sweep:
P(Z(i+1) = z? | Z(i) = z) =
{
Pz(Z=z? )
?
z?? Pz(Z=z
?? )I(z???v(z,i)=z?v(z,i) )
if z??v(z,i) = z?v(z,i)
0 otherwise
(8)
It is important to observe that, because the resampling step only changes the tree
structure below the ith node, the ith node in the new tree remains the same node. That
is, after making an update from z to z?, v(z, i) = v(z?, i), and, mathematically:
z??v(z,i) = z?v(z,i) ? v(z, i) = v(z?, i) ? z??v(z?,i) = z?v(z,i)
? z??v(z?,i) = z?v(z?,i)
Thus, the condition in Equation (8) is symmetric in z and z?, and we define the predicate
match(z, z?, i) to be equivalent to this condition. Substituting Equation (8) into the left-
hand side of Equation (7), we have:
Pz(Z = z)P(Z(i+1) = z? | Z(i) = z) =
{
Pz(Z=z)Pz(Z=z? )
?
z?? Pz(Z=z
?? )I(z???v(z,i)=z?v(z,i) )
if match(z, z?, i)
0 otherwise
(9)
By symmetry of the righthand side of Equation (9) in z and z?, we see that Equa-
tion (7) is satisfied. Because detailed balance implies stationarity, Pz is a stationary
distribution of P(Z(i+1) = z? | Z(i) = z). 
This lemma allows us to prove the correctness of our main algorithm:
Theorem 1
For any top?down sampling schedule v(z, i), and any desired distribution over trees
Pt that assigns non-zero probability to all trees in the forest, Algorithm 1 will converge
to Pt.
Proof
Because Pz is stationary for each step of the sweep, it is stationary for one entire sweep
from top to bottom.
To show that the Markov chain defined by an entire sweep is ergodic, we must show
that it is aperiodic and irreducible. It is aperiodic because the chain can stay in the same
configuration with non-zero probability by selecting the same setting for each variable
in the sweep. The chain is irreducible because any configuration can be reached in a
finite number of steps by sorting the variables in topological order bottom?up in the
forest, and then, for each variable, executing one sweep that selects a tree that includes
the desired variable with the desired setting.
209
Computational Linguistics Volume 40, Number 1
Because Pz is stationary for the chain defined by entire sweeps, and this chain is
ergodic, the chain will converge to Pz. Because Equation (2) guarantees that Pz(?(Z) =
t) = Pt(t), convergence to Pz implies convergence to Pt. 
2.3 Sampling Composed Rules
Our approach to sampling was motivated by a desire to learn TSG-style grammars,
where one grammar rule is the composition of a number of hyperedges in the forest.
We extend our sampling algorithm to handle this problem by using the same methods
that are used to learn a TSG from a single, fixed tree (Cohn, Goldwater, and Blunsom
2009; Post and Gildea 2009). We attach a binary variable to each node in the forest
indicating whether the node is a boundary between two TSG rules, or is internal to a
single TSG rule. Thus, the complete set of variables used by the sampler, z, now consists
of two variables at each node in the forest: one indicating the incoming hyperedge, and
one binary boundary variable. The proof of Section 2.1 carries through, with each new
binary variable v having deg(v) = 2 in Equation (2). As before, the current setting of z
partitions z into two sets of variables, those used in the current tree, z[?(z)], and those
outside the current tree, z[??(z)]. Given a fixed assignment to z, we can read off both
the current tree and its segmentation into TSG rules. We modify the tree probability of
Equation (1) to be a product over TSG rules r:
Pt(t) ?
?
r?t
w(r) (10)
in order to emphasize that grammar rules are no longer strictly equivalent to hyper-
edges in the forest. We modify the sampling algorithm of Algorithm 1 to make use of
this definition of Pt and to resample both variables at the current node. The incoming
hyperedge variable is resampled according to Equation (6), while the segmentation
variable is simply resampled according to Pt, as the update does not change the sets
z[?(z)] and z[??(z)].
The proof that the sampling algorithm converges to the correct distribution still
applies in the TSG setting, as it makes use of the partition of z into z[?(z)] and
z[??(z)], but does not depend on the functional form of the desired distribution over
trees Pt.
3. Phrase Decomposition Forest
In the remainder of this article, we will apply the algorithm developed in the previous
section to the problem of learning rules for machine translation in the context of a
Hiero-style, SCFG-based system. As in Hiero, our grammars will make use of a single
nonterminal X, and will contain rules with a mixture of nonterminals and terminals on
the right-hand side, with at most two nonterminal occurrences in the right-hand side
of a rule. In general, many overlapping rules of varying sizes are consistent with the
input word alignments, meaning that we must address a type of segmentation problem
in order to learn rules of the right granularity. Given the restriction to two right-hand
side nonterminals, the maximum number of rules that can be extracted from an input
sentence pair is O(n12) in the sentence length, because the left and right boundaries of
the left-hand side (l.h.s.) nonterminal and each of the two right-hand side nonterminals
can take O(n) positions in each of the two languages. This complexity leads us to
explore sampling algorithms, as dynamic programming approaches are likely to be
210
Chung et al. Sampling Tree Fragments from Forests
prohibitively slow. In this section, we show that the problem of learning rules can be
analyzed as a problem of identifying tree fragments of unknown size and shape in a
forest derived from the input word alignments for each sentence. These tree fragments
are similar to the tree fragments used in TSG learning. As in TSG learning, each rule
of the final grammar consists of some number of adjacent, minimal tree fragments:
one-level treebank expansions in the case of TSG learning and minimal SCFG rules,
defined subsequently, in the case of translation. The internal structure of TSG rules is
used during parsing to determine the final tree structure to output, and the internal
structure of machine translation rules will not be used at decoding time. This distinction
is irrelevant during learning. A more significant difference from TSG learning is that the
sets of minimal tree fragments in our SCFG application come not from a single, known
tree, but rather from a forest representing the set of bracketings consistent with the input
word alignments.
We now proceed to precisely define this phrase decomposition forest and discuss
some of its theoretical properties. The phrase decomposition forest is designed to extend
the phrase decomposition tree defined by Zhang, Gildea, and Chiang (2008) in order to
explicitly represent each possible minimal rule with a hyperedge.
A span [i, j] is a set of contiguous word indices {i, i + 1, . . . , j ? 1}. Given an aligned
Chinese?English sentence pair, a phrase n is a pair of spans n = ([i1, j1], [i2, j2]) such that
Chinese words in positions [i1, j1] are aligned only to English words in positions [i2, j2],
and vice versa. A phrase forest H = ?V,E? is a hypergraph made of a set of hypernodes
V and a set of hyperedges E. Each node n = ([i1, j1], [i2, j2]) ? V is a tight phrase as
defined by Koehn, Och, and Marcu (2003), namely, a phrase containing no unaligned
words at its boundaries. A phrase n = ([i1, j1], [i2, j2]) covers n? = ([i?1, j
?
1], [i
?
2, j
?
2]) if
i1 ? i?1 ? j?1 ? j1 ? i2 ? i?2 ? j?2 ? j2
Note that every phrase covers itself. It follows from the definition of phrases that if
i1 ? i?1 ? j?1 ? j1, then i2 ? i?2 ? j?2 ? j2. That means we can determine phrase coverage by
only looking at one language side of the phrases. We are going to use this property
to simplify the discussion of our proofs. We also define coverage between two sets of
phrases. Given two sets of phrases T and T?, we say T? covers T if for all t ? T, there
exists a t? ? T? such that t? covers t. We say that two phrases overlap if they intersect,
but neither covers the other.
If two phrases n = ([i1, j1], [i2, j2]) and n? = ([i?1, j
?
1], [i
?
2, j
?
2]) intersect, we can take the
union of the two phrases by taking the union of the source and target language spans,
respectively. That is, n1 ? n2 = ([i1, j1] ? [i?1, j?1], [i2, j2] ? [i?2, j?2]). An important property
of phrases is that if two phrases intersect, their union is also a phrase. For example,
given that have a date with her and with her today are both valid phrases in Figure 2, have
a date with her today must also be a valid phrase. Given a set T of phrases, we define
the union closure of the phrase set T, denoted
??(T), to be constructed by repeatedly
joining intersecting phrases until there are no intersecting phrases left.
Each edge in E, written as T ? n, is made of a set of non-intersecting tail nodes T ?
V, and a single head node n ? V that covers each tail node. Each edge is an SCFG rule
consistent with the word alignments. Each tail node corresponds to a right-hand-side
nonterminal in the SCFG rule, and any position included in n but not included in any tail
node corresponds to a right-hand-side terminal in the SCFG rule. For example, given the
aligned sentence pair of Figure 2, the edge {([3, 4], [5, 6]), ([5, 6], [3, 4])} ? ([2, 6], [1, 6]),
corresponds to a SCFG rule X ?? X1 ? X2, have a X2 with X1.
211
Computational Linguistics Volume 40, Number 1
?
I
??
have
?
a
?
date
?
with
??
her
today
Figure 2
Example word alignment, with boxes showing valid phrase pairs. In this example, all individual
alignment points are also valid phrase pairs.
For the rest of this section, we assume that there are no unaligned words. Unaligned
words can be temporarily removed from the alignment matrix before building the
phrase decomposition forest. After extracting the forest, they are put back into the
alignment matrix. For each derivation in the phrase decomposition forest, an unaligned
word appears in the SCFG rule whose left-hand side corresponds to the lowest forest
node that covers the unaligned word.
Definition 1
An edge T ? n is minimal if there does not exist another edge T? ? n such that T?
covers T.
A minimal edge is an SCFG rule that cannot be decomposed by factoring out some
part of its right-hand side as a separate rule. We define a phrase decomposition forest
to be made of all phrases from a sentence pair, connected by all minimal SCFG rules.
A phrase decomposition forest compactly represents all possible SCFG rules that are
consistent with word alignments. For the example word alignment shown in Figure 2,
the phrase decomposition forest is shown in Figure 3. Each boxed phrase in Figure 2
corresponds to a node in the forest of Figure 3, and hyperedges in Figure 3 represent
ways of building phrases out of shorter phrases.
A phrase decomposition forest has the important property that any SCFG rule
consistent with the word alignment corresponds to a contiguous fragment of some
complete tree found in the forest. For example, the highlighted tree fragment of the
forest in Figure 3 corresponds to the SCFG rule:
X ? ? X2 ? X1, have a X1 with X2
Thus any valid SCFG rule can be formed by selecting a set of adjacent hyperedges from
the forest and composing the minimal SCFG rules specified by each hyperedge. We
will apply the sampling algorithm developed in Section 2 to this problem of selecting
hyperedges from the forest.
212
Chung et al. Sampling Tree Fragments from Forests
Figure 3
A phrase decomposition forest extracted from the sentence pair ?????????, I have
a date with her today?. Each edge is a minimal SCFG rule, and the rules at the bottom level are
phrase pairs. Unaligned word ?a? shows up in the rule X ? X1X2,X1aX2 after unaligned
words are put back into the alignment matrix. The highlighted portion of the forest shows
an SCFG rule built by composing minimal rules.
The structure and size of phrase decomposition forests are constrained by the
following lemma:
Lemma 2
When there exists more than one minimal edge leading to the same head node n =
([i1, j1], [i2, j2]), each of these minimal edges is a binary split of phrase pair n, which
gives us either a straight or inverted binary SCFG rule with no terminals.
Proof
Suppose that there exist two minimal edges T1 ? n and T2 ? n leading to node n.
Consider the node set we get by taking the union closure of the tail nodes in T1
and T2:
??
(T1 ? T2) ? n
Figure 4 shows two cases of this construction. We show only the spans on the source
language side, which is enough to determine coverage properties. Let n have span [i, j]
on the source side. In the first case (left),
??(T1 ? T2) = {n}. We know
??(T1 ? T2) ? n
is also a valid edge because the unions of intersecting phrases are phrases, too. By the
definition of union closure,
??(T1 ? T2) covers both T1 and T2. Therefore T1 ? n and
213
Computational Linguistics Volume 40, Number 1
T2 ? n cannot both be minimal. In the second case (right),
??(T1 ? T2) = {n}. This
means that the phrases in T1 ? T2 overlap one another in a chain covering the entire span
of n. There must exist a phrase n1 = [i, k1] in T1 or T2 that begins at the left boundary
i of n. Without loss of generality, assume that n1 ? T1. There must exist another phrase
n2 = [k2, j2] ? T2 that overlaps with n such that k2 < k1 and j2 > k1. The span [k2, j] is
a valid phrase, because it consists of the union closure of all phrases that begin to the
right of k2:
??
{[i?, j?] | [i?, j?] ? T1 ? T2 ? i? ? k2} = {[k2, j]}
We also know that n1 ? n2 = [i, k2] is a valid phrase because the difference of two
overlapping phrases is also a valid phrase. Therefore k2 is a valid binary split point of n,
which means that either T2 is an edge formed by this binary split, or T2 is not minimal.
The span [k2, j]? n1 = [k1, j] is also a valid phrase formed by taking the difference of
two overlapping phrases, which makes k1 a valid binary split point for n. This makes T1
either an edge formed by the binary split at k1, or not a minimal phrase. Thus, whenever
we have two minimal edges, both consist of a binary split. 
Another interesting property of phrase decomposition forests relates to the length
of derivations. A derivation is a tree of minimal edges reaching from a given node all
the way down to the forest?s terminal nodes. The length of a derivation is the number
of minimal edges it contains.
Lemma 3
All derivations under a node in a phrase decomposition forest have the same length.
Proof
This is proved by induction. As the base case, all the nodes at the bottom of the
phrase decomposition forest have only one derivation of length 1. For the induction
step, we consider the two possible cases in Lemma 2. The case where a node n has
only a single edge underneath is trivial. It can have only one derivation length be-
cause the children under that single edge already do. For the case where there are
multiple valid binary splits for a node n at span (i, j), we assume the split points are
k1, . . . , ks. Because the intersection of two phrases is also a phrase, we know that spans
(i, k1), (k1, k2), . . . , (ks, j) are all valid phrases, and so is any concatenation of consecutive
phrases in these spans. Any derivation in this sub-forest structure leading from these
s + 1 spans to n has length s, which completes the proof under the assumption of the
induction. 
??(T1 ? T2)
T2
T1
??(T1 ? T2)
T2
i k1k2 j2 j
T1
Figure 4
Sketch of proof for Lemma 2. In the first case,
??(T1 ? T2) consists of more than one span, or
consists of one span that is strictly smaller than n. In the second case,
??(T1 ? T2) = {n}.
214
Chung et al. Sampling Tree Fragments from Forests
Because all the different derivations under the same node in a minimal phrase forest
contain the same number of minimal rules, we call that number the level of a node. The
fact that nodes can be grouped by levels forms the basis of our fast iterative sampling
algorithm as described in Section 5.3.
3.1 Constructing the Phrase Decomposition Forest
Given a word-aligned sentence pair, a phrase decomposition tree can be extracted with
a shift-reduce algorithm (Zhang, Gildea, and Chiang 2008). Whereas the algorithm of
Zhang, Gildea, and Chiang (2008) constructs a single tree which compactly represents
the set of possible phrase trees, we wish to represent the set of all trees as a forest.
We now describe a bottom?up parsing algorithm, shown in Algorithm 2, for building
this forest. The algorithm considers all spans (i, j) in order of increasing length. The
CYK-like loop over split points k (line 10) is only used for the case where a phrase can
be decomposed into two phrases, corresponding to a binary SCFG rule with no right-
hand side terminals. By Lemma 2, this is the only source of ambiguity in constructing
Algorithm 2 The CYK-like algorithm for building a phrase decomposition forest from
word-aligned sentence pair ?f, e?.
1: Extract all phrase pairs in the form of ([i1, j1], [i2, j2])
2: Build a forest node for each phrase pair, and let n(i, j) be the node corresponding to the phrase
pair whose source side is [i, j]
3: for s = 1 . . . |f | do
4: for i = 0 . . . |f | ? s do
5: j ? i + s
6: if n(i, j) exists then
7: continue
8: end if
9: split ? 0
10: for k = i + 1 . . . j ? 1 do
11: if both n(i, k) and n(k, j) exist then
12: add edge {n(i, k),n(k, j)} ? n(i, j)
13: split ? split + 1
14: end if
15: end for
16: if split = 0 then
17: T ? ?
18: l ? i
19: while l < j do
20: l? ? l + 1
21: for m ? j . . . l do
22: if n(l,m) exists then
23: T ? T ? n(l,m)
24: l? ? m
25: break
26: end if
27: end for
28: l ? l?
29: end while
30: add edge T ? n(i, j)
31: end if
32: end for
33: end for
215
Computational Linguistics Volume 40, Number 1
phrase decompositions. When no binary split is found (line 16), a single hyperedge is
made that connects the current span with all its maximal children. (A child is maximal
if it is not itself covered by another child.) This section can produce SCFG rules with
more than two right-hand side nonterminals, and it also produces any rules containing
both terminals and nonterminals in the right-hand side. Right-hand side nonterminals
correspond to previously constructed nodes n(l,m) in line 23, and right-hand side
terminals correspond to advancing a position in the string in line 20.
The running time of the algorithm is O(n3) in terms of the length of the Chinese
sentence f . The size of the resulting forests depends on the input alignments. The worst
case in terms of forest size is when the input consists of a monotonic, one-to-one word
alignment. In this situation, all (i, k, j) tuples correspond to valid hyperedges, and the
size of the output forest is O(n3). At the other extreme, when given a non-decomposable
permutation as an input alignment, the output forest consists of a single hyperedge.
In practice, given Chinese?English word alignments from GIZA++, we find that the
resulting forests are highly constrained, and the algorithm?s running time is negligible
in our overall system. In fact, we find it better to rebuild every forest from a word
alignment every time we re-sample a sentence, rather than storing the hypergraphs
across sampling iterations.
4. Comparison of Sampling Methods
To empirically verify the sampling methods presented in Section 2, we construct phrase
decomposition forests over which we try to learn composed translation rules. In this
section, we use a simple probability model for the tree probability Pt in order to study
the convergence behavior of our sampling algorithm. We will use a more sophisticated
probability model for our end-to-end machine translation experiments in Section 5.
For studying convergence, we desire a simpler model with a probability that can be
evaluated in closed form.
4.1 Model
We use a very basic generative model based on a Dirichlet process defined over
composed rules. The model is essentially the same as the TSG model used by Cohn,
Goldwater, and Blunsom (2009) and Post and Gildea (2009).
We define a single Dirichlet process over the entire set of rules. We draw the rule
distribution G from a Dirichlet process, and then rules from G.
G | ?,P0 ? Dir(?,P0)
r | G ? G
For the base distribution P0, we use a very simple uniform distribution where all rules
of the same size have equal probability:
P0(r) = Vf
?|rf |Ve
?|re|
where Vf is the vocabulary size of source language, and |rf | is the length of the source
side of the rule r. Integrating over G, we get a Chinese restaurant process for the
Dirichlet process. Customers in the Chinese restaurant analogy represent translation
rule instances in the machine translation setting, and tables represent rule types. The
216
Chung et al. Sampling Tree Fragments from Forests
Chinese restaurant has an infinite number of tables, and customers enter one by one and
choose a table to sit at. Let zi be the table chosen by ith customer. Then, the probability
of the customer choosing a table which is already occupied by customers who entered
the restaurant previously, or a new table, is given by following equations:
P(zi = t | z?i) =
{
nt
i?1+? 1 ? t ? T
?
i?1+? t = T + 1
where z?i is the current seating arrangement, t is the index of the table, nt is the number
of customers at the table t, and T is the total number of occupied tables in the restaurant.
In our model, a table t has a label indicating to which rule r the table is assigned. The
label of a table is drawn from the base distribution P0.
If we marginalize over tables labeled with the same rule, we get the following
probability of choosing r given the current analysis z?i of the data:
P(ri = r | z?i) =
nr + ?P0(r)
n + ? (11)
where nr is the number of times rule r has been observed in z?i, and n is total number
of rules observed in z?i.
4.2 Sampling Methods
We wish to sample from the set of possible decompositions into rules, including com-
posed rules, for each sentence in our training data. We follow the top?down sampling
schedule discussed in Section 2 and also implement tree-level rejection sampling as a
baseline.
Our rejection sampling baseline is a form of Metropolis-Hastings where a new tree
t is resampled from a simple proposal distribution Q(t), and then either accepted or
rejected according the Metropolis-Hastings acceptance rule, as shown in Algorithm 3.
As in Algorithm 1, we use v(z, i) to denote a top?down ordering of forest variables. As in
all our experiments, Pt is the current tree probability conditioned on the current trees for
all other sentences in our corpus, using Equation (11) as the rule probability in Equation
(10).
Our proposal distribution samples each variable with uniform probability working
top?down through the forest. The proposal distribution for an entire tree is thus:
Q(t) =
?
w?z[t]
1
deg(w)
This does not correspond to a uniform distribution over entire trees for the reasons dis-
cussed in Section 2. However, the Metropolis-Hastings acceptance probability corrects
for this, and thus the algorithm is guaranteed to converge to the correct distribution in
the long term. We will show that, because the proposal distribution does not re-use any
of the variable settings from the current tree, the rejection sampling algorithm converges
more slowly in practice than the more sophisticated alternative described in Section 2.2.
We now describe in more detail our implementation of the approach of Section 2.2.
We define two operations on a hypergraph node n, SAMPLECUT and SAMPLEEDGE, to
change the sampled tree from the hypergraph. SAMPLECUT(n) chooses whether n is a
217
Computational Linguistics Volume 40, Number 1
Algorithm 3 Metropolis-Hastings sampling algorithm.
Require: A function v(z, i) returning the index of the ith variable of z in a top-down ordering of
the variables of the tree ?(z).
1: i ? 1
2: while i < |Z[?(z)]| do
3: Sample znewv(z,i) according to uniform(z
new
v(z,i) )
4: i ? i + 1
5: end while
6: z ?
{
znew w/probmin
{
1, Pt (t(z
new ))Q(t(zold ))
Pt (t(zold ))Q(t(znew ))
}
zold otherwise
segmentation point or not, deciding if two rules should merge, while SAMPLEEDGE(n)
chooses a hyperedge under n, making an entire new subtree. Algorithm 4 shows our
implementation of Algorithm 1 in terms of tree operations and the sampling operations
SAMPLEEDGE(n) and SAMPLECUT(n).
4.3 Experiments
We used a Chinese?English parallel corpus available from the Linguistic Data Consor-
tium (LDC), composed of newswire text. The corpus consists of 41K sentence pairs,
which is 1M words on the English side. We constructed phrase decomposition forests
with this corpus and ran the top?down sampling algorithm and the rejection sampling
algorithm described in Section 4.2 for one hundred iterations.We used? = 100 for every
experiment. The likelihood of the current state was calculated for every iteration. Each
setting was repeated five times, and then we computed the average likelihood for each
iteration.
Figure 5 shows a comparison of the likelihoods found by rejection sampling and
top?down sampling. As expected, we found that the likelihood converged much more
quickly with top?down sampling. Figure 6 shows a comparison between two different
versions of top?down sampling: the first experiment was run with the density factor
described in Section 2, Equation (6), and the second one was run without the density
factor. The density factor has a much smaller effect on the convergence of our algorithm
than does the move from rejection sampling to top?down sampling, such that the dif-
ference between the two curves shown in Figure 6 is not visible at the scale of Figure 5.
(The first ten iterations are omitted in Figure 6 in order to highlight the difference.) The
small difference is likely due to the fact that our trees are relatively evenly balanced,
Algorithm 4 Top?down sampling algorithm.
1: queue.push(root)
2: while queue is not empty do
3: n = queue.pop()
4: SAMPLEEDGE(n)
5: SAMPLECUT(n)
6: for each child c of node n do
7: queue.push(c)
8: end for
9: end while
218
Chung et al. Sampling Tree Fragments from Forests
Figure 5
Likelihood graphs for rejection sampling and top?down sampling.
Figure 6
Likelihood graphs for top?down sampling with and without density factor. The first ten
iterations are omitted to highlight the difference.
such that the ratio of the density factor for two trees is not significant in comparison
to the ratio of their model probabilities. Nevertheless, we do find higher likelihood
states with the density factor than without it. This shows that, in addition to providing
a theoretical guarantee that our Markov chain converges to the desired distribution Pt
in the limit, the density factor also helps us find higher probability trees in practice.
219
Computational Linguistics Volume 40, Number 1
5. Application to Machine Translation
The results of the previous section demonstrate the performance of our algorithm in
terms of the probabilities of the model it is given, but do not constitute an end-to-end
application. In this section we demonstrate its use in a complete machine translation
system, using the SCFG rules found by the sampler in a Hiero-style MT decoder. We
discuss our approach and how it relates to previous work in machine translation in
Section 5.1 before specifying the precise probability model used for our experiments in
Section 5.2, discussing a technique to speed-up the model?s burn-in in Section 5.3, and
describing our experiments in Section 5.4.
5.1 Approach
A typical pipeline for training current statistical machine translation systems consists
of the following three steps: word alignment, rule extraction, and tuning of feature
weights. Word alignment is most often performed using the models of Brown et al.
(1993) and Vogel, Ney, and Tillmann (1996). Phrase extraction is performed differently
for phrase-based (Koehn, Och, and Marcu 2003), hierarchical (Chiang 2005), and syntax-
based (Galley et al. 2004) translation models, whereas tuning algorithms are generally
independent of the translation model (Och 2003; Chiang, Marton, and Resnik 2008;
Hopkins and May 2011).
Recently, a number of efforts have been made to combine the word alignment and
rule extraction steps into a joint model, with the hope both of avoiding some of the
errors of the word-level alignment, and of automatically learning the decomposition
of sentence pairs into rules (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al.
2009; Blunsom and Cohn 2010a; Neubig et al. 2011). This approach treats both word
alignment and rule decomposition as hidden variables in an EM-style algorithm. While
these efforts have been able to match the performance of systems based on two succes-
sive steps for word alignment and rule extraction, they have generally not improved
performance enough to become widely adopted. One possible reason for this is the
added complexity and in particular the increased computation time when compared
to the standard pipeline. The accuracy of word-level alignments from the standard
GIZA++ package has proved hard to beat, in particular when large amounts of training
data are available.
Given this state of affairs, the question arises whether static word alignments can
be used to guide rule learning in a model which treats the decomposition of a sentence
pair into rules as a hidden variable. Such an approach would favor rules which are
consistent with the other sentences in the data, and would contrast with the standard
practice inHiero-style systems of simply extracting all overlapping rules consistent with
static word alignments. Constraining the search over rule decomposition with word
alignments has the potential to significantly speed up training of rule decomposition
models, overcoming one of the barriers to their widespread use. Rule decomposition
models also have the benefit of producing much smaller grammars than are achieved
when extracting all possible rules. This is desirable given that the size of translation
grammars is one of the limiting computational factors in current systems, necessitating
elaborate strategies for rule filtering and indexing.
In this section, we apply our sampling algorithm to learn rules for the Hiero trans-
lation model of Chiang (2005). Hiero is based on SCFG, with a number of constraints on
the form that rules can take. The grammar has a single nonterminal, and each rule has
220
Chung et al. Sampling Tree Fragments from Forests
at most two right-hand side nonterminals. Most significantly, Hiero allows rules with
mixed terminals and nonterminals on the right-hand side. This has the great benefit
of allowing terminals to control re-ordering between languages, but also leads to very
large numbers of valid rules during the rule extraction process. We wish to see whether,
by adding a learned model of sentence decomposition to Hiero?s original method of
leveraging fixed word-level alignments, we can learn a small set of rules in a system that
is both efficient to train and efficient to decode. Our approach of beginning with fixed
word alignments is similar to that of Sankaran, Haffari, and Sarkar (2011), although
their sampling algorithm reanalyzes individual phrases extracted with Hiero heuristics
rather than entire sentences, and produces rules with no more than one nonterminal
on the right-hand side.
Most previous works on joint word alignment and rule extraction models were
evaluated indirectly by resorting to heuristic methods to extract rules from learned
word alignment or bracketing structures (DeNero, Bouchard-Cote, and Klein 2008;
Zhang et al. 2008; Blunsom et al. 2009; Levenberg, Dyer, and Blunsom 2012), and do not
directly learn the SCFG rules that are used during decoding. In this article, we work
with lexicalized translation rules with a mix of terminals and nonterminals, and we
use the rules found by our sampler directly for decoding. Because word alignments are
fixed in our model, any improvements we observe in translation quality indicate that
our model learns how SCFG rules interplay with each other, rather than fixing word
alignment errors.
The problem of rule decomposition is not only relevant to the Hiero model.
Translation models that make use of monolingual parsing, such as string-to-tree
(Galley et al. 2004), and tree-to-string (Liu, Liu, and Lin 2006), are all known to
benefit greatly from learning composed rules (Galley et al. 2006). In the particular
case of Hiero rule extraction, although there is no explicit rule composition step, the
extracted rules are in fact ?composed rules? in the sense of string-to-tree or tree-to-
string rule extraction, because they can be further decomposed into smaller SCFG rules
that are also consistent with word alignments. Although our experiments only include
the Hiero model, the method presented in this article is also applicable to string-to-
tree and tree-to-string models, because the phrase decomposition forest presented in
Section 3 can be extended to rule learning and extraction of other syntax-based MT
models.
5.2 Model
In this section, we describe a generative model based on the Pitman-Yor process (Pitman
and Yor 1997; Teh 2006) over derivation trees consisting of composed rules. Bayesian
methods have been applied to a number of segmentation tasks in natural language pro-
cessing, including word segmentation, TSG learning, and learning machine translation
rules, as a way of controlling the overfitting produced when Expectation Maximization
would tend to prefer longer segments. However, it is important to note that the Bayesian
priors in most cases control the size and number of the clusters, but do not explicitly
control the size of rules. In many cases, this type of Bayesian prior alone is not strong
enough to overcome the preference for longer, less generalizable rules. For example,
some previous work in word segmentation (Liang and Klein 2009; Naradowsky and
Toutanova 2011) adopts a ?length penalty? to remedy this situation. Because we have
the prior knowledge that longer rules are less likely to generalize and are therefore less
likely to be a good rule, we adopt a similar scheme to control the length of rules in
our model.
221
Computational Linguistics Volume 40, Number 1
In order to explicitly control the length of our rules, we generate a rule r in two
stages. First, we draw the length of a rule |r| =  from a probability distribution defined
over positive integers. We use a Poisson distribution:
P(; ?) = ?
e??
!
Because of the factorial in the denominator, the Poisson distribution decays quickly as
 becomes larger, which is ideal for selecting rule length because we want to encourage
learning of shorter rules and learn longer rules only when there is strong evidence for
them in the data.
A separate Pitman-Yor process is defined for the rules of each length . We draw
the rule distribution G from a Pitman-Yor process, and then rules of length  are drawn
from G.
G | ?, d,P0 ? PY(?, d,P0)
r | G ? G
The first two parameters, a concentration parameter ? and a discount parameter d,
control the shape of distribution G by controlling the size and the number of clusters.
The label of the cluster is decided by the base distribution P0. Because our alignment
is fixed, we do not need a complex base distribution that differentiates better aligned
phrases from others. We use a uniform distribution where each rule of the same size
has equal probability. Since the number of possible shorter rules is smaller than that of
longer rules, we need to reflect this fact and need to have larger uniform probability
for shorter rules and smaller uniform probability for longer rules. We reuse the Poisson
probability for the base distribution, essentially assuming that the number of possible
rules of length  is 1/P(; ?).
The Pitman-Yor process gives us the following probability of choosing r of size 
given the current analysis z?i of the data:
P(ri = r | , z?i) =
nr ? Trd + (Td + ?)P0(r)
n + ?
where nr is the number of times rule r has been observed in z?i, Tr is the number of
tables (in the Chinese restaurant metaphor) labeled r, and n is the total number of rules
of length  observed in z?i. Because we have drawn the length of the rule from a Poisson
distribution, the rule length probability is multiplied by this equation in order to obtain
the probability of the rule under our model.
Keeping track of table assignments during inference requires a lot of book-keeping.
In order to simplify the implementation, instead of explicitly keeping track of the
number of tables for each rule, we estimate the number of tables using the following
equations (Huang and Renals 2010):
Tr = ndr
T =
?
r:|r|=
ndr
222
Chung et al. Sampling Tree Fragments from Forests
In order to encourage learning rules with smaller parsing complexity and rules with
mixed terminals and nonterminals, which are useful for replicating re-orderings that
are seen in the data, we made use of the concept of scope (Hopkins and Langmead
2010) in our definition of rule length. The scope of a rule is defined as the number of
pairs of adjacent nonterminals in the source language right-hand side plus the number
of nonterminals at the beginning or end of the source language right-hand side. For
example,
X ? f1X1X2f2X3, X1e1X2X3e2
has scope 2 because X1 and X2 are adjacent in the source language and X3 is at the
end of the source language right-hand side. The target side of the rule is irrelevant.
The intuition behind this definition is that it measures the number of free indices into
the source language string required during parsing, under the assumption that the
terminals provide fixed anchor points into the string. Thus a rule of scope of k can be
parsed in O(nk). We define the length of a rule to be the number of terminals in the
source and the target side plus the scope of the rule. This is equivalent to counting the
total number of symbols in the rule, but only counting a nonterminal if it contributes
to parsing complexity. For example, the length of a rule that consists only of two
consecutive nonterminals would be 3, and the length of a rule that has two consecutive
nonterminals bounded by terminals on both sides would be 3 as well. This definition
of rule length encourages rules with mixed terminals and nonterminals over rules with
only nonterminals, which tend not to provide useful guidance to the translation process
during decoding.
5.3 Stratified Sampling
We follow the same Gibbs sampler introduced in Section 4.2. The SAMPLEEDGE oper-
ation in our Gibbs sampler can be a relatively expensive operation, because the entire
subtree under a node is being changed during sampling. We observe that in a phrase
decomposition forest, lexicalized rules, which are crucial to translation quality, appear
at the bottom level of the forest. This lexicalized information propagates up the forest
as rules get composed. It is reasonable to constrain initial sampling iterations to work
only on those bottom level nodes, and then gradually lift the constraint. This not only
makes the sampler much more efficient, but also gives it a chance to focus on getting
better estimates of the more important parameters, before starting to consider nodes
at higher levels, which correspond to rules of larger size. Fortunately, as mentioned in
Section 3, each node in a phrase decomposition forest already has a unique level, with
level 1 nodes corresponding to minimal phrase pairs. We design the sampler to use a
stratified sampling process (i.e., sampling level one nodes for K iterations, then level 1
and 2 nodes for K iterations, and so on). We emphasize that when we sample for level
2 nodes, level 1 nodes are also sampled, which means parameters for the smaller rules
are given more chance to mix, and thereby settle into a more stable distribution.
In our experiments, running the first 100 iterations of sampling with regular sam-
pling techniques took us about 18 hours. However, with stratified sampling, it took
only about 6 hours. We also compared translation quality as measured by decoding
with rules from the 100th sample, and by averaging over every 10th sample. Both
sampling methods gave us roughly the same translation quality as measured in BLEU.
We therefore used stratified sampling throughout our experiments.
223
Computational Linguistics Volume 40, Number 1
5.4 Experiments
We used a Chinese?English parallel corpus available from LDC,1 composed of
newswire text. The corpus consists of 41K sentence pairs, which is 1M words on the
English side. We used a 392-sentence development set with four references for parame-
ter tuning, and a 428-sentence test set with four references for testing.2 The development
set and the test set have sentences with less than 30 words. A trigram language model
was used for all experiments. BLEU (Papineni et al. 2002) was calculated for evaluation.
5.4.1 Baseline. For our baseline system, we extract Hiero translation rules using the
heuristic method (Chiang 2007), with the standard Hiero rule extraction constraints.
We use our in-house SCFG decoder for translation with both the Hiero baseline and our
sampled grammars. Our features for all experiments include differently normalized rule
counts and lexical weightings (Koehn, Och, and Marcu 2003) of each rule. Weights are
tuned using Pairwise Ranking Optimization (Hopkins and May 2011) using the baseline
grammar and development set, then used throughout the experiments.
Because our sampling procedure results in a smaller rule table, we also establish a
no-singleton baseline to compare our results to a simple heuristic method of reducing
rule table size. The no-singleton baseline discards rules that occur only once and that
have more than one word on the Chinese side during the Hiero rule extraction process,
before counting the rules and computing feature scores.
5.4.2 Experimental Settings.
Model parameters. For all experiments, we used d = 0.5 for the Pitman-Yor discount
parameter, except where we compared the Pitman-Yor process with Dirichlet process
(d = 0). Although we have a separate Pitman-Yor process for each rule length, we used
the same ? = 5 for all rule sizes in all experiments, including Dirichlet process experi-
ments. For rule length probability, a Poisson distribution where ? = 2 was used for all
experiments.
Sampling. The samples are initialized such that all nodes in a forest are set to be seg-
mented, and a random edge is chosen under each node. For all experiments, we ran the
sampler for 100 iterations and took the sample from the last iteration to compare with
the baseline. For stratified sampling, we increased the level we sample at every 10th
iteration. We also tried ?averaging? samples, where samples from every 10th iteration
are merged to a single grammar. For averaging samples, we took the samples from the
0th iteration (initialization) to the 70th iteration at every 10th iteration.3 We decided
on the 70th iteration (last iteration of level 7 sampling) as the last iteration because
we constrained the sampler not to sample nodes whose span covers more than seven
words (for SAMPLECUT only, SAMPLECUT always segments for these nodes), and the
likelihood becomes very stable at that point.
1 We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34,
LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92,
LDC2006E24). The language model is trained on the English side of entire data (1.65M sentences,
which is 39.3M words).
2 They are from newswire portion of NIST MT evaluation data from 2004, 2005, and 2006.
3 Not including initialization has negligible effect on translation quality.
224
Chung et al. Sampling Tree Fragments from Forests
Rule extraction. Because every tree fragment in the sampled derivation represents a
translation rule, we do not need to explicitly extract the rules; we merely need to collect
them and count them. However, derivations include purely nonlexical rules that do not
conform to the rule constraints of Hiero, and which are not useful for translation. To
get rid of this type of rule, we prune every rule that has a scope greater than 2. Whereas
Hiero does not allow two adjacent nonterminals in the source side, our pruning criterion
allows some rules of scope 2 that are not allowed by Hiero. For example, the following
rule (only source side shown) has scope 2 but is not allowed by Hiero:
X ? w1X1X2w2X3
In order to see if these rules have any positive or negative effects on translation, we
compare a rule set that strictly conforms to the Hiero constraints and a rule set that
includes all the rules of scope 2 or less.
5.4.3 Results. Table 2 summarizes our results. As a general test of our probability model,
we compare the result from initialization and the 100th sample. The translation perfor-
mance of the grammar from the 100th iteration of sampling is much higher than that
of the initialization state. This shows that states with higher probability in our Markov
chain generally do result in better translation, and that the sampling process is able to
learn valuable composed rules.
In order to determine whether the composed rules learned by our algorithm are
particularly valuable, we compare them to the standard baseline of extracting all rules.
The size of the grammar taken from the single sample (100th sample) is only about 9% of
the baseline but still produces translation results that are not far worse than the baseline.
A simple way to reduce the number of rules in the baseline grammar is to remove all
rules that occur only once in the training data and that contain more than a single word
on the Chinese side. This ?no-singleton? baseline still leaves us with more rules than
our algorithm, with translation results between those of the standard baseline and our
algorithm.
We also wish to investigate the trade-off between grammar size and translation
performance that is induced by including rules from multiple steps of the sampling
process. It is helpful for translation quality to include more than one analysis of each
sentence in the final grammar in order to increase coverage of new sentences. Averaging
samples also better approximates the long-term behavior of the Markov chain, whereas
taking a single sample involves an arbitrary random choice. When we average eight
Table 2
Comparisons of decoding results.
iteration model pruning #rules dev test time (s)
Baseline heuristic Hiero 3.59M 25.5 25.1 809
No-singleton heuristic Hiero 1.09M 24.7 24.2 638
Sampled 0th (init) Pitman-Yor scope < 3 212K 19.9 19.1 489
Sampled 100th Pitman-Yor scope < 3 313K 23.9 23.3 1,214
Sampled averaged (0 to 70) Pitman-Yor scope < 3 885K 26.2 24.5 1,488
Sampled averaged (0 to 70) Pitman-Yor Hiero 785K 25.6 25.1 532
Sampled averaged (0 to 70) Dirichlet scope < 3 774K 24.6 23.8 930
225
Computational Linguistics Volume 40, Number 1
different samples, we get a larger number of rules than from a single sample, but still
only a quarter as many rules as in the Hiero baseline. The translation results with eight
samples are comparable to the Hiero baseline (not significantly different according
to 1,000 iterations of paired bootstrap resampling [Koehn 2004]). Translation results
are better with the sampled grammar than with the no-singleton method of reducing
grammar size, while the sampled grammar was smaller than the no-singleton rule set.
Thus, averaging samples seems to produce a good trade-off between grammar size and
quality.
The filtering applied to the final rule set affects both the grammar size and de-
coding speed, because rules with different terminal/nonterminal patterns have vary-
ing decoding complexities. We experimented with two methods of filtering the final
grammar: retaining rules of scope no greater than three, and the more restrictive the
Hiero constraints. We do not see a consistent difference in translation quality between
these methods, but there is a large impact in terms of speed. The Hiero constraints
dramatically speeds decoding. The following is the full list of the Hiero constraints,
taken verbatim from Chiang (2007):
 If there are multiple initial phrase pairs containing the same set of
alignments, only the smallest is kept. That is, unaligned words are
not allowed at the edges of phrases.
 Initial phrases are limited to a length of 10 words on either side.
 Rules are limited to five nonterminals plus terminals on the French side.
 Rules can have at most two nonterminals, which simplifies the decoder
implementation. This also makes our grammar weakly equivalent to an
inversion transduction grammar (Wu 1997), although the conversion
would create a very large number of new nonterminal symbols.
 It is prohibited for nonterminals to be adjacent on the French side,
a major cause of spurious ambiguity.
 A rule must have at least one pair of aligned words, so that translation
decisions are always based on some lexical evidence.
Of these constraints, the differences between the Hiero constraints and scope filtering
are: First, the Hiero constraints limit the number of nonterminals in a rule to no more
than two. Second, the Hiero constraints do not allow two adjacent nonterminals in
the source side of a rule. As discussed previously, these two differences limit Hiero
grammar to be a subset of scope 2 grammar, whereas the scope-filtered grammar retains
all scope 2 rules. Among grammars with the Hiero constraint, smaller grammars are
generally faster. The relationship between the number of rules and the decoding time is
less than linear. This is because the decoder never considers rules containing sequences
of terminals not present in the source sentence. As the number of rules grows, we see
rules with larger numbers of terminals that in turn apply to fewer input sentences.
The sampled grammar has a more pronounced effect of reducing rule table size than
decoding speed. Our sampling method may be particularly valuable for very large data
sets where grammar size can become a limiting factor.
Finally, we wish to investigate whether the added power of the Pitman-Yor process
gives any benefit over the simpler Dirichlet process prior, using the same modeling
of word length in both cases. We find better translation quality with the Pitman-Yor
226
Chung et al. Sampling Tree Fragments from Forests
process, indicating that the additional strength of the Pitman-Yor process in suppressing
infrequent rules helps prevent overfitting.
6. Conclusion
We presented a hypergraph sampling algorithm that overcomes the difficulties inherent
in computing inside probabilities in applications where the segmentation of the tree into
rules is not known.
Given parallel text with word-level alignments, we use this algorithm to learn
sentence bracketing and SCFG rule composition. Our rule learning algorithm is based
on a compact structure that represents all possible SCFG rules extracted from word-
aligned sentences pairs, and works directly with highly lexicalized model parameters.
We show that by effectively controlling overfitting with a Bayesian model, and design-
ing algorithms that efficiently sample that parameter space, we are able to learn more
compact grammars with competitive translation quality. Based on the framework we
built in this work, it is possible to explore other rule learning possibilities that are known
to help translation quality, such as learning refined nonterminals.
Our general sampling algorithm is likely to be useful in settings beyond machine
translation. One interesting application would be unsupervised or partially supervised
learning of (monolingual) TSGs, given text where the tree structure is completely or
partially unknown, as in the approach of Blunsom and Cohn (2010b).
Acknowledgments
This work was partially funded by NSF
grant IIS-0910611.
References
Blunsom, P., T. Cohn, C. Dyer, and
M. Osborne. 2009. A Gibbs sampler for
phrasal synchronous grammar induction.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 2,
pages 782?790, Singapore.
Blunsom, Phil and Trevor Cohn. 2010a.
Inducing synchronous grammars with
slice sampling. In Proceedings of the Human
Language Technology: The 11th Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 238?241, Boulder, CO.
Blunsom, Phil and Trevor Cohn. 2010b.
Unsupervised induction of tree
substitution grammars for dependency
parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,204?1,213,
Cambridge, MA.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Chappelier, Jean-Ce?dric and Martin Rajman.
2000. Monte-Carlo sampling for NP-hard
maximization problems in the framework
of weighted parsing. In Natural Language
Processing (NLP 2000), pages 106?117,
Patras.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical
machine translation. In Proceedings of the
43rd Annual Conference of the Association
for Computational Linguistics (ACL-05),
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Chiang, David, Yuval Marton, and Philip
Resnik. 2008. Online large-margin training
of syntactic and structural translation
features. In Conference on Empirical
Methods in Natural Language Processing
(EMNLP-08), pages 224?233, Honolulu, HI.
Cohn, Trevor and Phil Blunsom. 2010.
Blocked inference in Bayesian tree
substitution grammars. In Proceedings of
the 48th Annual Meeting of the Association
for Computational Linguistics (ACL-10),
pages 225?230, Uppsala.
Cohn, Trevor, Sharon Goldwater, and
Phil Blunsom. 2009. Inducing compact
but accurate tree-substitution grammars.
In Proceedings of Human Language
227
Computational Linguistics Volume 40, Number 1
Technologies: The 2009 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 548?556, Boulder, CO.
DeNero, John, Alexandre Bouchard-Cote,
and Dan Klein. 2008. Sampling alignment
structure under a Bayesian translation
model. In Conference on Empirical
Methods in Natural Language Processing
(EMNLP-08), pages 314?323, Honolulu, HI.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic translation models.
In Proceedings of the International Conference
on Computational Linguistics/Association
for Computational Linguistics (COLING/
ACL-06), pages 961?968, Sydney.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the
2004 Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL-04), pages 273?280,
Boston, MA.
Hopkins, Mark and Greg Langmead.
2010. SCFG decoding without
binarization. In Proceedings of the
2010 Conference on Empirical Methods
in Natural Language Processing,
pages 646?655, Cambridge, MA.
Hopkins, Mark and Jonathan May. 2011.
Tuning as ranking. In Proceedings of the
2011 Conference on Empirical Methods
in Natural Language Processing,
pages 1,352?1,362, Edinburgh.
Huang, Songfang and Steve Renals. 2010.
Power law discounting for n-gram
language models. In Proceedings of the
IEEE International Conference on Acoustic,
Speech, and Signal Processing (ICASSP?10),
pages 5,178?5,181, Dallas, TX.
Johnson, Mark, Thomas L. Griffiths, and
Sharon Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of the 2007
Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-07), pages 139?146,
Rochester, NY.
Knuth, D. E. 1975. Estimating the efficiency
of backtrack programs. Mathematics of
Computation, 29(129):121?136.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation.
In 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 388?395, Barcelona.
Koehn, Philipp, Franz Josef Och, and
Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL-03), pages 48?54,
Edmonton.
Levenberg, Abby, Chris Dyer, and Phil
Blunsom. 2012. A Bayesian model for
learning SCFGs with discontiguous rules.
In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 223?232,
Jeju Island.
Liang, Percy and Dan Klein. 2009. Online
EM for unsupervised models. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 611?619,
Boulder, CO.
Liu, Yang, Qun Liu, and Shouxun Lin.
2006. Tree-to-string alignment template
for statistical machine translation.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 609?616,
Sydney.
Naradowsky, Jason and Kristina Toutanova.
2011. Unsupervised bilingual morpheme
segmentation and alignment with
context-rich hidden semi-Markov models.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 895?904, Portland, OR.
Neubig, Graham, Taro Watanabe, Eiichiro
Sumita, Shinsuke Mori, and Tatsuya
Kawahara. 2011. An unsupervised model
for joint phrase alignment and extraction.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 632?641, Portland, OR.
Och, Franz Josef. 2003. Minimum error rate
training for statistical machine translation.
In Proceedings of the 41th Annual Conference
of the Association for Computational
Linguistics (ACL-03), pages 160?167,
Sapporo.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the Association
for Computational Linguistics (ACL-02),
pages 311?318, Philadelphia, PA.
228
Chung et al. Sampling Tree Fragments from Forests
Pitman, Jim and Marc Yor. 1997. The
two-parameter Poisson-Dirichlet
distribution derived from a stable
subordinator. Annals of Probability,
25(2):855?900.
Post, Matt and Daniel Gildea. 2009. Bayesian
learning of a tree substitution grammar.
In Proceedings of the Association for
Computational Linguistics (short paper),
pages 45?48, Singapore.
Sankaran, Baskaran, Gholamreza Haffari,
and Anoop Sarkar. 2011. Bayesian
extraction of minimal SCFG rules for
hierarchical phrase-based translation.
In Proceedings of the Sixth Workshop
on Statistical Machine Translation,
pages 533?541, Edinburgh.
Teh, Yee Whye. 2006. A hierarchical
Bayesian language model based on
Pitman-Yor processes. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 985?992, Sydney.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING-96), pages 836?841, Copenhagen.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Zhang, Hao, Daniel Gildea, and David
Chiang. 2008. Extracting synchronous
grammar rules from word-level
alignments in linear time. In Proceedings
of the 22nd International Conference on
Computational Linguistics (COLING-08),
pages 1,081?1,088, Manchester.
Zhang, Hao, Chris Quirk, Robert C. Moore,
and Daniel Gildea. 2008. Bayesian learning
of non-compositional phrases with
synchronous parsing. In Proceedings of
the 46th Annual Meeting of the Association
for Computational Linguistics (ACL-08),
pages 97?105, Columbus, OH.
229

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 835?845,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Binarized Forest to String Translation
Hao Zhang
Google Research
haozhang@google.com
Licheng Fang
Computer Science Department
University of Rochester
lfang@cs.rochester.edu
Peng Xu
Google Research
xp@google.com
Xiaoyun Wu
Google Research
xiaoyunwu@google.com
Abstract
Tree-to-string translation is syntax-aware and
efficient but sensitive to parsing errors. Forest-
to-string translation approaches mitigate the
risk of propagating parser errors into transla-
tion errors by considering a forest of alterna-
tive trees, as generated by a source language
parser. We propose an alternative approach to
generating forests that is based on combining
sub-trees within the first best parse through
binarization. Provably, our binarization for-
est can cover any non-consitituent phrases in
a sentence but maintains the desirable prop-
erty that for each span there is at most one
nonterminal so that the grammar constant for
decoding is relatively small. For the purpose
of reducing search errors, we apply the syn-
chronous binarization technique to forest-to-
string decoding. Combining the two tech-
niques, we show that using a fast shift-reduce
parser we can achieve significant quality gains
in NIST 2008 English-to-Chinese track (1.3
BLEU points over a phrase-based system, 0.8
BLEU points over a hierarchical phrase-based
system). Consistent and significant gains are
also shown in WMT 2010 in the English to
German, French, Spanish and Czech tracks.
1 Introduction
In recent years, researchers have explored a wide
spectrum of approaches to incorporate syntax and
structure into machine translation models. The uni-
fying framework for these models is synchronous
grammars (Chiang, 2005) or tree transducers
(Graehl and Knight, 2004). Depending on whether
or not monolingual parsing is carried out on the
source side or the target side for inference, there are
four general categories within the framework:
? string-to-string (Chiang, 2005; Zollmann and
Venugopal, 2006)
? string-to-tree (Galley et al, 2006; Shen et al,
2008)
? tree-to-string (Lin, 2004; Quirk et al, 2005;
Liu et al, 2006; Huang et al, 2006; Mi et al,
2008)
? tree-to-tree (Eisner, 2003; Zhang et al, 2008)
In terms of search, the string-to-x models explore all
possible source parses and map them to the target
side, while the tree-to-x models search over the sub-
space of structures of the source side constrained
by an input tree or trees. Hence, tree-to-x mod-
els are more constrained but more efficient. Mod-
els such as Huang et al (2006) can match multi-
level tree fragments on the source side which means
larger contexts are taken into account for transla-
tion (Poutsma, 2000), which is a modeling advan-
tage. To balance efficiency and accuracy, forest-to-
string models (Mi et al, 2008; Mi and Huang, 2008)
use a compact representation of exponentially many
trees to improve tree-to-string models. Tradition-
ally, such forests are obtained through hyper-edge
pruning in the k-best search space of a monolin-
gual parser (Huang, 2008). The pruning parameters
that control the size of forests are normally hand-
tuned. Such forests encode both syntactic variants
and structural variants. By syntactic variants, we re-
fer to the fact that a parser can parse a substring into
either a noun phrase or verb phrase in certain cases.
835
We believe that structural variants which allow more
source spans to be explored during translation are
more important (DeNeefe et al, 2007), while syn-
tactic variants might improve word sense disam-
biguation but also introduce more spurious ambi-
guities (Chiang, 2005) during decoding. To focus
on structural variants, we propose a family of bina-
rization algorithms to expand one single constituent
tree into a packed forest of binary trees containing
combinations of adjacent tree nodes. We control the
freedom of tree node binary combination by restrict-
ing the distance to the lowest common ancestor of
two tree nodes. We show that the best results are
achieved when the distance is two, i.e., when com-
bining tree nodes sharing a common grand-parent.
In contrast to conventional parser-produced-forest-
to-string models, in our model:
? Forests are not generated by a parser but by
combining sub-structures using a tree binarizer.
? Instead of using arbitary pruning parameters,
we control forest size by an integer number that
defines the degree of tree structure violation.
? There is at most one nonterminal per span so
that the grammar constant is small.
Since GHKM rules (Galley et al, 2004) can cover
multi-level tree fragments, a synchronous grammar
extracted using the GHKM algorithm can have syn-
chronous translation rules with more than two non-
terminals regardless of the branching factor of the
source trees. For the first time, we show that simi-
lar to string-to-tree decoding, synchronous binariza-
tion significantly reduces search errors and improves
translation quality for forest-to-string decoding.
To summarize, the whole pipeline is as follows.
First, a parser produces the highest-scored tree for
an input sentence. Second, the parse tree is re-
structured using our binarization algorithm, result-
ing in a binary packed forest. Third, we apply the
forest-based variant of the GHKM algorithm (Mi
and Huang, 2008) on the new forest for rule extrac-
tion. Fourth, on the translation forest generated by
all applicable translation rules, which is not neces-
sarily binary, we apply the synchronous binarization
algorithm (Zhang et al, 2006) to generate a binary
translation forest. Finally, we use a bottom-up de-
coding algorithm with intergrated LM intersection
using the cube pruning technique (Chiang, 2005).
The rest of the paper is organized as follows. In
Section 2, we give an overview of the forest-to-
string models. In Section 2.1, we introduce a more
efficient and flexible algorithm for extracting com-
posed GHKM rules based on the same principle as
cube pruning (Chiang, 2007). In Section 3, we in-
troduce our source tree binarization algorithm for
producing binarized forests. In Section 4, we ex-
plain how to do synchronous rule factorization in a
forest-to-string decoder. Experimental results are in
Section 5.
2 Forest-to-string Translation
Forest-to-string models can be described as
e = Y( arg max
d?D(T ), T?F (f)
P (d|T ) ) (1)
where f stands for a source string, e stands for a tar-
get string, F stands for a forest, D stands for a set
of synchronous derivations on a given tree T , and
Y stands for the target side yield of a derivation.
The search problem is finding the derivation with
the highest probability in the space of all deriva-
tions for all parse trees for an input sentence. The
log probability of a derivation is normally a lin-
ear combination of local features which enables dy-
namic programming to find the optimal combination
efficiently. In this paper, we focus on the models
based on the Synchronous Tree Substitution Gram-
mars (STSG) defined by Galley et al (2004). In con-
trast to a tree-to-string model, the introduction of F
augments the search space systematically. When the
first-best parse is wrong or no good translation rules
are applicable to the first-best parse, the model can
recover good translations from alternative parses.
In STSG, local features are defined on tree-to-
string rules, which are synchronous grammar rules
defining how a sequence of terminals and nontermi-
nals on the source side translates to a sequence of
target terminals and nonterminals. One-to-one map-
ping of nonterminals is assumed. But terminals do
not necessarily need to be aligned. Figure 1 shows a
typical English-Chinese tree-to-string rule with a re-
ordering pattern consisting of two nonterminals and
different numbers of terminals on the two sides.
836
VP
VBD
was
VP-C
.x1:VBN PP
P
by
.x2:NP-C
? bei? x2 x1
Figure 1: An example tree-to-string rule.
Forest-to-string translation has two stages. The
first stage is rule extraction on word-aligned parallel
texts with source forests. The second stage is rule
enumeration and DP decoding on forests of input
strings. In both stages, at each tree node, the task on
the source side is to generate a list of tree fragments
by composing the tree fragments of its children. We
propose a cube-pruning style algorithm that is suit-
able for both rule extraction during training and rule
enumeration during decoding.
At the highest level, our algorithm involves three
steps. In the first step, we label each node in the in-
put forest by a boolean variable indicating whether it
is a site of interest for tree fragment generation. If it
is marked true, it is an admissible node. In the case
of rule extraction, a node is admissible if and only if
it corresponds to a phrase pair according to the un-
derlying word alignment. In the case of decoding,
every node is admissible for the sake of complete-
ness of search. An initial one-node tree fragment is
placed at each admissible node for seeding the tree
fragment generation process. In the second step,
we do cube-pruning style bottom-up combinations
to enumerate a pruned list of tree fragments at each
tree node. In the third step, we extract or enumerate-
and-match tree-to-string rules for the tree fragments
at the admissible nodes.
2.1 A Cube-pruning-inspired Algorithm for
Tree Fragment Composition
Galley et al (2004) defined minimal tree-to-string
rules. Galley et al (2006) showed that tree-to-string
rules made by composing smaller ones are impor-
tant to translation. It can be understood by the anal-
ogy of going from word-based models to phrase-
based models. We relate composed rule extraction
to cube-pruning (Chiang, 2007). In cube-pruning,
the process is to keep track of the k-best sorted lan-
guage model states at each node and combine them
bottom-up with the help of a priority queue. We
can imagine substituting k-best LM states with k
composed rules at each node and composing them
bottom-up. We can also borrow the cube pruning
trick to compose multiple lists of rules using a pri-
ority queue to lazily explore the space of combina-
tions starting from the top-most element in the cube
formed by the lists.
We need to define a ranking function for com-
posed rules. To simulate the breadth-first expansion
heuristics of Galley et al (2006), we define the fig-
ure of merit of a tree-to-string rule as a tuple m =
(h, s, t), where h is the height of a tree fragment,
s is the number of frontier nodes, i.e., bottom-level
nodes including both terminals and non-terminals,
and t is the number of terminals in the set of frontier
nodes. We define an additive operator +:
m1 + m2
= ( max{h1, h2} + 1, s1 + s2, t1 + t2 )
and a min operator based on the order <:
m1 < m2 ??
?
?
?
h1 < h2 ?
h1 = h2 ? s1 < s2 ?
h1 = h2 ? s1 = s2 ? t1 < t2
The + operator corresponds to rule compositions.
The < operator corresponds to ranking rules by their
sizes. A concrete example is shown in Figure 2,
in which case the monotonicity property of (+, <)
holds: if ma < mb, ma +mc < mb +mc. However,
this is not true in general for the operators in our def-
inition, which implies that our algorithm is indeed
like cube-pruning: an approximate k-shortest-path
algorithm.
3 Source Tree Binarization
The motivation of tree binarization is to factorize
large and rare structures into smaller but frequent
ones to improve generalization. For example, Penn
Treebank annotations are often flat at the phrase
level. Translation rules involving flat phrases are un-
likely to generalize. If long sequences are binarized,
837
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VBD (1, 1, 0)
VBD
was
(2, 1, 1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP-C (1, 1, 0)
VP-C
VPB PP
(2, 2, 0)
VP-C
VPB PP
P NP-C
(3, 3, 1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
=
(1, 1, 0) (2, 2, 0) (3, 3, 1)
(1, 1, 0) VP
VBD VP-C
(2, 2, 0) VP
VBD VP-C
VPB PP
(3, 3, 0) VP
VBD VP-C
VPB PP
P NP-C
(4, 4, 1)
(2, 1, 1) VP
VBD
was
VP-C
(3, 2, 1) VP
VBD
was
VP-C
VPB PP
(3, 3, 1) VP
VBD
was
VP-C
VPB PP
P NP-C
(4, 4, 2)
Figure 2: Tree-to-string rule composition as cube-pruning. The left shows two lists of composed rules sorted by their
geometric measures (height, # frontiers,# frontier terminals), under the gluing rule of VP ? VBD VP?C.
The right part shows a cube view of the combination space. We explore the space from the top-left corner to the
neighbors.
the commonality of subsequences can be discov-
ered. For example, the simplest binarization meth-
ods left-to-right, right-to-left, and head-out explore
sharing of prefixes or suffixes. Among exponentially
many binarization choices, these algorithms pick a
single bracketing structure for a sequence of sibling
nodes. To explore all possible binarizations, we use
a CYK algorithm to produce a packed forest of bi-
nary trees for a given sibling sequence.
With CYK binarization, we can explore any span
that is nested within the original tree structure, but
still miss all cross-bracket spans. For example,
translating from English to Chinese, The phrase
?There is? should often be translated into one verb
in Chinese. In a correct English parse tree, however,
the subject-verb boundary is between ?There? and
?is?. As a result, tree-to-string translation based on
constituent phrases misses the good translation rule.
The CYK-n binarization algorithm shown in Al-
gorithm 1 is a parameterization of the basic CYK
binarization algorithm we just outlined. The idea is
that binarization can go beyond the scope of parent
nodes to more distant ancestors. The CYK-n algo-
rithm first annotates each node with its n nearest
ancestors in the source tree, then generates a bina-
rization forest that allows combining any two nodes
with common ancestors. The ancestor chain labeled
at each node licenses the node to only combine with
nodes having common ancestors in the past n gener-
ations.
The algorithm creates new tree nodes on the fly.
New tree nodes need to have their own states in-
dicated by a node label representing what is cov-
ered internally by the node and an ancestor chain
representing which nodes the node attaches to ex-
ternally. Line 22 and Line 23 of Algorithm 1 up-
date the label and ancestor annotations of new tree
nodes. Using the parsing semiring notations (Good-
man, 1999), the ancestor computation can be sum-
marized by the (?,?) pair. ? produces the ances-
tor chain of a hyper-edge. ? produces the ancestor
chain of a hyper-node. The node label computation
can be summarized by the (concatenate, min) pair.
concatenate produces a concatenation of node la-
bels. min yields the label with the shortest length.
A tree-sequence (Liu et al, 2007) is a sequence of
sub-trees covering adjacent spans. It can be proved
that the final label of each new node in the forest
corresponds to the tree sequence which has the min-
imum length among all sequences covered by the
node span. The ancestor chain of a new node is the
common ancestors of the nodes in its minimum tree
sequence.
For clarity, we do full CYK loops over all O(|w|2)
spans and O(|w|3) potential hyper-edges, where |w|
is the length of a source string. In reality, only de-
scendants under a shared ancestor can combine. If
we assume trees have a bounded branching factor
b, the number of descendants after n generations is
still bounded by a constant c = bn. The algorithm is
O(c3 ? |w|), which is still linear to the size of input
sentence when the parameter n is a constant.
838
VP
VBD+VBN
VBD
was
VBN
PP
P
by
NP-C
VP
VBD
was
VP-C
VBN+P
VBN P
by
NP-C
(a) (b)
VP
VBD+VBN+P
VBD+VBN
VBD
was
VBN
P
by
NP-C
VP
VBD+VBN+P
VBD
was
VBN+P
VBN P
by
NP-C
(c) (d)
1 2 3 4
0 VBD VBD+VBN VBD+VBN+P VP
1 VBN VBN+P VP-C
2 P PP
3 NP-C
Figure 3: Alternative binary parses created for the origi-
nal tree fragment in Figure 1 through CYK-2 binarization
(a and b) and CYK-3 binarization (c and d). In the chart
representation at the bottom, cells with labels containing
the concatenation symbol + hold nodes created through
binarization.
Figure 3 shows some examples of alternative trees
generated by the CYK-n algorithm. In this example,
standard CYK binarization will not create any new
trees since the input is already binary. The CYK-2
and CYK-3 algorithms discover new trees with an
increasing degree of freedom.
4 Synchronous Binarization for
Forest-to-string Decoding
In this section, we deal with binarization of transla-
tion forests, also known as translation hypergraphs
(Mi et al, 2008). A translation forest is a packed
forest representation of all synchronous derivations
composed of tree-to-string rules that match the
source forest. Tree-to-string decoding algorithms
work on a translation forest, rather than a source for-
est. A binary source forest does not necessarily al-
ways result in a binary translation forest. In the tree-
to-string rule in Figure 4, the source tree is already
ADJP
RB+JJ
x0:RB JJ
responsible
PP
IN
for
NP-C
NPB
DT
the
x1:NN
x2:PP
? x0
fuze
?? x2
de
? x1
ADJP
RB+JJ
x0:RB JJ
responsible
x1:PP
? x0
fuze
?? x1
PP
IN
for
NP-C
NPB
DT
the
x0:NN
x1:PP
? x1
de
? x0
Figure 4: Synchronous binarization for a tree-to-string
rule. The top rule can be binarized into two smaller rules.
binary with the help of source tree binarization, but
the translation rule involves three variables in the set
of frontier nodes. If we apply synchronous binariza-
tion (Zhang et al, 2006), we can factorize it into
two smaller translation rules each having two vari-
ables. Obviously, the second rule, which is a com-
mon pattern, is likely to be shared by many transla-
tion rules in the derivation forest. When beams are
fixed, search goes deeper in a factorized translation
forest.
The challenge of synchronous binarization for a
forest-to-string system is that we need to first match
large tree fragments in the input forest as the first
step of decoding. Our solution is to do the matching
using the original rules and then run synchronous
binarization to break matching rules down to factor
rules which can be shared in the derivation forest.
This is different from the offline binarization scheme
described in (Zhang et al, 2006), although the core
algorithm stays the same.
5 Experiments
We ran experiments on public data sets for English
to Chinese, Czech, French, German, and Spanish
839
Algorithm 1 The CYK-n Binarization Algorithm
1: function CYKBINARIZER(T,n)
2: for each tree node ? T in bottom-up topological order do
3: Make a copy of node in the forest output F
4: Ancestors[node] = the nearest n ancestors of node
5: Label [node] = the label of node in T
6: L? the length of the yield of T
7: for k = 2...L do
8: for i = 0, ..., L? k do
9: for j = i + 1, ..., i + k ? 1 do
10: lnode ? Node[i, j]; rnode ? Node[j, i + k]
11: if Ancestors[lnode] ? Ancestors[rnode] 6= ? then
12: pnode ? GETNODE(i, i + k)
13: ADDEDGE(pnode, lnode, rnode)
return F
14: function GETNODE(begin, end)
15: if Node[begin, end] /? F then
16: Create a new node for the span (begin, end)
17: Ancestors[node] = ?
18: Label [node] = the sequence of terminals in the span (begin, end) in T
19:
return Node[begin, end]
20: function ADDEDGE(pnode, lnode, rnode)
21: Add a hyper-edge from lnode and rnode to pnode
22: Ancestors[pnode] = Ancestors[pnode] ? (Ancestors[lnode] ?Ancestors[rnode])
23: Label [pnode] = min{Label[pnode], CONCATENATE(Label[lnode], Label[rnode])}
translation to evaluate our methods.
5.1 Setup
For English-to-Chinese translation, we used all the
allowed training sets in the NIST 2008 constrained
track. For English to the European languages, we
used the training data sets for WMT 2010 (Callison-
Burch et al, 2010). For NIST, we filtered out sen-
tences exceeding 80 words in the parallel texts. For
WMT, the filtering limit is 60. There is no filtering
on the test data set. Table 1 shows the corpus statis-
tics of our bilingual training data sets.
Source Words Target Words
English-Chinese 287M 254M
English-Czech 66M 57M
English-French 857M 996M
English-German 45M 43M
English-Spanish 216M 238M
Table 1: The Sizes of Parallel Texts.
At the word alignment step, we did 6 iterations
of IBM Model-1 and 6 iterations of HMM. For
English-Chinese, we ran 2 iterations of IBM Model-
4 in addition to Model-1 and HMM. The word align-
ments are symmetrized using the ?union? heuris-
tics. Then, the standard phrase extraction heuristics
(Koehn et al, 2003) were applied to extract phrase
pairs with a length limit of 6. We ran the hierar-
chical phrase extraction algorithm with the standard
heuristics of Chiang (2005). The phrase-length limit
is interpreted as the maximum number of symbols
on either the source side or the target side of a given
rule. On the same aligned data sets, we also ran the
tree-to-string rule extraction algorithm described in
Section 2.1 with a limit of 16 rules per tree node.
The default parser in the experiments is a shift-
reduce dependency parser (Nivre and Scholz, 2004).
It achieves 87.8% labelled attachment score and
88.8% unlabeled attachment score on the standard
Penn Treebank test set. We convert dependency
parses to constituent trees by propagating the part-
of-speech tags of the head words to the correspond-
ing phrase structures.
We compare three systems: a phrase-based sys-
tem (Och and Ney, 2004), a hierarchical phrase-
based system (Chiang, 2005), and our forest-to-
string systemwith different binarization schemes. In
the phrase-based decoder, jump width is set to 8. In
the hierarchical decoder, only the glue rule is applied
840
to spans longer than 10. For the forest-to-string sys-
tem, we do not have such length-based reordering
constraints.
We trained two 5-gram language models with
Kneser-Ney smoothing for each of the target lan-
guages. One is trained on the target side of the par-
allel text, the other is on a corpus provided by the
evaluation: the Gigaword corpus for Chinese and
news corpora for the others. Besides standard fea-
tures (Och and Ney, 2004), the phrase-based decoder
also uses a Maximum Entropy phrasal reordering
model (Zens and Ney, 2006). Both the hierarchi-
cal decoder and the forest-to-string decoder only use
the standard features. For feature weight tuning, we
do Minimum Error Rate Training (Och, 2003). To
explore a larger n-best list more efficiently in train-
ing, we adopt the hypergraph-based MERT (Kumar
et al, 2009).
To evaluate the translation results, we use BLEU
(Papineni et al, 2002).
5.2 Translation Results
Table 2 shows the scores of our system with the
best binarization scheme compared to the phrase-
based system and the hierarchical phrase-based sys-
tem. Our system is consistently better than the other
two systems in all data sets. On the English-Chinese
data set, the improvement over the phrase-based sys-
tem is 1.3 BLEU points, and 0.8 over the hierarchi-
cal phrase-based system. In the tasks of translat-
ing to European languages, the improvements over
the phrase-based baseline are in the range of 0.5 to
1.0 BLEU points, and 0.3 to 0.5 over the hierar-
chical phrase-based system. All improvements ex-
cept the bf2s and hier difference in English-Czech
are significant with confidence level above 99% us-
ing the bootstrap method (Koehn, 2004). To demon-
strate the strength of our systems including the two
baseline systems, we also show the reported best re-
sults on these data sets from the 2010 WMT work-
shop. Our forest-to-string system (bf2s) outperforms
or ties with the best ones in three out of four lan-
guage pairs.
5.3 Different Binarization Methods
The translation results for the bf2s system in Ta-
ble 2 are based on the cyk binarization algorithm
with bracket violation degree 2. In this section, we
BLEU
dev test
English-Chinese pb 29.7 39.4
hier 31.7 38.9
bf2s 31.9 40.7??
English-Czech wmt best - 15.4
pb 14.3 15.5
hier 14.7 16.0
bf2s 14.8 16.3?
English-French wmt best - 27.6
pb 24.1 26.1
hier 23.9 26.1
bf2s 24.5 26.6??
English-German wmt best - 16.3
pb 14.5 15.5
hier 14.9 15.9
bf2s 15.2 16.3??
English-Spanish wmt best - 28.4
pb 24.1 27.9
hier 24.2 28.4
bf2s 24.9 28.9??
Table 2: Translation results comparing bf2s, the
binarized-forest-to-string system, pb, the phrase-based
system, and hier, the hierarchical phrase-based system.
For comparison, the best scores from WMT 2010 are also
shown. ?? indicates the result is significantly better than
both pb and hier. ? indicates the result is significantly
better than pb only.
vary the degree to generate forests that are incremen-
tally augmented from a single tree. Table 3 shows
the scores of different tree binarization methods for
the English-Chinese task.
It is clear from reading the table that cyk-2 is the
optimal binarization parameter. We have verified
this is true for other language pairs on non-standard
data sets. We can explain it from two angles. At
degree 2, we allow phrases crossing at most one
bracket in the original tree. If the parser is reason-
ably good, crossing just one bracket is likely to cover
most interesting phrases that can be translation units.
From another point of view, enlarging the forests
entails more parameters in the resulting translation
model, making over-fitting likely to happen.
5.4 Binarizer or Parser?
A natural question is how the binarizer-generated
forests compare with parser-generated forests in
translation. To answer this question, we need a
841
BLEU
rules dev test
no binarization 378M 28.0 36.3
head-out 408M 30.0 38.2
cyk-1 527M 31.6 40.5
cyk-2 803M 31.9 40.7
cyk-3 1053M 32.0 40.6
cyk-? 1441M 32.0 40.3
Table 3: Comparing different source tree binarization
schemes for English-Chinese translation, showing both
BLEU scores and model sizes. The rule counts include
normal phrases which are used at the leaf level during
decoding.
parser that can generate a packed forest. Our fast
deterministic dependency parser does not generate
a packed forest. Instead, we use a CRF constituent
parser (Finkel et al, 2008) with state-of-the-art ac-
curacy. On the standard Penn Treebank test set, it
achieves an F-score of 89.5%. It uses a CYK algo-
rithm to do full dynamic programming inference, so
is much slower. We modified the parser to do hyper-
edge pruning based on posterior probabilities. The
parser preprocesses the Penn Treebank training data
through binarization. So the packed forest it pro-
duces is also a binarized forest. We compare two
systems: one is using the cyk-2 binarizer to generate
forests; the other is using the CRF parser with prun-
ing threshold e?p, where p = 2 to generate forests.1
Although the parser outputs binary trees, we found
cross-bracket cyk-2 binarization is still helpful.
BLEU
dev test
cyk-2 14.9 16.0
parser 14.7 15.7
Table 4: Binarized forests versus parser-generated forests
for forest-to-string English-German translation.
Table 4 shows the comparison of binarization for-
est and parser forest on English-German translation.
The results show that cyk-2 forest performs slightly
1All hyper-edges with negative log posterior probability
larger than p are pruned. In Mi and Huang (2008), the thresh-
old is p = 10. The difference is that they do the forest pruning
on a forest generated by a k-best algorithm, while we do the
forest-pruning on the full CYK chart. As a result, we need more
aggressive pruning to control forest size.
better than the parser forest. We have not done full
exploration of forest pruning parameters to fine-tune
the parser-forest. The speed of the constituent parser
is the efficiency bottleneck. This actually demon-
strates the advantage of the binarizer plus forest-to-
string scheme. It is flexible, and works with any
parser that generates projective parses. It does not
require hand-tuning of forest pruning parameters for
training.
5.5 Synchronous Binarization
In this section, we demonstrate the effect of syn-
chronous binarization for both tree-to-string and
forest-to-string translation. The experiments are on
the English-Chinese data set. The baseline systems
use k-way cube pruning, where k is the branching
factor, i.e., the maximum number of nonterminals on
the right-hand side of any synchronous translation
rule in an input grammar. The competing system
does online synchronous binarization as described in
Section 4 to transform the grammar intersected with
the input sentence to the minimum branching factor
k? (k? < k), and then applies k?-way cube pruning.
Typically, k? is 2.
BLEU
dev test
head-out cube pruning 29.2 37.0
+ synch. binarization 30.0 38.2
cyk-2 cube pruning 31.7 40.5
+ synch. binarization 31.9 40.7
Table 5: The effect of synchronous binarization for tree-
to-string and forest-to-string systems, on the English-
Chinese task.
Table 5 shows that synchronous binarization does
help reduce search errors and find better translations
consistently in all settings.
6 Related Work
The idea of concatenating adjacent syntactic cate-
gories has been explored in various syntax-based
models. Zollmann and Venugopal (2006) aug-
mented hierarchial phrase based systems with joint
syntactic categories. Liu et al (2007) proposed tree-
sequence-to-string translation rules but did not pro-
vide a good solution to place joint subtrees into con-
nection with the rest of the tree structure. Zhang et
842
al. (2009) is the closest to our work. But their goal
was to augment a k-best forest. They did not bina-
rize the tree sequences. They also did not put con-
straint on the tree-sequence nodes according to how
many brackets are crossed.
Wang et al (2007) used target tree binarization to
improve rule extraction for their string-to-tree sys-
tem. Their binarization forest is equivalent to our
cyk-1 forest. In contrast to theirs, our binarization
scheme affects decoding directly because we match
tree-to-string rules on a binarized forest.
Different methods of translation rule binarization
have been discussed in Huang (2007). Their argu-
ment is that for tree-to-string decoding target side
binarization is simpler than synchronous binariza-
tion and works well because creating discontinous
source spans does not explode the state space. The
forest-to-string senario is more similar to string-to-
tree decoding in which state-sharing is important.
Our experiments show that synchronous binariza-
tion helps significantly in the forest-to-string case.
7 Conclusion
We have presented a new approach to tree-to-string
translation. It involves a source tree binarization
step and a standard forest-to-string translation step.
The method renders it unnecessary to have a k-best
parser to generate a packed forest. We have demon-
strated state-of-the-art results using a fast parser and
a simple tree binarizer that allows crossing at most
one bracket in each binarized node. We have also
shown that reducing search errors is important for
forest-to-string translation. We adapted the syn-
chronous binarization technqiue to improve search
and have shown significant gains. In addition, we
also presented a new cube-pruning-style algorithm
for rule extraction. In the new algorithm, it is easy to
adjust the figure-of-merit of rules for extraction. In
the future, we plan to improve the learning of trans-
lation rules with binarized forests.
Acknowledgments
We would like to thank the members of the MT team
at Google, especially Ashish Venugopal, Zhifei Li,
John DeNero, and Franz Och, for their help and dis-
cussions. We would also like to thank Daniel Gildea
for his suggestions on improving the paper.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and Metrics(MATR),
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270,
Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755?763,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, companion volume, pages 205?208, Sap-
poro, Japan.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 961?968, July.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (NAACL-04).
843
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proceedings
of the NAACL/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation (SSST), pages 33?40,
Rochester, NY.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Conference of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08:HLT), Columbus, OH. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), pages 625?630, Geneva, Switzerland.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the International Conference
on Computational Linguistics/Association for Compu-
tational Linguistics (COLING/ACL-06), Sydney, Aus-
tralia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Conference of the Associ-
ation for Computational Linguistics (ACL-07), Prague.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206?214, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th An-
nual Conference of the Association for Computational
Linguistics: Human Language Technologies (ACL-
08:HLT), pages 192?199.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
Coling 2004, pages 64?70, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02).
Arjen Poutsma. 2000. Data-oriented translation. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING-00).
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Con-
ference of the Association for Computational Linguis-
tics (ACL-05), pages 271?279, Ann Arbor, Michigan.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Conference of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), Columbus, OH.
ACL.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 746?
754, Prague, Czech Republic, June. Association for
Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
Association for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
844
North American chapter of the Association for Compu-
tational Linguistics (NAACL-06), pages 256?263, New
York, NY.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-08: HLT, pages 559?567, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 172?180,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City, June. As-
sociation for Computational Linguistics.
845
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 401?406,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Terminal-Aware Synchronous Binarization
Licheng Fang, Tagyoung Chung and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We present an SCFG binarization algorithm
that combines the strengths of early termi-
nal matching on the source language side and
early language model integration on the tar-
get language side. We also examine how dif-
ferent strategies of target-side terminal attach-
ment during binarization can significantly af-
fect translation quality.
1 Introduction
Synchronous context-free grammars (SCFG) are be-
hind most syntax-based machine translation mod-
els. Efficient machine translation decoding with an
SCFG requires converting the grammar into a bina-
rized form, either explicitly, as in synchronous bina-
rization (Zhang et al, 2006), where virtual nontermi-
nals are generated for binarization, or implicitly, as
in Earley parsing (Earley, 1970), where dotted items
are used.
Given a source-side binarized SCFG with termi-
nal set T and nonterminal set N , the time complex-
ity of decoding a sentence of length n with a m-gram
language model is (Venugopal et al, 2007):
O(n3(|N | ? |T |2(m?1))K)
where K is the maximum number of right-hand-side
nonterminals. SCFG binarization serves two impor-
tant goals:
? Parsing complexity for unbinarized SCFG
grows exponentially with the number of non-
terminals on the right-hand side of grammar
rules. Binarization ensures cubic time decod-
ing in terms of input sentence length.
? In machine translation, integrating language
model states as early as possible is essential to
reducing search errors. Synchronous binariza-
tion (Zhang et al, 2006) enables the decoder to
incorporate language model scores as soon as a
binarized rule is applied.
In this paper, we examine a CYK-like syn-
chronous binarization algorithm that integrates a
novel criterion in a unified semiring parsing frame-
work. The criterion we present has explicit consider-
ation of source-side terminals. In general, terminals
in a rule have a lower probability of being matched
given a sentence, and therefore have the effect of
?anchoring? a rule and limiting its possible applica-
tion points. Hopkins and Langmead (2010) formal-
ized this concept as the scope of a rule. A rule of
scope of k can be parsed in O(nk). The scope of a
rule can be calculated by counting the number of ad-
jacent nonterminal pairs and boundary nonterminals.
For example,
A? w1BCw2D
has scope two. Building on the concept of scope,
we define a cost function that estimates the expected
number of hyperedges to be built when a particular
binarization tree is applied to unseen data. This ef-
fectively puts hard-to-match derivations at the bot-
tom of the binarization tree, which enables the de-
coder to decide early on whether an unbinarized rule
can be built or not.
We also investigate a better way to handle target-
side terminals during binarization. In theory, differ-
ent strategies should produce equivalent translation
results. However, because decoding always involves
401
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
1 2 3 4 5 6 7
N
um
be
r o
f r
ul
es
Number of right-hand-side nonterminals
Total
Binarizable
Monotonic
Figure 1: Rule Statistics
pruning, we show that different strategies do have a
significant effect in translation quality.
Other works investigating alternative binarization
methods mostly focus on the effect of nonterminal
sharing. Xiao et al (2009) also proposed a CYK-
like algorithm for synchronous binarization. Appar-
ently the lack of virtual nonterminal sharing in their
decoder caused heavy competition between virtual
nonterminals, and they created a cost function to
?diversify? binarization trees, which is equivalent to
minimizing nonterminal sharing.
DeNero et al (2009b) used a greedy method to
maximize virtual nonterminal sharing on the source
side during the -LM parsing phase. They show that
effective source-side binarization can improve the ef-
ficiency of parsing SCFG. However, their method
works only on the source side, and synchronous bina-
rization is put off to the +LM decoding phase (DeN-
ero et al, 2009a).
Although these ideas all lead to faster decoding
and reduced search errors, there can be conflicts in
the constraints each of them has on the form of rules
and accommodating all of them can be a challenge.
In this paper, we present a cubic time algorithm to
find the best binarization tree, given the conflicting
constraints.
2 The Binarization Algorithm
An SCFG rule is synchronously binarizable if when
simultaneously binarizing source and target sides,
virtual nonterminals created by binarizations always
have contiguous spans on both sides (Huang, 2007).
Algorithm 1 The CYK binarization algorithm.
CYK-BINARIZE(X ? ??, ??)
for i = 0 . . . |?| ? 1 do
T [i, i + 1]? cinit(i)
for s = 2 . . . |?| do
for i = 0 . . . |?|-1 do
j ? i + s
for k = i + 1 . . . j ? 1 do
t? T [i, k] + T [k, j] + c(?i, k, j?)
T [i, j]? min(T [i, j], t)
Even with the synchronous binarization constraint,
many possible binarizations exist. Analysis of our
Chinese-English parallel corpus has shown that the
majority of synchronously binarizable rules with ar-
ity smaller than 4 are monotonic, i.e., the target-side
nonterminal permutation is either strictly increasing
or decreasing (See Figure 1). For monotonic rules,
any source-side binarization is also a permissible
synchronous binarization.
The binarization problem can be formulated as a
semiring parsing (Goodman, 1999) problem. We
define a cost function that considers different bina-
rization criteria. A CYK-like algorithm can be used
to find the best binarization tree according to the
cost function. Consider an SCFG rule X ? ??, ??,
where ? and ? stand for the source side and the tar-
get side. Let B(?) be the set of all possible bina-
rization trees for ?. With the cost function c defined
over hyperedges in a binarization tree t, the optimal
binarization tree t? is
t? = argmin
t?B(?)
?
h?t
c(h)
where c(h) is the cost of a hyperedge h in t.
The optimization problem can be solved by Al-
gorithm 1. ?i, k, j? denotes a hyperedge h that con-
nects the spans (i, k) and (k, j) to the span (i, j).
cinit is the initialization for the cost function c. We
can recover the optimal source-side binarization tree
by augmenting the algorithm with back pointers.
Binarized rules are generated by iterating over the
nodes in the optimal binarization tree, while attach-
ing unaligned target-side terminals. At each tree
node, we generate a virtual nonterminal symbol by
concatenating the source span it dominates.
We define the cost function c(h) to be a
tuple of component cost functions: c(h) =
402
(c1(h), c2(h), ...). When two costs a and b are com-
pared, the components are compared piecewise, i.e.
c < c? ? c1 < c?1 ? (c1 = c?1 ? c2 < c?2) ? . . .
If the (min,+) operators on each component cost
satisfy the semiring properties, the cost tuple is also
a semiring. Next, we describe our cost functions and
how we handle target-side terminals.
2.1 Synchronous Binarization as a Cost
We use a binary cost b to indicate whether a binariza-
tion tree is a permissible synchronous binarization.
Given a hyperedge ?i, k, j?, we say k is a permissible
split of the span (i, j) if and only if the spans (i, k)
and (k, j) are both synchronously binarizable and
the span (i, j) covers a consecutive sequence of non-
terminals on the target side. A span is synchronously
binarizable if and only if the span is of length one,
or a permissible split of the span exists. The cost b
is defined as:
b(?i, k, j?) =
{
T if k is a permissible split of (i, j)
F otherwise
binit(i) = T
Under this configuration, the semiring operators
(min,+) defined for the cost b are (?,?). Using b as
the first cost function in the cost function tuple guar-
antees that we will find a tree that is a synchronously
binarized if one exists.
2.2 Early Source-Side Terminal Matching
When a rule is being applied while parsing a sen-
tence, terminals in the rule have less chance of be-
ing matched. We can exploit this fact by taking ter-
minals into account during binarization and placing
terminals lower in the binarization tree. Consider the
following SCFG rule:
VP ? PP?? JJ NN,propose a JJ NN PP
The synchronous binarization algorithm of Zhang et
al. (2006) binarizes the rule1 by finding the right-
most binarizable points on the source side:
1We follow Wu (1997) and use square brackets for straight
rules and pointed brackets for inverted rules. We also mark
brackets with indices to represent virtual nonterminals.
VP ? PP [?? [JJ NN]1]2,[[propose a JJ NN]1]2 PP
The source side of the first binarized rule ?[]1 ? JJ
NN, propose a JJ NN? contains a very frequent non-
terminal sequence ?JJ NN?. If one were to parse
with the binarized rule, and if the virtual nontermi-
nal []1 has been built, the parser needs to continue
following the binarization tree in order to determine
whether the original rule would be matched. Further-
more, having two consecutive nonterminals adds to
complexity since the parser needs to test each split
point.
The following binarization is equally valid but in-
tegrates terminals early:
VP ? PP [[?? JJ]1 NN]2,[[propose a JJ]1 NN]2 PP
Here, the first binarized rule ?[]1 ? ?? JJ, pro-
pose a JJ? anchors on a terminal and enables earlier
pruning of the original rule.
We formulate this intuition by asking the ques-
tion: given a source-side string ?, what binarization
tree, on average, builds the smallest number of hy-
peredges when the rule is applied? This is realized
by defining a cost function e which estimates the
probability of a hyperedge ?i, k, j? being built. We
use a simple model: assume each terminal or non-
terminal in ? is matched independently with a fixed
probability, then a hyperedge ?i, k, j? is derived if
and only if all symbols in the source span (i, j) are
matched. The cost e is thus defined as2
e(?i, k, j?) =
?
i?`<j
p(?`)
einit(i) = 0
For terminals, p(?`) can be estimated by counting
the source side of the training corpus. For nontermi-
nals, we simply assume p(?`) = 1.
With the hyperedge cost e, the cost of a binariza-
tion tree t is
?
h?t e(h), i.e., the expected number of
hyperedges to be built when a particular binarization
of a rule is applied to unseen data.3 The operators
2In this definition, k does not appear on the right-hand side
of the equation because all edges leading to the same span share
the same cost value.
3Although this cost function is defined as an expectation, it
does not form an expectation semiring (Eisner, 2001) because
403
for the cost e are the usual (min,+) operators on
real numbers.
2.3 Maximizing Nonterminal Sharing
During binarization, newly created virtual nontermi-
nals are named according to the symbols (terminals
and nonterminals) that they generate. For example, a
new virtual nonterminal covering two nonterminals
NP and VP is named NP+VP. To achieve maximum
virtual nonterminal sharing, we also define a cost
function n to count the number new nonterminals
generated by a binarization tree. We keep track of
all the nonterminals that have been generated when
binarizing a rule set. When the i?th rule is being
binarized, a nonterminal is considered new if it is
previously unseen in binarizing rules 1 to i?1. This
greedy approach is similar to that of DeNero et al
(2009b). The cost function is thus defined as:
n(?i, k, j?) =
{
1 if the VT for span (i, j) is new
0 otherwise
ninit(i) = 0
The semiring operators for this cost are also
(min,+) on real numbers.
2.4 Late Target-Side Terminal Attachment
Once the optimal source-side binarization tree is
found, we have a good deal of freedom to attach
target-side terminals to adjacent nonterminals, as
long as the bracketing of nonterminals is not vio-
lated. The following example is taken from Zhang
et al (2006):
ADJP ? RB?? PP? NN,RB responsible for the NN PP
With the source-side binarization fixed, we can pro-
duce distinct binarized rules by choosing different
ways of attaching target-side terminals:
ADJP ? [RB??]1 ? [PP?]3 NN ?2,[RB]1 ? resp. for the NN [PP]3 ?2
ADJP ? [RB??]1 ? [PP?]3 NN ?2,[RB]1 resp. for the ? NN [PP]3 ?2
The first binarization is generated by attaching the
target-side terminals as low as possible in a post-
it is defined as an expectation over input strings, instead of an
expectation over trees.
order traversal of the binarization tree. The conven-
tional wisdom is that early consideration of target-
side terminals promotes early language model score
integration (Huang et al, 2009). The second bina-
rization, on the contrary, attaches the target-side ter-
minals as high as possible in the binarization tree.
We argue that this late target-side terminal attach-
ment is in fact better for two reasons.
First, as in the example above, compare the fol-
lowing two rules resulting from early attachment of
target terminals and late attachment of target termi-
nals:
??2 ? []3 NN, resp. for the NN []3
??2 ? []3 NN, NN []3
The former has a much smaller chance of sharing
the same target side with other binarized rules be-
cause on the target side, many nonterminals will be
attached without any lexical evidence. We are more
likely to have a smaller set of rules with the latter
binarization.
Second, with the presence of pruning, dynamic
programming states that are generated by rules with
many target-side terminals are disadvantaged when
competing with others in the same bin because of
the language model score. As a result, these would
be discarded earlier, even if the original unbinarized
rule has a high probability. Consequently, we lose
the benefit of using larger rules, which have more
contextual information. We show in our experiment
that late target side terminal attachment significantly
outperforms early target side terminal attachment.
Although the problem can be alleviated by pre-
computing a language model score for the original
unbinarized rule and applying the heuristic to its bi-
narized rules, this still grants no benefit over late ter-
minal attachment. We show in our experiment that
late target-side terminal attachment significantly out-
performs early target side terminal attachment.
3 Experiments
3.1 Setup
We test our binarization algorithm on an Chinese-
English translation task. We extract a GHKM gram-
mar (Galley et al, 2004) from a parallel corpus with
the parsed English side with some modification so
404
-395
-390
-385
-380
-375
-370
-365
-360
-355
 10  100
M
od
el
 S
co
re
 (lo
g-p
rob
ab
ilit
y)
Seconds / Sentence (log scale)
(b,n)-early
(b,n)-late
(b,e,n)-early
(b,e,n)-late
Figure 2: Model Scores vs. Decoding Time
 17.5
 18
 18.5
 19
 19.5
 20
 20.5
 10  100
BL
EU
Seconds / Sentence (log scale)
(b,n)-early
(b,n)-late
(b,e,n)-early
(b,e,n)-late
Figure 3: BLEU Scores vs Decoding Time
as not to extract unary rules (Chung et al, 2011).
The corpus consists of 250K sentence pairs, which
is 6.3M words on the English side. A 392-sentence
test set was to evaluate different binarizations.
Decoding is performed by a general CYK SCFG
decoder developed in-house and a trigram language
model is used. The decoder runs the CYK algorithm
with cube-pruning (Chiang, 2007). In all our exper-
iments, we discard unbinarizable rules, which have
been shown by Zhang et al (2006) to have no signif-
icant effect on translation accuracy.
3.2 Results
We first discuss effects of maximizing nonterminal
sharing. Having nonterminal sharing maximization
as a part of the cost function for binarization did
yield slightly smaller grammars. However, we could
not discern any noticeable difference or trend in
terms of BLEU score, decoding speed, or model
score when comparing translation results that used
grammars that employed nonterminal sharing max-
imization and ones that did not. In the rest of this
section, all the results we discuss use nonterminal
sharing maximization as a part of the cost function.
We then compare the effects of early target-side
terminal attachment and late attachment. Figure 2
shows model scores of each decoder run with vary-
ing bin sizes, and Figure 3 shows BLEU scores
for corresponding runs of the experiments. (b,n)-
early is conventional synchronous binarization with
early target-side terminal attachment and nontermi-
nal sharing maximization, (b,n)-late is the same set-
ting with late target-side terminal attachment. The
tuples represent cost functions that are discussed in
Section 2. The figures clearly show that late attach-
ment of target-side terminals is better. Although
Figure 3 does not show perfect correlation with Fig-
ure 2, it exhibits the same trend. The same goes for
(b,e,n)-early and (b,e,n)-late.
Finally, we examine the effect of including the
source-side terminal-aware cost function, denoted
?e? in our cost tuples. Comparing (b,e,n)-late with
(b,n)-late, we see that terminal-aware binarization
gives better model scores and BLEU scores. The
trend is the same when one compares (b,e,n)-early
and (b,n)-early.
4 Conclusion
We examined binarizing synchronous context-free
grammars within a semiring parsing framework. We
proposed binarization methods that explicitly take
terminals into consideration. We have found that al-
though binarized rules are already scope 3, we can
still do better by putting infrequent derivations as
low as possible in a binarization tree to promote
early pruning. We have also found that attaching
target side terminals as late as possible promotes
smarter pruning of rules thereby improving model
score and translation quality at decoding time. Im-
provements we discuss in this paper result in better
search, and hence better translation.
Acknowledgments We thank Hao Zhang for use-
ful discussions and the anonymous reviewers for
their helpful comments. This work was supported
by NSF grants IIS-0546554 and IIS-0910611.
405
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon, June.
Association for Computational Linguistics.
J. DeNero, A. Pauls, and D. Klein. 2009a. Asynchronous
binarization for synchronous grammars. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 141?144. Association for Computational
Linguistics.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009b. Efficient parsing for transducer grammars. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 227?235, Boulder, Colorado, June. Association
for Computational Linguistics.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
J. Eisner. 2001. Expectation semirings: Flexible EM
for learning finite-state transducers. In Proceedings of
the ESSLLI workshop on finite-state methods in NLP.
Citeseer.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proceedings
of the NAACL/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation (SSST), pages 33?40,
Rochester, NY.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In NAACL07,
Rochester, NY, April.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?403.
T. Xiao, M. Li, D. Zhang, J. Zhu, and M. Zhou. 2009.
Better synchronous binarization for machine transla-
tion. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing: Vol-
ume 1-Volume 1, pages 362?370. Association for Com-
putational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (NAACL-06), pages 256?263, New
York, NY.
406
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 413?417,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Issues Concerning Decoding with Synchronous Context-free Grammar
Tagyoung Chung, Licheng Fang and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We discuss some of the practical issues that
arise from decoding with general synchronous
context-free grammars. We examine problems
caused by unary rules and we also examine
how virtual nonterminals resulting from bina-
rization can best be handled. We also inves-
tigate adding more flexibility to synchronous
context-free grammars by adding glue rules
and phrases.
1 Introduction
Synchronous context-free grammar (SCFG) is
widely used for machine translation. There are many
different ways to extract SCFGs from data. Hiero
(Chiang, 2005) represents a more restricted form of
SCFG, while GHKM (Galley et al, 2004) uses a gen-
eral form of SCFG.
In this paper, we discuss some of the practical is-
sues that arise from decoding general SCFGs that
are seldom discussed in the literature. We focus on
parsing grammars extracted using the method put
forth by Galley et al (2004), but the solutions to
these issues are applicable to other general forms of
SCFG with many nonterminals.
The GHKM grammar extraction method produces
a large number of unary rules. Unary rules are the
rules that have exactly one nonterminal and no ter-
minals on the source side. They may be problematic
for decoders since they may create cycles, which are
unary production chains that contain duplicated dy-
namic programming states. In later sections, we dis-
cuss why unary rules are problematic and investigate
two possible solutions.
GHKM grammars often have rules with many
right-hand-side nonterminals and require binariza-
tion to ensure O(n3) time parsing. However, bina-
rization creates a large number of virtual nontermi-
nals. We discuss the challenges of, and possible so-
lutions to, issues arising from having a large num-
ber of virtual nonterminals. We also compare bina-
rizing the grammar with filtering rules according to
scope, a concept introduced by Hopkins and Lang-
mead (2010). By explicitly considering the effect
of anchoring terminals on input sentences, scope-
3 rules encompass a much larger set of rules than
Chomsky normal form but they can still be parsed in
O(n3) time.
Unlike phrase-based machine translation, GHKM
grammars are less flexible in how they can seg-
ment sentence pairs into phrases because they are
restricted not only by alignments between words in
sentence pairs, but also by target-side parse trees. In
general, GHKM grammars suffer more from data
sparsity than phrasal rules. To alleviate this issue,
we discuss adding glue rules and phrases extracted
using methods commonly used in phrase-based ma-
chine translation.
2 Handling unary rules
Unary rules are common in GHKM grammars. We
observed that as many as 10% of the rules extracted
from a Chinese-English parallel corpus are unary.
Some unary rules are the result of alignment er-
rors, but other ones might be useful. For example,
Chinese lacks determiners, and English determiners
usually remain unaligned to any Chinese words. Ex-
tracted grammars include rules that reflect this fact:
NP ? NP, the NP
NP ? NP, a NP
413
However, unary rules can be problematic:
? Unary production cycles corrupt the translation
hypergraph generated by the decoder. A hyper-
graph containing a unary cycle cannot be topo-
logically sorted. Many algorithms for parame-
ter tuning and coarse-to-fine decoding, such as
the inside-outside algorithm and cube-pruning,
cannot be run in the presence of unary cycles.
? The existence of many unary rules of the form
?NP ? NP, the NP? quickly fills a pruning bin
with guesses of English words to insert without
any source-side lexical evidence.
The most obvious way of eliminating problem-
atic unary rules would be converting grammars into
Chomsky normal form. However, this may result
in bloated grammars. In this section, we present
two different ways to handle unary rules. The first
involves modifying the grammar extraction method,
and the second involves modifying the decoder.
2.1 Modifying grammar extraction
We can modify the grammar extraction method such
that it does not extract any unary rules. Galley et al
(2004) extracts rules by segmenting the target-side
parse parse tree based on frontier nodes. We modify
the definition of a frontier node in the following way.
We label frontier nodes in the English parse tree, and
examine the Chinese span each frontier node cov-
ers. If a frontier node covers the same span as the
frontier node that immediately dominates it, then the
dominated node is no longer considered a frontier.
This modification prevents unary rules from being
extracted.
Figure 1 shows an example of an English-Chinese
sentence pair with the English side automatically
parsed. Frontier nodes in the tree in the original
GHKM rule extraction method are marked with a
box. With the modification, only the top bold-
faced NP would be considered a frontier node. The
GHKM rule extraction results in the following rules:
NPB ????, the snowy egret
NP ? NPB, NPB
PP ? NP, with NP
NP ? PP, romance PP
With the change, only the following rule is extracted:
NP
NPB
NNP
romance
PP
IN
with
NP
NPB
DT
the
JJ
snowy
NN
egret
?? ? ? ?
Figure 1: A sentence fragment pair with erroneous align-
ment and tokenization
NP ????, romance with the snowy egret
We examine the effect of this modification has on
translation performance in Section 5.
2.2 Modifying the decoder
Modifying how grammars are extracted has an ob-
vious down side, i.e., the loss of generality. In the
previous example, the modification results in a bad
rule, which is the result of bad alignments. Before
the modification, the rule set includes a good rule:
NPB ????, the snowy egret
which can be applied at test time. Because of this,
one may still want to decode with all available unary
rules. We handle unary rules inside the decoder in
the following ways:
? Unary cycle detection
The na?ve way to detect unary cycles is back-
tracking on a unary chain to see if a newly gen-
erated item has been generated before. The run-
ning time of this is constrained only by the num-
ber of possible items in a chart span. In prac-
tice, however, this is often not a problem: if all
unary derivations have positive costs and a pri-
ority queue is used to expand unary derivations,
414
only the best K unary items will be generated,
where K is the pruning constant.
? Ban negative cost unary rules
When tuning feature weights, an optimizer may
try feature weights that may give negative costs
to unary productions. This causes unary deriva-
tions to go on forever. The solution is to set
a maximum length for unary chains, or to ban
negative unary productions outright.
3 Issues with binarization
3.1 Filtering and binarization
Synchronous binarization (Zhang et al, 2006) is
an effective method to reduce SCFG parsing com-
plexity and allow early language model integration.
However, it creates virtual nonterminals which re-
quire special attention at parsing time. Alternatively,
we can filter rules that have more than scope-3 to
parse in O(n3) time with unbinarized rules. This
requires Earley (Earley, 1970) style parsing, which
does implicit binarization at decoding time. Scope-
filtering may filter out unnecessarily long rules that
may never be applied, but it may also throw out
rules with useful contextual information. In addi-
tion, scope-filtering does not accommodate early lan-
guage model state integration. We compare the two
with an experiment. For the rest of the section, we
discuss issues created by virtual nonterminals.
3.2 Handling virtual nonterminals
One aspect of grammar binarization that is rarely
mentioned is how to assign probabilities to binarized
grammar rules. The na?ve solution is to assign prob-
ability one to any rule whose left-hand side is a vir-
tual nonterminal. This maintains the original model.
However, it is generally not fair to put chart items of
virtual nonterminals and those of regular nontermi-
nals in the same bin, because virtual items have arti-
ficially low costs. One possible solution is adding a
heuristic to push up the cost of virtual items for fair
comparison.
For our experiments, we use an outside estimate
as a heuristic for a virtual item. Consider the follow-
ing rule binarization (only the source side shown):
A ? BCD : ? log(p) ? V ? BC : 0A ? VD : ? log(p)
A ? BCD is the orginal rule and ? log(p) is the cost
of the rule. In decoding time, when a chart item is
generated from the binarized rule V ? BC, we add
? log(p) to its total cost as an optimistic estimate of
the cost to build the original unbinarized rule. The
heuristic is used only for pruning purposes, and it
does not change the real cost. The idea is similar
to A* parsing (Klein and Manning, 2003). One com-
plication is that a binarized rule can arise from multi-
ple different unbinarized rules. In this case, we pick
the lowest cost among the unbinarized rules as the
heuristic.
Another approach for handling virtual nontermi-
nals would be giving virtual items separate bins and
avoiding pruning them at all. This is usually not
practical for GHKM grammars, because of the large
number of nonterminals.
4 Adding flexibility
4.1 Glue rules
Because of data sparsity, an SCFG extracted from
data may fail to parse sentences at test time. For
example, consider the following rules:
NP ? JJ NN, JJ NN
JJ ? c1, e1
JJ ? c2, e2
NN ? c3, e3
This set of rules is able to parse the word sequence
c1 c3 and c2 c3 but not c1 c2 c3, if we have not seen
?NP ? JJ JJ NN? at training time. Because SCFGs
neither model adjunction, nor are they markovized,
with a small amount of data, such problems can oc-
cur. Therefore, we may opt to add glue rules as used
in Hiero (Chiang, 2005):
S ? C, C
S ? S C, S C
where S is the goal state and C is the glue nonter-
minal that can produce any nonterminals. We re-
fer to these glue rules as the monotonic glue rules.
We rely on GHKM rules for reordering when we use
the monotonic glue rules. However, we can also al-
low glue rules to reorder constituents. Wu (1997)
presents a better-constrained grammar designed to
only produce tail-recursive parses. See Table 1 for
the complete set of rules. We refer to these rules as
ABC glue rules. These rules always generate left-
415
S ? A A ? [A B] B ? ? B A ?
S ? B A ? [B B] B ? ? A A ?
S ? C A ? [C B] B ? ? C A ?
A ? [A C] B ? ? B C ?
A ? [B C] B ? ? A C ?
A ? [C C] B ? ? C C ?
Table 1: The ABC Grammar. We follow the convention
of Wu (1997) that square brackets stand for straight rules
and angle brackets stand for inverted rules.
heavy derivations, weeding out ambiguity and mak-
ing search more efficient. We learn probabilities of
ABC glue rules by using expectation maximization
(Dempster et al, 1977) to train a word-level Inver-
sion Transduction Grammar from data.
In our experiments, depending on the configura-
tion, the decoder failed to parse about 5% of sen-
tences without glue rules, which illustrates their ne-
cessity. Although it is reasonable to believe that re-
ordering should always have evidence in data, as
with GHKM rules, we may wish to reorder based
on evidence from the language model. In our ex-
periments, we compare the ABC glue rules with the
monotonic glue rules.
4.2 Adding phrases
GHKM grammars are more restricted than the
phrase extraction methods used in phrase-based
models, since, in GHKM grammar extraction,
phrase segmentation is constrained by parse trees.
This may be a good thing, but it suffers from loss
of flexibility, and it also cannot use non-constituent
phrases. We use the method of Koehn et al (2003)
to extract phrases, and, for each phrase, we add a
rule with the glue nonterminal as the left-hand side
and the phrase pair as the right-hand side. We exper-
iment to see whether adding phrases is beneficial.
There have been other efforts to extend GHKM
grammar to allow more flexible rule extraction. Gal-
ley et al (2006) introduce composed rules where
minimal GHKM rules are fused to form larger rules.
Zollmann and Venugopal (2006) introduce a model
that allows more generalized rules to be extracted.
BLEU
Baseline + monotonic glue rules 20.99
No-unary + monotonic glue rules 23.83
No-unary + ABC glue rules 23.94
No-unary (scope-filtered) + monotonic 23.99
No-unary (scope-filtered) + ABC glue rules 24.09
No-unary + ABC glue rules + phrases 23.43
Table 2: BLEU score results for Chinese-English with
different settings
5 Experiments
5.1 Setup
We extracted a GHKM grammar from a Chinese-
English parallel corpus with the English side parsed.
The corpus consists of 250K sentence pairs, which
is 6.3M words on the English side. Terminal-aware
synchronous binarization (Fang et al, 2011) was ap-
plied to all GHKM grammars that are not scope-
filtered. MERT (Och, 2003) was used to tune pa-
rameters. We used a 392-sentence development set
with four references for parameter tuning, and a 428-
sentence test set with four references for testing. Our
in-house decoder was used for experiments with a
trigram language model. The decoder is capable
of both CNF parsing and Earley-style parsing with
cube-pruning (Chiang, 2007).
For the experiment that incorporated phrases, the
phrase pairs were extracted from the same corpus
with the same set of alignments. We have limited
the maximum size of phrases to be four.
5.2 Results
Our result is summarized in Table 2. The baseline
GHKM grammar with monotonic glue rules yielded
a worse result than the no-unary grammar with the
same glue rules. The difference is statistically signif-
icant at p < 0.05 based on 1000 iterations of paired
bootstrap resampling (Koehn, 2004).
Compared to using monotonic glue rules, using
ABC glue rules brought slight improvements for
both the no-unary setting and the scope-filtered set-
ting, but the differences are not statistically signifi-
cant. In terms of decoding speed and memory usage,
using ABC glues and monotonic glue rules were vir-
tually identical. The fact that glue rules are seldom
used at decoding time may account for why there is
416
little difference in using monotonic glue rules and us-
ing ABC glue rules. Out of all the rules that were ap-
plied to decoding our test set, less than one percent
were glue rules, and among the glue rules, straight
glue rules outnumbered inverted ones by three to
one.
Compared with binarized no-unary rules, scope-
3 filtered no-unary rules retained 87% of the rules
but still managed to have slightly better BLEU score.
However, the score difference is not statistically sig-
nificant. Because the size of the grammar is smaller,
compared to using no-unary grammar, it used less
memory at decoding time. However, decoding speed
was somewhat slower. This is because the decoder
employs Early-style dotted rules to handle unbina-
rized rules, and in order to decode with scope-3
rules, the decoder needs to build dotted items, which
are not pruned until a rule is completely matched,
thus leading to slower decoding.
Adding phrases made the translation result
slightly worse. The difference is not statistically
significant. There are two possible explanations for
this. Since there were more features to tune, MERT
may have not done a good job. We believe the
more important reason is that once a phrase is used,
only glue rules can be used to continue the deriva-
tion, thereby losing the richer information offered
by GHKM grammar.
6 Conclusion
In this paper, we discussed several issues concerning
decoding with synchronous context-free grammars,
focusing on grammars resulting from the GHKM
extraction method. We discussed different ways to
handle cycles. We presented a modified grammar
extraction scheme that eliminates unary rules. We
also presented a way to decode with unary rules in
the grammar, and examined several different issues
resulting from binarizing SCFGs. We finally dis-
cussed adding flexibility to SCFGs by adding glue
rules and phrases.
Acknowledgments We would like to thank the
anonymous reviewers for their helpful comments.
This work was supported by NSF grants IIS-
0546554 and IIS-0910611.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?21.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
Licheng Fang, Tagyoung Chung, and Daniel Gildea.
2011. Terminal-aware synchronous binarization. In
Proceedings of the ACL 2011 Conference Short Pa-
pers, Portland, Oregon, June. Association for Compu-
tational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-04, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-
rich syntactic translation models. In Proceedings of
COLING/ACL-06, pages 961?968, July.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceedings
of NAACL-03.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL-03, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL-
03.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?403.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of NAACL-06, pages 256?
263, New York, NY.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. Workshop on Statistical Machine Translation,
pages 138?141.
417
