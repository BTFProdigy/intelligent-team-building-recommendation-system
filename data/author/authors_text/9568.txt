Coling 2010: Poster Volume, pages 72?80,
Beijing, August 2010
Composition of Semantic Relations: Model and Applications
Eduardo Blanco, Hakki C. Cankaya and Dan Moldovan
Human Language Technology Research Institute
The University of Texas at Dallas
{eduardo,candan,moldovan}@hlt.utdallas.edu
Abstract
This paper presents a framework for com-
bining semantic relations extracted from
text to reveal even more semantics that
otherwise would be missed. A set of 26 re-
lations is introduced, with their arguments
defined on an ontology of sorts. A seman-
tic parser is used to extract these relations
from noun phrases and verb argument
structures. The method was successfully
used in two applications: rapid customiza-
tion of semantic relations to arbitrary do-
mains and recognizing entailments.
1 Introduction
Semantic representation of text facilitates infer-
ences, reasoning, and greatly improves the per-
formance of Question Answering, Information
Extraction, Machine Translation and other NLP
applications. Broadly speaking, semantic rela-
tions are unidirectional underlying connections
between concepts. For example, the noun phrase
the car engine encodes a PART-WHOLE relation:
the engine is a part of the car.
Semantic relations are the building blocks for
creating a semantic structure of a sentence. There
is a growing interest in text semantics fueled by
the new wave of semantic technologies and on-
tologies that aim at transforming unstructured text
into structured knowledge. More and more enter-
prises and academic organizations have adopted
the World Wide Web Consortium (W3C) Re-
source Description Framework (RDF) specifica-
tion as a standard representation of text knowl-
edge. This is based on semantic triples, which can
be used to represent semantic relations.
The work reported in this paper aims at extract-
ing as many semantic relations from text as possi-
ble. Semantic parsers (SP) extract semantic rela-
tions from text. Typically they detect relations be-
tween adjacent concepts or verb argument struc-
tures, leaving considerable semantics unrevealed.
For example, given John is a rich man, a typical
SP extracts John is a man and man is rich, but not
John is rich. The third relation can be extracted
by combining the two relations detected by the
parser. The observation that combining elemen-
tary semantic relations yields more relations is the
starting point and the motivation for this work.
2 Related Work
In Computational Linguistics, WordNet (Miller,
1995), FrameNet (Baker et al, 1998) and Prop-
Bank (Palmer et al, 2005) are probably the most
used semantic resources. Like our approach and
unlike PropBank, FrameNet annotates semantics
between concepts regardless of their position in a
parse tree. Unlike us, they use a predefined set of
frames to be filled. PropBank adds semantic an-
notation on top of the Penn TreeBank and it con-
tains only annotations between a verb and its ar-
guments. Moreover, the semantics of a given label
depends on the verb. For example, ARG2 is used
for INSTRUMENT and VALUE.
Copious work has been done lately on seman-
tic roles (Ma`rquez et al, 2008). Approaches to
detect semantic relations usually focus on partic-
ular lexical and syntactic patterns or kind of ar-
guments. There are both unsupervised (Turney,
2006) and supervised approaches. The SemEval-
2007 Task 4 (Girju et al, 2007) focused on rela-
tions between nominals. Work has been done on
detecting relations between noun phrases (Davi-
dov and Rappoport, 2008; Moldovan et al, 2004),
named entities (Hirano et al, 2007), and clauses
(Szpakowicz et al, 1995). There have been pro-
72
posals to detect a particular relation, e.g., CAUSE
(Chang and Choi, 2006), INTENT (Tatu, 2005) and
PART-WHOLE (Girju et al, 2006).
Researchers have also worked on combining se-
mantic relations. Harabagiu and Moldovan (1998)
combine WordNet relations and Helbig (2005)
transforms chains of relations into theoretical ax-
ioms. Some use logic as the underlying formal-
ism (Lakoff, 1970; Sa?nchez Valencia, 1991), more
ideas can be found in (Copestake et al, 2001).
3 Approach
In contrast to First Order Logic used in AI to rep-
resent text knowledge, we believe text semantics
should be represented using a fixed set of rela-
tions. This facilitates a more standard represen-
tation and extraction automation which in turn al-
lows reasoning. The fewer the relation types, the
easier it is to reason and perform inferences. Thus,
a compromise has to be made between having
enough relation types to adequately represent text
knowledge and yet keeping the number small for
making the extraction and manipulation feasible.
The main contributions of this paper are: (i) an
extended definition of a set of 26 semantic rela-
tions resulted after many iterations and pragmatic
considerations; (ii) definition of a semantic calcu-
lus, a framework to manipulate and compose se-
mantic relations (CSR); (iii) use of CSR to rapidly
customize a set of semantic relations; and (iv) use
of CSR to detect entailments. The adoption of
CSR to other semantic projects does not require
any modification of existing tools while being able
to detect relations ignored by such tools.
4 Semantic Relations
Formally, a semantic relation is represented as
R(x, y), where R is the relation type and x and
y the first and second argument. R(x, y) should be
read as x is R of y. The sentence ?John painted his
truck? yields AGENT(John, painted ), THEME(his
truck, painted ) and POSSESSION(truck, John).
Extended definition Given a semantic relation R,
DOMAIN(R) and RANGE(R) are defined as the set
of sorts of concepts that can be part of the first
and second argument. A semantic relation R(x,
y) is defined by its: (i) relation type R; (ii) DO-
MAIN(R); and (iii) RANGE(R). Stating restric-
tions for DOMAIN and RANGE has several advan-
tages: it (i) helps distinguishing between relations,
e.g., [tall]ql and [John]aco can be linked through
VALUE, but not POSSESSION; (ii) helps discard-
ing potential relations that do not hold, e.g., ab-
stract objects do not have INTENT; and (iii) helps
combining semantic relations (Section 5).
Ontology of Sorts In order to define DOMAIN(R)
and RANGE(R), we use a customized ontology
of sorts (Figure 1) modified from (Helbig, 2005).
The root corresponds to entities, which refers to all
things about which something can be said.
Objects can be either concrete or abstract. The
former occupy space, are touchable and tangi-
ble. The latter are intangible; they are somehow a
product of human reasoning. Concrete objects are
further divided into animate or inanimate. The for-
mer have life, vigor or spirit; the later are dull,
without life. Abstract objects are divided into tem-
poral or non temporal. The first corresponds to ab-
stractions regarding points or periods of time (e.g.
July, last week); the second to any other abstrac-
tion (e.g. disease, justice). Abstract objects can be
sensually perceived, e.g., pain, odor.
Situations are anything that happens at a time
and place. Simply put, if one can think of the time
and location of an entity, it is a situation. Events
(e.g. mix, grow) imply a change in the status of
other entities, states (e.g. standing next to the
door) do not. Situations can be expressed by verbs
(e.g. move, print) or nouns (e.g. party, hurricane).
Descriptors complement entities by stating prop-
erties about their spatial or temporal context. They
are composed of an optional non-content word
signaling the local or temporal context and another
entity. Local descriptors are further composed of
a concrete object or situation, e.g., [above]prep [the
roof]co; temporal descriptors by a temporal abstract
object or situation, e.g., [during]prep [the party]ev .
The non-content word signaling the local or tempo-
ral context is usually present, but not always, e.g.,
?The [birthplace]ev of his mother is [Ankara]loc?.
Qualities represent characteristics than can be
assigned to entities. They can be quantifiable like
tall and heavy, or unquantifiable like difficult and
sleepy. Quantities represent quantitative character-
istics of concepts, e.g., a few pounds, 22 yards.
73
Entity [ent]
Situation [si]
State [st] Event [ev]
Quantity [qn] Object [o]
Concrete [co]
Animate [aco] Inanimate [ico]
Abstract [ao]
Temporal [tao] Non temporal [ntao]
Quality [ql] Descriptor [des]
Temporal [tmp] Local [loc]
Figure 1: The ontology of sorts of concepts and their acronyms.
Properties
Cluster Relation type Abr. Class. r s t DOMAIN ? RANGE Example
Reason
CAUSE CAU iv - -
? [si] ? [si] CAU(earthquake, tsunami )
JUSTIFICATION JST iv - -
? [si ? ntao] ? [si] JST(it is forbidden, don?t smoke)
INFLUENCE IFL iv - -
? [si] ? [si] IFL(missing classes, poor grade)
Goal INTENT INT i - - - [si] ? [aco] INT(teach, professor)
PURPOSE PRP v - -
? [si ? ntao] ? [si ? co ? ntao] PRP(storage, garage)
Object modifiers VALUE VAL v - - - [ql] ? [o ? si] VAL(smart, kids)
SOURCE SRC ii - -
? [loc ? ql ? ntao ? ico] ? [o] SRC(Mexican, students)
Syntactic subjects
AGENT AGT iii - - - [aco] ? [si] AGT(John, bought )
EXPERIENCER EXP iii - - - [o] ? [si] EXP(John, heard )
INSTRUMENT INS iii - - - [co ? ntao] ? [si] INS(the hammer, broke)
Direct objects
THEME THM iii - - - [o] ? [ev] THM(a car, bought )
TOPIC TPC iii - - - [o ? si] ? [ev] TPC(flowers, gave)
STIMULUS STI iii - - - [o] ? [ev] STI(the train, heard )
Association ASSOCIATION ASO v
? ? ? [ent] ? [ent] ASO(fork, knife)
KINSHIP KIN ii
? ? ? [aco] ? [aco] KIN(John, his wife)
None
IS-A ISA ii - -
? [o] ? [o] ISA(gas guzzler, car)
PART-WHOLE PW ii - - * [o] ? [o] ? [l] ? [l] ? [t] ? [t] PW(engine, car)
MAKE MAK ii - - - [co ? ntao] ? [co ? ntao] MAK(cars, BMW)
POSSESSION POS ii - -
? [co] ? [co] POS(Ford F-150, John)
MANNER MNR iii - - - [ql ? st ? ntao] ? [si] MNR(quick, delivery)
RECIPIENT RCP iii - - - [co] ? [ev] RCP(Mary, gave)
SYNONYMY SYN v
? ? ? [ent] ? [ent] SYN(a dozen, twelve)
AT-LOCATION AT-L v
?
- * [o ? si] ? [loc] AT-L(party, John?s house)
AT-TIME AT-T v
?
- * [o ? si] ? [tmp] AT-T(party, last Saturday)
PROPERTY PRO v - - - [ntao] ? [o ? si] PRO(height, John)
QUANTIFICATION QNT v - - - [qn] ? [si ? o] QNT(a dozen, eggs)
Table 1: The set of 26 relations clustered and classified with their properties (reflexive, symmetric,
transitive) and examples. An asterisk indicates that the property holds under certain conditions.
4.1 Semantic Relation Types
This work focuses on the set of 26 semantic rela-
tions depicted in Table 1. We found this set spe-
cific enough to capture the most frequent seman-
tics of text without bringing unnecessary overspe-
cialization. The set is inspired by several pre-
vious proposals. Fillmore introduced the notion
of case frames and proposed a set of nine roles:
AGENT, EXPERIENCER, INSTRUMENT, OBJECT,
SOURCE, GOAL, LOCATION, TIME and PATH
(Fillmore, 1971). Fillmore?s work was extended
to FrameNet (Baker et al, 1998). PropBank
(Palmer et al, 2005) annotates a set of 17 seman-
tic roles in a per-verb basis.
We aim to encode relations not only between
a verb and its arguments, but also between and
within noun phrases and adjective phrases. There-
fore, more relations are added to the set. It
includes relations present in WordNet (Miller,
1995), such as IS-A, PART-WHOLE and CAUSE.
Szpakowicz et al (1995) proposed a set of nine
relations and Turney (2006) a set of five. Rosario
and Hearst (2004) proposed a set of 38 relations
including standard case roles and a set of specific
relations for medical domain. Helbig (2005) pro-
posed a set of 89 relations, including ANTONYMY
and several TEMPORAL relations, e.g. SUCCES-
SION, EXTENSION, END.
Our set clusters some of the previous propos-
als (e.g. we only consider AT-TIME) and discards
relations proposed elsewhere when they did not
occur frequently enough in our experiments. For
example, even though ANTONYMY and ENTAIL-
MENT are semantically grounded, they are very
infrequent and we do not deal with them. Our
pragmatic goal is to capture as many semantics as
possible with as few relations as possible. How-
74
ever, we show (Section 7.1) that our set can be
easily customized to a specific domain.
The 26 relations are clustered such that rela-
tions belonging to the same cluster are close in
meaning. Working with clusters is useful because
it allows us to: (i) map to other proposed relations,
justifying the chosen set of relations; (ii) work
with different levels of specificity; and (iii) reason
with the relations in a per cluster basis.
The reason cluster includes relations between a
concept having a direct impact on another. CAU(x,
y) holds if y would not hold if x did not happen.
JST(x, y) encodes a moral cause, motive or so-
cially convened norm. If IFL(x, y), x affects the
intensity of y, but it does not affect its occurrence.
The goal cluster includes INT and PRP. INT(x,
y) encodes intended consequences, which are vo-
litional. PRP(x, y) is a broader relation and can be
defined for inanimate objects.
The object modifiers cluster encodes descriptions
of objects and situations: SRC(x, y) holds if x ex-
presses the origin of y. VAL(x, y) holds for any
other attribute, e.g. heavy, handsome.
The syntactic subjects cluster includes relations
linking a syntactic subject and a situation. The dif-
ferences rely on the characteristics of the subject
and the connection per se. AGT(x, y) encodes an
intentional doer, x must be volitional. If EXP(x,
y), x does not change the situation, it only expe-
riences y; it does not participate intentionally in y
either. If INS(x, y), x is used to perform y, x is a
tool or device that facilitates y.
The direct objects cluster includes relations en-
coding syntactic direct objects. THM(x, y) holds
if x is affected or directly involved by y. TPC(x, y)
holds if y is a communication verb, like talk and
argue. STI(x, y) holds if y is a perception verb
and x a stimulus that makes y happen.
The association cluster includes ASO and KIN.
ASO is a broad relation between any pair of enti-
ties; KIN encodes a relation between relatives.
The rest of the relations do not fall into any
cluster. ISA, PW, SYN, AT-L and AT-T have been
widely studied in the literature. MAK(x, y) holds
if y makes or produces x; POS(x, y) holds if y
owns x; MNR encodes the way a situation occurs.
RCP captures the connection between an event and
an object which is the receiver of the event. PRO
describes links between a situation or object and
its characteristics, e.g., height, age. Values to the
characteristics are given through VAL. QNT(x, y)
holds if y is quantitatively determined by x.
Relations can also be classified depending on
the kind of concepts they describe and their in-
tra or inter nature into: (i) Intra-Object; (ii) Inter-
Objects; (iii) Intra-Situation; (iv) Inter-Situations;
and (v) for Object and Situation description.
4.2 Detection of Semantic Relations
Relations are extracted by an in-house SP from
a wide variety of syntactic realizations. For ex-
ample, the compound nominal steel knife con-
tains PW(steel, knife), whereas carving knife con-
tains PRP(carving, knife); the genitive Mary?s toy
contains POS(toy, Mary), whereas Mary?s brother
contains KIN(brother, Mary), and eyes of the baby
contains a PW(eyes, baby). Relations are also ex-
tracted from a verb and its arguments (NP verb,
verb NP, verb PP, verb ADVP and verb S), adjec-
tive phrases and adjective clauses.
The SP first uses a combination of state-of-the-
art text processing tools, namely, part-of-speech
tagging, named entity recognition, syntactic pars-
ing and word sense disambiguation. After a can-
didate syntactic pattern has been found, a series of
machine learning classifiers are applied to decide
if a relation holds. Four different algorithms are
used: decision trees, Naive Bayes, SVM and Se-
mantic Scattering combined in a hybrid approach.
Some algorithms use a per-relation approach (i.e.,
decide whether or not a given relation holds) and
others a per-pattern approach (i.e., which relation,
if any, holds for a particular pattern). Additionally,
human-coded rules are used for a few unambigu-
ous cases. The SP participated in the SemEval
2007 Task 4 (Badulescu and Srikanth, 2007).
5 Composition of Semantic Relations
The goal of semantic calculus (SC) is to provide
a formal framework for manipulating semantic re-
lations. CSR is a part of this, its goal is to apply
inference axioms over already identified relations
in text in order to infer more relations.
Semantic Calculus: Operators and Properties
The composition operator is represented by the
75
(R?1)?1 = R
Ri ? Rj = (Rj?1 ? Ri?1)?1
R?1 inherits all the properties of R
??1 = ?
?i: ? ?? Ri
R is reflexive iff ?x: R(x, x)
R is symmetric iff R(x, y) = R(y, x)
R is transitive iff R(x, y) ? R(y, z) ? R(x, z)
Ri ? Rj ? Ri?1 ? Rj?1
Ri ?? Rj ? Ri?1 ?? Rj?1
If Ri is symmetric and Ri ?? Rj , Ri?1 ?? Rj
If Rj is symmetric and Ri ?? Rj , Ri ?? Rj?1
Table 2: Semantic calculus properties
symbol ?. It combines two relations and yields
a third one. Formally, we denote R1 ? R2 ? R3.
The inverse of R is denoted R?1 and can be ob-
tained by simply switching its arguments. Given
R(x, y), R?1(y, x) always holds. The easiest way
to read R?1(y, x) is x is R of y.
R1 left dominates R2, denoted by R1 ? R2,
iff the composition of R1 and R2 yields R1, i.e.,
R1 ? R2 iff R1 ? R2 ? R1. R1 right dominates R2,
denoted by R1 ? R2, iff the composition of R2 and
R1 yields R1, i.e., R1 ? R2 iff R2 ? R1 ? R1. R1
completely dominates R2, denoted by R1 ?? R2, iff
R1 ? R2 and R1 ? R2, i.e., R1 ?? R2 iff R1 ? R2 ?
R1 and R2 ? R1 ? R1.
An OTHER (?) relation holds between x and y
if no relation from the given set holds. Formally,
?(x, y) iff ??Ri such that Ri(x, y).
Using the notation above, the properties de-
picted in Table 2 follow.
Necessary conditions for Combining Relations
Axioms can be defined only for compatible rela-
tions as premises. R1 and R2 are compatible if it
is possible, from a theoretical point of view, to ap-
ply the composition operator to them. Formally,
RANGE(R1) ? DOMAIN(R2) 6= ?
If R1 and R2 are compatible but not equal a
restriction occurs. Let us denote RANGE(R1) ?
DOMAIN(R2) = I . A backward restriction takes
place if RANGE(R1) 6= I and a forward restric-
tion if DOMAIN(R2) 6= I . In the former case
RANGE(R1) is reduced; in the later DOMAIN(R2)
is reduced. A forward and backward restriction
can be found with the same pair of relations.
It is important to note that two compatible rela-
tions may not be the premises for a valid axiom.
For example, KIN and AT-L are compatible but do
not yield any valid inference.
Another necessary condition for combining two
relations R1(x, y) and R2(y, z) is that they have to
have a common argument, y.
5.1 Unique axioms
An axiom is defined as a set of relations called
premises and a conclusion. Given the premises it
unequivocally yields a relation that holds as con-
clusion. The composition operator is the basic
way of combining two relations to form an axiom.
In general, for n relations there are
(n
2
)
=
n(n?1)
2 different pairs. For each pair, taking into
account the two relations and their inverses, there
are 4 ? 4 = 16 different possible combinations.
Applying property Ri ? Rj = (Rj?1 ? Ri?1)?1,
only 10 combinations are unique: (i) 4 combine
R1, R2 and their inverses; (ii) 3 combine R1 and
R1?1; and (iii) 3 combine R2 and R2?1. The most
interesting axioms fall into category (i), since the
other two can be resolved by the transitivity prop-
erty of a relation and its inverse.
For n relations there are 2n2 + n potential ax-
ioms:
(n
2
)
?4+3n = 2?n(n?1)+3n = 2n2+n.
For n = 26, there are 1300 potential axioms in (i),
820 of which are compatible.
The number can be further reduced. After man-
ual examination of combinations of ASO and KIN
with other relations, we conclude that they do not
yield any valid inferences, invalidating 150 poten-
tial axioms. This is due to the broad meaning of
these relations. QNT can be discarded as well, in-
validating 45 more potential axioms.
Some axioms can be easily validated. Because
synonymous concepts are interchangeable, SYN is
easily combined with any other relation: SYN(x,
y) ? R(y, z) ? R(x, z) and R(x, y) ? SYN(y, z) ?
R(x, z). Because hyponyms inherit relations from
their hypernyms, ISA(x, y) ? R(y, z) ? R(x, z)
and R(x, y) ? ISA?1(y, z) ? R(x, z) hold. These
observations allow us to validate 138 of the 625
potential axioms left, still leaving 487.
As noted before, relations belonging to the
same cluster tend to behave similarly. This is es-
pecially true for the reason and goal clusters due
to their semantic motivation. Working with these
two clusters instead of the relations brings the
76
(1) reason ? goal (2) reason?1 ? goal
x reason //
IFL
?
??
??
??
? y
goal

z
x
PRP
?
??
??
??
? y
goal

reasonoo
z
(3) goal ? reason (4) goal ? reason?1
x
goal

IFL
?
??
??
??
?
y
reason
// z
x
IFL?1
?
??
??
??
?
goal

y z
reason
oo
Table 3: The four axioms taking as premises rea-
son and goal clusters. Diagonal arrows indicate
inferred relations.
number of axioms to be examined down to 370.
Out of the 370 axioms left, we have extensively
analyzed and defined the 35 involving AT-L, the
43 involving reason and the 58 involving goal. Be-
cause of space constraints, in this paper we only
fully introduce the axioms for reason and goal
(Section 6), as well as a variety of axioms useful
to recognize textual entailments (Section 7.2).
6 Case Study: Reason and Goal
In this section, we present the four unique axioms
for reason and goal relations (Table 3).
(1) REA(x, y) ? GOA(y, z) ? IFL(x, z): an
event is influenced by the reason of its goal.
For example: Bill saves money because he is
unemployed; he spends far less than he used to.
Therefore, being unemployed can lead to spend
far less.
P REA(be unemployed, save money)
GOA(save money, spend far less)
C IFL(be unemployed, spend far less)
(2) REA?1(x, y) ? GOA(y, z) ? PRP(x, z):
events have as their purpose the effects of their
goals. This is a strong relation.
For example: Since they have a better view,
they can see the mountain range. They cut the tree
to have a better view. Therefore, they cut the tree
to see the mountain range.
P REA?1(see the mountain range, better view)
GOA(better view, cut the tree)
C PRP(see the mountain range, cut the tree)
Note that possible unintended effects of cutting
the tree (e.g. homeowners? association complains)
are caused by the event cut the tree, not by its ef-
fect get a better view.
(3) GOA(x, y) ? REA(y, z) ? IFL(x, z): the
goal of an action influences its effects.
For example: John crossed the street carelessly
to get there faster. He got run over by a propane
truck. Therefore, John got run over by a propane
truck influenced by (having the goal of) getting
there faster.
P GOA(get there faster, crossed carelessly)
REA(crossed carelessly, got run over )
C IFL(get there faster, got run over)
(4) GOA(x, y) ? REA?1(y, z) ? IFL?1(x, z).
Events influence the goals of its effects.
For example: Jane exercises to lose weight. She
exercised because of the good weather. Therefore,
good weather helps to lose weight.
P GOA(lose weight, exercise)
REA?1(exercise, good weather)
C IFL?1(lose weight, good weather)
The axioms have been evaluated using manu-
ally annotated data. PropBank CAU and PNC are
used as reason and goal. Reason annotation is fur-
ther collected from a corpus which adds causal
annotation to the Penn TreeBank (Bethard et al,
2008). A total of 5 and 29 instances for axioms
3 and 4 were found. For all of them, the ax-
ioms yield a valid inference. For example, Buick
[approached]y American express about [a joint
promotion]x because [its card holders generally
have a good credit history]z . PropBank annota-
tion states GOA(x, y) and REA?1(y, z), axiom 4
makes the implicit relation IFL?1(x, z) explicit.
7 Applications and Results
7.1 Customization of Semantic Relations
Problem There is no agreement on a set of rela-
tions that best represent text semantics. This is
rightfully so since different applications and do-
mains call for different relations. CSR can be used
to rapidly customize a set of relations without hav-
ing to train a new SP or modify any other tool.
Given a text, the SP extracts 26 elementary se-
mantic relations. Axioms within the framework
of CSR yield n new relations, resulting in a richer
semantic representation (Figure 2).
CSR axioms Two ways to get new relations are:
(i) Direct mapping. This is the easiest case and
it is equivalent to rename a relation. For example,
we can map POS to BELONG or IS-OWNER-OF.
77
Axiom Rest. on y Example
AGT(x, y) ? THM?1(y, z) ? ARRESTED(x, z) arrested concept [Police]x [apprehended]y 51 [football fans]z.
THM(x, y) ? AT-L(y, z) ? ARRESTED-AT(x, z) arrested concept Police [apprehended]y 51 [fans]x [near the Dome]z.
AGT(x, y) ? AT-L(y, z) ? BANKS-AT(x, z) banking activity [John]x [withdrew]y $20 [at the nearest Chase]z.
POS(x, y) ? AT-L(y, z) ? BANKS-AT(x, z) account concept [John]x got a [checkbook]y at [Chase]z.
Table 4: Examples of semantic relation customization using CSR.
Pair Text T Hypothesis H
113
Belknap married and lost his first two wives, Cora LeRoy and Carrie
Tomlinson, and married Mrs. John Bower, his second wife?s sister.
Belknap was married to Carrie Tomlinson.
T1 AGT(Belknap, married ) H1 AGT(Belknap, was married )
T2 THM(wives, married ) H2 THM(Carrie Tomlinson, was married )
T3 QNT(first two, wives)
T4 ISA(Carrie Tomlinson, wives)
429
India?s yearly pilgrimage to the Ganges river, worshiped by Hindus as
the goddess Ganga, is the world?s largest gathering of people, . . .
Ganga is a Hindu goddess.
T1 AGT(Hindus, worship) H1 ISA(Ganga, goddess)
T2 THM(Ganga, worship) H2 VAL(Hindu, goddess)
T3 ISA(Ganga, goddess)
445
[. . . ] At present day YouTube represents the most popular site sharing
on-line video.
YouTube is a video website.
T1 ISA(YouTube, site) H1 ISA(YouTube, website)
T2 EXP(site, sharing) H2 VAL(video, website)
T3 THM(video, sharing)
716
The Czech and Slovak republics have been unable to agree a political
basis for their future coexistence in one country.
The Czech and Slovak republics do not agree to coexist in one country.
T1 AGT(The Czech and Slovak republics, have been
unable to agree)
H1 AGT(The Czech and Slovak republics, do not
agree)
T2 THM(political basis, have been unable to agree) H2 PRP(coexist in one country, do not agree)
T3 PRP(their future coexistence in one country, po-
litical basis)
771
In 2003, Yunus brought the microcredit revolution to the streets of
Bangladesh to support more than 50,000 beggars, whom the Grameen
Bank respectfully calls Struggling Members.
Yunus supported more than 50,000 Struggling Members.
T1 AGT(Yunus, brought ) H1 AGT(Yunus, supported )
T2 PRP(support, brought )
T3 RCP(beggars, support ) H2 RCP(Struggling Members, support )
T4 QNT(more than 50,000, beggars) H3 QNT(more than 50,000, Struggling Members)
T5 SYN(beggars, Struggling Members)
Table 5: RTE3 examples and their elementary semantic relations (i.e., the ones the SP detects). Only
relevant semantic relations for entailment detection are shown for T .
Text // Semantic Parser 26 relations //

Inference axioms // CSR n new sr //EDBC@AOO
Figure 2: Flowchart for obtaining customized se-
mantic relations using CSR.
(ii) Combinations of two elementary relations
yield new specialized relations. In this case, re-
strictions on the arguments must be fulfilled.
Consider we need the new relation AR-
RESTED(x, y), which encodes the relation be-
tween two animate concrete objects x and y, where
x arrested y. We can infer this relation by using
the following axiom: AGENT(x, y) ? THEME?1(y,
z) ? ARRESTED(x, z) provided that y is an ar-
rested concept. A simple way of checking if a
given concept is of a certain kind is to check
WordNet. Collecting all the words belonging the
the synset arrest.v.1, we get the following list of
arrested concepts: collar, nail, apprehend, pick
up, nab and cop. Using lexical chains the list
could be further improved.
More examples of axioms for generating cus-
tomized semantic relations are shown in Table 4.
Results Virtually any domain could be covered
by applying customization over the set of 26
relations. The set has been successfully cus-
tomized to a law enforcement domain. Ax-
78
ioms for a total of 37 new relations were de-
fined and implemented. Among others, ax-
ioms to infer IS-EMPLOYER, IS-COWORKER, IS-
PARAMOUR, IS-INTERPRETER, WAS-ASSASSIN,
ATTENDS-SCHOOL-AT, JAILED-AT, COHABITS-
WITH, AFFILIATED-TO, MARRIED-TO, RENTED-
BY, KIDNAPPED-BY and the relations in Table 4
were defined. Note that a relation can be inferred
by several axioms. This customization effort to
add 37 new specialized relations took a person
only a few days and without modifying the SP.
7.2 Textual Entailment
Problem An application of CSR is recognizing
entailments. Given text T and hypothesis H , the
task consists on determining whether or not H can
be inferred by T (Giampiccolo et al, 2007).
CSR axioms Several examples of the RTE3 chal-
lenge can be solved by applying CSR (Table 5).
The rest of this section depicts the axioms in-
volved in detecting entailment for each pair.
Pair 113 is a simple one. A perfect match
for H in T can be obtained by an axiom reading
all concepts inherit the semantic relations of their
hypernyms. Formally, ISA(x, y) ? THM(y, z) ?
THM(x, z), T2 and T4 are the premises and the
conclusion matches H2. T1 matches H1.
Pair 429 can be solved by an axiom read-
ing agents are values for their themes. Formally,
AGT(x, y) ? THM?1(y, z) ? VAL(x, z); T1 and
T2 yield VAL(Hindu, Ganga), which combined
with T3 results in a match between T and H .
Pair 445 follows a similar pattern, but the way
an EXP combines with its THM differs from the
way an AGT does. The theme is a value of the
experiencer, THM(x, y) ? EXP?1(y, z) ? VAL(x,
z). Given T2 and T3, the axiom yields T4:
VAL(video, site). Assuming that SYN(site, web-
site), T1 and T4 match H .
Pair 716 also requires only one inference step.
Using T3 and T2, an axiom reading situations
have as their purpose the purpose of its theme in-
fers H2, yielding a perfect match between T and
H . Formally, PRP(x, y) ? THM(y, z) ? PRP(x, z).
Pair 771 Using as premises T1 and T2, an ax-
iom reading an agent performs the purposes of its
actions infers H1. Using T3 and T5, and T4
and T5 as premises, an axiom reading synony-
mous concepts are interchangeable infers H2 and
H3, resulting in a perfect match between T and
H . Formally, AGT(x, y) ? PRP?1(y, z) ? AGT(x,
z), RCP?1(x, y) ? SYN(y, z) ? RCP?1(x, z) and
QNT(x, y) ? SYN(y, z) ? QNT(x, z).
Results We conducted two experiments to quan-
tify the impact of CSR in detecting entailments.
First, 60 pairs were randomly selected from the
RTE3 challenge and parsed with the SP. 14 of
them (23%) could be solved by simply matching
the elementary relations in T and H . After apply-
ing CSR, 21 more pairs (35%) were solved. Thus,
adding CSR on top of the SP clearly improves en-
tailment detection. Out of the 25 pairs not solved,
5 (8%) need coreference resolution and 20 (34%)
require commonsense knowledge or fairly com-
plicated reasoning methods (e.g. a shipwreck is a
ship that sank).
CSR has also been added to a state of the art
system for detecting textual entailment (Tatu and
Moldovan, 2007). Prior to the addition, the sys-
tem made 222 errors consisting of 46 false nega-
tives (examples in Table 5) and 176 false positives.
CSR was able to correctly solve 18 (39%) of the
46 false negatives.
8 Conclusions
Although the idea of chaining semantic relations
has been proposed before, this paper provides a
formal framework establishing necessary condi-
tions for composition of semantic relations. The
CSR presented here can be used to rapidly cus-
tomize a set of relations to any arbitrary domain.
In addition to the customization of an informa-
tion extraction tool and recognizing textual entail-
ments, CSR has the potential to contribute to other
applications. For example, it can help improve a
semantic parser, it can be used to acquire com-
monsense knowledge axioms and more.
When an axiom that results from combining
two relations does not always hold, it may be pos-
sible to add constraints that limit the arguments of
the premises to only some concepts.
This work stems from the need to automate the
extraction of deep semantics from text and repre-
senting text as semantic triples. The paper demon-
strates that CSR is able to extract more relations
than a normal semantic parser would.
79
References
Badulescu, Adriana and Munirathnam Srikanth. 2007.
LCC-SRN: LCC?s SRN System for SemEval 2007
Task 4. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, pages 215?218.
Baker, Collin F., Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet Project. In
Proceedings of the 17th international conference on
Computational Linguistics, Montreal, Canada.
Bethard, Steven, William Corvey, Sara Klingenstein,
and James H. Martin. 2008. Building a Corpus of
Temporal-Causal Structure. In Proceedings of the
Sixth International Language Resources and Evalu-
ation Conference, Marrakech, Morocco.
Chang, Du S. and Key S. Choi. 2006. Incremental
cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing & Manage-
ment, 42(3):662?678.
Copestake, Ann, Alex Lascarides, and Dan Flickinger.
2001. An Algebra for Semantic Construction in
Constraint-based Grammars. In Proceedings of
39th Annual Meeting of the ACL, pages 140?147.
Davidov, Dmitry and Ari Rappoport. 2008. Classifi-
cation of Semantic Relationships between Nominals
Using Pattern Clusters. In Proceedings of ACL-08:
HLT, pages 227?235, Columbus, Ohio.
Fillmore, Charles J. 1971. Some Problems for Case
Grammar. Monograph Series on Languages and
Linguistics, 24:35?36.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nizing Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 1?9.
Girju, Roxana, Adriana Badulescu, and Dan
Moldovan. 2006. Automatic Discovery of
Part-Whole Relations. Computational Linguistics,
32(1):83?135.
Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ations, pages 13?18, Prague, Czech Republic.
Harabagiu, Sanda and Dan Moldovan. 1998. Knowl-
edge Processing on an Extended WordNet. In Fell-
baum, Christiane, editor, WordNet: An Electronic
Lexical Database and Some of its Applications,
chapter 17, pages 684?714. The MIT Press.
Helbig, Hermann. 2005. Knowledge Representation
and the Semantics of Natural Language. Springer.
Hirano, Toru, Yoshihiro Matsuo, and Genichiro Kikui.
2007. Detecting Semantic Relations between
Named Entities in Text Using Contextual Features.
In Proceedings of the 45th Annual Meeting of the
ACL, Demo and Poster Sessions, pages 157?160.
Lakoff, George. 1970. Linguistics and Natural Logic.
Synthese, 22(1):151?271.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Seman-
tic Role Labeling: An Introduction to the Special
Issue. Computational Linguistics, 34(2):145?159.
Miller, George A. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38:39?41.
Moldovan, Dan, Adriana Badulescu, Marta Tatu,
Daniel Antohe, and Roxana Girju. 2004. Mod-
els for the Semantic Classification of Noun Phrases.
In HLT-NAACL 2004: Workshop on Computational
Lexical Semantics, pages 60?67.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Rosario, Barbara and Marti Hearst. 2004. Classifying
Semantic Relations in Bioscience Texts. In Proc. of
the 42nd Meeting of the ACL, pages 430?437.
Sa?nchez Valencia, Victor. 1991. Studies on Natural
Logic and Categorial Grammar. Ph.D. thesis, Uni-
versity of Amsterdam.
Szpakowicz, Barker, Ken Barker, and Stan Szpakow-
icz. 1995. Interactive semantic analysis of Clause-
Level Relationships. In Proceedings of the Second
Conference of the Pacific ACL, pages 22?30.
Tatu, Marta and Dan Moldovan. 2007. COGEX
at RTE 3. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 22?27, Prague, Czech Republic.
Tatu, Marta. 2005. Automatic Discovery of Intentions
in Text and its Application to Question Answering.
In Proceedings of the ACL Student Research Work-
shop, pages 31?36, Ann Arbor, Michigan.
Turney, Peter D. 2006. Expressing Implicit Seman-
tic Relations without Supervision. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
ACL, pages 313?320, Sydney, Australia.
80
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 315?324,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Discovery of Manner Relations and its Applications
Eduardo Blanco and Dan Moldovan
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080 USA
{eduardo,moldovan}@hlt.utdallas.edu
Abstract
This paper presents a method for the auto-
matic discovery of MANNER relations from
text. An extended definition of MANNER is
proposed, including restrictions on the sorts
of concepts that can be part of its domain and
range. The connections with other relations
and the lexico-syntactic patterns that encode
MANNER are analyzed. A new feature set spe-
cialized on MANNER detection is depicted and
justified. Experimental results show improve-
ment over previous attempts to extract MAN-
NER. Combinations of MANNER with other
semantic relations are also discussed.
1 Introduction
Extracting semantic relations from text is an impor-
tant step towards understanding the meaning of text.
Many applications that use no semantics, or only
shallow semantics, could benefit by having available
more text semantics. Recently, there is a growing in-
terest in text semantics (Ma`rquez et al, 2008; Davi-
dov and Rappoport, 2008).
An important semantic relation for many appli-
cations is the MANNER relation. Broadly speaking,
MANNER encodes the mode, style, way or fashion
in which something is done or happened. For ex-
ample, quick delivery encodes a MANNER relation,
since quick is the manner in which the delivery hap-
pened.
An application of MANNER detection is Question
Answering, where many how questions refer to this
particular relation. Consider for example the ques-
tion How did the President communicate his mes-
sage?, and the text Through his spokesman, Obama
sent a strong message [. . . ]. To answer such ques-
tions, it is useful to identify first the MANNER rela-
tions in text.
MANNER occurs frequently in text and it is
expressed by a wide variety of lexico-syntactic
patterns. For example, PropBank annotates
8,037 ARGM-MNR relations (10.7%) out of 74,980
adjunct-like arguments (ARGMs). There are verbs
that state a particular way of doing something, e.g.,
to limp implicitly states a particular way of walk-
ing. Adverbial phrases and prepositional phrases
are the most productive patterns, e.g., The nation?s
industrial sector is now growing very slowly if at
all and He started the company on his own. Con-
sider the following example: The company said
Mr. Stronach will personally direct the restructur-
ing assisted by Manfred Gingl, [. . . ]1. There are
two MANNER relations in this sentence: the under-
lined chunks of text encode the way in which Mr.
Stronach will direct the restructuring.
2 Previous Work
The extraction of semantic relations in general has
caught the attention of several researchers. Ap-
proaches to detect semantic relations usually focus
on particular lexical and syntactic patterns. There
are both unsupervised (Davidov et al, 2007; Turney,
2006) and supervised approaches. The SemEval-
2007 Task 04 (Girju et al, 2007) aimed at relations
between nominals. Work has been done on detect-
ing relations within noun phrases (Nulty, 2007),
1Penn TreeBank, file wsj 0027, sentence 10.
315
named entities (Hirano et al, 2007), clauses (Sz-
pakowicz et al, 1995) and syntax-based comma res-
olution (Srikumar et al, 2008). There have been
proposals to detect a particular relation, e.g., CAUSE
(Chang and Choi, 2006), INTENT (Tatu, 2005),
PART-WHOLE (Girju et al, 2006) and IS-A (Hearst,
1992).
MANNER is a frequent relation, but besides the-
oretical studies there is not much work on detect-
ing it. Girju et al (2003) propose a set of fea-
tures to extract MANNER exclusively from adverbial
phrases and report a precision of 64.44% and re-
call of 68.67%. MANNER is a semantic role, and all
the works on the extraction of roles (Gildea and Ju-
rafsky, 2002; Giuglea and Moschitti, 2006) extracts
MANNER as well. However, these approaches treat
MANNER as any other role and do not use any spe-
cific features for its detection. As we show in this
paper, MANNER has its own unique characteristics
and identifying them improves the extraction accu-
racy. The two most used semantic role annotation
resources, FrameNet (Baker et al, 1998) and Prop-
Bank (Palmer et al, 2005), include MANNER.
The main contributions of this paper are: (1) em-
pirical study of MANNER and its semantics;
(2) analysis of the differences with other relations;
(3) lexico-syntactic patterns expressing MANNER;
(4) a set of features specialized on the detection of
MANNER; and (5) the way MANNER combines with
other semantic relations.
3 The Semantics of MANNER Relation
Traditionally, a semantic relation is defined by stat-
ing the kind of connection linking two concepts.
For example, MANNER is loosely defined by the
PropBank annotation guidelines2 as manner adverbs
specify how an action is performed [. . . ] manner
should be used when an adverb be an answer to
a question starting with ?how??. We find this kind
of definition weak and prone to confusion (Section
3.2). Nonetheless, to the best of our knowledge,
semantic relations have been mostly defined stating
only a vague definition.
Following (Helbig, 2005), we propose an ex-
tended definition for semantic relations, includ-
2http://verbs.colorado.edu/?mpalmer/projects/ace/PBguide
lines.pdf, page 26.
ing semantic restrictions for its domain and range.
These restrictions help deciding which relation
holds between a given pair of concepts. A relation
shall not hold between two concepts unless they be-
long to its domain and range. These restrictions are
based on theoretical and empirical grounds.
3.1 MANNER Definition
Formally, MANNER is represented as MNR(x, y),
and it should be read x is the manner in which
y happened. In addition, DOMAIN(MNR) and
RANGE(MNR) are the sets of sorts of concepts that
can be part of the first and second argument.
RANGE(MNR), namely y, is restricted to situa-
tions, which are defined as anything that happens
at a time and place. Situations include events and
states and can be expressed by verbs or nouns, e.g.,
conference, race, mix and grow. DOMAIN(MNR),
namely x, is restricted to qualities (ql), non temporal
abstract objects (ntao) and states (st). Qualities rep-
resent characteristics that can be assigned to other
concepts, such as slowly and abruptly. Non tempo-
ral abstract objects are intangible entities. They
are somehow product of human reasoning and are
not palpable. They do not encode periods or points
of time, such as week, or yesterday. For example,
odor, disease, and mile are ntao; book and car are
not because they are tangible. Unlike events, states
are situations that do not imply a change in the con-
cepts involved. For example, standing there or hold-
ing hands are states; whereas walking to the park
and pinching him are events. For more details about
these semantic classes, refer to (Helbig, 2005).
These semantic restrictions on MANNER come af-
ter studying previous definitions and manual exami-
nation of hundreds of examples. Their use and ben-
efits are described in Section 4.
3.2 MANNER and Other Semantic Relations
MANNER is close in meaning with several other rela-
tions, specifically INSTRUMENT, AT-LOCATION and
AT-TIME.
Asking how does not identify MANNER in many
cases. For example, given John broke the window
[with a hammer], the question how did John break
the window? can be answered by with the hammer,
and yet the hammer is not the MANNER but the IN-
STRUMENT of the broke event. Other relations that
316
may be confused as MANNER include AT-LOCATION
and AT-TIME, like in [The dog jumped]x [over the
fence]y and [John used to go]x [regularly]y.
A way of solving this ambiguity is by prioritiz-
ing the semantic relations among the possible can-
didates for a given pair of concepts. For exam-
ple, if both INSTRUMENT and MANNER are possi-
ble, the former is extracted. In a similar fashion, AT-
LOCATION and AT-TIME could have higher priority
than MANNER. This idea has one big disadvantage:
the correct detection of MANNER relies on the detec-
tion of several other relations, a problem which has
proven difficult and thus would unnecessarily intro-
duce errors.
Using the proposed extended definition one may
discard the false MANNER relations above. Hammer
is not a quality, non temporal abstract object or state
(hammers are palpable objects), so by definition a
relation of the form MNR(the hammer, y) shall not
hold. Similarly, fence and week do not fulfill the
domain restriction, so MNR(over the fence, y) and
MNR(every other week, y) are not valid either.
MANNER also relates to CAUSE. Again, ask-
ing how? does not resolve the ambiguity. Given
The legislation itself noted that it [was introduced]x
?by request,? [. . . ] (wsj 0041, 47), we believe
the underlined PP indicates the CAUSE and not the
MANNER of x because the introduction of the leg-
islation is the effect of the request. Using the ex-
tended definition, since request is an event (it im-
plies a change), MNR(by request, y) is discarded
based on the domain and range restrictions.
4 Argument Extraction
In order to implement domain and range restrictions,
one needs to map words to the four proposed se-
mantic classes: situations (si), states (st), qualities
(ql) and non temporal abstract objects (ntao). These
classes are the ones involved in MNR; work has been
done to define in a similar way more relations, but
we do not report on that in this paper.
First, the head word of a potential argument is
identified. Then, the head is mapped into a seman-
tic class using three sources of information: POS
tags, WordNet hypernyms and named entity (NE)
types. Table 1 presents the rules that define the map-
ping. We obtained them following a data-driven ap-
proach using a subset of MANNER annotation from
PropBank and FrameNet. Intermediate classes are
defined to facilitate legibility; intermediate classes
ending in -NE only involve named entity types.
Words are automatically POS tagged using a
modified Brill tagger. We do not perform word sense
disambiguation because in our experiments it did not
bring any improvement; all senses are considered
for each word. isHypo(x) for a given word w in-
dicates if any of the senses of w is a hyponym of x
in WordNet 2.0. An in-house NE recognizer is used
to assign NE types. It detects 90 types organized
in a hierarchy with an accuracy of 92% and it has
been used in a state-of-the-art Question Answering
system (Moldovan et al, 2007). As far as the map-
ping is concerned, only the following NE types are
used: human, organization, country, town, province,
other-loc, money, date and time. The mapping also
uses an automatically built list of verbs and nouns
that encode events (verb events and noun events).
The procedure to map words into semantic
classes has been evaluated on a subset of PropBank
which was not used to define the mapping. First,
we selected 1,091 sentences which contained a total
of 171 MANNER relations. We syntactically parsed
the sentences using Charniak?s parser and then per-
formed argument detection by matching the trees to
the syntactic patterns depicted in Section 5. 52,612
arguments pairs were detected as potential MAN-
NER. Because of parsing errors, 146 (85.4%) of the
171 MANNER relations are in this set.
After mapping and enforcing domain and range
constraints, the argument pairs were reduced to
11,724 (22.3%). The filtered subset includes 140
(81.8%) of the 171 MANNER relations. The filtering
does make mistakes, but the massive pruning mainly
filters out potential relations that do not hold: it fil-
ters 77.7% of argument pairs and it only misclassi-
fies 6 pairs.
5 Lexico-Syntactic Patterns Expressing
MANNER
MANNER is expressed by a wide variety of lexico-
syntactic patterns, implicitly or explicitly.
Table 2 shows the syntactic distribution of MAN-
NER relation in PropBank. We only consider rela-
tions between a single node in the syntactic tree and
317
Class Rule
situation state || event
state POStag=verb || isHypo(state.n.4)
event (POStag=verb && in(verb events)) || (POStag=noun &&
!animate object && (isHypo(phenomenon.n.1) || isHypo(event.n.1)
|| in(noun events)))
animate object livingNE || (POStag=noun && ((isHypo(entity.n.1) &&
!isHypo(thing.n.9) && !isHypo(anticipation.n.4)) ||
isHypo(social group.n.1)))
livingNE neType=(human | organization | country | town | province |
other-loc)
quality POStag=(adverb | gerund) || headPP=(with | without)
non temporal abstract object abstract object && !temporal
abstract object neType=money || isHypo(thing.n.9) || (!isHypo(social group.n.1)
&& (isHypo(abstraction.n.6 | psychological feature.n.1 |
possession.n.2 | event.n.1 | state.n.4 | group.n.1 | act.n.2)))
temporal TemporalNE || isHypo(time period.n.1) || isHypo(time.n.5)
temporalNE ne-type=(date | time)
Table 1: Mapping for the semantic classes used for defining DOMAIN(MNR) and RANGE(MNR).
.
Synt.
#Occ. %Occ.
Example
pattern File, #sent Sentence
ADVP 3559 45.3% wsj 0039, 24 This story line might [resonate]y [more strongly]ADVP if Mr. Lane
had as strong a presence in front of the camera as he does behind it.
PP 3499 44.6% wsj 2451, 0 NBC may yet find a way to [take]y a passive, minority interest in a
program-maker [without violating the rules]PP.
RB 286 3.6% wsj 0052, 3 Backe is [a [[closely]RB [held]y]ADJP media firm]NP run by former
CBS Inc. President Jon Backe.
S 148 1.9% wsj 1217, 25 Salomon [posted]y an unexpectedly big gain in quarterly earnings,
[aided by its securities trading and investments banking activities]S.
NP 120 1.5% wsj 0100, 21 [. . . ] he [graduated]y [Phi Beta Kappa]NP from the University of
Kentucky at age 18, after spending only 2 1/2 years in college.
Other 240 3.1% wsj 1337, 0 Tokyo stocks [closed]y [firmer]ADJP Monday, with the Nikkei index
making its fifth consecutive daily gain.
Table 2: Syntactic patterns encoding MANNER in PropBank, number of occurrences, and examples. A total of 7,852
MANNER relations are encoded in PropBank between a single node in the syntactic tree and a verb. In all examples,
MNR(x, y) holds, where x is the text underlined. Syntactic annotation comes straight from the Penn TreeBank.
a verb; MANNER relations expressed by trace chains
identifying coreference and split arguments are ig-
nored. This way, we consider 7,852 MANNER out
of the total of the 8,037 PropBank annotates. Be-
cause ADVPs and PPs represent 90% of MANNER
relations, in this paper we focus exclusively on these
two phrases.
For both ADVP and PP the most common direct
ancestor is either a VP or S, although examples are
found that do not follow this rule. Table 3 shows the
number of occurrences for several parent nodes and
examples. Only taking into account phrases whose
ancestor is either a VP or S yields a coverage of 98%
and thus those are the focus of this work.
5.1 Ambiguities of MANNER
Both ADVPs and PPs are highly ambiguous when
the task is to identify their semantics. The PropBank
authors (Palmer et al, 2005) report discrepancies
between annotators mainly with AT-LOCATION and
simply no relation, i.e., when a phrase does not en-
code a role at all. In their annotation, 22.2% of AD-
VPs encode MANNER (30.3% AT-TIME), whereas
only 4.6% of PPs starting with in and 6.1% start-
318
Parent #Occ.
Example
Phrase File, #sent Sentence
VP
3306 ADVP wsj 2341, 23 The company [was [officially]ADVP [merged]y with Bristol-Myers
Co. earlier this month]VP.
3107 PP wsj 2320, 7 This is something P&G [would [do]y [with or without Kao]PP]VP,
says Mr. Zurkuhlen.
S
215 ADVP wsj 0044, 6 [[Virtually word by word]ADVP, the notes [matched]y questions and
answers on the social-studies section of the test the student was
taking.]S
339 PP wsj 2454, 9 [[Under the laws of the land]PP, the ANC [remains]y an illegal or-
ganization, and its headquarters are still in Lusaka, Zambia.]S
ADJP
17 ADVP wsj 1057, 85 [. . . ] ABC touted ?Call to Glory,? but the military drama was
[[missing]y [in action]PP]ADJP within weeks.
4 PP wsj 2431, 14 Two former ministers [were]y [[so heavily]ADVP implicated]ADJP in
the Koskotas affair that PASOK members of Parliament voted [. . . ]
PP
9 ADVP wsj 1249, 24 In Japan, by contrast, companies tend to develop their talent and
[promote]y [from [within]PP]PP.
9 PP wsj 1505, 30 London share prices were [influenced]y [[largely]ADVP by declines
on Wall Street and weakness in the British pound]PP.
Table 3: Examples of ADVPs and PPs encoding MANNER with different nodes as parents. In all examples, MNR(x, y)
holds, where x is the underlined phrase. Syntactic annotation comes straight from the Penn TreeBank.
ing with at encode MANNER. The vast majority of
PPs encode either a AT-TIME or AT-LOCATION.
MANNER relations expressed by ADVPs are eas-
ier to detect since the adverb is a clear signal. Ad-
verbs ending in -ly are more likely to encode a MAN-
NER. Not surprisingly, the verb they attach to also
plays an important role. Section 6.2 depicts the fea-
tures used.
PPs are more complicated since the preposition
per se is not a signal of whether or not the phrase
encodes a MANNER. Even prepositions such as un-
der and over can introduce a MANNER. For ex-
ample, A majority of an NIH-appointed panel rec-
ommended late last year that the research con-
tinue under carefully controlled conditions, [. . . ]
(wsj 0047, 9) and [. . . ] bars where Japanese rev-
elers sing over recorded music, [. . . ] (wsj 0300, 3).
Note that in both cases, the head of the NP contained
in the PP encoding MANNER (conditions and music)
belongs to ntao (Section 4). Other prepositions, like
with and like are more likely to encode a MANNER,
but again it is not guaranteed.
6 Approach
We propose a supervised learning approach, where
instances are positive and negative MANNER exam-
ples. Due to their intrinsic difference, we build dif-
ferent models for ADVPs and PPs.
6.1 Building the Corpus
The corpus building procedure is as follows. First,
all ADVPs and PPs whose parent node is a VP or
S and encode a MANNER according to PropBank
are extracted, yielding 3559 and 3499 positive in-
stances respectively. Then, 10,000 examples of AD-
VPs and another 10,000 of PPs from the Penn Tree-
Bank not encoding a MANNER according to Prop-
Bank are added. These negative instances must have
as their parent node either VP or S as well and are
selected randomly.
The total number of instances, 13,559 for ADVPs
and 13,499 for PPs, are then divided into training
(60%), held-out (20%) and test (20%). The held-out
portion is used to tune the feature set and the final
results provided are the ones obtained with the test
portion, i.e., instances that have not been used in any
way to learn the models. Because PropBank adds se-
mantic role annotation on top of the Penn TreeBank,
we have gold syntactic annotation for all instances.
6.2 Selecting features
Selected features are derived from previous works
on detecting semantic roles, namely (Gildea and
Jurafsky, 2002) and the participating systems in
319
No. Feature Values Explanation
1 parent-node {S, VP} syntactic node of ADVP?s parent
2 num-leaves N number of words in ADVP
3 adverb {often, strongly, . . . } main adverb of ADVP
4 dictionary {yes, no} is adverb is in dictionary?
5 ends-with-ly {yes, no} does adverb end in -ly?
6 POS-tag-bef POStags POS tag word before adverb
7 POS-tag-aft POStags POS tag word after adverb
8 verb {assigned, go, . . . } main verb the ADVP attaches to
9 distance N number of words between adverb and verb
Table 4: Features used for extracting MANNER from ADVPs, their values and explanation. Features 4 and 5 are
specialized on MANNER detection.
No. Feature Values Explanation
1 parent-node {S, VP} syntactic node of PP?s parent
2 next-node {NP, SBAR, , . . . } syntactic node of sibling to the right of PP
3 num-pp-bef N number of sibling before PP which are PP
4 num-pp-aft N number of sibling after PP which are PP
5 first-word {with, after, . . . } first word in PP
6 first-POS-tag POStags first POS tag in PP
7 first-prep {by, on, . . . } first preposition in PP
8 POS-tag-bef POStags POS tag before first-word
9 POS-tag-aft POStags POs tag after first-word
10 word-aft {one, their, . . . } word after first-word
11 has-rb {yes, no} does the PP contain an adverb?
12 has-quotes {yes, no} does the PP have any quotes?
13 head-np-lemma {amount, year, . . . } head of the NP whose parent is the PP
14 head-is-last {yes, no} is head-np the last word of the sentence?
15 head-has-cap {yes, no} does the PP have a capitalized word?
16 verb {approved, fly, . . . } verb the PP attaches to
17 verb-lemma {approve, be, . . . } verb lemma the PP attaches to
18 verb-pas {yes, no} is verb in passive voice?
Table 5: Features used for extracting MANNER from PPs, their values and explanation. Features in bold letters are new
and specialized on detecting MANNER from PPs.
CoNLL-2005 Shared Task (Carreras and Ma`rquez,
2005), combined with new, manner-specific features
that we introduce. These new features bring a signif-
icant improvement and are dependent on the phrase
potentially encoding a MANNER. Experimentation
has shown that MANNER relations expressed by an
ADVP are easier to detect than the ones expressed
by a PP.
Adverbial Phrases The feature set used is depicted
in Table 4. Some features are typical of semantic
role labeling, but features adverb, dictionary
and ends-with-ly are specialized to MANNER
extraction from ADVPs. These three additional fea-
tures bring a significant improvement (Section 7).
We only provide details for the non-obvious fea-
tures.
The main adverb and verb are retrieved by select-
ing the last adverb or verb of a sequence. For exam-
ple, in more strongly, the main adverb is strongly,
and in had been rescued the main verb is rescued.
Dictionary tests the presence of the
adverb in a custom built dictionary which
contains all lemmas for adverbs in WordNet
whose gloss matches the regular expression in
a .* (manner|way|fashion|style). For example,
more.adv.1: used to form the comparative of some
adjectives and adverbs does not belong to the
dictionary, and strongly.adv.1: with strength or in a
320
strong manner does. This feature is an extension of
the dictionary presented in (Girju et al, 2003).
Given the sentence [. . . ] We [work
[damn hard]ADVP at what we do for damn lit-
tle pay]VP, and [. . . ] (wsj 1144, 128), the features
are: {parent-node:VP, num-leaves:2, adverb:hard,
dictionary:no, ends-with-ly:no, POS-tag-bef:RB,
POS-tag-aft:IN, verb:work, distance:1}, and it is a
positive instance.
Prepositional Phrases PPs are known to be highly
ambiguous and more features need to be added. The
complete set is depicted in Table 5.
Some features are typical of semantic role detec-
tion; we only provide a justification for the new
features added. Num-pp-bef and num-pp-aft
captures the number of PP siblings before and after
the PP. The relative order of PPs is typically MAN-
NER, AT-LOCATION and AT-TIME (Hawkins, 1999),
and this feature captures this idea without requiring
temporal or local annotation.
PPs having quotes are more likely to en-
code a MANNER, the chunk of text between
quotes being the manner. For example, use
in ?very modest amounts? (wsj 0003, 10) and re-
ward with ?page bonuses? (wsj 0012, 8).
head-np indicates the head noun of the NP
that attaches to the preposition to form the PP. It
is retrieved by selecting the last noun in the NP.
Certain nouns are more likely to indicate a MAN-
NER than others. This feature captures the do-
main restriction. For nouns, only non temporal
abstract objects and states can encode a MAN-
NER. Some examples of positive instances are
haul in the guests? [honor], lift in two [stages], win
at any [cost], plunge against the [mark] and ease
with little [fanfare]. However, counterexamples can
be found as well: say through his [spokesman] and
do over the [counter].
Verb-pas indicates if a verb is in passive
voice. In that case, a PP starting with by is much
more likely to encode an AGENT than a MAN-
NER. For example, compare (1) ?When the fruit is
ripe, it [falls]y from the tree [by itself]PP,? he says.
(wsj 0300, 23); and (2) Four of the planes [were
purchased]y [by International Lease]PP from Singa-
pore Airlines in a [. . . ] transaction (wsj 0243, 3).
In the first example a MANNER holds; in the second
an AGENT.
Given the sentence Kalipharma is a New Jersey-
based pharmaceuticals concern that [sells products
[under the Purepac label]PP]VP. (wsj 0023, 1), the
features are: {parent-node:VP, next-node:-, num-
pp-bef:0, num-pp-aft:0, first-word:under, first-POS-
tag:IN, first-prep:under, POS-tag-bef:NNS, POS-
tag-aft:DT, word-aft:the, has-rb:no, has-quotes:no,
head-np-lemma:label, head-is-last:yes, head-has-
cap:yes, verb:sells, verb-lemma:sell, verb-pas:no},
and it is a positive instance.
7 Learning Algorithm and Results
7.1 Experimental Results
As a learning algorithm we use a Naive Bayes clas-
sifier, well known for its simplicity and yet good per-
formance. We trained our models with the training
corpus using 10-fold cross validation, and used the
held-out portion to tune the feature set and adjust
parameters. More features than the ones depicted
were tried, but we only report the final set. For ex-
ample, named entity recognition and flags indicat-
ing the presence of AT-LOCATION and AT-TIME re-
lations for the verb were tried, but they did not bring
any significant improvement.
Table 6 summarizes the results obtained. We re-
port results only on the test corpus, which corre-
sponds to instances not seen before and therefore
they are a honest estimation of the performance.
The improvement brought by subsets of features
and statistical significance tests are also reported.
We test the significance of the difference in per-
formance between two feature sets i and j on a
set of ins instances with the Z-score test, where
z = abs(erri,errj)?d , errk is the error made using set
k, and ?d =
?
erri(1?erri)
ins +
errj(1?errj)
ins .
ADVPs The full set of features yields a F-measure
of 0.815. The three specialized features (3, 4 and
5) are responsible for an improvement of .168 in the
F-measure. This difference in performance yields a
Z-score of 7.1, which indicates that it is statistically
significant.
PPs All the features proposed yield a F-measure of
0.693. The novel features specialized in MANNER
detection from PPs (in bold letters in Table 5) bring
an improvement of 0.059, which again is significant.
321
Phrase #MNR Feat. Set #MNR retrieved #MNR correct P R F
ADVP 678
1,2,6-9 908 513 .565 .757 .647
all 757 585 .773 .863 .815
PP 705
1,2,5,6,8-10,16,17 690 442 .641 .627 .634
all 713 491 .689 .696 .693
Table 6: Results obtained during testing for different sets of features.
The Z-score is 2.35, i.e., the difference in perfor-
mance is statistically significant with a confidence
greater than 97.5%. Thus, adding the specialized
features is justified.
7.2 Error Analysis
The mapping of words to semantic classes is
data-driven and decisions were taken so that the
overall accuracy is high. However, mistakes
are made. Given We want to [see]y the mar-
ket from the inside, the underlined PP encodes a
MANNER and the mapping proposed (Table 1)
does not map inside to ntao. Similarly, given
Like their cohorts in political consulting, the litiga-
tion advisers [encourage]y their clients [. . . ], the
underlined text encodes a MANNER and yet cohorts
is subsumed by social group.n.1 and therefore is not
mapped to ntao.
The model proposed for MANNER detection
makes mistakes as well. For ADVPs, if the main
adverb has not been seen during training, chances of
detecting MANNER are low. For example, the classi-
fier fails to detect the following MANNER relations:
[. . . ] which together own about [. . . ] (wsj 0671, 1);
and who has ardently supported [. . . ] (wsj 1017,
26) even though ardently is present in the dictionary
and ends in -ly;
For PPs, some errors are due to the Prop-
Bank annotation. For example, in Shearson
Lehman Hutton began its coverage of the company
with favorable ratings. (wsj 2061, 57), the under-
lined text is annotated as ARG2, even though it
does encode a MANNER. Our model correctly de-
tects a MANNER but it is counted as a mistake.
Manners encoded by under and at are rarely de-
tected, as in that have been consolidated in fed-
eral court under U.S. District Judge Milton Pollack
(wsj 1022.mrg, 10).
8 Comparison with Previous Work
To the best of our knowledge, there have not been
much efforts to detect MANNER alone. Girju et al
(2003), present a supervised approach for ADVP
similar to the one reported in this paper, yielding
a F-measure of .665. Our augmented feature set
obtains a F-measure of .815, clearly outperforming
their method (Z-test, confidence > 97.5%). More-
over, ADVPs only represent 45.3% of MANNER as a
semantic role in PropBank. We also have presented
a model to detect MANNER encoded by a PP, the
other big chunk of MANNER (44.6%) in PropBank.
Complete systems for Semantic Role Labeling
perform poorly when detecting MANNER; the Top-
10 systems in CoNLL-2005 shared task3 obtained
F-measures ranging from .527 to .592. We have
trained our models using the training data provided
by the task organizers (using the Charniak parser
syntactic information), and tested with the provided
test set (test.wsj). Our models yield a Precision of
.759 and Recall of .626 (F-measure .686), bringing a
significant improvement over those systems (Z-test,
confidence > 97.5%). When calculating recall, we
take into account all MANNER in the test set, not
only ADVPs and PPs whose fathers are S or VP (i.e.
not only the ones our models are able to detect). This
evaluation is done with exactly the same data pro-
vided from the task organizers for both training and
test.
Unlike typical semantic role labelers, our features
do not include rich syntactic information (e.g. syn-
tactic path from verb to the argument). Instead, we
only require the value of the parent and in the case of
PPs, the sibling node. When repeating the CoNLL-
2005 Shared Task training and test using gold syn-
tactic information, the F-measure obtained is .714,
very close to the .686 obtained with Charniak syn-
tactic trees (not significant, confidence > 97.5%).
3http://www.lsi.upc.es/?srlconll/st05/st05.html
322
Even though syntactic parsers achieve a good perfor-
mance, they make mistakes and the less our models
rely on them, the better.
9 Composing MANNER with PURPOSE
MANNER can combine with other semantic rela-
tions in order to reveal implicit relations that oth-
erwise would be missed. The basic idea is to com-
pose MANNER with other relations in order to in-
fer another MANNER. A necessary condition for
combining MANNER with another relation R is the
compatibility of RANGE(MNR) with DOMAIN(R) or
RANGE(R) with DOMAIN(MNR). The extended def-
inition (Section 3) allows to quickly determine if two
relations are compatible (Blanco et al, 2010).
The new MANNER is automatically inferred
by humans when reading, but computers need
an explicit representation. Consider the follow-
ing example: [. . . ] the traders [place]y orders
[via computers]MNR [to buy the basket of stocks
. . . ]PRP (wsj 0118, 48). PropBank states the basic
annotation between brackets: via computers is the
MANNER and to buy the basket [. . . ] the PURPOSE
of the place orders event. We propose to combine
these two relations in order to come up with the new
relation MNR(via computers, buy the basket [. . . ] ).
This relation is obvious when reading the sentence,
so it is omitted by the writer. However, any seman-
tic representation of text needs as much semantics as
possible explicitly stated.
This claim is supported by several PropBank
examples: (1) The classics have [zoomed]y
[in price]MNR [to meet the competition]PRP,
and . . . (wsj 0071, 9) and (2) . . . the govern-
ment [curtailed]y production [with land-idling
programs]MNR [to reduce price-depressing
surpluses]PRP (wsj 0113, 12). In both exam-
ples, PropBank encodes the MANNER and PURPOSE
for event y indicated with brackets. After com-
bining both relations, two new MANNER arise:
MNR(in price, meet the competition) and MNR(with
land-idling programs, reduce price-depressing
surpluses).
Out of 237 verbs having in PropBank both PUR-
POSE and MANNER annotation, the above inference
method yields 189 new valid MANNER not present
in PropBank (Accuracy .797).
MANNER and other relations. MANNER does
not combine with relations such as CAUSE, AT-
LOCATION or AT-TIME. For example, given And
they continue [anonymously]x,MNR [attacking]y CIA
Director William Webster [for being too accom-
modating to the committee]z,CAU (wsj 0590, 27),
there is no relation between x and z. Similarly,
given [In the tower]x,LOC, five men and women
[pull]y [rhythmically]z,MNR on ropes attached to
[. . . ] (wsj 0089, 5) and [In May]x,TMP, the two
companies, [through their jointly owned holding
company]z,MNR, Temple, [offered]y [. . . ] (wsj 0063,
3), no connection exists between x and z.
10 Conclusions
We have presented a supervised method for the au-
tomatic discovery of MANNER. Our approach is
simple and outperforms previous work. Our mod-
els specialize in detecting the most common pattern
encoding MANNER. By doing so we are able to spe-
cialize our feature sets and outperform previous ap-
proaches that followed the idea of using dozens of
features, most of them potentially useless, and let-
ting a complicated machine learning algorithm de-
cide the actual useful features.
We believe that each relation or role has its own
unique characteristics and capturing them improves
performance. We have shown this fact for MANNER
by examining examples, considering the kind of ar-
guments that can be part of the domain and range,
and considering theoretical works (Hawkins, 1999).
We have shown performance using both gold syn-
tactic trees and the output from the Charniak parser,
and there is not a big performance drop. This is
mainly due to the fact that we do not use deep syn-
tactic information in our feature sets.
The combination of MANNER and PURPOSE
opens up a novel paradigm to perform semantic in-
ference. We envision a layer of semantics using a
small set of basic semantic relations and inference
mechanisms on top of them to obtain more seman-
tics on demand. Combining semantic relations in
order to obtain more relation is only one of the pos-
sible inference methods.
323
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional Linguistics, Montreal, Canada.
Eduardo Blanco, Hakki C. Cankaya, and Dan Moldovan.
2010. Composition of Semantic Relations: Model and
Applications. In Proceedings of the 23rd International
Conference on Computational Linguistics (COLING
2010), Beijing, China.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: semantic role label-
ing. In CONLL ?05: Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning,
pages 152?164, Morristown, NJ, USA.
Du S. Chang and Key S. Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing & Management,
42(3):662?678.
Dmitry Davidov and Ari Rappoport. 2008. Unsuper-
vised Discovery of Generic Relationships Using Pat-
tern Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. In Proceedings of
ACL-08: HLT, pages 692?700, Columbus, Ohio.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully Unsupervised Discovery of Concept-
Specific Relationships by Web Mining. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 232?239, Prague,
Czech Republic.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic La-
beling Of Semantic Roles. Computational Linguistics,
28:245?288.
Roxana Girju, Manju Putcha, and Dan Moldovan. 2003.
Discovery of Manner Relations and Their Applicabil-
ity to Question Answering. In Proceedings of the ACL
2003 Workshop on Multilingual Summarization and
Question Answering, pages 54?60, Sapporo, Japan.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic Discovery of Part-Whole Relations.
Computational Linguistics, 32(1):83?135.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13?18, Prague, Czech
Republic.
Ana M. Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via FrameNet, VerbNet and Prop-
Bank. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 929?936, Morristown, NJ, USA.
John A. Hawkins. 1999. The relative order of prepo-
sitional phrases in English: Going beyond Manner-
Place-Time. Language Variation and Change,
11(03):231?266.
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539?545.
Hermann Helbig. 2005. Knowledge Representation and
the Semantics of Natural Language. Springer.
Toru Hirano, Yoshihiro Matsuo, and Genichiro Kikui.
2007. Detecting Semantic Relations between Named
Entities in Text Using Contextual Features. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, Demo and Poster
Sessions, pages 157?160, Prague, Czech Republic.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,
and Suzanne Stevenson. 2008. Semantic Role Label-
ing: An Introduction to the Special Issue. Computa-
tional Linguistics, 34(2):145?159.
Dan Moldovan, Christine Clark, and Mitchell Bowden.
2007. Lymba?s PowerAnswer 4 in TREC 2007. In
Proceedings of the Sixteenth Text REtrieval Confer-
ence (TREC 2007).
Paul Nulty. 2007. Semantic Classification of Noun
Phrases Using Web Counts and Learning Algorithms.
In Proceedings of the ACL 2007 Student Research
Workshop, pages 79?84, Prague, Czech Republic.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Vivek Srikumar, Roi Reichart, Mark Sammons, Ari Rap-
poport, and Dan Roth. 2008. Extraction of Entailed
Semantic Relations Through Syntax-Based Comma
Resolution. In Proceedings of ACL-08: HLT, pages
1030?1038, Columbus, Ohio.
Barker Szpakowicz, Ken Barker, and Stan Szpakowicz.
1995. Interactive semantic analysis of Clause-Level
Relationships. In Proceedings of the Second Confer-
ence of the Pacific Association for Computational Lin-
guistics, pages 22?30.
Marta Tatu. 2005. Automatic Discovery of Intentions in
Text and its Application to Question Answering. In
Proceedings of the ACL Student Research Workshop,
pages 31?36, Ann Arbor, Michigan.
Peter D. Turney. 2006. Expressing Implicit Semantic
Relations without Supervision. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 313?320, Syd-
ney, Australia.
324
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1235?1245,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Semantically Enhanced Approach to Determine Textual Similarity
Eduardo Blanco and Dan Moldovan
Lymba Corporation
Richardson, TX 75080 USA
{eduardo,moldovan}@lymba.com
Abstract
This paper presents a novel approach to deter-
mine textual similarity. A layered methodol-
ogy to transform text into logic forms is pro-
posed, and semantic features are derived from
a logic prover. Experimental results show that
incorporating the semantic structure of sen-
tences is beneficial. When training data is
unavailable, scores obtained from the logic
prover in an unsupervised manner outperform
supervised methods.
1 Introduction
The task of Semantic Textual Similarity (Agirre et
al., 2012) measures the degree of semantic equiv-
alence between two sentences. Unlike textual en-
tailment (Giampiccolo et al, 2007), textual similar-
ity is symmetric, and unlike both textual entailment
and paraphrasing (Dolan and Brockett, 2005), tex-
tual similarity is modeled using a graded score rather
than a binary decision. For example, sentence pair
(1) below is very similar [5 out of 5], (2) is some-
what similar [3 out of 5] and (3) is not similar at all
[0 out of 5]:
1. Someone is removing the scales from the fish.
A person is descaling a fish.
2. A woman is chopping an herb.
A man is finely chopping a green substance.
3. A cat is playing with a watermelon on a floor.
A man is pouring oil into a pan.
State-of-the-art systems to determine textual sim-
ilarity (Ba?r et al, 2012; S?aric? et al, 2012; Banea
et al, 2012) do not account for the semantic struc-
ture of sentences, and mostly rely on word pair-
ings and knowledge derived from large corpora, e.g.,
man holding
AGENT
aa
THEME
==leaf
(a)
monkey fighting
AGENT
dd
THEME
==man
(b)
Figure 1: Semantic representation of 1(a) A man is hold-
ing a leaf and 1(b) A monkey is fighting a man.
Wikipedia. Regardless of details, each word in sent1
is paired with the word in sent2 that is most simi-
lar according to some similarity measure. Then, all
similarities are added and normalized by the length
of sent1 to obtain the similarity score from sent1 to
sent2. The process is repeated to obtain the simi-
larity score from sent2 to sent1, and both scores are
then averaged to determine the overall textual sim-
ilarity. Several word-to-word similarity measures
are often combined with other shallow features, e.g.,
n-gram overlap, syntactic dependencies, to obtain
the final similarity score.
Consider sentences 1(a) A man is holding a leaf
and 1(b) A monkey is fighting a man. These two
sentences are very dissimilar, the only commonal-
ity is the concept ?man?. Any approach that blindly
searches for the word in 1(b) that is the most similar
to word ?man? in 1(a) will find ?man? from 1(b) to
be a perfect match. One of three content words is a
match and thus the estimated similarity will be much
higher than it actually is.
Consider now the semantic representations for
sentences 1(a) and 1(b) in Figure 1. ?man? plays the
role of AGENT in 1(a), and THEME in 1(b). While
in both sentences the word ?man? encodes the same
concept, their semantic functions with respect to
other concepts are different. Intuitively, it seems rea-
sonable to penalize the similarity score based on the
role discrepancy.
1235
man used
AGENT
[[
THEME
AA
PURPOSE
&&
sword slice
THEME
88
INSTRUMENT
]]
AGENT
}} plastic bottle
VALUE

(a)
man sliced
AGENT
\\
THEME
77
INSTRUMENT
%%
plastic bottle
VALUE

sword
(b)
woman
PART
%%
applying
AGENT
``
THEME
;;
LOCATION
((
cosmetics face
(c)
woman putting
AGENT
__
THEME
==makeup
(d)
woman dancing
AGENT
__
LOCATION
@@rain
(e)
woman dances
AGENT
__
LOCATION
AArain
LOCATION
@@outside
(f)
Figure 2: Semantic representations of 2(a) The man used a sword to slice a plastic bottle, 2(b) A man sliced a plastic
bottle with a sword, 2(c) A woman is applying cosmetics to her face, 2(d) A woman is putting on makeup, 2(e) A
woman is dancing in the rain, and 2(f) A woman dances in the rain outside. Pairs (a, b), (c, d) and (e, f) are highly
similar even though concepts and relations only match partially.
This paper proposes a novel approach to deter-
mine textual similarity. Semantic representations
of sentences are exploited, syntactic features omit-
ted and the only external resource used in WordNet
(Miller, 1995). The main novelties of our approach
are: it (1) derives semantic features from a logic
prover to be used in a machine learning framework;
(2) uses three logic form transformations capturing
different levels of knowledge; and (3) incorporates
semantic representations extracted automatically.
1.1 Matching Semantic Representations and
Determining Textual Similarity
Throughout this paper, the semantic representation
of a sentence comprises the concepts in it, semantic
relations linking those concepts and named entities
qualifying them. First, we note that existing tools to
extract semantic relations and named entities are not
perfect, thus any system relying on them will suffer
from incomplete and incorrect representations. Sec-
ond, even if flawless representations were readily
available, the problem of determining textual simi-
larity cannot be reduced to matching semantic repre-
sentations: partial matches may correspond to com-
pletely similar sentences. The rest of this section
illustrates this point with the examples in Figure 2.
Our approach (Section 3) copes with the inherent er-
rors made by tools used to obtain semantic represen-
tations and learns which parts of a representation are
important to determine textual similarity.
Consider sentences 2(a) The man used a sword to
slice a plastic bottle and 2(b) A man sliced a plastic
bottle with a sword. Both sentences have high simi-
larity [5 out of 5], and yet their semantic representa-
tions only match partially. In this example, the verb
?used ? in 2(a) and its semantic links are somewhat
semantically superfluous. Note that in other cases,
missing a semantic relation signals lower similarity,
e.g., I had fun [at the party]LOCATION and I had fun,
while similar, do not convey the same meaning.
Sentence 2(c) A woman is applying cosmetics to
her face and 2(d) A woman is putting on makeup are
highly similar even though the latter specifies neither
the LOCATION where the ?makeup? is applied nor
the fact that a PART of the ?woman? is her ?face?.
Similarly, sentences 2(e) A woman is dancing in the
rain and 2(f) A woman dances in the rain outside
are semantically equivalent since ?rain? always has
LOCATION ?outside?: missing this information does
not carry loss of meaning.
2 Related Work
Determining similarity between text snippets is rele-
vant to information retrieval (Hatzivassiloglou et al,
1999), paraphrase recognition (Madnani and Dorr,
2010), grading answers to questions (Mohler et al,
2011) and many others. We focus on recent work
and emphasize the differences from our approach.
The SemEval 2012 Task 6: A Pilot on Semantic
Textual Similarity (Agirre et al, 2012) brought to-
gether 35 teams that competed against each other.
The top 3 performers (Ba?r et al, 2012; S?aric? et
al., 2012; Banea et al, 2012), followed a ma-
1236
Sentences
Logic Form
Transfor-
mation
Logic
Prover
Machine
Learning
Pairwise
Similarity
Measures
pairwise word similarity scores
logic forms
LFT-based scores
features
score
Figure 3: Main components of our system to determine textual similarity.
chine learning approach with features that do not
take into account the semantic structure of sen-
tences, e.g., n-grams, word overlap, evaluation mea-
sures for machine translation, pairwise word similar-
ities, syntactic dependencies. All three used Word-
Net, Wikipedia and other large corpora. In partic-
ular, Banea et al (2012) obtained models from 6
million Wikipedia articles and more than 9.5 mil-
lion hyperlinks; Ba?r et al (2012) used Wiktionary1,
which contains over 3 million entries; and S?aric? et
al. (2012) used The New York Times Annotated
Corpus (Sandhaus, 2008), which contains over 1.8
million news articles, and Google n-grams (Lin et
al., 2012), which consists of approximately 24GB
of compressed text files. Our approach only uses
WordNet, by far the smallest external resource with
less than 120,000 synsets.
Participants that incorporated information about
the semantic structure of sentences (Glinos, 2012;
Rios et al, 2012)2 did not perform at the top. Out
of 88 runs, they were ranked 16, 36 and 64. We be-
lieve this is because they use semantic relations to
calculate some ad-hoc similarity score. In contrast,
our approach derives features from semantic repre-
sentations encoded using logic, and combine these
features using machine learning. Moreover, we use
three logic form transformations capturing different
levels of knowledge, from only content words to se-
mantic structure. In turn, this allows us to boost
performance by relying on semantics when simpler
shallow methods fail.
A few logic-based approaches to recognize tex-
tual entailment are similar to the work presented
here. Bos and Markert (2006) extract semantic rep-
resentations with Boxer (Bos et al, 2004) and in-
corporate background knowledge from external re-
1http://www.wiktionary.org/
2A third team, spirin2, submitted results but a description
paper could not be found in the ACL anthology.
sources. They use a standard theorem prover and
extract 8 features that are later combined using ma-
chine learning. Raina et al (2005) use a logic form
transformation derived from dependency parses and
named entities. They use abductive reasoning and
define an assumption cost model to account for par-
tial entailments. Unlike them, we define three logic
from transformations, use a modified resolution step
and extract hundreds of features from the proofs.
Tatu and Moldovan (2005) use a modified logic
prover that drops predicates when a proof cannot
be found. Unlike us, they do not drop unbound
predicates and use a single logic form transforma-
tion. Another key difference is that they assign fixed
weights to predicates a priori instead of using ma-
chine learning to determine them.
3 Approach
Our approach to determine textual similarity (Fig-
ure 3) is grounded on using semantic features de-
rived from a logic prover that are later combined
in a standard supervised machine learning frame-
work. First, sentences are transformed into logic
forms (lft1, lft2). Then, a modified logic prover is
used to find a proof in both directions (lft1 to lft2
and lft2 to lft1). The prover yields similarity scores
based on the number of predicates dropped and fea-
tures characterizing the proofs. Additional similar-
ity scores are obtained using standard pairwise word
similarity measures. Finally, all scores and features
are combined using machine learning to yield the fi-
nal textual similarity score.
If training data is unavailable, only the LFT-based
and individual pairwise word similarity scores ap-
ply, the machine learning component is the only one
supervised. The rest of this section details each
component and exemplifies it with 2(e) A woman is
dancing in the rain and 2(f) A woman dances in the
rain outside.
1237
sent1: A woman is dancing in the rain.
semantic relations extracted: AGENT(dancing, woman), LOCATION(dancing, rain)
Basic woman N(x1) & dance V(x2) & rain N(x3)
SemRels woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3)
Full woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3)
sent2: A woman dances in the rain outside.
semantic relations extracted: AGENT(dances, woman), LOCATION(dances, rain)
Basic woman N(x1) & dance V(x2) & rain N(x3) & outside M(x4)
SemRels woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3)
Full woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3) &
outside M(x4)
Table 1: Examples of logic from transformation using modes Basic, SemRels and Full.
3.1 Logic Form Transformation
The logic form transformation (LFT) of a sentence
is derived from the concepts in it, the semantic
relations linking them and named entities. Un-
like other LFT proposals (Zettlemoyer and Collins,
2005; Poon and Domingos, 2009), transforming
sentences into logic forms is a straightforward step,
the quality of the logic forms is determined by the
output of standard NLP tools.
We distinguish six types of predicates:
? N for nouns, e.g., woman: woman N(x1).
? V for verbs, e.g., dances: dance V(x2).
? M for adjectives and adverbs, e.g., outside:
outside M(x3).
? O for concepts encoded by other POS tags.
? NE for named entities, e.g., guitar:
guitar N(x4) & instrument NE(x4).
? SR for semantic relations, e.g., A woman
dances: woman N(x1) & dance V(x2) &
AGENT SR(x2, x1).
In order to overcome semantic relation extraction
errors, we have experimented with three logic form
transformation modes. Each mode captures different
levels of knowledge:
Basic generates predicates for all nouns, verbs,
modifiers and named entities. This logic form
is parallel to accounting for content words,
their POS tags and named entity types.
SemRels generates predicates for all semantic rela-
tions, concepts that are arguments of relations
and named entities qualifying those concepts.
This mode ignores concepts not linked to other
concepts through a relation and might miss key
concepts if some relations are missing. If no
semantic relations are found, this mode backs
off to Basic to avoid empty logic forms.
Full generates predicates for all concepts, all se-
mantic relations and all named entities. It is
equivalent to SemRels after adding predicates
for concepts that are not arguments of a seman-
tic relation.
Table 1 exemplifies the three logic form modes.
If perfect semantic relations were always available,
SemRels would be the preferred mode. However,
this is often not the case and combining the three
logic forms yields better performance (Section 4).
Note that since relation LOCATION(rain, outside) is
not extracted from sent2, predicate outside M(x4)
is not present in mode SemRels.
3.2 Modified Logic Prover
Textual similarity is symmetric and therefore we
find proofs in both directions (from lft1 to lft2 and
from lft2 to lft1). The logic prover uses a modified
resolution procedure to calculate a similarity score
and features derived from the proof. The rest of this
section exemplifies one direction, lft1 to lft2. The
logic prover is a modification of OTTER3 (McCune
and Wos, 1997), an automated theorem prover for
first-order logic. For the textual similarity task, we
load lft1 and ?lft2 to the set of support and lexical
chain axioms to the usable list. Then, the logic
prover begins its search for a proof. Two scenar-
ios are possible: (1) a contradiction is found, i.e.,
a proof is found; or (2) a contradiction cannot be
found. The modifications to the standard resolution
3http://www.cs.unm.edu/?mccune/otter/
1238
sent1: A woman plays an electric guitar sent2: A man is cutting a potato
lft1: woman N(x1) & play V(x2) & AGENT SR(x2, x1) & electric M(x3) & guitar N(x4) &
instrument NE(x4) & VALUE SR(x4, x3) & THEME SR(x2, x4)
?lft2: ?man N(x1) ? ?cut V(x2) ? ?AGENT SR(x2, x1) ? ?potato N(x3) ? ?THEME SR(x2, x3)
Step Predicate dropped (regular) Score Predicate dropped (unbound) Score
1 woman N(x1) 0.875 n/a 0.875
2 play V(x2) 0.750 AGENT SR(x2, x1) 0.625
3 electric M(x3) 0.500 n/a 0.500
4 guitar N(x4) 0.375 instrument NE(x4), VALUE SR(x4, x3),
THEME SR(x2, x4)
0.000
Table 2: Example of predicate dropping step by step. Predicates AGENT SR(x2, x1) and THEME SR(x2, x4) would not
be dropped if unbound predicates were not dropped, yielding a score of 0.250 instead of 0.000.
procedure are used in scenario (2), when a proof
cannot be found. In this case, predicates from lft1 are
dropped until a proof is found. The worst case oc-
curs when all predicates in lft1 are dropped. The goal
of the dropping mechanism is to force the prover to
always find a proof, and penalize partial proofs ac-
cordingly.
Lexical chain axioms are extracted from WordNet.
Assuming each word w in sent1 has the first sense,
axioms w ? c, where c is at most distance 2 in
the WordNet hierarchy are generated. For exam-
ple, axioms derived from woman include woman?
female, woman ? mistress, woman ? widow and
woman? madam. Although simple, this WordNet
expansion proved useful in our experiments.
3.2.1 Predicate Dropping Criteria
When a proof cannot be found, individual predi-
cates from lft1 not present in lft2 are dropped. A
greedy algorithm was implemented for this step: out
of all predicates from lft1 not present in lft2, drop
whichever occurs first.
Dropping a predicate is not done in isolation. Af-
ter dropping a predicate, all predicates that become
unbound are dropped as well. With our current logic
form transformation, dropping a noun, verb or modi-
fier may make a semantic relation ( SR) or named en-
tity ( NE) predicate unbound. To avoid determining
high similarity between sentences with a common
semantic structure but unrelated concepts instantiat-
ing this structure, predicates encoding semantic rela-
tions and named entities are automatically dropped
when they become unbound.
3.2.2 Proof Scoring Criterion
The score assigned to the proof from lft1 to lft2
is calculated as the ratio of number of predicates in
lft1 not dropped to find the proof over the original
number of predicates in lft1.
Note that the dropping mechanism, and in par-
ticular whether predicates that become unbound
are automatically dropped, greatly impact the proof
obtained and its score (Table 2). If predi-
cates that become unbound were not automati-
cally dropped in each step, instrument NE(x4) and
VALUE SR(x4, x3) would be dropped in steps 5 and
6, AGENT SR(x2, x1) and THEME SR(x2, x4) would not
be dropped, and the final score would be 0.250 in-
stead of 0.000. In plain English, dropping unbound
predicates avoids matching semantic structures in-
stantiated by unrelated concepts.
3.2.3 Feature Selection
While the proof score can be used directly as an es-
timator of the similarity between lft1 and lft2, ad-
ditional features are extracted from the proof itself.
Namely, for each predicate type (N, V, M, O, SR,
NE), we count the number of predicates present in
lft1, the number of predicates dropped to find a proof
for lft2 and the ratio of the two counts. These three
counts are also calculated for each specific seman-
tic relation predicate (AGENT SR, LOCATION SR, etc.).
An example of score and feature calculation in both
directions is shown in Table 3.
The LFT-based scores and features are fed to a
machine learning algorithm. Specifically, there are
477 features derived from the logic prover:
? 9 LFT-based scores (3 ? 3; three scores (2 di-
rections and average), three LFT modes)
1239
lft1: woman N(x1) & dance V(x2) & AGENT SR(x2, x1) & rain N(x3) & LOCATION SR(x2, x3)
lft2: woman N(x1)&dance V(x2)&AGENT SR(x2, x1)&rain N(x3)&LOCATION SR(x2, x3)&outside M(x4)
lft1 to lft2
pred. dropped none
score 1
features
nt nd nr vt vd vr mt md mr net ned ner srt srd srr
2 0 0 1 0 0 0 0 0 0 0 0 2 0 0
lft2 to lft1
pred. dropped outside M(x4)
score 5/6 = 0.833
features
nt nd nr vt vd vr mt md mr net ned ner srt srd srr
2 0 0 1 0 0 1 1 1 0 0 0 2 0 0
Table 3: Two logic forms and output of logic prover in both directions. For each predicate type (n, v, m, o, ne, sr)
and semantic relation type (AGENT, LOCATION, etc.) features indicate the total number of predicates, the number of
predicates dropped until a proof is found and ratio of the two counts (t, d and r respectively). We omit the features for
predicate O and individual semantic relations because of space constraints.
? 108 features for predicates (3 ? 6 ? 3 ? 2 =
108; three features for each of the six predicate
types, three LFT modes, two directions)
? 360 features specific to a semantic relation (3?
20?3?2 = 360; three features for each of the
20 semantic relations types, three LFT modes,
two directions)
3.3 Pairwise Word Similarities
Pairwise word similarity measures between con-
cepts have been long studied, and they have been
used for the task of textual similarity before (Mihal-
cea et al, 2006). We incorporate scores derived us-
ing these measures for comparison purposes and to
improve robustness in our approach.
Basically, each open-class word in sent1 is paired
with the open-class word in sent2 that is most sim-
ilar according to some similarity measure. All these
individual similarities are summed and normalized
by the length of sent1 to find the similarity be-
tween sent1 and sent2. The process is repeated
from sent2 to sent1 to obtain the similarity between
sent2 and sent1, and both overall similarities are av-
eraged to determine the final similarity score.
We have experimented with measures Path
(distance in a taxonomy), LCH (Leacock and
Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu
and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin,
1998) and JCN (Jiang and Conrath, 1997), and use
the WordNet::Similarity package4.
4http://wn-similarity.sourceforge.net/
3.4 Machine Learning Algorithm
We follow a standard supervised machine learning
framework. Instances from the training split are
used to create a model that is later tested with test
instances not seen during training. The model was
tuned using 10-fold cross-validation over the train-
ing instances. As a learning algorithm, we use bag-
ging with M5P decision trees (Quinlan, 1992; Wang
and Witten, 1997) as implemented in the Weka soft-
ware package (Hall et al, 2009).
4 Experiments and Results
Logic forms are derived from the output of state-
of-the-art NLP tools developed previously and not
tuned in any way to the current task or corpora. Our
approach is not tied to any tool, set of named enti-
ties or relations. Any other semantic representation
could be used; the only required modification would
be the LFT component (Figure 3) so that it accounts
for the subtleties of the representation of choice.
The named entity recognizer extracts 35 fine-
grained types organized in a taxonomy (date, lan-
guage, city, instrument, etc.) and was first developed
for a question answering system (Moldovan et al,
2002). The implementation uses publicly available
gazetteers as well as machine learning.
Semantic relations are extracted with Polaris
(Moldovan and Blanco, 2012), a semantic parser
that given text extracts semantic relations. Polaris
is trained using FrameNet (Baker et al, 1998), Prop-
Bank (Palmer et al, 2005), NomBank (Meyers et al,
2004), several SemEval corpora (Girju et al, 2007;
1240
Score Sentence Pair Notes
MSRpar
(36/35) [750/750]
2.600 The unions also staged a five-day strike in March
that forced all but one of Yale?s dining halls to close.
Long sentences, difficult to
parse; often several details
are missing in one sentence
but the pair is similar
The unions also staged a five-day strike in March;
strikes have preceded eight of the last 10 contracts.
MSRvid 0.000 A woman is swimming underwater. Short sentences, easy to
parse(13/13), [750/750] A man is slicing some carrots.
SMTeuroparl 4.250 Then perhaps we could have avoided a catastrophe. One sentence often
ungrammatical (SMT)(56/21), [734/459] We would perhaps then able prevent a disaster.
surprise.OnWN 1.500 the alleviation of distress WN glosses, difficult to
parse with standard tools(?/16), [?/750] a change for the better.
surprise.SMTnews 3.000 He did, but the initiative did not get very far One sentence often
ungrammatical (SMT)(?/24), [?/399] What he has done without the initiative goes too far.
Table 4: Examples of sentence pairs belonging to the five sources. The numbers between round (square) parenthesis
indicate the average number of tokens per sentence pair (number of instances) in the train and test splits.
Pustejovsky and Verhagen, 2009; Hendrickx et al,
2010) and in-house annotations.
4.1 Corpora
We use the corpora released by SemEval 2012
Task 06: A Pilot on Semantic Textual Similarity5
(Agirre et al, 2012). These corpora consist of pairs
of sentences labeled with their semantic similar-
ity score, ranging from 0.0 to 5.0. Sentence pairs
come from five sources: (1) MSRpar, a corpus of
paraphrases; (2) MSRvid, short video descriptions;
(3) SMTeuroparl, output of machine translation sys-
tems and reference translations; (4) surprise.OnWN,
OntoNotes (Hovy et al, 2006) and WordNet (Miller,
1995) glosses; and (5) surprise.SMTnews, output of
machine translation systems in the news domain and
gold translations. Examples can be found in Table 4,
for more details refer to the aforementioned citation.
4.2 Results and Error Analysis
Results are reported using the same train and test
splits provided by the organization of SemEval 2012
Task 6. For surprise.OnWn and surprise.SMTnews,
only test data is available and supervised machine
learning is not an option.
Table 5 shows results obtained with the test split
not dropping and dropping unbound predicates. For
comparison purposes, results of the top-3 perform-
ers and participants using the semantic structure of
sentences are also shown. LFT-score systems output
5http://www.cs.york.ac.uk/semeval-2012/
task6/
the score (average of both directions) obtained with
the corresponding logic form transformation (Basic,
SemRels or Full) and are unsupervised: training data
with textual similarity scores is not used. The other
three systems presented are supervised. LFT-scores
+ features combines the 9 LFT-scores and 468 fea-
tures derived from the logic proof. WN-scores uses
as features the 7 scores derived using pairwise word
similarity measures. Finally, All combines the full
set of 484 features. We indicate that the performance
of one of our systems with respect to LFT score Ba-
sic not dropping unbound predicates is significant
with ? (confidence 99%) and ? (confidence 95%).
Overall, systems that drop unbound predicates
perform better than systems that do not drop them.
The only noticeable exception is LFT-score with
sentences from SMTeuroparl. However, best results
for SMTeuroparl are obtained dropping unbound
predicates and using All features. Henceforth, we
comment on results dropping unbound predicates as
they are higher.
Regarding logic form transformations, one can
see a trend depending on the source of sen-
tences. Polaris, the semantic parser, and the syn-
tactic parser Polaris relies on are mostly trained in
the news domain, and thus semantic representations
have higher quality in that domain. For SMTeu-
roparl and SMTnews, the two corpora closest to the
news domain, Full obtains better results than Ba-
sic and SemRels. The difference is most noticeable
in SMTnews, where Basic yields 0.4616, SemRels
1241
System MSRpar MSRvid SMTeuroparl OnWN SMTnews
not
dropping
unbound
predicates
no
M
L
LFT score
Basic 0.4963 0.8198 0.5101 0.6103 0.4588
SemRels ?0.3952 ?0.6753 0.4920 ?0.5055 0.4477
Full 0.4525 ?0.7024 0.5183 0.5895 0.4956
M
L
LFT scores + features ?0.5750 0.8466 0.4725 n/a n/a
WN scores 0.4978 0.8495 0.5217 n/a n/a
All ?0.5992 ?0.8660 0.5194 n/a n/a
dropping
unbound
predicates
no
M
L
LFT score
Basic ?0.5552 0.8234 0.4994 0.6120 0.4616
SemRels 0.4556 ?0.7388 0.4871 ?0.5113 0.4796
Full 0.5250 ?0.7672 0.5130 0.5895 ?0.5291
M
L
LFT scores + features ?0.5770 0.8440 0.5277 n/a n/a
WN scores 0.4977 0.8495 0.5217 n/a n/a
All ?0.6157 ?0.8709 ?0.5745 n/a n/a
Top
performer
(Ba?r et al, 2012) 0.6830 0.8739 0.5280 0.6641 0.4937
(S?aric? et al, 2012) 0.6985 0.8620 0.3612 0.7049 0.4683
(Banea et al, 2012) 0.5353 0.8750 0.4203 0.6715 0.4033
Team w/
semantic
structure
spirin2 0.5769 0.8203 0.4667 0.5835 0.4945
(Rios et al, 2012) 0.3628 0.6426 0.3074 0.2806 0.2082
(Glinos, 2012) 0.2312 0.6595 0.1504 0.2735 0.1426
Table 5: Correlations obtained with the test split using our approach (not dropping and dropping unbound predicates),
and results obtained by the top-3 performers and teams that included in their models features derived from the semantic
structure of sentences. Statistically significant differences in performance between our systems and LFT score Basic
not dropping unbound predicates are indicated with ? (confidence 99%) and ? (confidence 95%).
0.4796 (+0.0180) and Full 0.5291 (+0.0675 and
+0.0495 respectively).
Outside the news domain (MSRpar, MSRvid,
OnWN), Basic performs better than SemRels and
Full, and Full performs better than SemRels. This
leads to the conclusion that several semantic rela-
tions are often missing, and thus considering con-
cepts even if they are not linked to other concepts
via a semantic relation (Full) is more sound than ig-
noring them (SemRels).
When training data is available (MSRpar,
MSRvid, SMTeuroparl), LFT-scores + features al-
ways outperforms the scores obtained with a single
logic form transformation in an unsupervised man-
ner. In other words, combining the scores obtained
with the three logic form transformations and in-
corporating the additional features derived from the
proofs improves performance. These results demon-
strate that while a shallow logic form transforma-
tion (Basic) offers a strong baseline, it can be suc-
cessfully complemented with logic form transfor-
mations that consider the semantic structure of sen-
tences (SemRels, Full) and additional features char-
acterizing the proofs. The improvements LFT-scores
+ features brings over the LFT-score obtained with
Basic are substantial: 0.0218 (3.9%) for MSRpar,
0.0206 (2.5%) for MSRvid and 0.0283 (5.7%) for
SMTeuroparl.
WN scores, which only uses as features the
scores derived from pairwise word similarity mea-
sures, performs astonishingly well for some cor-
pora. Namely, the differences in performance be-
tween LFT scores + features and WN scores in
MSRvid and SMTeuroparl are minimal (?0.0055
and +0.0060). We believe this is due to the charac-
teristics of these two corpora. Sentence pairs from
MSRvid are very short with 13 tokens on average
(Table 4), i.e., 6.5 tokens per sentence, and SMTeu-
roparl pairs are hard to parse: at least one comes
from a machine translation system and is often un-
grammatical.
Finally, dropping unbound predicates and using
All features outperforms any other system. While
both LFT scores + features and WN scores yield
1242
good performance, the combination of the two out-
performs them. Features extracted successfully
complement each other for all corpora.
4.2.1 A Look at the ML Model
A benefit of decision trees is that one can inspect
them. This section briefly gives insight about the
most predictive features for All system.
The best features, i.e., features used in decisions
closer to the root, are the LFT-scores calculated us-
ing Basic and Full. The LFT-score obtained us-
ing SemRels is used only when the other two can-
not discriminate. Sorted by impact, the features ex-
tracted for verbs, nouns, semantic relations, named
entities and modifiers follow. Towards the bottom
of the tree, features for specific semantic relations
(AGENT SR, LOCATION SR, etc.) are used. All three
sources (MSRpar, MSRvid and SMTeuroparl) use
features for THEME, LOCATION, AGENT and QUAN-
TIFICATION. MSRpar also benefits from features for
TIME and only SMTeuroparl benefits from TOPIC
and MANNER.
4.2.2 Comparison with Previous Work
The semantic logic-based approach presented in this
paper either outperforms other systems or performs
in the top-3 (Table 5). Moreover, it clearly outper-
forms any other proposal that takes into account the
semantic structure of sentences. These results lead
to the conclusion that the semantic structure of sen-
tences is worth considering and more effort should
be devoted to deeper approaches.
When using sentences in the news domain (SM-
Teuroparl and SMTnews), i.e., when text is closer
to the domain in which the NLP tools are trained,
our semantic approach yields the best results known
to date. For MSRvid, the system presented here
performs as well as systems that use external
knowledge (Section 2), the differences are mini-
mal (+0.0030, ?0.0089, +0.0041) and not statis-
tically significant (confidence 99%). For MSRpar,
the system performs amongst the top-3 even though
two of these systems clearly obtained better results
(+0.0673, +0.0828); both differences are statisti-
cally significant (confidence 99%).
Performance using surprise.OnWN deserves spe-
cial comment. This corpus contains definitions, not
sentences (Table 4). Lin?s similarity measure alone
yields a correlation of 0.6787, beating all systems in
Table 5 except one of the top-3 performers (S?aric? et
al., 2012). Our semantic approach is not success-
ful because we cannot extract valid representations,
glosses are rarely a full sentence and are hard to
parse with generic NLP tools like the ones we use.
5 Conclusions
This paper presents a novel approach to determine
textual similarity that employs a logic prover to ex-
tract semantic features. A layered methodology to
transform text into logic forms using three logic
form transformations modes is presented. Each
mode captures different levels of knowledge, from
only content words to semantic representations auto-
matically extracted. Best results are obtained when
features derived from the logic prover are comple-
mented with simpler pairwise word similarity mea-
sures. Features that account for the semantic struc-
ture of sentences are incorporated when needed, as
the results obtained with systems All, LFT scores
and WN scores show.
Our approach is heavily dependent on the qual-
ity of semantic representations, and unlike current
top performers, does not require knowledge derived
from Wikipedia or other large corpora. State-of-
the-art NLP tools to extract semantic representations
from text, which are far from perfect, yield promis-
ing results. Indeed, the approach outperforms previ-
ous work when the source text is relatively familiar
to the tools, i.e., within the news domain, and per-
forms in the top-3 otherwise.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 385?393, Montre?al, Canada,
7-8 June.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional Linguistics, Montreal, Canada.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised syner-
gistic approach to semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
1243
mantic Evaluation (SemEval 2012), pages 635?642,
Montre?al, Canada, 7-8 June.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
435?440, Montre?al, Canada, 7-8 June.
Johan Bos and Katja Markert. 2006. Recognising tex-
tual entailment with robust logical inference. In Pro-
ceedings of the First international conference on Ma-
chine Learning Challenges: evaluating Predictive Un-
certainty Visual Object Classification, and Recogniz-
ing Textual Entailment, MLCW?05, pages 404?426,
Berlin, Heidelberg. Springer-Verlag.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a ccg parser. In Pro-
ceedings of Coling 2004, pages 1240?1246, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005). Association for Compu-
tational Linguistics.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9, Prague, June. Association for
Computational Linguistics.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13?18, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Demetrios Glinos. 2012. Ata-sem: Chunk-based deter-
mination of semantic text similarity. In Proceedings
of the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 547?551, Montre?al,
Canada, 7-8 June.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: exploring linguistic feature combina-
tions via machine learning. In In Proceedings of the
1999 Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 203?212.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O? Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. Semeval-2010 task 8: Multi-way classifica-
tion of semantic relations between pairs of nominals.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 33?38, Uppsala, Sweden,
July. Association for Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% Solution. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57?60, Morristown, NJ, USA. Association for
Computational Linguistics.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of the Int?l. Conf. on Research in Computational
Linguistics.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332. In C. Fellbaum (Ed.), MIT
Press.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, pages 24?26, New York, NY,
USA. ACM.
Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram cor-
pus. In Proceedings of the ACL 2012 System Demon-
strations, pages 169?174, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, ICML
?98, pages 296?304, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist., 36(3):341?387,
September.
William McCune and Larry Wos. 1997. Otter: The cade-
13 competition incarnations. Journal of Automated
Reasoning, 18:211?220.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. Annotating noun argument struc-
ture for nombank. In LREC. European Language Re-
sources Association.
1244
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the 21st national conference on Artificial intelligence,
AAAI?06, pages 775?780. AAAI Press.
George A. Miller. 1995. WordNet: A Lexical Database
for English. In Communications of the ACM, vol-
ume 38, pages 39?41.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Dan Moldovan and Eduardo Blanco. 2012. Po-
laris: Lymba?s semantic parser. In Nicoletta Calzo-
lari, Khalid Choukri, Thierry Declerck, Mehmet Ug?ur
Dog?an, Bente Maegaard, Joseph Mariani, and
Jan Odijk a nd Stelios Piperidis, editors, Proceedings
of the Eighth International Conference on Language
Resources and Evaluation (LREC-2012), pages 66?72,
Istanbul, Turkey, May. European Language Resources
Association (ELRA). ACL Anthology Identifier: L12-
1040.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu,
F. Lacatusu, A. Novischi, A. Badulescu, and O. Bolo-
han. 2002. Lcc tools for question answering. In
Voorhees and Buckland, editors, Proceedings of the
11th Text REtrieval Conference (TREC-2002), NIST,
Gaithersburg.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised Semantic Parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10, Singapore, August. As-
sociation for Computational Linguistics.
James Pustejovsky and Marc Verhagen. 2009. SemEval-
2010 Task 13: Evaluating Events, Time Expressions,
and Temporal Relations (TempEval-2). In Proceed-
ings of the Workshop on Semantic Evaluations: Re-
cent Achievements and Future Directions (SEW-2009),
pages 112?116, Boulder, Colorado, June. Association
for Computational Linguistics.
Ross J. Quinlan. 1992. Learning with continuous
classes. In 5th Australian Joint Conference on Arti-
ficial Intelligence, pages 343?348, Singapore. World
Scientific.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of the 20th na-
tional conference on Artificial intelligence - Volume 3,
AAAI?05, pages 1099?1105. AAAI Press.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th international joint conference on Artificial
intelligence - Volume 1, IJCAI?95, pages 448?453, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2012.
Uow: Semantically informed text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 673?678,
Montre?al, Canada, 7-8 June.
Evan Sandhaus. 2008. The new york times annotated
corpus. In Linguistic Data Consortium, Philadelphia,
PA.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, HLT ?05, pages 371?378, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?94, pages 133?138, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Structured
Classification with Probabilistic Categorial Grammars.
In Proceedings of the Proceedings of the Twenty-First
Conference Annual Conference on Uncertainty in Ar-
tificial Intelligence (UAI-05), pages 658?666, Arling-
ton, Virginia. AUAI Press.
1245
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 145?154,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Leveraging Verb-Argument Structures to Infer Semantic Relations
Eduardo Blanco and Dan Moldovan
Lymba Corporation
Richardson, TX 75080 USA
{eduardo,moldovan}@lymba.com
Abstract
This paper presents a methodology to in-
fer implicit semantic relations from verb-
argument structures. An annotation effort
shows implicit relations boost the amount
of meaning explicitly encoded for verbs.
Experimental results with automatically
obtained parse trees and verb-argument
structures demonstrate that inferring im-
plicit relations is a doable task.
1 Introduction
Automatic extraction of semantic relations is an
important step towards capturing the meaning of
text. Semantic relations explicitly encode links be-
tween concepts. For example, in The accident left
him a changed man, the ?accident? is the CAUSE
of the man undergoing some ?change?. A question
answering system would benefit from detecting
this relation when answering Why did he change?
Extracting all semantic relations from text is a
monumental task and is at the core of language
understanding. In recent years, approaches that
aim at extracting a subset of all relations have
achieved great success. In particular, previous re-
search (Carreras and Ma`rquez, 2005; Punyakanok
et al., 2008; Che et al., 2010; Zapirain et al., 2010)
focused on verb-argument structures, i.e., relations
between a verb and its syntactic arguments. Prop-
Bank (Palmer et al., 2005) is the corpus of refer-
ence for verb-argument relations. However, rela-
tions between a verb and its syntactic arguments
are only a fraction of the relations present in texts.
Consider the statement [Mr. Brown]NP
1
suc-
ceeds [Joseph W. Hibben, who retired last
August]NP
2
and its parse tree (Figure 1). Verb-
argument relations encode that NP
1
is the AGENT
and NP
2
is the THEME of verb ?succeeds? (Prop-
Bank uses labels ARG
0
and ARG
1
). Any se-
mantic relation between ?succeeds? and concepts
dominated in the parse tree by one of its syntac-
tic arguments NP
1
or NP
2
, e.g., ?succeeds? oc-
S
NP
1
VP
Mr. Brown VBZ NP
2
succeeds
AGENT
THEME
TIME-AFTER
[Joseph W. Hibben, who]AGENT
[retired]v [last August]TIME
Figure 1: Example of parse tree and verb-
argument structures (solid arrows). The relation
between ?succeeds? and ?last August? is missing,
but a TIME-AFTER holds (dashed arrow).
curred after ?last August?, are missing. Note that
in this example, verb-argument structures encode
that ?retired ? has TIME ?last August?, and this
knowledge could be exploited to infer the miss-
ing relation. The work presented here stems from
two observations: (1) verbs are semantically con-
nected with concepts that are not direct syntac-
tic arguments (henceforth, implicit relations); and
(2) verb-argument structures can be leveraged to
infer implicit relations.
This paper goes beyond verb-argument struc-
tures and targets implicit relations like the one
depicted above. TIME, LOCATION, MANNER,
PURPOSE and CAUSE are inferred without im-
posing syntactic restrictions between their argu-
ments: systems trained over PropBank do not at-
tempt to extract these relations. An annotation ef-
fort demonstrates implicit relations reveal as much
as 30% of meaning on top of verb-argument struc-
tures. The main contributions are: (1) empirical
study of verb-argument structures and implicit re-
lations in PropBank; (2) annotations of implicit re-
lations on top of PropBank; (3) novel features ex-
tracted from verb-argument structures; and (4) ex-
perimental results with features derived from gold
and automatically obtained linguistic information,
showing implicit relations can be extracted in a re-
alistic environment.
145
2 Related Work
Several systems to extract verb-argument struc-
tures from plain text have been proposed (Johans-
son and Nugues, 2008; Che et al., 2010). The
work presented here complements them with ad-
ditional semantic relations. The TimeBank corpus
(Pustejovsky et al., 2003) and TempEval compe-
titions (UzZaman et al., 2013) target events and
detailed temporal information; this work also tar-
gets LOCATION, MANNER, PURPOSE and CAUSE.
Extracting missing relations is not a new prob-
lem. Early work focused on a very limited domain
(Palmer et al., 1986; Tetreault, 2002) or did not
attempt to automate the task (Whittemore et al.,
1991). This section focuses on more recent work.
Gerber and Chai (2010) augment NomBank an-
notations (Meyers et al., 2004) of 10 predicates
with additional core arguments. Their supervised
systems obtain F-measures of 42.3 and 50.3 (Ger-
ber and Chai, 2012). Laparra and Rigau (2013a)
present a deterministic algorithm and obtain an F-
measure of 45.3. In contrast, our approach does
not focus on a few selected predicates or core argu-
ments. It targets all predicates and argument mod-
ifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose
meaning is shared across verbs.
The SemEval-2010 Task 10: Linking Events
and their Participants in Discourse (Ruppenhofer
et al., 2009) targeted cross-sentence missing core
arguments in both PropBank and FrameNet (Baker
et al., 1998). Ruppenhofer et al. (2013) detail
the annotations and results. The task proved ex-
tremely difficult, participants (Chen et al., 2010;
Tonelli and Delmonte, 2010) reported overall F-
measures around 2 (out of 100). Posterior work
(Silberer and Frank, 2012; Laparra and Rigau,
2013b) reported F-measures below 20 for the same
task. The work presented here does not target
missing core arguments but modifiers within the
same sentence. Furthermore, results show our pro-
posal is useful in a real environment.
Finally, our previous work (Blanco and
Moldovan, 2011; Blanco and Moldovan, 2014)
proposed composing new relations out of chains
of previously extracted relations. This approach
is unsupervised and accurate (88% with gold an-
notations), but inferences are made only between
the ends of chains of existing relations. Our cur-
rent proposal also leverages relations previously
extracted, but productivity is higher and results
with automatic annotations are presented.
[But]MDIS [the surprisingly durable seven-year economic
expansion]ARG
0
has [made]v [mincemeat]ARG
1
[of more
than one forecast]ARG
2
.
Also, financial planners advising on insurance say that
to their knowledge there has not yet been [a tax
ruling]ARG
0
[exempting]v [these advance payments]ARG
1
[from taxes]ARG
2
.
Table 1: Examples of verb-argument structures
from PropBank.
3 Verb-Argument Structures and
Implicit Relations
Throughout this paper, R(x, y) denotes a seman-
tic relation R holding between x and y. R(x,
y) is interpreted ?x has R y?, e.g., AGENT(took,
Bill ) could be read ?took has AGENT Bill?. Verb-
argument structures, or semantic roles, account for
semantic relations between a verb and its syntactic
arguments. In other words, R(x, y) is a semantic
role if ?x? is a verb and ?y? a syntactic argument
of ?x?, and all semantic roles with ?x? as first ar-
gument form the verb-argument structure of verb
?x?. Implicit relations are relations R(x, y) where
x is a verb and y is not a syntactic argument of x.
The work presented in this paper aims at com-
plementing verb-argument structures with implicit
semantic relations. We follow a practical approach
by inferring implicit relations from PropBank?s
verb-argument structures. We believe this is an
advantage since PropBank is well-known in the
field and several tools to predict PropBank annota-
tions are documented and publicly available.1 The
work presented here could be incorporated in any
NLP pipeline after role labeling without modifica-
tions to other components. Furthermore, working
on top of PropBank allows us to quantify the im-
pact of features derived from gold and automati-
cally extracted linguistic information when infer-
ring implicit relations (Section 6).
3.1 Verb-Argument structures in PropBank
PropBank (Palmer et al., 2005) annotates verb-
argument structures on top of the syntactic trees
of the Penn TreeBank (Marcus et al., 1994). It
uses a set of numbered arguments2 (ARG
0
, ARG
1
,
ARG
2
, etc.) and modifiers (AM-TMP, AM-MNR,
etc.). Numbered arguments do not share a com-
mon meaning across verbs, they are defined on a
1E.g., Illinois SRL, http://cogcomp.cs.illinois.edu/
page/software; SENNA, http://ml.nec-labs.com/senna/;
SwiRL, http://www.surdeanu.info/mihai/swirl/
2Numbered arguments are also referred to as core.
146
SNP
1
VP
1
NP
2
VP
2
VBD VP
3
The first hybrid
corn seeds
VBN S-ADV were VBD PP
produced
TIME
THEME
MANNER
using this mechanical
approach
introduced
THEME
TIME
in the 1930s
Figure 2: Verb-argument structures (solid arrows) and inferred implicit semantic relation (dashed arrow).
AM-LOC: location AM-CAU: cause
AM-EXT: extent AM-TMP: time
AM-DIS: discourse connective AM-PNC: purpose
AM-ADV: general-purpose AM-MNR: manner
AM-NEG: negation marker AM-DIR: direction
AM-MOD: modal verb
Table 2: Argument modifiers in PropBank.
Label # predicates % predicates
ARG
0
79,334 70.26%
ARG
1
106,331 94.17%
ARG
2
24,560 21.75%
AM-TMP 19,756 17.50%
AM-MNR 7,833 6.94%
AM-LOC 7,198 6.37%
AM-PNC 2,784 2.47%
AM-CAU 1,563 1.38%
Table 3: Counts of selected PropBank semantic
roles. Total number of predicates is 112,917.
verb by verb basis in each frameset. For exam-
ple, ARG
2
is used to indicate ?created-from, thing
changed? with verb make and ?entity exempted
from? with verb exempt (Table 1).
Unlike numbered arguments, modifiers share a
common meaning across verbs (Table 2). Some
modifiers are arguably not a semantic relation
and are not present in most relation invento-
ries (Tratz and Hovy, 2010; Hendrickx et al.,
2009). For example, AM-NEG and AM-MOD sig-
nal the presence of negation and modals, e.g.,
[wo]AM-MOD[n?t]AM-NEG [go]v. For more informa-
tion about PropBank annotations and examples,
refer to the annotation guidelines.3
Inspecting PropBank annotations one can eas-
ily conclude that numbered arguments dominate
the annotations and only a few modifiers are an-
3http://verbs.colorado.edu/
?
mpalmer/projects/ace/
PBguidelines.pdf
notated (Table 3). ARG
0
and ARG
1
are present
in most verb-argument structures, other numbered
arguments are often not defined in the correspond-
ing frameset and are thus not annotated.
Examining PropBank one can also conclude
that information regarding TIME, LOCATION,
MANNER, CAUSE and PURPOSE for a given verb
is often present, yet not annotated because the text
encoding this knowledge is not a direct syntactic
argument of the verb (Section 4.3). Because of this
fact, we decided to focus on these five relations.
3.2 Implicit relations in PropBank
Two scenarios are possible when inferring an im-
plicit relation R(x, y): (1) a semantic role R?(x?, y)
exists; or (2) such a semantic role does not exists.
In (1), y is a syntactic argument of some verb x?,
where x 6= x? and in (2) that is not the case. Infer-
ences under scenario (1) can be further classified
into (1a) when a semantic role R??(x, y?) such that
y? contains y exists; or (1b) when such a semantic
roles does not exist. The remainder of this section
exemplifies the three scenarios.
The example in Figure 1 falls under scenario
(1a). Semantic roles encode, among others, ?re-
tired ? has TIME ?last August?, and ?succeeds? has
AGENT ?Mr. Brown? and THEME ?Joseph W. Hi-
bben, who retired last August?. The second argu-
ment of implicit relation TIME-AFTER(succeeds,
last August) is a semantic role of ?retired ? and is
contained in the THEME of ?succeeds?.
Figure 2 shows a statement in which implicit re-
lation TIME(produced, in the 1930s) could be in-
ferred under scenario (1b). Semantic roles of ?pro-
duced ? only indicate that NP
2
is the THEME and
S-ADV the MANNER; roles of ?introduced ? indi-
cate that NP
1
is the THEME and ?[in the 1930s]PP?
the TIME. In this case, there is no connection be-
147
rs = {TIME, LOCATION, MANNER, CAUSE, PURPOSE};
foreach semantic role R?(x?, y) such that R? ? rs do
foreach verb x in the same sentence do
generate potential implicit relation R(x, y);
Algorithm 1. Procedure to generate all potential
implicit relations in scenario (1) (Section 3.2).
tween ?produced ? and ?[in the 1930s]PP? or any
other node subsuming this PP in the parse tree.
Scenario (2) occurs whenever the second argu-
ment of implicit relation R(x, y) is not a syntac-
tic argument of a verb. If it were, a semantic
role R?(x?, y) would exist and it would fall un-
der scenario (1). For example, in [I]AGENT [gave]v
[her]RECIPIENT [a book from 1945]THEME, we could
infer the implicit semantic relation ?gave occurred
after 1945?.
4 Annotating Implicit Relations
Inferring all implicit semantic relations is a chal-
lenging task. This paper targets implicit relations
that can be inferred under scenarios (1a, 1b); sce-
nario (2) is reserved for future work. All poten-
tial implicit relations under scenario (1) are gen-
erated using Algorithm 1. A manual annotation
effort discards potential implicit relations that do
not hold in order to create a gold standard.
4.1 Annotation Guidelines
Annotators are faced with the task of deciding
whether a potential implicit relation R(x, y) holds.
If it does, they mark it with YES, otherwise with
NO. Annotators were initially trained with the
original PropBank annotation guidelines4 as this
task is very similar to annotating PropBank se-
mantic roles. Indeed, the only difference is that
?y? is not a syntactic argument of ?x?.
After some preliminary annotations, we found
it useful to account for three subtypes of TIME.
This way, richer semantic connections are in-
ferred. When the task is to decide whether im-
plicit relation TIME(x, y) holds, annotators have
four labels to choose from: (1) TIME-BEFORE: x
occurred before y; (2) TIME-AFTER: x occurred
after y; (3) TIME-SAME x occurred at/during y; and
(4) NO: y does not describe temporal information
of x. If more than one label is valid, annotators
choose the one encoding the temporal context y
of x starting the earliest. Namely, TIME-BEFORE
4http://verbs.colorado.edu/
?
mpalmer/projects/ace/
PBguidelines.pdf
has the highest priority, followed by TIME-SAME,
TIME-AFTER and finally NO.
Annotation examples are detailed in Section
4.2, the more complex annotations involving TIME
are illustrated below. Consider the following state-
ment and PropBank annotations:
[The government?s decision]ARG
2
, v
1
[reflects]v
1
[their true desires before
[the next election]ARG
1
, v
2
, [expected]v
2
[in late 1991]TIME, v
2
]ARG
1
, v
1
.
When annotating potential implicit semantic re-
lation R(reflects, in late 1991 ), annotators may
select TIME-BEFORE, TIME-SAME and TIME-
AFTER. However, they select TIME-BEFORE be-
cause it indicates the temporal context of ?reflects?
that starts the earliest.
4.2 Annotation Examples
Several annotations examples are shown in Ta-
ble 4. Semantic roles for statement (1) in-
clude TIME(remain, in 1990 ), MANNER(remain,
at about 1,200 cars) and no other TIME or MAN-
NER. Implicit relations reveal two extra seman-
tic connections: TIME-BEFORE(said, in 1990 ) and
TIME-BEFORE(expects, in 1990 ), i.e., ?said ? and
?expects? occurred before ?1990 ?. The potential
implicit relations MANNER(said, at about 1,200
cars) and MANNER(expects, at about 1,200 cars)
do not hold and are annotated N.
Interpreting statement (2) one can see that ?this
past summer? is not only indicating the TIME of
?proposed ?; events encoded by verbs ?make? and
?exempt? occurred after ?this past summer?. In
this example, two implicit semantic relations are
inferred from a single semantic role.
Statement (3) shows that two potential implicit
relations R(x, y) and R(x?, y) sharing the sec-
ond argument ?y? may be assigned different la-
bels. Regarding time, semantic roles only in-
clude TIME(report, in December). Implicit rela-
tions add TIME-BEFORE(proposed, in December)
and TIME-SAME(allow, in December).
Two implicit LOCATION relations are inferred
in statement (4): ?discovered ? and ?preserving?
occurred ?in the test-tube experiments?. The po-
tential implicit relation LOCATION(said, in the
test-tube experiments) is discarded (annotated N).
Statement (5) shows two potential implicit MAN-
NER that can be inferred. The ?program? was
?aired ? and ?seen by 12 million viewers? in the fol-
lowing manner: ?With Mr. Vila as host?.
148
Statement TMP LOC MNR PRP CAU
B A S N Y N Y N Y N Y N
1: Rolls-Royce said it expects [its U.S. sales]ARG
1
to [remain]v [steady]ARG
3
[at about 1,200 cars]MANNER [in 1990]TIME .
? said, [in 1990]TIME X - - -
? expects, [in 1990]TIME X - - -
? said, [at about 1,200 cars]MANNER - X
? expects, [at about 1,200 cars]MANNER - X
2: They make the argument in letters to the agency about [rule changes]ARG
1
[proposed]v [this past summer]TIME that, among
other things, exempt many middle-management executives from government supervision.
? make, [this past summer]TIME - X - -
? exempt, [this past summer]TIME - X - -
3: The proposed changes also allow [executives]ARG
0
to [report]v [exercises of options]ARG
1
[in December]TIME .
? proposed, [in December]TIME X - - -
? allow, [in December]TIME - - X -
4: Two Japanese scientists said they discovered [an antibody that]ARG
0
, [in laboratory test-tube experiments]LOCATION , [kills]v
[AIDS-infected cells]ARG
1
[while preserving healthy cells]TIME .
? said, [in laboratory test-tube experiments]LOCATION - X
? discovered, [in laboratory test-tube experiments]LOCATION X -
? preserving, [in laboratory test-tube experiments]LOCATION X -
5: [With Mr. Vila as host]MANNER , ?[This Old House]ARG
1
? [became]v [one of the Public Broadcasting Service?s top 10
programs]ARG
2
, [airing weekly on about 300 of the network ?s stations and seen by an average of 12 million viewers]AM-ADV .
? airing, [With Mr. Vila as host]MANNER X -
? seen, [With Mr. Vila as host]MANNER X -
[6: It]ARG
0
[raised]v [financing of 300 billion lire]ARG
1
[for the purchase this summer by another Agnelli-related group of
the food concern Galbani S.p.A.]PURPOSE , [by selling a chunk of its IFI shares to Mediobanca S.p.A.]MANNER
? selling, [for the purchase this summer by another . . . ]PURPOSE X -
7: [Greece and Turkey]ARG
0
, for example, are suspected of [overstating]v [their arsenals]ARG
1
[in hopes that they can emerge
from the arms-reduction treaty with large remaining forces to deter each other]PURPOSE .
? suspected, [in hopes that they can emerge from the . . . ]PURPOSE - X
8: . . . the rationalization that [given the country?s lack of natural resources]CAUSE , [they]ARG
0
[must]AM-MOD [work]v
[hard]MANNER [to create value through exports]ARG
1
and buy food with the surplus.
? create, [given the country?s lack of natural resources]CAUSE X -
? buy, [given the country?s lack of natural resources]CAUSE X -
9: Its third-quarter earnings were lower than analysts had forecast, and the company said [it]ARG
0
had [lowered]v [its
projections for earnings growth through the end of 1990]ARG
1
[because of planned price cuts]CAUSE .
? forecast, [because of planned price cuts]CAUSE - X
? said, [because of planned price cuts]CAUSE - X
Table 4: Examples of potential implicit relations and their annotations. All of them but the ones annotated
with N can be inferred. B stands for BEFORE, A for AFTER, S for SAME, N for NO and Y for YES.
PropBank semantic roles from which implicit relations are generated are indicated between brackets.
Statement (6, 7) exemplify potential implicit
PURPOSE relations. While the ?selling? event in
statement (6) has as its purpose ?the purchase
[. . . ] ? (label Y), the ?suspected ? event in statement
(7) is clearly not done so that ?they (Greece and
Turkey) can emerge from the [. . . ] ? (label N).
Finally, statements (8, 9) exemplify potential
implicit CAUSE relations. In (8), both ?create? and
?buy? are done due to the ?country?s lack of natural
resources?. However, in (9), the analysts ?forecast-
ing? and the company ?saying? do not have as their
cause ?planned price cuts?.
4.3 Annotation Analysis
Table 5 shows counts for all potential implicit re-
lations annotated. All labels except N indicate a
valid implicit relation. 94.1% of potential implicit
relations generated from a TIME semantic role can
be inferred. Other roles yield less inferences in
relative terms, but substantial additional mean-
ing: LOCATION 39.4%, MANNER 16.7%, PUR-
POSE 29.4%, and CAUSE 30.2%.
Two annotators performed the annotations. A
simple script generated all potential implicit rela-
tions and prompted for a label: BEFORE, AFTER,
SAME or NO if the potential implicit relation was
generated from a TIME semantic role; YES or NO
otherwise. Annotators are not concerned with ar-
gument identification, as arguments of implicit re-
lations are retrieved from the verb-argument struc-
tures in PropBank (Algorithm 1). This makes the
annotation process easier and faster.
Annotation quality was calculated with two
agreement coefficients: observed agreement (raw
percentage of equal annotations) and Cohen?s ?
(Artstein and Poesio, 2008). The actual num-
149
Source No. Name Description
ba
sic
x
1,2 word, POS tag x?s surface form and part-of-speech tag
3 voice whether x is in active or passive voice
y
4,5 first word, POS tag first word and part of speech tag in y
6,7 last word, POS tag last word and part-of-speech tag in y
8,9 head, POS tag head of y and its part-of-speech tag
10?12 node, left and right sibling syntactic nodes of y, and its left and right siblings
13 subcategory concatenation of y?s children nodes
x, y
14 direction whether x occurs before or after y
15 subsumer common syntactic node between x and y
16 path syntactic path between x and y
pr
ed
st
ru
ct
u
re
s x ps 17?31 verb semantic roles flags indicating presence of semantic roles in x ps
y ps
32,33 verb, POS tag verb in y ps and its part-of-speech tag
34 arg label semantic role between verb in y ps and y
35?49 arg semantic roles flags indicating presence of semantic roles in y ps
x ps, 50 overlapping semantic role role R?? linking x and y?, where y? contains y
y ps 51 overlapping head head of y? in semantic role detected in feature 50
52 overlapping direct whether feature 51 is the verb in y ps
Table 6: Complete feature set to determine whether a potential implicit semantic relation R(x, y) should
be inferred. Second column indicates the source: first or second argument (x, y), or their respective
predicate structures (x ps, y ps). Features in bold are novel and specially designed for our task.
Label # instances % instances
TIME
B 3,033 38.4%
A 2,886 36.5%
S 1,514 19.2%
N 463 5.9%
All 7,896 100.0%
LOCATION
Y 3,345 39.4%
N 5,151 60.6%
All 8,496 100.0%
MANNER
Y 1,600 16.7%
N 7,987 83.3%
All 9,587 100.0%
PURPOSE Y 821 29.4%
N 1,971 70.6%
All 2,792 100.0%
CAUSE
Y 404 30.2%
N 909 69.2%
All 1,313 100.0%
Table 5: Number of potential implicit relations (in-
stances) annotated and counts for each label. Total
number of instances is 30,084.
bers are: 78.16% (observed) / 0.687 (?) for TIME,
86.63% / 0.733 for LOCATION, 93.02% / 0.782
for MANNER, 88.60% / 0.734 for PURPOSE, and
90.91% / 0.810 for CAUSE. These agreements
are either comparable or superior to similar pre-
vious annotation efforts. Girju et al. (2007) re-
ported observed agreements between 47.8% and
86.1% when annotating 7 semantic relations be-
tween nominals, and Bethard et al. (2008) ob-
served agreements of 81.2% and 77.8% (Kappa:
0.715 and 0.556) when annotating temporal and
causal relations between event pairs.
5 Inferring Implicit Relations
Inferring implicit relations is reduced to (1) gener-
ating potential implicit relations (Algorithm 1) and
(2) labeling them. The second task determines if
potential implicit relations should be discarded or
inferred, all labels but N indicate potential implicit
relations that should be inferred. We follow a stan-
dard supervised machine learning approach where
each potential implicit relation is an instance.
Instances were divided into training (70%) and
test (30%). The feature set (Section 5.1) and
model parameters were tuned using 10-fold strat-
ified cross-validation over the training split, and
results (Section 6) are reported using the test split.
More features than the ones presented were tried
and discarded because they did not improve per-
formance, e.g., syntactic path between verbs in the
verb-argument structures of x and y, depth of both
structures, number of tokens in y.
5.1 Feature Selection
The full set of features to determine whether a po-
tential implicit relation R(x, y) can be inferred is
summarized in Table 6. Features are classified
into basic and predicate structures. The former
are commonly used by semantic role labelers. The
latter exploit the output of role labelers, i.e., verb-
argument structures, and, to our knowledge, are
novel. Results show predicate structures features
improve performance (Section 6.2).
Basic features are derived from lexical and syn-
tactic information. We do not elaborate more on
150
Feat No. Value
1,2 succeeds, VBZ
3 active
4,5 last, JJ
6,7 August, NNP
8,9 August, NNP
10?12 NP, VBD, nil
13 JJ-NNP
14 after
15 VP
16 VBZ+VP-NP-SBAR-S-VP-NP
17?31 ARG
0
and ARG
1
true, rest false
32,33 retired, VBD
34 AM-TMP
35-49 ARG
0
and AM-TMP true, rest false
50 ARG
1
51 Hibben
52 false
Table 7: Feature values when deciding if
R(succeeds, last summer) can be inferred from the
verb-argument structures in Figure 1.
these features, detailed descriptions and examples
are provided by Gildea and Jurafsky (2002).
Features (17?52) are derived from the predicate
structures of x and y and specially defined to infer
implicit semantic relations. Features (17?31, 35?
49) are flags indicating the presence of semantic
roles in the predicate structures of x and y.
Features (32?34) characterize the semantic role
R?(x?, y) from which the potential implicit relation
was generated. They specify verb x?, its part-of-
speech, and label R?. Note that x? is not present in
the potential implicit relation R(x, y), but incorpo-
rating this information helps determining whether
a relation actually holds as well as label R (TIME-
BEFORE, TIME-AFTER, TIME-SAME, etc.).
Finally, features 50?52 apply to inferences un-
der scenario (1a) (Section 3.2). Feature (50) indi-
cates the semantic role R??(x, y?), if any, such that
y? contains y. Feature (51) indicates the head of ar-
gument y? found in feature (50). Feature (52) cap-
tures whether the head calculated in feature (51) is
the verb in the predicate structure of y.
Table 7 exemplifies all features when deciding
whether TIME-AFTER(succeeds, last August) can
be inferred from the verb-argument structures in
Mr. Brown succeeds Joseph W. Hibben, who re-
tired last August (Figure 1). Table 8 provides an
additional example for features 50?52.
6 Experiments and Results
Experiments were carried out using Support Vec-
tor Machines with RBF kernel as implemented in
Mr. Corr resigned to pursue other interests, the airline said.
ARG
0
(resigned, Mr. Corr)
AM-PNC(resigned, to pursue other interests)
ARG
0
(pursue, Mr. Corr)
ARG
1
(pursue, other interests)
ARG
0
(said, the airline)
ARG
1
(said, Mr. Corr resigned to pursue other interests)
feature 50, overlapping sem rel ARG
1
feature 51, overlapping head resigned
feature 52, overlapping direct true
Table 8: PropBank roles and values for features
(50?52) when predicting potential implicit relation
R(said, to pursue other interests), labeled N.
LIBSVM (Chang and Lin, 2011). Parameters ?
and ? were tuned by grid search using 10-fold
cross validation over training instances.
Results are reported using features extracted
from gold and automatic annotations. Gold anno-
tations are taken directly from the Penn TreeBank
and PropBank. Automatic annotations are ob-
tained with Polaris (Moldovan and Blanco, 2012),
a semantic parser that among others is trained with
PropBank. Results using gold (automatic) annota-
tions are obtained with a model trained with gold
(automatic) annotations.
6.1 Detailed Results
Table 9 presents per-relation and overall results. In
general terms, there is a decrease in performance
when using automatic annotations. The difference
is most noticeable in recall and it is due to missing
semantic roles, which in turn are often due to syn-
tactic parsing errors. This is not surprising as in
order for an implicit relation R(x, y) to be gener-
ated as potential and fed to the learning algorithm
for classification, a semantic role R?(x?, y) must be
extracted first (Algorithm 1). However, using au-
tomatic annotations brings very little decrease in
precision. This leads to the conclusion that as long
as ?y? is identified as a semantic role of some verb,
even if it is mislabeled, one can still infer the right
implicit relations. Since results obtained with au-
tomatic parse trees and semantic roles are a realis-
tic estimation of performance, the remainder of the
discussion focuses on those. Results with gold an-
notations are provided for informational purposes.
Overall results for inferring implicit semantic
relations are encouraging: precision 0.66, recall
0.58 and F-measure 0.616. Direct comparison
with previous work is not possible because the
implicit relations we aim at inferring have not
been considered before. However, we note the top
151
gold automatic
basic basic + ps basic basic + ps
P R F P R F P R F P R F
TIME
B .66 .72 .689 .72 .74 ?.730 .64 .65 .643 .68 .67 .677
A .63 .74 .681 .67 .75 .708 .61 .68 .642 .66 .72 .687
S .57 .41 .477 .54 .45 .491 .55 .36 .437 .55 .38 .450
LOCATION Y .71 .61 .656 .70 .64 .669 .71 .56 .624 .71 .58 .635
MANNER Y .65 .38 .480 .60 .45 .514 .54 .45 .489 .64 .41 .500
PURPOSE Y .65 .58 .613 .69 .60 .642 .56 .49 .525 .68 .49 .572
CAUSE Y .71 .60 .650 .74 .62 .675 .69 .65 .670 .71 .63 .669
All .66 .61 .625 .67 .64 ?.651 .63 .57 .591 .66 .58 ?.616
Table 9: Results obtained with the test split using features extracted from gold and automatic annotations,
and using basic and predicate structures (ps) features. Statistical significance between F-measures using
basic and basic + predicate structures features is indicated with ? (confidence 95%).
performer (Koomen et al., 2005) at CoNLL-2005
Shared Task on role labeling obtained the follow-
ing F-measures when extracting the same relations
between a verb and its syntactic arguments: 0.774
(TIME), 0.6033 (LOCATION), 0.5922 (MANNER),
0.4541 (PURPOSE) and 0.5397 (CAUSE).
The most difficult relations are TIME-SAME and
MANNER, F-measures are 0.450 and 0.500 re-
spectively. Even when using gold annotations
these two relations are challenging: F-measures
are 0.491 for TIME-SAME, an increase of 9.1%,
and 0.514 for MANNER, an increase of 2.8%. Re-
sults show that other relations can be inferred with
F-measures between 0.635 and 0.687, the only ex-
ception is PURPOSE with an F-measure of 0.572.
6.2 Feature Ablation
Results in Table 9 suggest that while implicit rela-
tions can be inferred using basic features, it is ben-
eficial to complement them with the novel features
derived from predicate structures. This is true for
all relations except CAUSE when using automatic
annotations with a negligible difference of 0.001.
When considering all implicit relations, the differ-
ence in performance is 0.616 ? 0.591 = 0.025,
an increase of 4.2% that is statistically significant
(Z-test, confidence 95%).
The positive impact of features derived from
predicate structures is most noticeable when infer-
ring PURPOSE, with an increase of 8.9% (0.572 ?
0.525 = 0.047). TIME-BEFORE and TIME-AFTER
also benefit, with increases of 5.3% (0.677 ?
0.643 = 0.034) and 7.0% (0.687?0.642 = 0.045)
respectively. The improvement predicate struc-
tures features bring is statistically significant when
taking into account all relations (confidence 95%).
However, due to the lower number of instances,
differences in performance when considering in-
dividual relations is not statistically significant.
7 Conclusions
Verb-argument structures, or semantic roles, com-
prise semantic relations between a verb and its
syntactic arguments. The work presented in this
paper leverages verb-argument structures to infer
implicit semantic relations. A relation R(x, y) is
implicit if x is a verb and y is not a syntactic ar-
gument of x. The method could be incorporated
into any NLP pipeline after role labeling without
modifications to other components.
An analysis of verb-argument structures and im-
plicit relations in PropBank has been presented.
Out of all potential implicit relations R(x, y), this
paper targets those that can be generated from a
semantic role R?(x?, y), where x 6= x?. A man-
ual annotation effort demonstrates implicit rela-
tions yield substantial additional meaning. Most
of the time (94.1%) a semantic role TIME(x? , y)
is present, we can infer temporal information for
other verbs within the same sentence. Productiv-
ity is lower but substantial with other roles: 39.4%
(LOCATION), 30.2% (CAUSE), 29.4% (PURPOSE)
and 16.7% (MANNER).
Experimental results show that implicit rela-
tions can be inferred using automatically obtained
parse trees and verb-argument structures. Stan-
dard machine learning is used to decide whether a
potential implicit relation should be inferred, and
novel features characterizing the verb-argument
structures we infer from have been proposed.
152
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Steven Bethard, William Corvey, Sara Klingenstein,
and James H. Martin. 2008. Building a Corpus of
Temporal-Causal Structure. In Proceedings of the
Sixth International Language Resources and Evalu-
ation (LREC?08), pages 908?915, Marrakech, Mo-
rocco. European Language Resources Association
(ELRA).
Eduardo Blanco and Dan Moldovan. 2011. Un-
supervised learning of semantic relation composi-
tion. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 1456?1465,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Eduardo Blanco and Dan Moldovan. 2014. Compo-
sition of semantic relations: Theoretical framework
and case study. ACM Trans. Speech Lang. Process.,
10(4):17:1?17:36, January.
Xavier Carreras and Llu??s Ma`rquez. 2005. Intro-
duction to the CoNLL-2005 shared task: semantic
role labeling. In CONLL ?05: Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning, pages 152?164, Morristown, NJ,
USA. Association for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Wanxiang Che, Ting Liu, and Yongqiang Li. 2010. Im-
proving Semantic Role Labeling with Word Sense.
In The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 246?249, Los Angeles, Califor-
nia, June. Association for Computational Linguis-
tics.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 264?267, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A Study of Implicit Arguments for Nomi-
nal Predicates. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1583?1592, Uppsala, Sweden, July.
Association for Computational Linguistics.
Matthew Gerber and Joyce Chai. 2012. Seman-
tic role labeling of implicit arguments for nominal
predicates. Computational Linguistics, 38:755?798,
2012.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling Of Semantic Roles. Computational Lin-
guistics, 28:245?288.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13?18, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Iris Hendrickx, Su N. Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid, Sebastian Pado?, Marco Pennac-
chiotti, Lorenza Romano, and Stan Szpakowicz.
2009. SemEval-2010 Task 8: Multi-Way Classifica-
tion of Semantic Relations Between Pairs of Nom-
inals. In Proceedings of the Workshop on Seman-
tic Evaluations: Recent Achievements and Future
Directions (SEW-2009), pages 94?99, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of prop-
bank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 69?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen T. Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005), pages
181?184, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Egoitz Laparra and German Rigau. 2013a. Impar: A
deterministic algorithm for implicit semantic role la-
belling. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1180?1189,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Egoitz Laparra and German Rigau. 2013b. Sources of
evidence for implicit argument resolution. In Pro-
ceedings of the 10th International Conference on
Computational Semantics (IWCS 2013) ? Long Pa-
pers, pages 155?166, Potsdam, Germany, March.
Association for Computational Linguistics.
Mitchel Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
153
corpus of English: The Penn Treebank. Computa-
tional linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
Annotating Noun Argument Structure for Nom-
Bank. In Proceedings of LREC-2004, pages 803?
806, Lisbon, Portugal.
Dan Moldovan and Eduardo Blanco. 2012. Polaris:
Lymba?s semantic parser. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation (LREC-2012), pages 66?
72, Istanbul, Turkey, May. European Language Re-
sources Association (ELRA).
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and
John Dowding. 1986. Recovering implicit infor-
mation. In Proceedings of the 24th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 10?19, New York, New York, USA, July.
Association for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287, June.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The timebank corpus. Corpus
linguistics, 2003:40.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2009.
SemEval-2010 Task 10: Linking Events and Their
Participants in Discourse. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009),
pages 106?111, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Josef Ruppenhofer, Russell Lee-Goldman, Caroline
Sporleder, and Roser Morante. 2013. Beyond
sentence-level semantic role labeling: linking argu-
ment structures in discourse. Language Resources
and Evaluation, 47(3):695?721.
Carina Silberer and Anette Frank. 2012. Casting im-
plicit role linking as an anaphora resolution task.
In *SEM 2012: The First Joint Conference on
Lexical and Computational Semantics, pages 1?10,
Montre?al, Canada, 7-8 June. Association for Com-
putational Linguistics.
Joel R Tetreault. 2002. Implicit role reference. In In-
ternational Symposium on Reference Resolution for
Natural Language Processing, pages 109?115.
Sara Tonelli and Rodolfo Delmonte. 2010. Venses++:
Adapting a deep semantic processing system to the
identification of null instantiations. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 296?299, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2010. A Taxonomy,
Dataset, and Classifier for Automatic Noun Com-
pound Interpretation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 678?687, Uppsala, Sweden, July.
Association for Computational Linguistics.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Greg Whittemore, Melissa Macpherson, and Greg
Carlson. 1991. Event-building through role-filling
and anaphora resolution. In Proceedings of the 29th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 17?24, Berkeley, Califor-
nia, USA, June. Association for Computational Lin-
guistics.
Be N. Zapirain, Eneko Agirre, Llu??s Ma`rquez, and Mi-
hai Surdeanu. 2010. Improving Semantic Role
Classification with Selectional Preferences. In The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 373?376, Los Angeles, California,
June. Association for Computational Linguistics.
154
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 456?465,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Fine-Grained Focus for Pinpointing Positive Implicit Meaning
from Negated Statements
Eduardo Blanco and Dan Moldovan
Lymba Corporation
Richardson, TX 75080 USA
{eduardo,moldovan}@lymba.com
Abstract
Negated statements often carry positive im-
plicit meaning. Regardless of the seman-
tic representation one adopts, pinpointing the
positive concepts within a negated statement
is needed in order to encode the statement?s
meaning. In this paper, novel ideas to reveal
positive implicit meaning using focus of nega-
tion are presented. The concept of granular-
ity of focus is introduced and justified. New
annotation and features to detect fine-grained
focus are discussed and results reported.
1 Introduction
Semantic representation of text is an important step
towards text understanding. Current approaches are
based on relatively shallow representations and ig-
nore pervasive linguistic phenomena such as nega-
tion and metaphor. Despite these weaknesses, shal-
low representations have been proven useful for sev-
eral tasks, e.g., coreference resolution (Kong et al,
2009), machine translation (Wu and Fung, 2009).
Consider statement (1) The company won?t ship
the new product to the United States until next year.
Existing approaches to represent the meaning of (1)
either indicate that the verb ship is negated or disre-
gard the negation altogether. Semantic role labelers
trained over PropBank would link n?t to ship with
MNEG (i.e., negate the verb); any system based on
FrameNet and more recent unsupervised proposals
(Poon and Domingos, 2009; Liang et al, 2011; Titov
and Klementiev, 2011) ignore negation.
In order to represent the meaning of (1), one must
first ascertain that the negation mark n?t is actually
negating the TEMPORAL context linked to ship and
not the verb per se; more specifically, n?t is negating
exclusively the preposition until. Only doing so one
can aim at representing the actual meaning of (1):
The company will ship the new product to the United
States during next year. Note that the verb ship, and
its AGENT, THEME and LOCATION (i.e., The com-
pany, the new product and to the United States) are
positive, as well as the temporal anchor next year.
Regardless of the semantic representation one fa-
vors (logic forms, predicate calculus, semantic re-
lations, semantic frames, etc.), we argue that pin-
pointing the numerous words that contribute to im-
plicit positive meanings within a negated statement
is a required subtask to obtain it. This paper aims
at extracting specific positive implicit meaning from
negated statements. The main contributions are:
(1) interpretation of negation using fine-grained fo-
cus; (2) fine-grained focus of negation annotation
over a subset of PropBank; (3) feature set to de-
tect fine-grained focus of negation; and (4) model
to retrieve precise positive implicit meaning from
negated statements.
2 Related Work
Negation has been widely studied from a theoreti-
cal point of view. The seminal work by Horn (1989)
presents the main thoughts in philosophy and psy-
chology. Work in linguistics has studied the in-
teraction of negation with quantifiers and anaphora
(Hintikka, 2002), as well as the role in reasoning
(Sa?nchez Valencia, 1991; Dowty, 1994): one can
perform downward (but not upward) monotone in-
ference with negative statements. Zeijlstra (2007)
analyzes the position and form of negative ele-
456
ments and negative concords; concepts such as intra
and inter-domain negation and strength of negation
(Ladusaw, 1996), syntactic and semantic negation
(Lo?bner, 2000) have been discussed in the extensive
literature, although we do not use them.
In computational linguistics, negation has mainly
drawn attention in sentiment analysis (Wilson et al,
2009; Wiegand et al, 2010) and the biomedical do-
main. Recently, two events (Morante and Sporleder,
2010; Farkas et al, 2010) targeted negation mostly
on those subfields. Among many others, Morante
and Daelemans (2009) and Li et al (2010) propose
scope detectors using the BioScope corpus. Consid-
ering scope is indeed a step forward, but focus must
also be taken into account to represent negated state-
ments and detect their positive implicit meanings.
Regarding corpora, BioScope annotates negation
marks and linguistic scopes exclusively on biomed-
ical texts. It does not annotate focus and it pur-
posely disregards negations such as the reactions
in NK3.3 cells are not always identical (Vincze et
al., 2008), which carry the kind of positive meaning
we aim at extracting (the reactions in NK3.3 cells
are sometimes identical). Recently, Morante et al
(2011) present scope annotation in two Conan Doyle
works, but they dismiss focus and positive meaning
extraction. As stated before, PropBank (Palmer et
al., 2005) treats negation superficially and FrameNet
(Baker et al, 1998) regrettably disregards negation.
Blanco and Moldovan (2011) introduce a seman-
tic representation of negation using focus detection.
They target verbal negation and work on top of Prop-
Bank, selecting as focus the role that corresponds
to the focus of negation. Simply put, they propose
that all roles but the one corresponding to the fo-
cus are actually positive. Their approach, however,
has a major drawback: selecting the whole role often
yields too coarse of a focus and the positive implicit
meaning is not fully specified (Section 3.1).
Focus-Sensitive Phenomena. The literature uses
the term focus for widely distinct phenomena; space
permits only a cursory review. Within functional
generative grammars, focus is defined as what is be-
ing asserted about the topic (Hajic?ova? et al, 1995).
The term is also used in pragmatics (Glanzberg,
2005), and in phonetics and phonology (Xu and Xu,
2005; Beaver et al, 2007).
In linguistics, focus is largely associated with the
theory presented in Mats Rooth?s dissertation (1985)
and posterior publications (Rooth, 1992). He ana-
lyzes the effect of focus in diverse phenomena, e.g.,
questions and answers, reasons and counterfactu-
als, conversational implicature, bare remnant ellip-
sis. His alternative semantics (e.g., they didn?t order
the right parts implies that some alternative of the
form they ordered X is true) (Rooth, 1997) was an
inspiration for this work. However, Rooth does not
discuss how to detect focus of negation or its granu-
larity and only provides simple made-up examples.
3 Scope and Focus
Negation has both scope and focus and they are key
to capture its meaning. Scope is the part of the
meaning that is negated. Focus is that part of the
scope that is most prominently or explicitly negated
(Huddleston and Pullum, 2002). All elements whose
individual falsity would make the negated statement
strictly true belong to the scope. Focus is the ele-
ment of the scope that is intended to be interpreted
as false to make the overall negative true.
Consider (1) We didn?t get an offer for more than
$40 and its positive counterpart (2) We got an offer
for more than $40. The truth conditions of (2) are:
(a) somebody got something; (b) we got; (c) an of-
fer was gotten; and (d) the offer was for more than
$40. In order for (2) to be true, (a?d) have to be
true. Conversely, the falsity of any of them is suffi-
cient to make (1) true: (1) would be true if nobody
got anything, we didn?t get, an offer wasn?t gotten
or the offer wasn?t for more than $40. Thus, all four
statements (a?d) are inside the scope of (1).
The focus is often more difficult to identify. Text
understanding is needed and context plays an impor-
tant role. The most probable focus for (1) is more
than, which corresponds to the interpretation we got
an offer for $40 or less. Another possible focus is
for more than $40, which yields we got an offer, but
not for more than $40. A third possible focus is an
offer for more than $40, which yields we got some-
thing, but not an offer for more than $40. Section
3.1 discusses coarse versus fine-grained focus.
Both scope and focus are primarily semantic,
highly ambiguous and context-dependent. More ex-
amples can be found in Table 1 and 3, and (Huddle-
ston and Pullum, 2002, Chap. 9).
457
No. Statement Interpretation
1 People don?t
::::::
always follow instructions. People sometimes follow instructions.
2 The new group isn?t doing
::::::::
any better
::::
than
:::
the
:::
old
:::
one. The new group is doing equal or worse than the old one.
3 The first two games didn?t finish
::
in
::
the
:::
top
:::
10. The first two games finished below the top 10.
4 They don?t sell
::
to
:::::::
as many
:::::
clients
:::
as
::::::::
Maryland
::::
Club. They sell to less clients than Maryland Club.
5 She said she is not going home
::::
until
:::
The
:::::
Word
:::::
Series
::
is
::::
over. She said she is going home when The Word Series is over.
6 People don?t believe I
::::
want
::
to
::::
give
::::
this
:::::
money
:::::
away. People believe I want to keep this money.
7 I cannot see
:::
how
::::
this
::::
news
::::::
doesn?t
::::::
benefit
:::::
them. I can see how this news benefits them.
8 I don?t believe
:
in
:::
this
:::::::
business
::::
you
:::
can
::
be
:::::
totally
::::::::::
laissez-faire
because of the high degree of public interest.
I believe in this business you can be only partially laissez-
faire because of the high degree of public interest.
Table 1: Examples of negated statements and their interpretation using fine-grained focus (regular underline). Using
coarse-grained focus (wavy underline) would yield a much more generic, less preferred interpretation.
3.1 Granularity of Focus
In this paper, we refer to the focus considered by
Blanco and Moldovan (2011) as coarse-grained and
indicate it with a wavy underline; we refer to the
focus we work with as fine-grained and indicate it
with a regular underline, e.g., We didn?t get
:::
an
:::::
offer
:::
for
::::::::::
more than
::::
$40. Whereas coarse-grained focus is
restricted to include all words belonging to a verb
argument (as per their definition and annotation, fo-
cus is the full text of a semantic role in PropBank),
fine-grained focus is not. This allows us to narrow
down the actual negative meaning and pinpoint more
positive implicit meaning.
Considering fine-grained focus is a substantial
step towards a comprehensive semantic representa-
tion of negation. Following with the example above,
encoding that we got something, but not an offer for
more than $40 (coarse-grained) is useful, but encod-
ing we got an offer for $40 or less (fine-grained) is
preferred. Several examples of coarse versus fine-
grained focus and the benefits of using the latter
over the former are provided in Table 1. In all state-
ments, using coarse-grained focus yields an interpre-
tation with all words underlined with a wavy under-
line negative and the rest positive, e.g., statement (8)
would be interpreted as I believe in something be-
cause of the high degree of public interest, but not
that in this business you can be totally laissez-faire.
Selecting the elements that belong to the fine-
grained focus is a difficult task. In example (1), both
coarse and fine-grained foci are the same and yield
the same interpretation. In the rest of examples and
in the vast majority of negations we annotated (Sec-
tion 4), fine-grained focus comprises fewer words
and yields more specific interpretations.
The coarse-grained focus in statements (1, 2) is
an adverbial phrase. In (1) coarse-grained focus is
a single word and thus fine-grained focus is trivially
that word. In statement (2), fine-grained focus al-
lows us to keep the comparison between the new and
old group in the interpretation.
Examples (3, 4) correspond to statements whose
coarse grained focus is a prepositional phrase. Sim-
ple rules based on part-of-speech tags are not suit-
able here, deep understanding of text is needed. The
fine-grained focus in example (3) is the preposition,
but that is not the case in (4). Fine-grained focus
in these statements allows us to obtain more com-
plete interpretations, namely spell out the location
(metaphorically speaking) were the games ended in
(3) and the quantity sold in (4).
Examples (5?8) correspond to statements whose
coarse-grained focus is a subordinate clause. Note
that a verb is contained in the coarse-grained focus
in these examples. In statement (5), the fine-grained
focus is the first word, a preposition. However, that
is not the case in (8), where the MANNER of the
verb within the subordinate clause (i.e., totally) is
selected as fine-grained focus. In (6), the phrasal
verb give away is the fine-grained focus. Statement
(7) is specially interesting because it contains a dou-
ble negation and fine-grained focus is the negation
mark within the coarse-grained focus.
Note that interpreting statements using coarse-
grained focus is by no means wrong, but it is not
optimal. The interpretation using fine-grained focus
entails the one using coarse-grained focus. For ex-
ample, in (2), The new group is doing equal or worse
than the old one (fine) entails The new group is do-
ing, but not any better than the old one (coarse).
458
Node # Negations % Negations
NP 1,051 39.93
PP 570 21.65
ADVP 415 15.75
SBAR 323 12.30
S 202 7.67
ADJP 33 1.26
Other 38 1.43
Table 2: Syntactic nodes for coarse-grained focus.
4 Annotating Fine-Grained Focus
We have annotated fine-grained focus of negation on
top of the coarse-grained focus annotated by Blanco
and Moldovan (2011). In this paper, we concen-
trate on negations whose coarse-grained focus is a
prepositional phrase (PP), adverbial phrase (ADVP)
or subordinate clause (SBAR). Excluding cases in
which the verb is the coarse-grained focus, these
syntactic nodes correspond to 49.70% of negations
(Table 2). When a verb is the coarse-grained focus,
it is not advantageous to consider fine-grained focus
because both of them are always the same. e.g., We
urge our citizens not to
:::::
wait until it is too late [inter-
pretation: we urge out citizens to act]. An example
of NP being coarse-grained focus is They realized
they didn?t order
::
the
::::::
right
:::::
parts.
We chose PP, ADVP and SBAR over noun
phrases (NP, the most common syntactic realiza-
tion) because they offer a variety of lexical and syn-
tactic realizations, and thus allow us to tackle the
task of fine-grained focus prediction in an assort-
ment of constructions (as opposed to target exclu-
sively NP). As we shall see, ADVP are shorter and
easier, whereas PP and SBAR often contain complex
syntactic (and semantic) structures and are tougher.
Annotation is done at the word level. Each word
belonging to the coarse grained focus is marked if it
also belongs to the fine-grained focus. This allows
us to narrow down the actual negative meaning and
reveal the most positive implicit meaning. In some
cases (32%, Table 4), coarse and fine-grained foci
include the same words (e.g., It doesn?t
:::::::
always hurt
[interpretation: it hurts sometimes]). However, fine-
grained focus usually (68%) comprises fewer words.
Annotators were first trained with examples sim-
ilar to the ones in Table 1. In a first round,
they were asked to select as fine-grained focus the
words within the coarse-grained focus that they be-
lieved were intended to be negated. These instruc-
tions were purposely vague to analyze disagree-
ments and allow us to define detailed guidelines.
Inter-annotator agreement (exact match) was 41%.
This number is low, but the task is challenging and
a mismatch of one token (potentially a noncontent
word (the, a, etc.) or even a punctuation mark
(comma, dash, etc.) is counted as disagreement.
Conflicts were resolved and their causes analyzed.
In a second round, sentences were annotated follow-
ing the improved guidelines (Section 4.1). In both
rounds, annotators were presented with plain text;
they did not have access to any other information.
4.1 Annotation Guidelines
We aim at annotating fine-grained focus in order to
pinpoint the numerous positive concepts within a
negated statement. All concepts but the ones belong-
ing to the fine-grained focus should be interpreted
positive. Our annotation criteria is succinctly sum-
marized by the following principles:
1. We annotate fine-grained focus of negation to
reveal specific positive implicit meaning; we do
not strictly follow any theory of focus.
2. We assume that fine-grained focus is contained
within the coarse-grained focus.
3. Decisions are made taking into account the cur-
rent sentence and context. Context is limited to
the previous and next sentence.
4. World knowledge is taken into account. Thus,
sentences are fully interpreted to identify posi-
tive implicit meaning.
5. In case of ambiguity, we prioritize:
(a) fine-grained focus that yields novel mean-
ing over foci yielding meaning already
stated elsewhere;
(b) narrow over wide fine-grained focus. The
narrower the focus, the more specific the
positive meaning revealed.
(c) the fine-grained focus that reveals the
most obvious positive implicit meaning,
i.e., meaning requiring the least world
knowledge and assumptions to hold.
6. If there are two options for fine-grained focus
yielding semantically equivalent positive im-
plicit meanings, we select the fine-grained fo-
cus occurring earlier within the sentence.
459
No. Example
1 The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic
criteria. But that doesn?t mean
::::::::::
?half of the
::::::::
students
:::::::::
attending?
::::
will
:::
be
::::::::
admitted
::::
this
::::
way.
2 ?[. . . ] and tied it to the stake with a chain,? he says proudly. ?And you can?t cut this chain
::::
with
:::::::::::
bolt cutters?.
3 Although other parties have stated they have no complaints, it is not growing
:::
fast
:::::::
enough
:::
for
::
us.
4 Mr. Katz happily agreed, sliding over the fact that California?s roads and bridges aren?t funded
::
by
::::::::
property
::::
taxes but by state and federal gasoline taxes.
5 [. . . ] in a criminal case , a prosecutor can not comment
::
on
:
a
:::::::::
defendant
::
?s
:::::::
failure
::
to
::::::
testify [. . . ].
6 You think you can go out and turn things around. The reason doesn?t relate
::
to
::::::::::::::::
your selling skills.
7 Respondents don?t think
:::
that
:::
an
::::::::
economic
::::::::::
slowdown
::::::
would
:::::
harm
:::
the
:::::
major
::::::::::
investment
::::::::
markets
::::::::::
very much.
8 The first two games of the World Series between [. . . ] didn?t finish
::
in
:::
the
:::
top
:::
10 [. . . ]
Table 3: Examples of annotation (and relevant context) exemplifying the annotation guidelines.
4.2 Examples of Annotation
In this section, we exemplify our annotation guide-
lines with the statements in Table 3. When example
(1) is interpreted in context [criterion 3], we obtain
at most half of the students will be admitted strictly
by academic criteria. Word knowledge [criterion 4]
allows us to determine that if 40?50% of students
are admitted a certain way, at most half of students
attending will be admitted this way (a student admit-
ted may not enroll). Word knowledge is also used in
example (2): however strong the chain is, one could
cut it with a stronger tool than bolt cutters.
Statement (3) implicitly states that it is growing
fast enough for other parties. Thus, we choose
enough [interpretation: it is growing insufficiently
fast for us] since it reveals novel positive meaning
[criterion 5a]. Another option discarded is us [inter-
pretation: it is growing fast enough for someone, but
not us]. Note that revealing novel positive implicit
meaning is not always possible, e.g., statement (4).
There are several options for statement (5): (5a) a
defendant?s failure to testify [interpretation: a prose-
cutor can comment, but not on a defendant?s failure
to testify]; (5b) a defendant?s [a prosecutor can com-
ment on somebody?s failure to testify, but not the
defendant?s]; and (5c) testify [a prosecutor can com-
ment on the defendant?s failures to do something,
but not to testify]. We prefer (5c) since it reveals the
most specific positive meaning [criterion 5b]. Note
that narrowing down the coarse-grained focus is not
always possible as exemplified in example (6): one
cannot tell if the reason relates to another skill or to
something else (e.g., economy, weather).
In example (7), we choose the fine-grained focus
that reveals the most obvious implicit positive mean-
#FGF %(CGF = FGF) #FGF/#CGF
PP 5.53 1.17% 0.44
ADVP 1.38 89.19% 0.94
SBAR 9.79 14.79% 0.32
All 5.25 32.41% 0.57
Table 4: Numeric analysis: average number of words
in fine-grained focus, percentage of negations in which
coarse and fine-grained focus are the same and average
ratio of words in fine versus coarse-grained focus.
ing [criterion 5c], very much [interpretation: an eco-
nomic slowdown would harm the major investment
markets a little]. Another option is slowdown, yield-
ing the plausible but less felicitous interpretation re-
sponders think that an economic recession/turmoil
(but not a slowdown) would harm the major invest-
ment markets very much. A third option is major
[responders think that an economic slowdown would
harm minor investment markets very much]. The
last two options are plausible but less likely.
Finally, statement (8), there are two semanti-
cally equivalent options: (8a) in [interpretation: the
games finished below the top 10] and (8b) 10 [in-
terpretation: the games finished in the top X, where
X is larger than 10]. We choose the former since it
occurs earlier in the sentence [criterion 6].
4.3 Annotation Analysis
The three syntactic realizations of coarse-grained fo-
cus we aim at narrowing down have significantly dif-
ferent characteristics. Table 4 summarizes some ba-
sic numeric analysis. Intuitively, ADVPs are fairly
easy (they are short and coarse-grained and fine-
grained foci are often the same). On the other hand,
PP and SBAR are longer and only 44% and 32% of
words belonging to the coarse grained focus belong
to the fine-grained focus respectively.
460
Baseline P R F
COARSE
PP 1.96 1.89 1.92
ADVP 92.86 92.86 92.86
SBAR 15.38 13.33 14.29
All 29.52 27.93 28.70
FIRST-WORD
PP 33.33 32.08 32.69
ADVP 92.86 92.86 92.86
SBAR 35.29 20.00 25.53
All 51.04 44.14 47.34
FIRST-JJ
PP 29.82 32.08 30.91
ADVP 92.86 92.86 92.86
SBAR 15.38 13.33 14.29
All 52.34 42.34 42.34
BASIC
PP 54.17 49.06 51.49
ADVP 92.86 92.86 92.86
SBAR 45.00 30.00 36.00
All 63.54 54.95 58.94
Table 5: Precision, recall and f-measure of baselines.
5 Learning Fine-Grained Focus
We follow a standard supervised learning approach.
Each token from each annotated negation becomes
an instance. The decision to be made is whether or
not an instance is part of the fine-grained focus. The
annotated sentences (comprising several instances)
were divided into training (70%), held-out (15%)
and test (15%). The held-out portion was used to
tune the feature set and results are reported for the
test split only, i.e., using unseen instances.
Detecting fine-grained focus is similar to text
chunking. Text chunking consists of dividing text
into syntactically related nonoverlapping groups of
words (Tjong Kim Sang and Buchholz, 2000). On
the other hand, we aim at dividing the words within
a negated statement into belonging or not belong-
ing to the fine-grained focus. Our problem can be
redefined as detecting one type of chunk indicating
the fine grained focus (FGF). We use the standard
BIO notation, in which the first element of a chunk
is prefixed by B- (beginning) and other elements of
the chunk are preceded by I- (inside). The label O
is used to indicate tokens outside any FGF chunk.
Baselines. We have implemented four baselines to
predict fine-grained focus from the elements within
the coarse-grained focus:
? COARSE: select all words.
? FIRST-WORD: select the first word.
? FIRST-JJ: select the first adjective; if none is
found, apply FIRST-WORD.
? BASIC: same as system in Section 5.2 but using
features POS-tag, word and coarse-chunk.
Table 5 shows the performance of these base-
lines. All of them obtain the same performance
for ADVPs, and BASIC yields the best results.
FIRST-WORD successfully predicts fine-grained fo-
cus mostly in cases in which the fine-grained focus
is a preposition positioned at the beginning of the
coarse-grained focus (e.g., Table 3, statement 8).
5.1 Selecting Features
We use a mixture of features proposed for standard
text chunking, semantic role labeling and novel fea-
tures characterizing negation (Table 6). We only
provide more details for the non-obvious ones.
Features 1?5 characterize the current token with
an emphasis on negation. Neg-prefix indicates if
a word is an adjective, starts with a negation prefix
and the reminder of it is a valid adjective. We con-
sider the following negation prefixes: a-, an-, anti-,
dis-, il-, im-, in-, ir-, non- and un- and check whether
the reminder is a valid adjective querying WordNet.
This successfully allows us to detect irrelevant (pre-
fix ir-; relevant is a valid adjective) and disregard ad-
jectives that just happen to start with a negated pre-
fix, e.g., artistic, intelligent. Any-prefix indicates
if a word starts with any (e.g., anytime). Huddle-
ston and Pullum (2002, p.823) refer to these words
as ?any class of items? and include them in the
negatively-oriented polarity-sensitive items (NPIs).
Features signaling other NPIs (until, dare, yet, etc.)
did not bring an improvement on the development
set. Ly-suffix typically signals an adverb indicat-
ing the manner in which something happened.
Features 6?18 describe the coarse-grained focus.
Coarse-path corresponds to four features indicat-
ing paths of length 1?4 from coarse-node to the
token. Including the full path did not yield an im-
provement on the development set. Coarse-head
is calculated following (Collins, 1999).
Finally, features coarse-verb and sem-role
are useful in cases in which the token is not only part
of the semantic role corresponding to the coarse-
grained focus (i.e., a role of verb pred-word), but
also a role of a verb within the coarse-grained focus
(i.e., a role of verb coarse-verb). For example in
Table 3, example (7), for token slowdown we have
word = slowdown, pred-word = think, coarse-role
= A1, coarse-verb = harm and sem-role = A0.
461
No. Feature Values Explanation
1?2 POS-tag and word {NN, VBD, . . .} POS tag and text of current token
3 neg-preffix (PP, SBAR) {yes, no} does word start with a negation preffix?
4 any-preffix (PP, SBAR) {yes, no} does word start with preffix -any?
5 ly-suffix (SBAR) {yes, no} does word end with suffix -ly?
6?7 coarse-{node,parent} {S, PP, . . .} syntactic node of coarse-grained focus and parent
8?9 coarse-{left,right} {NP, VP, . . .} syntactic node of coarse-node left and right siblings
10 coarse-struct {IN=NP, IN=S, . . .} syntactic nodes of of coarse-node daughters
11 coarse-length N lenght of coarse-grained focus
12?15 coarse-path (PP, SBAR) {PP, PP-NP,. . .} paths of length 1?4 from coarse-node to token
16 coarse-role {ARG1, MTMP, . . .} semantic role of coarse-grained focus
17 coarse-head (PP, SBAR) {clock, detail, . . .} head of coarse-grained focus
18 coarse-verb (SBAR) {think, predict, . . .} first verb within coarse-grained focus
19 pred-word {affected, go, . . .} predicate text
20 pred-POS {VB, VBN, . . .} predicate POS tag
21 sem-role (SBAR) {ARG1, MLOC, . . .} semantic role this token belongs to wrt coarse-verb
22 coarse-chunk {B-CFG, I-CFG, O} coarse-grained annotation using BIO
Table 6: Feature set used to predict fine-grained focus of negation. If a feature is especially useful for a particular
syntactic node, we indicate so between parenthesis in the right hand side of column 1 (otherwise it is useful for all).
5.2 Experiments and Results
We have carried our experiments using Yamcha (Ku-
doh and Matsumoto, 2000), a generic, customizable,
and open source text chunker1 implemented using
TinySVM2. Following Yamcha?s design, we distin-
guish between static and dynamic features. Static
features are the ones depicted in Table 6 for a fixed
size window. Dynamic features are the predicted
classes for a fixed set of previous instances. Whereas
values for static features are considered correct, val-
ues for dynamic features are predictions of previous
instances and therefore may contain errors. Varying
window size effectively varies the number of fea-
tures considered, the larger the window the more lo-
cal context is taken into account.
Window sizes are defined using ranges between
instances. The instance to be predicted has index
?0?, the previous one ??1?, the next one ?1?, and so
on. The range [i..j] indicates we take into account
from the ith to the jth instances to predict the cur-
rent instance. Ranges for dynamic features can only
contain instances preceding the current one.
The best performing system was obtained using a
window including the current and two previous in-
stances, and taking into account dynamic features.
This system uses a total of 68 features: 66 static fea-
tures (22?3 = 66, 22 features per instance, window
contains 3 instances) and 2 dynamic features.
1http://chasen.org/ taku/software/yamcha/
2http://chasen.org/ taku/software/TinySVM/
Window Size
P R F
static dynamic
[-1..0]
none 59.20 66.67 62.71
[-1..-1] 68.27 63.96 66.05
[-1..1]
none 66.04 63.06 64.52
[-1..-1] 70.10 61.26 65.38
[0..1]
none 57.85 63.06 60.34
[-1..-1] 63.92 55.86 59.62
[-2..0]
none 60.00 62.16 61.06
[-2..-1] 71.15 66.67 68.84
[-2..2]
none 62.96 61.26 62.10
[-2..-1] 68.42 58.56 63.11
[0..2]
none 60.00 59.46 59.73
[-2..-1] 64.21 54.95 59.22
[-3..0]
none 55.65 62.16 58.72
[-3..-1] 68.93 63.96 66.36
[-3..3]
none 62.62 60.36 61.47
[-3..-1] 67.01 58.56 62.50
[0..3]
none 57.80 56.76 57.27
[-3..-1] 64.13 53.15 58.13
Table 7: Results using different window sizes.
Table 7 provides results on the test split for several
window sizes considering and not considering dy-
namic features. The best performing system obtains
precision 71.15, recall 66.67 (f-measure 68.84). In
general, windows encompassing the i previous in-
stances (e.g., [?2..0]) perform better than windows
encompassing the i next instances (e.g., [0..2]). Win-
dows not considering the i next instances yield bet-
ter performance when using dynamic features (i.e.,
[?i..0] is superior to [?i..i]). Also, including dy-
462
Phrase P R F
PP 64.71 62.26 63.46
ADVP 92.86 92.86 92.86
SBAR 60.00 50.00 54.55
All 71.15 66.67 68.84
Table 8: Detailed results per phrase using the best win-
dow size of features (in bold in Table 7).
namic features is favorable for almost all window
sizes (the only exceptions are [0..1] and [0..2] by a
negligible margin). Larger and discontinuous win-
dows (e.g., [-4..-3, -1..-1]) did not bring an improve-
ment during development and were discarded.
Finally, we report detailed results for the best per-
forming system in Table 8.
6 Limitations and Future Work
The work presented here effectively extracts specific
positive implicit meaning from negated statements.
We depict below some limitations and shortcomings
that could be targeted as future work.
Types of negation. We only targeted verbal,
clausal and analytic negation (Huddleston and Pul-
lum, 2002). Analyzing other types (e.g., synthetic,
non-verbal: I ate nothing, Nobody liked the party) is
needed for a more comprehensive approach.
All positive meanings. Not all implicit positive
meanings are always detected. For example, If the
payment isn?t received
::
by
::::::
today, an eviction notice
will be send out [interpretation: If the payment is
received after today, an eviction notice will be send
out]. Our proposal fails to detect that if no payment
is received, the notice will also be send. Allowing
multiple fine-grained foci seems a valid solution.
Fine-grained within coarse-grained. In a few ex-
amples, interpreting a negated statement using fine-
grained focus requires modifications in other parts of
the sentence as well. For example, That increase in
the money supply would not have happened
:::::::
without
:::
the
::::::::
consent
:::
of
:::
the
::::::::
Federal
:::::::::
Reserve. The interpreta-
tion is That increase would have happened with the
consent of the Federal Reserve. This is not wrong,
but a better option is to remove the modal would in
the positive interpretation: the increase did happen
(with the consent of the Federal Reserve).
Overall Interpretation. A complete semantic rep-
resentation for a statement (not only the verbal
negation within) may require the same concept
with two polarities. Consider [
::::::::::
In the past]TEMPORAL,
[you]AGENT just wore an unknown brand and didn?t
[care]verb. The verbal negation is correctly inter-
preted now you care, but in the past remains as is
(i.e., positive) for the verb wore [interpretation: in
the past you just wore an unknown brand]. Strictly
speaking, this is not a limitation but something to
take into account to obtain a semantic representation
of the whole statement. Our proposal successfully
retrieves positive implicit meaning.
7 Conclusions
In this paper, we have argued that negated statements
often carry positive implicit meaning and that its de-
tection is key in order to capture their semantics, re-
gardless of the semantic representation one favors
(e.g., predicate calculus, semantic relations).
We have introduced the concept of granularity of
focus of negation. Going beyond previous work,
considering fine-grained focus allows us to reveal
narrow positive implicit meaning. In the majority
of cases (68%, Table 4) we are able to detect more
positive implicit meaning than previous work con-
sidering a coarse-grained focus. We do not impose
any syntactic restriction on which parts of a sen-
tence might belong to the fine-grained focus. The
annotation was done selecting words without taking
into account any syntactic or semantic information.
This approach effectively marks only the words that
should be negated, but arguably makes prediction
more difficult since fine-grained focus often does not
correspond to a single node in the syntactic tree.
We have approached the task of fine-grained fo-
cus detection as a chunking problem in which we
predict one chunk, FGF. The best model obtains an
f-measure of 68.84, calculated by considering exact
matches between chunks. In other words, unless the
model predicts as fine-grained focus exactly the ac-
tual fine-grained focus, it is considered wrong when
calculating performance. We believe this is the hon-
est way of evaluating performance, even though par-
tial matches could be useful for an actual applica-
tion. For example, in The U.S.?s largest suppliers
haven?t been filling their quotas
:
to
::::::::
the full
::::::
extent [in-
terpretation: they have been fullfilling their quotas
to a partial extent], if the model predicts full as the
only word belonging to fine-grained focus we count
it wrong even though it successfully detects the most
important part of it, i.e., the adjective full.
463
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional Linguistics, Montreal, Canada.
David Beaver, Brady Z. Clark, Edward Flemming, T. Flo-
rian Jaeger, and Maria Wolters. 2007. When Seman-
tics Meets Phonetics: Acoustical Studies of Second-
Occurrence Focus. Language, 83(2):245?276.
Eduardo Blanco and Dan Moldovan. 2011. Semantic
Representation of Negation Using Focus Detection. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 581?589, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. University of Pennsyl-
vania.
David Dowty. 1994. The Role of Negative Polarity
and Concord Marking in Natural Language Reason-
ing. In Proceedings of Semantics and Linguistics The-
ory (SALT) 4, pages 114?144.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1?12, Uppsala, Sweden,
July. Association for Computational Linguistics.
Michael Glanzberg. 2005. Focus: A Case Study on
the semanticspragmatics Boundary. In Zoltan G. Sz-
abo, editor, Semantics versus Pragmatics, chapter 3.
Oxford University Press, USA, first edition edition,
February.
Eva Hajic?ova?, Petr Sgall, and Hana Skoumalova?. 1995.
An automatic procedure for topic-focus identification.
Comput. Linguist., 21:81?94, March.
Jaakko Hintikka. 2002. Negation in Logic and in Natural
Language. Linguistics and Philosophy, 25(5/6).
Laurence R. Horn. 1989. A Natural History of Negation.
University Of Chicago Press, June.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Fang Kong, GuoDong Zhou, and Qiaoming Zhu. 2009.
Employing the Centering Theory in Pronoun Resolu-
tion from the Semantic Perspective. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 987?996, Singapore,
August. Association for Computational Linguistics.
Taku Kudoh and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In Pro-
ceedings of the 4th conference on Computational nat-
ural language learning (CoNLL-2000), ConLL ?00,
pages 142?144, Stroudsburg, PA, USA. Association
for Computational Linguistics.
William A. Ladusaw. 1996. Negation and polarity items.
In Shalom Lappin, editor, Handbook of Contemporary
Semantic Theory, pages 321?341. Blackwell.
Junhui Li, Guodong Zhou, Hongling Wang, and Qiaom-
ing Zhu. 2010. Learning the Scope of Negation via
Shallow Semantic Parsing. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 671?679, Beijing, China,
August. Coling 2010 Organizing Committee.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning Dependency-Based Compositional Seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590?599, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Sebastian Lo?bner. 2000. Polarity in Natural Language:
Predication, Quantification and Negation in Particular
and Characterizing Sentences. Linguistics and Philos-
ophy, 23(3):213?308?308, June.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the Scope of Hedge Cues in Biomedical Texts. In
Proceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Roser Morante and Caroline Sporleder, editors. 2010.
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing. University of
Antwerp, Uppsala, Sweden, July.
Roser Morante, Sara Schrauwen, and Walter Daelemans.
2011. Annotation of negation cues and their scope.
Guidelines v1.0. Technical report, CLiPS, University
of Antwerp.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised Semantic Parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10, Singapore, August. As-
sociation for Computational Linguistics.
Mats Rooth. 1985. Association with Focus. Ph.D. thesis,
Univeristy of Massachusetts, Amherst.
Mats Rooth. 1992. A Theory of Focus Interpretation.
Natural Language Semantics, 1:75?116.
464
Mats Rooth. 1997. Focus. In Shalom Lappin, edi-
tor, The Handbook of Contemporary Semantic The-
ory, Blackwell Handbooks in Linguistics, chapter 10.
Wiley-Blackwell, December.
Victor Sa?nchez Valencia. 1991. Studies on Natural Logic
and Categorial Grammar. Ph.D. thesis, University of
Amsterdam.
Ivan Titov and Alexandre Klementiev. 2011. A Bayesian
Model for Unsupervised Semantic Parsing. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1445?1455, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the CoNLL-2000 Shared Task: Chunk-
ing. In Claire Cardie, Walter Daelemans, Claire
Nedellec, and Erik Tjong Kim Sang, editors, Proceed-
ings of CoNLL-2000 and LLL-2000, pages 127?132.
Lisbon, Portugal.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60?68,
Uppsala, Sweden, July. University of Antwerp.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: An Explo-
ration of Features for Phrase-Level Sentiment Analy-
sis. Computational Linguistics, 35(3):399?433.
Dekai Wu and Pascale Fung. 2009. Semantic Roles
for SMT: A Hybrid Two-Pass Model. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, Companion
Volume: Short Papers, pages 13?16, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Y. Xu and C. Xu. 2005. Phonetic realization of focus in
English declarative intonation. Journal of Phonetics,
33(2):159?197, April.
H. Zeijlstra. 2007. Negation in Natural Language: On
the Form and Meaning of Negative Elements. Lan-
guage and Linguistics Compass, 1(5):498?518.
465
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 581?589,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semantic Representation of Negation Using Focus Detection
Eduardo Blanco and Dan Moldovan
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080 USA
{eduardo,moldovan}@hlt.utdallas.edu
Abstract
Negation is present in all human languages
and it is used to reverse the polarity of part
of statements that are otherwise affirmative by
default. A negated statement often carries pos-
itive implicit meaning, but to pinpoint the pos-
itive part from the negative part is rather dif-
ficult. This paper aims at thoroughly repre-
senting the semantics of negation by revealing
implicit positive meaning. The proposed rep-
resentation relies on focus of negation detec-
tion. For this, new annotation over PropBank
and a learning algorithm are proposed.
1 Introduction
Understanding the meaning of text is a long term
goal in the natural language processing commu-
nity. Whereas philosophers and linguists have pro-
posed several theories, along with models to rep-
resent the meaning of text, the field of computa-
tional linguistics is still far from doing this automati-
cally. The ambiguity of language, the need to detect
implicit knowledge, and the demand for common-
sense knowledge and reasoning are a few of the dif-
ficulties to overcome. Substantial progress has been
made, though, especially on detection of semantic
relations, ontologies and reasoning methods.
Negation is present in all languages and it is al-
ways the case that statements are affirmative by
default. Negation is marked and it typically sig-
nals something unusual or an exception. It may
be present in all units of language, e.g., words
(incredible), clauses (He doesn?t have friends).
Negation and its correlates (truth values, lying,
irony, false or contradictory statements) are exclu-
sive characteristics of humans (Horn, 1989; Horn
and Kato, 2000).
Negation is fairly well-understood in grammars;
the valid ways to express a negation are documented.
However, there has not been extensive research on
detecting it, and more importantly, on representing
the semantics of negation. Negation has been largely
ignored within the area of semantic relations.
At first glance, one would think that interpreting
negation could be reduced to finding negative key-
words, detect their scope using syntactic analysis
and reverse its polarity. Actually, it is more com-
plex. Negation plays a remarkable role in text un-
derstanding and it poses considerable challenges.
Detecting the scope of negation in itself is chal-
lenging: All vegetarians do not eat meat means that
vegetarians do not eat meat and yet All that glitters
is not gold means that it is not the case that all that
glitters is gold (so out of all things that glitter, some
are gold and some are not). In the former example,
the universal quantifier all has scope over the nega-
tion; in the latter, the negation has scope over all.
In logic, two negatives always cancel each other
out. On the other hand, in language this is only theo-
retically the case: she is not unhappy does not mean
that she is happy; it means that she is not fully un-
happy, but she is not happy either.
Some negated statements carry a positive implicit
meaning. For example, cows do not eat meat implies
that cows eat something other than meat. Otherwise,
the speaker would have stated cows do not eat. A
clearer example is the correct and yet puzzling state-
ment tables do not eat meat. This sentence sounds
581
unnatural because of the underlying positive state-
ment (i.e., tables eat something other than meat).
Negation can express less than or in between
when used in a scalar context. For example, John
does not have three children probably means that he
has either one or two children. Contrasts may use
negation to disagree about a statement and not to
negate it, e.g., That place is not big, it is massive
defines the place as massive, and therefore, big.
2 Related Work
Negation has been widely studied outside of com-
putational linguistics. In logic, negation is usu-
ally the simplest unary operator and it reverses the
truth value. The seminal work by Horn (1989)
presents the main thoughts in philosophy and psy-
chology. Linguists have found negation a complex
phenomenon; Huddleston and Pullum (2002) ded-
icate over 60 pages to it. Negation interacts with
quantifiers and anaphora (Hintikka, 2002), and in-
fluences reasoning (Dowty, 1994; Sa?nchez Valencia,
1991). Zeijlstra (2007) analyzes the position and
form of negative elements and negative concords.
Rooth (1985) presented a theory of focus in his
dissertation and posterior publications (e.g., Rooth
(1992)). In this paper, we follow the insights on
scope and focus of negation by Huddleston and Pul-
lum (2002) rather than Rooth?s (1985).
Within natural language processing, negation
has drawn attention mainly in sentiment analysis
(Wilson et al, 2009; Wiegand et al, 2010) and
the biomedical domain. Recently, the Negation
and Speculation in NLP Workshop (Morante and
Sporleder, 2010) and the CoNLL-2010 Shared Task
(Farkas et al, 2010) targeted negation mostly on
those subfields. Morante and Daelemans (2009) and
?Ozgu?r and Radev (2009) propose scope detectors
using the BioScope corpus. Councill et al (2010)
present a supervised scope detector using their own
annotation. Some NLP applications deal indirectly
with negation, e.g., machine translation (van Mun-
ster, 1988), text classification (Rose et al, 2003) and
recognizing entailments (Bos and Markert, 2005).
Regarding corpora, the BioScope corpus anno-
tates negation marks and linguistic scopes exclu-
sively on biomedical texts. It does not annotate fo-
cus and it purposely ignores negations such as (talk-
ing about the reaction of certain elements) in NK3.3
cells is not always identical (Vincze et al, 2008),
which carry the kind of positive meaning this work
aims at extracting (in NK3.3 cells is often identi-
cal). PropBank (Palmer et al, 2005) only indicates
the verb to which a negation mark attaches; it does
not provide any information about the scope or fo-
cus. FrameNet (Baker et al, 1998) does not con-
sider negation and FactBank (Saur?? and Pustejovsky,
2009) only annotates degrees of factuality for events.
None of the above references aim at detecting or
annotating the focus of negation in natural language.
Neither do they aim at carefully representing the
meaning of negated statements nor extracting im-
plicit positive meaning from them.
3 Negation in Natural Language
Simply put, negation is a process that turns a state-
ment into its opposite. Unlike affirmative state-
ments, negation is marked by words (e.g., not, no,
never) or affixes (e.g., -n?t, un-). Negation can inter-
act with other words in special ways. For example,
negated clauses use different connective adjuncts
that positive clauses do: neither, nor instead of ei-
ther, or. The so-called negatively-oriented polarity-
sensitive items (Huddleston and Pullum, 2002) in-
clude, among many others, words starting with any-
(anybody, anyone, anywhere, etc.), the modal aux-
iliaries dare and need and the grammatical units at
all, much and till. Negation in verbs usually requires
an auxiliary; if none is present, the auxiliary do is in-
serted (I read the paper vs. I didn?t read the paper).
3.1 Meaning of Negated Statements
State-of-the-art semantic role labelers (e.g., the ones
trained over PropBank) do not completely repre-
sent the meaning of negated statements. Given
John didn?t build a house to impress Mary, they en-
code AGENT(John, build ), THEME(a house, build ),
PURPOSE(to impress Mary, build ), NEGATION(n?t,
build ). This representation corresponds to the inter-
pretation it is not the case that John built a house
to impress Mary, ignoring that it is implicitly stated
that John did build a house.
Several examples are shown Table 1. For all state-
ments s, current role labelers would only encode it
is not the case that s. However, examples (1?7)
582
Statement Interpretation
1 John didn?t build a house
:
to
:::::::
impress
::::
Mary. John built a house for other purpose.
2 I don?t have a watch
:::
with
:::
me. I have a watch, but it is not with me.
3 We don?t have an evacuation plan
:::
for
:::::::
flooding. We have an evacuation plan for something else (e.g., fire).
4 They didn?t release the UFO files
::::
until
::::
2008. They released the UFO files in 2008.
5 John doesn?t know
:::::
exactly how they met. John knows how they met, but not exactly.
6 His new job doesn?t require
:::::
driving. His new job has requirements, but it does not require driving.
7 His new job doesn?t require driving
::
yet. His new job requires driving in the future.
8 His new job doesn?t
::::::
require anything. His new job has no requirements.
9 A panic on Wall Street doesn?t exactly
:::::
inspire confidence. A panic on Wall Streen discourages confidence.
Table 1: Examples of negated statements and their interpretations considering underlying positive meaning. A wavy
underline indicates the focus of negation (Section 3.3); examples (8, 9) do not carry any positive meaning.
carry positive meaning underneath the direct mean-
ing. Regarding (4), encoding that the UFO files
were released in 2008 is crucial to fully interpret
the statement. (6?8) show that different verb argu-
ments modify the interpretation and even signal the
existence of positive meaning. Examples (5, 9) fur-
ther illustrate the difficulty of the task; they are very
similar (both have AGENT, THEME and MANNER)
and their interpretation is altogether different. Note
that (8, 9) do not carry any positive meaning; even
though their interpretations do not contain a verbal
negation, the meaning remains negative. Some ex-
amples could be interpreted differently depending
on the context (Section 4.2.1).
This paper aims at thoroughly representing the se-
mantics of negation by revealing implicit positive
meaning. The main contributions are: (1) interpre-
tation of negation using focus detection; (2) focus of
negation annotation over all PropBank negated sen-
tences1; (3) feature set to detect the focus of nega-
tion; and (4) model to semantically represent nega-
tion and reveal its underlying positive meaning.
3.2 Negation Types
Huddleston and Pullum (2002) distinguish four con-
trasts for negation:
? Verbal if the marker of negation is grammati-
cally associated with the verb (I did not see any-
thing at all); non-verbal if it is associated with a
dependent of the verb (I saw nothing at all).
? Analytic if the sole function of the negated
mark is to mark negation (Bill did not go);
synthetic if it has some other function as well
([Nobody]AGENT went to the meeting).
1Annotation will be available on the author?s website
? Clausal if the negation yields a negative clause
(She didn?t have a large income); subclausal oth-
erwise (She had a not inconsiderable income).
? Ordinary if it indicates that something is not the
case, e.g., (1) She didn?t have lunch with my
old man: he couldn?t make it; metalinguistic if
it does not dispute the truth but rather reformu-
lates a statement, e.g., (2) She didn?t have lunch
with your ?old man?: she had lunch with your fa-
ther. Note that in (1) the lunch never took place,
whereas in (2) a lunch did take place.
In this paper, we focus on verbal, analytic, clausal,
and both metalinguistic and ordinary negation.
3.3 Scope and Focus
Negation has both scope and focus and they are ex-
tremely important to capture its semantics. Scope is
the part of the meaning that is negated. Focus is that
part of the scope that is most prominently or explic-
itly negated (Huddleston and Pullum, 2002).
Both concepts are tightly connected. Scope corre-
sponds to all elements any of whose individual fal-
sity would make the negated statement true. Focus
is the element of the scope that is intended to be in-
terpreted as false to make the overall negative true.
Consider (1) Cows don?t eat meat and its positive
counterpart (2) Cows eat meat. The truth conditions
of (2) are: (a) somebody eats something; (b) cows
are the ones who eat; and (c) meat is what is eaten.
In order for (2) to be true, (a?c) have to be true.
And the falsity of any of them is sufficient to make
(1) true. In other words, (1) would be true if nobody
eats, cows don?t eat or meat is not eaten. Therefore,
all three statements (a?c) are inside the scope of (1).
The focus is more difficult to identify, especially
583
1 AGENT(the cow, didn?t eat) THEME(grass, didn?t eat) INSTRUMENT(with a fork, didn?t eat)
2 NOT[AGENT(the cow, ate) THEME(grass, ate) INSTRUMENT(with a fork, ate)]
3 NOT[AGENT(the cow, ate)] THEME(grass, ate) INSTRUMENT(with a fork, ate)
4 AGENT(the cow, ate) NOT[THEME(grass, ate)] INSTRUMENT(with a fork, ate)
5 AGENT(the cow, ate) THEME(grass, ate) NOT[INSTRUMENT(with a fork, ate)]
Table 2: Possible semantic representations for The cow didn?t eat grass with a fork.
without knowing stress or intonation. Text under-
standing is needed and context plays an important
role. The most probable focus for (1) is meat, which
corresponds to the interpretation cows eat something
else than meat. Another possible focus is cows,
which yields someone eats meat, but not cows.
Both scope and focus are primarily semantic,
highly ambiguous and context-dependent. More ex-
amples can be found in Tables 1 and 3 and (Huddle-
ston and Pullum, 2002, Chap. 9).
4 Approach to Semantic Representation of
Negation
Negation does not stand on its own. To be useful, it
should be added as part of another existing knowl-
edge representation. In this Section, we outline how
to incorporate negation into semantic relations.
4.1 Semantic Relations
Semantic relations capture connections between
concepts and label them according to their nature.
It is out of the scope of this paper to define them
in depth, establish a set to consider or discuss their
detection. Instead, we use generic semantic roles.
Given s: The cow didn?t eat grass with a fork,
typical semantic roles encode AGENT(the cow, eat),
THEME(grass, eat), INSTRUMENT(with a fork, eat)
and NEGATION(n?t, eat). This representation only
differs on the last relation from the positive counter-
part. Its interpretation is it is not the case that s.
Several options arise to thoroughly represent s.
First, we find it useful to consider the seman-
tic representation of the affirmative counterpart:
AGENT(the cow, ate), THEME(grass, ate), and IN-
STRUMENT(with a fork, ate). Second, we believe
detecting the focus of negation is useful. Even
though it is open to discussion, the focus corre-
sponds to INSTRUMENT(with a fork, ate) Thus, the
negated statement should be interpreted as the cow
ate grass, but it did not do so using a fork.
Table 2 depicts five different possible semantic
representations. Option (1) does not incorporate any
explicit representation of negation. It attaches the
negated mark and auxiliary to eat; the negation is
part of the relation arguments. This option fails
to detect any underlying positive meaning and cor-
responds to the interpretation the cow did not eat,
grass was not eaten and a fork was not used to eat.
Options (2?5) embody negation into the represen-
tation with the pseudo-relation NOT. NOT takes as its
argument an instantiated relation or set of relations
and indicates that they do not hold.
Option (2) includes all the scope as the argument
of NOT and corresponds to the interpretation it is not
the case that the cow ate grass with a fork. Like typi-
cal semantic roles, option (2) does not reveal the im-
plicit positive meaning carried by statement s. Op-
tions (3?5) encode different interpretations:
? (3) negates the AGENT; it corresponds to the cow
didn?t eat, but grass was eaten with a fork.
? (4) applies NOT to the THEME; it corresponds to
the cow ate something with a fork, but not grass.
? (5) denies the INSTRUMENT, encoding the mean-
ing the cow ate grass, but it did not use a fork.
Option (5) is preferred since it captures the best
implicit positive meaning. It corresponds to the se-
mantic representation of the affirmative counterpart
after applying the pseudo-relation NOT over the fo-
cus of the negation. This fact justifies and motivates
the detection of the focus of negation.
4.2 Annotating the Focus of Negation
Due to the lack of corpora containing annotation for
focus of negation, new annotation is needed. An ob-
vious option is to add it to any text collection. How-
ever, building on top of publicly available resources
is a better approach: they are known by the commu-
nity, they contain useful information for detecting
the focus of negation and tools have already been
developed to predict their annotation.
584
Statement V A0 A1 A2 A4 TM
P
M
N
R
A
D
V
L
O
C
P
N
C
E
X
T
D
IS
M
O
D
1 Even if [that deal]A1 isn?t [::::::revived]V, NBC hopes to find another.
? Even if that deal is suppressed, NBC hopes to find another one. ? - + - - - - - - - - - -
2 [He]A0 [simply]MDIS [ca]MMODn?t [stomach]V [:::the::::taste:::of:::::Heinz]A1 , she says.
? He simply can stomach any ketchup but Heinz?s. + + ? - - - - - - - - + +
3 [A decision]A1 isn?t [expected]V [ ::::until:::::some::::time:::::next ::::year]MTMP .
? A decision is expected at some time next year. + - + - - ? - - - - - - -
4 [. . . ] it told the SEC [it]A0 [could]MMODn?t [provide]V [financial statements]A1 [by the end of its first
extension]MTMP ?[:::::::without::::::::::::unreasonable:::::::burden ::or::::::::expense]MMNR?.
? It could provide them by that time with a huge overhead. + + + - - + ? - - - - - +
5 [For example]MDIS, [P&G]A0 [up until now]MTMP hasn?t [sold]V [coffee]A1 [::to:::::::airlines]A2 and does only limited
business with hotels and large restaurant chains.
? Up until now, P&G has sold coffee, but not to airlines. + + + ? - + - - - - - + -
6 [Decent life . . . ]A1 [wo]MMODn?t be [restored]V [:::::unless:::the:::::::::::government::::::::reclaims:::the::::::streets:::::from:::the::::::gangs]MADV .
? It will be restored if the government reclaims the streets from the gangs. + - + - - - - ? - - - - +
7 But [
::::
quite
::
a
:::
few
:::::::
money
:::::::::
managers]A0 aren?t [buying]V [it]A1 .
? Very little managers are buying it. + ? + - - - - - - - - - -
8 [When]MTMP [she]A0 isn?t [performing]V [ ::for:::an::::::::audience]MPNC , she prepares for a song by removing the wad of
gum from her mouth, and indicates that she?s finished by sticking the gum back in.
? She prepares in that way when she is performing, but not for an audience. + + - - - + - - - ? - - -
9 [The company?s net worth]A1 [can]MMODnot [fall]V [::::::below:::::$185 ::::::million]A4 [after the dividends are issued]MTMP .
? It can fall after the dividends are issued, but not below $185 million. + - + - ? + - - - - - - +
10 Mario Gabelli, an expert at spotting takeover candidates, says that [takeovers]A1 aren?t [::::::totally]MEXT [gone]V.
? Mario Gabelli says that takeovers are partially gone. + - + - - - - - - - ? - -
Table 3: Negated statements from PropBank and their interpretation considering underlying positive meaning. Focus
is underlined; ?+? indicates that the role is present, ?-? that it is not and ??? that it corresponds to the focus of negation.
We decided to work over PropBank. Unlike other
resources (e.g., FrameNet), gold syntactic trees are
available. Compared to the BioScope corpus, Prop-
Bank provides semantic annotation and is not lim-
ited to the biomedical domain. On top of that, there
has been active research on predicting PropBank
roles for years. The additional annotation can be
readily used by any system trained with PropBank,
quickly incorporating interpretation of negation.
4.2.1 Annotation Guidelines
The focus of a negation involving verb v is resolved
as:
? If it cannot be inferred that an action v oc-
curred, focus is role MNEG.
? Otherwise, focus is the role that is most promi-
nently negated.
All decisions are made considering as context the
previous and next sentence. The mark -NOT is used
to indicate the focus. Consider the following state-
ment (file wsj 2282, sentence 16).
[While profitable]MADV1,2 , [it]A11 ,A02 ?was[n?t]MNEG1
[growing]v1 and was[n?t]MNEG2 [providing]v2 [a sat-
isfactory return on invested capital]A12 ,? he says.
The previous sentence is Applied, then a closely
held company, was stagnating under the manage-
ment of its controlling family. Regarding the first
verb (growing), one cannot infer that anything was
growing, so focus is MNEG. For the second verb
(providing), it is implicitly stated that the company
was providing a not satisfactory return on invest-
ment, therefore, focus is A1.
The guidelines assume that the focus corresponds
to a single role or the verb. In cases where more than
one role could be selected, the most likely focus is
chosen; context and text understanding are key. We
define the most likely focus as the one that yields the
most meaningful implicit information.
For example, in (Table 3, example 2) [He]A0
could be chosen as focus, yielding someone can
stomach the taste of Heinz, but not him. However,
given the previous sentence ([. . . ] her husband is
585
While profitable
MADV
55
MADV
**
it
A1
55
A0
**
was n?t
MNEG-NOT!!
growing and was n?t
MNEG
<<providing a satisfacory return . . .
A1-NOTuu
Figure 1: Example of focus annotation (marked with -NOT). Its interpretation is explained in Section 4.2.2.
adamant about eating only Hunt?s ketchup), it is
clear that the best option is A1. Example (5) has a
similar ambiguity between A0 and A2, example (9)
between MTMP and A4, etc. The role that yields the
most useful positive implicit information given the
context is always chosen as focus.
Table 3 provides several examples having as their
focus different roles. Example (1) does not carry
any positive meaning, the focus is V. In (2?10) the
verb must be interpreted as affirmative, as well as
all roles except the one marked with ??? (i.e., the
focus). For each example, we provide PropBank an-
notation (top), the new annotation (i.e., the focus,
bottom right) and its interpretation (bottom left).
4.2.2 Interpretation of -NOT
The mark -NOT is interpreted as follows:
? If MNEG-NOT(x, y), then verb y must be
negated; the statement does not carry positive
meaning.
? If any other role is marked with -NOT, ROLE-
NOT(x, y) must be interpreted as it is not the
case that x is ROLE of y.
Unmarked roles are interpreted positive; they cor-
respond to implicit positive meaning. Role labels
(A0, MTMP, etc.) maintain the same meaning from
PropBank (Palmer et al, 2005). MNEG can be ig-
nored since it is overwritten by -NOT.
The new annotation for the example (Figure 1)
must be interpreted as: While profitable, it (the com-
pany) was not growing and was providing a not sat-
isfactory return on investment. Paraphrasing, While
profitable, it was shrinking or idle and was providing
an unsatisfactory return on investment. We discover
an entailment and an implicature respectively.
4.3 Annotation Process
We annotated the 3,993 verbal negations signaled
with MNEG in PropBank. Before annotation began,
all semantic information was removed by mapping
all role labels to ARG. This step is necessary to en-
sure that focus selection is not biased by the seman-
Role #Inst. Focus
# ? %
A1 2,930 1,194 ? 40.75
MNEG 3,196 1,109 ? 34.70
MTMP 609 246 ? 40.39
MMNR 250 190 ? 76.00
A2 501 179 ? 35.73
MADV 466 94 ? 20.17
A0 2,163 73 ? 3.37
MLOC 114 22 ? 19.30
MEXT 25 22 ? 88.00
A4 26 22 ? 84.62
A3 48 18 ? 37.50
MDIR 35 13 ? 37.14
MPNC 87 9 ? 10.34
MDIS 287 6 ? 2.09
Table 4: Roles, total instantiations and counts corre-
sponding to focus over training and held-out instances.
tic labels provided by PropBank.
As annotation tool, we use Jubilee (Choi et al,
2010). For each instance, annotators decide the fo-
cus given the full syntactic tree, as well as the previ-
ous and next sentence. A post-processing step incor-
porates focus annotation to the original PropBank by
adding -NOT to the corresponding role.
In a first round, 50% of instances were annotated
twice. Inter-annotator agreement was 0.72. After
careful examination of the disagreements, they were
resolved and annotators were given clearer instruc-
tions. The main point of conflict was selecting a fo-
cus that yields valid implicit meaning, but not the
most valuable (Section 4.2.1). Due to space con-
straints, we cannot elaborate more on this issue. The
remaining instances were annotated once. Table 4
depicts counts for each role.
5 Learning Algorithm
We propose a supervised learning approach. Each
sentence from PropBank containing a verbal nega-
tion becomes an instance. The decision to be made
is to choose the role that corresponds to the focus.
586
No. Feature Values Explanation
1 role-present {y, n} is role present?
2 role-f-pos {DT, NNP, . . .} First POS tag of role
3 role-f-word {This, to, overseas, . . . } First word of role
4 role-length N number fo words in role
5 role-posit N position within the set of roles
6 A1-top {NP, SBAR, PP, . . .} syntactic node of A1
7 A1-postag {y, n} does A1 contain the tag postag?
8 A1-keyword {y, n} does A1 cotain the word keyword?
9 first-role {A1, MLOC, . . .} label of the first role
10 last-role {A1, MLOC, . . .} label of the last role
11 verb-word {appear, describe, . . . } main verb
12 verb-postag {VBN, VBZ, . . .} POS tag main verb
13 VP-words {were-n?t, be-quickly, . . . } sequence of words of VP until verb
14 VP-postags {VBP-RB-RB-VBG, VBN-VBG, . . .} sequence of POS tags of VP until verb
15 VP-has-CC {y, n} does the VP contain a CC?
16 VP-has-RB {y, n} does the VP contain a RB?
17 predicate {rule-out, come-up, . . . } predicate
18 them-role-A0 {preparer, assigner, . . . } thematic role for A0
19 them-role-A1 {effort, container, . . . } thematic role for A1
20 them-role-A2 {audience, loaner, . . . } thematic role for A2
21 them-role-A3 {intensifier, collateral, . . . } thematic role for A3
22 them-role-A4 {beneficiary, end point, . . . } thematic role for A4
Table 5: Full set of features. Features (1?5) are extracted for all roles, (7, 8) for all POS tags and keywords detected.
The 3,993 annotated instances are divided into
training (70%), held-out (10%) and test (20%). The
held-out portion is used to tune the feature set and
results are reported for the test split only, i.e., us-
ing unseen instances. Because PropBank adds se-
mantic role annotation on top of the Penn TreeBank,
we have available syntactic annotation and semantic
role labels for all instances.
5.1 Baselines
We implemented four baselines to measure the diffi-
culty of the task:
? A1: select A1, if not present then MNEG.
? FIRST: select first role.
? LAST: select last role.
? BASIC: same than FOC-DET but only using fea-
tures last role and flags indicating the presence
of roles.
5.2 Selecting Features
The BASIC baseline obtains a respectable accuracy
of 61.38 (Table 6). Most errors correspond to in-
stances having as focus the two most likely foci: A1
and MNEG (Table 4). We improve BASIC with an
extended feature set which targets especially A1 and
the verb (Table 5).
Features (1?5) are extracted for each role and
capture their presence, first POS tag and word,
length and position within the roles present for
that instance. Features (6?8) further characterize
A1. A1-postag is extracted for the following
POS tags: DT, JJ, PRP, CD, RB, VB and WP;
A1-keyword for the following words: any, any-
body, anymore, anyone, anything, anytime, any-
where, certain, enough, full, many, much, other,
some, specifics, too and until. These lists of POS
tags and keywords were extracted after manual ex-
amination of training examples and aim at signaling
whether this role correspond to the focus. Examples
of A1 corresponding to the focus and including one
of the POS tags or keywords are:
? [Apparently]MADV , [the respondents]A0 do n?t
think [
::::
that
:::
an
::::::::::
economic
::::::::::
slowdown
::::::
would
::::::
harm
:::
the
::::::
major
:::::::::::
investment
::::::::
markets
:::::::
veryRB
::::::
much]A1.
(i.e., the responders think it would harm the in-
vestements little).
587
? [The oil company]A0 does n?t anticipate
[
::::::::::
anykeyword
::::::::::::
additional
::::::::
charges]A1 (i.e., the
company anticipates no additional charges).
? [Money managers and other bond buyers]A0
haven?t [shown]V [ ::::::::::::muchkeyword::::::::interest ::in::::the
::::::::
Refcorp
::::::
bonds]A1 (i.e., they have shown little
interest in the bonds).
? He concedes H&R Block is well-entrenched
and a great company, but says ?[it]A1 doesn?t
[grow]V [::::fast::::::::::::::enoughkeyword::::for::us]A1? (i.e., it
is growing too slow for us).
? [We]A0 don?t [see]V [:a::::::::::domestic :::::::source::::for
::::::::::::
somekeyword
:::
of
::::
our
::::::::
HDTV
:::::::::::::
requirements ]A1,
and that?s a source of concern [. . . ] (i.e., we see
a domestic source for some other of our HDTV
requirements)
Features (11?16) correspond to the main verb.
VP-words (VP-postag) captures the full se-
quence of words (POS tags) from the beginning of
the VP until the main verb. Features (15?16) check
for POS tags as the presence of certain tags usually
signal that the verb is not the focus of negation (e.g.,
[Thus]MDIS, he asserts, [Lloyd?s]A0 [[ca]MMODn?t
[react]v [::::::::::quicklyRB ]MMNR [to competition]A1]VP).
Features (17?22) tackle the predicate, which in-
cludes the main verb and may include other words
(typically prepositions). We consider the words in
the predicate, as well as the specific thematic roles
for each numbered argument. This is useful since
PropBank uses different numbered arguments for
the same thematic role depending on the frame (e.g.,
A3 is used as PURPOSE in authorize.01 and as IN-
STRUMENT in avert.01).
6 Experiments and Results
As a learning algorithm, we use bagging with C4.5
decision trees. This combination is fast to train and
test, and typically provides good performance. More
features than the ones depicted were tried, but we
only report the final set. For example, the parent
node for all roles was considered and discarded. We
name the model considering all features and trained
using bagging with C4.5 trees FOC-DET.
Results over the test split are depicted in Table 6.
Simply choosing A1 as the focus yields an accuracy
of 42.11. A better baseline is to always pick the last
role (58.39 accuracy). Feeding the learning algo-
System Accuracy
A1 42.11
FIRST 7.00
LAST 58.39
BASIC 61.38
FOC-DET 65.50
Table 6: Accuracies over test split.
rithm exclusively the label corresponding to the last
role and flags indicating the presence of roles yields
61.38 accuracy (BASIC baseline).
Having an agreement of 0.72, there is still room
for improvement. The full set of features yields
65.50 accuracy. The difference in accuracy between
BASIC and FOC-DET (4.12) is statistically significant
(Z-value = 1.71). We test the significance of the dif-
ference in performance between two systems i and j
on a set of ins instances with the Z-score test, where
z = abs(erri,errj)?d , errk is the error made using set k
and ?d =
?
erri(1?erri)
ins +
errj(1?errj)
ins .
7 Conclusions
In this paper, we present a novel way to semantically
represent negation using focus detection. Implicit
positive meaning is identified, giving a thorough in-
terpretation of negated statements.
Due to the lack of corpora annotating the focus of
negation, we have added this information to all the
negations marked with MNEG in PropBank. A set
of features is depicted and a supervised model pro-
posed. The task is highly ambiguous and semantic
features have proven helpful.
A verbal negation is interpreted by considering all
roles positive except the one corresponding to the
focus. This has proven useful as shown in several
examples. In some cases, though, it is not easy to
obtain the meaning of a negated role.
Consider (Table 3, example 5) P&G hasn?t sold
coffee
::
to
::::::::
airlines. The proposed representation en-
codes P&G has sold coffee, but not to airlines. How-
ever, it is not said that the buyers are likely to have
been other kinds of companies. Even without fully
identifying the buyer, we believe it is of utmost im-
portance to detect that P&G has sold coffee. Empir-
ical data (Table 4) shows that over 65% of negations
in PropBank carry implicit positive meaning.
588
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional Linguistics, Montreal, Canada.
Johan Bos and Katja Markert. 2005. Recognising Tex-
tual Entailment with Logical Inference. In Proceed-
ings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 628?635, Vancouver, British
Columbia, Canada.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010.
Propbank Instance Annotation Guidelines Using a
Dedicated Editor, Jubilee. In Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta.
Isaac Councill, Ryan McDonald, and Leonid Velikovich.
2010. What?s great and what?s not: learning to clas-
sify the scope of negation for improved sentiment anal-
ysis. In Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing, pages
51?59, Uppsala, Sweden.
David Dowty. 1994. The Role of Negative Polarity
and Concord Marking in Natural Language Reason-
ing. In Proceedings of Semantics and Linguistics The-
ory (SALT) 4, pages 114?144.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1?12, Uppsala, Sweden.
Jaakko Hintikka. 2002. Negation in Logic and in Natural
Language. Linguistics and Philosophy, 25(5/6).
Laurence R. Horn and Yasuhiko Kato, editors. 2000.
Negation and Polarity - Syntactic and Semantic Per-
spectives (Oxford Linguistics). Oxford University
Press, USA.
Laurence R. Horn. 1989. A Natural History of Negation.
University Of Chicago Press.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press.
Roser Morante and Walter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 28?36,
Boulder, Colorado.
Roser Morante and Caroline Sporleder, editors. 2010.
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing. University of
Antwerp, Uppsala, Sweden.
Arzucan ?Ozgu?r and Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific Text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398?1407, Singapore.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Mats Rooth. 1985. Association with Focus. Ph.D. thesis,
Univeristy of Massachusetts, Amherst.
Mats Rooth. 1992. A Theory of Focus Interpretation.
Natural Language Semantics, 1:75?116.
Carolyn P. Rose, Antonio Roque, Dumisizwe Bhembe,
and Kurt Vanlehn. 2003. A Hybrid Text Classification
Approach for Analysis of Student Essays. In In Build-
ing Educational Applications Using Natural Language
Processing, pages 68?75.
Victor Sa?nchez Valencia. 1991. Studies on Natural Logic
and Categorial Grammar. Ph.D. thesis, University of
Amsterdam.
Roser Saur?? and James Pustejovsky. 2009. FactBank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Elly van Munster. 1988. The treatment of Scope and
Negation in Rosetta. In Proceedings of the 12th In-
ternational Conference on Computational Linguistics,
Budapest, Hungary.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60?68,
Uppsala, Sweden, July.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: An Explo-
ration of Features for Phrase-Level Sentiment Analy-
sis. Computational Linguistics, 35(3):399?433.
H. Zeijlstra. 2007. Negation in Natural Language: On
the Form and Meaning of Negative Elements. Lan-
guage and Linguistics Compass, 1(5):498?518.
589
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1456?1465,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Learning of Semantic Relation Composition
Eduardo Blanco and Dan Moldovan
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080 USA
{eduardo,moldovan}@hlt.utdallas.edu
Abstract
This paper presents an unsupervised method
for deriving inference axioms by composing
semantic relations. The method is indepen-
dent of any particular relation inventory. It
relies on describing semantic relations using
primitives and manipulating these primitives
according to an algebra. The method was
tested using a set of eight semantic relations
yielding 78 inference axioms which were eval-
uated over PropBank.
1 Introduction
Capturing the meaning of text is a long term goal
within the NLP community. Whereas during the last
decade the field has seen syntactic parsers mature
and achieve high performance, the progress in se-
mantics has been more modest. Previous research
has mostly focused on relations between particular
kind of arguments, e.g., semantic roles, noun com-
pounds. Notwithstanding their significance, they
target a fairly narrow text semantics compared to the
broad semantics encoded in text.
Consider the sentence in Figure 1. Semantic role
labelers exclusively detect the relations indicated
with solid arrows, which correspond to the sentence
syntactic dependencies. On top of those roles, there
are at least three more relations (discontinuous ar-
rows) that encode semantics other than the verb-
argument relations.
In this paper, we venture beyond semantic rela-
tion extraction from text and investigate techniques
to compose them. We explore the idea of inferring
S
NP VP
A man . . .
AGT
V PP NP VP
came
AGT before the . . .
LOC
LOC
yesterday
TMP TMP
to talk . . .
PRP
Figure 1: Semantic representation of A man from the
Bush administration came before the House Agricultural
Committee yesterday to talk about . . . (wsj 0134, 0).
a new relation linking the ends of a chain of rela-
tions. This scheme, informally used previously for
combining HYPERNYM with other relations, has not
been studied for arbitrary pairs of relations.
For example, it seems adequate to state the fol-
lowing: if x is PART-OF y and y is HYPERNYM of z,
then x is PART-OF z. An inference using this rule can
be obtained instantiating x, y and z with engine, car
and convertible. Going a step further, we consider
nonobvious inferences involving AGENT, PURPOSE
and other semantic relations.
The novelties of this paper are twofold. First,
an extended definition for semantic relations is pro-
posed, including (1) semantic restrictions for their
domains and ranges, and (2) semantic primitives.
Second, an algorithm for obtaining inference ax-
ioms is described. Axioms take as their premises
chains of two relations and output a new relation
linking the ends of the chain. This adds an extra
layer of semantics on top of previously extracted re-
1456
Primitive Description Inv. Ref.
1: Composable Relation can be meaningfully composed with other relations due to their fun-
damental characteristics
id. [3]
2: Functional x is in a specific spatial or temporal position with respect to y in order for the
connection to exist
id. [1]
3: Homeomerous x must be the same kind of thing as y id. [1]
4: Separable x can be temporally or spatially separated from y; they can exist independently id. [1]
5: Temporal x temporally precedes y op. [2]
6: Connected x is physically or temporally connected to y; connection might be indirect. id. [3]
7: Intrinsic Relation is an attribute of the essence/stufflike nature of x and y id. [3]
8: Volitional Relation requires volition between the arguments id. -
9: Universal Relation is always true between x and y id. -
10: Fully Implicational The existence of x implies the existence of y op. -
11: Weakly Implicational The existence of x sometimes implies the existence of y op. -
Table 1: List of semantic primitives. In the fourth column, [1] stands for (Winston et al, 1987), [2] for (Cohen and
Losielle, 1988) and [3] for (Huhns and Stephens, 1989).
lations. The conclusion of an axiom is identified us-
ing an algebra for composing semantic primitives.
We name this framework Composition of Seman-
tic Relations (CSR). The extended definition, set of
primitives, algebra to compose primitives and CSR
algorithm are independent of any particular set of
relations. We first presented CSR and used it over
PropBank in (Blanco and Moldovan, 2011). In this
paper, we extend that work using a different set of
primitives and relations. Seventy eight inference ax-
ioms are obtained and an empirical evaluation shows
that inferred relations have high accuracies.
2 Semantic Relations
Semantic relations are underlying relations between
concepts. In general, they are defined by a textual
definition accompanied by a few examples. For ex-
ample, Chklovski and Pantel (2004) loosely define
ENABLEMENT as a relation that holds between two
verbs V1 and V2 when the pair can be glossed as
V1 is accomplished by V2 and gives two examples:
assess::review and accomplish::complete.
We find this widespread kind of definition weak
and prone to confusion. Following (Helbig, 2005),
we propose an extended definition for semantic re-
lations, including semantic restrictions for its argu-
ments. For example, AGENT(x, y) holds between an
animate concrete object x and a situation y.
Moreover, we propose to characterize relations by
semantic primitives. Primitives indicate whether a
property holds between the arguments of a relation,
e.g., the primitive temporal indicates if the first ar-
gument must happen before the second.
Besides having a better understanding of each re-
lation, this extended definition allows us to identify
possible and not possible combinations of relations,
as well as to automatically determine the conclusion
of composing a possible combination.
Formally, for a relation R(x, y), the extended def-
initions specifies: (a) DOMAIN(R) and RANGE(R)
(i.e., semantic restrictions for x and y); and (b) PR
(i.e., values for the primitives). The inverse relation
R?1 can be obtained by switching domain and range,
and defining PR?1 as depicted in Table 1.
2.1 Semantic Primitives
Semantic primitives capture deep characteristics of
relations. They are independently determinable for
each relation and specify a property between an el-
ement of the domain and an element of the range of
the relation being described (Huhns and Stephens,
1989). Primitives are fundamental, they cannot be
explained using other primitives.
For each primitive, each relation takes a value
from the set V = {+,?, 0}. ?+? indicates that the
primitive holds, ??? that it does not hold, and ?0?
that it does not apply. Since a cause must precede its
effect, we have P temporalCAUSE = +.
Primitives complement the definition of a relation
and completely characterize it. Coupled with do-
main and range restrictions, primitives allow us to
automatically manipulate and reason over relations.
1457
1:Composable
R2
R1 ? 0 +
? ? 0 ?
0 0 0 0
+ ? 0 +
2:Functional
R2
R1 ? 0 +
? ? 0 +
0 0 0 0
+ + 0 +
3:Homeomerous
R2
R1 ? 0 +
? ? ? ?
0 ? 0 0
+ ? 0 +
4:Separable
R2
R1 ? 0 +
? ? ? ?
0 ? 0 +
+ ? + +
5:Temporal
R2
R1 ? 0 +
? ? ? ?
0 ? 0 +
+ ? + +
6:Connected
R2
R1 ? 0 +
? ? ? +
0 ? 0 +
+ + + +
7:Intrinsic
R2
R1 ? 0 +
? ? 0 ?
0 0 0 0
+ ? 0 +
8:Volitional
R2
R1 ? 0 +
? ? 0 +
0 0 0 +
+ + + +
9:Universal
R2
R1 ? 0 +
? ? 0 ?
0 0 0 0
+ ? 0 +
10:F. Impl.
R2
R1 ? 0 +
? ? 0 ?
0 0 0 0
+ ? 0 +
11:W. Impl.
R2
R1 ? 0 +
? ? ? ?
0 ? 0 +
+ ? + +
Table 2: Algebra for composing semantic primitives.
The set of primitives used in this paper (Table
1) is heavily based on previous work in Knowledge
Bases (Huhns and Stephens, 1989), but we consid-
ered some new primitives. The new primitives are
justified by the fact that we aim at composing rela-
tions capturing the semantics from natural language.
Whatever the set of relations, it will describe the
characteristics of events (who / what / where / when
/ why / how) and connections between them (e.g.,
CAUSE, CORRELATION). Time, space and volition
also play an important role. The third column in
Table 1 indicates the value of the primitive for the
inverse relation: id. means it takes the same; op. the
opposite. The opposite of ? is +, the opposite of +
is ?, and the opposite of 0 is 0.
2.1.1 An Algebra for Composing Semantic
Primitives
The key to automatically obtain inference axioms is
the ability to know the result of composing primi-
tives. Given P iR1 and P
i
R2 , i.e., the values of the ith
primitive for R1 and R2, we define an algebra for
P iR1 ? P iR2 , i.e., the result of composing them. Ta-
ble 2 depicts the algebra for all primitives. An ???
means that the composition is prohibited.
Consider, for example, the Intrinsic primitive: if
both relations are intrinsic (+), the composition is
intrinsic (+); else if intrinsic does not apply to ei-
ther relation (0), the primitive does not apply to the
composition either (0); else the composition is not
intrinsic (?).
3 Inference Axioms
Semantic relations are composed using inference ax-
ioms. An axiom is defined by using the composi-
R1 ? R2 R1?1 ? R2
x R1
R3
y
R2
z
x
R3
y
R2
R1
z
R2 ? R1 R2 ? R1?1
x
R2
R3
y
R1
z
x
R3
R2
y z
R1
Table 3: The four unique possible axioms taking as
premises R1 and R2. Conclusions are indicated by R3 and
are not guaranteed to be the same for the four axioms.
tion operator ???; it combines two relations called
premises and yields a conclusion. We denote an ax-
iom as R1(x, y) ? R2(y, z) ? R3(x, z), where R1 and
R2 are the premises and R3 the conclusion. In or-
der to instantiate an axiom, the premises must form
a chain by having argument y in common.
In general, for n relations there are
(n
2
)
pairs. For
each pair, taking into account inverse relations, there
are 16 possible combinations. Applying property
Ri ?Rj = (Rj?1 ?Ri?1)?1, only 10 are unique: (a) 4
combine R1, R2 and their inverses (Table 3); (b) 3
combine R1 and R1?1; and (c) 3 combine R2 and
R2?1. The most interesting axioms fall into category
(a) and there are (n2
)
? 4 + 3n = 2? n(n? 1) + 3n =
2n2 + n potential axioms in this category.
Depending on n, the number of potential axioms
to consider can be significantly large. For n = 20,
there are 820 axioms to explore and for n = 30,
1,830. Manual examination of those potential ax-
1458
Relation R Domain Range P 1R P 2R P 3R P 4R P 5R P 6R P 7R P 8R P 9R P 10R P 11R
a: CAU CAUSE si si + + - + + - + 0 - + +
b: INT INTENT si aco + + - + - - - + - 0 -
c: PRP PURPOSE si, ao si, co, ao + - - + - - - - - 0 -
d: AGT AGENT aco si + + - + 0 - - + - 0 0
e: MNR MANNER st, ao, ql si + - - + 0 - - + - 0 0
f : AT-L AT-LOCATION o, si loc + + - 0 0 + - 0 - 0 0
g: AT-T AT-TIME o, si tmp + + - 0 0 + - 0 - 0 0
h: SYN SYNONYMY ent ent + - + 0 0 0 + 0 + 0 0
Table 4: Extended definition for the set of relations.
ioms would be time-consuming and prone to errors.
We avoid this by using the extended definition and
the algebra for composing primitives.
3.1 Necessary Conditions for Composing
Semantic Relations
There are two necessary conditions for composing
R1 and R2:
? They have to be compatible. A pair of relations
is compatible if it is possible, from a theoretical
point of view, to compose them.
Formally, R1 and R2 are compatible iff
RANGE(R1) ? DOMAIN(R2) 6= ?.
? A third relation R3 must match as con-
clusion, i.e., ?R3 such that DOMAIN(R3) ?
DOMAIN(R1) 6= ? and RANGE(R3) ?
RANGE(R2) 6= ?. Furthermore, PR3 must
be consistent with PR1 ? PR2 .
3.2 CSR: An Algorithm for Composing
Semantic Relations
Consider any set of relations R defined using the ex-
tended definition. One can obtain inference axioms
using the following algorithm:
For (R1, R2) ? R ? R:
For (Ri, Rj) ? [(R1, R2), (R1?1, R2), (R2, R1), (R2, R1?1)]:
1. Domain and range compatibility
If RANGE(Ri) ? DOMAIN(Rj) = ?, break
2. Conclusion match
Repeat for R3 ? possible conc(R, Ri, Rj):
(a) If DOMAIN(R3) ? DOMAIN(Ri) = ? or
RANGE(R3) ? RANGE(Rj) = ?, break
(b) If consistent(PR3 , PRi ? PRj ),
axioms += Ri(x, y) ? Rj(y, z) ? R3(x, z)
Given R, R?1 can be automatically obtained (Sec-
tion 2). Possible conc(R, Ri, Rj) returns the set R
unless Ri (Rj) is universal (P 9 = +), in which case
it returns Rj (Ri). Consistent(PR1, PR2) is a simple
procedure that compares the values assigned to each
primitive; two values are consistent unless they have
different opposite values or any of them is ??? (i.e.,
the composition is prohibited).
3.3 An Example: Agent and Purpose
We present an example of applying the CSR algo-
rithm by inspecting the potential axiom AGENT(x,
y) ? PURPOSE?1(y, z) ? R3(x, z), where x is the
agent of y, and action y has as its purpose z. A state-
ment instantiating the premises is [Mary]x [came]y
to [talk]z about the issue. Knowing AGENT(Mary,
came) and PURPOSE?1(came, talk ), our goal is to
identify the links R3(Mary, talk ), if any.
We use the relations as defined in Table 4. First,
we note that both AGENT and PURPOSE?1 are com-
patible (Step 1). Second, we must identify the pos-
sible conclusions R3 that fit as conclusions (Step 2).
Given PAGENT and PPURPOSE?1 , we obtain PAGENT ?
PPURPOSE?1 using the algebra:
PAGENT = {+,+,?,+, 0,?,?,+,?,0, 0}
PPURPOSE?1 = {+,?,?,+,+,?,?,?,?,0,+}
PAGENT ? PPURPOSE?1 = {+,+,?,+,+,?,?,+,?,0,+}
Out of all relations (Section 4), AGENT and IN-
TENT?1 fit the conclusion match. First, their do-
mains and ranges are compatible with the composi-
tion (Step 2a). Second, both PAGENT and PINTENT?1
are consistent with PAGENT ? PPURPOSE?1 (Step 2b).
Thus, we obtain the following axioms: AGENT(x, y)
? PURPOSE?1(y, z) ? AGENT(x, z) and AGENT(x,
y) ? PURPOSE?1(y, z) ? INTENT?1(x, z).
Instantiating the axioms over [Mary]x [came]y to
[talk]z about the issue yields AGENT(Mary, talk )
and INTENT?1(Mary, talk ). Namely, the axioms
1459
R2 R2 R2
R1 a b c d e f g h R1 a b c d e f g h R1 a?1 b?1 c?1 d?1 e?1 f?1 g?1 h?1
a a : : - f g a a?1 : b b - f g a?1 a : : d?1 - a
b - f g b b?1 b?1 : : b?1,d?1 f g b?1 b : : b
c : b c - e f g c c?1 b?1 : : e f g c?1 c : : : b,d?1 e?1 c
d d - d d f g d d?1 - f g d?1 d d b?1,d - b,d d
e - b e e f g e e?1 - b,d e?1 e,e?1 f g e?1 e - e b?1,d?1 e,e?1 e
f f f?1 f?1 f?1 f?1 f?1 f?1 - - f?1 f - f
g g g?1 g?1 g?1 g?1 g?1 g?1 - - g?1 g - g
h a b c d e f g h h?1 a b c d e f g h,h?1 h a?1 b?1 c?1 d?1 e?1 f?1 g?1 h,h?1
Table 5: Inference axioms automatically obtained using the relations from Table 4. A letter indicates an axiom R1 ? R2
? R3 by indicating R3. An empty cell indicates that R1 and R2 do not have compatible domains and ranges; ?:? that
the composition is prohibited; and ?-? that a relation R3 such that PR3 is consistent with PR1 ? PR2 could not be found.
yield Mary is the agent of talking, and she has the in-
tention of talking. These two relations are valid but
most probably ignored by a role labeler since Mary
is not an argument of talk.
4 Case Study
In this Section, we apply the CSR algorithm over a
set of eight well-known relations. It is out of the
scope of this paper to explain in detail the semantics
of each relation or their detection. Our goal is to
obtain inference axioms and, taking for granted that
annotation is available, evaluate their accuracy.
The only requirement for the CSR algorithm is to
define semantic relations using the extended defini-
tion (Table 4). To define domains and ranges, we
use the ontology in Section 4.2. Values for the prim-
itives are assigned manually. The meaning of each
relations is as follows:
? CAU(x, y) encodes a relation between two situa-
tions, where the existence of y is due to the pre-
vious existence of x, e.g., He [got]y a bad grade
because he [didn?t submit]x the project.
? INT(x, y) links an animate concrete object and the
situations he wants to become true, e.g., [Mary]y
would like to [grow]x bonsais.
? PRP(x, y) holds between a concept y and its main
goal x. Purposes can be defined for situations,
e.g., [pruning]y allows new [growth]x; concrete
objects, e.g., the [garage]y is used for [storage]x ;
or abstract objects, e.g., [language]y is used to
[communicate]x .
? AGT(x, y) links a situation y and its intentional
doer x, e.g., [Mary]x [went]y to Paris. x is re-
stricted to animate concrete objects.
? MNR(x, y) holds between the mode, way, style or
fashion x in which a situation y happened. x can
be a state, e.g., [walking]y [holding]x hands; ab-
stract objects, e.g., [die]y [with pain]x; or qualities,
e.g. [fast]x [delivery]y .
? AT-L(x, y) defines the spatial context y of an ob-
ject or situation x, e.g., He [went]x [to Cancun]y,
[The car]x is [in the garage]y.
? AT-T(x, y) links an object or situation x, with
its temporal information y, e.g., He [went]x
[yesterday]y , [20th century]y [sculptures]x .
? SYN(x, y) can be defined between any two entities
and holds when both arguments are semantically
equivalent, e.g., SYN(dozen, twelve).
4.1 Inference Axioms Automatically Obtained
After applying the CSR algorithm over the relations
in Table 4, we obtain 78 unique inference axioms
(Table 5). Each sub table must be indexed with
the first and second premises as row and column re-
spectively. The table on the left summarizes axioms
R1 ? R2 ? R3 and R2 ? R1 ? R3, the one in the mid-
dle axiom R1?1 ? R2 ? R3 and the one on the right
axiom R2 ? R1?1 ? R3.
The CSR algorithm identifies several correct ax-
ioms and accurately marks as prohibited several
combinations that would lead to wrong inferences:
? For CAUSE, the inherent transitivity is detected
(a ? a ? a). Also, no relation is inferred between
two different effects of the same cause (a?1 ? a
? :) and between two causes of the same effect
(a ? a?1 ? :).
? The location and temporal information of con-
cept y is inherited by its cause, intention, pur-
pose, agent and manner (sub table on the left, f
and g columns).
1460
? As expected, axioms involving SYNONYMY as
one of their premises yield the other premise as
their conclusion (all sub tables).
? The AGENT of y is inherited by its causes, pur-
poses and manners (d row, sub table on the right).
In all examples below, AGT(x, y) holds, and
we infer AGT(x, z) after composing it with R2:
(1) [He]x [went]y after [reading]z a good review,
R2: CAU?1(y, z); (2) [They]x [went]y to [talk]z
about it, R2: PRP?1(y, z); and (3) [They]x [were
walking]y [holding]z hands, R2: MNR?1(y, z)
An AGENT for a situation y is also inherited by
its effects, and the situations that have y as their
manner or purpose (d row, sub table on the left).
? A concept intends the effects of its intentions
and purposes (b?1 ? a ? b?1, c?1 ? a ?
b?1). For example, [I]x printed the document to
[read]y and [learn]z the contents; INT?1(I, read )
? CAU(read, learn) ? INT?1(I, learn).
It is important to note that domain and range re-
strictions are not sufficient to identify inference ax-
ioms; they only filter out pairs of not compatible re-
lations. The algebra to compose primitives is used
to detect prohibited combinations of relations based
on semantic grounds and identify the conclusion of
composing them. Without primitives, the cells in Ta-
ble 5 would be either empty (marking the pair as not
compatible) or would simply indicate that the pair
has compatible domain and range (without identify-
ing the conclusion).
Table 5 summarizes 136 unique pairs of premises
(recall Ri ? Rj = (Rj?1 ? Ri?1)?1). Domain and
range restrictions mark 39 (28.7%) as not compati-
ble. The algebra labels 12 pairs as prohibited (8.8%,
[12.4% of the compatible pairs]) and is unable to
find a conclusion 14 times (10.3%, [14.4%]). Fi-
nally, conclusions are found for 71 pairs (52.2%,
[73.2%]). Since more than one conclusion might be
detected for the same pair of premises, 78 inference
axioms are ultimately identified.
4.2 Ontology
In order to define domains and ranges, we use a sim-
plified version of the ontology presented in (Helbig,
2005). We find enough to contemplate only seven
base classes: ev, st, co, aco, ao, loc and tmp. Entities
(ent) refer to any concept and are divided into situa-
tions (si), objects (o) and descriptors (des).
? Situations are anything that happens at a time and
place and are divided into events (ev) and states
(st). Events imply a change in the status of other
entities (e.g., grow, conference); states do not
(e.g., be standing, account for 10%).
? Objects can be either concrete (co, palpable, tan-
gible, e.g., table, keyboard) or abstract (ao, intan-
gible, product of human reasoning, e.g., disease,
weight). Concrete objects can be further classi-
fied as animate (aco) if they have life, vigor or
spirit (e.g. John, cat).
? Descriptors state properties about the local (loc,
e.g., by the table, in the box) or temporal (tmp,
e.g., yesterday, last month) context of an entity.
This simplified ontology does not aim at defining
domains and ranges for any relation set; it is a sim-
plification to fit the eight relations we work with.
5 Evaluation
An evaluation was performed to estimate the valid-
ity of the 78 axioms. Because the number of axioms
is large we have focused on a subset of them (Table
6). The 31 axioms having SYN as premise are intu-
itively correct: since synonymous concepts are in-
terchangeable, given veracious annotation they per-
form valid inferences.
We use PropBank annotation (Palmer et al, 2005)
to instantiate the premises of each axiom. First,
all instantiations of axiom PRP ? MNR?1 ? MNR?1
were manually checked. This axiom yields 237 new
MANNER, 189 of which are valid (Accuracy 0.80).
Second, we evaluated axioms 1?7 (Table 6).
Since PropBank is a large corpus, we restricted this
phase to the first 1,000 sentences in which there is an
instantiation of any axiom. These sentences contain
1,412 instantiations and are found in the first 31,450
sentences of PropBank.
Table 6 depicts the total number of instantiations
for each axiom and its accuracy (columns 3 and 4).
Accuracies range from 0.40 to 0.90, showing that the
plausibility of an axiom depends on the axiom. The
average accuracy for axioms involving CAU is 0.54
and for axioms involving PRP is 0.87.
Axiom CAU ? AGT?1 ? AGT?1 adds 201 rela-
tions, which corresponds to 0.89% in relative terms.
Its accuracy is low, 0.40. Other axioms are less pro-
ductive but have a greater relative impact and accu-
1461
no heuristic with heuristic
No. Axiom No. Inst. Acc. Produc. No. Inst. Acc. Produc.
1 CAU ? AGT?1 ? AGT?1 201 0.40 0.89% 75 0.67 0.33%
2 CAU ? AT-L ? AT-L 17 0.82 0.84% 15 0.93 0.74%
3 CAU ? AT-T ? AT-T 72 0.85 1.25% 69 0.87 1.20%
1?3 CAU ? R2 ? R3 290 0.54 0.96% 159 0.78 0.52%
4 PRP ? AGT?1 ? AGT?1 375 0.89 1.66% 347 0.94 1.54%
5 PRP ? AT-L ? AT-L 49 0.90 2.42% 48 0.92 2.37%
6 PRP ? AT-T ? AT-T 138 0.84 2.40% 129 0.88 2.25%
7 PRP ? MNR?1 ? MNR?1 71 0.82 3.21% 70 0.83 3.16%
4?7 PRP ? R2 ? R3 633 0.87 1.95% 594 0.91 1.83%
1?7 All 923 0.77 2.84% 753 0.88 2.32%
Table 6: Axioms used for evaluation, number of instances, accuracy and productivity (i.e., percentage of relations
added on top the ones already present). Results are reported with and without the heuristic.
. . . space officials
AGT
AGT
in Tokyo in July for an exhibit
CAU
AT-T
AT-L
stopped by . . .
AT-L
AT-T
Figure 2: Basic (solid arrows) and inferred relations (discontinuous) from A half-dozen Soviet space officials, in Tokyo
in July for an exhibit, stopped by to see their counterparts at the National . . . (wsj 0405, 1).
racy. For example, axiom PRP ? MNR?1 ? MNR?1,
only yields 71 new MNR, and yet it is adding 3.21%
in relative terms with an accuracy of 0.82.
Overall, applying the seven axioms adds 923 re-
lations on top of the ones already present (2.84% in
relative terms) with an accuracy of 0.77. Figure 2
shows examples of inferences using axioms 1?3.
5.1 Error Analysis
Because of the low accuracy of axiom 1, an error
analysis was performed. We found that unlike other
axioms, this axiom often yield a relation type that
is already present in the semantic representation.
Specifically, it often yields R(x, z) when R(x?, z) is
already known. We use the following heuristic in
order to improve accuracy: do not instantiate an ax-
iom R1(x, y) ? R2(y, z) ? R3(x, z) if a relation of the
form R3(x?, z) is already known.
This simple heuristic has increased the accuracy
of the inferences at the cost of lowering their pro-
ductivity. The last three columns in Table 6 show
results when using the heuristic.
6 Comparison with Previous Work
There have been many proposals to detect seman-
tic relations from text without composition. Re-
searches have targeted particular relations (e.g.,
CAUSE (Chang and Choi, 2006; Bethard and Mar-
tin, 2008)), relations within noun phrases (Nulty,
2007), named entities (Hirano et al, 2007) or clauses
(Szpakowicz et al, 1995). Competitions include
(Litkowski, 2004; Carreras and Ma`rquez, 2005;
Girju et al, 2007; Hendrickx et al, 2009).
Two recent efforts (Ruppenhofer et al, 2009; Ger-
ber and Chai, 2010) are similar to CSR in their goal
(i.e., extract meaning ignored by current semantic
parsers), but completely differ in their means. Their
merit relies on annotating and extracting semantic
connections not originally contemplated (e.g., be-
tween concepts from two different sentences) us-
ing an already known and fixed relation set. Unlike
CSR, they are dependent on the relation inventory,
require annotation and do not reason or manipulate
relations. In contrast to all the above references and
the state of the art, the proposed framework obtains
axioms that take as input semantic relations pro-
1462
duced by others and output more relations: it adds
an extra layer of semantics previously ignored.
Previous research has exploited the idea of using
semantic primitives to define and classify seman-
tic relations under the names of relation elements,
deep structure, aspects and primitives. The first at-
tempt on describing semantic relations using prim-
itives was made by Chaffin and Herrmann (1987);
they differentiate 31 relations using 30 relation el-
ements clustered into five groups (intensional force,
dimension, agreement, propositional and part-whole
inclusion). Winston et al (1987) introduce 3 rela-
tion elements (functional, homeomerous and sepa-
rable) to distinguish six subtypes of PART-WHOLE.
Cohen and Losielle (1988) use the notion of deep
structure in contrast to the surface relation and uti-
lizes two aspects (hierarchical and temporal). Huhns
and Stephens (1989) consider a set of 10 primitives.
In theoretical linguistics, Wierzbicka (1996) in-
troduced the notion of semantic primes to perform
linguistic analysis. Dowty (2006) studies composi-
tionality and identifies entailments associated with
certain predicates and arguments (Dowty, 2001).
There has not been much work on composing
relations in the field of computational linguistics.
The term compositional semantics is used in con-
junction with the principle of compositionality, i.e.,
the meaning of a complex expression is determined
from the meanings of its parts, and the way in which
those parts are combined. These approaches are
usually formal and use a potentially infinite set of
predicates to represent semantics. Ge and Mooney
(2009) extracts semantic representations using syn-
tactic structures while Copestake et al (2001) devel-
ops algebras for semantic construction within gram-
mars. Logic approaches include (Lakoff, 1970;
Sa?nchez Valencia, 1991; MacCartney and Manning,
2009). Composition of Semantic Relations is com-
plimentary to Compositional Semantics.
Previous research has manually extracted plau-
sible inference axioms for WordNet relations
(Harabagiu and Moldovan, 1998) and transformed
chains of relations into theoretical axioms (Helbig,
2005). The CSR algorithm proposed here automati-
cally obtains inference axioms.
Composing relations has been proposed before
within knowledge bases. Cohen and Losielle (1988)
combines a set of nine fairly specific relations (e.g.,
FOCUS-OF, PRODUCT-OF, SETTING-OF). The key
to determine plausibility is the transitivity charac-
teristic of the aspects: two relations shall not com-
bine if they have contradictory values for any aspect.
The first algebra to compose semantic primitives
was proposed by Huhns and Stephens (1989). Their
relations are not linguistically motivated and ten of
them map to some sort of PART-WHOLE (e.g. PIECE-
OF, SUBREGION-OF). Unlike (Cohen and Losielle,
1988; Huhns and Stephens, 1989), we use typical
relations that encode the semantics of natural lan-
guage, propose a method to automatically obtain the
inverse of a relation and empirically test the validity
of the axioms obtained.
7 Conclusions
Going beyond current research, in this paper we
investigate the composition of semantic relations.
The proposed CSR algorithm obtains inference ax-
ioms that take as their input semantic relations and
output a relation previously ignored. Regardless of
the set of relations and annotation scheme, an ad-
ditional layer of semantics is created on top of the
already existing relations.
An extended definition for semantic relations is
proposed, including restrictions on their domains
and ranges as well as values for semantic primitives.
Primitives indicate if a certain property holds be-
tween the arguments of a relation. An algebra for
composing semantic primitives is defined, allowing
to automatically determine the primitives values for
the composition of any two relations.
The CSR algorithm makes use of the extended
definition and algebra to discover inference axioms
in an unsupervised manner. Its usefulness is shown
using a set of eight common relations, obtaining 78
axioms. Empirical evaluation shows the axioms add
2.32% of relations in relative terms with an overall
accuracy of 0.88, more than what state-of-the-art se-
mantic parsers achieve.
The framework presented is completely indepen-
dent of any particular set of relations. Even though
different sets may call for different ontologies and
primitives, we believe the model is generally appli-
cable; the only requirement is to use the extended
definition. This is a novel way of retrieving seman-
tic relations in the field of computational linguistics.
1463
References
Steven Bethard and James H. Martin. 2008. Learning Se-
mantic Links from a Corpus of Parallel Temporal and
Causal Relations. In Proceedings of ACL-08: HLT,
Short Papers, pages 177?180, Columbus, Ohio.
Eduardo Blanco and Dan Moldovan. 2011. A Model
for Composing Semantic Relations. In Proceedings
of the 9th International Conference on Computational
Semantics (IWCS 2011), Oxford, UK.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: semantic role label-
ing. In CONLL ?05: Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning,
pages 152?164, Morristown, NJ, USA.
Roger Chaffin and Douglass J. Herrmann, 1987. Relation
Element Theory: A New Account of the Representation
and Processing of Semantic Relations.
Du S. Chang and Key S. Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing & Management,
42(3):662?678.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Proceedings of EMNLP 2004, pages 33?
40, Barcelona, Spain.
Paul R. Cohen and Cynthia L. Losielle. 1988. Beyond
ISA: Structures for Plausible Inference in Semantic
Networks. In Proceedings of the Seventh National
conference on Artificial Intelligence, St. Paul, Min-
nesota.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An Algebra for Semantic Construction in
Constraint-based Grammars. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 140?147, Toulouse, France.
David D. Dowty. 2001. The Semantic Asymmetry of
?Argument Alternations? (and Why it Matters). In
Geart van der Meer and Alice G. B. ter Meulen, ed-
itors, Making Sense: From Lexeme to Discourse, vol-
ume 44.
David Dowty. 2006. Compositionality as an Empirical
Problem. In Chris Barker and Polly Jacobson, editors,
Papers from the Brown University Conference on Di-
rect Compositionality. Oxford University Press.
Ruifang Ge and Raymond Mooney. 2009. Learning
a Compositional Semantic Parser using an Existing
Syntactic Parser. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 611?619, Sun-
tec, Singapore.
Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A Study of Implicit Arguments for Nominal
Predicates. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1583?1592, Uppsala, Sweden.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13?18, Prague, Czech
Republic.
Sanda Harabagiu and Dan Moldovan. 1998. Knowl-
edge Processing on an Extended WordNet. In Chris-
tiane Fellbaum, editor, WordNet: An Electronic Lex-
ical Database and Some of its Applications., chap-
ter 17, pages 684?714. The MIT Press.
Hermann Helbig. 2005. Knowledge Representation and
the Semantics of Natural Language. Springer, 1st edi-
tion.
Iris Hendrickx, Su N. Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid, Sebastian Pado?, Marco Pennac-
chiotti, Lorenza Romano, and Stan Szpakowicz. 2009.
SemEval-2010 Task 8: Multi-Way Classification of
Semantic Relations Between Pairs of Nominals. In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions
(SEW-2009), pages 94?99, Boulder, Colorado.
Toru Hirano, Yoshihiro Matsuo, and Genichiro Kikui.
2007. Detecting Semantic Relations between Named
Entities in Text Using Contextual Features. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, Demo and Poster
Sessions, pages 157?160, Prague, Czech Republic.
Michael N. Huhns and Larry M. Stephens. 1989.
Plausible Inferencing Using Extended Composition.
In IJCAI?89: Proceedings of the 11th international
joint conference on Artificial intelligence, pages 1420?
1425, San Francisco, CA, USA.
George Lakoff. 1970. Linguistics and Natural Logic.
22(1):151?271, December.
Ken Litkowski. 2004. Senseval-3 task: Automatic la-
beling of semantic roles. In Senseval-3: Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 9?12, Barcelona,
Spain.
Bill MacCartney and Christopher D. Manning. 2009. An
extended model of natural logic. In Proceedings of
the Eight International Conference on Computational
Semantics, pages 140?156, Tilburg, The Netherlands.
Paul Nulty. 2007. Semantic Classification of Noun
Phrases Using Web Counts and Learning Algorithms.
In Proceedings of the ACL 2007 Student Research
Workshop, pages 79?84, Prague, Czech Republic.
1464
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2009. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the Workshop on Se-
mantic Evaluations: Recent Achievements and Future
Directions (SEW-2009), pages 106?111, Boulder, Col-
orado.
Victor Sa?nchez Valencia. 1991. Studies on Natural Logic
and Categorial Grammar. Ph.D. thesis, University of
Amsterdam.
Barker Szpakowicz, Ken Barker, and Stan Szpakowicz.
1995. Interactive semantic analysis of Clause-Level
Relationships. In Proceedings of the Second Confer-
ence of the Pacific Association for Computational Lin-
guistics, pages 22?30.
Anna Wierzbicka. 1996. Semantics: Primes and Univer-
sals. Oxford University Press, USA.
Morton E. Winston, Roger Chaffin, and Douglas Her-
rmann. 1987. A Taxonomy of Part-Whole Relations.
Cognitive Science, 11(4):417?444.
1465
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 265?274,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
*SEM 2012 Shared Task: Resolving the Scope and Focus of Negation
Roser Morante
CLiPS - University of Antwerp
Prinsstraat 13, B-2000 Antwerp, Belgium
Roser.Morante@ua.ac.be
Eduardo Blanco
Lymba Corporation
Richardson, TX 75080 USA
eduardo@lymba.com
Abstract
The Joint Conference on Lexical and Compu-
tational Semantics (*SEM) each year hosts a
shared task on semantic related topics. In its
first edition held in 2012, the shared task was
dedicated to resolving the scope and focus of
negation. This paper presents the specifica-
tions, datasets and evaluation criteria of the
task. An overview of participating systems is
provided and their results are summarized.
1 Introduction
Semantic representation of text has received consid-
erable attention these past years. While early shal-
low approaches have been proven useful for several
natural language processing applications (Wu and
Fung, 2009; Surdeanu et al, 2003; Shen and La-
pata, 2007), the field is moving towards analyzing
and processing complex linguistic phenomena, such
as metaphor (Shutova, 2010) or modality and nega-
tion (Morante and Sporleder, 2012).
The *SEM 2012 Shared Task is devoted to nega-
tion, specifically, to resolving its scope and focus.
Negation is a grammatical category that comprises
devices used to reverse the truth value of proposi-
tions. Broadly speaking, scope is the part of the
meaning that is negated and focus the part of the
scope that is most prominently or explicitly negated
(Huddleston and Pullum, 2002). Although negation
is a very relevant and complex semantic aspect of
language, current proposals to annotate meaning ei-
ther dismiss negation or only treat it in a partial man-
ner.
The interest in automatically processing nega-
tion originated in the medical domain (Chapman
et al, 2001), since clinical reports and discharge
summaries must be reliably interpreted and indexed.
The annotation of negation and hedge cues and their
scope in the BioScope corpus (Vincze et al, 2008)
represented a pioneering effort. This corpus boosted
research on scope resolution, especially since it was
used in the CoNLL 2010 Shared Task (CoNLL
ST 2010) on hedge detection (Farkas et al, 2010).
Negation has also been studied in sentiment analy-
sis (Wiegand et al, 2010) as a means to determine
the polarity of sentiments and opinions.
Whereas several scope detectors have been de-
veloped using BioScope (Morante and Daelemans,
2009; Velldal et al, 2012), there is a lack of cor-
pora and tools to process negation in general domain
texts. This is why we have prepared new corpora
for scope and focus detection. Scope is annotated
in Conan Doyle stories (CD-SCO corpus). For each
negation, the cue, its scope and the negated event, if
any, are marked as shown in example (1a). Focus is
annotated on top of PropBank, which uses the WSJ
section of the Penn TreeBank (PB-FOC corpus). Fo-
cus annotation is restricted to verbal negations an-
notated with MNEG in PropBank, and all the words
belonging to a semantic role are selected as focus.
An annotated example is shown in (1b)1.
(1) a. [John had] never [said as much before]
b. John had never said {as much} before
The rest of this paper is organized as follows.
The two proposed tasks are described in Section 2,
and the corpora in Section 3. Participating systems
and their results are summarized in Section 4. The
approaches used by participating systems are de-
scribed in Section 5, as well as the analysis of re-
sults. Finally, Section 6 concludes the paper.
1Throughout this paper, negation cues are marked in bold
letters, scopes are enclosed in square brackets and negated
events are underlined; focus is enclosed in curly brackets.
265
2 Task description
The *SEM 2012 Shared Task2 was dedicated to re-
solving the scope and focus of negation (Task 1 and
2 respectively). Participants were allowed to engage
in any combination of tasks and submit at most two
runs per task. A pilot task combining scope and
focus detection was initially planned, but was can-
celled due to lack of participation. We received a
total of 14 runs, 12 for scope detection (7 closed, 5
open) and 2 for focus detection (0 closed, 2 open).
Submissions fall into two tracks:
? Closed track. Systems are built using exclusively
the annotations provided in the training set and are
tuned with the development set. Systems that do
not use external tools to process the input text or
that modify the annotations provided (e.g., simplify
parse tree, concatenate lists of POS tags, ) fall under
this track.
? Open track. Systems can make use of any external
resource or tool. For example, if a team uses an ex-
ternal semantic parser, named entity recognizer or
obtains the lemma for each token by querying ex-
ternal resources, it falls under the open track. The
tools used cannot have been developed or tuned us-
ing the annotations of the test set.
Regardless of the track, teams were allowed to
submit their final results on the test set using a sys-
tem trained on both the training and development
sets. The data format is the same as in several pre-
vious CoNLL Shared Tasks (Surdeanu et al, 2008).
Sentences are separated by a blank line. Each sen-
tence consists of a sequence of tokens, and a new
line is used for each token.
2.1 Task 1: Scope Resolution
Task 1 aimed at resolving the scope of negation cues
and detecting negated events. The task is divided
into 3 subtasks:
1. Identifying negation cues, i.e., words that express
negation. Cues can be single words (e.g., never),
multiwords (e.g., no longer, by no means), or affixes
(e.g.l im-, -less). Note that negation cues can be
discontinuous, e.g., neither [. . . ] nor.
2. Resolving the scope of negation. This subtask ad-
dresses the problem of determining which tokens
within a sentence are affected by the negation cue.
A scope is a sequence of tokens that can be discon-
tinuous.
2www.clips.ua.ac.be/sem2012-st-neg/
3. Identifying the negated event or property, if any.
The negated event or property is always within the
scope of a cue. Only factual events can be negated.
For the sentence in (2), systems have to identify
no and nothing as negation cues, after his habit he
said and after mine I asked questions as scopes, and
said and asked as negated events.
(2) [After his habit he said] nothing, and after mine I
asked no questions.
After his habit he said nothing, and [after mine I
asked] no [questions].
2.1.1 Evaluation measures
Previously, scope resolvers have been evaluated at
either the token or scope level. The token level eval-
uation checks whether each token is correctly la-
beled (inside or outside the scope), while the scope
level evaluation checks whether the full scope is cor-
rectly labeled. The CoNLL 2010 ST introduced pre-
cision and recall at scope level as performance mea-
sures and established the following requirements: A
true positive (TP) requires an exact match for both
the negation cue and the scope. False positives (FP)
occur when a system predicts a non-existing scope
in gold, or when it incorrectly predicts a scope exist-
ing in gold because: (1) the negation cue is correct
but the scope is incorrect; (2) the cue is incorrect
but the scope is correct; (3) both cue and scope are
incorrect. These three scenarios also trigger a false
negative (FN). Finally, FN also occur when the gold
annotations specify a scope but the system makes no
such prediction (Farkas et al, 2010).
As we see it, the CONLL 2010 ST evaluation
requirements were somewhat strict because for a
scope to be counted as TP, the negation cue had
to be correctly identified (strict match) as well as
the punctuation tokens within the scope. Addi-
tionally, this evaluation penalizes partially correct
scopes more than fully missed scopes, since partially
correct scopes count as FP and FN, whereas missed
scopes count only as FN. This is a standard prob-
lem when applying the F measures to the evaluation
of sequences. For this shared task we have adopted
a slightly different approach based on the following
criteria:
? Punctuation tokens are ignored.
? We provide a scope level measure that does not re-
quire strict cue match. To count a scope as TP this
266
measure requires that only one cue token is cor-
rectly identified, instead of all cue tokens.
? To count a negated event as TP we do not require
correct identification of the cue.
? To evaluate cues, scopes and negated events, partial
matches are not counted as FP, only as FN. This is to
avoid penalizing partial matches more than missed
matches.
The following evaluation measures have been
used to evaluate the systems:
? Cue-level F1-measures (Cue).
? Scope-level F1-measures that require only partial
cue match (Scope NCM).
? Scope-level F1-measures that require strict cue
match (Scope CM). In this case, all tokens of the
cue have to be correctly identified.
? F1-measure over negated events (Negated), com-
puted independently from cues and from scopes.
? Global F1-measure of negation (Global): the three
elements of the negation ? cue, scope and negated
event ? all have to be correctly identified (strict
match).
? F1-measure over scope tokens (Scope tokens). The
total of scope tokens in a sentence is the sum of to-
kens of all scopes. For example, if a sentence has
two scopes, one of five tokens and another of seven
tokens, then the total of scope tokens is twelve.
? Percentage of correct negation sentences (CNS).
A second version of the measures (Cue/Scope
CM/Scope NCM/Negated/Global-B) was calculated
and provided to participants, but was not used to
rank the systems, because it was introduced in the
last period of the development phase following the
request of a participant team. In the B version of the
measures, precision is not counted as (TP/(TP+FP)),
but as (TP / total of system predictions), counting in
this way the percentage of perfect matches among
all the system predictions. Providing this version of
the measures also allowed us to compare the results
of the two versions and to check if systems would
be ranked in a different position depending on the
version.
Even though we believe that relaxing scope eval-
uation by ignoring punctuation marks and relaxing
the strict cue match requirement is a positive feature
of our evaluation, we need to explore further in order
to define a scope evaluation measure that captures
the impact of partial matches in the scores.
2.2 Task 2: Focus Detection
This task tackles focus of negation detection. Both
scope and focus are tightly connected. Scope is the
part of the meaning that is negated and focus is that
part of the scope that is most prominently or explic-
itly negated (Huddleston and Pullum, 2002). Focus
can also be defined as the element of the scope that is
intended to be interpreted as false to make the over-
all negative true.
Detecting focus of negation is useful for retriev-
ing the numerous words that contribute to implicit
positive meanings within a negation. Consider the
statement The government didn?t release the UFO
files {until 2008}. The focus is until 2008, yielding
the interpretation The government released the UFO
files, but not until 1998. Once the focus is resolved,
the verb release, its AGENT The government and its
THEME the UFO files are positive; only the TEMPO-
RAL information until 2008 remains negated.
We only target verbal negations and focus is al-
ways the full text of a semantic role. Some examples
of annotation and their interpretation (Int) using fo-
cus detection are provided in (3?5).
(3) Even if that deal isn?t {revived}, NBC hopes to
find another.
Int: Even if that deal is suppressed, NBC hopes to
find another.
(4) A decision isn?t expected {until some time next
year}.
Int: A decision is expected at some time next year.
(5) . . . it told the SEC it couldn?t provide financial
statements by the end of its first extension
?{without unreasonable burden or expense}?.
Int: It could provide them by that time with a huge
overhead.
2.2.1 Evaluation measures
Task 2 is evaluated using precision, recall and F1.
Submissions are ranked by F1. For each negation,
the predicted focus is considered correct if it is a per-
fect match with the gold annotations.
3 Data Sets
We have released two datasets, which will be avail-
able from the web site of the task: CD-SCO for
scope detection and PB-FOC for focus detection.
The next two sections introduce the datasets.
267
WL2 108 0 After After IN (S(S(PP* After
WL2 108 1 his his PRP$ (NP* his
WL2 108 2 habit habit NN *)) habit
WL2 108 3 he he PRP (NP*) he
WL2 108 4 said say VBD (VP* said said
WL2 108 5 nothing nothing NN (NP*))) nothing
WL2 108 6 , , , *
WL2 108 7 and and CC *
WL2 108 8 after after IN (S(PP* after
WL2 108 9 mine mine NN (NP*)) mine
WL2 108 10 I I PRP (NP*) I
WL2 108 11 asked ask VBD (VP* asked asked
WL2 108 12 no no DT (NP* no
WL2 108 13 questions question NNS *))) questions
WL2 108 14 . . . *)
Figure 1: Example sentence from CD-SCO.
3.1 CD-SCO: Scope Annotation
The corpus for Task 1 is CD-SCO, a corpus of Co-
nan Doyle stories. The training corpus contains The
Hound of the Baskervilles, the development corpus,
The Adventure of Wisteria Lodge, and the test corpus
The Adventure of the Red Circle and The Adventure
of the Cardboard Box. The original texts are freely
available from the Gutenberg Project.3
CD-SCO is annotated with negation cues and
their scope, as well as the event or property that is
negated. The cues are the words that express nega-
tion and the scope is the part of a sentence that is
affected by the negation cues. The negated event
or property is the main event or property actually
negated by the negation cue. An event can be a pro-
cess, an action, or a state.
Figure 1 shows an example sentence. Column 1
contains the name of the file, column 2 the sentence
#, column 3 the token #, column 4 the word, column
5 the lemma, column 6 the PoS, column 7 the parse
tree information and columns 8 to end the negation
information. If a sentence does not contain a nega-
tion, column 8 contains ?***? and there are no more
columns. If it does contain negations, the informa-
tion for each one is encoded in three columns: nega-
tion cue, scope, and negated event respectively.
The annotation of cues and scopes is inspired by
the BioScope corpus, but there are several differ-
ences. First and foremost, BioScope does not an-
notate the negated event or property. Another im-
3http://www.gutenberg.org/browse/
authors/d\#a37238
Training Dev. Test
# tokens 65,450 13,566 19,216
# sentences 3644 787 1089
# negation sent. 848 144 235
% negation sent. 23.27 18.29 21.57
# cues 984 173 264
# unique cues 30 20 20
# scopes 887 168 249
# negated 616 122 173
Table 1: CD-SCO Corpus statistics.
portant difference concerns the scope model itself:
in CD-SCO, the cue is not considered to be part of
the scope. Furthermore, scopes can be discontinu-
ous and all arguments of the negated event are con-
sidered to be part of the scope, including the subject,
which is kept out of the scope in BioScope. A final
difference is that affixal negation is annotated in CD-
SCO, as in (6).
(6) [He] declares that he heard cries but [is] un[{able}
to state from what direction they came].
Statistics for the corpus is presented in Table 1.
More information about the annotation guidelines is
provided by Morante et al (2011) and Morante and
Daelemans (2012), including inter-annotator agree-
ment.
The corpus was preprocessed at the University
of Oslo. Tokenization was obtained by the PTB-
compliant tokenizer that is part of the LinGO En-
glish Resource Grammar. 4
4http://moin.delph-in.net/
268
Apart from the gold annotations, the corpus was
provided to participants with additional annotations:
? Lemmatization using the GENIA tagger (Tsuruoka
and Tsujii, 2005), version 3.0.1, with the ?-nt? com-
mand line option. GENIA PoS tags are comple-
mented with TnT PoS tags for increased compati-
bility with the original PTB.
? Parsing with the Charniak and Johnson (2005) re-
ranking parser.5 For compatibility with PTB con-
ventions, the top-level nodes in parse trees (?S1?),
were removed. The conversion of PTB-style syntax
trees into CoNLL-style format was performed using
the CoNLL 2005 Shared Task software.6
3.2 PB-FOC: Focus Annotation
We have adapted the only previous annotation effort
targeting focus of negation for PB-FOC (Blanco and
Moldovan, 2011). This corpus provides focus an-
notation on top of PropBank. It targets exclusively
verbal negations marked with MNEG in PropBank
and selects as focus the semantic role containing the
most likely focus. The motivation behind their ap-
proach, annotation guidelines and examples can be
found in the aforementioned paper.
We gathered all negations from sections 02?21,
23 and 24 and discarded negations for which the fo-
cus or PropBank annotations were not sound, leav-
ing 3,544 instances.7 For each verbal negation, PB-
FOC provides the current sentence, and the previous
and next sentences as context. For each sentence,
along with the gold focus annotations, PB-FOC con-
tains the following additional annotations:
? Token number;
? POS tags using the Brill tagger (Brill, 1992);
? Named Entities using the Stanford named en-
tity recognizer recognizer (Finkel et al, 2005);
? Chunks using the chunker by Phan (2006);
? Syntactic tree using the Charniak parser (Char-
niak, 2000);
? Dependency tree derived from the syntactic
tree (de Marneffe et al, 2006);
ErgTokenization, http://moin.delph-in.net/
ReppTop
5November 2009 release available from Brown University.
6http://www.lsi.upc.edu/?srlconll/
srlconll-1.1.tgz
7The original focus annotation targeted the 3,993 negations
marked with MNEG in the whole PropBank.
Train Devel Test
1 role 2,210 515 672
2 roles 89 15 38
3 roles 3 0 2
All 2,302 530 712
S
em
an
ti
c
ro
le
s
fo
cu
s
be
lo
ng
s
to
A1 980 222 309
AM-NEG 592 138 172
AM-TMP 161 35 46
AM-MNR 127 27 38
A2 112 28 36
A0 94 23 31
None 88 19 35
AM-ADV 78 23 26
C-A1 46 6 16
AM-PNC 33 8 12
AM-LOC 25 4 10
A4 11 2 5
R-A1 10 2 2
Other 40 8 16
Table 2: Basic numeric analysis for PB-FOC. The first 4
rows indicate the number of unique roles each negation
belongs to, the rest indicate the counts for each role.
? Semantic roles using the labeler described by
(Punyakanok et al, 2008); and
? Verbal negation, indicates with ?N? if that token
correspond to a verbal negation for which focus
must be predicted.
Figure 2 provides a sample of PB-FOC. Know-
ing that the original focus annotations were done on
top of PropBank and that focus corresponds to a sin-
gle role, semantic role information is key to predict
the focus. In Table 2, we show some basic numeric
analysis regarding focus annotation and the automat-
ically obtained semantic role labels. Most instances
of focus belong to a single role in the three splits
and the most common role focus belongs to is A1,
followed by AM-NEG, M-TMP and M-MNR. Note
that some instances have at least one word that does
not belong to any role (88 in training, 19 in develop-
ment and 35 in test).
4 Submissions and results
A total of 14 runs were submitted: 12 for scope de-
tection and 2 for focus detection. The unbalanced
number of submissions might be due to the fact that
both tasks are relatively new and the tight timeline
(six weeks) under which systems were developed.
269
Marketers 1 NNS O B-NP (S1(S(NP*) 2 nsubj (A0*) * - *
believe 2 VBP O B-VP (VP* 0 root (V*) * - *
most 3 RBS O B-NP (SBAR(S(NP* 4 amod (A1* (A0* - FOCUS
Americans 4 NNPS O I-NP *) 7 nsubj * *) - FOCUS
wo 5 MD O B-VP (VP* 7 aux * (AM-MOD*) - *
n?t 6 RB O I-VP * 7 neg * (AM-NEG*) - *
make 7 VB O I-VP (VP* 2 ccomp * (V*) N *
the 8 DT O B-NP (NP* 10 det * (A1* - *
convenience 9 NN O I-NP * 10 nn * * - *
trade-off 10 NN O I-NP *)))))) 7 dobj *) *) - *
... 11 : O O * 2 punct * * - *
. 12 . O O *)) 2 punct * * - *
Figure 2: Example sentence from PB-FOC.
Team Prec. Rec. F1
O
pe
n UConcordia, run 1 60.00 56.88 58.40
UConcordia, run 2 59.85 56.74 58.26
Table 3: Official results for Task 2.
Some participants showed interest in the second task
and expressed that they did not participate because
of lack of time. In this section, we present the results
for each task.
4.1 Task 1
Six teams (UiO1, UiO2, FBK, UWashington,
UMichigan, UABCoRAL) submitted results for the
closed track with a total of seven runs, and four
teams (UiO2, UGroningen, UCM-1, UCM-2) sub-
mitted results for the open track with a total of five
runs. The evaluation results are provided in Ta-
ble 4, which contains the official results, and Table 5,
which contains the results for evaluation measures
B.
The best Global score in the closed track was ob-
tained by UiO1 (57.63 F1). The best score for Cues
was obtained by FBK (92.34 F1), for Scopes CM
by UiO2 (73.39 F1), for Scopes NCM by UWash-
ington (72.40 F1), and for Negated by UiO1 (67.02
F1). The best Global score in the open track was ob-
tained by UiO2 (54.82 F1), as well as the best scores
for Cues (91.31 F1), Scopes CM (72.39 F1), Scopes
NCM (72.39 F1), and Negated (61.79 F1).
4.2 Task 2
Only one team participated in Task 2, UConcordia
from CLaC Lab at Concordia University. They sub-
mitted two runs and the official results are summa-
rized in Table 3. Their best run scored 58.40 F1.
5 Approaches and analysis
In this section we summarize the methodologies ap-
plied by participants to solve the tasks and we ana-
lyze the results.
5.1 Task 1
To solve Task 1 most teams develop a three module
pipeline with a module per subtask. Scope resolu-
tion and negated event detection are independent of
each other and both depend on cue detection. An
exception is the UiO1 system, which incorporates a
module for factuality detection. Most systems ap-
ply machine learning algorithms, either Conditional
Random Fields (CRFs) or Support Vector Machines
(SVMs), while less systems implement a rule-based
approach. Syntax information is widely employed,
either in the form of rules or incorporated in the
learning model. Multi-word and affixal negation
cues receive a special treatment in most cases, and
scopes are generally postprocessed.
The systems that participate in the closed track
are machine learning based. The UiO1 system is an
adaptation of another system (Velldal et al, 2012),
which combines SVM cue classification with SVM-
based ranking of syntactic constituents for scope
resolution. The approach is extended to identify
negated events by first classifying negations as fac-
tual or non-factual, and then applying an SVM
ranker over candidate events. The original treat-
ment of factuality in this system results in the high-
est score for both the negated event subtask and the
global task.
The UiO2 system combines SVM cue classifica-
tion with CRF-based sequence labeling. An original
aspect of the UiO2 approach is the model represen-
270
O
ffi
ci
al
re
su
lt
s
fo
r
Ta
sk
1 Cu
es
Sc
op
es
C
M
Sc
op
es
N
C
M
Sc
op
e
To
ke
ns
N
eg
at
ed
G
lo
ba
l
%
C
N
S
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
Closedtrack
U
iO
1
r2
89
.1
7
93
.5
6
91
.3
1
83
.8
9
60
.6
4
70
.3
9
83
.8
9
60
.6
4
70
.3
9
75
.8
7
90
.0
8
82
.3
7
60
.5
8
75
.0
0
67
.0
2
79
.8
7
45
.0
8
57
.6
3
43
.8
3
U
iO
1
r1
91
.4
2
92
.8
0
92
.1
0
87
.4
3
61
.4
5
72
.1
7
87
.4
3
61
.4
5
72
.1
7
81
.9
9
88
.8
1
85
.2
6
60
.5
0
72
.8
9
66
.1
2
83
.4
5
43
.9
4
57
.5
7
42
.1
3
U
iO
2
89
.1
7
93
.5
6
91
.3
1
85
.7
1
62
.6
5
72
.3
9
85
.7
1
62
.6
5
72
.3
9
86
.0
3
81
.5
5
83
.7
3
68
.1
8
52
.6
3
59
.4
0
78
.2
6
40
.9
1
53
.7
3
40
.0
0
F
B
K
93
.4
1
91
.2
9
92
.3
4
88
.9
6
58
.2
3
70
.3
9
88
.9
6
58
.2
3
70
.3
9
81
.5
3
82
.4
4
81
.9
8
64
.1
4
56
.7
1
60
.2
0
84
.9
6
36
.3
6
50
.9
3
35
.7
4
U
W
as
hi
ng
to
n
88
.0
4
92
.0
5
90
.0
0
82
.7
2
63
.4
5
71
.8
1
82
.9
0
64
.2
6
72
.4
0
83
.2
6
83
.7
7
83
.5
1
58
.0
4
50
.9
2
54
.2
5
74
.0
2
35
.6
1
48
.0
9
34
.0
4
U
M
ic
hi
ga
n
94
.3
1
87
.8
8
90
.9
8
90
.0
0
50
.6
0
64
.7
8
90
.0
0
50
.6
0
64
.7
8
84
.8
5
80
.6
6
82
.7
0
50
.0
0
52
.2
4
51
.1
0
84
.2
7
28
.4
1
42
.4
9
27
.2
3
U
A
B
C
oR
A
L
85
.9
3
85
.6
1
85
.7
7
79
.0
4
53
.0
1
63
.4
6
79
.5
3
54
.6
2
64
.7
6
85
.3
7
68
.8
6
76
.2
3
65
.0
0
38
.4
6
48
.3
3
66
.3
6
27
.6
5
39
.0
4
26
.8
1
Opentrack
U
iO
2
89
.1
7
93
.5
6
91
.3
1
85
.7
1
62
.6
5
72
.3
9
85
.7
1
62
.6
5
72
.3
9
82
.2
5
82
.1
6
82
.2
0
66
.9
0
57
.4
0
61
.7
9
78
.7
2
42
.0
5
54
.8
2
41
.2
8
U
G
ro
ni
ng
en
r2
88
.8
9
84
.8
5
86
.8
2
76
.1
2
40
.9
6
53
.2
6
76
.1
2
40
.9
6
53
.2
6
69
.2
0
82
.2
7
75
.1
7
56
.6
3
65
.2
9
60
.6
5
72
.0
0
27
.2
7
39
.5
6
27
.2
3
U
C
M
-1
89
.2
6
91
.2
9
90
.2
6
82
.8
6
46
.5
9
59
.6
4
82
.8
6
46
.5
9
59
.6
4
85
.3
7
68
.5
3
76
.0
3
66
.6
7
12
.7
2
21
.3
6
66
.2
8
21
.5
9
32
.5
7
18
.7
2
U
C
M
-2
81
.3
4
64
.3
9
71
.8
8
67
.1
3
38
.5
5
48
.9
8
66
.9
0
38
.9
6
49
.2
4
58
.3
0
67
.7
0
62
.6
5
46
.1
5
21
.1
8
29
.0
3
42
.6
5
10
.9
8
17
.4
6
11
.9
1
U
G
ro
ni
ng
en
r1
86
.9
0
82
.9
5
84
.8
8
46
.3
8
12
.8
5
20
.1
2
46
.3
8
12
.8
5
20
.1
2
69
.6
9
70
.3
0
69
.9
9
53
.9
4
52
.0
5
52
.9
8
37
.7
4
7.
58
12
.6
2
7.
66
Ta
bl
e
4:
O
ffi
ci
al
re
su
lt
s.
?r
1?
st
an
ds
fo
r
ru
n
1
nd
?r
2?
fo
r
ru
n
2.
C
N
S
st
an
ds
fo
r
C
or
re
ct
N
eg
at
io
n
S
en
te
nc
es
.
?C
M
?
st
an
ds
fo
r
C
ue
M
at
ch
an
d
?N
C
M
?
st
an
ds
fo
r
N
o
C
ue
M
at
ch
.
C
ue
s
B
Sc
op
es
B
C
M
Sc
op
es
B
N
C
M
N
eg
at
ed
B
G
lo
ba
lB
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
Closedtrack
U
iO
1
r2
86
.9
7
93
.5
6
90
.1
4
56
.5
5
60
.6
4
58
.5
2
56
.5
5
60
.6
4
58
.5
2
58
.6
0
75
.0
0
65
.7
9
41
.9
0
45
.0
8
43
.4
3
U
iO
1
r1
89
.0
9
92
.8
0
90
.9
1
59
.3
0
61
.4
5
60
.3
6
59
.3
0
61
.4
5
60
.3
6
57
.6
2
72
.8
9
64
.3
6
42
.1
8
43
.9
4
43
.0
4
U
iO
2
86
.9
7
93
.5
6
90
.1
4
59
.3
2
62
.6
5
60
.9
4
59
.3
2
62
.6
5
60
.9
4
67
.1
6
52
.6
3
59
.0
1
38
.0
3
40
.9
1
39
.4
2
F
B
K
91
.6
3
91
.2
9
91
.4
6
58
.2
3
58
.2
3
58
.2
3
58
.2
3
58
.2
3
58
.2
3
60
.3
9
56
.7
1
58
.4
9
38
.0
3
40
.9
1
39
.4
2
U
W
as
hi
ng
to
n
85
.2
6
92
.0
5
88
.5
2
58
.5
2
63
.4
5
60
.8
9
59
.2
6
64
.2
6
61
.6
6
53
.9
0
50
.9
2
52
.3
7
32
.9
8
35
.6
1
34
.2
4
U
M
ic
hi
ga
n
92
.8
0
87
.8
8
90
.2
7
55
.5
1
50
.6
0
52
.9
4
55
.5
1
50
.6
0
52
.9
4
38
.2
5
52
.2
4
44
.1
6
30
.0
0
28
.4
1
29
.1
8
U
A
B
C
oR
A
L
79
.5
8
85
.6
1
82
.4
8
55
.2
3
53
.0
1
54
.1
0
56
.9
0
54
.6
2
55
.7
4
62
.5
0
38
.4
6
47
.6
2
25
.7
0
27
.6
5
26
.6
4
Opentrack
U
iO
2
86
.9
7
93
.5
6
90
.1
4
59
.5
4
62
.6
5
61
.0
6
59
.5
4
62
.6
5
61
.0
6
63
.8
2
57
.4
0
60
.4
4
39
.0
8
42
.0
5
40
.5
1
U
G
ro
ni
ng
en
r2
85
.8
2
84
.8
5
85
.3
3
39
.8
4
40
.9
6
40
.3
9
39
.8
4
40
.9
6
40
.3
9
55
.2
2
65
.2
9
59
.8
3
27
.5
9
27
.2
7
27
.4
3
U
C
M
-1
86
.6
9
91
.2
9
88
.9
3
45
.6
7
46
.5
9
46
.1
3
45
.6
7
46
.5
9
46
.1
3
66
.6
7
12
.7
2
21
.3
6
20
.5
0
21
.5
9
21
.0
3
U
C
M
-2
72
.3
4
64
.3
9
68
.1
3
41
.2
0
38
.5
5
39
.8
3
41
.6
3
38
.9
6
40
.2
5
44
.4
4
21
.1
8
28
.6
9
12
.3
4
10
.9
8
11
.6
2
U
G
ro
ni
ng
en
r1
83
.9
1
82
.9
5
83
.4
3
12
.2
6
12
.8
5
12
.5
5
12
.2
6
12
.8
5
12
.5
5
52
.6
6
52
.0
5
52
.3
5
7.
66
7.
58
7.
62
Ta
bl
e
5:
R
es
ul
ts
w
it
he
va
lu
at
io
n
m
ea
su
re
s
B
.P
re
ci
si
on
is
ca
lc
ul
at
ed
as
:
tr
ue
po
si
tiv
es
/
to
ta
l
of
sy
st
em
pr
ed
ic
ti
on
s.
?r
1?
st
an
ds
fo
r
ru
n
1
nd
?r
2?
fo
r
ru
n
2.
?C
M
?
st
an
ds
fo
r
C
ue
M
at
ch
an
d
?N
C
M
?
st
an
ds
fo
r
N
o
C
ue
M
at
ch
.
Pa
rt
ic
ip
at
in
g
in
st
it
ut
io
ns
:
U
iO
:
U
ni
ve
rs
it
y
of
O
sl
o;
F
B
K
:
Fo
nd
az
io
ne
B
ru
no
K
es
sl
er
&
U
ni
ve
rs
it
y
of
T
re
nt
o;
U
W
as
hi
ng
to
n:
U
ni
ve
rs
it
y
of
W
as
hi
ng
to
n;
U
M
ic
hi
ga
n:
U
ni
ve
rs
it
y
of
M
ic
hi
ga
n;
U
A
B
C
oR
A
L
:
C
oR
A
L
L
ab
U
ni
ve
rs
it
y
of
A
la
ba
m
a;
U
G
ro
ni
ng
en
:
U
ni
ve
rs
it
y
of
G
ro
ni
ng
en
;U
C
M
:C
om
pl
ut
en
se
U
ni
ve
rs
it
y
of
M
ad
ri
d.
271
tation for scopes and negated events, where tokens
are assigned a set of labels that attempts to de-
scribe their behavior within the mechanics of nega-
tion. After unseen sequences are labeled, in-scope
and negated tokens are assigned to their respective
cues using simple post-processing heuristics.
The FBK system consists of three different CRF
classifiers, as well as the UMichigan. A character-
istic of the cue model of the UMichigan system is
that tokens are assigned five labels in order to rep-
resent the different types of negation. Similarly, the
UWashington system has a CRF sequence tagger for
scope and negated event detection, while the cue de-
tector learns regular expression matching rules from
the training set. The UABCoRAL system follows
the same strategy, but instead of CRFs it employs
SVM Light.
The resources utilized by participants in the open
track are diverse. UiO2 reparsed the data with Malt-
Parser in order to obtain dependency graphs. For the
rest, the system is the same as in the closed track.
The global results obtained by this system in the
closed track are higher than the results obtained in
the open track, which is mostly due to a higher per-
formance of the scope resolution module. This is the
only machine learning system in the open track and
the highest performing one.
The UGroningen system is based on tools that
produce complex semantic representations. The sys-
tem employs the C&C tools8 for parsing and Boxer9
to produce semantic representations in the form of
Discourse Representation Structures (DRSs). For
cue detection, the DRSs are converted to flat, non-
recursive structures, called Discourse Representa-
tion Graphs (DRGs). These DRGs allow for cue de-
tection by means of labelled tuples. Scope detection
is done by gathering the tokens that occur within the
scope of the negated DRSs. For negated event detec-
tion, a basic algorithm takes the detected scope and
returns the negated event based on information from
the syntax tree within the scope.
UCM-1 and UCM-2 are rule-based systems that
rely heavily on information from the syntax tree.
The UCM-1 system was initially designed for pro-
8http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/Documentation
9http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/boxer
cessing opinionated texts. It applies a dictionary ap-
proach to cue detection, with the detection of affixal
cues being performed using WordNet. Non-affixal
cue detection is performed by consulting a prede-
fined list of cues. It then uses information from the
syntax tree in order to get a first approximation to
the scope, which is later refined using a set of post-
processing rules. In the case of the UCM-2 system
an algorithm detects negation cues and their scope
by traversing Minipar dependency structures. Fi-
nally, the scope is refined with post-processing rules
that take into account the information provided by
the first algorithm and linguistic clause boundaries.
If we compare tracks, the Global best results ob-
tained in the closed track (57.63 F1) are higher than
the Global best results obtained in the open track
(54.82 F1). If we compare approaches, the best re-
sults in the two tracks are obtained with machine
learning-based systems. The rule-based systems
participating in the open track clearly score lower
(39.56 F1 the best) than the machine learning-based
system (54.82 F1).
Regarding subtasks, systems achieve higher re-
sults in the cue detection task (92.34 F1 the best) and
lower results in the scope resolution (72.40 F1 the
best) and negated event detection (67.02 F1 the best)
tasks. This is not surprising, not only because of
the error propagation effect, but also because the set
of negation cues is closed and comprises mostly sin-
gle tokens, whereas scope sequences are longer. The
best results in cue detection are obtained by the FBK
system that uses CRFs and applies a special proce-
dure to detect the negation cues that are subtokens.
The best scores for scope resolution (72.40, 72.39
F1) are obtained by two machine learning compo-
nents. UWashington uses CRFs with features de-
rived from the syntax tree. UiO2 uses CRFs mod-
els with syntactic and lexical features for scopes, to-
gether with a set of labels aimed at capturing the
behavior of certain tokens within the mechanics of
negation. The best scores for negated events (67.02
F1) are obtained by the UiO1 system that first clas-
sifies negations as factual or non-factual, and then
applies an SVM ranker over candidate events.
Finally, we would like to draw the attention to the
different scores obtained depending on the evalua-
tion measure used. When scope resolution is evalu-
ated with the Scope (NCM, CM) measure, results
272
are much lower than when using the Scope To-
kens measure, which does not reflect the ability of
systems to deal with sequences. Another observa-
tion is related to the difference in precision scores
between the two versions of the evaluation mea-
sures. Whereas for Cues and Negated the differ-
ences are not so big because most cues and negated
events span over a single token, for Scopes they are.
The best Scope NCM precision score is 90.00 %,
whereas the best Scope NCM B precision score is
59.54 %. This shows that the scores can change
considerably depending on how partial matches are
counted (as FP and FN, or only as FN). As a final
remark it is worth noting that the ranking of systems
does not change when using the B measures.
5.2 Task 2
UConcordia submitted two runs in the open track.
Both of them follow the same three component ap-
proach. First, negation cues are detected. Second,
the scope of negation is extracted based on depen-
dency relations and heuristics defined by Kilicoglu
and Bergler (2011). Third, the focus of negation
is determined within the elements belonging to the
scope following three heuristics.
6 Conclusions
In this paper we presented the description of the first
*SEM Shared Task on Resolving the Scope and Fo-
cus of Negation, which consisted of two different
tasks related to different aspects of negation: Task 1
on resolving the scope of negation, and Task 2 on
detecting the focus of negation. Task 1 was di-
vided into three subtasks: identifying negation cues,
resolving their scope, and identifying the negated
event. Two new datasets have been produced for this
Shared Task: the CD-SCO corpus of Conan Doyle
stories annotated with scopes, and the PB-FOC cor-
pus, which provides focus annotation on top of Prop-
Bank. New evaluation software was also developed
for this task. The datasets and the evaluation soft-
ware will be available on the web site of the Shared
Task. As far as we know, this is the first task that fo-
cuses on resolving the focus and scope of negation.
A total of 14 runs were submitted, 12 for scope
detection and 2 for focus detection. Of these, four
runs are from systems that take a rule-based ap-
proach, two runs from hybrid systems, and the rest
from systems that take a machine learning approach
using SVMs or CRFs. Most participants designed a
three component architecture.
For a future edition of the shared task we would
like to unify the annotation schemes of the two cor-
pora, namely the annotation of focus in PB-FOC and
negated events in CD-SCO. The annotation of more
data with both scope and focus would allow us to
study the two aspects jointly. We would also like to
provide better evaluation measures for scope reso-
lution. Currently, scopes are evaluated in terms of
F1, which demands a division of errors into the cat-
egories TP/FP/TN/FN borrowed from the evaluation
of information retrieval systems. These categories
are not completely appropriate to be assigned to se-
quence tasks, such as scope resolution.
Acknowledgements
We are very grateful to Vivek Srikumar for pre-
processing the PB-FOC corpus with the Illinois se-
mantic role labeler, and to Stephan Oepen for pre-
processing the CD-SCO corpus. We also thank the
*SEM organisers and the ST participants. Roser
Morante?s research was funded by the University of
Antwerp (GOA project BIOGRAPH).
References
Eduardo Blanco and Dan Moldovan. 2011. Semantic
Representation of Negation Using Focus Detection. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for C omputational Linguistics: Human Lan-
guage Technologies, pages 581?589, Portland, Ore-
gon, USA. Association for Computational Linguistics.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the third conference on Applied
natural language processing, ANLC ?92, pages 152?
155, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. J Biomed Inform,
34:301?310.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor.
273
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Lin-
guistics conference, NAACL 2000, pages 132?139,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology. The Stanford Natural
Language Processing Group.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1?12, Uppsala, Sweden.
Association for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press.
Halil Kilicoglu and Sabine Bergler. 2011. Effective bio-
event extraction using trigger words and syntactic de-
pendencies. Computational Intelligence, 27(4):583?
609.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the 13th Conference on Natu-
ral Language Learning, pages 21?29, Boulder, CO.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation cues and
their scope in Conan Doyle stories. In Proceedings
of LREC 2012, Istambul.
Roser Morante and Caroline Sporleder. 2012. Special is-
sue on modality and negation: An introduction. Com-
putational Linguistics.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope. guidelines v1.0. Technical Report Series CTR-
003, CLiPS, University of Antwerp, Antwerp, April.
Xuan-Hieu Phan. 2006. Crfchunker: Crf english phrase
chunker.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287, June.
Dan Shen and Mirella Lapata. 2007. Using Semantic
Roles to Improve Question Answering. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EM NLP-CoNLL),
pages 12?21.
Ekaterina Shutova. 2010. Models of Metaphor in NLP.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 688?697,
Uppsala, Sweden. ACL.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using Predicate-Argument Struc-
tures for Information Extraction. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 8?15, Sapporo, Japan. Asso-
ciation for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning, page 159177, Manchester.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 467?474, Vancouver.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of syntax. Computational Lin-
guistics.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60?68,
Uppsala, Sweden. University of Antwerp.
Dekai Wu and Pascale Fung. 2009. Semantic Roles
for SMT: A Hybrid Two-Pass Model. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, Companion
Volume: Short Papers, pages 13?16, Boulder, Col-
orado. Association for Computational Linguistics.
274
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 296?305, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Choosing the Right Words:
Characterizing and Reducing Error of the Word Count Approach
H. Andrew Schwartz,1 Johannes Eichstaedt,1 Lukasz Dziurzynski,1 Eduardo Blanco,2
Margaret L. Kern,1 Stephanie Ramones,1 Martin Seligman,1 and Lyle Ungar1
1University of Pennsylvania
2Lymba Corporation
hansens@seas.upenn.edu
Abstract
Social scientists are increasingly using the
vast amount of text available on social me-
dia to measure variation in happiness and
other psychological states. Such studies count
words deemed to be indicators of happiness
and track how the word frequencies change
across locations or time. This word count ap-
proach is simple and scalable, yet often picks
up false signals, as words can appear in differ-
ent contexts and take on different meanings.
We characterize the types of errors that occur
using the word count approach, and find lex-
ical ambiguity to be the most prevalent. We
then show that one can reduce error with a
simple refinement to such lexica by automat-
ically eliminating highly ambiguous words.
The resulting refined lexica improve precision
as measured by human judgments of word oc-
currences in Facebook posts.
1 Introduction
Massive social media corpora, such as blogs, tweets,
and Facebook statuses have recently peaked the in-
terest of social scientists. Compared to traditional
samples in tens or hundreds, social media sample
sizes are orders of magnitude larger, often contain-
ing millions or billions of posts or queries. Such text
provides potential for unobtrusive, inexpensive, and
real-time measurement of psychological states (such
as positive or negative affect) and aspects of sub-
jective well-being (such as happiness and engage-
ment). Social scientists have recently begun to use
social media text in a variety of studies (Cohn et
al., 2004; Kramer, 2010; Tausczik and Pennebaker,
2010; Kamvar and Harris, 2011; Dodds et al, 2011;
Golder and Macy, 2011).
One of the most popular approaches to estimate
psychological states is by using the word count
method (Pennebaker et al, 2007), where one tracks
the frequency of words that have been judged to be
associated with a given state. Greater use of such
words is taken to index the prevalence of the cor-
responding state. For example, the use of the word
?happy? is taken to index positive emotion, and ?an-
gry? to index negative emotion. The most widely
used tool to carry out such analysis, and the one we
investigate in this paper, is Pennebaker?s Linguistic
Inquiry and Word Count, (LIWC) (Pennebaker et al,
2001; Pennebaker et al, 2007). LIWC, originally de-
veloped to analyze writing samples for emotion and
control, has grown to include a variety of lexica for
linguistic and psychosocial topics including positive
and negative emotions, pronouns, money, work, and
religion. The word count approach has high appeal
to social scientists in need of a tool to approach so-
cial media, and although others have been used (see,
for example (Gottschalk and Bechtel, 1998; Bollen
et al, 2010), LIWC?s lexica are generally perceived
as a ?tried-and-tested? list of words (Miller, 2011).
Unfortunately, the word count approach has some
drawbacks when used as indicators for psycholog-
ical states. Words are the unit of measurement, but
words can carry many different meanings depending
on context. Consider the Facebook posts below con-
taining instances of ?play?, a word associated with
positive emotion in LIWC.
296
1. so everyone should come to the play tomor-
row...
2. Does anyone what type of file i need to convert
youtube videos to play on PS3???
3. Time to go play with Chalk from the Easter
Bunny!
Out of the three instances, only (3) seems to com-
municate positive emotion. In (1), ?play? is used as
a noun rather than the expected verb, while in (2),
?play? is a verb but it is used in a sense that is not
directly associated with positive emotion. (1) and
(2) demonstrate how lexical ambiguities (i.e. multi-
ple parts-of-speech or word senses) can affect accu-
racy of words in a lexicon. Additionally, even when
appearing as the expected part of speech and word
sense, signal from a word may change due to its con-
text, such as being within the scope of a negation as
in (4), or describing something desired as in (5).
4. ...all work no play :-(
5. i sure wish i had about 50 hours a day to play
cod
Our goal is to characterize the errors of the widely
used word count approach, and show that such lex-
ica can be significantly improved by employing an
ambiguity metric to refine such lexica. Rather than
work on a new method of measuring psychological
states, we work within the bounds of word count and
ask how accurate it is and whether we can improve
it without sacrificing its simplicity and scalability.
We attempt to reduce the erroneous signal of
the word count approach while maintaining legiti-
mate signal simply by refining the lexicon. In other
words, we would like to move closer to the goal in
Figure 1, by eliminating words that often carry er-
roneous signal such as ?play?, and keeping words
which often carry the sought-after signal, such as
?cheerful?. The difficulty in doing this is that we do
not have the data to tell us which words are most
likely to carry signal (even if we had such data we
would like to develop a method that could be applied
to any newly created lexica). Instead we leverage
part-of-speech and word sense data to help us deter-
mine which words are lexically ambiguous.
Figure 1: The relationship between text expressing posi-
tive emotion (POSEMO) and text containing LIWC terms
for POSEMO.
Our approach of eliminating ambiguous words
increases the precision at the expense of recall, a
reasonable trade-off in social media where we are
working with millions or even billions of word in-
stances. Additionally, it is minimally-supervised, in
that we do not require training data on human-state;
instead we use existing hand-labeled corpora, such
as SemCor (Miller et al, 1993), for word sense in-
formation. Not requiring training data also means
our refinement is flexible; it can be applied to mul-
tiple domains and lexica, it makes few assumptions
that might introduce problems of over-fitting, and it
is parsimonious in that it merely improves an estab-
lished approach.
This paper makes two primary contributions: (1)
an analysis of the types of errors common for the
word count approach (Section 3), and (2) a general
method for refining psychosocial lexica based on the
ambiguity of words (Section 4). Before describing
these contributions, we discuss related work, mak-
ing the case for using social media in social science
and surveying some work in computational linguis-
tics. We then evaluate both the original LIWC lex-
icon and our refinement of it against human judg-
ments of expression of positive and negative emo-
tions on hand-annotated Facebook posts, and show
the benefit of lexicon refinement for estimating well-
being over time for large aggregates of posts. Fi-
nally, we discuss the implications of our work and
possible future directions.
297
2 Background
Compared to traditional approaches in the social sci-
ences, large scale analysis of social media is cheap,
near real-time, unobtrusive, and gives high cover-
age. We outline these advantages below.
Inexpensive Extracting information from sources
such Facebook and Twitter is vastly cheaper than the
more conventional polling done by companies such
as Gallup ? and by many social science researchers.
Social media data does not require phone calls to be
made or doors to be knocked on. For example, a rep-
resentative survey asking 1,000 people by a leading
polling company costs to the order of $10,0001. In
contrast, once the software exists, social media data
from tens of millions of users can be obtained and
analyzed at a fraction of the cost.
Temporal Resolution Much of the attraction of
social media stems from the fact that it captures
a written live stream of collective thought. When
Google relied on search queries to monitor health-
seeking behavior to predict influenza epidemics, the
reporting lag was a mere day, whereas traditional
CDC surveillance systems take 1-2 weeks to pub-
lish their data (Ginsberg et al, 2009). Infrastructure
based on social media and Internet use data allows
reporting and analysis systems with little to no re-
porting lag. Additionally, traditional survey designs
are typically only designed to assess psychological
states at a given point in time.
Unobtrusive Estimation Traditional self-report
survey approaches, even those implemented on the
web, suffer from social desirability, priming, and
other biases. For example, Kahneman et al (Kah-
neman et al, 2006) found that the order in which
questions are asked on questionnaires can determine
how they are answered. By looking directly into the
social worlds, many of these self-report biases can
be avoided. The traces of human interactions in so-
cial media represent the goings-on in their original
ecologies of meaning and signification. This ap-
proach diminishes the inferential distance between
the context of the phenomena and the context of
measurement ? and thus decreases the room for sys-
tematic distortion of signal.
1Gallup, Personal correspondence.
2.1 The Word Count Approach
As previously noted, the word count approach is
most often used by social scientists through the tool
known as Linguistic Inquiry and Word Count or
LIWC (Pennebaker et al, 2007). The LIWC2007
dictionary is composed of almost 4,500 words and
word stems organized across one or more word cat-
egories, including 406 positive emotion words and
499 negative emotion words. When long form texts
are analyzed with LIWC, the program simply re-
turns the percentages of words belonging to the dif-
ferent analytical categories ? the simplicity of this
approach makes it popular with non-technical social
scientists.
LIWC?s positive and negative emotion lexica have
recently begun to be used on ?short form? writing in
social media. For example, Golder and Macy (2011)
used LIWC to study diurnal and seasonal variation
in mood in a collection of 400 million Twitter mes-
sages. Kramer (2010) proposed the ?Gross National
Happiness? index and Kivran-Swaine and Naaman
(2011) examined associations between user expres-
sions of positive and negative emotions and the size
and density of social networks. A comprehensive
review can be found in Tausczik and Pennebaker
(2010).
To our knowledge there is only one work which
has evaluated LIWC?s accuracy over social media.
Bantum and Owen (2009) evaluated LIWC on a set
of posts to an Internet-based breast cancer support
group. By annotating expression of emotion within
this text, they were able to produce accuracy figures
of sensitivity (much like recall) and predictive va-
lidity (precision). Sensitivity measured how often
a word (in context) expressing positive or negative
emotion was captured by LIWC. Predictive validity
measured how often a word (in context) captured
by LIWC as measuring positive or negative emotion
was indeed expressing positive or negative emotion.
While they found a recall of 0.88, the precision was
only 0.31 ? that is, only 31% of instances contain-
ing words indexed by LIWC actually conveyed the
associated emotion. We contend that this is a major
drawback for applying LIWC to social media, be-
cause while it is not important to catch every expres-
sion of emotion out of a million Tweets, it is impor-
tant that when something is captured it is an accurate
298
estimate of the true state.
2.2 Related Work in Computational
Linguistics
Researchers have been exploring the use of lexica
that define the subjective orientation of words for
tasks such as sentiment or subjectivity analysis. A
common weakly-supervised approach starts with a
small set of sentiment knowledge (seed words as-
sociated with a given sentiment) and expands the
words into a large lexicon (Hatzivassiloglou and
McKeown, 1997; Kamps and Marx, 2002; Kim and
Hovy, 2004; Kanayama and Nasukawa, 2006; Bac-
cianella et al, 2010). We take a different approach.
Rather than expanding lexica, we start with a large
set of words and refine the set. The refinement in-
creases precision at the cost of recall, which is a
reasonable exchange when we are looking at mil-
lions or even billions of word instances. Standard
applications of sentiment analysis, such as annotat-
ing movie reviews, may not be as inclined to skip
instances, since they want to make predictions for
items which have very few reviews.
Another line of work in sentiment analysis has
created lexicons using supervised learning. One of
the first works to do so was by Pang and colleagues
(2002), who used data including author ratings of
reviews, such as IMDB movie reviews. The author
ratings become training data for sentiment classifi-
cation. Pang et al showed that human-created lexi-
cons did not perform as well as lexicons based on
simple word statistics over the training data. In-
terestingly, they found that words like ?still? were
most predictive of positive movie reviews, and that
punctuation marks of ?!? and ??? were strong signs
of negative movie reviews. Unfortunately, training
data for subjective well-being or happiness is not
yet available, preventing the use of such supervised
learning methods. Additionally, this work seeks to
experiment within the bounds of what social sci-
entists are in fact using (with publications in high-
impact venues such as Science). We thus take a dif-
ferent approach, and automatically improve human
created lexicons.
Wiebe and Cardie (2005) generalized the task of
sentiment analysis to that of discovering subjectiv-
ity such as ?opinions, emotions, sentiments, specu-
lations, evaluations, etc.?. More recently, Wilson et
POSEMO NEGEMO
term frequency term frequency
like 774,663 hate 167,109
love 797,833 miss 158,274
good 571,687 bad 151,496
friend* 406,568 bore* 140,684
happy 384,797 shit* 114,923
LOL 370,613 hurt* 98,291
well* 284,002 craz* 94,518
great 263,113 lost 94,059
haha* 240,587 damn* 93,666
best 227,381 fuck 90,212
better 212,547 stupid* 85,587
fun 216,432 kill* 83,593
please* 174,597 hell 80,046
hope 170,998 fuckin* 79,959
thank 161,827 wrong* 70,714
Table 1: Most frequent POSEMO and NEGEMO terms in
LIWC in the 12.7 million Facebook posts. ?*? indicates a
wildcard, so that ?well*? matches ?wellness?.
al. (2009) contended that the context may neutralize
or change the polarity of the subjective orientation
of a word. It is difficult to determine where concepts
of happiness such as quality of relationships or de-
gree of achievement in life fit in with subjectivity.
Thus, we do not claim to be measuring subjectivity
and instead we use the general term of ?psychologi-
cal state?, referring to ?the way something [a person]
is with respect to its main attributes? (Miller, 1993).
To the best of our knowledge, while part-of-
speech tagging and word sense disambiguation are
staple tasks in the computational linguistics commu-
nity, the utility of a lexical ambiguity metric has yet
to be explored.
3 Annotation and Analysis of Errors from
the Word Count Method
One objective of our work is to document and de-
scribe how often different types of errors occur when
using the word count approach on social media. To
do this, we first judged a sample of 1,000 instances
of LIWC terms occurring in Facebook posts to indi-
cate whether they contribute signal towards the as-
sociated LIWC category (i.e. positive emotion). We
then took instances that were deemed to carry erro-
neous signal and annotated them with a label for the
299
category agreement instances base rate
POSEMO 0.742 500 .654
NEGEMO 0.746 500 .697
TOTAL 0.744 1,000 .676
random 0.343 - -
Table 2: Inter-annotator agreement over 1,000 instances
of LIWC terms in Facebook posts. Base rate is the aver-
age of how often an annotator answered true.
type of signal error. This section describes the pro-
cess we used in generating these annotations and the
results we found.
3.1 Annotation Process
Annotating social media instances of lexica terms
provides insight into how well the word count ap-
proach works, and also yields a ?ground truth? for
evaluating our lexicon refinement methods. We ran-
domly selected for labeling a sample of 1,000 sta-
tus updates containing words from a given lexicon
drawn from a collection of 12.7 million Facebook
status updates provided by the Cambridge myPer-
sonality project (Kosinski and Stillwell, 2012).
We used terms from the LIWC positive emotion
(POSEMO) and negative emotion (NEGEMO) lex-
ica, which are the same lexica used by the works of
Kramer (2010), Kivran-Swaine and Naaman (2011),
and Golder and Macy (2011). Table 1 lists the
most frequent POSEMO and NEGEMO terms in our
Facebook sample.
As mentioned above, we did two types of annota-
tions. First, we judged whether each given instance
of a word conveyed the correct associated type of
emotion. The second task took a sample of instances
judged to have incorrect signal and labeled them
with a reason for the error; We refer to this as signal
error type.
For the first task, we had three human judges inde-
pendently evaluate the 1,000 status update instances
as to whether they were indeed correct signal. The
question the judges were told to answer was ?Does
the word contribute to the associated psychological-
state (POSEMO or NEGEMO) within the sentence
it appears??. In other words, ?would the sentence
convey less [positive emotion or negative emotion]
without this word??. Subjective feedback from the
judges indicated that it was often difficult to make
a decision, so we used three judges per instance. In
the case of conflict between judges, the ?correct? la-
bel for validation of the refined lexicon was defined
to be the majority vote. A sampling of Facebook sta-
tuses demonstrates a mixed picture of relevance for
the unrefined LIWC dictionaries:
1. has had a very good day (?good? - POSEMO)
2. is so very bored. (?bore*? - NEGEMO)
3. damn, that octopus is good, lol (?damn? -
NEGEMO)
4. thank you for his number (?numb*? -
NEGEMO)
5. I got pranked sooooo bad (?bad? - NEGEMO)
6. don?t be afraid to fail (?afraid? - NEGEMO)
7. I wish I could . . . and we could all just be happy
(?happy? - POSEMO)
Some posts clearly use positive or negative lexicon
words such as (1) and (2). Curse words can signify
negative emotion or emphasize the opposite state as
in (3), which is clearly emphasizing positive emo-
tion here. Example (5) demonstrates the word sense
issue we discussed previously. Words with wild-
cards that expand into other words with different
meanings can be particularly problematic, as the ex-
panded word can be far more frequent ? and very
different in meaning ? from the original word. For
example, ?numb*? matches ?number? in 4.
A different problem occurs when the context
of the word changes its implication for the emo-
tional state of the writer. This can either occur
through negation such as in (6) where ?afraid? sig-
nals NEGEMO, but is negated with ?don?t? or the
signal can be changed indirectly through a variety of
words indicating that the writer desires (and hence
lacks) the state, as in (7) where someone is wishing
to be ?happy?.
Table 2 shows the agreement between an-
notators calculated as
?
i agree(A
(i)
1 ,A
(i)
2 ,A
(i)
3 )
1,000 , where
agree(A1, A2, A3) was 1 when all three annota-
tions matched and 0 otherwise. Given the aver-
age positive base rate across annotators was 0.676
the chance that all three reviewers agree accord-
ing to chance (random agreement) is calculated as
300
category precision instances
POSEMO 67.9% 500
NEGEMO 72.8% 500
both 70.4% 1,000
Table 4: Accuracy of LIWC POSEMO and NEGEMO
lexica over Facebook posts.
0.6763+(1?0.676)3 = 0.343, the probability of all
three answering yes plus the probability of all three
answering no.
For the second task, we selected 100 instances
judged to be incorrect signal from the first task, and
labeled them according to the best reason for the
mistake. This task required more linguistic exper-
tise and was performed by a single annotator. La-
bels and descriptions are given in Table 3, which
breaks down the cases into lexical ambiguity, direct
or indirect negation, and other reasons such as the
stemming issue (stem plus wildcard expanding into
words indicating a different (or no) emotional state).
3.2 Analysis of Errors
Before discussing the types of errors we found when
using the word count approach, we examine LIWC?s
overall accuracy on our dataset. Table 4 shows the
precision broken down for both the positive emotion
(POSEMO) and the negative emotion (NEGEMO)
lexica. We see that the precision for NEGEMO is
slightly higher than POSEMO, indicating the terms
in that category may be more likely to indicate their
associated state.
Although the overall accuracy seems decent, one
should keep in mind our subjective judgement crite-
ria were quite tolerant, allowing any amount of con-
tribution of the corresponding signal to be consid-
ered accurate. For example, a salutation like ?Happy
New Year? was judged to be a correct use of ?happy?
to signal POSEMO, even though it clearly does not
have as strong a signal as someone saying ?I feel
deliriously happy?.
Frequencies of signal errors are given in Table
5. The most common signal error was wrong word
sense, where the word did not signal emotional
state and some other sense or definition of the word
was intended (e.g. ?u feel like ur living in a mu-
sic video?; corresponding to the sense ?to inhabit?
rather than the intended sense, ?to have life; be
category label frequency
Lexical Ambiguity
Wrong POS 15
Wrong WS 38
Signal Negation
Strict Negation 16
Desiring 6
Other
Stem Issue 5
Other 24
Table 5: Frequency of the signal error types.
alive? (Miller, 1993)). Other common signal errors
include strict negation where the word is canceled
out by a clear negative quantifier (e.g. ?Don?t be
afraid to fail?) and wrong part of speech where the
word is signaling a different part of speech than the
emotion (e.g. ?well, we cant afford to go to NYC?).
There were also various other signal error types that
include stem issues where the stem matched clearly
unintended words, desiring statuses where the status
is commenting on wanting the emotion instead of
experiencing it and other less prevalent issues such
as non-English language post, memes, or clear sar-
casm.
4 Method for Refining Lexica
The idea behind our refinement method is to remove
words that are likely to carry erroneous signal about
the underlying state or emotion of the person writ-
ing the tweet or Facebook post.2 We do so in an
indirect fashion, without actually using training data
of which posts are, in fact indicative of positive or
negative emotion. Instead, we focus on reducing er-
rors that are due to lexical ambiguity. By remov-
ing words that are often used with multiple parts of
speech or multiple senses, we can tilt the balance to-
ward precision at some cost in recall (losing some
signal from the ambiguous words). This makes the
word count approach more suitable for use in the
massive corpora afforded by social media.
4.1 Lexical Ambiguity
We address lexical ambiguity at the levels of both
part of speech (POS) and word sense. As a metric
of inverse-ambiguity, we determine the probability
that a random instance is the most frequent sense
(mfs) of the most frequent part of speech (mfp) of
2Refinement tool is available at wwbp.org.
301
category label description examples
Lexical Ambiguity
Wrong POS Not a valid signal because it is
the wrong POS
so everyone should come to the
play tomorrow...
Wrong WS Not a valid signal because it is
the wrong word sense (includes
metaphorical senses)
Does anyone what type of file i
need to convert youtube videos
to play on PS3???
Signal Negation
Strict Negation Within the scope of a negation,
where there is a clear negative
quantifier
...all work no play :-(
Desiring Within the scope of a desire /
wishing for something
i sure wish i had about 50 hours
a day to play cod
Other
Stem Issue Clearly not intended to be
matched with the given stem
numb* for NEGEMO match-
ing number
Other Any other issue or difficult to
classify
Table 3: Signal error types.
the word, denoted TSP (for top sense probability).
Given a wordw, we consider all parts of speech ofw
(POS(w)) and all senses for the most frequent part
of speech (senses(mfp(w))):
pmfp(w) =
max
[wpos?POS(w)]
fp(wpos)
?
wpos?POS(w)
fp(wpos)
pmfs(w) =
max
[wsense?senses(mfp(w))]
fs(wsense)
?
wsense?senses(mfp(w))
fs(wsense)
TSP (w) = (pmfp(w) ? pmfs(w))
2 (1)
Here, fp and fs represent the frequencies of a cer-
tain part-of-speech and a certain sense of a word,
respectively. This is the squared-probability that an
instance of w is the top sense ? the most-frequent
part-of-speech and the most-frequency sense of that
part-of-speech. The probability is squared because
both the word in the lexicon and the word occurring
in context should be the top sense (two independent
probabilities: given an instance of a word in a cor-
pus, and another instance of the word in the lexicon,
what is the probability that both are the top POS
and sense?). Frequency data is provided for parts-
of-speech from the Google N-Grams 2.0 (Lin et al,
2010) and for word senses from SemCor (Miller et
al., 1993). This aspect of the refinement is inspired
by the most frequent sense heuristic for word sense
disambiguation (McCarthy et al, 2004; Yarowsky,
1993), in which the sense of a word is chosen with-
out regard to the context, but rather is simply based
on the frequencies of senses in corpora. In our case,
we restrict ourselves this way in order for the appli-
cation of the lexicon to remain unchanged.
For some words, we were unable to find sense fre-
quency data. We decided to keep such terms, on
the assumption that a lack in available frequency in-
formation implies that the word is not very ambigu-
ous. Many of these terms include Web speak such as
?haha? or ?lol?, which we believe can carry a strong
signal for positive and negative emotion.
Lastly, since TSP is only a metric for the in-
verse ambiguity of a word, we must apply a thresh-
old to determine which words to keep. We denote
this threshold as ?, and the description of the refined
lexicon for a category, cat, is below.
lex?(cat) = {w|w ? cat ? TSP (w) > ?}
4.2 Handling Stems
Some lexica, such as the LIWC dictionary, include
word stems that are intended to match multiple
forms of a word. Stems are marked by the suffix
?*?. LIWC describes the application of stems as fol-
lows ?the asterisk, then, denotes the acceptance of
all letters, hyphens, or numbers following its ap-
302
lex cat prec size
full
POSEMO 67.9% 500
NEGEMO 72.8% 500
both 70.4% 1,000
lex0.10
POSEMO 70.9% 392
NEGEMO 71.6% 423
both 71.3% 815
lex0.50
POSEMO 75.7% 239
NEGEMO 78.9% 232
both 77.3% 471
lex0.90
POSEMO 72.5% 109
NEGEMO 78.1% 128
both 75.5% 237
Table 6: Precision (prec) and instance subset size (size)
of refinements to the LIWC POSEMO and NEGEMO lex-
ica with various ? thresholds (0.10, 0.50, 0.90)
pearance.?3 This presents a problem because, while
the creators of such lexica obviously intended stems
to match multiple forms of a word, stems also often
match completely different words, such as ?numb*?
matching ?number? or ?won*? matching ?won?t?.
We identified how often unintended matches hap-
pen in Section 3. Finding that the stemming issues
were not the biggest problem, here, we just describe
how they fit into our lexical ambiguity metric, rather
than describe a technique to rid the lexicon of stem-
ming problems. One approach might be to deter-
mine how ambiguous a stem is ? i.e. determine
how many words, parts-of-speech, and senses a stem
could be expanded into, but this ignores the fact that
the dictionary creators obviously intended the stem
to match multiple words. Instead, we expand stems
into all words that they match and replace them into
the lexica.
We base our expansion on the actual terms used
in social media. We find all words matching stems
among 1 million randomly selected Twitter mes-
sages posted over a 6-month period (August 2009
- February 2010), and restrict to those occurring at
least 20 times. Then, each word stem in the lexicon
is replaced with the expanded set of matching words.
Figure 2: The relationship between precision and size
when increasing the TSP threshold (?).
5 Evaluation
We evaluate our refinement by comparing against
human judgements of the emotion conveyed by
words in individual posts. In the case of hu-
man judgements, we find that the subset of human-
annotated instances matching the refined lexica are
more accurate than the complete set.
In section 3 we discussed the method we used to
judge instances of LIWC POSEMO and NEGEMO
words as to whether they contributed the associated
affect. Each of the 1,000 instances in our evaluation
corpus were judged three times such that the major-
ity was taken as truth. In order to validate our refined
lexica, we find the accuracy (precision) of the subset
of instances which contain the refined lexica terms.
Table 6 shows the change in precision when us-
ing the refined lexica. size represents the number of
instances from the full evaluation corpus matching
words in the refined lexica. One can see that ini-
tially precision increase as the size becomes smaller.
This is more clearly seen in Figure 2. As discussed
in the method section, our goal with the refine-
ment is improving precision, making lexica more
suitable to applications over massive social media
where one can more readily afford to skip instances
(i.e. smaller size) in order to achieve more accu-
racy. Still, removing more ambiguous words does
3?How it works?: http://www.liwc.net/howliwcworks.php
303
not guarantee improved precision at capturing the
intended psychological state; it is possible that that
all senses of an ambiguous word do in fact carry in-
tended signal or that the intended sense a low ambi-
guity word is not the most frequent.
Our maximum precision occurs with a threshold
of 0.50, where things somewhat level-out. This rep-
resents approximately a 23% reduction in error, and
verifies that we can increase precision through the
automatic lexicon refinement based on lexical ambi-
guity.
6 Conclusions
Social scientists and other researchers are starting
to measure psychological states such as happiness
through text in Facebook and Twitter. We have
shown that the widely used word count method,
where one simply counts occurrences of positive or
negative words, can often produce noisy and inaccu-
rate estimates of expressions of psychological states.
We characterized and measured the frequency of
different types of errors that occur using this ap-
proach, and found that when counting words without
considering context, it is lexical ambiguities (unin-
tended parts-of-speech or word senses) which cause
the most errors. We proposed a method for refin-
ing lexica by removing those words most likely to
be ambiguous, and showed that we can significantly
reduce error as measured by human judgements.
Acknowledgments
Support for this research was provided by the
Robert Wood Johnson Foundation?s Pioneer Portfo-
lio, through a grant to Martin Seligman, ?Exploring
Concepts of Positive Health?. We thank the review-
ers for their constructive and insightful comments.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
Valletta, Malta, may. European Language Resources
Association (ELRA).
Erin O.C. Bantum and J.E. Owen. 2009. Evaluating the
validity of computerized content analysis programs for
identification of emotional expression in cancer narra-
tives. Psychological assessment, 21(1):79.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2010.
Twitter mood predicts the stock market. Computer and
Information Science, 1010:1?8.
Michael A. Cohn, M.R. Mehl, and J.W. Pennebaker.
2004. Linguistic markers of psychological change sur-
rounding september 11, 2001. Psychological Science,
15(10):687.
Peter Sheridan Dodds, Kameron Decker Harris, Isabel M
Kloumann, Catherine A Bliss, and Christopher M
Danforth. 2011. Temporal patterns of happiness and
information in a global social network: Hedonomet-
rics and twitter. Diversity, page 26.
Jeremy Ginsberg, M.H. Mohebbi, R.S. Patel, L. Bram-
mer, M.S. Smolinski, L. Brilliant, et al 2009. De-
tecting influenza epidemics using search engine query
data. Nature, 457(7232):1012?4.
Scott A. Golder and M.W. Macy. 2011. Diurnal and
seasonal mood vary with work, sleep, and daylength
across diverse cultures. Science, 333(6051):1878?
1881.
Louis A. Gottschalk and RJ Bechtel. 1998. Psychiatric
content analysis and diagnosis (pcad2000).
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Annual Meeting of the Association for Com-
putational Linguistics, pages 174?181.
Daniel Kahneman, A.B. Krueger, D. Schkade,
N. Schwarz, and A.A. Stone. 2006. Would you
be happier if you were richer? a focusing illusion.
Science, 312(5782):1908.
Jaap Kamps and Maarten Marx. 2002. Words with atti-
tude. In 1st International WordNet Conference, pages
332?341, Mysore, India.
Sepandar D. Kamvar and J. Harris. 2011. We feel fine
and searching the emotional web. In Proceedings
of the fourth ACM international conference on Web
search and data mining, pages 117?126. ACM.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 355?363, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Funda Kivran-Swaine and M. Naaman. 2011. Network
properties and social sharing of emotions in social
awareness streams. In Proceedings of the ACM 2011
304
conference on Computer supported cooperative work,
pages 379?382. ACM.
Michal. Kosinski and David J. Stillwell. 2012.
mypersonality research wiki. mypersonality project.
http://www.mypersonality.org/wiki/.
Adam D.I. Kramer. 2010. An unobtrusive behavioral
model of gross national happiness. In Proceedings of
the 28th international conference on Human factors in
computing systems, pages 287?290. ACM.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kaufman.
George A. Miller. 1993. Five papers on wordnet. Tech-
nical Report, Princeton University.
Greg Miller. 2011. Social scientists wade into the tweet
stream. Science, 333(6051):1814?1815.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Word Journal Of The International Lin-
guistic Association.
James W. Pennebaker, C.K. Chung, M. Ireland, A. Gon-
zales, and R.J. Booth. 2007. The development and
psychometric properties of liwc2007. Austin, TX,
LIWC. Net.
Yla R. Tausczik and J.W. Pennebaker. 2010. The psy-
chological meaning of words: Liwc and computerized
text analysis methods. Journal of Language and So-
cial Psychology, 29(1):24.
Janyce Wiebe and Claire Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. lan-
guage resources and evaluation. In Language Re-
sources and Evaluation.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35:399?433, September.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the workshop on Human Language
Technology, HLT ?93, pages 266?271, Stroudsburg,
PA, USA. Association for Computational Linguistics.
305
A Model for Composing Semantic Relations
Eduardo Blanco and Dan Moldovan
Human Language Technology Research Institute
The University of Texas at Dallas
{eduardo,moldovan}@hlt.utdallas.edu
Abstract
This paper presents a model to compose semantic relations. The model is independent of any
particular set of relations and uses an extended definition for semantic relations. This extended def-
inition includes restrictions on the domain and range of relations and utilizes semantic primitives
to characterize them. Primitives capture elementary properties between the arguments of a relation.
An algebra for composing semantic primitives is used to automatically identify the resulting rela-
tion of composing a pair of compatible relations. Inference axioms are obtained. Axioms take as
input a pair of semantic relations and output a new, previously ignored relation. The usefulness of
this proposed model is shown using PropBank relations. Eight inference axioms are obtained and
their accuracy and productivity are evaluated. The model offers an unsupervised way of accurately
extracting additional semantics from text.
1 Introduction
Semantic representation of text is an important step toward text understanding, performing inferences
and reasoning. Potentially, it could dramatically improve the performance of several Natural Language
Processing applications.
Semantic relations have been studied in linguistics for decades. They are unidirectional underlying
connections between concepts. For example, the sentence The construction slowed down the traffic
encodes a CAUSE and detecting it would help answer the question Why is traffic slower?
In Computational Linguistics, there have been several proposals to detect semantic relations. Current
approaches focus on a particular set of relations and given a text they output relations. There have
been competitions aiming at detecting semantic roles (i.e., relations between a verb and its arguments)
(Carreras and Ma`rquez, 2005), and between nominals (Girju et al, 2007; Hendrickx et al, 2009).
In this paper, we propose a model to compose semantic relations to extract previously ignored rela-
tions. The model allows us to automatically obtain inference axioms given a set of relations and is not
coupled to any particular set. Axioms take as their input semantic relations and yield a new semantic
relation as their conclusion.
Consider the sentence John went to the shop to buy flowers. Figure 1 shows semantic role annotation
with solid arrows. By composing this basic annotation with inference axioms, one can obtain the relations
shown with discontinuous arrows: John had the intention to buy, the buying event took place at the shop
and John and the flowers were at some point in the shop.
John
AGENT
99
AT-LOCe b ++\ Y
went
AT-LOC
77to the shop to buy
PURPOSE
cc
AT-LOC
T
X[tt cf
INTENT
N
P
S
U
X
Z]
xx
bd
g
i
l
n
q
flowers
THEME
ff
AT-LOC
Q
S
VX
Z
ww
df
h
k
m
Figure 1: Semantic representation of the sentence John went to the shop to buy flowers.
45
2 Semantic Relations
Semantic relations are the underlying relations between concepts expressed by words or phrases. In other
words, semantic relations are implicit associations between concepts in text.
In general, a semantic relation is defined by stating the kind of connection linking two concepts. For
example, Hendrickx et al (2009) loosely define ENTITY-ORIGIN as an entity is coming or is derived from
an origin (e.g., position or material) and give one example: Earth is located in the Milky Way. We find
this kind of definition weak and prone to confusion.
Following Helbig (2005), we propose an extended definition for semantic relations, including seman-
tic restrictions for its domain and range. For example, DOMAIN(AGENT) must be an animate concrete
object and RANGE(AGENT) must be a situation.
Moreover, we propose to characterize relations by semantic primitives. Primitives indicate if a certain
property holds between the arguments of a relation. For example, the primitive temporal indicates if the
first argument must happen before the second in order for the relation to hold. This primitive holds for
CAUSE (a cause must precede its effect) and it does not apply to PART-WHOLE since the later relation
does not consider time.
Besides having a better understanding of each relation, this extended definition allows us to create a
model that automatically obtains inference axioms for composing semantic relations. The model detects
possible combinations of relations and identifies the conclusion of composing them.
Formally, we represent a relation R as R(x, y), where R is the relation type and x and y are the first
and second argument respectively. R(x, y) should be read x is R of Y. DOMAIN(R) and RANGE(R) are the
sorts of concepts that can be part of the first and second argument respectively. Any ontology can be used
to define domains and ranges, e.g., Helbig (2005) defined one to define a set of 89 relations. Primitives
are represented by an array PR of length n, where n is the number of primitives and P iR indicates the
value R takes for the ith primitive.
The inverse of R is denoted R?1 and can be obtained by simply switching the arguments of R. Given
R(x, y), R?1(y, x) always holds. We can easily define R?1 given the definition for R: DOMAIN(R?1) =
RANGE(R), RANGE(R?1) = DOMAIN(R), and PR?1 is defined according to the fourth column of Table
1 for each primitive, i.e., ?i ? [1, n] : P iR?1 = Inverse(P iR).
2.1 Semantic Primitives
Relation primitives capture deep characteristics of relations. Huhns and Stephens (1989) define them as:
They [primitives] are independently determinable for each relation and relatively self-explanatory.
They specify a relationship between an element of the domain and an element of the range
of the semantic relation being described.
Relation primitives are fundamental properties that cannot be explained using other primitives; they are
elemental. They specify basic attributes of a relation by stating if a particular property must hold by
definition between the domain and range.
Each relation takes a value for each primitive from the set V = {+,?, 0}, where ?+? indicates that
the property holds, ??? that it does not hold and ?0? that it does not apply. For example, the primitive
volitional indicates if a relation requires volition between domain and range. AGENT takes as value +
for this primitive and PART-WHOLE takes 0.
Primitives complement the definition of a relation by stating if a particular property holds between its
arguments. They help to understand the inter-relation differences and clustering relations. Primitives can
be used as conditions to be fulfilled in order to determine if a potential relation holds. They are general
enough to be determined for a relation, not a particular instantiation. In other words, they state properties
that hold for all instances of a relation by definition.
Our set of primitives (Table 1) is inspired on previous work in Knowledge Bases (Huhns and Stephens,
1989). We only select from them useful primitives for our purpose and add more primitives. The
additional primitives are justified by the fact that we aim at combining relations capturing semantics
46
No. Primitive Description Inverse Ref.
1 Composable Relation can be meaningfully composed with other relations
due to their fundamental characteristics
same [3]
2 Functional Domain is in a specific spatial or temporal position with re-
spect to the range in order for the connection to exist
same [1]
3 Separable Domain can be temporally or spatially separated from the
range, and can thus exist independently of the range
same [1]
4 Temporal Domain temporally precedes the range opposite [2]
5 Connected Domain is physically or temporally connected to the range;
connection might be indirect.
same [3]
6 Intrinsic Relation is an attribute of the essence/stufflike nature of the
domain or range
same [3]
7 Volitional Relation requires volition between the arguments same -
8 Fully Implicational The existence of the domain implies the existence of the
range
opposite -
9 Weakly Implicational The existence of the domain generally implies the existence
of the range
opposite -
Table 1: Primitives for characterizing semantic relations, values for the inverse relation and references. In the fifth
column, [1] stands for Winston et al (1987), [2] for Cohen and Losielle (1988) and [3] for Huhns and Stephens
(1989). ?-? indicates new primitive.
1: Composable
R2
R1 - 0 +
- ? 0 ?
0 0 0 0
+ ? 0 +
2: Functional
R2
R1 - 0 +
- - 0 +
0 0 0 0
+ + 0 +
3: Separable
R2
R1 - 0 +
- - - -
0 - 0 +
+ - + +
4: Temporal
R2
R1 - 0 +
- - - ?
0 - 0 +
+ ? + +
5: Connected
R2
R1 - 0 +
- - - +
0 - 0 +
+ + + +
6: Intrinsic
R2
R1 - 0 +
- - 0 -
0 0 0 0
+ - 0 +
7: Volitional
R2
R1 - 0 +
- - 0 +
0 0 0 0
+ + 0 +
8: F Impl.
R2
R1 - 0 +
- - 0 -
0 0 0 0
+ - 0 +
9: W Impl.
R2
R1 - 0 +
- - 0 -
0 0 0 0
+ - 0 +
Table 2: Algebra for composing semantic primitives. Each cell of the ith table indicates P iR1 ? P iR2 .
from natural language. Whatever the set of chosen relations, it will describe the characteristics of
events (who/when/where/how something happened), which elements were involved, connections be-
tween events (e.g. CAUSE, CORRELATION). Time (whether an argument is guaranteed to happen before
than the other), space and volition (whether or not there must be volition between the arguments) also
play an important role.
The fourth column in Table 1 indicates the value of the primitive for the inverse relation. Same means
the inverse relation takes the same value, opposite means it takes the opposite. The opposite of ? is +,
the opposite of ? is +, and the opposite of 0 is 0.
For example, PAGENT = {+,+,+, 0,?,?,+, 0, 0}, indicating that P 5AGENT = ? and P 7AGENT = +,
i.e., AGENT(x, y) does not require x and y to be connected and it requires volition between the arguments.
Note that PAGENT?1 = PAGENT.
2.2 An Algebra for Composing Semantic Relations
The key to automatically obtaining inference axioms is the ability to know beforehand the result of
composing semantic primitives using an algebra. This way, one can identify prohibited combinations of
relations and determine conclusions for the composition of valid combinations.
47
Given P iR1 and P
i
R2 , i.e., the values of R1 and R2 for a primitive pi, we define an algebra that indicates
the result of composing them (i.e., P iR1 ? P iR2). Composing two primitives can yield three values: +, ?
or 0, indicating if the primitive holds, does not hold or does not apply to the composition of R1 and R2.
Additionally, the composition can be prohibited, indicated with ?. After composing all the primitives
for R1 and R2, we obtain the primitives values for the composition of R1 and R2 (i.e., PR1 ? PR2).
We define the values for the composition using a table for each primitive. Table 2 depicts the whole
algebra. The ith table indicates the rules for composing the ith primitive. For example, regarding the
intrinsic primitive, we have the following rules:
? If both relations are intrinsic, the composition is intrinsic;
? else if intrinsic does not apply to either relation, the primitive does not apply to the composition;
? else, the composition is not intrinsic.
Other rules stated by the algebra are: (1) two relations shall not compose if they have different
opposite values for the primitive temporal; (2) the composition of R1 and R2 is not separable if either
relation is not separable; and (3) if either R1 or R2 are connected, then the composition is connected.
3 Necessary Conditions for Composing Semantic Relations
In principle, one could define axioms for every single possible combination of relations. However, there
are two necessary conditions in order to compose R1 and R2:
1. They have to be compatible. A pair of relations is compatible if it is possible, from a theo-
retical point of view, to compose them. Formally, R1 and R2 are compatible iff RANGE(R1) ?
DOMAIN(R2) 6= ?.
2. A third relation R3 must fit as conclusion, that is, ?R3 such that DOMAIN(R3)?DOMAIN(R1) 6= ?
and RANGE(R3) ? RANGE(R2) 6= ?.
Furthermore, PR3 must be compatible with the result of composing PR1 and PR2 .
It is important to note that domain and range compatibility is not enough to compose two relations.
For example, given KINSHIP(Mary, John) and AT-LOCATION(John, Dallas), no relation can be inferred
between Mary and Dallas.
4 Inference Axioms
An axiom is defined as a set of relations called premises and a conclusion. The composition operator ?
is the basic way of combining two relations to form an axiom. We denote an inference axiom as R1(x, y)
? R2(y, z) ? R3(x, z), where R1 and R2 are the premises and R3 the conclusion. In order to instantiate
an axiom the premises must have an argument in common, y.
In general, for n relations there are
(n
2
)
= n(n?1)2 different pairs. For each pair, taking into account
the two relations and their inverses, there are 4? 4 = 16 different possible combinations.
We note that R1 ? R2 = (R2?1 ? R1?1)?1, reducing the total number of different combinations to
10. Out of these 10, (1) 4 combine R1, R2 and their inverses (Table 3); (2) 3 combine R1 and its inverse;
and (3) 3 combine R2 and its inverse. The most interesting combinations to use as premises for an axiom
fall into category (1), since the other two can be resolved by the transitivity property of a relation and its
inverse. Therefore, for n relations there are 2n2+n potential axioms:
(n
2
)
?4+3n = 2?n(n?1)+3n =
2n2 ? 2n+ 3n = 2n2 + n.
4.1 An Algorithm for Obtaining Inference Axioms
Given a set of relations R defined using the extended definition, one can automatically obtain inference
axioms using the following steps for each pair of relations R1 ? R and R2 ? R, where R1 6= R2:
48
R1 ? R2 R1?1 ? R2 R2 ? R1 R2 ? R1?1
x
R1 //
R3 ?
?
?
?
?
?
?
?
y
R2

z
x
R3 ?
?
?
?
?
?
?
?
y
R2

R1oo
z
x
R2

R3
?
?
?
?
?
?
?
?
y
R1
// z
x
R3
?
?
?
?
?
?
?
?
R2

y z
R1
oo
Table 3: The four unique axioms taking as premises R1 and R2. R3 indicates the conclusion.
Role Primitive Role Primitive
C
om
po
sa
bl
e
Fu
nc
tio
na
l
Se
pa
ra
bl
e
Te
m
po
ra
l
C
on
ne
ct
ed
In
tr
in
si
c
V
ol
iti
on
al
Fu
lly
Im
pl
.
W
ea
kl
y
Im
pl
.
C
om
po
sa
bl
e
Fu
nc
tio
na
l
Se
pa
ra
bl
e
Te
m
po
ra
l
C
on
ne
ct
ed
In
tr
in
si
c
V
ol
iti
on
al
Fu
lly
Im
pl
.
W
ea
kl
y
Im
pl
.
ARG0 + + + 0 - - + 0 0 ARG0?1 + + + 0 - - + 0 0
ARG1 + - + 0 - - - 0 0 ARG1?1 + - + 0 - - - 0 0
MLOC + + 0 0 + - 0 0 0 MLOC?1 + + 0 0 + - 0 0 0
MCAU + + + + - + 0 + + MCAU?1 + + + - - + 0 - -
MTMP + + 0 0 + - 0 0 0 MTMP?1 + + 0 0 + - 0 0 0
MPNC + - + - - - - 0 - MPNC?1 + - + + - - - 0 +
MMNR + - + 0 - - + 0 0 MMNR?1 + - + 0 - - + 0 0
Table 4: Semantic Roles in PropBank, their inverses and their primitives.
Repeat Steps 1, 2 and 3 for (Ri, Rj) ? [(R1, R2), (R1?1, R2), (R2, R1), (R2, R1?1)]:
1. Domain and range compatibility
If RANGE(Ri) ? DOMAIN(Rj) = ?, break
2. Primitives composition
Using the algebra for composing semantic primitives, calculate PRi ? PRj
3. Conclusion match Repeat for R3 ? R
If DOMAIN(R3) ? DOMAIN(Ri) 6= ? and RANGE(R3) ? RANGE(Rj) 6= ?
and consistent(PR3 , PRi ? PRj ), then
inference axioms += Ri(x, y) ? Rj(y, z) ? R3(x, z)
The method consistent(P1, P2) is a simple procedure that compares the values assigned to each
primitive one by one. Two values for the same primitive are compatible unless they have different
opposites or either value is ??? (i.e., prohibited).
5 Case Study: PropBank
PropBank (Palmer et al, 2005) adds a layer of predicate-argument information, or semantic role labels,
on top of the syntactic trees provided by the Penn TreeBank. Along with FrameNet, it is the resource
most widely used for semantic role annotation.
PropBank uses a series of numeric core roles (ARG0 - ARG5) and a set of more general roles, ARGMs
(e.g. MTMP, MLOC, MMNR). The interpretation of the numeric roles is determined by a verb-specific
framesets, although ARG0 and ARG1 usually correspond to the prototypical AGENT and THEME. On the
other hand, the meaning of AGRMs generalize across verbs.
An example of PropBank annotation is the following: [Winston]ARG0 [procrastinated]rel [a lot]MADV
[due to his nervous demeanor]MCAU. Palmer et al (2005) discuss the creation of PropBank. For more
information about the semantics of each role, we refer the reader to the annotation guidelines1.
Since ARG2, AGR3, ARG4 and ARG5 do not have a common meaning across verbs, they become not
composable. For example, ARG2 is used for INSTRUMENT in the frameset kick.01 and for BENEFACTIVE
in the frameset call.02.
1http://verbs.colorado.edu/?m palmer/projects/ace/PBguidelines.pdf
49
R2
R1 a:
A
R
G
0?
1
b:
A
R
G
1?
1
c:
M
L
O
C
?
1
d:
M
C
A
U
?
1
e:
M
T
M
P
?
1
f:
M
P
N
C
?
1
g:
M
M
N
R
?
1
a: ARG0 = - - a - a -
b: ARG1 - = - - - b -
c: MLOC - - = c - c -
d: MCAU a - c = e - -
e: MTMP - - - e = e -
f: MPNC a b c - e = g
g: MMNR - - - - - g =
Table 5: Results after applying the steps depicted in Section 4.1 using PropBank semantic roles. A letter indicates
an inference axiom R1?R2 ? R3 by indicating the conclusion R3. ?-? indicates that the combination is not prohibited
but a relation compatible with PR1 ? PR2 could not be found; ?=? indicates that the cell corresponds to a relation
and its inverse.
The remaining labels (ARG0, ARG1 and all ARGMs) do generalize in meaning across verbs. Roles
MEXT, MDIS, MADV, MNEG, MMOD, MDIR, are not composable because they encode a very narrow
semantic connection. Manual examination of several examples leads to this conclusion.
Table 4 depicts the primitives for the roles which are composable and their inverses. Note that for
any two relations their primitives are different.
PropBank does not provide domains and ranges for its roles, although we can specify our own. We
do so by using the ontology defined by Helbig (2005). All relations in PropBank are denoted as R(x, y),
where x is an argument of y, and y is a verb. The range of all relations is a situation. The domain of
AGR0 and ARG1 are objects, the domain of MLOC and MTMP local and temporal descriptors respectively,
the domain of MMNR qualities or states, and the domain of MPNC and MCAU are situations.
5.1 Inference Axioms from PropBank
Out of the four possible axioms between any pair of relations (Table 3), the only way to compose two
relations from PropBank is by using as common argument y a verb. This restriction is due to the fact
that PropBank exclusively annotates relations between a verb and its arguments. Thus, the only possible
axiom for any pair of roles R1 and R2 is R1(x, y) ? R2?1(y, z) ? R3(x, z), where y is a verb.
Table 5 shows the eight inference axioms obtained after following the steps depicted in Section 4.1.
Note that the matrix is symmetric as stated by the property R1 ? R2 = (R2?1 ? R1?1)?1.
Some of the axioms obtained are:
? MCAU ? MLOC?1 ? MLOC?1, the location of a cause is the same than the location of its effect.
? MPNC ? ARG0?1 ? ARG0?1, the agent of an action is inherited by its purpose.
? MPNC ? MMNR?1 ? MMNR?1, the manner of an action is inherited by its purpose.
5.2 Evaluation
First, we evaluated all the instantiations of axiom MPNC ? MMNR?1 ? MMNR?1. This axiom can be
instantiated 237 times using PropBank annotation, yielding 189 new MANNER not present in PropBank.
The overall accuracy is 0.797, superior to state-of-the art semantic role labelers.
Second, we have evaluated the accuracy of the eight inference axioms (Table 5). Since PropBank is
a large corpus, the amount of instantiations found for all axioms is too large to be checked by hand. We
have manually evaluated the first 1,000 sentences that are an instantiation of any axiom. Since a sentence
may instantiate several axioms, we have actually evaluated 1,412 instantiations. The first 1,000 sentences
which are an instantiation of any axiom are found within the first 31,450 sentences in PropBank. Table
6 shows the number of roles PropBank annotates for these sentences.
50
Role No. Instances
CAUSE 421
PURPOSE 768
AGENT 22,525
THEME 29,738
AT-LOCATION 2,024
AT-TIME 5,743
MANNER 2,212
Table 6: Number of relations in PropBank for the first 31,450 sentences.
no heuristic with heuristic
No. Axiom No. Inst. Acc. Produc. No. Inst. Acc. Produc.
1 CAU ? AGT?1 ? AGT?1 201 0.40 0.89% 75 0.67 0.33%
2 CAU ? AT-L ? AT-L 17 0.82 0.84% 15 0.93 0.74%
3 CAU ? AT-T ? AT-T 72 0.85 1.25% 69 0.87 1.20%
1-3 CAU ? R2 ? R3 290 0.53 0.96% 159 0.78 0.53%
4 PRP ? AGT?1 ? AGT?1 375 0.89 1.66% 347 0.94 1.54%
5 PRP ? THM?1 ? THM?1 489 0.12 1.64% 87 0.65 0.29%
6 PRP ? AT-L ? AT-L 49 0.90 2.42% 48 0.92 2.37%
7 PRP ? AT-T ? AT-T 138 0.84 2.40% 129 0.88 2.25%
8 PRP ? MNR?1 ? MNR?1 71 0.82 3.21% 70 0.83 3.16%
4-8 PRP ? R2 ? R3 1,122 0.54 1.80% 681 0.88 1.09%
1-8 All 1,412 0.54 2.26% 840 0.86 1.35%
Table 7: Axioms used during evaluation, number of instances, accuracy and productivity. Results are reported
both using and not using the heuristic. Productivity refers to the number of relations added by the axiom in relative
terms.
Table 7 depicts the total number of instantiations for each axiom and its accuracy (columns 3 and
4). Accuracies range from 0.12 to 0.90, showing that the plausibility of an axiom depends on the axiom.
The average accuracy for axioms involving MCAU is 0.53 and for axioms involving MPNC is 0.54.
Axiom MCAU ? ARG0?1 ? ARG0?1 adds 201 relations, which corresponds to 0.89% in relative
terms. Its accuracy is low, 0.40. Other axioms are less productive overall, but have a greater relative
impact and accuracy. For example, axiom MPNC ? MMNR?1 ? MMNR?1, only yields 71 new MMNR,
and yet it is adding 3.21% in relative terms with an accuracy of 0.82.
It is worth noting that overall, applying the eight axioms used during evaluation adds 1,412 relations
on top of the ones already present (2.26% in relative terms) with an accuracy of 0.54.
5.3 Error Analysis
Because of the low accuracy of axioms 1 and 5, an error analysis was performed. We found that unlike
other axioms, these axioms often yield a relation type that is already present in the semantic representa-
tion. Specifically, axioms 1 and 5 often yield R(x, z) when R(x?, z) is already known.
An example can be found in Figure 4, where axiom 5 yields ARG1(orders, to buy) when the relation
ARG1(the basket, to buy) is already present. We use the following heuristic in order to improve the
accuracy of axioms 1 and 5: do not instantiate an axiom R1(x, y) ? R2(y, z) ? R3(x, z) if a relation of
the form R3(x?, z) is already known.
This simple heuristic allows us to augment the accuracy of the inferences at the cost of lowering their
productivity. The last three columns in Table 7 show results when using the heuristic. The eight axioms
add 840 relations (1.35% in relative terms) with an accuracy of 0.86.
5.4 Examples
In this section we present several examples of instantiations. We provide the full text of each example,
but only the relevant semantic annotation for instantiating axioms. For all examples, solid arrows indicate
semantic role annotation from PropBank, and discontinuous arrows inferred relations.
51
In the fibers division profit remains33
MLOC
persistent overcapacity
MCAU
ii
**
MLOC
V
W
YZ
\]abd
e
g
h
Figure 2: In the fibers division, profit remains weak, largely because of persistent overcapacity. (wsj 0552, 28).
it
ARG0
99
ARG0
h
h g
f f
e e
d c
c b a
a ` `
++
^ ^ ] ] \ [ [ Z Z Y X X W
would takeii
MTMP
in the fourth quarter
MTMP
e d
c a ` ,,
^ ] [ Z Y
plans to expand its systems operations
MCAU
jj
Figure 3: First Tennessee National Corp. said it would take a $4 million charge in the fourth quarter, as a result
of plans to expand its systems operation. (wsj 0621, 0).
The traders
ARG0
77
ARG0
n
m
l
k
j
g
e
c a
''
] [
Y
W
T
S
R
Q
P
place orders
ARG1
ff
ARG1?
q
n
l
i
g d
&&
Z W U
R
P
via computers
MMNR
ee
MMNRi d ((
Z U
to buy
MPNC
ee the basket
ARG1
gg
Figure 4: When it occurs, the traders place orders via computers to buy the basket of stocks . . . in whichever
market is cheaper and sell them in the more expensive market; . . . (wsj 0118, 48).
A man . . .
ARG0
99
ARG0
k
k
j
i
h
g f
e d
c c b
a `
((
^ ] \ [ [ Z Y X W V
U
T
S
S
camehh
MLOC
gg
MTMP
before the House . . . yesterday to talk about
MPNC
gg
''
MLOC
P
R
T
VX
[]acf
h
j
l ((MTMP U
Zdi
Figure 5: A man from the Bush administration came before the House Agriculture Committee yesterday to talk
about . . . (wsj 0134, 0).
Figures 2 and 3 instantiate axioms 1, 2 and 3. For these examples, all inferences are correct.
Figures 4 and 5 instantiate the rest of axioms. Not using the heuristic leads to a wrong inference in
the example shown in Figure 4, indicated with *. Using the heuristic, all inferences are correct.
6 Comparison with Previous Work
There have been abundant proposals to detect semantic relations without taking into account composition
of relations. All these approaches, regardless of their particular details, take as their input text and output
the relations found in it. In contrast, the framework proposed in this article obtains axioms that take as
their input relations found in text and output more relations previously ignored.
Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a
single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART-WHOLE (Girju
et al, 2006). Automatic detection of semantic roles has received a lot of attention lately (Ma`rquez et al,
2008; Carreras and Ma`rquez, 2005). The SemEval-2007 Task 04 (Girju et al, 2007) and SemEval-2010
Task 08 (Hendrickx et al, 2009) aimed at relations between nominals. There has been work on detecting
relations within noun phrases (Moldovan et al, 2004; Nulty, 2007), clauses (Szpakowicz et al, 1995)
and syntax-based comma resolution (Srikumar et al, 2008).
Previous research has exploited the idea of using semantic primitives to define and classify semantic
relations under different names. Among others, the literature uses relation elements, deep structure,
aspects and primitives. To the best of our knowledge, the first effort on describing semantic relations
52
using primitives was made by Chaffin and Herrmann (1987). They introduce Relation Element Theory,
and differentiate relations by relation elements. The authors describe a set of 31 relations clustered in five
groups (CONTRAST, SIMILARS, CLASS INCLUSION, CASE-RELATIONS, PART-WHOLE), and distinguish
each relation by its relations elements and not just a definition and examples. Their 30 relation elements
are clustered into five groups (elements of intensional force, dimension elements, elements of agreement,
propositional elements, elements of part-whole inclusion). They only use the elements to define relations,
not to compose relations.
Winston et al (1987) work with six subtypes of PART-WHOLE and uses 3 relation elements (func-
tional, homeomerous and separable) to distinguish the subtypes. Cohen and Losielle (1988) introduce
the notion of deep structure and characterize it using two aspects: hierarchical and temporal. Huhns and
Stephens (1989) extend previous works by considering an extended set of 10 primitives.
In Computational Linguistics there have been previous proposals to combine semantic relations.
Harabagiu and Moldovan (1998) manually extract plausible inference axioms using WordNet relations.
Helbig (2005) transforms chains of relations into theoretical axioms. On the other hand, the model
presented in this paper extracts inference axioms automatically.
Composing relations has been proposed before in the more general field of Artificial Intelligence,
in particular in the context of Knowledge Bases. Cohen and Losielle (1988) point out that two relations
shall combine if and only if they do not have contradictory values for the aspect hierarchical or temporal.
They work with a set of nine specific relations (CAUSES, COMPONENT-OF, FOCUS-OF, MECHANISM-OF,
PRODUCT-OF, PURPOSE-OF, SETTING-OF, SUBJECT-OF and SUBFIELD-OF) and their inverses. Huhns
and Stephens (1989) are the first to propose an algebra for composing semantic primitives. Unlike ours,
their set of relations is not linguistically motivated; ten of them map to some sort of PART-WHOLE (e.g.
PIECE-OF, SUBREGION-OF).
7 Conclusions
In this paper, we have presented a model to compose semantic relations. The model is independent of
any particular set of relations and is able to obtain inference axioms. These axioms take as their input
two semantic relations and yield a previously ignored relation as conclusion.
The model is based on an extended definition of semantic relations, including restrictions on domains
and ranges and values for a set of semantic primitives. We have defined an algebra for composing
semantic primitives. This algebra is the key to automatically identify the resulting relation of composing
a pair of compatible relations and to form an axiom.
The proposed algorithm to compose semantic relations identifies eight inference axioms using Prop-
Bank relations. When instantiated in a subset of PropBank, these axioms add 2.26% of annotation in
relative terms with an accuracy of 0.54. We believe these results are worthwhile for a completely unsu-
pervised approach to obtain semantic relations. Adding a simple heuristic improves the accuracy to 0.86,
lowering the productivity in relative terms to 1.35%.
The model has limitations and is not always correct. First, relations are defined manually and mis-
takes could be made when assigning values to their primitives. Second, the algebra for composing
primitives is also manually defined.
We find the first problem easy to overcome. Whatever the set of relations one might use, we believe
thinking in terms of primitives helps to understand the nature of the relations and their differences. An
issue might be that the proposed set of primitives is not enough for a particular set, but more primitives
could be added to solve this eventuality.
A further issue with the algebra is the fact that primitives are composed orthogonally. This is a
simplification, but we have shown that this simplified algebra works.
Even though different sets of semantic relations may call for different ontologies to define domains
and ranges, and possibly an extended set of primitives, we believe the model presented in this paper is
applicable to any set. As far as we are concerned, this is a novel way to compose semantic relations in
the field of Computational Linguistics.
53
References
Bethard, S. and J. H. Martin (2008). Learning Semantic Links from a Corpus of Parallel Temporal and
Causal Relations. In Proceedings of ACL-08: HLT, Short Papers, Columbus, OH.
Carreras, X. and L. Ma`rquez (2005). Introduction to the CoNLL-2005 shared task: semantic role label-
ing. In Proc. of the 9th Conf. on Computational Natural Language Learning, Morristown, NJ.
Chaffin, R. and D. J. Herrmann (1987). Relation Element Theory: A New Account of the Representation
and Processing of Semantic Relations. In D. S. Gorfein and R. R. Hoffman (Eds.), Memory and
Learning. The Ebbinghaus Centennial Conference.
Chang, D. S. and K. S. Choi (2006). Incremental cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair probabilities. Information Processing & Manage-
ment 42(3), 662?678.
Cohen, P. R. and C. L. Losielle (1988). Beyond ISA: Structures for Plausible Inference in Semantic
Networks. In Proceedings of the Seventh National conference on Artificial Intelligence, St. Paul, MN.
Girju, R., A. Badulescu, and D. Moldovan (2006). Automatic Discovery of Part-Whole Relations. Com-
putational Linguistics 32(1), 83?135.
Girju, R., P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret (2007). SemEval-2007 Task 04:
Classification of Semantic Relations between Nominals. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, Prague, Czech Republic.
Harabagiu, S. and D. Moldovan (1998). Knowledge Processing on an ExtendedWordNet. In C. Fellbaum
(Ed.), WordNet: An Electronic Lexical Database and Some of its Applications, Chapter 17, pp. 684?
714. The MIT Press.
Helbig, H. (2005). Knowledge Representation and the Semantics of Natural Language (1st ed.). Springer.
Hendrickx, I., S. N. Kim, Z. Kozareva, P. Nakov, Diarmuid, S. Pado?, M. Pennacchiotti, L. Romano,
and S. Szpakowicz (2009). SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations
Between Pairs of Nominals. In Proceedings of the Workshop on Semantic Evaluations, Boulder, CO.
Huhns, M. N. and L. M. Stephens (1989). Plausible Inferencing Using Extended Composition. In
IJCAI?89: Proceedings of the 11th international joint conference on AI, San Francisco, CA.
Ma`rquez, L., X. Carreras, K. C. Litkowski, and S. Stevenson (2008, June). Semantic Role Labeling: An
Introduction to the Special Issue. Computational Linguistics 34(2), 145?159.
Moldovan, D., A. Badulescu, M. Tatu, D. Antohe, and R. Girju (2004). Models for the Semantic Classi-
fication of Noun Phrases. In HLT-NAACL 2004: Workshop on Computational Lexical Semantics.
Nulty, P. (2007). Semantic Classification of Noun Phrases Using Web Counts and Learning Algorithms.
In Proceedings of the ACL 2007 Student Research Workshop, Prague, Czech Republic.
Palmer, M., D. Gildea, and P. Kingsbury (2005). The Proposition Bank: An Annotated Corpus of
Semantic Roles. Computational Linguistics 31(1), 71?106.
Srikumar, V., R. Reichart, M. Sammons, A. Rappoport, and D. Roth (2008). Extraction of Entailed
Semantic Relations Through Syntax-Based Comma Resolution. In Proceedings of ACL-08: HLT,
Columbus, OH.
Szpakowicz, B., K. Barker, and S. Szpakowicz (1995). Interactive semantic analysis of Clause-Level
Relationships. In Proc. of the 2nd Conference of the Pacific Association for Computational Linguistics.
Winston, M. E., R. Chaffin, and D. Herrmann (1987). A Taxonomy of Part-Whole Relations. Cognitive
Science 11(4), 417?444.
54
