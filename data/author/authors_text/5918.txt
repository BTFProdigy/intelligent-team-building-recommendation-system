Proceedings of NAACL HLT 2009: Short Papers, pages 69?72,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Web and Corpus Methods for Malay Count Classifier Prediction
Jeremy Nicholson and Timothy Baldwin
NICTA Victoria Research Laboratories
University of Melbourne, VIC 3010, Australia
{jeremymn,tim}@csse.unimelb.edu.au
Abstract
We examine the capacity of Web and corpus
frequency methods to predict preferred count
classifiers for nouns in Malay. The observed
F-score for the Web model of 0.671 consid-
erably outperformed corpus-based frequency
and machine learning models. We expect that
this is a fruitful extension for Web?as?corpus
approaches to lexicons in languages other than
English, but further research is required in
other South-East and East Asian languages.
1 Introduction
The objective of this paper is to extend a Malay
lexicon with count classifier information for nomi-
nal types. This is done under the umbrella of deep
lexical acquisition: the process of automatically or
semi-automatically learning linguistic structures for
use in linguistically rich language resources such as
precision grammars or wordnets (Baldwin, 2007).
One might call Malay a ?medium-density? lan-
guage: some NLP resources exist, but substantially
fewer than those for English, and they tend to be
of low complexity. Resources like the Web seem
promising for bootstrapping further resources, aided
in part by simple syntax and a Romanised ortho-
graphic system. The vast size of the Web has been
demonstrated to combat the data sparseness prob-
lem, for example, in Lapata and Keller (2004).
We examine using a similar ?first gloss? strategy
to Lapata and Keller (akin to ?first sense? in WSD,
in this case, identifying the most basic surface form
that a speaker would use to disambiguate between
possible classes), where the Web is used a corpus to
query a set of candidate surface forms, and the fre-
quencies are used to disambiguate the lexical prop-
erty. Due to the heterogeneity of the Web, we expect
to observe a significant amount of blocking from In-
donesian, a language with which Malay is some-
what mutually intelligible (Gordon, 2005). Hence,
we contrast this approach with observing the cues
directly from a corpus strictly of Malay, as well as a
corpus-based supervised machine learning approach
which does not rely on a presupplied gloss.
2 Background
2.1 Count Classifiers
A count classifier (CL) is a noun that occurs in a
specifier phrase with one of a set of (usually nu-
meric) specifiers; the specifier phrase typically oc-
curs in apposition or as a genitive modifier (GEN) to
the head noun. In many languages, including many
South-East Asian, East Asian, and African families,
almost all nouns are uncountable and can only be
counted through specifier phrases. A Malay exam-
ple, where biji is the count classifier (CL) for fruit, is
given in (1).
(1) tiga
three
biji
CL
pisang
banana
?three bananas?
Semantically, a lexical entry for a noun will in-
clude a default (sortal) count classifier which se-
lects for a particular semantic property of the lemma.
Usually this is a conceptual class (e.g. HUMAN or
ANIMAL) or a description of some relative dimen-
sional property (e.g. FLAT or LONG-AND-THIN).
Since each count classifier has a precise seman-
tics, using a classifier other than the default can co-
erce a given lemma into different semantics. For ex-
ample, raja ?king? typically takes orang ?person?
as a classifier, as in 2 orang raja ?2 kings?, but can
take on an animal reading with ekor ?animal? in 2
ekor raja ?2 kingfishers?. An unintended classifier
69
can lead to highly marked or infelicitious readings,
such as #2 biji raja ?2 (chess) kings?.
Most research on count classifiers tends to discuss
generating a hierarchy or taxonomy of the classi-
fiers available in a given language (e.g. Bond and
Paik (1997) for Japanese and Korean, or Shirai et
al. (2008) cross-linguistically) or using language-
specific knowledge to predict tokens (e.g. Bond and
Paik (2000)) or both (e.g. Sornlertlamvanich et al
(1994)).
2.2 Malay Data
Little work has been done on NLP for Malay, how-
ever, a stemmer (Adriani et al, 2007) and a prob-
abilistic parser for Indonesian (Gusmita and Manu-
rung, 2008) have been developed. The mutually in-
telligibility suggests that Malay resources could pre-
sumably be extended from these.
In our experiments, we make use of a Malay?
English translation dictionary, KAMI (Quah et al,
2001), which annotates about 19K nominal lexical
entries for count classifiers. To limit very low fre-
quency entries, we cross-reference these with a cor-
pus of 1.2M tokens of Malay text, described in Bald-
win and Awab (2006). We further exclude the two
non-sortal count classifiers that are attested as de-
fault classifiers in the lexicon, as their distribution is
heavily skewed and not lexicalised.
In all, 2764 simplex common nouns are attested
at least once in the corpus data. We observe 2984
unique noun?to?default classifier assignments. Pol-
ysemy leads to an average of 1.08 count classifiers
assigned to a given wordform. The most difficult
exemplars to classify, and consequently the most in-
teresting ones, correspond to the dispreferred count
classifiers of the multi-class wordforms: direct as-
signment and frequency thresholding was observed
to perform poorly. Since this task is functionally
equivalent to the subcat learning problem, strategies
from that field might prove helpful (e.g. Korhonen
(2002)).
The final distribution of the most frequent classes
is as follows:
CL: orang buah batang ekor OTHER
Freq: 0.389 0.292 0.092 0.078 0.149
Of the 49 classes, only four have a relative frequency
greater than 3% of the types: orang for people,
batang for long, thin objects, ekor for animals, and
buah, the semantically empty classifier, for when no
other classifiers are suitable (e.g. for abstract nouns);
orang and buah account for almost 70% of the types.
3 Experiment
3.1 Methodology
Lapata and Keller (2004) look at a set of generation
and analysis tasks in English, identify simple surface
cues, and query a Web search engine to approximate
those frequencies. They then use maximum likeli-
hood estimation or a variety of normalisation meth-
ods to choose an output.
For a given Malay noun, we attempt to select the
default count classifier, which is a generation task
under their framework, and semantically most simi-
lar to noun countability detection. Specifier phrases
almost always premodify nouns in Malay, so the set
of surface cues we chose was satu CL NOUN ?one/a
NOUN?.1 This was observed to have greater cov-
erage than dua ?two? and other non-numeral spec-
ifiers. 49 queries were performed for each head-
word, and maximum likelihood estimation was used
to select the predicted classifier (i.e. taking most fre-
quently observed cue, with a threshold of 0). Fre-
quencies from the same cues were also obtained
from the corpus of Baldwin and Awab (2006).
We contrasted this with a machine learning model
for Malay classifiers, designed to be language-
independent (Nicholson and Baldwin, 2008). A fea-
ture vector is constructed for each headword by con-
catenating context windows of four tokens to the left
and right of each instance of the headword in the cor-
pus (for eight word unigram features per instance).
These are then passed into two kinds of maximum
entropy model: one conditioned on all 49 classes,
and one cascaded into a suite of 49 separate binary
classifiers designed to predict each class separately.
Evaluation is via 10-fold stratified cross-validation.
A majority class baseline was also examined, where
every headword was assigned the orang class.
For the corpus-based methods, if the frequency of
every cue is 0, no prediction of classifier is made.
Similarly, the suite can predict a negative assign-
1satu becomes cliticised to se- in this construction, so that
instead of cues like satu buah raja, satu orang raja, ..., we have
cues like sebuah raja, seorang raja, ....
70
Method Web Corpus Suite Entire Base
Prec. .736 .908 .652 .570 .420
Rec. .616 .119 .379 .548 .389
F? = 1 .671 .210 .479 .559 .404
Table 1: Performance of the five systems.
Back-off Web Suite Entire orang buah
Prec. .736 .671 .586 .476 .389
Rec. .616 .421 .561 .441 .360
F? = 1 .671 .517 .573 .458 .374
Table 2: Performance of corpus frequency assignment
(Corpus in Table 1), backed-off to the other systems.
ment for each of the 49 classes. Consequently, pre-
cision is calculated as the fraction of correctly pre-
dicted instances to the number of examplars where
a prediction was made. Only the suite of classifiers
could natively handle multi-assignment of classes:
recall was calculated as the fraction of correctly pre-
dicted instances to all 2984 possible headword?class
assignments, despite the fact that four of the systems
could not make 220 of the classifications.
3.2 Results
The observed precision, recall, and F-scores of the
various systems are shown in Table 1. The best
F-score is observed for the Web frequency system,
which also had the highest recall. The best precision
was observed for the corpus frequency system, but
with very low recall ? about 85% of the wordforms
could not be assigned to a class (the corresponding
figure for the Web system was about 9%). Conse-
quently, we attempted a number of back-off strate-
gies so as to improve the recall of this system.
The results for backing off the corpus frequency
system to the Web model, the two maximum entropy
models, and two baselines (the majority class, and
the semantically empty classifier) are shown in Ta-
ble 2. Using a Web back-off was nearly identical to
the basic Web system: most of the correct assign-
ments being made by the corpus frequency system
were also being captured through Web frequencies,
which indicates that these are the easier, high fre-
quency entries. Backing off to the machine learn-
ing models performed the same or slightly better
than using the machine learning model by itself. It
therefore seems that the most balanced corpus-based
model should take this approach.
The fact that the Web frequency system had the
best performance belies the ?noisiness? of the Web,
in that one expects to observe errors caused by
carelessness, laziness (e.g. using buah despite a
more specific classifier being available), or noise
(e.g. Indonesian count classifier attestation; more on
this below). While the corpus of ?clean?, hand-
constructed data did have a precision improvement
over the Web system, the back-off demonstrates that
it was not substantially better over those entries that
could be classified from the corpus data.
4 Discussion
As with many classification tasks, the Web-based
model notably outperformed the corpus-based mod-
els when used to predict count classifiers of Malay
noun types, particularly in recall. In a type-wise lex-
icon, precision is probably the more salient evalua-
tion metric, as recall is more meaningful on tokens,
and a low-precision lexicon is often of little utility;
the Web system had at least comparable precision
for the entries able to be classified by the corpus-
based systems.
We expected that the heterogeneity of the Web,
particularly confusion caused by a preponderance of
Indonesian, would cause performance to drop, but
this was not the case. The Ethnologue estimates that
there are more speakers of Indonesian than Malay
(Gordon, 2005), and one would expect the Web dis-
tribution to reflect this. Also, there are systematic
differences in the way count classifiers are used in
the two languages, despite the intelligibility; com-
pare ?five photographs?: lima keping foto in Malay
and lima lembar foto, lima foto in Indonesian.
While the use of count classifiers is obligatory in
Malay, it is optional in Indonesian for lower reg-
isters. Also, many classifiers that are available in
Malay are not used in Indonesian, and the small set
of Indonesian count classifiers that are not used in
Malay do not form part of the query set, so no confu-
sion results. Consequently, it seems that greater dif-
ficulty would arise when attempting to predict count
classifiers for Indonesian nouns, as their optional-
ity and blocking from Malay cognates would intro-
duce noise in cases where language identification
has not been used to generate the corpus (like the
71
Web) ? hand-constructed corpora might be neces-
sary in that case. Furthermore, the Web system ben-
efits from a very simple surface form, namely se-
CL NOUN: languages that permit floating quantifica-
tion, like Japanese, or require classifiers for stative
verb modification, like Thai, would need many more
queries or lower-precision queries to capture most of
the cues available from the corpus. We intend to ex-
amine these phenomena in future work.
An important contrast is noted between the ?un-
supervised? methods of the corpus-frequency sys-
tems and the ?supervised? machine learning meth-
ods. One presumed advantage of unsupervised sys-
tems is the lack of pre-annotated training data re-
quired. In this case, a comparable time investment
by a lexicographer would be required to generate the
set of surface forms for the corpus-frequency mod-
els. The performance dictates that the glosses for the
Web system give the most value for lexicographer
input; however, for other languages or other lexical
properties, generating a set of high-precision, high-
recall glosses is often non-trivial. If the Web is not
used, having both training data and high-precision,
low-recall glosses is valuable.
5 Conclusion
We examine an approach for using Web and cor-
pus data to predict the preferred generation form for
counting nouns in Malay, and observed greater pre-
cision than machine learning methods that do not
require a presupplied gloss. Most Web?as?corpus
research tends to focus on English; as the Web in-
creases in multilinguality, it becomes an important
resource for medium- and low-density languages.
This task was quite simple, with glosses amenable to
Web approaches, and is promising for automatically
extending the coverage of a Malay lexicon. How-
ever, we expect that the Malay glosses will block
readings of Indonesian classifiers, and classifiers in
other languages will require different strategies; we
intend to examine this in future work.
Acknowledgements
We would like to thank Francis Bond for his valuable in-
put on this research. NICTA is funded by the Australian
government as represented by Department of Broadband,
Communication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excellence
programme.
References
M. Adriani, J. Asian, B. Nazief, S.M.M. Tahaghoghi,
and H.E. Williams. 2007. Stemming Indonesian:
A confix-stripping approach. ACM Transactions on
Asian Language Information Processing, 6.
T. Baldwin and S. Awab. 2006. Open source corpus anal-
ysis tools for Malay. In Proc. of the 5th International
Conference on Language Resources and Evaluation,
pages 2212?5, Genoa, Italy.
T. Baldwin. 2007. Scalable deep linguistic processing:
Mind the lexical gap. In Proc. of the 21st Pacific Asia
Conference on Language, Information and Computa-
tion, pages 3?12, Seoul, Korea.
F. Bond and K. Paik. 1997. Classifying correspondence
in Japanese and Korean. In Proc. of the 3rd Confer-
ence of the Pacific Association for Computational Lin-
guistics, pages 58?67, Tokyo, Japan.
F. Bond and K. Paik. 2000. Reusing an ontology to
generate numeral classifiers. In Proc. of the 19th In-
ternational Conference on Computational Linguistics,
pages 90?96, Saarbru?cken, Germany.
R.G. Gordon, Jr, editor. 2005. Ethnologue: Languages
of the World, Fifteenth Edition. SIL International.
R.H. Gusmita and Ruli Manurung. 2008. Some initial
experiments with Indonesian probabilistic parsing. In
Proc. of the 2nd International MALINDO Workshop,
Cyberjaya, Malaysia.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, Cambridge,
UK.
M. Lapata and F. Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised
web-based models for a range of NLP tasks. In Proc.
of the 4th International Conference on Human Lan-
guage Technology Research and 5th Annual Meeting
of the NAACL, pages 121?128, Boston, USA.
J. Nicholson and T. Baldwin. 2008. Learning count
classifier preferences of Malay nouns. In Proc. of the
Australasian Language Technology Association Work-
shop, pages 115?123, Hobart, Australia.
C.K. Quah, F. Bond, and T. Yamazaki. 2001. De-
sign and construction of a machine-tractable Malay-
English lexicon. In Proc. of the 2nd Biennial Confer-
ence of ASIALEX, pages 200?205, Seoul, Korea.
K. Shirai, T. Tokunaga, C-R. Huang, S-K. Hsieh, T-
Y. Kuo, V. Sornlertlamvanich, and T. Charoenporn.
2008. Constructing taxonomy of numerative classi-
fiers for Asian languages. In Proc. of the Third Inter-
national Joint Conference on Natural Language Pro-
cessing, Hyderabad, India.
V. Sornlertlamvanich, W. Pantachat, and S. Meknavin.
1994. Classifier assignment by corpus-based ap-
proach. In Proc. of the 15th International Conference
on Computational Linguistics, pages 556?561, Kyoto,
Japan.
72
Proceedings of ACL-08: HLT, pages 613?621,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Enhancing Performance of Lexicalised Grammars
Rebecca Dridan?, Valia Kordoni?, Jeremy Nicholson??
?Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany
?Dept of Computer Science and Software Engineering and NICTA, University of Melbourne, Australia
{rdrid,kordoni}@coli.uni-sb.de, jeremymn@csse.unimelb.edu.au
Abstract
This paper describes how external resources
can be used to improve parser performance for
heavily lexicalised grammars, looking at both
robustness and efficiency. In terms of robust-
ness, we try using different types of external
data to increase lexical coverage, and find that
simple POS tags have the most effect, increas-
ing coverage on unseen data by up to 45%. We
also show that filtering lexical items in a su-
pertagging manner is very effective in increas-
ing efficiency. Even using vanilla POS tags we
achieve some efficiency gains, but when us-
ing detailed lexical types as supertags weman-
age to halve parsing time with minimal loss of
coverage or precision.
1 Introduction
Heavily lexicalised grammars have been used in ap-
plications such as machine translation and informa-
tion extraction because they can produce semantic
structures which provide more information than less
informed parsers. In particular, because of the struc-
tural and semantic information attached to lexicon
items, these grammars do well at describing com-
plex relationships, like non-projectivity and center
embedding. However, the cost of this additional in-
formation sometimes makes deep parsers that use
these grammars impractical. Firstly because, if the
information is not available, the parsers may fail to
produce an analysis, a failure of robustness. Sec-
ondly, the effect of analysing the extra information
can slow the parser down, causing efficiency prob-
lems. This paper describes experiments aimed at
improving parser performance in these two areas, by
annotating the input given to one such deep parser,
the PET parser (Callmeier, 2000), which uses lex-
icalised grammars developed under the HPSG for-
malism (Pollard and Sag, 1994).
2 Background
In all heavily lexicalised formalisms, such as LTAG,
CCG, LFG and HPSG, the lexicon plays a key role
in parsing. But a lexicon can never hope to contain
all words in open domain text, and so lexical cover-
age is a central issue in boosting parser robustness.
Some systems use heuristics based on numbers, cap-
italisation and perhaps morphology to guess the cat-
egory of the unknown word (van Noord and Mal-
ouf, 2004), while others have focused on automati-
cally expanding the lexicon (Baldwin, 2005; Hock-
enmaier et al, 2002; O?Donovan et al, 2005). An-
other method, described in Section 4, uses external
resources such as part-of-speech (POS) tags to select
generic lexical entries for out-of-vocabulary words.
In all cases, we lose some of the depth of informa-
tion the hand-crafted lexicon would provide, but an
analysis is still produced, though possibly less than
fully specified.
The central position of these detailed lexicons
causes problems, not only of robustness, but also of
efficiency and ambiguity. Many words may have
five, six or more lexicon entries associated with
them, and this can lead to an enormous search space
for the parser. Various means of filtering this search
space have been attempted. Kiefer et al (1999) de-
scribes a method of filtering lexical items by specify-
ing and checking for required prefixes and particles
613
which is particularly effective for German, but also
applicable to English. Other research has looked at
using dependencies to restrict the parsing process
(Sagae et al, 2007), but the most well known fil-
tering method is supertagging. Originally described
by Bangalore and Joshi (1994) for use in LTAG pars-
ing, it has also been used very successfully for CCG
(Clark, 2002). Supertagging is the process of assign-
ing probable ?supertags? to words before parsing to
restrict parser ambiguity, where a supertag is a tag
that includes more specific information than the typ-
ical POS tags. The supertags used in each formal-
ism differ, being elementary trees in LTAG and CCG
categories for CCG. Section 3.2 describes an exper-
iment akin to supertagging for HPSG, where the su-
pertags are HPSG lexical types. Unlike elementary
trees and CCG categories, which are predominantly
syntactic categories, the HPSG lexical types contain
a lot of semantic information, as well as syntactic.
In the case study we describe here, the tools,
grammars and treebanks we use are taken from
work carried out in the DELPH-IN1 collaboration.
This research is based on using HPSG along with
Minimal Recursion Semantics (MRS: Copestake et
al. (2001)) as a platform to develop deep natural
language processing tools, with a focus on multi-
linguality. The grammars are designed to be bi-
directional (used for generation as well as parsing)
and so contain very specific linguistic information.
In this work, we focus on techniques to improve
parsing, not generation, but, as all the methods in-
volve pre-processing and do not change the gram-
mar itself, we do not affect the generation capabil-
ities of the grammars. We use two of the DELPH-
IN wide-coverage grammars: the English Resource
Grammar (ERG: Copestake and Flickinger (2000))
and a German grammar, GG (Mu?ller and Kasper,
2000; Crysmann, 2003). We also use the PET parser,
and the [incr tsdb()] system profiler and treebanking
tool (Oepen, 2001) for evaluation.
3 Parser Restriction
An exhaustive parser, such as PET, by default pro-
duces every parse licensed by the grammar. How-
ever, in many application scenarios, this is unnec-
essary and time consuming. The benefits of us-
1http://wiki.delph-in.net/
ing a deep parser with a lexicalised grammar are
the precision and depth of the analysis produced,
but this depth comes from making many fine dis-
tinctions which greatly increases the parser search
space, making parsing slow. By restricting the lexi-
cal items considered during parsing, we improve the
efficiency of a parser with a possible trade-off of los-
ing correct parses. For example, the noun phrase
reading of The dog barks is a correct parse, although
unlikely. By blocking the use of barks as a noun
in this case, we lose this reading. This may be an
acceptable trade-off in some applications that can
make use of the detailed information, but only if it
can be delivered in reasonable time. An example
of such an application is the real-time speech trans-
lation system developed in the Verbmobil project
(Wahlster, 2000), which integrated deep parsing re-
sults, where available, into its appointment schedul-
ing and travel planning dialogues. In these exper-
iments we look at two methods of restricting the
parser, first by using POS tags and then using lexical
types. To control the trade-off between efficiency
and precision, we vary which lexical items are re-
stricted according to a likelihood threshold from the
respective taggers. Only open class words are re-
stricted, since it is the gross distinctions between, for
instance, noun and verb that we would like to utilise.
Any differences between categories for closed class
words are more subtle and we feel the parser is best
left to make these distinctions without restriction.
The data set used for these experiments is the jh5
section of the treebank released with the ERG. This
text consists of edited written English in the domain
of Norwegian hiking instructions from the LOGON
project (Oepen et al, 2004).
3.1 Part of Speech Tags
We use TreeTagger (Schmid, 1994) to produce POS
tags and then open class words are restricted if the
POS tagger assigned a tag with a probability over
a certain threshold. A lower threshold will lead to
faster parsing, but at the expense of losing more cor-
rect parses. We experiment with various thresholds,
and results are shown in Table 1. Since a gold stan-
dard treebank for our data set was available, it was
possible to evaluate the accuracy of the parser. Eval-
uation of deep parsing results is often reported only
in terms of coverage (number of sentences which re-
614
Threshold Coverage Precision Time
gold 93.5% 92.2% N/A
unrestricted 93.3% 92.4% 0.67s
1.00 90.7% 91.9% 0.59s
0.98 88.8% 89.3% 0.49s
0.95 88.4% 89.5% 0.48s
0.90 86.4% 88.5% 0.44s
0.80 84.3% 87.0% 0.43s
0.60 81.5% 87.3% 0.39s
Table 1: Results obtained when restricting the parser lex-
icon according to the POS tag, where words are restricted
according to a threshold of POS probabilities.
ceive an analysis), because, since the hand-crafted
grammars are optimised for precision over cover-
age, the analyses are assumed to be correct. How-
ever, in this experiment, we are potentially ?dilut-
ing? the precision of the grammar by using external
resources to remove parses and so it is important that
we have some idea of how the accuracy is affected.
In the table, precision is the percentage of sentences
that, having produced at least one parse, produced a
correct parse. A parse was judged to be correct if it
exactly matched the gold standard tree in all aspects,
syntactic and semantic.
The results show quite clearly how the coverage
drops as the average parse time per sentence drops.
In hybrid applications that can back-off to less infor-
mative analyses, this may be a reasonable trade-off,
enabling detailed analyses in shorter times where
possible, and using the shallower analyses other-
wise.
3.2 Lexical Types
Another option for restricting the parser is to use the
lexical types used by the grammar itself, in a simi-
lar method to that described by Prins and van Noord
(2003). This could be considered a form of supertag-
ging as used in LTAG and CCG. Restricting by lex-
ical types should have the effect of reducing ambi-
guity further than POS tags can do, since one POS
tag could still allow the use of multiple lexical items
with compatible lexical types. On the other hand, it
could be considered more difficult to tag accurately,
since there are many more lexical types than POS
tags (almost 900 in the ERG) and less training data
is available.
Configuration Coverage Precision Time
gold 93.5% 92.2% N/A
unrestricted 93.3% 92.4% 0.67s
0.98 with POS 93.5% 91.9% 0.63s
0.95 with POS 93.1% 92.4% 0.48s
0.90 with POS 92.9% 92.3% 0.37s
0.80 with POS 91.8% 91.8% 0.31s
0.60 with POS 86.2% 93.5% 0.21s
0.98 no POS 92.9% 92.3% 0.62s
0.95 no POS 90.9% 91.0% 0.48s
0.90 no POS 87.7% 89.2% 0.42s
0.80 no POS 79.7% 84.6% 0.33s
0.60 no POS 67.0% 84.2% 0.23s
Table 2: Results obtained when restricting the parser lex-
icon according to the predicted lexical type, where words
are restricted according to a threshold of tag probabilities.
Two models, with and without POS tags as features, were
used.
While POS taggers such as TreeTagger are com-
mon, and there some supertaggers are available, no-
tably that of Clark and Curran (2007) for CCG,
no standard supertagger exists for HPSG. Conse-
quently, we developed a Maximum Entropy model
for supertagging using the OpenNLP implementa-
tion.2 Similarly to Zhang and Kordoni (2006), we
took training data from the gold?standard lexical
types in the treebank associated with ERG (in our
case, the July-07 version). For each token, we ex-
tracted features in two ways. One used features only
from the input string itself: four characters from the
beginning and end of the target word token, and two
words of context (where available) either side of the
target. The second used the features from the first,
along with POS tags given by TreeTagger for the
context tokens.
We held back the jh5 section of the treebank for
testing the Maximum Entropy model. Again, the
lexical items that were to be restricted were con-
trolled by a threshold, in this case the probabil-
ity given by the maximum entropy model. Table
2 shows the results achieved by these two models,
with the unrestricted results and the gold standard
provided for comparison.
Here we see the same trends of falling coverage
2http://maxent.sourceforge.net/
615
with falling time for both models, with the POS
tagged model consistently outperforming the word-
form model. To give a clearer picture of the com-
parative performance of all three experiments, Fig-
ure 1 shows how the results vary with time for both
models, and for the POS tag restricted experiment.
Here we can see that the coverage and precision of
the lexical type restriction experiment that uses the
word-form model is just above that of the POS re-
stricted one. However the POS tagged model clearly
outperforms both, showing minimal loss of coverage
or precision at a threshold which halved the average
parsing time. At the lowest parsing time, we see
that precision of the POS tagged model even goes
up. This can be explained by noting that coverage
here goes down, and obviously we are losing more
incorrect parses than correct parses.
This echoes the main result from Prins and van
Noord (2003), that filtering the lexical categories
used by the parser can significantly reduce parsing
time, while maintaining, or even improving, preci-
sion. The main differences between our method and
that of Prins and van Noord are the training data and
the tagging model. The key feature of their exper-
iment was the use of ?unsupervised? training data,
that is, the uncorrected output of their parser. In this
experiment, we used gold standard training data, but
much less of it (just under 200 000 words) and still
achieved a very good precision. It would be inter-
esting to see what amount of unsupervised parser
output we would require to achieve the same level
of precision. The other difference was the tagging
model, maximum entropy versus Hidden Markov
Model (HMM). We selected maximum entropy be-
cause Zhang and Kordoni (2006) had shown that
they got better results using a maximum entropy tag-
ger instead of a HMM one when predicting lexical
types, albeit for a slightly different purpose. It is not
possible to directly compare results between our ex-
periments and those in Prins and van Noord, because
of different languages, data sets and hardware, but it
is worth noting that parsing times are much lower in
our setup, perhaps more so than can be attributed to
4 years hardware improvement. While the range of
sentence lengths appears to be very similar between
the data sets, one possible reason for this could be
the very large number of lexical categories used in
their ALPINO system.
65
70
75
80
85
90
95
0.2 0.3 0.4 0.5 0.6 0.7
Average time per sentence (seconds)
Coverage
Gold standard
POS tags
3
3
3
33
3
3
Lexical types (no POS model)
+
+
+
+
+
+
Lexical types (with POS model)
2
2 2 2
2
2
Unrestricted
?
?
75
80
85
90
95
0.2 0.3 0.4 0.5 0.6 0.7
Average time per sentence (seconds)
Precision
Gold standard
POS tags
3 3
3
33
3
3
Lexical types (no POS model)
+ +
+
+
+
+
Lexical types (with POS model)
2
2 2 2 2
2
Unrestricted
?
?
Figure 1: Coverage and precision varying with time for
the three restriction experiments. Gold standard and un-
restricted results shown for comparison.
While this experiment is similar to that of Clark
and Curran (2007), it differs in that their supertag-
ger assign categories to every word, while we look
up every word in the lexicon and the tagger is used to
filter what the lexicon returns, only if the tagger con-
fidence is sufficiently high. As Table 2 shows, when
we use the tags for which the tagger had a low confi-
dence, we lose significant coverage. In order to run
as a supertagger rather than a filter, the tagger would
need to be much more accurate. While we can look
at multi-tagging as an option, we believe much more
training data would be needed to achieve a sufficient
level of tag accuracy.
Increasing efficiency is important for enabling
these heavily lexicalised grammars to bring the ben-
efits of their deep analyses to applications, but simi-
616
larly important is robustness. The following section
is aimed at addressing this issue of robustness, again
by using external information.
4 Unknown Word Handling
The lexical information available to the parser is
what makes the depth of the analysis possible, and
the default configuration of the parser uses an all-
or-nothing approach, where a parse is not produced
if all the lexical information is not available. How-
ever, in order to increase robustness, it is possible to
use underspecified lexical information where a fully
specified lexical item is not available. One method
of doing this, built in to the PET parser, is to use
POS tags to select generic lexical items, and hence
allow a (less than fully specified) parse to be built.
The six data sets used for these experiments were
chosen to give a range of languages and genres.
Four sets are English text: jh5 described in Sec-
tion 3; trec consisting of questions from TREC and
included in the treebanks released with the ERG;
a00 which is taken from the BNC and consists of
factsheets and newsletters; and depbank, the 700
sentences of the Briscoe and Carroll version of Dep-
Bank (Briscoe and Carroll, 2006) taken from the
Wall Street Journal. The last two data sets are Ger-
man text: clef700 consisting of German questions
taken from the CLEF competition and eiche564 a
sample of sentences taken from a treebank parsed
with the German HPSG grammar, GG and consist-
ing of transcribed German speech data concerning
appointment scheduling from the Verbmobil project.
Vital statistics of these data sets are described in Ta-
ble 3.
We used TreeTagger to POS tag the six data sets,
with the tagger configured to assign multiple tags,
where the probability of the less likely tags was at
least half that of the most likely tag. The data was
input using a PET input chart (PIC), which allows
POS tags to be assigned to each token, and then
parsed each with the PET parser.3 All English data
sets used the July-07 CVS version of the ERG and
the German sets used the September 2007 version
of GG. Unlike the experiments described in Sec-
tion 3, adding POS tags in this way will have no
effect on sentences which the parser is already able
3Subversion revision 384
Language
Number
of
Sentences
Ave.
Sentence
Length
jh5 English 464 14.2
trec English 693 6.9
a00 English 423 17.2
depbank English 700 21.5
clef German 700 7.5
eiche564 German 564 11.5
Table 3: Data sets used in input annotation experiments.
to parse. The POS tags will only be considered when
the parser has no lexicon entry for a given word, and
hence can only increase coverage. Results are shown
in Table 4, comparing the coverage over each set to
that obtained without using POS tags to handle un-
known words. Coverage here is defined as the per-
centage of sentences with at least one parse.
These results show very clearly one of the poten-
tial drawbacks of using a highly lexicalised gram-
mar formalism like HPSG: unknown words are one
of the main causes of parse failure, as quantified in
Baldwin et al (2004) and Nicholson et al (2008).
In the results here, we see that for jh5, trec and
eiche564, adding unknown word handling made al-
most no difference, since the grammars (specifically
the lexicons) have been tuned for these data sets. On
the other hand, over unseen texts, adding unknown
word handling made a dramatic difference to the
coverage. This motivates strategies like the POS tag
annotation used here, as well as the work on deep
lexical acquisition (DLA) described in Zhang and
Kordoni (2006) and Baldwin (2005), since no gram-
mar could ever hope to cover all words used within
a language.
As mentioned in Section 3, coverage is not the
only evaluation metric that should be considered,
particularly when adding potentially less precise in-
formation to the parsing process (in this case POS
tags). Since the primary effect of adding POS tags
is shown with those data sets for which we do not
have gold standard treebanks, evaluating accuracy
in this case is more difficult. However, in order to
give some idea of the effects on precision, a sample
of 100 sentences from the a00 data set was evaluated
for accuracy, for this and the following experiments.
617
In this instance, we found there was only a slight
drop in precision, where the original analyses had a
precision of 82% and the precision of the analyses
when POS tags were used was 80%.
Since the parser has the means to accept named
entity (NE) information in the input, we also ex-
perimented with using generic lexical items gener-
ated from NE data. We used SProUT (Becker et al,
2002) to tag the data sets and used PET?s inbuilt NE
handling mechanism to add NE items to the input,
associated with the appropriate word tokens. This
works slightly differently from the POS annotation
mechanism, in that NE items are considered by the
parser, even when the associated words are in the
lexicon. This has the effect of increasing the number
of analyses produced for sentences that already have
a full lexical span, but could also increase coverage
by enabling parses to be produced where there is no
lexical span, or where no parse was possible because
a token was not recognised as part of a name. In or-
der to isolate the effect of the NE data, we ran one
experiment where the input was annotated only with
the SProUT data, and another where the POS tags
were also added. These results are also in Table 4.
Again, we see coverage increases in the three un-
seen data sets, a00, depbank and clef, but not to the
same extent as the POS tags. Examining the re-
sults in more detail, we find that the increases come
almost exclusively from sentences without lexical
span, rather than in sentences where a token was
previously not recognised as part of a name. This
means that the NE tagger is operating almost like a
POS tagger that only tags proper nouns, and as the
POS tagger tags proper nouns quite accurately, we
find the NE tagger gives no benefit here. When ex-
amining the precision over our sample evaluation set
from a00, we find that using the NE data alone adds
no correct parses, while using NE data with POS
tags actually removes correct parses when compared
with POS alone, since the (in these cases, incorrect)
NE data is preferred over the POS tags. It is possible
that another named entity tagger would give better
results, and this may be looked at in future experi-
ments.
Other forms of external information might also be
used to increase lexical coverage. Zhang and Kor-
doni (2006) reported a 20% coverage increase over
baseline using a lexical type predictor for unknown
words, and so we explored this avenue. The same
maximum entropy tagger used in Section 3 was used
and each open class word was tagged with its most
likely lexical type, as predicted by the maximum en-
tropy model. Table 5 shows the results, with the
baseline and POS annotated results for comparison.
As with the previous experiments, we see a cover-
age increase in those data sets which are considered
unseen text for these grammars. Again it is clear
that the use of POS tags as features obviously im-
proves the maximum entropy model, since this sec-
ond model has almost 10% better coverage on our
unseen texts. However, lexical types do not appear
to be as effective for increasing lexical coverage as
the POS tags. One difference between the POS and
lexical type taggers is that the POS tagger could pro-
duce multiple tags per word. Therefore, for the next
experiment, we altered the lexical type tagger so it
could also produce multiple tags. As with the Tree-
Tagger configuration we used for POS annotation,
extra lexical type tags were produced if they were at
least half as probable as the most likely tag. A lower
probability threshold of 0.01 was set, so that hun-
dreds of tags of equal likelihood were not produced
in the case where the tagger was unable to make an
informed prediction. The results with multiple tag-
ging are also shown in Table 5.
The multiple tagging version gives a coverage in-
crease of between 2 and 10% over the single tag ver-
sion of the tagger, but, at least for the English data
sets, it is still less effective than straight-forward
POS tagging. For the German unseen data set, clef,
we do start getting above what the POS tagger can
achieve. This may be in part because of the features
used by the lexical type tagger ? German, being
a more morphologically rich language, may benefit
more from the prefix and suffix features used in the
tagger.
In terms of precision measured on our sample
evaluation set, the single tag version of the lexical
type tagger which used POS tag features achieved
a very good precision of 87% where, of all the extra
sentences that could now be parsed, only one did not
have a correct parse. In an application where preci-
sion is considered much more important than cover-
age, this would be a good method of increasing cov-
erage without loss of accuracy. The single tag ver-
sion that did not use POS tags in the model achieved
618
Baseline with POS NE only NE+POS
jh5 93.1% 93.3% 93.1% 93.3%
trec 97.1% 97.5% 97.4% 97.7%
a00 50.1% 83.9% 53.0% 85.8%
depbank 36.3% 76.9% 51.1% 80.4%
clef 22.0% 67.7% 42.3% 75.3%
eiche564 63.8% 63.8% 64.0% 64.0%
Table 4: Parser coverage with baseline using no unknown word handling and unknown word handling using POS tags,
SProUT named entity data as the only annotation, or SProUT tags in addition to POS annotation.
Single Lexical Types Multiple Lexical Types
Baseline POS -POS +POS -POS +POS
jh5 93.1% 93.3% 93.3% 93.3% 93.5% 93.5%
trec 97.1% 97.5% 97.3% 97.4% 97.3% 97.4%
a00 50.1% 83.9% 63.8% 72.6% 65.7% 78.5%
depbank 36.3% 76.9% 51.7% 64.4% 53.9% 69.7%
clef 22.0% 67.7% 59.9% 66.8% 69.7% 76.9%
eiche564 63.8% 63.8% 63.8% 63.8% 63.8% 63.8%
Table 5: Parser coverage using a lexical type predictor for unknown word handling. The predictor was run in single tag
mode, and then in multi-tag mode. Two different tagging models were used, with and without POS tags as features.
the same precision as with using only POS tags, but
without the same increase in coverage. On the other
hand, the multiple tagging versions, which at least
started approaching the coverage of the POS tag ex-
periment, dropped to a precision of around 76%.
From the results of Section 3, one might expect
that at least the lexical type method of handling un-
known words might at least lead to quicker parsing
than when using POS tags, however POS tags are
used differently in this situation. When POS tags
are used to restrict the parser, any lexicon entry that
unifies with the generic part-of-speech lexical cate-
gory can be used by the parser. That is, when the
word is restricted to, for example, a verb, any lexi-
cal item with one of the numerous more specific verb
categories can be used. In contrast, in these experi-
ments, the lexicon plays no part. The POS tag causes
one underspecified lexical item (per POS tag) to be
considered in parsing. While these underspecified
items may allow more analyses to be built than if
the exact category was used, the main contribution
to parsing time turned out to be the number of tags
assigned to each word, whether that was a POS tag
or a lexical type. The POS tagger assigned multiple
tags much less frequently than the multiple tagging
lexical type tagger and so had a faster average pars-
ing time. The single tagging lexical type tagger had
only slightly fewer tags assigned overall, and hence
was slightly faster, but at the expense of a signifi-
cantly lower coverage.
5 Conclusion
The work reported here shows the benefits that can
be gained by utilising external resources to anno-
tate parser input in highly lexicalised grammar for-
malisms. Even something as simple and readily
available (for languages likely to have lexicalised
grammars) as a POS tagger can massively increase
the parser coverage on unseen text. While annotat-
ing with named entity data or a lexical type supertag-
ger were also found to increase coverage, the POS
tagger had the greatest effect with up to 45% cover-
age increase on unseen text.
In terms of efficiency, POS tags were also shown
to speed up parsing by filtering unlikely lexicon
items, but better results were achieved in this case
by using a lexical type supertagger. Again encour-
aging the use of external resources, the supertagging
was found to be much more effective when POS tags
619
were used to train the tagging model, and in this con-
figuration, managed to halve the parsing time with
minimal effect on coverage or precision.
6 Further Work
A number of avenues of future research were sug-
gested by the observations made during this work.
In terms of robustness and increasing lexical cover-
age, more work into using lexical types for unknown
words could be explored. In light of the encourag-
ing results for German, one area to look at is the ef-
fect of different features for different languages. Use
of back-off models might also be worth considering
when the tagger probabilities are low.
Different methods of using the supertagger could
also be explored. The experiment reported here used
the single most probable type for restricting the lex-
icon entries used by the parser. Two extensions of
this are obvious. The first is to use multiple tags
over a certain threshold, by either inputting multi-
ple types as was done for the unknown word han-
dling, or by using a generic type that is compatible
with all the predicted types over a certain threshold.
The other possible direction to try is to not check
the predicted type against the lexicon, but to simply
construct a lexical item from the most likely type,
given a (high) threshold probability. This would be
similar to the CCG supertagging mechanism and is
likely to give generous speedups at the possible ex-
pense of precision, but it would be illuminating to
discover how this trade-off plays out in our setup.
References
Timothy Baldwin, Emily M. Bender, Dan Flickinger, Ara
Kim, and Stephan Oepen. 2004. Road-testing the
English Resource Grammar over the British National
Corpus. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC 2004), pages 2047?50, Lisbon, Portugal.
Timothy Baldwin. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of the
ACL-SIGLEX 2005 Workshop on Deep Lexical Acqui-
sition, pages 67?76, Ann Arbor, USA.
Srinivas Bangalore and Aravind K. Joshi. 1994. Dis-
ambiguation of super parts of speech (or supertags):
Almost parsing. In Proceedings of the 15th COLING
Conference, pages 154?160, Kyoto, Japan.
Markus Becker, Witold Drozdzynski, Hans-Ulrich
Krieger, Jakub Piskorski, Ulrich Scha?fer, and Feiyu
Xu. 2002. SProUT - Shallow Processing with Typed
Feature Structures and Unification. In Proceedings of
the International Conference on NLP (ICON 2002),
Mumbai, India.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalised statistical parser on the
PARC DepBank. In Proceedings of the 44th Annual
Meeting of the ACL, pages 41?48, Sydney, Australia.
Ulrich Callmeier. 2000. PET - a platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6(1):99?107.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark. 2002. Supertagging for combinatory cat-
egorical grammar. In Proceedings of the 6th Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Frameworks, pages 101?106, Venice, Italy.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of the Second conference on Language Resources
and Evaluation (LREC-2000), Athens, Greece.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of the
39th Annual Meeting of the ACL and 10th Conference
of the EACL (ACL-EACL 2001), Toulouse, France.
Berthold Crysmann. 2003. On the efficient implemen-
tation of German verb placement in HPSG. In Pro-
ceedings of RANLP 2003, pages 112?116, Borovets,
Bulgaria.
Julia Hockenmaier, Gann Bierner, and Jason Baldridge.
2002. Extending the coverage of a CCG system. Re-
search in Language and Computation.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and
Rob Malouf. 1999. A bag of useful techniques for ef-
ficient and robust parsing. In Proceedings of the 37th
Annual Meeting of the ACL, pages 473?480, Mary-
land, USA.
Stefan Mu?ller and Walter Kasper. 2000. HPSG analysis
of German. In Verbmobil: Foundations of Speech-to-
Speech Translation, pages 238?253. Springer, Berlin,
Germany.
Jeremy Nicholson, Valia Kordoni, Yi Zhang, Timothy
Baldwin, and Rebecca Dridan. 2008. Evaluating and
extending the coverage of HPSG grammars. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC 2008), Mar-
rakech, Morocco.
620
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef van
Genabith, and Andy Way. 2005. Large-scale induc-
tion and evaluation of lexical resources from the Penn-
II and Penn-III treebanks. Computational Linguistics,
31:pp 329?366.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik
Velldal, Dorothee Beermann, John Carroll, Dan
Flickinger, Lars Hellan, Janne Bondi Johannessen,
Paul Meurer, Torbj?rn Nordga?rd, and Victoria Rose?n.
2004. Soma? kapp-ete med trollet? Towards MRS-
based Norwegian?English machine translation. In
Proceedings of the 10th International Conference on
Theoretical and Methodological Issues in Machine
Translation, Baltimore, USA.
Stephan Oepen. 2001. [incr tsdb()] ? competence and
performance laboratory. User manual, Computational
Linguistics, Saarland University, Saarbru?cken, Ger-
many.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago, USA.
Robbert Prins and Gertjan van Noord. 2003. Reinforcing
parser preferences through tagging. Traitement Au-
tomatique des Langues, 44(3):121?139.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Proceedings of the 45th Annual Meeting of the ACL,
pages 624?631, Prague, Czech Republic.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
Gertjan van Noord and Robert Malouf. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In IJCNLP-04 Workshop Beyond Shallow Anal-
yses ? Formalisms and statistical modelling for deep
analyses.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer-
Verlag, Berlin.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
pages 275?280, Genoa, Italy.
621
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 54?61,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Interpretation of Compound Nominalisations using Corpus and Web
Statistics
Jeremy Nicholson and Timothy Baldwin
Department of Computer Science and Software Engineering
University of Melbourne, VIC 3010, Australia
and
NICTA Victoria Research Laboratories
University of Melbourne, VIC 3010, Australia
{jeremymn,tim}@csse.unimelb.edu.au
Abstract
We present two novel paraphrase tests for
automatically predicting the inherent se-
mantic relation of a given compound nom-
inalisation as one of subject, direct object,
or prepositional object. We compare these
to the usual verb?argument paraphrase test
using corpus statistics, and frequencies ob-
tained by scraping the Google search en-
gine interface. We also implemented a
more robust statistical measure than max-
imum likelihood estimation ? the con-
fidence interval. A significant reduction
in data sparseness was achieved, but this
alone is insufficient to provide a substan-
tial performance improvement.
1 Introduction
Compound nouns are a class of multiword expres-
sion (MWE) that have been of interest in recent
computational linguistic work, as any task with a
lexical semantic dimension (like machine transla-
tion or information extraction) must take into ac-
count their semantic markedness. A compound
noun is a sequence of two or more nouns compris-
ing an N? , for example, polystyrene garden-gnome.
The productivity of compound nouns makes their
treatment equally desirable and difficult. They ap-
pear frequently: more than 1% of the words in the
British National Corpus (BNC: Burnard (2000))
participate in noun compounds (Tanaka and Bald-
win, 2003). However, unestablished compounds
are common: almost 70% of compounds identi-
fied in the BNC co-occur with a frequency of only
one (Lapata and Lascarides, 2003).
Analysis of the entire space of compound nouns
has been hampered to some degree as the space de-
fies some regular set of predicates to define the im-
plicit semantics between a modifier and its head.
This semantic underspecification led early analy-
sis to be primarily of a semantic nature, but more
recent work has advanced into using syntax to pre-
dict the semantics, in the spirit of the study by
Levin (1993) on diathesis alternations.
In this work, we examine compound nominal-
isations, a subset of compound nouns where the
head has a morphologically?related verb. For
example, product replacement has an underlying
verbal head replace, whereas garden-gnome has
no such form. While compound nouns in gen-
eral have a set of semantic relationships between
the head and modifier that is potentially non-finite,
compound nominalisations are better defined, in
that the modifier fills a syntactic argument rela-
tion with respect to the head. For example, prod-
uct might fill the direct object slot of the verb
to replace for the compound above. Compound
nominalisations comprise a substantial minority of
compound nouns, with figures of about 35% being
observed (Grover et al, 2005; Nicholson, 2005).
We propose two novel paraphrases for a corpus
statistical approach to predicting the relationship
for a set of compound nominalisations, and inves-
tigate how using the World Wide Web as a cor-
pus alleviates the common phenomenon of data
sparseness, and how the volume of data impacts
on the classification results. We also examine
a more robust statistical approach to interpreta-
tion of the statistics than maximum likelihood es-
timates, called the confidence interval.
The rest of the paper is structured as follows: in
Section 2, we present a brief background for our
work, with a listing of our resources in Section 3.
We detail our proposed method in Section 4, the
corresponding results in Section 5, with a discus-
54
sion in Section 6 and a brief conclusion in Sec-
tion 7.
2 Background
2.1 Compound Noun Interpretation
Compound nouns were seminally and thoroughly
analysed by Levi (1978), who hand?constructs a
nine?way set of semantic relations that she identi-
fies as broadly defining the observed relationships
between the compound head and modifier. War-
ren (1978) also inspects the syntax of compound
nouns, to create a somewhat different set of twelve
conceptual categories.
Early attempts to automatically classify com-
pound nouns have taken a semantic approach:
Finin (1980) and Isabelle (1984) use ?role nomi-
nals? derived from the head of the compound to
fill a slot with the modifier. Vanderwende (1994)
uses a rule?based technique that scores a com-
pound on possible semantic interpretations, while
Jones (1995) implements a graph?based unifica-
tion procedure over semantic feature structures for
the head. Finally, Rosario and Hearst (2001) make
use of a domain?specific lexical resource to clas-
sify according to neural networks and decision
trees.
Syntactic classification, using paraphrasing,
was first used by Leonard (1984), who uses a pri-
oritised rule?based approach across a number of
possible readings. Lauer (1995) employs a cor-
pus statistical model over a similar paraphrase
set based on prepositions. Lapata (2002) and
Grover et al (2005) again use a corpus statis-
tical paraphrase?based approach, but with verb?
argument relations for compound nominalisations
? attempting to define the relation as one of sub-
ject, direct object, or a number of prepositional ob-
jects in the latter.
2.2 Web?as?Corpus Approaches
Using the World Wide Web for corpus statistics
is a relatively recent phenomenon; we present a
few notable examples. Grefenstette (1998) anal-
yses the plausibility of candidate translations in
a machine translation task through Web statistics,
and avoids some data sparseness within that con-
text. Zhu and Rosenfeld (2001) train a language
model from a large corpus, and use the Web to
estimate low?density trigram frequencies. Keller
and Lapata (2003) show that Web counts can
obviate data sparseness for syntactic predicate?
argument bigrams. They also observe that the
noisiness of the Web, while unexplored in detail,
does not greatly reduce the reliability of their re-
sults. Nakov and Hearst (2005) demonstrate that
Web counts can aid in identifying the bracketing in
higher?arity noun compounds. Finally, Lapata and
Keller (2005) evaluate the performance of Web
counts on a wide range of natural language pro-
cessing tasks, including compound noun bracket-
ing and compound noun interpretation.
2.3 Confidence Intervals
Maximum likelihood statistics are not robust when
many sparse vectors are under consideration, i.e.
naively ?choosing the largest number? may not be
accurate in contexts when the relative value across
samplings may be relevant, for example, in ma-
chine learning. As such, we apply a statistical
test with confidence intervals (Kenney and Keep-
ing, 1962), where we compare sample z-scores in
a pairwise manner, instead of frequencies globally.
The confidence interval P , for z-score n, is:
P = 2?pi
? n/
?
2
0
e?t2dt (1)
t is chosen to normalise the curve, and P is strictly
increasing on n, so we are only required to find the
largest z-score.
Calculating the z-score exactly can be quite
costly, so we instead use the binomial approxi-
mation to the normal distribution with equal prior
probabilities and find that a given z-score Z is:
Z = f ? ?? (2)
where f is the frequency count, ? is the mean in
a pairwise test, and ? is the standard deviation of
the test. A more complete derivation appears in
Nicholson (2005).
3 Resources
We make use of a number of lexical resources
in our implementation and evaluation. For cor-
pus statistics, we use the written component of
the BNC, a balanced 90M token corpus. To find
verb?argument frequencies, we parse this using
RASP (Briscoe and Carroll, 2002), a tag sequence
grammar?based statistical parser. We contrast
the corpus statistics with ones collected from the
55
Web, using an implementation of a freely avail-
able Google ?scraper? from CPAN.1
For a given compound nominalisation, we wish
to determine all possible verbal forms of the head.
We do so using the combination of the morpho-
logical component of CELEX (Burnage, 1990), a
lexical database, NOMLEX (Macleod et al, 1998),
a nominalisation database, and CATVAR (Habash
and Dorr, 2003), an automatically?constructed
database of clusters of inflected words based on
the Porter stemmer (Porter, 1997).
Once the verbal forms have been identified, we
construct canonical forms of the present partici-
ple (+ing) and the past participle (+ed), using the
morph lemmatiser (Minnen et al, 2001). We con-
struct canonical forms of the plural head and plural
modifier (+s) in the same manner.
For evaluation, we have the two?way classified
data set used by Lapata (2002), and a three?way
classified data set constructed from open text.
Lapata automatically extracts candidates from
the British National Corpus, and hand?curates
a set of 796 compound nominalisations which
were interpreted as either a subjective relation
SUBJ (e.g. wood appearance ?wood appears?),
or a (direct) objective relation OBJ (e.g. stress
avoidance ?[SO] avoids stress?. We automatically
validated this data set for consistency, removing:
1. items that did not occur in the same chunk,
according to a chunker based on fnTBL 1.0
(Ngai and Florian, 2001),
2. items whose head did not have a verbal form
according to our lexical resources, and
3. items which consisted in part of proper
nouns,
to end up with 695 consistent compounds. We
used the method of Nicholson and Baldwin (2005)
to derive a small data set of 129 compound
nominalisations, also from the BNC, which we
instructed three unskilled annotators to identify
each as one of subjective (SUB), direct object
(DOB), or prepositional object (POB, e.g. side
show ?[SO] show [ST] on the side?). The an-
notators identified nine prepositional relations:
{about,against,for,from,in,into,on,to,with}.
1www.cpan.org: We limit our usage to examining the
?Estimated Results Returned?, so that our usage is identi-
cal to running the queries manually from the website. The
Google API (www.google.com/apis) gives a method
for examining the actual text of the returned documents.
4 Proposed Method
4.1 Paraphrase Tests
To derive preferences for the SUB, DOB, and var-
ious POB interpretations for a given compound
nominalisation, the most obvious approach is to
examine a parsed corpus for instances of the verbal
form of the head and the modifier occurring in the
corresponding verb?argument relation. There are
other constructions that can be informative, how-
ever.
We examine two novel paraphrase tests: one
prepositional and one participial. The preposi-
tional test is based in part on the work by Leonard
(1984) and Lauer (1995): for a given compound,
we search for instances of the head and modifier
nouns separated by a preposition. For example,
for the compound nominalisation leg operation,
we might search for operation on the leg, corre-
sponding to the POB relation on. Special cases are
by, corresponding to a subjective reading akin to a
passive construction (e.g. investor hesitancy, hesi-
tancy by the investor ? ?the investor hesitates?),
and of, corresponding to a direct object reading
(e.g. language speaker, speaker of the language
? ?[SO] speaks the language?).
The participial test is based on the paraphras-
ing equivalence of using the present participle of
the verbal head as an adjective before the modifier,
for the SUB relation (e.g. the hesitating investor ?
?the investor hesitates?), compared to the past par-
ticiple for the DOB relation (the spoken language
? ?[SO] speaks the language?). The correspond-
ing prepositional object construction is unusual in
English, but still possible: compare ?the operated-
on leg and the lived-in village.
4.2 The Algorithm
Given a compound nominalisation, we perform a
number of steps to arrive at an interpretation. First,
we derive a set of verbal forms for the head from
the combination of CELEX, NOMLEX, and CAT-
VAR. We find the participial forms of each of the
verbal heads, and plurals for the nominal head and
modifier, using the morph lemmatiser.
Next, we examine the BNC for instances of the
modifier and one of the verbal head forms oc-
curring in a verb?argument relation, with the aid
of the RASP parse. Using these frequencies, we
calculate the pairwise z-scores between SUB and
DOB, and between SUB and POB: the score given
to the SUB interpretation is the greater of the two.
56
We further examine the RASP parsed data for in-
stances of the prepositional and participial tests for
the compound, and calculate the z-scores for these
as well.
We then collect our Google counts. Because the
Web data is unparsed, we cannot look for syntactic
structures explictly. Instead, we query a number of
collocations which we expect to be representative
of the desired structure.
For the prepositional test, the head can be sin-
gular or plural, the modifier can be singular or plu-
ral, and there may or may not be an article be-
tween the preposition and the modifier. For exam-
ple, for the compound nominalisation product re-
placement and preposition of we search for all of
the following: (and similarly for the other prepo-
sitions)
replacement of product
replacement of the product
replacement of products
replacement of the products
replacements of product
replacements of the product
replacements of products
replacements of the products
For the participial test, the modifier can be sin-
gular or plural, and if we are examining a prepo-
sitional relation, the head can be either a present
or past participle. For product replacement, we
search for, as well as other prepositions:
the replacing product
the replacing products
the replaced product
the replaced products
the replacing?about product
the replacing?about products
the replaced?about product
the replaced?about products
We comment briefly on these tests in Section 6.
We choose to use the as our canonical article be-
cause it is a reliable marker of the left boundary of
an NP and number-neutral; using a/an represents
a needless complication.
We then calculate the z-scores using the method
described in Section 2, where the individual fre-
quency counts are the maximum of the results ob-
tained across the query set.
Once the z-scores have been obtained, we
choose a classification based on the greatest-
valued observed test. We contrast the confidence
interval?based approach with the maximum like-
lihood method of choosing the largest of the raw
frequencies. We also experiment with a machine
learning package, to examine the mutual predic-
tiveness of the separate tests.
5 Observed Results
First, we found majority-class baselines for each
of the data sets. The two?way data set had
258 SUBJ?classified items, and 437 OBJ?classified
items, so choosing OBJ each time gives a baseline
of 62.9%. The three?way set had 22 SUB items,
63 of DOB, and 44 of POB, giving a baseline of
48.8%.
Contrasting this with human performance on
the data set, Lapata recorded a raw inter-annotator
agreement of 89.7% on her test set, which cor-
responds to a Kappa value ? = 0.78. On the
three?way data set, three annotators had a agree-
ment of 98.4% for identification and classification
of observed compound nominalisations in open
text, and ? = 0.83. For the three-way data set,
the annotators were asked to both identify and
classify compound nominalisations in free text,
and agreement is thus calculated over all words
in the test. The high agreement figure is due to
the fact that most words could be trivially disre-
garded (e.g. were not nouns). Kappa corrects this
for chance agreement, so we conclude that this
task was still better-defined than the one posed
by Lapata. One possible reason for this was the
number of poorly?behaved compounds that we re-
moved due to chunk inconsistencies, lack of a ver-
bal form, or proper nouns: it would be difficult for
the annotators to agree over compounds where an
obvious well?defined interpretation was not avail-
able.
5.1 Comparison Classification
Results for classification over the Lapata two?way
data set are given in Table 1, and results over
the open data three?way set are given in Table 2.
For these, we selected the greatest raw frequency
count for a given test as the intended relation
(Raw), or the greatest confidence interval accord-
ing to the z-score (Z-Score). If a relation could not
be selected due to ties (e.g., the scores were all 0),
we selected the majority baseline. To deal with the
nature of the two?way data set with respect to our
three?way selection, we mapped compounds that
we would prefer to be POB to OBJ, as there are
57
Paraphrase Default Corpus Counts Web Counts
Raw Z-Score Raw Z-Score
Verb?Argument 62.9 67.9 68.3 ? ?
Prepositional 62.9 62.1 62.4 62.6 63.0
Participial 62.9 63.0 63.2 61.4 58.8
Table 1: Classification Results over the two?way data set, in %. Comparison of raw frequency counts
vs. confidence?based z-scores, for BNC data and Google scrapings shown.
Paraphrase Default Corpus Counts Web Counts
Raw Z-Score Raw Z-Score
Verb?Argument 48.8 54.3 55.0 ? ?
Prepositional 48.8 48.4 50.0 59.7 58.9
Participial 48.8 43.2 45.4 43.4 38.0
Table 2: Classification results over the three-way data set, in %. Comparison of raw frequency counts
vs. confidence-based z-scores, for BNC data and Google scrapings shown.
compounds in the set (e.g. adult provision) that
have a prepositional object reading (?provide for
adults?) but have been classified as a direct object
OBJ.
The verb?argument counts obtained from the
parsed BNC are significantly better than the base-
line for the Lapata data set (?2 = 4.12, p ? 0.05),
but not significantly better for the open data set
(?2 = 0.99, p ? 1). Similar results were reported
by Lapata (2002) over her data set using backed?
off smoothing, the most closely related method.
Neither the prepositional nor participial para-
phrases were significantly better than the baseline
for either the two?way (?2 = 0.00, p ? 1), or
the three?way data set (?2 = 3.52, p ? 0.10), al-
though the prepositional test did slightly improve
on the verb?argument results.
5.2 Machine Learning Classification
Although the results were not impressive, we still
believed that there was complementing informa-
tion within the data, which could be extracted with
the aid of a machine learner. For this, we made
use of TiMBL (Daelemans et al, 2003), a nearest-
neighbour classifier which stores the entire train-
ing set and extrapolates further samples, as a prin-
cipled method for combination of the data. We use
TiMBL?s in-built cross-validation method: 90% of
the data set is used as training data to test the other
10%, for each stratified tenth of the set. The results
it achieves are assumed to be able to generalise to
new samples if they are compared to the current
training data set.
The results observed using TiMBL are shown
Corpus Counts Web Counts
Two?way Set 72.4 74.2
Three?way Set 51.1 50.4
Table 3: TiMBL results for the combination of
paraphrase tests over the two?way and three?way
data sets for corpus and Web frequencies
in Table 3. This was from the combination
of all of the available paraphrase tests: verb?
argument, prepositional, and participial for the
corpus counts, and just prepositional and particip-
ial for the Web counts. The results for the two?
way data set derived from Lapata?s data set were a
good improvement over the simple classification
results, significantly so for the Web frequencies
(?2 = 20.3, p ? 0.01). However, we also no-
tice a corresponding decrease in the results for the
three?way open data set, which make these im-
provements immaterial.
Examining the other possible combinations for
the tests did indeed lead to varying results, but not
in a consistent manner. For example, the best com-
bination for the open data set was using the par-
ticipial raw scores and z-scores (58.1%), which
performed particularly badly in simple compar-
isons, and comparatively poorly (70.2%) for the
two?way set.
6 Discussion
Although the observed results failed to match, or
even approach, various benchmarks set by La-
pata (2002) (87.3% accuracy) and Grover et al
(2005) (77%) for the subject?object and subject?
58
direct object?prepositional objects classification
tasks respectively, the presented approach is not
without merit. Indeed, these results relied on
machine learning approaches incorporating many
features independent of corpus counts: namely,
context, suffix information, and semantic similar-
ity resources. Our results were an examination
of the possible contribution of lexical information
available from high?volume unparsed text.
One important concept used in the above bench-
marks was that of statistical smoothing, both
class?based and distance?based. The reason for
this is the inherent data sparseness within the
corpus statistics for these paraphrase tests. La-
pata (2002) observes that almost half (47%) of
the verb?noun pairs constructed are not attested
within the BNC. Grover et al (2005) also note the
sparseness of observed relations. Using the im-
mense data source of the Web allows one to cir-
cumvent this problem: only one compound (an-
archist prohibition) has no instances of the para-
phrases from the scraping,2 from more than 900
compounds between the two data sets. This ex-
tra information, we surmise, would be beneficial
for the smoothing procedures, as the comparative
accuracy between the two methods is similar.
On the other hand, we also observe that sim-
ply alleviating the data sparseness is insufficient
to provide a reliable interpretation. These results
reinforce the contribution made by the statistical
and semantic resources used in arriving at these
benchmarks.
The approach suggested by Keller and Lapata
(2003) for obtaining bigram information from the
Web could provide an approach for estimating the
syntactic verb?argument counts for a given com-
pound (dashes in Tables 1 and 2). In spite of
the inherent unreliability of approximating long?
range dependencies with n-gram information, re-
sults look promising. An examination of the effec-
tiveness of this approach is left as further research.
Similarly, various methods of combining corpus
counts with the Web counts, including smooth-
ing, backing?off, and machine learning, could also
lead to interesting performance impacts.
Another item of interest is the comparative dif-
ficulty of the task presented by the three?way data
set extracted from open data, and the two?way
data set hand?curated by Lapata. The baseline
2Interestingly, Google only lists 3 occurrences of this
compound anyway, so token relevance is low ? further in-
spection shows that those 3 are not well-formed in any case.
of this set is much lower, even compared that
of the similar task (albeit domain?specific) from
Grover et al (2005) of 58.6%. We posit that the
hand?filtering of the data set in these works con-
tributes to a biased sample. For example, remov-
ing prepositional objects for a two?way classifica-
tion, which make up about a third of the open data
set, renders the task somewhat artificial.
Comparison of the results between the maxi-
mum likelihood estimates used in earlier work,
and the more statistically robust confidence inter-
vals were inconclusive as to performance improve-
ment, and were most effective as a feature expan-
sion algorithm. The only obvious result is an aes-
thetic one, in using ?robust statistics?.
Finally, the paraphrase tests which we propose
are not without drawbacks. In the prepositional
test, a paraphrase with of does not strictly con-
tribute to a direct object reading: consider school
aim ?school aims?, for which instances of aim by
the school are overwhelmed by aim of the school.
We experimented with permutations of the avail-
able queries (e.g. requiring the head and modifier
to be of different number, to reflect the pluralis-
ability of the head in such compounds, e.g. aims
of the school), without observing substantially dif-
ferent results.
Another observation is the inherent bias of the
prepositional test to the prepositional object re-
lation. Apparent prepositional relations can oc-
cur in spite of the available verb frames: con-
sider cash limitation, where the most populous in-
stance is limitation on cash, despite the impossi-
bility of *to limit on cash (for to place a limit on
cash). Another example, is bank agreement: find-
ing instances of agreement with bank does not lead
to the pragmatically absurd [SO] agrees with the
bank.
Correspondingly, the participial relation has the
opposite bias: constructions of the form the lived-
in at ?[SO] lived in the flat? are usually lexi-
calised in English. As such, only 17% of com-
pounds in the two?way data set and 34% of the
three-way data set display non-zero values in the
prepositional object relation for the participial test.
We hoped that the inherent biases of the two tests
might balance each other, but there is little evi-
dence of that from the results.
59
7 Conclusion
We presented two novel paraphrase tests for au-
tomatically predicting the inherent semantic rela-
tion of a given compound nominalisation as one of
subject, direct object, or prepositional object. We
compared these to the usual verb?argument para-
phrase test, using corpus statistics, and frequen-
cies obtained by scraping the Google search en-
gine. We also implemented a more robust statisti-
cal measure than the insipid maximum likelihood
estimates ? the confidence interval. A significant
reduction in data sparseness was achieved, but this
alone is insufficient to provide a substantial per-
formance improvement.
Acknowledgements
We would like to thank the members of the Univer-
sity of Melbourne LT group and the three anony-
mous reviewers for their valuable input on this re-
search, as well as Mirella Lapata for allowing use
of the data.
References
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504, Las
Palmas, Canary Islands.
Gavin Burnage. 1990. CELEX: A guide for users.
Technical report, University of Nijmegen.
Lou Burnard. 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide.
ILK Technical Report 03-10.
Tim Finin. 1980. The semantic interpretation of nom-
inal compounds. In Proceedings of the First Na-
tional Conference on Artificial Intelligence, pages
310?315, Stanford, USA. AAAI Press.
Gregory Grefenstette. 1998. The World Wide Web
as a resource for example-based machine translation
tasks. In Proceedings of the ASLIB Conference on
Translating and the Computer, London, UK.
Claire Grover, Mirella Lapata, and Alex Lascarides.
2005. A comparison of parsing technologies for the
biomedical domain. Journal of Natural Language
Engineering, 11(01):27?65.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proceedings of
the 2003 Human Language Technology Conference
of the North American Chapter of the ACL, pages
17?23, Edmonton, Canada.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceeedings of the 10th International
Conference on Computational Linguistics and 22nd
Annual Meeting of the ACL, pages 509?516, Stan-
ford, USA.
Bernard Jones. 1995. Predicating nominal com-
pounds. In Proceedings of the 17th International
Conference of the Cognitive Science Society, pages
130?5, Pittsburgh, USA.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
John F. Kenney and E. S. Keeping, 1962. Mathematics
of Statistics, Pt. 1, chapter 11.4, pages 167?9. Van
Nostrand, Princeton, USA, 3rd edition.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1).
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional
evidence. In Proceedings of the 10th Conference
of the European Chapter of the Association for
Computional Linguistics, pages 235?242, Budapest,
Hungary.
Maria Lapata. 2002. The disambiguation of nomi-
nalizations. Computational Linguistics, 28(3):357?
388.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University, Sydney, Australia.
Rosemary Leonard. 1984. The Interpretation of En-
glish Noun Sequences on the Computer. Elsevier
Science, Amsterdam, the Netherlands.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, USA.
Beth Levin. 1993. English Verb Classes and Alter-
nations. The University of Chicago Press, Chicago,
USA.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In Proceedings of the
8th International Congress of the European Associ-
ation for Lexicography, pages 187?193, Liege, Bel-
gium.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?23.
60
Preslov Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to
noun compound bracketing. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning, pages 17?24, Ann Arbor, USA.
Grace Ngai and Radu Florian. 2001. Transformation-
based learning in the fast lane. In Proceedings of the
2nd Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 40?7, Pittsburgh, USA.
Jeremy Nicholson and Timothy Baldwin. 2005. Sta-
tistical interpretation of compound nominalisations.
In Proceeding of the Australasian Langugae Tech-
nology Workshop 2005, Sydney, Australia.
Jeremy Nicholson. 2005. Statistical interpretation of
compound nouns. Honours Thesis, University of
Melbourne, Melbourne, Australia.
Martin Porter. 1997. An algorithm for suffix strip-
ping. In Karen Sparck Jones and Peter Willett,
editors, Readings in information retrieval. Morgan
Kaufmann, San Francisco, USA.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 6th Conference on Empirical Methods in Natural
Language Processing, Pittsburgh, USA.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of
the ACL 2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment, pages 17?24,
Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 782?788, Kyoto, Japan.
Beatrice Warren. 1978. Semantic Patterns of Noun-
Noun Compounds. Acta Universitatis Gothoburgen-
sis, Go?teborg, Sweden.
Xiaojin Zhu and Ronald Rosenfeld. 2001. Improv-
ing trigram language modeling with the World Wide
Web. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing,
pages 533?6, Salt Lake City, USA.
61
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Chart Mining-based Lexical Acquisition with Precision Grammars
Yi Zhang,? Timothy Baldwin,?? Valia Kordoni,? David Martinez? and Jeremy Nicholson??
? DFKI GmbH and Dept of Computational Linguistics, Saarland University, Germany
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
yzhang@coli.uni-sb.de, tb@ldwin.net, kordoni@dfki.de,
{davidm,jeremymn}@csse.unimelb.edu.au
Abstract
In this paper, we present an innovative chart
mining technique for improving parse cover-
age based on partial parse outputs from preci-
sion grammars. The general approach of min-
ing features from partial analyses is applica-
ble to a range of lexical acquisition tasks, and
is particularly suited to domain-specific lexi-
cal tuning and lexical acquisition using low-
coverage grammars. As an illustration of the
functionality of our proposed technique, we
develop a lexical acquisition model for En-
glish verb particle constructions which oper-
ates over unlexicalised features mined from
a partial parsing chart. The proposed tech-
nique is shown to outperform a state-of-the-art
parser over the target task, despite being based
on relatively simplistic features.
1 Introduction
Parsing with precision grammars is increasingly
achieving broad coverage over open-domain texts
for a range of constraint-based frameworks (e.g.,
TAG, LFG, HPSG and CCG), and is being used in
real-world applications including information ex-
traction, question answering, grammar checking and
machine translation (Uszkoreit, 2002; Oepen et al,
2004; Frank et al, 2006; Zhang and Kordoni, 2008;
MacKinlay et al, 2009). In this context, a ?preci-
sion grammar? is a grammar which has been engi-
neered to model grammaticality, and contrasts with
a treebank-induced grammar, for example.
Inevitably, however, such applications demand
complete parsing outputs, based on the assumption
that the text under investigation will be completely
analysable by the grammar. As precision grammars
generally make strong assumptions about complete
lexical coverage and grammaticality of the input,
their utility is limited over noisy or domain-specific
data. This lack of complete coverage can make
parsing with precision grammars less attractive than
parsing with shallower methods.
One technique that has been successfully applied
to improve parser and grammar coverage over a
given corpus is error mining (van Noord, 2004;
de Kok et al, 2009), whereby n-grams with low
?parsability? are gathered from the large-scale out-
put of a parser as an indication of parser or (pre-
cision) grammar errors. However, error mining is
very much oriented towards grammar engineering:
its results are a mixture of different (mistreated) lin-
guistic phenomena together with engineering errors
for the grammar engineer to work through and act
upon. Additionally, it generally does not provide
any insight into the cause of the parser failure, and it
is difficult to identify specific language phenomena
from the output.
In this paper, we instead propose a chart min-
ing technique that works on intermediate parsing re-
sults from a parsing chart. In essence, the method
analyses the validity of different analyses for words
or constructions based on the ?lifetime? and prob-
ability of each within the chart, combining the con-
straints of the grammar with probabilities to evaluate
the plausibility of each.
For purposes of exemplification of the proposed
technique, we apply chart mining to a deep lexical
acquisition (DLA) task, using a maximum entropy-
based prediction model trained over a seed lexicon
and treebank. The experimental set up is the fol-
lowing: given a set of sentences containing puta-
tive instances of English verb particle constructions,
10
extract a list of non-compositional VPCs optionally
with valence information. For comparison, we parse
the same sentence set using a state-of-the-art statisti-
cal parser, and extract the VPCs from the parser out-
put. Our results show that our chart mining method
produces a model which is superior to the treebank
parser.
To our knowledge, the only other work that has
looked at partial parsing results of precision gram-
mars as a means of linguistic error analysis is that of
Kiefer et al (1999) and Zhang et al (2007a), where
partial parsing models were proposed to select a set
of passive edges that together cover the input se-
quence. Compared to these approaches, our pro-
posed chart mining technique is more general and
can be adapted to specific tasks and domains. While
we experiment exclusively with an HPSG grammar
in this paper, it is important to note that the proposed
method can be applied to any grammar formalism
which is compatible with chart parsing, and where it
is possible to describe an unlexicalised lexical entry
for the different categories of lexical item that are to
be extracted (see Section 3.2 for details).
The remainder of the paper is organised as fol-
lows. Section 2 defines the task of VPC extraction.
Section 3 presents the chart mining technique and
the feature extraction process for the VPC extraction
task. Section 4 evaluates the model performance
with comparison to two competitor models over sev-
eral different measures. Section 5 further discusses
the general applicability of chart mining. Finally,
Section 6 concludes the paper.
2 Verb Particle Constructions
The particular construction type we target for DLA
in this paper is English Verb Particle Constructions
(henceforth VPCs). VPCs consist of a head verb
and one or more obligatory particles, in the form
of intransitive prepositions (e.g., hand in), adjec-
tives (e.g., cut short) or verbs (e.g., let go) (Villav-
icencio and Copestake, 2002; Huddleston and Pul-
lum, 2002; Baldwin and Kim, 2009); for the pur-
poses of our dataset, we assume that all particles are
prepositional?by far the most common and produc-
tive of the three types?and further restrict our atten-
tion to single-particle VPCs (i.e., we ignore VPCs
such as get alng together).
One aspect of VPCs that makes them a partic-
ularly challenging target for lexical acquisition is
that the verb and particle can be non-contiguous (for
instance, hand the paper in and battle right on).
This sets them apart from conventional collocations
and terminology (cf., Manning and Schu?tze (1999),
Smadja (1993) and McKeown and Radev (2000))
in that they cannot be captured effectively using n-
grams, due to their variability in the number and type
of words potentially interceding between the verb
and the particle. Also, while conventional colloca-
tions generally take the form of compound nouns
or adjective?noun combinations with relatively sim-
ple syntactic structure, VPCs occur with a range of
valences. Furthermore, VPCs are highly productive
in English and vary in use across domains, making
them a prime target for lexical acquisition (Dehe?,
2002; Baldwin, 2005; Baldwin and Kim, 2009).
In the VPC dataset we use, there is an addi-
tional distinction between compositional and non-
compositional VPCs. With compositional VPCs,
the semantics of the verb and particle both corre-
spond to the semantics of the respective simplex
words, including the possibility of the semantics be-
ing specific to the VPC construction in the case of
particles. For example, battle on would be clas-
sified as compositional, as the semantics of bat-
tle is identical to that for the simplex verb, and
the semantics of on corresponds to the continua-
tive sense of the word as occurs productively in
VPCs (cf., walk/dance/drive/govern/... on). With
non-compositional VPCs, on the other hand, the
semantics of the VPC is somehow removed from
that of the parts. In the dataset we used for eval-
uation, we are interested in extracting exclusively
non-compositional VPCs, as they require lexicalisa-
tion; compositional VPCs can be captured via lexi-
cal rules and are hence not the target of extraction.
English VPCs can occur with a number of va-
lences, with the two most prevalent and productive
valences being the simple transitive (e.g., hand in
the paper) and intransitive (e.g., back off ). For the
purposes of our target task, we focus exclusively on
these two valence types.
Given the above, we define the English VPC ex-
traction task to be the production of triples of the
form ?v, p, s?, where v is a verb lemma, p is a prepo-
sitional particle, and s ? {intrans , trans} is the va-
11
lence; additionally, each triple has to be semantically
non-compositional. The triples are extracted relative
to a set of putative token instances for each of the
intransitive and transitive valences for a given VPC.
That is, a given triple should be classified as positive
if and only if it is associated with at least one non-
compositional token instance in the provided token-
level data.
The dataset used in this research is the one used
in the LREC 2008 Multiword Expression Workshop
Shared Task (Baldwin, 2008).1 In the dataset, there
is a single file for each of 4,090 candidate VPC
triples, containing up to 50 sentences that have the
given VPC taken from the British National Cor-
pus. When the valence of the VPC is ignored,
the dataset contains 440 unique VPCs among 2,898
VPC candidates. In order to be able to fairly com-
pare our method with a state-of-the-art lexicalised
parser trained over the WSJ training sections of the
Penn Treebank, we remove any VPC types from the
test set which are attested in the WSJ training sec-
tions. This removes 696 VPC types from the test
set, and makes the task even more difficult, as the
remaining testing VPC types are generally less fre-
quent ones. At the same time, it unfortunately means
that our results are not directly comparable to those
for the original shared task.2
3 Chart Mining for Parsing with a Large
Precision Grammar
3.1 The Technique
The chart mining technique we use in this paper
is couched in a constituent-based bottom-up chart
parsing paradigm. A parsing chart is a data struc-
ture that records all the (complete or incomplete) in-
termediate parsing results. Every passive edge on
the parsing chart represents a complete local analy-
sis covering a sub-string of the input, while each ac-
tive edge predicts a potential local analysis. In this
view, a full analysis is merely a passive edge that
spans the whole input and satisfies certain root con-
1Downloadable from http://www.csse.unimelb.
edu.au/research/lt/resources/vpc/vpc.tgz.
2In practice, there was only one team who participated in
the original VPC task (Ramisch et al, 2008), who used a vari-
ety of web- and dictionary-based features suited more to high-
frequency instances in high-density languages, so a simplistic
comparison would not have been meaningful.
ditions. The bottom-up chart parser starts with edges
instantiated from lexical entries corresponding to the
input words. The grammar rules are used to incre-
mentally create longer edges from smaller ones until
no more edges can be added to the chart.
Standardly, the parser returns only outputs that
correspond to passive edges in the parsing chart that
span the full input string. For those inputs without a
full-spanning edge, no output is generated, and the
chart becomes the only source of parsing informa-
tion.
A parsing chart takes the form of a hierarchy of
edges. Where only passive edges are concerned,
each non-lexical edge corresponds to exactly one
grammar rule, and is connected with one or more
daughter edge(s), and zero or more parent edge(s).
Therefore, traversing the chart is relatively straight-
forward.
There are two potential challenges for the chart-
mining technique. First, there is potentially a huge
number of parsing edges in the chart. For in-
stance, when parsing with a large precision gram-
mar like the HPSG English Resource Grammar
(ERG, Flickinger (2002)), it is not unusual for a
20-word sentence to receive over 10,000 passive
edges. In order to achieve high efficiency in pars-
ing (as well as generation), ambiguity packing is
usually used to reduce the number of productive
passive edges on the parsing chart (Tomita, 1985).
For constraint-based grammar frameworks like LFG
and HPSG, subsumption-based packing is used to
achieve a higher packing ratio (Oepen and Carroll,
2000), but this might also potentially lead to an in-
consistent packed parse forest that does not unpack
successfully. For chart mining, this means that not
all passive edges are directly accessible from the
chart. Some of them are packed into others, and the
derivatives of the packed edges are not generated.
Because of the ambiguity packing, zero or more
local analyses may exist for each passive edge on
the chart, and the cross-combination of the packed
daughter edges is not guaranteed to be compatible.
As a result, expensive unification operations must be
reapplied during the unpacking phase. Carroll and
Oepen (2005) and Zhang et al (2007b) have pro-
posed efficient k-best unpacking algorithms that can
selectively extract the most probable readings from
the packed parse forest according to a discrimina-
12
tive parse disambiguation model, by minimising the
number of potential unifications. The algorithm can
be applied to unpack any passive edges. Because
of the dynamic programming used in the algorithm
and the hierarchical structure of the edges, the cost
of the unpacking routine is empirically linear in the
number of desired readings, and O(1) when invoked
more than once on the same edge.
The other challenge concerns the selection of in-
formative and representative pieces of knowledge
from the massive sea of partial analyses in the pars-
ing chart. How to effectively extract the indicative
features for a specific language phenomenon is a
very task-specific question, as we will show in the
context of the VPC extraction task in Section 3.2.
However, general strategies can be applied to gener-
ate parse ranking scores on each passive edge. The
most widely used parse ranking model is the log-
linear model (Abney, 1997; Johnson et al, 1999;
Toutanova et al, 2002). When the model does not
use non-local features, the accumulated score on a
sub-tree under a certain (unpacked) passive edge can
be used to approximate the probability of the partial
analysis conditioned on the sub-string within that
span.3
3.2 The Application: Acquiring Features for
VPC Extraction
As stated above, the target task we use to illustrate
the capabilities of our chart mining method is VPC
extraction.
The grammar we apply our chart mining method
to in this paper is the English Resource Grammar
(ERG, Flickinger (2002)), a large-scale precision
HPSG for English. Note, however, that the method
is equally compatible with any grammar or grammar
formalism which is compatible with chart parsing.
The lexicon of the ERG has been semi-
automatically extended with VPCs extracted
by Baldwin (2005). In order to show the effective-
ness of chart mining in discovering ?unknowns?
and remove any lexical probabilities associated
with pre-existing lexical entries, we block the
3To have a consistent ranking model on any sub-analysis,
one would have to retrain the disambiguation model on every
passive edge. In practice, we find this to be intractable. Also,
the approximation based on full-parse ranking model works rea-
sonably well.
lexical entries for the verb in the candidate VPC
by substituting the input token with a DUMMY-V
token, which is coupled with four candidate lexical
entries of type: (1) intransitive simplex verb (v - e),
(2) transitive simplex verb (v np le), (3) intransitive
VPC (v p le), and (4) transitive VPC (v p-np le),
respectively. These four lexical entries represent the
two VPC valences we wish to distinguish between
in the VPC extraction task, and the competing
simplex verb candidates. Based on these lexical
types, the features we extract with chart mining are
summarised in Table 1. The maximal constituent
(MAXCONS) of a lexical entry is defined to be the
passive edge that is an ancestor of the lexical entry
edge that: (i) must span over the particle, and (ii)
has maximal span length. In the case of a tie,
the edge with the highest disambiguation score is
selected as the MAXCONS. If there is no edge found
on the chart that spans over both the verb and the
particle, the MAXCONS is set to be NULL, with a
MAXSPAN of 0, MAXLEVEL of 0 and MAXCRANK
of 4 (see Table 1). The stem of the particle is also
collected as a feature.
One important characteristic of these features is
that they are completely unlexicalised on the verb.
This not only leads to a fair evaluation with the ERG
by excluding the influence from the lexical coverage
of VPCs in the grammar, but it also demonstrates
that complete grammatical coverage over simplex
verbs is not a prerequisite for chart mining.
To illustrate how our method works, we present
the unpacked parsing chart for the candidate VPC
show off and input sentence The boy shows off his
new toys in Figure 1. The non-terminal edges are
marked with their syntactic categories, i.e., HPSG
rules (e.g., subjh for the subject-head-rule, hadj for
the head-adjunct-rule, etc.), and optionally their dis-
ambiguation scores. By traversing upward through
parent edges from the DUMMY-V edge, all features
can be efficiently extracted (see the third column in
Table 1).
It should be noted that none of these features are
used to deterministically dictate the predicted VPC
category. Instead, the acquired features are used as
inputs to a statistical classifier for predicting the type
of the VPC candidate at the token level (in the con-
text of the given sentence). In our experiment, we
used a maximum entropy-based model to do a 3-
13
Feature Description Examples
LE:MAXCONS
A lexical entry together with the maximal constituent
constructed from it
v - le:subjh, v np le:hadj,
v p le:subjh, v p-np le:subj
LE:MAXSPAN
A lexical entry together with the length of the span of
the maximal constituent constructed from the LE
v - le:7, v np le:5, v p le:4,
v p-np le:7
LE:MAXLEVEL
A lexical entry together with the levels of projections
before it reaches its maximal constituent
v - le:2, v np le:1, v p le:2,
v p-np le:3
LE:MAXCRANK
A lexical entry together with the relative disambigua-
tion score ranking of its maximal constituent among
all MaxCons from different LEs
v - le:4, v np le:3, v p le:1,
v p-np le:2
PARTICLE The stem of the particle in the candidate VPC off
Table 1: Chart mining features used for VPC extraction
his new toysoffshows
PREPPRTL
v_?_le
NP1
VP4?hcomp
NP2
VP5?hcomp
PP?hcomp
0 2 3 4 7
DUMMY?V
S1?subjh(.125)
S3?subjh(.875)
VP1?hadj VP3?hcomp
S2?subjh(.925)
VP2?hadj(.325)
v_p?np_lev_np_le v_p_le
the boy
Figure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERG
category classification: non-VPC, transitive VPC,
or intransitive VPC. For the parameter estimation
of the ME model, we use the TADM open source
toolkit (Malouf, 2002). The token-level predictions
are then combined with a simple majority voting to
derive the type-level prediction for the VPC candi-
date. In the case of a tie, the method backs off to
the na??ve baseline model described in Section 4.2,
which relies on the combined probability of the verb
and particle forming a VPC.
We have also experimented with other ways of de-
riving type-level predictions from token-level classi-
fication results. For instance, we trained a separate
classifier that takes the token-level prediction as in-
put in order to determine the type-level VPC predic-
tion. Our results indicate no significant difference
between these methods and the basic majority vot-
ing approach, so we present results exclusively for
this simplistic approach in this paper.
4 Evaluation
4.1 Experiment Setup
To evaluate the proposed chart mining-based VPC
extraction model, we use the dataset from the LREC
2008 Multiword Expression Workshop shared task
(see Section 2). We use this dataset to perform three
distinct DLA tasks, as detailed in Table 2.
The chart mining feature extraction is imple-
mented as an extension to the PET parser (Callmeier,
14
Task Description
GOLD VPC Determine the valence for a verb?preposition combination which is known to occur
as a non-compositional VPC (i.e. known VPC, with unknown valence(s))
FULL Determine whether each verb?preposition combination is a VPC or not, and further
predict its valence(s) (i.e. unknown if VPC, and unknown valence(s))
VPC Determine whether each verb?preposition combination is a VPC or not ignoring va-
lence (i.e. unknown if VPC, and don?t care about valence)
Table 2: Definitions of the three DLA tasks
2001). We use a slightly modified version of the
ERG in our experiments, based on the nov-06 re-
lease. The modifications include 4 newly-added
dummy lexical entries for the verb DUMMY-V and
the corresponding inflectional rules, and a lexical
type prediction model (Zhang and Kordoni, 2006)
trained on the LOGON Treebank (Oepen et al, 2004)
for unknown word handling. The parse disambigua-
tion model we use is also trained on the LOGON
Treebank. Since the parser has no access to any of
the verbs under investigation (due to the DUMMY-
V substitution), those VPC types attested in the
LOGON Treebank do not directly impact on the
model?s performance. The chart mining feature ex-
traction process took over 10 CPU days, and col-
lected a total of 44K events for 4,090 candidate VPC
triples.4 5-fold cross validation is used to train/test
the model. As stated above (Section 2), the VPC
triples attested in the WSJ training sections of the
Penn Treebank are excluded in each testing fold for
comparison with the Charniak parser-based model
(see Section 4.2).
4.2 Baseline and Benchmark
For comparison, we first built a na??ve baseline model
using the combined probabilities of the verb and par-
ticle being part of a VPC. More specifically, P (c|v)
and P (c|p) are the probabilities of a given verb
v and particle p being part of a VPC candidate
of type s ? {intrans , trans , null}, for transitive
4Not all sentences in the dataset are successfully chart-
mined. Due to the complexity of the precision grammar we
use, the parser is unlikely to complete the parsing chart for ex-
tremely long sentences (over 50 words). Moreover, sentences
which do not receive any spanning edge over the verb and the
particle are not considered as an indicative event. Nevertheless,
the coverage of the chart mining is much higher than the full-
parse coverage of the grammar.
VPC, intransitive VPC, and non-VPC, respectively.
P? (s|v, p) = P (s|v) ? P (s|p) is used to approxi-
mate the joint probability of verb-particle (v, p) be-
ing of type s, and the prediction type is chosen ran-
domly based on this probabilistic distribution. Both
P (s|v) and P (s|p) can be estimated from a list of
VPC candidate types. If v is unseen, P (s|v) is set to
be 1|V |
?
vi?V P (s|vi) estimated over all verbs |V |
seen in the list of VPC candidates. The na??ve base-
line performed poorly, mainly because there is not
enough knowledge about the context of use of VPCs.
This also indicates that the task of VPC extraction
is non-trivial, and that context (evidence from sen-
tences in which the VPC putatively occurs) must be
incorporated in order to make more accurate predic-
tions.
As a benchmark VPC extraction system, we use
the Charniak parser (Charniak, 2000). This sta-
tistical parser induces a context-free grammar and
a generative parsing model from a training set of
gold standard parse trees. Traditionally, it has been
trained over the WSJ component of the Penn Tree-
bank, and for this work we decided to take the same
approach and train over sections 1 to 22, and use sec-
tion 23 for parameter-tuning. After parsing, we sim-
ply search for the VPC triples in each token instance
with tgrep2,5 and decide on the classification of
the candidate by majority voting over all instances,
breaking ties randomly.
5Noting that the Penn POS tagset captures essentially the
compositional vs. non-compositional VPC distinction required
in the extraction task, through the use of the RP (prepositional
particle, for non-compositional VPCs) and RB (adverb, for com-
positional VPCs) tags.
15
4.3 Results
The results of our experiments are summarised in
Table 3. For the na??ve baseline and the chart mining-
based models, the results are averaged over 5-fold
cross validation.
We evaluate the methods in the form of the three
tasks described in Table 2. Formally, GOLD VPC
equates to extracting ?v, p, s? tuples from the sub-
set of gold-standard ?v, p? tuples; FULL equates to
extracting ?v, p, s? tuples for all VPC candidates;
and VPC equates to extracting ?v, p? tuples (ignor-
ing valence) over all VPC candidates. In each case,
we present the precision (P), recall (R) and F-score
(? = 1: F). For multi-category classifications (i.e.
the two tasks where we predict the valence s, indi-
cated as ?All? in Table 3), we micro-average the pre-
cision and recall over the two VPC categories, and
calculate the F-score as their harmonic mean.
From the results, it is obvious that the chart
mining-based model performs best overall, and in-
deed for most of the measures presented. The Char-
niak parser-based extraction method performs rea-
sonably well, especially in the VPC+valence extrac-
tion task over the FULL task, where the recall was
higher than the chart mining method. Although
not reported here, we observe a marked improve-
ment in the results for the Charniak parser when
the VPC types attested in the WSJ are not filtered
from the test set. This indicates that the statisti-
cal parser relies heavily on lexicalised VPC infor-
mation, while the chart mining model is much more
syntax-oriented. In error analysis of the data, we ob-
served that the Charniak parser was noticeably more
accurate at extracting VPCs where the verb was fre-
quent (our method, of course, did not have access
to the base frequency of the simplex verb), under-
lining again the power of lexicalisation. This points
to two possibilities: (1) the potential for our method
to similarly benefit from lexicalisation if we were to
remove the constraint on ignoring any pre-existing
lexical entries for the verb; and (2) the possibility
for hybridising between lexicalised models for fre-
quent verbs and unlexicalised models for infrequent
verbs. Having said this, it is important to reinforce
that lexical acquisition is usually performed in the
absence of lexicalised probabilities, as if we have
prior knowledge of the lexical item, there is no need
to extract it. In this sense, the first set of results in
Table 3 over Gold VPCs are the most informative,
and illustrate the potential of the proposed approach.
From the results of all the models, it would ap-
pear that intransitive VPCs are more difficult to ex-
tract than transitive VPCs. This is partly because the
dataset we use is unbalanced: the number of transi-
tive VPC types is about twice the number of intran-
sitive VPCs. Also, the much lower numbers over
the FULL set compared to the GOLD VPC set are due
to the fact that only 1/8 of the candidates are true
VPCs.
5 Discussion and Future Work
The inventory of features we propose for VPC ex-
traction is just one illustration of how partial parse
results can be used in lexical acquisition tasks.
The general chart mining technique can easily be
adapted to learn other challenging linguistic phe-
nomena, such as the countability of nouns (Bald-
win and Bond, 2003), subcategorization properties
of verbs or nouns (Korhonen, 2002), and general
multiword expression (MWE) extraction (Baldwin
and Kim, 2009). With MWE extraction, e.g., even
though some MWEs are fixed and have no internal
syntactic variability, such as ad hoc, there is a very
large proportion of idioms that allow various de-
grees of internal variability, and with a variable num-
ber of elements. For example, the idiom spill the
beans allows internal modification (spill mountains
of beans), passivisation (The beans were spilled in
the latest edition of the report), topicalisation (The
beans, the opposition spilled), and so forth (Sag et
al., 2002). In general, however, the exact degree of
variability of an idiom is difficult to predict (Riehe-
mann, 2001). The chart mining technique we pro-
pose here, which makes use of partial parse results,
may facilitate the automatic recognition task of even
more flexible idioms, based on the encouraging re-
sults for VPCs.
The main advantage, though, of chart mining is
that parsing with precision grammars does not any
longer have to assume complete coverage, as has
traditionally been the case. As an immediate con-
sequence, the possibility of applying our chart min-
ing technique to evolving medium-sized grammars
makes it especially interesting for lexical acquisi-
16
Task VPC Type Na??ve Baseline Charniak Parser Chart-Mining
P R F P R F P R F
GOLD VPC
Intrans-VPC 0.300 0.018 0.034 0.549 0.753 0.635 0.845 0.621 0.716
Trans-VPC 0.676 0.348 0.459 0.829 0.648 0.728 0.877 0.956 0.915
All 0.576 0.236 0.335 0.691 0.686 0.688 0.875 0.859 0.867
FULL
Intrans-VPC 0.060 0.018 0.028 0.102 0.593 0.174 0.153 0.155 0.154
Trans-VPC 0.083 0.348 0.134 0.179 0.448 0.256 0.179 0.362 0.240
All 0.080 0.236 0.119 0.136 0.500 0.213 0.171 0.298 0.218
VPC 0.123 0.348 0.182 0.173 0.782 0.284 0.259 0.332 0.291
Table 3: Results for the different methods over the three VPC extraction tasks detailed in Table 2
tion over low-density languages, for instance, where
there is a real need for rapid-prototyping of language
resources.
The chart mining approach we propose in this
paper is couched in the bottom-up chart parsing
paradigm, based exclusively on passive edges. As
future work, we would also like to look into the
top-level active edges (those active edges that are
never completed), as an indication of failed assump-
tions. Moreover, it would be interesting to investi-
gate the applicability of the technique in other pars-
ing strategies, e.g., head-corner or left-corner pars-
ing. Finally, it would also be interesting to in-
vestigate whether by using the features we acquire
from chart mining enhanced with information on the
prevalence of certain patterns, we could achieve per-
formance improvements over broader-coverage tree-
bank parsers such as the Charniak parser.
6 Conclusion
We have proposed a chart mining technique for lex-
ical acquisition based on partial parsing with preci-
sion grammars. We applied the proposed method
to the task of extracting English verb particle con-
structions from a prescribed set of corpus instances.
Our results showed that simple unlexicalised fea-
tures mined from the chart can be used to effec-
tively extract VPCs, and that the model outperforms
a probabilistic baseline and the Charniak parser at
VPC extraction.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram. The first was supported by the German Excellence
Cluster of Multimodal Computing and Interaction.
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23:597?618.
Timothy Baldwin and Francis Bond. 2003. Learning
the countability of English nouns from corpus data.
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2003),
pages 463?470, Sapporo, Japan.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing.
CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expres-
sions, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proceedings of the LREC 2008 Work-
shop: Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech, Morocco.
Ulrich Callmeier. 2001. Efficient parsing with large-
scale unification grammars. Master?s thesis, Univer-
sita?t des Saarlandes, Saarbru?cken, Germany.
John Carroll and Stephan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proceedings of the 2nd International Joint Confer-
ence on Natural LanguageProcessing (IJCNLP 2005),
pages 165?176, Jeju Island, Korea.
Eugene Charniak. 2000. A maximum entropy-based
parser. In Proceedings of the 1st Annual Meeting of
the North American Chapter of Association for Com-
putational Linguistics (NAACL2000), Seattle, USA.
Daniel de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error min-
ing in parsing results. In Proceedings of the ACL2009
Workshop on Grammar Engineering Across Frame-
works (GEAF), Singapore.
17
Nicole Dehe?. 2002. Particle Verbs in English: Syn-
tax, Information, Structure and Intonation. John Ben-
jamins, Amsterdam, Netherlands/Philadelphia, USA.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1?
17. CSLI Publications.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte Jo?rg, and Ul-
rich Scha?fer. 2006. Question answering from struc-
tured knowledge sources. Journal of Applied Logic,
Special Issue on Questions and Answers: Theoretical
and Applied Perspectives., 5(1):20?48.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic unifcation-based grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 1999), pages 535?541, Mary-
land, USA.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and
Rob Malouf. 1999. A Bag of Useful Techniques for
Efficient and Robust Parsing. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 473?480, Maryland, USA.
Anna Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Andrew MacKinlay, David Martinez, and Timothy Bald-
win. 2009. Biomedical event annotation with CRFs
and precision grammars. In Proceedings of BioNLP
2009: Shared Task, pages 77?85, Boulder, USA.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conferencde on Natural Language
Learning (CoNLL 2002), pages 49?55, Taipei, Taiwan.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl, and
Harold Somers, editors, Handbook of Natural Lan-
guage Processing.
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing ? practical results. In
Proceedings of the 1st Annual Meeting of the North
American Chapter of Association for Computational
Linguistics (NAACL 2000), pages 162?169, Seattle,
USA.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik
Velldal, Dorothee Beermann, John Carroll, Dan
Flickinger, Lars Hellan, Janne Bondi Johannessen,
Paul Meurer, Torbj?rn Nordga?rd, and Victoria Rose?n.
2004. Som a? kapp-ete med trollet? Towards MRS-
Based Norwegian?English Machine Translation. In
Proceedings of the 10th International Conference on
Theoretical and Methodological Issues in Machine
Translation, Baltimore, USA.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the
extraction of multiword expressions. In Proceedings
of the LREC 2008 Workshop: Towards a Shared Task
for Multiword Expressions (MWE 2008), pages 50?53,
Marrakech, Morocco.
Susanne Riehemann. 2001. A Constructional Approach
to Idioms and Word Formation. Ph.D. thesis, Stanford
University, CA, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002), pages 1?15, Mexico City, Mexico.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?178.
Masaru Tomita. 1985. An efficient context-free parsing
algorithm for natural languages. In Proceedings of the
9th International Joint Conference on Artificial Intel-
ligence, pages 756?764, Los Angeles, USA.
Kristina Toutanova, Christoper D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse ranking for a rich HPSG grammar. In Proceed-
ings of the 1st Workshop on Treebanks and Linguistic
Theories (TLT 2002), pages 253?263, Sozopol, Bul-
garia.
Hans Uszkoreit. 2002. New chances for deep linguis-
tic processing. In Proceedings of the 19th interna-
tional conference on computational linguistics (COL-
ING 2002), Taipei, Taiwan.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics), pages 446?453, Barcelona, Spain.
Aline Villavicencio and Ann Copestake. 2002. Verb-
particle constructions in a computational grammar of
English. In Proceedings of the 9th International Con-
ference on Head-Driven Phrase Structure Grammar
(HPSG-2002), Seoul, Korea.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 275?280, Genoa, Italy.
Yi Zhang and Valia Kordoni. 2008. Robust parsing
with a large HPSG grammar. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), Marrakech, Morocco.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007a.
Partial parse selection for robust deep processing. In
Proceedings of ACL 2007 Workshop on Deep Linguis-
tic Processing, pages 128?135, Prague, Czech Repub-
lic.
Yi Zhang, Stephan Oepen, and John Carroll. 2007b. Ef-
ficiency in unification-based N-best parsing. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies (IWPT 2007), pages 48?59, Prague,
Czech.
18
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 372?376,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Evaluating a Morphological Analyser of Inuktitut
Jeremy Nicholson?, Trevor Cohn? and Timothy Baldwin?
?Department of Computing and Information Systems, The University of Melbourne, Australia
?Department of Computer Science, The University of Sheffield, UK
jeremymn@csse.unimelb.edu.au, tcohn@dcs.shef.ac.uk, tb@ldwin.net
Abstract
We evaluate the performance of an morpho-
logical analyser for Inuktitut across a medium-
sized corpus, where it produces a useful anal-
ysis for two out of every three types. We
then compare its segmentation to that of sim-
pler approaches to morphology, and use these
as a pre-processing step to a word alignment
task. Our observations show that the richer ap-
proaches provide little as compared to simply
finding the head, which is more in line with
the particularities of the task.
1 Introduction
In this work, we evaluate a morphological analyser
of Inuktitut, whose polysynthetic morphosyntax can
cause particular problems for natural language pro-
cessing; but our observations are also relevant to
other languages with rich morphological systems.
The existing NLP task for Inuktitut is that of word
alignment (Martin et al, 2005), where Inuktitut to-
kens align to entire English clauses. While Langlais
et al (2005) theorises that a morphological analyser
could aid in this task, we observed little to no im-
provement over a baseline model by making use of
its segmentation. Nonetheless, morphological anal-
ysis does provide a great deal of information, but the
task structure tends to disprefer its contribution.
2 Background
2.1 Inuktitut
Inuktitut is a macrolanguage of many more-or-less
mutually intelligible dialects (Gordon, 2005). The
morphosyntax of Inuktitut is particularly marked by
a rich polysynthetic suffixing morphology, including
incorporation of arguments into verbal tokens, as in
natsiviniqtulauqsimavilli in (1). This phenomenon
causes an individual token in Inuktitut to be approx-
imately equivalent to an entire clause in English.
(1) natsiq-
seal
-viniq-
meat
-tuq-
eat
-lauq-
before
-sima-
ever
-vit
INT-2s
-li
but
?But have you ever eaten seal meat before??
Lowe (1996) analyses the morphology as a four-
place relationship: one head morpheme, zero or
more lexical morphemes, one or more grammatical
morphemes, and an optional enclitic. The morpho-
tactics causes, amongst other phenomena, the final
consonant of a morpheme to assimilate the manner
of the initial consonant of the following morpheme
(as in -villi), or to be dropped (as in natsiviniq-).
Consequently, morphemes are not readily accessible
from the realised surface form, thereby motivating
the use of a morphological analyser.
2.2 Morphological analysis
For many languages with a less rich morphol-
ogy than Inuktitut, an inflectional lexicon is of-
ten adequate for morphological analysis (for exam-
ple, CELEX for English (Burnage, 1990), Lefff for
French (Sagot et al, 2006) or Adolphs (2008) for
German). Another typical approach is to perform
morphological analysis at the same time as POS tag-
ging (as in Hajic? and Hladka? (1998) for the fusional
morphology in Czech), as it is often the case that
372
determining the part-of-speech and choosing the ap-
propriate inflectional paradigm are closely linked.
For highly inflecting languages more generally,
morphological analysis is often treated as a segment-
and-normalise problem, amenable to analysis by
weighted finite state transducer (wFST), for exam-
ple, Creutz and Lagus (2002) for Finnish.
3 Resources
3.1 A morphological analyser for Inuktitut
The main resource that we are evaluating in this
work is a morphological analyser of Inuktitut called
Uqa?Ila?Ut.1 It is a rule-based system based on reg-
ular morphological variations of about 3200 head,
350 lexical, and 1500 grammatical morphemes, with
heuristics for ranking the various readings. The head
and lexical morphemes are collated with glosses in
both English and French.
3.2 Word alignment
The training corpus we use in our experiments is a
sentence-aligned segment of the Nunavut Hansards
(Martin et al, 2003). The corpus consists of about
340K sentences, which comprise about 4.0M En-
glish tokens, and 2.2M Inuktitut. The challenge of
the morphology becomes apparent when we contrast
these figures with the types: about 416K for Inukti-
tut, but only 27K for English. On average, there are
only 5 token instances per Inuktitut type; some 338K
types (81%) are singletons.
Inuktitut formed part of one of the shared tasks
in the ACL 2005 workshop on building and us-
ing parallel texts (Martin et al, 2005); for this, the
above corpus was simplistically tokenised, and used
as unsupervised training data. 100 sentences from
this corpus were phrasally aligned by Inuit anno-
tators. These were then extended into word align-
ments, where phrasal alignments of one token in
both the source and target were (generally) called
sure alignments, and one-to-many or many-to-many
mappings were extended to their cartesian product,
and called probable. The test set was composed of
75 of these sentences (about 2K English tokens, 800
Inuktitut tokens, 293 gold-standard sure alignments,
1http://inuktitutcomputing.ca/Uqailaut/
en/IMA.html
and 1679 probable), which we use to evaluate word
alignments.
Our treatment of the alignment problem is most
similar to Schafer and Dra?bek (2005) who examine
four systems: GIZA++ models (Och and Ney, 2000)
for each source-target direction, another where the
Inuktitut input has been syllabised, and a wFST
model. They observe that aggregating these results
through voting can create a very competitive system
for Inuktitut word alignment.
4 Experimental approach
We used an out-of-the-box implementation of the
Berkeley Aligner (DeNero and Klein, 2007), a com-
petitive word alignment system, to construct an un-
supervised alignment over the 75 test sentences,
based on the larger training corpus. The default
implementation of the system involves two jointly-
trained HMMs (one for each source-target direc-
tion) over five iterations,2 with so-called compet-
itive thresholding in the decoding step; these are
more fully described in DeNero and Klein (2007)
and Liang et al (2006).
Our approach examines morphological pre-
processing of the Inuktitut training and test sets,
with the idea of leveraging the morphological in-
formation into a corpus which is more amenable to
alignment. The raw corpus appears to be under-
segmented, where data sparseness from the many
singletons would prevent reliable alignments. Seg-
mentation might aid in this process by making sub-
lexical units with semantic overlap transparent to the
alignment system, so that types appear to have a
greater frequency through the data. Through this,
we attempt to examine the hypothesis that one-to-
one alignments between English and Inuktitut would
hold with the right segmentation. On the other hand,
oversegmentation (for example, down to the charac-
ter level) can leave the resulting sub-lexical items se-
mantically meaningless and cause spurious matches.
We consider five different ways of tackling Inuk-
titut morphology:
1. None: simply treat each Inuktitut token as a
monolithic entity. This is our baseline ap-
proach.
2Better performance was observed with three iterations, but
we preferred to maintain the default parameters of the system.
373
2. Head: attempt to separate the head morpheme
from the non-head periphery. Our hypothesis
is that we will be able to align the clausal head
more reliably, as it tends to correspond to a sin-
gle English token more reliably than the other
morphemes, which may not be realised in the
same manner in English. Head morphs in Inuk-
titut correspond to the first one or two syllables
of a token; we treated them uniformly as two
syllables, as other values caused a substantial
degredation in performance.
3. Syllabification: treat the text as if Inuktitut
had isolating morphology, and transform each
token into a series of single-syllable pseudo-
morphs. This effectively turns the task on its
head, from a primarily one Inukitut-to-many
English token problem to that of one English-
to-many Inuktitut. Despite the overzealousness
of this approach (as most Inuktitut morphemes
are polysyllabic, and consequently there will be
many plausible but spurious matches between
tokens that share a syllable but no semantics),
Schafer and Dra?bek (2005) observed it to be
quite competitive.
4. Morphs: segment each word into morphs,
thereby treating the morphology problem as
pure segmentation. This uses the top output of
the morphological analyser as the oracle seg-
mentation of each Inuktitut token.
5. Morphemes: as previous, except include the
normalisation of each morph to a morpheme,
as provided by the morphological analyser, as
a sort of ?lemmatisation? step. The major ad-
vantage over the morph approach is due to the
regular morphophonemic effects in Inuktitut,
which cause equivalent morphemes to have dif-
ferent surface realisations.
5 Results
5.1 Analyser
In our analysis, the morphological analyser finds at
least one reading for about 218K (= about 65%) of
the Inuktitut types. Of the 120K types without read-
ings, resource contraints account for about 11K. 3
Another 6K types caused difficulties due to punctu-
ation, numerical characters or encoding issues, all of
which could be handled through more sophisticated
tokenisation.
A more interesting cause of gaps for
the analyser was typographical errors (e.g.
*kiinaujaqtaaruasirnirmut for kiinaujaqtaarusiar-
nirmut ?requests for proposals?). This was often
due to consonant gemination, where it was either
missing (e.g. nunavummut ?in Nunavut? appeared
in the corpus as *nunavumut) or added (e.g.
*tamakkununnga instead of tamakkununga ?at
these ones here?). While one might expect these
kinds of error to be rare, because Inuktitut has an
orthography that closely reflects pronunciation,
they instead are common, which means that the
morphological analyser should probably accept
incorrect gemination with a lower weighting.
More difficult to analyse directly is the impact
of foreign words (particularly names) ? these are
typically subjectively transliterated based on Inukti-
tut morphophonology. Schafer and Dra?bek (2005)
use these as motivation for an approach based on
a wFST, but found few instances to analyse its ac-
curacy. Finally, there are certainly missing roots,
and possibly some missing affixes as well, for ex-
ample pirru- ?accident? (cf. pirruaqi- ?to have an
accident?). Finding these automatically remains as
future work.
As for tokens, we briefly analysed the 768 tokens
in the test set, of which 228 (30%) were not given
a reading. Punctuation (typically commas and peri-
ods) account for 117 of these, and numbers another
7. Consonant gemination and foreign words cause
gaps for at least 16 and 6 tokens, respectively (that
we could readily identify).
5.2 Word Alignment
Following Och and Ney (2000), we assess using
alignment error rate (AER) and define precision with
respect to the probable set, and recall with respect to
3We only attempted to parse tokens of 30 characters or
shorter; longer tokens tended to cause exceptions ? this could
presumably be improved with a more efficient analyser. While
the number of analyses will continue to grow with the token
length, which has implications in agglutinative languages, here
there are only about 300 tokens of length greater than 40.
374
Approach Prec Rec AER
None 0.783 0.863 0.195
Head 0.797 0.922 0.176
Syllabification 0.789 0.881 0.192
Morphs 0.777 0.860 0.207
Morphemes 0.777 0.863 0.206
S&D E-I 0.646 0.829 0.327
S&D Syll 0.849 0.826 0.156
Table 1: Precision, recall, and alignment error rate for
various approaches to morphology, with Schafer and
Dra?bek (2005) for comparison
the sure set.
We present word alignment results of the vari-
ous methods ? contrasted with Schafer and Dra?bek
(2005) ? in Table 1. The striking result is in
terms of statistical significance: according to ?2,
most of the various approaches to morphology fail
to give a significantly (P < 0.05) different result
to the baseline system of using entire tokens. For
comparison, whereas our baseline system is signifi-
cantly better than the baseline system of Schafer and
Dra?bek (2005) ? which demonstrates the value that
the Berkeley Aligner provides by training in both
source-target directions ? their syllablised model
is significantly superior in precision (P < 0.001),
while their recall is still worse than our model (P <
0.05). Intuitively, this seems to indicate that their
model is making fewer judgments, but actually the
opposite is true. It seems that their model achieves
better performance than ours because it leverages
many candidate probable alignments into high qual-
ity aggregates using a most-likely heuristic on the
mapping of Inuktitut syllables to English words,
whereas the Berkeley Aligner culls the candidate set
in joint training.
Of the approaches toward morphology that we
consider, only the recall of the head?based sys-
tem improves upon the baseline (P < 0.025).
This squares with our intuitions, where segment-
ing the root morpheme from the larger token al-
lows for more effective alignment of the semanti-
cally straightforward sure alignments.
The three systems that involve a finer segmenta-
tion over the tokens are equivalent in performance to
the baseline system. The oversegmentation seemed
to caused the alignment system to abandon an im-
plicit preference for monotonicity of the order of
tokens between the source and target (which holds
pretty well for the baseline system over the test data,
thanks partly to the fidelity-focused structure of a
Hansard corpus): presumably because the aligner
perceives lexical similarity between disparate tokens
due to them sharing a sublexical unit. This relax-
ing of monotonicity is most apparent for punctua-
tion, where a comma with a correct alignment in the
baseline becomes incorrectly aligned to a different
comma in the sentence for the segmented system.
6 Conclusion
The only improvement toward the task that we ob-
served using morphological approaches is that of
head segmentation, where using two syllables as a
head-surrogate allowed us to capture more of the
sure (one-to-one) alignments in the test set. One
possible extension would be to take the head mor-
pheme as given the analyser, rather than the some-
what arbitrary syllabic approach. For other lan-
guages with rich morphology, it may be similarly
valuable to target substantives for segmentation to
improve alignment.
All in all, it appears that the lexical encoding of
morphology of Inuktitut is so strikingly different
than English, that the assumption of Inuktitut mor-
phemes aligning to English words is untrue or at
least unfindable within the current framework. Nu-
merous common morphemes have no English equiv-
alent, for example, -liaq- ?to go to? which seems to
act as a light verb, or -niq-, a (re-)nominaliser for
abstract nominals. While the output of the morpho-
logical analyser could probably be used more effec-
tively in other tasks, there are still important impacts
in word alignment and machine translation, includ-
ing leveraging a dictionary (which is based on mor-
phemes, not tokens, and as such requires segmenta-
tion and normalisation) or considering grammatical
forms for syntactic approaches.
References
Peter Adolphs. 2008. Acquiring a poor man?s inflec-
tional lexicon for German. In Proc. of the 6th LREC,
375
Marrakech, Morocco.
Gavin Burnage. 1990. CELEX: A guide for users. Tech-
nical report, University of Nijmegen.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proc. of the 6th Workshop
of ACL SIGPHON, pages 21?30, Philadelphia, USA.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Proc.
of the 45th Annual Meeting of the ACL, pages 17?24,
Prague, Czech Republic.
Raymund G. Gordon, Jr, editor. 2005. Ethnologue: Lan-
guages of the World, Fifteenth Edition. SIL Interna-
tional.
Jan Hajic? and Barbora Hladka?. 1998. Tagging inflective
languages: Prediction of morphological categories for
a rich, structured tagset. In Proc. of the 36th Annual
Meeting of the ACL and 17th International Conference
on COLING, pages 483?490, Montre?al, Canada.
Philippe Langlais, Fabrizio Gotti, and Guihong Cao.
2005. NUKTI: English-Inuktitut word alignment sys-
tem description. In Proc. of the ACL Workshop on
Building and Using Parallel Texts, pages 75?78, Ann
Arbor, USA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. of the HLT Conference
of the NAACL, pages 104?111, New York City, USA.
Ronald Lowe. 1996. Grammatical sketches: Inuktitut.
In Jacques Maurais, editor, Quebec?s Aboriginal Lan-
guages: History, Planning and Development, pages
204?232. Multilingual Matters.
Joel Martin, Howard Johnson, Benoit Farley, and Anna
Maclachlan. 2003. Aligning and using an English-
Inuktitut parallel corpus. In Proc. of the HLT-NAACL
2003 Workshop on Building and Using Parallel Texts,
pages 115?118, Edmonton, Canada.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignment for languages with scarce resources.
In Proc. of the ACL Workshop on Building and Using
Parallel Texts, pages 65?74, Ann Arbor, USA.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proc. of the 38th Annual
Meeting of the ACL, pages 440?447, Saarbru?cken,
Germany.
Beno??t Sagot, Lionel Cle?ment, Eric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff syntac-
tic lexicon for French: Architecture, acquisition, use.
In Proc. of the 5th LREC, pages 1348?1351, Genoa,
Italy.
Charles Schafer and Elliott Dra?bek. 2005. Models for
Inuktitut-English word alignment. In Proc. of the ACL
Workshop on Building and Using Parallel Texts, pages
79?82, Ann Arbor, USA.
376
