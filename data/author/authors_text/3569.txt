Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 647?655,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Capturing Salience with a Trainable Cache Model for Zero-anaphora
Resolution
Ryu Iida
Department of Computer Science
Tokyo Institute of Technology
2-12-1, ?Ookayama, Meguro,
Tokyo 152-8552, Japan
ryu-i@cl.cs.titech.ac.jp
Kentaro Inui Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma
Nara 630-0192, Japan
{inui,matsu}@is.naist.jp
Abstract
This paper explores how to apply the notion
of caching introduced by Walker (1996) to
the task of zero-anaphora resolution. We
propose a machine learning-based imple-
mentation of a cache model to reduce the
computational cost of identifying an an-
tecedent. Our empirical evaluation with
Japanese newspaper articles shows that the
number of candidate antecedents for each
zero-pronoun can be dramatically reduced
while preserving the accuracy of resolving
it.
1 Introduction
There have been recently increasing concerns
with the need for anaphora resolution to make
NLP applications such as IE and MT more reli-
able. In particular, for languages such as Japanese,
anaphora resolution is crucial for resolving a
phrase in a text to its referent since phrases, es-
pecially nominative arguments of predicates, are
frequently omitted by anaphoric functions in dis-
course (Iida et al, 2007b).
Many researchers have recently explored ma-
chine learning-based methods using considerable
amounts of annotated data provided by, for exam-
ple, the Message Understanding Conference and
Automatic Context Extraction programs (Soon et
al., 2001; Ng and Cardie, 2002; Yang et al, 2008;
McCallum and Wellner, 2003, etc.). These meth-
ods reach a level comparable to or better than the
state-of-the-art rule-based systems (e.g. Baldwin
(1995)) by recasting the task of anaphora resolution
into classification or clustering problems. How-
ever, such approaches tend to disregard theoretical
findings from discourse theories, such as Center-
ing Theory (Grosz et al, 1995). Therefore, one of
the challenging issues in this area is to incorporate
such findings from linguistic theories into machine
learning-based approaches.
A typical machine learning-based approach
to zero-anaphora resolution searches for an an-
tecedent in the set of candidates appearing in all
the preceding contexts. However, computational
time makes this approach largely infeasible for
long texts. An alternative approach is to heuristi-
cally limit the search space (e.g. the system deals
with candidates only occurring in the N previous
sentences). Various research such as Yang et al
(2008) has adopted this approach, but it also leads
to problems when an antecedent is located far from
its anaphor, causing it to be excluded from target
candidate antecedents.
On the other hand, rule-based methods derived
from theoretical background such as Centering
Theory (Grosz et al, 1995) only deal with the
salient discourse entities at each point of the dis-
course status. By incrementally updating the dis-
course status, the set of candidates in question
is automatically limited. Although these meth-
ods have a theoretical advantage, they have a
serious drawback in that Centering Theory only
retains information about the previous sentence.
A few methods have attempted to overcome this
fault (Suri and McCoy, 1994; Hahn and Strube,
1997), but they are overly dependent upon the re-
strictions fundamental to the notion of centering.
We hope that by relaxing such restrictions it will
be possible for an anaphora resolution system to
achieve a good balance between accuracy and com-
putational cost.
From this background, we focus on the issue
of reducing candidate antecedents (discourse en-
tities) for a given anaphor. Inspired by Walker?s
argument (Walker, 1996), we propose a machine
learning-based caching mechanism that captures
the most salient candidates at each point of the
discourse for efficient anaphora resolution. More
specifically, we choose salient candidates for each
sentence from the set of candidates appearing in
that sentence and the candidates which are already
647
in the cache. Searching only through the set of
salient candidates, the computational cost of zero-
anaphora resolution is effectively reduced. In the
empirical evaluation, we investigate how efficiently
this caching mechanism contributes to reducing the
search space while preserving accuracy. This pa-
per focuses on Japanese though the proposed cache
mechanism may be applicable to any language.
This paper is organized as follows. First,
Section 2 presents the task of zero-anaphora res-
olution and then Section 3 gives an overview
of previous work. Next, in Section 4 we pro-
pose a machine learning-based cache model.
Section 5 presents the antecedent identification and
anaphoricity determination models used in the ex-
periments. To evaluate the model, we conduct sev-
eral empirical evaluations and report their results
in Section 6. Finally, we conclude and discuss the
future direction of this research in Section 7.
2 Zero-anaphora resolution
In this paper, we consider only zero-pronouns
that function as an obligatory argument of a predi-
cate. A zero-pronoun may or may not have its an-
tecedent in the discourse; in the case it does, we say
the zero-pronoun is anaphoric. On the other hand,
a zero-pronoun whose referent does not explicitly
appear in the discourse is called a non-anaphoric
zero-pronoun. A zero-pronoun is typically non-
anaphoric when it refers to an extralinguistic entity
(e.g. the first or second person) or its referent is
unspecified in the context.
The task of zero-anaphora resolution can be
decomposed into two subtasks: anaphoricity de-
termination and antecedent identification. In
anaphoricity determination, the model judges
whether a zero-pronoun is anaphoric (i.e. a zero-
pronoun has an antecedent in a text) or not. If a
zero-pronoun is anaphoric, the model must detect
its antecedent. For example, in example (1) the
model has to judge whether or not the zero-pronoun
in the second sentence (i.e. the nominative argu-
ment of the predicate ?to hate?) is anaphoric, and
then identify its correct antecedent as ?Mary.?
(1) Mary
i
-wa John
j
-ni (?
j
-ga) tabako-o
Mary
i
-TOP John
j
-DAT (?
j
-NOM) smoking-OBJ
yameru-youni it-ta .
quit-COMP say-PAST PUNC
Mary told John to quit smoking.
(?
i
-ga) tabako-o kirai-dakarada .
(?
i
-NOM) smoking-OBJ hate-BECAUSE PUNC
Because (she) hates people smoking.
3 Previous work
Early methods for zero-anaphora resolution were
developed with rule-based approaches in mind.
Theory-oriented rule-based methods (Kameyama,
1986; Walker et al, 1994), for example, focus
on the Centering Theory (Grosz et al, 1995) and
are designed to collect the salient candidate an-
tecedents in the forward-looking center (Cf ) list,
and then choose the most salient candidate, Cp,
as an antecedent of a zero-pronoun according to
heuristic rules (e.g. topic > subject > indirect ob-
ject > direct object > others1). Although these
methods have a theoretical advantage, they have
a serious drawback in that the original Centering
Theory is restricted to keeping information about
the previous sentence only. In order to loosen this
restriction, the Centering-based methods have been
extended for reaching an antecedent appearing fur-
ther from its anaphor. For example, Suri and Mc-
Coy (1994) proposed a method for capturing two
kinds of Cp, that correspond to the most salient
discourse entities within the local transition and
within the global focus of a text. Hahn and Strube
(1997) estimate hierarchical discourse segments of
a text by taking into account a series of Cp and then
the resolution model searches for an antecedent in
the estimated segment. Although these methods
remedy the drawback of Centering, they still overly
depend on the notion of Centering such as Cp.
On the other hand, the existing machine
learning-based methods (Aone and Bennett, 1995;
McCarthy and Lehnert, 1995; Soon et al, 2001;
Ng and Cardie, 2002; Seki et al, 2002; Isozaki
and Hirao, 2003; Iida et al, 2005; Iida et al,
2007a, etc.) have been developed with less atten-
tion given to such a problem. These methods ex-
haustively search for an antecedent within the list
of all candidate antecedents until the beginning of
the text. Otherwise, the process to search for an-
tecedents is heuristically carried out in a limited
search space (e.g. the previous N sentences of an
anaphor) (Yang et al, 2008).
4 Machine learning-based cache model
As mentioned in Section 2, the procedure for
zero-anaphora resolution can be decomposed into
two subtasks, namely anaphoricity determination
and antecedent identification. In this paper,
these two subtasks are carried out according to
the selection-then-classification model (Iida et al,
1
?A > B? means A is more salient than B.
648
2005), chosen because it it has the advantage of
using broader context information for determining
the anaphoricity of a zero-pronoun. It does this by
examining whether the context preceding the zero-
pronoun in the discourse has a plausible candidate
antecedent or not. With this model, antecedent
identification is performed first, and anaphoricity
determination second, that is, the model identifies
the most likely candidate antecedent for a given
zero-pronoun and then it judges whether or not the
zero-pronoun is anaphoric.
As discussed by Iida et al (2007a), intra-
sentential and inter-sentential zero-anaphora reso-
lution should be dealt with by taking into account
different kinds of information. Syntactic patterns
are useful clues for intra-sentential zero-anaphora
resolution, whereas rhetorical clues such as con-
nectives may be more useful for inter-sentential
cases. Therefore, the intra-sentential and inter-
sentential zero-anaphora resolution models are sep-
arately trained by exploiting different feature sets
as shown in Table 2.
In addition, as mentioned in Section 3, inter-
sentential cases have a serious problem where the
search space of zero-pronouns grows linearly with
the length of the text. In order to avoid this prob-
lem, we incorporate a caching mechanism origi-
nally addressed by Walker (1996) into the follow-
ing procedure of zero-anaphora resolution by lim-
iting the search space at step 3 and by updating the
cache at step 5.
Zero-anaphora resolution process:
1. Intra-sentential antecedent identification: For
a given zero-pronoun ZP in a given sentence S,
select the most-likely candidate antecedent A
1
from the candidates appearing in S by the intra-
sentential antecedent identification model.
2. Intra-sentential anaphoricity determination:
Estimate plausibility p
1
that A
1
is the true an-
tecedent, and return A
1
if p
1
? ?
intra
2 or go to
3 otherwise.
3. Inter-sentential antecedent identification: Se-
lect the most-likely candidate antecedent A
2
from the candidates appearing in the cache as
explained in Section 4.1 by the inter-sentential
antecedent identification model.
4. Inter-sentential anaphoricity determination:
Estimate plausibility p
2
that A
2
is the true an-
tecedent, and return A
2
if p
2
? ?
inter
3 or return
2?
intra
is a preselected threshold.
3?
inter
is a preselected threshold.
non-anaphoric otherwise.
5. After processing all zero-pronouns in S, the
cache is updated. The resolution process is con-
tinued until the end of the discourse.
4.1 Dynamic cache model
Because the original work of the cache model by
Walker (1996) is not fully specified for implemen-
tation, we specify how to retain the salient candi-
dates based on machine learning in order to capture
both local and global foci of discourse.
In Walker (1996)?s discussion of the cache
model in discourse processing, it was presumed to
operate under a limited attention constraint. Ac-
cording to this constraint, only a limited number of
candidates can be considered in processing. Ap-
plying the concept of cache to computer hardware,
the cache represents working memory and the main
memory represents long-term memory. The cache
only holds the most salient entities, while the rest
are moved to the main memory for possible later
consideration as a cache candidate. If a new can-
didate antecedent is retrieved from main memory
and inserted into the cache, or enters the cache di-
rectly during processing, other candidates in the
cache have to be displaced due to the limited ca-
pacity of the cache. Which candidate to displace is
determined by a cache replacement policy. How-
ever, the best policy for this is still unknown.
In this paper, we recast the cache replacement
policy as a ranking problem in machine learning.
More precisely, we choose the N best candidates
for each sentence from the set of candidates ap-
pearing in that sentence and the candidates that are
already in the cache. Following this cache model,
named the dynamic cache model, anaphora resolu-
tion is performed by repeating the following two
processes.
1. Cache update: cache C
i
for sentence S
i
is cre-
ated from the candidates in the previous sen-
tence S
i?1
and the ones in the previous cache
C
i?1
.
2. Inter-sentential zero-anaphora resolution:
cache C
i
is used as the search space for
inter-sentential zero-anaphora resolution in
sentence S
i
(see Step 3 of the aforementioned
zero-anaphora resolution process).
For each cache update (see Figure 1), a current
cache C
i
is created by choosing the N most salient
candidates from the M candidates in S
i?1
and the
N candidates in the previous cache C
i?1
. In order
to implement this mechanism, we train the model
649
...
1)1( ?i
c
2)1( ?i
c
Mi
c
)1( ?
...
2)1( ?i
e
Ni
e
)1( ?
1?i
S
1?i
C
i
C
cache sentence
cache update
antecedent identification
1)1( ?i
e
...
2i
e
iN
e
1i
e
ij
?
Figure 1: Anaphora resolution using the dynamic
cache model
so that it captures the salience of each candidate.
To reflect this, each training instance is labeled
as either retained or discarded. If an instance is re-
ferred to by an zero-pronoun appearing in any of
the following sentences, it is labeled as retained;
otherwise, it is labeled as discarded. Training in-
stances are created in the algorithm detailed in
Figure 2. The algorithm is designed with the fol-
lowing two points in mind.
First, the cache model must capture the salience
of each discourse entity according to the recency
of its entity at each discourse status because typi-
cally the more recently an entity appears, the more
salient it is. To reflect this, training instances
are created from candidates as they appear in the
text, and are labeled as retained from the point of
their appearance until their referring zero-pronoun
is reached, at which time they are labeled as dis-
carded if they are never referred to by any zero-
pronouns in the succeeding context.
Suppose, the situation shown in Figure 3, where
c
ij
is the j-th candidate in sentence S
i
. In this
situation, for example, candidate c
12
is labeled
as retained when creating training instances for
sentence S
1
, but labeled as discarded from S
2
onwards, because of the appearance of its zero-
pronoun. Another candidate c
13
which is never re-
ferred to in the text is labeled as discarded for all
training instances.
Second, we need to capture the ?relative?
salience of candidates appearing in the current dis-
course for each cache update, as also exploited in
the tournament-based or ranking-based approaches
to anaphora resolution (Iida et al, 2003; Yang et
al., 2003; Denis and Baldridge, 2008). To solve
it, we use a ranker trained on the instances created
as described above. In order to train the ranker,
we adopt the Ranking SVM algorithm (Joachims,
2002), which learns a weight vector to rank candi-
dates for a given partial ranking of each discourse
entity. Each training instance is created from the
set of retained candidates, R
i
, paired with the set
of discarded candidates, D
i
, in each sentence. To
Function makeTrainingInstances (T : input text)
C := NULL // set of preceding candidates
S := NULL // set of training instances
i := 1; // init
while (exists s
i
) // s
i
: i-th sentence in T
E
i
:= extractCandidates(s
i
)
R
i
:= extractRetainedInstances(E
i
, T )
D
i
:= E
i
\R
i
r
i
:= extractRetainedInstances(C, T )
R
i
:= R
i
? r
i
D
i
:= D
i
? (C\r
i
)
S := S ? {?R
i
, D
i
?}
C := updateSalienceInfo(C)
C := C ? E
i
i := i + 1
endwhile
return S
end
Function extractRetainedInstances (S, T )
R := NULL // init
while (elm ? S)
if (elm is anaphoric with a zero-pronoun located
in the following sentences of T )
R := R ? elm
endif
endwhile
return R
end
Function updateSalienceInfo (C, s
i
)
while (c ? C)
if (c is anaphoric with a zero pronoun in s
i
)
c.position := i; // update the position information
endif
endwhile
return C
end
Figure 2: Pseudo-code for creating training in-
stances
1
S
11
c
12
c
13
c
14
c
2
S
21
c
22
c
23
c
i
?
j
?
3
S
31
c
32
c
33
c
k
?
retained discarded
11
c
12
c
13
c
14
c
l
?
training instances
retained discarded
11
c
22
c
13
c
14
c
21
c
23
c
12
c
Figure 3: Creating training instnaces
define the partial ranking of candidates, we simply
rank candidates in R
i
as first place and candidates
in D
i
as second place.
4.2 Static cache model
Other research on discourse such as Grosz and
Sidner (1986) has studied global focus, which gen-
erally refers to the entity or set of entities that
are salient throughout the entire discourse. Since
global focus may not be captured by Centering-
based models, we also propose another cache
model which directly captures the global salience
of a text.
To train the model, all the candidates in a text
which have an inter-sentential anaphoric relation
with zero-pronouns are used as positive instances
and the others used as negative ones. Unlike the
650
Table 1: Feature set used in the cache models
Feature Description
POS Part-of-speech of C followed by
IPADIC4.
IN QUOTE 1 if C is located in a quoted sentence;
otherwise 0.
BEGINNING 1 if C is located in the beginnig of a text;
otherwise 0.
CASE MARKER Case marker, such as wa (TOPIC) and
ga (SUBJECT), of C.
DEP END 1 if C has a dependency relation with
the last bunsetsu unit (i.e. a basic unit
in Japanese) in a sentence ; otherwise 0.
CONN* The set of connectives intervening be-
tween C and Z. Each conjunction is en-
coded into a binary feature.
IN CACHE* 1 if C is currently stored in the cache;
otherwise 0.
SENT DIST* Distance between C and Z in terms of a
sentence.
CHAIN NUM The number of anaphoric chain, i.e. the
number of antecedents of Z in the situa-
tion that zero-pronouns in the preceding
contexts are completely resolved by the
zero-anaphora resolution model.
C is a candidate antecedent, and Z stands for a target zero-
pronoun. Features marked with an asterisk are only used in
the dynamic cache model.
dynamic cache model, this model does not update
the cache dynamically, but simply selects for each
given zero-pronoun the N most salient candidates
from the preceding sentences according to the rank
provided by the trained ranker. We call this model
the static cache model.
4.3 Features used in the cache models
The feature set used in the cache model is shown
in Table 1. The ?CASE MARKER? feature roughly
captures the salience of the local transition dealt
with in Centering Theory, and is also intended to
capture the global foci of a text coupled with the
BEGINNING feature. The CONN feature is expected
to capture the transitions of a discourse relation be-
cause each connective functions as a marker of a
discourse relation between two adjacent discourse
segments.
In addition, the recency of a candidate an-
tecedent can be even important when an entity oc-
curs as a zero-pronoun in discourse. For example,
when a discourse entity e appearing in sentence s
i
is referred to by a zero-pronoun later in sentence
s
j(i<j)
, entity e is considered salient again at the
point of s
j
. To reflect this way of updating salience,
we overwrite the information about the appearance
position of candidate e in s
j
, which is performed by
the function updateSalienceInfo in Figure 2. This
allows the cache model to handle updated salience
4http://chasen.naist.jp/stable/ipadic/
features such as CHAIN NUM in proceeding cache
updates.
5 Antecedent identification and anaphoric-
ity determination models
As an antecedent identification model, we adopt
the tournament model (Iida et al, 2003) because
in a preliminary experiment it achieved better per-
formance than other state-of-the-art ranking-based
models (Denis and Baldridge, 2008) in this task
setting. To train the tournament model, the training
instances are created by extracting an antecedent
paired with each of the other candidates for learn-
ing a preference of which candidate is more likely
to be an antecedent. At the test phase, the model
conducts a tournament consisting of a series of
matches in which candidate antecedents compete
with one another. Note that in the case of inter-
sentential zero-anaphora resolution the tournament
is arranged between candidates in the cache. For
learning the difference of two candidates in the
cache, training instances are also created by only
extracting candidates from the cache.
For anaphoricity determination, the model has to
judge whether a zero-pronoun is anaphoric or not.
To create the training instances for the binary clas-
sifier, the most likely candidate of each given zero-
pronoun is chosen by the tournament model and
then it is labeled as anaphoric (positive) if the cho-
sen candidate is indeed the antecedent of the zero-
pronoun5, or otherwise labeled as non-anaphoric
(negative).
To create models for antecedent identification
and anaphoricity determination, we use a Support
Vector Machine (Vapnik, 1998)6 with a linear ker-
nel and its default parameters. To use the feature
set shown in Table 2, morpho-syntactic analysis of
a text is performed by the Japanese morpheme ana-
lyzer Chasen and the dependency parser CaboCha.
In the tournament model, the features of two com-
peting candidates are distinguished from each other
by adding the prefix of either ?left? or ?right.?
6 Experiments
We investigate how the cache model contributes
to candidate reduction. More specifically, we ex-
5In the original selection-then-classification model (Iida et
al., 2005), positive instances are created by all the correct pairs
of a zero-pronoun and its antecedent, however in this paper we
use only antecedents selected by the tournament model as the
most likely candidates in the set of candidates because this
method leads to better performance.
6http://svmlight.joachims.org/
651
Table 2: Feature set used in zero-anaphora resolution
Feature Type Feature Description
Lexical HEAD BF Characters of right-most morpheme in NP (PRED).
PRED FUNC Characters of functional words followed by PRED.
Grammatical PRED VOICE 1 if PRED contains auxiliaries such as ?(ra)reru?; otherwise 0.
POS Part-of-speech of NP (PRED) followed by IPADIC (Asahara and Matsumoto, 2003).
PARTICLE Particle followed by NP, such as ?wa (topic)?, ?ga (subject)?, ?o (object)?.
Semantic NE Named entity of NP: PERSON, ORGANIZATION, LOCATION, ARTIFACT, DATE, TIME,
MONEY, PERCENT or N/A.
SELECT PREF The score of selectional preference, which is the mutual information estimated from a
large number of triplets ?Noun, Case, Predicate?.
Positional SENTNUM Distance between NP and PRED.
BEGINNING 1 if NP is located in the beggining of sentence; otherwise 0.
END 1 if NP is located in the end of sentence; otherwise 0.
PRED NP 1 if PRED precedes NP; otherwise 0.
NP PRED 1 if NP precedes PRED; otherwise 0.
Discourse CL RANK A rank of NP in forward looking-center list.
CL ORDER A order of NP in forward looking-center list.
CONN** The connectives intervesing between NP and PRED.
Path PATH FUNC* Characters of functional words in the shortest path in the dependency tree between
PRED and NP.
PATH POS* Part-of-speech of functional words in shortest patn in the dependency tree between
PRED and NP.
NP and PRED stand for a bunsetsu-chunk of a candidate antecedent and a bunsetsu-chunk of a predicate which has a target
zero-pronoun respectively. The features marked with an asterisk are used during intra-sentential zero-anaphora resolution. The
feature marked with two asterisks is used during inter-sentential zero-anaphora resolution.
plore the candidate reduction ratio of each cache
model as well as its coverage, i.e. how of-
ten each cache model retains correct antecedents
(Section 6.2). We also evaluate the performance
of both antecedent identification on inter-sentential
zero-anaphora resolution (Section 6.3) and the
overall zero-anaphora resolution (Section 6.4).
6.1 Data set
In this experiment, we take the ellipsis of nom-
inative arguments of predicates as target zero-
pronouns because they are most frequently omitted
in Japanese, for example, 45.5% of the nominative
arguments of predicates are omitted in the NAIST
Text Corpus (Iida et al, 2007b).
As the data set, we use part of the NAIST Text
Corpus, which is publicly available, consisting of
287 newspaper articles in Japanese. The data set
contains 1,007 intra-sentential zero-pronouns, 699
inter-sentential zero-pronouns and 593 exophoric
zero-pronouns, totalling 2299 zero-pronouns. We
conduct 5-fold cross-validation using this data set.
A development data set consists of 60 articles for
setting parameters of inter-sentential anaphoricity
determination, ?
inter
, on overall zero-anaphora res-
olution. It contains 417 intra-sentential, 298 inter-
sentential and 174 exophoric zero-pronouns.
6.2 Evaluation of the caching mechanism
In this experiment, we directly compare the pro-
posed static and dynamic cache models with the
heuristic methods presented in Section 2. Note that
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.2  0.4  0.6  0.8  1
co
ve
ra
ge
# of classification in antecedent identification process
n=5
n=10
n=15 n=20
n=all
CM
SM (s=1)
SM (s=2)
SM (s=3)
DCM (w/o ZAR)
DCM (with ZAR)
SCM
CM: centering-based cache model, SM: sentence-based cache
model, SCM: static cache model, DCM (w/o ZAR): dynamic
cache model disregarding updateSalienceInfo, DCM (with
ZAR): dynamic cache model using the information of correct
zero-anaphoric relations, n: cache size and s: # of sentences.
Figure 4: Coverage of each cache model
the salience information (i.e. the function update-
SalienceInfo) in the dynamic cache model is disre-
garded in this experiment because its performance
crucially depends on the performance of the zero-
anaphora resolution model. The performance of
the cache model is evaluated by coverage, which
is a percentage of retained antecedents when ap-
pearing zero-pronouns refer to an antecedent in a
preceding sentence, i.e. we evaluate the cases of
inter-sentential anaphora resolution.
As a baseline, we adopt the following two cache
models. One is the Centering-derived model which
only stores the preceding ?wa? (topic)-marked or
652
?ga? (subject)-marked candidate antecedents in the
cache. It is an approximation of the model pro-
posed by Nariyama (2002) for extending the lo-
cal focus transition defined by Centering Theory.
We henceforth call this model the centering-based
cache model. The other baseline model stores can-
didates appearing in the N previous sentences of a
zero-pronoun to simulate a heuristic approach used
in works like Soon et al (2001). We call this model
the sentence-based cache model. By comparing
these baselines with our cache models, we can see
whether our models contribute to more efficiently
storing salient candidates or not.
The above dynamic cache model retains the
salient candidates independently of the results of
antecedent identification conducted in the preced-
ing contexts. However, if the zero-anaphora res-
olution in the current utterance is performed cor-
rectly, it will be available for use as information
about the recency of candidates and the anaphoric
chain of each candidate. Therefore, we also in-
vestigate whether correct zero-anaphora resolution
contributes to the dynamic cache model or not.
To integrate zero-anaphora resolution information,
we create training instances of the dynamic cache
model by updating the recency using the function
?updateSalienceInfo? shown in Figure 2 and also
using an additional feature, CHAIN NUM, defined
in Table 1.
The results are shown in Figure 47. We can
see the effect of the machine learning-based cache
models in comparison to the other two heuristic
models. The results demonstrate that the former
achieves good coverage at each point compared to
the latter. In addition, the difference between the
static and dynamic cache models demonstrates that
the dynamic one is always better then the static. It
may be this way because the dynamic cache model
simultaneously retains global focus of a given text
and the locally salient entities in the current dis-
course.
By comparing the dynamic cache model using
correct zero-anaphora resolution (denoted by DCM
(with ZAR) in Figure 4) and the one without it
(DCM (w/o ZAR)), we can see that correct zero-
anaphora resolution contributes to improving the
caching for every cache size. However, in the
practical setting the current zero-anaphora resolu-
7Expressions such as verbs were rarely annotated as an-
tecedents, so these are not extracted as candidate antecedents
in our current setting. This is the reason why the coverage of
using all the candidates is less than 1.0.
tion system sometimes chooses the wrong candi-
date as an antecedent or does not choose any can-
didate due to wrong anaphoricity determination,
negatively impacting the performance of the cache
model. For this reason, in the following two exper-
iments we decided not to use zero-anaphora reso-
lution in the dynamic cache model.
6.3 Evaluation of inter-sentential zero-
anaphora resolution
We next investigate the impact of the dynamic
cache model shown in Section 4.1 on the an-
tecedent identification task of inter-sentential zero-
anaphora resolution altering the cache size from
5 to the number of all candidates. We compare
the following three cache model within the task
of inter-sentential antecedent identification: the
centering-based cache model, the sentence-based
cache model and the dynamic cache model disre-
garding updateSalienceInfo (i.e. DCM (w/o ZAR)
in Figure 4). We also investigate the computational
time of the process of inter-sentential antecedent
identification with each cache model altering its pa-
rameter 8.
The results are shown in Table 3. From these
results, we can see the antecedent identification
model using the dynamic cache model obtains al-
most the same accuracy for every cache size. It
indicates that if the model can acquire a small num-
ber of the most salient discourse entities in the cur-
rent discourse, the model achieves accuracy com-
parable to the model which searches all the pre-
ceding discourse entities, while drastically reduc-
ing the computational time.
The results also show that the current antecedent
identification model with the dynamic cache model
does not necessarily outperform the model with the
baseline cache models.
For example, the sentence-based cache model
using the preceding two sentences (SM (s=2))
achieved an accuracy comparable to the dynamic
cache model with the cache size 15 (DCM (n=15)),
both spending almost the same computational time.
This is supposed to be due to the limited accu-
racy of the current antecedent identification model.
Since the dynamic cache models provide much bet-
ter search spaces than the baseline models as shown
in Figure 4, there is presumably more room for im-
provement with the dynamic cache models. More
investigations are to be concluded in our future
8All experiments were conducted on a 2.80 GHz Intel
Xeon with 16 Gb of RAM.
653
Table 3: Results on antecedent identification
model accuracy runtime coverage
(Figure 4)
CM 0.441 (308/699) 11m03s 0.651
SM(s=1) 0.381 (266/699) 6m54s 0.524
SM(s=2) 0.448 (313/699) 13m14s 0.720
SM(s=3) 0.466 (326/699) 19m01s 0.794
DCM(n=5) 0.446 (312/699) 4m39s 0.664
DCM(n=10) 0.441 (308/699) 8m56s 0.764
DCM(n=15) 0.442 (309/699) 12m53s 0.858
DCM(n=20) 0.443 (310/699) 16m35s 0.878
DCM(n=1000) 0.452 (316/699) 53m44s 0.928
CM: centering-based cache model, SM: sentence-based cache
model, DCM: dynamic cache model, n: cache size, s: number
of the preceding sentences.
work.
6.4 Overall zero-anaphora resolution
We finally investigate the effects of introducing
the proposed model on overall zero-anaphora res-
olution including intra-sentential cases. The res-
olution is carried out according to the procedure
described in Section 4. By comparing the zero-
anaphora resolution model with different cache
sizes, we can see whether or not the model using
a small number of discourse entities in the cache
achieves performance comparable to the original
one in a practical setting.
For intra-sentential zero-anaphora resolution, we
adopt the model proposed by Iida et al (2007a),
which exploits syntactic patterns as features that
appear in the dependency path of a zero-pronoun
and its candidate antecedent. Note that for sim-
plicity we use bag-of-functional words and their
part-of-speech intervening between a zero-pronoun
and its candidate antecedent as features instead
of learning syntactic patterns with the Bact algo-
rithm (Kudo and Matsumoto, 2004).
We illustrated the recall-precision curve of each
model by altering the threshold parameter of intra-
sentential anaphoricity determination, which is
shown in Figure 5. The results show that all mod-
els achieved almost the same performance when
decreasing the cache size. It indicates that it is
enough to cache a small number of the most salient
candidates in the current zero-anaphora resolution
model, while coverage decreases when the cache
size is smaller as shown in Figure 4.
7 Conclusion
We propose a machine learning-based cache
model in order to reduce the computational cost of
zero-anaphora resolution. We recast discourse sta-
tus updates as ranking problems of discourse en-
tities by adopting the notion of caching originally
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6
pr
ec
isi
on
recall
n=5
n=10
n=15
n=20
n=1000
Figure 5: Recall-precision curves on overall ze-
ro-anaphora resolution
introduced by Walker (1996). More specifically,
we choose the N most salient candidates for each
sentence from the set of candidates appearing in
that sentence and the candidates which are already
in the cache. Using this mechanism, the compu-
tational cost of the zero-anaphora resolution pro-
cess is reduced by searching only the set of salient
candidates. Our empirical evaluation on Japanese
zero-anaphora resolution shows that our learning-
based cache model drastically reduces the search
space while preserving accuracy.
The procedure for zero-anaphora resolution
adopted in our model assumes that resolution is
carried out linearly, i.e. an antecedent is inde-
pendently selected without taking into account any
other zero-pronouns. However, trends in anaphora
resolution have shifted from such linear approaches
to more sophisticated ones which globally opti-
mize the interpretation of all the referring expres-
sions in a text. For example, Poon and Domingos
(2008) has empirically reported that such global
approaches achieve performance better than the
ones based on incrementally processing a text. Be-
cause their work basically builds on inductive logic
programing, we can naturally extend this to incor-
porate our caching mechanism into the global op-
timization by expressing cache constraints as pred-
icate logic, which is one of our next challenges in
this research area.
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proceedings of 33th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 122?129.
M. Asahara and Y. Matsumoto, 2003. IPADIC User Manual.
Nara Institute of Science and Technology, Japan.
B. Baldwin. 1995. CogNIAC: A Discourse Processing En-
gine. Ph.D. thesis, Department of Computer and Informa-
tion Sciences, University of Pennsylvania.
P. Denis and J. Baldridge. 2008. Specialized models and
ranking for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 660?669.
654
B. J. Grosz and C. L. Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Linguistics,
12:175?204.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: A
framework for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?226.
U. Hahn and M. Strube. 1997. Centering in-the-large: com-
puting referential discourse segments. In Proceedings of
the 8th conference on European chapter of the Association
for Computational Linguistics, pages 104?111.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. In-
corporating contextual cues in trainable models for coref-
erence resolution. In Proceedings of the 10th EACL Work-
shop on The Computational Treatment of Anaphora, pages
23?30.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora resolu-
tion by antecedent identification followed by anaphoricity
determination. ACM Transactions on Asian Language In-
formation Processing (TALIP), 4(4):417?434.
R. Iida, K. Inui, and Y. Matsumoto. 2007a. Zero-anaphora
resolution by learning rich syntactic pattern features. ACM
Transactions on Asian Language Information Processing
(TALIP), 6(4).
R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007b.
Annotating a japanese text corpus with predicate-argument
and coreference relations. In Proceeding of the ACL Work-
shop ?Linguistic Annotation Workshop?, pages 132?139.
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun res-
olution based on ranking rules and machine learning. In
Proceedings of the 2003 Conference on Empirical Methods
in Natural Language Processing, pages 184?191.
T. Joachims. 2002. Optimizing search engines using click-
through data. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD), pages
133?142.
M. Kameyama. 1986. A property-sharing constraint in cen-
tering. In Proceedings of the 24th ACL, pages 200?206.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for
classification of semi-structured text. In Proceedings of the
2004 EMNLP, pages 301?308.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In Proceedings of the IJCAI Workshop on In-
formation Integration on the Web, pages 79?84.
J. F. McCarthy and W. G. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the 14th
International Joint Conference on Artificial Intelligence,
pages 1050?1055.
S. Nariyama. 2002. Grammar for ellipsis resolution in
japanese. In Proceedings of the 9th International Confer-
ence on Theoretical and Methodological Issues in Machine
Translation, pages 135?145.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th ACL, pages 104?111.
H. Poon and P. Domingos. 2008. Joint unsupervised corefer-
ence resolution with Markov Logic. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 650?659.
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic
method for analyzing japanese anaphora integrating zero
pronoun detection and resolution. In Proceedings of the
19th COLING, pages 911?917.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
L. Z. Suri and K. F. McCoy. 1994. Raft/rapr and center-
ing: a comparison and discussion of problems related to
processing complex sentences. Computational Linguistics,
20(2):301?317.
V. N. Vapnik. 1998. Statistical Learning Theory. Adaptive
and Learning Systems for Signal Processing Communica-
tions, and control. John Wiley & Sons.
M. Walker, M. Iida, and S. Cote. 1994. Japanese discourse
and the process of centering. Computational Linguistics,
20(2):193?233.
M. A. Walker. 1996. Limited attention and discourse struc-
ture. Computational Linguistics, 22(2):255?264.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st ACL, pages 176?183.
X. Yang, J. Su, J. Lang, C. L. Tan, T. Liu, and S. Li. 2008.
An entity-mention model for coreference resolution with
inductive logic programming. In Proceedings of ACL-08:
HLT, pages 843?851.
655
Proceedings of the 12th European Workshop on Natural Language Generation, pages 110?113,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Japanese corpus of referring expressions used in a situated
collaboration task
Philipp Spanger Yasuhara Masaaki Iida Ryu Tokunaga Takenobu
Department of Computer Science
Tokyo Institute of Technology
{philipp, yasuhara, ryu-i, take}@cl.cs.titech.ac.jp
Abstract
In order to pursue research on generating
referring expressions in a situated collab-
oration task, we set up a data-collection
experiment based on the Tangram puzzle.
For a pair of participants we recorded ev-
ery utterance in synchronisation with the
current state of the puzzle as well as all
operations by the participants. Referring
expressions were annotated with their ref-
erents in order to build a referring expres-
sion corpus in Japanese. We provide pre-
liminary results on the analysis of the cor-
pus from various standpoints, focussing on
action-mentioning expressions.
1 Introduction
Referring expressions are a linguistic device to re-
fer to a certain object, enabling smooth collabo-
ration between humans and agents where physical
operations are involved. Previous research often
either selectively focussed only on a limited num-
ber of expression-types or set up overly controlled
experiments. In contrast, we intend to work to-
wards analysing the whole breadth of referring ex-
pressions in a situated domain. For this purpose
we created a corpus (in Japanese) and analysed it
from various standpoints.
From very early on in referring expression re-
search, there has been some interest in the col-
laborative aspect of the reference process (Clark
and Wilkes-Gibbs, 1986). This has more recently
developed into creating situated corpora in order
to analyse the referring expressions occurring in
situated collaborative tasks. The COCONUT cor-
pus (Di Eugenio et al, 2000) is collected from
keyboard-input dialogues between two partici-
pants who are collaboratively working on a sim-
ple 2-D design task (buying and arranging furni-
ture for two rooms). In contrast, the QUAKE cor-
pus (Byron et al, 2005) ? as well as the more re-
cent SCARE corpus (Stoia et al, 2008), which is
an extension of QUAKE ? is based on an interac-
tion captured in a 3-D virtual reality (VR) world
where two participants collaboratively carry out
a treasure hunting task. There has been ongoing
work to exploit these two resources for research on
different aspects of referring expressions (Pamela
W. Jordan, 2005; Byron, 2005).
However, while these resources have inspired
new research into different aspects of referring ex-
pressions, at the same time they have clear limi-
tations. The COCONUT corpus is collected from
dialogues in which participants refer to symbol-
like objects in a 2-D world. It thus resem-
bles the more recent (non-collaborative) TUNA-
corpus (van Deemter, 2007) in tending to en-
courage very simple types of expressions. Fur-
thermore, limiting participants? interaction to key-
board input makes the dialogue less natural. While
the QUAKE corpus deals with a more complex do-
main (3-D virtual world), the participating sub-
jects were only able to carry out limited kinds of
actions (pushing buttons, picking up or dropping
objects) as compared with the complexity of the
three-dimensional target domain.
In contrast to these two corpora, we set up a
comparatively simple collaborative task (Tangram
Puzzle) allowing participants to freely communi-
cate via speech and to perform actions various
enough to accomplish the given task, e.g. pick-
ing, moving, turning and rotating pieces. All ut-
terances by participants were recorded in synchro-
nisation with operations on objects and the object
arrangement. The utterances were transcribed and
all referring expressions found were annotated to-
gether with their referents. Thus, this corpus al-
lows us to study in detail human-human interac-
tion, particularly referring expressions in a situ-
ated setting. In what follows, we first describe de-
tails of the building of the corpus and then provide
110
results of our preliminary analysis. This analysis
reveals a novel type of referring expression men-
tioning an action on objects, which we call action-
mentioning expressions.
2 Building the corpus
Figure 1: Screenshot of the Tangram simulator
2.1 Experimental setting
We recruited 12 Japanese graduate students (4 fe-
males, 8 males) and split them into 6 pairs. Each
pair was instructed to solve the Tangram puzzle
(an ancient Chinese geometrical puzzle) coopera-
tively. The goal of Tangram is to construct a given
shape by arranging seven pieces of simple figures
as shown in Figure 1.
In order to record detailed information of the
interaction (position of pieces, participants? ac-
tions), we implemented a Tangram simulator in
which the pieces on the computer display can be
moved, rotated and flipped with simple mouse op-
erations. Figure 1 shows the simulator interface in
which the left shows the goal shape area and the
right the working area. We assigned two differ-
ent roles to participants, a solver and an operator;
the solver thinks of the arrangement of the pieces
to make the goal shape and gives instructions to
the operator, while the operator manipulates the
pieces with the mouse according to the solver?s in-
structions.
A solver and an operator sit side by side in front
of their own computer display. Both participants
share the same working area of the simulator. The
operator can manipulate the pieces, but cannot see
the goal shape. In contrast, the solver sees the goal
shape but cannot move pieces. A shield screen was
set between the participants in order to prevent
them from peeking at their partner?s display. In
this asymmetrical interaction, we can expect many
referring expressions during the interaction.
Each pair is assigned four exercises and the par-
ticipants exchanged roles after two exercises. We
set a time limit of 15 minutes for an exercise.
Utterances by the participants are recorded sep-
arately in stereo through headset microphones in
synchronisation with the position of the pieces and
the mouse actions. In total, we collected 24 dia-
logues of about four hours. The average length of
a dialogue was 10 minutes 43 seconds.
2.2 Annotation
Recorded dialogues were transcribed with a time
code attached to each utterance. Since our main
concern is collecting referring expressions, we de-
fined an utterance to be a complete sentence to
prevent a referring expression being split into sev-
eral utterances. Referring expressions were an-
notated together with their referents by using the
multi-purpose annotation tool SLAT (Noguchi et
al., 2008). Two annotators (two of the authors) an-
notated four dialogue texts separately. We anno-
tated all 24 dialogue texts and corrected discrep-
ancies by discussion between the annotators.
3 Analysis of the corpus
We collected a total of 1,509 tokens and 449 types
of referring expressions in 24 dialogues. Our
asymmetric experimental setting tended to encour-
age referring expressions from the solver, while
the operator was constrained to confirming his un-
derstanding of the solver?s instructions. This is re-
flected in the number of referring expressions by
the solver (1,287) largely outnumbering those of
the operator (222). There are a number of expres-
sions (215 expressions; 15% of the total) referring
to multiple objects (referring to 2 or more pieces)
and we excluded them from our current analysis.
We exclusively deal here with expressions refer-
ring to a specific single piece or indefinite expres-
sions, i.e. those that have no definite referent (in
total 1,294 tokens).
We found the following syntactic/semantic fea-
tures used in the expressions: i) demonstratives
(adjectives and pronouns), ii) object attribute-
values, iii) spatial relations, iv) actions on an ob-
ject and v) others. The number of these features is
summarised in Table 1. (Note that multiple fea-
tures can be used in a single expression.) The
right-most column shows an example with its En-
111
Table 1: Features of referring expressions
Feature types tokens Example
i) demonstrative 118 745
adjective 100 196 ?ano migigawa no sankakkei (that triangle at the right side)?
pronoun 19 551 ?kore (this)?
ii) attribute 303 641
size 165 267 ?tittyai sankakkei (the small triangle)?
shape 271 605 ?o?kii sankakkei (the large triangle)?
direction 6 6 ?ano sita muiteru dekai sankakkei (that large triangle facing to the bottom)?
iii) spatial relations 129 148
projective 125 144 ?hidari no okkii sankakkei (the small triangle on the left)?
topological 2 2 ?o?kii hanareteiru yatu (the big distant one)?
overlapping 2 2 ?sono sita ni aru sankakkei (the triangle underneath it)?
iv) action-mentioning 78 85 ?migi ue ni doketa sankakkei (the triangle you put away to the top right)?
v) others 29 30
remaining 15 15 ?nokotteiru o?kii sankakkei (the remaining large triangle)?
similarity 14 15 ?sore to onazi katati no (the one of the same shape as that one)?
glish translation. The identified feature in the re-
ferring expression is underlined.
We note here a tendency to employ object at-
tributes, particularly the attribute ?shape? as well
as use of demonstratives, particularly demonstra-
tive pronouns. These kinds of referring expres-
sions are quite general and appear in a variety of
other non-situated settings as well. In addition,
we found another kind of expression not usually
employed by humans outside of situated collabo-
ration tasks; referring expressions mentioning an
action on an object. We have 85 expressions (over
6% of the total) of this type in our corpus.
4 Action-mentioning expressions
We further analysed those expressions that men-
tion an action on an object, which we call action-
mentioning expressions hereafter. Although there
was significant variation in usage of action-
mentioning expressions among the pairs, all 6
pairs of participants used at least one action-
mentioning expression, indicating that it is a fun-
damental type of expression for this task set-
ting. Action-mentioning expressions are different
from haptic-ostensive referring expressions (Fos-
ter et al, 2008) since action-mentioning expres-
sions are not necessarily accompanied by simulta-
neous physical operation on an object.
Action-mentioning expressions can be again di-
vided into three categories: i) combination of a
temporal adverbial with a verb indicating an ac-
tion (?turned?, ?put?, ?moved?, etc) (55 tokens or
about 65% of action-mentioning expression), ii)
use of temporal adverbials without a verb, i.e. verb
ellipsis (22 tokens or about 26%) and iii) expres-
sions with a verb without temporal adverbials (8
tokens or about 9%). The second category includ-
ing verb ellipsis would be rare in English, but it is
quite natural in Japanese.
Only less than 10% of this kind of expression
did not include any temporal adverbial, indicating
that humans tend to describe the temporal aspect
of an action. This needs to be integrated into any
generation algorithm for this task domain. The
temporal adverbials used by the participants were
the Japanese ?sakki no NP (the NP [verb-ed] just
before)? or ?ima no NP (the current NP/the NP
[you are verb-ing] now/the NP [verb-ed] just be-
fore)?. ?Ima? generally refers to the current time
point (?now?). It can, however, refer to a past time
point as well, thus it is ambiguous.
Participants tended to use ?ima? largely in its
perfect meaning (completed action). The fre-
quency of use of ?ima? in its perfect meaning in
comparison to its progressive meaning was about
2:1. The distribution of the two types of tempo-
ral adverbials ?sakki? and ?ima? was about 2:3.
The slight preference here for ?ima? might be ex-
plained by its dual meanings (progressive and per-
fect) in contrast to the exclusive use of ?sakki? for
past actions.
Figure 2 shows the distribution of ?sakki (just
before)? and past-cases of ?ima (now)? dependent
on the time-distance to the action they refer to. For
actions occurring within a timeframe of about 10
seconds previous to uttering an expression, partic-
ipants had an overwhelming preference for ?ima?.
The frequency of ?ima? decreases quickly for ac-
tions that occurred 10-20 seconds prior to the ut-
terance. In contrast, after 20 seconds from the ac-
112
04
8
12
16
0
?
1
0
1
0
?
2
0
2
0
?
3
0
3
0
?
4
0
4
0
?
5
0
5
0
?
6
0
6
0
?
7
0
7
0
?
8
0
8
0
?
9
0
9
0
?
1
0
0
1
0
0
?
f
r
e
q
u
e
n
c
y
time (sec)
"sakki (just before)"
"ima (now)"
Figure 2: Frequency of ?sakki? and ?ima? over the
time-distance to referenced action
tion, participants prefered ?sakki?.
In addition, we investigated what actions oc-
curred in between the utterance and the action
mentioned. The actions we take into account here
are basic manipulations of an object like ?move?,
?flip?, ?click? and so on. Referring to an immedi-
ately preceding action, participants had a strong
preference for using ?ima?. Interestingly, with
only one other action in between, the participants?
preference becomes opposite (i.e. ?sakki? is pre-
ferred.). For referring to actions further in the past
(i.e. more actions in between), there was a con-
tinous preference for ?sakki? over ?ima?. Further
analysis should also investigate the phenomenon
of the difference in use of temporal adverbials for
other languages and whether this is related to char-
acteristics of the Japanese language or rather an in-
herent property of the use of temporal adverbials
in natural language.
5 Conclusion and future work
We collected a corpus of Japanese referring ex-
pressions as a first step towards developing algo-
rithms for generating referring expressions in a sit-
uated collaboration. We carried out an initial anal-
ysis of the collected expressions, focussing on ex-
pressions that include a mention of an action on
an object. We noted that they are often combined
with temporal adverbials with participants seek-
ing to make a temporal ordering of actions. In
addition, we intend to further analyse other types
of expressisons (demonstratives, etc) and work on
developing generation algorithms for this domain.
In future work, we intend to generalise this exper-
iment in the Tangram-domain to other domains.
Furthermore, information such as gestures and eye
movements should be incorporated in data collec-
tion. This will lay the basis for the development of
more general models for the generation of refer-
ring expressions in a situated collaborative task.
References
Donna Byron, Thomas Mampilly, Vinay Sharma, and
Tianfang Xu. 2005. Utilizing visual attention for
cross-modal coreference interpretation. In CON-
TEXT 2005, pages 83?96.
Donna K. Byron. 2005. The OSU Quake 2004 cor-
pus of two-party situated problem-solving dialogs.
Technical report, Department of Computer Science
and Enginerring, The Ohio State University.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as
a collaborative process. Cognition, 22:1?39.
B. Di Eugenio, P. W. Jordan, R. H. Thomason, and J. D
Moore. 2000. The agreement process: An empirical
investigation of human-human computer-mediated
collaborative dialogues. International Journal of
Human-Computer Studies, 53(6):1017?1076.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring expres-
sions in cooperative, task-based human-robot dia-
logue. In Proceedings of 3rd Human-Robot Inter-
action, pages 295?302.
Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,
Ryu Iida, Mamoru Komachi, and Kentaro Inui.
2008. Multiple purpose annotation using SLAT ?
Segment and link-based annotation tool. In Pro-
ceedings of 2nd Linguistic Annotation Workshop,
pages 61?64.
Marilyn A. Walker Pamela W. Jordan. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence
Research, 24:157?194.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2008. SCARE: A sit-
uated corpus with annotated referring expressions.
In Proceedings of the Sixth International Confer-
ence on Language Resources and Evaluation (LREC
2008).
Kees van Deemter. 2007. TUNA: Towards a UNified
Algorithm for the generation of referring expres-
sions. Technical report, Aberdeen University.
www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
113
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 88?95,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Automatic Extraction of Citation Contexts for Research Paper
Summarization: A Coreference-chain based Approach
Dain Kaplan Ryu Iida
Department of Computer Science
Tokyo Institute of Technology
{dain,ryu-i,take}@cl.cs.titech.ac.jp
Takenobu Tokunaga
Abstract
This paper proposes a new method based
on coreference-chains for extracting cita-
tions from research papers. To evaluate
our method we created a corpus of cita-
tions comprised of citing papers for 4 cited
papers. We analyze some phenomena of
citations that are present in our corpus,
and then evaluate our method against a
cue-phrase-based technique. Our method
demonstrates higher precision by 7?10%.
1 Introduction
Review and comprehension of existing research is
fundamental to the ongoing process of conducting
research; however, the ever increasing volume of
research papers makes accomplishing this task in-
creasingly more difficult. To mitigate this problem
of information overload, a form of knowledge re-
duction may be necessary.
Past research (Garfield et al, 1964; Small,
1973) has shown that citations contain a plethora
of latent information available and that much
can be gained by exploiting it. Indeed, there
is a wealth of literature on topic-clustering, e.g.
bibliographic coupling (Kessler, 1963), or co-
citation analysis (Small, 1973). Subsequent re-
search demonstrated that citations could be clus-
tered on their quality, using keywords that ap-
peared in the running-text of the citation (Wein-
stock, 1971; Nanba et al, 2000; Nanba et al,
2004; Teufel et al, 2006).
Similarly, other work has shown the utility in
the IR domain of ranking the relevance of cited pa-
pers by using supplementary index terms extracted
from the content of citations in citing papers,
including methods that search through a fixed
character-length window (O?Connor, 1982; Brad-
shaw, 2003), or that focus solely on the sentence
containing the citation (Ritchie et al, 2008) for
acquiring these terms. A prior case study (Ritchie
et al, 2006) pointed out the challenges in proper
identification of the full span of a citation in run-
ning text and acknowledged that fixed-width win-
dows have their limits. In contrast to this, en-
deavors have been made to extract the entire span
of a citation by using cue-phrases collected and
deemed salient by statistical merit (Nanba et al,
2000; Nanba et al, 2004). This has met in evalua-
tions with some success.
The Cite-Sum system (Kaplan and Tokunaga,
2008) also aims at knowledge reduction through
use of citations. It receives a paper title as a query
and attempts to generate a summary of the paper
by finding citing papers1 and extracting citations
in the running-text that refer to the paper. Before
outputting a summary, it also classifies extracted
citation text, and removes citations with redun-
dant content. Another similar study (Qazvinian
and Radev, 2008) aims at using the content of ci-
tations within citing papers to generate summaries
of fields of research.
It is clear that merit exists behind extraction
of citations in running text. This paper proposes
a new method for performing this task based on
coreference-chains. To evaluate our method we
created a corpus of citations comprised of citing
papers for 4 cited papers. We also analyze some
phenomena of citations that are present in our cor-
pus.
The paper organization is as follows. We first
define terminology, discuss the construction of our
corpus and the results found through its analysis,
and then move on to our proposed method us-
ing coreference-chains. We evaluate the proposed
method by using the constructed corpus, and then
conclude the paper.
1Papers are downloaded automatically from the web.
88
2 Terminology
So that we may dispense with convoluted explana-
tions for the rest of this paper, we introduce several
terms.
An anchor is the string of characters that marks
the occurrence of a citation in the running-text of a
paper, such as ?(Fakeman 2007)? or ?[57]?.2 The
sentence that this anchor resides within is then the
anchor sentence. The citation continues from be-
fore and after this anchor as long as the text con-
tinues to refer to the cited work; this block of text
may span more than a single sentence. We intro-
duce the citation-site, or c-site for short, to rep-
resent this block of text that discusses the cited
work. Since more than once sentence may discuss
the cited work, each of these sentences is called a
c-site sentence. For clarity will also call the an-
chor the c-site anchor henceforth. A citing paper
contains the c-site that refers to the cited paper.
Finally, the reference at the end of the paper pro-
vides details about a c-site anchor (and the c-site).
Figure 1 shows a sample c-site with the c-site
anchor wavy-underlined, and the c-site itself itali-
cized; the non-italicized text is unrelated to the c-
site. The reference for this c-site is also provided
below the dotted line. In all subsequent examples,
the c-site will be in italics and the current place of
emphasis wavy-underlined.
?. . . Our area of interest is plant growth. In past
research (
:::::::
Fakeman
::
et
:::
al.,
::::
2001), the relationship
between sunlight and plant growth was shown to
directly correlate. It was also shown to adhere
to simple equations for deducing this relation-
ship, the equation varying by plant. We propose
a method that . . . ?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
J. Fakeman: Changing Plant Growth Factors
during Global Warming. In: Proceedings of
SCANLP 2001.
Figure 1: A sample c-site and its reference
3 Corpus Construction and Analysis
We created a corpus comprised of 38 papers citing
4 (cited) papers taken from Computational Lin-
guistics: Special Issue on the Web as Corpus, Vol-
ume 29, Number 3, 2003 as our data set and pre-
processed it to automatically mark c-site anchors
2In practice the anchor does not include brackets, though
the brackets do signal the start/end of the anchor. This is be-
cause multiple anchors may be present at once, e.g. (Fakeman
2007; Noman 2008).
to facilitate the annotation process. The citing pa-
pers were downloaded from CiteSeer-X;3 see Ta-
ble 1 for details.
We then proceeded to manually annotate the
corpus using SLAT (Noguchi et al, 2008), a
browser-based multi-purpose annotation tool. We
devised the following guidelines for annotation.
Since the tool allows for two types of annotation,
namely segments that demarcate a region of text,
and links, that allow an annotator to assign rela-
tionships between them, we created four segment
types and three link types. Segments were used
to mark c-site anchors, c-sites, background infor-
mation (explained presently), and references. We
used the term background information to refer to
any running-text that elaborates on a c-site but is
not strictly part of the c-site itself (refer to Fig-
ure 2 for an example). Even during annotation,
however, we encountered situations that felt am-
biguous, making this a rather contentious issue.
Our corpus had a limited number of background
information annotations, or we would likely have
experienced more issues. That being said, it is at
least important to recognize that such kinds of sup-
plementary content exist (that may not be part of
the c-site but is still beneficial to be included), and
needs to be considered more in the future.
We then linked each c-site to its anchor, each an-
chor to its reference, and any background informa-
tion to the c-site supplemented. We also decided
on annotating entire sentences, even if only part
of a sentence referred to the cited paper. Table 1
outlines our corpus.
Table 1: Corpus composition
Paper ID 1 2 3 4 Total
Citing papers 2 14 15 7 38
C-sites 3 17 18 12 50
C-site sentences 6 27 33 28 94
To our knowledge, this is the first corpus con-
structed in the context of paper summarization re-
lated to collections of citing papers.4
Analysis of the corpus provided some interest-
ing insights, though a larger corpus is required to
confirm the frequency and validity of such phe-
nomena. The more salient discoveries are item-
ized below. These phenomena may also co-occur.
3http://citeseerx.ist.psu.edu
4Though not specific to the task of summarization through
use of c-sites, citation corpora have been constructed in the
past, e.g. (Teufel et al, 2006).
89
Background Information Though not strictly
part of a c-site, background information may need
to be included for the citation to be comprehensi-
ble. Take Figure 2 for example (background infor-
mation is wavy-underlined) for the c-site anchor
?(Resnik & Smith 2003)?. The authors insert their
own research into the c-site (illustrated with wavy-
underlines); this information is important for un-
derstanding the following c-site sentence, but is
not strictly discussing the cited paper. Background
information is thus a form of ?meta-information?
about the c-site.
In well written papers, often the flow of content
is gradual, which can make distinguishing back-
ground information difficult.
?. . .Resnik and his colleagues (Resnik & Smith
2003) proposed a new approach, STRAND,
. . . The databases for parallel texts in several lan-
guages with download tools are available from
the STRAND webpage. Recently they also ap-
plied the same technique for collecting a set of
links to monolingual pages identified as Russian
by http://www.archive.org, and Internet archiv-
ing service.
::
We
:::::
have
:::::::::
evaluated
:::
the
:::::::
Russian
:::::::
database
::::::::
produced
::
by
:::
this
:::::::
method
:::
and
::::::::
identified
:
a
:::::::
number
::
of
::::::
serious
::::::::
problems
:::::
with
::
it. First, it
does not identify the time when the page was
downloaded and stored in the Internet archive
. . . ?
Figure 2: A non-contiguous c-site w/ background
information (from (Sharoff, 2006))
Contiguity C-sites are not necessarily contigu-
ous. We found in fact that authors tend to in-
sert opinions or comments related to their own
work with sentences/clauses in between actual c-
site sentences/clauses, that would be best omitted
from the c-site. In Figure 2 the wavy-underlined
text shows the author?s opinion portion. This cre-
ates problems for cue-phrase based techniques, as
though they detect the sentence following it, they
fail on the opinion sentence. Incorporation of a le-
niency for a gap in such techniques may be pos-
sible, but seems more problematic and likely to
misidentify c-site sentences altogether.
Related/Itemization Authors often list several
works (namely, insert several c-site anchors) in the
same sentence using connectives. The works may
likely be related, and though this may be useful
information for certain tasks, it is important to dif-
ferentiate which material is related to the c-site,
and which is the c-site itself.
In Figure 3 the second sentence discusses both
c-site anchors (and should be included in both
their c-sites); the first sentence, however, contains
two main clauses connected with a connective,
each clause a different c-site (one with the anchor
?[3]? and one with ?[4]?). Sub-clausal analysis is
necessary for resolving issues such as these. For
our current task, however, we annotated only sen-
tences, and so in this example the second c-site
anchor is included in the first.
?. . . STRAND system [4] searches the web for
parallel text
:::
and
:::
[3]
:::::::
extracts
::::::::::
translations
::::
pairs
:::::
among
::::::
anchor
::::
texts
:::::::
pointing
:::::::
together
::
to
:::
the
::::
same
:::::::
webpage. However they all suffered from the lack
of such bilingual resources available on the web
. . . ?
Figure 3: Itemized c-sites partially overlapping
(from (Zhang et al, 2005))
Nesting C-sites may be nested. In Figure 4
the nested citation (?[Lafferty and Zhai 2001,
Lavrenko and Croft 2001]?) should be included in
the parent one (?[Kraaij et al 2002]?). The wavy-
underlined portion shows the sentence needed for
full comprehension of the c-site.
?. . .
::
In
:::::
recent
:::::
years,
:::
the
:::
use
::
of
::::::::
language
::::::
models
::
in
::
IR
:::
has
::::
been
::
a
::::
great
::::::
success
::::::::
[Lafferty
:::
and
::::
Zhai
::::
2001,
::::::::
Lavrenko
::::
and
:::::
Croft
::::::
2001]. It is possible
to extend the approach to CLIR by integrating a
translation model. This is the approach proposed
in [Kraaij et al 2002] . . . ?
Figure 4: Separate c-site anchors does not mean
separate c-sites (from (Nie, 2002))
Aliases Figure 5 demonstrates another issue:
aliasing. The author redefines how they cite the
paper, in this case using the acronym ?K&L?.
?. . . To address the data-sparsity issue, we em-
ployed the technique used in Keller and Lapata
(2003, K&L) to get a more robust approxima-
tion of predicate-argument counts.
::::
K&L use this
technique to obtain frequencies for predicate-
argument bigrams that were unseen in a given
corpus, showing that the massive size of the web
outweighs the noisy and unbalanced nature of
searches performed on it to produce statistics
that correlate well with corpus data . . . ?
Figure 5: C-Site with Aliasing for anchor ?Keller
and Lapata (2003, K&L)? (from (Kehler, 2004))
4 Coreference Chain-based Extraction
Some of the issues found in our corpus, namely
identification of background information, non-
contiguous c-sites, and aliases, show promise of
90
Table 2: Evaluation results for coreference resolution against the MUC-7 formal corpus.
MUC-7 Task Sentence Eval.
System Setting R P F R P F
All Features 35.71 74.71 48.33 36.27 80.49 50.00
w/o SOON STR MATCH 48.35 83.81 61.32 48.35 88.00 62.41
w/o COSINE SIMILARITY 46.70 82.52 59.65 46.70 86.73 60.71
resolution with coreference-chains. This is be-
cause coreference-chains match noun phrases that
appear with other noun phrases to which they re-
fer, a characteristic present in these three cate-
gories. On the other hand, cue-phrases do not
detect any c-site sentence that does not use key-
words (e.g. ?In addition?). In the following sec-
tion we discuss our implementation of a corefer-
ence chain-based extraction technique, and how
we then applied it to the c-site extraction task. An
analysis of the results then follows.
4.1 Training the Coreference Resolver
To create and train our coreference resolver, we
used a combination of techniques as outlined orig-
inally by (Soon et al, 2001) and subsequently
extended by (Ng and Cardie, 2002). Mim-
icking their approaches, we used the corpora
provided for the MUC-7 coreference resolution
task (LDC2001T02, 2001), which includes sets of
newspaper articles, annotated with coreference re-
lations, for both training and testing. They also
outlined a list of features to extract for training
the resolver to recognize the coreference relations.
Specifically, (Soon et al, 2001) established a list
of 12 features that compare a given anaphor with
a candidate antecedent, e.g. gender agreement,
number agreement, both being pronouns, both part
of the same semantic class (i.e. WordNet synset
hyponyms/hypernyms), etc.
For training the resolver, a corpus annotated
with anaphors and their antecedents is processed,
and pairs of anaphor and candidate antecedents are
created so as to have only one positive instance
per anaphor (the annotated antecedent). Negative
examples are created by taking all occurrences of
noun phrases that occur between the anaphor and
its antecedent in the text. The antecedent in these
steps is also always considered to be to the left of,
or preceding, the anaphor; cataphors are not ad-
dressed in this technique.
We implemented, at least minimally, all 12 of
these features, with a few additions of what (Ng
and Cardie, 2002) hand selected as being most
salient for increased performance. We also ex-
tended this list by adding a cosine-similarity met-
ric between two noun phrases; it uses bag-of-
words to create a vector for each noun phrase
(where each word is a term in the vector) to com-
pute their similarity. The intuition behind this is
that noun phrases with more similar surface forms
should be more likely to corefer.
We further optimized string recognition and
plurality detection for handling citation-strings.
See Table 3 for the full list of our features. While
both (Soon et al, 2001) and (Ng and Cardie, 2002)
induced decision trees (C5 and C4.5, respectively)
we opted for using an SVM-based approach in-
stead (Vapnik, 1998; Joachims, 1999). SVMs are
known for being reliable and having good perfor-
mance.
4.2 Evaluating the Coreference Resolver
We ran our trained SVM classifier against the
MUC-7 formal evaluation corpus; the results are
shown in Table 2.
The results using all features listed in Table 3
are inferior to those set forth by (Soon et al,
2001; Ng and Cardie, 2002); likely this is due
to poorer selection of features. Upon analysis, it
seems that half of the misidentified antecedents
were still chosen within the correct sentence and
more than 10% identified the proper antecedent,
but selected the entire noun phrase (when that
antecedent was marked as, for example, only its
head); the majority of these cases involved the
antecedent being only one sentence away from
the anaphor. Since the former seemed suspect of
a partial string matching feature, we decided to
re-run the tests first excluding our implementa-
tion of the SOON STR MATCH feature, and then
our COSINE SIMILARITY feature. The results
for this are shown in Table 2. It can be seen
that using either of the two string comparison fea-
tures works substantially better than with both of
them in tandem, with the COSINE SIMILARITY
feature showing signs of overall better perfor-
mance which is competitive to (Soon et al,
91
Table 3: Features used for coreference resolution.
Feature Possible Values Brief Description (where necessary)
ANAPHOR IS PRONOUN T/F
ANAPHOR IS INDEFINITE T/F
ANAPHOR IS DEMONSTRATIVE T/F
ANTECEDENT IS PRONOUN T/F
ANTECEDENT IS EMBEDDED T/F Boolean indicating if the candidate antecedent is within another
NP.
SOON STR MATCH T/F As per (Soon et al, 2001). Articles and demonstrative pronouns
removed before comparing NPs. If any part of the NP matches
between candidate and anaphor set to true (T); false otherwise.
ALIAS MATCH T/F Creates abbreviations for organizations and proper names in an
attempt to find an alias.
BOTH PROPER NAMES T/F
BOTH PRONOUNS T/F/?
NUMBER AGREEMENT T/F/? Basic morphological rules applied to the words to see if they are
plural.
COSINE SIMILARITY NUM A cosine similarity score between zero and one is applied to the
head words of each NP.
GENDER AGREEMENT T/F/? If the semantic class is Male or Female, use that gender, other-
wise if a salutation is present, or lastly set to Unknown.
SEMANTIC CLASS AGREEMENT T/F/? Followed (Soon et al, 2001) specifications for using basic
WordNet synsets, specifically: Female and Male belonging to
Person, Organization, Location, Date, Time, Money, Percent
belonging to Object. Any other semantic classes mapped to
Unknown.
2001; Ng and Cardie, 2002). We exclude the
SOON STR MATCH feature in the following ex-
periments.
However, the MUC-7 task measures the ability
to identity the proper antecedent from a list of can-
didates; the c-site extraction task is less ambitious
in that it must only identify if a sentence contains
the antecedent, not which noun phrase it is. When
we evaluate our resolver using these loosened con-
ditions it is expected that it will perform better.
To accomplish this we reevaluate the results
from the resolver in a sentence-wise manner; we
group the test instances by anaphor, and then by
sentence. If any noun phrase within the sentence
is marked as positive when there is in fact a pos-
itive noun phrase in the sentence, the sentence is
marked as correct, and incorrect otherwise. The
results in Table 2 for this simplified task show
an increase in recall, and subsequently F-measure.
The numbers for the loosened constraints eval-
uation are counted by sentence; the original is
counted by noun phrase only.
Our system also generates many fewer training
instances than the previous research, which we at-
tribute to a more stringent noun phrase extraction
procedure, but have not investigated thoroughly
yet.
4.3 Application to the c-site extraction task
As outlined above, we used the resolver with the
loosened constraints, namely evaluating the sen-
tence a potential antecedent is in as likely or not,
and not which noun phrase within the sentence is
the actual antecedent. Using this principle as a
base, we devised an algorithm for scanning sen-
tences around a c-site anchor sentence to deter-
mine their likelihood of being part of the c-site.
The algorithm, shown in simplified form in Fig-
ure 6, is described below.
Starting at the beginning of a c-site anchor
sentence AS, scan left-to-right; for every noun
phrase encountered within AS, begin a right-to-
left sentence-by-sentence search; prepend any sen-
tence S containing an antecedent above a certain
likelihood THRESHOLD, until DISTANCE sen-
tences have been scanned and no suitable candi-
date sentences have been found. We set the like-
lihood score to 1.0, tested ad-hoc for best results,
and the distance-threshold to 5 sentences, having
noted in our corpus that no citation is discontinu-
ous by more than 4.
In a similar fashion, the algorithm then pro-
ceeds to scan text following AS; for every noun
phrase NP encountered (moving left-to-right), be-
gin a right-to-left search for a suitable antecedent.
If a sentence is not evaluated above THRESHOLD,
92
Table 4: Evaluation results for c-site extraction w/o background information
Sentence (Micro-average) C-site (Macro-average)
Method R P F R P F
Baseline 1 (anchor sentence) 53.2 100 69.4 74.6 100 85.5
Baseline 2 (random) 75.5 58.2 65.7 87.4 71.2 78.5
Cue-phrases (CP) 64.9 64.9 64.9 84.0 80.9 82.4
Coref-chains (CC)) 64.9 74.4 69.3 81.0 87.2 84.0
CP/CC Union 74.5 58.8 65.7 88.4 75.0 81.1
CP/CC Intersection 55.3 91.2 69.0 76.6 95.7 85.1
set CSITE to AS
pre:
foreach NP in AS
foreach sentence S preceding AS
if DISTANCE > MAX-DIST goto post
if likelihood > THRESHOLD then
set CSITE to S + CSITE
reset DISTANCE
end
end
end
post:
foreach sentence S after AS
foreach NP in S
foreach sentence S2 until S
if DISTANCE > MAX-DIST stop
if S2 has link then
if likelihood > THRESHOLD then
set S2 has link
end
end
end
end
end
Figure 6: Simplified c-site extraction algorithm
using coreference-chains
it will be ignored when the algorithm backtracks
to look for candidate noun phrases for a subse-
quent sentence, thus preserving the coreference-
chain and preventing additional spurious chains.
If more than DISTANCE sentences are scanned
without finding a c-site sentence, the process is
aborted and the collection of sentences returned.
4.4 Experiment Setup
To evaluate our coreference-chain extraction
method we compare it with a cue-phrases tech-
nique (Nanba et al, 2004) and two baselines.
Baseline 1 extracts only the c-site anchor sen-
tence as the c-site; baseline 2 includes sentences
before/after the c-site anchor sentence as part of
the c-site with a 50/50 probability ? it tosses
a coin for each consecutive sentence to decide
its inclusion. We also created two hybrid meth-
ods that combine the results of the cue-phrases
and coreference-chain techniques, one the union
of their results (includes the extracted sentences
of both methods), and the other the intersection
(includes sentences only for which both methods
agree), to measure their mutual compatibility.
The annotated corpus provided the locations of
c-site anchors for the cited paper within the citing
paper?s running-text. We then compared the ex-
tracted c-sites of each method to the c-sites of the
annotated corpus.
4.5 Evaluation
The results of our experiments are presented in Ta-
ble 4. We evaluated each method as follows. Re-
call and precision were measured for a c-site based
on the number of extracted sentences; if an ex-
tracted sentence was annotated as part of the c-site,
it counted as correct, and if an extracted sentence
was not part of a c-site, incorrect; sentences an-
notated as being part of the c-site not extracted by
the method counted as part of the total sentences
for that c-site. As an example, if an annotated c-
site has 3 sentences (including the c-site anchor
sentence), and the evaluated method extracted 2 of
these and 1 incorrect sentence, then the recall for
this c-site using this method would be 2/3, and the
precision 2/(2 + 1).
Since the evaluation is inherently sentence-
based, we provide two averages in Table 4. The
micro-average is for sentences across all c-sites;
in other words, we tallied the correct and incorrect
sentence count for the whole corpus and then di-
vided by the total number of sentences (94). This
average provides a clearer picture on the efficacy
of each method than does the macro-average. The
macro-average was computed per c-site (as ex-
plained above) and then averaged over the total
number of c-sites in the corpus (50).
With the exception of a 3% lead in macro-
average recall, coreference-chains outperform
cue-phrases in every way. We can see a substan-
93
tial difference in micro-average precision (74.4
vs. 64.9), which results in nearly a 5% higher
F-measure. The macro-average precision is also
higher by more than 6%. It matches more and
misses far less. The loss in the macro-average
recall can be attributed to the coreference-chain
method missing one of two sentences for several
c-sites, which would lower its overall recall score;
keep in mind that since in the macro-average all c-
sites are treated equally, even large c-sites in which
the coreference-chain method performs well, such
an advantage will be reduced with averaging and
is therefore misleading.
Baseline 2 performed as expected, i.e. higher
than baseline 1 for recall. Looking only at F-
measures for evaluating performance in this case
is misleading. This is particularly the case because
precision is more important than recall ? we want
accuracy. Coreference-chains achieved a precision
of over 87.2 compared to the 71.2 of baseline 2.
The combined methods also showed promise.
In particular, the intersection method had very
high precision (91.2 and 95.7), and marginally
managed to extract more sentences than base-
line 1. The union method has more conservative
scores.
We also understood from our corpus that only
about half of c-sites were represented by c-site an-
chor sentences. The largest c-site in the corpus
was 6 sentences, and the average 1.8. This means
using the c-site anchor sentence alone excludes on
average about half of the valuable data.
These results are promising, but a larger corpus
is necessary to validate the results presented here.
5 Conclusions and Future Work
The results demonstrate that a coreference-chain-
based approach may be useful to the c-site ex-
traction task. We can also see that there is still
much work to be done. The scores for the hy-
brid methods also indicate potential for a method
that more tightly couples these two tasks, such
as Rhetorical Structure Theory (RST) (Thompson
and Mann, 1987; Marcu, 2000). Though it has
demonstrated superior performance, coreference
resolution is not a light-weight task; this makes
real-time application more difficult than with cue-
phrase-based approaches.
Our plans for future work include the construc-
tion of a larger corpus of c-sites, investigation of
other features for improving our coreference re-
solver, and applying RST to c-site extraction.
Acknowledgments
The authors would like to express appreciation to
Microsoft for their contribution to this research by
selecting it as a recipient of the 2008 WEBSCALE
Grant (Web-Scale NLP 2008, 2008).
References
Shannon Bradshaw. 2003. Reference directed index-
ing: Redeeming relevance for subject search in cita-
tion indexes. In Proceedings of the 7th ECDL, pages
499?510.
Eugene Garfield, Irving H. Sher, and Richard J. Torpie.
1964. The use of citation data in writing the his-
tory of science. Institute for Scientific Information,
Philadelphia, Pennsylvania.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Scho?lkopf, Christopher J. C. Burges, and Alexan-
der J. Smola, editors, Advances in kernel methods:
support vector learning, pages 169?184. MIT Press,
Cambridge, MA, USA.
Dain Kaplan and Takenobu Tokunaga. 2008. Sighting
citation sites: A collective-intelligence approach for
automatic summarization of research papers using
c-sites. In ASWC 2008 Workshops Proceedings.
Andrew Kehler. 2004. The (non)utility of predicate-
argument frequencies for pronoun interpretation. In
In: Proceedings of 2004 North American chapter of
the Association for Computational Linguistics an-
nual meeting, pages 289?296.
M. M. Kessler. 1963. Bibliographic coupling be-
tween scientific papers. American Documentation,
14(1):10?25.
LDC2001T02. 2001. Message understanding confer-
ence (MUC) 7.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Hidetsugu Nanba, Noriko Kando, and Manabu Oku-
mura. 2000. Classification of research papers using
citation links and citation types: Towards automatic
review article generation. In Proceedings of 11th
SIG/CR Workshop, pages 117?134.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Oku-
mura, and Suguru Saito. 2004. Bilingual presri inte-
gration of multiple research paper databases. In Pro-
ceedings of RIAO 2004, pages 195?211, Avignon,
France.
94
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 104?
111.
J. Nie. 2002. Towards a unified approach to clir and
multilingual ir. In In: Workshop on Cross Language
Information Retrieval: A Research Roadmap in the
25th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 8?14.
Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,
Ryu Iida, Mamoru Komachi, and Kentaro Inui.
2008. Multiple purpose annotation using SLAT ?
Segment and link-based annotation tool ?. In Pro-
ceedings of 2nd Linguistic Annotation Workshop,
pages 61?64, May.
John O?Connor. 1982. Citing statements: Computer
recognition and use to improve retrieval. Informa-
tion Processing & Management., 18(3):125?131.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006. How to find better index terms through cita-
tions. In Proceedings of the Workshop on How Can
Computational Linguistics Improve Information Re-
trieval?, pages 25?32, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for informa-
tion retrieval. In CIKM ?08: Proceedings of the
17th ACM conference on Information and knowl-
edge management, pages 213?222, New York, NY,
USA. ACM.
Serge Sharoff. 2006. Creating general-purpose cor-
pora using automated search engine queries. In
WaCky! Working papers on the Web as Corpus.
Gedit.
H. Small. 1973. Co-citation in the scientific literature:
A newmeasure of the relationship between two doc-
uments. JASIS, 24:265?269.
Wee Meng Soon, Daniel Chung, Daniel Chung Yong
Lim, Yong Lim, and Hwee Tou Ng. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521?544.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In In Proceedings of EMNLP-06.
Sandra A. Thompson and William C. Mann. 1987.
Rhetorical structure theory: A framework for the
analysis of texts. Pragmatics, 1(1):79?105.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Adaptive and Learning Systems for Signal Pro-
cessing Communications, and control. JohnWiley &
Sons.
Web-Scale NLP 2008. 2008. http:
//research.microsoft.com/ur/asia/
research/NLP.aspx.
M. Weinstock. 1971. Citation indexes. Encyclopedia
of Library and Information Science, 5:16?41.
Ying Zhang, Fei Huang, and Stephan Vogel. 2005.
Mining translations of oov terms from the web
through. In International Conference on Natural
Language Processing and Knowledge Engineering
(NLP-KE ?03), pages 669?670.
95
Opinion Extraction Using a Learning-Based
Anaphora Resolution Technique
Nozomi Kobayashi Ryu Iida Kentaro Inui Yuji Matsumoto
Nara Institute of Science and Technology
Takayama, Ikoma, Nara, 630-0192, Japan
{nozomi-k,ryu-i,inui,matsu}@is.naist.jp
Abstract
This paper addresses the task of extract-
ing opinions from a given document
collection. Assuming that an opinion
can be represented as a tuple ?Subject,
Attribute, Value?, we propose a compu-
tational method to extract such tuples
from texts. In this method, the main
task is decomposed into (a) the pro-
cess of extracting Attribute-Value pairs
from a given text and (b) the process of
judging whether an extracted pair ex-
presses an opinion of the author. We
apply machine-learning techniques to
both subtasks. We also report on the
results of our experiments and discuss
future directions.
1 Introduction
The explosive spread of communication on the
Web has attracted increasing interest in technolo-
gies for automatically mining large numbers of
message boards and blog pages for opinions and
recommendations.
Previous approaches to the task of mining a
large-scale document collection for opinions can
be classified into two groups: the document clas-
sification approach and the information extrac-
tion approach. In the document classification
approach, researchers have been exploring tech-
niques for classifying documents according to se-
mantic/sentiment orientation such as positive vs.
negative (e.g. (Dave et al, 2003; Pang and Lee,
2004; Turney, 2002)). The information extraction
approach, on the other hand, focuses on the task
of extracting elements which constitute opinions
(e.g. (Kanayama and Nasukawa, 2004; Hu and
Liu, 2004; Tateishi et al, 2001)).
The aim of this paper is to extract opinions
that represent an evaluation of a products together
with the evidence. To achieve this, we consider
our task from the information extraction view-
point. We term the above task opinion extraction
in this paper.
While they can be linguistically realized in
many ways, opinions on a product are in fact often
expressed in the form of an attribute-value pair.
An attribute represents one aspect of a subject and
the value is a specific language expression that
qualifies or quantifies the aspect. Given this ob-
servation, we approach our goal by reducing the
task to a general problem of extracting four-tuples
?Product, Attribute, Value, Evaluation? from a
large-scale text collection. Technology for this
opinion extraction task would be useful for col-
lecting and summarizing latent opinions from the
Web. A straightforward application might be gen-
eration of radar charts from collected opinions as
suggested by Tateishi et al (2004).
Consider an example from the automobile do-
main, I am very satisfied with the powerful engine
(of a car). We can extract the four-tuple ?CAR, en-
gine, powerful, satisfied? from this sentence. Note
that the distinction between Value and Evaluation
is not easy. Many expressions used to express a
Value can also be used to express an Evaluation.
For this reason, we do not distinguish value and
evaluation, and therefore consider the task of ex-
tracting triplets ?Product, Attribute, Value?. An-
other problem with opinion extraction is that we
want to get only subjective opinions. Given this
setting, the opinion extraction task can be decom-
posed into two subtasks: extraction of attribute-
value pairs related to a product and determination
of its subjectivity.
As we discuss in section 3, an attribute and its
value may not appear in a fixed expression and
may be separated. In some cases, the attribute
may be missing from a sentence. In this respect,
finding the attribute of a value is similar to finding
the missing antecedent of an anaphoric expres-
sion. In this paper, we discuss the similarities
and differences between opinion extraction and
anaphora resolution. Then, we apply a machine
learning-based method used for anaphora reso-
173
lution to the opinion extraction problem and re-
port on our experiments conducted on a domain-
restricted set of Japanese texts excerpted from re-
view pages on the Web.
2 Related work
In this section, we discuss previous approaches
to the opinion extraction problem. In the pattern-
based approach (Murano and Sato, 2003; Tateishi
et al, 2001), pre-defined extraction patterns and a
list of evaluative expressions are used. These ex-
traction patterns and the list of evaluation expres-
sions need to be manually created. However, as
is the case in information extraction, manual con-
struction of rules may require considerable cost to
provide sufficient coverage and accuracy.
Hu and Liu (2004) attempt to extract the at-
tributes of target products on which customers
have expressed their opinions using association
mining, and to determine whether the opinions
are positive or negative. Their aim is quite sim-
ilar to our aim, however, our work differs from
theirs in that they do not identify the value corre-
sponding to an attribute. Their aim is to extract
the attributes and their semantic orientations.
Taking the semantic parsing-based approach,
Kanayama and Nasukawa (2004) apply the idea
of transfer-based machine translation to the ex-
traction of attribute-value pairs. They regard the
extraction task as translation from a text to a sen-
timent unit which consists of a sentiment value,
a predicate, and its arguments. Their idea is
to replace the translation patterns and bilingual
lexicons with sentiment expression patterns and
a lexicon that specifies the polarity of expres-
sions. Their method first analyzes the predicate-
argument structure of a given input sentence mak-
ing use of the sentence analysis component of an
existing machine translation engine, and then ex-
tracts a sentiment unit from it, if any, using the
transfer component.
One important problem the semantic parsing
approach encounters is that opinion expres-
sions often appear with anaphoric expressions
and ellipses, which need to be resolved to
accomplish the opinion extraction task. Our
investigation of an opinion-tagged Japanese
corpus (described below) showed that 30% of
the attribute-value pairs we found did not have a
direct syntactic dependency relation within the
sentence, mostly due to ellipsis. For example1,
?dezain-wa?a hen-daga watashi-wa ?-ga ?suki-da?v
?design?a weird I [it] ?like?v
(The design is weird, but I like it.)
This type of case accounted for 46 out of 100
pairs that did not have direct dependency rela-
tions. To analyze predicate argument structure
robustly, we have to solve this problem. In the
next section, we discuss the similarity between
the anaphora resolution task and the opinion
extraction task and propose to apply to opinion
extraction a method used for anaphora resolution.
3 Method for opinion extraction
3.1 Analogy with anaphora resolution
We consider the task of extracting opinion tu-
ples ?Product, Attribute, Value? from review sites
and message boards on the Web dedicated to pro-
viding and exchanging information about retail
goods. On these Web pages, products are often
specified clearly and so it is frequently a trivial
job to extract the information for the Product slot.
We therefore in this paper focus on the problem
of extracting ?Attribute, Value? pairs.
In the process of attribute-value pair identifi-
cation for opinion extraction, we need to deal
with the following two cases: (a) both a value
and its corresponding attribute appear in the text,
and (b) a value appears in the text while its at-
tribute is missing since it is inferable form the
value expression and the context. The upper half
of Figure 1 illustrates these two cases in the auto-
mobile domain. In (b), the writer is talking about
the ?size? of the car, but the expression ?size? is
not explicitly mentioned in the text. In addition,
(b) includes the case where the writer evaluates
the product itself. For example, ?I?m very satis-
fied with my car!?: in this case, a value expres-
sion ?satisfied? evaluates the product as a whole,
therefore a corresponding attribute does not ex-
ists.
For the case (a), we first identify a value ex-
pression (like in Figure 1) in a given text and then
look for the corresponding attribute in the text.
Since we also see the case (b), on the other hand,
we additionally need to consider the problem of
whether the corresponding attribute of the identi-
fied value expression appears in the text or not.
The structure of these problems is analogous to
that of anaphora resolution; namely, there are ex-
actly two cases in anaphora resolution that have
a clear correspondence with the above two cases
as illustrated in Figure 1: in (a) the noun phrase
(NP) is anaphoric; namely, the NP?s antecedent
appears in the text, and in (b) the noun phrase is
non-anaphoric. A non-anaphoric NP is either ex-
1
??
a
denotes the word sequence corresponding to the At-
tribute. Likewise, we also use ??
v
for the Value.
174
Taro-wa shisetsu-wo??-ga?shirabe-te
houkokusho-o sakusei-shita
(a) (b)
Dezain-wa     hen-desuga
watashi-wa ??-ga? suki-desu
?????
(?-ga) Ookii-kedo atsukai-yasui
( it )        large   but    easy to handle
(a) (b)
anaphora resolution
opinion extraction
anaphorantecedent
Attribute
Value
(The design is weird, but I like it.)
omitted Attribute
(It is large, but easy to handle)
Tar?-NOM  attendance-ACC                   noted
report-ACC             wrote
(Taro noted the attendance 
and wrote a report.)
design-NOM            weird
I-NOM          ( it )             like
Value
Onaka-ga hetta-node
kaerouto (?-ga) omou
hungry
go home       (I)
exophora
anaphor
(I think I?ll go home  because I?m hungry.)
Figure 1: Similarity between opinion extraction
and anaphora resolution
ophoric (i.e. the NP has an implicit referent) or in-
definite. While the figure shows Japanese exam-
ples, the similarity between anaphora resolution
and opinion extraction is language independent.
This analogy naturally leads us to think of apply-
ing existing techniques for anaphora resolution to
our opinion extraction task since anaphora reso-
lution has been studied for a considerably longer
period in a wider range of disciplines as we briefly
review below.
3.2 Existing techniques for anaphora
resolution
Corpus-based empirical approaches to anaphora
resolution have been reasonably successful. This
approach, as exemplified by (Soon et al, 2001;
Iida et al, 2003; Ng, 2004), is cost effective,
while achieving a better performance than the
best-performing rule-based systems for the test
sets of MUC-6 and MUC-7 2.
As suggested by Figure 1, anaphora resolution
can be decomposed into two subtasks: anaphoric-
ity determination and antecedent identification.
Anaphoricity determination is the task of judg-
ing whether a given NP is anaphoric or non-
anaphoric. Recent research advances have pro-
vided several important findings as follows:
? Learning-based methods for antecedent
identification can also benefit from the use of
linguistic clues inspired by Centering The-
ory (Grosz et al, 1995).
? One useful clue for anaphoricity determina-
tion is the availability of a plausible candi-
date for the antecedent. If an appropriate
candidate for the antecedent is found in the
preceding discourse context, the NP is likely
to be anaphoric.
For these reasons, an anaphora resolution model
performs best if it carries out the following pro-
2The 7th Message Understanding Conference (1998):
www.itl.nist.gov/iaui/894.02/related projects/muc/
??????????????interia ?????seki??
Dezain-wa   hen-desuga   watashi-wa suki-desu ?????
interior                  seat
design-NOM       weird    I-NOM           like
candidates
design like
interior like
seat like
design like
candidate attributes
real attribute
Select the best 
candidate attribute
Decide whether the 
candidate attribute 
stands for the real 
attribute or not
design like
design like
real attribute
pairedness
determination
attribute 
identification
opinionhood
determination
Judge whether the pair 
expresses an opinion or not
opinion
Attribute
dictionary
Value
dictionary
interior
seat
design
like
good
?.
target value
initialization
pair extraction
Figure 2: Process of opinion extraction
cess in the given order (Iida et al, 2005): (1)
Antecedent identification: Given an NP, iden-
tify the best candidate antecedent for it, and (2)
Anaphoricity determination: Judge whether the
candidate really stands for the true antecedent of
the NP.
3.3 An opinion extraction model inspired by
analogy with anaphora resolution
As illustrated in Figure 2, an opinion extraction
model derived from the aforementioned analogy
with anaphora resolution as follows:
1. Initialization: Identify attribute and value
candidates by dictionary lookup
2. Attribute identification: Select a value and
identify the best candidate attribute corre-
sponding to the value
3. Pairedness determination: Decide whether
the candidate attribute stands for the real at-
tribute of the value or not (i.e. the value
has no explicit corresponding attribute in the
text)
4. Opinionhood determination: Judge wheth-
er the obtained attribute-value pair3 ex-
presses an opinion or not
Here, the attribute identification and pairedness
determination processes respectively correspond
to the antecedent identification and anaphoricity
determination processes in anaphora resolution.
Note that our opinion extraction task requires
an additional subtask, opinionhood determination
? an attribute-value pair appearing in a text does
not necessarily constitute an opinion. We elabo-
rate on the notion of opinionhood in section 4.1.
From the above discussion, we can expect that
the findings for anaphora resolution mentioned in
3.2 stated above apply to opinion extraction as
well. In fact, the information about the candidate
3For simplicity, we call a value both with and without an
attribute uniformly by the term attribute-value pair unless
the distinction is important.
175
attribute is likely to be useful for pairedness deter-
mination. We therefore expect that carrying out
attribute identification before pairedness determi-
nation should outperform the counterpart model
which executes the two subtasks in the reversed
order. The same analogy also applies to opinion-
hood determination; namely, we expect that opin-
ion determination is bet performed after attribute
determination. Furthermore, our opinion extrac-
tion model also can be implemented in a totally
machine learning-based fashion.
4 Evaluation
We conducted experiments with Japanese Web
documents to empirically evaluate the perfor-
mance of our opinion extraction model, focus-
ing particularly on the validity of the analogy dis-
cussed in the previous section.
4.1 Opinionhood
In these experiments, we define an opinion as fol-
lows: An opinion is a description that expresses
the writer?s subjective evaluation of a particular
subject or a certain aspect of it.
By this definition, we exclude requests, factual
or counter-factual descriptions and hearsay evi-
dence from our target opinions. For example, The
engine is powerful is an opinion, while a counter-
factual sentence such as If only the engine were
more powerful is not regarded as opinion.
4.2 Opinion-tagged corpus
We created an opinion-tagged Japanese corpus
consisting of 288 review articles in the automo-
bile domain (4,442 sentences). While it is not
easy to judge whether an expression is a value or
an attribute, we asked the annotator to identify at-
tribute and value expressions according to their
subjective judgment.
If some attributes are in a hierarchical rela-
tion with each other, we asked the annotator to
choose the attribute lowest in the hierarchy as the
attribute of the value. For example, in a sound
system with poor sound, only sound is annotated
as the attribute of the value poor.
The corpus contains 2,191 values with an at-
tribute and 420 values without an attribute. Most
of the attributes appear in the same sentence as
their corresponding values or in the immediately
preceding sentence (99% of the total number of
pairs). Therefore, we extract attributes and their
corresponding values from the same sentence or
from the preceding sentence.
4.3 Experimental method
As preprocessing, we analyzed the opinion-
tagged corpus using the Japanese morphological
analyzer ChaSen4 and the Japanese dependency
structure analyzer CaboCha 5.
We used Support Vector Machines to train the
models for attribute identification, pairedness de-
termination and opinionhood determination. We
used the 2nd order polynomial kernel as the ker-
nel function for SVMs. Evaluation was per-
formed by 10-fold cross validation using all the
data.
4.3.1 Dictionaries
We use dictionaries for identification of at-
tribute and value candidates. We constructed a
attribute dictionary and a value dictionary from
review articles about automobiles (230,000 sen-
tences in total) using the semi-automatic method
proposed by Kobayashi et al (2004). The data
used in this process was different from the
opinion-tagged corpus. Furthermore, we added
to the dictionaries expressions which frequently
appearing in the opinion-tagged corpus. The final
size of the dictionaries was 3,777 attribute expres-
sions and 3,950 value expressions.
4.3.2 Order of model application
To examine the effects of appropriately choos-
ing the order of model application we mentioned
in the previous section, we conducted four ex-
periments using different orders (AI indicates at-
tribute identification, PD indicates pairedness de-
termination and OD indicates opinion determina-
tion):
Proc.1: OD?PD?AI, Proc.2: OD?AI?PD
Proc.3: AI?OD?PD, Proc.4: AI?PD?OD
Note that Proc.4 is our proposed ordering.
In addition to these models, we adopted a base-
line model. In this model, if the candidate value
and a candidate attribute are connected via a de-
pendency relation, the candidate value is judged
to have an attribute. When none of the candidate
attributes have a dependency relation, the candi-
date value is judged not to have an attribute.
We adopted the tournament model for attribute
identification (Iida et al, 2003). This model im-
plements a pairwise comparison (i.e. a match)
between two candidates in reference to the given
value treating it as a binary classification prob-
lem, and conducting a tournament which consists
of a series of matches, in which the one that pre-
vails through to the final round is declared the
4http://chasen.naist.jp/
5http://chasen.org/?taku/software/cabocha/
176
winner, namely, it is identified as the most likely
candidate attribute. Each of the matches is con-
ducted as a binary classification task in which one
or other of the candidate wins.
The pairedness determination task and the
opinionhood determination task are also binary
classification tasks. In Proc.1, since pair identifi-
cation is conducted before finding the best candi-
date attribute, we used Soon et al?s model (Soon
et al, 2001) for pairedness determination. This
model picks up each possible candidate attribute
for a value and determines if it is the attribute for
that value. If all the candidates are determined not
to be the attribute, the value is judged not to have
an attribute. In Proc.4, we can use the information
about whether the value has a corresponding at-
tribute or not for opinionhood determination. We
therefore create two separate models for when the
value does and does not have an attribute.
4.3.3 Features
We extracted the following two types of fea-
tures from the candidate attribute and the candi-
date value:
(a) surface spelling and part-of-speech of the
target value expression, as well as those of its
dependent phrase and those in its depended
phrase(s)
(b) relation between the target value and can-
didate attribute (distance between them, ex-
istence of dependency, existence of a co-
occurrence relation)
We extracted (b) if the model could use both the
attribute and the value information. Existence of a
co-occurrence relation is determined by reference
to a predefined co-occurrence list that contains
attribute-value pair information such as ?height
of vehicle ? low?. We created the list from the
230,000 sentences described in section 4.3.1 by
applying the attribute and value dictionary and
extracting attribute-value pairs if there is a de-
pendency relation between the attribute and the
value. The number of pairs we extracted was
about 48,000.
4.4 Results
Table 1 shows the results of opinion extraction.
We evaluated the results by recall R and preci-
sion P defined as follows (For simplicity, we sub-
stitute ?A-V? for attribute-value pair):
R =
correctly extracted A-V opinions
total number of A-V opinions
,
P =
correctly extracted A-V opinions
total number of A-V opinions found by the system
.
In order to demonstrate the effectiveness of
the information about the candidate attribute, we
evaluated the results of pair extraction and opin-
ionhood determination separately. Table 2 shows
the results. In the pair extraction, we assume that
the value is given, and evaluate how successfully
attribute-value pairs are extracted.
4.5 Discussions
As Table 1 shows, our proposed ordering is out-
performed on the recall in Proc.3, however, the
precision is higher than Proc.3 and get the best F-
measure. In what follows, we discuss the results
of pair extraction and opinionhood determination.
Pair extraction From Table 2, we can see that
carrying out attribute identification before paired-
ness determination outperforms the reverse order-
ing by 11% better precision and 3% better recall.
This result supports our expectation that knowl-
edge of attribute information assists attribute-
value pair extraction. Focusing on the rows la-
beled ?(dependency)? and ?(no dependency)? in
Table 2, while 80% of the attribute-value pairs in
a direct dependency relation are successfully ex-
tracted with high precision, the model achieves
only 51.7% recall with 61.7% precision for the
cases where an attribute and value are not in a di-
rect dependency relation.
According to our error analysis, a major source
of errors lies in the attribute identification task. In
this experiment, the precision of attribute identifi-
cation is 78%. A major reason for this problem
was that the true attributes did not exist in our
dictionary. In addition, a major cause of error in
the pair determination stage is cases where an at-
tribute appearing in the preceding sentence causes
a false decision. We need to conduct further in-
vestigations in order to resolve these problems.
Opinionhood determination Table 2 also
shows that carrying out attribute identification
followed by opinionhood determination out-
performs the reverse ordering, which supports
our expectation that knowing the attribute
information aids opinionhood determination.
While it produces better results, our proposed
method still has room for improvement in both
precision and recall. Our current error analysis
has not identified particular error patterns ? the
types of errors are very diverse. However, we
need to at least address the issue of modifying
the feature set to make the model more sensitive
to modality-oriented distinctions such as subjunc-
tive and conditional expressions.
177
Table 1: The precision and the recall for opinion extraction
procedure value with attribute value without attribute attribute-value pairs
baseline precision 60.5% (1130/1869) 10.6% (249/2340) 32.8% (1379/4209)
recall 51.6% (1130/2191) 59.3% (249/420) 52.8% (1379/2611)
F-measure 55.7 21.0 40.5
Proc.1 precision 47.3% (864/1828) 21.6% ( 86/399) 42.7% ( 950/2227)
recall 39.4% (864/2191) 20.5% ( 86/420) 36.4% ( 950/2611)
F-measure 43.0 21.0 39.3
Proc.2 precision 63.0% (1074/1706) 38.0% (198/521) 57.1% (1272/2227)
recall 49.0% (1074/2191) 47.1% (198/420) 48.7% (1272/2611)
F-measure 55.1 42.0 52.6
Proc.3 precision 74.9% (1277/1632) 29.1% (151/519) 63.8% (1373/2151)
recall 55.8% (1222/2191) 36.0% (151/420) 52.6% (1373/2611)
F-measure 64.0 32.2 57.7
Proc.4 precision 80.5% (1175/1460) 30.2% (150/497) 67.7% (1325/1957)
recall 53.6% (1175/2191) 35.7% (150/420) 50.7% (1325/2611)
F-measure 64.4 32.7 58.0
Table 2: The result of pair extraction and opinionhood determination
procedure precision recall
pair extraction
baseline (dependency) 71.1% (1385/1929) 63.2% (1385/2191)
PD?AI 65.3% (1579/2419) 72.1% (1579/2191)
AI?PD 76.6% (1645/2148) 75.1% (1645/2191)
(dependency) 87.7% (1303/1486) 79.6% (1303/1637)
(no dependency) 51.7% ( 342/ 662) 61.7% ( 342/ 554)
opinionhood determination OD 74.0% (1554/2101) 60.2% (1554/2581)AI?OD 82.2% (1709/2078) 66.2% (1709/2581)
5 Conclusion
In this paper, we have proposed a machine
learning-based method for the extraction of opin-
ions on consumer products by reducing the prob-
lem to that of extracting attribute-value pairs from
texts. We have pointed out the similarity between
the tasks of anaphora resolution and opinion ex-
traction, and have applied the machine learning-
based method designed for anaphora resolution to
opinion extraction. The experimental results re-
ported in this paper show that identifying the cor-
responding attribute for a given value expression
is effective in both pairedness determination and
opinionhood determination.
References
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Min-
ing the peanut gallery: opinion extraction and semantic
classification of product reviews. In Proc. of the 12th In-
ternational World Wide Web Conference, pages 519?528.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Center-
ing: A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2):203?226.
M. Hu and B. Liu. 2004. Mining and summarizing customer
reviews. In Proc. of the Tenth International Conference
on Knowledge Discovery and Data Mining, pages 168?
177.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. In-
corporating contextual cues in trainable models for coref-
erence resolution. In Proc. of the EACL Workshop on the
Computational Treatment of Anaphora, pages 23?30.
R. Iida, K. Inui, Y. Matsumoto, and S. Sekine. 2005. Noun
phrase coreference resolution in Japanese base on most
likely antecedant candidates. Journal of Information Pro-
cessing Society of Japan, 46(3). (in Japanese).
H. Kanayama and T. Nasukawa. 2004. Deeper sentiment
analysis using machine translation technology. In Pro-
ceedings of the 20th International Conference on Com-
putational Linguistics, pages 494?500.
N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expressions
for opinion extraction. In Proc. of the 1st International
Joint Conference on Natural Language Processing, pages
584?589.
S. Murano and S. Sato. 2003. Automatic extraction of sub-
jective sentences using syntactic patterns. In Proc. of the
Ninth Annual Meeting of the Association for Natural Lan-
guage Processing, pages 67?70. (in Japanese).
V. Ng. 2004. Learning noun phrase anaphoricity to improve
coreference resolution: Issues in representation and opti-
mization. In Proc. of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 152?159.
B. Pang and L. Lee. 2004. A sentiment education: Sen-
timent analysis using subjectivity summarization based
on minimum cuts. In Proc. of the 42nd Annual Meeting
of the Association for Computational Linguistics, pages
271?278.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
K. Tateishi, Y. Ishiguro, and T. Fukushima. 2001. Opinion
information retrieval from the Internet. In IPSJ SIGNL
Note 144-11, pages 75?82. (in Japanese).
K. Tateishi, T. Fukushima, N. Kobayashi, T. Takahashi,
A. Fujita, K. Inui, and Y. Matsumoto. 2004. Web opin-
ion extraction and summarization based on viewpoints
of products. In IPSJ SIGNL Note 163, pages 1?8. (in
Japanese).
P. D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of re-
views. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 417?424.
178
Gloss-Based Semantic Similarity Metrics for Predominant Sense Acquisition
Ryu Iida
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
ryu-i@is.naist.jp
Diana McCarthy and Rob Koeling
University of Sussex
Falmer, East Sussex
BN1 9QH, UK
{dianam,robk}@sussex.ac.uk
Abstract
In recent years there have been various ap-
proaches aimed at automatic acquisition of
predominant senses of words. This infor-
mation can be exploited as a powerful back-
off strategy for word sense disambiguation
given the zipfian distribution of word senses.
Approaches which do not require manually
sense-tagged data have been proposed for
English exploiting lexical resources avail-
able, notably WordNet. In these approaches
distributional similarity is coupled with a se-
mantic similarity measure which ties the dis-
tributionally related words to the sense in-
ventory. The semantic similarity measures
that have been used have all taken advantage
of the hierarchical information in WordNet.
We investigate the applicability to Japanese
and demonstrate the feasibility of a mea-
sure which uses only information in the dic-
tionary definitions, in contrast with previ-
ous work on English which uses hierarchi-
cal information in addition to dictionary def-
initions. We extend the definition based
semantic similarity measure with distribu-
tional similarity applied to the words in dif-
ferent definitions. This increases the recall
of our method and in some cases, precision
as well.
1 Introduction
Word sense disambiguation (WSD) has been an ac-
tive area of research over the last decade because
many researches believe it will be important for
applications which require, or would benefit from,
some degree of semantic interpretation. There has
been considerable skepticism over whether WSD
will actually improve performance of applications,
but we are now starting to see improvement in per-
formance due to WSD in cross-lingual information
retrieval (Clough and Stevenson, 2004; Vossen et
al., 2006) and machine translation (Carpuat and Wu,
2007; Chan et al, 2007) and we hope that other ap-
plications such as question-answering, text simplifi-
cation and summarisation might also benefit as WSD
methods improve.
In addition to contextual evidence, most WSD sys-
tems exploit information on the most likely mean-
ing of a word regardless of context. This is a pow-
erful back-off strategy given the skewed nature of
word sense distributions. For example, in the En-
glish coarse grained all words task (Navigli et al,
2007) at the recent SemEval Workshop the base-
line of choosing the most frequent sense using the
first WordNet sense attained precision and recall of
78.9%which is only a few percent lower than the top
scoring system which obtained 82.5%. This finding
is in line with previous results (Snyder and Palmer,
2004). Systems using a first sense heuristic have
relied on sense-tagged data or lexicographer judg-
ment as to which is the predominant sense of a word.
However sense-tagged data is expensive and further-
more the predominant sense of a word will vary de-
pending on the domain (Koeling et al, 2005; Chan
and Ng, 2007).
One direction of research following McCarthy et
al. (2004) has been to learn the most predominant
561
sense of a word automatically. McCarthy et als
method relies on two methods of similarity. Firstly,
distributional similarity is used to estimate the pre-
dominance of a sense from the number of distribu-
tionally similar words and the strength of their dis-
tributional similarity to the target word. This is done
on the premise that more prevalent meanings have
more evidence in the corpus data used for the distri-
butional similarity calculations and the distribution-
ally similar words (nearest neighbours) to a target
reflect the more predominant meanings as a conse-
quence. Secondly, the senses in the sense inventory
are linked to the nearest neighbours using semantic
similarity which incorporates information from the
sense inventory. It is this semantic similarity mea-
sure which is the focus of our paper in the context of
the method for acquiring predominant senses.
Whilst the McCarthy et al?s method works well
for English, other inventories do not always have
WordNet style resources to tie the nearest neigh-
bours to the sense inventory. WordNet has many se-
mantic relations as well as glosses associated with
its synsets (near synonym sets). While traditional
dictionaries do not organise senses into synsets, they
do typically have sense definitions associated with
the senses. McCarthy et al (2004) suggest that dic-
tionary definitions can be used with their method,
however in the implementation of the measure based
on dictionary definitions that they use, the dictionary
definitions are extended to those of related words us-
ing the hierarchical structure of WordNet (Banerjee
and Pedersen, 2002). This extension to the original
method (Lesk, 1986) was proposed because there is
not always sufficient overlap of the individual words
for which semantic similarity is being computed. In
this paper we refer to the original method (Lesk,
1986) as lesk and the extended measure proposed
by Banerjee and Pedersen as Elesk.
This paper investigates the potential of using
the overlap of dictionary definitions with the Mc-
Carthy et al?s method. We test the method for
obtaining a first sense heuristic using two publicly
available datasets of sense-tagged data in Japanese,
EDR (NICT, 2002) and the SENSEVAL-2 Japanese
dictionary task (Shirai, 2001). We contrast an imple-
mentation of lesk (Lesk, 1986) which uses only dic-
tionary definitions with the Jiang-Conrath measure
(jcn) (Jiang and Conrath, 1997) which uses man-
ually produced hyponym links and was used pre-
viously for this purpose on English datasets (Mc-
Carthy et al, 2004). The jcn measure is only ap-
plicable to the EDR dataset because the dictionary
has hyponymy links which are not available in the
SENSEVAL-2 Japanese dictionary task. We also pro-
pose a new extension to lesk which does not require
hand-crafted hyponym links but instead uses distri-
butional similarity to increase the possibilities for
overlap of the word definitions. We refer to this new
measure as DSlesk. We compare this to the original
lesk on both datasets and show that it increases re-
call, and sometimes precision too whilst not requir-
ing hyponym links.
In the next section we place our contribution in re-
lation to previous work. In section 3 we summarise
the methods we adopt from previous work, and de-
scribe our proposal for a semantic similarity method
that can supplement the information from dictionary
definitions with information from raw text. In sec-
tion 4 we describe the experiments on EDR and the
SENSEVAL-2 Japanese dictionary task and we con-
clude in section 5.
2 Related Work
This work builds upon that of McCarthy et al (2004)
which acquires predominant senses for target words
from a large sample of text using distributional sim-
ilarity (Lin, 1998) to provide evidence for predomi-
nance. The evidence from the distributional similar-
ity is allocated to the senses using semantic similar-
ity fromWordNet (Patwardhan and Pedersen, 2003).
We will describe the method more fully below in
section 3. McCarthy et al (2004) reported results
for English using their automatically acquired first
sense heuristic on SemCor (Miller et al, 1993) and
the SENSEVAL-2 English all words dataset (Sny-
der and Palmer, 2004). The results from this are
promising, given that hand-labelled data is not re-
quired. On polysemous nouns from SemCor they
obtained 48% WSD using their method with Elesk
and 46% with jcn where the random baseline was
24% and the upper-bound was 67% (derived from
the SemCor test data itself). On SENSEVAL-2 all
words dataset using the jcn measure 1 they obtained
63% recall which is encouraging compared to the
1They did not apply lesk to this dataset.
562
SemCor heuristic which obtained 68% but requires
hand-labelled data. The upper-bound on the dataset
was 72% from the test data itself. These results cru-
cially depend on the information in the sense inven-
tory WordNet. WordNet contains hierarchical rela-
tions between word senses which are used in both
jcn and Elesk. There is an issue that such infor-
mation may not be available in other sense invento-
ries, and other inventories will be needed for other
languages. In this paper, we implement the lesk se-
mantic similarity (Lesk, 1986) for the two Japanese
lexicons used in our test datasets, i) the EDR dic-
tionary (NICT, 2002) ii) the Iwanami Kokugo Jiten
Dictionary (Nishio et al, 1994). We investigate the
potential of lesk and jcn, where the latter is applica-
ble. In addition to implementing the original lesk
measure, we propose an extension to the method
inspired by Mihalcea et al (2006). Mihalcea et
al. (2006) used various text based similarity mea-
sures, including WordNet and corpus based similar-
ity methods, to determine if two phrases are para-
phrases. They contrasted this approach with previ-
ous methods which used overlap of the words be-
tween the candidate paraphrases. For each word in
each of the two texts they obtain the maximum sim-
ilarity between the word and any of the words from
the putative paraphrase. The similarity scores for
each word of both phrases contribute to an overall
semantic similarity between 0 and 1 and a threshold
of 0.5 is used to decide if the candidate phrases are
paraphrases. In our work, we compare glosses of
words senses (senses of the target word and senses
of the nearest neighbour) rather than paraphrases. In
this approach we extend the definition overlap by
considering the distributional similarity (Lin, 1998)
rather than identify of the words in the two defini-
tions.
In addition to McCarthy et al (2004) there are
other approaches to finding predominant senses.
Chan and Ng (2005) use parallel data to provide
estimates for sense frequency distributions to feed
into a supervised WSD system. Mohammad and
Hirst (2006) propose an approach to acquiring pre-
dominant senses from corpora which makes use
of the category information in the Macquarie The-
saurus (Barnard, 1986). Lexical chains (Galley and
McKeown, 2003) may also provide a useful first
sense heuristic (Brody et al, 2006) but are produced
usingWordNet relations. We use theMcCarthy et al
approach because this is applicable without aligned
corpus data, semantic category and relation informa-
tion and is applicable to any language assuming the
minimum requirements of i) dictionary definitions
associated with the sense inventory and ii) raw cor-
pus data. We adapt their technique to remove the
reliance on hyponym links.
3 Gloss-based semantic similarity
We first summarise the McCarthy et al method
and the WordNet based semantic similarity func-
tions (jcn and Elesk) that they use for automatic
acquisition of a first sense heuristic applied to dis-
ambiguation of English WordNet datasets. We then
describe the additional semantic similarity method
that we propose for comparison with lesk and jcn.
McCarthy et al use a distributional similarity the-
saurus acquired from corpus data using the method
of Lin (1998) for finding the predominant sense of
a word where the senses are defined by WordNet.
The thesaurus provides the k nearest neighbours to
each target word, along with the distributional sim-
ilarity score between the target word and its neigh-
bour. The WordNet similarity package (Patwardhan
and Pedersen, 2003) is used to weight the contribu-
tion that each neighbour makes to the various senses
of the target word.
Let w be a target word and Nw = {n1,n2...nk}
be the ordered set of the top scoring k
neighbours of w from the thesaurus with
associated distributional similarity scores
{dss(w,n1),dss(w,n2), ...dss(w,nk)} using (Lin,
1998). Let senses(w) be the set of senses of w
for each sense of w (wsi ? senses(w)) a ranking is
obtained using:
Prevalence Score(wsi) =
?
n j?Nw
dss(w,n j)?
wnss(wsi,n j)
?wsi??senses(w)wnss(wsi? ,n j)
(1)
where wnss is the maximum WordNet similarity
score between wsi and the WordNet sense of the
neighbour (n j) that maximises this score. McCarthy
et al compare two different WordNet similarity
scores, jcn and Elesk.
jcn (Jiang and Conrath, 1997) uses corpus data
to estimate a frequency distribution over the classes
563
(synsets) in the WordNet hierarchy. Each synset, is
incremented with the frequency counts from the cor-
pus of all words belonging to that synset, directly or
via the hyponymy relation. The frequency data is
used to calculate the ?information content? (IC) of a
class or sense (s):
IC(s) =?log(p(s))
Jiang and Conrath specify a distance measure be-
tween two senses (s1,s2):
D jcn(s1,s2) = IC(s1)+ IC(s2)?2? IC(s3)
where the third class (s3) is the most informative, or
most specific, superordinate synset of the two senses
s1 and s2. This is transformed from a distance mea-
sure in the WordNet Similarity package by taking
the reciprocal:
jcn(s1,s2) = 1/D jcn(s1,s2)
McCarthy et al use the above measure with wsi
as s1 and whichever sense of the neigbour (n j) that
maximises this WordNet similarity score.
Elesk (Banerjee and Pedersen, 2002) extends the
original lesk algorithm (Lesk, 1986) so we describe
that original algorithm lesk first. This simply cal-
culates the overlap of the content words in the defi-
nitions, frequently referred to as glosses, of the two
word senses.
lesk(s1,s2) =
?
a?g1
member(a,g2)
member(a,g2) =
{
1 if a appears in g2
0 otherwise
where g1 is the gloss of word sense s1, g2 is the gloss
of s2 and a is one of words appearing in g1. In Elesk
which McCarthy et al use the measure is extended
by considering related synsets to s1 and s2, again
where s1 is wsi and s2 is the sense from all senses
of n j that maximises the Elesk WordNet similar-
ity score. Elesk relies heavily on the relationships
that are encoded in WordNet such as hyponymy and
meronymy. Not all languages have resources sup-
plied with these relations, and where they are sup-
plied there may not be as much detail as there is in
WordNet.
In this paper we will examine the use of jcn and
the original lesk in Japanese on the EDR dataset
to see how well the pure definition based measure
fares compared to one using hyponym links. EDR
has hyponym links so we can make this comparison.
The performance of jcn will depend on the coverage
of the hyponym links. For lesk meanwhile there is
an issue that using only overlap of sense definitions
may give poor results because the sense definitions
are usually succinct and the overlap of words may
be low. For example, given the glosses for the words
pigeon and bird:2
pigeon: a fat grey and white bird with
short legs.
bird: a creature that is covered with feath-
ers and has wings and two legs.
If only content words are considered then there
is only one word (leg) which overlaps in the two
glosses, so the resultant lesk score is low (1) even
though the word pigeon is intuitively similar to bird.
The Elesk extension addressed this issue using
WordNet relations to extend the definitions over
which the overlap is calculated for a given pair of
senses. We propose addressing the same issue us-
ing corpus data to supplement the lesk overlap mea-
sure. We propose using distributional similarity (us-
ing (Lin, 1998)) as an approximation of semantic
distance between the words in the two glosses, rather
than requiring an exact match. We refer to this mea-
sure as DSlesk as defined:
DSlesk(s1,s2) = 1
|a ? g1| ?a?g1
max
b?g2
dss(a,b) (2)
where g1 is the gloss of word sense s1, g2 is the gloss
of s2, again s1 is the target word sense wsi in equa-
tion 1 for which we are obtaining the predominance
ranking score and s2 is whichever sense of the neigh-
bour (n j) in equation 1 which maximises this seman-
tic similarity score, as McCarthy et al did with the
wnss in equation 1. a (b) is a word appearing in g1
(g2).
In the calculation of equation (2), we first extract
the most similar word b from g2 to each word (a) in
2These two glosses are defined in OXFORD Advanced
Learner?s Dictionary.
564
dss(bird,creature) = 0.84, dss(bird, f eather) = 0.77,
dss(bird,wing) = 0.55, dss(bird, leg) = 0.43,
dss(leg,creature) = 0.56, dss(leg, f eather) = 0.66,
dss(leg,wing) = 0.74, dss(leg, leg) = 1.00
Figure 1: Examples of distributional similarity
the gloss of s1. We then output the average of the
maximum distributional similarity of all the words
in g1 to any of the words in g2 as the similarity score
between s1 and s2. We acknowledge that DSlesk is
not symmetrical since it depends on the number of
words in the gloss of s1, but not s2. Also our sum-
mation is over these words in s1 and we are not look-
ing for identity but maximum distributional similar-
ity with any of the words in g2 so the summation
will not give the same result as if we did the sum-
mation over the words in g2. It is perfectly reason-
able to have a semantic similarity measure which is
not symmetrical. One may want a measure where
a more specific sense, such as the meat sense of
chicken is closer to the ?animal flesh used as food?
sense of meat than vice versa. We do not believe
that this asymmetry is problematic for our applica-
tion as all the senses of w which we are ranking are
all treated equally with respect to the neighbour n,
and the ranking measure is concerned with finding
evidence for the meaning of w, which we do by fo-
cusing on its definitions, and not the meaning of n.
It would however be worthwhile investigating sym-
metrical versions of the score in the future.
Here is an example given the definitions of bird
and pigeon above and the distributional similarity
scores of all combinations of the two nouns as shown
in Figure 1. In this case, the similarity is estimated
as 1/2(0.84+1.00) = 0.92.
4 Experiments
To investigate how well the McCarthy et al method
ports to other language, we conduct empirical eval-
uation of word sense disambiguation by using the
two available sense-tagged datasets, EDR and the
SENSEVAL-2 Japanese dictionary task. In the ex-
periments, we compare the three semantic similari-
ties, jcn, lesk and DSlesk3, for use in the method to
3Elesk can be used when several semantic relations such as
hypnoymy and meronomy are available. However, we cannot
directly apply Elesk as it was used in (McCarthy et al, 2004) to
find the most likely sense in the set of word senses
defined in each inventory following the approach
of McCarthy et al (2004). For the thesaurus con-
struction we used <verb, case, noun> triplets ex-
tracted from Japanese newspaper articles (9 years of
the Mainichi Shinbun (1991-1999) and 10 years of
the Nihon Keizai Shinbun (1991-2000)) and parsed
by CaboCha (Kudo and Matsumoto, 2002). This re-
sulted in 53 million triplet instances for acquiring
the distributional thesaurus. We adopt the similarity
score proposed by Lin (1998) as the distributional
similarity score and use 50 nearest neighbours in
line with McCarthy et al
For the random baseline we select one word sense
at random for each word token and average the pre-
cision over 100 trials. For contrast with a supervised
approach we show the performance if we use hand-
labelled training data for obtaining the predominant
sense of the test words. This method usually outper-
forms an automatic approach, but crucially relies on
there being hand-labelled data which is expensive to
produce. The method cannot be applied where there
is no hand-labelled training data, it will be unreli-
able for low frequency data and a general dataset
may not be applicable when one moves to domain
specific text (Koeling et al, 2005). Since we are
not using context for disambiguation, but just a first
sense heuristic, we also give the upper-bound which
is the first sense heuristic calculated from the test
data itself.
4.1 EDR
We conduct empirical evaluation using 3,836 poly-
semous nouns in the sense-tagged corpus provided
with EDR (183,502 instances) where the glosses are
defined in the EDR dictionary. We evaluated on this
dataset using WSD precision and recall of this corpus
using only our first-sense heuristic (no context). The
results are shown in Table 1. The WSD performance
of all the automatic methods is much lower than the
supervised method, however, the main point of this
paper is to compare the McCarthy et al method for
finding a first sense in Japanese using jcn, lesk and
our experiments because the meronomy relation is not defined
in the EDR dictionary. In the experiments reported here we fo-
cus on the comparison of the three similarity measures jcn, lesk
and DSlesk for use in the method to determine the predomi-
nant sense of each word. We leave further exploration of other
adaptations of semantic similarity scores for future work.
565
Table 1: Results of EDR
recall precision
baseline 0.402 0.402
jcn 0.495 0.495
lesk 0.474 0.488
DSlesk 0.495 0.495
upper-bound 0.745 0.745
supervised 0.731 0.731
Table 2: Precision on EDR at low frequencies
all freq ? 10 freq ? 5
baseline 0.402 0.405 0.402
jcn 0.495 0.445 0.431
lesk 0.474 0.448 0.426
DSlesk 0.495 0.453 0.433
upper-bound 0.745 0.674 0.639
supervised 0.731 0.519 0.367
DSlesk. Table 1 shows that DSlesk is comparable to
jcn without the requirement for semantic relations
such as hyponymy.
Furthermore, we evaluate precision of each
method at low frequencies of words (? 10, ? 5),
shown in Table 2. Table 2 shows that all methods for
finding a predominant sense outperform the super-
vised one for items with little data (? 5), indicating
that these methods robustly work even for low fre-
quency data where hand-tagged data is unreliable.
Whilst the results are significantly different to the
baseline 4 we note that the difference to the random
baseline is less than for McCarthy et al who ob-
tained 48% for Elesk on polysemous nouns in Sem-
Cor and 46% for jcn against a random baseline of
24%. These differences are probably explained by
differences in the lexical resources. Both Elesk and
jcn rely on semantic relations including hyponymy
with Elesk also using the glosses. jcn in both ap-
proaches use the hyponym links. WordNet 1.6 (used
by McCarthy et al) has 66025 synsets with 66910
hyponym links between these 5. For EDR there are
166868 nodes (word sense groupings) and 53747
4For significance testing we used McNemar?s test ? = 0.05.
5These figures are taken from
http://www.lsi.upc.es/?batalla/wnstats.html#wn16
Table 3: Results of SENSEVAL-2
precision = recall
fine coarse
baseline 0.282 0.399
lesk 0.344 0.501
DSlesk 0.386 0.593
upper-bound 0.747 0.834
supervised 0.742 0.842
hyponym links. So in EDR the ratio of these links
to the nodes is much lower. This and other differ-
ences between EDR and WordNet are likely to be
the reason for the difference in results.
4.2 SENSEVAL-2
We also evaluate the performance using the Japanese
dictionary task in SENSEVAL-2 (Shirai, 2001). In
this experiment, we use 50 nouns (5,000 instances).
For this task, since semantic relations such as hy-
ponym links are not defined, use of jcn is not pos-
sible. Therefore, we just compare lesk and DSlesk
along with our random baseline, the supervised ap-
proach and the upper-bound as before.
The results are evaluated in two ways; one is for
fine-grained senses in the original task definition and
the other is coarse-grained version which is evalu-
ated discarding the finer categorical information of
each definition. The results are shown in Table 3. As
with the EDR results, all unsupervised methods sig-
nificantly outperform the baseline method, though
the supervised methods still outperform the unsu-
pervised ones. In this experiment, DSlesk is also
significantly better than lesk in both fine and coarse-
grained evaluations. It indicates that applying dis-
tributional similarity score to calculating inter-gloss
similarities improves performance.
5 Conclusion
In this paper, we examined different measures of se-
mantic similarity for finding a first sense heuristic
for WSD automatically in Japanese. We defined a
new gloss-based similarity (DSlesk) and evaluated
the performance on two Japanese WSD datasets, out-
performing lesk and achieving a performance com-
parable to the jcn method which relies on hyponym
links which are not always available.
566
There are several issues for future directions of
automatic detection of a first sense heuristic. In this
paper, we proposed an adaptation of the lesk mea-
sure of gloss-based similarity, by using the aver-
age similarity between nouns in the two glosses un-
der comparison in a bag-of-words approach without
recourse to other information. However, it would
be worthwhile exploring other information in the
glosses, such as words of other PoS and predicate
argument relations. We also hope to investigate ap-
plying alignment techniques introduced for entail-
ment recognition (Hickl and Bensley, 2007).
Another important issue in WSD is to group fine-
grained word senses into clusters, making the task
suitable for NLP applications (Ide and Wilks, 2006).
We believe that our gloss-based similarity DSlesk
might be very suitable for this task and we plan to
investigate the possibility.
There are other approaches we would like to ex-
plore in future. Mihalcea (2005) uses dictionary def-
initions alongside graphical algorithms for unsuper-
vised WSD. Whilst the results are not directly com-
parable to ours because we have not included con-
textual evidence in our models, it would be worth-
while exploring if unsupervised graphical models
using only the definitions we have in our lexical re-
sources can perform WSD on a document and give
more reliable first sense heuristics.
Acknowledgements
This work was supported by the UK EPSRC project
EP/C537262 ?Ranking Word Senses for Disam-
biguation: Models and Applications?, and a UK
Royal Society Dorothy Hodgkin Fellowship to the
second author. We would like to thank John Carroll
for several useful discussions on this work.
References
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
Lesk algorithm for word sense disambiguation using
WordNet. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-02), Mexico City.
J.R.L. Barnard, editor. 1986. Macquaire Thesaurus.
Macquaire Library, Sydney.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised wsd. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 61?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2005. Word sense
disambiguation with distribution estimation. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005), Edinburgh, Scot-
land.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense disam-
biguation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, Prague, Czech Republic, June. Association for
Computational Linguistics.
Paul Clough and Mark Stevenson. 2004. Evaluating the
contribution of EuroWordNet and word sense disam-
biguation to cross-language retrieval. In Second In-
ternational Global WordNet Conference (GWC-2004),
pages 97?105.
Michel Galley and Kathleen McKeown. 2003. Improv-
ing word sense disambiguation in lexical chaining. In
IJCAI-03, Proceedings of the Eighteenth International
Joint Conference on Artificial Intelligence, Acapulco,
Mexico, August 9-15, 2003, pages 1486?1488. Morgan
Kaufmann.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing textual
entailment. In Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, pages
171?176.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Jay Jiang and David Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In-
ternational Conference on Research in Computational
Linguistics, Taiwan.
567
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the joint confer-
ence on Human Language Technology and Empirical
methods in Natural Language Processing, pages 419?
426, Vancouver, B.C., Canada.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Natural Language
Learning 2002 (CoNLL), pages 63?69.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from and ice cream cone. In Proceedings of the ACM
SIGDOC Conference, pages 24?26, Toronto, Canada.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL 98,
Montreal, Canada.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence (AAAI
2006), Boston, MA, July.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the joint conference on Human Language Technology
and Empirical methods in Natural Language Process-
ing, Vancouver, B.C., Canada.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?.308. Morgan Kaufman.
Saif Mohammad and Graeme Hirst. 2006. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), pages 121?128, Trento, Italy, April.
Roberto Navigli, C. Litkowski, Kenneth, and Orin Har-
graves. 2007. SemEval-2007 task 7: Coarse-
grained English all-words task. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 30?35, Prague,
Czech Republic.
NICT. 2002. EDR electronic dic-
tionary version 2.0, technical guide.
http://www2.nict.go.jp/kk/e416/EDR/.
Minoru Nishio, Etsutaro Iwabuchi, and ShizuoMitzutani.
1994. Iwanami kokugo jiten dai go han.
Siddharth Patwardhan and Ted Pedersen. 2003.
The CPAN WordNet::Similarity Package.
http://search.cpan.org/author/SID/WordNet-
Similarity-0.03/.
Kiyoaki Shirai. 2001. SENSEVAL-2 Japanese Dictio-
nary Task. In Proceedings of the SENSEVAL-2 work-
shop, pages 33?36.
Benjamin Snyder and Martha Palmer. 2004. The English
all-words task. In Proceedings of the ACL SENSEVAL-
3 workshop, pages 41?43, Barcelona, Spain.
Piek Vossen, German Rigau, Inaki Alegria, Eneko Agirre,
David Farwell, and Manuel Fuentes. 2006. Mean-
ingful results for information retrieval in the meaning
project. In Proceedings of the 3rd Global WordNet
Conference. http://nlpweb.kaist.ac.kr/gwc/.
568
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 625?632,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploiting Syntactic Patterns as Clues in Zero-Anaphora Resolution
Ryu Iida, Kentaro Inui and Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{ryu-i,inui,matsu}@is.naist.jp
Abstract
We approach the zero-anaphora resolu-
tion problem by decomposing it into
intra-sentential and inter-sentential zero-
anaphora resolution. For the former prob-
lem, syntactic patterns of the appearance
of zero-pronouns and their antecedents are
useful clues. Taking Japanese as a target
language, we empirically demonstrate that
incorporating rich syntactic pattern fea-
tures in a state-of-the-art learning-based
anaphora resolution model dramatically
improves the accuracy of intra-sentential
zero-anaphora, which consequently im-
proves the overall performance of zero-
anaphora resolution.
1 Introduction
Zero-anaphora is a gap in a sentence that has an
anaphoric function similar to a pro-form (e.g. pro-
noun) and is often described as ?referring back?
to an expression that supplies the information nec-
essary for interpreting the sentence. For example,
in the sentence ?There are two roads to eternity,
a straight and narrow, and a broad and crooked,?
the gaps in ?a straight and narrow (gap)? and ?a
broad and crooked (gap)? have a zero-anaphoric
relationship to ?two roads to eternity.?
The task of identifying zero-anaphoric relations
in a given discourse, zero-anaphora resolution,
is essential in a wide range of NLP applications.
This is the case particularly in such a language as
Japanese, where even obligatory arguments of a
predicate are often omitted when they are inferable
from the context. In fact, in our Japanese newspa-
per corpus, for example, 45.5% of the nominative
arguments of verbs are omitted. Since such gaps
can not be interpreted only by shallow syntac-
tic parsing, a model specialized for zero-anaphora
resolution needs to be devised on the top of shal-
low syntactic and semantic processing.
Recent work on zero-anaphora resolution can
be located in two different research contexts. First,
zero-anaphora resolution is studied in the con-
text of anaphora resolution (AR), in which zero-
anaphora is regarded as a subclass of anaphora. In
AR, the research trend has been shifting from rule-
based approaches (Baldwin, 1995; Lappin and Le-
ass, 1994; Mitkov, 1997, etc.) to empirical, or
corpus-based, approaches (McCarthy and Lehnert,
1995; Ng and Cardie, 2002a; Soon et al, 2001;
Strube and Mu?ller, 2003; Yang et al, 2003) be-
cause the latter are shown to be a cost-efficient
solution achieving a performance that is compa-
rable to best performing rule-based systems (see
the Coreference task in MUC1 and the Entity De-
tection and Tracking task in the ACE program2).
The same trend is observed also in Japanese zero-
anaphora resolution, where the findings made in
rule-based or theory-oriented work (Kameyama,
1986; Nakaiwa and Shirai, 1996; Okumura and
Tamura, 1996, etc.) have been successfully
incorporated in machine learning-based frame-
works (Seki et al, 2002; Iida et al, 2003).
Second, the task of zero-anaphora resolution
has some overlap with Propbank3-style semantic
role labeling (SRL), which has been intensively
studied, for example, in the context of the CoNLL
SRL task4. In this task, given a sentence ?To at-
tract younger listeners, Radio Free Europe inter-
sperses the latest in Western rock groups?, an SRL
1http://www-nlpir.nist.gov/related projects/muc/
2http://projects.ldc.upenn.edu/ace/
3http://www.cis.upenn.edu/?mpalmer/project pages/ACE.htm
4http://www.lsi.upc.edu/?srlconll/
625
model is asked to identify the NP Radio Free Eu-
rope as the A0 (Agent) argument of the verb at-
tract. This can be seen as the task of finding
the zero-anaphoric relationship between a nomi-
nal gap (the A0 argument of attract) and its an-
tecedent (Radio Free Europe) under the condition
that the gap and its antecedent appear in the same
sentence.
In spite of this overlap between AR and SRL,
there are some important findings that are yet to
be exchanged between them, partly because the
two fields have been evolving somewhat indepen-
dently. The AR community has recently made two
important findings:
? A model that identifies the antecedent of an
anaphor by a series of comparisons between
candidate antecedents has a remarkable ad-
vantage over a model that estimates the ab-
solute likelihood of each candidate indepen-
dently of other candidates (Iida et al, 2003;
Yang et al, 2003).
? An AR model that carries out antecedent
identification before anaphoricity determina-
tion, the decision whether a given NP is
anaphoric or not (i.e. discourse-new), sig-
nificantly outperforms a model that executes
those subtasks in the reverse order or simulta-
neously (Poesio et al, 2004; Iida et al, 2005).
To our best knowledge, however, existing SRL
models do not exploit these advantages. In SRL,
on the other hand, it is common to use syntactic
features derived from the parse tree of a given in-
put sentence for argument identification. A typ-
ical syntactic feature is the path on a parse tree
from a target predicate to a noun phrase in ques-
tion (Gildea and Jurafsky, 2002; Carreras and Mar-
quez, 2005). However, existing AR models deal
with intra- and inter-sentential anaphoric relations
in a uniform manner; that is, they do not use as rich
syntactic features as state-of-the-art SRL models
do, even in finding intra-sentential anaphoric rela-
tions. We believe that the AR and SRL communi-
ties can learn more from each other.
Given this background, in this paper, we show
that combining the aforementioned techniques de-
rived from each research trend makes signifi-
cant impact on zero-anaphora resolution, taking
Japanese as a target language. More specifically,
we demonstrate the following:
? Incorporating rich syntactic features in a
state-of-the-art AR model dramatically im-
proves the accuracy of intra-sentential zero-
anaphora resolution, which consequently im-
proves the overall performance of zero-
anaphora resolution. This is to be considered
as a contribution to AR research.
? Analogously to inter-sentential anaphora, de-
composing the antecedent identification task
into a series of comparisons between candi-
date antecedents works remarkably well also
in intra-sentential zero-anaphora resolution.
We hope this finding to be adopted in SRL.
The rest of the paper is organized as follows.
Section 2 describes the task definition of zero-
anaphora resolution in Japanese. In Section 3,
we review previous approaches to AR. Section 4
described how the proposed model incorporates
effectively syntactic features into the machine
learning-based approach. We then report the
results of our experiments on Japanese zero-
anaphora resolution in Section 5 and conclude in
Section 6.
2 Zero-anaphora resolution
In this paper, we consider only zero-pronouns that
function as an obligatory argument of a predicate
for two reasons:
? Providing a clear definition of zero-pronouns
appearing in adjunctive argument positions
involves awkward problems, which we be-
lieve should be postponed until obligatory
zero-anaphora is well studied.
? Resolving obligatory zero-anaphora tends to
be more important than adjunctive zero-
pronouns in actual applications.
A zero-pronoun may have its antecedent in the dis-
course; in this case, we say the zero-pronoun is
anaphoric. On the other hand, a zero-pronoun
whose referent does not explicitly appear in the
discourse is called a non-anaphoric zero-pronoun.
A zero-pronoun may be non-anaphoric typically
when it refers to an extralinguistic entity (e.g. the
first or second person) or its referent is unspecified
in the context.
The following are Japanese examples. In sen-
tence (1), zero-pronoun ?i is anaphoric as its an-
tecedent, ?shusho (prime minister)?, appears in the
same sentence. In sentence (2), on the other hand,
?j is considered non-anaphoric if its referent (i.e.
the first person) does not appear in the discourse.
(1) shushoi-wa houbeisi-te ,
prime ministeri-TOP visit-U.S.-CONJ PUNC
626
ryoukoku-no gaikou-o
both countries-BETWEEN diplomacy-OBJ
(?i-ga) suishinsuru
(?i-NOM) promote-ADNOM
houshin-o akirakanisi-ta .
plan-OBJ unveil-PAST PUNC
The prime minister visited the united states
and unveiled the plan to push diplomacy
between the two countries.
(2) (?j-ga) ie-ni kaeri-tai .
(?j -NOM) home-DAT want to go back PUNC
(I) want to go home.
Given this distinction, we consider the task of
zero-anaphora resolution as the combination of
two sub-problems, antecedent identification and
anaphoricity determination, which is analogous to
NP-anaphora resolution:
For each zero-pronoun in a given dis-
course, find its antecedent if it is
anaphoric; otherwise, conclude it to be
non-anaphoric.
3 Previous work
3.1 Antecedent identification
Previous machine learning-based approaches to
antecedent identification can be classified as ei-
ther the candidate-wise classification approach or
the preference-based approach. In the former ap-
proach (Soon et al, 2001; Ng and Cardie, 2002a,
etc.), given a target anaphor, TA, the model esti-
mates the absolute likelihood of each of the candi-
date antecedents (i.e. the NPs preceding TA), and
selects the best-scored candidate. If all the can-
didates are classified negative, TA is judged non-
anaphoric.
In contrast, the preference-based ap-
proach (Yang et al, 2003; Iida et al, 2003)
decomposes the task into comparisons of the
preference between candidates and selects the
most preferred one as the antecedent. For exam-
ple, Iida et al (2003) proposes a method called
the tournament model. This model conducts a
tournament consisting of a series of matches in
which candidate antecedents compete with each
other for a given anaphor.
While the candidate-wise classification model
computes the score of each single candidate inde-
pendently of others, the tournament model learns
the relative preference between candidates, which
is empirically proved to be a significant advan-
tage over candidate-wise classification (Iida et al,
2003).
3.2 Anaphoricity determination
There are two alternative ways for anaphoric-
ity determination: the single-step model and the
two-step model. The single-step model (Soon et
al., 2001; Ng and Cardie, 2002a) determines the
anaphoricity of a given anaphor indirectly as a
by-product of the search for its antecedent. If
an appropriate candidate antecedent is found, the
anaphor is classified as anaphoric; otherwise, it is
classified as non-anaphoric. One disadvantage of
this model is that it cannot employ the preference-
based model because the preference-based model
is not capable of identifying non-anaphoric cases.
The two-step model (Ng, 2004; Poesio et al,
2004; Iida et al, 2005), on the other hand, car-
ries out anaphoricity determination in a separate
step from antecedent identification. Poesio et
al. (2004) and Iida et al (2005) claim that the lat-
ter subtask should be done before the former. For
example, given a target anaphor (TA), Iida et al?s
selection-then-classification model:
1. selects the most likely candidate antecedent
(CA) of TA using the tournament model,
2. classifies TA paired with CA as either
anaphoric or non-anaphoric using an
anaphoricity determination model. If the
CA-TA pair is classified as anaphoric, CA is
identified as the antecedent of TA; otherwise,
TA is conclude to be non-anaphoric.
The anaphoricity determination model learns the
non-anaphoric class directly from non-anaphoric
training instances whereas the single-step model
cannot not use non-anaphoric cases in training.
4 Proposal
4.1 Task decomposition
We approach the zero-anaphora resolution prob-
lem by decomposing it into two subtasks: intra-
sentential and inter-sentential zero-anaphora reso-
lution. For the former problem, syntactic patterns
in which zero-pronouns and their antecedents ap-
pear may well be useful clues, which, however,
does not apply to the latter problem. We there-
fore build a separate component for each sub-
task, adopting Iida et al (2005)?s selection-then-
classification model for each component:
1. Intra-sentential antecedent identification:
For a given zero-pronoun ZP in a given
sentence S, select the most-likely candidate
antecedent C?1 from the candidates appearing
in S by the intra-sentential tournament model
627
2. Intra-sentential anaphoricity determination:
Estimate plausibility p1 that C?1 is the true an-
tecedent, and return C?1 if p1 ? ?intra (?intra
is a preselected threshold) or go to 3 other-
wise
3. Inter-sentential antecedent identification:
Select the most-likely candidate antecedent
C?2 from the candidates appearing outside of
S by the inter-sentential tournament model.
4. Inter-sentential anaphoricity determination:
Estimate plausibility p2 that C?2 is the true
antecedent, and return C?2 if p2 ? ?inter
(?inter is a preselected threshold) or return
non-anaphoric otherwise.
4.2 Representation of syntactic patterns
In the first two of the above four steps, we use syn-
tactic pattern features. Analogously to SRL, we
extract the parse path between a zero-pronoun to
its antecedent to capture the syntactic pattern of
their occurrence. Among many alternative ways
of representing a path, in the experiments reported
in the next section, we adopted a method as we
describe below, leaving the exploration of other al-
ternatives as future work.
Given a sentence, we first use a standard depen-
dency parser to obtain the dependency parse tree,
in which words are structured according to the de-
pendency relation between them. Figure 1(a), for
example, shows the dependency tree of sentence
(1) given in Section 2. We then extract the path
between a zero-pronoun and its antecedent as in
Figure 1(b). Finally, to encode the order of sib-
lings and reduce data sparseness, we further trans-
form the extracted path as in Figure 1(c):
? A path is represented by a subtree consist-
ing of backbone nodes: ? (zero-pronoun),
Ant (antecedent), Node (the lowest common
ancestor), LeftNode (left-branch node) and
RightNode.
? Each backbone node has daughter nodes,
each corresponding to a function word asso-
ciated with it.
? Content words are deleted.
This way of encoding syntactic patterns is used
in intra-sentential anaphoricity determination. In
antecedent identification, on the other hand, the
tournament model allows us to incorporate three
paths, a path for each pair of a zero-pronoun and
left and right candidate antecedents, as shown in
  
	
 

 	
 


Text Simplification for Reading Assistance: A Project Note
Kentaro Inui Atsushi Fujita Tetsuro Takahashi Ryu Iida
Nara Advanced Institute of Science and Technology
Takayama, Ikoma, Nara, 630-0192, Japan
finui,atsush-f,tetsu-ta,ryu-ig@is.aist-nara.ac.jp
Tomoya Iwakura
Fujitsu Laboratories Ltd.
Kamikodanaka, Nakahara, Kawasaki, Kanagawa, 211-8588, Japan
iwakura.tomoya@jp.fujitsu.com
Abstract
This paper describes our ongoing research
project on text simplification for congenitally
deaf people. Text simplification we are aiming
at is the task of offering a deaf reader a syn-
tactic and lexical paraphrase of a given text for
assisting her/him to understand what it means.
In this paper, we discuss the issues we should
address to realize text simplification and re-
port on the present results in three different
aspects of this task: readability assessment,
paraphrase representation and post-transfer er-
ror detection.
1 Introduction
This paper reports on our ongoing research into
text simplification for reading assistance. Potential
users targeted in this research are congenitally deaf
people (more specifically, students at (junior-)high
schools for the deaf), who tend to have difficulties
in reading and writing text. We are aiming at the
development of the technology of text simplification
with which a reading assistance system lexically and
structurally paraphrases a given text into a simpler
and plainer one that is thus more comprehensible.
The idea of using paraphrases for reading as-
sistance is not necessarily novel. For example,
Carroll et al (1998) and Canning and Taito (1999)
report on their project in which they address syn-
tactic transforms aiming at making newspaper text
accessible to aphasics. Following this trend of re-
search, in this project, we address four unexplored
issues as below besides the user- and task-oriented
evaluation of the overall system.
Before going to the detail, we first clarify the four
issues we have addressed in the next section. We
then reported on the present results on three of the
four, readability assessment, paraphrase representa-
tion and post-transfer error detection, in the subse-
quent sections.
2 Research issues and our approach
2.1 Readability assessment
The process of text simplification for reading as-
sistance can be decomposed into the following three
subprocesses:
a. Problem identification: identify which portions of
a given text will be difficult for a given user to
read,
b. Paraphrase generation: generate possible candi-
date paraphrases from the identified portions, and
c. Evaluation: re-assess the resultant texts to choose
the one in which the problems have been resolved.
Given this decomposition, it is clear that one of the
key issues in reading assistance is the problem of as-
sessing the readability or comprehensibility1 of text
because it is involved in subprocesses (a) and (c).
Readability assessment is doubtlessly a tough is-
sue (Williams et al, 2003). In this project, however,
we argue that, if one targets only a particular popu-
lation segment and if an adequate collection of data
is available, then corpus-based empirical approaches
may well be feasible. We have already proven that
one can collect such readability assessment data by
conducting survey questionnaires targeting teachers
at schools for the deaf.
1In this paper, we use the terms readability and comprehen-
sibility interchangeably, while strictly distinguishing them from
legibility of each fragment (typically, a sentence or paragraph)
of a given text.
2.2 Paraphrase acquisition
One of the good findings that we obtained through
the aforementioned surveys is that there are a broad
range of paraphrases that can improve the readabil-
ity of text. A reading assistance system is, therefore,
hoped to be able to generate sufficient varieties of
paraphrases of a given input. To create such a sys-
tem, one needs to feed it with a large collection of
paraphrase patterns. Very timely, the acquisition of
paraphrase patterns has been actively studied in re-
cent years:
 Manual collection of paraphrases in the context of
language generation, e.g. (Robin and McKeown,
1996),
 Derivation of paraphrases through existing lexical
resources, e.g. (Kurohashi et al, 1999),
 Corpus-based statistical methods inspired by the
work on information extraction, e.g. (Jacquemin,
1999; Lin and Pantel, 2001), and
 Alignment-based acquisition of paraphrases from
comparable corpora, e.g. (Barzilay and McKe-
own, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003).
One remaining issue is how effectively these meth-
ods contribute to the generation of paraphrases in our
application-oriented context.
2.3 Paraphrase representation
One of the findings obtained in the previous stud-
ies for paraphrase acquisition is that the automatic
acquisition of candidates of paraphrases is quite re-
alizable for various types of source data but acquired
collections tend to be rather noisy and need manual
cleaning as reported in, for example, (Lin and Pan-
tel, 2001). Given that, it turns out to be important to
devise an effective way of facilitating manual correc-
tion and a standardized scheme for representing and
storing paraphrase patterns as shared resources.
Our approach is (a) to define first a fully express-
ible formalism for representing paraphrases at the
level of tree-to-tree transformation and (b) devise an
additional layer of representation on its top that is de-
signed to facilitate handcoding transformation rules.
2.4 Post-transfer text revision
In paraphrasing, the morpho-syntactic informa-
tion of a source sentence should be accessible
throughout the transfer process since a morpho-
syntactic transformation in itself can often be a mo-
tivation or goal of paraphrasing. Therefore, such
an approach as semantic transfer, where morpho-
syntactic information is highly abstracted away as
in (Dorna et al, 1998; Richardson et al, 2001),
does not suit this task. Provided that the morpho-
syntactic stratum be an optimal level of abstraction
for representing paraphrasing/transfer patterns, one
must recall that semantic-transfer approaches such as
those cited above were motivated mainly by the need
for reducing the complexity of transfer knowledge,
which could be unmanageable in morpho-syntactic
transfer.
Our approach to this problem is to (a) leave the de-
scription of each transfer pattern underspecified and
(b) implement the knowledge about linguistic con-
straints that are independent of a particular trans-
fer pattern separately from the transfer knowledge.
There are a wide range of such transfer-independent
linguistic constraints. Constraints on morpheme
connectivity, verb conjugation, word collocation,
and tense and aspect forms in relative clauses are typ-
ical examples of such constraints.
These four issues can be considered as different
aspects of the overall question how one can make
the development and maintenance of a gigantic re-
source for paraphrasing tractable. (1) The introduc-
tion of readability assessment would free us from
cares about the purposiveness of each paraphrasing
rule in paraphrase acquisition. (2) Paraphrase ac-
quisition is obviously indispensable for scaling up
the resource. (3) A good formalism for representing
paraphrasing rules would facilitate the manual re-
finement and maintenance of them. (4) Post-transfer
error detection and revision would make the system
tolerant to flows in paraphrasing rules.
While many researchers have addressed the issue
of paraphrase acquisition reporting promising results
as cited above, the other three issues have been left
relatively unexplored in spite of their significance in
the above sense. Motivated by this context, in the
rest of this paper, we address these remaining three.
3 Readability assessment
To the best of our knowledge, there have never
been no reports on research to build a computational
model of the language proficiency of deaf people, ex-
cept for the remarkable reports by Michaud and Mc-
Coy (2001). As a subpart of their research aimed at
developing the ICICLE system (McCoy and Master-
man, 1997), a language-tutoring application for deaf
learners of written English, Michaud and McCoy de-
veloped an architecture for modeling the writing pro-
ficiency of a user called SLALOM. SLALOM is de-
signed to capture the stereotypic linear order of ac-
quisition within certain categories of morphological
and/or syntactic features of language. Unfortunately,
the modeling method used in SLALOM cannot be
directly applied to our domain for three reasons.
 Unlike writing tutoring, in reading assistance, tar-
get sentences are in principle unlimited. We
therefore need to take a wider range of morpho-
syntactic features into account.
 SLALOM is not designed to capture the difficulty
of any combination of morpho-syntactic features,
which it is essential to take into account in reading
assistance.
 Given the need to consider feature combinations,
a simple linear order model that is assumed in
SLALOM is unsuitable.
3.1 Our approach: We ask teachers
To overcome these deficiencies, we took yet an-
other approach where we designed a survey ques-
tionnaire targeting teachers at schools for the deaf,
and have been collecting readability assessment data.
In this questionnaire, we ask the teachers to compare
the readability of a given sentence with paraphrases
of it. The use of paraphrases is of critical importance
in our questionnaire since it makes manual readabil-
ity assessment significantly easier and more reliable.
3.1.1 Targets
We targeted teachers of Japanese or English liter-
acy at schools for the deaf for the following reasons.
Ideally, this sort of survey would be carried out
by targeting the population segment in question, i.e.,
deaf students in our study. In fact, pedagogists and
psycholinguists have made tremendous efforts to ex-
amine the language proficiency of deaf students by
giving them proficiency tests. Such efforts are very
important, but they have had difficulty in capturing
enough of the picture to develop a comprehensive
and implementable reading proficiency model of the
population due to the expense of extensive language
proficiency testing.
In contrast, our approach is an attempt to model
the knowledge of experts in this field (i.e., teaching
deaf students). The targeted teachers have not only
rich experiential knowledge about the language pro-
ficiency of their students but are also highly skilled in
paraphrasing to help their students? comprehension.
Since such knowledge gleaned from individual ex-
periences already has some generality, extracting it
through a survey should be less costly and thus more
comprehensive than investigation based on language
proficiency testing.
3.1.2 Questionnaire
In the questionnaire, each question consists of sev-
eral paraphrases, as shown in Figure 1 (a), where
(A) is a source sentence, and (B) and (C) are para-
phrases of (A). Each respondent was asked to as-
sess the relative readability of the paraphrases given
for each source sentence, as shown in Figure 1 (b).
The respondent judged sentence (A) to be the most
difficult and judged (B) and (C) to be comparable.
A judgment that sentence s
i
is easier than sentence
s
j
means that s
i
is judged likely to be understood
by a larger subset of students than s
j
. We asked
the respondents to annotate the paraphrases with
format-free comments, giving the reasons for their
judgments, alternative paraphrases, etc., as shown in
Figure 1 (b).
To make our questionnaire efficient for model ac-
quisition, we had to carefully control the variation in
paraphrases. To do that, we first selected around 50
morpho-syntactic features that are considered influ-
ential in sentence readability for deaf people. For
each of those features, we collected several sim-
ple example sentences from various sources (literacy
textbooks, grammar references, etc.). We then man-
ually produced several paraphrases from each of the
collected sentences so as to remove the feature that
characterized the source sentence from each para-
phrase. For example, in Figure 1, the feature char-
acterizing sentence (A) is a non-restrictive relative
clause (i.e., sentence (A) was selected as an example
of this feature). Neither (B) nor (C) has this feature.
We also controlled the lexical variety to minimize
the effect of lexical factors on readability; we also
restricted the vocabulary to a top-2000 basic word
set (NIJL, 1991).
3.1.3 Administration
We administrated a preliminary survey targeting
three teachers. Through the survey, we observed that
(a) the teachers largely agreed in their assessments of
relative readability, (b) their format-free comments
indicated that the observed differences in readabil-
ity were largely explainable in terms of the morpho-
syntactic features we had prepared, and (c) a larger-
scaled survey was needed to obtain a statistically re-
liable model. Based on these observations, we con-
ducted a more comprehensive survey, in which we
prepared 770 questions and sent questionnaires with
a random set of 240 of them to teachers of Japanese
or English literacy at 50 schools for the deaf. We
Figure 1: Sample question and response
asked them to evaluate as many as possible anony-
mously. We obtained 4080 responses in total (8.0
responses per question).
3.2 Readability ranking model
The task of ranking a set of paraphrases can be de-
composed into comparisons between two elements
combinatorially selected from the set. We consider
the problem of judging which of a given pair of para-
phrase sentences is more readable/comprehensible
for deaf students. More specifically, given para-
phrase pair (s
i
; s
j
), our problem is to classify it into
either left (s
i
is easier), right (s
j
is easier), or com-
parable (s
i
and s
j
are comparable).
Once the problem is formulated this way, we can
use various existing techniques for classifier learn-
ing. So far, we have examined a method of using the
support vector machine (SVM) classification tech-
nique.
A training/testing example is paraphrase pair
(s
i
; s
j
) coupled with its quantified class label
D(s
i
; s
j
) 2 [ 1; 1]. Each sentence s
i
is character-
ized by a binary feature vector F
s
i
, and each pair
(s
i
; s
j
) is characterized by a triple of feature vectors
hF
C
s
i
s
j
; F
L
s
i
s
j
; F
R
s
i
s
j
i, where
 F
C
s
i
s
j
= F
s
i
^ F
s
j
(features shared by s
i
and s
j
),
 F
L
s
i
s
j
= F
s
i
^F
s
j
(features belonging only to s
i
),
 F
R
s
i
s
j
= F
s
i
^F
s
j
(features belonging only to s
j
).
D(s
i
; s
j
) represents the difference in readability be-
tween s
i
and s
j
; it is computed in the following way.
1. Let T
s
i
s
j
be the set of respondents who assessed
(s
i
; s
j
).
2. Given the degree of readability respondent t as-
signed to s
i
(s
j
), map it to real value dor(t; s) 2
[0; 1] so that the lowest degree maps to 0 and the
highest degree maps to 1. For example, the de-
gree of readability assigned to (A) in Figure 1 (b)
maps to around 0.1, whereas that assigned to (B)
maps to around 0.9.
3. D(s
i
; s
j
) =
1
jT
s
i
s
j
j
P
t2T
s
i
s
j
dor(t; s
i
)  dor(t; s
j
):
Output score Sc
M
(s
i
; s
j
) 2 [ 1; 1] for input
(s
i
; s
j
) was given by the normalized distance be-
tween (s
i
; s
j
) and the hyperplane.
3.3 Evaluation and discussion
To evaluate the two modeling methods, we con-
ducted a ten-fold cross validation on the set of 4055
paraphrase pairs derived from the 770 questions used
in the survey. To create a feature vector space, we
used 355 morpho-syntactic features. Feature annota-
tion was done semi-automatically with the help of a
morphological analyzer and dependency parser.
The task was to classify a given paraphrase pair
into either left, right, or comparable. Model M ?s
output class for (s
i
; s
j
) was given by
Cls
M
(s
i
; s
j
) =
(
left (Sc
M
(s
i
; s
j
)   
m
)
right (Sc
M
(s
i
; s
j
)  
m
)
comparable (otherwise)
;
where 
m
2 [ 1; 1] is a variable threshold used to
balance precision with recall.
We used the 473 paraphrase pairs that satisfied the
following conditions:
 jD(s
i
; s
j
)j was not less than threshold 
a
(
a
=
0:5). The answer of (s
i
; s
j
) is given by
Cls
Ans
(s
i
; s
j
) =
n
left (D(s
i
; s
j
)   
a
)
right (D(s
i
; s
j
)  
a
) :
 (s
i
; s
j
) must have been assessed by more then one
respondent, i.e., jT
s
i
s
j
j > 1:
 Agreement ratio Agr(s
i
; s
j
) must be suffi-
ciently high, i.e., Agr(s
i
; s
j
)  0:9, where
Agr(s
i
; s
j
) = (for (s
i
; s
j
)   agst(s
i
; s
j
))=
jT
s
i
s
j
j, and for (s
i
; s
j
) and agst(s
i
; s
j
) are the
number of respondents who agreed and disagreed
with Cls
Ans
(s
i
; s
j
), respectively.
We judged output class Cls
M
(s
i
; s
j
) correct if and
only if Cls
M
(s
i
; s
j
) = Cls
Ans
(s
i
; s
j
). The overall
performance was evaluated based on recall Rc and
precision Pr:
Rc =
jf(s
i
;s
j
)j Cls
M
(s
i
; s
j
) is correctgj
jf(s
i
;s
j
)j Cls
Ans
(s
i
;s
j
)2fleft;rightggj
Pr =
jf(s
i
;s
j
)j Cls
M
(s
i
; s
j
) is correctgj
jf(s
i
;s
j
)j Cls
M
(s
i
;s
j
)2fleft;rightgj
.
The model achieved 95% precision with 89% re-
call. This result confirmed that the data we collected
through the questionnaires were reasonably noiseless
and thus generalizable. Furthermore, both models
exhibited a clear trade-off between recall and preci-
sion, indicating that their output scores can be used
as a confidence measure.
4 Paraphrase representation
We represent paraphrases as transfer patterns be-
tween dependency trees. In this section, we propose
a three-layered formalism for representing transfer
patterns.
4.1 Types of paraphrases of concern
There are various levels of paraphrases as the fol-
lowing examples demonstrate:
(1) a. She burst into tears, and he tried to comfort
her.
b. She cried, and he tried to console her.
(2) a. It was a Honda that John sold to Tom.
b. John sold a Honda to Tom.
c. Tom bought a Honda from John.
(3) a. They got married three years ago.
b. They got married in 2000.
Lexical vs. structural paraphrases Example (1)
includes paraphrases of the single word ?comfort?
and the canned phrase ?burst into tears?. The sen-
tences in (2), on the other hand, exhibit structural
and thus more general patterns of paraphrasing. Both
types of paraphrases, lexical and structural para-
phrases, are considered useful for many applications
including reading assistance and thus should be in
the scope our discussion.
Atomic vs. compositional paraphrases The pro-
cess of paraphrasing (2a) into (2c) is compositional
because it can be decomposed into two subpro-
cesses, (2a) to (2b) and (2b) to (2c). In develop-
ing a resource for paraphrasing, we have only to
cover non-compositional (i.e., atomic) paraphrases.
Compositional paraphrases can be handled if an ad-
ditional computational mechanism for combining
atomic paraphrases is devised.
Meaning-preserving vs. reference-preserving
paraphrases It is also useful to distinguish
reference-preserving paraphrases from meaning-
preserving ones. The above example in (3) is of the
reference-preserving type. This types of paraphras-
ing requires the computation of reference to objects
outside discourse and thus should be excluded from
our scope for the present purpose.
4.2 Dependency trees (MDSs)
Previous work on transfer-based machine transla-
tion (MT) suggests that the dependency-based repre-
sentation has the advantage of facilitating syntactic
transforming operations (Meyers et al, 1996; Lavoie
et al, 2000). Following this, we adopt dependency
trees as the internal representations of target texts.
We suppose that a dependency tree consists of a set
of nodes each of which corresponds to a lexeme or
compound and a set of edges each of which repre-
sents the dependency relation between its ends. We
call such a dependency tree a morpheme-based de-
pendency structure (MDS). Each node in an MDS is
supposed to be annotated with an open set of typed
features that indicate morpho-syntactic and semantic
information. We also assume a type hierarchy in de-
pendency relations that consists of an open set of de-
pendency classes including dependency, compound,
parallel, appositive and insertion.
4.3 Three-layered representation
Previous work on transfer-based MT sys-
tems (Lavoie et al, 2000; Dorna et al, 1998)
and alignment-based transfer knowledge acqui-
sition (Meyers et al, 1996; Richardson et al,
2001) have proven that transfer knowledge can be
best represented by declarative structure mapping
(transforming) rules each of which typically consists
of a pair of source and target partial structures as in
the middle of Figure 2.
Adopting such a tree-to-tree style of representa-
tion, however, one has to address the issue of the
trade-off between expressibility and comprehensi-
bility. One may want a formalism of structural
rule editing
translation
compilation
simplified MDS transfer rule
N shika V- nai  ->  V no wa N dake da.
(someone does not V to nothing but N)   (it is only to N that someone does V)
MDS transfer rule
sp_rule(108, negation, RefNode) :-
  match(RefNode, X4=[pos:postp,lex: shika]),
  depend(X3=[pos:verb], empty, X4),
  depend(X1=[pos:aux_verb,lex: nai],
         X2=[pos:aux_verb*], X3),
  depend(X4, empty, X5=[pos:noun]),
  replace(X1, X6=[pos:aux_verb,lex: da]),
  substitute(X5, X12=[pos:noun]),
  move_dtrs(X5, X12),
  substitute(X3, X10=[pos:verb]),
                            :
pos: postp
lex: shika (except)
pos: aux_verb
lex:  da (copula)
pos: postp
lex: wa (TOP)
X6
X11
X12pos: nounlex:  no (thing)
pos: postp
lex: dake (only)
pos: noun
pos: noun
aux_verb*
pos: aux_verb
lex: nai (not)
pos: verbX3
X4
X1
X5
X2 X7
X8
X10 pos: verb
X9 vws
MDS processing operators
(=X5)
(=X2)
(=X3)
Figure 2: Three-layered rule representation
transformation patterns that is powerful enough to
represent a sufficiently broad range of paraphrase
patterns. However, highly expressible formalisms
would make it difficult to create and maintain rules
manually.
To mediate this trade-off, we devised a new layer
of representation to add on the top of the layer of
tree-to-tree pattern representation as illustrated in
Figure 2. At this new layer, we use an extended natu-
ral language to specify transformation patterns. The
language is designed to facilitate the task of hand-
coding transformation rules. For example, to define
the tree-to-tree transformation pattern given in the
middle of Figure 2, a rule editor needs only to spec-
ify its simplified form:
(4) N shika V- nai ! V no ha N dake da.
(Someone does V to nothing but N ! It is only to
N that someone does V)
A rule of this form is then automatically translated
into a fully-specified tree-to-tree transformation rule.
We call a rule of the latter form an MDS rewriting
rule (SR rule), and a rule of the former form a sim-
plified SR rule (SSR rule).
The idea is that most of the specifications of an SR
rule can usually be abbreviated if a means to auto-
matically complement it is provided. We use a parser
and macros to do so; namely, the rule translator com-
plements an SSR rule by macro expansion and pars-
ing to produce the corresponding SR rule specifica-
tions. The advantages of introducing the SSR rule
layer are the following:
 The SSR rule formalism allows a rule writer to
edit rules with an ordinary text editor, which
makes the task of rule editing much more efficient
than providing her/him with a GUI-based com-
plex tool for editing SR rules directly.
 The use of the extended natural language also
has the advantage in improving the readability of
rules for rule writers, which is particularly impor-
tant in group work.
 To parse SSR rules, one can use the same parser
as that used to parse input texts. This also im-
proves the efficiency of rule development because
it significantly reduces the burden of maintaining
the consistency between the POS-tag set used for
parsing input and that used for rule specifications.
The SSR rule layer shares underlying motiva-
tions with the formalism reported by Hermjakob et
al. (2002). Our formalism is, however, considerably
extended so as to be licensed by the expressibility of
the SR rule representation and to be annotated with
various types of rule applicability conditions includ-
ing constraints on arbitrary features of nodes, struc-
tural constraints, logical specifications such as dis-
junction and negation, closures of dependency rela-
tions, optional constituents, etc.
The two layers for paraphrase representation
are fully implemented on our paraphrasing engine
KURA (Takahashi et al, 2001) coupled with another
layer for processing MDSs (the bottom layer illus-
trated in Figure 2). The whole system of KURA
and part of the transer rules implemented on it
(see Section 5 below) are available at http://cl.aist-
nara.ac.jp/lab/kura/doc/.
5 Post-transfer error detection
What kinds of transfer errors tend to occur in lex-
ical and structural paraphrasing? To find it out, we
conducted a preliminary investigation. This section
reports a summary of the results. See (Fujita and
Inui, 2002) for further details.
We implemented over 28,000 transfer rules for
Japanese paraphrases on the KURA paraphrasing en-
gine based on the rules previously reported in (Sato,
1999; Kondo et al, 1999; Kondo et al, 2001; Iida et
al., 2001) and existing lexical resources such as the-
sauri and case frame dictionaries. The implemented
rules ranged from such lexical paraphrases as those
that replace a word with its synonym to such syn-
tactic/structural paraphrases as those that remove a
cleft construction from a sentence, devide a sentence,
etc. We then fed KURA with a set of 1,220 sentences
randomly sampled from newspaper articles and ob-
tained 630 transferred output sentences.
The following are the tendencies we observed:
 The transfer errors observed in the experiment ex-
hibited a wide range of variety from morphologi-
cal errors to semantic and discourse-related ones.
 Most types of errors tended to occur regardless
of the types of transfer. This suggests that if one
creates an error detection module specialized for
a particular error type, it works across different
types of transfer.
 The most frequent error type involved inappropri-
ate conjugation forms of verbs. It is, however,
a matter of morphological generation and can be
easily resolved.
 Errors in regard to verb valency and selectional
restriction also tended to be frequent and fatal,
and thus should have preference as a research
topic.
 The next frequent error type was related to the
difference of meaning between near synonyms.
However, this type of errors could often be de-
tected by a model that could detect errors of verb
valency and selectional restriction.
Based on these observations, we concluded that
the detection of incorrect verb valences and verb-
complement cooccurrence was one of the most se-
rious problems that should have preference as a re-
search topic. We are now conducting experiments
on empirical methods for detecting this type of er-
rors (Fujita et al, 2003).
6 Conclusion
This paper reported on the present results of our
ongoing research on text simplification for reading
assistance targeting congenitally deaf people. We
raised four interrelated issues that we needed address
to realize this application and presented our previ-
ous activities focuing on three of them: readabil-
ity assessment, paraphrase representation and post-
transfer error detection.
Regarding readability assessment, we proposed a
novel approach in which we conducted questionnaire
surveys to collect readability assessment data and
took a corpus-based empirical method to obtain a
readability ranking model. The results of the sur-
veys show the potential impact of text simplification
on reading assistance. We conducted experiments on
the task of comparing the readability of a given para-
phrase pair and obtained promising results by SVM-
based classifier induction (95% precision with 89%
recall). Our approach should be equally applicable
to other population segments such as aphasic read-
ers and second-language learners. Our next steps
includes the investigation of the drawbacks of the
present bag-of-features modeling approach. We also
need to consider a method to introduce the notion
of user classes (e.g. beginner, intermediate and ad-
vanced). Textual aspects of readability will also need
to be considered, as discussed in (Inui and Nogami,
2001; Siddahrthan, 2003).
Regarding paraphrase representation, we pre-
sented our revision-based lexico-structural para-
phrasing engine. It provides a fully expressible
scheme for representating paraphrases, while pre-
serving the easiness of handcraft paraphrasing rules
by providing an extended natural language as a
means of pattern editting. We have handcrafted over
a thousand transfer rules that implement a broad
range of lexical and structural paraphrasing.
The problem of error detection is also critical.
When we find a effective solution to it, we will be
ready to integrate the technologies into an applica-
tion system of text simplification and conduct user-
and task-oriented evaluations.
Acknowledgments
The research presented in this paper was partly
funded by PREST, Japan Science and Technology
Corporation. We thank all the teachers at the schools
for the deaf who cooperated in our questionnaire sur-
vey and Toshihiro Agatsuma (Joetsu University of
Education) for his generous and valuable coopera-
tion in the survey. We also thank Yuji Matsumoto
and his colleagues (Nara Advanced Institute of Sci-
ence and Technology) for allowing us to use their
NLP tools ChaSen and CaboCha, Taku Kudo (Nara
Advanced Institute of Science and Technology) for
allowing us to use his SVM tool, and Takaki Makino
and his colleagues (Tokyo University) for allow-
ing us to use LiLFeS, with which we implemented
KURA. We also thank the anonymous reviewers for
their suggestive and encouraging comments.
References
Barzilay, R. and McKeown, K. 2001. Extracting para-
phrases from a parallel corpus. In Proc. of the 39th An-
nual Meeting and the 10th Conference of the European
Chapter of Association for Computational Linguistics
(EACL), pages 50?57.
Barzilay, R. and Lee, L. 2003. Learning to paraphrases: an
unsupervised approach using multiple-sequence align-
ment. In Proc. of HLT-NAACL.
Canning, Y. and Taito, J. 1999. Syntactic simplification of
newspaper text for aphasic readers. In Proc. of the 22nd
Annual International ACM SIGIR Conference (SIGIR).
Carroll, J., Minnen, G., Canning, Y., Devlin, S. and Tait, J.
1998. Practical simplification of English newspaper
text to assist aphasic readers. In Proc. of AAAI-98
Workshop on Integrating Artificial Intelligence and As-
sistive Technology.
Dorna, M., Frank, A., Genabith, J. and Emele, M. 1998.
Syntactic and semantic transfer with F-structures. In
Proc. of COLING-ACL, pages 341?347.
Fujita, A. and Inui, K. 2002. Decomposing linguistic
knowledge for lexical paraphrasing. In Information
Processing Society of Japan SIG Technical Reports,
NL-149, pages 31?38. (in Japanese)
Fujita, A., Inui, K. and Matsumoto, Y. 2003. Automatic
detection of verb valency errors in paraphrasing. In In-
formation Processing Society of Japan SIG Technical
Reports, NL-156. (in Japanese)
Hermjakob, U., Echihabi, A. and Marcu, D. 2002. Nat-
ural language based reformulation resource and Web
exploitation for question answering. In Proc. of the
TREC-2002 Conference.
Iida, R., Tokunaga, Y., Inui, K. and Eto, J. 2001. Explo-
ration of clause-structural and function-expressional
paraphrasing using KURA. In Proc. of the 63th Annual
Meeting of Information Processing Society of Japan,
pages 5?6. (in Japanese).
Inui, K. and Nogami, M. 2001. A paraphrase-based explo-
ration of cohesiveness criteria. In Proc. of the Eighth
European Workshop on Natulan Language Generation,
pages 101?110.
Jacquemin, C. 1999. Syntagmatic and paradigmatic rep-
resentations of term variations. In Proc. of the 37th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 341?349.
Kondo, K., Sato, S. and Okumura, M. 1999. Paraphras-
ing of ?sahen-noun + suru?. Journal of Information
Processing Society of Japan, 40(11):4064?4074. (in
Japanese).
Kondo, K., Sato, S. and Okumura, M. 2001. Para-
phrasing by case alternation. Journal of Informa-
tion Processing Society of Japan, 42(3):465?477. (in
Japanese).
Kurohashi, S. and Sakai, Y. 1999. Semantic analysis of
Japanese noun phrases: a new approach to dictionary-
based understanding. In Proc. of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 481?488.
Lavoie, B. Kittredge, R. Korelsky, T. Rambow, O. 2000.
A framework for MT and multilingual NLG ystems
based on uniform lexico-structural processing. In Proc.
of ANLP-NAACL.
Lin, D. and Pantel, P. 2001. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
McCoy ,K. F. and Masterman (Michaud), L. N. 1997. A
Tutor for Teaching English as a Second Language for
Deaf Users of American Sign Language, In Proc. of
ACL/EACL ?97 Workshop on Natural Language Pro-
cessing for Communication Aids.
Meyers, A., Yangarber, R. and Grishman, R. 1996. Align-
ment of shared forests for bilingual corpora. In Proc.
of the 16th International Conference on Computational
Linguistics (COLING), pages 460?465.
Michaud, L. N. and McCoy, K. F. 2001. Error profiling:
toward a model of English acquisition for deaf learn-
ers. In Proc. of the 39th Annual Meeting and the 10th
Conference of the European Chapter of Association for
Computational Linguistics (EACL), pages 386?393.
NIJL, the National Institute for Japanese Language. 1991.
Nihongo Kyo?iku-no tame-no Kihon-Goi Cho?sa (The
basic lexicon for the education of Japanese). Shuei
Shuppan, Japan. (In Japanese)
Richardson, S., Dolan, W., Menezes, A. and Corston-
Oliver, M. 2001. Overcoming the customization bottle-
neck using example-based MT. In Proc. of the 39th An-
nual Meeting and the 10th Conference of the European
Chapter of Association for Computational Linguistics
(EACL), pages 9?16.
Robin, J. and McKeown, K. 1996. Empirically designing
and evaluating a new revision-based model for sum-
mary generation. Artificial Intelligence, 85(1?2):135?
179.
Sato, S. 1999. Automatic paraphrase of technical pa-
pers? titles. Journal of Information Processing Society
of Japan, 40(7):2937?2945. (in Japanese).
Shinyama, Y., Sekine, S. Kiyoshi, Sudo. and Grishman,
R. 2002. Automatic paraphrase acquisition from news
articles. In Proc. of HLT, pages 40?46.
Siddahrthan, A. 2003. Preserving discourse structure
when simplifying text. In Proc. of European Workshop
on Natural Language Generation, pages 103?110.
Takahashi, T., Iwakura, T., Iida, R., Fujita, A. and Inui, K.
2001. KURA: a transfer-based lexico-structural para-
phrasing engine. In Proc. of the 6th Natural Language
Processing Pacific Rim Symposium (NLPRS) Workshop
on Automatic Paraphrasing: Theories and Applica-
tions, pages 37?46.
Williams, S., Reiter, E. and Osman, L. 2003. Experiments
with discourse-level choices and readability. In Proc. of
European Workshop on Natural Language Generation,
pages 127?134.
Proceedings of the Linguistic Annotation Workshop, pages 132?139,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating a Japanese Text Corpus with
Predicate-Argument and Coreference Relations
Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{ryu-i,mamoru-k,inui,matsu}@is.naist.jp
Abstract
In this paper, we discuss how to anno-
tate coreference and predicate-argument re-
lations in Japanese written text. There
have been research activities for building
Japanese text corpora annotated with coref-
erence and predicate-argument relations as
are done in the Kyoto Text Corpus version
4.0 (Kawahara et al, 2002) and the GDA-
Tagged Corpus (Hasida, 2005). However,
there is still much room for refining their
specifications. For this reason, we discuss
issues in annotating these two types of re-
lations, and propose a new specification for
each. In accordance with the specification,
we built a large-scaled annotated corpus, and
examined its reliability. As a result of our
current work, we have released an anno-
tated corpus named the NAIST Text Corpus1,
which is used as the evaluation data set in
the coreference and zero-anaphora resolu-
tion tasks in Iida et al (2005) and Iida et al
(2006).
1 Introduction
Coreference resolution and predicate-argument
structure analysis has recently been a growing field
of research due to the demands from NLP appli-
cation such as information extraction and machine
translation. With the research focus placed on these
tasks, the specification of annotating corpora and the
1The NAIST Text Corpus is downloadable from
http://cl.naist.jp/nldata/corpus/, and it has already been
downloaded by 102 unique users.
data sets used in supervised techniques (Soon et al,
2001; Ng and Cardie, 2002, etc.) have also grown in
sophistication.
For English, several annotation schemes have al-
ready been proposed for both coreference relation
and argument structure, and annotated corpora have
been developed accordingly (Hirschman, 1997; Poe-
sio et al, 2004; Doddington et al, 2004). For in-
stance, in the Coreference task on Message Under-
standing Conference (MUC) and the Entity Detec-
tion and Tracking (EDT) task in the Automatic Con-
tent Extraction (ACE) program, which is the suc-
cessor of MUC, the details of specification of anno-
tating coreference relation have been discussed for
several years. On the other hand, the specification
of predicate-argument structure analysis has mainly
been discussed in the context of the CoNLL shared
task2 on the basis of the PropBank (Palmer et al,
2005).
In parallel with these efforts, there have also been
research activities for building Japanese text corpora
annotated with coreference and predicate-argument
relations such as the Kyoto Text Corpus version 4.0
(Kawahara et al, 2002) and the GDA3-Tagged Cor-
pus (Hasida, 2005). However, as we discuss in this
paper, there is still much room for arguing and re-
fining the specification of such sorts of semantic an-
notation. In fact, for neither of the above two cor-
pora, the adequacy and reliability of the annotation
scheme has been deeply examined.
In this paper, we discuss how to annotate coref-
erence and predicate-argument relations in Japanese
2http://www.lsi.upc.edu/?srlconll/
3The Global Document Annotation
132
text. In Section 2 to Section 4, we examine the an-
notation issues of coreference, predicate-argument
relations, and event-nouns and their argument rela-
tions respectively, and define adequate specification
of each annotation task. Then, we report the results
of actual annotation taking the Kyoto Corpus 3.0 as a
starting point. Section 6 discusses the open issues of
each annotation task and we conclude in Section 7.
2 Annotating coreference relations
2.1 Approaches to coreference annotation
Coreference annotation in English has been evolving
mainly in the context of information extraction. For
instance, in the 6th and 7th Message Understand-
ing Conferences (MUC), coreference resolution is
treated as a subtask of information extraction4. The
annotated corpora built in the MUC contain coref-
erence relations between NPs, which are used as a
gold standard data set for machine learning-based
approaches to coreference resolution by researchers
such as Soon et al (2001) and Ng and Cardie (2002).
However, van Deemter and Kibble (1999) claim
that the specification of the MUC coreference task
guides us to annotate expressions that are not nor-
mally considered coreferential, such as appositive
relations (e.g. Julius Caesari, a well-known em-
perori, ...).
In the task of Entity Detection and Tracking
(EDT) in the Automatic Content Extraction (ACE)
program (Doddington et al, 2004), the successor
of MUC, the coreference relations are redefined in
terms of two concepts, mentions and entities, in or-
der to avoid inappropriate co-indexing. In the speci-
fication of EDT, mentions are defined as the expres-
sions appearing in the texts, and entities mean the
collective set of specific entities referred to by the
mentions in the texts. Entities are limited to named
entities such as PERSON and ORGANIZATION for
adequacy and reliability of annotation. Therefore,
the ACE data set has the drawback that not all coref-
erence relations in the text are exhaustively anno-
tated. It is insufficient to resolve only the annotated
coreference relations in order to properly analyze a
text.
4http://www-nlpir.nist.gov/related projects/muc/
proceedings/co task.html
2.2 Coreference annotated corpora of Japanese
In parallel with these efforts, Japanese corpora have
been developed that are annotated with coreference
relations, such as the Kyoto Text Corpus version
4.0 (Kawahara et al, 2002) and GDA-Tagged Cor-
pus (Hasida, 2005). Before reviewing these works,
we explain the relationship between anaphora and
coreference in Japanese, referring to the following
examples. In example (1), the pronoun sorei (it)
points back to iPodi, and these two mentions refer
to the same entity in the world and thus are consid-
ered both anaphoric and coreferential.
(1) Tom-wa iPodi-o ka-tta .
Tom-TOP iPodi-ACC buy-PAST PUNC
Tom bought an iPod.
kare-wa sorei-de ongaku-o ki-ita .
he-TOP iti-INS music-ACC listen to-PAST PUNC
He listened to music on it.
On the other hand, in example (2), we still see an
anaphoric relation between iPodi (iPodi) and sorej
(itj) and sorej points back to iPodi. However, these
two mentions are not coreferential since they refer
to different entities in the world.
(2) Tom-wa iPodi-o ka-tta .
Tom-TOP iPodi-ACC buy-PAST PUNC
Tom bought an iPod.
Mary-mo sorej-o ka-tta .
Mary-TOP onej -ACC buy-PAST PUNC
Mary also bought one.
As in the above examples, an anaphoric relation
can be either coreferential or not. The former case is
called an identity-of-reference anaphora (IRA) and
the latter an identity-of-sense anaphora (ISA) (see
Mitkov (2002)). In English the difference between
IRA and ISA is clearly expressed by the anaphoric
relations formed with ?it? and ?one? respectively.
This makes it possible to treat these classes sepa-
rately. However, in Japanese, no such clear lexical
distinction can be drawn. In both the Kyoto Cor-
pus and GDA-Tagged Corpus, there is no discussion
in regards to distinction between ISA and IRA, thus
it is unclear what types of coreference relations the
annotators annotated. To make matters worse, their
approaches do not consider whether or not a mention
refers to a specific entity like in the EDT task.
2.3 Annotating IRA relations in Japanese
As described in the previous section, conventional
specifications in Japanese are not based on a pre-
133
cise definition of coreference relations, resulting in
inappropriate annotation. On the other hand, in our
specification, we consider two or more mentions as
coreferential in case they satisfy the following two
conditions:
? The mentions refer to not a generic entity but
to a specific entity.
? The relation between the mentions is consid-
ered as an IRA relation.
3 Annotating predicate-argument relations
3.1 Labels of predicate-argument relations
One debatable issue in the annotation of predicate-
argument relations is what level of abstraction we
should label those relations at.
The GDA-Tagged Corpus, for example, adopts a
fixed set of somewhat ?traditional? semantic roles
such as Agent, Theme, and Goal that are defined
across verbs. The PropBank (Palmer et al, 2005),
on the other hand, defines a set of semantic roles (la-
beled ARG0, ARG1, and AM-ADV, etc.) for each
verb and annotates each sentence in the corpus with
those labels as in (3).
(3) [ARGM?TMP A year earlier], [ARG0 the refiner] [rel
earned] [ARG1 $66 million, or $1.19 a share].
In the FrameNet (Fillmore and Baker, 2000), a spe-
cific set of semantic roles is defined for each set of
semantically-related verbs called a FrameNet frame.
However, there is still only limited consensus on
how many kinds of semantic roles should be iden-
tified and which linguistic theory we should adopt
to define them at least for the Japanese language.
An alternative way of labeling predicate-
argument relations is to use syntactic cases as
labels. In Japanese, arguments of a verb are marked
by a postposition, which functions as a case marker.
In sentence (4), for example, the verb tabe has
two arguments, each of which is marked by a
postposition, ga or o.
(4) Tom-ga ringo-o tabe-ru
Tom-NOM apple-ACC eat-PRES
(Tom eats an apple.)
Labeling predicate-argument relations in terms of
syntactic cases has a few more advantages over se-
mantic roles as far as Japanese is concerned:
? Manual annotation of syntactic cases is likely
to be more cost-efficient than semantic roles
because they are often explicitly marked by
case markers. This fact also allows us to avoid
the difficulties in defining a label set.
? In Japanese, the mapping from syntactic cases
to semantic roles tends to be reasonably
straightforward if a semantically rich lexicon of
verbs like the VerbNet (Kipper et al, 2000) is
available.
? Furthermore, we have not yet found many NLP
applications for which the utility of seman-
tic roles is actually demonstrated. One may
think of using semantic roles in textual infer-
ence as exemplified by, for example, Tatu and
Moldovan (2006). However, similar sort of
inference may well be realized with syntactic
cases as demonstrated in the information ex-
traction and question answering literature.
Taking these respects into account, we choose to
label predicate-argument relations in terms of syn-
tactic cases, which follows the annotation scheme
adopted in the Kyoto Corpus.
3.2 Syntactic case alternation
Once the level of syntactic cases is chosen for our
annotation, another issue immediately arises, alter-
ation of syntactic cases by syntactic transformations
such as passivization and causativization. For exam-
ple, sentence (5) is an example of causativization,
where Mary causes Tom?s eating action.
(5) Mary-ga Tom-ni ringo-o tabe-saseru
Mary-NOM Tom-DAT apple-ACC eat-CAUSATIVIZED
(Mary helps Tom eat an apple.)
One way of annotating these arguments is some-
thing like (6), where the relations between the
causativized predicate tabe-saseru (to make some-
one eat) and its arguments are indicated in terms of
surface syntactic cases.
(6) [REL=tabe-saseru (eat-CAUSATIVE),
GA=Mary, NI=Tom, O=ringo (apple)]
In fact, the Kyoto Corpus adopts this way of label-
ing.
An alternative way of treating such case alterna-
tions is to identify logical (or deep) case relations,
i.e. the relations between the base form of each pred-
icate and its arguments. (7) illustrates how the ar-
guments in sentence (5) are annotated with logical
case relations: Tom is labeled as the ga-case (Nom-
inative) filler of the verb tabe (to eat) and Mary is
134
labeled as the Extra-Nominative (EX-GA) which we
newly invent to indicate the Causer of a syntactically
causativized clause.
(7) [REL=tabe-(ru) (eat), GA=Tom, O=ringo (ap-
ple), EX-GA=Mary]
In the NAIST Text Corpus, we choose to this lat-
ter way of annotation motivated by such considera-
tions as follows:
? Knowing that, for example, Tom is the filler of
the ga-case (Nominative) of the verb tabe (to
eat) in (5) is more useful than knowing that Tom
is the ni-case (Dative) of the causativized verb
tabe-saseru (to make someone eat) for such ap-
plications as information extraction.
? The mapping from syntactic cases to semantic
roles should be described in terms of logical
case relations associated with bare verbs.
3.3 Zero-anaphora
In the PropBank the search space for a given pred-
icate?s arguments is limited to the sentence that
predicate appears in, because, syntactically, English
obligatory arguments are overtly expressed except
pro-form (e.g. John hopes [PRO to leave.]).
In contrast, Japanese is characterized by extensive
use of nominal ellipses, called zero-pronouns, which
behave like pronouns in English texts. Thus, if an
argument is omitted, and an expression correspond-
ing to that argument does not appear in the same
sentence, annotators should search for its antecedent
outside of the sentence. Furthermore, if an argument
is not explicitly mentioned in the text, they need to
annotate that relation as ?exophoric.? In the second
sentence of example (8), for instance, the ga (Nomi-
native) argument of the predicate kaeru (go back) is
omitted and refers to Tom in the first sentence. The
kara (Ablative) argument of that predicate is also
omitted, however the corresponding argument does
not explicitly appear in the text. In such cases, omit-
ted arguments should be considered as ?exophoric.?
(8) Tomi-wa kyo gakko-ni it-ta .
Tomi-TOP today school-LOC go-PAST PUNC
Tom went to school today.
(?i-ga) (?exophoric-kara) kae-tte suguni
?i-NOM ?exophoric-ABL go back immediately
(?i-ga) kouen-ni dekake-ta .
?i-NOM park-LOC go out-PAST PUNC
He went to the park as soon as he came back
from school.
Table 1: Comparison of annotating predicate-
argument relations
corpus label search space
PropBank semantic role intra
GDA Corpus semantic role inter, exo
Kyoto Corpus surface case intra, inter,
(voice alternation involved) exo
NAIST Corpus logical (deep) case intra, inter,
(our corpus) (relation with bare verb) exo
intra: intra-sentential relations, inter: inter-sentential relations,
exo: exophoric relations
To the best of our knowledge, the GDA-Tagged Cor-
pus does not contain intra-sentential zero-anaphoric
relations as predicate-argument relations, so it has a
serious drawback when used as training data in ma-
chine learning approaches.
Unlike coreference between two explicit nouns
where only an IRA is possible, the relation between
a zero-pronoun and its antecedent can be either IRA
or ISA. For example, in example (8), ?i is annotated
as having an IRA relation with its antecedent Tomi.
In contrast, example (9) exhibits an ISA relation be-
tween iPodi and ?i.
(9) Tom-wa iPodi-o kaa-tta .
Tom-TOP iPodi-ACC buya-PAST PUNC
Tom bought an iPod.
Mary-mo (?i-o) kab-tta .
Mary-TOP ?i-ACC buyb-PAST PUNC
Mary also bought one.
[REL=ka-(u) (buy), GA=Mary, O=iPodi]
The above examples indicate that predicate-
argument annotation in Japanese can potentially be
annotated as either an IRA or ISA relation. Note that
in Japanese these two relations cannot be explicitly
separated by syntactic clues. Thus, in our corpus
we annotate them without explicit distinction. It is
arguable that separate treatment of IRA and ISA in
predicate-argument annotation could be preferable.
We consider this issue as a task of future work.
A comparison of the specification is summarized
in Table 1.
4 Annotating event-noun-argument
relations
Meyers et al (2004) propose to annotate seman-
tic relations between nouns referring to an event
in the context, which we call event-nouns in this
135
paper. They release the NomBank corpus, in
which PropBank-style semantic relations are anno-
tated for event-nouns. In (10), for example, the
noun ?growth? refers to an event and ?dividends?
and ?next year? are annotated as ARG1 (roughly
corresponding to the theme role) and ARGM-TMP
(temporal adjunct).
(10) 12% growth in dividends next year [REL=growth,
ARG1=in dividends, ARGM-TMP=next year]
Following the PropBank-style annotation, the Nom-
Bank also restricts the search space for the argu-
ments of a given event-noun to the sentence in which
the event-noun appears. In Japanese, on the other
hand, since predicate-argument relations are often
zero-anaphoric, this restriction should be relaxed.
4.1 Labels of event-noun-relations
Regarding the choice between semantic roles and
syntactic cases, we take the same approach as
that for predicate-argument relations, which is also
adopted in the Kyoto Corpus. For example, in (11),
akajii (deficit) is identified as the ga argument of the
event-noun eikyo (influence).
(11) kono boueki akajii-wa waga kuni-no
this trade deficit-TOP our country-OF
kyosoryokuj-ni eikyo-o oyobosu
competitiveness-DAT influence-ACC affect
[REL=eikyo (influence), GA=akajii (deficit),
O=kyosoryokuj (competitiveness)]
The trade deficit affects our competitiveness.
Note that unlike verbal predicates, event-nouns can
never be a subject of voice alternation. An event-
noun-argument relation is, therefore, necessarily an-
notated in terms of the relation between the bare
verb corresponding to the event-noun and its argu-
ment. This is another reason why we consider it
reasonable to annotate the logical case relations be-
tween bare verbs and their arguments for predicate-
argument relations.
4.2 Event-hood
Another issue to be addressed is on the determina-
tion of the ?event-hood? of noun phrases, i.e. the
task of determining whether a given noun refers to
an event or not. In Japanese, since neither singular-
plural nor definite-indefinite distinction is explic-
itly marked, event-hood determination tends to be
highly context-dependent. In sentence (12), for ex-
ample, the first occurrence of denwa (phone-call),
subscripted with i, should be interpreted as Tom?s
calling event, whereas the second occurrence of the
same noun denwa should be interpreted as a physical
telephone (cellphone).
(12) karea-karano denwai-niyoruto watashib-wa
hea-ABL phone-calli according to Ib-NOM
kare-no ie-ni denwaj-o wasure-tarasii
his-OF home-LOC phonej -ACC leave-PAST
According to his phone call, I might have left
my cell phone at his home.
To control the quality of event-hood determina-
tion, we constrain the range of potential event-nouns
from two different points of view, neither of which
is explicitly discussed in designing the specifications
of the Kyoto Corpus.
First, we impose a POS-based constraint. In our
corpus annotation, we consider only verbal nouns
(sahen-verbs; e.g. denwa (phone) ) and deverbal
nouns (the nominalized forms of verbs; e.g. furumai
(behavior)) as potential event-nouns. This means
that event-nouns that are not associated with a verb,
such as jiko (accident), are out of scope of our anno-
tation.
Second, the determination of the event-hood of
a noun tends to be obscure when the noun consti-
tutes a compound. In (13), for example, the ver-
bal noun kensetsu (construction) constituting a com-
pound douro-kensetsu (road construction) can be in-
terpreted as a constructing event. We annotate it as
an event and douro (road) as the o argument.
(13) (?-ga) douro-kensetsu-o tsuzukeru
?-NOM road construction-ACC continue
Someone continues road construction.
In (14), on the other hand, since the compound
furansu kakumei (French Revolution) is a named-
entity and is not semantically decomposable, it is
not reasonable to consider any sort of predicate-
argument-like relations between its constituents fu-
ransu (France) and kakumei (revolution).
(14) furansu-kakumei-ga okoru
French Revolution-NOM take place
The French Revolution took place.
We therefore do not consider constituents of such se-
mantically non-decomposable compounds as a tar-
get of annotation.
5 Statistics of the new corpus
Two annotators annotated predicate-argument and
coreference relations according to the specifications,
136
using all the documents in Kyoto Text Corpus ver-
sion 3.0 (containing 38,384 sentences in 2,929 texts)
as a target corpus. We have so far annotated
predicate-argument relations with only three major
cases: ga (Nominative), o (Accusative) and ni (Da-
tive). We decided not to annotate other case relations
like kara-case (Ablative) because the annotation of
those cases was considered even further unreliable at
the point where we did not have enough experiences
in this annotation task. Annotating other cases is one
of our future directions.
The numbers of the annotated predicate-argument
relations are shown in Table 2. These relations are
categorized into five cases: (a) a predicate and its
argument appear in the same phrase, (b) the argu-
ment syntactically depends on its predicate or vice
versa, (c) the predicate and its argument have an
intra-sentential zero-anaphora relation, (d) the pred-
icate and its argument have an inter-sentential zero-
anaphora relation and (e) the argument does not ex-
plicitly appear in the text (i.e. exophoric). Table 2
shows that in annotation for predicates over 80%
of both o- and ni-arguments were found in depen-
dency relations, while around 60% of ga-arguments
were in zero-anaphoric relations. In comparison, in
the case of event-nouns, o- and ni-arguments are
likely to appear in the same phrase of given event-
nouns, and about 80% of ga-arguments have zero-
anaphoric relations with event-nouns. With respect
to the corpus size, we created a large-scaled anno-
tated corpus with predicate-argument and corefer-
ence relations. The data size of our corpus along
with other corpora is shown in Table 3.
Next, to evaluate the agreement between the two
human annotators, 287 randomly selected articles
were annotated by both of them. The results are
evaluated by calculating recall and precision in
which one annotation result is regarded as correct
and the other?s as the output of system. Note that
only the predicates annotated by both annotators are
used in calculating recall and precision. For eval-
uation of coreference relations, we calculated re-
call and precision based on the MUC score (Vilain
et al, 1995). The results are shown in Table 4,
where we can see that most annotating work was
done with high quality except for the ni-argument of
event-nouns. The most common source of error was
caused by verb alternation, and we will discuss this
Table 3: Data size of each corpus
corpus size
PropBank I 7,891 sentences
NomBank 0.8 24,311 sentences
ACE (2005 English) 269 articles
GDA Corpus 2,177 articles
Kyoto Corpus 555 articles (5,127 sentences)
NAIST Corpus (ours) 2,929 articles (38,384 sentences)
Table 4: Agreement of annotating each relation
recall precision
predicate 0.947 (6512/6880) 0.941 (6512/6920)
ga (NOM) 0.861 (5638/6549) 0.856 (5638/6567)
o (ACC) 0.943 (2447/2595) 0.919 (2447/2664)
ni (DAT) 0.892 (1060/1189) 0.817 (1060/1298)
event-noun 0.905 (1281/1415) 0.810 (1281/1582)
ga (NOM) 0.798 (1038/1300) 0.804 (1038/1291)
o (ACC) 0.893 (469/525) 0.765 (469/613)
ni (DAT) 0.717 (66/92) 0.606 (66/109)
coreference 0.893 (1802/2019) 0.831 (1802/2168)
issue in detail in Section 6. Such investigation of the
reliability of annotation has not been reported for ei-
ther the Kyoto Corpus or the GDA-Tagged Corpus.
However, our results also show that each annotating
task still leaves room for improvement. We summa-
rize open issues and discuss the future directions in
the next section.
6 Discussion
6.1 Identification of predicates and
event-nouns
Identification of predicates is sometimes unreliable
due to the ambiguity between a literal usage and a
compound functional usage. For instance, the ex-
pression ?to-shi-te?, which includes the verb shi (to
do), is ambiguous: either the verb shi functions as a
content word, i.e. an event-denoting word, or it con-
stitutes a multi-word expression together with to and
te. In the latter case, it does not make sense to inter-
pret the verb shi to denote an event. However, this
judgment is highly context-dependent and we have
not been able to devise a reliable criterion for it.
Tsuchiya et al (2006) have built a functional
expression-tagged corpus for automatically classify-
ing these usages. They reported that the agreement
ratio of functional expressions is higher than ours.
We believe their findings to also become helpful in-
formation for annotating predicates in our corpus.
With regards to event-nouns, a similar problem
137
Table 2: Statistics: annotating predicate-arguments relations
ga (Nominative) o (Accusative) ni (Dative)
predicates (a) in same phrase 177 (0.002) 60 (0.001) 591 (0.027)
106,628 (b) dependency relations 44,402 (0.419) 35,882 (0.835) 18,912 (0.879)
(c) zero-anaphoric (intra-sentential) 32,270 (0.305) 5,625 (0.131) 1,417 (0.066)
(d) zero-anaphoric (inter-sentential) 13,181 (0.124) 1,307 (0.030) 542 (0.025)
(e) exophoric 15,885 (0.150) 96 (0.002) 45 (0.002)
total 105,915 (1.000) 42,970 (1.000) 21,507 (1.000)
event-nouns (a) in same phrase 2,195 (0.077) 5,574 (0.506) 846 (0.436)
28,569 (b) dependency relations 4,332 (0.152) 2,890 (0.263) 298 (0.154)
(c) zero-anaphoric (intra-sentential) 9,222 (0.324) 1,645 (0.149) 586 (0.302)
(d) zero-anaphoric (inter-sentential) 5,190 (0.183) 854 (0.078) 201 (0.104)
(e) exophoric 7,525 (0.264) 42 (0.004) 10 (0.005)
total 28,464 (1.000) 11,005 (1.000) 1,941 (1.000)
also arises. If, for example, a compound noun con-
tains a verbal noun, we have to judge whether the
verbal noun can be interpreted as an event-noun or
not. Currently, we ask annotators to check if the
meaning of a given compound noun can be compo-
sitionally decomposed into those of its constituents.
However, the judgement of compositionality tends
to be highly subjective, causing the degradation of
the agreement ratio of event-nouns as shown in
Table 4. We are planning to investigate this problem
more closely and refine the current compositionality
criterion. One option is to build lexical resources of
multi-word expressions and compounds.
6.2 Identification of arguments
As we mentioned in 3.1, we use (deep) cases instead
of semantic roles as labels of predicate-argument re-
lations. While it has several advantages as discussed
in 3.1, this choice has also a drawback that should
be removed. The problem arises from lexical verb
alternation. It can sometimes be hard for annota-
tors to determine a case frame of a given predicate
when verb alternation takes place. For example, sen-
tence (15) can be analyzed simply as in (16a). How-
ever, since the verb shibaru (bind) has also another
alternative case frame as in (16b), the labeling of the
case of the argument kisoku (rule), i.e. either GA
(NOM) or DE (INST) may be undecidable if the argu-
ment is omitted.
(15) kisoku-ga hitobito-o shibaru
rule-NOM people-ACC bind
The rule binds people.
(16) a. [REL = shibaru (bind), GA = kisoku (rule), O = hitobito
(people)]
b. [REL = shibaru (bind), GA = ? (exophoric), O = hito-
bito (people), DE (Instrumental) = kisoku (rule)]
Similar problems occur for event-nouns as well.
For example, the event-noun hassei (realization) has
both transitive and intransitive readings, which may
produce awkward ambiguities.
To avoid this problem, we have two options; one
is to predefine the preference in case frames as a
convention for annotation and the other is to deal
with such alternations based on generic resources of
lexical semantics such as Lexical Conceptual Struc-
ture (LCS) (Jackendoff, 1990). Creating a Japanese
LCS dictionary is another on-going project, so we
can collaborate with them in developing the valuable
resources.
6.3 Event-hood determination
Event-nouns of some semantic types such as keiyaku
(contract), kisei (regulation) and toushi (investment)
are interpreted as either an event or an entity result-
ing from an event depending on are context. How-
ever, it is sometimes difficult to judge whether such
an event-noun should be interpreted as an event or a
resultant entity even by considering the whole con-
text, which degrades the stability of annotation. This
phenomena is also discussed in the NomBank, and
we will share their insights and refine our annotation
manual in the next step.
6.4 Identification of coreference relation
Even though coreference relation is defined as IRA
relations, the lack of agreement on the granularity of
noun classes makes the agreement ratio worse. In
other words, it is crucial to decide how to annotate
abstract nouns in order to improve the annotation.
138
Annotators judge coreference relations as whether
or not abstract nouns refer to the same entity in the
world. However, the equivalence of the referents of
abstract nouns cannot be reconciled based on real-
world existence since by definition abstract nouns
have no physical entities in the real world.
As far as predicate-argument relation is con-
cerned, there might be a need for treating generic
entities in addition to specific entities as coreferen-
tial in some application. For example, one may want
to relate kids to children in sentence (17).
(17) We all want children to be fit and healthy.
However, the current invasion of fast food is
creating overweight and unhealthy kids.
The coreference relation between generic nouns are
missed in the current specification since we annotate
only IRA relations between specific nouns. Even
though there are various discussions in the area of
semantics, the issue of how to deal with generic
nouns as either coreferential or not in real texts is
still left open.
7 Conclusion
In this paper, we reported on the current specifica-
tion of our annotated corpus for coreference reso-
lution and predicate-argument analysis. Taking the
previous work of corpus annotation into account, we
decided to annotate predicate-argument relations by
ISA and IRA relations, and coreference relations ac-
cording to IRA relations. With the Kyoto Text Cor-
pus version 3.0 as a starting point, we built a large
annotated corpus. We also discussed the revelations
made from annotating our corpus, and discussed fu-
ture directions for refining our specifications of the
NAIST Text Corpus.
Acknowledgement
This work is partially supported by the Grant-in-Aid
for Scientific Research in Priority Areas JAPANESE
CORPUS (http://tokuteicorpus.jp).
References
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. Automatic content
extraction (ace) program - task definitions and performance
measures. In Proceedings of the 4rd International Confer-
ence on Language Resources and Evaluation (LREC-2004),
pages 837?840.
Charles J. Fillmore and Collin F. Baker. 2000. Framenet:
Frame semantics meets the corpus. In Proceedings of the
74th Annual Meeting of the Linguistic Society of America.
K. Hasida. 2005. Global document annotation (gda) annotation
manual. http://i-content.org/tagman.html.
L. Hirschman. 1997. MUC-7 coreference task definition. ver-
sion 3.0.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora reso-
lution by antecedent identification followed by anaphoricity
determination. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 4:417?434.
R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting syntac-
tic patterns as clues in zero-anaphora resolution. In Proced-
dings of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 625?632.
R. Jackendoff. 1990. Semantic Structures. Current Studies in
Linguistics 18. The MIT Press.
D. Kawahara, T. Kurohashi, and K. Hasida. 2002. Construc-
tion of a japanese relevance-tagged corpus (in japanese). In
Proceedings of the 8th Annual Meeting of the Association for
Natural Language Processing, pages 495?498.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-based con-
struction of a verb lexicon. In Proceedings of the 17th Na-
tional Conference on Artificial Intelligence and 12th Confer-
ence on on Innovative Applications of Artificial Intelligence,
pages 691?696.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska,
B. Young, and R. Grishman. 2004. The nombank project:
An interimreport. In Proceedings of the HLT-NAACL Work-
shop on Frontiers in Corpus Annotation.
Ruslan Mitkov. 2002. Anaphora Resolution. Studies in Lan-
guage and Linguistics. Pearson Education.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th ACL, pages 104?111.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposi-
tion bank: An annotated corpus of semantic roles. Compu-
tational Linguistics, 31(1):71?106.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceddings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 144?151.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
M. Tatu and D. Moldovan. 2006. A logic-based semantic ap-
proach to recognizing textual entailment. In Proceddings
of the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 819?826.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2006. Development and analysis of an exam-
ple database of japanese compound functional expressions.
IPSJ Journal, 47(6):1728?1741.
K. van Deemter and R. Kibble. 1999. What is coreference, and
what should coreference annotation be? In Proceedings of
the ACL ?99 Workshop on Coreference and its applications,
pages 90?96.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings of the 6th Message Understanding
Conference (MUC-6), pages 45?52.
139
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1259?1267,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Incorporating Extra-linguistic Information into Reference Resolution in
Collaborative Task Dialogue
Ryu Iida Shumpei Kobayashi Takenobu Tokunaga
Tokyo Institute of Technology
2-12-1, O?okayama, Meguro, Tokyo 152-8552, Japan
{ryu-i,skobayashi,take}@cl.cs.titech.ac.jp
Abstract
This paper proposes an approach to ref-
erence resolution in situated dialogues
by exploiting extra-linguistic information.
Recently, investigations of referential be-
haviours involved in situations in the real
world have received increasing attention
by researchers (Di Eugenio et al, 2000;
Byron, 2005; van Deemter, 2007; Spanger
et al, 2009). In order to create an accurate
reference resolution model, we need to
handle extra-linguistic information as well
as textual information examined by exist-
ing approaches (Soon et al, 2001; Ng and
Cardie, 2002, etc.). In this paper, we incor-
porate extra-linguistic information into an
existing corpus-based reference resolution
model, and investigate its effects on refer-
ence resolution problems within a corpus
of Japanese dialogues. The results demon-
strate that our proposed model achieves an
accuracy of 79.0% for this task.
1 Introduction
The task of identifying reference relations includ-
ing anaphora and coreferences within texts has re-
ceived a great deal of attention in natural language
processing, from both theoretical and empirical
perspectives. Recently, research trends for refer-
ence resolution have drastically shifted from hand-
crafted rule-based approaches to corpus-based ap-
proaches, due predominately to the growing suc-
cess of machine learning algorithms (such as Sup-
port Vector Machines (Vapnik, 1998)); many re-
searchers have examined ways for introducing var-
ious linguistic clues into machine learning-based
models (Ge et al, 1998; Soon et al, 2001; Ng
and Cardie, 2002; Yang et al, 2003; Iida et al,
2005; Yang et al, 2005; Yang et al, 2008; Poon
and Domingos, 2008, etc.). Research has contin-
ued to progress each year, focusing on tackling the
problem as it is represented in the annotated data
sets provided by the Message Understanding Con-
ference (MUC)1 and the Automatic Content Ex-
traction (ACE)2. In these data sets, coreference re-
lations are defined as a limited version of a typ-
ical coreference; this generally means that only
the relations where expressions refer to the same
named entities are addressed, because it makes
the coreference resolution task more information
extraction-oriented. In other words, the corefer-
ence task as defined by MUC and ACE is geared
toward only identifying coreference relations an-
chored to an entity within the text.
In contrast to this research trend, investigations
of referential behaviour in real world situations
have continued to gain interest in the language
generation community (Di Eugenio et al, 2000;
Byron, 2005; van Deemter, 2007; Foster et al,
2008; Spanger et al, 2009), aiming at applica-
tions such as human-robot interaction. Spanger
et al (2009) for example constructed a corpus by
recording dialogues of two participants collabo-
ratively solving the Tangram puzzle. The corpus
includes extra-lingustic information synchronised
with utterances (such as operations on the puzzle
pieces). They analysed the relations between re-
ferring expressions and the extra-linguistic infor-
mation, and reported that the pronominal usage of
referring expressions is predominant. They also
revealed that the multi-modal perspective of refer-
ence should be dealt with for more realistic refer-
ence understanding. Thus, a challenging issue in
reference resolution is to create a model bridging a
referring expression in the text and its object in the
real world. As a first step, this paper focuses on
incorporating extra-linguistic information into an
existing corpus-based approach, taking Spanger et
al. (2009)?s REX-J corpus3 as the data set. In our
1www-nlpir.nist.gov/related projects/muc/
2www.itl.nist.gov/iad/mig//tests/ace/
3The corpus was named REX-J after their publication of
1259
problem setting, a referent needs to be identified
by taking into account extra-linguistic informa-
tion, such as the spatiala relations of puzzle pieces
and the participants? operations on them, as well
as any preceding utterances in the dialogue. We
particularly focus on the participants? operation of
pieces and so introduce it as several features in a
machine learning-based approach.
This paper is organised as follows. We first ex-
plain the corpus of collaborative work dialogues
in Section 2, and then present our approach for
identifying a referent given a referring expres-
sion in situated dialogues in Section 3. Section 4
shows the results of our empirical evaluation.
In Section 5 we compare our work with exist-
ing work on reference resolution, and then con-
clude this paper and discuss future directions in
Section 6.
2 REX-J corpus: a corpus of
collaborative work dialogue
For investigating dialogue from the multi-modal
perspective, researchers have developed data sets
including extra-linguistic information, bridging
objects in the world and their referring expres-
sions. The COCONUT corpus (Di Eugenio et al,
2000) is collected from keyboard-dialogues be-
tween two participants, who are collaborating on
a simple 2D design task. The setting tends to en-
courage simple types of expressions by the partic-
ipants. The COCONUT corpus is also limited to
annotations with symbolic information about ob-
jects, such as object attributes and location in dis-
crete coordinates. Thus, in addition to the artifi-
cial nature of interaction, such as using keyboard
input, this corpus only records restricted types of
data.
On the other hand, though the annotated corpus
by Spanger et al (2009) focuses on a limited do-
main (i.e. collaborative work dialogues for solving
the Tangram puzzle using a puzzle simulator on
the computer), the required operations to solve the
puzzle, and the situation as it is updated by a series
of operations on the pieces are both recorded by
the simulator. The relationship between a referring
expression in a dialogue and its referent on a com-
puter display is also annotated. For this reason,
we selected the REX-J corpus for use in our em-
pirical evaluations on reference resolution. Before
explaining the details of our evaluation, we sketch
Spanger et al (2009), which describes its construction.
goal shape area
working area
Figure 1: Screenshot of the Tangram simulator
out the REX-J corpus and some of its prominent
statistics.
2.1 The REX-J corpus
In the process of building the REX-J corpus,
Spanger et al (2009) recruited 12 Japanese grad-
uate students (4 females and 8 males), and split
them into 6 pairs. All pairs knew each other previ-
ously and were of the same sex and approximately
the same age. Each pair was instructed to solve
the Tangram puzzle. The goal of the puzzle is to
construct a given shape by arranging seven pieces
of simple figures as shown in Figure 1. The pre-
cise position of every piece and every action that
the participants make are recorded by the Tangram
simulator in which the pieces on the computer dis-
play can be moved, rotated and flipped with sim-
ple mouse operations. The piece position and the
mouse actions were recorded at intervals of 10
msec. The simulator displays two areas: a goal
shape area (the left side of Figure 1) and a work-
ing area (the right side of Figure 1) where pieces
are shown and can be manipulated.
A different role was assigned to each participant
of a pair: a solver and an operator. Given a cer-
tain goal shape, the solver thinks of the necessary
arrangement of the pieces and gives instructions
to the operator for how to move them. The op-
erator manipulates the pieces with the mouse ac-
cording to the solver?s instructions. During this
interaction, frequent uttering of referring expres-
sions are needed to distinguish the pieces of the
puzzle. This collaboration is achieved by placing
a set of participants side by side, each with their
own display showing the work area, and a shield
screen set between them to prevent the operator
from seeing the goal shape, which is visible only
on the solver?s screen, and to further restrict their
1260
interaction to only speech.
2.2 Statistics
Table 1 lists the syntactic and semantic features of
the referring expressions in the corpus with their
respective frequencies. Note that multiple fea-
tures can be used in a single expression. This list
demonstrates that ?pronoun? and ?shape? features
are frequently uttered in the corpus. This is be-
cause pronominal expressions are often used for
pointing to a piece on a computer display. Expres-
sions representing ?shape? frequently appear in di-
alogues even though they may be relatively redun-
dant in the current utterance. From these statistics,
capturing these two features can be judged as cru-
cial as a first step toward accurate reference reso-
lution.
3 Reference Resolution using
Extra-linguistic Information
Before explaining the treatment of extra-linguistic
information, let us first describe the task defini-
tion, taking the REX-J corpus as target data. In
the task of reference resolution, the reference res-
olution model has to identify a referent (i.e. a
piece on a computer display)4. In comparison to
conventional problem settings for anaphora reso-
lution, where the model searches for an antecedent
out of a set of candidate antecedents from pre-
ceding utterances, expressions corresponding to
antecedents are sometimes omitted because refer-
ring expressions are used as deixis (i.e. physically
pointing to a piece on a computer display); they
may also refer to a piece that has just been manip-
ulated by an operator due to the temporal salience
in a series of operations. For these reasons, even
though the model checks all candidates in the pre-
ceding utterances, it may not find the antecedent
of a given referring expression. However, we do
know that each referent exists as a piece on the
display. We can therefore establish that when a re-
ferring expression is uttered by either a solver or
an operator, the model can choose one of seven
pieces as a referent of the current referring expres-
sion.
3.1 Ranking model to identify referents
To investigate the impact of extra-linguistic infor-
mation on reference resolution, we conduct an em-
4In the current task on reference resolution, we deal only
with referring expressions referring to a single piece to min-
imise complexity.
pirical evaluation in which a reference resolution
model chooses a referent (i.e. a piece) for a given
referring expression from the set of pieces illus-
trated on the computer display.
As a basis for our reference resolution model,
we adopt an existing model for reference res-
olution. Recently, machine learning-based ap-
proaches to reference resolution (Soon et al, 2001;
Ng and Cardie, 2002, etc.) have been developed,
particularly focussing on identifying anaphoric re-
lations in texts, and have achieved better perfor-
mance than hand-crafted rule-based approaches.
These models for reference resolution take into ac-
count linguistic factors, such as relative salience of
candidate antecedents, which have been modeled
in Centering Theory (Grosz et al, 1995) by rank-
ing candidate antecedents appearing in the preced-
ing discourse (Iida et al, 2003; Yang et al, 2003;
Denis and Baldridge, 2008). In order to take ad-
vantage of existing models, we adopt the ranking-
based approach as a basis for our reference resolu-
tion model.
In conventional ranking-based models, Yang et
al. (2003) and Iida et al (2003) decompose the
ranking process into a set of pairwise compar-
isons of two candidate antecedents. However, re-
cent work by Denis and Baldridge (2008) reports
that appropriately constructing a model for rank-
ing all candidates yields improved performance
over those utilising pairwise ranking.
Similarly we adopt a ranking-based model, in
which all candidate antecedents compete with
one another to decide the most likely candi-
date antecedent. Although the work by Denis
and Baldridge (2008) uses Maximum Entropy to
create their ranking-based model, we adopt the
Ranking SVM algorithm (Joachims, 2002), which
learns a weight vector to rank candidates for a
given partial ranking of each referent. Each train-
ing instance is created from the set of all referents
for each referring expression. To define the par-
tial ranking of referents, we simply rank referents
referred to by a given referring expression as first
place and other referents as second place.
3.2 Use of extra-linguistic information
Recent work on multi-modal reference resolution
or referring expression generation (Prasov and
Chai, 2008; Foster et al, 2008; Carletta et al,
2010) indicates that extra-linguistic information,
such as eye-gaze and manipulation of objects, is
1261
Table 1: Referring expressions in REX-J corpus
feature tokens example
demonstratives 742
adjective 194 ?ano migigawa no sankakkei (that triangle at the right side)?
pronoun 548 ?kore (this)?
attribute 795
size 223 ?tittyai sankakkei (the small triangle)?
shape 566 ?o?kii sankakkei (the large triangle)?
direction 6 ?ano sita muiteru dekai sankakkei (that large triangle facing to the bottom)?
spatial relations 147
projective 143 ?hidari no okkii sankakkei (the small triangle on the left)?
topological 2 ?o?kii hanareteiru yatu (the big distant one)?
overlapping 2 ? sono sita ni aru sankakkei (the triangle underneath it)?
action-mentioning 85 ?migi ue ni doketa sankakkei (the triangle you put away to the top right)?
one of essential clues for distinguishing deictic
reference from endophoric reference.
For instance, Prasov and Chai (2008) demon-
strated that integrating eye-gaze information (es-
pecially, relative fixation intensity, the amount of
time spent fixating a candidate object) into the
conventional dialogue history-based model im-
proved the performance of reference resolution.
Foster et al (2008) investigated the relationship of
referring expressions and the manupluation of ob-
jects on a collaborative construction task, which
is similar to our Tangram task5. They reported
about 36% of the initial mentioned referring ex-
pressions in their corpus were involved with par-
ticipant?s operations of objects, such as mouse ma-
nipulation.
From these background, in addition to the in-
formation about the history of the preceding dis-
course, which has been used in previous machine
learning-based approaches, we integrate extra-
linguistic information into the reference resolution
model shown in Section 3.1. More precisely, we
introduce the following extra-linguistic informa-
tion: the information with regards to the history
of a piece?s movement and the mouse cursor po-
sitions, and the information of the piece currently
manipulated by an operator. We next elaborate on
these three kinds of features. All the features are
summarised in Table 2.
3.2.1 Discourse history features
First, ?type of? features are acquired from the ex-
pressions of a given referring expression and its
antecedent in the preceding discourse if the an-
5Note that the task defined in Foster et al (2008) makes no
distinction between two roles; a operator and a solver. Thus,
two partipants both can mamipulate pieces on a computer dis-
play, but need to jointly construct to create a predefined goal
shape.
tecedent explicitly appears. These features have
been examined by approaches to anaphora or
coreference resolution (Soon et al, 2001; Ng and
Cardie, 2002, etc.) to capture the salience of a can-
didate antecedent. To capture the textual aspect
of dialogues for solving Tangram puzzle, we ex-
ploit the features such as a binary value indicating
whether a referring expression has no antecedent
in the preceding discourse and case markers fol-
lowing a candidate antecedent.
3.2.2 Action history features
The history of the operations may yield important
clues that indicate the salience in terms of the tem-
poral recency of a piece within a series of opera-
tions. To introduce this aspect as a set of features,
we can use, for example, the time distance of a
candidate referent (i.e. a piece in the Tangram puz-
zle) since the mouse cursor was moved over it. We
call this type of feature the action history feature.
3.2.3 Current operation features
The recency of operations of a piece is also an im-
portant factor on reference resolution because it is
directly associated with the focus of attention in
terms of the cognition in a series of operations.
For example, since a piece which was most re-
cently manipulated is most salient from cognitive
perspectives, it might be expected that the piece
tends to be referred to by unmarked referring ex-
pressions such as pronouns. To incorporate such
clues into the reference resolution model, we can
use, for example, the time distance of a candidate
referent since it was last manipulated in the pre-
ceding utterances. We call this type of feature the
current operation feature.
1262
Table 2: Feature set
(a) Discourse history features
DH1 : yes, no a binary value indicating that P is referred to by the most recent referring expression.
DH2 : yes, no a binary value indicating that the time distance to the last mention of P is less than or equal to 10 sec.
DH3 : yes, no a binary value indicating that the time distance to the last mention of P is more than 10 sec and less
than or equal to 20 sec.
DH4 : yes, no a binary value indicating that the time distance to the last mention of P is more than 20 sec.
DH5 : yes, no a binary value indicating that P has never been referred to by any mentions in the preceding utterances.
DH6 : yes, no, N/A a binary value indicating that the attributes of P are compatible with the attributes of R.
DH7 : yes, no a binary value indicating that R is followed by the case marker ?o (accusative)?.
DH8 : yes, no a binary value indicating that R is followed by the case marker ?ni (dative)?.
DH9 : yes, no a binary value indicating that R is a pronoun and the most recent reference to P is not a pronoun.
DH10 : yes, no a binary value indicating that R is not a pronoun and was most recently referred to by a pronoun.
(b) Action history features
AH1 : yes, no a binary value indicating that the mouse cursor was over P at the beginning of uttering R.
AH2 : yes, no a binary value indicating that P is the last piece that the mouse cursor was over when feature AH1 is
?no?.
AH3 : yes, no a binary value indicating that the time distance is less than or equal to 10 sec after the mouse cursor
was over P.
AH4 : yes, no a binary value indicating that the time distance is more than 10 sec and less than or equal to 20 sec
after the mouse cursor was over P.
AH5 : yes, no a binary value indicating that the time distance is more than 20 sec after the mouse cursor was over P.
AH6 : yes, no a binary value indicating that the mouse cursor was never over P in the preceding utterances.
(c) Current operation features
CO1 : yes, no a binary value indicating that P is being manipulated at the beginning of uttering R.
CO2 : yes, no a binary value indicating that P is the most recently manipulated piece when feature CO1 is ?no?.
CO3 : yes, no a binary value indicating that the time distance is less than or equal to 10 sec after P was most recently
manipulated.
CO4 : yes, no a binary value indicating that the time distance is more than 10 sec and less than or equal to 20 sec
after P was most recently manipulated.
CO5 : yes, no a binary value indicating that the time distance is more than 20 sec after P was most recently manipu-
lated.
CO6 : yes, no a binary value indicating that P has never been manipulated.
P stands for a piece of the Tangram puzzle (i.e. a candidate referent of a referring expression) and R stands for the target
referring expression.
4 Empirical Evaluation
In order to investigate the effect of the extra-
linguistic information introduced in this paper, we
conduct an empirical evaluation using the REX-J
corpus.
4.1 Models
As we see in Section 2.2, the feature testing
whether a referring expression is a pronoun or
not is crucial because it is directly related to the
?deictic? usage of referring expressions, whereas
other expressions tend to refer to an expression ap-
pearing in the preceding utterances. As described
in Denis and Baldridge (2008), when the size of
training instances is relatively small, the models
induced by learning algorithms (e.g. SVM) should
be separately created with regards to distinct fea-
tures. Therefore, focusing on the difference of
the pronominal usage of referring expressions, we
separately create the reference resolution models;
one is for identifying a referent of a given pro-
noun, and the other is for all other expressions.
We henceforth call the former model the pronoun
model and the latter one the non-pronoun model
respectively. At the training phase, we use only
training instances whose referring expressions are
pronouns for creating the pronoun model, and
all other training instances are used for the non-
pronoun model. The model using one of these
models depending on the referring expression to
be solved is called the separate model.
To verify Denis and Baldridge (2008)?s premise
mentioned above, we also create a model using all
training instances without dividing pronouns and
other. This model is called the combined model
hereafter.
4.2 Experimental setting
We used 40 dialogues in the REX-J corpus6, con-
taining 2,048 referring expressions. To facilitate
the experiments, we conduct 10-fold crossvalida-
tion using 2,035 referring expressions, each of
which refers to a single piece in a computer dis-
6Spanger et al (2009)?s original corpus contains only 24
dialogues. In addition to this, we obtained anothor 16 dia-
logues by favour of the authors.
1263
Table 3: Results on reference resolution: accuracy
model discourse history +action history* +current operation +action history,
(baseline) +current operation*
separated model (a+b) 0.664 (1352/2035) 0.790 (1608/2035) 0.685 (1394/2035) 0.780 (1587/2035)
a) pronoun model 0.648 (660/1018) 0.886 (902/1018) 0.692 (704/1018) 0.875 (891/1018)
b) non-pronoun model 0.680 (692/1017) 0.694 (706/1017) 0.678 (690/1017) 0.684 (696/1017)
combined model 0.664 (1352/2035) 0.749 (1524/2035) 0.650 (1322/2035) 0.743 (1513/2035)
?*? means the extra-lingustic features (or the combinations of them) significantly contribute to improving performance. For the
significant tests, we used McNemar test with Bonferroni?s correction for multiple comparisons, i.e. ?/K = 0.05/4 = 0.01.
play7.
As a baseline model, we adopted a model only
using the discourse history features. We utilised
SVMrank8 as an implementation of the Ranking
SVM algorithm, in which the parameter c was set
as 1.0 and the remaining parameters were set to
their defaults.
4.3 Results
The results of each model are shown in Table 3.
First of all, by comparing the models with and
without extra-linguistic information (i.e. the
model using all features shown in Table 2 and
the baseline model), we can see the effectiveness
of extra-linguistic information. The results typi-
cally show that the former achieved better perfor-
mance than the latter. In particular, it indicates that
exploiting the action history features are signifi-
cantly useful for reference resolution in this data
set.
Second, we can also see the impact of extra-
linguistic information (especially, the action his-
tory features) with regards to the pronoun and
non-pronoun models. In the former case, the
model with extra-linguistic information improved
by about 22% compared with the baseline model.
On the other hand, in the latter case, the accuracy
improved by only 7% over the baseline model.
The difference may be caused by the fact that pro-
nouns are more sensitive to the usage of the ac-
tion history features because pronouns are often
uttered as deixis (i.e. a pronoun tends to directly
refer to a piece shown in a computer display).
The results also show that the model using
the discourse history and action history features
achieved better performance than the model using
all the features. This may be due to the duplicated
definitions between the action history and current
7The remaining 13 instances referred to either more than
one piece or a class of pieces, thus were excluded in this ex-
periment.
8www.cs.cornell.edu/people/tj/svm light/svm rank.html
Table 4: Weights of the features in each model
pronoun model non-pronoun model
rank feature weight feature weight
1 AH1 0.6371 DH6 0.7060
2 AH3 0.2721 DH2 0.2271
3 DH1 0.2239 AH3 0.2035
4 DH2 0.2191 AH1 0.1839
5 CO1 0.1911 DH1 0.1573
6 DH9 0.1055 DH7 0.0669
7 AH2 0.0988 CO5 0.0433
8 CO3 0.0852 CO3 0.0393
9 DH6 0.0314 CO1 0.0324
10 CO2 0.0249 DH3 0.0177
11 DH10 0 AH4 0.0079
12 DH7 -0.0011 AH2 0.0069
13 DH3 -0.0088 CO4 0.0059
14 CO6 -0.0228 DH10 0.0059
15 CO4 -0.0308 DH9 0
16 CO5 -0.0317 CO2 -0.0167
17 DH8 -0.0371 DH8 -0.0728
18 AH6 -0.0600 CO6 -0.0885
19 AH4 -0.0761 DH4 -0.0924
20 DH5 -0.0910 AH5 -0.1042
21 DH4 -0.1193 AH6 -0.1072
22 AH5 -0.1361 DH5 -0.1524
operation features. As we can see in the feature
definitions of CO1 and AH1, some current opera-
tion features partially overlap with the action his-
tory features, which is effectively used in the rank-
ing process. However, the other current operation
features may have bad effects for ranking refer-
ents due to their ill-formed definitions. To shed
light on this problem, we need additional investi-
gation of the usage of features, and to refine their
definitions.
Finally, the results show that the performance
of the separated model is significantly better than
that of the combined model9, which indicates that
separately creating models to specialise in distinct
factors (i.e. whether a referring expression is a
pronoun or not) is important as suggested by Denis
and Baldridge (2008).
We next investigated the significance of each
9For the significant tests, we used McNemar test (? =
0.05).
1264
Table 5: Frequencies of REs relating to on-mouse
pronouns others total
# all REs 548 693 1,241
# on-mouse 452 155 607
(82.5%) (22.4%) (48.9%)
?# all REs? stands for the frequency of referring expressions
uttered in the corpus and ?# on-mouse? is the frequency of re-
ferring expressions in the situation when a referring expres-
sion is uttered and a mouse cursor is over the piece referred
to by the expression.
feature of the pronoun and non-pronoun models.
We calculate the weight of feature f shown in
Table 2 according to the following formula.
weight(f) =
?
x?SV s
wxzx(f) (1)
where SVs is a set of the support vectors in a ranker
induced by SVMrank, wx is the weight of the sup-
port vector x, zx(f) is the function that returns 1
if f occurs in x, respectively.
The feature weights are shown in Table 4. This
demonstrates that in the pronoun model the ac-
tion history features have the highest weight, while
with the non-pronoun model these features are less
significant. As we can see in Table 5, pronouns
are strongly related to the situation where a mouse
cursor is over a piece, directly causing the weights
of the features associated with the ?on-mouse? sit-
uation to become higher than other features.
On the other hand, in the non-pronoun model,
the discourse history features, such as DH6 and
DH2, are the most significant, indicating that the
compatibility of the attributes of a piece and a re-
ferring expression is more crucial than other ac-
tion history and current operation features. This is
compatible with the previous research concerning
textual reference resolution (Mitkov, 2002).
Table 4 shows that feature AH3 (aiming at cap-
turing the recency in terms of a series of oper-
ations) is also significant. It empirically proves
that the recent operation is strongly related to the
salience of reference as a kind of ?focus? by hu-
mans.
5 Related Work
There have been increasing concerns about ref-
erence resolution in dialogue. Byron and Allen
(1998) and Eckert and Strube (2000) reported
about 50% of pronouns had no antecedent in
TRAINS93 and Switchboard corpora respectively.
Strube and Mu?ller (2003) attempted to resolve
pronominal anaphora in the Switchboard corpus
by porting a corpus-based anaphora resolution
model focusing on written texts (e.g. Soon et al
(2001) and Ng and Cardie (2002)). They used
specialised features for spoken dialogues as well
as conventional features. They reported relatively
worse results than with written texts. The reason
is that the features in their work capture only in-
formation derived from transcripts of dialogues,
while it is also essential to bridge objects and con-
cepts in the real (or virtual) world and their expres-
sions (especially pronouns) for recognising refer-
ential relations intrinsically.
To improve performance on reference resolu-
tion in dialogue, researchers have focused on
anaphoricity determination, which is the task of
judging whether an expression explicitly has an
antecedent in the text (i.e. in the preceding ut-
terances) (Mu?ller, 2006; Mu?ller, 2007). Their
work presented implementations of pronominal
reference resolution in transcribed, multi-party di-
alogues. Mu?ller (2006) focused on the determina-
tion of non-referential it, categorising instances of
it in the ICSI Meeting Corpus (Janin et al, 2003)
into six classes in terms of their grammatical cat-
egories. They also took into account each charac-
teristic of these types by using a refined feature set.
In the work by Mu?ller (2007), they conducted an
empirical evaluation including antecedent identifi-
cation as well as anaphoricity determination. They
used the relative frequencies of linguistic patterns
as clues to introduce specific patterns for non-
referentials. They reported that their performance
for detecting non-referentials was relatively high
(80.0% in precision and 60.9% in recall), while
the overall performance was still low (18.2% in
precision and 19.1% in recall). These results indi-
cate the need for advancing research in reference
resolution in dialogue.
In contrast to the above mentioned research, our
task includes the treatment of entity disambigua-
tion (i.e. selecting a referent out of a set of pieces
on a computer display) as well as conventional
anaphora resolution. Although our task setting is
limited to the problem of solving the Tangram puz-
zle, we believe it is a good starting point for incor-
porating real (or virtual) world entities into coven-
tional anaphora resolution.
1265
6 Conclusion
This paper presented the task of reference reso-
lution bridging pieces in the real world and their
referents in dialogue. We presented an imple-
mentation of a reference resolution model ex-
ploiting extra-linguistic information, such as ac-
tion history and current operation features, to cap-
ture the salience of operations by a participant
and the arrangement of the pieces. Through our
empirical evaluation, we demonstrated that the
extra-linguistic information introduced in this pa-
per contributed to improving performance. We
also analysed the effect of each feature, showing
that while action history features were useful for
pronominal reference, discourse history features
made sense for the other references.
In order to enhance this kind of reference res-
olution, there are several possible future direc-
tions. First, in the current problem setting, we
exclude zero-anaphora (i.e. omitted expressions
refer to either an expression in the previous utter-
ances or an object on a display deictically). How-
ever, zero-anaphora is essential for precise mod-
eling and recognition of reference because it is
also directly related with the recency of referents,
either textually or situationally. Second, repre-
senting distractors in a reference resolution model
is also a key. Although, this paper presents an
implementation of a reference model considering
only the relationship between a referring expres-
sion and its candidate referents. However, there
might be cases when the occurrence of expressions
or manipulated pieces intervening between a refer-
ring expression and its referent need to be taken
into account. Finally, more investigation is needed
for considering other extra-linguistic information,
such as eye-gaze, for exploring what kinds of in-
formation is critical to recognising reference in di-
alogue.
References
D. K. Byron and J. F. Allen. 1998. Resolving demon-
strative pronouns in the trains93 corpus. In Proceed-
ings of the 2nd Colloquium on Discourse Anaphora
and Anaphor Resolution (DAARC2), pages 68?81.
D. K. Byron. 2005. Utilizing visual attention for
cross-model coreference interpretation. In CON-
TEXT 2005, pages 83?96.
J. Carletta, R. L. Hill, C. Nicol, T. Taylor, J. P.
de Ruiter, and E. G. Bard. 2010. Eyetracking
for two-person tasks with manipulation of a virtual
world. Behavior Research Methods, 42:254?265.
P. Denis and J. Baldridge. 2008. Specialized models
and ranking for coreference resolution. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 660?669.
B. P. W. Di Eugenio, R. H. Thomason, and J. D. Moore.
2000. The agreement process: An empirical investi-
gation of human-human computer-mediated collab-
orative dialogues. International Journal of Human-
Computer Studies, 53(6):1017?1076.
M. Eckert and M. Strube. 2000. Dialogue acts, syn-
chronising units and anaphora resolution. Journal
of Semantics, 17(1):51?89.
M. E. Foster, E. G. Bard, M. Guhe, R. L. Hill, J. Ober-
lander, and A. Knoll. 2008. The roles of haptic-
ostensive referring expressions in cooperative, task-
based human-robot dialogue. In Proceedings of the
3rd ACM/IEEE international conference on Human
robot interaction (HRI ?08), pages 295?302.
N. Ge, J. Hale, and E. Charniak. 1998. A statistical ap-
proach to anaphora resolution. In Proceedings of the
6th Workshop on Very Large Corpora, pages 161?
170.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203?226.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto.
2003. Incorporating contextual cues in trainable
models for coreference resolution. In Proceedings
of the 10th EACL Workshop on The Computational
Treatment of Anaphora, pages 23?30.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora
resolution by antecedent identification followed by
anaphoricity determination. ACM Transactions on
Asian Language Information Processing (TALIP),
4(4):417?434.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting cor-
pus. In Proceedings of the IEEE International Con-
ference on Acoustics, Speech and Signal Processing,
pages 364?367.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 133?142.
R. Mitkov. 2002. Anaphora Resolution. Studies in
Language and Linguistics. Pearson Education.
C. Mu?ller. 2006. Automatic detection of nonrefer-
ential It in spoken multi-party dialog. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 49?56.
1266
C. Mu?ller. 2007. Resolving It, This, and That in un-
restricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 816?823.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
104?111.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with Markov Logic. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 650?
659.
Z. Prasov and J. Y. Chai. 2008. What?s in a gaze?:
the role of eye-gaze in reference resolution in mul-
timodal conversational interfaces. In Proceedings of
the 13th international conference on Intelligent user
interfaces (IUI ?08), pages 20?29.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521?544.
P. Spanger, Y. Masaaki, R. Iida, and T. Takenobu.
2009. Using extra linguistic information for gen-
erating demonstrative pronouns in a situated collab-
oration task. In Proceedings of Workshop on Pro-
duction of Referring Expressions: Bridging the gap
between computational and empirical approaches to
reference.
M. Strube and C. Mu?ller. 2003. A machine learning
approach to pronoun resolution in spoken dialogue.
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, pages 168?
175.
K. van Deemter. 2007. TUNA: Towards a unified al-
gorithm for the generation of referring expressions.
Technical report, Aberdeen University.
V. N. Vapnik. 1998. Statistical Learning Theory.
Adaptive and Learning Systems for Signal Process-
ing Communications, and control. John Wiley &
Sons.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003.
Coreference resolution using competition learning
approach. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 176?183.
X. Yang, J. Su, and C. L. Tan. 2005. Improving pro-
noun resolution using statistics-based semantic com-
patibility information. In Proceeding of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 165?172.
X. Yang, J. Su, J. Lang, C. L. Tan, T. Liu, and S. Li.
2008. An entity-mention model for coreference
resolution with inductive logic programming. In
Proceedings of Annual Meeting of the Association
for Computational Linguistics (ACL): Human Lan-
guage Technologies (HLT), pages 843?851.
1267
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 804?813,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Cross-Lingual ILP Solution to Zero Anaphora Resolution
Ryu Iida
Tokyo Institute of Technology
2-12-1, ?Ookayama, Meguro,
Tokyo 152-8552, Japan
ryu-i@cl.cs.titech.ac.jp
Massimo Poesio
Universita` di Trento,
Center for Mind / Brain Sciences
University of Essex,
Language and Computation Group
massimo.poesio@unitn.it
Abstract
We present an ILP-based model of zero
anaphora detection and resolution that builds
on the joint determination of anaphoricity and
coreference model proposed by Denis and
Baldridge (2007), but revises it and extends it
into a three-way ILP problem also incorporat-
ing subject detection. We show that this new
model outperforms several baselines and com-
peting models, as well as a direct translation of
the Denis / Baldridge model, for both Italian
and Japanese zero anaphora. We incorporate
our model in complete anaphoric resolvers for
both Italian and Japanese, showing that our
approach leads to improved performance also
when not used in isolation, provided that sep-
arate classifiers are used for zeros and for ex-
plicitly realized anaphors.
1 Introduction
In so-called ?pro-drop? languages such as Japanese
and many romance languages including Italian,
phonetic realization is not required for anaphoric
references in contexts in which in English non-
contrastive pronouns are used: e.g., the subjects of
Italian and Japanese translations of buy in (1b) and
(1c) are not explicitly realized. We call these non-
realized mandatory arguments zero anaphors.
(1) a. [EN] [John]
i
went to visit some friends. On
the way, [he]
i
bought some wine.
b. [IT] [Giovanni]
i
ando` a far visita a degli am-
ici. Per via, ?
i
compro` del vino.
c. [JA] [John]
i
-wa yujin-o houmon-sita.
Tochu-de ?
i
wain-o ka-tta.
The felicitousness of zero anaphoric reference
depends on the referred entity being sufficiently
salient, hence this type of data?particularly in
Japanese and Italian?played a key role in early
work in coreference resolution, e.g., in the devel-
opment of Centering (Kameyama, 1985; Walker et
al., 1994; Di Eugenio, 1998). This research high-
lighted both commonalities and differences between
the phenomenon in such languages. Zero anaphora
resolution has remained a very active area of study
for researchers working on Japanese, because of the
prevalence of zeros in such languages1 (Seki et al,
2002; Isozaki and Hirao, 2003; Iida et al, 2007a;
Taira et al, 2008; Imamura et al, 2009; Sasano et
al., 2009; Taira et al, 2010). But now the availabil-
ity of corpora annotated to study anaphora, includ-
ing zero anaphora, in languages such as Italian (e.g.,
Rodriguez et al (2010)), and their use in competi-
tions such as SEMEVAL 2010 Task 1 on Multilin-
gual Coreference (Recasens et al, 2010), is lead-
ing to a renewed interest in zero anaphora resolu-
tion, particularly at the light of the mediocre results
obtained on zero anaphors by most systems partici-
pating in SEMEVAL.
Resolving zero anaphora requires the simulta-
neous decision that one of the arguments of a
verb is phonetically unrealized (and which argu-
ment exactly?in this paper, we will only be con-
cerned with subject zeros as these are the only
type to occur in Italian) and that a particular en-
tity is its antecedent. It is therefore natural to
view zero anaphora resolution as a joint inference
1As shown in Table 1, 64.3% of anaphors in the NAIST Text
Corpus of Anaphora are zeros.
804
task, for which Integer Linear Programming (ILP)?
introduced to NLP by Roth and Yih (2004) and suc-
cessfully applied by Denis and Baldridge (2007) to
the task of jointly inferring anaphoricity and deter-
mining the antecedent?would be appropriate.
In this work we developed, starting from the ILP
system proposed by Denis and Baldridge, an ILP
approach to zero anaphora detection and resolu-
tion that integrates (revised) versions of Denis and
Baldridge?s constraints with additional constraints
between the values of three distinct classifiers, one
of which is a novel one for subject prediction. We
demonstrate that treating zero anaphora resolution
as a three-way inference problem is successful for
both Italian and Japanese. We integrate the zero
anaphora resolver with a coreference resolver and
demonstrate that the approach leads to improved re-
sults for both Italian and Japanese.
The rest of the paper is organized as follows.
Section 2 briefly summarizes the approach proposed
by Denis and Baldridge (2007). We next present our
new ILP formulation in Section 3. In Section 4 we
show the experimental results with zero anaphora
only. In Section 5 we discuss experiments testing
that adding our zero anaphora detector and resolver
to a full coreference resolver would result in overall
increase in performance. We conclude and discuss
future work in Section 7.
2 Using ILP for joint anaphoricity and
coreference determination
Integer Linear Programming (ILP) is a method for
constraint-based inference aimed at finding the val-
ues for a set of variables that maximize a (linear) ob-
jective function while satisfying a number of con-
straints. Roth and Yih (2004) advocated ILP as a
general solution for a number of NLP tasks that re-
quire combining multiple classifiers and which the
traditional pipeline architecture is not appropriate,
such as entity disambiguation and relation extrac-
tion.
Denis and Baldridge (2007) defined the following
object function for the joint anaphoricity and coref-
erence determination problem.
min
?
?i,j??P
cC
?i,j?
? x
?i,j?
+ c?C
?i,j?
? (1 ? x
?i,j?
)
+
?
j?M
cA
j
? y
j
+ c?A
j
? (1 ? y
j
) (2)
subject to
x
?i,j?
? {0, 1} ??i, j? ? P
y
j
? {0, 1} ?j ? M
M stands for the set of mentions in the document,
and P the set of possible coreference links over these
mentions. x
?i,j?
is an indicator variable that is set to
1 if mentions i and j are coreferent, and 0 otherwise.
y
j
is an indicator variable that is set to 1 if mention
j is anaphoric, and 0 otherwise. The costs cC
?i,j?
=
?log(P (COREF|i, j)) are (logs of) probabilities
produced by an antecedent identification classifier
with ?log, whereas cA
j
= ?log(P (ANAPH|j)),
are the probabilities produced by an anaphoricity de-
termination classifier with ?log. In the Denis &
Baldridge model, the search for a solution to an-
tecedent identification and anaphoricity determina-
tion is guided by the following three constraints.
Resolve only anaphors: if a pair of mentions ?i, j?
is coreferent (x
?i,j?
= 1), then mention j must be
anaphoric (y
j
= 1).
x
?i,j?
? y
j
??i, j? ? P (3)
Resolve anaphors: if a mention is anaphoric (y
j
=
1), it must be coreferent with at least one antecedent.
y
j
?
?
i?M
j
x
?i,j?
?j ? M (4)
Do not resolve non-anaphors: if a mention is non-
anaphoric (y
j
= 0), it should have no antecedents.
y
j
?
1
|M
j
|
?
i?M
j
x
?i,j?
?j ? M (5)
3 An ILP-based account of zero anaphora
detection and resolution
In the corpora used in our experiments, zero
anaphora is annotated using as markable the first
verbal form (not necessarily the head) following the
position where the argument would have been real-
ized, as in the following example.
805
(6) [Pahor]
i
e` nato a Trieste, allora porto princi-
pale dell?Impero Austro-Ungarico.
A sette anni [vide]
i
l?incendio del Narodni
dom,
The proposal of Denis and Baldridge (2007) can be
easily turned into a proposal for the task of detecting
and resolving zero anaphora in this type of data by
reinterpreting the indicator variables as follows:
? y
j
is 1 if markable j (a verbal form) initiates a
verbal complex whose subject is unrealized, 0
otherwise;
? x
?i,j?
is 1 if the empty mention realizing the
subject argument of markable j and markable
i are mentions of the same entity, 0 otherwise.
There are however a number of ways in which this
direct adaptation can be modified and extended. We
discuss them in turn.
3.1 Best First
In the context of zero anaphora resolution, the ?Do
not resolve non-anaphors? constraint (5) is too weak,
as it allows the redundant choice of more than one
candidate antecedent. We developed therefore the
following alternative, that blocks selection of more
than one antecedent.
Best First (BF):
y
j
?
?
i?M
j
x
?i,j?
?j ? M (7)
3.2 A subject detection model
The greatest difficulty in zero anaphora resolution
in comparison to, say, pronoun resolution, is zero
anaphora detection. Simply relying for this on the
parser is not enough: most dependency parsers are
not very accurate at identifying cases in which the
verb does not have a subject on syntactic grounds
only. Again, it seems reasonable to suppose this
is because zero anaphora detection requires a com-
bination of syntactic information and information
about the current context. Within the ILP frame-
work, this hypothesis can be implemented by turn-
ing the zero anaphora resolution optimization prob-
lem into one with three indicator variables, with the
objective function in (8). The third variable, z
j
, en-
codes the information provided by the parser: it is
1 with cost cS
j
= ?log(P (SUBJ |j)) if the parser
thinks that verb j has an explicit subject with proba-
bility P (SUBJ |j), otherwise it is 0.
min
?
?i,j??P
cC
?i,j?
? x
?i,j?
+ c?C
?i,j?
? (1 ? x
?i,j?
)
+
?
j?M
cA
j
? y
j
+ c?A
j
? (1 ? y
j
)
+
?
j?M
cS
j
? z
j
+ c?S
j
? (1 ? z
j
) (8)
subject to
x
?i,j?
? {0, 1} ??i, j? ? P
y
j
? {0, 1} ?j ? M
z
j
? {0, 1} ?j ? M
The crucial fact about the relation between z
j
and
y
j
is that a verb has either a syntactically realized NP
or a zero pronoun as a subject, but not both. This is
encoded by the following constraint.
Resolve only non-subjects: if a predicate j syntac-
tically depends on a subject (z
j
= 1), then the predi-
cate j should have no antecedents of its subject zero
pronoun.
y
j
+ z
j
? 1 ?j ? M (9)
4 Experiment 1: zero anaphora resolution
In a first round of experiments, we evaluated the per-
formance of the model proposed in Section 3 on zero
anaphora only (i.e., not attempting to resolve other
types of anaphoric expressions).
4.1 Data sets
We use the two data sets summarized in Table 1.
The table shows that NP anaphora occurs more fre-
quently than zero-anaphora in Italian, whereas in
Japanese the frequency of anaphoric zero-anaphors2
is almost double the frequency of the remaining
anaphoric expressions.
Italian For Italian coreference, we used the anno-
tated data set presented in Rodriguez et al (2010)
and developed for the Semeval 2010 task ?Corefer-
ence Resolution in Multiple Languages? (Recasens
et al, 2010), where both zero-anaphora and NP
2In Japanese, like in Italian, zero anaphors are often used
non-anaphorically, to refer to situationally introduced entities,
as in I went to John?s office, but they told me that he had left.
806
#instances (anaphoric/total)
language type #docs #sentences #words zero-anaphors others all
Italian train 97 3,294 98,304 1,093 / 1,160 6,747 / 27,187 7,840 / 28,347
test 46 1,478 41,587 792 / 837 3,058 / 11,880 3,850 / 12,717
Japanese train 1,753 24,263 651,986 18,526 / 29,544 10,206 / 161,124 28,732 / 190,668
test 696 9,287 250,901 7,877 / 11,205 4,396 / 61,652 12,273 / 72,857
In the 6th column we use the term ?anaphoric? to indicate the number of zero anaphors that have an antecedent in
the text, whereas the total figure is the sum of anaphoric and exophoric zero-anaphors - zeros with a vague / generic
reference.
Table 1: Italian and Japanese Data Sets
coreference are annotated. This dataset consists
of articles from Italian Wikipedia, tokenized, POS-
tagged and morphologically analyzed using TextPro,
a freely available Italian pipeline (Pianta et al,
2008). We parsed the corpus using the Italian ver-
sion of the DESR dependency parser (Attardi et al,
2007).
In Italian, zero pronouns may only occur as omit-
ted subjects of verbs. Therefore, in the task of
zero-anaphora resolution all verbs appearing in a
text are considered candidates for zero pronouns,
and all gold mentions or system mentions preced-
ing a candidate zero pronoun are considered as can-
didate antecedents. (In contrast, in the experiments
on coreference resolution discussed in the following
section, all mentions are considered as both candi-
date anaphors and candidate antecedents. To com-
pare the results with gold mentions and with system
detected mentions, we carried out an evaluation us-
ing the mentions automatically detected by the Ital-
ian version of the BART system (I-BART) (Poesio
et al, 2010), which is freely downloadable.3
Japanese For Japanese coreference we used the
NAIST Text Corpus (Iida et al, 2007b) version
1.4?, which contains the annotated data about NP
coreference and zero-anaphoric relations. We also
used the Kyoto University Text Corpus4 that pro-
vides dependency relations information for the same
articles as the NAIST Text Corpus. In addition, we
also used a Japanese named entity tagger, CaboCha5
for automatically tagging named entity labels. In
the NAIST Text Corpus mention boundaries are not
annotated, only the heads. Thus, we considered
3http://www.bart-coref.org/
4http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/corpus.html
5http://chasen.org?taku/software/cabocha/
as pseudo-mentions all bunsetsu chunks (i.e. base
phrases in Japanese) whose head part-of-speech was
automatically tagged by the Japanese morphologi-
cal analyser Chasen6 as either ?noun? or ?unknown
word? according to the NAIST-jdic dictionary.7
For evaluation, articles published from January
1st to January 11th and the editorials from January
to August were used for training and articles dated
January 14th to 17th and editorials dated October
to December are used for testing as done by Taira
et al (2008) and Imamura et al (2009). Further-
more, in the experiments we only considered subject
zero pronouns for a fair comparison to Italian zero-
anaphora.
4.2 Models
In these first experiments we compared the three
ILP-based models discussed in Section 3: the direct
reimplementation of the Denis and Baldridge pro-
posal (i.e., using the same constrains), a version re-
placing Do-Not-Resolve-Not-Anaphors with Best-
First, and a version with Subject Detection as well.
As discussed by Iida et al (2007a) and Imamura
et al (2009), useful features in intra-sentential zero-
anaphora are different from ones in inter-sentential
zero-anaphora because in the former problem syn-
tactic information between a zero pronoun and its
candidate antecedent is essential, while the lat-
ter needs to capture the significance of saliency
based on Centering Theory (Grosz et al, 1995).
To directly reflect this difference, we created two
antecedent identification models; one for intra-
sentential zero-anaphora, induced using the training
instances which a zero pronoun and its candidate an-
tecedent appear in the same sentences, the other for
6http://chasen-legacy.sourceforge.jp/
7http://sourceforge.jp/projects/naist-jdic/
807
inter-sentential cases, induced from the remaining
training instances.
To estimate the feature weights of each classifier,
we used MEGAM8, an implementation of the Max-
imum Entropy model, with default parameter set-
tings. The ILP-based models were compared with
the following baselines.
PAIRWISE: as in the work by Soon et al (2001),
antecedent identification and anaphoricity determi-
nation are simultaneously executed by a single clas-
sifier.
DS-CASCADE: the model first filters out non-
anaphoric candidate anaphors using an anaphoric-
ity determination model, then selects an antecedent
from a set of candidate antecedents of anaphoric
candidate anaphors using an antecedent identifica-
tion model.
4.3 Features
The feature sets for antecedent identification and
anaphoricity determination are briefly summarized
in Table 2 and Table 3, respectively. The agreement
features such as NUM AGREE and GEN AGREE are
automatically derived using TextPro. Such agree-
ment features are not available in Japanese because
Japanese words do not contain such information.
4.4 Creating subject detection models
To create a subject detection model for Italian, we
used the TUT corpus9 (Bosco et al, 2010), which
contains manually annotated dependency relations
and their labels, consisting of 80,878 tokens in
CoNLL format. We induced an maximum entropy
classifier by using as items all arcs of dependency
relations, each of which is used as a positive instance
if its label is subject; otherwise it is used as a nega-
tive instance.
To train the Japanese subject detection model we
used 1,753 articles contained both in the NAIST
Text Corpus and the Kyoto University Text Corpus.
By merging these two corpora, we can obtain the an-
notated data including which dependency arc is sub-
ject10. To create the training instances, any pair of
a predicate and its dependent are extracted, each of
8http://www.cs.utah.edu/?hal/megam/
9http://www.di.unito.it/?tutreeb/
10Note that Iida et al (2007b) referred to this relation as
?nominative?.
feature description
SUBJ PRE 1 if subject is included in the preceding
words of ZERO in a sentence; otherwise 0.
TOPIC PRE* 1 if topic case marker appears in the preced-
ing words of ZERO in a sentence; otherwise
0.
NUM PRE
(GEN PRE)
1 if a candidate which agrees with ZERO
with regards to number (gender) is included
in the set of NP; otherwise 0.
FIRST SENT 1 if ZERO appears in the first sentence of a
text; otherwise 0.
FIRST WORD 1 if the predicate which has ZERO is the
first word in a sentence; otherwise 0.
POS / LEMMA
/ DEP LABEL
part-of-speech / dependency label / lemma
of the predicate which has ZERO.
D POS /
D LEMMA /
D DEP LABEL
part-of-speech / dependency label / lemma
of the dependents of the predicate which has
ZERO.
PATH* dependency labels (functional words) of
words intervening between a ZERO and the
sentence head
The features marked with ?*? used only in Japanese.
Table 3: Features for anaphoricity determination
which is judged as positive if its relation is subject;
as negative otherwise.
As features for Italian, we used lemmas, PoS tag
of a predicate and its dependents as well as their
morphological information (i.e. gender and num-
ber) automatically computed by TextPro (Pianta et
al., 2008). For Japanese, the head lemmas of predi-
cate and dependent chunks as well as the functional
words involved with these two chunks were used as
features. One case specially treated is when a de-
pendent is placed as an adnominal constituent of a
predicate, as in this case relation estimation of de-
pendency arcs is difficult. In such case we instead
use the features shown in Table 2 for accurate esti-
mation.
4.5 Results with zero anaphora only
In zero anaphora resolution, we need to find all pred-
icates that have anaphoric unrealized subjects (i.e.
zero pronouns which have an antecedent in a text),
and then identify an antecedent for each such argu-
ment.
The Italian and Japanese test data sets contain
4,065 and 25,467 verbal predicates respectively. The
performance of each model at zero-anaphora detec-
tion and resolution is shown in Table 4, using recall
808
feature description
HEAD LEMMA characters of the head lemma in NP.
POS part-of-speech of NP.
DEFINITE 1 if NP contains the article corresponding to DEFINITE ?the?; otherwise 0.
DEMONSTRATIVE 1 if NP contains the article corresponding to DEMONSTRATIVE such as ?that? and ?this?; otherwise 0.
POSSESSIVE 1 if NP contains the article corresponding to POSSESSIVE such as ?his? and ?their?; otherwise 0.
CASE MARKER** case marker followed by NP, such as ?wa (topic)?, ?ga (subject)?, ?o (object)?.
DEP LABEL* dependency label of NP.
COOC MI** the score of well-formedness model estimated from a large number of triplets ? NP, Case, Predicate?.
FIRST SENT 1 if NP appears in the first sentence of a text; otherwise 0.
FIRST MENTION 1 if NP first appears in the set of candidate antecedents; otherwise 0.
CL RANK** a rank of NP in forward looking-center list based on Centering Theory (Grosz et al, 1995)
CL ORDER** a order of NP in forward looking-center list based on Centering Theory (Grosz et al, 1995)
PATH dependency labels (functional words) of words intervening between a ZERO and NP
NUM (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to number; otherwise 0.
GEN (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to gender; otherwise 0.
HEAD MATCH 1 if ANA and NP have the same head lemma; otherwise 0.
REGEX MATCH 1 if the string of NP subsumes the string of ANA; otherwise 0.
COMP MATCH 1 if ANA and NP have the same string; otherwise 0.
NP, ANA and ZERO stand for a candidate antecedent, a candidate anaphor and a candidate zero pronoun respectively. The features
marked with ?*? are only used in Italian, while the features marked with ?**? are only used in Japanese.
Table 2: Features used for antecedent identification
Italian Japanese
system mentions gold mentions
model R P F R P F R P F
PAIRWISE 0.864 0.172 0.287 0.864 0.172 0.287 0.286 0.308 0.296
DS-CASCADE 0.396 0.684 0.502 0.404 0.697 0.511 0.345 0.194 0.248
ILP 0.905 0.034 0.065 0.929 0.028 0.055 0.379 0.238 0.293
ILP +BF 0.803 0.375 0.511 0.834 0.369 0.511 0.353 0.256 0.297
ILP +SUBJ 0.900 0.034 0.066 0.927 0.028 0.055 0.371 0.315 0.341
ILP +BF +SUBJ 0.777 0.398 0.526 0.815 0.398 0.534 0.345 0.348 0.346
Table 4: Results on zero pronouns
/ precision / F over link detection as a metric (model
theoretic metrics do not apply for this task as only
subsets of coreference chains are considered). As
can be seen from Table 4, the ILP version with Do-
Not-Resolve-Non-Anaphors performs no better than
the baselines for either languages, but in both lan-
guages replacing that constraint with Best-First re-
sults in a performance above the baselines; adding
Subject Detection results in further improvement for
both languages. Notice also that the performance of
the models on Italian is quite a bit higher than for
Japanese although the dataset is much smaller, pos-
sibly meaning that the task is easier in Italian.
5 Experiment 2: coreference resolution for
all anaphors
In a second series of experiments we evaluated the
performance of our models together with a full
coreference system resolving all anaphors, not just
zeros.
5.1 Separating vs combining classifiers
Different types of nominal expressions display very
different anaphoric behavior: e.g., pronoun res-
olution involves very different types of informa-
tion from nominal expression resolution, depend-
ing more on syntactic information and on the local
context and less on commonsense knowledge. But
the most common approach to coreference resolu-
809
tion (Soon et al, 2001; Ng and Cardie, 2002, etc.)
is to use a single classifier to identify antecedents of
all anaphoric expressions, relying on the ability of
the machine learning algorithm to learn these differ-
ences. These models, however, often fail to capture
the differences in anaphoric behavior between dif-
ferent types of expressions?one of the reasons be-
ing that the amount of training instances is often too
small to learn such differences.11 Using different
models would appear to be key in the case of zero-
anaphora resolution, which differs even more from
the rest of anaphora resolution, e.g., in being partic-
ularly sensitive to local salience, as amply discussed
in the literature on Centering discussed earlier.
To test the hypothesis that using what we will
call separated models for zero anaphora and every-
thing else would work better than combined mod-
els induced from all the learning instances, we man-
ually split the training instances in terms of these
two anaphora types and then created two classifiers
for antecedent identification: one for zero-anaphora,
the other for NP-anaphora, separately induced from
the corresponding training instances. Likewise,
anaphoricity determination models were separately
induced with regards to these two anaphora types.
5.2 Results with all anaphors
In Table 5 and Table 6 we show the (MUC scorer)
results obtained by adding the zero anaphoric reso-
lution models proposed in this paper to both a com-
bined and a separated classifier. For the separated
classifier, we use the ILP+BF model for explicitly
realized NPs, and different ILP models for zeros.
The results show that the separated classi-
fier works systematically better than a combined
classifier. For both Italian and Japanese the
ILP+BF+SUBJ model works clearly better than the
baselines, whereas simply applying the original De-
nis and Baldridge model unchanged to this case we
obtain worse results than the baselines. For Italian
we could also compare our results with those ob-
tained on the same dataset by one of the two sys-
tems that participated to the Italian section of SE-
MEVAL, I-BART. I-BART?s results are clearly bet-
ter than those with both baselines, but also clearly in-
11E.g., the entire MUC-6 corpus contains a grand total of 3
reflexive pronouns.
Japanese
combined separated
model R P F R P F
PAIRWISE 0.345 0.236 0.280 0.427 0.240 0.308
DS-CASCADE 0.207 0.592 0.307 0.291 0.488 0.365
ILP 0.381 0.330 0.353 0.490 0.304 0.375
ILP +BF 0.349 0.390 0.368 0.446 0.340 0.386
ILP +SUBJ 0.376 0.366 0.371 0.484 0.353 0.408
ILP +BF +SUBJ 0.344 0.450 0.390 0.441 0.415 0.427
Table 6: Results for overall coreference: Japanese (MUC
score)
ferior to the results obtained with our models. In par-
ticular, the effect of introducing the separated model
with ILP+BF+SUBJ is more significant when us-
ing the system detected mentions; it obtained perfor-
mance more than 13 points better than I-BART when
the model referred to the system detected mentions.
6 Related work
We are not aware of any previous machine learn-
ing model for zero anaphora in Italian, but there
has been quite a lot of work on Japanese zero-
anaphora (Iida et al, 2007a; Taira et al, 2008; Ima-
mura et al, 2009; Taira et al, 2010; Sasano et al,
2009). In work such as Taira et al (2008) and Ima-
mura et al (2009), zero-anaphora resolution is con-
sidered as a sub-task of predicate argument structure
analysis, taking the NAIST text corpus as a target
data set. Taira et al (2008) and Taira et al (2010) ap-
plied decision lists and transformation-based learn-
ing respectively in order to manually analyze which
clues are important for each argument assignment.
Imamura et al (2009) also tackled to the same prob-
lem setting by applying a pairwise classifier for each
argument. In their approach, a ?null? argument is ex-
plicitly added into the set of candidate argument to
learn the situation where an argument of a predicate
is ?exophoric?. They reported their model achieved
better performance than the work by Taira et al
(2008).
Iida et al (2007a) also used the NAIST text
corpus. They adopted the BACT learning algo-
rithm (Kudo and Matsumoto, 2004) to effectively
learn subtrees useful for both antecedent identifica-
tion and zero pronoun detection. Their model drasti-
cally outperformed a simple pairwise model, but it is
still performed as a cascaded process. Incorporating
810
Italian
system mentions gold mentions
combined separated combined separated
model R P F R P F R P F R P F
PAIRWISE 0.508 0.208 0.295 0.472 0.241 0.319 0.582 0.261 0.361 0.566 0.314 0.404
DS-CASCADE 0.225 0.553 0.320 0.217 0.574 0.315 0.245 0.609 0.349 0.246 0.686 0.362
I-BART 0.324 0.294 0.308 ? ? ? 0.532 0.441 0.482 ? ? ?
ILP 0.539 0.321 0.403 0.535 0.316 0.397 0.614 0.369 0.461 0.607 0.384 0.470
ILP +BF 0.471 0.404 0.435 0.483 0.409 0.443 0.545 0.517 0.530 0.563 0.519 0.540
ILP +SUBJ 0.537 0.325 0.405 0.534 0.318 0.399 0.611 0.372 0.463 0.606 0.387 0.473
ILP +BF +SUBJ 0.464 0.410 0.435 0.478 0.418 0.446 0.538 0.527 0.533 0.559 0.536 0.547
R: Recall, P: Precision, F: f -score, BF: best first constraint, SUBJ: subject detection model.
Table 5: Results for overall coreference: Italian (MUC score)
their model into the ILP formulation proposed here
looks like a promising further extension.
Sasano et al (2009) obtained interesting experi-
mental results about the relationship between zero-
anaphora resolution and the scale of automatically
acquired case frames. In their work, their case
frames were acquired from a very large corpus con-
sisting of 100 billion words. They also proposed
a probabilistic model to Japanese zero-anaphora
in which an argument assignment score is esti-
mated based on the automatically acquired case
frames. They concluded that case frames acquired
from larger corpora lead to better f -score on zero-
anaphora resolution.
In contrast to these approaches in Japanese, the
participants to Semeval 2010 task 1 (especially the
Italian coreference task) simply solved the prob-
lems using one coreference classifier, not distin-
guishing zero-anaphora from the other types of
anaphora (Kobdani and Schu?tze, 2010; Poesio et al,
2010). On the other hand, our approach shows sep-
arating problems contributes to improving perfor-
mance in Italian zero-anaphora. Although we used
gold mentions in our evaluations, mention detection
is also essential. As a next step, we also need to take
into account ways of incorporating a mention detec-
tion model into the ILP formulation.
7 Conclusion
In this paper, we developed a new ILP-based model
of zero anaphora detection and resolution that ex-
tends the coreference resolution model proposed by
Denis and Baldridge (2007) by introducing modi-
fied constraints and a subject detection model. We
evaluated this model both individually and as part
of the overall coreference task for both Italian and
Japanese zero anaphora, obtaining clear improve-
ments in performance.
One avenue for future research is motivated by the
observation that whereas introducing the subject de-
tection model and the best-first constraint results in
higher precision maintaining the recall compared to
the baselines, that precision is still low. One of the
major source of the errors is that zero pronouns are
frequently used in Italian and Japanese in contexts in
which in English as so-called generic they would be
used: ?I walked into the hotel and (they) said ..?. In
such case, the zero pronoun detection model is often
incorrect. We are considering adding a generic they
detection component.
We also intend to experiment with introducing
more sophisticated antecedent identification models
in the ILP framework. In this paper, we used a very
basic pairwise classifier; however Yang et al (2008)
and Iida et al (2003) showed that the relative com-
parison of two candidate antecedents leads to obtain-
ing better accuracy than the pairwise model. How-
ever, these approaches do not output absolute prob-
abilities, but relative significance between two can-
didates, and therefore cannot be directly integrated
with the ILP-framework. We plan to examine ways
of appropriately estimating an absolute score from a
set of relative scores for further refinement.
Finally, we would like to test our model with
English constructions which closely resemble zero
anaphora. One example were studied in the Semeval
2010 ?Linking Events and their Participants in Dis-
course? task, which provides data about null instan-
811
tiation, omitted arguments of predicates like ?We
arrived ?goal at 8pm.?. (Unfortunately the dataset
available for SEMEVAL was very small.) Another
interesting area of application of these techniques
would be VP ellipsis.
Acknowledgments
Ryu Iida?s stay in Trento was supported by the Ex-
cellent Young Researcher Overseas Visit Program
of the Japan Society for the Promotion of Science
(JSPS). Massimo Poesio was supported in part by
the Provincia di Trento Grande Progetto LiveMem-
ories, which also funded the creation of the Italian
corpus used in this study. We also wish to thank
Francesca Delogu, Kepa Rodriguez, Olga Uryupina
and Yannick Versley for much help with the corpus
and BART.
References
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
Prague.
C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo,
F. Dell?Orletta, A. Lenci, L. Lesmo, G. Attardi,
M. Simi, A. Lavelli, J. Hall, J. Nilsson, and J. Nivre.
2010. Comparing the influence of different treebank
annotations on dependency parsing. In Proceedings of
LREC, pages 1794?1801.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proc. of HLT/NAACL, pages 236?
243.
B. Di Eugenio. 1998. Centering in Italian. In M. A.
Walker, A. K. Joshi, and E. F. Prince, editors, Cen-
tering Theory in Discourse, chapter 7, pages 115?138.
Oxford.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Center-
ing: A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2):203?226.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003.
Incorporating contextual cues in trainable models for
coreference resolution. In Proceedings of the 10th
EACL Workshop on The Computational Treatment of
Anaphora, pages 23?30.
R. Iida, K. Inui, and Y. Matsumoto. 2007a. Zero-
anaphora resolution by learning rich syntactic pattern
features. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 6(4).
R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007b.
Annotating a Japanese text corpus with predicate-
argument and coreference relations. In Proceeding of
the ACL Workshop ?Linguistic Annotation Workshop?,
pages 132?139.
K. Imamura, K. Saito, and T. Izumi. 2009. Discrimi-
native approach to predicate-argument structure anal-
ysis with zero-anaphora resolution. In Proceedings of
ACL-IJCNLP, Short Papers, pages 85?88.
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun
resolution based on ranking rules and machine learn-
ing. In Proceedings of EMNLP, pages 184?191.
M. Kameyama. 1985. Zero Anaphora: The case of
Japanese. Ph.D. thesis, Stanford University.
H. Kobdani and H. Schu?tze. 2010. Sucre: A modular
system for coreference resolution. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 92?95.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proceed-
ings of EMNLP, pages 301?308.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of the 40th ACL, pages 104?111.
E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro
tool suite. In In Proceedings of LREC, pages 28?30.
M. Poesio, O. Uryupina, and Y. Versley. 2010. Creating a
coreference resolution system for Italian. In Proceed-
ings of LREC.
M. Recasens, L. Ma`rquez, E. Sapena, M. A. Mart??,
M. Taule?, V. Hoste, M. Poesio, and Y. Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1?8.
K-J. Rodriguez, F. Delogu, Y. Versley, E. Stemle, and
M. Poesio. 2010. Anaphoric annotation of wikipedia
and blogs in the live memories corpus. In Proc. LREC.
D. Roth and W.-T. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of CONLL.
R. Sasano, D. Kawahara, and S. Kurohashi. 2009. The
effect of corpus size on case frame acquisition for dis-
course analysis. In Proceedings of HLT/NAACL, pages
521?529.
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic
method for analyzing Japanese anaphora integrating
zero pronoun detection and resolution. In Proceedings
of the 19th COLING, pages 911?917.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
812
H. Taira, S. Fujita, and M. Nagata. 2008. A Japanese
predicate argument structure analysis using decision
lists. In Proceedings of EMNLP, pages 523?532.
H. Taira, S. Fujita, and M. Nagata. 2010. Predicate ar-
gument structure analysis using transformation based
learning. In Proceedings of the ACL 2010 Conference
Short Papers, pages 162?167.
M. A. Walker, M. Iida, and S. Cote. 1994. Japanese
discourse and the process of centering. Computational
Linguistics, 20(2):193?232.
X. Yang, J. Su, and C. L. Tan. 2008. Twin-candidate
model for learning-based anaphora resolution. Com-
putational Linguistics, 34(3):327?356.
813
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 349?353,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Sentence Compression with Semantic Role Constraints
Katsumasa Yoshikawa
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
IBM Research-Tokyo, IBM Japan, Ltd.
katsumasay@gmail.com
Ryu Iida
Department of Computer Science,
Tokyo Institute of Technology, Japan
ryu-i@cl.cs.titech.ac.jp
Tsutomu Hirao
NTT Communication Science Laboratories,
NTT Corporation, Japan
hirao.tsutomu@lab.ntt.co.jp
Manabu Okumura
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
oku@lr.pi.titech.ac.jp
Abstract
For sentence compression, we propose new se-
mantic constraints to directly capture the relations
between a predicate and its arguments, whereas
the existing approaches have focused on relatively
shallow linguistic properties, such as lexical and
syntactic information. These constraints are based
on semantic roles and superior to the constraints
of syntactic dependencies. Our empirical eval-
uation on the Written News Compression Cor-
pus (Clarke and Lapata, 2008) demonstrates that
our system achieves results comparable to other
state-of-the-art techniques.
1 Introduction
Recent work in document summarization do not
only extract sentences but also compress sentences.
Sentence compression enables summarizers to re-
duce the redundancy in sentences and generate in-
formative summaries beyond the extractive summa-
rization systems (Knight and Marcu, 2002). Con-
ventional approaches to sentence compression ex-
ploit various linguistic properties based on lexical
information and syntactic dependencies (McDonald,
2006; Clarke and Lapata, 2008; Cohn and Lapata,
2008; Galanis and Androutsopoulos, 2010).
In contrast, our approach utilizes another property
based on semantic roles (SRs) which improves weak-
nesses of syntactic dependencies. Syntactic depen-
dencies are not sufficient to compress some complex
sentences with coordination, with passive voice, and
with an auxiliary verb. Figure 1 shows an example
with a coordination structure. 1
1This example is from Written News Compression Cor-
pus (http://jamesclarke.net/research/resources).
Figure 1: Semantic Role vs. Dependency Relation
In this example, a SR labeler annotated thatHarari
is an A0 argument of left and an A1 argument of
became. Harari is syntactically dependent on left ?
SBJ(left-2, Harari-1). However, Harari is not depen-
dent on became and we are hence unable to utilize a
dependency relation between Harari and became di-
rectly. SRs allow us to model the relations between
a predicate and its arguments in a direct fashion.
SR constraints are also advantageous in that we
can compress sentences with semantic information.
In Figure 1, became has three arguments, Harari as
A1, businessman as A2, and shortly afterward as
AM-TMP. As shown in this example, shortly after-
word can be omitted (shaded boxes). In general,
modifier arguments like AM-TMP or AM-LOC are
more likely to be reduced than complement cases
like A0-A4. We can implement such properties by
SR constraints.
Liu and Gildea (2010) suggests that SR features
contribute to generating more readable sentence in
machine translation. We expect that SR features also
help our system to improve readability in sentence
compression and summarization.
2 Why are Semantic Roles Useful for Com-
pressing Sentences?
Before describing our system, we show the statis-
tics in terms of predicates, arguments and their rela-
349
Label In Compression / Total Ratio
A0 1454 / 1607 0.905
A1 1916 / 2208 0.868
A2 427 / 490 0.871
AM-TMP 261 / 488 0.535
AM-LOC 134 / 214 0.626
AM-ADV 115 / 213 0.544
AM-DIS 8 / 85 0.094
Table 1: Statistics of Arguments in Compression
tions in the Written News Compression (WNC) Cor-
pus. It has 82 documents (1,629 sentences). We di-
vided them into three: 55 documents are used for
training (1106 sentences); 10 for development (184
sentences); 17 for testing (339 sentences).
Our investigation was held in training data. There
are 3137 verbal predicates and 7852 unique argu-
ments. We performed SR labeling by LTH (Johans-
son and Nugues, 2008), an SR labeler for CoNLL-
2008 shared task. Based on the SR labels annotated
by LTH, we investigated that, for all predicates in
compression, how many their arguments were also
in. Table 1 shows the survival ratio of main argu-
ments in compression. Labels A0, A1, and A2 are
complement case roles and over 85% of them survive
with their predicates. On the other hand, for modifier
arguments (AM-X), survival ratios are down to lower
than 65%. Our SR constraints implement the differ-
ence of survival ratios by SR labels. Note that de-
pendency labels SBJ and OBJ generally correspond
to SR labels A0 and A1, respectively. But their total
numbers are 777 / 919 (SBJ) and 918 / 1211 (OBJ)
and much fewer than A0 and A1 labels. Thus, SR la-
bels can connect much more arguments to their pred-
icates.
3 Approach
This section describes our new approach to sen-
tence compression. In order to introduce rich syn-
tactic and semantic constraints to a sentence com-
pression model, we employ Markov Logic (Richard-
son and Domingos, 2006). Since Markov Logic sup-
ports both soft and hard constraints, we can imple-
ment our SR constraints in simple and direct fash-
ion. Moreover, implementations of learning and
inference methods are already provided in existing
Markov Logic interpreters such as Alchemy 2 and
Markov thebeast. 3 Thus, we can focus our effort
2http://alchemy.cs.washington.edu/
3http://code.google.com/p/thebeast/
on building a set of formulae called Markov Logic
Network (MLN). So, in this section, we describe our
proposed MLN in detail.
3.1 Proposed Markov Logic Network
First, let us define our MLN predicates. We sum-
marize the MLN predicates in Table 2. We have only
one hidden MLN predicate, inComp(i) which mod-
els the decision we need to make: whether a token i
is in compression or not. The other MLN predicates
are called observed which provide features. With our
MLN predicates defined, we can now go on to in-
corporate our intuition about the task using weighted
first-order logic formulae. We define SR constraints
and the other formulae in Sections 3.1.1 and 3.1.2,
respectively.
3.1.1 Semantic Role Constraints
Semantic role labeling generally includes the three
subtasks: predicate identification; argument role la-
beling; sense disambiguation. Our model exploits
the results of predicate identification and argument
role labeling. 4 pred(i) and role(i, j, r) indicate the
results of predicate identification and role labeling,
respectively.
First, the formula describing a local property of a
predicate is
pred(i) ? inComp(i) (1)
which denotes that, if token i is a predicate then i is
in compression. A formula with exact one hidden
predicate is called local formula.
A predicate is not always in compression. The for-
mula reducing some predicates is
pred(i) ? height(i,+n) ? ?inComp(i) (2)
which implies that a predicate i is not in compression
with n height in a dependency tree. Note the + nota-
tion indicates that the MLN contains one instance of
the rule, with a separate weight, for each assignment
of the variables with a plus sign.
As mentioned earlier, our SR constraints model
the difference of the survival rate of role labels in
compression. Such SR constraints are encoded as:
role(i, j, +r) ? inComp(i) ? inComp( j) (3)
role(i, j,+r) ? ?inComp(i) ? ?inComp( j) (4)
which represent that, if a predicate i is (not) in com-
pression, then its argument j is (not) also in with
4Sense information is too sparse because the size of the
WNC Corpus is not big enough.
350
predicate definition
inComp(i) Token i is in compression
pred(i) Token i is a predicate
role(i, j, r) Token i has an argument j with role r
word(i, w) Token i has word w
pos(i, p) Token i has Pos tag p
dep(i, j, d) Token i is dependent on token j with
dependency label d
path(i, j, l) Tokens i and j has syntactic path l
height(i, n) Token i has height n in dependency tree
Table 2: MLN Predicates
role r. These formulae are called global formulae
because they have more than two hidden MLN pred-
icates. With global formulae, our model makes two
decisions at a time. When considering the example
in Figure 1, Formula (3) will be grounded as:
role(9, 1, A0) ? inComp(9) ? inComp(1) (5)
role(9, 7, AM-TMP) ? inComp(9) ? inComp(7). (6)
In fact, Formula (5) gains a higher weight than For-
mula (6) by learning on training data. As a re-
sult, our system gives ?1-Harari? more chance to
survive in compression. We also add some exten-
sions of Formula (3) combined with dep(i, j, +d) and
path(i, j, +l) which enhance SR constraints. Note, all
our SR constraints are ?predicate-driven? (only ?
not ? as in Formula (13)). Because an argument is
usually related to multiple predicates, it is difficult to
model ?argument-driven? formula.
3.1.2 Lexical and Syntactic Features
For lexical and syntactic features, we mainly refer
to the previous work (McDonald, 2006; Clarke and
Lapata, 2008). The first two formulae in this sec-
tion capture the relation of the tokens with their lexi-
cal and syntactic properties. The formula describing
such a local property of a word form is
word(i,+w) ? inComp(i) (7)
which implies that a token i is in compression with a
weight that depends on the word form.
For part-of-speech (POS), we add unigram and bi-
gram features with the formulae,
pos(i, +p) ? inComp(i) (8)
pos(i, +p1) ? pos(i + 1,+p2) ? inComp(i). (9)
POS features are often more reasonable than word
form features to combine with the other properties.
The formula,
pos(i, +p) ? height(i, +n) ? inComp(i). (10)
is a combination of POS features and a height in a
dependency tree.
The next formula combines POS bigram features
with dependency relations.
pos(i,+p1) ? pos( j, +p2) ?
dep(i, j,+d) ? inComp(i). (11)
Moreover, our model includes the following
global formulae,
dep(i, j,+d) ? inComp(i) ? inComp( j) (12)
dep(i, j,+d) ? inComp(i) ? inComp( j) (13)
which enforce the consistencies between head and
modifier tokens. Formula (12) represents that if
we include a head token in compression then its
modifier must also be included. Formula (13) en-
sures that head and modifier words must be simul-
taneously kept in compression or dropped. Though
Clarke and Lapata (2008) implemented these depen-
dency constraints by ILP, we implement them by
soft constraints of MLN. Note that Formula (12) ex-
presses the same properties as Formula (3) replacing
dep(i, j, +d) by role(i, j,+r).
4 Experiment and Result
4.1 Experimental Setup
Our experimental setting follows previous
work (Clarke and Lapata, 2008). As stated in
Section 2, we employed the WNC Corpus. For
preprocessing, we performed POS tagging by
stanford-tagger. 5 and dependency parsing by
MST-parser (McDonald et al, 2005). In addition,
LTH 6 was exploited to perform both dependency
parsing and SR labeling. We implemented our
model by Markov Thebeast with Gurobi optimizer. 7
Our evaluation consists of two types of automatic
evaluations. The first evaluation is dependency based
evaluation same as Riezler et al (2003). We per-
formed dependency parsing on gold data and system
outputs by RASP. 8 Then we calculated precision, re-
call, and F1 for the set of label(head, modi f ier).
In order to demonstrate how well our SR con-
straints keep correct predicate-argument structures
in compression, we propose SRL based evalua-
tion. We performed SR labeling on gold data
5http://nlp.stanford.edu/software/tagger.shtml
6http://nlp.cs.lth.se/software/semantic_
parsing:_propbank_nombank_frames
7http://www.gurobi.com/
8http://www.informatics.susx.ac.uk/research/
groups/nlp/rasp/
351
Original [A0 They] [pred say] [A1 the refugees will enhance productivity and economic growth].
MLN with SRL [A0 They] [pred say] [A1 the refugees will enhance growth].
Gold Standard [A1? the refugees will enhance productivity and growth].
Original [A0 A ?16.1m dam] [AM?MOD will] [pred hold] back [A1 a 2.6-mile-long artificial lake to be
known as the Roadford Reservoir].
MLN with SRL [A0 A dam] will [pred hold] back [A1 a artificial lake to be known as the Roadford Reservoir].
Gold Standard [A0 A ?16.1m dam] [AM?MOD will] [pred hold back [A1 a 2.6-mile-long Roadford Reservoir].
Table 4: Analysis of Errors
Model CompR F1-Dep F1-SRL
McDonald 73.6% 38.4% 49.9%
MLN w/o SRL 68.3% 51.3% 57.2%
MLN with SRL 73.1% 58.9% 64.1%
Gold Standard 73.3% ? ?
Table 3: Results of Sentence Compression
and system outputs by LTH. Then we calculated
precision, recall, and F1 value for the set of
role(predicate, argument).
The training time of our MLN model are approx-
imately 8 minutes on all training data, with 3.1GHz
Intel Core i3 CPU and 4G memory. While the pre-
diction can be done within 20 seconds on the test
data.
4.2 Results
Table 3 shows the results of our compression
models by compression rate (CompR), dependency-
based F1 (F1-Dep), and SRL-based F1 (F1-SRL). In
our experiment, we have three models. McDonald
is a re-implementation of McDonald (2006). Clarke
and Lapata (2008) also re-implemented McDonald?s
model with an ILP solver and experimented it on the
WNC Corpus. 9 MLN with SRL and MLN w/o
SRL are our Markov Logic models with and with-
out SR Constraints, respectively.
Note our three models have no constraint for the
length of compression. Therefore, we think the com-
pression rate of the better system should get closer to
that of human compression. In comparison between
MLNmodels and McDonald, the former models out-
perform the latter model on both F1-Dep and F1-
SRL. Because MLN models have global constraints
and can generate syntactically correct sentences.
Our concern is how a model with SR constraints
is superior to a model without them. MLN with
SRL outperforms MLN without SRL with a 7.6
points margin (F1-Dep). The compression rate of
MLN with SRL goes up to 73.1% and gets close
9Clarke?s re-implementation got 60.1% for CompR and
36.0%pt for F1-Dep
to that of gold standard. SRL-based evaluation also
shows that SR constraints actually help extract cor-
rect predicate-argument structures. These results are
promising to improve readability.
It is difficult to directly compare our results with
those of state-of-the-art systems (Cohn and Lapata,
2009; Clarke and Lapata, 2010; Galanis and An-
droutsopoulos, 2010) since they have different test-
ing sets and the results with different compression
rates. However, though our MLN model with SR
constraints utilizes no large-scale data, it is the only
model which achieves close on 60% in F1-Dep.
4.3 Error Analysis
Table 4 indicates two critical examples which our
SR constraints failed to compress correctly. For the
first example, our model leaves an argument with its
predicate because our SR constraints are ?predicate-
driven?. In addition, ?say? is the main verb in this
sentence and hard to be deleted due to the syntactic
significance.
The second example in Table 4 requires to iden-
tify a coreference relation between artificial lake and
Roadford Reservour. We consider that discourse
constraints (Clarke and Lapata, 2010) help our model
handle these cases. Discourse and coreference infor-
mation enable our model to select important argu-
ments and their predicates.
5 Conclusion
In this paper, we proposed new semantic con-
straints for sentence compression. Our model with
global constraints of semantic roles selected correct
predicate-argument structures and successfully im-
proved performance of sentence compression.
As future work, we will compare our model with
the other state-of-the-art systems. We will also inves-
tigate the correlation between readability and SRL-
based score by manual evaluations. Furthermore, we
would like to combine discourse constraints with SR
constraints.
352
References
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31(1):399?429.
James Clarke and Mirella Lapata. 2010. Discourse con-
straints for document compression. Computational
Linguistics, 36(3):411?441.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
the 22nd International Conference on Computational
Linguistics-Volume 1, pages 137?144. Association for
Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 885?893, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 183?187. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelligence,
139(1):91?107.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 716?724, Beijing, China,
August. Coling 2010 Organizing Committee.
RyanMcDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic?. 2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
118?125. Association for Computational Linguistics.
353
Proceedings of the 8th Workshop on Asian Language Resources, pages 38?46,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Construction of bilingual multimodal corpora of referring expressions in
collaborative problem solving
TOKUNAGA Takenobu IIDA Ryu YASUHARA Masaaki TERAI Asuka
{take,ryu-i,yasuhara}@cl.cs.titech.ac.jp asuka@nm.hum.titech.ac.jp
Tokyo Institute of Technology
David MORRIS Anja BELZ
D.Morris@brighton.ac.uk a.s.belz@itri.brighton.ac.uk
University of Brighton
Abstract
This paper presents on-going work on
constructing bilingual multimodal corpora
of referring expressions in collaborative
problem solving for English and Japanese.
The corpora were collected from dia-
logues in which two participants collab-
oratively solved Tangram puzzles with
a puzzle simulator. Extra-linguistic in-
formation such as operations on puzzle
pieces, mouse cursor position and piece
positions were recorded in synchronisa-
tion with utterances. The speech data
was transcribed and time-aligned with the
extra-linguistic information. Referring
expressions in utterances that refer to puz-
zle pieces were annotated in terms of their
spans, their referents and their other at-
tributes. The Japanese corpus has already
been completed, but the English counter-
part is still undergoing annotation. We
have conducted a preliminary comparative
analysis of both corpora, mainly with re-
spect to task completion time, task suc-
cess rates and attributes of referring ex-
pressions. These corpora showed signif-
icant differences in task completion time
and success rate.
1 Introduction
A referring expression (RE) is a linguistic de-
vice that refers to a certain object of interest (e.g.
used in describing where the object is located in
space). REs have attracted a great deal of atten-
tion in both language analysis and language gen-
eration research. In language analysis research,
reference resolution, particularly anaphora resolu-
tion (Mitkov, 2002), has a long research history as
far back as the mid-1970s (Hobbs, 1978). Much
research has been conducted from both theoretical
and empirical perspectives, mainly concerning the
identification of antecedents or entities mentioned
within the same text. This trend, targeting refer-
ence resolution in written text, is still dominant in
the language analysis, perhaps because such tech-
niques are intended for use in applications such as
information extraction.
In contrast, in language generation research in-
terest has recently shifted from the generation of
one-off references to entities to generation of REs
in discourse context (Belz et al, 2010) and inves-
tigating human referential behaviour in real world
situations, with the aim of using such techniques
in applications like human-robot interaction (Pi-
wek, 2007; Foster et al, 2008; Bard et al, 2009).
In both analysis and generation, machine-
learning approaches have come to replace rule-
based approaches as the predominant research
trend since the 1990s. This trend has made anno-
tated corpora an indispensable component of re-
search for training and evaluating proposed meth-
ods. In fact, research on reference resolution has
developed significantly as a result of large scale
corpora, e.g. those provided by the Message Un-
derstanding Conference (MUC)1 and the Auto-
matic Content Extraction (ACE)2 project. These
corpora were constructed primarily for informa-
tion extraction research, thus were annotated with
co-reference relations within texts. Also in the
language generation community, several corpora
1http://www.nlpir.nist.gov/related projects/muc/
2http://www.itl.nist.gov/iad/tests/ace/
38
have been developed (Di Eugenio et al, 2000; By-
ron, 2005; van Deemter et al, 2006; Foster and
Oberlander, 2007; Foster et al, 2008; Stoia et al,
2008; Spanger et al, 2009a; Belz et al, 2010).
Unlike the corpora of MUC and ACE, many are
collected from situated dialogues, and therefore
include multimodal information (e.g. gestures and
eye-gaze) other than just transcribed text (Martin
et al, 2007). Foster and Oberlander (2007) em-
phasised that any corpus for language generation
should include all possible contextual information
at the appropriate granularity. Since constructing
a dialogue corpus generally requires experiments
for data collection, this kind of corpus tends to be
small-scale compared with corpora for reference
resolution.
Against this background, we have been de-
veloping multimodal corpora of referring expres-
sions in collaborative problem-solving settings.
This paper presents on-going work of construct-
ing bilingual (English and Japanese) comparable
corpora in this domain. We achieve our goal by
replicating, for the English corpus, the same pro-
cess of data collection and annotation as we used
for our existing Japanese corpus (Spanger et al,
2009a). Our aim is to create bilingual multimodal
corpora collected from dialogues in dynamic situ-
ations. From the point of view of reference anal-
ysis, our corpora contribute to augmenting the re-
sources of multimodal dialogue corpora annotated
with reference relations which have been minor
in number compared to other types of text cor-
pora. From the point of view of reference gen-
eration, our corpora contribute to increasing the
resources available that can be used to further re-
search of this kind. In addition, our corpora con-
tribute to comparative studies of human referential
behaviour in different languages
The structure of the paper is as follows. Sec-
tion 2 describes the experimental set-up for data
collection which was introduced in our previous
work (Spanger et al, 2009a). The setting is basi-
cally the same for the construction of the English
corpus. Section 3 explains the annotation scheme
adopted in our corpora, followed by a description
of a preliminary analysis of the corpora in sec-
tion 4. Section 5 briefly mentions related work
to highlight the characteristics of our corpora. Fi-
nally, Section 6 concludes the paper and looks at
possible future directions.
!"#$%&'#()
*"+,-.!%#+)#
Figure 1: Screenshot of the Tangram simulator
2 Data collection
2.1 Experimental set-up
We recruited subjects in pairs of friends and col-
leagues. Each pair was instructed to solve Tan-
gram puzzles collaboratively. Tangram puzzles
are geometrical puzzles that originated in ancient
China. The goal of a Tangram puzzle is to con-
struct a given goal shape by arranging seven sim-
ple shapes, as shown in Figure 1. The pieces in-
clude two large triangles, a medium-sized trian-
gle, two small triangles, a parallelogram and a
square.
With the aim of recording the precise position
of every piece and every action the participants
made during the solving process, we implemented
a Tangram simulator in which the pieces can be
moved, rotated and flipped with simple mouse op-
erations on a computer display. The simulator dis-
plays two areas: a goal shape area and a work-
ing area where the pieces can be manupulated and
their movements are shown in real time.
We assigned a different role to each participant
of a pair: one acted as the solver and the other as
the operator. The operator has a mouse for manip-
ulating Tangram pieces, but does not have a goal
shape on the screen. The solver has a goal shape
on the screen but does not have a mouse. This set-
ting naturally leads to a situation where given a
certain goal shape, the solver thinks of the neces-
sary arrangement of the pieces and gives instruc-
tions to the operator how to move them, while the
operator manipulates the pieces with the mouse
39
according to the solver?s instructions.
Figure 2: Picture of the experiment setting
As we mentioned in our previous
study (Spanger et al, 2009a), this interaction
produces frequent use of referring expressions
intended to distinguish specific pieces of the
puzzle. In our Tangram simulator, all pieces
are of the same color, thus color is not useful
in identifying a specific piece, i.e. only size
and shape are discriminative object-intrinsic
attributes. Instead, we can expect other attributes
such as spatial relations and deictic reference to
be used more often.
Each pair of participants sat side by side as
shown in Figure 2. Each participant had his/her
own computer display showing the shared work-
ing area. A room-divider screen was set between
the solver (right side) and operator (left side) to
prevent the operator from seeing the goal shape on
the solver?s screen, and to restrict their interaction
to speech only.
!"#$%&'()"*(
!+#$,-"$.&/0
!.#$1/"(
!2#$34"*5$
Figure 3: The goal shapes given to the subjects
Each participant pair was assigned 4 trials con-
sisting of two symmetric and two asymmetric
goal shapes as shown in Figure 3. In Cogni-
tive Science, a wide variety of different kinds of
puzzles have been employed extensively in the
field of Insight Problem solving. This has been
termed the ?puzzle-problem approach? (Sternberg
and Davidson, 1996; Suzuki et al, 2001) and
in the case of physical puzzles has relatively of-
ten involved puzzle tasks of symmetric shapes
like the so-called T-puzzle, e.g. (Kiyokawa and
Nakazawa, 2006). In more recent work Tangram
puzzles have been used as a means to study var-
ious new aspects of human problem solving ap-
proaches, including collection of of eye-gaze in-
formation (Baran et al, 2007). In order to col-
lect data as broadly as possible in this context, we
set up puzzle-problems including both symmetri-
cal as well as asymmetrical ones as shown in Fig-
ure 3.
The participants exchanged their roles after two
trials, i.e. a participant first solves a symmetric and
then an asymmetric puzzle as the solver and then
does the same as the operator, and vice versa. The
order of the puzzle trials is the same for all pairs.
Before starting the first trial as the operator,
each participant had a short training exercise in
order to learn how to manipulate pieces with the
mouse. The initial arrangement of the pieces was
randomised every time. We set a time limit of 15
minutes for the completion of each trial (i.e. con-
struction of the goal shape). In order to prevent the
solver from getting into deep thought and keeping
silent, the simulator is designed to give a hint ev-
ery five minutes by showing a correct piece posi-
tion in the goal shape area. After 10 minutes have
passed, a second hint is provided, while the pre-
vious hint disappears. A trial ends when the goal
shape is complete or the time is up. Utterances by
the participants are recorded separately in stereo
through headset microphones in synchronisation
with the position of the pieces and the mouse op-
erations. Piece positions and mouse actions were
automatically recorded by the simulator at inter-
vals of 10 msec.
40
Table 1: The ELAN Tiers of the corpus
Tier meaning
OP-UT utterances by the operator
SV-UT utterances by the solver
OP-REX referring expressions by the operator
OP-Ref referents of OP-REX
OP-Attr attributes of OP-REX
SV-REX referring expressions by the solver
SV-Ref referents of SV-REX
SV-Attr attributes of SV-REX
Action action on a piece
Target the target piece of Action
Mouse the piece on which the mouse is hovering
? Indentation of Tier denotes parent-child relations.
2.2 Subjects and collected data
For our Japanese corpus, we recruited 12 Japanese
graduate students of the Cognitive Science depart-
ment, 4 females and 8 males, and split them into 6
pairs. All pairs knew each other previously and
were of the same sex and approximately same
age3. We collected 24 dialogues (4 trials by 6
pairs) of about 4 hours and 16 minutes. The av-
erage length of a dialogue was 10 minutes 40 sec-
onds (SD = 3 minutes 18 seconds).
For the comparable English corpus, we re-
cruited 12 native English speakers of various oc-
cupations, 6 males and 6 females. Their aver-
age age was 30. There were 6 pairs all of whom
knew each other beforehand except for one pair.
Whereas during the creation of the Japanese cor-
pus we had to give extra attention to ensuring that
social relationships did not have an impact on how
the subjects communicated with one another, for
the English corpus there was no such concern. We
collected 24 dialogues (4 trials by 6 pairs) of 5
hours and 7 minutes total length. The average
length of a dialogue was 12 minutes 47 seconds
(SD = 3 minutes 34 seconds).
3 Annotation
The recorded speech data was transcribed and
the referring expressions were annotated with
the Web-based multi-purpose annotation tool
3In Japan, the relationship of senior to junior or socially
higher to lower placed might affect the language use. We
carefully recruited pairs to avoid the effects of this social re-
lationship such as the possible use of overly polite and indi-
rect language, reluctance to correct mistakes etc.
Table 2: Attributes of referring expressions
dpr : demonstrative pronoun, e.g. ?the same one?,
?this?, ?that?, ?it?
dad : demonstrative adjective, e.g. ?that triangle?
siz : size, e.g. ?the large triangle?
typ : type, e.g. ?the square?
dir : direction of a piece, e.g. ?the triangle facing the
left?.
prj : projective spatial relation (including directional
prepositions or nouns such as ?right?, ?left?,
?above?. . . ) e.g. ?the triangle to the left of the
square?
tpl : topological spatial relation (including non-
directional prepositions or nouns such as ?near?,
?middle?. . . ), e.g. ?the triangle near the square?
ovl : overlap, e.g. ?the small triangle under the large
one?
act : action on pieces, e.g ?the triangle that you are
holding now?, ?the triangle that you just rotated?
cmp : complement, e.g. ?the other one?
sim : similarity, e.g. ?the same one?
num : number, e.g. ?the two triangle?
rpr : repair, e.g. ?the big, no, small triangle?
err : obvious erroneous expression, e.g. ?the square?
referring to a triangle
nest : nested expression; when a referring expression
includes another referring expression, only the
outermost expression is annotated with this at-
tribute, e.g. ?(the triangle to the left of (the small
triangle))?
meta: metaphorical expression, e.g. ?the leg?, ?the
head?
SLAT (Noguchi et al, 2008)4. Our target expres-
sions in this corpus are referring expressions re-
ferring to a puzzle piece or a set of puzzle pieces.
We do not deal with expressions referring to a lo-
cation, a part of a piece or a constructed shape.
These expressions are put aside for future work.
The annotation of referring expressions is three-
fold: (1) identification of the span of expressions,
(2) identification of their referents, and (3) assign-
ment of a set of attributes to each referring expres-
sion.
Using the multimodal annotation tool ELAN,5
the annotations of referring expressions were then
merged with extra-linguistic data recorded by the
Tangram simulator. The available extra-linguistic
information from the simulator consists of (1) the
action on a piece, (2) the coordinates of the mouse
cursor and (3) the position of each piece in the
4We did not use SLAT for English corpus annotation. In-
stead, ELAN was directly used for annotating referring ex-
pressions.
5http://www.lat-mpi.eu/tools/elan/
41
Table 3: Summary of trials
ID time success OP-REX SV-REX ID time success OP-REX SV-REX
E01 15:00 J01 8:40 o 10 48
E02 15:00 J02 11:49 o 7 55
E03 15:00 J03 11:36 o 5 26
E04 15:00 J04 7:31 o 2 21
E05 15:00 J05 15:00 23 78
E06 15:00 J06 11:12 o 5 60
E07 15:00 J07 12:11 o 3 59
E08 15:00 J08 11:20 o 4 61
E09 10:39 o J09 14:59 o 36 84
E10 15:00 J10 6:20 o 3 47
E11 15:00 J11 5:21 o 2 14
E12 8:30 o J12 13:40 o 37 77
E13 14:33 o 8 95 J13 15:00 8 56
E14 7:27 o 1 62 J14 4:48 o 1 29
E15 14:02 o 16 127 J15 9:30 o 20 39
E16 3:57 o 1 31 J16 5:07 o 3 17
E17 13:00 o J17 13:37 o 10 46
E18 6:40 o J18 8:57 o 4 51
E19 15:00 J19 8:02 o 0 37
E20 12:32 o J20 11:23 o 1 59
E21 15:00 J21 10:12 o 7 71
E22 15:00 J22 10:24 o 9 64
E23 15:00 J23 15:00 0 69
E24 5:36 o J24 14:22 o 0 76
Ave. 12:47 6.5 78.8 Ave. 10:40 8.3 51.8
SD 3:34 7.14 41.4 SD 3:18 10.4 20.1
Total 5:06:56 10 26 315 Total 4:16:01 21 200 1,244
working area. Actions and mouse cursor positions
are recorded at intervals of 10 msec, and are ab-
stracted into (1) a time span labeled with an action
symbol (?move?, ?rotate? or ?flip?) and its target
piece number (1?7), and (2) a time span labeled
with a piece number which is under the mouse
cursor during that span. The position of pieces is
updated and recorded with a timestamp when the
position of any piece changes. Information about
piece positions is not merged into the ELAN files
and is kept in separate files. As a result, we have
11 time-aligned ELAN Tiers as shown in Table 1.
Two annotators (two of the authors) first an-
notated four Japanese dialogues separately and
based on a discussion of discrepancies, decided
on the following criteria to identify a referring ex-
pression.
? The minimum span of a noun phrase in-
cluding necessary information to identify a
referent is annotated. The span might in-
clude repairs with their reparandum and dis-
fluency (Nakatani and Hirschberg, 1993) if
needed.
? Demonstrative adjectives are included in ex-
pressions.
? Erroneous expressions are annotated with a
special attribute.
? An expression without a definite referent (i.e.
a group of possible referents or none) is as-
signed a referent number sequence consist-
ing of a prefix, followed by the sequence of
possible referents as its referent, if any are
present.
? All expressions appearing in muttering to
oneself are excluded.
Table 2 shows a list of attributes of referring
expressions used in annotating the corpus.
The rest of the 20 Japanese dialogues were an-
notated by two of the authors and discrepancies
were resolved by discussion. Four English dia-
logues have been annotated so far by one of the
authors.
4 Preliminary corpus analysis
We have already completed the Japanese corpus,
which is named REX-J (2008-08), but only 4 out
of 24 dialogues have been annotated for the En-
glish counterpart (REX-E (2010-03)). Table 3
shows a summary of the trials. The horizontal
42
lines divide the trials by pairs, ?o? in the ?suc-
cess? column denotes that the trial was success-
fully completed in the time limit (15 minutes), and
the ?OP-REX? and ?SV-REX? columns show the
number of referring expressions used by the op-
erator and the solver respectively. The following
subsections describe a preliminary comparison of
the English and Japanese corpora.
Table 4: Task completion time
Lang.\Shape (a) (b) (c) (d)
English 832.0 741.2 890.3 605.8
(105.4) (246.5) (23.7) (287.2)
Japanese 774.7 535.0 571.7 633.8
(167.3) (168.5) (242.2) (215.2)
* Average (SD)
4.1 Task performance
We conducted a two-way ANOVA with the task
completion time as the dependent variable, and
the goal shape and the language as the indepen-
dent variables. Only the main effect of the lan-
guage was significant (F (1, 40) = 5.82, p <
0.05). Table 4 shows the average and the standard
deviation of the completion time. Note that we set
a time limit (15 minutes) for solving the puzzle.
We considered the completion time as 15 minutes
even when a puzzle was not actually solved in the
time limit. We also conducted a two-way ANOVA
using only the successful cases. Both main effects
and their interaction were not significant.
We then conducted an ANOVA with the num-
ber of successfully solved puzzles by each pair as
the dependent variable and the language as the in-
dependent variable. The main effect was signifi-
cant (F (1, 10) = 6.79, p < 0.05). Table 5 shows
the average number of success goals per pair and
the success rate with their standard deviations in
parentheses.
Finally, we conducted an ANOVA with the
number of pairs who succeeded in solving a goal
Table 5: The number of solved trials and success
rates
Lang. solved trials success rate [%]
Japanese 3.50 (0.55) 87.5 (13.7)
English 1.67 (1.63) 41.7 (40.8)
* Average (SD)
shape as the dependent variable and the goal shape
as the independent variable. The main effect was
not significant.
In summary, we found a difference in the task
performance between the languages in terms of
the task completion time and the success rate, but
no difference among the goal shapes. This dif-
ference could be explained by the diversity of the
subjects rather than the difference of languages.
The Japanese subject group consisted of univer-
sity graduate students from the same department
(Cognitive Science) and roughly of the same age
(Average = 23.3, SD = 1.5). In contrast, the En-
glish subjects have diverse backgrounds (e.g. high
school students, university faculty, writer, pro-
grammer, etc.) and age (Average = 30.8, SD =
11.7). In addition, a familiarity with this kind of
geometric puzzle might have some effect. How-
ever, we collected a familiarity with the puzzle
only from the English subjects, we could not con-
duct further analysis on this viewpoint. Anyhow,
in this respect, the independent variable should
have been named ?subject group? instead of ?lan-
guage?.
4.2 Referring expressions
It is important to note that since we have only
completed the annotation of four dialogs, all by
one pair of subjects, our analyses of referring ex-
pressions are tentative and pending further analy-
sis.
We have 200 and 1,243 referring expressions by
the operator and the solver respectively, 1,444 in
total in the 24 Japanese dialogues. On the other
hand we have 26 (operator) and 315 (solver) re-
ferring expressions in 4 English dialogues. The
average number of referring expressions per di-
alogue in Table 3 suggests that English subjects
use more referring expressions than Japanese sub-
jects. Since we have only the data from a single
pair, we cannot say whether this tendency applies
to the other pairs. We cannot draw a decisive con-
clusion until we complete the annotation of the
English corpus.
Table 6 shows the total frequencies of the at-
tributes and their frequencies per dialogue. The
table gives us an impression of significantly fre-
quent use of demonstrative pronouns (dpr) by the
43
Table 6: Comparison of attribute distribution
English Japanese
(4 dialogues) (24 dialogues)
attribute frq frq/dlg frq frq/dlg
dpr 226 56.5 678 28.3
dad 29 7.3 178 7.4
siz 68 17.0 288 12.0
typ 103 25.8 655 27.3
dir 0 0 7 0.3
prj 10 2.5 141 5.9
tpl 4 1 9 0.4
ovl 0 0 2 0.1
act 5 1.3 103 4.3
cmp 17 4.3 33 1.4
sim 0 0 7 0.3
num 22 5.5 35 1.5
rpr 0 0 1 0
err 0 0 1 0
nest 1 0.3 31 1.3
meta 1 0.3 6 0.3
English subjects. The Japanese subjects use more
attributes of projective spatial relations (prj) and
actions on the referent (act).6 The English subjects
use more complement attributes (cmp) as well as
more number attributes (num).
5 Related work
Over the last decade, with a growing recogni-
tion that referring expressions frequently appear
in collaborative task dialogues (Clark and Wilkes-
Gibbs, 1986; Heeman and Hirst, 1995), a num-
ber of corpora have been constructed to study the
nature of their use. This tendency also reflects
the recognition that this area yields both challeng-
ing research topics as well as promising applica-
tions such as human-robot interaction (Foster et
al., 2008; Kruijff et al, 2010).
The COCONUT corpus (Di Eugenio et al,
2000) was collected from keyboard-dialogs be-
tween two participants, who worked together on
a simple 2-D design task, buying and arranging
furniture for two rooms. The COCONUT cor-
pus is limited in annotations which describe sym-
bolic object information such as object intrinsic
attributes and location in discrete co-ordinates. As
an initial work of constructing a corpus for collab-
orative tasks, the COCONUT corpus can be char-
acterised as having a rather simple domain as well
6We called such expressions as action-mentioning expres-
sions (AME) in our previous work.
as limited annotation.
The QUAKE corpus (Byron, 2005) and its suc-
cessor, the SCARE corpus (Stoia et al, 2008) deal
with a more complex domain, where two partici-
pants collaboratively play a treasure hunting game
in a 3-D virtual world. Despite the complexity
of the domain, the participants were only allowed
limited actions, e.g. moving step forward, pushing
a button etc.
As a part of the JAST project, the Joint Con-
struction Task (JCT) corpus was created based on
dialogues in which two participants constructed a
puzzle (Foster et al, 2008). The setting of the
experiment is quite similar to ours except that
both participants have even roles. Since our main
concern is referring expressions, we believe our
asymmetric setting elicits more referring expres-
sions than the symmetric setting of the JCT cor-
pus.
In contrast to these previous corpora, our cor-
pora record a wide range of information useful
for analysis of human reference behaviour in situ-
ated dialogue. While the domain of our corpora is
simple compared to the QUAKE and SCARE cor-
pora, we allowed a comparatively large flexibil-
ity in the actions necessary for achieving the goal
shape (i.e. flipping, turning and moving of puzzle
pieces at different degrees), relative to the com-
plexity of the domain. Providing this relatively
larger freedom of actions to the participants to-
gether with the recording of detailed information
allows for research into new aspects of referring
expressions.
As for a multilingual aspect, all the above cor-
pora are English. There have been several recent
attempts at collecting multilingual corpora in situ-
ated domains. For instance, (Gargett et al, 2010)
collected German and English corpora in the same
setting. Their domain is similar to the QUAKE
corpus. Van der Sluis et al (2009) aim at a com-
parative study of referring expressions between
English and Japanese. Their domain is still static
at the moment. Our corpora aim at dealing with
the dynamic nature of situated dialogues between
very different languages, English and Japanese.
44
Table 7: The REX-J corpus family
name puzzle #pairs #dialg. #valid status
T2008-08 Tangram 6 24 24 completed
T2009-03 Tangram 10 40 16 completed
T2009-11 Tangram 10 36 27 validating
N2009-11 Tangram 5 20 8 validating
P2009-11 Polyomino 7 28 24 annotating
D2009-11 2-Tangram 7 42 24 annotating
6 Conclusion and future work
This paper presented an overview of our English-
Japanese bilingual multimodal corpora of refer-
ring expressions in a collaborative problem solv-
ing setting. The Japanese corpus was completed
and has already been used for research (Spanger et
al., 2009b; Spanger et al, 2010; Iida et al, 2010),
but the English counterpart is still undergoing an-
notation. We have also presented a preliminary
comparative analysis of these corpora in terms of
the task performance and usage of referring ex-
pressions. We found a significant difference of the
task performance, which could be attributed to the
difference in diversity of subjects. We have tenta-
tive results on the usage of referring expressions,
since only four English dialogues are available at
the moment.
The data collection experiments were con-
ducted in August 2008 for Japanese and in March
2010 for English. Between these periods, we
conducted various data collections to build differ-
ent types of Japanese corpora (March, 2009 and
November 2009). These experiments involve cap-
turing eye-gaze information of participants during
problem solving, and introducing variants of puz-
zles (Polyomino, Double Tangram and Tangram
without any hints7). They are also under prepa-
ration for publication. Table 7 gives an overview
of the REX-J corpus family, where ?#valid? de-
notes the number of dialogues with valid eye-
gaze data. Eye-gaze data is difficult to capture
cleanly throughout a dialogue. We discarded di-
alogues in which eye-gaze was captured success-
fully less than 70% of the total time of the dia-
logue. Namely, we annotated or will annotate di-
alogues with validated eye-gaze data only.
These corpora enable research on utilising eye-
gaze information in reference resolution and gen-
7N2009-11 in Table 7
eration, and evaluation in different tasks (puzzles)
as well. We are planning to distribute the REX-J
corpus family through GSK (Language Resources
Association in Japan)8, and the REX-E corpus
from both University of Brighton and GSK.
References
Baran, Bahar, Berrin Dogusoy, and Kursat Cagiltay.
2007. How do adults solve digital tangram prob-
lems? Analyzing cognitive strategies through eye
tracking approach. InHCI International 2007 - 12th
International Conference - Part III, pages 555?563.
Bard, Ellen Gurman, Robin Hill, Manabu Arai, and
Mary Ellen Foster. 2009. Accessibility and atten-
tion in situated dialogue: Roles and regulations. In
Proceedings of the Workshop on Production of Re-
ferring Expressions Pre-CogSci 2009.
Belz, Anja, Eric Kow, Jette Viethen, and Albert Gatt.
2010. Referring expression generation in context:
The GREC shared task evaluation challenges. In
Krahmer, Emiel and Marie?t Theune, editors, Empir-
ical Methods in Natural Language Generation, vol-
ume 5980 of Lecture Notes in Computer Science.
Springer-Verlag, Berlin/Heidelberg.
Byron, Donna K. 2005. The OSU Quake 2004 cor-
pus of two-party situated problem-solving dialogs.
Technical report, Department of Computer Science
and Enginerring, The Ohio State University.
Clark, H. Herbert. and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22:1?39.
Di Eugenio, Barbara, Pamela W. Jordan, Richmond H.
Thomason, and Johanna. D. Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogues. International Journal of Human-Computer
Studies, 53(6):1017?1076.
Foster, Mary Ellen and Jon Oberlander. 2007. Corpus-
based generation of head and eyebrow motion for
an embodied conversational agent. Language Re-
sources and Evaluation, 41(3?4):305?323, Decem-
ber.
Foster, Mary Ellen, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring ex-
pressions in cooperative, task-based human-robot
dialogue. In Proceedings of 3rd Human-Robot In-
teraction, pages 295?302.
8http://www.gsk.or.jp/index e.html
45
Gargett, Andrew, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The give-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC 2010), pages 2401?2406.
Heeman, Peter A. and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21:351?382.
Hobbs, Jerry R. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Iida, Ryu, Shumpei Kobayashi, and Takenobu Toku-
naga. 2010. Incorporating extra-linguistic informa-
tion into reference resolution in collaborative task
dialogue. In Proceedings of 48th Annual Meeting
of the Association for Computational Linguistics,
pages 1259?1267.
Kiyokawa, Sachiko and Midori Nakazawa. 2006. Ef-
fects of reflective verbalization on insight problem
solving. In Proceedings of 5th International Con-
ference of the Cognitive Science, pages 137?139.
Kruijff, Geert-Jan M., Pierre Lison, Trevor Ben-
jamin, Henrik Jacobsson, Hendrik Zender, and
Ivana Kruijff-Korbayova. 2010. Situated dialogue
processing for human-robot interaction. In Cogni-
tive Systems: Final report of the CoSy project, pages
311?364. Springer-Verlag.
Martin, Jean-Claude, Patrizia Paggio, Peter Kuehnlein,
Rainer Stiefelhagen, and Fabio Pianesi. 2007. Spe-
cial issue on Mulitmodal corpora for modeling hu-
man multimodal behaviour. Language Resources
and Evaluation, 41(3-4).
Mitkov, Ruslan. 2002. Anaphora Resolution. Long-
man.
Nakatani, Christine and Julia Hirschberg. 1993. A
speech-first model for repair identification and cor-
rection. In Proceedings of 31th Annual Meeting of
ACL, pages 200?207.
Noguchi, Masaki, Kenta Miyoshi, Takenobu Toku-
naga, Ryu Iida, Mamoru Komachi, and Kentaro
Inui. 2008. Multiple purpose annotation using
SLAT ? Segment and link-based annotation tool.
In Proceedings of 2nd Linguistic Annotation Work-
shop, pages 61?64.
Piwek, Paul L. A. 2007. Modality choise for gen-
eration of referring acts. In Proceedings of the
Workshop on Multimodal Output Generation (MOG
2007), pages 129?139.
Spanger, Philipp, Masaaki Yasuhara, Ryu Iida, and
Takenobu Tokunaga. 2009a. A Japanese corpus
of referring expressions used in a situated collab-
oration task. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 110 ? 113.
Spanger, Philipp, Masaaki Yasuhara, Ryu Iida, and
Takenobu Tokunaga. 2009b. Using extra linguistic
information for generating demonstrative pronouns
in a situated collaboration task. In Proceedings of
PreCogSci 2009: Production of Referring Expres-
sions: Bridging the gap between computational and
empirical approaches to reference.
Spanger, Philipp, Ryu Iida, Takenobu Tokunaga,
Asuka Teri, and Naoko Kuriyama. 2010. Towards
an extrinsic evaluation of referring expressions in
situated dialogs. In Kelleher, John, Brian Mac
Namee, and Ielka van der Sluis, editors, Proceed-
ings of the Sixth International Natural Language
Generation Conference (INGL 2010), pages 135?
144.
Sternberg, Robert J. and Janet E. Davidson, editors.
1996. The Nature of Insight. The MIT Press.
Stoia, Laura, Darla Magdalene Shockley, Donna K.
Byron, and Eric Fosler-Lussier. 2008. SCARE:
A situated corpus with annotated referring expres-
sions. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC 2008), pages 28?30.
Suzuki, Hiroaki, Keiga Abe, Kazuo Hiraki, and
Michiko Miyazaki. 2001. Cue-readiness in in-
sight problem-solving. In Proceedings of the 23rd
Annual Meeting of the Cognitive Science Society,
pages 1012 ? 1017.
van Deemter, Kees, Ielka van der Sluis, and Albert
Gatt. 2006. Building a semantically transparent
corpus for the generation of referring expressions.
In Proceedings of the Fourth International Natural
Language Generation Conference, pages 130?132.
van der Sluis, Ielka, Junko Nagai, and Saturnino Luz.
2009. Producing referring expressions in dialogue:
Insights from a translation exercise. In Proceedings
of PreCogSci 2009: Production of Referring Ex-
pressions: Bridging the gap between computational
and empirical approaches to reference.
46
Towards an Extrinsic Evaluation of Referring Expressions
in Situated Dialogs
Philipp SPANGER IIDA Ryu TOKUNAGA Takenobu
{philipp,ryu-i,take}@cl.cs.titech.ac.jp
TERAI Asuka KURIYAMA Naoko
asuka@nm.hum.titech.ac.jp kuriyama@hum.titech.ac.jp
Tokyo Institute of Technology
Abstract
In the field of referring expression gener-
ation, while in the static domain both in-
trinsic and extrinsic evaluations have been
considered, extrinsic evaluation in the dy-
namic domain, such as in a situated col-
laborative dialog, has not been discussed
in depth. In a dynamic domain, a cru-
cial problem is that referring expressions
do not make sense without an appropriate
preceding dialog context. It is unrealistic
for an evaluation to simply show a human
evaluator the whole period from the be-
ginning of a dialog up to the time point
at which a referring expression is used.
Hence, to make evaluation feasible it is
indispensable to determine an appropriate
shorter context. In order to investigate the
context necessary to understand a referring
expression in a situated collaborative dia-
log, we carried out an experiment with 33
evaluators and a Japanese referring expres-
sion corpus. The results contribute to find-
ing the proper contexts for extrinsic evalu-
tion in dynamic domains.
1 Introduction
In recent years, the NLG community has paid sig-
nificant attention to the task of generating referring
expressions, reflected in the seting-up of several
competitive events such as the TUNA and GIVE-
Challenges at ENLG 2009 (Gatt et al, 2009; By-
ron et al, 2009).
With the development of increasingly complex
generation systems, there has been heightened in-
terest in and an ongoing significant discussion on
different evaluation measures for referring expres-
sions. This discussion is carried out broadly in the
field of generation, including in the multi-modal
domain, e.g. (Stent et al, 2005; Foster, 2008).
!"#$%&
!"#$%&'()$)&'
'($)*$+%"&,#-+."/
*+),&#(&'
&#),&#(&'
-$,$./#&0!"#$%1
234456
78$#0!"#$%10
234496
:$#0!*,0;<=&(0!"#$%1
2344>6
;)/&$0!"#$%1
234456
?/,!$#0@A$<B*,
2344C6
D*<E0@0F$))0
2344G6
H8&(0I$I*,
234J46
K/()*,#!"#$%&#
234496
Figure 1: Overview of recent work on evaluation
of referring expressions
Figure 1 shows a schematic overview of recent
work on evaluation of referring expressions along
the two axes of evaluation method and domain in
which referring expressions are used.
There are two different evaluation methods cor-
responding to the bottom and the top of the verti-
cal axis in Figure 1: intrinsic and extrinsic eval-
uations (Sparck Jones and Galliers, 1996). In-
trinsic methods often measure similarity between
the system output and the gold standard corpora
using metrics such as tree similarity, string-edit-
distance and BLEU (Papineni et al, 2002). Intrin-
sic methods have recently become popular in the
NLG community. In contrast, extrinsic methods
evaluate generated expressions based on an exter-
nal metric, such as its impact on human task per-
formance.
While intrinsic evaluations have been widely
used in NLG, e.g. (Reiter et al, 2005), (Cahill
and van Genabith, 2006) and the competitive 2009
TUNA-Challenge, there have been a number of
criticisms against this type of evaluation. (Reiter
and Sripada, 2002) argue, for example, that gener-
ated text might be very different from a corpus but
still achieve the specific communicative goal.
An additional problem is that corpus-similarity
metrics measure how well a system reproduces
what speakers (or writers) do, while for most NLG
systems ultimately the most important considera-
tion is its effect on the human user (i.e. listener
or reader). Thus (Khan et al, 2009) argues that
?measuring human-likeness disregards effective-
ness of these expressions?.
Furthermore, as (Belz and Gatt, 2008) state
?there are no significant correlations between in-
trinsic and extrinsic evaluation measures?, con-
cluding that ?similarity to human-produced refer-
ence texts is not necessarily indicative of quality
as measured by human task performance?.
From early on in the NLG community, task-
based extrinsic evaluations have been considered
as the most meaningful evaluation, especially
when having to convince people in other commu-
nities of the usefulness of a system (Reiter and
Belz, 2009). Task performance evaluation is rec-
ognized as the ?only known way to measure the ef-
fectiveness of NLG systems with real users? (Re-
iter et al, 2003). Following this direction, the
GIVE-Challenges (Koller et al, 2009) at INLG
2010 (instruction generation) also include a task-
performance evaluation.
In contrast to the vertical axis of Figure 1, there
is the horizontal axis of the domain in which refer-
ring expressions are used. Referring expressions
can thus be distinguished according to whether
they are used in a static or a dynamic domain, cor-
responding to the left and right of the horizontal
axis of Figure 1. A static domain is one such as the
TUNA corpus (van Deemter, 2007), which col-
lects referring expressions based on a motionless
image. In contrast, a dynamic domain comprises a
constantly changing situation where humans need
context information to identify the referent of a re-
ferring expression.
Referring expressions in the static domain have
been evaluated relatively extensively. A recent ex-
ample of an intrinsic evaluation is (van der Sluis
et al, 2007), who employed the Dice-coefficient
measuring corpus-similarity. There have been a
number of extrinsic evaluations as well, such as
(Paraboni et al, 2006) and (Khan et al, 2009), re-
spectively measuring the effect of overspecifica-
tion on task performance and the impact of gener-
ated text on accuracy as well as processing speed.
They belong thus in the top-left quadrant of Fig-
ure 1.
Over a recent period, research in the generation
of referring expressions has moved to dynamic do-
mains such as situated dialog, e.g. (Jordan and
Walker, 2005) and (Stoia et al, 2006). However,
both of them carried out an intrinsic evaluation
measuring corpus-similarity or asking evaluators
to compare system output to expressions used by
human (the right bottom quadrant in Figure 1).
The construction of effective generation sys-
tems in the dynamic domain requires the imple-
mentation of an extrinsic task performance evalu-
ation. There has been work on extrinsic evalua-
tion of instructions in the dynamic domain on the
GIVE-2 challenge (Byron et al, 2009), which is a
task to generate instructions in a virtual world. It is
based on the GIVE-corpus (Gargett et al, 2010),
which is collected through keyboard interaction.
The evaluation measures used are e.g. the number
of successfully completed trials, completion time
as well as the numbers of instructions the system
sent to the user. As part of the JAST project, a
Joint Construction Task (JCT) puzzle construction
corpus (Foster et al, 2008) was created which is
similar in some ways in its set-up to the REX-
J corpus which we use in the current research.
There has been some work on evaluating gener-
ation strategies of instructions for a collaborative
construction task on this corpus, both considering
intrinsic as well as extrinsic measures (Foster et
al., 2009). Their main concern is, however, the in-
teraction between the text structure and usage of
referring expressions. Therefore, their ?context?
was given a priori.
However, as can be seen from Figure 1, in the
field of referring expression generation, while in
the static domain both intrinsic and extrinsic eval-
uations have been considered, the question of re-
alizing an extrinsic evaluation in the dynamic do-
main has not been dealt with in depth by previous
work. This paper addresses this shortcoming of
previous work and contributes to ?filling in? the
missing quadrant of Figure 1 (the top-right).
The realization of such an extrinsic evaluation
faces one key difficulty. In a static domain, an ex-
trinsic evaluation can be realized relatively easily
by showing evaluators the static context (e.g. any
image) and a referring expression, even though
this is still costly in comparison to intrinsic meth-
ods (Belz and Gatt, 2008).
In contrast, an extrinsic evaluation in the dy-
namic domain needs to present an evaluator with
the dynamic context (e.g. a certain length of the
recorded dialog) preceding a referring expression.
It is clearly not feasible to simply show the whole
preceding dialog; this would make even a very
small-scale evaluation much too costly. Thus, in
order to realize a cost-effective extrinsic evalua-
tion in a dynamic domain we have to deal with the
additional parameter of time length and content of
the context shown to evaluators.
This paper investigates the context necessary for
humans to understand different types of referring
expressions in a situated domain. This work thus
charts new territory and contributes to developing
a extrinsic evaluation in a dynamic domain. Sig-
nificantly, we consider not only linguistic but also
extra-linguistic information as part of the context,
such as the actions that have been carried out in the
preceding interaction. Our results indicate that, at
least in this domain, extrinsic evaluation results
in dynamic domains can depend on the specific
amount of context shown to the evaluator. Based
on the results from our evaluation experiments, we
discuss the broader conclusions to be drawn and
directions for future work.
2 Referring Expressions in the REX-J
Corpus
We utilize the REX-J corpus, a Japanese corpus
of referring expressions in a situated collaborative
task (Spanger et al, 2009a). It was collected by
recording the interaction of a pair of dialog partic-
ipants solving the Tangram puzzle cooperatively.
The goal of the Tangram puzzle is to construct a
given shape by arranging seven pieces of simple
figures as shown in Figure 2
!"#$%&'#()
*"+,-.!%#+)#
Figure 2: Screenshot of the Tangram simulator
In order to record the precise position of every
piece and every action by the participants, we im-
plemented a simulator. The simulator displays two
areas: a goal shape area, and a working area where
pieces are shown and can be manipulated.
We assigned different roles to the two partici-
pants of a pair: solver and operator. The solver
can see the goal shape but cannot manipulate the
pieces and hence gives instructions to the opera-
tor; by contrast, the operator can manipulate the
pieces but can not see the goal shape. The two
participants collaboratively solve the puzzle shar-
ing the working area in Figure 2.
In contrast to other recent corpora of refer-
ring expressions in situated collaborative tasks
(e.g. COCONUT corpus (Di Eugenio et al, 2000)
and SCARE corpora (Byron et al, 2005)), in
the REX-J corpus we allowed comparatively large
real-world flexibility in the actions necessary to
achieve the task (such as flipping, turning and
moving of puzzle pieces at different degrees), rel-
ative to the task complexity. The REX-J corpus
thus allows us to investigate the interaction of lin-
guistic and extra-linguistic information. Interest-
ingly, the GIVE-2 challenge at INLG 2010 notes
its ?main novelty? is allowing ?continuous moves
rather than discrete steps as in GIVE-1?. Our work
is in line with the broader research trend in the
NLG community of trying to get away from sim-
ple ?discrete? worlds to more realistic settings.
The REX-J corpus contains a total of 1,444 to-
kens of referring expressions in 24 dialogs with a
total time of about 4 hours and 17 minutes. The
average length of each dialog is 10 minutes 43
seconds. The asymmetric data-collection setting
encouraged referring expressions from the solver
(solver: 1,244 tokens, operator: 200 tokens). We
exclude from consideration 203 expressions refer-
ring to either groups of pieces or whose referent
cannot be determined due to ambiguity, thus leav-
ing us 1,241 expressions.
We identified syntactic/semantic features in the
collected referring expressions as listed in Table 1:
(a) demonstratives (adjectives and pronouns), (b)
object attribute-values, (c) spatial relations and (d)
actions on an object. The underlined part of the
examples denotes the feature in question.
3 Design of Evaluation Experiment
The aim of our experiment is to investigate the
?context? (content of the time span of the recorded
Table 1: Syntactic and semantic features of refer-
ring expressions in the REX-J corpus
Feature Tokens Example
(a) demonstrative 742 ano migigawa no sankakkei
(that triangle at the right side)
(b) attribute 795 tittyai sankakkei
(the small triangle)
(c) spatial relations 147 hidari no okkii sankakkei
(the small triangle on the left)
(d) action-mentioning 85 migi ue ni doketa sankakkei
(the triangle you put away to
the top right)
interaction prior to the uttering of the referring ex-
pression) necessary to enable successful identifi-
cation of the referent of a referring expression.
Our method is to vary the context presented to
evaluators and then to study the impact on human
referent identification. In order to realize this, for
each instance of a referring expression, we vary
the length of the video shown to the evaluator.
!"#$ %&'()*+,!-./0#
!1#$ 234*&,&5,678+*4,9&+:3(;,8+*8,3(,)7*,63<'=8)&+
!%#$ >))*+8(?*,3(?='43(;,)7*,+*5*++3(;,*@A+*663&(,)&,
*B8='8)*,!67&9(,3(,+*4#
!C#$ D)8+)E+*A*8),F'))&(
!G#$ D*=*?)3&(,F'))&(6,!-.H#,8(4,IJ,4&(K),:(&9I.F'))&(,,,,,,,,,,
!"#
!1#
!%#
!C#
!G#
Figure 3: The interface presented to evaluators
The basic procedure of our evaluation experi-
ment is as follows:
(1) present human evaluators with speech and
video from a dialog that captures shared
working area of a certain length previous to
the uttering of a referring expression,
(2) stop the video and display as text the next
solver?s utterance including the referring ex-
pression (shown in red),
(3) ask the evaluator to identify the referent
of the presented referring expression (if the
evaluator wishes, he/she can replay the video
as many times as he likes),
(4) proceed to the next referring expression (go
to (1)).
Figure 3 shows a screenshot of the interface pre-
pared for this experiment.
The test data consists of three types of referring
expressions: DPs (demonstrative pronouns),
AMEs (action-mentioning expressions), and
OTHERs (any other expression that is neither a
DP nor AME, e.g intrinsic attributes and spatial
relations). DPs are the most frequent type of
referring expression in the corpus. AMEs are
expressions that utilize an action on the referent
such as ?the triangle you put away to the top
right? (see Table 1)1. As we pointed out in our
previous paper (Spanger et al, 2009a), they are
also a fundamental type of referring expression in
this domain.
The basic question in investigating a suitable
context is what information to consider about the
preceding interaction; i.e. over what parameters to
vary the context. In previous work on the gener-
ation of demonstrative pronouns in a situated do-
main (Spanger et al, 2009b), we investigated the
role of linguistic and extra-linguistic information,
and found that time distance from the last action
(LA) on the referent as well as the last mention
(LM) to the referent had a significant influence on
the usage of referring expressions. Based on those
results, we focus on the information on the refer-
ent, namely LA and LM.
For both AMEs and OTHERs, we only consider
two possibilities of the order in which LM and LA
appear before a referring expression (REX), de-
pending on which comes first. These are shown in
Figure 4, context patterns (a) LA-LM and (b) LM-
LA. Towards the very beginning of a dialog, some
referring expressions have no LM and LA; those
expressions are not considered in this research.
All instances of AMEs and OTHERs in our test
data belong to either the LA-LM or the LM-LA
1An action on the referent is usually described by a verb
as in this example. However, there are cases with a verb el-
lipsis. While this would be difficult in English, it is natural
and grammatical in Japanese.
!"#$%&'()&*+'()&
,($%
-./%+ !"#$
%&
'"()
!"#$%&'%($)"**+,-
!"#*+'()&$%&'()&
,($%
-./%+ !"#$
%*
'"()
!.#$%('%&$)"**+,-
!"#
$%&'()&
,($%
-./%+
%*
'"()
!/#$%('%&0$)"**+,-
*+'()&
-./%+
-./%+
Figure 4: Schematic overview of the three context
Patterns
pattern. For each of these two context patterns,
there are three possible contexts2: Both (including
both LA and LM), LA/LM (including either LA or
LM) and None (including neither). Depending on
the order of LA and LM prior to an expression,
only one of the variations of LA/LM is possible
(see Figure 4 (a) and (b)).
In contrast, DPs tend to be utilized in a deic-
tic way in such situated dialogs (Piwek, 2007).
We further noted in (Spanger et al, 2009b), that
DPs in a collaborative task are also frequently used
when the referent is under operation. While they
belong neither to the LA-LM nor the LM-LA pat-
tern, it would be inappropriate to exclude those
cases. Hence, for DPs we consider another situa-
tion where the last action on the referent overlaps
with the utterance of the DP (Figure 4 (c) LM-LA?
pattern). In this case, we consider an ongoing op-
eration on the referent as a ?last action?. Another
peculiarity of the LM-LA? pattern is that we have
no None context in this case, since there is no way
to show a video without showing LA (the current
operation).
Given the three basic variations of context, we
recruited 33 university students as evaluators and
2To be more precise, we set a margin at the beginning of
contexts as shown in Figure 4.
divided them equally into three groups, i.e. 11
evaluators per group. As for the referring ex-
pressions to evaluate, we selected 60 referring ex-
pressions used by the solver from the REX-J cor-
pus (20 from each category), ensuring all were
correctly understood by the operator during the
recorded dialog. We selected those 60 instances
from expressions where both LM and LA ap-
peared within the last 30 secs previous to the re-
ferring expression. This selection excludes initial
mentions, as well as expressions where only LA
or only LM exists or they do not appear within 30
secs. Hence the data utilized for this experiment
is limited in this sense. We need further experi-
ments to investigate the relation between the time
length of contexts and the accuarcy of evaluators.
We will return to this issue in the conclusion.
We combined 60 referring expressions and the
three contexts to make the test instances. Follow-
ing the Latin square design, we divided these test
instances into three groups, distributing each of
the three contexts for every referring expression
to each group. The number of contexts was uni-
formly distributed over the groups. Each instance
group was assigned to each evaluator group.
For each referring expression instance, we
record whether the evaluator was able to correctly
identify the referent, how long it took them to
identify it and whether they repeated the video
(and if so how many times).
Reflecting the distribution of the data available
in our corpus, the number of instances per context
pattern differs for each type of referring expres-
sion. For AMEs, overwhelmingly the last action
on the referent was more recent than the last men-
tion. Hence we have only two LA-LM patterns
among the 20 AMEs in our data. For OTHERs, the
balance is 8 to 12, with a slight majority of LM-
LA patterns. For DPs, there is a strong tendency to
use a DP when a piece is under operation (Spanger
et al, 2009b). Of the 20 DPs in the data, 2 were
LA-LM, 5 were LM-LA pattern while 13 were of
the LM-LA? pattern (i.e. their referents were under
operation at the time of the utterance). For these
13 instances of LM-LA? we do not have a None
context.
The average stimulus times, i.e. time period of
presented context, were 7.48 secs for None, 11.04
secs for LM/LA and 18.10 secs for Both.
Table 2: Accuracy of referring expression identification per type and context
Type context pattern\Context None LM/LA Both Increase [None ? Both]
(LA-LM) 0.909 0.955 0.955 0.046
DP (20/22) (21/22) (21/22)
(LM-LA) 0.455 0.783 0.843 0.388
(25/55) (155/198) (167/198)
Total 0.584 0.800 0.855 0.271
(LA-LM) 0.227 0.455 0.682 0.455
AME (5/22) (10/22) (15/22)
(LM-LA) 0.530 0.859 0.879 0.349
(105/198) (170/198) (174/198)
Total 0.500 0.818 0.859 0.359
(LA-LM) 0.784 0.852 0.943 0.159
OTHER (69/88) (75/88) (83/88)
(LM-LA) 0.765 0.788 0.879 0.114
(101/132) (104/132) (116/132)
Total 0.773 0.814 0.905 0.132
Overall 0.629 0.811 0.903 0.274
(325/517) (535/660) (576/638)
4 Results and Analysis
In this section we discuss the results of our evalua-
tion experiment. In total 33 evaluators participated
in our experiment, each solving 60 problems of
referent identification. Taking into account the ab-
sence of the None context for the DPs of the LM-
LA? pattern (see (c) in Figure 4), we have 1,815
responses to analyze. We focus on the impact of
the three contexts on the three types of referring
expressions, considering the two context patterns
LA-MA and LM-LA.
4.1 Overview of Results
Table 2 shows the micro averages of the accura-
cies of referent identification of all evaluators over
different types of referring expressions with differ-
ent contexts. Accuracies increase with an increase
in the amount of information in the context; from
None to Both by between 13.2% (OTHERs) and
35.9% (AMEs). The average increase of accuracy
is 27.4%.
Overall, for AMEs the impact of the context is
the greatest, while for OTHERs it is the smallest.
This is not surprising given that OTHERs tend to
include intrinsic attributes of the piece and its spa-
tial relations, which are independent of the pre-
ceding context. We conducted ANOVA with the
context as the independent variable, testing its ef-
fect on identification accuracy. The main effect
of the context was significant on accuracy with
F (2, 1320) = 9.17, p < 0.01. Given that for
DPs we did not have an even distribution between
contexts, we only utilized the results of AMEs and
OTHERs.
There are differences between expression types
in terms of the impact of addition of LM/LA into
the context, which underlines that when studying
context, the relative role and contribution of LA
and LM (and their interaction) must be looked at in
detail for different types of referring expressions.
Over all referring expressions, the addition into
a None context of LM yields an average increase
in accuracy of 9.1% for all referring expression
types, while for the same conditions the addition
of LA yields an average increase of 21.3%. Hence,
interestingly for our test data, the addition of LA
to the context has a positive impact on accuracy by
more than two times over the addition of LM.
It is also notable that even with neither LA nor
LM present (i.e. the None context), the evaluators
were still able to correctly identify referents in be-
tween 50?68.6% (average: 62.9%) of the cases.
While this accuracy would be insufficient for the
evaluation of machine generated referring expres-
sions, it is still higher than one might expect and
further investigation of this case is necessary.
4.2 Demonstrative Pronouns
For DPs, there is a very clear difference between
the two patterns (LM-LA and LA-LM) in terms of
the increase of accuracy with a change of context.
While accuracy for the LA-LM pattern remains at
a high level (over 90%) for all three contexts (and
there is only a very small increase from None to
Both), for the LM-LA pattern there is a strong in-
crease from None to Both of 38.8%.
The difference in accuracy between the two
context patterns of DPs in the None context might
come from the mouse cursor effect. The two ex-
pressions of LA-LM pattern happened to have a
mouse cursor on the referent, when they were
used, resulting in high accuracy. On the other
hand, 4 out of 5 expressions of LM-LA pattern did
not have a mouse cursor on the referent. We have
currently no explanation for the relation between
context patterns and the mouse position. While
we have only 7 expressions in the None context
for DPs and hence cannot draw any decisive con-
clusions, we note that the impact of the mouse po-
sition is a likely factor.
For the LM-LA pattern, there is an increase
in accuracy of 32.8% from None to the LA-
context. Overwhelmingly, this represents in-
stances in which the referents are being operated
at the point in time when the solver utters a DP
(this is in fact the LM-LA? pattern, which has no
None context). For those instances, the current
operation information is sufficient to identify the
referents. In contrast, addition of LM leads only
to a small increase in accuracy of 5.6%. This re-
sult is in accordance with our previous work on the
generation of DPs, which stressed the importance
of extra-linguistic information in the framework of
considering the interaction between linguistic and
extra-linguistic information.
4.3 Action-mentioning Expressions
While for AMEs the number of instances is very
uneven between patterns (similar to the distribu-
tion for DPs), there is a strong increase in accuracy
from the None context to the Both context for both
patterns (between 30% to almost 50%). However,
there is a difference between the two patterns in
terms of the relative contribution of LM and LA to
this increase.
While for the LA-LM pattern the impact of
adding LM and LA is very similar, for the LM-LA
pattern the major increase in accuracy is due to
adding LA into the None context. This indicates
that for AMEs, LA has a stronger impact on ac-
curacy than LM, as is to be expected. The strong
increase for AMEs of the LM-LA pattern when
adding LA into the context is not surprising, given
that the evaluators were able to see the action men-
tioned in the AME.
For the opposite reason, it is not surprising that
AMEs show the lowest accuracy in the None con-
text, given that the last action on the referent is
not seen by the evaluators. However, accuracy
was still slightly over 50% in the LM-LA pattern.
Overall, of the 18 instances of AMEs of the LM-
LA pattern, in the None context a majority of eval-
uators correctly identified 9 and erred on the other
9. Further analysis of the difference between cor-
rectly and incorrectly identified AMEs led us to
note again the important role of the mouse cursor
also for AMEs.
Comparing to the LM-LA pattern, we had very
low accuracy even with the Both context. As we
mentioned in the previous section, we had very
skewed test instances for AME, i.e. 18 LM-LA
patterns vs. 2 LA-LM patterns. We need further
investigation on the LA-LM pattern of AME with
more large number of instances.
Of the 18 LM-LA instances of AMEs, there are
14 instances that mention a verb describing an ac-
tion on the referent. The referents of 6 of those
14 AMEs were correctly determined by the evalu-
ators and in all cases the mouse cursor played an
important role in enabling the evaluator to deter-
mine the referent. The evaluators seem to utilize
the mouse position at the time of the uttering of the
referring expression as well as mouse movements
in the video shown. In contrast, for 8 out of the
9 incorrectly determined AMEs no such informa-
tion from the mouse was available. There was a
very similar pattern for AMEs that did not include
a verb. These points indicate that movements and
the position of the mouse both during the video as
well as the time point of the uttering of the refer-
ring expression give important clues to evaluators.
4.4 Other Expressions
There is a relatively even gain in identification ac-
curacy from None to Both of between about 10?
15% for both patterns. However, there is a simi-
lar tendency as for AMEs, since there is a differ-
ence between the two patterns in terms of the rel-
ative contribution of LM and LA to this increase.
While for the LA-LM pattern the impact of adding
LM and LA is roughly equivalent, for the LM-LA
pattern the major increase in accuracy is due to
adding LM into the LA-context.
For this pattern of OTHERs, LM has a stronger
impact on accuracy than LA, which is exactly the
opposite tendency to AMEs. For OTHERs (e.g.
use of attributes for object identification), seeing
the last action on the target has a less positive im-
pact than listening to the last linguistic mention.
Furthermore, we note the relatively high accuracy
in the None context for OTHERs, underlining the
context-independence of expressions utilizing at-
tributes and spatial relations of the pieces.
4.5 Error Analysis
We analyzed those instances whose referents were
not correctly identified by a majority of evalua-
tors in the Both context. Among the three expres-
sion types, there were about 13?16% of wrong an-
swers. In total for 7 of the 60 expressions a ma-
jority of evaluators gave wrong answers (4 DPs, 2
AMEs and 1 OTHER). Analysis of these instances
indicates that some improvements of our concep-
tion of ?context? is needed.
For 3 out of the 4 DPs, the mouse was not over
the referent or was closer to another piece. In addi-
tion, these DPs included expressions that pointed
to the role of a piece in the overall construction of
the goal shape, e.g. ?soitu ga atama (that is the
head)?, or where a DP is used as part of a more
complex referring expression, e.g. ?sore to onazi
katati . . . (the same shape as this)?, intended to
identify a different piece. For a non-participant
of the task, such expressions might be difficult to
understand in any context. This phenomenon is
related to the ?overhearer-effect? (Schober et al,
1989).
The two AMEs that the majority of evaluators
failed to identify in the Both context were also
misidentified in the LA context. Both AMEs were
missing a verb describing an action on the referent.
While for AMEs including a verb the accuracy in-
creased from None to Both by 50%, for AMEs
without a verb there was an increase by slightly
over 30%, indicating that in the case where an
AME lacks a verb, the context has a smaller pos-
itive impact on accuracy than for AMEs that in-
clude a verb. In order to account for those cases,
further work is necessary, such as investigating
how to account for the information on the distrac-
tors.
5 Conclusions and Future Work
In order to address the task of designing a flexi-
ble experiment set-up with relatively low cost for
extrinsic evaluations of referring expressions, we
investigated the context that needs to be shown to
evaluators in order to correctly determine the ref-
erent of an expression.
The analysis of our results showed that the con-
text had a significant impact on referent identifi-
cation. The impact was strongest for AMEs and
DPs and less so for OTHERs. Interestingly, we
found for both DPs and AMEs that including LA
in the context had a stronger positive impact than
including LM. This emphasizes the importance of
taking into account extra-linguistic information in
a situated domain, as considered in this study.
Our analysis of those expressions whose refer-
ent was incorrectly identified in the Both context
indicated some directions for improving the ?con-
text? used in our experiments, for example look-
ing further into AMEs without a verb describing
an action on the referent. Generally, there is a
necessity to account for mouse movements during
the video shown to evaluators as well as the prob-
lem for extrinsic evaluations of how to address the
?overhearer?s effect?.
While likely differing in the specifics of the set-
up, the methodology in the experiment design dis-
cussed in this paper is applicable to other domains,
in that it allows a low-cost flexible design of eval-
uating referring expressions in a dynamic domain.
In order to avoid the additional effort of analyzing
cases in relation to LM and LA, in the future it will
be desirable to simply set a certain time period and
base an evaluation on such a set-up.
However, we cannot simply assume that a
longer context would yield a higher identification
accuracy, given that evaluators in our set-up are
not actively participating in the interaction. Thus
there is a possibility that identification accuracy
actually decreases with longer video segments,
due to a loss of the evaluator?s concentration. Fur-
ther investigation of this question is indicated.
Based on the work reported in this paper, we
plan to implement an extrinsic task-performance
evaluation in the dynamic domain. Even with
the large potential cost-savings based on the re-
sults reported in this paper, extrinsic evaluations
will remain costly. Thus one important future task
for extrinsic evaluations will be to investigate the
correlation between extrinsic and intrinsic evalua-
tion metrics. This in turn will enable the use of
cost-effective intrinsic evaluations whose results
are strongly correlated to task-performance eval-
uations. This paper made an important contribu-
tion by pointing the direction for further research
in extrinsic evaluations in the dynamic domain.
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197?200.
Donna Byron, Thomas Mampilly, Vinay Sharma, and
Tianfang Xu. 2005. Utilizing visual attention for
cross-modal coreference interpretation. In CON-
TEXT 2005, pages 83?96.
Donna Byron, Alexander Koller, Kristina Striegnitz,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2009. Report on the First NLG
Challenge on Generating Instructions in Virtual En-
vironments (GIVE). In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 165?173.
Aoife Cahill and Josef van Genabith. 2006. Ro-
bust PCFG-based generation using automatically
acquired lfg approximations. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033?
1040.
Barbara Di Eugenio, Pamela. W. Jordan, Richmond H.
Thomason, and Johanna. D Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogues. International Journal of Human-Computer
Studies, 53(6):1017?1076.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring expres-
sions in cooperative, task-based human-robot dia-
logue. In Proceedings of 3rd Human-Robot Inter-
action, pages 295?302.
Mary Ellen Foster, Manuel Giuliani, Amy Isard, Colin
Matheson, Jon Oberlander, and Alois Knoll. 2009.
Evaluating description and reference strategies in a
cooperative human-robot dialogue system. In Pro-
ceedings of the 21st international jont conference
on Artifical intelligence (IJCAI 2009), pages 1818?
1823.
Mary Ellen Foster. 2008. Automated metrics that
agree with human judgements on generated output
for an embodied conversational agent. In Proceed-
ings of the 5th International Natural Language Gen-
eration Conference (INLG 2008), pages 95?103.
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The give-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC 2010), pages 2401?2406.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 174?182.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. Journal of Artificial
Intelligence Research, 24:157?194.
Imtiaz Hussain Khan, Kees van Deemter, Graeme
Ritchie, Albert Gatt, and Alexandra A. Cleland.
2009. A hearer-oriented evaluation of referring ex-
pression generation. In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 98?101.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jon
Oberlander, and Johanna Moore. 2009. Validating
the web-based evaluation of nlg systems. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 301?304.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics (ACL 2002), pages 311?318.
Ivandre? Paraboni, Judith Masthoff, and Kees van
Deemter. 2006. Overspecified reference in hierar-
chical domains: Measuring the benefits for readers.
In Proceedings of the 4th International Natural Lan-
guage Generation Conference (INLG 2006), pages
55?62.
Paul L.A. Piwek. 2007. Modality choise for generation
of referring acts. In Proceedings of the Workshop on
Multimodal Output Generation (MOG 2007), pages
129?139.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529?558.
Ehud Reiter and Somayajulu Sripada. 2002. Should
corpora texts be gold standards for NLG? In Pro-
ceesings of 2nd International Natural Language
Generation Conference (INLG 2002), pages 97?104.
Ehud Reiter, Roma Robertson, and Liesl M. Osman.
2003. Lessons from a failure: generating tailored
smoking cessation letters. Artificial Intelligence,
144(1-2):41?58.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167(1-2):137?169.
Michael F. Schober, Herbert, and H. Clark. 1989. Un-
derstanding by addressees and overhearers. Cogni-
tive Psychology, 21:211?232.
Philipp Spanger, Masaaki Yasuhara, Ryu Iida, and
Takenobu Tokunaga. 2009a. A Japanese corpus
of referring expressions used in a situated collabo-
ration task. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 110 ? 113.
Philipp Spanger, Masaaki Yasuhara, Iida Ryu, and
Tokunaga Takenobu. 2009b. Using extra linguistic
information for generating demonstrative pronouns
in a situated collaboration task. In Proceedings of
PreCogSci 2009: Production of Referring Expres-
sions: Bridging the gap between computational and
empirical approaches to reference.
Karen Sparck Jones and Julia R. Galliers. 1996. Eval-
uating Natural Language Processing Systems: An
Analysis and Review. Springer-Verlag.
Amanda Stent, Matthew Marge, and Mohit Singhai.
2005. Evaluating evaluation methods for generation
in the presence of variation. In Linguistics and In-
telligent Text Processing, pages 341?351. Springer-
Verlag.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase
generation for situated dialogs. In Proceedings of
the 4th International Natural Language Generation
Conference (INLG 2006), pages 81?88.
Kees van Deemter. 2007. TUNA: Towards a unified
algorithm for the generation of referring expres-
sions. Technical report, Aberdeen University.
www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
Ielka van der Sluis, Albert Gatt, and Kees van Deemter.
2007. Evaluating algorithms for the generation of
referring expressions: Going beyond toy domains.
In Proceedings of Recent Advances in Natural Lan-
guae Processing (RANLP 2007).
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 237?246,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Unified Probabilistic Approach to Referring Expressions
Kotaro Funakoshi Mikio Nakano
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako,
Saitama 351-0188, Japan
{funakoshi,nakano}@jp.honda-ri.com
Takenobu Tokunaga Ryu Iida
Tokyo Institute of Technology
2-12-1 Oookayama, Meguro,
Tokyo 152-8550, Japan
{take,ryu-i}@cl.cs.titech.ac.jp
Abstract
This paper proposes a probabilistic approach
to the resolution of referring expressions for
task-oriented dialogue systems. The approach
resolves descriptions, anaphora, and deixis in
a unified manner. In this approach, the notion
of reference domains serves an important role
to handle context-dependent attributes of enti-
ties and references to sets. The evaluation with
the REX-J corpus shows promising results.
1 Introduction
Referring expressions (REs) are expressions in-
tended by speakers to identify entities to hearers.
REs can be classified into three categories: descrip-
tions, anaphora, and deixis; and, in most cases,
have been studied within each category and with a
narrowly focused interest. Descriptive expressions
(such as ?the blue glass on the table?) exploit at-
tributes of entities and relations between them to
distinguish an entity from the rest. They are well
studied in natural language generation, e.g., (Dale
and Reiter, 1995; Krahmer et al, 2003; Dale and Vi-
ethen, 2009). Anaphoric expressions (such as ?it?)
refer to entities or concepts introduced in the pre-
ceding discourse and are studied mostly on textual
monologues, e.g., (Kamp and Reyle, 1993; Mitkov,
2002; Ng, 2010). Deictic (exophoric) expressions
(such as ?this one?) refer to entities outside the pre-
ceding discourse. They are often studied focusing
on pronouns accompanied with pointing gestures in
physical spaces, e.g., (Gieselmann, 2004).
Dialogue systems (DSs) as natural human-
machine (HM) interfaces are expected to han-
dle all the three categories of referring expres-
sions (Salmon-Alt and Romary, 2001). In fact, the
three categories are not mutually exclusive. To be
concrete, a descriptive expression in conversation is
either deictic or anaphoric. It is, however, not easy to
tell whether a RE is deictic or anaphoric in advance
of a resolution (regardless of whether the RE is de-
scriptive or not). Therefore, we propose a general
unified approach to the above three kinds of REs.
We employ a Bayesian network (BN) to model a
RE. Dealing with continuous information and vague
situations is critical to handle real world problems.
Probabilistic approaches enable this for reference re-
solvers. Each BN is dynamically constructed based
on the structural analysis result of a RE and contex-
tual information available at that moment. The BN
is used to estimate the probability with which the
corresponding RE refers to an entity.
One of the two major contributions of this paper is
our probabilistic formulation that handles the above
three kinds of REs in a unified manner. Previously
Iida et al (2010) proposed a quantitative approach
that handles anaphoric and deictic expressions in a
unified manner. However it lacks handling of de-
scriptive expressions. Our formulation subsumes
and extends it to handle descriptive REs. So far, no
previously proposed method for reference resolution
handles all three types of REs.
The other contribution is bringing reference
domains into that formulation. Reference do-
mains (Salmon-Alt and Romary, 2000) are sets of
referents implicitly presupposed at each use of REs.
By considering them, our approach can appropri-
ately interpret context-dependent attributes. In ad-
dition, by treating a reference domain as a referent,
REs referring to sets of entities are handled, too. As
far as the authors know, this work is the first that
takes a probabilistic approach to reference domains.
237
1.1 Reference domains
First, we explain reference domains concretely. Ref-
erence domains (RDs) (Salmon-Alt and Romary,
2000; Salmon-Alt and Romary, 2001; Denis, 2010)
are theoretical constructs, which are basically sets
of entities presupposed at each use of REs. RDs in
the original literature are not mere sets of entities
but mental objects equipped with properties such
as type, focus, or saliency and internally structured
with partitions. In this paper, while we do not ex-
plicitly handle partitions, reference domains can be
nested as an approximation of partitioning, that is,
an entity included in a RD is either an individual en-
tity or another RD. Each RD d has its focus and de-
gree of saliency (a non-negative real number). Here-
after, two of them are denoted as foc(d) and sal(d)
respectively. RDs are sorted in descending order ac-
cording to saliency.
We illustrate reference domains with figure 1. It
shows a snapshot of solving a Tangram puzzle (the
puzzle and corpus are explained in section 3.1). RDs
are introduced into our mental spaces either linguis-
tically (by hearing a RE) or visually (by observing
a physical situation). If one says ?the two big tri-
angles? in the situation shown in figure 1, we will
recognize a RD consisting of pieces 1 and 2. If we
observe one moves piece 1 and attaches it to piece
2, we will perceptually recognize a RD consisting
of pieces 1, 2, and 6 due to proximity (Tho?risson,
1994). In a similar way, a RD consisting of pieces 5
and 7 also can be recognized. Hereafter, we indicate
a RD with the mark @ with an index, and denote
its elements by enclosing them with [ ]. E.g., @1 =
[1, 2], @2 = [1, 2, 6], @3 = [5, 7]. The focused en-
tity is marked by ?*?. Thus, foc([1?, 2]) = 1.
The referent of a RE depends on which RD is pre-
supposed. That is, if one presupposes @1 or @2, the
referent of ?the right piece? should be piece 1. If
one presupposes @3, the referent of the same RE
should be piece 5. This is the context-dependency
mentioned above.
Previous work on RDs (Salmon-Alt and Romary,
2000; Salmon-Alt and Romary, 2001; Denis, 2010)
employ not probabilistic but formal approaches.
1.2 Probabilistic approaches to REs
Here, previous probabilistic approaches to REs are
explained and differences between ours and theirs
Figure 1: Tangram puzzle. (The labels 1 to 7 are for il-
lustration purposes and not visible to participants.)
are highlighted. Bayesian networks (Pearl, 1988;
Jensen and Nielsen, 2007) have been not often but
occasionally applied to problems in natural language
processing/computational linguistics since (Char-
niak and Goldman, 1989). With regard to REs,
Burger and Connolly (1992) proposed a BN special-
ized for anaphora resolution. Weissenbacher (2005;
2007) proposed a BN for the resolution of non-
anaphoric ?it? and also a BN for the resolution of
pronominal anaphora. They used pre-defined fixed
BNs for their tasks while our approach dynamically
tailors a BN for each RE.
Cho and Maida (1992) and Roy (2002) adopted
not exactly BNs but similar probabilistic approaches
for reference resolution and generation respectively.
However, their foci are only on descriptions.
Lison et al (2010) proposed an approach using
Markov logic networks (MLNs) (Richardson and
Domingos, 2006) to reference resolution. They
dealt with only deictic and descriptive REs. Even
though MLNs are also a probabilistic framework, it
is difficult for DS developers to provide quantitative
domain knowledge needed to resolve REs because
MLNs accept domain knowledge in the form of for-
mal logic rules with weights, which must be deter-
mined globally. In contrast, BNs are more flexible
and easy in providing quantitative knowledge to DSs
in the form of conditional probability tables, which
can be determined locally.
As just described, there are several probabilis-
tic approaches to REs but none of them incorpo-
rates reference domains. In the next section, we in-
troduce our REBNs (Referring Expression Bayesian
Networks), a novel Bayesian network-based model-
ing approach to REs that incorporates reference do-
mains.
238
W C X D
Figure 2: WCXD fundamental structure.
2 Bayesian Network-based Modeling of
Referring Expressions
Each REBN is dedicated for a RE in the context at
the moment. Its structure is determined by the syn-
tactic and semantic information in the RE and prob-
ability tables are determined by the context.
2.1 Structures
Figure 2 shows the fundamental network structure
of REBNs. We call this structure WCXD. The four
nodes (random variables)W ,C,X , andD represent
an observed word, the concept denoted by the word,
the referent of the RE, and the presupposed RD, re-
spectively. Here, a word means a lexical entry in
the system dictionary defined by the DS developer
(concept dictionary; section 3.2.1).
Each REBN is constructed by modifying or mul-
tiply connecting the WCXD structure as shown in
figures 3 and 4. Figure 3 shows the network for REs
indicating one referent such as ?that table.? EachWi
node has a corresponding word wi. Figure 4 shows
the network for REs indicating two referents such as
?his table.? We call the class of the former REs s-
REX (simple Referring EXpression) and the class of
the latter REs c-REX (compound Referring EXpres-
sion). Although REBNs have the potential to deal
with c-REX, hereafter we concentrate on s-REX be-
cause the page space is limited and the corpus used
for evaluation contains very few c-REX instances.
Although, in section 1, we explained that (Iida et
al., 2010) handles anaphoric and deictic expressions
in a unified manner, it handles anaphora to instances
only and does not handle that to concepts. There-
fore, it cannot satisfactorily resolve such an expres-
sion ?Bring me the red box, and the blue one, too.?
Here, ?one? does not refer to the physical referent
of ?the red box? but refers to the concept of ?box?.
TheC nodes will enable handling of such references
to concepts. This is one of the important features of
REBNs but will be investigated in future work.
W
1
C
1
X D
W
2
C
2
Figure 3: BN for two-word REs indicating one referent.
W
1
C
1
X
1
D
1
W
2
C
2
X
2
D
2
Figure 4: BN for two-word REs indicating two referents.
2.2 Domains of random variables
A REBN for an s-REX instance of N words
has 2N + 2 discrete random variables:
W1, . . . ,WN , C1, . . . , CN , X , and D. The do-
main of each variable depends on the corresponding
RE and the context at the moment. Here, D(V )
denotes the domain of a random variable V .
D(Wi) contains the corresponding observed word
wi and a special symbol ? that represents other pos-
sibilities, i.e., D(Wi) = {wi,?}. Each Wi has a
corresponding node Ci.
D(Ci) containsM concepts that can be expressed
by wi and a special concept ? that represents other
possibilities, i.e., D(Ci) = {c1i , . . . , cMi ,?}. cji
(j = 1 . . .M ) are looked up from the concept dic-
tionary (see section 3.2.1, table 2).
D(D) contains L + 1 RDs recognized up to that
point in time, i.e., D(D) = {@0,@1, . . . ,@L}. @0
is the ground domain that contains all the individ-
ual entities to be referred to in a dialogue. At the
beginning of the dialogue, D(D) = {@0}. Other
L RDs are incrementally added in the course of the
dialogue.
D(X) contains all the possible referents, i.e., K
individual entities and L + 1 RDs. Thus, D(X) =
{x1, . . . , xK ,@0, . . . ,@L}. Including RDs enables
handling of references to sets.
Then reference resolution is formalized as below:
x? = argmax
x?D(X)
P (X = x|W1 = w1, . . . ,WN = wN ). (1)
P (X|W1, . . . ,WN ) is obtained by marginalizing
the joint probabilities that are computed with the
probability tables described in the next subsection.
239
2.3 Probability tables
Probability distributions are given as (conditional)
probability tables since all the random variables
used in a REBN are discrete. Here, four types of
probability tables used by REBNs are described.
2.3.1 P (Wi|Ci, X)
P (Wi = w|Ci = c,X = x) is the probability that
a hearer observes w from c and x which the speaker
intends to indicate.
In most cases, Wi does not depend on X , i.e.,
P (Wi|Ci, X) ? P (Wi|Ci). X is, however, nec-
essary to handle individualized terms (names).
There are several conceivable ways of probabil-
ity assignment. One simple way is: for each cji ,
P (W = wi|C = cji ) = 1/T, P (W = ?|C =
cji ) = (T ? 1)/T , and for ?, P (W = wi|C =
?) = ", P (W = ?|C = ?) = 1 ? ". Here T is the
number of possible words for cji . " is a predefined
small number such as 10?8. We use this assignment
in the evaluation.
2.3.2 P (Ci|X,D)
P (Ci = c|X = x,D = d) is the probability that
concept c is chosen from D(Ci) to indicate x in d.
The developers of DSs cannot provide
P (Ci|X,D) in advance because D(Ci) is context-
dependent. Therefore, we take an approach of
composing P (Ci|X = x,D = d) from R(cji , x, d)
(cji ? D(Ci)\{?}). Here R(cji , x, d) is the rele-
vancy of concept cji to referent x with regard to d,
and 0 ? R(cji , x, d) ? 1. 1 means full relevancy
and 0 means no relevancy. 0.5 means neutral. For
example, a concept BOX will have a high relevancy
to a suitcase such as 0.8 but a concept BALL will
have a low relevancy to the suitcase such as 0.1.
If x is not in d, R(cji , x, d) is 0. Algorithm 1
in appendix A shows an algorithm to compose
P (Ci|X = x,D = d) from R(cji , x, d). Concept
? will be assigned a high probability if none of
cji ? D(Ci)\{?} has a high relevancy to x.
If cji is static,1 R(cji , x, d) is numerically given in
advance in the form of a table. If not static, it is im-
plemented as a function by the DS developer, that is,
R(cji , x, d) = fcji (x, d, I). Here I is all the informa-tion available from the DS.
1Whether a concept is static or not depends on each DS.
For example, given a situation such as shown in
figure 1, the relevancy function of a positional con-
cept LEFT (suppose a RE such as ?the left piece?)
can be implemented as below:
fLEFT(x, d, I) = (ux ? ur)/(ul ? ur). (2)
Here, ux, ul and ur are respectively the horizontal
coordinates of x, the leftmost piece in d, and the
rightmost piece in d, which are obtained from I . If
x is a RD, the relevancy is given as the average of
entities included in the RD.
2.3.3 P (X|D)
P (X = x|D = d) is the probability that entity x
in RD d is referred to, which is estimated according
to the contextual information at the time the corre-
sponding RE is uttered but irrespective of attributive
information in the RE. The contextual information
includes the history of referring so far (discourse)
and physical statuses such as the gaze of the referrer
(situation). We call P (X = x|D = d) the predic-
tion model.
The prediction model can be constructed by us-
ing a machine learning-based method. We use a
ranking-based method (Iida et al, 2010). The score
output by the method is input into the standard sig-
moid function and normalized to be a probability. If
x is not in d, P (X = x|D = d) is 0.
2.3.4 P (D)
P (D = d) is the probability that RD d is presup-
posed at the time the RE is uttered. We cannot col-
lect data to estimate this probabilistic model because
RDs are implicit. Therefore, we examine three a pri-
ori approximation functions based on the saliency of
d. Saliency is proportional to recency.2
Uniformmodel This model ignores saliency. This
is introduced to see the importance of saliency.
P (D = d) = 1/|D(D)| (3)
Linear model This model distributes probabilities
in proportion to saliency. This is an analogy of the
method used in (Denis, 2010).
P (D = d) = sal(d)?
d??D(D) sal(d?)
(4)
2Assignment of saliency is described in section 3.2.3.
240
Exponential model This model puts emphasis on
recent RDs. This function is so called soft-max.
P (D = d) = exp(sal(d))?
d??D(D) exp(sal(d?))
(5)
3 Experimental Evaluation
We evaluated the potential of the proposed frame-
work by using a situated human-human (HH) dia-
logue corpus.
3.1 Corpus
We used the REX-J Japanese referring expression
corpus (Spanger et al, 2010). The REX-J corpus
consists of 24 HH dialogues in each of which two
participants solve a Tangram puzzle of seven pieces
(see figure 1). The goal of the puzzle is combining
seven pieces to form a designated shape (such as a
swan). One of two subjects takes the role of opera-
tor (OP) and the other takes the role of solver (SV).
The OP can manipulate the virtual puzzle pieces dis-
played on a PC monitor by using a computer mouse
but does not know the goal shape. The SV knows
the goal shape but cannot manipulate the pieces. The
states of the pieces and the mouse cursor operated by
the OP are shared by the two subjects in real time.
Thus, the two participants weave a collaborative dia-
logue including many REs to the pieces. In addition
to REs, the positions and directions of the pieces, the
position of the mouse cursor, and the manipulation
by the OP were recorded with timestamps and the
IDs of relevant pieces.
3.1.1 Annotation
Each RE is annotated with its referent(s) as shown
in table 1. The 1st RE okkiisankaku3 big triangle ?a
big triangle? in the table is ambiguous and refers to
either piece 1 or 2. The 7th and 8th REs refer to
the set of pieces 1 and 2. The other REs refer to an
individual piece.
To skip the structural analysis of REs to avoid
problems due to errors in such analysis, we have
additionally annotated the corpus with intermediate
structures, from which REBNs are constructed. Be-
cause we focus on s-REX only in this paper, the
3Words are not separated by white spaces in Japanese.
intermediate structures are straightforward:4 paren-
thesized lists of separated words as shown in ta-
ble 1. The procedure to generate a REBN of s-REX
from such an intermediate structure is also straight-
forward and thus it is not explained due to the page
limitation.
3.2 Implementations
We use BNJ5 for probabilistic computation. Here
we describe the implementations of resources and
procedures that are more or less specific to the task
domain of REX-J.
3.2.1 Concept dictionary
Table 2 shows an excerpt of the concept dictio-
nary defined for REX-J. We manually defined 40
concepts by observing the dialogues.
3.2.2 Static relevancy table and relevancy
functions
For 13 concepts out of 40, their relevancy values
were manually determined by the authors. Table 3
shows an excerpt of the static relevancy table defined
for the seven pieces shown in figure 1. TRI is rele-
vant only to pieces 1 to 5, and SQR is relevant only
to pieces 6 and 7 but is not totally relevant to piece 7
because it is not a square in a precise sense. FIG is
equally but not very relevant to all the pieces,6
For the remaining 27 concepts, we implemented
relevancy functions (see appendix B).
3.2.3 Updating the list of RDs
In our experiment, REs are sequentially resolved
from the beginning of each dialogue in the corpus.
In the course of resolution, RDs are added into a list
and updated by the following procedure. RDs are
sorted in descending order according to saliency.
At each time of resolution, we assume that all the
previous REs are correctly resolved. Therefore, af-
ter each time of resolution, if the correct referent of
the last RE is a set, we add a new RD equivalent
to the set into the list of RDs, unless the list con-
tains another equivalent RD already. In either case,
the saliency of the RD equivalent to the set is set to
?+1 unless the RD is at the head of the list already.
4In the case of c-REX, graph-like structures are required.
5http://bnj.sourceforge.net/
6This is because concept FIG in REX-J is usually used to
refer to not a single piece but a shaped form (combined pieces).
241
D-ID Role Start End Referring expression Referents Intermediate structure
0801 SV 17.345 18.390 okkiisankaku big triangle 1 or 2 (okkii sankaku)
0801 SV 20.758 21.368 sore it 1 (sore)
0801 SV 23.394 24.720 migigawanookkiisankaku right big triangle 1 (migigawano okkii sankaku)
0801 SV 25.084 25.277 kore this 1 (kore)
0801 SV 26.512 26.671 sono that 1 (sono)
0801 SV 28.871 29.747 konookkiisankaku this big triangle 2 (kono okkii sankaku)
0801 OP 46.497 48.204 okkinasankakkei big triangle 1, 2 (okkina sankakkei)
0801 OP 51.958 52.228 ryo?ho? both 1, 2 (ryo?ho?)
?D-ID? means dialogue ID. ?Start? and ?End? mean the end points of a RE.
Table 1: Excerpt of the corpus annotation (w/ English literal translations).
Concept Words
TRI triangle, right triangle
SQR quadrate, square, regular tetragon
FIG figure, shape
Table 2: Dictionary (excerpted and translated in English).
Concept Relevancy values by piece(1) (2) (3) (4) (5) (6) (7)
TRI 1 1 1 1 1 0 0
SQR 0 0 0 0 0 1 0.8
FIG 0.3 0.3 0.3 0.3 0.3 0.3 0.3
Table 3: Static relevancy table.
Here, ? is the largest saliency value in the list at the
moment (the saliency value of the head RD).
Before each time of resolution, we check whether
the piece that is most recently manipulated after the
previous RE constitutes a perceptual group by using
the method explained in section 3.2.4 at the onset
time of the target RE. If such a group is recognized,
we add a new RD equivalent to the recognized group
unless the list contains another equivalent RD. In ei-
ther case, the saliency of the RD equivalent is set to
?+1 unless the RD is at the head of the list already,
and the focus of the equivalent RD is set to the most
recently manipulated piece.
When a new RD@m is added to the list, a comple-
mentary RD @n and a subsuming RD @l are also in-
serted just after @m in the list. Here, @n = @0\@m
and @l = [@m?,@n]. This operation is required to
handle a concept REST, e.g., ?the remaining pieces.?
3.2.4 Perceptual grouping
There is a generally available method of simulated
perceptual grouping (Tho?risson, 1994). It works
well in a spread situation such as shown in figure 1
but tends to produce results that do not match our
intuition when pieces are tightly packed at the end
of a dialogue. Therefore, we adopt a simple method
that recognizes a group when a piece is attached to
another. This method is less general but works sat-
isfactorily in the REX-J domain due to the nature of
the Tangram puzzle.
3.2.5 Ranking-based prediction model
As mentioned in section 2.3.3, a ranking-based
method (Iida et al, 2010) using SVMrank (Joachims,
2006) was adopted for constructing the prediction
model P (X|D). This model ranks entities accord-
ing to 16 binary features such as whether the tar-
get entity is previously referred to (a discourse fea-
ture), whether the target is under the mouse cursor
(a mouse cursor feature), etc.7
When a target is a set (i.e., a RD), discourse fea-
tures for it are computed as in the case of a piece;
meanwhile, mouse cursor features are handled in a
different manner. That is, if one of the group mem-
bers meets the criterion of a mouse cursor feature,
the group is judged as meeting the criterion.
In (Iida et al, 2010), preparing different models
for pronouns and non-pronouns achieved better per-
formance. Therefore we trained two linear kernel
SVM models for pronouns and non-pronouns with
the 24 dialogues.
3.3 Experiment
We used the 24 dialogues for evaluation.8 As men-
tioned in section 2.1, we focused on s-REX. These
24 dialogues contain 1,474 s-REX instances and 28
c-REX instances. In addition to c-REX, we ex-
cluded REs mentioning complicated concepts, for
which it is difficult to implement relevancy func-
tions in a short time.9 After excluding those REs,
7Following the results shown in (Iida et al, 2010), we did
not use the 6 manipulation-related features (CO1 . . . CO6).
8We used the same data to train the SVM-rank models. This
is equivalent to assuming that we have data large enough to sat-
urate the performance of the prediction model.
9Mostly, those are metaphors such as ?neck? and concepts
related to operations such as ?put.? For example, although
242
P (D) model Most-recent Mono-domain Uniform Linear Exponential
Category Single Plural Total Single Plural Total Single Plural Total Single Plural Total Single Plural Total
w/o S/P info. 42.4 28.8 40.0 77.5 47.3 73.3 77.1 40.6 72.0 78.3 45.1 73.7 76.2 48.4 72.3
w/ S/P info. 44.3 35.4 42.7 84.8 58.8 81.2 84.4 55.0 80.3 85.6 61.0 82.1 83.4 68.1 81.3
Table 4: Results of reference resolution (Accuracy in %).
1,310 REs were available. Out of the 1,310 REs, 182
REs (13.9%) refers to sets, and 612 REs (46.7%) are
demonstrative pronouns such as sore ?it.?
3.3.1 Settings
We presupposed the following conditions.
Speaker role independence: We assumed REs
are independent of speaker roles, i.e., SV and OP.
All REs were mixed and processed serially.
Perfect preprocessing and past information:
As mentioned in sections 3.1.1 and 3.2.3, we as-
sumed that no error comes from preprocessing in-
cluding speech recognition, morphological analysis,
and syntactic analysis;10 and all the correct referents
of past REs are known.11
No future information: In HH dialogue, some-
times information helpful for resolving a RE is pro-
vided after the RE is uttered. We, however, do not
consider such future information.
Numeral information: Many languages includ-
ing English grammatically require indication of nu-
meral distinctions by using such as articles, singu-
lar/plural forms of nouns and copulas, etc. Although
Japanese does not have such grammatical devices,12
it would be possible to predict such distinctions by
using a machine learning technique with linguistic
?putting a piece? and ?getting a piece out? are distinguished
due to speakers? intentions, they are (at least superficially) ho-
mogeneous in the physical data available from the corpus and
difficult for machines to distinguish each other.
10In general, the speech and expressions in human-machine
(HM) dialogue are less complex and less difficult to process
than those in HH dialogue data. This is typcially observed as
fewer disfluencies (Shriberg, 2001) and simpler sentences with
fewer omissions (Itoh et al, 2002). Therefore, when we apply
our framework to real DSs, we can expect clearer and simpler
input and thus better performance. We supposed that the condi-
tion of perfect preprocessing in HH dialogue approximates the
results to those obtained when HM dialogue data is used.
11If a reference is misinterpreted (i.e., wrongly resolved) in a
dialogue, usually that misinterpretation will be repaired by the
interlocutors in the succeeding interaction once the misinterpre-
tation becomes apparent. Therefore, accumulating all past er-
rors in resolution is rather irrational as an experimental setting.
12Japanese has a plurality marker -ra (e.g., sore-ra), but use
of it is not mandatory (except for personal pronouns).
and gestural information. Therefore we observed the
effect of providing such information. In the follow-
ing experiment we provide the singular/plural dis-
tinction information to REBNs by looking at the an-
notations of the correct referents in advance. This
is achieved by adding a special evidence node C0,
where D(C0) = {S,P}. P (C0 = S|X = x) = 1
and P (P|x) = 0 if x is a piece. On the contrary,
P (S|x) = 0 and P (P|x) = 1 if x is a set.
3.3.2 Baselines
To our best knowledge, there is no directly com-
parable method. We set up two baselines. The first
baseline uses the most recent as the resolved refer-
ent for each RE (Initial resolution of each dialogue
always fails). This baseline is called Most-recent.
As the second baseline, we prepared another
P (D) model in addition to those explained in sec-
tion 2.3.4, which is called Mono-domain. In Mono-
domain, D(D) consists of only a single RD @?0,
which contains individual pieces and the RDs recog-
nized up to that point in time. That is, @?0 = D(X).
Resolution using this model can be considered as
a straightforward extension of (Iida et al, 2010),
which enables handling of richer concepts in REs13
and handling of REs to sets14.
3.3.3 Results
The performance of reference resolution is pre-
sented by category and by condition in terms of ac-
curacy (# of correctly resolved REs/# of REs).
We set up the three categories in evaluating res-
olution, that is, Single, Plural, and Total. Category
Single is the collection of REs referring to a single
piece. Plural is the collection of REs referring to a
set of pieces. Total is the sum of them. Ambigu-
ous REs such as the first one in table 1 are counted
as ?Single? and the resolution of such a RE is con-
sidered correct if the resolved result is one of the
possible referents.
13(Iida et al, 2010) used only object types and sizes. Other
concepts such as LEFT were simply ignored.
14(Iida et al, 2010) did not deal with REs to sets.
243
?w/o S/P info.? indicates experimental results
without singular/plural distinction information. ?w/
S/P info.? indicates experimental results with it.
Table 4 shows the results of reference resolution
per P (D) modeling method.15 Obviously S/P infor-
mation has a significant impact.
While the best performance for category Single
was achieved with the Linear model, the best perfor-
mance for Plural was achieved with the Exponen-
tial model. If it is possible to know whether a RE
is of Single or Plural, that is, if S/P information is
available, we can choose a suitable P (D) model.
Therefore, by switching models, the best perfor-
mance of Total with S/P information reached 83.4%,
and a gain of 2.0 points against Mono-domain was
achieved (sign test, p < 0.0001).
Because the corpus did not include many in-
stances to which the notion of reference domains is
effective, the impact of RDs may appear small on the
whole. In fact, the impact was not small. By intro-
ducing RDs, resolution in category Plural achieved
a significant advancement. The highest gain from
Mono-domain was 9.3 points (sign test, p < 0.005).
Moreover, more REs containing positional concepts
such as LEFT and RIGHT were correctly resolved
in the cases of Uniform, Linear, and Exponential.
Table 5 summarizes the resolution results of four
positional concepts (with S/P information). While
Mono-domain resolved 65% of them, Linear cor-
rectly resolved 75% (sign test, p < 0.05).
As shown in table 4, the performance of the Uni-
form model was worse than that of Mono-domain.
This indicates that RDs introduced without an ap-
propriate management of them would be harmful
noise. Conversely, it also suggests that there might
be a room for improvement by looking deeply into
the management of RDs (e.g., forgetting old RDs).
4 Conclusion
This paper proposed a probabilistic approach to ref-
erence resolution, REBNs, which stands for Refer-
ring Expression Bayesian Networks. At each time
of resolution, a dedicated BN is constructed for the
15According to the results of preliminary experiments, even
in the case of the Uniform/Linear/Exponential models, we re-
solved the REs having demonstratives with the Mono-domain
model. This is in line with the finding of separating models
between pronouns and non-pronouns in (Iida et al, 2010).
Concept Count Mono Uni. Lin. Exp.
LEFT 21 11 12 16 13
RIGHT 33 23 23 25 27
UPPER 9 6 6 6 4
LOWER 6 5 4 5 4
Total 69 45 45 52 48
(Count means the numbers of occurrence of each concept. Mono, Uni.,
Lin., and Exp. correspond to Mono-domain, Uniform, Linear and Ex-
ponential.)
Table 5: Numbers of correctly resolved REs containing
positional concepts.
RE in question. The constructed BN deals with ei-
ther descriptive, deictic or anaphoric REs in a uni-
fied manner. REBNs incorporate the notion of ref-
erence domains (RDs), which enables the resolution
of REs with context-dependent attributes and han-
dling of REs to sets. REBNs are for task-oriented
dialogue systems and presuppose a certain amount
of domain-dependent manual implementation by de-
velopers. Therefore, REBNs would not be suited
to general text processing or non-task-oriented sys-
tems. However, REBNs have the potential to be a
standard approach that can be used for any and all
task-oriented applications such as personal agents in
smart phones, in-car systems, service robots, etc. ?
The proposed approach was evaluated with the
REX-J human-human dialogue corpus and promis-
ing results were obtained. The impact of incorpo-
rating RDs in the domain of the REX-J corpus was
recognizable but not so large on the whole. How-
ever, in other types of task domains where grouping
and comparisons of objects occur frequently, the im-
pact would be larger. Note that REBNs are not lim-
ited to Japanese, even though the evaluation used a
Japanese corpus. Evaluations with human-machine
dialogue are important future work.
Although this paper focused on the simple type of
REs without relations, REBNs are potentially able
to deal with complex REs with relations. The eval-
uation for complex REs is necessary to validate this
potential of REBN. Currently REBN assumes REs
whose referents are concrete entities. An extension
for handling abstract entities (Byron, 2002; Mu?ller,
2007) is important future work. Another direction
would be generating REs with REBNs. A generate-
and-test approach is a naive application of REBN
for generation. More efficient method is, however,
necessary.
244
References
John D. Burger and Dennis Connoly. 1992. Probabilistic
resolution of anaphoric reference. In Proceedings of
the AAAI Fall Symposium on Intelligent Probabilistic
Approaches to Natural Language, pages 17?24.
Donna Byron. 2002. Resolving pronominal reference
to abstract entities. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 80?87.
Eugene Charniak and Robert Goldman. 1989. A se-
mantics for probabilistic quantifier-free first-order lan-
guages with particular application to story understand-
ing. In Proceedings of the Eleventh International Joint
Conference on Artificial Intelligence (IJCAI), pages
1074?1079, Menlo Park, CA, USA.
Sehyeong Cho and Anthony Maida. 1992. Using a
Bayesian framework to identify the referent of definite
descriptions. In Proceedings of the AAAI Fall Sympo-
sium on Intelligent Probabilistic Approaches to Natu-
ral Language, pages 39?46.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the Gricean maxims in the generation
of referring expressions. Cognitive Science, 18:233?
263.
Robert Dale and Jette Viethen. 2009. Referring expres-
sion generation through attribute-based heuristics. In
Proceedings of the the 12th European Workshop on
Natural Language Generation (ENLG), pages 59?65,
Athens, Greece, March.
Alexandre Denis. 2010. Generating referring expres-
sions with reference domain theory. In Proceedings
of the 6th International Natural Language Generation
Conference (INLG), pages 27?35.
Petra Gieselmann. 2004. Reference resolution mech-
anisms in dialogue management. In Proceedings of
the 8th workshop on the semantics and pragmatics of
dialogue (CATALOG), pages 28?34, Barcelona, Italy,
July.
Ryu Iida, Shumpei Kobayashi, and Takenobu Tokunaga.
2010. Incorporating extra-linguistic information into
reference resolution in collaborative task dialogue. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1259?
1267, Uppsala, Sweden, July.
Toshihiko Itoh, Atsuhiko Kai, Tatsuhiro Konishi, and
Yukihiro Itoh. 2002. Linguistic and acoustic changes
of user?s utterances caused by different dialogue situa-
tions. In Proceedings of the 7th International Confer-
ence on Spoken Language Processing (ICSLP), pages
545?548.
Finn V. Jensen and Thomas D. Nielsen. 2007. Bayesian
Networks and Decision Graphs. Springer, second edi-
tion.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD), pages
217?226, Philadelphia, PA, USA, August.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer Academic Publishers.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29:53?72.
Pierre Lison, Carsten Ehrler, and Geert-Jan M. Kruijff.
2010. Belief modelling for situation awareness in
human-robot interaction. In Proceedings of the 19th
International Symposium on Robot and Human In-
teractive Communication (RO-MAN), pages 138?143,
Viareggio, Italy, September.
Ruslan Mitkov. 2002. Anaphora Resolution. Studies in
Language and Linguistics. Pearson Education.
Christoph Mu?ller. 2007. Resolving it, this, and that in
unrestricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 816?823.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo, CA, USA.
Matthew Richardson and Pedor Domingos. 2006.
Markov logic networks. Machine Learning, 62(1?
2):107?136.
Deb K. Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3):353?385.
Susanne Salmon-Alt and Laurent Romary. 2000. Gen-
erating referring expressions in multimodal contexts.
In Proceedings of the INLG 2000 workshop on Coher-
ence in Generated Multimedia, Mitzpe Ramon, Israel,
June.
Susanne Salmon-Alt and Laurent Romary. 2001. Ref-
erence resolution within the framework of cognitive
grammar. In Proceedings of the International Col-
loqium on Cognitive Science, San Sebastian, Spain,
May.
Elizabeth Shriberg. 2001. To ?errrr? is human: ecology
and acoustics of speech disfluencies. Journal of the
International Phonetic Association, 31(1):153?169.
Philipp Spanger, Masaaki Yasuhara, Ryu Iida, Takenobu
Tokunaga, Asuka Terai, and Naoko Kuriyama. 2010.
REX-J: Japanese referring expression corpus of sit-
uated dialogs. Language Resources and Evaluation.
Online First, DOI: 10.1007/s10579-010-9134-8.
245
Kristinn R. Tho?risson. 1994. Simulated perceptual
grouping: An application to human-computer interac-
tion. In Proceedings of the 16th Annual Conference
of the Cognitive Science Society, pages 876?881, At-
lanta, GA, USA.
Davy Weissenbacher and Adeline Nazarenko. 2007. A
Bayesian approach combining surface clues and lin-
guistic knowledge: Application to the anaphora reso-
lution problem. In Proceedings of the International
Conference Recent Advances in Natural Language
Processing (RANLP), Borovets, Bulgaria.
Davy Weissenbacher. 2005. A Bayesian network for the
resolution of non-anaphoric pronoun it. In Proceed-
ings of the NIPS 2005 Workshop on Bayesian Meth-
ods for Natural Language Processing, Whistler, BC,
Canada.
A Algorithm to compose P (C|X,D)
Algorithm 1 Composing P (C|X = x,D = d).
Input: D(C); R(c, x, d) for all c ? D(C)\{?}
Output: P (C|X = x,D = d)
1: n ? 0, s ? 0, S = D(C)\{?}
2: for all c ? S do
3: r[c] ? R(c, x, d) #{Relevancy of concept c}
4: s ? s+ r[c] #{Sum of relevancy r[c]}
5: n ? n+ (1? r[c]) #{Sum of residual (1? r[c])}
6: end for
7: r[?] ? n/|S|
8: s ? s+ r[?]
9: for all c ? D(C) do
10: P (C = c|X = x,D = d) ? r[c]/s
11: end for
(#{. . . } is a comment.)
B Relevancy functions
As explained in section 2.3.2, the relevancy func-
tions for positional concepts such as LEFT and
RIGHT were implemented as geometric calcula-
tions. Here several other relevancy functions are
shown with corresponding example REs.
?this figure?:
R(FIG, x, d)
=
?
?
?
0.3 : if single(x)
1 : if not single(x) and shape(x)
0 : otherwise
(single(x) means x is a single piece. shape(x)
means x is a set of pieces that are concatenated and
form a shape. 0.3 comes from the static relevancy
table.)
?both the triangles?:
R(BOTH, x, d) =
{
1 : if |x| = 2
0 : otherwise
?another one?:
R(ANOTHER, x, d) =
{
1 : if foc(d) 6= x
0 : otherwise
?the remaining ones?:
R(REST, x, d) =
{
1 : if d = [x, y?]
0 : otherwise
(REST requires |d| = 2, and both x and y are sets.
ANOTHER does not.)
?all?:
R(ALL, x, d) =
{
1 : if x = d
0 : otherwise
(ALL does not always refer to @0.)
246
Proceedings of the 14th European Workshop on Natural Language Generation, pages 147?151,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Automatic Voice Selection in Japanese based on Various Linguistic
Information
Ryu Iida and Takenobu Tokunaga
Department of Computer Science, Tokyo Institute of Technology
W8-73, 2-12-1 Ookayama Meguro Tokyo, 152-8552 Japan
{ryu-i,take}@cl.cs.titech.ac.jp
Abstract
This paper focuses on a subtask of natu-
ral language generation (NLG), voice se-
lection, which decides whether a clause is
realised in the active or passive voice ac-
cording to its contextual information. Au-
tomatic voice selection is essential for re-
alising more sophisticated MT and sum-
marisation systems, because it impacts
the readability of generated texts. How-
ever, to the best of our knowledge, the
NLG community has been less concerned
with explicit voice selection. In this pa-
per, we propose an automatic voice se-
lection model based on various linguistic
information, ranging from lexical to dis-
course information. Our empirical evalua-
tion using a manually annotated corpus in
Japanese demonstrates that the proposed
model achieved 0.758 in F-score, outper-
forming the two baseline models.
1 Introduction
Generating a readable text is the primary goal
in natural language generation (NLG). To realise
such text, we need to arrange discourse entities
(e.g. NPs) in appropriate positions in a sentence
according to their discourse salience. Consider the
two following Japanese texts, each of which con-
sists of two sentences.
(1) Tomi-wa kouenj-ni it-ta .
Tomi-TOP parkj-IOBJ go-PAST
(Tomi went to a parkj .)
Karei-wa sokoj-de ookina inu-ni oikake-rareta .
hei-TOP therej-LOC big dog-IOBJ chase-PASSIVE/PAST
(Hei was chased by a big dog therej .)
(2) Tomi-wa kouenj-ni it-ta .
Tomi-TOP parkj-IOBJ go-PAST
(Tomi went to a parkj .)
Ookina inu-ga sokoj-de karei-o oikake-ta .
big dog-SUBJ therej-LOC hei-OBJ chase-PAST
(A big dog chased himi therej .)
In (1), ?Tomi? is topicalised in the first sentence,
and then it appears at the subject position in the
second sentence. In contrast, the same argu-
ment, i.e. ?hei? is realised at the object position
in the second sentence of text (2). Intuitively,
text (1) is relatively more natural than text (2).
Thus, given the two predicate argument relations,
go(SUBJ:Tomi, IOBJ:parkj) and chase(SUBJ:big
dog, OBJ:Tomi, IOBJ:parkj), a generation system
should choose text (1).
The realisation from a semantic representation
(e.g. predicate argument structures) to an actual
text has been mainly developed in the area of nat-
ural language generation (Reiter and Dale, 2000),
and has been applied to various NLP applications
such as multi-document summarisation (Radev
and McKeown, 1998) and tutoring systems (Di
Eugenio et al, 2005). During the course of a
text generation process, various kinds of decisions
should be made, including decisions on textual
content, clustering the content of each clause, dis-
course structure of the clauses, lexical choices,
types of referring expressions and syntactic struc-
tures. Since these different kinds of decisions are
interrelated to each other, it is not a trivial prob-
lem to find an optimal order among these deci-
sions. This issue has been much discussed in terms
of architecture of generation systems. Although a
variety of architectures has been proposed in the
past, e.g. an integrated architecture (Appelt, 1985)
and a revision-based architecture (Inui et al, 1994;
Robin, 1994), a pipeline architecture is considered
as a consensus architecture in which decisions
are made in a predetermined order (Reiter, 1994).
Voice selection is a syntactic decision that tends to
be made in a later stage of the pipeline architec-
ture, even though it influences various decisions,
such as discourse structure and lexical choice. Un-
like referring expression generation, voice selec-
tion has received less attention and been less dis-
cussed in the past. Against this background, this
147
research tackles the problem of voice selection
considering a wide range of linguistic information
that is assumed to be already decided in the pre-
ceding stages of a generation process.
The paper is organised as follows. We first
overview the related work in Section 2, and then
propose a voice selection model based on the four
kinds of information that impact voice selection
in Section 3. Section 4 then demonstrates the re-
sults of empirical evaluation using the NAIST Text
Corpus (Iida et al, 2007) as training and evalu-
ation data sets. Finally, Section 5 concludes and
discusses our future directions.
2 Related work
The task of automatic voice selection has been
mainly developed in the NLG community. How-
ever, it has attracted less attention compared with
other major NLG problems, such as generating re-
ferring expressions. There is less work focusing
singly on voice selection, but not entirely with-
out exception, such as Abb et al (1993). In their
work, passivisation is performed by taking into ac-
count both linguistic and extra-linguistic informa-
tion. The linguistic information explains passivi-
sation in an incremental generation process; realis-
ing the most salient discourse entity in short term
memory as a subject eventually leads to passivi-
sation. In contrast, extra-linguistic information is
used to move a less salient entity to a subject posi-
tion when an explicit agent is missing in the text.
Although these two kinds of information seem ad-
equate for explaining passivisation, their applica-
bility was not examined in empirical evaluations.
Sheikha and Inkpen (2011) focused attention on
voice selection in the generation task distinguish-
ing formal and informal sentences. In their work,
passivisation is considered as a rhetorical tech-
nique for conveying formal intentions. However,
they did not discuss passivisation in terms of dis-
course coherence.
3 Voice selection model
We recast the voice selection task into a binary
classification problem, i.e. given a predicate with
its arguments and its preceding context, we clas-
sify the predicate into either an active or passive
class, taking into account predicate argument rela-
tions and the preceding context of the predicate.
As shown in examples (1) and (2) in Section 1,
several factors have an impact on voice selection
in a text. In this work, we take into account the
following four information as features The details
of the feature set are shown in Table 1.
Passivisation preference of each verb An im-
portant factor of voice selection is the preference
for how frequently a verb is used in passive sen-
tences. This means each verb has a potential ten-
dency of being used in passive sentences in a do-
main. For example, the verb ?yosou-suru (to ex-
pect)? tends to be realised in the passive in the
newspaper domain because Japanese journalists
tend to write their opinions objectively by omitting
the agent role. To take into account this preference
of verb passivisation, we define a preference score
by the following formula:
scorepas(vi) =
freqpas(vi)
freqall(vi)
? log freqall(vi) (1)
where vi is a verb in question1, freqall(vi) is
the frequency of vi appearing in corpora, and
freqpas(vi) is the frequency of vi with the passive
marker, (ra)reru. The logarithm of freqall(vi) is
multiplied due to avoiding the overestimation of
the score for less frequent instances. In the evalua-
tion, the preference score was calculated based on
the frequency of each verb in the 12 years worth
of newspaper articles, which had been morpho-
syntactically analysed by a Japanese morpholog-
ical analyser Mecab3 and a dependency parser
CaboCha4.
Syntactic decisions As described in Section 1,
various kinds of decisions are interrelated to voice
selection. Particularly, syntactic decisions includ-
ing voice selection directly impact sentence struc-
ture. Therefore, we introduce syntactic informa-
tion except for voice selection which prescribes
how an input predicate-argument structure will be
realised in an actual text.
Semantic category of arguments Animacy of
the arguments of a predicate has an impact on their
syntactic positions. Unlike in English, inanimate
subjects tend to be avoided in Japanese. In order
to capture this tendency, we use the semantic cate-
gory of the arguments of the verb in question (e.g.
1Note that the preference needs to be defined for each
word sense. However, we here ignore the difference of senses
because selecting a correct verb sense for a given context is
still difficult.
1Bunsetsu is a basic unit in Japanese, consisting of at least
one content word and more than zero functional words.
2http://nlp.cs.nyu.edu/irex/index-e.html
3https://code.google.com/p/mecab/
4https://code.google.com/p/cabocha/
148
type feature definition
PRED scorepas passivisation preference score defined in equation (1).
lexical lemma of P .
func lemma of functional words following P , excluding passive markers.
SYN sent end 1 if P appears in the last bunsetsu1-unit in a sentence; otherwise 0.
adnom 1 if P appears in an adnominal clause; otherwise 0.
first sent (last sent) 1 if P appears in the first (last) sentence of a text; otherwise 0.
subj(obj,iobj) embedded 1 if the head of the adnominal clause including P is semantic subject (object, indi-
rect object) of P ; otherwise 0.
ARG subj(obj,iobj) ne named entity class (based on IREX2) of the subject (object, indirect object) of P .
subj(obj,iobj) sem semantic class of the subject (object, indirect object) of P in terms of Japanese
ontology, nihongo goi taikei (Ikehara et al, 1997).
COREF subj(obj,iobj) exo 1 if the subject (object, indirect object) of P is unrealised and it is annotated as
exophoric; otherwise 0.
subj(obj,iobj) srl order order of the subject (object, indirect object) of P in the SRL.
subj(obj,iobj) srl rank rank of the subject (object, indirect object) of P in the SRL.
subj(obj,iobj) coref num number of discourse entities in the coreference chain including P?s subject (object,
indirect object) in the preceding context.
P stands for the predicate in question. The four feature types (PRED, SYN, ARG and COREF) correspond to each information
described in Section 3.
Table 1: Feature set for voice selection
named entity labels provided by CaboCha, such as
Person and Organisation, and the ontological in-
formation defined in a Japanese ontology, nihongo
goi taikei (Ikehara et al, 1997)) as features.
Coreference and anaphora of arguments As
discussed in discourse theories such as Centering
Theory (Grosz et al, 1995), arguments which have
been already most salient in the preceding context
tend to be placed at the beginning of a sentence for
reducing the cognitive cost of reading, as argued in
Functional Grammar (Halliday and Matthiessen,
2004). In order to consider the characteristic, we
employ an extension of Centering Theory (Grosz
et al, 1995), proposed by Nariyama (2002) for
implementing the COREF type features in Table 1.
She proposed a generalised version of the forward
looking-center list, called the Salient Reference
List (SRL), which stores all salient discourse en-
tities (e.g. NP) in the preceding contexts in the or-
der of their saliency. A highly ranked argument?s
entity in the SRL tends to be placed in the sub-
ject position, resulting in a passive sentence if that
salient entity has a THEME role in the predicate-
argument structure. To capture this characteristic,
the order and rank of discourse entities in the SRL
are used as features5.
In addition, as described in Abb et al (1993),
if the agent filler of a predicate is underspecified,
the passive voice is preferred so as to unfocus the
underspecified agent. Likewise, if the argument
5In Table 1 ?* srl rank? stands for how highly the argu-
ment?s referent ranked out of the discourse entities in the
SRL, while ?* srl order? stands for which slot (e.g. TOP slot
or SUBJ slot, etc.) stores the argument?s referent.
(in this case, the agent filler) of a predicate is ex-
ophoric, the passive voice is selected.
4 Experiments
We conducted an empirical evaluation using man-
ually annotated newspaper articles in Japanese. To
estimate the feature weights of each classifier, we
used MEGAM6, an implementation of the Maxi-
mum Entropy model, with default parameter set-
tings. We also used SVM7 with a polynomial ker-
nel for explicitly handling the dependency of the
proposed features.
4.1 Data and baseline models
For training and evaluation, we used the NAIST
Text Corpus (Iida et al, 2007). Because the cor-
pus contains manually annotated predicate argu-
ment relations and coreference relations, we used
those for the inputs of voice selection. In our prob-
lem setting, we conducted an intrinsic evaluation;
given manually annotated predicate argument re-
lations and coreference relations of arguments, a
model determines whether a predicate in question
is actually realised in the passive or active voice
in the original text. The performance is measured
based on recall, precision and F-score of correctly
detecting passive voice. For evaluation, we di-
vided the texts in the corpus into two sets; one is
used for training and the other for evaluation. The
details of this division are shown in Table 2.
We employed two baseline models for compar-
6http://www.cs.utah.edu/?hal/megam/
7http://svmlight.joachims.org/
149
#articles #predicates #passive predicates
training 1,753 65,592 4,974 (7.6%)
test 696 24,884 1,891 (7.6%)
Table 2: Data set division for evaluation
R P F
? = 0.1 0.768 0.269 0.399
? = 0.2 0.573 0.357 0.440
? = 0.3 0.403 0.450 0.425
? = 0.4 0.293 0.512 0.373
? = 0.5 0.161 0.591 0.253
? = 0.6 0.091 0.692 0.162
? = 0.7 0.060 0.717 0.111
? = 0.8 0.030 0.851 0.058
? = 0.9 0.014 1.000 0.027
Table 3: Effect of threshold ? for scorepas
ison. One is based on the passivisation preference
of each verb. The model uses only scorepas(vi)
defined in equation (1), that is, it selects the pas-
sive voice if the score is more than the threshold
parameter ?; otherwise, it selects the active voice.
The other baseline model is based on the infor-
mation that the existence of an exophoric subject
results in selecting the passive voice. To capture
this characteristic, the model classifies a verb in
question as passive if the annotated subject is ex-
ophoric; otherwise, it selects the active voice.
4.2 Results
We first evaluated performance of the first baseline
model with various ?. The results are shown in
Table 3, demonstrating that the baseline achieved
its best F-score when ? is 0.2. Therefore, we set
the ? to 0.2 in the following comparison.
Table 4 shows the results of the baselines and
proposed models. To investigate the impact of
each feature type, we conducted feature ablation
when using the maximum entropy model (ME:* in
Table 4). Table 4 shows that the model using the
feature type PRED achieves the best performance
among the four models when using a single feature
type. In addition, by adding feature type(s), the F-
score monotonically improves. Finally, the results
of the model using the PRED, ARG and COREF fea-
tures achieved the best F-score, 0.605, out of the
two baselines and models based on the maximum
entropy model. It indicates that each of the fea-
tures except SYN feature contributes to improving
performance in a complementary manner.
Furthermore, the results of the model using
SVM with the second degree polynomial kernel
show better performance than any model based on
model R P F
baseline1: scorepas ? 0.2 0.573 0.357 0.440
baseline2: exophora 0.493 0.329 0.395
ME: PRED 0.270 9.612 0.374
ME: SYN 0.000 N/A N/A
ME: ARG 0.095 0.516 0.161
ME: COREF 0.092 0.574 0.159
ME: PRED+SYN 0.282 0.618 0.387
ME: PRED+ARG 0.380 0.647 0.479
ME: PRED+COREF 0.480 0.762 0.589
ME: SYN+ARG 0.133 0.558 0.215
ME: SYN+COREF 0.147 9.618 9.238
ME: ARG+COREF 0.267 0.661 0.380
ME: PRED+SYN+ARG 0.397 0.656 0.494
ME: PRED+SYN+COREF 0.485 0.760 0.592
ME: PRED+ARG+COREF 0.506 0.752 0.605
ME: SYN+ARG+COREF 0.281 0.673 0.397
ME: ALL 0.507 0.747 0.604
SVM(linear): ALL 0.456 0.792 0.579
SVM(poly-2d): ALL 0.679 0.858 0.758
Table 4: Results of automatic voice selection
the maximum entropy model. This means that the
combination of features is important in this task
because of the dependency among the four kinds
of information introduced in Section 3.
5 Conclusion
This paper focused on the task of automatic voice
selection in text generation, taking into account
four kinds of linguistic information: passivisa-
tion preference of verbs, syntactic decisions, se-
mantic category of the arguments of a predicate,
and coreference or anaphoric relations of the argu-
ments. For empirical evaluation of voice selection
in Japanese, we used the predicate argument re-
lations and coreference relations annotated in the
NAIST Text Corpus (Iida et al, 2007). Integrat-
ing the four kinds of linguistic information into
a machine learning-based approach contributed to
improving F-score by about 0.3, compared to the
best baseline model, which utilises only the pas-
sivisation preference. Finally, we achieved 0.758
in F-score by combining features using SVM.
As future work, we are planning to incorpo-
rate the proposed voice selection model into natu-
ral language generation models for more sophisti-
cated text generation. In particular, generating re-
ferring expressions and voice selection are closely
related because both tasks utilise similar linguistic
information (e.g. salience and semantic informa-
tion of arguments) for generation. Therefore, our
next challenge is to solve problems about gener-
ating referring expressions and voice selection si-
multaneously by using optimisation techniques.
150
References
B. Abb, M. Herweg, and K. Lebeth. 1993. The incre-
mental generation of passive sentences. In Proceed-
ings of the 6th EACL, pages 3?11.
Douglas E. Appelt. 1985. Planning English referring
expressions. Artificial Intelligence, 26(1):1?33.
Barbara Di Eugenio, Davide Fossati, Dan Yu, Susan
Haller, and Michael Glass. 2005. Natural language
generation for intelligent tutoring systems: A case
study. In Proceedings of the 2005 conference on Ar-
tificial Intelligence in Education: Supporting Learn-
ing through Intelligent and Socially Informed Tech-
nology, pages 217?224.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203?226.
M. A. K. Halliday and C. Matthiessen. 2004. An Intro-
duction to Functional Grammar. Routledge.
R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007.
Annotating a Japanese text corpus with predicate-
argument and coreference relations. In Proceeding
of the ACL Workshop ?Linguistic Annotation Work-
shop?, pages 132?139.
S. Ikehara, M. Miyazaki, S. Shirai A. Yokoo,
H. Nakaiwa, K. Ogura, Y. Ooyama, and Y. Hayashi.
1997. Nihongo Goi Taikei (in Japanese). Iwanami
Shoten.
Kentaro Inui, Takenobu Tokunaga, and Hozumi
Tanaka. 1994. Text revision: A model and its
implementation. In Aspects of Automated Natural
Language Generation: Proceedings of the 6th Inter-
national Natural Language Generation Workshop,
pages 215?230.
S. Nariyama. 2002. Grammar for ellipsis resolution
in Japanese. In Proceedings of the 9th International
Conference on Theoretical and Methodological Is-
sues in Machine Translation, pages 135?145.
D. R. Radev and K. R. McKeown. 1998. Generat-
ing natural language summaries from multiple on-
line sources. Computational Linguistics, 24(3):469?
500.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
Ehud Reiter. 1994. Has a consensus NL generation
architecture appeared, and is it psycholinguistically
plausible? In Proceedings of the Seventh Interna-
tional Workshop on Natural Language Generation,
pages 163?170.
Jacques Robin. 1994. Revision-based Generation
of Natural Language Summaries Providing His-
torical Background ? Corpus-based Analysis, De-
sign, Implementation and Evaluation. Ph.D. thesis,
Columbia University.
F. Abu Sheikha and D. Inkpen. 2011. Generation of
formal and informal sentences. In Proceedings of
the 13th European Workshop on Natural Language
Generation, pages 187?193.
151
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 214?222,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Investigation of annotator?s behaviour using eye-tracking data
Ryu Iida Koh Mitsuda Takenobu Tokunaga
Department of Computer Science, Tokyo Institute of Technology
{ryu-i,mitsudak,take}@cl.cs.titech.ac.jp
Abstract
This paper presents an analysis of an anno-
tator?s behaviour during her/his annotation
process for eliciting useful information for
natural language processing (NLP) tasks.
Text annotation is essential for machine
learning-based NLP where annotated texts
are used for both training and evaluat-
ing supervised systems. Since an annota-
tor?s behaviour during annotation can be
seen as reflecting her/his cognitive process
during her/his attempt to understand the
text for annotation, analysing the process
of text annotation has potential to reveal
useful information for NLP tasks, in par-
ticular semantic and discourse processing
that require deeper language understand-
ing. We conducted an experiment for col-
lecting annotator actions and eye gaze dur-
ing the annotation of predicate-argument
relations in Japanese texts. Our analysis
of the collected data suggests that obtained
insight into human annotation behaviour
is useful for exploring effective linguis-
tic features in machine learning-based ap-
proaches.
1 Introduction
Text annotation is essential for machine learn-
ing (ML)-based natural language processing
(NLP) where annotated texts are used for
both training and evaluating supervised systems.
This annotation-then-learning approach has been
broadly applied to various NLP tasks, ranging
from shallow processing tasks, such as POS tag-
ging and NP chunking, to tasks requiring deeper
linguistic information, such as coreference resolu-
tion and discourse relation classification, and has
been largely successful for shallow NLP tasks in
particular. The key to this success is how use-
ful information can be effectively introduced into
ML algorithms as features. With shallow NLP
tasks, surface information like words and their
POS within a window of a certain size can be eas-
ily employed as useful features. In contrast, in
semantic and discourse processing, such as coref-
erence resolution and discourse structure analy-
sis, it is not trivial to employ as features deeper
linguistic knowledge and human linguistic intu-
ition that are indispensable for these tasks. In
order to improve system performance, past at-
tempts have integrated deeper linguistic knowl-
edge through manually constructed linguistic re-
sources such as WordNet (Miller, 1995) and lin-
guistic theories such as Centering Theory (Grosz
et al, 1995). They partially succeed in improv-
ing performance, but there is still room for further
improvement (duVerle and Prendinger, 2009; Ng,
2010; Lin et al, 2010; Pradhan et al, 2012).
Unlike past attempts relying on heuristic fea-
ture engineering, we take a cognitive science ap-
proach to improving system performance. In stead
of employing existing resources and theories, we
look into human behaviour during annotation and
elicit useful information for NLP tasks requir-
ing deeper linguistic knowledge. Particularly we
focus on annotator eye gaze during annotation.
Because of recent developments in eye-tracking
technology, eye gaze data has been widely used
in various research fields, including psycholin-
guistics and problem solving (Duchowski, 2002).
There have been a number of studies on the rela-
tions between eye gaze and language comprehen-
sion/production (Griffin and Bock, 2000; Richard-
son et al, 2007). Compared to the studies on
language and eye gaze, the role of gaze in gen-
eral problem solving settings has been less stud-
ied (Bednarik and Tukiainen, 2008; Rosengrant,
2010; Tomanek et al, 2010). Since our current in-
terest, text annotation, can be considered a prob-
lem solving as well as language comprehension
task, we refer to them when defining our prob-
214
lem setting. Through analysis of annotators? eye-
tracking data, we aim at finding useful information
which can be employed as features in ML algo-
rithms.
This paper is organised as follows. Section 2
presents the details of the experiment for collect-
ing annotator behavioural data during annotation
as well as details on the collected data. Section 3
explains the structure of the annotation process
for a single annotation instance. Section 4 pro-
vides a detailed analysis of human annotation pro-
cesses, suggesting usages of those results in NLP.
Section 5 reviews the related work and Section 6
concludes and discusses future research direc-
tions.
2 Data collection
2.1 Materials and procedure
We conducted an experiment for collecting anno-
tator actions and eye gaze during the annotation
of predicate-argument relations in Japanese texts.
Given a text in which candidates of predicates
and arguments were marked as segments (i.e. text
spans) in an annotation tool, the annotators were
instructed to add links between correct predicate-
argument pairs by using the keyboard and mouse.
We distinguished three types of links based on the
case marker of arguments, i.e. ga (nominative),
o (accusative) and ni (dative). For elliptical argu-
ments of a predicate, which are quite common in
Japanese texts, their antecedents were linked to the
predicate. Since the candidate predicates and ar-
guments were marked based on the automatic out-
put of a parser, some candidates might not have
their counterparts.
We employed a multi-purpose annotation tool
Slate (Kaplan et al, 2012), which enables anno-
tators to establish a link between a predicate seg-
ment and its argument segment with simple mouse
and keyboard operations. Figure 1 shows a screen-
shot of the interface provided by Slate. Segments
for candidate predicates are denoted by light blue
rectangles, and segments for candidate arguments
are enclosed with red lines. The colour of links
corresponds to the type of relations; red, blue and
green denote nominative, accusative and dative re-
spectively.
In order to collect every annotator operation, we
modified Slate so that it could record several im-
portant annotation events with their time stamp.
The recorded events are summarised in Table 1.
Event label Description
create link start creating a link starts
create link end creating a link ends
select link a link is selected
delete link a link is deleted
select segment a segment is selected
select tag a relation type is selected
annotation start annotating a text starts
annotation end annotating a text ends
Table 1: Recorded annotation events
Figure 2: Snapshot of annotation using Tobii T60
Annotator gaze was captured by the Tobii T60
eye tracker at intervals of 1/60 second. The Tobii?s
display size was 1, 280?1, 024 pixels and the dis-
tance between the display and the annotator?s eye
was maintained at about 50 cm. The five-point cal-
ibration was run before starting annotation. In or-
der to minimise the head movement, we used a
chin rest as shown in Figure 2.
We recruited three annotators who had experi-
ences in annotating predicate-argument relations.
Each annotator was assigned 43 texts for annota-
tion, which were the same across all annotators.
These 43 texts were selected from a Japanese bal-
anced corpus, BCCWJ (Maekawa et al, 2010). To
eliminate unneeded complexities for capturing eye
gaze, texts were truncated to about 1,000 charac-
ters so that they fit into the text area of the annota-
tion tool and did not require any scrolling. It took
about 20?30 minutes for annotating each text. The
annotators were allowed to take a break whenever
she/he finished annotating a text. Before restart-
ing annotation, the five-point calibration was run
every time. The annotators accomplished all as-
signed texts after several sessions for three or more
days in total.
215
SL
A
P
Figure 1: Screenshot of the annotation tool Slate
2.2 Results
The number of annotated links between predicates
and arguments by three annotators A0, A1 and A2
were 3,353 (A0), 3,764 (A1) and 3,462 (A2) re-
spectively. There were several cases where the
annotator added multiple links with the same link
type to a predicate, e.g. in case of conjunctive ar-
guments; we exclude these instances for simplicity
in the analysis below. The number of the remain-
ing links were 3,054 (A0), 3,251 (A1) and 2,996
(A2) respectively. In addition, because our anal-
yses explained in Section 4 require an annotator?s
fixation on both a predicate and its argument, the
number of these instances were reduced to 1,776
(A0), 1,430 (A1) and 1,795 (A2) respectively. The
details of the instances for our analysis are sum-
marised in Table 2. These annotation instances
were used for the analysis in the rest of this paper.
3 Anatomy of human annotation
From a qualitative analysis of the annotator?s be-
haviour in the collected data, we found the an-
case A0 A1 A2 total
ga (nominative) 1,170 904 1,105 3,179
o (accusative) 383 298 421 1,102
ni (dative) 223 228 269 720
total 1,776 1,430 1,795 5,001
Table 2: Results of annotation by each annotator
notation process for predicate-argument relations
could be decomposed into the following three
stages.
1. An annotator reads a given text and under-
stands its contents.
2. Having fixed a target predicate, she/he
searches for its argument in the set of preced-
ing candidate arguments considering a type
of relations with the predicate.
3. Once she/he finds a probable argument in a
text, she/he looks around its context in order
to confirm the relation. The confirmation is
finalised by creating a link between the pred-
icate and its argument.
216
The strategy of searching for arguments after fix-
ing a predicate would reflect the linguistic knowl-
edge that a predicate subcategorises its arguments.
In addition, since Japanese is a head-final lan-
guage, a predicate basically follows its arguments.
Therefore searching for each argument within a
sentence can begin at the same position, i.e. the
predicate, toward the beginning of the sentence,
when the predicate-first search strategy is adopted.
The idea of dividing a cognitive process into
different functional stages is common in cogni-
tive science. For instance, Just and Carpenter
(1985) divided a problem solving process into
three stages: searching, comparison and confirma-
tion. In their task, given a picture of two cubes
with a letter on each surface, a participant is in-
structed to judge whether they can be the same or
not. Since one of the cubes is relatively rotated
in a certain direction and amount, the participant
needs to mentally rotate the cubes for matching.
Russo and Leclerc (1994) divided a visual deci-
sion making process into three stages: orienta-
tion, evaluation and verification. In their exper-
iment, participants were asked to choose one of
several daily food products that were visually pre-
sented. The boundaries of the above three stages
were identified based on the participants? eye gaze
and their verbal protocols. Malcolm and Hender-
son (2009) applied the idea to a visual search pro-
cess, dividing it into initiation, scanning and ver-
ification. Gidlo?f et al (2013) discussed the dif-
ference between a decision making process and a
visual search process in terms of the process divi-
sion. Although the above studies deal with the dif-
ferent cognitive processes, it is common that the
first stage is for capturing an overview of a prob-
lem, the second is for searching for a tentative so-
lution, and the third is for verifying their solution.
Our division of the annotation process conforms
with this idea. Particularly, our task is similar to
the decision making process as defined by Russo
and Leclerc (1994). Unlike these past studies,
however, the beginning of an orientation stage1 is
not clear in our case, since we collected the data
in a natural annotation setting, i.e. a single anno-
tation session for a text includes creation of mul-
tiple links. In other words, the first stage might
correspond to multiple second and third stages. In
addition, in past research on decision making, a
single object is chosen, but our annotation task in-
1We follow the wording by Russo and Leclerc (1994).
???
link creation
first dwell on the linked argument
first dwell on the target predicate
? ?? ?
orientation
? ?? ?
evaluation
? ?? ?
verification
-
time
Figure 3: Division of an annotation process
volves two objects to consider, i.e. a predicate and
an argument.
Considering these differences and the propos-
als of previous studies (Russo and Leclerc, 1994;
Gidlo?f et al, 2013)?we define the three stages as
follows. As explained above, we can not identify
the beginning of an orientation stage based on any
decisive clue. We define the end of an orientation
stage as the onset of the first dwell2 on a predi-
cate being considered. The succeeding evaluation
stage starts at the onset of the first dwell on the
predicate and ends at the onset of the first dwell on
the argument that is eventually linked to the pred-
icate. The third stage, a verification stage, starts
at the onset of the first dwell on the linked argu-
ment and ends at the creation of the link between
the predicate and argument. These definitions and
the relations between the stages are illustrated in
Figure 3.
The time points indicating the stage boundaries
can be identified from the recorded eye gaze and
tool operation data. First, gaze fixations were ex-
tracted by using the Dispersion-Threshold Identi-
fication (I-DT) algorithm (Salvucci and Goldberg,
2000). Based on a rationale that the eye movement
velocity slows near fixations, the I-DT algorithm
identifies fixations as clusters of consecutive gaze
points within a particular dispersion. It has two pa-
rameters, the dispersion threshold that defines the
maximum distance between gaze points belonging
to the same cluster, and the duration threshold that
constrains the minimum fixation duration. Con-
sidering the experimental configurations, i.e. (i)
the display size and its resolution, (ii) the distance
between the display and the annotator?s eyes, and
(iii) the eye-tracker resolution, we set the disper-
sion threshold to 16 pixels. Following Richard-
son et al (2007), we set the duration threshold
to 100 msec. Based on fixations, a dwell on a
segment was defined as a series of fixations that
consecutively stayed on the same segment where
2A dwell is a collection of one or several fixations within
a certain area of interest, a segment in our case.
217
two consecutive fixations were not separated by
more than 100 msec. We allowed a horizontal er-
ror margin of 16 pixels (one-character width) for
both sides of a segment when identifying a dwell.
Time points of link creation were determined by
the ?create link start? event in Table 1.
Among these three stages, the evaluation stage
would be most informative for extracting useful
features for ML algorithms, because an annotator
identifies a probable argument for a predicate un-
der consideration during this stage. Analysing an-
notator eye gaze during this stage could reveal use-
ful information for predicate-argument analysis. It
is, however, insufficient to regard only fixated ar-
guments as being under the annotator?s consider-
ation during the evaluation stage. The annotator
captures an overview of the current problem dur-
ing the previous orientation stage, in which she/he
could remember several candidate arguments in
her/his short-term memory, then moves on to the
evaluation stage. Therefore, all attended argu-
ments are not necessarily observed through gaze
dwells. As we explained earlier, we have no means
to identify a rigid duration of an orientation stage,
thus it is difficult to identify a precise set of can-
didate arguments under the annotator?s considera-
tion in the evaluation stage. For this purpose, we
need a different experimental design so that every
predicate-argument relation is annotated at a time
in the same manner as the above decision making
studies conducted. Another possibility is using an
annotator?s verbal protocols together with her/his
eye gaze as done in Russo and Leclerc (1994).
On the other hand, in the verification stage a
probable argument has been already determined
and its validity confirmed by investigating its com-
petitors. We would expect considered competi-
tors are explicitly fixated during this stage. Since
we have a rigid definition of the verification stage
duration, it is possible to analyse the annotator?s
behaviour during this stage based on her/his eye
gaze. For this reason, we concentrate on the anal-
ysis of the verification stage of annotation hence-
forth.
4 Analysis of the verification stage
Given the set of annotation instances, i.e. pred-
icate, argument and case triplets, we categorise
these instances based on the annotator?s behaviour
during the verification stage. We focus on two fac-
tors for categorising annotation instances: (i) the
1100
10,000
0 10 20 30 40 50 60 70 80 90 100# I
nstan
ces
Distance between predicate and argument
0%50%
100%
0 10 20 30 40 50 60 70 80 90 100Dis
tribu
tion
Distance between predicate and argument
? Distracted      ? Concentrated
Figure 4: Distance of predicate and argument
distance of a predicate and if its argument is ei-
ther near or far, and (ii) whether annotator gaze
dwelled on other arguments than the eventually
linked argument before creating the link. We call
the former factor Near/Far distinction, and the lat-
ter Concentrated/Distracted distinction.
To decide the Near/Far distinction, we inves-
tigated the distribution of distances of predicates
and their argument. The result is shown in the
upper graph of Figure 4, where the x-axis is the
character-based distance and the y-axis shows the
number of instances in each distance bin. Figure 4
demonstrates that the instances concentrate at the
bin of distance 1. This reflects the frequently
occurring instances where a one-character case
maker follows an argument, and immediately pre-
cedes its predicate. The lower graph in Figure 4
shows the ratio of Distracted instances to Con-
centrated at each bin. The distribution indi-
cates that there is no remarkable relation between
the distance and Concentrated/Distracted distinc-
tion. The correlation coefficient between the dis-
tance and the number of Concentrated instances
is ?0.26. We can conclude that the distance of
a predicate and its argument does not impact the
Concentrated/Distracted distinction. Considering
the above tendency, we set the distance threshold
to 22, the average distance of all annotation in-
stances; instances with a distance of less than 22
are considered Near.
These two factors make four combinations
in total, i.e. Near-Concentrated (NC), Near-
Distracted (ND), Far-Concentrated (FC) and Far-
Distracted (FD). We analysed 5,001 instances
shown in Table 2 to find three kinds of tendencies,
which are described in the following sections.
218
case Near Far total
ga (nominative) 2,201 (0.44) 978 (0.90) 3,179 (0.64)
o (accusative) 1,042 (0.34) 60 (0.05) 1,102 (0.22)
ni (dative) 662 (0.22) 58 (0.05) 720 (0.14)
Table 3: Distribution of cases over Near/Far
NC ND FC FD
ga 0.40 0.47 0.92 0.90
o, ni 0.60 0.53 0.08 0.10
Table 4: Distribution of arguments across four cat-
egories
4.1 Predicate-argument distance and
argument case
We hypothesise that an annotator changes her/his
behaviour with regard to the case of the argu-
ment. The argument case in Japanese is marked
by a case marker which roughly corresponds to
the argument?s semantic role, such as Agent and
Theme. We therefore analysed the relationship
between the Near/Far distinction and argument
case. The results are shown in Table 3. The ta-
ble shows the distribution of argument cases, il-
lustrating that Near instances are dispersed over
three cases, while Far instances are concentrated
in the ga (nominative) case. In other words, ga-
arguments tend to appear far from their predi-
cate. This tendency reflects the characteristic of
Japanese where a nominative argument tends to be
placed in the beginning of a sentence; furthermore,
ga-arguments are often omitted to make ellipses.
In our annotation guideline, a predicate with an el-
liptical argument should be linked to the referent
of the ellipsis, which would be realised at a fur-
ther distant position in the preceding context. In
contrast, o (accusative) and ni (dative) arguments
less frequently appeared as Far instances because
they are rarely omitted due to their tighter rela-
tion with arguments. This observation suggests
that each case requires an individual specific treat-
ment in the model of predicate argument analysis;
the model searches for o and ni arguments close to
its predicate, while it considers all preceding can-
didates for a ga argument.
Table 4 shows the break down of the
Near/Far columns with regards to the Con-
centrated/Distracted distinction, demonstrating
that the Concentrated/Distracted distinction does
not impact the distribution of the argument types.
05
1015
20
110
1001000
0
5
10
15
20
# instances
# existing links # dw
ells o
n co
mpe
titor
s
Figure 5: Relationship between the number of
dwells on competitors and already-existing links
4.2 Effect of already-existing links
In the Concentrated instances, an annotator can
verify if an argument is correct without inspect-
ing its competitors. As illustrated in Figure 1, al-
ready annotated arguments are marked by explicit
links to their predicate. These links make the ar-
guments visually as well as cognitively salient in
an annotator?s short-term memory because they
have been frequently annotated in the preceding
annotation process. Thus, we expected that both
types of saliency help to confirm the predicate-
argument relation under consideration. For in-
stance, when searching for an argument of pred-
icate P in Figure 1, argument A that already has
six links (SL) is more salient than other competi-
tors.
To verify this hypothesis, we examined the re-
lation of the number of already-existing links and
the number of dwells on competitors, which is
shown in Figure 5. In this analysis, we used only
Far instances because the Near arguments tended
to have less already-existing links as they were
under current interest. Figure 5 shows a three-
dimensional declining slope that peaks around the
intersection for instances with the fewest number
of links and dwells on competitors. It reveals
a mostly symmetrical relation between existing
links and dwells on competitors for instances with
a lower number of existing links, but that this sym-
metry brakes for instances with a higher number
of existing links, visible by the conspicuous hole
219
toward the left of the figure. This suggests that
visual and cognitive saliency reduces annotators?
cognitive load, and thus contributes to efficiently
confirming the correct argument.
This result implies that the number of already-
existing links of a candidate argument would re-
flect its saliency, thus more linked candidates
should be preferred in the analysis of predicate-
argument relations. Although we analysed the ver-
ification stage, the same effect could be expected
in the evaluation stage as well. Introducing such
information into ML algorithms may contribute to
improving system performance.
4.3 Specificity of arguments and dispersal of
eye gaze
Existing Japanese corpora annotated with
predicate-argument relations (Iida et al, 2007;
Kawahara et al, 2002) have had syntactic heads
(nouns) of their projected NPs related their pred-
icates. Since Japanese is a head-final language,
a head noun is always placed in the last position
of an NP. This scheme has the advantage that
predicate-argument relations can be annotated
without identifying the starting boundary of the
argument NP under consideration. The scheme
is also reflected in the structure of automatically
constructed Japanese case frames, e.g. Sasano et
al. (2009), which consist of triplets in the form
of ?Noun, Case, Verb?. Noun is a head noun
extracted from its projected NP in the original
text. We followed this scheme in our annotation
experiments.
However, a head noun of an argument does not
always have enough information. A nominaliser
which often appears in the head position in an
NP does not have any semantic meaning by it-
self. For instance, in the NP ?benkyo? suru koto
(to study/studying)?, the head noun ?koto? has no
specific semantic meaning, corresponding to an
English morpheme ?to? or ?-ing?. In such cases,
inspecting a whole NP including its modifiers is
necessary to verify the validity of the NP for an
argument in question. We looked at our data to
see if annotators actually behaved like this.
For analysis, the annotation instances were dis-
tinguished if an argument had any modifier or not
(column ?w/o mod? and ?w/ mod? in Table 5).
The ?w/ mod? instances are further divided into
two classes: ?within NP? and ?out of NP?, the for-
mer if all dwells remain ?within? the region of the
w/o mod w/ mod total
within NP out of NP
Concentrated 1,562 1190 ? 2,752
Distracted 1,168 242 839 2,249
Table 5: Relation of argument modifiers and gaze
dispersal
argument NP or the later if they go ?out of? the
region. Note that our annotation scheme creates
a link between a predicate and the head of its ar-
gument as described earlier. Thus, a Distracted
instance does not always mean an ?out of NP? in-
stance, since a distracted dwell might still remains
on a segment within the NP region despite not be-
ing its head. Table 5 shows the distribution of the
instances over this categorisation.
We found that the number of instances is almost
the same between Concentrated and Distracted,
i.e. (2752 : 2249 = 0.55 : 0.45). In this re-
spect, both Concentrated and Distracted instances
can be treated in the same way in the analysis of
predicate-argument relations. A closer look at the
break down of the ?w/ mod? category, however, re-
veals that almost 22% of the Distracted arguments
with any modifier attracted gaze dwells within the
NP region. This fact suggests that we need to treat
candidate arguments differently depending on if
they have modifiers or not. In addition to argument
head information, we could introduce information
of modifiers into ML algorithms as features that
characterise a candidate argument more precisely.
5 Related work
Recent developments in the eye-tracking technol-
ogy enables various research fields to employ eye-
gaze data (Duchowski, 2002).
Bednarik and Tukiainen (2008) analysed eye-
tracking data collected while programmers debug
a program. They defined areas of interest (AOI)
based on the sections of the integrated develop-
ment environment (IDE): the source code area,
the visualised class relation area and the program
output area. They compared the gaze transitions
among these AOIs between expert and novice pro-
grammers to find different transition patterns be-
tween them. Since the granularity of their AOIs
is coarse, it could be used for evaluating a pro-
grammer?s expertise, but hardly explains why the
expert transition pattern realises a good program-
ming skill. In order to find useful information for
language processing, we employed smaller AOIs
220
at the character level.
Rosengrant (2010) proposed an analysis method
named gaze scribing where eye-tracking data is
combined with a subject?s thought process derived
by the think-aloud protocol (TAP) (Ericsson and
Simon, 1984). As a case study, he analysed a pro-
cess of solving electrical circuit problems on the
computer display to find differences of problem
solving strategy between novice and expert sub-
jects. The AOIs are defined both at a macro level,
i.e. the circuit, the work space for calculation,
and a micro level, i.e. electrical components of
the circuit. Rosengrant underlined the importance
of applying gaze scribing to the solving process
of other problems. Although information obtained
from TAP is useful, it increases her/his cognitive
load, and thus might interfere with her/his achiev-
ing the original goal.
Tomanek et al (2010) utilised eye-tracking data
to evaluate the degree of difficulty in annotating
named entities. They are motivated by selecting
appropriate training instances for active learning
techniques. They conducted experiments in vari-
ous settings by controlling characteristics of target
named entities. Compared to their named entity
annotation task, our annotation task, annotating
predicate-argument relations, is more complex. In
addition, our experimental setting is more natural,
meaning that all possible relations in a text were
annotated in a single session, while each session
targeted a single named entity (NE) in a limited
context in the setting of Tomanek et al (2010).
Finally, our fixation target is more precise, i.e.
words, rather than a coarse area around the target
NE.
We have also discussed evaluating annotation
difficulty for predicate-argument relations by us-
ing the same data introduced in this paper (Toku-
naga et al, 2013). Through manual analysis of
the collected data, we suggested that an annotation
time necessary for annotating a single predicate-
argument relation was correlated with the agree-
ment ratio among multiple human annotators.
6 Conclusion
This paper presented an analysis of an annota-
tor?s behaviour during her/his annotation process
for eliciting useful information for NLP tasks.
We first conducted an experiment for collect-
ing three annotators? actions and eye gaze dur-
ing their annotation of predicate-argument rela-
tions in Japanese texts. The collected data were
analysed from three aspects: (i) the relationship
of predicate-argument distances and argument?s
cases, (ii) the effect of already-existing links and
(iii) specificity of arguments and dispersal of eye
gaze. The analysis on these aspects suggested that
obtained insight into human annotation behaviour
could be useful for exploring effective linguistic
features in ML-based approaches.
As future work, we need to further investigate
the data from other aspects. There are advantages
to manual analysis, such as done in this paper.
Mining techniques for finding unknown but useful
information may also be advantageous. Therefore,
we are planning to employ mining techniques for
finding useful gaze patterns for various NLP tasks.
In this paper, we suggested useful information
that could be incorporated into ML algorithms as
features. It is necessary to implement these fea-
tures in a specific ML algorithm and evaluate their
effectiveness empirically.
Our analysis was limited to the verification
stage of annotation, in which a probable argument
of a predicate was confirmed by comparing it with
other competitors. The preceding evaluation stage
should be also analysed, since it is the stage where
annotators search for a correct argument of a pred-
icate in question, thus probably includes useful in-
formation for computational models in identifying
predicate-argument relations. For the analysis of
the evaluation stage, a different design of experi-
ments would be necessary, as already mentioned,
employing single annotation at a time scheme as
Tomanek et al (2010) did, or using an annota-
tor?s verbal protocol together as Russo and Leclerc
(1994), and Rosengrant (2010) did.
Last but not least, data collection and analy-
sis in different annotation tasks are indispensable.
It is our ultimate goal to establish a methodol-
ogy for collecting an analysing annotators? be-
havioural data during annotation in order to elicit
effective features for ML-based NLP.
References
Roman Bednarik and Markku Tukiainen. 2008. Tem-
poral eye-tracking data: Evolution of debugging
strategies with multiple representations. In Proceed-
ings of the 2008 symposium on Eye tracking re-
search & applications (ETRA ?08), pages 99?102.
Andrew T. Duchowski. 2002. A breadth-first survey of
eye-tracking applications. Behavior Research Meth-
221
ods, Instruments, and Computers, 34(4):455?470.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine
classification. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 665?673.
K. Anders Ericsson and Herbert A. Simon. 1984. Pro-
tocol Analysis ? Verbal Reports as Data ?. The MIT
Press.
Kerstin Gidlo?f, Annika Wallin, Richard Dewhurst, and
Kenneth Holmqvist. 2013. Using eye tracking to
trace a cognitive process: Gaze behaviour during de-
cision making in a natural environment. Journal of
Eye Movement Research, 6(1):1?14.
Zenzi M. Griffin and Kathryn Bock. 2000. What the
eyes say about speaking. Psychological Science,
11(4):274?279.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceeding of the ACL Workshop ?Linguis-
tic Annotation Workshop?, pages 132?139.
Marcel Adam Just and Patricia A. Carpenter. 1985.
Cognitive coordinate systems: Accounts of mental
rotation and individual differences in spatial ability.
Psychological Review, 92(2):137?172.
Dain Kaplan, Ryu Iida, Kikuko Nishina, and Takenobu
Tokunaga. 2012. Slate ? a tool for creating and
maintaining annotated corpora. Journal for Lan-
guage Technology and Computational Linguistics,
26(2):89?101.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus (in Japanese). In Proceed-
ings of the 8th Annual Meeting of the Association for
Natural Language Processing, pages 495?498.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A PDTB-styled end-to-end discourse parser. Tech-
nical Report TRB8/10, School of Computing, Na-
tional University of Singapore.
Kikuo Maekawa, Makoto Yamazaki, Takehiko
Maruyama, Masaya Yamaguchi, Hideki Ogura,
Wakako Kashino, Toshinobu Ogiso, Hanae Koiso,
and Yasuharu Den. 2010. Design, compilation,
and preliminary analyses of balanced corpus of
contemporary written Japanese. In Proceedings of
the Eigth International Conference on Language
Resources and Evaluation (LREC 2010), pages
1483?1486.
George L. Malcolm and John M. Henderson. 2009.
The effects of target template specificity on visual
search in real-world scenes: Evidence from eye
movements. Journal of Vision, 9(11):8:1?13.
George A. Miller. 1995. WordNet: A lexical database
for English. Communications of the ACM, 38:39?
41.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: The first fifteen years. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
1396?1411.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL ? Shared Task, pages
1?40.
Daniel C. Richardson, Rick Dale, and Michael J.
Spivey. 2007. Eye movements in language and cog-
nition: A brief introduction. In Monica Gonzalez-
Marquez, Irene Mittelberg, Seana Coulson, and
Michael J. Spivey, editors, Methods in Cognitive
Linguistics, pages 323?344. John Benjamins.
David Rosengrant. 2010. Gaze scribing in physics
problem solving. In Proceedings of the 2010 sym-
posium on Eye tracking research & applications
(ETRA ?10), pages 45?48.
J. Edward Russo and France Leclerc. 1994. An
eye-fixation analysis of choice processes for con-
sumer nondurables. Journal of Consumer Research,
21(2):274?290.
Dario D. Salvucci and Joseph H. Goldberg. 2000.
Identifying fixations and saccades in eye-tracking
protocols. In Proceedings of the 2000 symposium on
Eye tracking research & applications (ETRA ?00),
pages 71?78.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case frame
acquisition for discourse analysis. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL-
HLT 2009), pages 521?529.
Takenobu Tokunaga, Ryu Iida, and Koh Mitsuda.
2013. Annotation for annotation - toward elicit-
ing implicit linguistic knowledge through annota-
tion -. In Proceedings of the 9th Joint ISO - ACL
SIGSEM Workshop on Interoperable Semantic An-
notation (ISA-9), pages 79?83.
Katrin Tomanek, Udo Hahn, Steffen Lohmann, and
Ju?rgen Ziegler. 2010. A cognitive cost model of
annotations based on eye-tracking data. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
1158?1167.
222
