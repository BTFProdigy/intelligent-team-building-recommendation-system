Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 900?909,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Statistical Estimation of Word Acquisition with
Application to Readability Prediction
Paul Kidwell
Department of Statistics
Purdue University
West Lafayette, IN
kidwellpaul@gmail.com
Guy Lebanon
College of Computing
Georgia Institute of Technology
Atlanta, GA
lebanon@cc.gatech.edu
Kevyn Collins-Thompson
Microsoft Research
Redmond, WA
kevynct@microsoft.com
Abstract
Models of language learning play a cen-
tral role in a wide range of applica-
tions: from psycholinguistic theories of
how people acquire new word knowledge,
to information systems that can automati-
cally match content to users? reading abil-
ity. We present a novel statistical ap-
proach that can infer the distribution of
a word?s likely acquisition age automati-
cally from authentic texts collected from
the Web. We then show that combining
these acquisition age distributions for all
words in a document provides an effective
semantic component for predicting read-
ing difficulty of new texts. We also com-
pare our automatically inferred acquisition
ages with norms from existing oral stud-
ies, revealing interesting historical trends
as well as differences between oral and
written word acquisition processes.
1 Introduction
Word acquisition refers to the temporal process by
which children learn the meaning and understand-
ing of new words. Some words are acquired at
a very early age, some are acquired at early pri-
mary school grades, and some are acquired at high
school or even later in life as the individual under-
goes experiences related to that word. A related
concept to acquisition age is document grade level
readability which refers to the school grade level
of the document?s intended audience. It applies
in situations where documents are written with the
expressed intent of being understood by children
in a certain school grade. For example, textbooks
authored specifically for fourth graders are said to
have readability grade level four.
We develop and evaluate a novel statistical
model that draws a connection between document
grade level readability and age acquisition distri-
butions. Based on previous work in the area, we
define a model for document readability using a
logistic Rasch model and the quantiles of the ac-
quisition age distributions. We then proceed to in-
fer the age acquisition distributions for different
words from document readability data collected
by crawling the web.
We examine the inferred acquisition distribu-
tions from two perspectives. First, we analyze and
contrast them with previous studies on oral word
acquisition, revealing interesting historical trends
as well as differences between oral and written
word acquisition processes. Second, the inferred
acquisition distributions serve as parameters for
the readability model, which enables us to predict
the readability level of novel documents.
To our knowledge, this is the first published
study of a method to ?reverse-engineer? individ-
ual word acquisition statistics from graded texts.
By obtaining such a fine-grained model of how
language evolves over time, we obtain a new,
rich source of semantic features for a document.
The increasing amounts of content available from
the Web and other sources also means that these
flexible models of authentic usage can be eas-
ily adapted for different tasks and populations.
Our work serves to complement the growing body
of research using statistics and machine learn-
ing for language learning tasks, and has appli-
cations including predicting reading difficulty for
Web pages and other non-traditional documents,
reader-specific example and question generation
for lexical practice in intelligent tutoring systems,
and analysis tools for language learning research.
2 A Model for Document Readability
and Word Acquisition
For a fixed word and a fixed population of indi-
viduals T the age of acquisition (AoA) distribu-
tion p
w
represents the age at which word w was
900
acquired by the population. Existing AoA norm
studies almost universally summarize AoA ratings
in terms of two parameters: mean and standard
deviation, ignoring higher-level moments such as
skew. For direct comparison with these studies we
follow this convention and thus our goal is to esti-
mate AoA for a word w in terms of mean ?
w
and
standard deviation ?
w
parameters using the (trun-
cated) normal distribution
p
w
(t) ? N(t ;?
w
, ?
w
) =
e
?(t??
w
)
2
/(2?
2
w
)
?
2??
2
w
(1)
where the proportionality constant ensures that the
distribution is normalized over the range of ages
under consideration e.g., t ? [6, 18] for school
grades. It is important to note that our model is
not restricted by the assumption of (1) and can be
readily extended to the Gamma family of distribu-
tions, if modeling asymmetric spread in the distri-
bution is appropriate.
For a fixed vocabulary V of distinct words the
age acquisition distributions for all words w ? V
are defined using 2|V | parameters
{(?
w
, ?
w
) : w ? V }. (2)
These parameters, which are the main objects of
interest, can in principle be estimated from data
using standard statistical techniques. Unfortu-
nately, data containing explicit acquisition ages is
very difficult to obtain reliably. Explicit word ac-
quisition data is based on interviewing adults re-
garding their age acquisition process during child-
hood and so may be unreliable and difficult to ob-
tain for a large representative group of people.
On the other hand, it is possible to reliably col-
lect large quantities of readability data defined as
pairs of documents and ages of intended audience.
As we demonstrate later in the paper, such data
may be automatically obtained by crawling spe-
cialized resources on the Web. We demonstrate
how to use such data to estimate the word acqui-
sition parameters (2) and to use the estimates to
predict future readability ages.
Traditionally, document readability has been
defined in terms of the school grade level at which
a large portion of the words have been acquired
by most children (Chall and Dale, 1995). We pro-
pose the following interpretation of that definition,
which is made appropriate for quantitative studies
by taking into account the inherent randomness in
the acquisition process.
Definition 1. A document d = (w
1
, . . . , w
m
) is
said to have (1 ? ?
1
, 1 ? ?
2
)-readability level t if
by age t no less than 1? ?
1
percent of the words in
d have been acquired each by no less than 1 ? ?
2
percent of the population.
We denote by q
w
the quantile function of the cdf
corresponding to the acquisition distribution p
w
.
In other words, q
w
(r) represents the age at which
r percent of the population T have acquired word
w. Despite the fact that it does not have a closed
form, it is a continuous and smooth function of the
parameters ?
w
, ?
w
in (1) (assuming T is infinite)
and can be tabulated before inference begins.
Following Definition 1 we define a logistic
Rasch readability model:
log
P (d is (s, r)-readable at age t)
1 ? P (d is (s, r)-readable at age t)
= ?(q
d
(s, r) ? t) (3)
where q
d
(s, r) is the s quantile of {q
w
i
(r) : i =
1, . . . ,m}. An equivalent formulation to (3) that
makes the probability model more explicit is
P (d is (s, r)-readable at age t)
=
exp(?(q
d
(s, r) ? t))
1 + exp(?(q
d
(s, r) ? t))
. (4)
In other words, the probability of a document d
being (s, r)-readable increases exponentially with
q
d
(s, r) which is the age at which s percent of the
words in d have been acquired each by r percent
of the population.
The parameter r = 1 ? ?
2
determines what it
means for a word to be acquired and is typically
considered to be a high value such as 0.8. The
parameter s = 1 ? ?
1
determines how many of
the document words need to be acquired for it to
be readable. It can be set to a high value such as
0.9 if a very precise understanding is required for
readability but can be reduced when a more mod-
est definition of readability applies.
We note that due to the discreteness of the set
{q
w
i
(r) : i = 1, . . . ,m}, neither q
d
(s, r) nor the
loglikelihood are differentiable in the parameters
(2). This raises some practical difficulties with
respect to the computational maximization of the
likelihood and subsequent estimation of (2). How-
ever, for long documents containing a large num-
ber of words, q
d
(s, r) is approximately smooth
which motivates a maximum likelihood procedure
using gradient descent on a smoothed version of
901
qd
(s). Alternative optimization techniques which
do not require smoothness may also be used.
In the case of a normal distribution (1) we have
that a word is acquired by r percent of the pop-
ulation at age w = ? + ??1(r)?, where ? is
the cumulative distribution function (cdf) of the
normal distribution. To investigate the distribu-
tion of acquisition ages we assume that the ?, ?
parameters corresponding to different words in a
document are drawn from Gamma distributions
? ? G(?
1
, ?
1
) and ? ? G(?
2
, ?
2
). The normal
and Gamma distributions are chosen in part be-
cause they are flexible enough to model many sit-
uations and also admit good statistical estimation
theory. Noting that ??1(r)? ? G(?
2
,?
?1
(r)?
2
),
we can write the distribution of the acquisition
ages as the following convolution
f
W
(w) =
w
?
1
+?
2
?1
e
?w/?
2
?(?
1
)?(?
2
)?
?
1
1
?
?
2
2
?
?
1
0
t
?
1
?1
e
(?
1
??
2
)tw
?
1
?
2
(1 ? t)
1??
2
dt
which reverts to a Gamma when ?
1
= ?
2
.
The distribution of the s-percentile of f
W
,
which amounts to (r, s)-readability of documents,
can be analyzed by combining f
W
above with a
standard normal approximation of order statistics
(e.g., (David and Nagaraja, 2003))
X
?mp?
? N
(
F
?1
W
(p),
p(1 ? p)
m[f
W
(F
?1
W
(p))]
2
)
where m is the document length and F
W
is the cdf
corresponding to f
W
.
Figure 1 shows the relationship between docu-
ment length and confidence interval (CI) width in
readability prediction. It contrasts the CI widths
for model based intervals and empirical intervals.
In both cases, documents of lengths larger than
100 words provide CI widths shorter than 1 year.
This finding is also noteworthy as it provides
empirical support for the long-standing ?rule-of-
thumb? that readability measures become unreli-
able for passages of less than 100 words (Fry,
1990).
3 Experimental Results
Our experimental study is divided into three parts.
The first part examines the word acquisition dis-
tributions that were estimated based on readabil-
ity data. The second part compares the estimated
Document Length
95
%
 C
I W
id
th
0.5
1.0
1.5
2.0
2.5
3.0
50 100 150 200
Figure 1: A comparison of model (dashed) vs. em-
pirical (solid) 95% confidence interval widths as a
function of document length (r = 0.9 and s =
0.7). CI widths were computed using 1000 Monte
Carlo samples generated from the f
W
model fit to
the data and from the empirical distribution. Word
distributions correspond to a 1577 word document
written for a 7th grade audience taken from the
Web 1-12 corpus.
(written) acquisition ages with oral acquisition
ages obtained from interview studies reported in
the literature. The third part focuses on using the
estimated word acquisition distributions to predict
document readability. These three experimental
studies are described in the three subsections be-
low.
In our experiments we used three readability
datasets. The corpora were compiled by crawl-
ing web pages containing documents authored for
audiences of specific grade levels. The Web 1-
12 data contains 373 documents, with each doc-
ument written for a particular school grade level
in the range 1-12. The Weekly Reader (WR)
dataset, was obtained by crawling the commercial
website www.wrtoolkit.com after receiving spe-
cial permission. That dataset contains a total of
1780 documents, with 4 readability levels rang-
ing from 2 to 5 indicating the school grade lev-
els of the intended audience. A total of 788 doc-
uments with readability between grades 2 and 5
and having length greater than 50 words were se-
lected from 1780 documents. The Reading A-Z
dataset, contains a set of 215 documents was ob-
tained from Reading A-Z.com, spanning grade 1
through grade 6.
The grade levels in these three corpora, which
correspond to US school grades, were either ex-
plicitly specified by the organization or authors
902
who created the text, or implicit in the class-
room curriculum page where the document was
acquired. The pages were drawn from a wide
range of subject areas, including history, science,
geography, and fiction.
To reduce the possibility of overfitting, we used
a common feature selection technique of eliminat-
ing words appearing in less than 4 documents. In
the experiments we used maximum likelihood to
estimate the model parameters {(?
w
, ?
2
w
) : w ?
V } for the Rasch model (3). The maximum likeli-
hood was obtained using a non-smooth coordinate
descent procedure.
3.1 Estimation of Word Acquisition
Distributions
Figure 2 displays the inferred age acquisition dis-
tributions and empirical word appearances of three
words: thought (left), multitude (middle),
and assimilate (right). In these plots, the em-
pirical cdf of word appearances is indicated by a
piecewise constant line while the probability den-
sity function of the estimated AoA distribution is
indicated by a dashed line. The vertical line in-
dicates the 0.8 quantile of the AoA distribution
which corresponds to the grade by which 80% of
the children have acquired the word.
The word assimilation appears in 2 doc-
uments having 12th grade readability. The high
grade level of these documents results in a high es-
timated acquisition age and the paucity of observa-
tions leads to a large uncertainty in this estimate as
seen by the variance of the acquisition age distri-
bution. The word thought appears several times
in multiple grades. It is first observed in the 1st
grade and not again until the 4th grade resulting in
an estimated acquisition age falling between the
two. The variance of this acquisition distribution
is relatively small due to the frequent use of this
word. The empirical cdf shows that multitude
is used in grades 6, 8, and 9. Relative to thought
and assimilation the word multitude was
used less and more frequently respectively, which
leads to an acquisition age distribution with a
larger variance than that of thought and smaller
than that of assimilation.
The relationship in Figure 2 between the em-
pirical word appearances and the age acquisition
distribution demonstrates the following behavior:
(a) The variance of the age acquisition distribu-
tion goes down as the word appears in more doc-
uments, and (b) the mean of the AoA distribution
tends to be lower than the mean of the empirical
word appearance distribution, and in many cases
even smaller than the first grade in which the word
appeared. This is to be expected as authors use
specific words only after they believe the words
were acquired by a large portion of the intended
audience.
3.2 Comparison with Oral Studies
Among the related work in the linguistic commu-
nity, are several studies concerning oral acquisi-
tions of words. These studies estimate the age
at which a word is acquired for oral use based
an interview processes with participating adults.
We focus specifically on the seminal study of ac-
quisition ages performed by Gilhooly and Logie
(GL) (1980) and made available through the MRC
database (Coltheart, 1981).
There are some substantial differences between
these previous studies and our approach. We an-
alyze the age acquisition process through docu-
ment readability which leads to a written, rather
than oral, notion of word acquisition. Further-
more, our estimates are based on documents writ-
ten with a specific audience in mind, while the pre-
vious studies are based on interviewing adults re-
garding their childhood word acquisition process
which is arguably less reliable due to the age dif-
ference between the acquisition and the interview.
Finally, the GL study was performed in the late
1970s while our study uses contemporary internet
data. Conceivably, the word acquisition process
changed over the past 3 decades.
Despite these differences, it is interesting to
contrast our inferred age acquisitions with the GL
study and consider the differences and similari-
ties. Figure 3 displays the relationship between
the GL age of acquisition (AoA) and the acquisi-
tion ages obtained from readability data based on
the s = 0.8 quantile. Some correlation is present
(r2 = 0.34) but the two measures differ consid-
erably. As expected, the acquisition ages obtained
from written readability data tend to be higher than
the oral studies. The distributions of differences
between the GL acquisition ages and the ones in-
ferred from the readability data appears in Fig-
ure 4.
Comparing the acquisition ages obtained from
readability data to the GL study results in a mean
absolute error of 0.9 to 1.5, depending on the spe-
903
5 10 15
mu: 1.9
sigma: 0.5
5 10 15
mu: 5
sigma: 1.2
5 10 15
mu: 9
sigma: 3.4
Figure 2: A comparison of empirical word appearances and AoA distributions for three words:
thought (left), multitude (middle), and assimilation (right). The empirical cdf of word ap-
pearances appears as a piecewise constant line and the estimated pdf is indicated by the dashed curve
with its 0.8 quantile indicated by a vertical line.
cific value of the Rasch parameter ?. Interestingly,
the tendency for the written acquisition age to ex-
ceed the oral one diminishes as the grade level in-
creases. This represents the notion that at higher
grades words are acquired in both oral and written
senses at the same age.
Predicted versus Oral Acquisition Age
GL AoA
Pr
ed
ict
ed
 A
oA
2
4
6
8
10
2 4 6 8 10
Figure 3: A scatter plot (s = 80, n = 50) of pre-
dicted age of acquisition versus Gilhooly and Lo-
gie?s values reveals the tendency for the written
estimate to exceed the oral estimate (r2 = 0.34).
A comparison to two more recent studies con-
firms relationships that are similar to those ob-
served with GL AoA. The Bristol Norm study
(Stadthagen-Gonzalez and Davis, 2006) was per-
formed in an identical way to the GL study and
comparing the lists of acquisition ages results
in a mean absolute error of approximately 0.5
which is much lower than the .9 to 1.5 relative to
GL. The recent AoA list of Cortese and Khanna
(2008) showed an increase in correlation relative
to the GL study (r2 = 0.43) potentially reflecting
change in the acquisition process due to temporal
effects.
Residual Distribution: Predicted AoA versus Oral AoA 
 S?percentile=80
Error (Predicted AoA ? Actual AoA)
Pe
rc
en
t
0
5
10
15
20
?4 ?2 0 2 4
Figure 4: The difference distribution between
the GL and the inferred AoA from Web 1-12 is
skewed to the right as would be expected since
written AoA is higher than oral AoA. Relaxing
the definition of readability by decreasing s re-
sults in higher inferred acquisition ages. Values
of s in [0.5, 0.9] produced reasonable results, with
s = 0.65 achieving smallest mean absolute error.
Those words that have the same written and
verbal acquisition age are partially attributable to
those words learned prior to first grade. Many
words are learned between the ages of 2 and 5,
while reading materials are typically not assigned
a grade level of less than 1 or age 6. Approxi-
mately 40% of the words assigned the same grade
level by both Gilhooly and our prediction had an
AoA of 1st grade.
In some cases, the ages of acquisition obtained
from readability data is actually lower than the
ages reported in the older oral studies. This phe-
nomenon is likely caused by a combination of
a shift in educational standards, a change in so-
cial standards, or estimation errors due to sample
size and modeling assumptions. Approximately
904
30 years have passed since Gilhooly and Logie?s
study was conducted. Specifically, society has
made efforts to enhance the safety and health of
children and to increase the attention to science
education in very early grades. For example, the
word drug appeared in writing 0.94 grades ear-
lier than the age in which it was acquired orally
according to the GL study. The newer Bristol
Norm study confirms this observation as it pre-
dicts a decrease in grade level for drug of 0.88
over GL as well. A similar decrease in acqui-
sition age relative to the GL norms was noted
for many other words such as hypothesis,
conclusion, engineer, diet, exercise,
and vitamin.
3.3 Global Readability Prediction
Once acquisition age distributions are available,
whether estimated statistically from data or ob-
tained from a survey, they may be used to predict
the grade level of novel documents. Specifically,
the model predicts readability level t? for a novel
document d if it is the minimal grade for which
readability is established:
t
?
= min{t : P (d is readable at age t) ? ?(t)}
(5)
where ?(t) is a parameter describing the strictness
of the readability requirement. Note that we allow
?(t) to vary as a function of time (grade level). We
discuss the justification for this below.
A critical issue for reading difficulty predic-
tion is how to handle words that appear in a new
document that have never been seen in the train-
ing/development texts. In a statistical approach,
the solution to this smoothing problem has two
steps. First, we must decide how much total proba-
bility mass to allocate to all unknown words. Sec-
ond, we must decide how to subdivide this total
mass for individual words or classes of words us-
ing word-specific priors.
Our experience suggests that the first step of
estimating total probability mass is particularly
important: the likelihood of seeing an unknown
word increases as a function of total vocabulary
size, which is continuously growing with time.
We model this by defining the following dynamic
threshold
?(t) =
exp(at? 0.5)
1 + exp(at? 0.5)
. (6)
We learn the growth rate parameter a in (6) from
the data at the same time as we learn the read-
ability model?s quantile parameters s = 1 ? ?
1
,
r = 1 ? ?
2
. The range of the resulting ?(t) is
typically 0.5 in lower grades, increasing to 0.9 in
higher grades. We discuss fitting these parameters
and their optimal values further in Sec. 3.3.1. We
found that using any fixed ? value for all grades
was generally much less effective than a dynamic
?(t) threshold, and so we focus on the latter in our
evaluation.
For the second (word-specific) smoothing step,
we simply assign uniform probability across
grades, once the total unseen mass is determined.
More sophisticated word-specific priors incorpo-
rating word length, morphological features, se-
mantic clusters and so on are certainly possible
and an interesting direction for future work.
In the following section we conduct three exper-
iments involving readability prediction. First, we
confirm the effectiveness of the AoA-based model
compared to other predictive models. Second, we
examine how prediction effectiveness is affected
when our learned (written) acquisition ages are re-
placed with existing oral AoA norms. Third, we
examine the ability of our model to generalize to
new content by training and testing on different
(non-overlapping) corpora.
3.3.1 Effectiveness of Readability Prediction
In order to assess the effectiveness of our model
in predicting the readability grade levels of novel
documents we apply the model to two corpora.
First, we use the Web 1-12 corpus to learn opti-
mal parameter values for a , r, and s and then as-
sess prediction error using a test-training paradigm
for the proposed model, Naive Bayes, and support
vector regression. Second, the trained model is ap-
plied with to the Reader A-Z corpus and the results
are compared with alternative semantic variables.
Because corpora can vary significantly in text ho-
mogeneity, amount of noise, document size, and
other factors, training and testing across different
corpora ? rather than relying on cross-validation
with a single pooled dataset ? gives valuable in-
formation about how a prediction method might
be expected to perform on data with widely differ-
ent characteristics. This particular choice of Web
1-12 for training and ReadingA-Z for testing was
arbitrary.
To evaluate the best values for the a parameter
in (6) and s, r parameters in Definition 1 we gen-
905
Readability Level Prediction: MAE and Correlation
S?th Percentile
M
ea
n 
Ab
so
lu
te
 E
rro
r
1
2
3
0.5 0.6 0.7 0.8 0.9
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Correlation
Figure 5: Mean absolute error (MAE) and correla-
tion coefficient as functions of the quantile param-
eter s at optimal levels of a and r, averaged over
100 training/test samples. The MAE is displayed
as the solid line and is aligned with the left axis
while the correlation is displayed as a dashed line
and is aligned with the right axis. 90% bootstrap
confidence intervals are displayed.
erated 100 independent test and training samples
and computed the mean absolute prediction error
(MAE) and the correlation coefficient between the
predicted and actual levels. Figure 5 (left) shows
these two quantities: in each group of three lines,
the top and bottom lines delineate the upper and
lower 90% confidence bounds for the middle line.
Each middle line gives mean error or correlation
as a function of the quantile parameter s at opti-
mal levels of r and a, averaged over the 100 train-
ing/test samples. The optimal value of s for both
quantities is around 0.6 (0.65 for the MAE). The
optimal value for parameter a was approximately
1.55. The best MAE is 1.4 which compares favor-
ably to the 2.92 MAE obtained by always predict-
ing Grade 6 which is the optimal ?dumb? classifier
in the sense that of all constant predictors it pro-
vides the smallest expected MSE over a uniform
grade distribution as is the case with the Web1-
12 corpus. Figure 6 is a scatter plot comparing
predicted grades vs. actual grades, with a strong
correlation of 0.89.
We compared the predictions of model (3) to
two standard classifiers: naive Bayes and support
vector regression (SVR). SVR was applied twice
using different sets of features - once with the doc-
ument word frequencies and once with the esti-
Predicted v Actual Grade Level
Actual Grade Level
Pr
ed
ict
ed
 G
ra
de
 L
ev
el
2
4
6
8
10
12
2 4 6 8 10 12
Figure 6: The scatter plot demonstrates the strong
relationship between predicted and actual global
readability levels.
Prediction Rule MAE LB UB
Age of Acquisition 1.40 1.19 1.67
Naive Bayes 1.98 1.71 2.26
SVR (word frequency) 1.86 1.69 2.06
SVR (AoA percentiles) 1.36 1.22 1.58
Grade 6 2.92 - -
Figure 7: A comparison of mean absolute error
(MAE) across prediction algorithms shows the age
of acquisition model compares favorably. The
confidence bounds (LB,UB) were computed by re-
peating each model building procedure 100 times.
mated AoA percentiles for the document words.
The document word frequency vector is compa-
rable to the semantic component of the machine
learning approach used by (Heilman et al, 2008).
The 75-25 training-test model building paradigm
was used over documents from grades 1 to 12
to obtain predicted values. The MAE for these
predictors and their 90% confidence intervals are
shown in Figure 7. Predicting readability using
word frequencies had inferior performance, with
the naive Bayes model performing poorly and the
SVR and Rasch model obtaining MAE around 1.4.
In the second experiment, we compared our
model to published correlation results (Collins-
Thompson and Callan, 2005) for multiple alter-
native semantic variables using the same Reading
A-Z corpus, with the results shown in Fig. 8. De-
tails on these semantic variables, which have been
used in previous statistical learning approaches,
are available in the same study. Interestingly, the
correlation of the model was comparable to ex-
906
Correlation Correlation
GL (Web) .65 UNK .78
GL (WR) .40 Type .86
Bristol (Web) .76 MLF .49
Bristol (WR) .57 FK .30
Inferred (Web) .59 Unigram .63
Figure 8: Comparison of the correlation of AoA
and other semantic variables with grade level for
the Reading A-Z corpus, showing the AoA model
with the dynamic threshold compares well to ex-
isting methods. The competitor methods used
are from (Collins-Thompson and Callan, 2005)
and comprise the Smoothed Unigram, UNK (rel-
ative to revised Dale-Chall), TYPE (number of
unique words), MLF (mean log frequency), and
FK (Flesch-Kincaid readability).
isting variables, but did vary depending upon the
source of AoA. Note that because the Reading A-Z
texts were assigned grades by their creators using
some of the same semantic variables (e.g. Type),
it is not surprising that those variables perform es-
pecially well on this dataset.
High quality readability prediction is a worth-
while result in itself; however, we can also use the
prediction mechanism to study the validity of Def-
inition 1 and the Rasch model. We do so by apply-
ing other predictive algorithms using the inferred
acquisition age distribution for each document as
the predictor variables and comparing the MAE
with the MAE obtained by the estimated Rasch
model. In particular, we examine the performance
of support vector regression (SVR) using the esti-
mated AoA percentiles for each document as pre-
dictor variables. The results displayed in Fig-
ure 7 show that SVR and the dynamic threshold
prediction rule perform similarly well, suggesting
that Definition 1 and the Rasch model are suitable
models for readability prediction.
3.3.2 Prediction with Existing Acquisition
Age Norms
We now examine how predicting readability of
novel documents using acquisition ages obtained
in surveys perform in comparison to the ages ob-
tained from the maximum likelihood estimation.
We use the GL and Bristol age of acquisition
norms. The intersection of AoA norm data and the
Web Corpus are 1217 and 1012 words respectively
for the GL and Bristol measure; additionally, the
highest grade level associated with these word sets
S-th Dynamic
Prediction Rule Percentile Threshold
Age of Acquisition 1.69 1.40
GL Norms 1.73 1.42
Bristol Norms 1.97 1.79
Figure 9: The Gilhooly and Logie AoA norms and
the Bristol norms are independent sources for ages
of acquisition. A comparison of the prediction
quality using these norms shows two things: 1) the
definition provides comparable prediction quality
using expert norms, and 2) the dynamic threshold
?(t) improves prediction over the static threshold
(optimal s-th percentile) for the norms.
AoA Weekly
Source Web 1-12 Reader
Inferred (Weekly Reader) - .91
Inferred (Web 1-12) 1.89 -
GL 2.05 1.14
Bristol 1.57 1.34
Figure 10: The readability of WR documents was
predicted using 4 sources of AoA data. The pa-
rameters of the prediction model were fit using
only the Web data, or the WR data, or both sources
in the case of the GL and Bristol norms AoA data.
are eight and seven respectively. When applying
the prediction rule using AoA norms r is implic-
itly selected in the norming process as the result
is a single value instead of a distribution. Interest-
ingly, the optimal ranges of s-percentile, from 92
to 100, were the same for both the GL and Bristol
norms. Table 9 shows that the prediction accuracy
obtained using the GL Norms was almost identical
to that obtained with the inferred AoA, while the
Bristol Norms performed as well as some of the
competitor procedures.
3.3.3 Prediction Effectiveness across
Different Corpora
To provide additional evidence for our model?s
ability to generalize to new corpora, we exam-
ine how the learned r and s values vary when the
model is learned on one corpus and evaluated on
another, and how this affects the accuracy of the
readability prediction.
Figure 10 demonstrates the corpus used for tun-
ing the readability prediction has a large impact
on the quality of the prediction. Comparing the
MAE of the readability predictions on WR data
907
when the age of acquisition is inferred from Web
data to the MAE when the AoA is inferred from
WR data shows the error rate more than doubles
from 0.90 to 1.89. The increase in error rate also
appears when the age of acquisition for WR data
is predicted using the AoA norm data. In this case
the prediction was performed using the parameters
identified when the model was trained on Web data
and when the model was trained on WR data. In
each case a tendency to overfit appears as the MAE
increases from 1.14 to 2.05 for the GL norms and
1.34 to 1.57 for the Bristol norms. Interestingly,
the Bristol norms perform better on WR data when
fit using the Web data, while the GL norms per-
form better when fit using the WR data.
4 Related Work
Age of acquisition for word reading and under-
standing has been extensively studied as a learn-
ing factor in the psycholinguistics literature, where
AoA norms have been obtained using surveys. Ex-
amples of relevant literature are (Gilhooly and Lo-
gie, 1980; Zevin and Seidenberg, 2002). Our ap-
proach differs by connecting AoA to readability
through Definition 1 and using readability data to
estimate AoA norms from large amounts of au-
thentic language data. A related study is that by
Crossley et al (2007) who used AoA to help dis-
criminate between authentic and simplified texts
for second-language readers.
In the past decade, there has been renewed in-
terest in corpus-based statistical models for read-
ability prediction. One example is the popular
Lexile measure (Stenner, 1996) which uses word
frequency statistics from a large English corpus.
Collins-Thompson and Callan (2005) introduced a
new approach based on statistical language mod-
eling, treating a document as a mixture of lan-
guage models for individual grades. Further re-
cent refinements in methods for readability predic-
tion include using machine learning methods such
as Support Vector Machines (Schwarm and Os-
tendorf, 2005), log-linear models (Heilman et al,
2008), k-NN classifiers and combining semantic
and grammatical features (Heilman et al, 2007).
The growing number of features investigated by
these machine learning approaches reflect the fact
that reading difficulty is a complex phenomenon
involving many factors, from semantic difficulty
(vocabulary) to syntax and discourse complex-
ity, reader background, and others. While a full-
featured comparison between previous approaches
that includes AoA features would be very inter-
esting, our goal in this study was to provide a
clear analysis of the most fundamental factor of
readability, semantic difficulty, which accounts for
80-90% of the variance in readability prediction
scores (Chall and Dale, 1995). Because AoA is
a semantic, vocabulary-based representation, we
compare its effectiveness with the correspond-
ing semantic components from previous machine-
learning approaches in Sec. 3.3.1.
5 Discussion
While there have been several recent studies re-
garding word acquisition and readability our work
is the first to provide a quantitative connection be-
tween these two concepts in a statistically mean-
ingful way. The core assumption that we make
is Definition 1 which is consistent with standard
readability definitions e.g., (Chall and Dale, 1995)
and states that document readability level is deter-
mined by most people understanding most words.
The connection between word acquisition and
readability is both intuitive and useful. It allows
two degrees of freedom s = 1? ?
1
and r = 1? ?
2
to handle situations where different readability no-
tions exist. Experiments validate the model and
demonstrate interesting trends in word acquisi-
tions as compared to older oral acquisition stud-
ies. Experimental results show that the proposed
model is also effective in terms of predicting read-
ability level of documents on multiple datasets.
It compares favorably to naive Bayes and sup-
port vector regression, the latter being one of the
strongest regression baselines.
Acknowledgments
The authors thank Joshua Dillon for downloading
the weekly reader data and pre-processing it. The
work described in this paper was funded in part by
NSF grant DMS-0604486.
References
J. S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brook-
line Books, Brookline, MA.
K. Collins-Thompson and J. Callan. 2005. Predicting
reading difficulty with statistical language models.
J. of the American Soc. for Info. Science and Tech.,
56(13):598?605.
908
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
M. Cortese and M. Khanna. 2008. Age acquisition
ratings for 3000 monosyllabic words. Behavior Re-
search Methods, 40:791?794.
S. A. Crossley, P. M. McCarthy, and D. S. McNa-
mara. 2007. Discriminating between second lan-
guage learning text-types. In Proc. of the Twenti-
eth International Florida Artificial Intelligence Re-
search Society Conference.
H. A. David and H. N. Nagaraja. 2003. Order Statis-
tics. Wiley, Marblehead, MA.
E. Fry. 1990. A readability formula for short passages.
Journal of Reading.
K. J. Gilhooly and R. H. Logie. 1980. Age of acquisi-
tion, imagery, concreteness, familiarity and ambigu-
ity measures for 1944 words. Behaviour Research
Methods and Instrumentation, 12:395?427.
M. Heilman, K. Collins-Thompson, J. Callan, and
M. Eskenazi. 2007. Combining lexical and gram-
matical features to improve readability measures for
first and second language texts. In Proc. of the Hu-
man Language Technology Conference.
M. Heilman, K. Collins-Thompson, and M. Eskenazi.
2008. An analysis of statistical models and features
for reading difficulty prediction. In The 3rd Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications.
S. E. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and sta-
tistical language models. In Proc. of the Association
of Computational Linguistics.
H. Stadthagen-Gonzalez and C. J. Davis. 2006. The
bristol norms for age of acquisition, imageabil-
ity, and familiarity. Behavior Research Methods,
38:598?605.
A. J. Stenner. 1996. Measuring reading comprehen-
sion with the Lexile Framework. Metametrics, Inc.,
Durham, NC.
J. D. Zevin and M. S. Seidenberg. 2002. Age of acqui-
sition effects in word reading and other tasks. Jour-
nal of Memory and Language, 47(1):1?29.
909
Coling 2010: Poster Volume, pages 552?560,
Beijing, August 2010
Local Space-Time Smoothing for Version Controlled Documents
Seungyeon Kim
Georgia Institute of Technology
Guy Lebanon
Georgia Institute of Technology
Abstract
Unlike static documents, version con-
trolled documents are continuously edited
by one or more authors. Such collabo-
rative revision process makes traditional
modeling and visualization techniques in-
appropriate. In this paper we propose a
new representation based on local space-
time smoothing that captures important
revision patterns. We demonstrate the ap-
plicability of our framework using experi-
ments on synthetic and real-world data.
1 Introduction
Most computational linguistics studies concen-
trate on modeling or analyzing documents as se-
quences of words. In this paper we consider
modeling and visualizing version controlled doc-
uments which is the authoring process leading to
the final word sequence. In particular, we focus on
documents whose authoring process naturally seg-
ments into consecutive versions. The revisions, as
the differences between consecutive versions are
often called, may be authored by a single author
or by multiple authors working collaboratively.
One popular way to keep track of version con-
trolled documents is using a version control sys-
tem such as CVS or Subversion (SVN). This is
often the case with books or with large com-
puter code projects. In other cases, more special-
ized computational infrastructure may be avail-
able, as is the case with the authoring API of
Wikipedia.org, Slashdot.com, and Google Wave.
Accessing such API provides information about
what each revision contains, when was it sub-
mitted, and who edited it. In any case, we for-
mally consider a version controlled document as
a sequence of documents d1, . . . , dl indexed by
their revision number where di typically contains
some locally concentrated additions or deletions,
as compared to di?1.
In this paper we develop a continuous represen-
tation of version controlled documents that gener-
alizes the locally weighted bag of words represen-
tation (Lebanon et al, 2007). The representation
smooths the sequence of version controlled doc-
uments across two axes-time t and space s. The
time axis t represents the revision and the space
axis s represents document position. The smooth-
ing results in a continuous map from a space-time
domain to the simplex of term frequency vectors
? : ? ? PV where ? ? R2, and (1)
PV =
?
?
?w ? R
|V | : wi ? 0,
|V |?
i=1
wi = 1
?
?
? .
The mapping above (V is the vocabulary) cap-
tures the variation in the local distribution of word
content across time and space. Thus [?(s, t)]w is
the (smoothed) probability of observing word w
in space s (document position) and time t (ver-
sion). Geometrically, ? realizes a divergence-free
vector field (since ?w[?(s, t)]w = 1, ? has zero
divergence) over the space-time domain ?.
We consider the following four version con-
trolled document analysis tasks. The first task is
visualizing word-content changes with respect to
space (how quickly the document changes its con-
tent), time (how much does the current version
differs from the previous one), or mixed space-
time. The second task is detecting sharp transi-
tions or edges in word content. The third task
is concerned with segmenting the space-time do-
main into a finite partition reflecting word content.
The fourth task is predicting future revisions. Our
main tool in addressing tasks 1-4 above is to an-
alyze the values of the vector field ? and its first
552
order derivatives fields
?? = (??s, ??t) . (2)
2 Space-Time Smoothing for Version
Controlled Documents
With no loss of generality we identify the vocabu-
lary V with positive integers {1, . . . , V } and rep-
resent a word w ? V by a unit vector1 (all zero
except for 1 at the w-component)
e(w) = (0, . . . , 0, 1, 0, . . . , 0)> w ? V. (3)
We extend this definition to word sequences
thus representing documents ?w1, . . . , wN ? (wi ?
V ) as sequences of V -dimensional vectors
?e(w1), . . . , e(wN )?. Similarly, a version con-
trolled document is sequence of documents
d(1), . . . , d(l) of potentially different lengths
d(j) = ?w(j)1 , . . . , w
(j)
N(j)?. Using (3) we represent
a version controlled document as the array
e(w(1)1 ), . . . , e(w
(1)
N(1))
.
.
.
.
.
.
.
.
.
e(w(l)1 ), . . . , e(w
(l)
N(l))
(4)
where columns and rows correspond to space
(document position) and time (versions).
The array (4) of high dimensional vectors repre-
sents the version controlled document without any
loss of information. Nevertheless the high dimen-
sionality of V suggests we smooth the vectors in
(4) with neighboring vectors in order to better cap-
ture the local word content. Specifically we con-
volve each component of (4) with a 2-D smooth-
ing kernel Kh to obtain a smooth vector field ?
over space-time (Wand and Jones, 1995) e.g.,
?(s, t) =
?
s?
?
t?
Kh(s? s?, t? t?)e(w(t
?)
s? )
Kh(x, y) ? exp
(
?(x2 + y2)/(2h2)
)
. (5)
Thus as (s, t) vary over a continuous domain ? ?
R2, ?(s, t), which is a weighted combination of
neighboring unit vectors, traces a continuous sur-
face in PV ? RV . Assuming that the kernel
Kh is a normalized density it can be shown that
1Note the slight abuse of notation as V represents both a
set of words and an integer V = {1, . . . , V } with V = |V |.
?(s, t) is a non-negative normalized vector i.e.,
?(s, t) ? PV (see (1) for a definition of PV ) mea-
suring the local distribution of words around the
space-time location (s, t). It thus extends the con-
cept of lowbow (locally weighted bag of words)
introduced in (Lebanon et al, 2007) from single
documents to version controlled documents.
One difficulty with the above scheme is that
the document versions d1, . . . , dl may be of dif-
ferent lengths. We consider two ways to resolve
this issue. The first pads shorter document ver-
sions with zero vectors as needed. We refer to the
resulting representation ? as the non-normalized
representation. The second approach normalizes
all document versions to a common length, say?l
j=1 N(j). That is each word in the first doc-
ument is expanded into
?
j 6=1 N(j) words, each
word in the second document is expanded into?
j 6=2 N(j) words etc. We refer to the resulting
representation ? as the normalized representation.
The non-normalized representation has the ad-
vantage of conveying absolute lengths. For ex-
ample, it makes it possible to track how differ-
ent portions of the document grow or shrink (in
terms of number of words) with the version num-
ber. The normalized representation has the advan-
tage of conveying lengths relative to the document
length. For example, it makes it possible to track
how different portions of the document grow or
shrink with the version number relative to the to-
tal document length. In either case, the space-time
domain ? on which ? is defined (5) is a two di-
mensional rectangular domain ? = [0, I]? [0, J ].
Before proceeding to examine how ? may be
used in the four tasks described in Section 1 we
demonstrate our framework with a simple low di-
mensional example. Assuming a vocabulary of
two words V = {1, 2} we can visualize ? by
displaying its first component as a grayscale im-
age (since [?(s, t)]2 = 1 ? [?(s, t)]1 the sec-
ond component is redundant). Specifically, we
created a version controlled document with three
contiguous segments whose {1, 2} words were
sampled from Bernoulli distributions with param-
eters 0.3 (first segment), 0.7 (second segment),
and 0.5 (third segment). That is, the probability
of getting 1 is highest for the second segment,
equal for the third and lowest for the first seg-
ment. The initial lengths of the segments were
553
Figure 1: Four space-time representations of a simple synthetic version controlled document over V = {1, 2} (see text
for more details). The left panel displays the first component of (4) (non-smoothed array of unit vectors corresponding to
words). The second and third panels display [?(s, t)]1 for the non-normalized and normalized representations respectively.
The fourth panel displays the gradient vector field (??s(s, t), ??t(s, t)) (contour levels represent the gradient magnitude). The
black portions of the first two panels correspond to zero padding due to unequal lengths of the different versions.
30, 40 and 120 words with the first segment in-
creasing and the third segment decreasing at half
the rate of the first segment with each revision.
The length of the second segment was constant
across the different versions. Figure 1 displays
the nonsmoothed ragged array (4) (left), the non-
normalized [?(s, t)]1 (middle left) and the normal-
ized [?(s, t)]1 (middle right).
While the left panel doesn?t distinguish much
between the second and third segment the two
smoothed representations display a nice seg-
mentation of the space-time domain into three
segments, each with roughly uniform values.
The non-normalized representation (middle left)
makes it easy to see that the total length of the
version controlled document is increasing but it
is not easy to judge what happens to the relative
sizes of the three segments. The normalized rep-
resentation (middle right) makes it easy to see that
the first segment increases in size, the second is
constant, and the third decreases in size. It is also
possible to notice that the growth rate of the first
segment is higher than the decay rate of the third.
3 Visualizing Change in Space-Time
We apply the space-time representation to four
tasks. The first task, visualizing change, is de-
scribed in this section. The remaining three tasks
are described in the next three section.
The space-time domain ? represents the union
of all document versions and all document posi-
tions. Some parts of ? are more homogeneous
and some are less in terms of their local word dis-
tribution. Locations in ? where the local word
distribution substantially diverges from its neigh-
bors correspond to sharp content transitions. On
the other hand, locations whose word distribution
is more or less constant correspond to slow con-
tent variation.
We distinguish between three different types of
changes. The first occurs when the word content
changes substantially between neighboring doc-
ument positions within a certain document ver-
sion. As an example consider a document loca-
tion whose content shifts from high level introduc-
tory motivation to a detailed technical description.
Such change is represented by
???s(s, t)?2 =
V?
w=1
(?[?(s, t)]w
?s
)2
. (6)
A second type of change occurs when a certain
document position undergoes substantial change
in local word distribution across neighboring ver-
sions. An example is erroneous content in one
version being heavily revised in the next version.
Such change along the time axis corresponds to
the magnitude of
???t(s, t)?2 =
V?
w=1
(?[?(s, t)]w
?t
)2
. (7)
Expression (6) may be used to measure the in-
stantaneous rate of change in the local word dis-
tribution. Alternatively, integrating (6) provides a
global measure of change
h(s) =
?
???s(s, t)?2 dt, g(t) =
?
???t(s, t)?2 ds
with h(s) describing the total amount of spatial
change across all revisions and g(t) describing
554
Figure 2: Gradient and edges for a portion of the version controlled Wikipedia Religion article. The left panel displays
???s(s, t)?2 (amount of change across document locations for different versions). The second panel displays ???t(s, t)?2
(amount of change across versions for different document positions). The third panel displays the local maxima of
???s(s, t)?2 + ???t(s, t)?2 which correspond to potential edges, either vertical lines (section and subsection boundaries) or
horizontal lines (between substantial revisions). The fourth panel displays boundaries of sections and subsections as black
and gray lines respectively.
the total amount of version change across differ-
ent document positions. h(s) may be used to de-
tect document regions undergoing repeated sub-
stantial content revisions and g(t) may be used to
detect revisions in which substantial content has
been modified across the entire document.
We conclude with the integrated directional
derivative
? 1
0
???s(r)??s(?(r)) + ??t(r)??t(?(r))?2 dr (8)
where ? : [0, 1] ? ? is a parameterized curve in
the space-time and ?? its tangent vector. Expres-
sion (8) may be used to measure change along a
dynamically moving document anchor such as the
boundary between two book chapters. The space
coordinate of such anchor shifts with the version
number (due to the addition and removal of con-
tent across versions) and so integrating the gra-
dient across one of the two axis as in (7) is not
appropriate. Defining ?(r) to be a parameterized
curve in space-time realizing the anchor positions
(s, t) ? ? across multiple revisions, (8) measures
the amount of change at the anchor point.
3.1 Experiments
The right panel of Figure 1 shows the gradient
vector field corresponding to the synthetic ver-
sion controlled document described in the previ-
ous section. As expected, it tends to be orthog-
onal to the segment boundaries. Its magnitude is
displayed by the contour lines which show highest
magnitudes around segment boundaries.
Figure 2 shows the norm ???s(s, t)?2 (left),
???t(s, t)?2 (middle left) and the local maxima
of ???s(s, t)?2 + ???t(s, t)?2 (middle right) for a
portion of the version controlled Wikipedia Re-
ligion article. The first panel shows the amount
of change in local word distribution within doc-
uments. High values correspond to boundaries
between sections, topics or other document seg-
ments. The second panel shows the amount of
change as one version is replaced with another.
It shows which revisions change the word distri-
butions substantially and which result in a rela-
tively minor change. The third panel shows only
the local maxima which correspond to edges be-
tween topics or segments (vertical lines) or revi-
sions (horizontal lines).
4 Edge Detection
In many cases documents may be divided to
semantically coherent segments. Examples of
text segments include individual news stories in
streaming broadcast news transcription, sections
in article or books, and individual messages in a
discussion board or an email trail. For non-version
controlled documents finding the text segments is
equivalent to finding the boundaries or edges be-
tween consecutive segments. See (Hearst, 1997;
Beeferman et al, 1999; McCallum et al, 2000)
for several recent studies in this area.
Things get a bit more complicated in the case
of version controlled documents. Segments, and
their boundaries exist in each version. As in
case of image processing, we may view segment
boundaries as edges in the space-time domain
?. These boundaries separate the segments from
each other, much like borders separate countries
555
Figure 3: Gradient and edges of a portion of the version controlled Atlanta Wikipedia article (top row) and the Google
Wave Amazon Kindle FAQ (bottom row). The left column displays the magnitude of the gradient in both space and time
???s(s, t)?2 + ???t(s, t)?. The middle column displays the local maxima of the gradient magnitude (left column). The
right column displays the actual segment boundaries as vertical lines (section headings for Wikipedia and author change in
Google Wave). The gradient maxima corresponding to vertical lines in the middle column matches nicely the Wikipedia
section boundaries. The gradient maxima corresponding to horizontal lines in the middle column correspond nicely to major
revisions indicated by a discontinuities in the location of the section boundaries.
in a two dimensional geographical map.
Assuming all edges are correctly identified, we
can easily identify the segments as the interior
points of the closed boundaries. In general, how-
ever, attempts to identify segment boundaries or
edges will only be partially successful. As a result
predicted edges in practice are not closed and do
not lead to interior segments. We consider now the
task of predicting segment boundaries or edges in
? and postpone the task of predicting a segmenta-
tion to the next section.
Edges, or transitions between segments, corre-
spond to abrupt changes in the local word dis-
tribution. We thus characterize them as points
in ? having high gradient value. In particu-
lar, we distinguish between vertical edges (transi-
tions across document positions), horizontal edges
(transitions across versions), and diagonal edges
(transitions across both document position and
version). These three types of edges may be di-
agnosed based on the magnitudes of ??s, ??t, and
??1?s + ??2?t respectively.
4.1 Experiments
Besides the synthetic data results in Figure 2,
we conducted edge detection experiments on six
different real world datasets. Five datasets are
Wikipedia.com articles: Atlanta, Religion, Lan-
guage, European Union, and Beijing. Religion
and European Union are version controlled docu-
ments with relatively frequent updates, while At-
lanta, language, and Beijing have less frequent
changes. The sixth dataset is the Google Wave
Amazon Kindle FAQ which is a less structured
version controlled document.
Preprocessing included removing html tags and
pictures, word stemming, stop-word removal, and
removing any non alphabetic characters (numbers
and punctuations). The section heading informa-
tion of Wikipedia and the information of author
of each posting in Google Wave is used as ground
truth for segment boundaries. This information
was separated from the dataset and was used for
training and evaluation (on testing set).
Figure 3 displays a gradient information, local
maxima, and ground truth segment boundaries for
556
Article Rev. Voc. p(y) Error Rate F1 Measure
Size a b c a b c
Atlanta 2000 3078 0.401 0.401 0.424 0.339 0.000 0.467 0.504
Religion 2000 2880 0.403 0.404 0.432 0.357 0.000 0.470 0.552
Language 2000 3727 0.292 0.292 0.450 0.298 0.000 0.379 0.091
European Union 2000 2382 0.534 0.467 0.544 0.435 0.696 0.397 0.663
Beijing 2000 3857 0.543 0.456 0.474 0.391 0.704 0.512 0.682
Amazon Kindle FAQ 100 573 0.339 0.338 0.522 0.313 0.000 0.436 0.558
Figure 4: Test set error rate and F1 measure for edge prediction (section boundaries in Wikipedia articles and author
change in Google Wave). The space-time domain ? was divided to a grid with each cell labeled edge (y = 1) or no edge
(y = 0) depending on whether it contained any edges. Method a corresponds to a predictor that always selects the majority
class. Method b corresponds to the TextTiling test segmentation algorithm (Hearst, 1997) without paragraph boundaries
information. Method c corresponds to a logistic regression classifier whose feature set is composed of statistical summaries
(mean, median, max, min) of ??s(s, t) within the grid cell in question as well as neighboring cells.
the version controlled Wikipedia articles Religion
and Atlanta. The local gradient maxima nicely
match the segment boundaries which lead us to
consider training a logistic regression classifier on
a feature set composed of gradient value statis-
tics (min, max, mean, median of ???s(s, t)? in the
appropriate location as well as its neighbors (the
space-time domain?was divided into a finite grid
where each cell either contained an edge (y = 1)
or did not (y = 0)). The table in Figure 4 displays
the test set accuracy and F1 measure of three pre-
dictors: our logistic regression (method c) as well
as two baselines: predicting edge/no-edge based
on the marginal p(y) distribution (method a) and
TextTiling (method b) (Hearst, 1997) which is a
popular text segmentation algorithm. Since we do
not assume paragraph information in our experi-
ment we ignored this component and considered
the document as a sequence with w = 20 and
29 minimum depth gaps parameters (see (Hearst,
1997)). We conclude from the figure that the gra-
dient information leads to better prediction than
TextTiling (on both accuracy and F1 measure).
5 Segmentation
As mentioned in the previous section, predicting
edges may not result in closed boundaries. It is
possible to analyze the location and direction of
the predicted edges and aggregate them into a se-
quence of closed boundaries surrounding the seg-
ments. We take a different approach and partition
points in ? to k distinct values or segments based
on local word content and space-time proximity.
For two points (s1, t2), (s2, t2) ? ? to be in the
same segment we expect ?(s1, t1) to be similar to
?(s2, t2) and for (s1, t1) to be close to (s2, t2).
The first condition asserts that the two locations
discuss the same topic. The second condition as-
serts that the two locations are not too far from
each other in the space time domain. More specif-
ically, we propose to segment ? by clustering its
points based on the following geometry
d((s1, t1), (s2, t2)) = dH(?(s1, t1), ?(s2, t2))
+
?
c1(s1 ? s2)2 + c2(t1 ? t2)2 (9)
where dH : PV ? PV ? R is Hellinger distance
d2H(u, v) =
V?
i=1
(?ui ?
?vi)2. (10)
The weights c1, c2 are used to balance the contri-
butions of word content similarity with the simi-
larity in time and space.
5.1 Experiments
Figure 5 displays the ground truth segment bound-
aries and the segmentation results obtained by ap-
plying k-means clustering (k = 11) to the metric
(9). The figure shows that the predicted segments
largely match actual edges in the documents even
though no edge or gradient information was used
in the segmentation process.
6 Predicting Future Operations
The fourth and final task is predicting a future
revision dl+1 based on the smoothed representa-
tion of the present and past versions d1, . . . , dl. In
557
Figure 5: Predicted segmentation (top) and ground truth segment boundaries (bottom) of portions of the version controlled
Wikipedia articles Religion (left), Atlanta (middle) and the Google Wave Amazon Kindle FAQ(right). The predicted segments
match the ground truth segment boundaries. Note that the first 100 revisions are used in Google Wave result. The proportion
of the segments that appeared in the beginning is keep decreasing while the revisions increases and new segments appears.
terms of ?, this means predicting features associ-
ated with ?(s, t), t ? t? based on ?(s, t), t < t?.
6.1 Experiments
We concentrate on predicting whether Wikipedia
edits are reversed in the next revision. This ac-
tion, marked by a label UNDO or REVERT in the
Wikipedia API, is important for preventing con-
tent abuse or removing immature content (by pre-
dicting ahead of time suspicious revisions).
We predict whether a version will undergo
UNDO in the next version using a support vec-
tor machine based on statistical summaries (mean,
median, min, max) of the following feature
set ???s(s, t)?, ???s(s, t)?, ???t(s, t)?), ???t(s, t)?,
g(h), and h(s). Figure 6 shows the test set er-
ror and F1 measure for the logistic regression
based on the smoothed space-time representation
(method c), as well as two baselines. The first
baseline (method a) predicts the majority class
and the second baseline (method b) is a logistic
regression based on the term frequency content of
the current test version. Using the derivatives of
?, we obtain a prediction that is better than choos-
ing majority class or logistic regression based on
word content. We thus conclude that the deriva-
tives above provide more useful information (re-
sulting in lower error and higher F1) for predicting
future operations than word content features.
7 Related Work
While document analysis is a very active research
area, there has been relatively little work on ex-
amining version controlled documents. Our ap-
proach is the first to consider version controlled
documents as continuous mappings from a space-
time domain to the space of local word distribu-
tions. It extends the ideas in (Lebanon et al, 2007)
of using kernel smoothing to create a continuous
representation of documents. In fact, our frame-
work generalizes (Lebanon et al, 2007) as it re-
verts to it in the case of a single revision.
Other approaches to sequential analysis of doc-
uments concentrate on discrete spaces and dis-
crete models, with the possible extension of
(Wang et al, 2009). Related papers on segmenta-
tion and sequential document analysis are (Hearst,
558
Article Rev. Voc. p(y) Error Rate F1 Measure
Size a b c a b c
Atlanta 2000 3078 0.218 0.219 0.313 0.212 0.000 0.320 0.477
Religion 2000 2880 0.123 0.122 0.223 0.125 0.000 0.294 0.281
Language 2000 3727 0.189 0.189 0.259 0.187 0.000 0.334 0.455
European Union 2000 2382 0.213 0.208 0.331 0.209 0.000 0.275 0.410
Beijing 2000 3857 0.137 0.137 0.219 0.136 0.000 0.247 0.284
Figure 6: Error rate and F1 measure over held out test set of predicting future UNDO operation in Wikipedia articles.
Method a corresponds to a predictor that always selects the majority class. Method b corresponds to a logistic regression
based on the term frequency vector of the current version. Method c corresponds a logistic regression that uses summaries
(mean, median, max, min) of ???s(s, t)?, ???s(s, t)?, g(t), and h(s).
1997; Beeferman et al, 1999; McCallum et al,
2000) with (Hearst, 1997) being the closest in
spirit to our approach. An influential model for
topic modeling within and across documents is la-
tent Dirichlet alocation (Blei et al, 2003; Blei
and Lafferty, 2006). Our approach differs in be-
ing fully non-parametric and in that it does not
require iterative parametric estimation or integra-
tion. The interpretation of local word smoothing
as a non-parametric statistical estimator (Lebanon
et al, 2007) may be extended to our paper in a
straightforward manner.
Several attempts have been made to visualize
themes and topics in documents, either by keep-
ing track of the word distribution or by dimen-
sionality reduction techniques e.g., (Fortuna et al,
2005; Havre et al, 2002; Spoerri, 1993; Thomas
and Cook, 2005). Such studies tend to visualize a
corpus of unrelated documents as opposed to or-
dered collections of revisions which we explore.
8 Summary and Discussion
The task of analyzing and visualizing version con-
trolled document is an important one. It allows
external control and monitoring of collaboratively
authored resources such as Wikipedia, Google
Wave, and CVS or SVN documents. Our frame-
work is the first to develop analysis and visualiza-
tion tools in this setting. It presents a new rep-
resentation for version controlled documents that
uses local smoothing to map a space-time domain
? to the simplex of tf vectors PV . We demon-
strate the applicability of the representation for
four tasks: visualizing change, predicting edges,
segmentation, and predicting future revision oper-
ations.
Visualizing changes may highlight significant
structural changes for the benefit of users and help
the collaborative authoring process. Improved
edge prediction and text segmentation may assist
in discovering structural or semantic changes and
their evolution with the authoring process. Pre-
dicting future operation may assist authors as well
as prevent abuse in coauthoring projects such as
Wikipedia.
The experiments described in this paper were
conducted on synthetic, Wikipedia and Google
Wave articles. They show that the proposed for-
malism achieves good performance both qualita-
tively and quantitatively as compared to standard
baseline algorithms.
It is intriguing to consider the similarity be-
tween our representation and image processing.
Predicting segment boundaries are similar to edge
detection in images. Segmenting version con-
trolled documents may be reduced to image seg-
mentation. Predicting future operations is similar
to completing image parts based on the remain-
ing pixels and a statistical model. Due to its long
and successful history, image processing is a good
candidate for providing useful tools for version
controlled document analysis. Our framework fa-
cilitates this analogy and we believe is likely to re-
sult in novel models and analysis tools inspired by
current image processing paradigms. A few po-
tential examples are wavelet filtering, image com-
pression, and statistical models such as Markov
random fields.
Acknowledgements
The research described in this paper was funded
in part by NSF grant IIS-0746853.
559
References
Beeferman, D., A. Berger, and J. D. Lafferty. 1999.
Statistical models for text segmentation. Machine
Learning, 34(1-3):177?210.
Blei, D. and J. Lafferty. 2006. Dynamic topic models.
In Proc. of the International Conference onMachine
Learning.
Blei, D., A. Ng, , and M. Jordan. 2003. Latent dirich-
let alocation. Journal of Machine Learning Re-
search, 3:993?1022.
Fortuna, B., M. Grobelnik, and D. Mladenic. 2005.
Visualization of text document corpus. Informatica,
29:497?502.
Havre, S., E. Hetzler, P. Whitney, and L. Nowell. 2002.
Themeriver: Visualizing thematic changes in large
document collections. IEEE Transactions on Visu-
alization and Computer Graphics, 8(1).
Hearst, M. A. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Lebanon, G., Y. Mao, and J. Dillon. 2007. The lo-
cally weighted bag of words framework for doc-
uments. Journal of Machine Learning Research,
8:2405?2441, October.
McCallum, A., D. Freitag, and F. Pereira. 2000. Max-
imum entropy Markov models for information ex-
traction and segmentation. In Proc. of the Interna-
tional Conference on Machine Learning.
Spoerri, A. 1993. InfoCrystal: A visual tool for infor-
mation retrieval. In Proc. of IEEE Visualization.
Thomas, J. J. and K. A. Cook, editors. 2005. Illu-
minating the Path: The Research and Development
Agenda for Visual Analytics. IEEE Computer Soci-
ety.
Wand, M. P. and M. C. Jones. 1995. Kernel Smooth-
ing. Chapman and Hall/CRC.
Wang, C., D. Blei, and D. Heckerman. 2009. Continu-
ous time dynamic topic models. In Proc. of Uncer-
tainty in Artificial Intelligence.
560
Coling 2010: Poster Volume, pages 801?809,
Beijing, August 2010
Dimensionality Reduction for Text using Domain Knowledge
Yi Mao and Krishnakumar Balasubramanian and Guy Lebanon
Georgia Institute of Technology
Abstract
Text documents are complex high dimen-
sional objects. To effectively visualize
such data it is important to reduce its di-
mensionality and visualize the low dimen-
sional embedding as a 2-D or 3-D scatter
plot. In this paper we explore dimension-
ality reduction methods that draw upon
domain knowledge in order to achieve a
better low dimensional embedding and vi-
sualization of documents. We consider
the use of geometries specified manually
by an expert, geometries derived automat-
ically from corpus statistics, and geome-
tries computed from linguistic resources.
1 Introduction
Visual document analysis systems such as IN-
SPIRE have demonstrated their applicability in
managing large text corpora, identifying topics
within a document and quickly identifying a set
of relevant documents by visual exploration. The
success of such systems depends on several fac-
tors with the most important one being the qual-
ity of the dimensionality reduction. This is ob-
vious as visual exploration can be made possible
only when the dimensionality reduction preserves
the structure of the original space, i.e., documents
that convey similar topics are mapped to nearby
regions in the low dimensional 2D or 3D space.
Standard dimensionality reduction methods
such as principal component analysis (PCA), lo-
cally linear embedding (LLE) (Roweis and Saul,
2000), or t-distributed stochastic neighbor embed-
ding (t-SNE) (van der Maaten and Hinton, 2008)
take as input a set of feature vectors such as bag
of words. An obvious drawback is that such meth-
ods ignore the textual nature of documents and in-
stead consider the vocabulary words v1, . . . , vn as
abstract orthogonal dimensions.
In this paper we introduce a framework for in-
corporating domain knowledge into dimensional-
ity reduction for text documents. Our technique
does not require any labeled data, therefore is
completely unsupervised. In addition, it applies
to a wide variety of domain knowledge.
We focus on the following type of non-
Euclidean geometry where the distance between
document x and y is defined as
dT (x, y) =
?
(x? y)>T (x? y). (1)
Here T ? Rn?n is a symmetric positive semidef-
inite matrix, and we assume that documents x, y
are represented as term-frequency (tf) column
vectors. Since T can always be written as H>H
for some matrix H ? Rn?n, an equivalent but
sometimes more intuitive interpretation of (1) is
to compose the mapping x 7? Hx with the Eu-
clidean geometry
dT (x, y) = dI(Hx,Hy) = ?Hx?Hy?2. (2)
We can view T as encoding the semantic similar-
ity between pairs of words and H as smoothing
the tf vector by mapping observed words to re-
lated but unobserved words. Therefore, the geom-
etry realized by (1) or (2) may be used to derive
novel dimensionality reduction methods that are
customized to text in general and to specific text
domains in particular. The main challenge is to
obtain the matrices H or T that describe the rela-
tionship among vocabulary words appropriately.
We consider three general ways of obtaining
H or T using domain knowledge. The first cor-
responds to manually specifying H or T based
on the semantic relationship among words (de-
termined by domain expert). The second corre-
sponds to constructing H or T by analyzing re-
lationships between different words using corpus
statistics. The third is based on knowledge ob-
tained from linguistic resources. Whether to spec-
ify H directly or indirectly by specifying T =
801
H>H depends on the knowledge type and is dis-
cussed in detail in Section 4.
We investigate the performance of the proposed
dimensionality reduction methods for three text
domains: sentiment visualization for movie re-
views, topic visualization for newsgroup discus-
sion articles, and visual exploration of ACL pa-
pers. In each of these domains we evaluate the
dimensionality reduction using several different
quantitative measures. All the techniques men-
tioned in this paper are unsupervised, making use
of labels only for evaluation purposes.
Our take home message is that all three ap-
proaches mentioned above improves dimension-
ality reduction for text upon standard embedding
(H = I). Furthermore, geometries obtained
from corpus statistics are superior to manually
constructed geometries and to geometries derived
from standard linguistic resources such as Word-
Net. Combining heterogenous types of knowl-
edge provides the best results.
2 Related Work
Despite having a long history, dimensionality re-
duction is still an active research area. Broadly
speaking, dimensionality reduction methods may
be classified as projective or manifold based
(Burges, 2009). The first projects data onto a
linear subspace (e.g., PCA and canonical corre-
lation analysis) while the second traces a low di-
mensional nonlinear manifold on which data lies
(e.g., multidimensional scaling, isomap, Lapla-
cian eigenmaps, LLE and t-SNE). The use of di-
mensionality reduction for text documents is sur-
veyed by Thomas and Cook (2005) who also de-
scribe current homeland security applications.
Dimensionality reduction is closely related to
metric learning. Xing et al (2003) is one of the
earliest papers that focus on learning metrics of
the form (1). In particular they try to learn ma-
trix T in an supervised way by expressing rela-
tionships between pairs of samples. A representa-
tive paper on unsupervised metric learning for text
documents is Lebanon (2006) which learns a met-
ric on the simplex based on the geometric volume
of the data.
We focus in this paper on visualizing a cor-
pus of text documents using a 2-D scatter plot.
While this is perhaps the most popular and prac-
tical text visualization technique, other methods
such as Spoerri (1993), Hearst (1997), Havre et
al. (2002), Paley (2002), Blei et al (2003), Mao
et al (2007) exist. Techniques developed in this
paper may be ported to enhance these alternative
visualization methods as well.
3 Non-Euclidean Geometries
Dimensionality reduction methods often assume,
either explicitly or implicitly, Euclidean geome-
try. For example, PCA minimizes the reconstruc-
tion error for a family of Euclidean projections.
LLE uses the Euclidean geometry as a local met-
ric. t-SNE is based on a neighborhood structure,
determined again by the Euclidean geometry. The
generic nature of the Euclidean geometry makes
it somewhat unsuitable for visualizing text docu-
ments as the relationship between words conflicts
with Euclidean orthogonality. We consider in this
paper several alternative geometries of the form
(1) or (2) which are more suited for text and com-
pare their effectiveness in visualizing documents.
As mentioned in Section 1, H smooths the tf
vector x by mapping the observed words into ob-
served and non-observed (but related) words. In
case H is nonnegative, it can be further decom-
posed into a product of a non-negative column
normalized matrix R ? Rn?n and a non-negative
diagonal matrix D ? Rn?n. The decomposition
H = RD shows that H has two key roles. It
smooths related vocabulary words (realized by R)
and it emphasizes some words over others (real-
ized by D). Setting Rij to a high value if wi, wj
are similar and 0 if they are unrelated maps an
observed word to a probability vector over re-
lated words in the vocabulary. The value Dii cap-
tures the importance of vi and therefore should be
higher for important content words than for less
important words or stop-words1.
It is instructive to examine the matrices R and
D in the case where the vocabulary words clus-
ter in some meaningful way. Figure 1 gives
an example where vocabulary words form two
clusters. The matrix R may become block-
diagonal with non-zero elements occupying di-
agonal blocks representing within-cluster word
1The nonnegativity assumption of H is useful when con-
structing H by domain experts such as the method A in Sec-
tion 4. In general, H needs not to be nonnegative for dimen-
sionality reduction as in (2).
802
?
?????
0.8 0.1 0.1 0 0
0.1 0.8 0.1 0 0
0.1 0.1 0.8 0 0
0 0 0 0.9 0.1
0 0 0 0.1 0.9
?
?????
?
?????
5 0 0 0 0
0 5 0 0 0
0 0 5 0 0
0 0 0 3 0
0 0 0 0 3
?
?????
Figure 1: An example of a decomposition H = RD in
the case of two word clusters {v1, v2, v3}, {v4, v5}. The
block diagonal elements in R represent the fact that words
are mostly mapped to themselves, but sometimes are mapped
to other words in the same cluster. The diagonal matrix indi-
cates that the first cluster is more important than the second
cluster for the purposes of dimensionality reduction.
blending, i.e., words within each cluster are in-
terchangeable to some degree. The diagonal ma-
trix D represents the importance of different clus-
ters. The word clusters are formed with respect
to the visualization task at hand. For example,
in the case of visualizing the sentiment content
of reviews we may have word clusters labeled as
?positive sentiment words?, ?negative sentiment
words? and ?objective words?.
In general, the matrices R,D may be defined
based on the language or may be specific to docu-
ment domain and visualization purpose. It is rea-
sonable to expect that the words emphasized for
visualizing topics in news stories might be dif-
ferent than the words emphasized for visualizing
writing styles or sentiment content.
Applying the geometry (1) or (2) to dimen-
sionality reduction is easily accomplished by first
mapping document tf vectors x 7? Hx and pro-
ceeding with standard dimensionality reduction
techniques such as PCA or t-SNE. The resulting
dimensionality reduction is Euclidean in the trans-
formed space but non-Euclidean in the original
space. In many cases, the vocabulary contains
tens of thousands of words or more making the
specification of T or H a complicated and error
prone task. We describe in the next section several
techniques for specifying these matrices in prac-
tice.
4 Domain Knowledge
Method A: Manual Specification
In this method, a domain expert manually spec-
ifies H = RD by specifying (R,D) based on
the perceived relationship among the vocabulary
words. More specifically, the user first constructs
a hierarchical word clustering that may depend on
the current text domain, and then specifies the ma-
trices (R,D) based on the clustering.
Denoting the clusters byC1, . . . , Cr (a partition
of {v1, . . . , vn}), R is set to
Rij ?
{
?a, i = j, vi ? Ca
?ab, i 6= j, vi ? Ca, vj ? Cb
.
The values ?ab, a 6= b capture the semantic simi-
larity between two clusters and the value ?aa cap-
tures the similarity of two different words within
the cluster a. These values may be set manu-
ally by domain expert or automatically computed
based on the clustering hierarchy (for example ?ab
can be the inverse of the minimal number of tree
edges traversed in moving from a to b). To main-
tain a probabilistic interpretation, the matrix R
should be normalized so that its columns sum to
1. The diagonal matrix D is specified by setting
the values
Dii = da, vi ? Ca
according to the importance of word cluster Ca to
the current visualization task.
We emphasize that as with the rest of the meth-
ods in this paper, the manual specification is done
without access to labeled data. Since manual clus-
tering assumes some form of human intervention,
it is reasonable to also consider cases where the
user specifiesH or T in an interactive manner. For
example, the expert specifies an initial clustering
of words and values for (R,D), views the result-
ing embeddings and adjusts the selection interac-
tively until reaching a satisfactory embedding.
Method B: Contextual Diffusion
An alternative to manually specifying T =
DR>RD is to construct it based on similarity be-
tween the contextual distributions of the vocabu-
lary words. The contextual distribution of word v
is defined as
qv(w) = p(w appears in x|v appears in x) (3)
where x is a randomly drawn document. In other
words qv is the distribution governing the words
appearing in the context of word v.
803
A natural similarity measure between distribu-
tions is the Fisher diffusion kernel proposed by
Lafferty and Lebanon (2005). Applied to contex-
tual distributions as in Dillon et al (2007) we ar-
rive at the following similarity matrix
T (u, v) = exp
(
?c arccos2
(?
w
?
qu(w)qv(w)
))
.
where c > 0. Intuitively, the word u will be dif-
fused into v depending on the geometric diffusion
between the distributions of likely contexts.
We use the following formula to estimate the
contextual distribution from a corpus
qv(w) =
?
x?
p(w, x?|v) =
?
x?
p(w|x?, v)p(x?|v)
=
?
x?
tf(w, x?) tf(v, x
?)?
x?? tf(v, x??)
(4)
=
( 1?
x? tf(v, x?)
)(?
x?
tf(w, x?)tf(v, x?)
)
where tf(w, x) is the number of times word w ap-
pears in document x divided by the length of the
document x. The contextual distribution qv or dif-
fusion matrix T above may be computed in an un-
supervised manner without labels.
Method C: Web n-Grams
In method B the contextual distribution is com-
puted using a large external corpus that is similar
to the text being analyzed. An alternative that is
especially useful when such a corpus is not eas-
ily available is to use generic resources to esti-
mate the contextual distribution (3)-(4). One op-
tion is to use the publicly available Google n-gram
dataset (Brants and Franz, 2006) to estimate T .
More specifically, we compute the contextual dis-
tribution by considering the proportion of times
two words appear together within the n-grams
e.g., for n = 2 we have
qv(w) =
# of bigrams containing both w and v
# of bigrams containing v .
Method D: Word-Net
In the last method, we consider using Word-Net,
a standard linguistic resource, to specify T . This
Vocabulary
Sports
Others
Canoeing
catch
boxing
innings
soccer
Team
Name
Places
EU Asia
Mid east
US
Arizona
francisco
carolina
atlanta
austin
Others
Figure 2: Manually specified hierarchical word clustering
for the 20 newsgroup domain. The words in the frames are
examples of words belonging to several bottom level clusters.
is similar to manual specification (method A) in
that it builds upon experts? knowledge rather than
corpus statistics. In contrast to method A, how-
ever, Word-Net is a carefully built resource con-
taining more accurate and comprehensive linguis-
tic information such as synonyms, hyponyms and
holonyms. On the other hand, its generality puts
it at a disadvantage as method A may be adapted
to a specific text domain.
We follow Budanitsky and Hirst (2001) who
compared five similarity measures between words
based on Word-Net. In our experiments we use
the measure of Jiang and Conrath (1997) (see also
Jurafsky and Martin (2008))
T (u, v) = log p(u)p(v)2p(lcs(u, v))
as it was shown to outperform the others. Above,
lcs stands for the lowest common subsumer, i.e.,
the lowest node in the hierarchy that subsumes (is
a hypernym of) both u and v. The quantity p(u)
is the probability that a randomly selected word
in a corpus is an instance of the synonym set that
contains word u.
Combination of Methods
In addition to individual methods we also consider
their convex combinations
H? =
?
i
?iHi s.t. ?i ? 0,
?
i
?i = 1 (5)
where Hi are matrices from methods A-D (ob-
tained implicitly by specifying R and D for
method A and T for methods B-D). Doing so al-
lows us to combine heterogeneous types of do-
main knowledge including experts? knowledge
804
and corpus statistics, leverage their diverse nature
and potentially achieve better performance than
any of the methods on its own.
5 Experiments
We evaluate the proposed methods by experiment-
ing on two text datasets where domain knowledge
is relatively easy to obtain (especially for method
A and B). Preprocessing includes lower-casing,
stop words removal, stemming, and selecting the
most frequent 2000 words for both datasets.
The first is the Cornell sentiment scale dataset
of movie reviews from 4 critics (Pang and Lee,
2004). The visualization in this case focuses on
the sentiment quantity of either 1 (very bad) or 4
(very good) (Pang et al, 2002). For method A,
we use the General Inquirer resource2 to partition
the vocabulary into three clusters conveying pos-
itive, negative or neutral sentiment. While visu-
alizing documents from one particular author, the
rest of the reviews from other three authors can be
used as an estimate of contextual distribution for
method B.
The second text dataset is the 20 newsgroups.
It consists of newsgroup articles from 20 distinct
newsgroups and is meant to demonstrate topic vi-
sualization. In this case one of the authors de-
signed a hierarchical clustering of the vocabulary
words based on general knowledge of English lan-
guage (see Figure 2 for a partial clustering hier-
archy) without access to labels. The contextual
distribution for method B is estimated from the
Reuters RCV1 dataset (Lewis et al, 2004) which
consists of news articles from Reuters.com in the
year 1996 and 1997.
Method C uses Google n-gram which provides
a massive scale resource for estimating the con-
textual distribution. In the case of Word-Net
(method D) we used Pedersen?s implementation
of Jiang and Conrath?s similarity measure3. Note,
for these two methods, the obtained information
is not domain specific but rather represents gen-
eral semantic relationships between words.
In our experiments belowwe focused on two di-
mensionality reduction methods: PCA and t-SNE.
PCA is a well known classical method while t-
SNE (van der Maaten and Hinton, 2008) is a re-
2http://www.wjh.harvard.edu/?inquirer/
3http://wn-similarity.sourceforge.net/
cent dimensionality reduction technique for visu-
alization purposes. The use of t-SNE is motivated
by the fact that it was shown to outperform LLE,
CCA, MVU, Isomap, and Laplacian eigenmaps
when the dimensionality of the data is reduced to
two or three.
To measure the dimensionality reduction qual-
ity, we visualize the data as a scatter plot with dif-
ferent data groups (topics, sentiments) displayed
with different markers and colors. Our quantita-
tive evaluation of the visualization is based on the
fact that documents belonging to different groups
(topics, sentiments) should be spatially separated
in the 2-D space. Specifically, we used the follow-
ing indices:
(i) The weighted intra-inter criteria is a standard
clustering quality index that is invariant to
non-singular linear transformations of the
embedded data. It equals tr(S?1T SW ) where
SW is the within-cluster scatter matrix, ST =
SW + SB is the total scatter matrix, and SB
is the between-cluster scatter matrix (Duda et
al., 2001).
(ii) The Davies Bouldin index is an alternative
to (i) that is similarly based on the ratio
of within-cluster scatter to between-cluster
scatter (Davies and Bouldin, 2000).
(iii) Classification error rate of a k-NN classifier
that applies to data groups in the 2-D em-
bedded space. Despite the fact that we are
not interested in classification per se (other-
wise we would classify in the original high
dimensional space), it is an intuitive and in-
terpretable measure of cluster separation.
(iv) An alternative to (iii) is to project the em-
bedded data onto a line which is the direc-
tion returned by applying Fisher?s linear dis-
criminant analysis to the embedded data. The
projected data from each group is fitted to a
Gaussian whose separation is used as a proxy
for visualization quality. In particular, we
summarize the separation of the two Gaus-
sians by measuring the overlap area. While
(iii) corresponds to the performance of a k-
NN classifier, method (iv) corresponds to the
performance of Fisher?s LDA classifier.
Labeled data is not used during the dimensionality
reduction stage but it is used in each of the above
measures for evaluation purposes.
805
Figure 3 displays both qualitative and quanti-
tative evaluation of PCA and t-SNE for the senti-
ment and newsgroup domains forH = I (left col-
umn), manual specification (middle column) and
contextual distribution (right column). In general
for both domains, methods A and B perform bet-
ter both qualitatively and quantitatively (indicat-
ing by the numbers in the top two rows) than the
original dimensionality reduction with method B
outperforming method A.
Tables 1-2 compare evaluation measures (i)
and (iii) for different types of domain knowl-
edge. Table 1 corresponds to the sentiment do-
main where we conducted separate experiments
for four movie critics. Table 2 corresponds to
the newsgroup domain where two tasks were
considered. The first involves three newsgroups
(comp.sys.mac.hardware vs. rec.sports.hockey
vs. talk.politics.mideast) and the second involves
four newsgroups (rec.autos vs. rec.motorcycles
vs. rec.sports.baseball vs. rec.sports.hockey). It is
clear from these two tables that the contextual dif-
fusion, Google n-gram, and Word-Net generally
outperform the original H = I matrix. The best
method varies from task to task but the contextual
diffusion and Google n-gram in general result in
good performance.
PCA (1) PCA (2) t-SNE (1) t-SNE (2)
H = I 1.5391 1.4085 1.1649 1.1206
B 1.2570 1.3036 1.2182 1.2331
C 1.2023 1.3407 0.7844 1.0723
D 1.4475 1.3352 1.1762 1.1362
PCA (1) PCA (2) t-SNE (1) t-SNE (2)
H = I 0.8461 0.5630 0.9056 0.7281
B 0.7381 0.6815 0.9110 0.6724
C 0.8420 0.5898 0.9323 0.7359
D 0.8532 0.5868 0.9013 0.7728
Table 2: Quantitative evaluation of dimensionality reduc-
tion for visualization for two tasks in the news article domain.
The numbers in the top five rows correspond to measure (i)
(lower is better), and the numbers in the bottom five rows
correspond to measure (iii) (k = 5) (higher is better). We
conclude that contextual diffusion (B), Google n-gram (C),
and Word-Net (D) tend to outperform the original H = I .
We also examined convex combinations
?1HA + ?2HB + ?3HC + ?4HD (6)
with
??i = 1 and ?i ? 0. Table 3 displays
quantitative results using evaluation measures (i),
(ii) and (iii) where k is chosen to be 5 for (iii).
The first four rows correspond to method A, B, C
(?1, ?2, ?3, ?4) (i) (ii) (iii) (k=5)
(1,0,0,0) 0.5756 -3.9334 0.7666
(0,1,0,0) 0.5645 -4.6966 0.7765
(0,0,1,0) 0.5155 -5.0154 0.8146
(0,0,0,1) 0.6035 -3.1154 0.8245
(0.3,0.4,0.1,0.2) 0.4735 -5.1154 0.8976
Table 3: Three evaluation measures (i), (ii), and (iii) (see
the beginning of the section for description) for convex com-
binations (6) using different values of ?. The first four rows
represent methods A, B, C, and D. The bottom row repre-
sents a convex combination whose coefficients were obtained
by searching for the minimizer of measure (ii). Interestingly
the minimizer also performs well on measure (i) and more
impressively on the labeled measure (iii).
and D and the bottom row corresponds to a convex
combination found which minimizes the unsuper-
vised evaluation measure (ii) (i.e. the search for
the optimal combination is based on (ii) that does
not require labeled data). Note that the convex
combination also outperforms method A, B, C,
and D for measure (i) and more impressively for
measure (iii) which is a supervised measure that
uses labeled data. In general, by combining het-
erogeneous types of domain knowledge, we may
further improve the quality of dimensionality re-
duction for visualization, and the search for such
a combination may be accomplished without the
use of labeled data.
Finally, we demonstrate the effect of domain
knowledge on a new dataset that consists of all
oral papers appearing in ACL 2001 ? 2009. For
the purpose of manual specification, we obtain
1545 unique words from paper titles, and as-
sign for each word relatedness scores for the
following clusters: morphology/phonology, syn-
tax/parsing, semantics, discourse/dialogue, gen-
eration/summarization, machine translation, re-
trieval/categorization and machine learning. The
score takes value from 0 to 2, where 2 represents
the most relevant. The score information is then
used to generate the transformation matrix R. We
also assign for each word an importance value
ranging from 0 to 3 (larger the value, more impor-
tant the word). This information is used to gener-
ate the diagonal matrix D.
Figure 4 shows the projection of all 2009 pa-
pers using t-SNE (papers from 2001 to 2008 are
used to estimate contextual diffusion). Using Eu-
clidean geometry H = I (Figure 4 left) results in
a Gaussian like distribution which does not pro-
vide much insight into the data. Using a manually
806
(a) 0.3284 (b) 0.1794 (c) 0.1385
(d) 0.3008 (e) 0.2295 (f) 0.1093
Figure 3: Qualitative evaluation of dimensionality reduction for the sentiment domain (top two rows) and the newsgroup
domain (bottom two rows). The first and the third rows display PCA reduction while the second and the fourth display t-SNE.
The left column correspond to no domain knowledge (H = I) reverting PCA and t-SNE to their original form. The middle
column corresponds to manual specification (method A). The right column corresponds to contextual diffusion (method B).
Different groups (sentiment labels or newsgroup labels) are marked with different colors and marks.
In the sentiment case (top two rows) the graphs were rotated such that the direction returned by applying Fisher linear
discriminant onto the projected 2D coordinates aligns with the positive x-axis. The bell curves are Gaussian distributions
fitted from the x-coordinates of the projected data points (after rotation). The numbers displayed in each sub-figure are
computed from measure (iv).
807
Dennis Schwartz James Berardinelli Scott Renshaw Steve Rhodes
PCA t-SNE PCA t-SNE PCA t-SNE PCA t-SNE
H = I 1.8625 1.8781 1.4704 1.5909 1.8047 1.9453 1.8013 1.8415
A 1.8474 1.7909 1.3292 1.4406 1.6520 1.8166 1.4844 1.6610
B 1.4254 1.5809 1.3140 1.3276 1.5133 1.6097 1.5053 1.6145
C 1.6868 1.7766 1.3813 1.4371 1.7200 1.8605 1.7750 1.7979
H = I 0.6404 0.7465 0.8481 0.8496 0.6559 0.6821 0.6680 0.7410
A 0.6011 0.7779 0.9224 0.8966 0.7424 0.7411 0.8350 0.8513
B 0.8831 0.8554 0.9188 0.9377 0.8215 0.8332 0.8124 0.8324
C 0.7238 0.7981 0.8871 0.9093 0.6897 0.7151 0.6724 0.7726
Table 1: Quantitative evaluation of dimensionality reduction for visualization in the sentiment domain. Each of the four
columns corresponds to a different movie critic from the Cornell dataset (see text). The top five rows correspond to measure
(i) (lower is better) and the bottom five rows correspond to measure (iii) (k = 5, higher is better). Results were averaged over
40 cross validation iterations. We conclude that all methods outperform the original H = I with the contextual diffusion and
manual specification generally outperforming the others.
  4
 70
 81
 46
 49
 12
 85
103
 20
 77
 50
 48
  9
 29
  3
 13
  7
 56
105
 10
 44
 21
 16
 98
 14
 96
 95
 93
 64
 41
 92
 99
 75109
 79
 36
 62
  5
 51
 22
 8
  2 71
101
 66
 67
 90
 30
104
 37
121
 11
 55
 39
 59
 31
 84
 80
 73
108
 19
 72
107
 78
 97
119
 94
 53
 65
 83
116
102
 91
 24
112
 28
 32
 26
 25
100
 63
  8
 86
 47
 58
 68
118
110
  6
 38  54
 33
 40
 57
 76
 6
 18
 45
117
106
 43
114113
 60
 15
 27
 34
 74
 88
 69
 89
 42
 17
115
 82
111
 23
120
 35
 52
  4
 70
 81
 46
 49
 12
 85
103
 20
 77
 50
 48
  9
 29
  3
 13
  7
 56
105
 10
 44
 21
 16
 98
 14
 96
 95
 93
 64
 41
 92
 99
 75
109
 79
 36
 62
  5
 51
 22
 87
  2
 71
101
 66
 67
 90
 30
104
 37
121
 11
 55
 39
 59
 31
 84
 80
 73
108
 19
 72
107
 78
 97
119
 94
 53
 65
 83
116
102
 91
 24
112
 28
 32
 26
 25
100
 63
  8
 86
 47
 58
 68
118
110
  6
 38
 54
 33
 40
 57
 76
 61
 18
 45117
106
 43
114
113
 60
 15
 27
 34
 74
 88
 69
 89
 42
 17
115
 82
111
 23
120
 35
 52
  4
 70
 81
 46
 49
 12
 85
103
 20
 77
 50
 48
  9
 29
  3
 13
  7
 56
105 10
 44
 21
 16
 98
 14
 96
 95
 93 64
 41
 92
 99
 75
109
 79
 36
 62
  5
 51
 22
 87
  2
 71
101 66
 67
 90
 30
104
 37
121
 11
 55
 39
 59
 31
 84
 80
 73
108
 19
 72
107
 78
 97
119
 94
 53
65
 83
116
102
 91
 24
1 2
 28
 32
 26
 25
100
 63
  8
 8
 47
 58 68
118
110
  6
 38
 54
 33
 40
 57
 76
 61
 18
 45
117
106
 43
114
113
 0  15
 27
 34
 74
 88
 69
 89
 42
 17
115
 82111
 23
120
 5
 52
Figure 4: Qualitative evaluation of dimensionality reduction for the ACL dataset using t-SNE. Left: no domain knowledge
(H = I); Middle: manual specification (method A); Right: contextual diffusion (method B). Each document is labeled by its
assigned id from ACL anthology. See text for more details.
specified H (Figure 4 left) we get two clear clus-
ters, the smaller containing papers dealing with
machine translation and multilingual tasks. Inter-
estingly, the contextual diffusion results in a one-
dimensional manifold. Investigating the papers
along the curve (from bottom to top) we find that
it starts with papers discussing semantics and dis-
course (south), continues to structured prediction
and segmentation (east), continues to parsing and
machine learning (north), and then moves to senti-
ment prediction, summarization and IR (west) be-
fore returning to the center. Another interesting
insight that we can derive is the relative disconti-
nuity between the bottom part (semantics and dis-
course) and the rest of the curve. It seems spatial
separability is higher in that area than in the other
areas where the curve nicely traverses different re-
gions continuously.
6 Discussion
In this paper we introduce several ways of incor-
porating domain knowledge into dimensionality
reduction for visualizing text documents. The pro-
posed methods all outperform in general the base-
line H = I , which is the one currently used in
most text visualization systems.
The answer to the question of which method is
best depends on both the domain and the task at
hand. For small tasks with limited vocabulary,
manual specification could achieve best results.
A large vocabulary size makes manual specifica-
tion less accurate and effective. In cases where
we have access to a large external corpus that is
similar to the one we are interested in visualizing,
contextual diffusion is an excellent choice. Lack-
ing such a domain specific dataset estimating the
contextual distribution using the generic Google
n-gram is a good substitute. Word-Net captures
relationships (such as synonyms and hyponyms)
other than occurrence statistics between vocabu-
lary words, and could be useful for certain tasks.
Finally, the effectiveness of dimensionality reduc-
tion methods can be increased further by carefully
combining different types of domain knowledge
ranging from semantic similarity to occurrence
statistics.
808
References
Blei, D., A. Ng, , and M. Jordan. 2003. Latent dirich-
let alocation. Journal of Machine Learning Re-
search, 3:993?1022.
Brants, T. and A. Franz. 2006. Web 1T 5-gram Ver-
sion 1.
Budanitsky, A. and G. Hirst. 2001. Semantic distance
in wordnet: An experimental, application-oriented
evaluation of five measures. In NAACL Workshop
on WordNet and other Lexical Resources.
Burges, C. 2009. Dimension reduction: A guided
tour. Technical Report MSR-TR-2009-2013, Mi-
crosoft Research.
Davies, D. L. and D. W. Bouldin. 2000. A cluster
separation measure. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 1(4):224?227.
Dillon, J., Y. Mao, G. Lebanon, and J. Zhang. 2007.
Statistical translation, heat kernels, and expected
distances. In Uncertainty in Artificial Intelligence,
pages 93?100. AUAI Press.
Duda, R. O., P. E. Hart, and D. G. Stork. 2001. Pattern
classification. Wiley New York.
Havre, S., E. Hetzler, P. Whitney, and L. Nowell. 2002.
Themeriver: Visualizing thematic changes in large
document collections. IEEE Transactions on Visu-
alization and Computer Graphics, 8(1).
Hearst, M. A. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Jiang, J. J. and D. W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical tax-
onomy. In International Conference Research on
Computational Linguistics (ROCLING X).
Jurafsky, D. and J. H. Martin. 2008. Speech and Lan-
guage Processing. Prentice Hall.
Lafferty, J. and G. Lebanon. 2005. Diffusion kernels
on statistical manifolds. Journal of Machine Learn-
ing Research, 6:129?163.
Lebanon, G. 2006. Metric learning for text docu-
ments. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 28(4):497?508.
Lewis, D., Y. Yang, T. Rose, and F. Li. 2004. RCV1:
A new benchmark collection for text categorization
research. Journal of Machine Learning Research,
5:361?397.
Mao, Y., J. Dillon, and G. Lebanon. 2007. Sequen-
tial document visualization. IEEE Transactions on
Visualization and Computer Graphics, 13(6):1208?
1215.
Paley, W. B. 2002. TextArc: Showing word frequency
and distribution in text. In IEEE Symposium on In-
formation Visualization Poster Compendium.
Pang, B. and L. Lee. 2004. A sentimental eduction:
sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of the Association
of Computational Linguistics.
Pang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learn-
ing techniques. In Proc. of the Conference on Em-
pirical Methods in Natural Language Processing.
Roweis, S. and L. Saul. 2000. Nonlinear dimensional-
ity reduction by locally linear embedding. Science,
290:2323?2326.
Spoerri, A. 1993. InfoCrystal: A visual tool for infor-
mation retrieval. In Proc. of IEEE Visualization.
Thomas, J. J. and K. A. Cook, editors. 2005. Illu-
minating the Path: The Research and Development
Agenda for Visual Analytics. IEEE Computer Soci-
ety.
van der Maaten, L. and G. Hinton. 2008. Visualiz-
ing data using t-sne. Journal of Machine Learning
Research, 9:2579?2605.
Xing, E., A. Ng, M. Jordan, and S. Russel. 2003. Dis-
tance metric learning with applications to clustering
with side information. In Advances in Neural Infor-
mation Processing Systems, 15.
809
