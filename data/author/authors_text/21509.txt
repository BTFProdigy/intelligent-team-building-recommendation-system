Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631?1642,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng and Christopher Potts
Stanford University, Stanford, CA 94305, USA
richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu
{jeaneis,manning,cgpotts}@stanford.edu
Abstract
Semantic word spaces have been very use-
ful but cannot express the meaning of longer
phrases in a principled way. Further progress
towards understanding compositionality in
tasks such as sentiment detection requires
richer supervised training and evaluation re-
sources and more powerful models of com-
position. To remedy this, we introduce a
Sentiment Treebank. It includes fine grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment composition-
ality. To address them, we introduce the
Recursive Neural Tensor Network. When
trained on the new treebank, this model out-
performs all previous methods on several met-
rics. It pushes the state of the art in single
sentence positive/negative classification from
80% up to 85.4%. The accuracy of predicting
fine-grained sentiment labels for all phrases
reaches 80.7%, an improvement of 9.7% over
bag of features baselines. Lastly, it is the only
model that can accurately capture the effects
of negation and its scope at various tree levels
for both positive and negative phrases.
1 Introduction
Semantic vector spaces for single words have been
widely used as features (Turney and Pantel, 2010).
Because they cannot capture the meaning of longer
phrases properly, compositionality in semantic vec-
tor spaces has recently received a lot of attention
(Mitchell and Lapata, 2010; Socher et al, 2010;
Zanzotto et al, 2010; Yessenalina and Cardie, 2011;
Socher et al, 2012; Grefenstette et al, 2013). How-
ever, progress is held back by the current lack of
large and labeled compositionality resources and
?
0
0This 0film
?
?
?
0does 0n?t
0
+care +0about ++
+
+
+cleverness 0,
0wit
0or
+
0
0any 00other +kind
+
0of ++intelligent + +humor
0.
Figure 1: Example of the Recursive Neural Tensor Net-
work accurately predicting 5 sentiment classes, very neg-
ative to very positive (? ?, ?, 0, +, + +), at every node of a
parse tree and capturing the negation and its scope in this
sentence.
models to accurately capture the underlying phe-
nomena presented in such data. To address this need,
we introduce the Stanford Sentiment Treebank and
a powerful Recursive Neural Tensor Network that
can accurately predict the compositional semantic
effects present in this new corpus.
The Stanford Sentiment Treebank is the first cor-
pus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser (Klein and Manning, 2003) and includes a
total of 215,154 unique phrases from those parse
trees, each annotated by 3 human judges. This new
dataset alows us to analyze the intricacies of senti-
ment and to capture complex linguistic phenomena.
Fig. 1 shows one of the many examples with clear
compositional structure. The granularity and size of
1631
this dataset will enable the community to train com-
positional models that are based on supervised and
structured machine learning techniques. While there
are several datasets with document and chunk labels
available, there is a need to better capture sentiment
from short comments, such as Twitter data, which
provide less overall signal per document.
In order to capture the compositional effects with
higher accuracy, we propose a new model called the
Recursive Neural Tensor Network (RNTN). Recur-
sive Neural Tensor Networks take as input phrases
of any length. They represent a phrase through word
vectors and a parse tree and then compute vectors for
higher nodes in the tree using the same tensor-based
composition function. We compare to several super-
vised, compositional models such as standard recur-
sive neural networks (RNN) (Socher et al, 2011b),
matrix-vector RNNs (Socher et al, 2012), and base-
lines such as neural networks that ignore word order,
Naive Bayes (NB), bi-gram NB and SVM. All mod-
els get a significant boost when trained with the new
dataset but the RNTN obtains the highest perfor-
mance with 80.7% accuracy when predicting fine-
grained sentiment for all nodes. Lastly, we use a test
set of positive and negative sentences and their re-
spective negations to show that, unlike bag of words
models, the RNTN accurately captures the sentiment
change and scope of negation. RNTNs also learn
that sentiment of phrases following the contrastive
conjunction ?but? dominates.
The complete training and testing code, a live
demo and the Stanford Sentiment Treebank dataset
are available at http://nlp.stanford.edu/
sentiment.
2 Related Work
This work is connected to five different areas of NLP
research, each with their own large amount of related
work to which we cannot do full justice given space
constraints.
Semantic Vector Spaces. The dominant ap-
proach in semantic vector spaces uses distributional
similarities of single words. Often, co-occurrence
statistics of a word and its context are used to de-
scribe each word (Turney and Pantel, 2010; Baroni
and Lenci, 2010), such as tf-idf. Variants of this idea
use more complex frequencies such as how often a
word appears in a certain syntactic context (Pado
and Lapata, 2007; Erk and Pado?, 2008). However,
distributional vectors often do not properly capture
the differences in antonyms since those often have
similar contexts. One possibility to remedy this is to
use neural word vectors (Bengio et al, 2003). These
vectors can be trained in an unsupervised fashion
to capture distributional similarities (Collobert and
Weston, 2008; Huang et al, 2012) but then also be
fine-tuned and trained to specific tasks such as sen-
timent detection (Socher et al, 2011b). The models
in this paper can use purely supervised word repre-
sentations learned entirely on the new corpus.
Compositionality in Vector Spaces. Most of
the compositionality algorithms and related datasets
capture two word compositions. Mitchell and La-
pata (2010) use e.g. two-word phrases and analyze
similarities computed by vector addition, multiplica-
tion and others. Some related models such as holo-
graphic reduced representations (Plate, 1995), quan-
tum logic (Widdows, 2008), discrete-continuous
models (Clark and Pulman, 2007) and the recent
compositional matrix space model (Rudolph and
Giesbrecht, 2010) have not been experimentally val-
idated on larger corpora. Yessenalina and Cardie
(2011) compute matrix representations for longer
phrases and define composition as matrix multipli-
cation, and also evaluate on sentiment. Grefen-
stette and Sadrzadeh (2011) analyze subject-verb-
object triplets and find a matrix-based categorical
model to correlate well with human judgments. We
compare to the recent line of work on supervised
compositional models. In particular we will de-
scribe and experimentally compare our new RNTN
model to recursive neural networks (RNN) (Socher
et al, 2011b) and matrix-vector RNNs (Socher et
al., 2012) both of which have been applied to bag of
words sentiment corpora.
Logical Form. A related field that tackles com-
positionality from a very different angle is that of
trying to map sentences to logical form (Zettlemoyer
and Collins, 2005). While these models are highly
interesting and work well in closed domains and
on discrete sets, they could only capture sentiment
distributions using separate mechanisms beyond the
currently used logical forms.
Deep Learning. Apart from the above mentioned
1632
work on RNNs, several compositionality ideas re-
lated to neural networks have been discussed by Bot-
tou (2011) and Hinton (1990) and first models such
as Recursive Auto-associative memories been exper-
imented with by Pollack (1990). The idea to relate
inputs through three way interactions, parameterized
by a tensor have been proposed for relation classifi-
cation (Sutskever et al, 2009; Jenatton et al, 2012),
extending Restricted Boltzmann machines (Ranzato
and Hinton, 2010) and as a special layer for speech
recognition (Yu et al, 2012).
Sentiment Analysis. Apart from the above-
mentioned work, most approaches in sentiment anal-
ysis use bag of words representations (Pang and Lee,
2008). Snyder and Barzilay (2007) analyzed larger
reviews in more detail by analyzing the sentiment
of multiple aspects of restaurants, such as food or
atmosphere. Several works have explored sentiment
compositionality through careful engineering of fea-
tures or polarity shifting rules on syntactic structures
(Polanyi and Zaenen, 2006; Moilanen and Pulman,
2007; Rentoumi et al, 2010; Nakagawa et al, 2010).
3 Stanford Sentiment Treebank
Bag of words classifiers can work well in longer
documents by relying on a few words with strong
sentiment like ?awesome? or ?exhilarating.? How-
ever, sentiment accuracies even for binary posi-
tive/negative classification for single sentences has
not exceeded 80% for several years. For the more
difficult multiclass case including a neutral class,
accuracy is often below 60% for short messages
on Twitter (Wang et al, 2012). From a linguistic
or cognitive standpoint, ignoring word order in the
treatment of a semantic task is not plausible, and, as
we will show, it cannot accurately classify hard ex-
amples of negation. Correctly predicting these hard
cases is necessary to further improve performance.
In this section we will introduce and provide some
analyses for the new Sentiment Treebank which in-
cludes labels for every syntactically plausible phrase
in thousands of sentences, allowing us to train and
evaluate compositional models.
We consider the corpus of movie review excerpts
from the rottentomatoes.com website orig-
inally collected and published by Pang and Lee
(2005). The original dataset includes 10,662 sen-
nerdy ?folks
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
phenomenal ?fantasy ?best ?sellers
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
 ?
 ?
Figure 3: The labeling interface. Random phrases were
shown and annotators had a slider for selecting the senti-
ment and its degree.
tences, half of which were considered positive and
the other half negative. Each label is extracted from
a longer movie review and reflects the writer?s over-
all intention for this review. The normalized, lower-
cased text is first used to recover, from the origi-
nal website, the text with capitalization. Remaining
HTML tags and sentences that are not in English
are deleted. The Stanford Parser (Klein and Man-
ning, 2003) is used to parses all 10,662 sentences.
In approximately 1,100 cases it splits the snippet
into multiple sentences. We then used Amazon Me-
chanical Turk to label the resulting 215,154 phrases.
Fig. 3 shows the interface annotators saw. The slider
has 25 different values and is initially set to neutral.
The phrases in each hit are randomly sampled from
the set of all phrases in order to prevent labels being
influenced by what follows. For more details on the
dataset collection, see supplementary material.
Fig. 2 shows the normalized label distributions at
each n-gram length. Starting at length 20, the ma-
jority are full sentences. One of the findings from
labeling sentences based on reader?s perception is
that many of them could be considered neutral. We
also notice that stronger sentiment often builds up
in longer phrases and the majority of the shorter
phrases are neutral. Another observation is that most
annotators moved the slider to one of the five po-
sitions: negative, somewhat negative, neutral, posi-
tive or somewhat positive. The extreme values were
rarely used and the slider was not often left in be-
tween the ticks. Hence, even a 5-class classification
into these categories captures the main variability
of the labels. We will name this fine-grained senti-
ment classification and our main experiment will be
to recover these five labels for phrases of all lengths.
1633
5 10 15 20 25 30 35 40 45
N-Gram Length
0%
20%
40%
60%
80%
100%
%
 o
f S
en
tim
en
t V
al
ue
s
Neutral
SomeZhat 3ositiYe
3ositiYe
Ver\ 3ositiYe
SomeZhat NegatiYe
NegatiYe
Ver\ NegatiYe
(a)
(a)
(b)
(b)
(c)
(c)
(d)
(d)
Distributions of sentiment values for (a) unigrams, 
(b) 10-grams, (c) 20-grams, and (d) full sentences.
Figure 2: Normalized histogram of sentiment annotations at each n-gram length. Many shorter n-grams are neutral;
longer phrases are well distributed. Few annotators used slider positions between ticks or the extreme values. Hence
the two strongest labels and intermediate tick positions are merged into 5 classes.
4 Recursive Neural Models
The models in this section compute compositional
vector representations for phrases of variable length
and syntactic type. These representations will then
be used as features to classify each phrase. Fig. 4
displays this approach. When an n-gram is given to
the compositional models, it is parsed into a binary
tree and each leaf node, corresponding to a word,
is represented as a vector. Recursive neural mod-
els will then compute parent vectors in a bottom
up fashion using different types of compositional-
ity functions g. The parent vectors are again given
as features to a classifier. For ease of exposition,
we will use the tri-gram in this figure to explain all
models.
We first describe the operations that the below re-
cursive neural models have in common: word vector
representations and classification. This is followed
by descriptions of two previous RNN models and
our RNTN.
Each word is represented as a d-dimensional vec-
tor. We initialize all word vectors by randomly
sampling each value from a uniform distribution:
U(?r, r), where r = 0.0001. All the word vec-
tors are stacked in the word embedding matrix L ?
Rd?|V |, where |V | is the size of the vocabulary. Ini-
tially the word vectors will be random but the L ma-
trix is seen as a parameter that is trained jointly with
the compositionality models.
We can use the word vectors immediately as
parameters to optimize and as feature inputs to
a softmax classifier. For classification into five
classes, we compute the posterior probability over
    not      very       good ...
        a          b             c 
p1 =g(b,c)
p2 = g(a,p1)
0 0 +
+ +
-
Figure 4: Approach of Recursive Neural Network mod-
els for sentiment: Compute parent vectors in a bottom up
fashion using a compositionality function g and use node
vectors as features for a classifier at that node. This func-
tion varies for the different models.
labels given the word vector via:
ya = softmax(Wsa), (1)
where Ws ? R5?d is the sentiment classification
matrix. For the given tri-gram, this is repeated for
vectors b and c. The main task of and difference
between the models will be to compute the hidden
vectors pi ? Rd in a bottom up fashion.
4.1 RNN: Recursive Neural Network
The simplest member of this family of neural net-
work models is the standard recursive neural net-
work (Goller and Ku?chler, 1996; Socher et al,
2011a). First, it is determined which parent already
has all its children computed. In the above tree ex-
ample, p1 has its two children?s vectors since both
are words. RNNs use the following equations to
compute the parent vectors:
1634
p1 = f
(
W
[
b
c
])
, p2 = f
(
W
[
a
p1
])
,
where f = tanh is a standard element-wise nonlin-
earity, W ? Rd?2d is the main parameter to learn
and we omit the bias for simplicity. The bias can be
added as an extra column to W if an additional 1 is
added to the concatenation of the input vectors. The
parent vectors must be of the same dimensionality to
be recursively compatible and be used as input to the
next composition. Each parent vector pi, is given to
the same softmax classifier of Eq. 1 to compute its
label probabilities.
This model uses the same compositionality func-
tion as the recursive autoencoder (Socher et al,
2011b) and recursive auto-associate memories (Pol-
lack, 1990). The only difference to the former model
is that we fix the tree structures and ignore the re-
construction loss. In initial experiments, we found
that with the additional amount of training data, the
reconstruction loss at each node is not necessary to
obtain high performance.
4.2 MV-RNN: Matrix-Vector RNN
The MV-RNN is linguistically motivated in that
most of the parameters are associated with words
and each composition function that computes vec-
tors for longer phrases depends on the actual words
being combined. The main idea of the MV-RNN
(Socher et al, 2012) is to represent every word and
longer phrase in a parse tree as both a vector and
a matrix. When two constituents are combined the
matrix of one is multiplied with the vector of the
other and vice versa. Hence, the compositional func-
tion is parameterized by the words that participate in
it.
Each word?s matrix is initialized as a d?d identity
matrix, plus a small amount of Gaussian noise. Sim-
ilar to the random word vectors, the parameters of
these matrices will be trained to minimize the clas-
sification error at each node. For this model, each n-
gram is represented as a list of (vector,matrix) pairs,
together with the parse tree. For the tree with (vec-
tor,matrix) nodes:
(p2,P2)
(a,A) (p1,P1)
(b,B) (c,C)
the MV-RNN computes the first parent vector and its
matrix via two equations:
p1 = f
(
W
[
Cb
Bc
])
, P1 = f
(
WM
[
B
C
])
,
where WM ? Rd?2d and the result is again a d ? d
matrix. Similarly, the second parent node is com-
puted using the previously computed (vector,matrix)
pair (p1, P1) as well as (a,A). The vectors are used
for classifying each phrase using the same softmax
classifier as in Eq. 1.
4.3 RNTN:Recursive Neural Tensor Network
One problem with the MV-RNN is that the number
of parameters becomes very large and depends on
the size of the vocabulary. It would be cognitively
more plausible if there was a single powerful com-
position function with a fixed number of parameters.
The standard RNN is a good candidate for such a
function. However, in the standard RNN, the input
vectors only implicitly interact through the nonlin-
earity (squashing) function. A more direct, possibly
multiplicative, interaction would allow the model to
have greater interactions between the input vectors.
Motivated by these ideas we ask the question: Can
a single, more powerful composition function per-
form better and compose aggregate meaning from
smaller constituents more accurately than many in-
put specific ones? In order to answer this question,
we propose a new model called the Recursive Neu-
ral Tensor Network (RNTN). The main idea is to use
the same, tensor-based composition function for all
nodes.
Fig. 5 shows a single tensor layer. We define the
output of a tensor product h ? Rd via the follow-
ing vectorized notation and the equivalent but more
detailed notation for each slice V [i] ? Rd?d:
h =
[
b
c
]T
V [1:d]
[
b
c
]
;hi =
[
b
c
]T
V [i]
[
b
c
]
.
where V [1:d] ? R2d?2d?d is the tensor that defines
multiple bilinear forms.
1635
            Slices of       Standard   
                Tensor Layer          Layer
p = f             V[1:2]        +   W
Neural Tensor Layer
b
c
b
c
b
c
T
p = f                             +          
Figure 5: A single layer of the Recursive Neural Ten-
sor Network. Each dashed box represents one of d-many
slices and can capture a type of influence a child can have
on its parent.
The RNTN uses this definition for computing p1:
p1 = f
([
b
c
]T
V [1:d]
[
b
c
]
+W
[
b
c
])
,
where W is as defined in the previous models. The
next parent vector p2 in the tri-gram will be com-
puted with the same weights:
p2 = f
([
a
p1
]T
V [1:d]
[
a
p1
]
+W
[
a
p1
])
.
The main advantage over the previous RNN
model, which is a special case of the RNTN when
V is set to 0, is that the tensor can directly relate in-
put vectors. Intuitively, we can interpret each slice
of the tensor as capturing a specific type of compo-
sition.
An alternative to RNTNs would be to make the
compositional function more powerful by adding a
second neural network layer. However, initial exper-
iments showed that it is hard to optimize this model
and vector interactions are still more implicit than in
the RNTN.
4.4 Tensor Backprop through Structure
We describe in this section how to train the RNTN
model. As mentioned above, each node has a
softmax classifier trained on its vector representa-
tion to predict a given ground truth or target vector
t. We assume the target distribution vector at each
node has a 0-1 encoding. If there are C classes, then
it has length C and a 1 at the correct label. All other
entries are 0.
We want to maximize the probability of the cor-
rect prediction, or minimize the cross-entropy error
between the predicted distribution yi ? RC?1 at
node i and the target distribution ti ? RC?1 at that
node. This is equivalent (up to a constant) to mini-
mizing the KL-divergence between the two distribu-
tions. The error as a function of the RNTN parame-
ters ? = (V,W,Ws, L) for a sentence is:
E(?) =
?
i
?
j
tij log y
i
j + ????
2 (2)
The derivative for the weights of the softmax clas-
sifier are standard and simply sum up from each
node?s error. We define xi to be the vector at node
i (in the example trigram, the xi ? Rd?1?s are
(a, b, c, p1, p2)). We skip the standard derivative for
Ws. Each node backpropagates its error through to
the recursively used weights V,W . Let ?i,s ? Rd?1
be the softmax error vector at node i:
?i,s =
(
W Ts (y
i ? ti)
)
? f ?(xi),
where ? is the Hadamard product between the two
vectors and f ? is the element-wise derivative of f
which in the standard case of using f = tanh can
be computed using only f(xi).
The remaining derivatives can only be computed
in a top-down fashion from the top node through the
tree and into the leaf nodes. The full derivative for
V and W is the sum of the derivatives at each of
the nodes. We define the complete incoming error
messages for a node i as ?i,com. The top node, in
our case p2, only received errors from the top node?s
softmax. Hence, ?p2,com = ?p2,s which we can
use to obtain the standard backprop derivative for
W (Goller and Ku?chler, 1996; Socher et al, 2010).
For the derivative of each slice k = 1, . . . , d, we get:
?Ep2
?V [k]
= ?p2,comk
[
a
p1
] [
a
p1
]T
,
where ?p2,comk is just the k?th element of this vector.
Now, we can compute the error message for the two
1636
children of p2:
?p2,down =
(
W T ?p2,com + S
)
? f ?
([
a
p1
])
,
where we define
S =
d?
k=1
?p2,comk
(
V [k] +
(
V [k]
)T
)[
a
p1
]
The children of p2, will then each take half of this
vector and add their own softmax error message for
the complete ?. In particular, we have
?p1,com = ?p1,s + ?p2,down[d+ 1 : 2d],
where ?p2,down[d + 1 : 2d] indicates that p1 is the
right child of p2 and hence takes the 2nd half of the
error, for the final word vector derivative for a, it
will be ?p2,down[1 : d].
The full derivative for slice V [k] for this trigram
tree then is the sum at each node:
?E
?V [k]
=
Ep2
?V [k]
+ ?p1,comk
[
b
c
] [
b
c
]T
,
and similarly for W . For this nonconvex optimiza-
tion we use AdaGrad (Duchi et al, 2011) which con-
verges in less than 3 hours to a local optimum.
5 Experiments
We include two types of analyses. The first type in-
cludes several large quantitative evaluations on the
test set. The second type focuses on two linguistic
phenomena that are important in sentiment.
For all models, we use the dev set and cross-
validate over regularization of the weights, word
vector size as well as learning rate and minibatch
size for AdaGrad. Optimal performance for all mod-
els was achieved at word vector sizes between 25
and 35 dimensions and batch sizes between 20 and
30. Performance decreased at larger or smaller vec-
tor and batch sizes. This indicates that the RNTN
does not outperform the standard RNN due to sim-
ply having more parameters. The MV-RNN has or-
ders of magnitudes more parameters than any other
model due to the word matrices. The RNTN would
usually achieve its best performance on the dev set
after training for 3 - 5 hours. Initial experiments
Model
Fine-grained Positive/Negative
All Root All Root
NB 67.2 41.0 82.6 81.8
SVM 64.3 40.7 84.6 79.4
BiNB 71.0 41.9 82.7 83.1
VecAvg 73.3 32.7 85.1 80.1
RNN 79.0 43.2 86.1 82.4
MV-RNN 78.7 44.4 86.8 82.9
RNTN 80.7 45.7 87.6 85.4
Table 1: Accuracy for fine grained (5-class) and binary
predictions at the sentence level (root) and for all nodes.
showed that the recursive models worked signifi-
cantly worse (over 5% drop in accuracy) when no
nonlinearity was used. We use f = tanh in all ex-
periments.
We compare to commonly used methods that use
bag of words features with Naive Bayes and SVMs,
as well as Naive Bayes with bag of bigram features.
We abbreviate these with NB, SVM and biNB. We
also compare to a model that averages neural word
vectors and ignores word order (VecAvg).
The sentences in the treebank were split into a
train (8544), dev (1101) and test splits (2210) and
these splits are made available with the data release.
We also analyze performance on only positive and
negative sentences, ignoring the neutral class. This
filters about 20% of the data with the three sets hav-
ing 6920/872/1821 sentences.
5.1 Fine-grained Sentiment For All Phrases
The main novel experiment and evaluation metric
analyze the accuracy of fine-grained sentiment clas-
sification for all phrases. Fig. 2 showed that a fine
grained classification into 5 classes is a reasonable
approximation to capture most of the data variation.
Fig. 6 shows the result on this new corpus. The
RNTN gets the highest performance, followed by
the MV-RNN and RNN. The recursive models work
very well on shorter phrases, where negation and
composition are important, while bag of features
baselines perform well only with longer sentences.
The RNTN accuracy upper bounds other models at
most n-gram lengths.
Table 1 (left) shows the overall accuracy numbers
for fine grained prediction at all phrase lengths and
full sentences.
1637
    
1*UDP/HQJWK





$
F
F
X
U
D
F
\
    
1*UDP/HQJWK

Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1225?1236,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Human Effort and Machine Learnability in Computer Aided Translation
Spence Green, Sida Wang, Jason Chuang,
*
Jeffrey Heer,
*
Sebastian Schuster,
and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,sidaw,sebschu,manning}@stanford.edu
*
Computer Science Department, University of Washington
{jcchuang,jheer}@uw.edu
Abstract
Analyses of computer aided translation typi-
cally focus on either frontend interfaces and
human effort, or backend translation and
machine learnability of corrections. How-
ever, this distinction is artificial in prac-
tice since the frontend and backend must
work in concert. We present the first holis-
tic, quantitative evaluation of these issues
by contrasting two assistive modes: post-
editing and interactive machine translation
(MT). We describe a new translator inter-
face, extensive modifications to a phrase-
based MT system, and a novel objective
function for re-tuning to human correc-
tions. Evaluation with professional bilin-
gual translators shows that post-edit is faster
than interactive at the cost of translation
quality for French-English and English-
German. However, re-tuning the MT sys-
tem to interactive output leads to larger, sta-
tistically significant reductions in HTER
versus re-tuning to post-edit. Analysis
shows that tuning directly to HTER results
in fine-grained corrections to subsequent
machine output.
1 Introduction
The goal of machine translation has always been to
reduce human effort, whether by partial assistance
or by outright replacement. However, preoccupa-
tion with the latter?fully automatic translation?at
the exclusion of the former has been a feature of
the research community since its first nascent steps
in the 1950s. Pessimistic about progress during
that decade and future prospects, Bar-Hillel (1960,
p.3) argued that more attention should be paid to a
?machine-post-editor partnership,? whose decisive
problem is ?the region of optimality in the contin-
uum of possible divisions of labor.? Today, with
human-quality, fully automatic machine translation
(MT) elusive still, that decades-old recommenda-
tion remains current.
This paper is the first to look at both sides of
the partnership in a single user study. We compare
two common flavors of machine-assisted transla-
tion: post-editing and interactive MT. We analyze
professional, bilingual translators working in both
modes, looking first at user productivity. Does the
additional machine assistance available in the inter-
active mode affect translation time and/or quality?
Then we turn to the machine side of the part-
nership. The user study results in corrections to
the baseline MT output. Do these corrections help
the MT system, and can it learn from them quickly
enough to help the user? We perform a re-tuning
experiment in which we directly optimize human
Translation Edit Rate (HTER), which correlates
highly with human judgments of fluency and ade-
quacy (Snover et al., 2006). It is also an intuitive
measure of human effort, making fine distinctions
between 0 (no editing) and 1 (complete rewrite).
We designed a new user interface (UI) for the
experiment. The interface places demands on the
MT backend?not the other way around. The most
significant new MT system features are prefix de-
coding, for translation completion based on a user
prefix; and dynamic phrase table augmentation, to
handle target out-of-vocabulary (OOV) words. Dis-
criminative re-tuning is accomplished with a novel
cross-entropy objective function.
We report three main findings: (1) post-editing
is faster than interactive MT, corroborating Koehn
(2009a); (2) interactive MT yields higher quality
translation when baseline MT quality is high; and
(3) re-tuning to interactive feedback leads to larger
held-out HTER gains relative to post-edit. Together
these results show that a human-centered approach
to computer aided translation (CAT) may involve
tradeoffs between human effort and machine
learnability. For example, if speed is the top
priority, then a design geared toward post-editing
1225
AB
C
D E
Figure 1: Main translation interface. The user sees the full document context, with French source inputs
(A) interleaved with suggested English translations (B). The sentence in focus is indicated by the blue
rectangle, which can be moved via two hot keys. Source coverage (C) of the user prefix?shaded in
blue?updates as the user works, as do autocomplete suggestions (D) and a full completion (E).
is appropriate. However, if reductions in HTER
ultimately correspond to lower human effort, then
investing slightly more time in the interactive mode,
which results in more learnable output, may be op-
timal. Mixed UI designs may offer a compromise.
Code and data from our experiments are available at:
http://nlp.stanford.edu/software/phrasal/
A holistic comparison with human subjects nec-
essarily involves many moving parts. Section 2
briefly describes the interface, focusing on NLP
components. Section 3 describes changes to the
backend MT system. Section 4 explains the user
study, and reports human translation time and qual-
ity results. Section 5 describes the MT re-tuning
experiment. Analysis (section 6) and related work
(section 7) round out the paper.
2 New Translator User Interface
Figure 1 shows the translator interface, which is
designed for expert, bilingual translators. Previ-
ous studies have shown that expert translators work
and type quickly (Carl, 2010), so the interface is
designed to be very responsive, and to be primar-
ily operated by the keyboard. Most aids can be
accessed via either typing or four hot keys. The
current design focuses on the point of text entry
and does not include conventional translator work-
bench features such as workflow management, spell
checking, and text formatting tools.
In the trivial post-edit mode, the interactive aids
are disabled and a 1-best translation pre-populates
the text entry box.
We have described the HCI-specific motivations
for and contributions of this new interface in Green
et al. (2014c). This section focuses on interface
elements built on NLP components.
2.1 UI Overview and Walkthrough
We categorized interactions into three groups:
source comprehension: word lookups, source cov-
erage highlighting; target gisting: 1-best transla-
tion, real-time target completion; target genera-
tion: real-time autocomplete, target reordering, in-
sert complete translation. The interaction designs
are novel; those in italic have, to our knowledge,
never appeared in a translation workbench.
Source word lookup When the user hovers over
a source word, a menu of up to four ranked trans-
lation suggestions appears (Figure 2). The menu
is populated by a phrase-table query of the word
plus one token of left context. This query usually
returns in under 50ms. The width of the horizontal
bars indicates confidence, with the most confident
suggestion ?regularly? placed at the bottom, near-
est to the cursor. The user can insert a translation
suggestion by clicking.
Source coverage highlighting The source cover-
age feature (Figure 1C) helps the user quickly find
untranslated words in the source. The interaction is
1226
Figure 2: Source word lookup and target autocom-
plete menus. The menus show different suggestions.
The word lookupmenu (top) is not dependent on the
target context Teachers, whereas the autocomplete
dropdown (bottom) is.
based on the word alignments between source and
target generated by the MT system. We found that
the raw alignments are too noisy to show users, so
the UI filters them with phrase-level heuristics.
1-best translation The most common use of MT
output is gisting (Koehn, 2010, p.21). The gray text
below each black source input shows the best MT
system output (Figure 1B).
Real-time target completion When the user ex-
tends the black prefix, the gray text will update to
the most probable completion (Figure 1E). This up-
date comes from decoding under the full translation
model. All previous systems performed inference
in a word lattice.
Real-time autocomplete The autocomplete
dropdown at the point of text entry is the main
translation aid (Figures 1D and 2). Each real-time
update actually contains a distinct 10-best list for
the full source input. The UI builds up a trie from
these 10-best lists. Up to four distinct suggestions
are then shown at the point of translation. The
suggestion length is based on a syntactic parse of
the fixed source input. As an offline, pre-processing
step, we parse each source input with Stanford
CoreNLP (Manning et al., 2014). The UI combines
those parses with word alignments from the full
translation suggestions to project syntactic con-
stituents to each item on the n-best list. Syntactic
projection is a very old idea that underlies many
MT systems (see: Hwa et al. (2002)). Here we
make novel use of it for suggestion prediction
filtering.
1
Presently, we project noun phrases,
verb phrases (minus the verbal arguments), and
prepositional phrases. Crucially, these units are
natural to humans, unlike statistical target phrases.
Target Reordering Carl (2010) showed that ex-
pert translators tend to adopt local planning: they
read a few words ahead and then translate in a
roughly online fashion. However, word order differ-
ences between languages will necessarily require
longer range planning and movement. To that end,
the UI supports keyboard-based reordering. Sup-
pose that the user wants to move a span in gray
text to the insertion position for editing. Typing
the prefix of this string will update the autocom-
plete dropdown with matching strings from the gray
text. Consequently, sometimes the autocomplete
dropdown will contain suggestions from several
positions in the full suggested translation.
Insert complete translation The user can insert
the full completion via a hot key. Notice that if
the user presses this hot key immediately, all gray
text becomes black, and the interface effectively
switches to post-edit mode. This feature greatly ac-
celerates translation when the MT is mostly correct,
and the user only wants to make a few changes.
2.2 User Activity Logging
A web application serves the Javascript-based in-
terface, relays translation requests to the MT sys-
tem, and logs user records to a database. Each user
record is a tuple of the form (f, e?, h, u), where f
is the source sequence, e? is the latest 1-best ma-
chine translation of f , h is the correction of e?, and
u is the log of interaction events during the transla-
tion session. Our evaluation corpora also include
independently generated references e for each f .
3 Interactive MT Backend
Now we describe modifications to Phrasal (Green
et al., 2014b), the phrase-based MT system that sup-
ports the interface. Phrasal follows the log-linear
approach to phrase-based translation (Och and Ney,
2004) in which the decision rule has the familiar
linear form
e? = arg max
e
w
>
?(e, f) (1)
1
The classic TransType system included a probabilistic
prediction length component (Foster et al., 2002), but we find
that the simpler projection technique works well in practice.
1227
where w ? R
d
is the model weight vector and
?(?) ? R
d
is a feature map.
3.1 Decoding
The default Phrasal search algorithm is cube prun-
ing (Huang and Chiang, 2007). In the post-edit con-
dition, search is executed as usual for each source
input, and the 1-best output is inserted into the tar-
get textbox. However, in interactive mode, the full
search algorithm is executed each time the user
modifies the partial translation. Machine sugges-
tions e? must match user prefix h. Define indicator
function pref(e?, h) to return true if e? begins with
h, and false otherwise. Eq. 1 becomes:
e? = arg max
e s.t.pref(e,h)
w
>
?(e, f) (2)
Cube pruning can be straightforwardly modified to
satisfy this constraint by simple string matching of
candidate translations. Also, the pop limit must be
suspended until at least one legal candidate appears
on each beam, or the priority queue of candidates is
exhausted. We call this technique prefix decoding.
2
There is another problem. Human translators are
likely to insert unknown target words, including
new vocabulary, misspellings, and typographical
errors. They might also reorder source text so as to
violate the phrase-based distortion limit. To solve
these problems, we perform dynamic phrase table
augmentation, adding new synthetic rules specific
to each search. Rules allowing any source word to
align with any unseen or ungeneratable (due to the
distortion limit) target word are created.
3
These
synthetic rules are given rule scores lower than any
other rules in the set of queried rules for that source
input f . Then candidates are allowed to compete
on the beam. Candidates with spurious alignments
will likely be pruned in favor of those that only turn
to synthetic rules as a last resort.
3.2 Tuning
We choose BLEU (Papineni et al., 2002) for base-
line tuning to independent references, and HTER
for re-tuning to human corrections. Our rationale
is as follows: Cer et al. (2010) showed that BLEU-
tuned systems score well across automatic metrics
and also correlate with human judgment better than
2
Och et al. (2003) describe a similar algorithm for word
graphs.
3
Ortiz-Mart?nez et al. (2009) describe a related technique
in which all source and target words can align, with scores set
by smoothing.
systems tuned to other metrics. Conversely, sys-
tems tuned to edit-distance-based metrics like TER
tend to produce short translations that are heavily
penalized by other metrics.
When human corrections become available, we
switch to HTER, which correlates with human judg-
ment and is an interpretable measure of editing
effort. Whereas TER is computed as TER(e, e?),
HTER is HTER(h, e?). HBLEU is an alternative,
but since BLEU is invariant to some permutations
(Callison-Burch et al., 2006), it is less interpretable.
We find that it also does not work as well in practice.
We previously proposed a fast, online tuning al-
gorithm (Green et al., 2013b) based on AdaGrad
(Duchi et al., 2011). The default loss function is
expected error (EE) (Och, 2003; Cherry and Foster,
2012). Expected BLEU is an example of EE, which
we found to be unstable when switching metrics.
This may result from direct incorporation of the
error metric into the gradient computation.
To solve this problem, we propose a cross-
entropy loss which, to our knowledge, is new in
MT. Let
?
E = {e?
i
}
n
i=1
be an n-best list ranked
by a gold metric G(e, e?) ? 0. Assume we
have a preference of a higher G (e.g., BLEU or
1?HTER). Define the model distribution over
?
E
as q(e?|f) ? exp[w
>
?(e?, f)] normalized so that
?
e??
?
E
q(e?|f) = 1; q indicates howmuch the model
prefers each translation. Similarly, define p(e?|f)
based on any function of the gold metric so that
?
e??
?
E
p(e?|f) = 1; p indicates how much the met-
ric prefers each translation. We choose a DCG-
style
4
parameterization that skews the p distribu-
tion toward higher-ranked items on the n-best list:
p(e?
i
|f) ? G(e, e?
i
)/ log(1 + i) for the ith ranked
item. The cross-entropy (CE) loss function is:
`
CE
(w;E) = E
p(e?|f)
[? log(q(e?|f)] (3)
It turns out that if p is simply the posterior distribu-
tion of the metric, then this loss is related to the log
of the standard EE loss:
5
`
EE
(w;E) = ? log[E
p(e?|f)
[q(e?|f)]] (4)
We can show that `
CE
? `
EE
by applying Jensen?s
inequality to the function ? log(?). So minimizing
`
CE
also minimizes a convex upper bound of the
log expected error. This convexity given the n-
4
Discounted cumulative gain (DCG) is widely used in infor-
mation retrieval learning-to-rank settings. n-best MT learning
is standardly formulated as a ranking task.
5
For expected error, p(e?
i
) = G(e, e?
i
) is not usually nor-
malized. Normalizing p adds a negligible constant.
1228
best list does not mean that the overall MT tuning
loss is convex, since the n-best list contents and
order depend on the parameters w. However, all
regret bounds and other guarantees of online con-
vex optimization would now apply in the CE case
since `
CE,t
(w
t?1
;E
t
) is convex for each t. This
is attractive compared to expected error, which is
non-convex even given the n-best list. We empiri-
cally observed that CE converges faster and is less
sensitive to hyperparameters than EE.
Faster decoding trick We found that online tun-
ing also permits a trick that speeds up decoding
during deployment. Whereas the Phrasal default
beam size is 1,200, we were able to reduce the beam
size to 800 and run the tuner longer to achieve the
same level of translation quality. For example, at
the default beam size for French-English, the algo-
rithm converges after 12 iterations, whereas at the
lower beam size it achieves that level after 20 itera-
tions. In our experience, batch tuning algorithms
seem to be more sensitive to the beam size.
3.3 Feature Templates
The baseline system contains 19 dense feature tem-
plates: the nine Moses (Koehn et al., 2007) baseline
features, the eight-feature hierarchical lexicalized
re-ordering model of Galley and Manning (2008),
the (log) count of each rule in the bitext, and an
indicator for unique rules. We found that sparse
features, while improving translation quality, came
at the cost of slower decoding due to feature extrac-
tion and inner products with a higher dimensional
feature map ?. During prototyping, we observed
that users found the system to be sluggish unless
it responded in approximately 300ms or less. This
budget restricted us to dense features.
When re-tuning to corrections, we extract fea-
tures from the user logs u and add them to the
baseline dense model. For each tuning input f ,
the MT system produces candidate derivations d =
(f, e?, a), where a is a word alignment. The user log
u also contains the last MT derivation
6
accepted
by the user d
u
= (f, e?
u
, a
u
). We extract features
by comparing d and d
u
. The heuristic we take is
intersection: ?(d)? ?(d) ? ?(d
u
).
Lexicalized and class-based alignments Con-
sider the alignment in Figure 3. We find that
user derivations often contain many unigram rules,
6
Extracting features from intermediate user editing actions
is an interesting direction for future work.
tarceva
parvient
ainsi
?
stopper
la
croissance
t
a
r
c
e
v
a
w
a
s
t
h
u
s
a
b
l
e
t
o
h
a
l
t
t
h
e
g
r
o
w
t
h
Figure 3: User translation word alignment obtained
via prefix decoding and dynamic phrase table aug-
mentation.
which are less powerful than larger phrases, but
nonetheless provide high-precision lexical choice
information. We fire indicators for both unigram
links and multiword cliques. We also fire class-
based versions of this feature.
Source OOV blanket Source OOVs are usually
more frequent when adapting to a new domain. In
the case of European languages?our experimental
setting?many of the words simply transfer to the
target, so the issue is where to position them. In Fig-
ure 3, the proper noun tarceva is unknown, so the de-
coder OOV model generates an identity translation
rule. We add features in which the source word is
concatenated with the left, right, and left/right con-
texts in the target, e.g., {<s>-tarceva, tarceva-
was, <s>-tarceva-was}. We also add versions
with target words mapped to classes.
3.4 Differences from Previous Work
Our backend innovations support the UI and enable
feature-based learning from human corrections. In
contrast, most previous work on incremental MT
learning has focused on extracting new translation
rules, language model updating, and modifying
translation model probabilities (see: Denkowski
et al. (2014a)). We regard these features as ad-
ditive to our own work: certainly extracting new,
unseen rules should help translation in a new do-
main. Moreover, to our knowledge, all previous
work on updating the weight vector w has consid-
ered simulated post-editing, in which the indepen-
dent references e are substituted for corrections h.
Here we extract features from and re-tune to actual
corrections to the baseline MT output.
1229
4 Translation User Study
We conducted a human translation experiment with
a 2 (translation conditions) ? n (source sentences)
mixed design, where n depended on the language
pair. Translation conditions (post-edit and interac-
tive) and source sentences were the independent
variables (factors). Experimental subjects saw all
factor levels, but not all combinations, since one
exposure to a sentence would influence another.
Subjects completed the experiment remotely on
their own hardware. They received personalized
login credentials for the translation interface, which
administered the experiment. Subjects first com-
pleted a demographic questionnaire about prior ex-
perience with CAT and language proficiency. Next,
they completed a training module that included a
4-minute tutorial video and a practice ?sandbox? for
developing proficiency with the UI. Then subjects
completed the translation experiment. Finally, they
completed an exit questionnaire.
Unlike the experiment of Koehn (2009a), sub-
jects were under time pressure. An idle timer pre-
vented subjects from pausing for more than three
minutes while the translator interface was open.
This constraint eliminates a source of confound in
the timing analysis.
We randomized the order of translation condi-
tions and the assignment of sentences to conditions.
At most five sentences appeared per screen, and
those sentences appeared in the source document
order. Subjects could move among sentences within
a screen, but could not revise previous screens. Sub-
jects received untimed breaks both between trans-
lation conditions and after about every five screens
within a translation condition.
4.1 Linguistic Materials
We chose two language pairs: French-English (Fr-
En) and English-German (En-De). Anecdotally,
French-English is an easy language pair for MT,
whereas English-German is very hard due to re-
ordering and complex German morphology.
We chose three text genres: software, medical,
and informal news. The software text came from
the graphical interfaces of Autodesk AutoCAD and
Adobe Photoshop. The medical text was a drug re-
view from the European Medicines Agency. These
two data sets came from TAUS
7
and included inde-
pendent reference translations. The informal news
text came from the WMT 2013 shared task test set
7http://www.tausdata.org/
(Bojar et al., 2013). The evaluation corpus was con-
structed from equal proportions of the three genres.
The Fr-En dataset contained 3,003 source tokens
(150 segments); the En-De dataset contained 3,002
(173 segments). As a rule of thumb, a human trans-
lator averages about 2,700 source tokens per day
(Ray, 2013, p.36), so the experiment was designed
to replicate a slightly demanding work day.
4.2 Selection of Subjects
For each language pair, we recruited 16 profes-
sional, freelance translators on Proz, which is the
largest online translation community.
8
We posted
ads for both language pairs at a fixed rate of $0.085
per source word, an average rate in the industry. In
addition, we paid $10 to each translator for complet-
ing the training module. All subjects had significant
prior experience with a CAT workbench.
4.3 Results
We analyze the translation conditions in terms of
two response variables: time and quality. We ex-
cluded one Fr-En subject and two En-De subjects
from the models. One subject misunderstood the in-
structions of the experiment and proceeded without
clarification; another skipped the training module
entirely. The third subject had a technical problem
that prevented logging. Finally, we also filtered
segment-level sessions for which the log of transla-
tion time was greater than 2.5 standard deviations
from the mean.
4.3.1 Translation Time
We analyze time with a linear mixed effects model
(LMEM) estimated with the lme4 (Bates, 2007) R
package. When experimental factors are sampled
from larger populations?e.g., humans, sentences,
words?LMEMs are more robust to type II errors
(see: Baayen et al. (2008)). The log-transformed
time is the response variable and translation condi-
tion is the main independent variable. The maximal
random effects structure (Barr et al., 2013) contains
intercepts for subject, sentence id, and text genre,
each with random slopes for translation condition.
We found significant main effects for translation
condition (Fr-En, p < 0.05; En-De, p < 0.01).
The orientation of the coefficients indicates that
interactive is slower for both language pairs. For Fr-
En, the LMEM predicts a mean time (intercept) of
46.0 sec/sentence in post-edit vs. 54.6 sec/sentence
8http://www.proz.com
1230
Fr-En En-De
TER HTER TER HTER
post-edit 47.32 23.51 56.16 37.15
interactive 47.05 24.14 55.89 39.55
Table 1: Automatic assessment of translation qual-
ity. Here we change the definitions of TER and
HTER slightly. TER is the human translations com-
pared to the independent references. HTER is the
baseline MT compared to the human corrections.
in interactive, or 18.7% slower. For En-De, the
mean is 51.8 sec/sentence vs. 63.3 sec/sentence in
interactive, or 22.1% slower.
We found other predictive covariates that reveal
more about translator behavior. When subjects did
not edit the MT suggestion, they were significantly
faster. When token edit distance fromMT or source
input length increased, they were slower. Subjects
were usually faster as the experiment progressed, a
result that may indicate increased proficiency with
practice. Note that all subjects reported profes-
sional familiarity with post-edit, whereas the in-
teractive mode was entirely new to them. In the
exit survey many translators suggested that with
more practice, they could have been as fast in the
interactive mode.
9
4.3.2 Translation Quality
We evaluated translation quality with both auto-
matic and manual measures. Table 1 shows that
in the interactive mode, TER is lower and HTER
is higher: subjects created translations closer to
the references (lower TER), but performed more
editing (higher HTER). This result suggests better
translations in the interactive mode.
To confirm that intuition, we elicited judgments
from professional human raters. The setup followed
the manual quality evaluation of the WMT 2014
shared task (Bojar et al., 2014). We hired six raters?
three for each language pair?who were paid be-
tween $15?20 per hour. The raters logged into Ap-
praise (Federmann, 2010) and for each source seg-
ment, ranked five randomly selected translations.
From these 5-way rankings we extracted pairwise
judgments pi = {<,=}, where u
1
< u
2
indicates
that subject u
1
provided a better translation than
subject u
2
for a given source input (Table 2).
9
See (Green et al., 2014c) for significance levels of the
other covariates along with analysis of subject learning rates,
subject behavior, and qualitative feedback.
Fr-En En-De
#pairwise 14,211 15,001
#ties (=) 5,528 2,964
IAA 0.419 (0.357) 0.407 (0.427)
EW (inter.) 0.512 0.491
Table 2: Pairwise judgments for the manual qual-
ity assessment. Inter-annotator agreement (IAA)
? scores are measured with the official WMT14
script. For comparison, the WMT14 IAA scores
are given in parentheses. EW (inter.) is expected
wins of interactive according to Eq. (6).
Fr-En En-De
sign p sign p
ui (interactive) + ? ?
log edit distance ? ??? + ???
gender (female) ? + ?
log session order ? + ?
Table 3: LMEM manual translation quality results
for each fixed effect with contrast conditions for
binary predictors in (). The signs of the coefficients
can be interpreted as in ordinary regression. edit
distance is token-level edit distance from baseline
MT. session order is the order in which the subject
translated the sentence during the experiment. Sta-
tistical significance was computed with a likelihood
ratio test: ??? p < 0.001; ? p < 0.05.
In WMT the objective is to rank individual sys-
tems; here we need only compare interface condi-
tions. However, we should control for translator
variability. Therefore, we build a binomial LMEM
for quality. The model is motivated by the simple
and intuitive expected wins (EW) measure used at
WMT. Let S be the set of pairwise judgments and
wins(u
1
, u
2
) = |{(u
1
, u
2
, pi) ? S | pi = <}|. The
standard EW measure is:
e(u
1
) =
1
|S|
?
u
1
6=u
2
wins(u
1
, u
2
)
wins(u
1
, u
2
) + wins(u
2
, u
1
)
(5)
Sakaguchi et al. (2014) showed that, despite its sim-
plicity, Eq. (5) is nearly as effective as model-based
methods given sufficient high-quality judgments.
Since we care only about the two translation condi-
tions, we reinterpret the u
i
as interface conditions,
i.e., u
1
= int and u
2
= pe. We can then disregard
1231
the normalizing term to obtain:
e(u
1
) =
wins(u
1
, u
2
)
wins(u
1
, u
2
) + wins(u
2
, u
1
)
(6)
which is the expected value of a Bernoulli distribu-
tion (so e(u
2
) = 1 ? e(u
1
)). The intercept-term
of the binomial LMEM will be approximately this
value subject to other fixed and random effects.
To estimate the model, we convert each pairwise
judgment u
1
< u
2
to two examples where the re-
sponse is 1 for u
1
and 0 for u
2
. We add the fixed
effects shown in Table 3, where the numeric effects
are centered and scaled by their standard deviations.
The maximal random effects structure contains in-
tercepts for sentence id nested within subject along
with random slopes for interface condition.
Table 3 shows the p-values and coefficient orien-
tations. The models yield probabilities that can be
interpreted like Eq. (6) but with all fixed predictors
set to 0. For Fr-En, the value for post-edit is 0.472
vs. 0.527 for interactive. For En-De, post-edit is
0.474 vs. 0.467 for interactive. The difference is
statistically significant for Fr-En, but not for En-De.
When MT quality was anecdotally high (Fr-En),
high token-level edit distance from the initial sug-
gestion decreased quality. When MT was poor (En-
De), significant editing improved quality. Female
En-De translators were better than males, possibly
due to imbalance in the subject pool (12 females vs.
4 males). En-De translators seemed to improve with
practice (positive coefficient for session order).
The Fr-En results are the first showing an inter-
active UI that improves over post-edit.
5 MT Re-tuning Experiment
The human translators corrected the output of the
BLEU-tuned, baseline MT system. No updating of
the MT system occurred during the experiment to
eliminate a confound in the time and quality analy-
ses. Now we investigate re-tuning the MT system
to the corrections by simply re-starting the online
learning algorithm from the baseline weight vector
w, this time scoring with HTER instead of BLEU.
Conventional incremental MT learning experi-
ments typically resemble domain adaptation: small-
scale baselines are trained and tuned on mostly out-
of-domain data, and then re-tuned incrementally
on in-domain data. In contrast, we start with large-
scale systems. This is more consistent with a pro-
fessional translation environment where translators
receive suggestions from state-of-the-art systems
like Google Translate.
Bilingual Monolingual
#Segments #Tokens #Tokens
En-De 4.54M 224M 1.7B
Fr-En 14.8M 842M 2.24B
Table 4: Gross statistics of MT training corpora.
En-De Fr-En
baseline-tune 9,469 8,931
baseline-dev 9,012 9,030
int-tune 680 589
int-test 457 368
pe-tune 764 709
pe-test 492 447
Table 5: Tuning, development, and test corpora
(#segments). tune and dev were used for baseline
system preparation. Re-tuning was performed on
int-tune and pe-tune, respectively. We report held-
out results on the two test data sets. All sets are
supplied with independent references.
5.1 Datasets
Table 4 shows the monolingual and parallel train-
ing corpora. Most of the data come from the con-
strained track of the WMT 2013 shared task (Bojar
et al., 2013). We also added 61k parallel segments
of TAUS data to the En-De bitext, and 26k TAUS
segments to the Fr-En bitext. We aligned the par-
allel data with the Berkeley Aligner (Liang et al.,
2006) and symmetrized the alignments with the
grow-diag heuristic. For each target language we
used lmplz (Heafield et al., 2013) to estimate unfil-
tered, 5-gram Kneser-Ney LMs from the concate-
nation of the target side of the bitext and the mono-
lingual data. For the class-based features, we esti-
mated 512-class source and target mappings with
the algorithm of Green et al. (2014a).
The upper part of Table 5 shows the baseline
tuning and development sets, which also contained
1/3 TAUS medical text, 1/3 TAUS software text,
and 1/3 WMT newswire text (see section 4).
The lower part of Table 5 shows the organization
of the human corrections for re-tuning and testing.
Recall that for each unique source input, eight hu-
man translators produced a correction in each con-
dition. First, we filtered all corrections for which a
log u was not recorded (due to technical problems).
Second, we de-duplicated the corrections so that
each h was unique. Finally, we split the unique
(f, h) tuples according to a natural division in the
1232
System tune BLEU? TER? HTER
baseline bleu 23.12 60.29 44.05
re-tune hter 22.18 60.85 43.99
re-tune+feat hter 21.73 59.71 42.35
(a) En-De int-test results.
System tune BLEU? TER? HTER
baseline bleu 39.33 45.29 28.28
re-tune hter 39.99 45.73 26.96
re-tune+feat hter 40.30 45.28 26.40
(b) Fr-En int-test results.
Table 6: Main re-tuning results for interactive
data. baseline is the BLEU-tuned system used
in the translation user study. re-tune is the base-
line feature set re-tuned to HTER on int-tune. re-
tune+feat adds the human feature templates de-
scribed in section 3.3. bold indicates statistical
significance relative to the baseline at p < 0.001;
italic at p < 0.05 by the permutation test of Riezler
and Maxwell (2005).
data. There were five source segments per docu-
ment, and each document was rendered as a single
screen during the translation experiment. Segment
order was not randomized, so we could split the
data as follows: assign the first three segments of
each screen to tune, and the last two to test. This is
a clean split with no overlap.
This tune/test split has two attractive properties.
First, if we can quickly re-tune on the first few sen-
tences on a screen and provide better translations
for the last few, then presumably the user experience
improves. Second, source inputs f are repeated?
eight translators translated each input in each condi-
tion. This means that a reduction in HTER means
better average suggestions for multiple human trans-
lators. Contrast this experimental design with tun-
ing to the corrections of a single human translator.
There the system might overfit to one human style,
and may not generalize to other human translators.
5.2 Results
Table 6 contains the main results for re-tuning to in-
teractive MT corrections. For both language pairs,
we observe large statistically significant reductions
inHTER.However, the results for BLEU and TER?
which are computed with respect to the independent
references?are mixed. The lower En-De BLEU
score is explained by a higher brevity penalty for
the re-tuned output (0.918 vs. 0.862). However, the
re-tuned 4-gram and 3-gram precisions are signif-
System HTER? System HTER?
int pe
baseline 44.05 baseline 41.05
re-tune (int) 43.99 re-tune (pe) 40.34
re-tune+feat 42.35 ? ?
? ?1.80 ?0.71
Table 7: En-De test results for re-tuning to post-edit
(pe) vs. interactive (int). Features cannot be ex-
tracted from the post-edit data, so the re-tune+feat
system cannot be learned. The Fr-En results are
similar but are omitted due to space.
icantly higher. The unchanged Fr-En TER value
can be explained by the observation that no human
translators produced TER scores higher than the
baselineMT. This odd result has also been observed
for BLEU (Culy and Riehemann, 2003), although
here we do observe a slight BLEU improvement.
The additional features (854 for Fr-En; 847 for
En-De) help significantly and do not slow down
decoding. We used the same L
1
regularization
strength as the baseline, but feature growth could
be further constrained by increasing this parame-
ter. Tuning is very fast at about six minutes for the
whole dataset, so tuning during a live user session
is already practical.
Table 7 compares re-tuning to interactive vs.
post-edit corrections. Recall that the int-test and
pe-test datasets are different and contain different
references. The post-edit baseline is lower because
humans performed less editing in the baseline con-
dition (see Table 1). Features account for the great-
est reduction in HTER. Of course, the features are
based mostly on word alignments, which could be
obtained for the post-edit data by running an online
word alignment tool (see: Farajian et al. (2014)).
However, the interactive logs contain much richer
user state information that we could not exploit due
to data sparsity. We also hypothesize that the fi-
nal interactive corrections might be more useful
since suggestions prime translators (Green et al.,
2013a), and the MT system was able to refine its
suggestions.
6 Re-tuning Analysis
Tables 6 and 7 raise two natural questions: what
accounts for the reduction in HTER, and why are
the TER/BLEU results mixed? Comparison of the
BLEU-tuned baseline to the HTER re-tuned sys-
tems gives some insight. For both questions, fine-
1233
grained corrections appear to make the difference.
Consider this French test example (with gloss):
(1) une
one
ligne
line
de
of
chimioth?rapie
chemotherapy
ant?rieure
previous
The independent reference for une ligne de chimio-
th?rapie is ?previous chemotherapy treatment?, and
the baseline produces ?previous chemotherapy line.?
The source sentence appears seven times with the
following user translations: ?one line or more
of chemotherapy?, ?one prior line of chemother-
apy?, ?one previous line of chemotherapy? (2), ?one
line of chemotherapy before? (2), ?one protocol of
chemotherapy?. The re-tuned, feature-based sys-
tem produces ?one line of chemotherapy before?,
matching two of the humans exactly, and six of the
humans in terms of idiomatic medical jargon (?line
of chemotherapy? vs. ?chemotherapy treatment?).
However, the baseline output would have received
better BLEU and TER scores.
Sometimes re-tuning improves the translations
with respect to both the reference and the human
corrections. This English phrase appears in the
En-De test set:
(2) depending
abh?ngig
on
von
the
der
file
datei
The baseline produces exactly the gloss shown in Ex.
(2). The human translators produced: ?je nach datei?
(6), ?das dokument?, and ?abh?ngig von der datei?.
The re-tuned system rendered the phrase ?je nach
dokument?, which is closer to both the independent
reference ?je nach datei? and the human corrections.
This change improves TER, BLEU, and HTER.
7 Related Work
The process study most similar to ours is that of
Koehn (2009a), who compared scratch, post-edit,
and simple interactive modes. However, he used un-
dergraduate, non-professional subjects, and did not
consider re-tuning. Our experimental design with
professional bilingual translators follows our previ-
ous work Green et al. (2013a) comparing scratch
translation to post-edit.
Many research translation UIs have been pro-
posed including TransType (Langlais et al., 2000),
Caitra (Koehn, 2009b), Thot (Ortiz-Mart?nez and
Casacuberta, 2014), TransCenter (Denkowski et
al., 2014b), and CasmaCat (Alabau et al., 2013).
However, to our knowledge, none of these inter-
faces were explicitly designed according to mixed-
initiative principles from the HCI literature.
Incremental MT learning has been investigated
several times, usually starting from no data (Bar-
rachina et al., 2009; Ortiz-Mart?nez et al., 2010),
via simulated post-editing (Mart?nez-G?mez et al.,
2012; Denkowski et al., 2014a), or via re-ranking
(W?schle et al., 2013). No previous experiments
combined large-scale baselines, full re-tuning of
the model weights, and HTER optimization.
HTER tuning can be simulated by re-
parameterizing an existing metric. Snover et
al. (2009) tuned TERp to correlate with HTER,
while Denkowski and Lavie (2010) did the same
for METEOR. Zaidan and Callison-Burch (2010)
showed how to solicit MT corrections for HTER
from Amazon Mechanical Turk.
Our learning approach is related to coactive learn-
ing (Shivaswamy and Joachims, 2012). Their basic
preference perceptron updates toward a correction,
whereas we use the correction for metric scoring
and feature extraction.
8 Conclusion
We presented a new CAT interface that supports
post-edit and interactive modes. Evaluation with
professional, bilingual translators showed post-edit
to be faster, but prior subject familiarity with post-
edit may have mattered. For French-English, the
interactive mode enabled higher quality translation.
Re-tuning the MT system to interactive corrections
also yielded large HTER gains. Technical contri-
butions that make re-tuning possible are a cross-
entropy objective, prefix decoding, and dynamic
phrase table augmentation. Larger quantities of cor-
rections should yield further gains, but our current
experiments already establish the feasibility of Bar-
Hillel?s virtuous ?machine-post-editor partnership?
which benefits both humans and machines.
Acknowledgements
We thank TAUS for access to their data reposi-
tory. We also thank John DeNero, Chris Dyer,
Alon Lavie, and Matt Post for helpful conversa-
tions. The first author is supported by a National
Science Foundation Graduate Research Fellowship.
This work was also supported by the Defense Ad-
vanced Research Projects Agency (DARPA) Broad
Operational Language Translation (BOLT) program
through IBM. Any opinions, findings, and conclu-
sions or recommendations expressed are those of
the author(s) and do not necessarily reflect the view
of either DARPA or the US government.
1234
References
V. Alabau, R. Bonk, C. Buck, M. Carl, F. Casacuberta,
M. Garc?a-Mart?nez, et al. 2013. Advanced com-
puter aided translation with a web-based workbench.
In 2nd Workshop on Post-Editing Technologies and
Practice.
R.H. Baayen, D.J. Davidson, and D.M. Bates. 2008.
Mixed-effects modeling with crossed random effects
for subjects and items. Journal of Memory and Lan-
guage, 59(4):390?412.
Y. Bar-Hillel. 1960. The present status of automatic
translation of languages. Advances in Computers,
1:91?163.
D. J. Barr, R. Levy, C. Scheepers, and H. J. Tily. 2013.
Random effects structure for confirmatory hypothe-
sis testing: Keep it maximal. Journal of Memory
and Language, 68(3):255?278.
S. Barrachina, O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, et al. 2009. Statistical ap-
proaches to computer-assisted translation. Compu-
tational Linguistics, 35(1):3?28.
D. M. Bates. 2007. lme4: Linear mixed-
effects models using S4 classes. Technical re-
port, R package version 1.1-5, http://cran.r-
project.org/package=lme4.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, et al. 2013. Findings of the
2013 Workshop on Statistical Machine Translation.
In WMT.
O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn,
J. Leveling, et al. 2014. Findings of the 2014 Work-
shop on Statistical Machine Translation. In WMT.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine transla-
tion research. In EACL.
M. Carl. 2010. A computational framework for a cogni-
tive model of human translation processes. In Aslib
Translating and the Computer Conference.
D. Cer, C. D.Manning, and D. Jurafsky. 2010. The best
lexical metric for phrase-based statistical MT system
optimization. In NAACL.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In NAACL.
C. Culy and S. Z. Riehemann. 2003. The limits of n-
gram translation evaluation metrics. In MT Summit
IX.
M. Denkowski and A. Lavie. 2010. Extending the ME-
TEOR machine translation evaluation metric to the
phrase level. In NAACL.
M. Denkowski, C. Dyer, and A. Lavie. 2014a. Learn-
ing from post-editing: Online model adaptation for
statistical machine translation. In EACL.
M. Denkowski, A. Lavie, I. Lacruz, and C. Dyer.
2014b. Real time adaptive machine translation for
post-editing with cdec and TransCenter. In Work-
shop on Humans and Computer-assisted Translation.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
M. A. Farajian, N. Bertoldi, and M. Federico. 2014.
Online word alignment for online adaptive machine
translation. InWorkshop on Humans and Computer-
assisted Translation.
C. Federmann. 2010. Appraise: An open-source
toolkit for manual phrase-based evaluation of trans-
lations. In LREC.
G. Foster, P. Langlais, and G. Lapalme. 2002. User-
friendly text prediction for translators. In EMNLP.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, J. Heer, and C. D. Manning. 2013a. The effi-
cacy of human post-editing for language translation.
In CHI.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
S. Green, D. Cer, and C. D. Manning. 2014a. An em-
pirical comparison of features and tuning for phrase-
based machine translation. In WMT.
S. Green, D. Cer, and C. D. Manning. 2014b. Phrasal:
A toolkit for new directions in statistical machine
translation. In WMT.
S. Green, J. Chuang, J. Heer, andC.D.Manning. 2014c.
Predictive Translation Memory: A mixed-initiative
system for human language translation. In UIST.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
ACL.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Koehn. 2009a. A process study of computer-aided
translation. Machine Translation, 23:241?263.
P. Koehn. 2009b. A web-based interactive computer
aided translation tool. In ACL-IJCNLP, Software
Demonstrations.
1235
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Langlais, G. Foster, and G. Lapalme. 2000.
TransType: a computer-aided translation typing sys-
tem. In Workshop on Embedded Machine Transla-
tion Systems.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C. Manning, M. Surdeanu, J. Bauer, J. Finkel,
S. Bethard, and D. McClosky. 2014. The Stanford
CoreNLP natural language processing toolkit. In
ACL, System Demonstrations.
P. Mart?nez-G?mez, G. Sanchis-Trilles, and F. Casacu-
berta. 2012. Online adaptation strategies for sta-
tistical machine translation in post-editing scenarios.
Pattern Recognition, 45(9):3193?3203.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och, R. Zens, and H. Ney. 2003. Efficient
search for interactive statistical machine translation.
In EACL.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
D. Ortiz-Mart?nez and F. Casacuberta. 2014. The new
Thot toolkit for fully automatic and interactive statis-
tical machine translation. In EACL, System Demon-
strations.
D. Ortiz-Mart?nez, I. Garc?a-Varea, and F. Casacuberta.
2009. Interactive machine translation based on par-
tial statistical phrase-based alignments. In RANLP.
D. Ortiz-Mart?nez, I. Garc?a-Varea, and F. Casacuberta.
2010. Online learning for interactive statistical ma-
chine translation. In NAACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
R. Ray. 2013. Ten essential research findings for 2013.
In 2013 Resource Directory & Index. Multilingual.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
K. Sakaguchi, M. Post, and B. Van Durme. 2014. Effi-
cient elicitation of annotations for human evaluation
of machine translation. In WMT.
P. Shivaswamy and T. Joachims. 2012. Online struc-
tured prediction via coactive learning. In ICML.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric.
In WMT.
K. W?schle, P. Simianer, N. Bertoldi, S. Riezler, and
M. Federico. 2013. Generative and discriminative
methods for online adaptation in SMT. In MT Sum-
mit XIV.
O. F. Zaidan and C. Callison-Burch. 2010. Predicting
human-targeted translation edit rate via untrained hu-
man annotators. In NAACL.
1236
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 79?82,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Concurrent Visualization of Relationships between Words and Topics in
Topic Models
Alison Smith
?
, Jason Chuang
?
, Yuening Hu
?
, Jordan Boyd-Graber
?
, Leah Findlater
?
?
University of Maryland, College Park, MD
?
University of Washington, Seattle, WA
amsmit@cs.umd.edu, jcchuang@cs.washington.edu, ynhu@cs.umd.edu, jbg@umiacs.umd.edu, leahkf@umd.edu
Abstract
Analysis tools based on topic models are
often used as a means to explore large
amounts of unstructured data. Users of-
ten reason about the correctness of a model
using relationships between words within
the topics or topics within the model. We
compute this useful contextual informa-
tion as term co-occurrence and topic co-
variance and overlay it on top of stan-
dard topic model output via an intuitive
interactive visualization. This is a work
in progress with the end goal to combine
the visual representation with interactions
and online learning, so the users can di-
rectly explore (a) why a model may not
align with their intuition and (b) modify
the model as needed.
1 Introduction
Topic modeling is a popular technique for analyz-
ing large text corpora. A user is unlikely to have
the time required to understand and exploit the raw
results of topic modeling for analysis of a corpus.
Therefore, an interesting and intuitive visualiza-
tion is required for a topic model to provide added
value. A common topic modeling technique is La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003),
which is an unsupervised algorithm for perform-
ing statistical topic modeling that uses a ?bag of
words? approach. The resulting topic model repre-
sents the corpus as an unrelated set of topics where
each topic is a probability distribution over words.
Experienced users who have worked with a text
corpus for an extended period of time often think
of the thematic relationships in the corpus in terms
of higher-level statistics such as (a) inter-topic cor-
relations or (b) word correlations. However, stan-
dard topic models do not explicitly provide such
contextual information to the users.
Existing tools based on topic models, such
as Topical Guide (Gardner et al., 2010), Top-
icViz (Eisenstein et al., 2012), and the topic vi-
sualization of (Chaney and Blei, 2012) support
topic-based corpus browsing and understanding.
Visualizations of this type typically represent stan-
dard topic models as a sea of word clouds; the in-
dividual topics within the model are presented as
an unordered set of word clouds ? or something
similar ? of the top words for the topic
1
where
word size is proportional to the probability of the
word for the topic. A primary issue with word
clouds is that they can hinder understanding (Har-
ris, 2011) due to the fact that they lack information
about the relationships between words. Addition-
ally, topic model visualizations that display topics
in a random layout can lead to a huge, inefficiently
organized search space, which is not always help-
ful in providing a quick corpus overview or assist-
ing the user to diagnose possible problems with
the model.
The authors of Correlated Topic Models (CTM)
(Lafferty and Blei, 2006) recognize the limitation
of existing topic models to directly model the cor-
relation between topics, and present an alterna-
tive algorithm, CTM, which models the correla-
tion between topics discovered for a corpus by us-
ing a more flexible distribution for the topic pro-
portions in the model. Topical n-gram models
(TNG) (Wang et al., 2007) discover phrases in
addition to topics. TNG is a probabilistic model
which assigns words and n-grams based on sur-
rounding context, instead of for all references in
the corpus. These models independently account
for the two limitations of statistical topic modeling
discussed in this paper by modifying the underly-
ing topic modeling algorithm. Our work aims to
provide a low-cost method for incorporating this
1
This varies, but typically is either the top 10 to 20 words
or the number of words which hold a specific portion of the
distribution weight.
79
information as well as visualizing it in an effec-
tive way. We compute summary statistics, term
co-occurrence and topic covariance, which can be
overlaid on top of any traditional topic model. As
a number of application-specific LDA implemen-
tations exist, we propose a meta-technique which
can be applied to any underlying algorithm.
We present a relationship-enriched visualiza-
tion to help users explore topic models through
word and topic correlations. We propose inter-
actions to support user understanding, validation,
and refinement of the models.
2 Group-in-a-box Layout for Visualizing
a Relationship-Enriched Topic Model
Existing topic model visualizations do not eas-
ily support displaying the relationships between
words in the topics and topics in the model. In-
stead, this requires a layout that supports intuitive
visualization of nested network graphs. A group-
in-a-box (GIB) layout (Rodrigues et al., 2011) is a
network graph visualization that is ideal for our
scenario as it is typically used for representing
clusters with emphasis on the edges within and
between clusters. The GIB layout visualizes sub-
graphs within a graph using a Treemap (Shneider-
man, 1998) space filling technique and layout al-
gorithms for optimizing the layout of sub-graphs
within the space, such that related sub-graphs are
placed together spatially. Figure 1 shows a sample
group-in-a-box visualization.
We use the GIB layout to visually separate top-
ics of the model as groups. We implement each
topic as a force-directed network graph (Fruchter-
man and Reingold, 1991) where the nodes of the
graph are the top words of the topic. An edge ex-
ists between two words in the network graph if
the value of the term co-occurrence for the word
pair is above a certain threshold,
2
and the edge is
weighted by this value. Similarly, the edges be-
tween the topic clusters represent the topic covari-
ance metric. Finally, the GIB layout optimizes the
visualization such that related topic clusters are
placed together spatially. The result is a topic visu-
alization where related words are clustered within
the topics and related topics are clustered within
the overall layout.
2
There are a variety of techniques for setting this thresh-
old; currently, we aim to display fewer, stronger relationships
to balance informativeness and complexity of the visualiza-
tion
Figure 1: A sample GIB layout from (Rodrigues
et al., 2011). The layout visualizes clusters dis-
tributed in a treemap structure where the partitions
are based on the size of the clusters.
3 Relationship Metrics
We compute the term and topic relationship in-
formation required by the GIB layout as term
co-occurrence and topic covariance, respectively.
Term co-occurrence is a corpus-level statistic that
can be computed independently from the LDA al-
gorithm. The results of the LDA algorithm are re-
quired to compute the topic covariance.
3.1 Corpus-Level Term Co-Occurrence
Prior work has shown that Pointwise Mutual
Information (PMI) is the most consistent scor-
ing method for evaluating topic model coher-
ence (Newman et al., 2010). PMI is a statistical
technique for measuring the association between
two observations. For our purposes, PMI is used
to measure the correlation between each term pair
within each topic on the document level
3
. The
PMI is calculated for every possible term pair in
the ingested data set using Equation 1. The visu-
alization uses only the PMI for the term pairs for
the top terms for each topic, which is a small sub-
set of the calculated PMI values. Computing the
PMI is trivial compared to the LDA calculation,
and computing the values for all pairs allows the
job to be run in parallel, as opposed to waiting for
the results of the LDA job to determine the top
term pairs.
PMI(x, y) = log
p(x, y)
p(x)p(y)
(1)
The PMI measure represents the probability of
observing x given y and vice-versa. PMI can be
3
We use document here, but the PMI can be computed at
various levels of granularity as required by the analyst intent.
80
positive or negative, where 0 represents indepen-
dence, and PMI is at its maximum when x and y
are perfectly associated.
3.2 Topic Covariance
To quantify the relationship between topics in the
model, we calculate the topic covariance metric
for each pair of topics. To do this, we use the
theta vector from the LDA output. The theta vec-
tor describes which topics are used for which doc-
uments in the model, where theta(d,i) represents
how much the ith topic is expressed in document
d. The equations for calculation the topic covari-
ance are shown below.
?
d
i
=
?
d
i
?
j
(?
d
j
)
(2)
?
i
=
1
D
?
d
(?
d
i
) (3)
?(i, j) =
1
D
?
d
(?
d
i
? ?
i
)(?
d
j
? ?
j
)) (4)
4 Visualization
The visualization represents the individual topics
as network graphs where nodes represent terms
and edges represent frequent term co-occurrence,
and the layout of the topics represents topic co-
variance. The most connected topic is placed in
the center of the layout, and the least connected
topics are placed at the corners. Figure 2 shows
the visualization for a topic model generated for
a 1,000 document NSF dataset. As demonstrated
in Figure 3, a user can hover over a topic to see
the related topics
4
. In this example, the user has
hovered over the {visualization, visual, interac-
tive} topic, which is related to {user, interfaces},
{human, computer, interaction}, {design, tools},
and {digital, data, web} among others. Unlike
other topical similarity measures, such as cosine
similarity or a count of shared words, the topic co-
variance represents topics which are typically dis-
cussed together in the same documents, helping
the user to discover semantically similar topics.
On the topic level, the size of the node in the
topic network graph represents the probability of
the word given the topic. By mapping word proba-
bility to the area of the nodes instead of the height
4
we consider topics related if the topic co-occurrence is
above a certain pre-defined threshold.
Figure 2: The visualization utilizes a group-in-a-
box-inspired layout to represent the topic model as
a nested network graph.
of words, the resulting visual encoding is not af-
fected by the length of the words, a well-known
issue with word cloud presentations that can visu-
ally bias longer terms. Furthermore, circles can
overlap without affecting a user?s ability to visu-
ally separate them, and lead to more compact and
less cluttered visual layout. Hovering over a word
node highlights the same word in other topics as
shown in Figure 4.
This visualization is an alternative interface
for Interactive Topic Modeling (ITM) (Hu et al.,
2013). ITM presents users with topics that can be
modified as appropriate. Our preliminary results
show that topics containing highly-weighted sub-
clusters may be candidates for splitting, whereas
positively correlated topics are likely to be good
topics, which do not need to be modified. In fu-
ture work, we intend to perform an evaluation to
show that this visualization enhances quality and
efficiency of the ITM process.
To support user interactions required by the
ITM algorithm, the visualization has an edit mode,
which is shown in Figure 5. Ongoing work in-
cludes developing appropriate visual operations to
support the following model-editing operations:
1. Adding words to a topic
2. Removing words from a topic
3. Requiring two words to be linked within a
topic (must link)
4. Requiring two words to be forced into sepa-
rate topics (cannot link)
5 Conclusion and Future Work
The visualization presented here provides a novel
way to explore topic models with incorporated
81
Figure 3: The user has hovered over the most-
central topic in the layout, which is the most con-
nected topic. The hovered topic is outlined, and
the topic name is highlighted in turquoise. The
topic names of the related topics are also high-
lighted.
Figure 4: The visualization where the user has
hovered over a word of interest. The same word
is highlighted turquoise in other topics.
Figure 5: The edit mode for the visualization.
From this mode, the user can add words, remove
words, or rename the topic.
term and topic correlation information. This is a
work in progress with the end goal to combine the
visual representation with interactive topic mod-
eling to allow users to explore (a) why a model
may not align with their intuition and (b) modify
the model as needed. We plan to deploy the tool
on real-world domain users to iteratively refine the
visualization and evaluate it in ecologically valid
settings.
References
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet
allocation. Machine Learning Journal, 3:993?1022.
Allison June-Barlow Chaney and David M Blei. 2012. Visualizing topic mod-
els. In ICWSM.
Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and Eric Xing. 2012. Top-
icviz: interactive topic exploration in document collections. In CHI?12
Extended Abstracts, pages 2177?2182. ACM.
Thomas MJ Fruchterman and Edward M Reingold. 1991. Graph draw-
ing by force-directed placement. Software: Practice and experience,
21(11):1129?1164.
Matthew J Gardner, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric
Ringger, and Kevin Seppi. 2010. The topic browser: An interactive tool
for browsing topic models. In NIPS Workshop on Challenges of Data Vi-
sualization.
Jacon Harris. 2011. Word clouds considered harm-
ful. http://www.niemanlab.org/2011/10/
word-clouds-considered-harmful/.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2013.
Interactive topic modeling. Machine Learning, pages 1?47.
JD Lafferty and MD Blei. 2006. Correlated topic models. In NIPS, Proceed-
ings of the 2005 conference, pages 147?155. Citeseer.
David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Au-
tomatic evaluation of topic coherence. In HLT, pages 100?108. ACL.
Eduarda Mendes Rodrigues, Natasa Milic-Frayling, Marc Smith, Ben Shnei-
derman, and Derek Hansen. 2011. Group-in-a-box layout for multi-
faceted analysis of communities. In ICSM, pages 354?361. IEEE.
Ben Shneiderman. 1998. Treemaps for space-constrained visualization of hi-
erarchies.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams:
Phrase and topic discovery, with an application to information retrieval. In
ICDM, pages 697?702. IEEE.
82
