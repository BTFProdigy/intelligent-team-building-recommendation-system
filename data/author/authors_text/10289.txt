Proceedings of the 7th Workshop on Statistical Machine Translation, pages 114?119,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LORIA System for the WMT12 Quality Estimation Shared Task
Langlois David
langlois@loria.fr
Raybaud Sylvain
LORIA, Universite? de Lorraine
615 rue du Jardin Botanique
54602 Villers les Nancy, France
raybauds@loria.fr
Sma??li Kamel
smaili@loria.fr
Abstract
In this paper we present the system we sub-
mitted to the WMT12 shared task on Quality
Estimation. Each translated sentence is given
a score between 1 and 5. The score is ob-
tained using several numerical or boolean fea-
tures calculated according to the source and
target sentences. We perform a linear regres-
sion of the feature space against scores in the
range [1:5]. To this end, we use a Support Vec-
tor Machine. We experiment with two kernels:
linear and radial basis function. In our submis-
sion we use the features from the shared task
baseline system and our own features. This
leads to 66 features. To deal with this large
number of features, we propose an in-house
feature selection algorithm. Our results show
that a lot of information is already present in
baseline features, and that our feature selec-
tion algorithm discards features which are lin-
early correlated.
1 Introduction
Machine translation systems are not reliable enough
to be used directly. They can only be used to grasp
the general meaning of texts or help human transla-
tors. Confidence measures detect erroneous words
or sentences. Such information could be useful for
users to decide whether or not to post-edit translated
sentences (Specia, 2011; Specia et al, 2010) or se-
lect documents mostly correctly translated (Soricut
and Echihabi, 2010). Moreover, it is possible to use
confidence measures to compare outputs from dif-
ferent systems and to recommend the best one (He
et al, 2010). One can also imagine that confidence
measures at word-level could be also useful for a
machine translation system to automatically correct
parts of output: for example, a translation system
translates the source sentence, then, this output is
translated with another translation system (Simard
et al, 2007). This last step could be driven by confi-
dence measures.
In previous works (Raybaud et al, 2011; Raybaud
et al, 2009a; Raybaud et al, 2009b) we used state-
of-the-art features to predict the quality of a transla-
tion at sentence- and word-level. Moreover, we pro-
posed our own features based on previous works on
cross-lingual triggers (Lavecchia et al, 2008; Latiri
et al, 2011). We evaluated our work in terms of Dis-
crimination Error Trade-off, Equal Error Rate and
Normalised Mutual Information.
In this article, we compare the features used in the
shared task baseline system and our own features.
This leads to 66 features which will be detailed in
sections 3 and 4. We therefore deal with many fea-
tures. We used a machine learning approach to per-
form regression of the feature space against scores
given by humans. Machine learning algorithms may
not efficiently deal with high dimensional spaces.
Moreover, some features may be less discriminant
descriptors and then in some cases could add more
noise than information. That is why, in this article
we propose an in-house feature selection algorithm
to remove useless features.
The article is structured as follows. In Section 2,
we give an overview of our quality estimation sys-
tem. Then, in Sections 3 and 4, we describe the
features we experimented with. In section 6, we de-
scribe the algorithm we propose for feature selec-
114
tion. Then we give the results of several configura-
tions in Section 7.
2 Overview of our quality estimation
submission
Each translated sentence is assigned a score between
1 and 5. 5 means that the machine translation output
is perfectly clear and intelligible and 1 means that it
is incomprehensible. The score is calculated using
several numerical or boolean features extracted ac-
cording to the source and target sentences. We per-
form a regression of the feature space against [1 : 5].
3 The baseline features
The quality estimation shared task organizers pro-
vided a baseline system including several interesting
features. Among them, several are yet used in (Ray-
baud et al, 2011) but we give below a brief review
of the whole baseline features set1:
? Source and target sentences lengths: there is a
correlation between the sizes of source and tar-
get sentences.
? Average source token length: this is the average
number of letters of the words in the sentence.
We guess that this feature can be useful because
short words have more chance to be tool words.
? Language model likelihood of source and target
sentences: a source sentence with low likeli-
hood is certainly far from training corpus statis-
tics. There is a risk it is badly translated. A tar-
get sentence with low likelihood is not suitable
in terms of target language.
? Average number of occurrences of the words
within the target sentence: too many occur-
rences of the same word in the target sentence
may indicate a bad translation.
? Average number of translations per source
word in the sentence: for each word in the
source sentence, the feature indicates how
many words of the target sentence are indeed
translations of this word in the IBM1 table
(with probability higher than 0.2).
1Indeed, our system takes into input a set of features, and is
able to discard redundant features (see Section 6).
? Weighted average number of translations per
source word in the sentence: this feature is sim-
ilar to the previous one, but a frequent word is
given a low weight in the averaging.
? n-gram frequency based features: the baseline
system proposes to group the n-gram frequen-
cies into 4 quartiles. The features indicate how
many n-gram (unigram to trigram) in source
sentence are in quartiles 1 and 4. These fea-
tures indicate if the source sentence contains
n-grams relevant to the training corpus.
? Punctuation based features: there may exist a
correlation between punctuation of source and
target sentences. The count of punctuation
marks in both sentences may then be useful.
Overall, the baseline system proposes 17 features.
4 The LORIA features
In a previous work (Raybaud et al, 2011), we tested
several confidence measures. The Quality Measure
Task campaign constitutes a good opportunity for us
to compare our approach to others. We give below
a brief review of our features (we cite again features
which are yet presented in baseline features because
sometimes, we use a variant of them):
? lengths: three features are generated, lengths of
source and target sentences (already presented
in baseline features), and ratio of target over
source length
? n-gram based features (Duchateau et al, 2002):
each word in the source and target sentences
is given its 5-gram probability. Then, the
sentence-level score is the average of the scores
across all words in the sentence. There are 4
features: one for each language (source and tar-
get) and one for each direction (left-to-right and
right-to-left 5-gram).
? backoff n-gram based features: in the same
way, a score is assigned to a word according
to how many times the language model had to
back off in order to assign a probability to the
sequence (Uhrik and Ward, 1997). Here too,
word scores are averaged and we get 4 scores.
115
? averaged features: a common property of all n-
gram based and backoff based features is that a
word can get a low score if it is actually correct
but its neighbours are wrong. To compensate
for this phenomenon we took into account the
average score of the neighbours of the word be-
ing considered. More precisely, for every rele-
vant feature x. defined at word level we also
computed:
xleft. (wi) = x.(wi?2) ? x.(wi?1) ? x.(wi)
xcentred. (wi) = x.(wi?1) ? x.(wi) ? x.(wi+1)
xright. (wi) = x.(wi) ? x.(wi+1) ? x.(wi+2)
A sentence level feature is then calculated ac-
cording to the average of each new ?averaged
feature?.
? intra-lingual features: the intra-lingual score
of a word in a sentence is the average of the
mutual information between that word and the
other words in that sentence. Mutual informa-
tion is defined by:
I(w1, w2) = P (w1, w2)?log
(
P (w1, w2)
P (w1)P (w2)
)
(1)
The intra-lingual score of a sentence is the av-
erage of the intra-lingual scores of the words in
this sentence. There are two features, one for
each language.
? cross-lingual features: the cross-lingual score
of a word in a target sentence is the average of
the mutual information between that word and
the words in the source sentence. The cross-
lingual score of a target sentence is the average
of its constituents.
? IBM1 features: the score of the target sentence
is the average translation probability provided
by the IBM1 model.
? basic parser: this produces two scores, a bi-
nary flag indicating whether any bracketing in-
side the target sentence is correct, and one in-
dicating if the sentence ends with an end of
sentence symbol (period, colon, semi-colon,
question/exclamation/quotation mark, comma,
apostrophe, close parenthese)
? out-of-vocabulary: this generates two scores,
the number of out-of-vocabulary words in the
sentence, and the same one but normalized by
the length of the sentence. These scores are
used for both sides.
This leads to 49 features. A few ones are equiv-
alent to or are strongly correlated to baseline ones.
As we want to be able to integrate several sets of fea-
tures without prior knowledge, our system is able to
discard redundant features (see Section 6).
5 Regression
Our system predicts a score between 1 and 5 for each
test sentence. For that, we used the training corpus
to perform the linear regression of the input features
against scores given by humans. We used SVM al-
gorithm to perform this regression (LibSVM toolkit
(Chang and Lin, 2011)). We experimented two ker-
nels: linear, and radial basis function. For the radial
basis function, we used grid search to optimise pa-
rameters.
6 Feature Selection
We experimented with many features. Some of them
may be very poor predictors. Then, these features
may disturb the convergence of the training algo-
rithm of SVM. To prevent this drawback, we applied
an in-house feature selection algorithm. A feature
selection algorithm selects the most relevant features
by maximizing a criterion. Feature selection algo-
rithms can be divided into two classes: backward
and forward (Guyon and Elisseeff, 2003). Backward
algorithms remove useless features from a set. For-
ward algorithms start with an empty feature set and
insert useful features. We implemented a greedy
backward elimination algorithm for feature selec-
tion. It discards features until a quality criterion
stops to decrease. The criterion used is the Mean Av-
erage Error (MAE) calculated on the development
corpus:
MAE(s, r) =
?n
i=1 |si ? ri|
n
(2)
where s is the list of scores predicted by the sys-
tem, r is the list of scores given by experts, n is the
size of these lists.
The algorithm is described below:
116
Algorithm 1: Feature Selection algorithm
begin
Start with a set S of features
while two features in S are linearly
correlated (more than 0.999) do
discard one of them from S
Calculate MAE for S
repeat
DecreaseMax? 0
forall the feature f ? S do
S? ? S \ f
Calculate newMAE for S?
if MAE-newMAE>
DecreaseMax then
DecreaseMax?
MAE-newMAE
fchosen? f
if DecreaseMax> 0 then
S ? S\ fchosen
MAE? MAE-DecreaseMax
until DecreaseMax=0;
For calculating the MAE for a feature set, several
steps are necessary: performing the regression be-
tween the features and the expert scores on the train-
ing corpus, using this regression to predict the scores
on the development corpus, calculate the MAE be-
tween the predicted scores and the expert scores on
this development corpus.
7 Results
We used the data provided by the shared task
on Quality Estimation2, without additional corpus.
This data is composed of a parallel English-Spanish
training corpus. This corpus is made of the con-
catenation of europarl-v5 and news-commentary10
corpora (from WMT-2010), followed by tokeniza-
tion, cleaning (sentences with more than 80 tokens
removed) and truecasing. It has been used for base-
line models provided in the baseline package by the
shared task organizers. We used the same train-
ing corpus to train additional language models (for-
ward and backward 5-gram with kneyser-ney dis-
counting, obtained with the SRILM toolkit) and trig-
gers required for our features. For feature extrac-
2http://dl.dropbox.com/u/6447503/resources.tbz
tion, we used the files provided by the organizers:
1832 source english sentences, their translations by
the baseline translation system, and the score given
by humans to these translations. We split these files
into a training part (1000 sentences) and a develop-
ment part (832 sentences). We used the train part
to perform the regression between the features and
the scores. We used the development corpus to opti-
mise the parameters of the regression and for feature
selection. We did not use additional provided infor-
mation such as phrase alignment, word alignment,
word graph, etc.
Table 1 presents our results in terms of MAE
and Root Mean Squared Error (RMSE). MAE is de-
scribed in Formula 2, and RMSE is defined by:
RMSE(s, r) =
?
?n
i=1(si ? ri)2
n
(3)
Each line of Table 1 gives the performance for a
set of features. BASELINE+LORIA constitutes the
union of both features BASELINE (Section 3) and
LORIA (Section 4). the ?feature selection? column
indicates if feature selection algorithm is applied.
We experimented the SVM with two kernels: lin-
ear (LIN in Table 1) and radial basis function (RBF
in Table 1). As the radial basis function uses pa-
rameters, we proposed results with default values
(DEF) and with values optimised by grid search on
the development corpus (OPT). MAE and RMSE are
given for development corpus and for the test cor-
pus. This test corpus (and its reference scores given
by humans) is the one released for the shared 2012
task3. MAE and RMSE has been computed against
the scores given by humans to the translations in this
test corpus4.
The results show that the performance on devel-
opment corpus are always confirmed by those of the
test corpus. The BASELINE features alone achieve
already good performance, better than ours. Al-
though the differences are well inside the confidence
interval, the fusion of both sets outperforms slightly
the BASELINE. The feature selection algorithm al-
lows to gain 0.01 point. The gain is the same for
3https://github.com/lspecia/QualityEstimation/blob/master/
test set.tar.gz
4available at https://github.com/lspecia/QualityEstimation/
blob/master/test set.likert
117
the optimisation of the radial basis function param-
eters. Surprisingly, the linear kernel, simpler than
other kernels, yields the same performance as radial
basis function.
In addition to MAE and RMSE results, we stud-
ied the linear correlations between features: our ob-
jective is to check if BASELINE and LORIA com-
plement each other. We computed the linear cor-
relation between all features (BASELINE+LORIA).
This leads to 2145 values. Table 2 shows in line +/-
the number of features pairs which correlate with an
absolute score higher than thresholds 0.9, 0.8 or 0.7.
Among these pairs we give in line + the number of
pairs with positive correlation, and in line - the num-
ber of pairs with negative correlation. For lines +
and -, we give 4 numbers: number of pairs, num-
ber of LORIA-LORIA (e.g. the number of correla-
tions between a LORIA feature and another LORIA
feature) pairs, number of BASELINE-BASELINE
pairs, number of LORIA-BASELINE pairs. We re-
mark that only 6% of the pairs correlates (column
0.7, line +/-) and that the correlations are mostly be-
tween LORIA features. This last point is not sur-
prising because there are more LORIA features than
BASELINE ones. There are very few correlations
between LORIA and BASELINE features. We stud-
ied precisely the correlated pairs. There is a strong
(more than 0.9) positive correlation between n-gram
and backoff based features and their averaged fea-
ture versions. Sometimes, there is also a strong cor-
relation between ?forward? and ?backward? features.
Source and target sentences lengths linearly corre-
late (0.98). This is the same case for source and tar-
get language model likelihoods. There is also a high
correlation between forward and backward 5-gram
scores (0.89). There are very few negative correla-
tions between features. As they are not numerous,
one can list these pairs with correlation between -
1 and -0.7: target sentence length and target lan-
guage model probability; source sentence length and
source language model probability; ratio of OOV
words over sentence length in source sentence and
percentage of unigrams in the source sentence seen
in the SMT training corpus; and number of OOV
words in source sentence and percentage of uni-
grams in the source sentence seen in the SMT train-
ing corpus. These correlations are not surprising.
First, language model probability is not normalized
? 0.9 ? 0.8 ? 0.7
+/- 64 103 127
+ 56/49/3/4 94/87/3/4 117/105/6/6
- 8/0/4/4 9/0/4/5 10/0/4/6
Table 2: Statistics on the linear correlations between LO-
RIA+BASELINE features
by the number of tokens: the more tokens, the lower
probability. Second, the more OOV in the sentence,
the fewer known unigrams.
Last, we present the set of features discarded by
our feature selection algorithm. We give only this
description for the LORIA+BASELINE set, with
linear kernel. The algorithm discards 18 LORIA
features out of 49 (37%) and 3 BASELINE out of
17 (18%). The features discarded from LORIA are
mostly averaged features based on n-gram and back-
off. This is consistent with the fact that these fea-
tures are strongly correlated with n-gram and back-
off features. We remark that very few BASELINE
features are discarded: lengths of source and target
language because these features are yet included in
LORIA features, and ?average number of transla-
tions per source word in the sentence? maybe be-
cause the LORIA feature giving the average IBM1
probabilities is more precise. Last, we remark that
the target length feature is discarded, and only ratio
between target and source length is kept.
8 Conclusion
In this paper, we present our system to evaluate the
quality of machine translated sentences. A sentence
is given a score between 1 and 5. This score is pre-
dicted using a machine learning approach. We use
the training data provided by the organizers to per-
form the regression between numerical features cal-
culated from source and target sentences and scores
given by human experts. The features are the base-
line ones provided by the organizers and our own
features. We proposed a feature selection algorithm
to discard useless features. Our results show that
baseline features contain already the main part of in-
formation for prediction. Concerning our own fea-
tures, a study of the linear correlations shows that
averaged features do not provide new information
compared to n-gram and backoff features. This last
118
Dev Test
Set of features feature kernel MAE RMSE MAE RMSE
selection
BASELINE no RBF DEF 0.63 0.79 0.69 0.83
LORIA no RBF DEF 0.66 0.82 0.73 0.87
BASELINE+LORIA no RBF DEF 0.62 0.78 0.69 0.82
BASELINE+LORIA yes RBF DEF 0.61 0.77 0.69 0.83
BASELINE+LORIA no RBF OPT 0.62 0.77 0.68 0.82
BASELINE+LORIA no LIN 0.62 0.78 0.69 0.83
BASELINE+LORIA yes LIN 0.61 0.77 0.68 0.82
Table 1: Results of the various sets of features in terms of MAE and RMSE
remark is confirmed by our feature selection algo-
rithm. Our feature selection algorithm seems to dis-
card features linearly correlated with others while
keeping relevant features for prediction. Last, we
remark that the choice of kernel, optimisation of pa-
rameters and feature selection have not a strong ef-
fect on performance. The main effort may have to
be concentrated on features in the future.
References
C.-C. Chang and C.-J. Lin. 2011. LIBSVM:
A library for support vector machines. ACM
Transactions on Intelligent Systems and Tech-
nology, 2:27:1?27:27. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
J. Duchateau, K. Demuynck, and P. Wambacq. 2002.
Confidence scoring based on backward language mod-
els. In Proceedings of IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 221?224.
I. Guyon and A. Elisseeff. 2003. An introduction to vari-
able and feature selection. Journal of Machine Learn-
ing Research (Special Issue on Variable and Feature
Selection), pages 1157?1182.
Y. He, Y. Ma, J. van Genabith, and A. Way. 2010. Bridg-
ing SMT and TM with translation recommendation. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 622?630.
C. Latiri, K. Sma??li, C. Lavecchia, C. Nasri, and D. Lan-
glois. 2011. Phrase-based machine translation based
on text mining and statistical language modeling tech-
niques. In Proceedings of the 12th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics.
C. Lavecchia, K. Sma??li, and D. Langlois. 2008. Dis-
covering phrases in machine translation by simulated
annealing. In Proceedings of the Eleventh Interspeech
Conference.
S. Raybaud, C. Lavecchia, D. Langlois, and K. Sma??li.
2009a. New confidence measures for statistical ma-
chine translation. In Proceedings of the Interna-
tional Conference on Agents and Artificial Intelli-
gence, pages 61?68.
S. Raybaud, C. Lavecchia, D. Langlois, and K. Sma??li.
2009b. Word- and sentence-level confidence measures
for machine translation. In Proceedings of the 13th
Annual Conference of the European Association for
Machine Translation, pages 104?111.
S. Raybaud, D. Langlois, and K. Sma??li. 2011. ?This
sentence is wrong.? Detecting errors in machine-
translated sentences. Machine Translation, 25(1):1?
34.
M. Simard, N. Ueffing, P. Isabelle, and R. Kuhn. 2007.
Rule-based translation with statistical phrase-based
post-editing. In Proceedings of the ACL-2007 Work-
shop on Statistical Machine Translation (WMT-07),
pages 203?206.
R. Soricut and A. Echihabi. 2010. Trustrank: Inducing
trust in automatic translations via ranking. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 612?621.
L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2010.
Predicting machine translation adequacy. In Proceed-
ings of the Machine Translation Summit XIII, pages
612?621.
L. Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proceed-
ings of the 15th Conference of the European Associa-
tion for Machine Translation, pages 73?80.
C. Uhrik and W. Ward. 1997. Confidence metrics based
on n-gram language model backoff behaviors. In Fifth
European Conference on Speech Communication and
Technology, pages 2771?2774.
119
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 380?385,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LORIA System for the WMT13 Quality Estimation Shared Task
Langlois David
LORIA
(Universite? de Lorraine, INRIA, CNRS)
615 rue du Jardin Botanique,
54602 Villers les Nancy, France
david.langlois@loria.fr
Sma??li Kamel
LORIA
(Universite? de Lorraine, INRIA, CNRS)
615 rue du Jardin Botanique,
54602 Villers les Nancy, France
kamel.smaili@loria.fr
Abstract
In this paper we present the system we
submitted to the WMT13 shared task on
Quality Estimation. We participated in
the Task 1.1. Each translated sentence
is given a score between 0 and 1. The
score is obtained by using several numeri-
cal or boolean features calculated accord-
ing to the source and target sentences. We
perform a linear regression of the feature
space against scores in the range [0..1]. To
this end, we use a Support Vector Machine
with 66 features. In this paper, we propose
to increase the size of the training corpus.
For that, we use the post-edited and refer-
ence corpora during the training step. We
assign a score to each sentence of these
corpora. Then, we tune these scores on a
development corpus. This leads to an im-
provement of 10.5% on the development
corpus, in terms of Mean Average Error,
but achieves only a slight improvement on
the test corpus.
1 Introduction
In the scope of Machine Translation (MT), Qual-
ity Estimation (QE) is the task consisting to evalu-
ate the translation quality of a sentence or a docu-
ment. This process may be useful for post-editors
to decide or not to revise a sentence produced by
a MT system (Specia, 2011; Specia et al, 2010).
Moreover, it can be useful to decide if a translated
document can be broadcasted or not (Soricut and
Echihabi, 2010). The most obvious way to give a
score to a translated sentence consists in using a
machine learning approach. This approach is su-
pervised: experts are asked to score translated sen-
tences and with the obtained material, one learns a
prediction model of scores. The main drawback of
the machine learning approach is that it is super-
vised and requires huge data. To score a sentence
is time-consuming. Moreau et al in (Moreau and
Vogel, 2012) dealt with this issue by proposing un-
supervised similarity measures. In fact, the score
of a translated sentence is defined by a measure
giving the distance between it and the contents of
an external corpus. The authors improve the re-
sults of the supervised approach but this method
can be used only in the ranking task. Raybaud et
al. (Raybaud et al, 2011) proposed a method to
add errors in reference sentences (deletion, sub-
stitution, insertion). By this way, they build addi-
tional corpus in which each word can be associated
with a label correct/not correct. But, it is not pos-
sible to predict the translation quality of sentences
including these erroneous words.
In this paper, we propose to increase the size
of the training corpus. For that, we use the score
given by experts to evaluate additional sentences
from the post-edited and reference corpora. Practi-
cally, we extract from source and target sentences
numerical vectors (features) and we learn a pre-
diction model of the scores. Then, we apply this
model to predict the scores of the post-edited and
the reference sentences. And finally, we tune the
predicted scores on a development corpus.
The article is structured as follows. In Section
2, we give an overview of our machine learning
approach and of the features we use. Then, in Sec-
tions 3 and 4 we describe the corpora and how we
increase the size of the training corpus by a partly-
unsupervised approach. In section 5, we give re-
sults about this method and we end by a conclu-
sion and perspectives.
2 Overview of our quality estimation
submission
We submit a system for the task 1.1: one has to
evaluate each translated sentence with a score be-
tween 0 and 1. This score is read as the HTER be-
tween the translated sentence and its post-edited
version. Each translated sentence is assigned a
380
score between 0 and 1. The score is calculated
using several numerical or boolean features ex-
tracted according to the source and target sen-
tences. We perform a regression of the feature
space against [0..1]. To this end, we use the Sup-
port Vector Machine algorithm (LibSVM toolkit
(Chang and Lin, 2011)). We experimented only
the linear kernel because our experience from last
year (Langlois et al, 2012) showed that its perfor-
mance are yet good while no parameters have to
be tuned on a development corpus.
2.1 The baseline features
The QE shared task organizers provided a base-
line system including the same features as last
year: source and target sentences lengths; aver-
age source word length; source and target likeli-
hood computed with 3-gram (source) and 5-gram
(target) language models; average number of oc-
currences of the words within the target sentence;
average number of translations per source word in
the sentence, using IBM1 translation table (only
translations higher than 0.2); weighted average
number of translations per source word in the sen-
tence (similar to the previous one, but a frequent
word is given a low weight in the averaging); dis-
tribution by frequencies of the source n-gram into
the quartiles; match between punctuation in source
and target. Overall, the baseline system proposes
17 features. We remark that only 5 features take
into account the target sentence.
2.2 The LORIA features
In previous works (Raybaud et al, 2011; Langlois
et al, 2012), we tested several confidence mea-
sures. As last year (Langlois et al, 2012), we
use the same features. We extract information by
the way of language model (perplexity, level of
back-off, intra-lingual triggers) and translation ta-
ble (IBM1 table, inter-lingual triggers). The fea-
tures are defined at word level, and the features
at sentence level are computed by averaging over
each word in the sentence. In our system, we use,
in addition to baseline features, ratio of source and
target lengths; source and target likelihood com-
puted with 5-gram language models (Duchateau
et al, 2002) (in addition to 3-gram features from
baseline); level of backoff n-gram based features
(Uhrik and Ward, 1997). This feature indicates
if the 3-gram, the 2-gram or the unigram corre-
sponding to the word is in the language model. For
likelihoods and levels of backoff, we use models
trained on corpus read from left to right (classical
way), and from right to left (sentences are reversed
before training language models). This leads to
two language models, and therefore to two val-
ues for each feature and side (source and target).
Moreover, a common property of all n-gram and
backoff based features is that a word can get a low
score if it is actually correct but its neighbours are
wrong. To compensate for this phenomenon we
took into account the average score of the neigh-
bours of the word being considered. More pre-
cisely, for every relevant feature x. defined at word
level we also computed:
xleft. (wi) = x.(wi?2) ? x.(wi?1) ? x.(wi)
xcentred. (wi) = x.(wi?1) ? x.(wi) ? x.(wi+1)
xright. (wi) = x.(wi) ? x.(wi+1) ? x.(wi+2)
The other features are intra-lingual features:
each word is assigned its average mutual informa-
tion with the other words in the sentence; inter-
lingual features: each word in target sentence is
assigned its average mutual information with the
words in source sentence; IBM1 features: con-
trary to IBM1 based baseline features which take
into account the number of translations, we use
the probability values in the translation table be-
tween source and target words; basic parser (cor-
rection of bracketing, presence of end-of-sentence
symbol); number and ratio of out-of-vocabulary
words in source and target sentences. This leads
to 49 features. A few ones are equivalent to or are
strongly correlated to baseline ones. We remark
that 27 features take into account the target sen-
tence.
The union of the both sets baseline+loria im-
proved slightly the baseline system on the test set
provided by the QE Shared Task 2012 (Callison-
Burch et al, 2012).
3 Corpora
The organizers provide a set of files for training
and development. We list below the ones we used:
? source.eng: 2,254 source sentences taken
from three WMT data sets (English): news-
test2009, news-test2010, and news-test2012.
In the following, this file is named src
? target system.spa: translations for the source
sentences (Spanish) generated by a PB-SMT
system built using Moses. In the following,
this file is named syst
381
? target system.HTER official-score: HTER
scores between MT and post-edited version,
to be used as the official score in the shared
task. In the following, this file is named
hteroff
? target reference.spa: reference translation
(Spanish) for source sentences as originally
given by WMT; In the following, this file is
named ref
? target postedited.spa: human post-edited ver-
sion (Spanish) of the machine translations in
target system.spa. In the following, this file
is named post
We split these files into two parts: a training part
made up of the 1,832 first sentences, and a devel-
opment part made up of the 442 remaining sen-
tences. This choice is motivated by the fact that in
the previous evaluation campaign we had exactly
the same experimental conditions.
For each given file f, we use therefore a part
named f.train for training and a part named
f.dev for development.
4 Training Algorithm
This section describes the approach we propose to
increase the size of the training corpus.
We have to train the prediction model of scores
from the source and target sentences.
The common way to train such a prediction
model consists in extracting a features vector
for each couple (source,target) from
the (src.train,syst.train) corpus.
For each vector, the score associated by ex-
perts to the corresponding sentence is assigned.
Then, we use a machine learning approach to
learn the regression between the vectors and
the scores. And finally, we use the triplet
(src.dev,syst.dev,hteroff.dev) to
tune parameters.
With machine learning approach, the number
of examples is crucial for a relevant training, but
unfortunately the evaluation campaign provides a
training corpus of only 1,832 examples.
To increase the training corpus, we propose
to use the ref and post files. But for that,
we have to associate a score to these new target
sentences. One way could be to calculate the
HTER score between each sentence and its
corresponding sentence in the post edited file.
But this leads to a drawback: all the couples
(src,post) would have a score equal to 0, and
then there is a risk of overtraining on the 0 value.
To prevent this problem, we preferred to learn
a prediction model from the (src.train,-
syst.train,hteroff.train) triplet.
Then we apply this prediction model to
the (src.train,post.train) and to
the (src.train,ref.train). By this
way, we get a training corpus made up of
1, 832 ? 3 = 3, 696 examples with their scores.
Consequently, it is possible to learn a prediction
model from this new training corpus. These
scores are not optimal because the features cannot
describe all the information from sentences, and a
machine learning approach is limited if data are
not sufficiently huge. Therefore, we propose an
anytime randomized algorithm to tune the refer-
ence and post-edited scores on the development
corpus. We give below the algorithm we propose.
1. Prediction model
(a) Learn the prediction model
using only features from
(src.train,syst.train)
and HTER target scores from experts
2. Predict initial scores for postedited and
reference sentences
(a) Use this model to predict the scores
associated to the features from
(src.train,post.train)
and (src.train,ref.train).
The predicted scores for
(src.train,post.train)
are called post best and the ones
for (src.train,ref.train) are
called ref best
3. Learn initial prediction model using the 3
trains (system part, post-edited part and
reference part)
(a) Learn the prediction model using fea-
tures from the three sets of features
and the scores associated to these
sets (experts scores, post best and
ref best)
(b) Evaluate this model. This leads to a per-
formance equal to best
4. Tune scores for postedited and reference
sentences
(a) Repeat the following steps until stop
382
(b) Build a new set of scores named
post new (resp. ref new) by dis-
turbing each score of post best
(resp. ref best) with a probability
equal to pdisturb. A modified score
is shifted by a value randomly chosen in
[-disturb,+disturb]
(c) Learn the prediction model using fea-
tures from the three sets of features
and the new scores associated to these
sets (experts scores for system set,
post new and ref new for the post-
edited and reference sets)
(d) Evaluate this model. This leads to a per-
formance equal to perf
(e) If perf<best then replace best by
perf, post best by post new and
ref best by ref new.
To evaluate a model, we use it to predict the
scores on the development corpus. Then we com-
pare the predicted scores to the expert scores and
we compute the Mean Average Error (MAE) given
by the formula MAE(s, r) =
?n
i=1 |si?ri|
n ? 100where s and r are two sets of n scores.
5 Results
We used the data provided by the shared task
on QE, without additional corpus. This data is
composed of a parallel English-Spanish training
corpus. This corpus is made of the concatena-
tion of europarl-v5 and news-commentary10 cor-
pora (from WMT-2010), followed by tokeniza-
tion, cleaning (sentences with more than 80 to-
kens removed) and truecasing. It has been used
for baseline models provided in the baseline pack-
age by the shared task organizers. We used the
same training corpus to train additional language
models (5-gram with kneyser-ney discounting, ob-
tained with the SRILM toolkit) and triggers re-
quired for our features. For feature extraction, we
used the files provided by the organizers: 2,254
source english sentences, their translations by the
baseline system, and the score of these transla-
tions. This score is the HTER between the pro-
posed translation and the post-edited sentence. We
used the train part to perform the regression be-
tween the features and the scores. Therefore, the
system we propose in this campaign is the same as
the one we presented for the previous campaign in
terms of features. But, we only use a SVM with a
linear kernel and we do not use any feature selec-
tion. The added value of the new system is the fact
that we increase the size of the training corpus.
To evaluate the different configurations, we
used the MAE measure. The performance of
our system with only the classical train set
(src.train,syst.train) are given in Ta-
ble 1. In this table, BASELINE+LORIA use
both features BASELINE and LORIA (Section 2).
We remark that, contrary to last year, the BASE-
LINE+LORIA do not improve the performance of
the BASELINE features on the development set.
Set of features Dev
BASELINE 13.46
LORIA 14.04
BASELINE+LORIA 13.88
Table 1: Performance in terms of MAE without
increasing the training corpus
Now, we increase the training corpus
with the method described in previous sec-
tion. First, we use the system trained on
(src.train,syst.train) to predict
scores for the sentences in post.train and
ref.train. We know that these scores should
represent the HTER score, then a well translated
sentence should be assigned a higher score.
Therefore, we can make the hypothesis that
sentences from post.train and ref.train
are better than those in syst.train. We check
this hypothesis by comparing the distributions of
HTER scores in the three files (true HTER scores
in syst.train, and predicted scores in the two
other files). We present in Table 2 the Minimum,
Maximum, Mean and Standard Deviation of
this score for the three corpora. We remark
that the scores are not well predicted because
some of them are negative while all scores in
syst.train are between 0 and 1. This is due
to the fact that the constraint of HTER in terms of
limit values is not explicitly taken into account by
SVM. We give more details about these scores out
of [0..1] in Table 3. For post.train, 2 scores
are under 0 with a mean value equal to -0.123, and
no scores are higher than 1. For ref.train,
4 scores are under 0 with a mean value equal to
-3.023, and 26 scores are higher than 1 with a
mean equal to 1.126. Comparing to the 1,832
sentences in the training corpus, we can conclude
that the ?outliers? are very rare. In Table 2 Mean
383
and Standard Deviation are computed only for
scores predicted between 0 and 1. The obtained
mean values are quite similar, but the standard
deviation is very low for predicted scores.
This configuration leads to a performance equal
to 13.88 on the development corpus, which is
slightly worse than the BASELINE system but
slightly better than the BASELINE+LORIA sys-
tem.
Because, SVM predicts scores which do not repre-
sent exactly HTER and because the model is learnt
on a relatively small corpus (1,832 sentences), we
decided to modify randomly some scores. This
operation is called in the following the tuning pro-
cess.
Set Min Max Mean SD
syst.
train 0 1 0.317 0.169
post.
train -0.147 0.708 0.315 0.083
ref.
train -11.314 0.746 0.329 0.081
Table 2: Statistics on HTER for the three sets of
sentences used in the training corpus
lower than 0 higher than 1
Set Nb Mean Nb Mean
syst.train 0 - 0 -
post.train 2 -0.123 0 -
ref.train 4 -3.023 26 1.126
Table 3: Statistics on HTER for the three sets of
sentences used in the training corpus. Nb is the
number of sentences
For the tuning process, after several tests, we
fixed to 0.1 the probability pdisturb to modify
the score of a sentence. Then, the score is modi-
fied by randomly shifting it in [?0.01... + 0.01].
We start with the initial predicted scores (MAE
= 13.88). Then we randomly modify a subset of
scores and keep a new configuration if its MAE is
improved. The process is stopped when MAE con-
verges. Figure 1 presents the evolution of MAE on
the development corpus.
The process stopped after 22, 248 iterations.
Only 274 (1.2%) iterations led to an improvement.
We present the results of this approach on the de-
velopment corpus and on the official test set of the
Figure 1: Evolution of the MAE on the develop-
ment corpus
campaign (500 sentences). We group in Table 4
the results on development and test corpus for the
BASELINE features and the BASELINE+LORIA
features with and without using the post-edited
and reference sentences. Finally, we achieve a
MAE of 12.05 on the development set. This con-
stitutes an improvement of 10.5% in comparison
to the BASELINE system. But we improve only
slightly the performance of the baseline system on
the test set. We conclude that there is an overtrain-
ing on the development corpus. In order to prevent
from this problem, we could use a leaving-one-out
approach on training and development corpora.
With the tuned values of scores, we calculated
the same statistics as in Tables 2 and 3. We present
these statistics in Tables 5 and 6. As we can see,
the tuning process leads to an increasing of the
mean value of the scores. Moreover, the number
of scores out of range increases. This analysis re-
inforces our conclusion about overtraining: pre-
dicted scores may be strongly modified to obtain a
good performance on the development corpus.
Set of features Dev Test
BASELINE 13.46 14.81
BASELINE+LORIA 13.88 nc
+ postedited + ref 13.78 nc
+ tuning 12.05 14.79
Table 4: Performance in terms of MAE of the fea-
tures with and without increasing the training cor-
pus
To conclude the experiments, we try to fix the
problem of scores predicted out of range. For that,
we set to 0 the scores lower than 0 and to 1 the
384
Set Min Max Mean SD
post.
train -0.811 1.322 0.407 0.235
ref.
train -10.485 1.320 0.409 0.242
Table 5: Statistics on HTER for the post and ref
sets of sentences used in the training corpus, after
tuning
lower than 0 higher than 1
Set of sentences Nb Mean Nb Mean
post.train 318 -0.164 29 1.118
ref.train 282 -0.205 28 1.123
Table 6: Statistics on HTER for the post and ref
sets of sentences used in the training corpus, after
tuning. Nb is the number of sentences.
ones greater than 1. Then we learn a new SVM
model using these new scores. This leads to a
MAE equal to 12.18 on the development corpus
and 14.83 on the test corpus, which is worse than
the performance without correction. This is for us
a drawback of the machine learning approach. For
this approach, the scores have no semantic. SVM
do not ?know? that the scores are HTER between
0 and 1. Then, if tuning leads to no reasonable val-
ues, this is not a problem if it increases the perfor-
mance. Moreover, maybe the features do not ex-
tract from all sentences information representative
of their quality, and this quality is overestimated:
then the tuning system has to lower strongly the
corresponding scores to counteract this problem.
6 Conclusion and perpespectives
In this paper we propose a method to increase the
size of the training corpus for QE in the scope of
Task 1.1. We add to the initial training corpus
(sentences translated by a machine translation sys-
tem) the post-edited and the reference sentences.
We associate to these sentences scores predicted
by using a model learnt on the system sentences.
Then we tune the predicted scores on the devel-
opment corpus. This method leads to an improve-
ment of 10.5% on the development corpus in terms
of MAE, but achieves only a slight improvement
on the test corpus. A statistical study shows that
tuning scores leads to out of range values. This
surprising behavior have to be investigated. In ad-
dition, we will test another machine learning tools
(neural networks for example). Another point is
that, contrary to last year, the whole set of features
leads to worse performance than baseline features.
This could be explained by the fact that no select-
ing algorithm has been used to choose the best fea-
tures. In fact, we preferred, this year to investigate
the underlying knowledge on the post-edited and
reference corpora. Last, we conclude that the good
improvement on the development corpus is not re-
produced on the test corpus. In order to prevent
from this problem, we will use a leaving-one-out
approach on the training.
References
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 workshop on statistical machine translation. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, pages 10?51.
C.-C. Chang and C.-J. Lin. 2011. LIBSVM:
A library for support vector machines. ACM
Transactions on Intelligent Systems and Tech-
nology, 2:27:1?27:27. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
J. Duchateau, K. Demuynck, and P. Wambacq. 2002.
Confidence scoring based on backward language
models. In Proceedings of IEEE International Con-
ference on Acoustics, Speech, and Signal Process-
ing, volume 1, pages 221?224.
D. Langlois, S. Raybaud, and Kamel Sma??li. 2012. Lo-
ria system for the WMT12 quality estimation shared
task. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, pages 114?119.
E. Moreau and C. Vogel. 2012. Quality estimation:
an experimental study using unsupervised similarity
measures. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 120?126.
S. Raybaud, D. Langlois, and K. Sma??li. 2011. ?This
sentence is wrong.? Detecting errors in machine-
translated sentences. Machine Translation, 25(1):1?
34.
R. Soricut and A. Echihabi. 2010. Trustrank: Inducing
trust in automatic translations via ranking. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 612?621.
L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2010.
Predicting machine translation adequacy. In Pro-
ceedings of the Machine Translation Summit XIII,
pages 612?621.
L. Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation, pages 73?80.
C. Uhrik and W. Ward. 1997. Confidence metrics
based on n-gram language model backoff behaviors.
In Fifth European Conference on Speech Communi-
cation and Technology, pages 2771?2774.
385
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 105?111,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Comparing Multilingual Comparable Articles Based On Opinions
Motaz Saad David Langlois Kamel Sma??li
Speech Group, LORIA
INRIA, Villers-le`s-Nancy, F-54600, France
Universite? de Lorraine, LORIA, UMR 7503, Villers-le`s-Nancy, F-54600, France
CNRS, LORIA, UMR 7503, Villers-le`s-Nancy, F-54600, France
{firstName.lastName}@loria.fr
Abstract
Multilingual sentiment analysis attracts in-
creased attention as the massive growth
of multilingual web contents. This con-
ducts to study opinions across different
languages by comparing the underlying
messages written by different people hav-
ing different opinions. In this paper, we
propose Sentiment based Comparability
Measures (SCM) to compare opinions in
multilingual comparable articles without
translating source/target into the same lan-
guage. This will allow media trackers
(journalists) to automatically detect public
opinion split across huge multilingual web
contents. To develop SCM, we need either
to get or to build parallel sentiment cor-
pora. Because this kind of corpora are not
available, we decided to build them. For
that, we propose a new method to automat-
ically label parallel corpora with sentiment
classes. Then we use the extracted parallel
sentiment corpora to develop multilingual
sentiment analysis system. Experimental
results show that, the proposed measure
can capture differences in terms of opin-
ions. The results also show that compara-
ble articles variate in their objectivity and
positivity.
1 Introduction
We can distinguish two kinds of sentiments anal-
ysis depending on monolingual or multilingual ar-
ticles.
In the following, as in (Pang and Lee, 2008), the
terms Sentiment Analysis (SA) and Opinion Min-
ing (OM) are used as synonyms. Mining opinions
is to identify the subjectivity and/or the polarity of
a given text at article or sentence level. Subjectiv-
ity identification is to classify the text into subjec-
tive or objective, while polarity identification is to
classify the text into negative or positive.
Popular methods for monolingual sentiment
analysis are based on lexicon and corpus. Lexi-
con based methods use string matching techniques
between texts and annotated lexicons. The most
common sentiment lexicons for English language
are WordNet-Affect (Valitutti, 2004) and Senti-
WordNet (Esuli and Sebastiani, 2006), which are
extensions of WordNet. Additionally, SenticNet
(Cambria et al, 2010) is a knowledge-base ex-
tension of aforementioned lexicons. On the other
hand, corpus based approach is popular for sen-
timent analysis (Pang and Lee, 2008). It uses
corpora and machine learning algorithms to build
sentiment classification systems. For example,
Pang et al used polarity (Pang et al, 2002) and
subjectivity (Pang and Lee, 2004) English cor-
pora to train machine learning algorithms to build
sentiment classifiers. These resources have been
adapted to other languages by many researchers
as we will see in the following.
Multilingual sentiments analysis becomes a re-
ality because of the massive growth of multilin-
gual web contents. In this case, sentiment analy-
sis identifies sentiments across multiple languages
instead of one language. This can be done by
creating sentiment resources for new languages
by translating existing English resources (lexi-
cons/corpora) into the target language, or by trans-
lating target text into English, then pass the trans-
lated text to English models for sentiment analysis
(Rushdi-Saleh et al, 2011; Bautin et al, 2008; De-
necke, 2008; Ghorbel, 2012). However, (Brooke
et al, 2009) reported that creating new resources
to build sentiment models from scratch works bet-
ter than using the approach based on machine
translation.
As we see in the previous discussion, works on
multilingual sentiment analysis just try to iden-
tify sentiments across multiple languages. How-
105
ever, it is worthy to compare opinions about a
given topic in several languages, not just to iden-
tify these opinions. If people from different cul-
tures wrote an article about political/societal top-
ics, they may judge these topics differently ac-
cording to their cultures. In fact, detecting dis-
agreement of opinions in multiple languages is a
promising research area. So, our goal is to en-
able media trackers (journalists) to automatically
detect the split of public opinions about a given
topic across multiple languages. To the best of our
knowledge, there are no work in the literature that
serve our goal, therefore, we propose to develop
automatic measures that compare opinions in mul-
tilingual comparable articles. These comparabil-
ity measures will be the core of our goal which is
building multilingual automatic journalist review
system.
For that, we propose a Sentiment based Com-
parability Measures (SCM) which identify senti-
ments, score them and compare them across mul-
tilingual documents. Therefore, we need to iden-
tify and score sentiments in multiple languages.
Namely, SCM needs a multilingual sentiment
analysis system to identify and score sentiments.
To build this system, we need parallel sentiment
corpora from different topics. Unfortunately, we
do not have such corpora, we only have English
sentiment corpus. So, we propose in Section 2 a
new method to build parallel sentiment corpora.
We start from English sentiment corpora (movie
reviews domain), then use it to build sentiment
classifier for English language and then label a
new parallel English/target corpora which is dif-
ferent from the movie one. In section 3, we use the
obtained parallel sentiment corpora to build a mul-
tilingual sentiment analysis system which is used
to develop SCM, then we use SCM to compare
multilingual comparable articles in terms of opin-
ions. The advantage of this idea is that we do not
need to translate corpora/lexicons to analyse mul-
tilingual text.
The rest of this article is organized as fol-
lows, Section 2 describes our method to build par-
allel sentiment corpora, Section 3 presents our
proposed sentiment based comparability measures
(SCM) and experimental results conducted on cor-
pora. Finally, we state the conclusions.
2 Sentiment Corpora Extraction
As we introduced earlier, we need parallel cor-
pora to build the sentiment comparability measure.
Therefore, we present in this section a method
to annotate parallel corpora with sentiment la-
bels. This method can be applied on any En-
glish/target language pairs. In this work, we la-
bel English/Arabic parallel sentences. The idea is
to use an English sentiment classifier to label each
English sentence in the new parallel corpora, then
we can assign the same label to the target (Ara-
bic) sentence, because sentences are parallel and
convey the same opinions.
The widely used approach to build a classifier
is to build a Naive Bayes model using n-grams
linguistic features (Pang et al, 2002; Dave et al,
2003; Pang and Lee, 2004; Kim and Hovy, 2004;
Cui et al, 2006; Tan et al, 2009). So, we use this
method on bigrams extracted from English sen-
timent corpora of movie reviews. These corpora
are manually labelled with subjectivity and polar-
ity labels. Each review in the collection is rep-
resented as a vector composed of bigram occur-
rences. Then, each vector is feed to Naive Bayes
classifier with corresponding class label for train-
ing. Naive Bayes classifies the vector to the high-
est probable class. Our objective in this paper is to
compare opinions, this is why we used this tradi-
tional method for building the sentiment classifier.
The parallel corpora, that we annotate, cover
variant topics (newspapers, UN resolutions, and
transcribed talks), and are available in many lan-
guages. The newspapers are collection of parallel
articles from AFP, ANN, ASB, and provided by
LDC1. UN corpora2 is a collection of United Na-
tions General Assembly Resolutions. Transcribed
talks are collection of multilingual transcriptions
from TED provided by WIT33.
Figure 1 illustrates our method and Table 1 de-
scribes corpora denoted in the figure. The men-
tioned corpora are: senti-corp, parallel, and new-
senti-corp. senti-corp represents the monolingual
(English) manually labelled, parallel represents
parallel corpora in variant topics, and new-senti-
corp represents the extracted corpora. Corpora
sizes are presented in Tables 2 and 3. Table 2
presents the number of reviews of senti-corp with
1LDC - Linguistic Data Consortium: ldc.upenn.edu
2Corpora of the United Nations: uncorpora.org
3WIT3 Web Inventory of Transcribed and Translated
Talks wit3.fbk.eu
106
respect to sentiment classes, and Table 3 presents
the number of sentences of parallel corpora.
Table 1: Corpora description
Corpora Description
senti-corp
Monolingual manually
labelled sentiment corpus
(polarity or subjectivity)
senti-corp-p1
Part 1 of senti-corp (90%):
used to build classification
models which are used for
labelling task
senti-corp-p2
Part 2 of senti-corp (10%):
This is the (test corpus)
which is used to test the
extracted corpora
parallel Multilingual parallel corpora
parallel-p1
Part 1 of the parallel corpora
(90%): to be labelled
automatically
parallel-p2
Part 2 of the parallel corpora
(10%): to be used to evaluate
SCM
new-senti-corp
Multilingual automatically
labelled sentiment corpus
Table 2: Senti-corp size (number of reviews)
Class senti-corp-p1 senti-corp-p2
subjective 4500 500
objective 4500 500
negative 900 100
positive 900 100
Table 3: Parallel Corpora size
Corpus # of sentences
parallel-p1 364K
parallel-p2 40K
The following steps describe the method we
propose:
1. Split senti-corp into two parts: senti-corp-p1
is 90%, and senti-corp-p2 is 10%.
2. Use senti-corp-p1 to train a Naive Bayes
classifier to build a monolingual sentiment
model.
3. Split the parallel corpora into two parts:
parallel-p1 is 90%, and parallel-p2 is 10%.
4. Using the sentiment classification model ob-
tained in step 2, classify and label English
sentences of parallel-p1 and assign the same
sentiment class to the corresponding Arabic
sentences.
5. Refine and filter sentences which are labelled
in step 4. The filtering process keeps only
sentences that have high sentiment score.
Then, we obtain new-senti-corp which is
Arabic/English parallel sentiment labelled
corpora in different domains.
6. Use the English part of new-senti-corp which
is obtained in step 5 to train a Naive Bayes
classifier.
7. Evaluate the classifier built in step 6 on senti-
corp-p2. If the classification accuracy is ac-
cepted, then continues, otherwise, try other
corpora and/or models.
This method is independent of the the sentiment
class labels. So, it can be applied for subjectivity
or polarity corpus.
Tables 4 and 5 present the experimental results
of steps 4 and 5 of the Figure 1. Table 4 shows
the statistical information of sentiment scores of
the labelled corpora, where Rate is the class label
distribution (percentage) with respect to the whole
dataset. ?, ?, Min, and Max are the mean, stan-
dard deviation, minimum, and maximum values
of sentiment scores respectively. For subjectiv-
ity labels, 54% and 46% of sentences are labelled
as subjective and objective respectively. For po-
larity labels, 58% and 42% of sentences are la-
belled as negative and positive respectively. Table
5 presents the frequency table of intervals of sen-
timent scores of the labelled sentences. We can
see from Table 5 that most of sentences have high
sentiment scores (from 0.9 to 1.0). To extract high
quality labelled sentences, we keep only sentences
with score greater than 0.8.
In order to evaluate the quality of the extracted
corpora (step 7 in Figure 1), we need first to build a
sentiment classifier based on this corpora and then
evaluate the accuracy of this classifier. The detail
of this process is given bellow:
1. Train a Naive Bayes classifier on the parallel
sentiment corpora new-senti-corp.
2. Test the obtained classifiers on the manually
labelled corpus senti-corp-p2.
107
Figure 1: Approach for parallel sentiment corpora extraction and evaluation
Table 4: Sentiment classes statistics for labelled sentences scores of parallel-p1 corpora
Label Count Rate ? ? Min Max
subjective 231,180 54% 0.93 0.11 0.60 1.00
objective 197,981 46% 0.93 0.11 0.60 1.00
negative 219,070 58% 0.84 0.12 0.60 0.99
positive 159,396 42% 0.83 0.12 0.60 1.0
Table 5: Frequency table of sentiment scores intervals of labelled sentences of parallel-p1 corpora
Label [0.6,0.7) [0.7,0.8) [0.8,0.9) [0.9,1]
subjective 6.1% 9.0% 11.9% 73.0%
objective 6.8% 8.1% 10.8% 74.3%
negative 17.7% 18.0% 21.6% 42.7%
positive 20.4% 20.8% 21.7% 37.2%
In the following, senti-corp-p2 is the test cor-
pus. The evaluation is presented in Table 6.
The metrics include classification accuracy, and
F-measures. F-neg, F-pos, F-sub, and F-obj are
the F-measures for negative, positive, subjective,
and objective classes respectively. For subjectiv-
108
Table 6: Evaluation of extracted corpus (step 7)
Subjectivity Polarity
Accuracy 0.765 Accuracy 0.720
F-sub 0.717 F-neg 0.754
F-obj 0.799 F-pos 0.674
ity test, the classifier achieved 76.5% of accuracy
and an average of 75.8% of f-measure. For polar-
ity test, the classifier leads to 72% of accuracy and
an average of 71% of F-measure.
We wanted to compare these results with oth-
ers works in sentiment classification, but unfortu-
nately the used corpora are not the same. Anyway,
these results are only indicative for us, because our
objective is not to propose a new method for auto-
matic sentiment classification, but to build a senti-
ment based comparability measure.
Now, we obtained English/Arabic parallel sen-
timent corpora in multiple topics. We use these
corpora to develop sentiment based comparability
measures that will be described in the next section.
Notice that at the beginning the only avail-
able sentiment corpus was a collection of movie
reviews in English language, with the proposed
method, we got multilingual sentiment corpora
of different topics. Furthermore, using this
method, one can obtain sentiment corpus for
under-resourced languages. The advantage of the
parallel corpora is to build sentiment classifiers
that can be used to develop sentiment based com-
parability measures.
3 Sentiment Based Comparability
Measures
As we stated in the introduction, there are no work
in the literature that serve our goal, which is to
compare multilingual articles in terms opinions.
Therefore, we propose to develop automatic mea-
sures that compare opinions in multilingual com-
parable articles.
In the previous section, we built a parallel sen-
timent corpora where both source and its corre-
sponding sentence have the same sentiment label.
In this section, we compare multilingual compa-
rable articles in terms of sentiments. Obviously,
in this case we do not have the same sentiment la-
bels since articles are comparable and not parallel.
So, we develop Sentiment based Comparability
Measures (SCM) which measure the differences
of opinions in multilingual corpora. For that, we
use the achieved parallel sentiment corpora new-
senti-corp to build multilingual sentiment analysis
systems, using the same method as in Section 2.
The idea is to identify and score sentiments in
the source and target comparable articles and pro-
vide these information to SCM to compare their
opinions. In the following, we describe how to
compute SCM for comparable articles based on
average score of all sentences.
We use formula 1 which is derived from Naive
Bayes to compute opinion score and assign the
corresponding label:
classify(S) = argmax
c
P (c)
n?
k=1
P (fk|c) (1)
where S is a sentence, fk are the features of S,
c ? {o, o?} for subjectivity and c ? {p, p?} for po-
larity, where o is objective, o? is subjective, p is
positive, p? is negative.
An article may contain some sentences belong-
ing to the subjective class, and others belonging
to the objective class (idem for positive and nega-
tive). So, for a given pair of comparable articles,
SCM has three parameters dx, dy, c, where dx, dy
are the source and the target articles respectively,
and c is the class label. This score is calculated as
follows:
SCM(dx, dy, c) =
?
?
?
?
?
?
?
?
C(Sx)=c
P (Sx|c)
Nx
?
?
C(Sy)=c
P (Sy|c)
Ny
?
?
?
?
?
?
?
(2)
Where Sx ? dx, Sy ? dy, and
?
C(Sx)=c
P (Sx|c)
and
?
C(Sy)=c
P (Sy|c) are the sum of probabilities
for all source and target sentences respectively that
belong to class c. Nx and Ny are the number of
source and target sentences respectively that be-
long to the class c. Formally speaking, for a given
pair of documents dx, dy, we have four measures:
SCM(dx, dy, o), SCM(dx, dy, o?) for subjectiv-
ity, and SCM(dx, dy, p), SCM(dx, dy, p?) for po-
larity.
In our experiments, we calculate SCM for pair
of articles in parallel and comparable corpora.
Calculating SCM for parallel corpora could be
very surprising, but we did it in order to show
that for this kind of corpora, the proposed measure
should be better than the one achieved for compa-
rable corpora.
109
Table 7: Comparable corpora information
AFEWC eNews
English Arabic English Arabic
Articles 40290 40290 34442 34442
Sentences 4.8M 1.2M 744K 622K
Average #sentences/article 119 30 21 17
Average #words/article 2266 548 198 161
Words 91.3M 22M 6.8M 5.5M
Vocabulary 2.8M 1.5M 232K 373K
Table 8: Average Sentiment Based Comparability Measures (SCM)
Corpora SCM(dx, dy, o?) SCM(dx, dy, o) SCM(dx, dy, p?) SCM(dx, dy, p)
parallel-p2
AFP 0.02 0.02 0.1 0.12
ANN 0.05 0.06 0.1 0.1
ASB 0.07 0.1 0.12 0.14
TED 0.06 0.06 0.08 0.07
UN 0.05 0.02 0.07 0.08
Comparable
ENews 0.07 0.15 0.11 0.15
AFEWC 0.11 0.19 0.11 0.16
The comparable corpora that we use for our
experiments are AFEWC and eNews which were
collected and aligned at article level (Saad et al,
2013). Each pair of comparable articles is related
to the same topic. AFEWC corpus is collected
from Wikipedia and eNews is collected from Eu-
ronews website. Table 7 presents the number of
articles, sentences, average sentences per article,
average words per article, words, and vocabulary
of these corpora.
Table 8 presents the experimental results of
SCM computed using formula 2. SCM is com-
puted for the source and target articles for par-
allel corpora parallel-p2 and comparable corpora
(AFEWC and eNews). We note that SCM for AFP,
ANN, ASB, TED, and UN corpora are small be-
cause they are parallel. This shows that the pro-
posed measure is well adapted to capture the sim-
ilarity between parallel articles. Indeed, they have
the same sentiments. On the other hand, SCM be-
come larger for comparable corpora, because the
concerned articles do not necessary have the same
sentiments. The only exception to what have been
claimed is that the subjectivity SCM for eNews
comparable corpora is similar to the one of ASB
which is parallel corpora. In contrast, the objec-
tivity SCM is larger (0.15) for eNews, that means
pair of articles in eNews corpora have similar sub-
jective but different objective sentiments. In other
words, source and target are considered similar in
terms of subjectivity but different in terms of ob-
jectivity (idem for negative and positive). Con-
sequently, comparable articles do not necessary
have the same opinions. Additionally, we note
that the SCM for AFEWC corpora are the largest
in comparison to the others, this is maybe because
Wikipedia has been written by many different con-
tributors from different cultures.
4 Conclusions
We presented a new method for comparing mul-
tilingual sentiments through comparable articles
without the need of translating source/target arti-
cles into the same language. Our results showed
that it is possible now for media trackers to au-
tomatically detect difference in public opinions
across huge multilingual web contents. The re-
sults showed that the comparable articles variate
in their objectivity and positivity. To develop our
system, we required parallel sentiment corpora.
So, we presented in this paper an original method
to build parallel sentiment corpora. We started
from an English movie corpus annotated in terms
of sentiments, we trained NB classier to classify
an English text concerning topics different from
movie, and then we deduced the sentiment labels
of the the corresponding target parallel text by as-
signing the same labels. This method is interest-
110
ing because it allows us to produce several parallel
sentiment corpora concerning different topics. We
built SCM using these parallel sentiment corpora,
then, SCM identifies sentiments, scores them and
compares them across multilingual documents. In
the future works, we will elaborate our journalist
review system by developing a multilingual com-
parability measure that can handle semantics and
integrate it with the sentiment based measure.
References
M. Bautin, L. Vijayarenu, and S. Skiena. 2008. Inter-
national sentiment analysis for news and blogs. In
Proceedings of the International Conference on We-
blogs and Social Media (ICWSM).
J. Brooke, M. Tofiloski, and M. Taboada. 2009. Cross-
linguistic sentiment analysis: From english to span-
ish. In International Conference RANLP, pages 50?
54.
E. Cambria, R. Speer, C. Havasi, and A. Hussain.
2010. Senticnet: A publicly available semantic re-
source for opinion mining. Artificial Intelligence,
pages 14?18.
H. Cui, V. Mittal, and M. Datar. 2006. Compara-
tive experiments on sentiment classification for on-
line product reviews. In proceedings of the 21st na-
tional conference on Artificial intelligence - Volume
2, AAAI?06, pages 1265?1270. AAAI Press.
K. Dave, S. Lawrence, and D. M. Pennock. 2003.
Mining the peanut gallery: opinion extraction and
semantic classification of product reviews. In Pro-
ceedings of the 12th international conference on
World Wide Web, WWW ?03, pages 519?528, New
York, NY, USA. ACM.
K. Denecke. 2008. Using sentiwordnet for multilin-
gual sentiment analysis. In Data Engineering Work-
shop, 2008. ICDEW 2008. IEEE 24th International
Conference on, pages 507?512.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion min-
ing. In In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation, pages 417?422.
H. Ghorbel. 2012. Experiments in cross-lingual sen-
timent analysis in discussion forums. In K. Aberer,
A. Flache, W. Jager, L. Liu, J. Tang, and C. Guret,
editors, Social Informatics, volume 7710 of Lec-
ture Notes in Computer Science, pages 138?151.
Springer Berlin Heidelberg.
S.-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of the 20th inter-
national conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
B. Pang and L. Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
B. Pang and L. Lee. 2008. Opinion mining and sen-
timent analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learn-
ing techniques. In Proceedings of the ACL-02 con-
ference on Empirical methods in natural language
processing-Volume 10, pages 79?86. Association for
Computational Linguistics.
M. Rushdi-Saleh, M. T. Mart??n-Valdivia, L. A. Uren?a
Lo?pez, and J. M. Perea-Ortega. 2011. Bilingual
experiments with an arabic-english corpus for opin-
ion mining. In Proceedings of the International
Conference Recent Advances in Natural Language
Processing 2011, pages 740?745, Hissar, Bulgaria,
September. RANLP 2011 Organising Committee.
M. Saad, D. Langlois, and K. Sma??li. 2013. Extract-
ing comparable articles from wikipedia and measur-
ing their comparabilities. In V International Confer-
ence on Corpus Linguistics. University of Alicante,
Spain.
S. Tan, X. Cheng, Y. Wang, and H. Xu. 2009. Adapt-
ing naive bayes to domain adaptation for sentiment
analysis. In Advances in Information Retrieval,
pages 337?349. Springer.
R. Valitutti. 2004. Wordnet-affect: an affective exten-
sion of wordnet. In In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 1083?1086.
111
