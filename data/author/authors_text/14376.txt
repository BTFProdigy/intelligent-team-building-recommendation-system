Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1245?1254, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Learning for Coreference Resolution with Markov Logic
Yang Song1, Jing Jiang2, Wayne Xin Zhao3, Sujian Li1, Houfeng Wang1
1Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
2School of Information Systems, Singapore Management University, Singapore
3School of Electronics Engineering and Computer Science, Peking University, China
{ysong, lisujian, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg, batmanfly@gmail.com
Abstract
Pairwise coreference resolution models must
merge pairwise coreference decisions to gen-
erate final outputs. Traditional merging meth-
ods adopt different strategies such as the best-
first method and enforcing the transitivity con-
straint, but most of these methods are used
independently of the pairwise learning meth-
ods as an isolated inference procedure at the
end. We propose a joint learning model which
combines pairwise classification and mention
clustering with Markov logic. Experimen-
tal results show that our joint learning sys-
tem outperforms independent learning sys-
tems. Our system gives a better performance
than all the learning-based systems from the
CoNLL-2011 shared task on the same dataset.
Compared with the best system from CoNLL-
2011, which employs a rule-based method,
our system shows competitive performance.
1 Introduction
The task of noun phrase coreference resolution is to
determine which mentions in a text refer to the same
real-world entity. Many methods have been pro-
posed for this problem. Among them the mention-
pair model (McCarthy and Lehnert, 1995) is one of
the most influential ones and can achieve the state-
of-the-art performance (Bengtson and Roth, 2008).
The mention-pair model splits the task into three
parts: mention detection, pairwise classification and
mention clustering. Mention detection aims to iden-
tify anaphoric noun phrases, including proper nouns,
common noun phrases and pronouns. Pairwise clas-
sification takes a pair of detected anaphoric noun
phrase candidates and determines whether they re-
fer to the same entity. Because these classification
decisions are local, they do not guarantee that can-
didate mentions are partitioned into clusters. There-
fore a mention clustering step is needed to resolve
conflicts and generate the final mention clusters.
Much work has been done following the mention-
pair model (Soon et al 2001; Ng and Cardie, 2002).
In most work, pairwise classification and mention
clustering are done sequentially. A major weak-
ness of this approach is that pairwise classification
considers only local information, which may not be
sufficient to make correct decisions. One way to
address this weakness is to jointly learn the pair-
wise classification model and the mention cluster-
ing model. This idea has been explored to some
extent by McCallum and Wellner (2005) using con-
ditional undirected graphical models and by Finley
and Joachims (2005) using an SVM-based super-
vised clustering method.
In this paper, we study how to use a different
learning framework, Markov logic (Richardson and
Domingos, 2006), to learn a joint model for both
pairwise classification and mention clustering un-
der the mention-pair model. We choose Markov
logic because of its appealing properties. Markov
logic is based on first-order logic, which makes
the learned models readily interpretable by humans.
Moreover, joint learning is natural under the Markov
logic framework, with local pairwise classification
and global mention clustering both formulated as
weighted first-order clauses. In fact, Markov logic
has been previously used by Poon and Domingos
(2008) for coreference resolution and achieved good
1245
results, but it was used for unsupervised coreference
resolution and the method was based on a different
model, the entity-mention model.
More specifically, to combine mention cluster-
ing with pairwise classification, we adopt the com-
monly used strategies (such as best-first clustering
and transitivity constraint), and formulate them as
first-order logic formulas under the Markov logic
framework. Best-first clustering has been previously
studied by Ng and Cardie (2002) and Bengtson and
Roth (2008) and found to be effective. Transitivity
constraint has been applied to coreference resolution
by Klenner (2007) and Finkel and Manning (2008),
and also achieved good performance.
We evaluate Markov logic-based method on the
dataset from CoNLL-2011 shared task. Our ex-
periment results demonstrate the advantage of joint
learning of pairwise classification and mention clus-
tering over independent learning. We examine
best-first clustering and transitivity constraint in our
methods, and find that both are very useful for coref-
erence resolution. Compared with the state of the
art, our method outperforms a baseline that repre-
sents a typical system using the mention-pair model.
Our method is also better than all learning systems
from the CoNLL-2011 shared task based on the re-
ported performance. Even with the top system from
CoNLL-2011, our performance is still competitive.
In the rest of this paper, we first describe a stan-
dard pairwise coreference resolution system in Sec-
tion 2. We then present our Markov logic model for
pairwise coreference resolution in Section 3. Exper-
imental results are given in Section 4. Finally we
discuss related work in Section 5 and conclude in
Section 6.
2 Standard Pairwise Coreference
Resolution
In this section, we describe standard learning-based
framework for pairwise coreference resolution. The
major steps include mention detection, pairwise
classification and mention clustering.
2.1 Mention Detection
For mention detection, traditional methods include
learning-based and rule-based methods. Which kind
of method to choose depends on specific dataset. In
this paper, we first consider all the noun phrases
in the given text as candidate mentions. With-
out gold standard mention boundaries, we use a
well-known preprocessing tool from Stanford?s NLP
group1 to extract noun phrases. After obtaining all
the extracted noun phrases, we also use a rule-based
method to remove some erroneous candidates based
on previous studies (e.g. Lee et al(2011), Uryupina
et al(2011)). Some examples of these erroneous
candidates include stop words (e.g. uh, hmm), web
addresses (e.g. http://www.google.com),
numbers (e.g. $9,000) and pleonastic ?it? pronouns.
2.2 Pairwise Classification
For pairwise classification, traditional learning-
based methods usually adopt a classification model
such as maximum entropy models and support vec-
tor machines. Training instances (i.e. positive and
negative mention pairs) are constructed from known
coreference chains, and features are defined to rep-
resent these instances.
In this paper, we build a baseline system that uses
maximum entropy models as the classification algo-
rithm. For generation of training instances, we fol-
low the method of Bengtson and Roth (2008). For
each predicted mention m, we generate a positive
mention pair between m and its closest preceding
antecedent, and negative mention pairs by pairing m
with each of its preceding predicted mentions which
are not coreferential with m. To avoid having too
many negative instances, we impose a maximum
sentence distance between the two mentions when
constructing mention pairs. This is based on the in-
tuition that for each anaphoric mention, its preced-
ing antecedent should appear quite near it, and most
coreferential mention pairs which have a long sen-
tence distance can be resolved using string match-
ing. During the testing phase, we generate men-
tion pairs for each mention candidate with each of
its preceding mention candidates and use the learned
model to make coreference decisions for these men-
tion pairs. We also impose the sentence distance
constraint and use string matching for mention pairs
with a sentence distance exceeding the threshold.
1http://nlp.stanford.edu/software/corenlp.shtml
1246
2.3 Mention Clustering
After obtaining the coreferential results for all men-
tion pairs, some clustering method should be used to
generate the final output. One strategy is the single-
link method, which links all the mention pairs that
have a prediction probability higher than a threshold
value. Two other alternative methods are the best-
first clustering method and clustering with the tran-
sitivity constraint. Best-first clustering means that
for each candidate mention m, we select the best
one from all its preceding candidate mentions based
on the prediction probabilities. A threshold value
is given to filter out those mention pairs that have a
low probability to be coreferential. Transitivity con-
straint means that if a and b are coreferential and
b and c are coreferential, then a and c must also be
coreferential. Previous work has found that best-first
clustering and transitivity constraint-based cluster-
ing are better than the single-link method. Finally
we remove all the singleton mentions.
3 Markov Logic for Pairwise Coreference
Resolution
In this section, we present our method for joint
learning of pairwise classification and mention clus-
tering using Markov logic. For mention detection,
training instance generation and postprocessing, our
method follows the same procedures as described in
Section 2. In what follows, we will first describe
the basic Markov logic networks (MLN) framework,
and then introduce the first-order logic formulas we
use in our MLN including local formulas and global
formulas which perform pairwise classification and
mention clustering respectively. Through this way,
these two isolated parts are combined together, and
joint learning and inference can be performed in a
single framework. Finally we present inference and
parameter learning methods.
3.1 Markov Logic Networks
Markov logic networks combine Markov networks
with first-order logic (Richardson and Domingos,
2006; Riedel, 2008). A Markov logic network con-
sists of a set of first-order clauses (which we will re-
fer to as formulas in the rest of the paper) just like in
first-order logic. However, different from first-order
logic where a formula represents a hard constraint,
in an MLN, these constraints are softened and they
can be violated with some penalty. An MLN M
is therefore a set of weighted formulas {(?i, wi)}i,
where ?i is a first order formula andwi is the penalty
(the formula?s weight). These weighted formulas
define a probability distribution over sets of ground
atoms or so-called possible worlds. Let y denote a
possible world, then we define p(y) as follows:
p(y) = 1
Z
exp
(
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y)
)
. (1)
Here each c is a binding of free variables in ?i to
constants. Each f?ic represents a binary feature func-
tion that returns 1 if the ground formula we get by
replacing the free variables in ?i with the constants
in c under the given possible world y is true, and 0
otherwise. n?i denotes the number of free variables
of a formula ?i. Cn?i is the set of all bindings for the
free variables in ?i. Z is a normalization constant.
This distribution corresponds to a Markov network
where nodes represent ground atoms and factors rep-
resent ground formulas.
Each formula consists of a set of first-order predi-
cates, logical connectors and variables. Take the fol-
lowing formula as one example:
(?i, wi) : headMatch(a, b)?(a ?= b) ? coref (a, b).
The formula above indicates that if two different
candidate mentions a and b have the same head
word, then they are coreferential. Here a and b are
variables which can represent any candidate men-
tion, headMatch and coref are observed predicate
and hidden predicate respectively. An observed
predicate is one whose value is known from the ob-
servations when its free variables are assigned some
constants. A hidden predicate is one whose value is
not known from the observations. From this exam-
ple, we can see that headMatch is an observed pred-
icate because we can check whether two candidate
mentions have the same head word. coref is a hid-
den predicate because this is something we would
like to predict.
3.2 Formulas
We use two kinds of formulas for pairwise classi-
fication and mention clustering, respectively. For
1247
describing the attributes ofmi
mentionType(i,t) mi has mention type NAM(named entities), NOM(nominal) or PRO(pronouns).
entityType(i,e) mi has entity type PERSON, ORG, GPE or UN...
genderType(i,g) mi has gender type MALE, FEMALE, NEUTRAL or UN.
numberType(i,n) mi has number type SINGULAR, PLURAL or UN.
hasHead(i,h) mi has head word h, here h can represent all possible head words.
firstMention(i) mi is the first mention in its sentence.
reflexive(i) mi is reflexive.
possessive(i) mi is possessive.
definite(i) mi is definite noun phrase.
indefinite(i) mi is indefinite noun phrase.
demonstrative(i) mi is demonstrative.
describing the attributes of relations betweenmj andmi
mentionDistance(j,i,m) Distance between mj and mi in mentions.
sentenceDistance(j,i,s) Distance between mj and mi in sentences.
bothMatch(j,i,b) Gender and number of both mj and mi match: AGREE YES, AGREE NO
and AGREE UN).
closestMatch(j,i,c) mj is the first agreement in number and gender when looking backward
from mi: CAGREE YES, CAGREE NO and CAGREE UN.
exactStrMatch(j,i) Exact strings match between mj and mi.
pronounStrMatch(j,i) Both are pronouns and their strings match.
nopronounStrMatch(j,i) Both are not pronouns and their strings match.
properStrMatch(j,i) Both are proper names and their strings match.
headMatch(j,i) Head word strings match between mj and mi.
subStrMatch(j,i) Sub-word strings match between mj and mi.
animacyMatch(j,i) Animacy types match between mj and mi.
nested(j,i) mj/i is included in mi/j .
c command(j,i) mj/i C-Commands mi/j .
sameSpeaker(j,i) mj and mi have the same speaker.
entityTypeMatch(j,i) Entity types match between mj and mi.
alias(j,i) mj/i is an alias of mi/j .
srlMatch(j,i) mj and mi have the same semantic role.
verbMatch(j,i) mj and mi have semantic role for the same verb.
Table 1: Observed predicates.
pairwise classification, because the decisions are lo-
cal, we use a set of local formulas. For mention
clustering, we use global formulas to implement
best-first clustering or transitivity constraint. We
naturally combine pairwise classification with men-
tion clustering via local and global formulas in the
Markov logic framework, which is the essence of
?joint learning? in our work.
3.2.1 Local Formulas
A local formula relates any observed predicates to
exactly one hidden predicate. For our problem, we
define a list of observed predicates to describe the
properties of individual candidate mentions and the
relations between two candidate mentions, shown in
Table 1. For our problem, we have only one hidden
predicate, i.e. coref. Most of our local formulas are
from existing work (e.g. Soon et al(2001), Ng and
Cardie (2002), Sapena et al(2011)). They are listed
in Table 2, where the symbol ?+? indicates that for
every value of the variable preceding ?+? there is a
separate weight for the corresponding formula.
3.2.2 Global Formulas
Global formulas are designed to add global con-
straints for hidden predicates. Since in our problem
there is only one hidden predicate, i.e. coref, our
global formulas incorporate correlations among dif-
ferent ground atoms of the coref predicates. Next we
will show the best-first and transitivity global con-
straints. Note that we treat them as hard constraints
so we do not set any weights for these global formu-
las.
1248
Lexical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? exactStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? pronounStrMatch (j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? properStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nopronounStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? headMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? subStrMatch(j,i) ? j ?= i ? coref(j,i)
hasHead(j,h1+) ? hasHead(i,h2+) ? j ?= i ? coref(j,i)
Grammatical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? genderType(j,g1+) ? genderType(i,g2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? numberType(j,n1+) ? numberType(i,n2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? bothMatch(j,i,b+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? closestMatch(j,i,c+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? animacyMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nested(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? c command(j,i) ? j ?= i ? coref(j,i)
(mentionType(j,t1+) ? mentionType(i,t2+)) ? j ?= i ? coref(j,i)
(reflexive(j) ? reflexive(i)) ? j ?= i ? coref(j,i)
(possessive(j) ? possessive(i)) ? j ?= i ? coref(j,i)
(definite(j) ? definite(i)) ? j ?= i ? coref(j,i)
(indefinite(j) ? indefinite(i)) ? j ?= i ? coref(j,i)
(demonstrative(j) ? demonstrative(i)) ? j ?= i ? coref(j,i)
Distance and position Features
mentionType(j,t1+) ? mentionType(i,t2+) ? sentenceDistance(j,i,s+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? mentionDistance (j,i,m+) ? j ?= i ? coref(j,i)
(firstMention(j) ? firstMention(i)) ? j ?= i ? coref(j,i)
Semantic Features
mentionType(j,t1+) ? mentionType(i,t2+) ? alias(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? sameSpeaker(j,i) ? j?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? entityTypeMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? srlMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? verbMatch(j,i) ? j ?= i ? coref(j,i)
(entityType(j,e1+) ? entityType(i,e2+)) ? j ?= i ? coref(j,i)
Table 2: Local Formulas.
Best-First constraint:
coref(j, i) ? ?coref(k, i) ?j, k < i(k ?= j) (2)
Here we assume that coref(j,i) returns true if can-
didate mentions j and i are coreferential and false
otherwise. Therefore for each candidate mention i,
we should only select at most one candidate mention
j to return true for the predicate coref(j,i) from all its
preceding candidate mentions.
Transitivity constraint:
coref(j, k)?coref(k, i)?j < k < i ? coref(j, i) (3)
coref(j, k)?coref(j, i)?j < k < i ? coref(k, i) (4)
coref(j, i)?coref(k, i)?j < k < i ? coref(j, k) (5)
With the transitivity constraint, it means for given
mentions j, k and i, if any two pairs of them are
coreferential, then the third pair of them should be
also coreferential.
We use best-first clustering and transitivity con-
straint in our joint learning model respectively. De-
tailed comparisons between them will be shown in
Section 4.
3.3 Inference
We use MAP inference which is implemented by In-
teger Linear Programming (ILP). Its objective is to
maximize a posteriori probability as follows. Here
we use x to represent all the observed ground atoms
and y to represent the hidden ground atoms. For-
mally, we have
y? = argmax
y
p(y|x) ? argmax
y
s(y, x),
where
s(y, x) =
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y, x). (6)
Each hidden ground atom can only takes a value of
either 0 or 1. And global formulas should be satis-
fied as hard constraints when inferring the best y?. So
1249
the problem can be easily solved using ILP. Detailed
introduction about transforming groundMarkov net-
works in Markov logic into an ILP problem can be
found in (Riedel, 2008).
3.4 Parameter Learning
For parameter learning, we employ the online
learner MIRA (Crammer and Singer, 2003), which
establishes a large margin between the score of the
gold solution and all wrong solutions to learn the
weights. This is achieved by solving the quadratic
program as follows
min ? wt ?wt?1 ? . (7)
s.t. s(yi, xi)? s(y?, xi) ? L(yi, y?)
?y? ?= yi, (yi, xi) ? D
Here D = {(yi, xi)}Ni=1 represents N training in-
stances (each instance represents one single docu-
ment in the dataset) and t represents the number of
iterations. In our problem, we adopt 1-best MIRA,
which means that in each iteration we try to find wt
which can guarantee the difference between the right
solution yi and the best solution y? (i.e. the one with
the highest score s(y?, xi), equivalent to y? in Section
3.3)) is at least as big as the loss L(yi, y?), while
changing wt?1 as little as possible. The number of
false ground atoms of coref predicate is selected as
loss function in our experiments. Hard global con-
straints (i.e. best-first clustering or transitivity con-
straint) must be satisfied when inferring the best y?
in each iteration, which can make learned weights
more effective.
4 Experiments
In this section, we will first describe the dataset and
evaluation metrics we use. We will then present the
effect of our joint learning method, and finally dis-
cuss the comparison with the state of the art.
4.1 Data Set
We use the dataset from the CoNLL-2011 shared
task, ?Modeling Unrestricted Coreference in
OntoNotes? (Pradhan et al 2011)2. It uses the En-
glish portion of the OntoNotes v4.0 corpus. There
are three important differences between OntoNotes
2http://conll.cemantix.org/2011/
and another well-known coreference dataset from
ACE. First, OntoNotes does not label any singleton
entity cluster, which has only one reference in the
text. Second, only identity coreference is tagged in
OntoNotes, but not appositives or predicate nomi-
natives. Third, ACE only considers mentions which
belong to ACE entity types, whereas OntoNotes
considers more entity types. The shared task is to
automatically identify both entity coreference and
event coreference, although we only focus on entity
coreference in this paper. We don?t assume that
gold standard mention boundaries are given. So we
develop a heuristic method for mention detection.
See details in Section 2.1.
The training set consists of 1674 documents from
newswire, magazine articles, broadcast news, broad-
cast conversations and webpages, and the develop-
ment set consists of 202 documents from the same
source. For training set, there are 101264 mentions
from 26612 entities. And for development set, there
are 14291 mentions from 3752 entities (Pradhan et
al., 2011).
4.2 Evaluation Metrics
We use the same evaluation metrics as used in
CoNLL-2011. Specifically, for mention detection,
we use precision, recall and the F-measure. A men-
tion is considered to be correct only if it matches
the exact same span of characters in the annotation
key. For coreference resolution, MUC (Vilain et al
1995), B-CUBED (Bagga and Baldwin, 1998) and
CEAF-E (Luo, 2005) are used for evaluation. The
unweighted average F score of them is used to com-
pare different systems.
4.3 The Effect of Joint Learning
To assess the performance of our method, we set up
several variations of our system to compare with the
joint learning system. The MLN-Local system uses
only the local formulas described in Table 2 with-
out any global constraints under the MLN frame-
work. By default, the MLN-Local system uses the
single-link method to generate clustering results.
The MLN-Local+BF system replaces the single-link
method with best-first clustering to infer mention
clustering results after learning the weights for all
the local formulas. The MLN-Local+Trans sys-
tem replaces the best-first clustering with transitivity
1250
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Local 62.52 74.75 68.09 56.07 65.55 60.44 65.67 72.95 69.12 45.55 37.19 40.95 56.84
MLN-Local+BF 65.74 73.2 69.27 56.79 64.08 60.22 65.71 74.18 69.69 47.29 40.53 43.65 57.85
MLN-Local+Trans 68.49 70.32 69.40 57.16 60.98 59.01 66.97 72.90 69.81 46.96 43.34 45.08 57.97
MLN-Joint(BF) 64.36 75.25 69.38 55.47 66.95 60.67 64.14 77.75 70.29 50.47 39.85 44.53 58.50
MLN-Joint(Trans) 64.46 75.37 69.49 55.48 67.15 60.76 64.00 78.11 70.36 50.63 39.84 44.60 58.57
Table 3: Comparison between different MLN-based systems, using 10-fold cross validation on the training dataset.
constraint. The MLN-Joint system is a joint model
for both pairwise classification and mention cluster-
ing. It can combine either best-first clustering or en-
forcing transitivity constraint with pairwise classifi-
cation, and we denote these two variants of MLN-
Joint as MLN-Joint(BF) and MLN-Joint(Trans) re-
spectively.
To compare the performance of the various sys-
tems above, we use 10-fold cross validation on
the training dataset. We empirically find that our
method has a fast convergence rate, to learn the
MLN model, we set the number of iterations to be
10.
The performance of these compared systems is
shown in Table 3. To provide some context for
the performance of this task, we report the median
average F-score of the official results of CoNLL-
2011, which is 50.12 (Pradhan et al 2011). We can
see that MLN-Local achieves an average F-score of
56.84, which is well above the median score. When
adding best-first or transitivity constraint which
is independent of pairwise classification, MLN-
Local+BF and MLN-Local+Trans achieve better re-
sults of 57.85 and 57.97. Most of all, we can see
that the joint learning model (MLN-Joint(BF) or
MLN-Joint(Trans)) significantly outperforms inde-
pendent learning model (MLN-Local+BF or MLN-
Local+Trans) no matter whether best-first clustering
or transitivity constraint is used (based on a paired 2-
tailed t-test with p < 0.05) with the score of 58.50
or 58.57, which shows the effectiveness of our pro-
posed joint learning method.
Best-first clustering and transitivity constraint
are very useful in Markov logic framework, and
both MLN-Local and MLN-Joint benefit from them.
For MLN-Joint, these two clustering methods re-
sult in similar performance. But actually, transi-
tivity is harder than best-first, because it signifi-
cantly increases the number of formulas for con-
straints and slows down the learning process. In
our experiments, we find that MLN-Joint(Trans)3 is
much slower than MLN-Joint(BF). Overall, MLN-
Joint(BF) has a good trade-off between effectiveness
and efficiency.
4.4 Comparison with the State of the Art
In order to compare our method with the state-of-
the-art systems, we consider the following systems.
We implemented a traditional pairwise coreference
system using Maximum Entropy as the base classi-
fier and best-first clustering to link the results. We
used the same set of local features in MLN-Joint.
We refer to this system as MaxEnt+BF. To replace
best-first clustering with transitivity constraint, we
have another system named as MaxEnt+Trans. We
also consider the best 3 systems from CoNLL-2011
shared task. Chang?s system uses ILP to perform
best-first clustering after training a pairwise corefer-
ence model. Sapena?s system uses a relaxation label-
ing method to iteratively perform function optimiza-
tion for labeling each mention?s entity after learning
the weights for features under a C4.5 learner. Lee?s
system is a purely rule-based one. They use a battery
of sieves by precision (from highest to lowest) to it-
eratively choose antecedent for each mention. They
obtained the highest score in CoNLL-2011.
Table 4 shows the comparisons of our system with
the state-of-the-art systems on the development set
of CoNLL-2011. From the results, we can see that
our joint learning systems are obviously better than
3For MLN-Joint(Trans), not all training instances can be
learnt in a reasonable amount of time, so we set up a time out
threshold of 100 seconds. If the model cannot response in 100
seconds for some training instance, we remove it from the train-
ing set.
1251
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Joint(BF) 67.33 72.94 70.02 58.03 64.05 60.89 67.11 73.88 70.33 47.6 41.92 44.58 58.60
MLN-Joint(Trans) 67.28 72.88 69.97 58.00 64.10 60.90 67.12 74.13 70.45 47.70 41.96 44.65 58.67
MaxEnt+BF 60.54 76.64 67.64 52.20 68.52 59.26 60.85 80.15 69.18 51.6 37.05 43.13 57.19
MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31
Lee?s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60
Sapena?s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61
Chang?s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35
Table 4: Comparisons with state-of-the-art systems on the development dataset.
MaxEnt+BF and MaxEnt+Trans. They also out-
perform the learning-based systems of Sapena et al
(2011) and Chang et al(2011), and perform com-
petitively with Lee?s system (Lee et al 2011). Note
that Lee?s system is purely rule-based, while our
methods are developed in a theoretically sound way,
i.e., Markov logic framework.
5 Related Work
Supervised noun phrase coreference resolution has
been extensively studied. Besides the mention-pair
model, two other commonly used models are the
entity-mention model (Luo et al 2004; Yang et al
2008) and ranking models (Denis and Baldridge,
2008; Rahman and Ng, 2009). Interested readers
can refer to the literature review by Ng (2010).
Under the mention-pair model, Klenner (2007)
and Finkel and Manning (2008) applied Integer Lin-
ear Programming (ILP) to enforce transitivity on the
pairwise classification results. Chang et al(2011)
used the same ILP technique to incorporate best-first
clustering and generate the mention clusters. In all
these studies, however, mention clustering is com-
bined with pairwise classification only at the infer-
ence stage but not at the learning stage.
To perform joint learning of pairwise classifi-
cation and mention clustering, in (McCallum and
Wellner, 2005), each mention pair corresponds to
a binary variable indicating whether the two men-
tions are coreferential, and the dependence between
these variables is modeled by conditional undirected
graphical models. Finley and Joachims (2005) pro-
posed a general SVM-based framework for super-
vised clustering that learns item-pair similarity mea-
sures, and applied the framework to noun phrase
coreference resolution. In our work, we take a differ-
ent approach and apply Markov logic. As we have
shown in Section 3, given the flexibility of Markov
logic, it is straightforward to perform joint learning
of pairwise classification and mention clustering.
In recent years, Markov logic has been widely
used in natural language processing problems (Poon
and Domingos, 2009; Yoshikawa et al 2009; Che
and Liu, 2010). For coreference resolution, the most
notable one is unsupervised coreference resolution
by Poon and Domingos (2008). Poon and Domin-
gos (2008) followed the entity-mention model while
we follow the mention-pair model, which are quite
different approaches. To seek good performance in
an unsupervised way, Poon and Domingos (2008)
highly rely on two important strong indicators:
appositives and predicate nominatives. However,
OntoNotes corpus (state-of-art NLP data collection)
on coreference layer for CoNLL-2011 has excluded
these two conditions of annotations (appositives and
predicate nominatives) from their judging guide-
lines. Compared with it, our methods are more ap-
plicable for real dataset. Huang et al(2009) used
Markov logic to predict coreference probabilities
for mention pairs followed by correlation cluster-
ing to generate the final results. Although they also
perform joint learning, at the inference stage, they
still make pairwise coreference decisions and clus-
ter mentions sequentially. Unlike their method, We
formulate the two steps into a single framework.
Besides combining pairwise classification and
mention clustering, there has also been some work
that jointly performs mention detection and coref-
erence resolution. Daume? and Marcu (2005) de-
veloped such a model based on the Learning as
1252
Search Optimization (LaSO) framework. Rahman
and Ng (2009) proposed to learn a cluster-ranker
for discourse-new mention detection jointly with
coreference resolution. Denis and Baldridge (2007)
adopted an Integer Linear Programming (ILP) for-
mulation for coreference resolution which models
anaphoricity and coreference as a joint task.
6 Conclusion
In this paper we present a joint learning method with
Markov logic which naturally combines pairwise
classification and mention clustering. Experimental
results show that the joint learning method signifi-
cantly outperforms baseline methods. Our method
is also better than all the learning-based systems in
CoNLL-2011 and reaches the same level of perfor-
mance with the best system.
In the future we will try to design more global
constraints and explore deeper relations between
training instances generation and mention cluster-
ing. We will also attempt to introduce more predi-
cates and transform structure learning techniques for
MLN into coreference problems.
Acknowledgments
Part of the work was done when the first author
was a visiting student in the Singapore Manage-
ment University. And this work was partially sup-
ported by the National High Technology Research
and Development Program of China(863 Program)
(No.2012AA011101), the National Natural Science
Foundation of China (No.91024009, No.60973053,
No.90920011), and the Specialized Research Fund
for the Doctoral Program of Higher Education of
China (Grant No. 20090001110047).
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference pro-
tocols for coreference resolution. In CoNLL Shared
Task, pages 40?44, Portland, Oregon, USA. Associa-
tion for Computational Linguistics.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 161?169.
Tsinghua University Press.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
III Hal Daume? and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT ?05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 97?104, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236?243, Rochester, New York, April. Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
EMNLP, pages 660?669.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
ACL (Short Papers), pages 45?48. The Association for
Computer Linguistics.
T. Finley and T. Joachims. 2005. Supervised clustering
with support vector machines. In International Con-
ference on Machine Learning (ICML), pages 217?224.
Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jia-
jun Chen. 2009. Coreference resolution using markov
logic networks. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing: 10th Interna-
tional Conference, CICLing 2009.
M. Klenner. 2007. Enforcing consistency on coreference
sets. In RANLP.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, A Kamb-
hatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of the ACL, pages 135?142.
1253
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proc. of HLT/EMNLP, pages 25?
32.
Andrew McCallum and Ben Wellner. 2005. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems, pages 905?912. MIT Press.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104?111.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In ACL, pages 1396?
1411. The Association for Computer Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
EMNLP, pages 650?659.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1?10.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968?977.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of map inference for markov logic. In UAI,
pages 468?475. AUAI Press.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2011. Re-
laxcor participation in conll shared task on coreference
resolution. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 35?39, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo
Poesio. 2011. Multi-metric optimization for coref-
erence: The unitn / iitp / essex submission to the 2011
conll shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 61?65, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Marc B. Vilain, John D. Burger, John S. Aberdeen, Den-
nis Connolly, and Lynette Hirschman. 1995. Amodel-
theoretic coreference scoring scheme. In MUC, pages
45?52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In ACL, pages 843?851. The Association for
Computer Linguistics.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In ACL/AFNLP,
pages 405?413. The Association for Computer Lin-
guistics.
1254
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1466?1477, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Identifying Event-related Bursts via Social Media Activities
Wayne Xin Zhao?, Baihan Shu?, Jing Jiang?, Yang Song?, Hongfei Yan?? and Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,baihan.shu,yhf1029}@gmail.com,ysong@pku.edu.cn
jingjiang@smu.edu.sg, lxm@pku.edu.cn
Abstract
Activities on social media increase at a dra-
matic rate. When an external event happens,
there is a surge in the degree of activities re-
lated to the event. These activities may be
temporally correlated with one another, but
they may also capture different aspects of an
event and therefore exhibit different bursty
patterns. In this paper, we propose to iden-
tify event-related bursts via social media activ-
ities. We study how to correlate multiple types
of activities to derive a global bursty pattern.
To model smoothness of one state sequence,
we propose a novel function which can cap-
ture the state context. The experiments on a
large Twitter dataset shows our methods are
very effective.
1 Introduction
Online social networks (e.g., Twitter, Facebook,
Myspace) significantly influence the way we live.
Activities on social media increase at a dramatic
rate. Millions of users engage in a diverse range
of routine activities on social media such as posting
blog messages, images, videos or status messages,
as well as interacting with items generated by oth-
ers such as forwarding messages. When an event
interesting to a certain group of individuals takes
place, there is usually a surge in the degree of ac-
tivities related to the event (e.g., a sudden explosion
of tweets). Since social media activities may indi-
cate the happenings of external events, can we lever-
age on the rich social media activities to help iden-
tify meaningful external events? This is the research
problem we study in this paper. By external events,
we refer to real-world events that happen external to
the online space.
?Corresponding author.
2 4 6 8 100
2040
6080
100120
140
Time index 
 all?tweetsretweetsurl?embeddedtweetsNoise
(a) Query=?Amazon.?
2 4 6 8 100
50
100
150
Time index 
 all?tweetsretweetsurl?embeddedtweetsNoise
(b) Query=?Eclipse?.
Figure 1: The amount of activities within a 10-hour
window for two queries. Three types of activities
are considered: (1) posting a tweet (upward triangle),
(2) retweet (downward triangle), (3) posting a URL-
embedded tweet (excluding retweet) (filled circle). As
explained in Table 1, both bursts above are noisy.
Mining events from text streams is usually
achieved by detecting bursty patterns (Swan and Al-
lan, 2000; Kleinberg, 2003; Fung et al 2005). How-
ever, previous work has mostly focused on tradi-
tional text streams such as scientific publications and
news articles. There is still a lack of systematic in-
vestigations into the problem of identifying event-
related bursty patterns via social media activities.
There are at least two basic characteristics of social
media that make the problem more interesting and
challenging.
First, social media involve various types of activ-
ities taking place in real time. These activities may
be temporally correlated with one another, but they
may also capture different aspects of an event and
therefore exhibit different bursty patterns. Most of
previous methods (Swan and Allan, 2000; Klein-
berg, 2003; Fung et al 2005) deal with a single type
of textual activities. When applied to social media,
they oversimplify the complex nature of online so-
1466
Bursty Activity Time # in Sr # in Su # in St Noisy?
Sr,St 23:00?23:59, Nov. 23, 2009 108 5 147 Y
See Fig. 1(b) [Query=eclipse] major bursty reason: The tweet from Robert Pattinson ?@twilight: from rob cont .
- i hope you are looking forward to eclipse as much as i am .? has been retweeted many times.
Su,St 07:00?07:59, Jul. 25, 2009 6 122 133 Y
See Fig. 1(a) [Query=Amazon] major bursty reason: Advertisement tweets like ?@fitnessjunkies amazon.com deals
: http://tinyurl.com/lakz3h.? have been posted many times.
St,Su,Sr 09:00?9:59, Oct. 9, 2009 1562 423 2848 N
[Query=Nobel] major bursty reason: The news ?Obama won Nobel Peace Prize? flood Twitter.
Table 1: Examples of bursts. The first two bursts are judged as noise since they do not correspond to any meaningful
external events. In fact, the reasons why a burst appears in social media can be quite diverse. In this paper, we only
focus on event-related bursts. St denotes posting a tweet, Su denotes posting a url-embedded tweet, and Sr denotes
retweet.
cial activities, and therefore they may not be well
suitable to social media. Let us consider a moti-
vating example. Figure 1 shows the change of the
amount of activities of three different types over a
10-hour time window for two queries. If we consider
only the total number of tweets, we can see that for
both queries there is a burst. However, neither of the
two bursts corresponds to a real-world event. The
first burst was caused by the broadcast of an adver-
tisement from several Twitter bots, and the second
burst was caused by numerous retweets of a status
update of a movie star1. The detailed explanations
of why the two bursts are noisy are also shown in
Table 1. On the other hand, interestingly, we can see
that not all the activity streams display noisy bursty
patterns at the same time. It indicates that we may
make use of multiple views of different activities
to detect event-related bursts. The intuition is that
using multiple types of activities may help learn a
better global picture of event-related bursty patterns.
Learning may also be more resistant to noisy bursts.
Second, in social media, burst detection is chal-
lenged by irregular, unpredictable and spurious
noisy bursts. To overcome this challenge, a reason-
able assumption is that a burst corresponding to a
real event should not fluctuate too much within a
relatively short time window. To illustrate it, we
present an example in Figure 2, in which we first
use a simple threshold method to detect bursts and
then analyze the effect of local smoothness. In par-
ticular, if the amount of activities at a certain time
is above a pre-defined threshold, we set its state to
1, which indicates a bursty state. Otherwise, we set
the state to 0. Figure 2(a) shows that for the query
?Eclipse,? with a threshold of 50, the state sequence
for the time window we consider is ?0000100000.?
1The reasons for these bursts were revealed by manually
checking the tweets during the corresponding periods.
1 2 3 4 5 6 7 8 9 100
50
100
150 Correct0000000000Threshold0000100000
(a) Query=?Eclipse?.
1 2 3 4 5 6 7 8 9 100
5001000
15002000
25003000 Correct0000011111Threshold0000011111
(b) Query=?Nobel?.
Figure 2: Analysis of the effect of local smoothness on
threshold method. It shows two examples of threshold
methods for burst detection in a 10-hour window. The
red line denotes the bursty threshold. If the number of
activities is above the threshold in one time interval, the
state of this time interval is judge as bursty. Detailed de-
scriptions of these cases are shown in Table 1.
Although there is a burst in this sequence, its dura-
tion is very short. In fact, this is the first example
shown in Table 1, which is a noisy burst. In con-
trast, in Figure 2(b), the state sequence for the query
?Nobel? is ?0000011111,? in which the longer and
smoother burst corresponds to a true event. A good
function for evaluating the smoothness of a state se-
quence should be able to discriminate these cases
and model the context of state sequences effectively.
With its unique characteristics and challenges,
there is an emergent need to deeply study the prob-
lem of event-related burst detection via social me-
dia activities. In this paper, we conduct a system-
atic investigation on this problem. We formulate
this problem as burst detection from time series of
social media activities. We develop an optimiza-
tion model to learn bursty patterns based on multiple
types of activities. We propose to detect bursts by
considering both local state smoothness and correla-
tion across multiple streams. We define a function to
1467
quantitatively measure local smoothness of one sin-
gle state sequence. We systematically evaluate three
types of activities for burst detection on a large Twit-
ter dataset and analyze different properties of these
three streams for burst detection.
2 Problem Definition
Before formally introducing our problems, we first
define some basic concepts.
Activity: An activity refers to some type of action
that users perform when they are interested in some
topic or event.
Activity Stream: An activity stream of length
N and type m is a sequence of numbers
(nm1 , n
m
2 , ..., n
m
N ), where each n
m
i denotes the
amount of activities of type m that occur during the
ith time interval.
Query: A queryQ is a sequence of terms q1, ..., q|Q|
which can represent the information needs of users.
For example, an example query related to President
Obama is ?barack obama.?
Event-related Burst: Given a query Q, an event-
related burst is defined as a period [ts, te] in which
some event related with Q takes place, where ts and
te are the start timestamp and end timestamp of the
event period respectively. During the event period
the amount of activities is significantly higher than
average.
Based on these definitions, our task is to try to
identify event-related bursts via multiple social me-
dia activity streams.
3 Identifying Event-related Bursts from
Social Media
In this section, we discuss how to identify event-
related bursts via social media activities. For-
mally, given a query Q, we first build M ac-
tivity streams related with Q on T timestamps:
{(nm1 , ..., n
m
T )}
M
m=1. The definition of activity in our
methods is very general; it includes various types of
social media activities, including textual and non-
textual activities, e.g., a click on a shared photo and
a link formation between two users.
Given the input, we try to infer a state sequence
over these T timestamps: z = (z1, ..., zT ), where
zi is 1 or 0. 1 indicates a time point within a burst
while 0 indicates a non-bursty time point.
3.1 Modeling a Single Activity Stream
3.1.1 Generation function
In probability theory and statistics, the Poisson
distribution2 is a discrete probability distribution
that can measure the probability of a given number
of ?activities? occurring in a fixed time interval. We
use the Poisson distribution to study the probability
of observing the number of social media activities,
and we treat one hour as one time interval in this
paper.
Homogeneous Poisson Distribution The genera-
tive probability of the ith number in one activity
stream of type m is defined as f(nmi , i, z
m
i ) =
(?zmi
)n
m
i exp(??zmi
)
nmi !
, where ?0 is the (normal) expec-
tation of the number of activities in one time inter-
val. If one state is bursty, it would emit activities
with a faster rate and result in a larger expectation
?1. We can set ?1 = ?0 ? ?, where ? > 1.
Heterogeneous Poisson Distribution The two-state
machine in (Kleinberg, 2003) used two global refer-
ences for all the time intervals: one for bursty and
the other for non-bursty. In our experiments, we ob-
serve temporal patterns of user behaviors, i.e., activ-
ities in some hours are significantly more than those
in the others. Instead of using fixed global rates ?0
and ?1, we try to model temporal patterns of user
behaviors by parameterizing ?(?) with the time in-
dex. By following (Ihler et al 2006), we use a
set of hour-specific rates {?1,h}24h=1 and {?0,h}
24
h=1.
3
Given a time index h, we set ?0,h to be the expecta-
tion of the number of activities in hth time interval
every day, then we have ?1,h = ?0,h ? ?. In this
paper, ? is empirally set as 1.5.
3.1.2 Smoothness of a State Sequence
For burst detection, the major aim is to identify
steady and meaningful bursts and to discard tran-
sient and spurious bursts. Given a state sequence
z1z2...zT , to quantitatively measure the smoothness
and compactness of it, we introduce some measures.
One simple method is to count the number of
change in the state sequence. Formally, we use the
following formula:
g1(z) = T ?
T?1?
i=1
I(zi 6= zi+1), (1)
2http://en.wikipedia.org/wiki/Poisson distribution
3We can also make the rates both day-specific and hour-
specific, i.e., {?(?),d,h}h?{1,...,24},d?{1,...,7}.
1468
where T is length of the state sequence and I(?)
is an indicator function which returns 1 only if the
statement is true. Let us take the state sequence
?0000100000? (shown in Figure 2(a)) as an example
to see how g1 works. State changes 0pos=4 ? 1pos=5
and 1pos=5 ? 0pos=6 each incur a cost of 1, there-
fore g1(0000100000) = 10 ? 2 = 8. Similarly, we
can get g1(0000000000) = 10. There is a cost dif-
ference between these two sequences, i.e., ?g1 = 2.
Kleinberg (2003) uses state transition probabilities
to model the smoothness of state sequences. With
simple derivations, we can show that Kleinberg?s
model essentially also uses a cost function that is
linear in terms of the number of state changes in a
sequence, and therefore similar to g1.
In social media, very short noisy bursts like
?0000100000? are very frequent. To discard such
noises, we may multiply g1 by a big cost factor to
punish short-term fluctuations. However, it is not
sensitive to the state context4 and may affect the
detection of meaningful bursts. For example, state
change 0pos=4 ? 1pos=5 in ?0000111100? would
receive the same cost as that of 0pos=4 ? 1pos=5
in ?0000100000? although the later is more like a
noise.
To better measure the smoothness of a state se-
quence , we propose a novel context-sensitive func-
tion, which sums the square of the length of the max-
imum subsequences in which all states are the same.
Formally, we have
g2(z1, z2, ..., zT ) =
?
si<ei
(ei ? si + 1)
2, (2)
where si and ei are the start index and end in-
dex of the ith subsequence respectively. To define
?maximum?, we have the constraints zsi = zsi+1 =
... = zei , zsi?1 6= zsi , zei 6= zei+1. For example,
g2(0000000000)= 102 = 100, g2(0000100000)=
42 + 12 + 52 = 42, we can see that ?g2 =
100 ? 42 = 58, which is significantly larger than
?g1(= 2). g2 rewards the continunity of state se-
quences while punish the fluctuating changes, and it
is context-sensitive. State change 0pos=4 ? 1pos=5
in ?0000111100? receives a cost of 4,5 which is
4Here context refers to the window of hidden state se-
quences.
5Indeed, g2 is not designed for a single state change but for
the overall smoothness patterns, so we choose a referring se-
quence generated by making the corresponding state negative to
compute the cost, i.e., |g2(0000011110)?g2(0000001110)| =
4.
much smaller than that of 0pos=4 ? 1pos=5 in
?0000100000?. g2 is also sensitive to the po-
sition of state changes, e.g., g2(0000100000) 6=
g2(0100000000).
3.2 Burst Detection from a Single Activity
Stream
Given an activity stream (nm1 , ..., n
m
T ), we would
like to infer a state sequence over these T times-
tamps, i.e., to find out the most possible state se-
quence z = (zm1 , ..., z
m
T ) based on the data, where
zmi = 1 or 0. We formulate this problem as
an optimization problem. The cost of a state se-
quence includes two parts: generation of activities
and smoothness of the state sequence. The objective
function is to find a state sequence which incurs the
minimum cost. Formally, we define the total cost
function as
Cost(z) = ?
T?
i=1
log f(nmi , i, z
m
i )
? ?? ?
generating cost
+
(
? ?(zm1 , ..., z
m
T ) ? ?1
)
? ?? ?
smoothness cost
,
(3)
where ?1 > 0 is a scaling factor which balance these
two parts. ?(?) function is the smoothness function,
and we can set it as either g1(?) or g2(?).
To seek the optimal state sequence, we can min-
imize Equation 3. However, exact inference is hard
due to the exponential search space. Instead of ex-
amining the smoothness of the whole state sequence,
we propose to measure the smoothness of all the L-
length subsequences, so called ?local smoothness?.
The assumption is that the states in a relatively short
time window should not change too much. The new
objective function is defined as
Cost(z) = ?
T?
i=1
log f(nmi , i, z
m
i ) (4)
?
(
?
i?L
?(zmi , ..., z
m
i+L?1)
)
? ?1.
The objective function in Equation 4 can be
solved efficiently by a dynamic programming algo-
rithm shown in Algorithm 1. The time complexity
of this algorithm is O(T ? 2L). Note that the meth-
ods we present in Equation 4 and Algorithm 1 are
quite general. They are independent of the concrete
forms of f(?) and ?(?), which leaves room for flexi-
ble adaptation or extension in specific tasks. In pre-
vious methods (Kleinberg, 2003), L is often fixed as
1469
2. Indeed, as shown in Figure 2, in some cases, we
may need a longer window to infer the global pat-
terns. In our model, L can be tuned based on real
datasets. We can seek a trade-off between efficiency
and length of context windows.
Algorithm 1: Dynamic Programming for Equation 4.
d[i][s][zi...zi?L+1] denotes the minimum cost of the first1
i timestamps with the state subsequence: zi...zi?L+1 and
zi = s;
set d[0][?][?] = 0;2
set c1[i] = log f(nmi , i, z
m
i );3
set c2[i] = ?(zi, ..., zi?L+1);4
b, b?: previous and current state window are represented as5
L-bit binary numbers;
for i = 1 to T do6
for s = 0 to 1 do7
for b = 0 to 2L ? 1 do8
b? = (b << 1|s)&(1 << L? 1);9
d[i][s][b?]??10
min(d[i][s][b?], d[i?1][s][b]+c1[i]+c2[i]);
end11
end12
end13
3.3 Correlating Multiple Activity Streams
In this section, we discuss how to correlate multi-
ple activity streams to learn a global bursty patterns.
The hidden state sequences corresponding to these
activity streams are not fully independent. An ex-
ternal event may intricate surges in multiple activity
streams simultaneously.
We propose to correlate multiple activity streams
in an optimization model. The idea is that activ-
ity streams related with one query might be depen-
dent, i.e., the states of multiple activity streams on
the same timestamp tend to be the same6; if not,
it would incur a cost. To implement this idea, we
develop an optimization model. For convenience,
we call the states of each activity stream as ?local
states? while the overall states learnt from multiple
activity streams as ?global states?.
The idea is that although various activity streams
are different in the scale of frequencies, they tend to
share similar trend patterns. We incorporate the cor-
relation between local states on the same timestamp.
6In our experiments, we compute the cross correlation be-
tween different streams with a lag factor ?, we find the cross
correlation achieves maximum consistantly when ? = 0.
Formally, we have
Cost(Z) =
M?
m=1
{
?
T?
i=1
log f(nmi , i, z
m
i )
?
?
i?L
?(zmi , ..., z
m
i+L?1) ? ?1
}
+
T?
i=1
?
m1,m2
I(zm1i 6= z
m2
i ) ? ?2, (5)
where I(?) is indicator function, and ?2 is the cost
when a pair of states are different across multiple
streams on the same timestamp.
The objective function in Equation 5 can be
solved by a dynamic programming algorithm pre-
sented in Algorithm 2. The time complexity of this
algorithm is O(T ? 2M ?L+M ). Generally, L can be
set as one small value, e.g., L =2 to 6, and we can
select just a few representative activity streams, i.e.,
M =2 to 6. In this case, the algorithm can be effi-
cient.
Algorithm 2: Dynamic Programming for Equation 5.
d[i][z1i ...z
1
i?L+1; ...; z
M
i ...z
M
i?L+1] denotes the minimum1
cost of the first i timestamps with the local state
subsequence zmi ...z
m
i?L+1 in the mth stream;
set d[0][...] = 0;2
bl, bl
?
: previous and current state windows represented as3
M ? L-bit binary numbers;
c[i, bl, bl
?
] denotes all the cost in the tth timestamp;4
for i = 1 to T do5
for bl = 0 to 2M?L ? 1 do6
deriving current local state sequences bl
?
from bl;7
d[i][b?l]??8
min(d[i][bl
?
], d[i? 1][bl] + c[i, bl, bl
?
]);
end9
end10
Given M types of activity streams, we can get
M (local) state sequences {(zm1 , ..., z
m
T )}
M
m=1. The
next question is how to learn a global state sequence
(zG1 , ..., z
G
T ) based on local state sequences. Here we
give a few options:
CONJUNCT: we set a global state zi as bursty if
all local states are bursty, i.e., zGi = ?
M
m=1z
m
i .
DISJUNCT: we set a global state zi as bursty if
one of the local states is bursty, i.e., zGi = ?
M
m=1z
m
i .
BELIEF: we set a global state zi as the most con-
fident local state, i.e., zGi = argmaxmbelief(z
m
i ).
The belief(?) function can be defined as the ratio be-
tween generating costs from states zmi and 1 ? z
m
i :
belief(zmi ) =
f(nmi ,i,z
m
i )
f(nmi ,i,1?z
m
i )
.
1470
Table 2: Basic statistics of our golden test collection.
# of queries 17
Aver. # of event-related bursts per query 19
Min. bursty interval 3 hours
Max. bursty interval 163 hours
Aver. bursty interval 17.8 hours
L2G: we treat the states of one local stream as the
global states.
4 Experiments
4.1 Construction of Test Collection
We test our algorithms on a large Twitter dataset,
which contains about 200 million tweets and ranges
from July, 2009 to December 2009. We manually
constructed a list of 17 queries that have high vol-
umes of relevant tweets during this period. These
queries have a very broad coverage of topics. Exam-
ple queries are ?Barack Obama?, ?Apple?, ?Earth-
quake?, ?F1? and ?Nobel Prize?. For each query, we
invite two senior graduate students to manually iden-
tify their golden bursty intervals, and each bursty in-
terval is represented as a pair of timestamps in terms
of hours. Specifically, to generate the golden stan-
dard, given a query, the judges first manually gen-
erate a candidate list of external events7; then for
each event, they look into the tweets within the cor-
responding period and check whether there is a surge
on the frequency of tweets. If so, the judges fur-
ther determine the start timepoint and end timepoint
of it. If there is a conflict, a third judge will make
the final decision. We used Cohen?s kappa coeffi-
cient to measure the agreement of between the first
two judges, which turned out to be 0.67, indicating a
good level of agreement8. We present basic statistics
of the test collection in Table 2.
4.2 Evaluation Metrics
Before introducing our evaluation metrics, we first
define the Bursty Interval Overlap Ratio (BIOR)
BIOR(f,X ) =
?
f ??X ?l(f, f
?)
L(f)
,
f is a bursty interval, ?l(f, f ?) is the length of
overlap between f ? and f , L(f) is the length of
7We refer to some gold news resources, e.g., Google News
and Yahoo! News.
8http://en.wikipedia.org/wiki/Cohen?s kappa
Figure 3: Examples to illustrate BIOR. X0, X1
and X2 are three sets of bursty intervals. X0
and X2 consist of one interval, and X1 consists of
two intervals. BIOR(f,X0)=1, BIOR(f,X1)=0.5 and
BIOR(f,X2)=0.5.
bursty period of f . X is a set of bursty intervals,
BIOR measures the proportion of the timestamps in
f which are covered by one of bursty intervals in
X . We use BIOR to measure partial match of inter-
vals, because a system may not return all the exact
bursty intervals9. We show some examples of BIOR
in Figure 3.
We use modified Precision, Recall and F as ba-
sic measures. Given one query, P, R and F can be
defined as follows
R =
?
f?B I
(
1
|Mf |
BIOR(f,M) > 0.5
)
|B|
,
P =
1
|M|
?
f ??M
(BIOR(f ?,B)),
F =
2? P ?R
P + R
,
where M is the set of bursty intervals identified
by one candidate method, B is the set of bursty in-
tervals in golden standards, and Mf is the set of in-
tervals which overlap with f in M. We incorporate
the factor 1|Mf | in Recall to penalize the incontin-
uous coverage of the golden interval, and we also
require that the overlap ratio with penalized factor
is higher than a threshold of 0.5. Given two sets of
bursty intervals which have the same value of BIOR,
we prefer the one with fewer intervals. In Figure 3,
we can easily derive X1 and X2 have the same value
9A simple evaluation method is that we label each one hour
time slot as being part of a burst or not and compare with the
gold standard. However, in our experiments, we find that some
methods tend to break one meaningful burst into small parts and
easier to be affected by small fluctuations although they may
have a good coverage of bursty points. This is why we adopt a
different evaluation approach.
1471
Table 3: Average cross-correlation between different
streams.
St Sr Su
St 1 0.830235 0.851514
Sr 0.830235 1 0.59905
Su 0.851514 0.59905 1
of BIOR, when computing Recall, we prefer X2 to
X1 since X2 consists of only one complete inter-
val whileX1 consists of two inconsecutive intervals.
I(?) is an indicator function which returns 1 only if
the statement if true. In our experiments, we use the
average of R, P and F over all test queries.
4.3 Experiment Setup
Selecting activity streams
We consider three types of activity streams in
Twitter: 1) posting a tweet, denoted as St; 2) for-
warding a tweet (retweet), denoted as Sr; 3) post-
ing a URL-embedded tweet, denoted as Su. It is
natural to test the performance of St in discover-
ing bursty patterns, while Su and Sr measure the
influence of external events on users in Twitter in
two different aspects. Sr: An important convention
in Twitter is the ?retweeting? mechanism, through
which users can actively spread the news or related
information; Su: Another characteristic of Twitter is
that the length of tweets is limited to 140 characters,
which constrains the capacity of information. Users
often embed a URL link in the tweets to help others
know more about the corresponding information.
We compute the average cross correlation be-
tween different activity streams for these 17 queries
in our test collection, and we summarize the results
in Table 3. We can see that both Sr and Su have a
high correlation with St, and Sr has a relatively low
correlation with Su. 10
Methods for comparisons
S(?): using Equation 4 and considers a single ac-
tivity stream, namely St, Su and Sr.
MBurst(?): using Equation 5 and considers mul-
tiple activity streams.
To compare our methods with previous methods,
we adopt the following baselines:
StateMachine: This is the method proposed
in (Kleinberg, 2003). We use heterogeneous Poisson
10We also consider the frequencies of unique users by hours,
however, we find it has a extremely high correlation coefficient
with St, about 0.99, so we do not incorporate it.
function as generating functions instead of binomial
function Cnk because sometimes it is difficult to get
the exact total number n in social media.
Threshold: If we find that the count in one time
interval is higher than a predefined threshold, it is
treated as a burst. The threshold is set as 1.5 times
of the average number.
PeakFinding: This is the method proposed
in (Marcus et al 2011), which aims to automatically
discover peaks from tweets.
Binomial: This is the method proposed in (Fung et
al., 2007a), which uses a cumulative binomial distri-
bution with a base probability estimated by remov-
ing abnormal frequencies.
As for multiple-stream burst detection, to the best
of our knowledge, the only existing work is pro-
posed by (Yao et al 2010), which is supervised and
requires a considerable amount of training time, so
we do not compare our work with it. We compare
our method with the following heuristic baselines:
SimpleConjunct: we first find the optimal state se-
quences for each single activity stream. We then de-
rive a global state sequence by taking the conjunc-
tion of all local states.
SimpleDisjunct: we first find the optimal state se-
quences for each single activity stream, and then we
derive a global state sequence by take the disjunction
of all local states.
Another possible baseline is that we first merge
all the activities, then apply the single-stream algo-
rithm. However, in our data set, we find that the
number of activities in St is significantly larger than
that of the two types. St dominantly determines the
final performance, so we do not incorporate it here
as a comparison.
4.4 Experimental Results
Preliminary results on a single stream
We first examine the performance of our proposed
method on a single stream. Note that, our method
in Equation 4 has two merits: 1) the length of lo-
cal window can be tuned on different datasets; 2) a
novel state smoothness function is adopted.
We set the ? function in Equation 4 respectively
as g1 and g2, and apply our proposed methods to
three streams (St,Sr,Su) mentioned above. Note
that, when L = 2 and ? = g1, our method becomes
the algorithm in (Kleinberg, 2003). We tune the pa-
rameter ?1 in Equation 4 from 2 to 20 with a step of
2. We record the best F performance and compute
1472
the corresponding standard deviation. In Table 5, we
can observe that 1) streams St and Sr perform better
than Su; 2) the length of local window significantly
affects the performance; 3) g2 is much better than g1
in our proposed burst detection algorithm; 4) gen-
erally speaking, a longer window size (L = 3, 4)
performs better than the most common used size 2
in (Kleinberg, 2003).
We can see that our proposed method is more ef-
fective than the other baselines. The major reason is
that none of these methods consider state smooth-
ness in a systematic way. In our preliminary ex-
periments, we find that these baselines usually out-
put a lot of bursts, most of which are broken mean-
ingful bursts. To overcome this, baseline method
StateMachine (g1 + L = 2) requires larger ? and
?1, which may discard relatively small meaningful
bursts; while our proposed single stream method
(g2 + L = 3, 4) tends to identify steady and con-
secutive bursts through the help of longer context
window and context sensitive smoothness function
g2, it is more suitable to be applied to social media
for burst detection.
Compared with the other baselines, (Kleinberg,
2003) is still one good and robust baseline since it
models the state smoothness partially. These prelim-
inary findings indicate that state smoothness is very
important for burst detection, and the length of state
context window will affect the performance signifi-
cantly.
To get a deep analysis of the performance of dif-
ferent streams, we set up three classes, and each
class corresponds to a single stream. Since for each
query, we can obtain multiple results in different ac-
tivity streams, we further categorize the 17 anno-
tated queries to the stream which leads to the opti-
mal performance on that query. Interestingly, we can
see: 1) the url stream gives better performance on
queries about big companies because users in Twit-
ter usually talk about the release of new products
or important evolutionary news via url-embedded
tweets; 2) the retweet stream gives better perfor-
mance on queries which correspond to unexpected
or significant events, e.g., diasters. It is consistent
with our intuitions that users in Twitter do actively
spread such information. Combining previous anal-
ysis of Table 5, overall we find the retweet stream is
more capable to identify bursts which correspond to
significant events.
Table 4: Categorization of 17 queries according to the
optimal performance.
Streams Queries
url Apple,Microsoft,Nokia, climate
retweet bomb,crash,earthquake,typhoon,
F1,Google,Olympics
all tweet Amazon, eclipse, Lakers,
NASA, Nobel Prize, Barack Obama
Table 5: Performance (average F) on a single stream.
???? indicates that the improvement our proposed single-
stream methodg2,L=4 over all the other baselines is ac-
cepted at the confidence level of 0.95, i.e., StateMachine,
PeakingFinding, Binomial and Threshold.
? L St Sr Su
4 0.545/0.015 0.543/0.037 0.451/0.036
g2 3 0.536/0.013 0.549??/0.019 0.464/0.025
2 0.468/0.055 0.542/0.071 0.455/0.045
4 0.513/0.059 0.546/0.058 0.465/0.047
g1 3 0.469/0.055 0.542/0.071 0.455/0.045
2 0.396/0.043 0.489/0.074 0.374/0.035
StateMachine 0.396 0.489 0.374
PeakFinding 0.410 0.356 0.302
Binomial 0.315 0.420 0.341
Threshold 0.195 0.181 0.175
Preliminary results on multiple streams
After examining the basic results on a single
stream, we continue to evaluate the performance of
our proposed models on multiple activity streams.
For MBurst in Equation 5, we have three parame-
ters to set, namely L, ?1 and ?2. We do a grid search
for both ?1 and ?2 from 1 to 12 with a step of 1, and
we also examine the performance when L = 2, 3, 4.
We can see that MBurst has four candidate meth-
ods to derive global states from local states; for L2G,
we use the states of St as the final states, and we em-
pirically find that it performs best compared with the
other two streams in L2G.
Recall that our proposed single-stream method
is better than all the other single-stream baselines,
so here single-best denotes our method in Equa-
tion 4 (? = g2, L = 4) on Sr. For SimpleConjunct
and SimpleDisjunct, we first find the optimal state
sequences for each single activity stream using our
proposed method in Equation 4 (? = g2, L = 4),
and then we derive a global state sequence by take
the conjunction or disjunction of all local states re-
spectively.
Besides the best performance, we further compute
the average of the top 10 results of each method
by tuning parameters to check the average perfor-
1473
Table 6: Performance (average F) on multiple streams.
??? indicates that the improvement our proposed
multiple-stream method over our proposed single-stream
method at the confidence level of 0.9 in terms of average
performance.
Methods best average
single-best (g2 + Sr) 0.549 0.526
SimpleConjunct 0.548 -
SimpleDisjunct 0.465 -
MBurst+CONJUNCTr,t,u 0.555 0.548
MBurst+DISJUNCTr,t,u 0.576 0.570?
MBurst+BELIEFr,t,u 0.568 0.561
MBurst+L2Gr,t,u(t) 0.574 0.567
MBurst+L2Gr,t,u(r) 0.560 0.558
mance. The average performance can show the sta-
bility of models in some degree. If one model out-
puts the maximum in a very limited set of parame-
ters, it may not work well in real data, especially in
social media.
In Table 6, we can seeMBurst+DISJUNCTr,t,u
gives the best performance. MBurst performs
consistently better than single-best which is a very
strong single-stream method, especially for average
performance. MBurst+DISJUNCTr,t,u has an im-
provement of average performance over single-best
by 8.4%. And simply combining three different
streams may hit results (SimpleConjunct and Sim-
pleDisjunct). It indicates that MBurst is more sta-
ble and shows a higher performance.
For different methods to derive global bursty pat-
terns, we can see that MBurst+DISJUNCT per-
forms best while MBurst+CONJUNCT performs
worst. Interestingly, however, SimpleConjunct is
better than SimpleDisjunct, the major reason is that
MBurst performs a local-state correlation of mul-
tiple activity streams to correct possible noisy fluc-
tuations from single streams before the conjunction
or disjunction of local states. After such correlation,
the performance of each activity stream should im-
prove. To see this, we present the optimal results of a
single stream without/with local-state correlation in
Table 7. Local-state correlation significantly boosts
the performance of a single stream. Indeed, we find
that the step of local-state correlation is more impor-
tant for our multiple stream algorithm than the step
of how to derive global states based on local states.
We test our MBurst algorithm with the setting:
T = 4416, L = 4 and M = 3, and for all the test
Table 7: Comparison between the optimal results of a
single stream with/without local-state correlation.
all retweet retweet url
without 0.536 0.549 0.464
with 0.574 0.560 0.547
2 4 6 8 10 120.54
0.550.56
0.570.58
0.590.6
?1
Average
 F
 
 MBurst+orsingle?best
(a) ?2 = 4, varying ?1.
2 4 6 8 10 120.54
0.550.56
0.570.58
0.590.6
?2
Average
 F
 
 MBurst+orsingle?best
(b) ?1 = 11, varying ?2.
Figure 4: Parameter sensitivity of MBurst + DIS-
JUNCT.
queries, our algorithm can respond in 2 seconds 11,
which is efficient to be deployed in social media.
Parameter sensitivity
We have shown the performance of different pa-
rameter settings for single stream algorithm in Ta-
ble 5. Next, we check parameter sensitivity in
MBurst. In our experiments, we find a longer lo-
cal window (L = 3, 4) is better than L = 2, so
we first set L = 4, then we select parameter set-
tings of ?2 = 4 and ?1 = 11, which give best per-
formance for MBurst+DISJUNCT. We vary one
with the other fixed to see how one single parame-
ter affects the performance. The results are shown in
Figure 4, and we can see MBurst+DISJUNCT is
consistently better than single-best.
5 Related Work
Our work is related to burst detection from text
streams. Pioneered by the automaton model pro-
posed in (Kleinberg, 2003), many techniques have
been proposed for burst detection such as the ?2-
test based method (Swan and Allan, 2000), the
parameter-free method (Fung et al 2005) and mov-
ing average method (Vlachos et al 2004). Our work
is related to the applications of these burst detection
algorithms for event detection (He et al 2007; Fung
et al 2007b; Shan et al 2012; Zhao et al 2012).
11All experiments are tested in a Mac PC, 2.4GHz Intel Core
2 Duo.
1474
Some recent work try to identify hot trends (Math-
ioudakis and Koudas, 2010; Zubiaga et al 2011;
Budak et al 2011; Naaman et al 2011) or make
use of the burstiness (Sakaki et al 2010; Aramki
et al 2011; Marcus et al 2011) in social media.
However, few of these methods consider modeling
the local smoothness of one state sequence in a sys-
tematic way and often use a fixed window length of
2.
Little work considers making use of different
types of social media activities for burst detection.
(Yao et al 2010; Kotov et al 2011; Wang et al
2007; Wang et al 2009) conducted some prelim-
inary studies of mining correlated bursty patterns
from multiple sources. However, they either highly
relies on high-quality training datasets or require a
considerable amount of training time. Online social
activities are dynamic, with a large number of new
items generated continuously. In such a dynamic
setting, burst detection algorithms should effectively
collect evidence, efficiently adjust prediction models
and respond to the users as social media activities
evolve. Therefore it is not suitable to deploy such
algorithms in social media.
Our work is also similar to studies which aim
to mine and leverage knowledge from social me-
dia (Mathioudakis et al 2010; Ruiz et al 2012;
Morales et al 2012). We share the common point
with these studies that we try to utilize the under-
lying rich knowledge in social media, while our fo-
cus of this work is quite different from theirs, i.e., to
identify event-related bursts.
Another line of related research is Twitter related
studies (Kwak et al 2010; Sakaki et al 2010). Our
proposed methods can provide event-related bursts
for downstream applications.
6 Conclusion
In this paper, we propose to identify event-related
bursts via social media activities. We propose one
optimization model to correlate multiple activity
streams to learn the bursty patterns. To better mea-
sure local smoothness of the state sequence, we pro-
pose a novel state cost function. We test our meth-
ods in a large Twitter dataset. The experiment re-
sults show that our methods are both effective and
efficient. Our work can provide a preliminary un-
derstanding of the correlation between the happen-
ings of events and the degree of online social media
activities.
Finally, we present a few promising directions
which may potentially improve or enrich current
work.
1) Variable-length context. In this paper, L is a
pre-determined parameter which controls the size of
context window. It cannot be modified when the al-
gorithm runs. A large L will significantly increases
the algorithm complexity, and we may not need a
large L for all the states in a Markov chain. This
problem can be addressed by using the variable-
length hidden Markov model (Wang et al 2006),
which is able to learn the ?minimum? context length
for accurately determining each state.
2) Incorporation of more useful features. Our
current model mainly considers temporal variations
of streaming data and searches the surge patterns ex-
isting in it. In some cases, simple frequency infor-
mation may not be capable to identify all the mean-
ingful bursts. It can be potentially useful to leverage
up more features to help filter out noisy bursts, e.g.,
semantic information (Zhao et al 2010).
3) Modeling multi-modality data. We have ex-
amined our multi-stream algorithm by using three
different activity streams. These streams are textual-
based. It will be interesting to check our algorithm in
multi-modality data streams. E.g., in Facebook, we
may collect a stream consisting of the daily frequen-
cies of photo sharing and another stream consisting
of the daily frequencies of text status updates.
4) Evaluation of the identified bursts. In most
of previous work, they seldom construct a gold stan-
dard for quantitative test, instead they qualitatively
evaluate their methods. In our work, we invite hu-
man judges to generate the gold standard. It is time-
consuming, and the bias from human judges cannot
be completely eliminated although more judges can
be invited. A possible evaluation method is to exam-
ine the identified bursts in downstream applications,
e.g., event detection.
Acknowledgement
This work is partially supported by NSFC Grant
61073082, 60933004 and 70903008. Xin Zhao is
supported by Google PhD Fellowship (China). We
thank the insightful comments from Junjie Yao and
the anonymous reviewers.
1475
References
Eiji Aramki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using twitter. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1568?1576, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Ceren Budak, Divyakant Agrawal, and Amr El Abbadi.
2011. Structural trend analysis for online social net-
works. Proc. VLDB Endow., 4, July.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In VLDB.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007a. Time-dependent event hierar-
chy construction. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?07.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007b. Time-dependent event hierarchy
construction. In SIGKDD.
Qi He, Kuiyu Chang, and Ee-Peng Lim. 2007. Analyz-
ing feature trajectories for event detection. In SIGIR.
Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD, pages 207?216, New
York, NY, USA. ACM.
J. Kleinberg. 2003. Bursty and hierarchical structure in
streams. Data Mining and Knowledge Discovery.
Alexander Kotov, ChengXiang Zhai, and Richard Sproat.
2011. Mining named entities with temporally corre-
lated bursts from multilingual web news streams. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM, pages
237?246.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a
news media? In WWW ?10: Proceedings of the 19th
international conference on World wide web, pages
591?600.
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: aggregating and visualizing
microblogs for event exploration. In Proceedings of
the 2011 annual conference on Human factors in com-
puting systems, CHI ?11.
Michael Mathioudakis and Nick Koudas. 2010. Twit-
termonitor: trend detection over the twitter stream.
In Proceedings of the 2010 international conference
on Management of data, SIGMOD ?10, pages 1155?
1158.
Michael Mathioudakis, Nick Koudas, and Peter Marbach.
2010. Early online identification of attention gather-
ing items in social media. In Proceedings of the third
ACM international conference on Web search and data
mining, WSDM ?10, pages 301?310, New York, NY,
USA. ACM.
Gianmarco De Francisci Morales, Aristides Gionis, and
Claudio Lucchese. 2012. From chatter to headlines:
harnessing the real-time web for personalized news
recommendation. In WSDM, pages 153?162.
Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip
and trendy: Characterizing emerging trends on twitter.
JASIST, 62(5):902?918.
Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo, Aris-
tides Gionis, and Alejandro Jaimes. 2012. Correlat-
ing financial time series with micro-blogging activity.
pages 513?522.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. WWW, pages 851?860,
New York, NY, USA. ACM.
Dongdong Shan, Wayne Xin Zhao, Rishan Chen, Shu
Baihan, Hongfei Yan, and Xiaoming Li. 2012.
Eventsearch: A system for event discovery and re-
trieval on multi-type historical data. In KDD?12, De-
mostration.
Russell Swan and James Allan. 2000. Automatic gener-
ation of overview timelines. In SIGIR.
Michail Vlachos, Christopher Meek, Zografoula Vagena,
and Dimitrios Gunopulos. 2004. Identifying similari-
ties, periodicities and bursts for online search queries.
In SIGMOD.
Yi Wang, Lizhu Zhou, Jianhua Feng, JianyongWang, and
Zhi-Qiang Liu. 2006. Mining complex time-series
data by learning markovian models. In Proceedings
of the Sixth International Conference on Data Min-
ing, ICDM, pages 1136?1140, Washington, DC, USA.
IEEE Computer Society.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pat-
terns from coordinated text streams. In Proceedings
of the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, WSDM, pages 192?201.
Junjie Yao, Bin Cui, Yuxin Huang, and Xin Jin. 2010.
Temporal and social context based burst detection
from folksonomies. In AAAI.
Wayne Xin Zhao, Jing Jiang, Jing He, Dongdong Shan,
Hongfei Yan, and Xiaoming Li. 2010. Context mod-
eling for ranking and tagging bursty features in text
1476
streams. In Proceedings of the 19th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?10.
Wayne Xin Zhao, Rishan Chen, Kai Fan, Hongfei Yan,
and Xiaoming Li. 2012. A novel burst-based text
representation model for scalable event detection. In
ACL?12.
Arkaitz Zubiaga, Damiano Spina, V??ctor Fresno, and
Raquel Mart??nez. 2011. Classifying trending topics:
a typology of conversation triggers on twitter. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM.
1477
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 426?435,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Efficient Collective Entity Linking with Stacking
Zhengyan He? Shujie Liu? Yang Song? Mu Li? Ming Zhou? Houfeng Wang??
? Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
? Microsoft Research Asia
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
songyangmagic@gmail.com wanghf@pku.edu.cn
Abstract
Entity disambiguation works by linking am-
biguous mentions in text to their correspond-
ing real-world entities in knowledge base. Re-
cent collective disambiguation methods en-
force coherence among contextual decisions
at the cost of non-trivial inference processes.
We propose a fast collective disambiguation
approach based on stacking. First, we train a
local predictor g0 with learning to rank as base
learner, to generate initial ranking list of can-
didates. Second, top k candidates of related
instances are searched for constructing expres-
sive global coherence features. A global pre-
dictor g1 is trained in the augmented feature
space and stacking is employed to tackle the
train/test mismatch problem. The proposed
method is fast and easy to implement. Exper-
iments show its effectiveness over various al-
gorithms on several public datasets. By learn-
ing a rich semantic relatedness measure be-
tween entity categories and context document,
performance is further improved.
1 Introduction
When extracting knowledge from natural language
text into a machine readable format, ambiguous
names must be resolved in order to tell which real-
world entity the name refers to. The task of linking
names to knowledge base is known as entity linking
or disambiguation (Ji et al, 2011). The resulting text
is populated with semantic rich links to knowledge
base like Wikipedia, and ready for various down-
stream NLP applications.
?Corresponding author
Previous researches have proposed several kinds
of effective approaches for this problem. Learning
to rank (L2R) approaches use hand-crafted features
f(d, e) to describe the similarity or dissimilarity be-
tween contextual document d and entity definition
e. L2R approaches are very flexible and expres-
sive. Features like name matching, context similar-
ity (Li et al, 2009; Zheng et al, 2010; Lehmann et
al., 2010) and category context correlation (Bunescu
and Pasca, 2006) can be incorporated with ease.
Nevertheless, decisions are made independently and
inconsistent results are found from time to time.
Collective approaches utilize dependencies be-
tween different decisions and resolve all ambiguous
mentions within the same context simultaneously
(Han et al, 2011; Hoffart et al, 2011; Kulkarni
et al, 2009; Ratinov et al, 2011). Collective ap-
proaches can improve performance when local ev-
idence is not confident enough. They often utilize
semantic relations across different mentions, and is
why they are called global approaches, while L2R
methods fall into local approaches (Ratinov et al,
2011). However, collective inference processes are
often expensive and involve an exponential search
space.
We propose a collective entity linking method
based on stacking. Stacked generalization (Wolpert,
1992) is a powerful meta learning algorithm that
uses two levels of learners. The predictions of the
first learner are taken as augmented features for the
second learner. The nice property of stacking is that
it does not restrict the form of the base learner. In
this paper, our base learner, an L2R ranker, is first
employed to generate a ranking list of candidates.
426
At the next level, we search for semantic coherent
entities from the top k candidates of neighboring
mentions. The second learner is trained on the aug-
mented feature space to enforce semantic coherence.
Stacking is employed to handle train/test mismatch
problem. Compared with existing collective meth-
ods, the inference process of our method is much
faster because of the simple form of its base learner.
Wikipedians annotate each entity with categories
which provide another source of valuable seman-
tic information. (Bunescu and Pasca, 2006) pro-
pose to generalize beyond context-entity correla-
tion s(d, e) with word-category correlation s(w, c).
However, this method works at word level, and does
not scale well to large number of categories. We
explore a representation learning technique to learn
the category-context association in latent semantic
space, which scales much better to large knowledge
base.
Our contributions are as follows: (1) We pro-
pose a fast and accurate stacking-based collective
entity linking method, which combines the benefits
of both coherence modeling of collective approaches
and expressivity of L2R methods. We show an
effective usage of ranking list as global features,
which is a key improvement for the global predictor.
(2) To overcome problems of scalability and shal-
low word-level comparison, we learn the category-
context correlation with recent advances of repre-
sentation learning, and show that this extra seman-
tic information indeed helps improve entity linking
performance.
2 Related Work
Most popular entity linking systems use the L2R
framework (Bunescu and Pasca, 2006; Li et al,
2009; Zheng et al, 2010; Lehmann et al, 2010).
Its discriminative nature gives the model enough
flexibility and expressivity. It can include any fea-
tures that describe the similarity or dissimilarity of
context d and candidate entity e. They often per-
form well even on small training set, with carefully-
designed features. This category falls into the local
approach as the decision processes for each mention
are made independently (Ratinov et al, 2011).
(Cucerzan, 2007) first suggests to optimize an ob-
jective function that is similar to the collective ap-
proach. However, the author adopts an approxi-
mation method because of the large search space
(which is O(nm) for a document with m mentions,
each with n candidates). Various other methods
like integer linear programming (Kulkarni et al,
2009), personalized PageRank (Han et al, 2011) and
greedy graph cutting (Hoffart et al, 2011) have been
explored in literature. Our method without stacking
resembles the method of (Ratinov et al, 2011) in
that they use the predictions of a local ranker to gen-
erate features for global ranker. The differences are
that we use stacking to train the local ranker to han-
dle the train/test mismatch problem and top k candi-
dates to generate features for the global ranker.
Stacked generalization (Wolpert, 1992) is a meta
learning algorithm that uses multiple learners out-
puts to augment the feature space of subsequent
learners. It utilizes a cross-validation strategy to ad-
dress the train set / testset label mismatch problem.
Various applications of stacking in NLP have been
proposed, such as collective document classification
(Kou and Cohen, 2007), stacked dependency parsing
(Martins et al, 2008) and joint Chinese word seg-
mentation and part-of-speech tagging (Sun, 2011).
(Kou and Cohen, 2007) propose stacked graphical
learning which captures dependencies between data
with relational template. Our method is inspired by
their approach. The difference is our base learner is
an L2R model. We search related entity candidates
in a large semantic relatedness graph, based on the
assumption that true candidates are often semanti-
cally correlated while false ones scattered around.
Wikipedians annotate entries in Wikipedia with
category network. This valuable information gener-
alizes entity-context correlation to category-context
correlation. (Bunescu and Pasca, 2006) utilize
category-word as features in their ranking model.
(Kataria et al, 2011) employ a hierarchical topic
model where each inner node in the hierarchy is a
category. Both approaches must rely on pruned cate-
gories because the large number of noisy categories.
We try to address this problem with recent advances
of representation learning (Bai et al, 2009), which
learns the relatedness of category and context in la-
tent continuous space. This method scales well to
potentially large knowledge base.
427
3 Method
In this section, we first introduce our base learner
and local features used; next, the stacking train-
ing strategy is given, followed by an explana-
tion of our global coherence model with aug-
mented feature space; finally we explain how to
learn category-context correlation with representa-
tion learning technique.
3.1 Base learner and local predictor g0
Entity linking is formalized as follows: given
an ambiguous name mention m with its con-
textual document d, a list of candidate entities
e1, e2, . . . , en(m) ? C(m) is generated for m, our
predictor g will generate a ranking score g(ei) for
each candidate ei. The ranking score will be used
to construct augmented features for the next level
learner, or used by our end system to select the an-
swer:
e? = arg max
e?C(m)
g(e) (1)
In an L2R framework, the model is often defined
as a linear combination of features. Here, our fea-
tures f?(d, e) are derived from document d and can-
didate e. The model is defined as g(e) = w?f?(d, e).
In our problem, we are given a list of training data
D = {(di, ei)}. We want to optimize the parameter
w?, such that the correct entity has a higher score over
negative ones. This is done via a preference learning
technique SVM rank, first introduced by (Joachims,
2002). The following margin based loss is mini-
mized w.r.t w?:
L = 1
2
?w??2 + C
?
?d,e? (2)
s.t. w?(f?(d, e)? f?(d, e?)) ? 1? ?d,e? (3)
?d,e? ? 0 (4)
where C is a trade-off between training error and
margin size; ? is slacking variable and loops over
all query documents d and negative candidates e? ?
C(m)? {e}.
This model is expressive enough to include any
form of features describing the similarity and dis-
similarity of d and e. We only include some typical
features seen in literature. The inclusion of these
features is not meant to be exhaustive. Our purpose
is to build a moderate model in which some of the
Surface matching:
1. mention string m exactly matches candidate
e, i.e. m = e
2. neither m is a substring of e nor e is a sub-
string of m
3. m ?= e and m is a substring of e
4. m ?= e and e is a substring of m
5. m ?= e and m is a redirect pointing to e in
Wikipedia
6. m ?= e and e starts with m
7. m ?= e and e ends with m
Context matching:
1. cosine similarity of TF-IDF score between
context and entire Wikipedia page of candidate
2. cosine similarity of TF-IDF score between
context and introduction of Wikipedia page
3. jaccard distance between context and entire
Wikipedia page of candidate
4. jaccard distance between context and intro-
duction of Wikipedia page
Popularity or prominence feature:
percentage of Wikipedia hyperlinks pointing to
e given mention m, i.e. P(e|m)
Category-context coherence model:
cat0 and cat1 (details in Section 3.4)
Table 1: Features for local predictor g0.
useful features like string matching and entity pop-
ularity cannot be easily expressed by collective ap-
proaches like (Hoffart et al, 2011; Han et al, 2011).
The features for level 0 predictor g0 are described
in Table 1. The reader can consult (Li et al, 2009;
Zheng et al, 2010; Lehmann et al, 2010) for further
reference.
3.2 Stacking training for global predictor g1
Stacked generalization (Wolpert, 1992) is a meta
learning algorithm that stacks two ?levels? of pre-
dictors. Level 0 includes one or more predictors
h(0)1 , h
(0)
2 , . . . , h
(0)
K : Rd ? R, each one is trained on
the original d-dimensional feature space. The level
1 predictor h(1) : Rd+K ? R is trained in the aug-
mented (d+K)-dimensional feature space, in which
predictions at level 0 are taken as extra features in
h(1).
(Kou and Cohen, 2007) proposed stacked graphi-
428
cal learning for learning and inference on relational
data. In stacked graphical learning, dependencies
among data are captured by relational template, with
which one searches for related instances of the cur-
rent instance. The augmented feature space does
not necessarily to be d + K. Instead, one can con-
struct any declarative feature with the original data
and predictions of related instances. For instance,
in collective document classification (Kou and Co-
hen, 2007) employ relational template to extract
documents that link to this document, then apply a
COUNT aggregator over each category on neighbor-
ing documents as level 1 features.
In our entity linking task, we use a single predic-
tor g0 trained with local features at level 0. Com-
pared with (Kou and Cohen, 2007), both g0 and g1
are L2R models rather than classifier. At level 1, for
each document-candidate entity pair, we use the re-
lational templateN (x) to find related entities for en-
tity x, and construct global features with some func-
tion G({g0(n)|n ? N (x)}) (details in Sec. 3.3).
The global predictor g1 receives as input the origi-
nal features plus G.
One problem is that if we use g0 trained on the en-
tire training set to predict related instances in train-
ing set, the accuracy can be somehow different (typ-
ically lower) for future unseen data. g1 with this pre-
diction as input doesn?t generalize well to test data.
This is known as train/test mismatch problem. To
mimic test time behavior, training is performed in a
cross-validation-like way. Let D be the entire train-
ing set:
1. Split D into L partitions {D1, . . . ,DL}
2. For each split Di:
2.1 Train an instance of g0 on D ?Di
2.2 Predict all related instances inDi with this
predictor g0
2.3 Augment feature space for x ? Di, with G
applied on predictions of N (x)
3. Train level 0 predictor g0 on entire D, for ex-
panding feature space for test data
4. Train level 1 predictor g1 on entire D, in the
augmented feature space.
In the next subsection, we will describe how to
construct global features from the predictions of g0
on neighbors N (x) with G.
3.3 Enforcing coherence with global features G
If one wants to identify the correct entity for an am-
biguous name, he would possibly look for related
entities in its surrounding context. However, sur-
rounding entities can also exhibit some degree of
ambiguity. In ideal cases, most true candidates are
inter-connected with semantic links while negative
candidates are scattered around (Fig. 1). Thus, we
ask the following question: Is there any highly rele-
vant entity to this candidate in context? Or, is there
any mention with highly relevant entity to this can-
didate in the top k ranking list of this mention? And
how many those mentions are? The reason to look
up top k candidates is to improve recall. g0 may not
perfectly rank related entity at the first place, e.g.
?Mitt Romney? in Figure 1.
Assume the ambiguous mention set is M . For
each mention mi ? M , we rank each entity ei,j ?
C(mi) by its score g0(ei,j). Denote its rank as
Rank(ei,j). For each entity e in the candidate set
E = {ei,j |?ei,j ? C(mi), ?mi ? M}, we search
related instances for e as follows:
1. search in E for entities with semantic related-
ness above a threshold ({0.1,0.3,0.5,0.7,0.9});
2. select those entities in step (1) with Rank(e)
less than or equal to k (k ? {1, 3, 5});
3. map entities in step (2) to unique set of men-
tions U , excluding current m, i.e. e ? C(m).
This process is relatively fast. It only involves a
sparse matrix slicing operation on the large pre-
computed semantic relatedness matrix in step (1),
and logical operation in step (2,3). The following
features are fired concerning the unique set U :
- if U is empty;
- if U is not empty;
- if the percentage |U |/|M | is above a threshold
(e.g. 0.3).
The above process generates a total of 45 (5?3?3)
global features.
429
Barack Obama Democratic Party (United States)
Mitt Romney
Republican Party (United States)
Obama, Fukui
Obama, Nagasaki
Democratic Party (Italy)
Democratic Party (Serbia)
Republican Party of Minnesota
Republicanism
Romney, West Virginia
HMS Romney (1694)
... ... ... ...
received national attention during his campaign  ...  with his vectory in the March   [[Obama|Barack Obama]]
[[Democratic Party|Democratic Party (United States)]] primary  ...  He was re-elected president in November
2012, defeating [[Republican|Republican Party (United States)]] nominee [[Romney|Mitt Romney]]
Figure 1: Semantic links for collective entity linking. Annotation [[mention|entity]] follows Wikipedia conventions.
Finally, the semantic relatedness measure of two
entities ei,ej is defined as the common in-links of ei
and ej in Wikipedia (Milne and Witten, 2008; Han
et al, 2011):
SR(ei, ej) = 1?
log(max(|A|, |B|))? log(|A ?B|)
log(|W |)? log(min(|A|, |B|))
(5)
where A and B are the set of in-links for entity ei
and ej respectively, andW is the set of all Wikipedia
pages.
Our method is a trade-off between exact collec-
tive inference and approximating related instance
with top ranked entities produced by g0. Most
collective approaches take all ambiguous mentions
into consideration and disambiguate them simulta-
neously, resulting in difficulty when inference in
large search space (Kulkarni et al, 2009; Hoffart
et al, 2011). Others resolve to some kinds of ap-
proximation. (Cucerzan, 2007) construct features as
the average of all candidates for one mention, in-
troducing considerable noise. (Ratinov et al, 2011)
also employ a two level architecture but only take
top 1 prediction for features. This most resembles
our approach, except we use stacking to tackle the
train/test mismatch problem, and construct different
set of features from top k candidates predicted by
g0. We will show in our experiments that this indeed
helps boost performance.
3.4 Learning category-context coherence
model cat
Entities in Wikipedia are annotated with rich se-
mantic structures. Category network provides us
with another valuable information for entity link-
ing. Take the mention ?Romney? as an exam-
ple, one candidate ?Mitt Romney? with category
?Republican party presidential nominee? co-occurs
frequently with context like ?election? and ?cam-
paign?, while another candidate ?Milton Romney?
with category ?Utah Utes football players? is fre-
quently observed with context like ?quarterback?
and ?backfield?. The category network forms a di-
rected acyclic graph (DAG). Some entities can share
category through the network, e.g. ?Barack Obama?
with category ?Democratic Party presidential nom-
inees? shares the category ?United States presiden-
tial candidates by party? with ?Mitt Romney? when
travelling two levels up the network.
(Bunescu and Pasca, 2006) propose to learn the
category-context correlation at word level through
category-word pair features. This method creates
sparsity problem and does not scale well because
the number of features grows linearly with both the
number of categories and the vocabulary size. More-
over, the category network is somewhat noisy, e.g.
travelling up four levels of the hierarchy can result
in over ten thousand categories, with many irrelevant
ones.
Rather than learning the correlation at word level,
we explore a representation learning method that
learns category-context correlation in the latent se-
mantic space. Supervised Semantic Indexing (SSI)
(Bai et al, 2009) is trained on query-document pairs
to predict their degree of matching. The compar-
ison is performed in the latent semantic space, so
that synonymy and polysemy are implicitly handled
by its inner mechanism. The score function between
query q and document d is defined as:
f(q, d) = qTWd (6)
430
where W is learned with supervision like click-
through data.
Given training data {(qi, di)}, training is done by
randomly sampling a negative target d?. The model
optimizes W such that f(q, d+) > f(q, d?). Thus,
the training objective is to minimize the following
margin-based loss function:
?
q,d+,d?
max(0, 1? f(q, d+) + f(q, d?)) (7)
which is also known as contrastive estimation
(Smith and Eisner, 2005).
W can become very large and inefficient when we
have a big vocabulary size. This is addressed by re-
placing W with its low rank approximation:
W = UTV + I (8)
here, the identity term I is a trade-off between the
latent space model and a vector space model. The
gradient step is performed with Stochastic Gradient
Descent (SGD):
U ?U + ?V (d+ ? d?)qT ,
if 1? f(q, d+) + f(q, d?) > 0 (9)
V ?V + ?Uq(d+ ? d?)T ,
if 1? f(q, d+) + f(q, d?) > 0. (10)
where ? is the learning rate.
The query and document are not necessary real
query and document. In our case, we treat our
problem as: given the occurring context of an en-
tity, retrieving categories corresponding to this en-
tity. Thus, we use context as query q and the cat-
egories of this candidate entity as d. We also treat
the definition page of an entity as its context, and
first train the model with definition pages, because
definition pages exhibit more focused topic. This
considerably accelerates the training process. To
reduce noise, We input the categories directly con-
nected with one entity as a word vector. The input
can be a TF-IDF vector or binary vector. We denote
model trained with normalized TF-IDF and with bi-
nary input as cat0 and cat1 respectively.
4 Experiments
4.1 Datasets
Previous researches have used diverse datasets for
evaluation, which makes it hard for comparison
with others? approaches. TAC-KBP has several
years of data for evaluating entity linking system,
but is not well suited for evaluating collective ap-
proaches. Recently, (Hoffart et al, 2011) anno-
tated a clean and much larger dataset AIDA 1 for
collective approaches evaluation based on CoNLL
2003 NER dataset. (Ratinov et al, 2011) also re-
fined previous work and contribute four publicly
available datasets 2. Thanks to their great works,
we have enough data to evaluate against. Accord-
ing to the setting of (Hoffart et al, 2011), we
split the AIDA dataset for train/development/test
with 946/216/231 documents. We train a separate
model on the Wikipedia training set for evaluating
ACE/QUAINT/WIKI dataset (Ratinov et al, 2011).
Table 2 gives a brief overview of the datasets used.
For knowledge base, we use the Wikipedia XML
dump 3 to extract over 3.3 million entities. We use
annotation from Wikipedia to build a name dictio-
nary from mention string m to entity e for can-
didate generation, including redirects, disambigua-
tion pages and hyperlinks, follows the approach of
(Cucerzan, 2007). For candidate generation, we
keep the top 30 candidates by popularity (Tbl. 1).
Note that our name dictionary is different from
(Ratinov et al, 2011) and has a much higher recall.
Since (Ratinov et al, 2011) evaluate on ?solvable?
mentions and we have no way to recover those men-
tions, we re-implement their global features and the
final scores are not directly comparable to theirs.
4.2 Methods under comparison
We compare our algorithm with several state-of-the-
art collective entity disambiguation systems. The
AIDA system proposed by (Hoffart et al, 2011) use
a greedy graph cutting algorithm that iteratively re-
move entities with low confidence scores. (Han et
al., 2011) employ personalized PageRank to prop-
agate evidence between different decisions. Both
algorithms use simple local features without dis-
criminative training. (Kulkarni et al, 2009) pro-
pose to use integer linear programming (ILP) for
inference. Except our re-implementation of Han?s
1available at http://www.mpi-inf.mpg.de/yago-naga/aida/
2http://cogcomp.cs.illinois.edu/Data, we don?t find the
MSNBC dataset in the zip file.
3available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
431
Dataset ndocs non-
NIL
identified solvable
AIDA dev 216 4791 4791 4707
AIDA test 231 4485 4485 4411
ACE 36 257 238 209(185)
AQUAINT 50 727 697 668(588)
Wikipedia 40 928 918 854(843)
Table 2: Number of mentions in each dataset. ?identi-
fied? means the mention exists in our name dictionary
and ?solvable? means the true entity are among the top 30
candidates by popularity. Number in parenthesis shows
the results of (Ratinov et al, 2011).
method, both AIDA and ILP solution are quite slow
at running time. The online demo of AIDA takes
over 10 sec to process one document with mod-
erate size, while the ILP solution takes around 2-
3 sec/doc. In contrast, our method takes only 0.3
sec/doc, and is easy to implement.
(Ratinov et al, 2011) also utilize a two layer
learner architecture. The difference is that their
method use top 1 candidate generated by local
learner for global feature generation , while we
search the top k candidates. Moreover, stacking is
used to tackle the train/test mismatch problem in
our model. We re-implement the global features of
(Ratinov et al, 2011) and use our local predictor
g0 for level 0 predictor. Note that we only imple-
ment their global features concerning common in-
links and inter-connection (totally 9 features) for fair
comparison because all other models don?t use com-
mon outgoing links for global coherence.
4.3 Settings
We implement SVM rank with an adaptation of lin-
ear SVM in scikit-learn (which is a wrapper of Li-
blinear). The category-context coherence model is
implemented with Numpy configured with Open-
Blas library, and we train this model on the entire
Wikipedia hyperlink annotation. It takes about 1.5d
for one pass over the entire dataset. The learning
rate ? is set to 1e-4 and training cost before update
is below 0.02.
Parameter tuning: there aren?t many parameters
to tune for both g0 and g1. The context document
window size is fixed as 100 for compatibility with
(Ratinov et al, 2011; Hoffart et al, 2011). The num-
ber of candidates is fixed to top 30 ranked by entity?s
popularity. Increase this value will generally boost
recall at the cost of lower precision.
We introduce the following default parameter for
global features in g1. The number of fold for stack-
ing is set to {1,5,10} (see Table 4, default is 10; 1
means no stacking, i.e. training g0 with all training
data and generating level 1 features for training data
directly with this g0). The number k for searching
neighboring entities with relational template is set
to {1,3,5,7} (e.g. in step 2 of Section 3.3 k = 5;
default is 5).
For category-context modeling, the vocabulary
sizes of context and category are set to top 10k and
6k unigrams by frequency. The latent dimension of
low rank approximation is set to 200.
Performance measures: For all non-NIL
queries, we evaluate performance with micro pre-
cision averaged over queries and macro precision
averaged over documents. Mean Reciprocal Rank
(MRR) is an information retrieval measure and is
defined as 1|Q|
?|Q|
i
1
ranki , where ranki is the rank
of correct answer in response to query i. For
ACE/AQUAINT/WIKI we also give the accuracy of
?solvable? mentions, but this is not directly compa-
rable to (Ratinov et al, 2011). Our name dictionary
is different from theirs and ours has a higher recall
rate (Tbl. 2). Hence, the ?solvable? set is different.
k recall k recall
1 78.56 6 96.31
2 89.59 7 97.04
3 93.01 8 97.37
4 94.97 9 97.62
5 95.78 10 97.81
Table 3: Top k recall for local predictor g0.
4.4 Discussions
Table 4 shows the evaluation results on AIDA
dataset and Table 5 shows results on datasets
ACE/AQUAINT/WIKI.
Effect of cat:The first group in Table 4 shows
some baseline features for comparison. We can see
even if the categories only carry incomplete and
noisy information about an entity, it performs much
432
Methods Devset Testset
micro
p@1
macro
p@1
MRR micro
p@1
macro
p@1
MRR
cosine 33.25 28.61 46.03 33.33 28.63 46.54
jaccard 44.71 36.56 57.76 45.66 36.89 57.08
cat0 54.75 47.14 67.70 61.52 54.72 72.55
cat1 60.15 54.64 72.98 65.46 61.04 76.84
popularity 69.21 67.59 79.26 69.07 72.63 79.45
g0 76.04 73.63 84.21 76.16 78.17 84.58
g0+global(Ratinov) 81.30 78.03 88.14 81.45 81.89 88.70
g1+1fold 82.01 78.52 88.90 83.59 83.58 90.05
g1+5fold 81.99 78.42 88.87 83.52 83.37 89.99
g1+10fold 82.01 78.53 88.91 83.59 83.55 90.03
g1+top1 81.65 78.76 88.51 81.81 82.55 89.06
g1+top3 82.20 78.64 88.98 83.52 83.34 89.94
g1+top5 82.01 78.57 88.90 83.63 83.76 90.05
g1+top7 82.05 78.40 88.90 83.75 83.58 90.08
g0+cat 79.36 76.14 86.66 79.64 80.47 87.32
g1+cat 82.24 78.49 89.02 84.88 84.49 90.65
g1+cat+all context 82.99 78.56 89.51 86.49 85.11 91.55
(Hoffart et al, 2011) - - - 82.29 82.02 -
(Shirakawa et al, 2011) - - - 81.40 83.57 -
(Kulkarni et al, 2009) - - - 72.87 76.74 -
(Han et al, 2011) - - - 78.97 75.77 -
Table 4: Performance on AIDA dataset. Maximal value in each group are highlighted with bold font. top k means up
to k candidates are used for searching related instances with relational template.
better than word level features. Group 5 in Table
4 shows cat information generally boosts perfor-
mance for both predictor g0 and g1.
Effect of stacking: Group 3 in Table 4 shows the
results with different fold in stacking training. 1 fold
means training g0 with all training data and directly
augment training data with this g0. Surprisingly, we
do not observe any substantial difference with vari-
ous fold size. We deduce it is possible the way we
fire global features with top k candidates that alle-
viates the problem of train/test mismatch when ex-
tending feature space for g1. Despite the ranking of
true entity can be lower in testset than in training
set, the semantic coherence information can still be
captured with searching over top k candidates.
Effect of top k global features: Group 4 in Table
4 shows the effect of k on g1 performance. Clearly,
increasing k generally improves precision and one
possible reason is the improvement in recall when
searching for related instances. Table 3 shows the
top k recall of local predictor g0. Further increasing
k does not show any improvement.
Our method benefits from such a searching strat-
egy, and consistently outperforms the global fea-
tures of (Ratinov et al, 2011). While their method
is a trade-off between expensive exact search over
all mentions and greedy assigning all mentions
with local predictor, we show this idea can be fur-
ther extended, somewhat like increasing the beam
search size without additional computational over-
head. The only exception is the ACE dataset, since
this dataset is so small, the difference translates to
only one mention. One may notice the improvement
on ACE/AQUAINT datasets is a little inconsistent.
These datasets are much smaller and the results only
differ within 4 mentions. Because these models are
433
Method micro
p@1
macro
p@1
MRR correct
/ solv-
able
ACE
g0 77.43 81.30 79.03 95.22
Ratinov 77.43 80.70 78.81 95.22
g1+5fold 77.04 79.85 78.96 94.74
g0+cat 77.82 81.48 79.31 95.69
g1+cat 77.43 80.16 79.25 95.22
AQUAINT
g0 84.46 84.69 87.49 91.92
Ratinov 85.14 85.29 87.90 92.66
g1+5fold 85.83 85.55 88.27 93.41
g0+cat 85.01 85.00 87.89 92.51
g1+cat 85.28 85.14 88.23 92.81
Wikipedia test
g0 83.19 84.30 86.63 90.40
Ratinov 84.48 85.96 87.62 91.80
g1+5fold 84.81 86.29 88.13 92.15
g0+cat 84.38 86.13 87.51 91.69
g1+cat 85.45 87.16 88.31 92.86
Table 5: Evaluation on ACE/AQUAINT/WIKI datasets.
trained on Wikipedia, the annotation style can be
quite different.
Finally, as we analyze the development set of
AIDA, we discover that some location entities rely
on more distant information across the context, as
we increase the context to the entire contextual doc-
ument, we can gain extra performance boost.
4.5 Error analysis
As we analyze the development set of AIDA, we find
some general problems with location names. Loca-
tion name generally is not part of the main topic
of one document. Thus, comparing context with
its definition is not realistic. Most of the time, we
can find some related location names in context; but
other times, it is not easily distinguished. For in-
stance, in ?France beats Turkey in men?s football...?
France refers to ?France national football team? but
our system links it to the country page ?France? be-
cause it is more popular. This can be addressed by
modeling finer context (Sen, 2012) or local syntac-
tic pattern (Hoffart et al, 2011). In other cases,
our system misclassifies ?New York City? for ?New
York? and ?Netherlands? for ?Holland? and ?Peo-
ple?s Republic of China? for ?China?, because in
all these cases, the latter ones are the most popu-
lar in Wikipedia. It is even hard for us humans to
tell the difference based only on context or global
coherence.
5 Conclusions
We propose a stacking based collective entity link-
ing method, which stacks a global predictor on top
of a local predictor to collect coherence information
from neighboring decisions. It is fast and easy to im-
plement. Our method trades off between inefficient
exact search and greedily assigning mention with lo-
cal predictor. It can be seen as searching related
entities with relational template in stacked graphi-
cal learning, with beam size k. Furthermore, we
adopt recent progress in representation learning to
learn category-context coherence model. It scales
better than existing approaches on large knowledge
base and performs comparison in the latent semantic
space. Combining these two techniques, our model
consistently outperforms all existing more sophisti-
cated collective approaches in our experiments.
Acknowledgments
This research was partly supported by Ma-
jor National Social Science Fund of China(No.
12&ZD227),National High Technology Research
and Development Program of China (863 Program)
(No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009).
References
B. Bai, J. Weston, D. Grangier, R. Collobert, O. Chapelle,
and K. Weinberger. 2009. Supervised semantic index-
ing. In The 18th ACM Conference on Information and
Knowledge Management (CIKM).
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP-CoNLL, volume 6, pages 708?716.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: a graph-based method. In Pro-
434
ceedings of the 34th international ACM SIGIR con-
ference on Research and development in Information
Retrieval, pages 765?774. ACM.
J. Hoffart, M.A. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 782?792. Association for Computational Lin-
guistics.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2011. Overview of the tac 2011
knowledge base population track. In Proceedings of
the Fourth Text Analysis Conference.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
ACM.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hierar-
chical topic models. In Proceedings of KDD.
Zhenzhen Kou and William W Cohen. 2007. Stacked
graphical models for efficient inference in markov ran-
dom fields. In SDM.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages
457?466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and Y. Shi.
2010. Lcc approaches to knowledge base population
at tac 2010. In Proc. TAC 2010 Workshop.
F. Li, Z. Zheng, F. Bu, Y. Tang, X. Zhu, and M. Huang.
2009. Thu quanta at tac 2009 kbp and rte track. In
Proceedings of Test Analysis Conference 2009 (TAC
09).
Andre? FT Martins, Dipanjan Das, Noah A Smith, and
Eric P Xing. 2008. Stacking dependency parsers. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 157?166. As-
sociation for Computational Linguistics.
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceedings of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proceedings of the Annual Meeting of
the Association of Computational Linguistics (ACL).
P. Sen. 2012. Collective context-aware topic models
for entity disambiguation. In Proceedings of the 21st
international conference on World Wide Web, pages
729?738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011.
Entity disambiguation based on a probabilistic
taxonomy. Technical report, Technical Report
MSR-TR-2011-125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 354?362. Asso-
ciation for Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In ACL, pages 1385?1394.
David H Wolpert. 1992. Stacked generalization. Neural
networks, 5(2):241?259.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
483?491, Los Angeles, California, June. Association
for Computational Linguistics.
435
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 379?388,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Topical Keyphrase Extraction from Twitter
Wayne Xin Zhao? Jing Jiang? Jing He? Yang Song? Palakorn Achananuparp?
Ee-Peng Lim? Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,peaceful.he,songyangmagic}@gmail.com,
{jingjiang,eplim,palakorna}@smu.edu.sg, lxm@pku.edu.cn
Abstract
Summarizing and analyzing Twitter content is
an important and challenging task. In this pa-
per, we propose to extract topical keyphrases
as one way to summarize Twitter. We propose
a context-sensitive topical PageRank method
for keyword ranking and a probabilistic scor-
ing function that considers both relevance and
interestingness of keyphrases for keyphrase
ranking. We evaluate our proposed methods
on a large Twitter data set. Experiments show
that these methods are very effective for topi-
cal keyphrase extraction.
1 Introduction
Twitter, a new microblogging website, has attracted
hundreds of millions of users who publish short
messages (a.k.a. tweets) on it. They either pub-
lish original tweets or retweet (i.e. forward) oth-
ers? tweets if they find them interesting. Twitter
has been shown to be useful in a number of appli-
cations, including tweets as social sensors of real-
time events (Sakaki et al, 2010), the sentiment pre-
diction power of Twitter (Tumasjan et al, 2010),
etc. However, current explorations are still in an
early stage and our understanding of Twitter content
still remains limited. How to automatically under-
stand, extract and summarize useful Twitter content
has therefore become an important and emergent re-
search topic.
In this paper, we propose to extract keyphrases
as a way to summarize Twitter content. Tradition-
ally, keyphrases are defined as a short list of terms to
summarize the topics of a document (Turney, 2000).
It can be used for various tasks such as document
summarization (Litvak and Last, 2008) and index-
ing (Li et al, 2004). While it appears natural to use
keyphrases to summarize Twitter content, compared
with traditional text collections, keyphrase extrac-
tion from Twitter is more challenging in at least two
aspects: 1) Tweets are much shorter than traditional
articles and not all tweets contain useful informa-
tion; 2) Topics tend to be more diverse in Twitter
than in formal articles such as news reports.
So far there is little work on keyword or keyphrase
extraction from Twitter. Wu et al (2010) proposed
to automatically generate personalized tags for Twit-
ter users. However, user-level tags may not be suit-
able to summarize the overall Twitter content within
a certain period and/or from a certain group of peo-
ple such as people in the same region. Existing work
on keyphrase extraction identifies keyphrases from
either individual documents or an entire text collec-
tion (Turney, 2000; Tomokiyo and Hurst, 2003).
These approaches are not immediately applicable
to Twitter because it does not make sense to ex-
tract keyphrases from a single tweet, and if we ex-
tract keyphrases from a whole tweet collection we
will mix a diverse range of topics together, which
makes it difficult for users to follow the extracted
keyphrases.
Therefore, in this paper, we propose to study the
novel problem of extracting topical keyphrases for
summarizing and analyzing Twitter content. In other
words, we extract and organize keyphrases by top-
ics learnt from Twitter. In our work, we follow the
standard three steps of keyphrase extraction, namely,
keyword ranking, candidate keyphrase generation
379
and keyphrase ranking. For keyword ranking, we
modify the Topical PageRank method proposed by
Liu et al (2010) by introducing topic-sensitive score
propagation. We find that topic-sensitive propaga-
tion can largely help boost the performance. For
keyphrase ranking, we propose a principled proba-
bilistic phrase ranking method, which can be flex-
ibly combined with any keyword ranking method
and candidate keyphrase generation method. Ex-
periments on a large Twitter data set show that
our proposed methods are very effective in topical
keyphrase extraction from Twitter. Interestingly, our
proposed keyphrase ranking method can incorporate
users? interests by modeling the retweet behavior.
We further examine what topics are suitable for in-
corporating users? interests for topical keyphrase ex-
traction.
To the best of our knowledge, our work is the
first to study how to extract keyphrases from mi-
croblogs. We perform a thorough analysis of the
proposed methods, which can be useful for future
work in this direction.
2 Related Work
Our work is related to unsupervised keyphrase ex-
traction. Graph-based ranking methods are the
state of the art in unsupervised keyphrase extrac-
tion. Mihalcea and Tarau (2004) proposed to use
TextRank, a modified PageRank algorithm to ex-
tract keyphrases. Based on the study by Mihalcea
and Tarau (2004), Liu et al (2010) proposed to de-
compose a traditional random walk into multiple
random walks specific to various topics. Language
modeling methods (Tomokiyo and Hurst, 2003) and
natural language processing techniques (Barker and
Cornacchia, 2000) have also been used for unsuper-
vised keyphrase extraction. Our keyword extraction
method is mainly based on the study by Liu et al
(2010). The difference is that we model the score
propagation with topic context, which can lower the
effect of noise, especially in microblogs.
Our work is also related to automatic topic label-
ing (Mei et al, 2007). We focus on extracting topical
keyphrases in microblogs, which has its own chal-
lenges. Our method can also be used to label topics
in other text collections.
Another line of relevant research is Twitter-
related text mining. The most relevant work is
by Wu et al (2010), who directly applied Tex-
tRank (Mihalcea and Tarau, 2004) to extract key-
words from tweets to tag users. Topic discovery
from Twitter is also related to our work (Ramage et
al., 2010), but we further extract keyphrases from
each topic for summarizing and analyzing Twitter
content.
3 Method
3.1 Preliminaries
Let U be a set of Twitter users. Let C =
{{du,m}
Mu
m=1}u?U be a collection of tweets gener-
ated by U , where Mu is the total number of tweets
generated by user u and du,m is the m-th tweet of
u. Let V be the vocabulary. du,m consists of a
sequence of words (wu,m,1, wu,m,2, . . . , wu,m,Nu,m)
where Nu,m is the number of words in du,m and
wu,m,n ? V (1 ? n ? Nu,m). We also assume
that there is a set of topics T over the collection C.
Given T and C, topical keyphrase extraction is to
discover a list of keyphrases for each topic t ? T .
Here each keyphrase is a sequence of words.
To extract keyphrases, we first identify topics
from the Twitter collection using topic models (Sec-
tion 3.2). Next for each topic, we run a topical
PageRank algorithm to rank keywords and then gen-
erate candidate keyphrases using the top ranked key-
words (Section 3.3). Finally, we use a probabilis-
tic model to rank the candidate keyphrases (Sec-
tion 3.4).
3.2 Topic discovery
We first describe how we discover the set of topics
T . Author-topic models have been shown to be ef-
fective for topic modeling of microblogs (Weng et
al., 2010; Hong and Davison, 2010). In Twit-
ter, we observe an important characteristic of tweets:
tweets are short and a single tweet tends to be about
a single topic. So we apply a modified author-topic
model called Twitter-LDA introduced by Zhao et al
(2011), which assumes a single topic assignment for
an entire tweet.
The model is based on the following assumptions.
There is a set of topics T in Twitter, each represented
by a word distribution. Each user has her topic inter-
ests modeled by a distribution over the topics. When
a user wants to write a tweet, she first chooses a topic
based on her topic distribution. Then she chooses a
380
1. Draw ?B ? Dir(?), pi ? Dir(?)
2. For each topic t ? T ,
(a) draw ?t ? Dir(?)
3. For each user u ? U ,
(a) draw ?u ? Dir(?)
(b) for each tweet du,m
i. draw zu,m ? Multi(?u)
ii. for each word wu,m,n
A. draw yu,m,n ? Bernoulli(pi)
B. draw wu,m,n ? Multi(?B) if
yu,m,n = 0 and wu,m,n ?
Multi(?zu,m) if yu,m,n = 1
Figure 1: The generation process of tweets.
bag of words one by one based on the chosen topic.
However, not all words in a tweet are closely re-
lated to the topic of that tweet; some are background
words commonly used in tweets on different topics.
Therefore, for each word in a tweet, the user first
decides whether it is a background word or a topic
word and then chooses the word from its respective
word distribution.
Formally, let ?t denote the word distribution for
topic t and ?B the word distribution for background
words. Let ?u denote the topic distribution of user
u. Let pi denote a Bernoulli distribution that gov-
erns the choice between background words and topic
words. The generation process of tweets is described
in Figure 1. Each multinomial distribution is gov-
erned by some symmetric Dirichlet distribution pa-
rameterized by ?, ? or ?.
3.3 Topical PageRank for Keyword Ranking
Topical PageRank was introduced by Liu et al
(2010) to identify keywords for future keyphrase
extraction. It runs topic-biased PageRank for each
topic separately and boosts those words with high
relevance to the corresponding topic. Formally, the
topic-specific PageRank scores can be defined as
follows:
Rt(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
Rt(wj)+ (1??)Pt(wi),
(1)
where Rt(w) is the topic-specific PageRank score
of word w in topic t, e(wj , wi) is the weight for the
edge (wj ? wi), O(wj) =
?
w? e(wj , w
?) and ?
is a damping factor ranging from 0 to 1. The topic-
specific preference value Pt(w) for each word w is
its random jumping probability with the constraint
that
?
w?V Pt(w) = 1 given topic t. A large Rt(?)
indicates a word is a good candidate keyword in
topic t. We denote this original version of the Topi-
cal PageRank as TPR.
However, the original TPR ignores the topic con-
text when setting the edge weights; the edge weight
is set by counting the number of co-occurrences of
the two words within a certain window size. Tak-
ing the topic of ?electronic products? as an exam-
ple, the word ?juice? may co-occur frequently with a
good keyword ?apple? for this topic because of Ap-
ple electronic products, so ?juice? may be ranked
high by this context-free co-occurrence edge weight
although it is not related to electronic products. In
other words, context-free propagation may cause the
scores to be off-topic.
So in this paper, we propose to use a topic context
sensitive PageRank method. Formally, we have
Rt(wi) = ?
?
j:wj?wi
et(wj , wi)
Ot(wj)
Rt(wj)+(1??)Pt(wi).
(2)
Here we compute the propagation from wj to wi in
the context of topic t, namely, the edge weight from
wj to wi is parameterized by t. In this paper, we
compute edge weight et(wj , wi) between two words
by counting the number of co-occurrences of these
two words in tweets assigned to topic t. We denote
this context-sensitive topical PageRank as cTPR.
After keyword ranking using cTPR or any other
method, we adopt a common candidate keyphrase
generation method proposed by Mihalcea and Tarau
(2004) as follows. We first select the top S keywords
for each topic, and then look for combinations of
these keywords that occur as frequent phrases in the
text collection. More details are given in Section 4.
3.4 Probabilistic Models for Topical Keyphrase
Ranking
With the candidate keyphrases, our next step is to
rank them. While a standard method is to simply
aggregate the scores of keywords inside a candidate
keyphrase as the score for the keyphrase, here we
propose a different probabilistic scoring function.
Our method is based on the following hypotheses
about good keyphrases given a topic:
381
Figure 2: Assumptions of variable dependencies.
Relevance: A good keyphrase should be closely re-
lated to the given topic and also discriminative. For
example, for the topic ?news,? ?president obama? is
a good keyphrase while ?math class? is not.
Interestingness: A good keyphrase should be inter-
esting and can attract users? attention. For example,
for the topic ?music,? ?justin bieber? is more inter-
esting than ?song player.?
Sometimes, there is a trade-off between these two
properties and a good keyphrase has to balance both.
Let R be a binary variable to denote relevance
where 1 is relevant and 0 is irrelevant. Let I be an-
other binary variable to denote interestingness where
1 is interesting and 0 is non-interesting. Let k denote
a candidate keyphrase. Following the probabilistic
relevance models in information retrieval (Lafferty
and Zhai, 2003), we propose to use P (R = 1, I =
1|t, k) to rank candidate keyphrases for topic t. We
have
P (R = 1, I = 1|t, k)
= P (R = 1|t, k)P (I = 1|t, k, R = 1)
= P (I = 1|t, k, R = 1)P (R = 1|t, k)
= P (I = 1|k)P (R = 1|t, k)
= P (I = 1|k)?
P (R = 1|t, k)
P (R = 1|t, k) + P (R = 0|t, k)
= P (I = 1|k)?
1
1 + P (R=0|t,k)P (R=1|t,k)
= P (I = 1|k)?
1
1 + P (R=0,k|t)P (R=1,k|t)
= P (I = 1|k)?
1
1 + P (R=0|t)P (R=1|t) ?
P (k|t,R=0)
P (k|t,R=1)
= P (I = 1|k)?
1
1 + P (R=0)P (R=1) ?
P (k|t,R=0)
P (k|t,R=1)
.
Here we have assumed that I is independent of t and
R given k, i.e. the interestingness of a keyphrase is
independent of the topic or whether the keyphrase is
relevant to the topic. We have also assumed that R
is independent of t when k is unknown, i.e. without
knowing the keyphrase, the relevance is independent
of the topic. Our assumptions can be depicted by
Figure 2.
We further define ? = P (R=0)P (R=1) . In general we
can assume that P (R = 0)  P (R = 1) because
there are much more non-relevant keyphrases than
relevant ones, that is, ?  1. In this case, we have
logP (R = 1, I = 1|t, k) (3)
= log
(
P (I = 1|k)?
1
1 + ? ? P (k|t,R=0)P (k|t,R=1)
)
? log
(
P (I = 1|k)?
P (k|t, R = 1)
P (k|t, R = 0)
?
1
?
)
= logP (I = 1|k) + log
P (k|t, R = 1)
P (k|t, R = 0)
? log ?.
We can see that the ranking score logP (R = 1, I =
1|t, k) can be decomposed into two components, a
relevance score log P (k|t,R=1)P (k|t,R=0) and an interestingness
score logP (I = 1|k). The last term log ? is a con-
stant and thus not relevant.
Estimating the relevance score
Let a keyphrase candidate k be a sequence of
words (w1, w2, . . . , wN ). Based on an independent
assumption of words given R and t, we have
log
P (k|t, R = 1)
P (k|t, R = 0)
= log
P (w1w2 . . . wN |t, R = 1)
P (w1w2 . . . wN |t, R = 0)
=
N?
n=1
log
P (wn|t, R = 1)
P (wn|t, R = 0)
. (4)
Given the topic model ?t previously learned for
topic t, we can set P (w|t, R = 1) to ?tw, i.e. the
probability of w under ?t. Following Griffiths and
Steyvers (2004), we estimate ?tw as
?tw =
#(Ct, w) + ?
#(Ct, ?) + ?|V|
. (5)
Here Ct denotes the collection of tweets assigned to
topic t, #(Ct, w) is the number of times w appears in
Ct, and #(Ct, ?) is the total number of words in Ct.
P (w|t, R = 0) can be estimated using a smoothed
background model.
P (w|R = 0, t) =
#(C, w) + ?
#(C, ?) + ?|V|
. (6)
382
Here #(C, ?) denotes the number of words in the
whole collection C, and #(C, w) denotes the number
of times w appears in the whole collection.
After plugging Equation (5) and Equation (6) into
Equation (4), we get the following formula for the
relevance score:
log
P (k|t, R = 1)
P (k|t, R = 0)
=
?
w?k
(
log
#(Ct, w) + ?
#(C, w) + ?
+ log
#(C, ?) + ?|V|
#(Ct, ?) + ?|V|
)
=
(?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?, (7)
where ? = #(C,?)+?|V|#(Ct,?)+?|V| and |k| denotes the number
of words in k.
Estimating the interestingness score
To capture the interestingness of keyphrases, we
make use of the retweeting behavior in Twitter. We
use string matching with RT to determine whether
a tweet is an original posting or a retweet. If a
tweet is interesting, it tends to get retweeted mul-
tiple times. Retweeting is therefore a stronger indi-
cator of user interests than tweeting. We use retweet
ratio |ReTweetsk||Tweetsk| to estimate P (I = 1|k). To prevent
zero frequency, we use a modified add-one smooth-
ing method. Finally, we get
logP (I = 1|k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
. (8)
Here |ReTweetsk| and |Tweetsk| denote the num-
bers of retweets and tweets containing the keyphrase
k, respectively, and lavg is the average number of
tweets that a candidate keyphrase appears in.
Finally, we can plug Equation (7) and Equa-
tion (8) into Equation (3) and obtain the following
scoring function for ranking:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(9)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?.
#user #tweet #term #token
13,307 1,300,300 50,506 11,868,910
Table 1: Some statistics of the data set.
Incorporating length preference
Our preliminary experiments with Equation (9)
show that this scoring function usually ranks longer
keyphrases higher than shorter ones. However, be-
cause our candidate keyphrase are extracted without
using any linguistic knowledge such as noun phrase
boundaries, longer candidate keyphrases tend to be
less meaningful as a phrase. Moreover, for our task
of using keyphrases to summarize Twitter, we hy-
pothesize that shorter keyphrases are preferred by
users as they are more compact. We would there-
fore like to incorporate some length preference.
Recall that Equation (9) is derived from P (R =
1, I = 1|t, k), but this probability does not allow
us to directly incorporate any length preference. We
further observe that Equation (9) tends to give longer
keyphrases higher scores mainly due to the term
|k|?. So here we heuristically incorporate our length
preference by removing |k|? from Equation (9), re-
sulting in the following final scoring function:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(10)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
.
4 Experiments
4.1 Data Set and Preprocessing
We use a Twitter data set collected from Singapore
users for evaluation. We used Twitter REST API1
to facilitate the data collection. The majority of the
tweets collected were published in a 20-week period
from December 1, 2009 through April 18, 2010. We
removed common stopwords and words which ap-
peared in fewer than 10 tweets. We also removed all
users who had fewer than 5 tweets. Some statistics
of this data set after cleaning are shown in Table 1.
We ran Twitter-LDA with 500 iterations of Gibbs
sampling. After trying a few different numbers of
1http://apiwiki.twitter.com/w/page/22554663/REST-API-
Documentation
383
topics, we empirically set the number of topics to
30. We set ? to 50.0/|T | as Griffiths and Steyvers
(2004) suggested, but set ? to a smaller value of 0.01
and ? to 20. We chose these parameter settings be-
cause they generally gave coherent and meaningful
topics for our data set. We selected 10 topics that
cover a diverse range of content in Twitter for eval-
uation of topical keyphrase extraction. The top 10
words of these topics are shown in Table 2.
We also tried the standard LDA model and the
author-topic model on our data set and found that
our proposed topic model was better or at least com-
parable in terms of finding meaningful topics. In ad-
dition to generating meaningful topics, Twitter-LDA
is much more convenient in supporting the compu-
tation of tweet-level statistics (e.g. the number of
co-occurrences of two words in a specific topic) than
the standard LDA or the author-topic model because
Twitter-LDA assumes a single topic assignment for
an entire tweet.
4.2 Methods for Comparison
As we have described in Section 3.1, there are three
steps to generate keyphrases, namely, keyword rank-
ing, candidate keyphrase generation, and keyphrase
ranking. We have proposed a context-sensitive top-
ical PageRank method (cTPR) for the first step of
keyword ranking, and a probabilistic scoring func-
tion for the third step of keyphrase ranking. We now
describe the baseline methods we use to compare
with our proposed methods.
Keyword Ranking
We compare our cTPR method with the original
topical PageRank method (Equation (1)), which rep-
resents the state of the art. We refer to this baseline
as TPR.
For both TPR and cTPR, the damping factor is
empirically set to 0.1, which always gives the best
performance based on our preliminary experiments.
We use normalized P (t|w) to set Pt(w) because our
preliminary experiments showed that this was the
best among the three choices discussed by Liu et al
(2010). This finding is also consistent with what Liu
et al (2010) found.
In addition, we also use two other baselines for
comparison: (1) kwBL1: ranking by P (w|t) = ?tw.
(2) kwBL2: ranking by P (t|w) = P (t)?
t
w?
t? P (t
?)?t?w
.
Keyphrase Ranking
We use kpRelInt to denote our relevance and inter-
estingness based keyphrase ranking function P (R =
1, I = 1|t, k), i.e. Equation (10). ? and ? are em-
pirically set to 0.01 and 500. Usually ? can be set to
zero, but in our experiments we find that our rank-
ing method needs a more uniform estimation of the
background model. We use the following ranking
functions for comparison:
? kpBL1: Similar to what is used by Liu et al
(2010), we can rank candidate keyphrases by
?
w?k f(w), where f(w) is the score assigned
to word w by a keyword ranking method.
? kpBL2: We consider another baseline ranking
method by
?
w?k log f(w).
? kpRel: If we consider only relevance but
not interestingness, we can rank candidate
keyphrases by
?
w?k log
#(Ct,w)+?
#(C,w)+? .
4.3 Gold Standard Generation
Since there is no existing test collection for topi-
cal keyphrase extraction from Twitter, we manually
constructed our test collection. For each of the 10
selected topics, we ran all the methods to rank key-
words. For each method we selected the top 3000
keywords and searched all the combinations of these
words as phrases which have a frequency larger than
30. In order to achieve high phraseness, we first
computed the minimum value of pointwise mutual
information for all bigrams in one combination, and
we removed combinations having a value below a
threshold, which was empirically set to 2.135. Then
we merged all these candidate phrases. We did not
consider single-word phrases because we found that
it would include too many frequent words that might
not be useful for summaries.
We asked two judges to judge the quality of the
candidate keyphrases. The judges live in Singapore
and had used Twitter before. For each topic, the
judges were given the top topic words and a short
topic description. Web search was also available.
For each candidate keyphrase, we asked the judges
to score it as follows: 2 (relevant, meaningful and in-
formative), 1 (relevant but either too general or too
specific, or informal) and 0 (irrelevant or meaning-
less). Here in addition to relevance, the other two
criteria, namely, whether a phrase is meaningful and
informative, were studied by Tomokiyo and Hurst
384
T2 T4 T5 T10 T12 T13 T18 T20 T23 T25
eat twitter love singapore singapore hot iphone song study win
food tweet idol road #singapore rain google video school game
dinner blog adam mrt #business weather social youtube time team
lunch facebook watch sgreinfo #news cold media love homework match
eating internet april east health morning ipad songs tomorrow play
ice tweets hot park asia sun twitter bieber maths chelsea
chicken follow lambert room market good free music class world
cream msn awesome sqft world night app justin paper united
tea followers girl price prices raining apple feature math liverpool
hungry time american built bank air marketing twitter finish arsenal
Table 2: Top 10 Words of Sample Topics on our Singapore Twitter Dateset.
(2003). We then averaged the scores of the two
judges as the final scores. The Cohen?s Kappa co-
efficients of the 10 topics range from 0.45 to 0.80,
showing fair to good agreement2. We further dis-
carded all candidates with an average score less than
1. The number of the remaining keyphrases for each
topic ranges from 56 to 282.
4.4 Evaluation Metrics
Traditionally keyphrase extraction is evaluated using
precision and recall on all the extracted keyphrases.
We choose not to use these measures for the fol-
lowing reasons: (1) Traditional keyphrase extraction
works on single documents while we study topical
keyphrase extraction. The gold standard keyphrase
list for a single document is usually short and clean,
while for each Twitter topic there can be many
keyphrases, some are more relevant and interesting
than others. (2) Our extracted topical keyphrases are
meant for summarizing Twitter content, and they are
likely to be directly shown to the users. It is there-
fore more meaningful to focus on the quality of the
top-ranked keyphrases.
Inspired by the popular nDCG metric in informa-
tion retrieval (Ja?rvelin and Keka?la?inen, 2002), we
define the following normalized keyphrase quality
measure (nKQM) for a methodM:
nKQM@K =
1
|T |
?
t?T
?K
j=1
1
log2(j+1)
score(Mt,j)
IdealScore(K,t)
,
where T is the set of topics, Mt,j is the j-
th keyphrase generated by method M for topic
2We find that judgments on topics related to social me-
dia (e.g. T4) and daily life (e.g. T13) tend to have a higher
degree of disagreement.
t, score(?) is the average score from the two hu-
man judges, and IdealScore(K,t) is the normalization
factor?score of the top K keyphrases of topic t un-
der the ideal ranking. Intuitively, ifM returns more
good keyphrases in top ranks, its nKQM value will
be higher.
We also use mean average precision (MAP) to
measure the overall performance of keyphrase rank-
ing:
MAP =
1
|T |
?
t?T
1
NM,t
|Mt|?
j=1
NM,t,j
j
1(score(Mt,j) ? 1),
where 1(S) is an indicator function which returns
1 when S is true and 0 otherwise, NM,t,j denotes
the number of correct keyphrases among the top j
keyphrases returned byM for topic t, and NM,t de-
notes the total number of correct keyphrases of topic
t returned byM.
4.5 Experiment Results
Evaluation of keyword ranking methods
Since keyword ranking is the first step for
keyphrase extraction, we first compare our keyword
ranking method cTPR with other methods. For each
topic, we pooled the top 20 keywords ranked by all
four methods. We manually examined whether a
word is a good keyword or a noisy word based on
topic context. Then we computed the average num-
ber of noisy words in the 10 topics for each method.
As shown in Table 5, we can observe that cTPR per-
formed the best among the four methods.
Since our final goal is to extract topical
keyphrases, we further compare the performance
of cTPR and TPR when they are combined with a
keyphrase ranking algorithm. Here we use the two
385
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
kpBL1 TPR 0.5015 0.54331 0.5611 0.5715 0.5984
kwBL1 0.6026 0.5683 0.5579 0.5254 0.5984
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6279
cTPR 0.6109 0.6218 0.6139 0.6062 0.6608
kpBL2 TPR 0.7294 0.7172 0.6921 0.6433 0.6379
kwBL1 0.7111 0.6614 0.6306 0.5829 0.5416
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6545
cTPR 0.7491 0.7429 0.6930 0.6519 0.6688
Table 3: Comparisons of keyphrase extraction for cTPR and baselines.
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
cTPR+kpBL1 0.61095 0.62182 0.61389 0.60618 0.6608
cTPR+kpBL2 0.74913 0.74294 0.69303 0.65194 0.6688
cTPR+kpRel 0.75361 0.74926 0.69645 0.65065 0.6696
cTPR+kpRelInt 0.81061 0.75184 0.71422 0.66319 0.6694
Table 4: Comparisons of keyphrase extraction for different keyphrase ranking methods.
kwBL1 kwBL2 TPR cTPR
2 3 4.9 1.5
Table 5: Average number of noisy words among the top
20 keywords of the 10 topics.
baseline keyphrase ranking algorithms kpBL1 and
kpBL2. The comparison is shown in Table 3. We
can see that cTPR is consistently better than the three
other methods for both kpBL1 and kpBL2.
Evaluation of keyphrase ranking methods
In this section we compare keypharse ranking
methods. Previously we have shown that cTPR is
better than TPR, kwBL1 and kwBL2 for keyword
ranking. Therefore we use cTPR as the keyword
ranking method and examine the keyphrase rank-
ing method kpRelInt with kpBL1, kpBL2 and kpRel
when they are combined with cTPR. The results are
shown in Table 4. From the results we can see the
following: (1) Keyphrase ranking methods kpRelInt
and kpRel are more effective than kpBL1 and kpBL2,
especially when using the nKQM metric. (2) kpRe-
lInt is better than kpRel, especially for the nKQM
metric. Interestingly, we also see that for the nKQM
metric, kpBL1, which is the most commonly used
keyphrase ranking method, did not perform as well
as kpBL2, a modified version of kpBL1.
We also tested kpRelInt and kpRel on TPR, kwBL1
and kwBL2 and found that kpRelInt and kpRel are
consistently better than kpBL2 and kpBL1. Due to
space limit, we do not report all the results here.
These findings support our assumption that our pro-
posed keyphrase ranking method is effective.
The comparison between kpBL2 with kpBL1
shows that taking the product of keyword scores is
more effective than taking their sum. kpRel and
kpRelInt also use the product of keyword scores.
This may be because there is more noise in Twit-
ter than traditional documents. Common words (e.g.
?good?) and domain background words (e.g. ?Sin-
gapore?) tend to gain higher weights during keyword
ranking due to their high frequency, especially in
graph-based method, but we do not want such words
to contribute too much to keyphrase scores. Taking
the product of keyword scores is therefore more suit-
able here than taking their sum.
Further analysis of interestingness
As shown in Table 4, kpRelInt performs better
in terms of nKQM compared with kpRel. Here we
study why it worked better for keyphrase ranking.
The only difference between kpRel and kpRelInt is
that kpRelInt includes the factor of user interests. By
manually examining the top keyphrases, we find that
the topics ?Movie-TV? (T5), ?News? (T12), ?Music?
(T20) and ?Sports? (T25) particularly benefited from
kpRelInt compared with other topics. We find that
well-known named entities (e.g. celebrities, politi-
cal leaders, football clubs and big companies) and
significant events tend to be ranked higher by kpRe-
lInt than kpRel.
We then counted the numbers of entity and event
keyphrases for these four topics retrieved by differ-
ent methods, shown in Table 6 . We can see that
in these four topics, kpRelInt is consistently better
than kpRel in terms of the number of entity and event
keyphrases retrieved.
386
T2 T5 T10 T12 T20 T25
chicken rice adam lambert north east president obama justin bieber manchester united
ice cream jack neo rent blk magnitude earthquake music video champions league
fried chicken american idol east coast volcanic ash lady gaga football match
curry rice david archuleta east plaza prime minister taylor swift premier league
chicken porridge robert pattinson west coast iceland volcano demi lovato f1 grand prix
curry chicken alexander mcqueen bukit timah chile earthquake youtube channel tiger woods
beef noodles april fools street view goldman sachs miley cyrus grand slam(tennis)
chocolate cake harry potter orchard road coe prices telephone video liverpool fans
cheese fries april fool toa payoh haiti earthquake song lyrics final score
instant noodles andrew garcia marina bay #singapore #business joe jonas manchester derby
Table 7: Top 10 keyphrases of 6 topics from cTPR+kpRelInt.
Methods T5 T12 T20 T25
cTPR+kpRel 8 9 16 11
cTPR+kpRelInt 10 12 17 14
Table 6: Numbers of entity and event keyphrases re-
trieved by different methods within top 20.
On the other hand, we also find that for some
topics interestingness helped little or even hurt the
performance a little, e.g. for the topics ?Food? and
?Traffic.? We find that the keyphrases in these top-
ics are stable and change less over time. This may
suggest that we can modify our formula to handle
different topics different. We will explore this direc-
tion in our future work.
Parameter settings
We also examine how the parameters in our model
affect the performance.
?: We performed a search from 0.1 to 0.9 with a
step size of 0.1. We found ? = 0.1 was the optimal
parameter for cTPR and TPR. However, TPR is more
sensitive to ?. The performance went down quickly
with ? increasing.
?: We checked the overall performance with
? ? {400, 450, 500, 550, 600}. We found that ? =
500 ? 0.01|V| gave the best performance gener-
ally for cTPR. The performance difference is not
very significant between these different values of ?,
which indicates that the our method is robust.
4.6 Qualitative evaluation of cTPR+kpRelInt
We show the top 10 keyphrases discovered by
cTPR+kRelInt in Table 7. We can observe that these
keyphrases are clear, interesting and informative for
summarizing Twitter topics.
We hypothesize that the following applications
can benefit from the extracted keyphrases:
Automatic generation of realtime trendy phrases:
For exampoe, keyphrases in the topic ?Food? (T2)
can be used to help online restaurant reviews.
Event detection and topic tracking: In the topic
?News? top keyphrases can be used as candidate
trendy topics for event detection and topic tracking.
Automatic discovery of important named entities:
As discussed previously, our methods tend to rank
important named entities such as celebrities in high
ranks.
5 Conclusion
In this paper, we studied the novel problem of topical
keyphrase extraction for summarizing and analyzing
Twitter content. We proposed the context-sensitive
topical PageRank (cTPR) method for keyword rank-
ing. Experiments showed that cTPR is consistently
better than the original TPR and other baseline meth-
ods in terms of top keyword and keyphrase extrac-
tion. For keyphrase ranking, we proposed a prob-
abilistic ranking method, which models both rele-
vance and interestingness of keyphrases. In our ex-
periments, this method is shown to be very effec-
tive to boost the performance of keyphrase extrac-
tion for different kinds of keyword ranking methods.
In the future, we may consider how to incorporate
keyword scores into our keyphrase ranking method.
Note that we propose to rank keyphrases by a gen-
eral formula P (R = 1, I = 1|t, k) and we have made
some approximations based on reasonable assump-
tions. There should be other potential ways to esti-
mate P (R = 1, I = 1|t, k).
Acknowledgements
This work was done during Xin Zhao?s visit to the
Singapore Management University. Xin Zhao and
Xiaoming Li are partially supported by NSFC under
387
the grant No. 60933004, 61073082, 61050009 and
HGJ Grant No. 2011ZX01042-001-001.
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In Pro-
ceedings of the 13th Biennial Conference of the Cana-
dian Society on Computational Studies of Intelligence:
Advances in Artificial Intelligence, pages 40?52.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in Twitter. In Proceedings of
the First Workshop on Social Media Analytics.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of ir techniques. ACM
Transactions on Information Systems, 20(4):422?446.
John Lafferty and Chengxiang Zhai. 2003. Probabilistic
relevance models based on document and query gener-
ation. Language Modeling and Information Retrieval,
13.
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin Chen.
2004. Incorporating document keyphrases in search
results. In Proceedings of the 10th Americas Confer-
ence on Information Systems.
Marina Litvak and Mark Last. 2008. Graph-based key-
word extraction for single-document summarization.
In Proceedings of the Workshop on Multi-source Mul-
tilingual Information Extraction and Summarization,
pages 17?24.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong
Sun. 2010. Automatic keyphrase extraction via topic
decomposition. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 366?376.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 490?499.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing or-
der into texts. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing micorblogs with topic models. In Pro-
ceedings of the 4th International Conference on We-
blogs and Social Media.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International World Wide Web Conference.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 33?40.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the 4th International
Conference on Weblogs and Social Media.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, (4):303?336.
Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010. TwitterRank: finding topic-sensitive influential
twitterers. In Proceedings of the third ACM Interna-
tional Conference on Web Search and Data Mining.
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. Au-
tomatic generation of personalized annotation tags for
twitter users. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 689?692.
Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Lim Ee-
Peng, Hongfei Yan, and Xiaoming Li. 2011. Compar-
ing Twitter and traditional media using topic models.
In Proceedings of the 33rd European Conference on
Information Retrieval.
388
A Pipeline Approach to Chinese Personal Name
Disambiguation
Yang Song, Zhengyan He, Chen Chen, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
{ysong, hezhengyan, chenchen, wanghf}@pku.edu.cn
Abstract
In this paper, we describe our sys-
tem for Chinese personal name dis-
ambiguation task in the first CIPS-
SIGHAN joint conference on Chinese
Language Processing(CLP2010). We
use a pipeline approach, in which pre-
processing, unrelated documents dis-
carding, Chinese personal name exten-
sion and document clustering are per-
formed separately. Chinese personal
name extension is the most important
part of the system. It uses two addi-
tional dictionaries to extract full per-
sonal names in Chinese text. And then
document clustering is performed un-
der different personal names. Exper-
imental results show that our system
can achieve good performances.
1 Introduction
Personal name search is one of the most im-
portant tasks for search engines. When a per-
sonal name query is given to a search engine,
a list of related documents will be shown. But
not all of the returned documents refer to the
same person whom users want to find. For ex-
ample, the query name ?jordan? is submitted
to a search engine, we can get a lot of doc-
uments containing ?jordan?. Some of them
may refer to the computer scientist, others
perhaps refer to the basketball player. For
English, there have been three Web People
Search (WePS1) evaluation campaigns on per-
sonal name disambiguation. But for Chinese,
1http://nlp.uned.es/weps/
this is the first time. It encounters more chal-
lenge for Chinese personal name disambigua-
tion. There are no word boundary in Chinese
text, so it becomes difficult to recognize the
full personal names from Chinese text. For ex-
ample, a query name ???? is given, but the
full personal name from some documents may
be an extension of ????, like ????? or
?????, and so on. Meanwhile, ???? can
also be a common Chinese word. So we need
to discard those documents which are not ref-
ered to any person related to the given query
name.
To solve the above-mentioned problem, we
explore a pipeline approach to Chinese per-
sonal name disambiguation. The overview of
our system is illustrated in Figure 1. We split
this task into four parts: preprocessing, unre-
lated documents discarding, Chinese personal
name extension and document clustering. In
preprocessing and unrelated documents dis-
carding, we use word segmentation and part-
of-speech tagging tools to process the given
dataset and documents are discarded when
the given query name is not tagged as a per-
sonal name or part of a personal name. After
that we perform personal name extension in
the documents for a given query name. When
the query name has only two characters. We
extend it to the left or right for one character.
For example, we can extend ???? to ???
?? or ?????. The purpose of extending
the query name is to obtain the full personal
name. In this way, we can get a lot of full per-
sonal names for a given query name from the
documents. And then document clustering
Figure 1: Overview of the System
is performed under different personal names.
HAC (Hierarchical Agglomerative Clustering)
is selected here. We represent documents with
bag of words and solve the problem in vector
space model, nouns, verbs, bigrams of nouns
or verbs and named entities are selected as
features. The feature weight value takes 0 or
1. In HAC, we use group-average link method
as the distance measure and consine similar-
ity as the similarity computing measure. The
stopping criteria is dependent on a threshold
which is obtained from training data. Our sys-
tem produces pretty good results in the final
evaluation.
The remainder of this paper is organized as
follows. Section 2 introduces related work.
Section 3 gives a detailed description about
our pipeline approach. It includes preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering. Section 4 presents the experimental
results. The conclusions are given in Section
5.
2 Related Work
Several important studies have tried to
solve the task introduced in the previous sec-
tion. Most of them treated it as an cluster-
ing problem. Bagga & Baldwin (1998) first
selected tokens from local context as features
to perform intra-document coreference resolu-
tion. Mann & Yarowsky (2003) extracted lo-
cal biographical information as features. Niu
et al (2004) used relation extraction results
in addition to local context features and get a
perfect results. Al-Kamha and Embley (2004)
clustered search results with feature set in-
cluding attributes, links and page similarities.
In recent years, this problem has attracted
a great deal of attention from many research
institutes. Ying Chen et al (2009) used a
Web 1T 5-gram corpus released by Google
to extract additional features for clustering.
Masaki Ikeda et al (2009) proposed a two-
stage clustering algorithm to improve the low
recall values. In the first stage, some reliable
features (like named entities) are used to con-
nect documents about the same person. Af-
ter that, the connected documents (document
cluster) are used as a source from which new
features (compound keyword features) are ex-
tracted. These new features are used in the
second stage to make additional connections
between documents. Their approach is to im-
prove clusters step by step, where each step
refines clusters conservatively. Han & Zhao
(2009) presented a system named CASIANED
to disambiguate personal names based on pro-
fessional categorization. They first catego-
rize different personal name appearances into
a real world professional taxonomy, and then
the personal name appearances are clustered
into a single cluster. Chen Chen et al (2009)
explored a novel feature weight computing
method in clustering. It is based on the point-
wise mutual information between the ambigu-
ous name and features. In their paper, they
also develop a trade-off point based cluster
stopping criteria which find the trade-off point
between intra-cluster compactness and inter-
cluster separation.
Our approach is based on Chinese per-
sonal name extension. We recognize the full
personal names in Chinese text and perform
document clustering under different personal
names.
3 Methodology
In this section, we will explain preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering in order.
3.1 Preprocessing
We use ltp-service2 to process the given Chi-
nese personal name disambiguation dataset
(a detailed introduction to it will be given
in section 4). Training data in the dataset
contains 32 query names. There are 100-300
documents under every query name. All the
documents are collected from Xinhua News
Agency. They contain the exact same string
as query names. Ltp-service is a web ser-
vice interface for LTP3(Language Technology
Platform). LTP has integrated many Chinese
processing modules, including word segmen-
tation, part-of-speech tagging, named entity
recognition, word sense disambiguation, and
so on. Jun Lang et al (2006) give a detailed
introduction to LTP. Here we only use LTP
to generate word segmentation, part-of-speech
tagging and named entity recognition results
for the given dataset.
3.2 Unrelated documents discarding
Under every query name, there are 100-300
documents. But not all of them are really re-
lated. For example, ???? is a query name in
training data. In corresponding documents,
some are refered to real personal names like
???? or ?????. But others may be a sub-
string of an expression such as ??????
??. These documents are needed to be fil-
tered out. We use the preprocessing tool LTP
to slove this problem. LTP can do word seg-
mentation and part-of-speech tagging for us.
For each document under a given query name,
if the query name in the document is tagged as
a personal name or part of some extended per-
sonal name, the document will be marked as
undiscarded, otherwise the document will be
discarded. Generally speaking, for the query
name containing three characters, we don?t
need to discard any of the corresponding doc-
uments. But in practice, we find that for some
query names, LTP always gives the invariable
2http://code.google.com/p/ltp-service/
3http://ir.hit.edu.cn/ltp/
part-of-speech. For example, no matter what
the context of ???? is, it is always tagged
as a geographic name. So we use another pre-
processing tool ICTCLAS4. Only when both
of them mark one document as discarded, we
discard the corresponding document.
3.3 Chinese personal name extension
After discarding unrelated documents, we
need to recognize the full Chinese personal
names. We hypothesize that the full Chinese
personal name has not more than three char-
acters (We don?t consider the compound sur-
names here). So the query names containing
only two Chinese characters are considered to
extend. In our approach, we use two Chinese
personal names dictionaries. One is a sur-
name dictionary containing 423 one-character
entries. We use it to do left extend for the
query name. For example, the query name
is ???? and its left character in a docu-
ment is ???, we will extend it to full per-
sonal name ?????. The other is a non-
ending Chinese character dictionary contain-
ing 64 characters which could not occur at the
end of personal names. It is constructed by a
personal title dictionary. We use every title?s
first character and some other special charac-
ters (such as numbers or punctuations) to con-
stuct the dictionary. Some manual work has
also been done to filter a few incorrect charac-
ters. Several examples of the two dictionaries
are shown in Table 1.
Through the analysis of Xinhua News arti-
cles, we also find that nearly half of the docu-
ments under given query name actually refer
to the reporters. And they often appear in
the first or last brackets in the body of cor-
responding document. For example, ?(??
????????)? is a sentence containing
query name ????. We use some simple but
efficient rules to get full personal names for
this case.
3.4 Document clustering
For every query name, we can get a list of
full peronal names. For example, when the
4http://ictclas.org/
Table 1: Several Examples of the two Dictionaries
Dictionaries Examples
Surnames ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?...
Non-ending Chinese characters ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?...
query name is ????, we can get the per-
sonal names like ?????, ?????, ???
??, ?????. And then document clustering
is performed under different personal names.
3.4.1 Features
We use bag of words to represent docu-
ments. Some representative words need to be
chosen as features. LTP can give us POS tag-
ging and NER results. We select all the nouns,
verbs and named entities which appear in the
same paragraph with given query name as fea-
tures. Meanwhile, the bigrams of nouns or
verbs are also selected. We take 0 or 1 for
feature weight value. 0 represents that the
feature doesn?t appear in corresponding para-
graphs, and 1 represents just the opposite. We
find that this weighting scheme is more effec-
tive than TFIDF.
3.4.2 Clustering
All features are represented in vector space
model. Every document is modeled as a ver-
tex in the vector space. So every document
can be seen as a feature vector. Before cluster-
ing, the similarity between documents is com-
puted by cosine value of the angle between
feature vectors. We use HAC to do document
clustering. It is a bottom-up algorithm which
treats each document as a singleton cluster at
the outset and then successively merges (or
agglomerates) pairs of clusters until all clus-
ters have been merged into a single cluster
that contains all documents. From our ex-
perience, single link and group-average link
method seem to work better than complete
link one. We use group-average link method
in the final submission. The stopping criteria
is a difficult problem for clustering. Here we
use a threshold for terminating condition. So
it is not necessary to determine the number
of clusters beforehand. We select a threshold
which produces the best performance in train-
ing data.
4 Experimental Results
The dataset for Chinese personal name dis-
ambiguation task contains training data and
testing data. The training data contains
32 query names. Every query name folder
contains 100-300 news articles. Given the
query name, all the documents are retrived
by character-based matching from a collection
of Xinhua news documents in a time span of
fourteen years. The testing data contains 25
query names. Two threshold values as termi-
nating conditions are obtained from training
data. They are 0.4 and 0.5. For evaluation,
we use P-IP score and B-cubed score (Bagga
and Baldwin, 1998). Table 2 & Table 3 show
the official evaluation results.
Table 2: Official Results for P-IP score
Threshold P-IP
P IP F score
0.4 88.32 94.9 91.15
0.5 91.3 91.77 91.18
Table 3: Official Results for B-Cubed score
Threshold B-Cubed
Precision Recall F score
0.4 83.68 92.23 86.94
0.5 87.87 87.49 86.84
Besides the formal evaluation, the organizer
also provide a diagnosis test designed to ex-
plore the relationship between Chinese word
segmentation and personal name disambigua-
tion. That means the query names in the
documents are segmented correctly by manual
work. Table 4 & Table 5 show the diagnosis
results.
Table 4: Diagnosis Results for P-IP score
Threshold P-IP
P IP F score
0.4 89.01 95.83 91.96
0.5 91.85 92.68 91.96
Table 5: Diagnosis Results for B-Cubed score
Threshold B-Cubed
Precision Recall F score
0.4 84.53 93.42 87.96
0.5 88.59 88.59 87.8
The official results show that our method
performs pretty good. The diagnosis results
show that correct word segmentation can im-
prove the evaluation results. But the improve-
ment is rather limited. That is mainly because
Chinese personal name extension is done well
in our approach. So the diagnosis results don?t
gain much profit from query names? correct
segmentation.
5 Conclusions
We describe our framework in this paper.
First, we use LTP to do preprocessing for orig-
inal dataset which comes from Xinhua news
articles. LTP can produce good results for
Chinese text processing. And then we use
two additional dictionaries(one is Chinese sur-
name dictionary, the other is Non-ending Chi-
nese character dictionary) to do Chinese per-
sonal name extension. After that we perform
document clustering under different personal
names. Official evaluation results show that
our method can achieve good performances.
In the future, we will attempt to use other
features to represent corresponding persons in
the documents. We will also investigate auto-
matic terminating condition.
6 Acknowledgments
This research is supported by National
Natural Science Foundation of Chinese
(No.60973053) and Research Fund for the
Doctoral Program of Higher Education of
China (No.20090001110047).
References
J. Artiles, J. Gonzalo, and S. Sekine. 2009. WePS
2 evaluation campaign: overview of the web peo-
ple search clustering task. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Bagga and B. Baldwin. 1998. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of 17th Interna-
tional Conference on Computational Linguis-
tics, 79?85.
Mann G. and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In Proceedings
of CoNLL-2003, 33?40, Edmonton, Canada.
C. Niu, W. Li, and R. K. Srihari. 2004. Weakly
Supervised Learning for Cross-document Person
Name Disambiguation Supported by Informa-
tion Extraction. In Proceedings of ACL 2004.
Al-Kamha. R. and D. W. Embley. 2004. Group-
ing search-engine returned citations for person-
name queries. In Proceedings of WIDM 2004,
96-103, Washington, DC, USA.
Ying Chen, Sophia Yat Mei Lee, and Chu-Ren
Huang. 2009. PolyUHK:A Robust Information
Extraction System for Web Personal Names.
In 2nd Web People Search Evaluation Work-
shop(WePS 2009), 18th WWW Conference.
Masaki Ikeda, Shingo Ono, Issei Sato, Minoru
Yoshida, and Hiroshi Nakagawa. 2009. Person
Name Disambiguation on the Web by Two-Stage
Clustering. In 2nd Web People Search Evalua-
tion Workshop(WePS 2009), 18th WWW Con-
ference.
Xianpei Han and Jun Zhao. 2009. CASIANED:
Web Personal Name Disambiguation Based on
Professional Categorization. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Chen Chen, Junfeng Hu, and Houfeng Wang.
2009. Clustering technique in multi-document
personal name disambiguation. In Proceed-
ings of the ACL-IJCNLP 2009 Student Research
Workshop, pages 88?95.
Jun Lang, Ting Liu, Huipeng Zhang and Sheng Li.
2006. LTP: Language Technology Platform. In
Proceedings of SWCL 2006.
Bagga, Amit and B. Baldwin. 1998. Algorithms
for scoring co-reference chains. In Proceedings
of the First International Conference on Lan-
guage Resources and Evaluation Workshop on
Linguistic co-reference.
Applying Spectral Clustering for Chinese Word Sense Induction
Zhengyan He, Yang Song, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
{hezhengyan, ysong, wanghf}@pku.edu.cn
Abstract
Sense Induction is the process of identify-
ing the word sense given its context, often
treated as a clustering task. This paper ex-
plores the use of spectral cluster method
which incorporates word features and n-
gram features to determine which cluster
the word belongs to, each cluster repre-
sents one sense in the given document set.
1 Introduction
Word Sense Induction(WSI) is defined as the
process of identifying different senses of a tar-
get word in a given context in an unsupervised
method. It?s different from word sense disam-
biguation(WSD) in that senses in WSD are as-
sumed to be known. The disadvantage of WSD
is that it derives the senses of word from existing
dictionaries or other corpus and the senses cannot
be extended to other domains. WSI can overcome
this problem as it can automatically derive word
senses from the given document set, or a specific
domain.
Many different approaches based on co-
occurence have been proposed so far. Bordag
(2006) proposes an approach that uses triplets
of co-occurences. The most significant co-
occurences of target word are used to build triplets
that consist of the target word and its two co-
occurences. Then intersection built from the co-
occurence list of each word in the triplet is used
as feature vector. After merging similar triplets
that have more than 80% overlapping words, clus-
tering is performed on the triplets. Triplets with
fewer than 4 intersection words are removed in or-
der to reduce noise.
LDA model has also been applied to WSI
(Brody and Lapata, 2009). Brody proposes a
method that treats document and topics in LDA
as word context and senses respectively. The pro-
cess of generating the context words is as follows:
first generate sense from a multinomial distribu-
tion given context, then generate context words
given sense. They also derive a layered model
to incorporate different kind of features and use
Gibbs sampling method to solve the problem.
Graph-based methods become popular recently.
These methods use the co-occurence graph of
context words to obtain sense clusters based on
sub-graph density. Markov clustering(MCL) has
been used to identify dense regions of graph
(Agirre and Soroa, 2007).
Spectral clustering performs well on problems
in which points cluster based on shape. The
method is that first compute the Laplace matrix
of the affinity matrix, then reform the data points
by stacking the largest eigenvectors of the Laplace
matrix in columns, finally cluster the new data
points using a more simple clustering method like
k-means (Ng et al, 2001).
2 Methodology
Our approach follows a common cluster model
that represents the given context as a word vec-
tor and later uses a spectral clustering method to
group each instance in its own cluster.
Different types of polysemy may arise and the
most significant distinction may be the syntactic
classes of the word and the conceptually differ-
ent senses (Bordag, 2006). Thus we must extract
the features able to distinguish these differences.
They are:
Local tokens: the word occuring in the window
-3 ? +3;
Local bigram feature: bigram within -5 ? +5
Chinese character range;
The above two features model the syntactic us-
age of a specific sense of a Chinese word.
Topical or conceptual feature: the content
words (pos-tagged as noun, verb, adjective) within
the given sentence. As the sentence in the training
set seems generally short, a short window may not
contains enough infomation.
We represent the words in a 0-1 vector accord-
ing to their existence in a given sentence. Then the
similarity measure between two given sentences is
derived from their cosine similarity. We find that it
is difficult to define the relative importance of dif-
ferent types of features in order to combine them
in one vector space, and find that ignoring weight
achieve better result. Brody (2009) achieves this
in LDA model through a layered model with dif-
ferent probability of feature given sense.
Later we use a spectral clustering method from
R kernlab package (Karatzoglou et al, 2004)
which implements the algorithm described in (Ng
et al, 2001). Instead of using the Gaussian kernel
matrix as the similarity matrix we use the cosine
similarity derived above.
One observation is that instances with the same
target word sense often appear in the same con-
text. However, for some verb in Chinese, it is of-
ten the case that one sense relates to a concrete
object while the other relates to a more broad and
abstract concept and the context varies consider-
ably. Simple word co-occurence cannot define a
good similarity measure to group these cases into
one cluster. We must consider semantic related-
ness measures between contexts.
3 Performance
Our system performs well on the training set. Two
methods are used to evaluate the performance un-
der different features.
method precision recall F-score
Purity-based 81.11 83.19 81.99
B-cubed 74.41 76.51 75.33
Table 1: The performance of training set
Our system finally gets a F-score of 0.7598 on
the test set.
4 Conclusion
Our experiment in the Chinese word sense induc-
tion task performs good with respect to the relative
small corpus(only the training set). But only con-
sidering token co-occurence cannot achieve better
result. Moreover, it is difficult to define a simi-
larity measure solely based on lexicon infomation
with no regard to semantic relatedness. Finally,
combining different types of features seems to be
another challenge in our model.
5 Acknowledgments
This research is supported by National Natural
Science Foundation of Chinese (No.9092001).
References
Agirre, Eneko and Aitor Soroa. 2007. Ubc-as: a graph
based unsupervised system for induction and classi-
fication. In SemEval ?07: Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 346?349, Morristown, NJ, USA. Association
for Computational Linguistics.
Bordag, Stefan. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. In
EACL. The Association for Computer Linguistics.
Brody, Samuel and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL, pages 103?111.
The Association for Computer Linguistics.
Karatzoglou, Alexandros, Alex Smola, Kurt Hornik,
and Achim Zeileis. 2004. kernlab ? an S4 pack-
age for kernel methods in R. Journal of Statistical
Software, 11(9):1?20.
Ng, Andrew Y., Michael I. Jordan, and Yair Weiss.
2001. On spectral clustering: Analysis and an al-
gorithm. In Advances in Neural Information Pro-
cessing Systems 14, pages 849?856. MIT Press.
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 131?135,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Link Type Based Pre-Cluster Pair Model for Coreference Resolution
Yang Song?, Houfeng Wang? and Jing Jiang?
?Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
?School of Information Systems, Singapore Management University, Singapore
{ysong, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg
Abstract
This paper presents our participation in the
CoNLL-2011 shared task, Modeling Unre-
stricted Coreference in OntoNotes. Corefer-
ence resolution, as a difficult and challenging
problem in NLP, has attracted a lot of atten-
tion in the research community for a long time.
Its objective is to determine whether two men-
tions in a piece of text refer to the same en-
tity. In our system, we implement mention de-
tection and coreference resolution seperately.
For mention detection, a simple classification
based method combined with several effective
features is developed. For coreference resolu-
tion, we propose a link type based pre-cluster
pair model. In this model, pre-clustering of all
the mentions in a single document is first per-
formed. Then for different link types, different
classification models are trained to determine
wheter two pre-clusters refer to the same en-
tity. The final clustering results are generated
by closest-first clustering method. Official test
results for closed track reveal that our method
gives a MUC F-score of 59.95%, a B-cubed
F-score of 63.23%, and a CEAF F-score of
35.96% on development dataset. When using
gold standard mention boundaries, we achieve
MUC F-score of 55.48%, B-cubed F-score of
61.29%, and CEAF F-score of 32.53%.
1 Introduction
The task of coreference resolution is to recognize
all the mentions (also known as noun phrases, in-
cluding names, nominal mentions and pronouns)
in a text and cluster them into equivalence classes
where each quivalence class refers to a real-world
entity or abstract concept. The CoNLL-2011 shared
task1 uses OntoNotes2 as the evaluation corpus. The
coreference layer in OntoNotes constitutes one part
of a multi-layer, integrated annotation of the shal-
low semantic structures in the text with high inter-
annotator agreement. In addition to coreference,
this data set is also tagged with syntactic trees, high
coverage verb and some noun propositions, partial
verb and noun word senses, and 18 named entity
types. The main difference between OntoNotes and
another wellknown coreference dataset ACE is that
the former does not label any singleton entity clus-
ter, which has only one reference in the text. We can
delete all the singleton clusters as a postprocessing
step for the final results. Alternatively, we can also
first train a classifier to separate singleton mentions
from the rest and apply this mention detection step
before coreference resolution. In this work we adopt
the second strategy.
In our paper, we use a traditional learning based
pair-wise model for this task. For mention detec-
tion, we first extract all the noun phrases in the text
and then use a classification model combined with
some effective features to determine whether each
noun phrase is actually a mention. The features in-
clude word features, POS features in the given noun
phrase and its context, string matching feature in
its context, SRL features, and named entity features
among others. More details will be given in Sec-
tion 3. From our in-house experiments, the final F-
scores for coreference resolution can be improved
by this mention detection part. For coreference res-
1http://conll.bbn.com
2http://www.bbn.com/ontonotes/
131
Features describing ci or cj
Words The first and last words of the given NP in ci ( or cj) , also including the words in the
context with a window size 2
POS Tags The part of speech tags corresponding to the words
Pronoun Y if mentions in ci( or cj) are pronouns; else N
Definite Y if mentions in ci( or cj) are definite NP; else N
Demonstrative Y if mentions in ci( or cj) are demonstrative NP; else N
Number Singular or Plural, determined using a data file published by Bergsma and Lin (2006)
Gender Male, Female, Neuter, or Unknown, determined using a data file published by Bergsma
and Lin (2006)
Semantic Class Semantic Classes are given by OntoNotes for named entities
Mentino Type Common Noun Phrases or Pronouns
Table 1: The feature set describing ci or cj .
olution, a traditinal pair-wise model is applied, in
which we first use exact string matching to generate
some pre-clusters. It should be noted that each pro-
noun must be treated as a singleton pre-cluster, be-
cause they are not like names or nominal mentions,
which can be resolved effectively with exact string
matching. We then implement a classification based
pre-cluster pair model combined with several ef-
fective coreference resolution features to determine
whether two pre-clusters refer to the same entity. Fi-
nally, we use closest-first clustering method to link
all the coreferential pre-clusters and generate the fi-
nal cluster results. As mentioned before, mentions
have three types: names, nominal mentions and pro-
nouns. Among them pronouns are very different
from names and nominal mentions, because they can
only supply limited information literally. So we de-
fine three kinds of link types for pre-cluster pairs:
NP-NP link, NP-PRP link and PRP-PRP link. (Here
NP means Noun Phrases and PRP means Pronom-
inal Phrases.) One link represents one pre-cluster
pair. Intuitively, different link types tend to use dif-
ferent features to determine whether this kind of link
is coreferential or not. We implement three kinds
of pre-cluster pair model based on three link types.
Experimental results show that combined with out-
puts from different link type based pre-cluster pair
model can give better results than using an uni-
fied classification model for three different kinds of
link types. For all the classification models, we use
opennlp.maxent3 package.
The rest of this paper is organized as follows. Sec-
tion 2 describes our mention detection method. We
discuss our link type based pre-cluster pair model
for coreference resolution in Section 3, evaluate it in
Section 4, and conclude in Section 5.
2 Mention Detection
We select all the noun phrases tagged by the
OntoNotes corpus as mention candidates and im-
plement a classification-based model combined
with several commonly used features to determine
whether a given noun phrase is a mention. The fea-
tures are given below:
? Word Features - They include the first word and the
last word in each given noun phrase. We also use
words in the context of the noun phrase within a
window size of 2.
? POS Features - We use the part of speech tags of
each word in the word features.
? Position Features - These features indicate where
the given noun phrase appears in its sentence: be-
gining, middle, or end.
? SRL Features - The Semantic Role of the given
noun phrase in its sentence.
? Verb Features - The verb related to the Semantic
Role of the given noun phrase.
3http://incubator.apache.org/opennlp/
132
Features describing the relationship between ci and cj
Distance The minimum distance between mentions in ci and cj
String Match Y if mentions are the same string; else N
Substring Match Y if one mention is a substring of another; else N
Levenshtein Distance Levenshtein Distance between the mentions
Number Agreement Y if the mentions agree in number; else N
Gender Agreement Y if the mentions agree in gender; else N
N & G Agreement Y if mentions agree in both number and gender; else N
Both Pronouns Y if the mentions are both pronouns; else N
Verb Agreement Y if the mentions have the same verb.
SRL Agreement Y if the mentions have the same semantic role
Position Agreement Y if the mentions have the same position (Beginning, Middle or End) in sentences
Table 2: The feature set describing the relationship between ci and cj .
? Entity Type Features - The named entity type for the
given noun phrase.
? String Matching Features - True if there is another
noun phrase wich has the same string as the given
noun phrase in the context.
? Definite NP Features - True if the given noun phrase
is a definite noun phrase.
? Demonstrative NP Features - True if the given noun
phrase is a demonstrative noun phrase.
? Pronoun Features - True if the given noun phrase is
a pronoun.
Intutively, common noun phrases and pronouns
might have different feature preferences. So we train
classification models for them respectively and use
the respective model to predicate for common noun
phrases or pronouns. Our mention detection model
can give 52.9% recall, 80.77% precision and 63.93%
F-score without gold standard mention boundaries
on the development dataset. When gold standard
mention boundaries are used, the results are 53.41%
recall, 80.8% precision and 64.31% F-score. (By us-
ing the gold standard mention boundaries, we mean
we use the gold standard noun phrase boundaries.)
3 Coreference Resolution
After getting the predicated mentions, we use some
heuristic rules to cluster them with the purpose of
generating highly precise pre-clusters. For this task
Metric Recall Precision F-score
MUC 49.64% 67.18% 57.09%
BCUBED 59.42% 70.99% 64.69%
CEAF 45.68% 30.56% 36.63%
AVERAGE 51.58% 56.24% 52.80%
Table 3: Evaluation results on development dataset with-
out gold mention boundaries
Metric Recall Precision F-score
MUC 48.94% 67.72% 56.82%
BCUBED 58.52% 72.61% 64.81%
CEAF 46.49% 30.45% 36.8%
AVERAGE 51.32% 56.93% 52.81%
Table 4: Evaluation results on development dataset with
gold mention boundaries
only identity coreference is considered while attribu-
tive NP and appositive construction are excluded.
That means we cannot use these two important
heuristic rules to generate pre-clusters. In our sys-
tem, we just put all the mentions (names and nomi-
nal mentions, except pronouns) which have the same
string into the identical pre-clusters. With these pre-
clusters and their coreferential results, we imple-
ment a classification based pre-cluster pair model to
determine whether a given pair of pre-clusters re-
fer to the same entity. We follow Rahman and Ng
(2009) to generate most of our features. We also
include some other features which intuitively seem
effective for coreference resolution. These features
133
Metric Recall Precision F-score
MUC 42.66% 53.7% 47.54%
BCUBED 61.05% 74.32% 67.04%
CEAF 40.54% 32.35% 35.99%
AVERAGE 48.08% 53.46% 50.19%
Table 5: Evaluation results on development dataset
with gold mention boundaries using unified classification
model
Metric Recall Precision F-score
MUC 53.73% 67.79% 59.95%
BCUBED 60.65% 66.05% 63.23%
CEAF 43.37% 30.71% 35.96%
AVERAGE 52.58% 54.85% 53.05%
Table 6: Evaluation results on test dataset without gold
mention boundaries
are shown in Table 1 and Table 2. For simplicity, we
use ci and cj to represent pre-clusters i and j. Each
pre-cluster pair can be seen as a link. We have three
kinds of link types: NP-NP link, NP-PRP link and
PRP-PRP link. Different link types may have differ-
ent feature preferences. So we train the classifica-
tion based pre-cluster pair model for each link type
separately and use different models to predicate the
results. With the predicating results for pre-cluster
pairs, we use closest-first clustering to link them and
form the final cluster results.
4 Experimental Results
We present our evaluation results on development
dataset for CoNLL-2011 shared Task in Table 3, Ta-
ble 4 and Table 5. Official test results are given
in Table 6 and Table 7. Three different evaluation
metrics were used: MUC (Vilain et al, 1995), B3
(Bagga and Baldwin, 1998) and CEAF (Luo, 2005).
Finally, the average scores of these three metrics are
used to rank the participating systems. The differ-
ence between Table 3 and Table 4 is whether gold
standard mention boundaries are given. Here ?men-
tion boundaries? means a more broad concept than
the mention definition we gave earlier. We should
also detect real mentions from them. From the ta-
bles, we can see that the scores can be improved litt-
tle by using gold standard mention boundaries. Also
the results from Table 5 tell us that combining differ-
ent link-type based classification models performed
Metric Recall Precision F-score
MUC 46.66% 68.40% 55.48%
BCUBED 54.40% 70.19% 61.29%
CEAF 43.77% 25.88% 32.53%
AVERAGE 48.28% 54.82% 49.77%
Table 7: Evaluation results on test dataset with gold men-
tion boundaries
better than using an unified classification model. For
official test results, our system did not perform as
well as we had expected. Some possible reasons are
as follows. First, verbs that are coreferential with a
noun phrase are also tagged in OntoNotes. For ex-
ample, ?grew ? and ?the strong growth? should be
linked in the following case: ?Sales of passenger
cars grew 22%. The strong growth followed year-
to-year increases.? But we cannot solve this kind
of problem in our system. Second, we should per-
form feature selection to avoid some useless features
harming the scores. Meanwhile, we did not make
full use of the WordNet, PropBank and other back-
ground knowledge sources as features to represent
pre-cluster pairs.
5 Conclusion
In this paper, we present our system for CoNLL-
2011 shared Task, Modeling Unrestricted Corefer-
ence in OntoNotes. First some heuristic rules are
performed to pre-cluster all the mentions. And then
we use a classification based pre-cluster pair model
combined with several cluster level features. We
hypothesize that the main reason why we did not
achieve good results is that we did not carefully ex-
amine the features and dropped the feature selec-
tion procedure. Specially, we did not make full use
of background knowledge like WordNet, PropBank,
etc. In our future work, we will make up for the
weakness and design a more reasonable model to ef-
fectively combine all kinds of features.
Acknowledgments
This research is supported by National Natu-
ral Science Foundation of Chinese (No.60973053,
No.91024009) and Research Fund for the Doc-
toral Program of Higher Education of China
(No.20090001110047).
134
References
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011), Portland, Oregon.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A Model-Theoretic Coreference
Scoring Scheme. In Proceedings of the Sixth Message
Understanding Conference (MUC-6), pages 4552, San
Francisco, CA. Morgan Kaufmann.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the 1st
International Conference on Language Resources and
Evaluation, Granada, Spain, pp. 563566.
Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Vancouver, B.C., Canada, pp. 2532.
Vincent Ng. 2008. Unsupervised Models for Corefer-
ence Resolution. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pp. 640?649.
Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing.
Vincent Ng. 2010. Supervised Noun Phrase Coreference
Research: The First Fifteen Years. In Proceedings of
the 48th Meeting of the Association for Computational
Linguistics (ACL 2010), Uppsala, pages 1396-1411.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
Path-Based Pronoun Resolution. In COLING?ACL
2006, pages 33?40.
135
