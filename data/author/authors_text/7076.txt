Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 129?132,
New York, June 2006. c?2006 Association for Computational Linguistics
Parser Combination by Reparsing 
 
Kenji Sagae and Alon Lavie 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
{sagae,alavie@cs.cmu.edu} 
  
 
Abstract 
We present a novel parser combination 
scheme that works by reparsing input sen-
tences once they have already been parsed 
by several different parsers.  We apply this 
idea to dependency and constituent parsing, 
generating results that surpass state-of-the-
art accuracy levels for individual parsers. 
1 Introduction 
Over the past decade, remarkable progress has 
been made in data-driven parsing.  Much of this 
work has been fueled by the availability of large 
corpora annotated with syntactic structures, espe-
cially the Penn Treebank (Marcus et al, 1993).  In 
fact, years of extensive research on training and 
testing parsers on the Wall Street Journal (WSJ) 
corpus of the Penn Treebank have resulted in the 
availability of several high-accuracy parsers. 
We present a framework for combining the out-
put of several different accurate parsers to produce 
results that are superior to those of each of the in-
dividual parsers.  This is done in a two stage proc-
ess of reparsing.  In the first stage, m different 
parsers analyze an input sentence, each producing 
a syntactic structure.  In the second stage, a parsing 
algorithm is applied to the original sentence, taking 
into account the analyses produced by each parser 
in the first stage.  Our approach produces results 
with accuracy above those of the best individual 
parsers on both dependency and constituent pars-
ing of the standard WSJ test set. 
2 Dependency Reparsing 
In dependency reparsing we focus on unlabeled 
dependencies, as described by Eisner (1996).  In 
this scheme, the syntactic structure for a sentence 
with n words is a dependency tree representing 
head-dependent relations between pairs of words. 
When m parsers each output a set of 
dependencies (forming m dependency structures) 
for a given sentence containing n words,  the 
dependencies can be combined in a simple word-
by-word voting scheme, where each parser votes 
for the head of each of the n words in the sentence, 
and the head with most votes is assigned to each 
word.  This very simple scheme guarantees that the 
final set of dependencies will have as many votes 
as possible, but it does not guarantee that the final 
voted set of dependencies will be a well-formed 
dependency tree.  In fact, the resulting graph may 
not even be connected.  Zeman & ?abokrtsk? 
(2005) apply this dependency voting scheme to 
Czech with very strong results.  However, when 
the constraint that structures must be well-formed 
is enforced, the accuracy of their results drops 
sharply. 
Instead, if we reparse the sentence based on the 
output of the m parsers, we can maximize the 
number of votes for a well-formed dependency 
structure.  Once we have obtained the m initial 
dependency structures to be combined, the first 
step is to build a graph where each word in the 
sentence is a node.  We then create weighted 
directed edges between the nodes corresponding to 
words for which dependencies are obtained from 
each of the initial structures.1  In cases where more 
than one dependency structure indicates that an 
edge should be created, the corresponding weights 
are simply added.  As long as at least one of the m 
initial structures is a well-formed dependency 
structure, the directed graph created this way will 
be connected. 
                                                 
1
 Determining the weights is discussed in section 4.1. 
129
Once this graph is created, we reparse the 
sentence using a dependency parsing algorithm 
such as, for example, one of the algorithms 
described by McDonald et al (2005).  Finding the 
optimal dependency structure given the set of 
weighted dependencies is simply a matter of 
finding the maximum spanning tree (MST) for the 
directed weighted graph, which can be done using 
the Chu-Liu/Edmonds directed MST algorithm 
(Chu & Liu, 1965; Edmonds, 1967).  The 
maximum spanning tree maximizes the votes for 
dependencies given the constraint that the resulting 
structure must be a tree.  If projectivity (no 
crossing branches) is desired, Eisner?s (1996) 
dynamic programming algorithm  (similar to CYK) 
for dependency parsing can be used instead.   
3 Constituent Reparsing 
In constituent reparsing we deal with labeled con-
stituent trees, or phrase structure trees, such as 
those in the Penn Treebank (after removing traces, 
empty nodes and function tags).  The general idea 
is the same as with dependencies.  First, m parsers 
each produce one parse tree for an input sentence.  
We then use these m initial parse trees to guide the 
application of a parse algorithm to the input. 
Instead of building a graph out of words (nodes) 
and dependencies (edges), in constituent reparsing 
we use the m initial trees to build a weighted parse 
chart.  We start by decomposing each tree into its 
constituents, with each constituent being a 4-tuple 
[label, begin, end, weight], where label is the 
phrase structure type, such as NP or VP, begin is 
the index of the word where the constituent starts, 
end is the index of the word where the constituent 
ends plus one, and weight is the weight of the con-
stituent.  As with dependencies, in the simplest 
case the weight of each constituent is simply 1.0, 
but different weighting schemes can be used.  
Once the initial trees have been broken down into 
constituents, we put all the constituents from all of 
the m trees into a single list.  We then look for each 
pair of constituents A and B where the label, begin, 
and end are identical, and merge A and B into a 
single constituent with the same label, begin, and 
end, and with weight equal to the weight of A plus 
the weight of B.  Once no more constituent mergers 
are possible, the resulting constituents are placed 
on a standard parse chart, but where the constitu-
ents in the chart do not contain back-pointers indi-
cating what smaller constituents they contain.  
Building the final tree amounts to determining 
these back-pointers.  This can be done by running a 
bottom-up chart parsing algorithm (Allen, 1995) 
for a weighted grammar, but instead of using a 
grammar to determine what constituents can be 
built and what their weights are, we simply con-
strain the building of constituents to what is al-
ready in the chart (adding the weights of constitu-
ents when they are combined).  This way, we per-
form an exhaustive search for the tree that repre-
sents the heaviest combination of constituents that 
spans the entire sentence as a well-formed tree. 
A problem with simply considering all constitu-
ents and picking the heaviest tree is that this favors 
recall over precision.  Balancing precision and re-
call is accomplished by discarding every constitu-
ent with weight below a threshold t before the 
search for the final parse tree starts.  In the simple 
case where each constituent starts out with weight 
1.0 (before any merging), this means that a con-
stituent is only considered for inclusion in the final 
parse tree if it appears in at least t of the m initial 
parse trees.  Intuitively, this should increase preci-
sion, since we expect that a constituent that ap-
pears in the output of more parsers to be more 
likely to be correct.  By changing the threshold t 
we can control the precision/recall tradeoff.  
Henderson and Brill (1999) proposed two parser 
combination schemes, one that picks an entire tree 
from one of the parsers, and one that, like ours, 
builds a new tree from constituents from the initial 
trees.  The latter scheme performed better, produc-
ing remarkable results despite its simplicity.  The 
combination is done with a simple majority vote of 
whether or not constituents should appear in the 
combined tree.  In other words, if a constituent ap-
pears at least (m + 1)/2 times in the output of the m 
parsers, the constituent is added to the final tree.  
This simple vote resulted in trees with f-score sig-
nificantly higher than the one of the best parser in 
the combination.  However, the scheme heavily 
favors precision over recall.  Their results on WSJ 
section 23 were 92.1 precision and 89.2 recall 
(90.61 f-score), well above the most accurate 
parser in their experiments (88.6 f-score). 
4 Experiments 
In our dependency parsing experiments we used 
unlabeled dependencies extracted from the Penn 
130
Treebank using the same head-table as Yamada 
and Matsumoto (2003), using sections 02-21 as 
training data and section 23 as test data, following 
(McDonald et al, 2005; Nivre & Scholz, 2004; 
Yamada & Matsumoto, 2003).  Dependencies ex-
tracted from section 00 were used as held-out data, 
and section 22 was used as additional development 
data.  For constituent parsing, we used the section 
splits of the Penn Treebank as described above, as 
has become standard in statistical parsing research. 
4.1 Dependency Reparsing Experiments 
Six dependency parsers were used in our combina-
tion experiments, as described below. 
The deterministic shift-reduce parsing algorithm 
of (Nivre & Scholz, 2004) was used to create two 
parsers2, one that processes the input sentence from 
left-to-right (LR), and one that goes from right-to-
left (RL).  Because this deterministic algorithm 
makes a single pass over the input string with no 
back-tracking, making decisions based on the pars-
er?s state and history, the order in which input to-
kens are considered affects the result.  Therefore, 
we achieve additional parser diversity with the 
same algorithm, simply by varying the direction of 
parsing.  We refer to the two parsers as LR and RL. 
The deterministic parser of Yamada and Ma-
tsumoto (2003) uses an algorithm similar to Nivre 
and Scholz?s, but it makes several successive left-
to-right passes over the input instead of keeping a 
stack.  To increase parser diversity, we used a ver-
sion of Yamada and Matsumoto?s algorithm where 
the direction of each of the consecutive passes over 
the input string alternates from left-to-right and 
right-to-left.  We refer to this parser as LRRL. 
The large-margin parser described in 
(McDonald et al, 2005) was used with no altera-
tions.  Unlike the deterministic parsers above, this 
parser uses a dynamic programming algorithm 
(Eisner, 1996) to determine the best tree, so there 
is no difference between presenting the input from 
left-to-right or right-to-left. 
Three different weight configurations were con-
sidered: (1) giving all dependencies the same 
weight; (2) giving dependencies different weights, 
depending only on which parser generated the de-
pendency; and (3) giving dependencies different 
                                                 
2
 Nivre and Scholz use memory based learning in their 
experiments.  Our implementation of their parser uses 
support vector machines, with improved results. 
weights, depending on which parser generated the 
dependency, and the part-of-speech of the depend-
ent word.  Option 2 takes into consideration that 
parsers may have different levels of accuracy, and 
dependencies proposed by more accurate parsers 
should be counted more heavily.  Option 3 goes a 
step further, attempting to capitalize on the specific 
strengths of the different parsers. 
The weights in option 2 are determined by com-
puting the accuracy of each parser on the held-out 
set (WSJ section 00).  The weights are simply the 
corresponding parser?s accuracy (number of cor-
rect dependencies divided by the total number of 
dependencies).  The weights in option 3 are deter-
mined in a similar manner, but different accuracy 
figures are computed for each part-of-speech. 
Table 1 shows the dependency accuracy and 
root accuracy (number of times the root of the de-
pendency tree was identified correctly divided by 
the number of sentences) for each of the parsers, 
and for each of the different weight settings in the 
reparsing experiments (numbered according to 
their descriptions above). 
 
System Accuracy Root Acc. 
LR 91.0 92.6 
RL 90.1 86.3 
LRRL 89.6 89.1 
McDonald 90.9 94.2 
Reparse dep 1 91.8 96.0 
Reparse dep 2 92.1 95.9 
Reparse dep 3 92.7 96.6 
Table 1: Dependency accuracy and root accuracy of 
individual dependency parsers and their combination 
under three different weighted reparsing settings. 
4.2 Constituent Reparsing Experiments 
The parsers that were used in the constituent 
reparsing experiments are: (1) Charniak and John-
son?s (2005) reranking parser; (2) Henderson?s 
(2004) synchronous neural network parser; (3) 
Bikel?s (2002) implementation of the Collins 
(1999) model 2 parser; and (4) two versions of Sa-
gae and Lavie?s (2005) shift-reduce parser, one 
using a maximum entropy classifier, and one using 
support vector machines. 
Henderson and Brill?s voting scheme mentioned 
in section 3 can be emulated by our reparsing ap-
proach by setting all weights to 1.0 and t to (m + 
1)/2, but better results can be obtained by setting 
appropriate weights and adjusting the preci-
sion/recall tradeoff.  Weights for different types of 
131
constituents from each parser can be set in a simi-
lar way to configuration 3 in the dependency ex-
periments.  However, instead of measuring accu-
racy for each part-of-speech tag of dependents, we 
measure precision for each non-terminal label.   
The parameter t is set using held-out data (from 
WSJ section 22) and a simple hill-climbing proce-
dure.  First we set t to (m + 1)/2 (which heavily 
favors precision).  We then repeatedly evaluate the 
combination of parsers, each time decreasing the 
value of t (by 0.01, say).  We record the values of t 
for which precision and recall were closest, and for 
which f-score was highest. 
Table 2 shows the accuracy of each individual 
parser and for three reparsing settings.  Setting 1 is 
the emulation of Henderson and Brill?s voting.  In 
setting 2, t is set for balancing precision and recall.  
In setting 3, t is set for highest f-score.  
 
System Precision Recall F-score 
Charniak/Johnson 91.3 90.6 91.0 
Henderson 90.2 89.1 89.6 
Bikel (Collins) 88.3 88.1 88.2 
Sagae/Lavie (a) 86.9 86.6 86.7 
Sagae/Lavie (b) 88.0 87.8 87.9 
Reparse 1 95.1 88.5 91.6 
Reparse 2 91.8 91.9 91.8 
Reparse 3 93.2 91.0 92.1 
Table 2: Precision, recall and f-score of each constituent 
parser and their combination under three different 
reparsing settings. 
5 Discussion 
We have presented a reparsing scheme that pro-
duces results with accuracy higher than the best 
individual parsers available by combining their 
results.  We have shown that in the case of de-
pendencies, the reparsing approach successfully 
addresses the issue of constructing high-accuracy 
well-formed structures from the output of several 
parsers.  In constituent reparsing, held-out data can 
be used for setting a parameter that allows for bal-
ancing precision and recall, or increasing f-score.  
By combining several parsers with f-scores ranging 
from 91.0% to 86.7%, we obtain reparsed results 
with a 92.1% f-score. 
References 
Allen, J. (1995). Natural Language Understanding (2nd 
ed.). Redwood City, CA: The Benjamin/Cummings 
Publishing Company, Inc. 
Bikel, D. (2002). Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings 
of HLT2002. San Diego, CA. 
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In 
Proceedings of the 43rd meeting of the Association 
for Computational Linguistics. Ann Arbor, MI. 
Chu, Y. J., & Liu, T. H. (1965). On the shortest arbores-
cence of a directed graph. Science Sinica(14), 1396-
1400. 
Edmonds, J. (1967). Optimum branchings. Journal of 
Research of the National Bureau of Standards(71B), 
233-240. 
Eisner, J. (1996). Three new probabilistic models for 
dependency parsing: An exploration. In Proceedings 
of the International Conference on Computational 
Linguistics (COLING'96). Copenhagen, Denmark. 
Henderson, J. (2004). Discriminative training of a neu-
ral network statistical parser. In Proceedings of the 
42nd Meeting of the Association for Computational 
Linguistics. Barcelona, Spain. 
Henderson, J., & Brill, E. (1999). Exploiting diversity in 
natural language processing: combining parsers. In 
Proceedings of the Fourth Conference on Empirical 
Methods in Natural Language Processing (EMNLP). 
Marcus, M. P., Santorini, B., & Marcinkiewics, M. A. 
(1993). Building a large annotated corpus of English: 
The Penn Treebank. Computational Linguistics, 19. 
McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. 
(2005). Non-Projective Dependency Parsing using 
Spanning Tree Algorithms. In Proceedings of the 
Conference on Human Language Technolo-
gies/Empirical Methods in Natural Language Proc-
essing (HLT-EMNLP). Vancouver, Canada. 
Nivre, J., & Scholz, M. (2004). Deterministic depend-
ency parsing of English text. In Proceedings of the 
20th International Conference on Computational Lin-
guistics (pp. 64-70). Geneva, Switzerland. 
Sagae, K., & Lavie, A. (2005). A classifier-based parser 
with linear run-time complexity. In Proceedings of 
the Ninth International Workshop on Parsing Tech-
nologies. Vancouver, Canada. 
Yamada, H., & Matsumoto, Y. (2003). Statistical de-
pendency analysis using support vector machines. In 
Proceedings of the Eighth International Workshop on 
Parsing Technologies. Nancy, France. 
Zeman, D., & ?abokrtsk?, Z. (2005). Improving Parsing 
Accuracy by Combining Diverse Dependency Pars-
ers. In Proceedings of the International Workshop on 
Parsing Technologies. Vancouver, Canada. 
132
Proceedings of the 43rd Annual Meeting of the ACL, pages 197?204,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic Measurement of Syntactic Development in Child Language
Kenji Sagae and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15232
{sagae,alavie}@cs.cmu.edu
Brian MacWhinney
Department of Psychology
Carnegie Mellon University
Pittsburgh, PA 15232
macw@cmu.edu
Abstract
To facilitate the use of syntactic infor-
mation in the study of child language
acquisition, a coding scheme for Gram-
matical Relations (GRs) in transcripts of
parent-child dialogs has been proposed by
Sagae, MacWhinney and Lavie (2004).
We discuss the use of current NLP tech-
niques to produce the GRs in this an-
notation scheme. By using a statisti-
cal parser (Charniak, 2000) and memory-
based learning tools for classification
(Daelemans et al, 2004), we obtain high
precision and recall of several GRs. We
demonstrate the usefulness of this ap-
proach by performing automatic measure-
ments of syntactic development with the
Index of Productive Syntax (Scarborough,
1990) at similar levels to what child lan-
guage researchers compute manually.
1 Introduction
Automatic syntactic analysis of natural language has
benefited greatly from statistical and corpus-based
approaches in the past decade. The availability of
syntactically annotated data has fueled the develop-
ment of high quality statistical parsers, which have
had a large impact in several areas of human lan-
guage technologies. Similarly, in the study of child
language, the availability of large amounts of elec-
tronically accessible empirical data in the form of
child language transcripts has been shifting much of
the research effort towards a corpus-based mental-
ity. However, child language researchers have only
recently begun to utilize modern NLP techniques
for syntactic analysis. Although it is now common
for researchers to rely on automatic morphosyntactic
analyses of transcripts to obtain part-of-speech and
morphological analyses, their use of syntactic pars-
ing is rare.
Sagae, MacWhinney and Lavie (2004) have
proposed a syntactic annotation scheme for the
CHILDES database (MacWhinney, 2000), which
contains hundreds of megabytes of transcript data
and has been used in over 1,500 studies in child lan-
guage acquisition and developmental language dis-
orders. This annotation scheme focuses on syntactic
structures of particular importance in the study of
child language. In this paper, we describe the use
of existing NLP tools to parse child language tran-
scripts and produce automatically annotated data in
the format of the scheme of Sagae et al We also
validate the usefulness of the annotation scheme and
our analysis system by applying them towards the
practical task of measuring syntactic development in
children according to the Index of Productive Syn-
tax, or IPSyn (Scarborough, 1990), which requires
syntactic analysis of text and has traditionally been
computed manually. Results obtained with current
NLP technology are close to what is expected of hu-
man performance in IPSyn computations, but there
is still room for improvement.
2 The Index of Productive Syntax (IPSyn)
The Index of Productive Syntax (Scarborough,
1990) is a measure of development of child lan-
guage that provides a numerical score for grammat-
ical complexity. IPSyn was designed for investigat-
ing individual differences in child language acqui-
197
sition, and has been used in numerous studies. It
addresses weaknesses in the widely popular Mean
Length of Utterance measure, or MLU, with respect
to the assessment of development of syntax in chil-
dren. Because it addresses syntactic structures di-
rectly, it has gained popularity in the study of gram-
matical aspects of child language learning in both
research and clinical settings.
After about age 3 (Klee and Fitzgerald, 1985),
MLU starts to reach ceiling and fails to properly dis-
tinguish between children at different levels of syn-
tactic ability. For these purposes, and because of its
higher content validity, IPSyn scores often tells us
more than MLU scores. However, the MLU holds
the advantage of being far easier to compute. Rel-
atively accurate automated methods for computing
the MLU for child language transcripts have been
available for several years (MacWhinney, 2000).
Calculation of IPSyn scores requires a corpus of
100 transcribed child utterances, and the identifica-
tion of 56 specific language structures in each ut-
terance. These structures are counted and used to
compute numeric scores for the corpus in four cat-
egories (noun phrases, verb phrases, questions and
negations, and sentence structures), according to a
fixed score sheet. Each structure in the four cate-
gories receives a score of zero (if the structure was
not found in the corpus), one (if it was found once
in the corpus), or two (if it was found two or more
times). The scores in each category are added, and
the four category scores are added into a final IPSyn
score, ranging from zero to 112.1
Some of the language structures required in the
computation of IPSyn scores (such as the presence
of auxiliaries or modals) can be recognized with the
use of existing child language analysis tools, such
as the morphological analyzer MOR (MacWhinney,
2000) and the part-of-speech tagger POST (Parisse
and Le Normand, 2000). However, more complex
structures in IPSyn require syntactic analysis that
goes beyond what POS taggers can provide. Exam-
ples of such structures include the presence of an
inverted copula or auxiliary in a wh-question, con-
joined clauses, bitransitive predicates, and fronted
or center-embedded subordinate clauses.
1See (Scarborough, 1990) for a complete listing of targeted
structures and the IPSyn score sheet used for calculation of
scores.
Sentence (input):
We eat the cheese sandwich
Grammatical Relations (output):
[Leftwall]     We     eat     the     cheese     sandwich
SUBJ
ROOT OBJ
DET
MOD
Figure 1: Input sentence and output produced by our
system.
3 Automatic Syntactic Analysis of Child
Language Transcripts
A necessary step in the automatic computation of
IPSyn scores is to produce an automatic syntac-
tic analysis of the transcripts being scored. We
have developed a system that parses transcribed
child utterances and identifies grammatical relations
(GRs) according to the CHILDES syntactic annota-
tion scheme (Sagae et al, 2004). This annotation
scheme was designed specifically for child-parent
dialogs, and we have found it suitable for the iden-
tification of the syntactic structures necessary in the
computation of IPSyn.
Our syntactic analysis system takes a sentence
and produces a labeled dependency structure repre-
senting its grammatical relations. An example of the
input and output associated with our system can be
seen in figure 1. The specific GRs identified by the
system are listed in figure 2.
The three main steps in our GR analysis are: text
preprocessing, unlabeled dependency identification,
and dependency labeling. In the following subsec-
tions, we examine each of them in more detail.
3.1 Text Preprocessing
The CHAT transcription system2 is the format
followed by all transcript data in the CHILDES
database, and it is the input format we use for syn-
tactic analysis. CHAT specifies ways of transcrib-
ing extra-grammatical material such as disfluency,
retracing, and repetition, common in spontaneous
spoken language. Transcripts of child language may
contain a large amount of extra-grammatical mate-
2http://childes.psy.cmu.edu/manuals/CHAT.pdf
198
SUBJ, ESUBJ, CSUBJ, XSUBJ
COMP, XCOMP
JCT, CJCT, XJCT
OBJ, OBJ2, IOBJ
PRED, CPRED, XPRED
MOD, CMOD, XMOD
AUX NEG DET QUANT POBJ PTL
CPZR COM INF VOC COORD ROOT
Subject, expletive subject, clausal subject (finite and non?finite) Object, second object, indirect object
Clausal complement (finite and non?finite) Predicative, clausal predicative (finite and non?finite)
Adjunct, clausal adjunct (finite and non?finite) Nominal modifier, clausal nominal modifier (finite and non?finite)
Auxiliary Negation Determiner Quantifier Prepositional object Verb particle
CommunicatorComplementizer Infinitival "to" Vocative Coordinated item Top node
Figure 2: Grammatical relations in the CHILDES syntactic annotation scheme.
rial that falls outside of the scope of the syntactic an-
notation system and our GR identifier, since it is al-
ready clearly marked in CHAT transcripts. By using
the CLAN tools (MacWhinney, 2000), designed to
process transcripts in CHAT format, we remove dis-
fluencies, retracings and repetitions from each sen-
tence. Furthermore, we run each sentence through
the MOR morphological analyzer (MacWhinney,
2000) and the POST part-of-speech tagger (Parisse
and Le Normand, 2000). This results in fairly clean
sentences, accompanied by full morphological and
part-of-speech analyses.
3.2 Unlabeled Dependency Identification
Once we have isolated the text that should be ana-
lyzed in each sentence, we parse it to obtain unla-
beled dependencies. Although we ultimately need
labeled dependencies, our choice to produce unla-
beled structures first (and label them in a later step)
is motivated by available resources. Unlabeled de-
pendencies can be readily obtained by processing
constituent trees, such as those in the Penn Tree-
bank (Marcus et al, 1993), with a set of rules to
determine the lexical heads of constituents. This
lexicalization procedure is commonly used in sta-
tistical parsing (Collins, 1996) and produces a de-
pendency tree. This dependency extraction proce-
dure from constituent trees gives us a straightfor-
ward way to obtain unlabeled dependencies: use an
existing statistical parser (Charniak, 2000) trained
on the Penn Treebank to produce constituent trees,
and extract unlabeled dependencies using the afore-
mentioned head-finding rules.
Our target data (transcribed child language) is
from a very different domain than the one of the data
used to train the statistical parser (the Wall Street
Journal section of the Penn Treebank), but the degra-
dation in the parser?s accuracy is acceptable. An
evaluation using 2,018 words of in-domain manu-
ally annotated dependencies shows that the depen-
dency accuracy of the parser is 90.1% on child lan-
guage transcripts (compared to over 92% on section
23 of the Wall Street Journal portion of the Penn
Treebank). Despite the many differences with re-
spect to the domain of the training data, our domain
features sentences that are much shorter (and there-
fore easier to parse) than those found in Wall Street
Journal articles. The average sentence length varies
from transcript to transcript, because of factors such
as the age and verbal ability of the child, but it is
usually less than 15 words.
3.3 Dependency Labeling
After obtaining unlabeled dependencies as described
above, we proceed to label those dependencies with
the GR labels listed in Figure 2.
Determining the labels of dependencies is in gen-
eral an easier task than finding unlabeled dependen-
cies in text.3 Using a classifier, we can choose one
of the 30 possible GR labels for each dependency,
given a set of features derived from the dependen-
cies. Although we need manually labeled data to
train the classifier for labeling dependencies, the size
of this training set is far smaller than what would be
necessary to train a parser to find labeled dependen-
3Klein and Manning (2002) offer an informal argument that
constituent labels are much more easily separable in multidi-
mensional space than constituents/distituents. The same argu-
ment applies to dependencies and their labels.
199
cies in one pass.
We use a corpus of about 5,000 words with man-
ually labeled dependencies to train TiMBL (Daele-
mans et al, 2003), a memory-based learner (set to
use the k-nn algorithm with k=1, and gain ratio
weighing), to classify each dependency with a GR
label. We extract the following features for each de-
pendency:
? The head and dependent words;
? The head and dependent parts-of-speech;
? Whether the dependent comes before or after
the head in the sentence;
? How many words apart the dependent is from
the head;
? The label of the lowest node in the constituent
tree that includes both the head and dependent.
The accuracy of the classifier in labeling depen-
dencies is 91.4% on the same 2,018 words used to
evaluate unlabeled accuracy. There is no intersec-
tion between the 5,000 words used for training and
the 2,018-word test set. Features were tuned on a
separate development set of 582 words.
When we combine the unlabeled dependencies
obtained with the Charniak parser (and head-finding
rules) and the labels obtained with the classifier,
overall labeled dependency accuracy is 86.9%, sig-
nificantly above the results reported (80%) by Sagae
et al (2004) on very similar data.
Certain frequent and easily identifiable GRs, such
as DET, POBJ, INF, and NEG were identified with
precision and recall above 98%. Among the most
difficult GRs to identify were clausal complements
COMP and XCOMP, which together amount to less
than 4% of the GRs seen the training and test sets.
Table 1 shows the precision and recall of GRs of par-
ticular interest.
Although not directly comparable, our results
are in agreement with state-of-the-art results for
other labeled dependency and GR parsers. Nivre
(2004) reports a labeled (GR) dependency accuracy
of 84.4% on modified Penn Treebank data. Briscoe
and Carroll (2002) achieve a 76.5% F-score on a
very rich set of GRs in the more heterogeneous and
challenging Susanne corpus. Lin (1998) evaluates
his MINIPAR system at 83% F-score on identifica-
tion of GRs, also in data from the Susanne corpus
(but using simpler GR set than Briscoe and Carroll).
GR Precision Recall F-score
SUBJ 0.94 0.93 0.93
OBJ 0.83 0.91 0.87
COORD 0.68 0.85 0.75
JCT 0.91 0.82 0.86
MOD 0.79 0.92 0.85
PRED 0.80 0.83 0.81
ROOT 0.91 0.92 0.91
COMP 0.60 0.50 0.54
XCOMP 0.58 0.64 0.61
Table 1: Precision, recall and F-score (harmonic
mean) of selected Grammatical Relations.
4 Automating IPSyn
Calculating IPSyn scores manually is a laborious
process that involves identifying 56 syntactic struc-
tures (or their absence) in a transcript of 100 child
utterances. Currently, researchers work with a par-
tially automated process by using transcripts in elec-
tronic format and spreadsheets. However, the ac-
tual identification of syntactic structures, which ac-
counts for most of the time spent on calculating IP-
Syn scores, still has to be done manually.
By using part-of-speech and morphological anal-
ysis tools, it is possible to narrow down the num-
ber of sentences where certain structures may be
found. The search for such sentences involves pat-
terns of words and parts-of-speech (POS). Some
structures, such as the presence of determiner-noun
or determiner-adjective-noun sequences, can be eas-
ily identified through the use of simple patterns.
Other structures, such as front or center-embedded
clauses, pose a greater challenge. Not only are pat-
terns for such structures difficult to craft, they are
also usually inaccurate. Patterns that are too gen-
eral result in too many sentences to be manually ex-
amined, but more restrictive patterns may miss sen-
tences where the structures are present, making their
identification highly unlikely. Without more syntac-
tic analysis, automatic searching for structures in IP-
Syn is limited, and computation of IPSyn scores still
requires a great deal of manual inspection.
Long, Fey and Channell (2004) have developed
a software package, Computerized Profiling (CP),
for child language study, which includes a (mostly)
200
automated computation of IPSyn.4 CP is an exten-
sively developed example of what can be achieved
using only POS and morphological analysis. It does
well on identifying items in IPSyn categories that
do not require deeper syntactic analysis. However,
the accuracy of overall scores is not high enough to
be considered reliable in practical usage, in particu-
lar for older children, whose utterances are longer
and more sophisticated syntactically. In practice,
researchers usually employ CP as a first pass, and
manually correct the automatic output. Section 5
presents an evaluation of the CP version of IPSyn.
Syntactic analysis of transcripts as described in
section 3 allows us to go a step further, fully au-
tomating IPSyn computations and obtaining a level
of reliability comparable to that of human scoring.
The ability to search for both grammatical relations
and parts-of-speech makes searching both easier and
more reliable. As an example, consider the follow-
ing sentences (keeping in mind that there are no ex-
plicit commas in spoken language):
(a) Then [,] he said he ate.
(b) Before [,] he said he ate.
(c) Before he ate [,] he ran.
Sentences (a) and (b) are similar, but (c) is dif-
ferent. If we were looking for a fronted subordinate
clause, only (c) would be a match. However, each
one of the sentences has an identical part-speech-
sequence. If this were an isolated situation, we
might attempt to fix it by having tags that explic-
itly mark verbs that take clausal complements, or by
adding lexical constraints to a search over part-of-
speech patterns. However, even by modifying this
simple example slightly, we find more problems:
(d) Before [,] he told the man he was cold.
(e) Before he told the story [,] he was cold.
Once again, sentences (d) and (e) have identical
part-of-speech sequences, but only sentence (e) fea-
tures a fronted subordinate clause. These limited toy
examples only scratch the surface of the difficulties
in identifying syntactic structures without syntactic
4Although CP requires that a few decisions be made man-
ually, such as the disambiguation of the lexical item ??s? as
copula vs. genitive case marker, and the definition of sentence
breaks for long utterances, the computation of IPSyn scores is
automated to a large extent.
analysis beyond part-of-speech and morphological
tagging. In these sentences, searching with GRs
is easy: we simply find a GR of clausal type (e.g.
CJCT, COMP, CMOD, etc) where the dependent is
to the left of its head.
For illustration purposes of how searching for
structures in IPSyn is done with GRs, let us look
at how to find other IPSyn structures5:
? Wh-embedded clauses: search for wh-words
whose head, or transitive head (its head?s head,
or head?s head?s head...) is a dependent in
GR of types [XC]SUBJ, [XC]PRED, [XC]JCT,
[XC]MOD, COMP or XCOMP;
? Relative clauses: search for a CMOD where the
dependent is to the right of the head;
? Bitransitive predicate: search for a word that is
a head of both OBJ and OBJ2 relations.
Although there is still room for under- and over-
generalization with search patterns involving GRs,
finding appropriate ways to search is often made
trivial, or at least much more simple and reliable
than searching without GRs. An evaluation of our
automated version of IPSyn, which searches for IP-
Syn structures using POS, morphology and GR in-
formation, and a comparison to the CP implemen-
tation, which uses only POS and morphology infor-
mation, is presented in section 5.
5 Evaluation
We evaluate our implementation of IPSyn in two
ways. The first is Point Difference, which is cal-
culated by taking the (unsigned) difference between
scores obtained manually and automatically. The
point difference is of great practical value, since
it shows exactly how close automatically produced
scores are to manually produced scores. The second
is Point-to-Point Accuracy, which reflects the overall
reliability over each individual scoring decision in
the computation of IPSyn scores. It is calculated by
counting how many decisions (identification of pres-
ence/absence of language structures in the transcript
being scored) were made correctly, and dividing that
5More detailed descriptions and examples of each structure
are found in (Scarborough, 1990), and are omitted here for
space considerations, since the short descriptions are fairly self-
explanatory.
201
number by the total number of decisions. The point-
to-point measure is commonly used for assessing the
inter-rater reliability of metrics such as the IPSyn. In
our case, it allows us to establish the reliability of au-
tomatically computed scores against human scoring.
5.1 Test Data
We obtained two sets of transcripts with correspond-
ing IPSyn scoring (total scores, and each individual
decision) from two different child language research
groups. The first set (A) contains 20 transcripts of
children of ages ranging between two and three. The
second set (B) contains 25 transcripts of children of
ages ranging between eight and nine.
Each transcript in set A was scored fully manu-
ally. Researchers looked for each language structure
in the IPSyn scoring guide, and recorded its pres-
ence in a spreadsheet. In set B, scoring was done
in a two-stage process. In the first stage, each tran-
script was scored automatically by CP. In the second
stage, researchers checked each automatic decision
made by CP, and corrected any errors manually.
Two transcripts in each set were held out for de-
velopment and debugging. The final test sets con-
tained: (A) 18 transcripts with a total of 11,704
words and a mean length of utterance of 2.9, and
(B) 23 transcripts with a total of 40,819 words and a
mean length of utterance of 7.0.
5.2 Results
Scores computed automatically from transcripts
parsed as described in section 3 were very close
to the scores computed manually. Table 2 shows a
summary of the results, according to our two eval-
uation metrics. Our system is labeled as GR, and
manually computed scores are labeled as HUMAN.
For comparison purposes, we also show the results
of running Long et al?s automated version of IPSyn,
labeled as CP, on the same transcripts.
Point Difference
The average (absolute) point difference between au-
tomatically computed scores (GR) and manually
computed scores (HUMAN) was 3.3 (the range of
HUMAN scores on the data was 21-91). There was
no clear trend on whether the difference was posi-
tive or negative. In some cases, the automated scores
were higher, in other cases lower. The minimum dif-
System Avg. Pt. Difference Point-to-Point
to HUMAN Reliability
GR (Total) 3.3 92.8%
CP (Total) 8.3 85.4%
GR (Set A) 3.7 92.5%
CP (Set A) 6.2 86.2%
GR (Set B) 2.9 93.0%
CP (Set B) 10.2 84.8%
Table 2: Summary of evaluation results. GR is our
implementation of IPSyn based on grammatical re-
lations, CP is Long et al?s (2004) implementation of
IPSyn, and HUMAN is manual scoring.
Histogram of Point Differences (3 point bins)
0
10
20
30
40
50
60
3 6 9 12 15 18 21Point Difference
Freque
ncy (%) GRCP
Figure 3: Histogram of point differences between
HUMAN scores and GR (black), and CP (white).
ference was zero, and the maximum difference was
12. Only two scores differed by 10 or more, and 17
scores differed by two or less. The average point dif-
ference between HUMAN and the scores obtained
with Long et al?s CP was 8.3. The minimum was
zero and the maximum was 21. Sixteen scores dif-
fered by 10 or more, and six scores differed by 2 or
less. Figure 3 shows the point differences between
GR and HUMAN, and CP and HUMAN.
It is interesting to note that the average point dif-
ferences between GR and HUMAN were similar on
sets A and B (3.7 and 2.9, respectively). Despite the
difference in age ranges, the two averages were less
than one point apart. On the other hand, the average
difference between CP and HUMAN was 6.2 on set
A, and 10.2 on set B. The larger difference reflects
CP?s difficulty in scoring transcripts of older chil-
dren, whose sentences are more syntactically com-
plex, using only POS analysis.
202
Point-to-Point Accuracy
In the original IPSyn reliability study (Scarborough,
1990), point-to-point measurements using 75 tran-
scripts showed the mean inter-rater agreement for
IPSyn among human scorers at 94%, with a min-
imum agreement of 90% of all decisions within a
transcript. The lowest agreement between HUMAN
and GR scoring for decisions within a transcript was
88.5%, with a mean of 92.8% over the 41 transcripts
used in our evaluation. Although comparisons of
agreement figures obtained with different sets of
transcripts are somewhat coarse-grained, given the
variations within children, human scorers and tran-
script quality, our results are very satisfactory. For
direct comparison purposes using the same data, the
mean point-to-point accuracy of CP was 85.4% (a
relative increase of about 100% in error).
In their separate evaluation of CP, using 30 sam-
ples of typically developing children, Long and
Channell (2001) found a 90.7% point-to-point ac-
curacy between fully automatic and manually cor-
rected IPSyn scores.6 However, Long and Channell
compared only CP output with manually corrected
CP output, while our set A was manually scored
from scratch. Furthermore, our set B contained
only transcripts from significantly older children (as
in our evaluation, Long and Channell observed de-
creased accuracy of CP?s IPSyn with more com-
plex language usage). These differences, and the
expected variation from using different transcripts
from different sources, account for the difference in
our results and Long and Channell?s.
5.3 Error Analysis
Although the overall accuracy of our automatically
computed scores is in large part comparable to man-
ual IPSyn scoring (and significantly better than the
only option currently available for automatic scor-
ing), our system suffers from visible deficiencies in
the identification of certain structures within IPSyn.
Four of the 56 structures in IPSyn account for al-
most half of the number of errors made by our sys-
tem. Table 3 lists these IPSyn items, with their re-
spective percentages of the total number of errors.
6Long and Channell?s evaluation also included samples
from children with language disorders. Their 30 samples of
typically developing children (with a mean age of 5) are more
directly comparable to the data used in our evaluation.
IPSyn item Error
S11 (propositional complement) 16.9%
V15 (copula, modal or aux for 12.3%
emphasis or ellipsis)
S16 (relative clause) 10.6%
S14 (bitransitive predicate) 5.8%
Table 3: IPSyn structures where errors occur most
frequently, and their percentages of the total number
of errors over 41 transcripts.
Errors in items S11 (propositional complements),
S16 (relative clauses), and S14 (bitransitive predi-
cates) are caused by erroneous syntactic analyses.
For an example of how GR assignments affect IP-
Syn scoring, let us consider item S11. Searching for
the relation COMP is a crucial part in finding propo-
sitional complements. However, COMP is one of
the GRs that can be identified the least reliably in
our set (precision of 0.6 and recall of 0.5, see table
1). As described in section 2, IPSyn requires that
we credit zero points to item S11 for no occurrences
of propositional complements, one point for a single
occurrence, and two points for two or more occur-
rences. If there are several COMPs in the transcript,
we should find about half of them (plus others, in
error), and correctly arrive at a credit of two points.
However, if there are very few or none, our count is
likely to be incorrect.
Most errors in item V15 (emphasis or ellipsis)
were caused not by incorrect GR assignments, but
by imperfect search patterns. The searching failed to
account for a number of configurations of GRs, POS
tags and words that indicate that emphasis or ellip-
sis exists. This reveals another general source of er-
ror in our IPSyn implementation: the search patterns
that use GR analyzed text to make the actual IP-
Syn scoring decisions. Although our patterns are far
more reliable than what we could expect from POS
tags and words alone, these are still hand-crafted
rules that need to be debugged and perfected over
time. This was the first evaluation of our system,
and only a handful of transcripts were used during
development. We expect that once child language
researchers have had the opportunity to use the sys-
tem in practical settings, their feedback will allow us
to refine the search patterns at a more rapid pace.
203
6 Conclusion and Future Work
We have presented an automatic way to annotate
transcripts of child language with the CHILDES
syntactic annotation scheme. By using existing re-
sources and a small amount of annotated data, we
achieved state-of-the-art accuracy levels.
GR identification was then used to automate the
computation of IPSyn scores to measure grammati-
cal development in children. The reliability of our
automatic IPSyn was very close to the inter-rater re-
liability among human scorers, and far higher than
that of the only other computational implementation
of IPSyn. This demonstrates the value of automatic
GR assignment to child language research.
From the analysis in section 5.3, it is clear that the
identification of certain GRs needs to be made more
accurately. We intend to annotate more in-domain
training data for GR labeling, and we are currently
investigating the use of other applicable GR parsing
techniques.
Finally, IPSyn score calculation could be made
more accurate with the knowledge of the expected
levels of precision and recall of automatic assign-
ment of specific GRs. It is our intuition that in a
number of cases it would be preferable to trade re-
call for precision. We are currently working on a
framework for soft-labeling of GRs, which will al-
low us to manipulate the precision/recall trade-off
as discussed in (Carroll and Briscoe, 2002).
Acknowledgments
This work was supported in part by the National Sci-
ence Foundation under grant IIS-0414630.
References
Edward J. Briscoe and John A. Carroll. 2002. Robust ac-
curate statistical annotation of general text. Proceed-
ings of the 3rd International Conference on Language
Resources and Evaluation, (pp. 1499?1504). Las Pal-
mas, Gran Canaria.
John A. Carroll and Edward J. Briscoe. 2002. High pre-
cision extraction of grammatical relations. Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics, (pp. 134-140). Taipei, Taiwan.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics. Seattle, WA.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. Proceedings of the 34th
Meeting of the Association for Computational Linguis-
tics (pp. 184-191). Santa Cruz, CA.
Walter Daelemans, Jacub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. TiMBL: Tilburg Memory
Based Learner, version 5.1, Reference Guide. ILK Re-
search Group Technical Report Series no. 04-02, 2004.
T. Klee and M. D. Fitzgerald. 1985. The relation be-
tween grammatical development and mean length of
utterance in morphemes. Journal of Child Language,
12, 251-269.
Dan Klein and Christopher D. Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics (pp.
128-135).
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems. Granada, Spain.
Steve H. Long and Ron W. Channell. 2001. Accuracy of
four language analysis procedures performed automat-
ically. American Journal of Speech-Language Pathol-
ogy, 10(2).
Steven H. Long, Marc E. Fey, and Ron W. Channell.
2004. Computerized Profiling (Version 9.6.0). Cleve-
land, OH: Case Western Reserve University.
Brian MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Mahwah, NJ: Lawrence Erlbaum
Associates.
Mitchel P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewics. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19.
Joakim Nivre and Mario Scholz. 2004. Deterministic de-
pendency parsing of English text. Proceedings of In-
ternational Conference on Computational Linguistics
(pp. 64-70). Geneva, Switzerland.
Christophe Parisse and Marie-Thrse Le Normand. 2000.
Automatic disambiguation of the morphosyntax in
spoken language corpora. Behavior Research Meth-
ods, Instruments, and Computers, 32, 468-481.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2004.
Adding Syntactic annotations to transcripts of parent-
child dialogs. Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC 2004). Lisbon, Portugal.
Hollis S. Scarborough. 1990. Index of Productive Syn-
tax. In Applied Psycholinguistics, 11, 1-22.
204
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 425?432,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Fast, Accurate Deterministic Parser for Chinese
Mengqiu Wang Kenji Sagae Teruko Mitamura
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{mengqiu,sagae,teruko}@cs.cmu.edu
Abstract
We present a novel classifier-based deter-
ministic parser for Chinese constituency
parsing. Our parser computes parse trees
from bottom up in one pass, and uses
classifiers to make shift-reduce decisions.
Trained and evaluated on the standard
training and test sets, our best model (us-
ing stacked classifiers) runs in linear time
and has labeled precision and recall above
88% using gold-standard part-of-speech
tags, surpassing the best published re-
sults. Our SVM parser is 2-13 times faster
than state-of-the-art parsers, while produc-
ing more accurate results. Our Maxent
and DTree parsers run at speeds 40-270
times faster than state-of-the-art parsers,
but with 5-6% losses in accuracy.
1 Introduction and Background
Syntactic parsing is one of the most fundamental
tasks in Natural Language Processing (NLP). In
recent years, Chinese syntactic parsing has also
received a lot of attention in the NLP commu-
nity, especially since the release of large collec-
tions of annotated data such as the Penn Chi-
nese Treebank (Xue et al, 2005). Corpus-based
parsing techniques that are successful for English
have been applied extensively to Chinese. Tradi-
tional statistical approaches build models which
assign probabilities to every possible parse tree
for a sentence. Techniques such as dynamic pro-
gramming, beam-search, and best-first-search are
then employed to find the parse tree with the high-
est probability. The massively ambiguous nature
of wide-coverage statistical parsing,coupled with
cubic-time (or worse) algorithms makes this ap-
proach too slow for many practical applications.
Deterministic parsing has emerged as an attrac-
tive alternative to probabilistic parsing, offering
accuracy just below the state-of-the-art in syn-
tactic analysis of English, but running in linear
time (Sagae and Lavie, 2005; Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004). Encour-
aging results have also been shown recently by
Cheng et al (2004; 2005) in applying determin-
istic models to Chinese dependency parsing.
We present a novel classifier-based determin-
istic parser for Chinese constituency parsing. In
our approach, which is based on the shift-reduce
parser for English reported in (Sagae and Lavie,
2005), the parsing task is transformed into a suc-
cession of classification tasks. The parser makes
one pass through the input sentence. At each parse
state, it consults a classifier to make shift/reduce
decisions. The parser then commits to a decision
and enters the next parse state. Shift/reduce deci-
sions are made deterministically based on the lo-
cal context of each parse state, and no backtrack-
ing is involved. This process can be viewed as a
greedy search where only one path in the whole
search space is considered. Our parser produces
both dependency and constituent structures, but in
this paper we will focus on constituent parsing.
By separating the classification task from the
parsing process, we can take advantage of many
machine learning techniques such as classifier en-
semble. We conducted experiments with four
different classifiers: support vector machines
(SVM), Maximum-Entropy (Maxent), Decision
Tree (DTree) and memory-based learning (MBL).
We also compared the performance of three differ-
ent classifier ensemble approaches (simple voting,
classifier stacking and meta-classifier).
Our best model (using stacked classifiers) runs
in linear time and has labeled precision and
recall above 88% using gold-standard part-of-
speech tags, surpassing the best published results
(see Section 5). Our SVM parser is 2-13 times
faster than state-of-the-art parsers, while produc-
425
ing more accurate results. Our Maxent and DTree
parsers are 40-270 times faster than state-of-the-
art parsers, but with 5-6% losses in accuracy.
2 Deterministic parsing model
Like other deterministic parsers, our parser as-
sumes input has already been segmented and
tagged with part-of-speech (POS) information
during a preprocessing step1. The main data struc-
tures used in the parsing algorithm are a queue and
a stack. The input word-POS pairs to be processed
are stored in the queue. The stack holds the partial
parse trees that are built during parsing. A parse
state is represented by the content of the stack and
queue.
The classifier makes shift/reduce decisions
based on contextual features that represent the
parse state. A shift action removes the first item
on the queue and puts it onto the stack. A reduce
action is in the form of Reduce-{Binary|Unary}-
X, where {Binary|Unary} denotes whether one or
two items are to be removed from the stack, and X
is the label of a new tree node that will be domi-
nating the removed items. Because a reduction is
either unary or binary, the resulting parse tree will
only have binary and/or unary branching nodes.
Parse trees are also lexicalized to produce de-
pendency structures. For lexicalization, we used
the same head-finding rules reported in (Bikel,
2004). With this additional information, reduce
actions are now in the form of Reduce-{Binary
|Unary}-X-Direction. The ?Direction? tag gives
information about whether to take the head-node
of the left subtree or the right subtree to be the
head of the new tree, in the case of binary reduc-
tion. A simple transformation process as described
in (Sagae and Lavie, 2005) is employed to con-
vert between arbitrary branching trees and binary
trees. This transformation breaks multi-branching
nodes down into binary-branching nodes by in-
serting temporary nodes; temporary nodes are col-
lapsed and removed when we transform a binary
tree back into a multi-branching tree.
The parsing process succeeds when all the items
in the queue have been processed and there is only
one item (the final parse tree) left on the stack.
If the classifier returns a shift action when there
are no items left on the queue, or a reduce ac-
tion when there are no items on the stack, the
1We constructed our own POS tagger based on SVM; see
Section 3.3.
parser fails. In this case, the parser simply com-
bines all the items on the stack into one IP node,
and outputs this as a partial parse. Sagae and
Lavie (2005) have shown that this algorithm has
linear time complexity, assuming that classifica-
tion takes constant time. The next example il-
lustrates the process for the input ?Y? (Brown)
6? (visits)?0 (Shanghai)? that is tagged with
the POS sequence ?NR (Proper Noun) VV (Verb)
NR (Proper Noun)?.
1. In the initial parsing state, the stack (S) is
empty, and the queue (Q) holds word and
POS tag pairs for the input sentence.
(S): Empty
(Q): NR
Y?
VV
6?
NR
?0
2. The first action item that the classifier gives
is a shift action.
(S): NR
Y?
(Q): VV
6?
NR
?0
3. The next action is a reduce-Unary-NP, which
means reducing the first item on the stack to a
NP node. Node (NRY?) becomes the head
of the new NP node and this information is
marked by brackets. The new parse state is:
(S): NP (NRY?)
NR
Y?
(Q): VV
6?
NR
?0
4. The next action is shift.
(S): NP (NRY?)
NR
Y?
VV
6?
(Q): NR
?0
5. The next action is again shift.
(S): NP (NRY?)
NR
Y?
VV
6?
NR
?0
(Q): Empty
6. The next action is reduce-Unary-NP.
(S): NP (NRY?)
NR
Y?
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
7. The next action is reduce-Binary-VP-Left.
The node (VV6?) will be the head of the
426
new VP node.
(S): NP (NRY?)
NR
Y?
VP (VV6?)
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
8. The next action is reduce-Binary-IP-Right.
Since after the action is performed, there will
be only one tree node(IP) left on the stack and
no items on the queue, this is the final action.
The final state is:
(S): IP (VV6?)
NP (NRY?)
NR
Y?
VP (VV6?)
VV
6?
NP (NR?0)
NR
?0
(Q): Empty
3 Classifiers and Feature Selection
Classification is the key component of our parsing
model. We conducted experiments with four dif-
ferent types of classifiers.
3.1 Classifiers
Support Vector Machine: Support Vector Ma-
chine is a discriminative classification technique
which solves the binary classification problem by
finding a hyperplane in a high dimensional space
that gives the maximum soft margin, based on
the Structural Risk Minimization Principle. We
used the TinySVM toolkit (Kudo and Matsumoto,
2000), with a degree 2 polynomial kernel. To train
a multi-class classifier, we used the one-against-all
scheme.
Maximum-Entropy Classifier: In a
Maximum-entropy model, the goal is to esti-
mate a set of parameters that would maximize
the entropy over distributions that satisfy certain
constraints. These constraints will force the model
to best account for the training data (Ratnaparkhi,
1999). Maximum-entropy models have been used
for Chinese character-based parsing (Fung et al,
2004; Luo, 2003) and POS tagging (Ng and Low,
2004). In our experiments, we used Le?s Maxent
toolkit (Zhang, 2004). This implementation uses
the Limited-Memory Variable Metric method for
parameter estimation. We trained all our models
using 300 iterations with no event cut-off, and
a Gaussian prior smoothing value of 2. Maxent
classifiers output not only a single class label, but
also a number of possible class labels and their
associated probability estimate.
Decision Tree Classifier: Statistical decision
tree is a classic machine learning technique that
has been extensively applied to NLP. For exam-
ple, decision trees were used in the SPATTER sys-
tem (Magerman, 1994) to assign probability dis-
tribution over the space of possible parse trees.
In our experiment, we used the C4.5 decision
tree classifier, and ignored lexical features whose
counts were less than 7.
Memory-Based Learning: Memory-Based
Learning approaches the classification problem
by storing training examples explicitly in mem-
ory, and classifying the current case by finding
the most similar stored cases (using k-nearest-
neighbors). We used the TiMBL toolkit (Daele-
mans et al, 2004) in our experiment, with k = 5.
3.2 Feature selection
For each parse state, a set of features are
extracted and fed to each classifier. Fea-
tures are distributionally-derived or linguistically-
based, and carry the context of a particular parse
state. When input to the classifier, each feature is
treated as a contextual predicate which maps an
outcome and a context to true, false value.
The specific features used with the classifiers
are listed in Table 1.
Sun and Jurafsky (2003) studied the distribu-
tional property of rhythm in Chinese, and used the
rhythmic feature to augment a PCFG model for
a practical shallow parsing task. This feature has
the value 1, 2 or 3 for monosyllabic, bi-syllabic or
multi-syllabic nouns or verbs. For noun and verb
phrases, the feature is defined as the number of
words in the phrase. Sun and Jurafsky found that
in NP and VP constructions there are strong con-
straints on the word length for verbs and nouns
(a kind of rhythm), and on the number of words
in a constituent. We employed these same rhyth-
mic features to see whether this property holds for
the Penn Chinese Treebank data, and if it helps in
the disambiguation of phrase types. Experiments
show that this feature does increase classification
accuracy of the SVM model by about 1%.
In both Chinese and English, there are punctu-
ation characters that come in pairs (e.g., parenthe-
ses). In Chinese, such pairs are more frequent
(quotes, single quotes, and book-name marks).
During parsing, we note how many opening punc-
427
1 A Boolean feature indicates if a closing punctuation is expected or not.
2 A Boolean value indicates if the queue is empty or not.
3 A Boolean feature indicates whether there is a comma separating S(1) and S(2) or not.
4 Last action given by the classifier, and number of words in S(1) and S(2).
5 Headword and its POS of S(1), S(2), S(3) and S(4), and word and POS of Q(1), Q(2), Q(3) and Q(4).
6 Nonterminal label of the root of S(1) and S(2), and number of punctuations in S(1) and S(2).
7 Rhythmic features and the linear distance between the head-words of the S(1) and S(2).
8 Number of words found so far to be dependents of the head-words of S(1) and S(2).
9 Nonterminal label, POS and headword of the immediate left and right child of the root of S(1) and S(2).
10 Most recently found word and POS pair that is to the left of the head-word of S(1) and S(2).
11 Most recently found word and POS pair that is to the right of the head-word of S(1) and S(2).
Table 1: Features for classification
tuations we have seen on the stack. If the number
is odd, then feature 2 will have value 1, otherwise
0. A boolean feature is used to indicate whether or
not an odd number of opening punctuations have
been seen and a closing punctuation is expected;
in this case the feature gives a strong hint to the
parser that all the items in the queue before the
closing punctuation, and the items on the stack
after the opening punctuation should be under a
common constituent node which begins and ends
with the two punctuations.
3.3 POS tagging
In our parsing model, POS tagging is treated as
a separate problem and it is assumed that the in-
put has already been tagged with POS. To com-
pare with previously published work, we evaluated
the parser performance on automatically tagged
data. We constructed a simple POS tagger using
an SVM classifier. The tagger makes two passes
over the input sentence. The first pass extracts fea-
tures from the two words and POS tags that came
before the current word, the two words follow-
ing the current word, and the current word itself
(the length of the word, whether the word con-
tains numbers, special symbols that separates for-
eign first and last names, common Chinese family
names, western alphabets or dates). Then the tag
is assigned to the word according to SVM classi-
fier?s output. In the second pass, additional fea-
tures such as the POS tags of the two words fol-
lowing the current word, and the POS tag of the
current word (assigned in the first pass) are used.
This tagger had a measured precision of 92.5% for
sentences ? 40 words.
4 Experiments
We performed experiments using the Penn Chi-
nese Treebank. Sections 001-270 (3484 sentences,
84,873 words) were used for training, 271-300
(348 sentences, 7980 words) for development, and
271-300 (348 sentences, 7980 words) for testing.
The whole dataset contains 99629 words, which is
about 1/10 of the size of the English Penn Tree-
bank. Standard corpus preparation steps were
done prior to parsing, so that empty nodes were
removed, and the resulting A over A unary rewrite
nodes are collapsed. Functional labels of the non-
terminal nodes are also removed, but we did not
relabel the punctuations, unlike in (Jiang, 2004).
Bracket scoring was done by the EVALB pro-
gram2, and preterminals were not counted as con-
stituents. In all our experiments, we used labeled
recall (LR), labeled precision (LP) and F1 score
(harmonic mean of LR and LP) as our evaluation
metrics.
4.1 Results of different classifiers
Table 2 shows the classification accuracy and pars-
ing accuracy of the four different classifiers on the
development set for sentences ? 40 words, with
gold-standard POS tagging. The runtime (Time)
of each model and number of failed parses (Fail)
are also shown.
Classification Parsing Accuracy
Model Accuracy LR LP F1 Fail Time
SVM 94.3% 86.9% 87.9% 87.4% 0 3m 19s
Maxent 92.6% 84.1% 85.2% 84.6% 5 0m 21s
DTree1 92.0% 78.8% 80.3% 79.5% 42 0m 12s
DTree2 N/A 81.6% 83.6% 82.6% 30 0m 18s
MBL 90.6% 74.3% 75.2% 74.7% 2 16m 11s
Table 2: Comparison of different classifier mod-
els? parsing accuracies on development set for sen-
tences ? 40 words, with gold-standard POS
For the DTree learner, we experimented with
two different classification strategies. In our first
approach, the classification is done in a single
stage (DTree1). The learner is trained for a multi-
2http://nlp.cs.nyu.edu/evalb/
428
class classification problem where the class labels
include shift and all possible reduce actions. But
this approach yielded a lot of parse failures (42 out
of 350 sentences failed during parsing, and par-
tial parse tree was returned). These failures were
mostly due to false shift actions in cases where
the queue is empty. To alleviate this problem, we
broke the classification process down to two stages
(DTree2). A first stage classifier makes a binary
decision on whether the action is shift or reduce.
If the output is reduce, a second-stage classifier de-
cides which reduce action to take. Results showed
that breaking down the classification task into two
stages increased overall accuracy, and the number
of failures was reduced to 30.
The SVM model achieved the highest classifi-
cation accuracy and the best parsing results. It
also successfully parsed all sentences. The Max-
ent model?s classification error rate (7.4%) was
30% higher than the error rate of the SVM model
(5.7%), and its F1 (84.6%) was 3.2% lower than
SVM model?s F1 (87.4%). But Maxent model was
about 9.5 times faster than the SVM model. The
DTree classifier achieved 81.6% LR and 83.6%
LP. The MBL model did not perform well; al-
though MBL and SVM differed in accuracy by
only about 3 percent, the parsing results showed
a difference of more than 10 percent. One pos-
sible explanation for the poor performance of
the MBL model is that all the features we used
were binary features, and memory-based learner
is known to work better with multivalue features
than binary features in natural language learning
tasks (van den Bosch and Zavrel, 2000).
In terms of speed and accuracy trade-off, there
is a 5.5% trade-off in F1 (relative to SVM?s F1)
for a roughly 14 times speed-up between SVM
and two-stage DTree. Maxent is more balanced
in the sense that its accuracy was slightly lower
(3.2%) than SVM, and was just about as fast as the
two-stage DTree on the development set. The high
speed of the DTree and Maxent models make them
very attractive in applications where speed is more
critical than accuracy. While the SVM model
takes more CPU time, we show in Section 5 that
when compared to existing parsers, SVM achieves
about the same or higher accuracy but is at least
twice as fast.
Using gold-standard POS tagging, the best clas-
sifier model (SVM) achieved LR of 87.2% and LP
of 88.3%, as shown in Table 4. Both measures sur-
pass the previously known best results on parsing
using gold-standard tagging. We also tested the
SVM model using data automatically tagged by
our POS tagger, and it achieved LR of 78.1% and
LP of 81.1% for sentences ? 40 words, as shown
in Table 3.
4.2 Classifier Ensemble Experiments
Classifier ensemble by itself has been a fruitful
research direction in machine learning in recent
years. The basic idea in classifier ensemble is
that combining multiple classifiers can often give
significantly better results than any single classi-
fier alone. We experimented with three different
classifier ensemble strategies: classifier stacking,
meta-classifier, and simple voting.
Using the SVM classifier?s results as a baseline,
we tested these approaches on the development
set. In classifier stacking, we collect the outputs
from Maxent, DTree and TiMBL, which are all
trained on a separate dataset from the training set
(section 400-650 of the Penn Chinese Treebank,
smaller than the original training set). We use their
classification output as features, in addition to the
original feature set, to train a new SVM model
on the original training set. We achieved LR of
90.3% and LP of 90.5% on the development set,
a 3.4% and 2.6% improvement in LR and LP, re-
spectively. When tested on the test set, we gained
1% improvement in F1 when gold-standard POS
tagging is used. When tested with automatic tag-
ging, we achieved a 0.5% improvement in F1. Us-
ing Bikel?s significant tester with 10000 times ran-
dom shuffle, the p-value for LR and LP are 0.008
and 0.457, respectively. The increase in recall
is statistically significant, and it shows classifier
stacking can improve performance.
On the other hand, we did not find meta-
classification and simple voting very effective. In
simple voting, we make the classifiers to vote in
each step for every parse action. The F1 of sim-
ple voting method is downgraded by 5.9% rela-
tive to SVM model?s F1. By analyzing the inter-
agreement among classifiers, we found that there
were no cases where Maxent?s top output and
DTree?s output were both correct and SVM?s out-
put was wrong. Using the top output from Maxent
and DTree directly does not seem to be comple-
mentary to SVM.
In the meta-classifier approach, we first col-
lect the output from each classifier trained on sec-
429
MODEL ? 40 words ? 100 words UnlimitedLR LP F1 POS LR LP F1 POS LR LP F1 POS
Bikel & Chiang 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -
Levy & Manning 2003 79.2% 78.4% 78.8% - - - - - - - - -
Xiong et al 2005 78.7% 80.1% 79.4% - - - - - - - - -
Bikel?s Thesis 2004 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -
Chiang & Bikel 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -
Jiang?s Thesis 2004 80.1% 82.0% 81.1% 92.4% - - - - - - - -
Sun & Jurafsky 2004 85.5% 86.4% 85.9% - - - - - 83.3% 82.2% 82.7% -
DTree model 71.8% 76.9% 74.4% 92.5% 69.2% 74.5% 71.9% 92.2% 68.7% 74.2% 71.5% 92.1%
SVM model 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%
Stacked classifier model 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%
Table 3: Comparison with related work on the test set using automatically generated POS
tion 1-210 (roughly 3/4 of the entire training set).
Then specifically for Maxent, we collected the top
output as well as its associated probability esti-
mate. Then we used the outputs and probabil-
ity estimate as features to train an SVM classifier
that makes a decision on which classifier to pick.
Meta-classifier results did not change at all from
our baseline. In fact, the meta-classifier always
picked SVM as its output. This agrees with our
observation for the simple voting case.
5 Comparison with Related Work
Bikel and Chiang (2000) constructed two parsers
using a lexicalized PCFG model that is based on
Collins? model 2 (Collins, 1999), and a statisti-
cal Tree-adjoining Grammar(TAG) model. They
used the same train/development/test split, and
achieved LR/LP of 76.8%/77.8%. In Bikel?s the-
sis (2004), the same Collins emulation model
was used, but with tweaked head-finding rules.
Also a POS tagger was used for assigning tags
for unseen words. The refined model achieved
LR/LP of 78.0%/81.2%. Chiang and Bikel (2002)
used inside-outside unsupervised learning algo-
rithm to augment the rules for finding heads, and
achieved an improved LR/LP of 78.8%/81.1%.
Levy and Manning (2003) used a factored model
that combines an unlexicalized PCFG model with
a dependency model. They achieved LR/LP
of 79.2%/78.4% on a different test/development
split. Xiong et al (2005) used a similar model to
the BBN?s model in (Bikel and Chiang, 2000),
and augmented the model by semantic categori-
cal information and heuristic rules. They achieved
LR/LP of 78.7%/80.1%. Hearne and Way (2004)
used a Data-Oriented Parsing (DOP) approach
that was optimized for top-down computation.
They achieved F1 of 71.3 on a different test and
training set. Jiang (2004) reported LR/LP of
80.1%/82.0% on sentences ? 40 words (results
not available for sentences ? 100 words) by ap-
plying Collins? parser to Chinese. In Sun and
Jurafsky (2004)?s work on Chinese shallow se-
mantic parsing, they also applied Collin?s parser
to Chinese. They reported up-to-date the best
parsing performance on Chinese Treebank. They
achieved LR/LP of 85.5%/86.4% on sentences ?
40 words, and LR/LP of 83.3%/82.2% on sen-
tences ? 100 words, far surpassing all other pre-
viously reported results. Luo (2003) and Fung et
al. (2004) addressed the issue of Chinese text seg-
mentation in their work by constructing character-
based parsers. Luo integrated segmentation, POS
tagging and parsing into one maximum-entropy
framework. He achieved a F1 score of 81.4% in
parsing. But the score was achieved using 90% of
the 250K-CTB (roughly 2.5 times bigger than our
training set) for training and 10% for testing. Fung
et al(2004) also took the maximum-entropy mod-
eling approach, but augmented by transformation-
based learning. They used the standard training
and testing split. When tested with gold-standard
segmentation, they achieved a F1 score of 79.56%,
but POS-tagged words were treated as constituents
in their evaluation.
In comparison with previous work, our parser?s
accuracy is very competitive. Compared to Jiang?s
work and Sun and Jurafsky?s work, the classifier
ensemble model of our parser is lagging behind by
1% and 5.8% in F1, respectively. But compared
to all other works, our classifier stacking model
gave better or equal results for all three measures.
In particular, the classifier ensemble model and
SVM model of our parser achieved second and
third highest LP, LR and F1 for sentences ? 100
words as shown in Table 3. (Sun and Jurafsky did
not report results on sentences ? 100 words, but
it is worth noting that out of all the test sentences,
430
only 2 sentences have length > 100).
Jiang (2004) and Bikel (2004)3 also evaluated
their parsers on the test set for sentences ? 40
words, using gold-standard POS tagged input. Our
parser gives significantly better results as shown
in Table 4. The implication of this result is two-
fold. On one hand, it shows that if POS tagging
accuracy can be increased, our parser is likely to
benefit more than the other two models; on the
other hand, it also indicates that our deterministic
model is less resilient to POS errors. Further de-
tailed analysis is called for, to study the extent to
which POS tagging errors affects the deterministic
parsing model.
Model LR LP F1
Bikel?s Thesis 2004 80.9% 84.5% 82.7%
Jiang?s Thesis 2004 84.5% 88.0% 86.2%
DTree model 80.5% 83.9% 82.2%
Maxent model 81.4% 82.8% 82.1%
SVM model 87.2% 88.3% 87.8%
Stacked classifier model 88.3% 88.1% 88.2%
Table 4: Comparison with related work on the test
set for sentence ? 40 words, using gold-standard
POS
To measure efficiency, we ran two publicly
available parsers (Levy and Manning?s PCFG
parser (2003) and Bikel?s parser (2004)) on
the standard test set and compared the run-
time4. The runtime of these parsers are shown
in minute:second format in Table 5. Our SVM
model is more than 2 times faster than Levy and
Manning?s parser, and more than 13 times faster
than Bikel?s parser. Our DTree model is 40 times
faster than Levy and Manning?s parser, and 270
times faster than Bikel?s parser. Another advan-
tage of our parser is that it does not take as much
memory as these other parsers do. In fact, none
of the models except MBL takes more than 60
megabytes of memory at runtime. In compari-
son, Levy and Manning?s PCFG parser requires
more than 400 mega-bytes of memory when pars-
ing long sentences (70 words or longer).
6 Discussion and future work
One unique attraction of this deterministic pars-
ing framework is that advances in machine learn-
ing field can be directly applied to parsing, which
3Bikel?s parser used gold-standard POS tags for unseen
words only. Also, the results are obtained from a parser
trained on 250K-CTB, about 2.5 times bigger than CTB 1.0.
4All the experiments were conducted on a Pentium IV
2.4GHz machine with 2GB of RAM.
Model runtime
Bikel 54m 6s
Levy & Manning 8m 12s
Our DTree model 0m 14s
Our Maxent model 0m 24s
Our SVM model 3m 50s
Table 5: Comparison of parsing speed
opens up lots of possibilities for continuous im-
provements, both in terms of accuracy and effi-
ciency. For example, in this paper we experi-
mented with one method of simple voting. An al-
ternative way of doing simple voting is to let the
parsers vote on membership of constituents after
each parser has produced its own parse tree (Hen-
derson and Brill, 1999), instead of voting at each
step during parsing.
Our initial attempt to increase the accuracy of
the DTree model by applying boosting techniques
did not yield satisfactory results. In our exper-
iment, we implemented the AdaBoost.M1 (Fre-
und and Schapire, 1996) algorithm using re-
sampling to vary the training set distribution.
Results showed AdaBoost suffered severe over-
fitting problems and hurts accuracy greatly, even
with a small number of samples. One possible
reason for this is that our sample space is very
unbalanced across the different classes. A few
classes have lots of training examples while a large
number of classes are rare, which could raise the
chance of overfitting.
In our experiments, SVM model gave better re-
sults than the Maxent model. But it is important
to note that although the same set of features were
used in both models, a degree 2 polynomial ker-
nel was used in the SVM classifier while Maxent
only has degree 1 features. In our future work, we
will experiment with degree 2 features and L1 reg-
ularization in the Maxent model, which may give
us closer performance to the SVM model with a
much faster speed.
7 Conclusion
In this paper, we presented a novel determinis-
tic parser for Chinese constituent parsing. Us-
ing gold-standard POS tags, our best model (us-
ing stacked classifiers) runs in linear time and has
labeled recall and precision of 88.3% and 88.1%,
respectively, surpassing the best published results.
And with a trade-off of 5-6% in accuracy, our
DTree and Maxent parsers run at speeds 40-270
times faster than state-of-the-art parsers. Our re-
431
sults have shown that the deterministic parsing
framework is a viable and effective approach to
Chinese parsing. For future work, we will fur-
ther improve the speed and accuracy of our mod-
els, and apply them to more Chinese and multi-
lingual natural language applications that require
high speed and accurate parsing.
Acknowledgment
This work was supported in part by ARDA?s
AQUAINT Program. We thank Eric Nyberg for
his help during the final preparation of this paper.
References
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the Chinese Tree-
bank. In Proceedings of the Second Chinese Lan-
guage Processing Workshop, ACL ?00.
Daniel M. Bikel. 2004. On the Parameter Space of
Generative Lexicalized Statistical Parsing Models.
Ph.D. thesis, University of Pennsylvania.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2004. Deterministic dependency structure
analyzer for Chinese. In Proceedings of IJCNLP
?04.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2005. Machine learning-based dependency
analyzer for Chinese. In Proceedings of ICCC ?05.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
COLING ?02.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. Timbl version 5.1 ref-
erence guide. Technical report, Tilburg University.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceed-
ings of ICML ?96.
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy Chinese
parser augmented by transformation-based learning.
ACM Transactions on Asian Language Information
Processing, 3(2):159?168.
Mary Hearne and Andy Way. 2004. Data-oriented
parsing and the Penn Chinese Treebank. In Proceed-
ings of IJCNLP ?04.
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: Combining
parsers. In Proceedings of EMNLP ?99.
Zhengping Jiang. 2004. Statistical Chinese parsing.
Honours thesis, National University of Singapore.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL and LLL ?00.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank?
In Proceedings of ACL ?03.
Xiaoqiang Luo. 2003. A maximum entropy Chinese
character-based parser. In Proceedings of EMNLP
?03.
David M. Magerman. 1994. Natural Language Pars-
ing as Statistical Pattern Recognition. Ph.D. thesis,
Stanford University.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP ?04.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING ?04.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the IWPT ?05.
Honglin Sun and Daniel Jurafsky. 2003. The effect of
rhythm on structural disambiguation in Chinese. In
Proceedings of SIGHAN Workshop ?03.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantic parsing of Chinese. In Proceedings of the
HLT/NAACL ?04.
Antal van den Bosch and Jakub Zavrel. 2000. Un-
packing multi-valued symbolic features and classes
in memory-based language learning. In Proceedings
of ICML ?00.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with semantic knowledge. In Proceedings
of IJCNLP ?05.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT ?03.
Le Zhang, 2004. Maximum Entropy Modeling Toolkit
for Python and C++. Reference Manual.
432
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 691?698,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Best-First Probabilistic Shift-Reduce Parser
Kenji Sagae and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{sagae,alavie}@cs.cmu.edu
Abstract
Recently proposed deterministic classifier-
based parsers (Nivre and Scholz, 2004;
Sagae and Lavie, 2005; Yamada and Mat-
sumoto, 2003) offer attractive alternatives
to generative statistical parsers. Determin-
istic parsers are fast, efficient, and sim-
ple to implement, but generally less ac-
curate than optimal (or nearly optimal)
statistical parsers. We present a statis-
tical shift-reduce parser that bridges the
gap between deterministic and probabilis-
tic parsers. The parsing model is essen-
tially the same as one previously used
for deterministic parsing, but the parser
performs a best-first search instead of a
greedy search. Using the standard sec-
tions of the WSJ corpus of the Penn Tree-
bank for training and testing, our parser
has 88.1% precision and 87.8% recall (us-
ing automatically assigned part-of-speech
tags). Perhaps more interestingly, the pars-
ing model is significantly different from
the generative models used by other well-
known accurate parsers, allowing for a
simple combination that produces preci-
sion and recall of 90.9% and 90.7%, re-
spectively.
1 Introduction
Over the past decade, researchers have devel-
oped several constituent parsers trained on an-
notated data that achieve high levels of accu-
racy. Some of the more popular and more ac-
curate of these approaches to data-driven parsing
(Charniak, 2000; Collins, 1997; Klein and Man-
ning, 2002) have been based on generative mod-
els that are closely related to probabilistic context-
free grammars. Recently, classifier-based depen-
dency parsing (Nivre and Scholz, 2004; Yamada
and Matsumoto, 2003) has showed that determin-
istic parsers are capable of high levels of accu-
racy, despite great simplicity. This work has led to
the development of deterministic parsers for con-
stituent structures as well (Sagae and Lavie, 2005;
Tsuruoka and Tsujii, 2005). However, evaluations
on the widely used WSJ corpus of the Penn Tree-
bank (Marcus et al, 1993) show that the accuracy
of these parsers still lags behind the state-of-the-
art.
A reasonable and commonly held assumption is
that the accuracy of deterministic classifier-based
parsers can be improved if determinism is aban-
doned in favor of a search over a larger space of
possible parses. While this assumption was shown
to be true for the parser of Tsuruoka and Tsu-
jii (2005), only a moderate improvement resulted
from the addition of a non-greedy search strategy,
and overall parser accuracy was still well below
that of state-of-the-art statistical parsers.
We present a statistical parser that is based on
a shift-reduce algorithm, like the parsers of Sagae
and Lavie (2005) and Nivre and Scholz (2004), but
performs a best-first search instead of pursuing a
single analysis path in deterministic fashion. The
parser retains much of the simplicity of determin-
istic classifier-based parsers, but achieves results
that are closer in accuracy to state-of-the-art statis-
tical parsers. Furthermore, a simple combination
of the shift-reduce parsing model with an existing
generative parsing model produces results with ac-
curacy that surpasses any that of any single (non-
reranked) parser tested on the WSJ Penn Tree-
bank, and comes close to the best results obtained
with discriminative reranking (Charniak and John-
691
son, 2005).
2 Parser Description
Our parser uses an extended version of the basic
bottom-up shift-reduce algorithm for constituent
structures used in Sagae and Lavie?s (2005) de-
terministic parser. For clarity, we will first de-
scribe the deterministic version of the algorithm,
and then show how it can be extended into a proba-
bilistic algorithm that performs a best-first search.
2.1 A Shift-Reduce Algorithm for
Deterministic Constituent Parsing
In its deterministic form, our parsing algorithm
is the same single-pass shift-reduce algorithm as
the one used in the classifer-based parser of Sagae
and Lavie (2005). That algorithm, in turn, is sim-
ilar to the dependency parsing algorithm of Nivre
and Scholz (2004), but it builds a constituent tree
and a dependency tree simultaneously. The al-
gorithm considers only trees with unary and bi-
nary productions. Training the parser with arbi-
trary branching trees is accomplished by a sim-
ple procedure to transform those trees into trees
with at most binary productions. This is done
by converting each production with n children,
where n > 2, into n ? 1 binary productions.
This binarization process is similar to the one de-
scribed in (Charniak et al, 1998). Additional non-
terminal nodes introduced in this conversion must
be clearly marked. Transforming the parser?s out-
put into arbitrary branching trees is accomplished
using the reverse process.
The deterministic parsing algorithm involves
two main data structures: a stack S, and a queue
W . Items in S may be terminal nodes (part-of-
speech-tagged words), or (lexicalized) subtrees of
the final parse tree for the input string. Items in W
are terminals (words tagged with parts-of-speech)
corresponding to the input string. When parsing
begins, S is empty and W is initialized by insert-
ing every word from the input string in order, so
that the first word is in front of the queue.
The algorithm defines two types of parser ac-
tions, shift and reduce, explained below:
? Shift: A shift action consists only of remov-
ing (shifting) the first item (part-of-speech-
tagged word) from W (at which point the
next word becomes the new first item), and
placing it on top of S.
? Reduce: Reduce actions are subdivided into
unary and binary cases. In a unary reduction,
the item on top of S is popped, and a new
item is pushed onto S. The new item consists
of a tree formed by a non-terminal node with
the popped item as its single child. The lex-
ical head of the new item is the same as the
lexical head of the popped item. In a binary
reduction, two items are popped from S in
sequence, and a new item is pushed onto S.
The new item consists of a tree formed by a
non-terminal node with two children: the first
item popped from S is the right child, and the
second item is the left child. The lexical head
of the new item may be the lexical head of its
left child, or the lexical head of its right child.
If S is empty, only a shift action is allowed. If
W is empty, only a reduce action is allowed. If
both S and W are non-empty, either shift or re-
duce actions are possible, and the parser must de-
cide whether to shift or reduce. If it decides to re-
duce, it must also choose between a unary-reduce
or a binary-reduce, what non-terminal should be at
the root of the newly created subtree to be pushed
onto the stack S, and whether the lexical head of
the newly created subtree will be taken from the
right child or the left child of its root node. Fol-
lowing the work of Sagae and Lavie, we consider
the complete set of decisions associated with a re-
duce action to be part of that reduce action. Pars-
ing terminates when W is empty and S contains
only one item, and the single item in S is the parse
tree for the input string.
2.2 Shift-Reduce Best-First Parsing
A deterministic shift-reduce parser based on the
algorithm described in section 2.1 does not handle
ambiguity. By choosing a single parser action at
each opportunity, the input string is parsed deter-
ministically, and a single constituent structure is
built during the parsing process from beginning to
end (no other structures are even considered).
A simple extension to this idea is to eliminate
determinism by allowing the parser to choose sev-
eral actions at each opportunity, creating different
paths that lead to different parse trees. This is es-
sentially the difference between deterministic LR
parsing (Knuth, 1965) and Generalized-LR pars-
ing (Tomita, 1987; Tomita, 1990). Furthermore,
if a probability is assigned to every parser action,
the probability of a parse tree can be computed
692
simply as the product of the probabilities of each
action in the path that resulted in that parse tree
(the derivation of the tree). This produces a prob-
abilistic shift-reduce parser that resembles a gen-
eralized probabilistic LR parser (Briscoe and Car-
roll, 1993), where probabilities are associated with
an LR parsing table. In our case, although there
is no LR table, the action probabilities are associ-
ated with several aspects of the current state of the
parser, which to some extent parallel the informa-
tion contained in an LR table. Instead of having
an explicit LR table and pushing LR states onto
the stack, the state of the parser is implicitly de-
fined by the configurations of the stack and queue.
In a way, there is a parallel between how mod-
ern PCFG-like parsers use markov grammars as
a distribution that is used to determine the proba-
bility of any possible grammar rules, and the way
a statistical model is used in our parser to assign
a probability to any transition of parser states (in-
stead of a symbolic LR table).
Pursuing every possible sequence of parser ac-
tions creates a very large space of actions for
even moderately sized sentences. To find the most
likely parse tree efficiently according to the prob-
abilistic shift-reduce parsing scheme described so
far, we use a best-first strategy. This involves an
extension of the deterministic shift-reduce algo-
rithm into a best-first shift-reduce algorithm. To
describe this extension, we first introduce a new
data structure Ti that represents a parser state,
which includes a stack Si and a queue Wi. In
the deterministic algorithm, we would have a sin-
gle parser state T that contains S and W . The
best-first algorithm, on the other hand, has a heap
H containing multiple parser states T1 ... Tn.
These states are ordered in the heap according to
their probabilities, so that the state with the highest
probability is at the top. State probabilities are de-
termined by multiplying the probabilities of each
of the actions that resulted in that state. Parser ac-
tions are determined from and applied to a parser
state Ti popped from the top of H . The parser
actions are the same as in the deterministic ver-
sion of the algorithm. When the item popped from
the top of the heap H contains a stack Si with a
single item and an empty queue (in other words,
meets the acceptance criteria for the determinis-
tic version of the algorithm), the item on top of
Si is the tree with the highest probability. At that
point, parsing terminates if we are searching for
the most probable parse. To obtain a list of n-best
parses, we simply continue parsing once the first
parse tree is found, until either n trees are found,
or H is empty.
We note that this approach does not use dy-
namic programming, and relies only on the best-
first search strategy to arrive at the most prob-
able parse efficiently. Without any pruning of
the search space, the distribution of probability
mass among different possible actions for a parse
state has a large impact on the behavior of the
search. We do not use any normalization to ac-
count for the size (in number of actions) of dif-
ferent derivations when calculating their probabili-
ties, so it may seem that shorter derivations usually
have higher probabilities than longer ones, causing
the best-first search to approximate a breadth-first
search in practice. However, this is not the case if
for a given parser state only a few actions (or, ide-
ally, only one action) have high probability, and all
other actions have very small probabilities. In this
case, only likely derivations would reach the top of
the heap, resulting in the desired search behavior.
The accuracy of deterministic parsers suggest that
this may in fact be the types of probabilities a clas-
sifier would produce given features that describe
the parser state, and thus the context of the parser
action, specifically enough. The experiments de-
scribed in section 4 support this assumption.
2.3 Classifier-Based Best-First Parsing
To build a parser based on the deterministic al-
gorithm described in section 2.1, a classifier is
used to determine parser actions. Sagae and Lavie
(2005) built two deterministic parsers this way,
one using support vector machines, and one using
k-nearest neighbors. In each case, the set of fea-
tures and classes used with each classifier was the
same. Items 1 ? 13 in figure 1 shows the features
used by Sagae and Lavie. The classes produced
by the classifier encode every aspect of a parser
action. Classes have one of the following forms:
SHIFT : represents a shift action;
REDUCE-UNARY-XX : represents a unary re-
duce action, where the root of the new sub-
tree pushed onto S is of type XX (where XX
is a non-terminal symbol, typically NP , V P ,
PP , for example);
REDUCE-LEFT-XX : represents a binary re-
duce action, where the root of the new sub-
693
tree pushed onto S is of non-terminal type
XX. Additionally, the head of the new subtree
is the same as the head of the left child of the
root node;
REDUCE-RIGHT-XX : represents a binary re-
duce action, where the root of the new sub-
tree pushed onto S is of non-terminal type
XX. Additionally, the head of the new sub-
tree is the same as the head of the right child
of the root node.
To implement a parser based on the best-first al-
gorithm, instead of just using a classifier to de-
termine one parser action given a stack and a
queue, we need a classification approach that pro-
vides us with probabilities for different parser ac-
tions associated with a given parser state. One
such approach is maximum entropy classification
(Berger et al, 1996), which we use in the form
of a library implemented by Tsuruoka1 and used
in his classifier-based parser (Tsuruoka and Tsujii,
2005). We used the same classes and the same fea-
tures as Sagae and Lavie, and an additional feature
that represents the previous parser action applied
the current parser state (figure 1).
3 Related Work
As mentioned in section 2, our parsing approach
can be seen as an extension of the approach of
Sagae and Lavie (2005). Sagae and Lavie eval-
uated their deterministic classifier-based parsing
framework using two classifiers: support vector
machines (SVM) and k-nearest neighbors (kNN).
Although the kNN-based parser performed poorly,
the SVM-based parser achieved about 86% preci-
sion and recall (or 87.5% using gold-standard POS
tags) on the WSJ test section of the Penn Tree-
bank, taking only 11 minutes to parse the test set.
Sagae and Lavie?s parsing algorithm is similar to
the one used by Nivre and Scholz (2004) for de-
terministic dependency parsing (using kNN). Ya-
mada and Matsumoto (2003) have also presented
a deterministic classifier-based (SVM-based) de-
pendency parser, but using a different parsing al-
gorithm, and using only unlabeled dependencies.
Tsuruoka and Tsujii (2005) developed a
classifier-based parser that uses the chunk-parsing
algorithm and achieves extremely high parsing
speed, but somewhat low recall. The algorithm
1The SS MaxEnt library is publicly available from
http://www-tsujii.is.s.u-tokyo.ac.jp/ tsuruoka/maxent/.
is based on reframing the parsing task as several
sequential chunking tasks.
Finally, our parser is in many ways similar to
the parser of Ratnaparkhi (1997). Ratnaparkhi?s
parser uses maximum-entropy models to deter-
mine the actions of a parser based to some extent
on the shift-reduce framework, and it is also capa-
ble of pursuing several paths and returning the top-
n highest scoring parses for a sentence. However,
in addition to using different features for parsing,
Ratnaparkhi?s parser uses a different, more com-
plex algorithm. The use of a more involved algo-
rithm allows Ratnaparkhi?s parser to work with ar-
bitrary branching trees without the need of the bi-
narization transform employed here. It breaks the
usual reduce actions into smaller pieces (CHECK
and BUILD), and uses two separate passes (not
including the part-of-speech tagging pass) for de-
termining chunks and higher syntactic structures
separately. Instead of keeping a stack, the parser
makes multiple passes over the input string, like
the dependency parsing algorithm used by Ya-
mada and Matsumoto. Our parser, on the other
hand, uses a simpler stack-based shift-reduce (LR-
like) algorithm for trees with only unary and bi-
nary productions.
4 Experiments
We evaluated our classifier-based best-first parser
on the Wall Street Journal corpus of the Penn Tree-
bank (Marcus et al, 1993) using the standard split:
sections 2-21 were used for training, section 22
was used for development and tuning of parame-
ters and features, and section 23 was used for
testing. Every experiment reported here was per-
formed on a Pentium4 3.2GHz with 2GB of RAM.
Each tree in the training set had empty-node and
function tag information removed, and the trees
were lexicalized using the same head-table rules as
in the Collins (1999) parser (these rules were taken
from Bikel?s (2002) implementation of the Collins
parser). The trees were then converted into trees
containing only unary and binary productions, us-
ing the binarization transform described in section
2. Classifier training instances of features paired
with classes (parser actions) were extracted from
the trees in the training set, and the total number
of training instances was about 1.9 million. It is in-
teresting to note that the procedure of training the
best-first parser is identical to the training of a de-
terministic version of the parser: the deterministic
694
Let:
S(n) denote the nth item from the top of the stack S, and
W (n) denote the nth item from the front of the queue W .
Features:
1. The head-word (and its POS tag) of: S(0), S(1), S(2), andS(3)
2. The head-word (and its POS tag) of: W (0), W (1), W (2) and W (3)
3. The non-terminal node of the root of: S(0), and S(1)
4. The non-terminal node of the left child of the root of: S(0), and S(1)
5. The non-terminal node of the right child of the root of: S(0), and S(1)
6. The POS tag of the head-word of the left child of the root of: S(0), and
S(1)
7. The POS tag of the head-word of the right child of the root of: S(0),
and S(1)
8. The linear distance (number of words apart) between the head-words of
S(0) and S(1)
9. The number of lexical items (words) that have been found (so far) to
be dependents of the head-words of: S(0), and S(1)
10. The most recently found lexical dependent of the head-word of S(0)
that is to the left of S(0)?s head
11. The most recently found lexical dependent of the head-word of S(0)
that is to the right of S(0)?s head
12. The most recently found lexical dependent of the head-word of S(1)
that is to the left of S(1)?s head
13. The most recently found lexical dependent of the head-word of S(1)
that is to the right of S(1)?s head
14. The previous parser action applied to the current parser state
Figure 1: Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). The
features described in items 1 ? 7 are more directly related to the lexicalized constituent trees that are built
during parsing, while the features described in items 8 ? 13 are more directly related to the dependency
structures that are built simultaneously to the constituent structures.
695
algorithm is simply run over all sentences in the
training set, and since the correct trees are known
in advance, we can simply record the features and
correct parser actions that lead to the construction
of the correct tree.
Training the maximum entropy classifier with
such a large number (1.9 million) of training in-
stances and features required more memory than
was available (the maximum training set size we
were able to train with 2GB of RAM was about
200,000 instances), so we employed the training
set splitting idea used by Yamada and Matsumoto
(2003) and Sagae and Lavie (2005). In our case,
we split the training data according to the part-
of-speech (POS) tag of the head-word of the item
on top of the stack, and trained each split of the
training data separately. At run-time, every trained
classifier is loaded, and the choice of classifier
to use is made by looking at the head-word of
the item on top of the stack in the current parser
state. The total training time (a single machine
was used and each classifier was trained in se-
ries) was slightly under nine hours. For compar-
ison, Sagae and Lavie (2005) report that train-
ing support vector machines for one-against-all
multi-class classification on the same set of fea-
tures for their deterministic parser took 62 hours,
and training a k-nearest neighbors classifier took
11 minutes.
When given perfectly tagged text (gold part-of-
speech tags extracted from the Penn Treebank),
our parser has labeled constituent precision and re-
call of 89.40% and 88.79% respectively over all
sentences in the test set, and 90.01% and 89.32%
over sentences with length of at most 40 words.
These results are at the same level of accuracy as
those obtained with other state-of-the-art statisti-
cal parsers, although still well below the best pub-
lished results for this test set (Bod, 2003; Char-
niak and Johnson, 2005). Although the parser is
quite accurate, parsing the test set took 41 minutes.
By implementing a very simple pruning strategy,
the parser can be made much faster. Pruning the
search space is done by only adding a new parser
state to the heap if its probability is greater than
1/b of the probability of the most likely state in
the heap that has had the same number of parser
actions. By setting b to 50, the parser?s accuracy
is only affected minimally, and we obtain 89.3%
precision and 88.7% recall, while parsing the test
set in slightly under 17 minutes and taking less
than 60 megabytes of RAM. Under the same con-
ditions, but using automatically assigned part-of-
speech tags (at 97.1% accuracy) using the SVM-
Tool tagger (Gimenez and Marquez, 2004), we
obtain 88.1% precision and 87.8% recall. It is
likely that the deterioration in accuracy is aggra-
vated by the training set splitting scheme based on
POS tags.
A deterministic version of our parser, obtained
by simply taking the most likely parser action as
the only action at each step (in other words, by set-
ting b to 1), has precision and recall of 85.4% and
84.8%, respectively (86.5% and 86.0% using gold-
standard POS tags). More interestingly, it parses
all 2,416 sentences (more than 50,000 words) in
only 46 seconds, 10 times faster than the deter-
ministic SVM parser of Sagae and Lavie (2005).
The parser of Tsuruoka and Tsujii (Tsuruoka and
Tsujii, 2005) has comparable speed, but we obtain
more accurate results. In addition to being fast,
our deterministic parser is also lean, requiring only
about 25 megabytes of RAM.
A summary of these results is shown in table 1,
along with the results obtained with other parsers
for comparison purposes. The figures shown in
table 1 only include experiments using automat-
ically assigned POS tags. Results obtained with
gold-standard POS tags are not shown, since they
serve little purpose in a comparison with existing
parsers. Although the time figures reflect the per-
formance of each parser at the stated level of ac-
curacy, all of the search-based parsers can trade
accuracy for increased speed. For example, the
Charniak parser can be made twice as fast at the
cost of a 0.5% decrease in precision/recall, or ten
times as fast at the cost of a 4% decrease in preci-
sion/recall (Roark and Charniak, 2002).
4.1 Reranking with the Probabililstic
Shift-Reduce Model
One interesting aspect of having an accurate pars-
ing model that is significantly different from other
well-known generative models is that the com-
bination of two accurate parsers may produce
even more accurate results. A probabilistic shift-
reduce LR-like model, such as the one used in
our parser, is different in many ways from a lex-
icalized PCFG-like model (using markov a gram-
mar), such as those used in the Collins (1999)
and Charniak (2000) parsers. In the probabilis-
tic LR model, probabilities are assigned to tree
696
Precision Recall F-score Time (min)
Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17
Deterministic (MaxEnt) (this paper) 85.4 84.8 85.1 < 1
Charniak & Johnson (2005) 91.3 90.6 91.0 Unk
Bod (2003) 90.8 90.7 90.7 145*
Charniak (2000) 89.5 89.6 89.5 23
Collins (1999) 88.3 88.1 88.2 39
Ratnaparkhi (1997) 87.5 86.3 86.9 Unk
Tsuruoka & Tsujii (2005): deterministic 86.5 81.2 83.8 < 1*
Tsuruoka & Tsujii (2005): search 86.8 85.0 85.9 2*
Sagae & Lavie (2005) 86.0 86.1 86.0 11*
Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse
the test set. We first show results for the parsers described here, then for four of the most accurate or
most widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally for three recent
classifier-based parsers. For the purposes of direct comparisons, only results obtained with automatically
assigned part-of-speech tags are shown (tags are assigned by the parser itself or by a separate part-of-
speech tagger). * Times reported by authors running on different hardware.
derivations (not the constituents themselves) based
on the sequence of parser shift/reduce actions.
PCFG-like models, on the other hand, assign prob-
abilities to the trees directly. With models that dif-
fer in such fundamental ways, it is possible that
the probabilities assigned to different trees are in-
dependent enough that even a very simple combi-
nation of the two models may result in increased
accuracy.
We tested this hypothesis by using the Char-
niak (2000) parser in n-best mode, producing the
top 10 trees with corresponding probabilities. We
then rescored the trees produced by the Charniak
parser using our probabilistic LR model, and sim-
ply multiplied the probabilities assigned by the
Charniak model and our LR model to get a com-
bined score for each tree2. On development data
this resulted in a 1.3% absolute improvement in f-
score over the 1-best trees produced by the Char-
niak parser. On the test set (WSJ Penn Treebank
section 23), this reranking scheme produces preci-
sion of 90.9% and recall of 90.7%, for an f-score
of 90.8%.
2The trees produced by the Charniak parser may include
the part-of-speech tags AUX and AUXG, which are not part
of the original Penn Treebank tagset. See (Charniak, 2000)
for details. These are converted deterministically into the ap-
propriate Penn Treebank verb tags, possibly introducing a
small number of minor POS tagging errors. Gold-standard
tags or the output of a separate part-of-speech tagger are not
used at any point in rescoring the trees.
5 Conclusion
We have presented a best-first classifier-based
parser that achieves high levels of precision and
recall, with fast parsing times and low memory re-
quirements. One way to view the parser is as an
extension of recent work on classifier-based deter-
ministic parsing. It retains the modularity between
parsing algorithms and learning mechanisms asso-
ciated with deterministic parsers, making it simple
to understand, implement, and experiment with.
Another way to view the parser is as a variant of
probabilistic GLR parsers without an explicit LR
table.
We have shown that our best-first strategy re-
sults in significant improvements in accuracy over
deterministic parsing. Although the best-first
search makes parsing slower, we have imple-
mented a beam strategy that prunes much of the
search space with very little cost in accuracy. This
strategy involves a parameter that can be used to
control the trade-off between accuracy and speed.
At one extreme, the parser is very fast (more than
1,000 words per second) and still moderately ac-
curate (about 85% f-score, or 86% using gold-
standard POS tags). This makes it possible to
apply parsing to natural language tasks involv-
ing very large amounts of text (such as question-
answering or information extraction with large
corpora). A less aggressive pruning setting results
in an f-score of about 88% (or 89%, using gold-
standard POS tags), taking 17 minutes to parse the
WSJ test set.
697
Finally, we have shown that by multiplying the
probabilities assigned by our maximum entropy
shift-reduce model to the probabilities of the 10-
best trees produced for each sentence by the Char-
niak parser, we can rescore the trees to obtain
more accurate results than those produced by ei-
ther model in isolation. This simple combination
of the two models produces an f-score of 90.8%
for the standard WSJ test set.
Acknowledgements
We thank John Carroll for insightful discussions at
various stages of this work, and the reviewers for
their detailed comments. This work was supported
in part by the National Science Foundation under
grant IIS-0414630.
References
A. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of HLT2002. San Diego, CA.
R. Bod. 2003. An efficient implementation of a new
dop model. In Proceedings of the European chapter
of the 2003 meeting of the Association for Computa-
tional Linguistics. Budapest, Hungary.
E. Briscoe and J. Carroll. 1993. Generalised proba-
bilistic lr parsing of natural language (corpora) with
unification-based grammars. Computational Lin-
guistics, 19(1):25?59.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd meeting of
the Association for Computational Linguistics. Ann
Arbor, MI.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora. Montreal, Canada.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 132?139.
Seattle, WA.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics, pages 16?23.
M. Collins. 1999. Head-Driven Models for Natural
Language Parsing. Phd thesis, University of Penn-
sylvania.
J. Gimenez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector
machines. In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Lisbon, Portugal.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002). Vancouver,
BC.
D. E. Knuth. 1965. On the translation of lan-
guages from left to right. Information and Control,
8(6):607?639.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewics.
1993. Building a large annotated corpus of english:
The penn treebank. Computational Linguistics, 19.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics, pages 64?70. Geneva, Switzer-
land.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of the Second Conference on Empir-
ical Methods in Natural Language Processing. Prov-
idence, RI.
B. Roark and E. Charniak. 2002. Measuring effi-
ciency in high-accuracy, broad coverage statistical
parsing. In Proceedings of the Efficiency in Large-
scale Parsing Systems Workshop at COLING-2000.
Luxembourg.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies. Vancouver, BC.
Masaru Tomita. 1987. An efficient augmented
context-free parsing algorithm. Computational Lin-
guistics, 13:31?46.
Masaru Tomita. 1990. The generalized lr
parser/compiler - version 8.4. In Proceedings of
the International Conference on Computational Lin-
guistics (COLING?90), pages 59?63. Helsinki, Fin-
land.
Y. Tsuruoka and K. Tsujii. 2005. Chunk parsing
revisited. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technologies. Vancou-
ver, Canada.
H. Yamada and Yuji Matsumoto. 2003. Statistical de-
pendency analysis using support vector machines.
In Proceedings of the Eighth International Work-
shop on Parsing Technologies. Nancy, France.
698
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 125?132,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Classifier-Based Parser with Linear Run-Time Complexity 
 
 
Kenji Sagae and Alon Lavie 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
{sagae,alavie}@cs.cmu.edu 
 
 
 
 
 
 
 
Abstract 
We present a classifier-based parser that 
produces constituent trees in linear time.  
The parser uses a basic bottom-up shift-
reduce algorithm, but employs a classifier 
to determine parser actions instead of a 
grammar.  This can be seen as an exten-
sion of the deterministic dependency 
parser of Nivre and Scholz (2004) to full 
constituent parsing.  We show that, with 
an appropriate feature set used in classifi-
cation, a very simple one-path greedy 
parser can perform at the same level of 
accuracy as more complex parsers.  We 
evaluate our parser on section 23 of the 
WSJ section of the Penn Treebank, and 
obtain precision and recall of 87.54% and 
87.61%, respectively. 
1 Introduction 
Two classifier-based deterministic dependency 
parsers for English have been proposed recently 
(Nivre and Scholz, 2004; Yamada and Matsumoto, 
2003).  Although they use different parsing algo-
rithms, and differ on whether or not dependencies 
are labeled, they share the idea of greedily pursu-
ing a single path, following parsing decisions made 
by a classifier.  Despite their greedy nature, these 
parsers achieve high accuracy in determining de-
pendencies.  Although state-of-the-art statistical 
parsers (Collins, 1997; Charniak, 2000) are more 
accurate, the simplicity and efficiency of determi-
nistic parsers make them attractive in a number of 
situations requiring fast, light-weight parsing, or 
parsing of large amounts of data.  However, de-
pendency analyses lack important information con-
tained in constituent structures.  For example, the 
tree-path feature has been shown to be valuable in 
semantic role labeling (Gildea and Palmer, 2002). 
We present a parser that shares much of the 
simplicity and efficiency of the deterministic de-
pendency parsers, but produces both dependency 
and constituent structures simultaneously.  Like the 
parser of Nivre and Scholz (2004), it uses the basic 
shift-reduce stack-based parsing algorithm, and 
runs in linear time.  While it may seem that the 
larger search space of constituent trees (compared 
to the space of dependency trees) would make it 
unlikely that accurate parse trees could be built 
deterministically, we show that the precision and 
recall of constituents produced by our parser are 
close to those produced by statistical parsers with 
higher run-time complexity. 
One desirable characteristic of our parser is its 
simplicity.  Compared to other successful ap-
proaches to corpus-based constituent parsing, ours 
is remarkably simple to understand and implement.  
An additional feature of our approach is its modu-
larity with regard to the algorithm and the classifier 
that determines the parser?s actions.  This makes it 
very simple for different classifiers and different 
sets of features to be used with the same parser 
with very minimal work.  Finally, its linear run-
time complexity allows our parser to be considera-
bly faster than lexicalized PCFG-based parsers.  
On the other hand, a major drawback of the classi-
fier-based parsing framework is that, depending on 
125
the classifier used, its training time can be much 
longer than that of other approaches. 
Like other deterministic parsers (and unlike 
many statistical parsers), our parser considers the 
problem of syntactic analysis separately from part-
of-speech (POS) tagging.  Because the parser 
greedily builds trees bottom-up in one pass, con-
sidering only one path at any point in the analysis, 
the task of assigning POS tags to words is done 
before other syntactic analysis.  In this work we 
focus only on the processing that occurs once POS 
tagging is completed.  In the sections that follow, 
we assume that the input to the parser is a sentence 
with corresponding POS tags for each word. 
2 Parser Description 
Our parser employs a basic bottom-up shift-reduce 
parsing algorithm, requiring only a single pass over 
the input string.  The algorithm considers only 
trees with unary and binary branching.  In order to 
use trees with arbitrary branching for training, or 
generating them with the parser, we employ an 
instance of the transformation/detransformation 
process described in (Johnson, 1998).  In our case, 
the transformation step involves simply converting 
each production with n children (where n > 2) into 
n ? 1 binary productions.  Trees must be lexical-
ized1, so that the newly created internal structure of 
constituents with previous branching of more than 
two contains only subtrees with the same lexical 
head as the original constituent.  Additional non-
terminal symbols introduced in this process are 
clearly marked.  The transformed (or ?binarized?) 
trees may then be used for training.  Detransforma-
tion is applied to trees produced by the parser.  
This involves the removal of non-terminals intro-
                                                          
1
 If needed, constituent head-finding rules such as those men-
tioned in Collins (1996) may be used. 
 
                                        Transform 
 
 
                                                                                NP 
 
                    NP                                                               NP*                                          
                                                                                                                                                                                                                              
                                    PP                                                      NP*                                   
                                                                                                                                                                                                                                     
                                           NP                                                       PP                                       
                                                                                                                                                                  
  Det     Adj     N        P         N                                                              NP                                                                                             
                                                                                                                                                                                            
   The    big    dog    with    fleas                    Det   Adj      N       P         N                       
                                                                                                                                                                                                                     
                                                                      The   big    dog    with    fleas                    
                                                                                                                                                                                                                              
                                                                                                                                                                                                                              
                                     Detransform                                                                                                                                                        
                                                                                                                              
                                                                                                                              
Figure 1: An example of the binarization transform/detransform.  The original tree (left) has one 
node (NP) with four children.  In the transformed tree, internal structure (marked by nodes with as-
terisks) was added to the subtree rooted by the node with more than two children.  The word ?dog? 
is the head of the original NP, and it is kept as the head of the transformed NP, as well as the head of 
each NP* node. 
126
duced in the transformation process, producing 
trees with arbitrary branching.  An example of 
transformation/detransformation is shown in figure 
1. 
2.1 Algorithm Outline 
The parsing algorithm involves two main data 
structures: a stack S, and a queue W.  Items in S 
may be terminal nodes (POS-tagged words), or 
(lexicalized) subtrees of the final parse tree for the 
input string.  Items in W are terminals (words 
tagged with parts-of-speech) corresponding to the 
input string.  When parsing begins, S is empty and 
W is initialized by inserting every word from the 
input string in order, so that the first word is in 
front of the queue. 
Only two general actions are allowed: shift and 
reduce.  A shift action consists only of removing 
(shifting) the first item (POS-tagged word) from W 
(at which point the next word becomes the new 
first item), and placing it on top of S.  Reduce ac-
tions are subdivided into unary and binary cases.  
In a unary reduction, the item on top of S is 
popped, and a new item is pushed onto S.  The new 
item consists of a tree formed by a non-terminal 
node with the popped item as its single child.  The 
lexical head of the new item is the same as the 
lexical head of the popped item.  In a binary reduc-
tion, two items are popped from S in sequence, and 
a new item is pushed onto S.  The new item con-
sists of a tree formed by a non-terminal node with 
two children: the first item popped from S is the 
right child, and the second item is the left child.  
The lexical head of the new item is either the lexi-
cal head of its left child, or the lexical head of its 
right child. 
If S is empty, only a shift action is allowed.  If 
W is empty, only a reduce action is allowed.  If 
both S and W are non-empty, either shift or reduce 
actions are possible.  Parsing terminates when W is 
empty and S contains only one item, and the single 
item in S is the parse tree for the input string.  Be-
cause the parse tree is lexicalized, we also have a 
dependency structure for the sentence.  In fact, the 
binary reduce actions are very similar to the reduce 
actions in the dependency parser of Nivre and 
Scholz (2004), but they are executed in a different 
order, so constituents can be built.  If W is empty, 
and more than one item remain in S, and no further 
reduce actions take place, the input string is re-
jected. 
2.2 Determining Actions with a Classifier 
A parser based on the algorithm described in the 
previous section faces two types of decisions to be 
made throughout the parsing process.  The first 
type concerns whether to shift or reduce when both 
actions are possible, or whether to reduce or reject 
the input when only reduce actions are possible.  
The second type concerns what syntactic structures 
are created.  Specifically, what new non-terminal is 
introduced in unary or binary reduce actions, or 
which of the left or right children are chosen as the 
source of the lexical head of the new subtree pro-
duced by binary reduce actions.  Traditionally, 
these decisions are made with the use of a gram-
mar, and the grammar may allow more than one 
valid action at any single point in the parsing proc-
ess.  When multiple choices are available, a gram-
mar-driven parser may make a decision based on 
heuristics or statistical models, or pursue every 
possible action following a search strategy.  In our 
case, both types of decisions are made by a classi-
fier that chooses a unique action at every point, 
based on the local context of the parsing action, 
with no explicit grammar.  This type of classifier-
based parsing where only one path is pursued with 
no backtracking can be viewed as greedy or deter-
ministic. 
In order to determine what actions the parser 
should take given a particular parser configuration, 
a classifier is given a set of features derived from 
that configuration.  This includes, crucially, the 
two topmost items in the stack S, and the item in 
front of the queue W.  Additionally, a set of context 
features is derived from a (fixed) limited number 
of items below the two topmost items of S, and 
following the item in front of W.  The specific fea-
tures are shown in figure 2. 
The classifier?s target classes are parser actions 
that specify both types of decisions mentioned 
above.  These classes are: 
? SHIFT: a shift action is taken; 
? REDUCE-UNARY-XX: a unary reduce ac-
tion is taken, and the root of the new subtree 
pushed onto S is of type XX (where XX is a 
non-terminal symbol, typically NP, VP, PP, 
for example); 
? REDUCE-LEFT-XX: a binary reduce action 
is taken, and the root of the new subtree 
pushed onto S is of non-terminal type XX.  
127
Additionally, the head of the new subtree is 
the same as the head of the left child of the 
root node; 
? REDUCE-RIGHT-XX: a binary reduce ac-
tion is taken, and the root of the new subtree 
pushed onto S is of non-terminal type XX.  
Additionally, the head of the new subtree is 
the same as the head of the right child of the 
root node. 
2.3 A Complete Classifier-Based Parser than 
Runs in Linear Time 
When the algorithm described in section 2.1 is 
combined with a trained classifier that determines 
its parsing actions as described in section 2.2, we 
have a complete classifier-based parser.  Training 
the parser is accomplished by training its classifier.  
To that end, we need training instances that consist 
of sets of features paired with their classes corre-
Let: 
 
 S(n) denote the nth item from the top of the stack S, and 
 W(n) denote the nth item from the front of the queue W. 
 
Features: 
 
? The head-word (and its POS tag) of: S(0), S(1), S(2), and S(3)  
? The head-word (and its POS tag) of: W(0), W(1), W(3) and W(3)  
? The non-terminal node of the root of: S(0), and S(1) 
? The non-terminal node of the left child of the root of: S(0), and S(1) 
? The non-terminal node of the right child of the root of: S(0), and S(1) 
? The non-terminal node of the left child of the root of: S(0), and S(1) 
? The non-terminal node of the left child of the root of: S(0), and S(1) 
? The linear distance (number of words apart) between the head-words of S(0) and S(1) 
? The number of lexical items (words) that have been found (so far) to be dependents of 
the head-words of: S(0), and S(1) 
? The most recently found lexical dependent of the head of the head-word of S(0) that is 
to the left of S(0)?s head 
? The most recently found lexical dependent of the head of the head-word of S(0) that is 
to the right of S(0)?s head 
? The most recently found lexical dependent of the head of the head-word of S(0) that is 
to the left of S(1)?s head 
? The most recently found lexical dependent of the head of the head-word of S(0) that is 
to the right of S(1)?s head 
Figure 2: Features used for classification.  The features described in items 1 ? 7 are more di-
rectly related to the lexicalized constituent trees that are built during parsing, while the fea-
tures described in items 8 ? 13 are more directly related to the dependency structures that are 
built simultaneously to the constituent structures. 
128
sponding to the correct parsing actions.  These in-
stances can be obtained by running the algorithm 
on a corpus of sentences for which the correct 
parse trees are known.  Instead of using the classi-
fier to determine the parser?s actions, we simply 
determine the correct action by consulting the cor-
rect parse trees.  We then record the features and 
corresponding actions for parsing all sentences in 
the corpus into their correct trees.  This set of fea-
tures and corresponding actions is then used to 
train a classifier, resulting in a complete parser. 
When parsing a sentence with n words, the 
parser takes n shift actions (exactly one for each 
word in the sentence).  Because the maximum 
branching factor of trees built by the parser is two, 
the total number of binary reduce actions is n ? 1, 
if a complete parse is found.  If the input string is 
rejected, the number of binary reduce actions is 
less than n ? 1.  Therefore, the number of shift and 
binary reduce actions is linear with the number of 
words in the input string.  However, the parser as 
described so far has no limit on the number of 
unary reduce actions it may take.  Although in 
practice a parser properly trained on trees reflect-
ing natural language syntax would rarely make 
more than 2n unary reductions, pathological cases 
exist where an infinite number of unary reductions 
would be taken, and the algorithm would not ter-
minate.  Such cases may include the observation in 
the training data of sequences of unary productions 
that cycle through (repeated) non-terminals, such 
as A->B->A->B.  During parsing, it is possible that 
such a cycle may be repeated infinitely. 
This problem can be easily prevented by limit-
ing the number of consecutive unary reductions 
that may be made to a finite number.  This may be 
the number of non-terminal types seen in the train-
ing data, or the length of the longest chain of unary 
productions seen in the training data.  In our ex-
periments (described in section 3), we limited the 
number of consecutive unary reductions to three, 
although the parser never took more than two 
unary reduction actions consecutively in any sen-
tence.  When we limit the number of consecutive 
unary reductions to a finite number m, the parser 
makes at most (2n ? 1)m unary reductions when 
parsing a sentence of length n.  Placing this limit 
not only guarantees that the algorithm terminates, 
but also guarantees that the number of actions 
taken by the parser is O(n), where n is the length of 
the input string.  Thus, the parser runs in linear 
time, assuming that classifying a parser action is 
done in constant time. 
3 Similarities to Previous Work 
As mentioned before, our parser shares similarities 
with the dependency parsers of Yamada and Ma-
tsumoto (2003) and Nivre and Scholz (2004) in 
that it uses a classifier to guide the parsing process 
in deterministic fashion.  While Yamada and Ma-
tsumoto use a quadratic run-time algorithm with 
multiple passes over the input string, Nivre and 
Scholz use a simplified version of the algorithm 
described here, which handles only (labeled or 
unlabeled) dependency structures. 
Additionally, our parser is in some ways similar 
to the maximum-entropy parser of Ratnaparkhi 
(1997).  Ratnaparkhi?s parser uses maximum-
entropy models to determine the actions of a shift-
reduce-like parser, but it is capable of pursuing 
several paths and returning the top-K highest scor-
ing parses for a sentence.  Its observed time is lin-
ear, but parsing is somewhat slow, with sentences 
of length 20 or more taking more than one second 
to parse, and sentences of length 40 or more taking 
more than three seconds.  Our parser only pursues 
one path per sentence, but it is very fast and of 
comparable accuracy (see section 4).  In addition, 
Ratnaparkhi?s parser uses a more involved algo-
rithm that allows it to work with arbitrary branch-
ing trees without the need of the binarization 
transform employed here.  It breaks the usual re-
duce actions into smaller pieces (CHECK and 
BUILD), and uses two separate passes (not includ-
ing the POS tagging pass) for determining chunks 
and higher syntactic structures separately. 
Finally, there have been other deterministic 
shift-reduce parsers introduced recently, but their 
levels of accuracy have been well below the state-
of-the-art.  The parser in Kalt (2004) uses a similar 
algorithm to the one described here, but the classi-
fication task is framed differently.  Using decision 
trees and fewer features, Kalt?s parser has signifi-
cantly faster training and parsing times, but its ac-
curacy is much lower than that of our parser.  
Kalt?s parser achieves precision and recall of about 
77% and 76%, respectively (with automatically 
tagged text), compared to our parser?s 86% (see 
section 4).  The parser of Wong and Wu (1999) 
uses a separate NP-chunking step and, like Ratna-
parkhi?s parser, does not require a binary trans-
129
form.  It achieves about 81% precision and 82% 
recall with gold-standard tags (78% and 79% with 
automatically tagged text).  Wong and Wu?s parser 
is further differentiated from the other parsers 
mentioned here in that it does not use lexical items, 
working only from part-of-speech tags. 
4 Experiments 
We conducted experiments with the parser de-
scribed in section 2 using two different classifiers: 
TinySVM (a support vector machine implementa-
tion by Taku Kudo)2, and the memory-based 
learner TiMBL (Daelemans et al, 2004).  We 
trained and tested the parser on the Wall Street 
Journal corpus of the Penn Treebank (Marcus et 
al., 1993) using the standard split: sections 2-21 
were used for training, section 22 was used for de-
velopment and tuning of parameters and features, 
and section 23 was used for testing.  Every ex-
periment reported here was performed on a Pen-
tium IV 1.8GHz with 1GB of RAM. 
Each tree in the training set had empty-node 
and function tag information removed, and the 
                                                          
2
 http://chasen.org/~taku/software/TinySVM 
trees were lexicalized using similar head-table 
rules as those mentioned in (Collins, 1996).  The 
trees were then converted into trees containing 
only unary and binary branching, using the binari-
zation transform described in section 2.  Classifier 
training instances of features paired with classes 
(parser actions) were extracted from the trees in the 
training set, as described in section 2.3.  The total 
number of training instances was about 1.5 million. 
The classifier in the SVM-based parser (de-
noted by SVMpar) uses the polynomial kernel with 
degree 2, following the work of Yamada and Ma-
tsumoto (2003) on SVM-based deterministic de-
pendency parsing, and a one-against-all scheme for 
multi-class classification.  Because of the large 
number of training instances, we used Yamada and 
Matsumoto?s idea of splitting the training instances 
into several parts according to POS tags, and train-
ing classifiers on each part.  This greatly reduced 
the time required to train the SVMs, but even with 
the splitting of the training set, total training time 
was about 62 hours.  Training set splitting comes 
with the cost of reduction in accuracy of the parser, 
but training a single SVM would likely take more 
than one week.  Yamada and Matsumoto experi-
enced a reduction of slightly more than 1% in de-
 
Precision Recall Dependency Time (min) 
Charniak 89.5 89.6 92.1 28 
Collins 88.3 88.1 91.5 45 
Ratnaparkhi 87.5 86.3 Unk Unk 
Y&M - - 90.3 Unk 
N&S - - 87.3 21 
MBLpar 80.0 80.2 86.3 127 
SVMpar 87.5 87.6 90.3 11 
 
Table 1: Summary of results on labeled precision and recall of constituents, dependency accu-
racy, and time required to parse the test set.  The parsers of Yamada and Matsumoto (Y&M) and 
Nivre and Scholz (N&S) do not produce constituent structures, only dependencies.  ?unk? indi-
cates unknown values.  Results for MBLpar and SVMpar using correct POS tags (if automatically 
produced POS tags are used, accuracy figures drop about 1.5% over all metrics).  
 
130
pendency accuracy due to training set splitting, and 
we expect that a similar loss is incurred here. 
When given perfectly tagged text (gold tags ex-
tracted from the Penn Treebank), SVMpar has la-
beled constituent precision and recall of 87.54% 
and 87.61%, respectively, and dependency accu-
racy of 90.3% over all sentences in the test set.  
The total time required to parse the entire test set 
was 11 minutes.  Out of more than 2,400 sen-
tences, only 26 were rejected by the parser (about 
1.1%).  For these sentences, partial analyses were 
created by combining the items in the stack in flat 
structures, and these were included in the evalua-
tion.  Predictably, the labeled constituent precision 
and recall obtained with automatically POS-tagged 
sentences were lower, at 86.01% and 86.15%.  The 
part-of-speech tagger used in our experiments was 
SVMTool (Gim?nez and M?rquez, 2004), and its 
accuracy on the test set is 97%. 
The MBL-based parser (denoted by MBLpar) 
uses the IB1 algorithm, with five nearest 
neighbors, and the modified value difference met-
ric (MVDM), following the work of Nivre and 
Scholz (2004) on MBL-based deterministic de-
pendency parsing.  MBLpar was trained with all 
training instances in under 15 minutes, but its ac-
curacy on the test set was much lower than that of 
SVMpar, with constituent precision and recall of 
80.0% and 80.2%, and dependency accuracy of 
86.3% (24 sentences were rejected).  It was also 
much slower than SVMpar in parsing the test set, 
taking 127 minutes.  In addition, the total memory 
required for running MBLpar (including the classi-
fier) was close to 1 gigabyte (including the trained 
classifier), while SVMpar required only about 200 
megabytes (including all the classifiers). 
Table 1 shows a summary of the results of our 
experiments with SVMpar and MBLpar, and also 
results obtained with the Charniak (2000) parser, 
the Bikel (2003) implementation of the Collins 
(1997) parser, and the Ratnaparkhi (1997) parser.  
We also include the dependency accuracy from 
Yamada and Matsumoto?s (2003) SVM-based de-
pendency parser, and Nivre and Scholz?s (2004) 
MBL-based dependency parser.  These results 
show that the choice of classifier is extremely im-
portant in this task.  SVMpar and MBLpar use the 
same algorithm and features, and differ only on the 
classifiers used to make parsing decisions.  While 
in many natural language processing tasks different 
classifiers perform at similar levels of accuracy, we 
have observed a dramatic difference between using 
support vector machines and a memory-based 
learner.  Although the reasons for such a large dis-
parity in results is currently the subject of further 
investigation, we speculate that a relatively small 
difference in initial classifier accuracy results in 
larger differences in parser performance, due to the 
deterministic nature of the parser (certain errors 
may lead to further errors).  We also believe classi-
fier choice to be one major source of the difference 
in accuracy between Nivre and Scholz?s parser and 
Yamada and Matsumoto?s parser.  
While the accuracy of SVMpar is below that of 
lexicalized PCFG-based statistical parsers, it is 
surprisingly good for a greedy parser that runs in 
linear time.  Additionally, it is considerably faster 
than lexicalized PCFG-based parsers, and offers a 
good alternative for when fast parsing is needed.  
MBLpar, on the other hand, performed poorly in 
terms of accuracy and speed. 
5 Conclusion and Future Work 
We have presented a simple shift-reduce parser 
that uses a classifier to determine its parsing ac-
tions and runs in linear time.  Using SVMs for 
classification, the parser has labeled constituent 
precision and recall higher than 87% when using 
the correct part-of-speech tags, and slightly higher 
than 86% when using automatically assigned part-
of-speech tags.  Although its accuracy is not as 
high as those of state-of-the-art statistical parsers, 
our classifier-based parser is considerably faster 
than several well-known parsers that employ 
search or dynamic programming approaches.  At 
the same time, it is significantly more accurate 
than previously proposed deterministic parsers for 
constituent structures. 
We have also shown that much of the success 
of a classifier-based parser depends on what classi-
fier is used.  While this may seem obvious, the dif-
ferences observed here are much greater than what 
would be expected from looking, for example, at 
results from chunking/shallow parsing (Zhang et 
al., 2001; Kudo and Matsumoto, 2001; Veenstra 
and van den Bosch, 2000). 
Future work includes the investigation of the ef-
fects of individual features, the use of additional 
classification features, and the use of different clas-
sifiers.  In particular, the use of tree features seems 
appealing.  This may be accomplished with SVMs 
131
using a tree kernel, or the tree boosting classifier 
BACT described in (Kudo and Matsumoto, 2004).  
Additionally, we plan to investigate the use of the 
beam strategy of Ratnaparkhi (1997) to pursue 
multiple parses while keeping the run-time linear. 
References 
Charniak, E.  2000.  A maximum-entropy-inspired 
parser.  Proceedings of the First Annual Meeting of 
the North American Chapter of the Association for 
Computational Linguistics.  Seattle, WA. 
Collins, M. 1997.  Three generative, lexicalized models 
for statistical parsing.  Proceedings of the 35th An-
nual Meeting of the Association for Computational 
Linguistics (pp. 16-23).  Madrid, Spain. 
Daelemans, W., Zavrel, J., van der Sloot, K., and van 
den Bosch, A.  2004.  TiMBL: Tilburg Memory 
Based Learner, version 5.1, reference guide.  ILK Re-
search Group Technical Report Series no. 04-02, 
2004. 
Gildea, D., and Palmer, M.  2002.  The necessity of syn-
tactic parsing for predicate argument recognition.  
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (pp. 239-246).  
Philadelphia, PA. 
Kalt, T. 2004.  Induction of greedy controllers for de-
terministic treebank parsers.  Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing.  Barcelona, Spain. 
Kudo, T., and Matsumoto, Y. 2004.  A boosting algo-
rithm for classification of semi-structured text.  Pro-
ceedings of the 2004 Conference on Empirical 
Methods in Natural Language Processing.  Barce-
lona, Spain. 
Kudo, T., and Matsumoto, Y.  2001.  Chunking with 
support vector machines.  Proceedings of the Second 
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics.  Pittsburgh, 
PA. 
Johnson, M. 1998.  PCFG models of linguistic tree rep-
resentations.  Computational Linguistics, 24:613-632. 
Marcus, M. P., Santorini, B., and Marcinkiewics, M. A. 
1993.  Building a large annotated corpus of English: 
the Penn Treebank.  Computational Linguistics, 19. 
Nivre, J., and Scholz, M.  2004.  Deterministic depend-
ency parsing of English text.  Proceedings of the 20th 
International Conference on Computational Linguis-
tics  (pp. 64-70).  Geneva, Switzerland. 
Ratnaparkhi, A. 1997.  A linear observed time statistical 
parser based on maximum entropy models.  Proceed-
ings of the Second Conference on Empirical Methods 
in Natural Language Processing.  Providence, Rhode 
Island. 
Veenstra, J., van den Bosch, A.  2000.  Single-classifier 
memory-based phrase chunking.  Proceedings of 
Fourth Workshop on Computational Natural Lan-
guage Learning (CoNLL 2000).  Lisbon, Portugal. 
Wong, A., and Wu. D. 1999.  Learning a lightweight 
robust deterministic parser.  Proceedings of the Sixth 
European Conference on Speech Communication and 
Technology.  Budapest. 
Yamada, H., and Matsumoto, Y.  2003.  Statistical de-
pendency analysis with support vector machines.  
Proceedings of the Eighth International Workshop on 
Parsing Technologies.  Nancy, France. 
Zhang, T., Damerau, F., and Johnson, D. 2002.  Text 
chunking using regularized winnow.  Proceedings of 
the 39th Annual Meeting of the Association for Com-
putational Linguistics.  Tolouse, France. 
  
132
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 753?760
Manchester, August 2008
Shift-Reduce Dependency DAG Parsing 
Kenji Sagae? 
Institute for Creative Technologies 
University of Southern California 
13274 Fiji Way 
Marina del Rey, CA 90292 
sagae@ict.usc.edu 
Jun?ichi Tsujii 
Department of Computer Science 
University of Tokyo 
School of Computer Science 
University of Manchester 
National Center for Text Mining 
tsujii@is.s.u-tokyo.ac.jp 
 
 
Abstract? 
Most data-driven dependency parsing 
approaches assume that sentence struc-
ture is represented as trees. Although 
trees have several desirable properties 
from both computational and linguistic 
perspectives, the structure of linguistic 
phenomena that goes beyond shallow 
syntax often cannot be fully captured by 
tree representations.  We present a pars-
ing approach that is nearly as simple as 
current data-driven transition-based de-
pendency parsing frameworks, but out-
puts directed acyclic graphs (DAGs). We 
demonstrate the benefits of DAG parsing 
in two experiments where its advantages 
over dependency tree parsing can be 
clearly observed: predicate-argument 
analysis of English and syntactic analysis 
of Danish with a representation that in-
cludes long-distance dependencies and 
anaphoric reference links.  
1 Introduction 
Natural language parsing with data-driven de-
pendency-based frameworks has received an in-
creasing amount of attention in recent years 
(McDonald et al, 2005; Buchholz and Marsi, 
2006; Nivre et al, 2006).  Dependency represen-
tations directly reflect word-to-word relation-
                                               
? ? This work was conducted while the author was at 
the Computer Science Department of the University 
of Tokyo. 
 
? 2008.  Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0).  Some rights reserved.  
ships in a dependency graph, where the words in 
a sentence are the nodes, and labeled edges cor-
respond to head-dependent syntactic relations.  In 
addition to being inherently lexicalized, depen-
dency analyses can be generated efficiently and 
have been show to be useful in a variety of prac-
tical tasks, such as question answering (Wang et 
al., 2007), information extraction in biomedical 
text (Erkan et al, 2007; Saetre et al 2007) and 
machine translation (Quirk and Corston-Oliver, 
2006).  
However, despite rapid progress in the devel-
opment of parsers for several languages (Nivre et 
al., 2007) and algorithms for more linguistically 
adequate non-projective structures (McDonald et 
al., 2005; Nivre and Nilsson, 2006), most of the 
current data-driven dependency parsing ap-
proaches are limited to producing only depen-
dency trees, where each word has exactly one 
head.  Although trees have desirable properties 
from both computational and linguistic perspec-
tives, the structure of linguistic phenomena that 
goes beyond shallow syntax often cannot be fully 
captured by tree representations.  Well-known 
linguistically-motivated dependency-based syn-
tactic frameworks, such as Hudson?s Word 
Grammar (Hudson, 1984), recognize that to 
represent phenomena such as relative clauses, 
control relations and other long-distance depen-
dencies, more general graphs are needed.  Hud-
son (2005) illustrates his syntactic framework 
with the analysis shown in figure 1.  In this ex-
ample, the arcs above the sentence correspond to 
a typical dependency tree commonly used in de-
pendency parsing.  It is clear, however, that the 
entire dependency structure is not a tree, but a 
directed acyclic graph (DAG), where words may 
have one or more heads. The arcs below the sen-
tence represent additional syntactic dependencies 
commonly ignored in current dependency pars-
ing approaches that are limited to producing tree 
753
structures.  There are several other linguistic 
phenomena that cannot be represented naturally 
with dependency trees, but can easily be 
represented with dependency DAGs, including 
anaphoric reference and semantically motivated 
predicate-argument relations.  Although there are 
parsing approaches (often referred to as deep 
parsing approaches) that compute DAG depen-
dency structures, this is usually done through 
more complex lexicalized grammar formalisms 
(such as HPSG, CCG and LFG) and unification 
operations with tree-based parsing algorithms. 
We introduce a new data-driven framework 
for dependency parsing that produces dependen-
cy DAGs directly from input strings, in a manner 
nearly as simple as other current transition-based 
dependency parsers (Nivre et al, 2007) produce 
dependency trees.  By moving from tree struc-
tures to DAGs, it is possible to use dependency 
parsing techniques to address a wider range of 
linguistic phenomena beyond surface syntax.  
We show that this framework is effective and 
efficient in analysis of predicate-argument de-
pendencies represented as DAGs, and in syntac-
tic parsing using DAGs that include long-
distance dependencies, gapping dependents and 
anaphoric reference information, in addition to 
surface syntactic dependents. 
Our parsing framework, based on shift-reduce 
dependency parsing, is presented in section 2.  
Experiments and results are presented and dis-
cussed in section 3.  We review related work in 
section 4, and conclude in section 5. 
2 A shift-reduce parsing framework for 
dependency DAGs 
One of the key assumptions in both graph-based 
(McDonald et al, 2005) and transition-based 
(Nivre, 2004; Nivre and Nilsson, 2006) ap-
proaches to data-driven dependency parsing is 
that the dependency structure produced by the 
parser is a tree, where each word has exactly one 
head (except for a single root word, which has no 
head in the sentence).  This assumption, of 
course, has to be abandoned in dependency DAG 
parsing.  McDonald et al (2006) point out that, 
while exact inference is intractable if the tree 
constraints are abandoned in their graph-based 
parsing framework, it is possible to compute 
more general graphs (such as DAGs) using ap-
proximate inference, finding a tree first, and add-
ing extra edges that increase the graph?s overall 
score.  Our approach, in contrast, extends shift-
reduce (transition-based) approaches, finding a 
DAG directly.  Because data-driven shift-reduce 
dependency parsing is based on local decisions 
(informed by rich a rich feature set), the addi-
tional computational cost of computing DAGs 
instead of trees is small in practice, as we will 
show. 
We first describe how the basic shift-reduce 
bottom-up dependency parsing algorithm de-
scribed by Nivre (2004) can be modified to allow 
multiple heads per word.  We then explore the 
same type of modification to Nivre?s arc-eager 
algorithm, which is a variant of the basic shift-
reduce algorithm where arcs can be created at the 
first opportunity.  Like their tree counterparts, 
our algorithms for dependency DAGs produce 
only projective structures, assuming that projec-
tivity for DAGs is defined in much the same way 
as for trees.  Informally, we define a projective 
DAG to be a DAG where all arcs can be drawn 
above the sentence (written sequentially in its 
original order) in a way such that no arcs cross 
and there are no covered roots (although a root is 
not a concept associated with DAGs, we borrow 
the term from trees to denote words with no 
heads in the sentence).  However, non-
projectivity is predictably more wide-spread in 
DAG representations, since there are at least as 
many arcs as in a tree representation, and often 
more, including arcs that represent non-local re-
lationships.  We then discuss the application of 
pseudo-projective transformations (Nivre and 
Nilsson, 2005) and an additional arc-reversing 
transform to dependency DAGs.  Using a shift-
reduce algorithm that allows multiple heads per 
word and pseudo-projective transformations to-
What do you   think we should wait for ? 
r 
x 
x 
x 
x 
x,c 
s 
s 
s s 
o 
r r 
Figure 1: Word Grammar dependency graph 
(Hudson, 2005). Key for edge types: com-
plement (c), object (o), sharer/xcomp (r), 
subject (s), and extractee (x). 
754
gether forms a complete dependency DAG pars-
ing framework. 
2.1 Basic shift-reduce parsing with multiple 
heads 
The basic bottom-up left-to-right dependency 
parsing algorithm described by Nivre (2004) 
keeps a list of tokens (initialized to contain the 
input string) and a stack (initialized to be empty), 
and allows three types of actions: (1) shift, which 
removes the next item from the input list and 
pushes it onto the top of the stack; (2) left-
reduce, which pops the top two items from the 
stack, creates a left-arc between the words they 
represent in the sentence, and push the top item 
(which is now the head of the item previously 
below it) back onto the stack; and (3) right-
reduce, which works in the same way as left-
reduce, but creates a right-arc instead, and push-
es back onto the stack the item that was below 
the top item on the stack (which is now the head 
of the item previously on top of the stack)1.  New 
dependency edges (or arcs) are only created by 
reduce actions, which are constrained so that 
they can only be applied to create a head-
dependent pair where the dependent has already 
found all of its own dependents (if any).  This is 
necessary because once a word is assigned a 
head it is popped from the stack and never visited 
again, since each word has only one head.  This 
constraint, responsible for the parser?s bottom-up 
behavior, should be kept in mind, as it is relevant 
in the design of the multiple-head parsing algo-
rithm below. 
To allow words to have multiple heads, we 
first need to create two new parser actions that 
create dependency arcs without removing the 
dependent from further consideration for being a 
dependent of additional heads.  The first new 
action is left-attach, which creates a left depen-
dency arc attaching the top two items on the 
stack, making the top item the head of the item 
immediately below, as long as a right arc be-
tween the two items does not already exist.  This 
action is similar to left-reduce, except that nei-
ther item is removed from the stack (no reduction 
occurs).  The second new action, right-attach, 
includes one additional final step: first, it creates 
a right dependency arc between the top two items 
on the stack (as long as a left arc between the two 
items does not already exist), making the top 
item a dependent of the item immediately below; 
                                               
1 Like Nivre (2004), we consider the direction of the 
dependency arc to be from the head to the dependent. 
X Y Z 
Initial state: 
(a) Desired output: 
X Y Z 
Input tokens Stack 
Action: SHIFT 
Y Z 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Current arcs:   X    Y    Z 
Action: SHIFT 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: LEFT-ATTACH 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: SHIFT 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: LEFT-REDUCE 
Z 
Input tokens Stack 
X 
Current arcs: X      Y     Z 
Action: LEFT-REDUCE 
Z 
Input tokens Stack 
Current arcs: X      Y     Z 
X Y Z 
Initial state: 
(b) Desired output: 
X Y Z 
Input tokens Stack 
Action: SHIFT 
Y Z 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Current arcs:   X    Y    Z 
Action: SHIFT 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: SHIFT 
Z 
Y 
Input tokens Stack 
X 
Current arcs:   X    Y    Z 
Action: RIGHT-ATTACH 
Y 
Input tokens Stack 
X 
Current arcs: X      Y     Z 
Action: RIGHT-REDUCE 
X 
Input Stack 
Current arcs: X      Y     Z 
Z 
Z 
Action:  SHIFT 
X 
Input tokens Stack 
Current arcs: X      Y     Z 
Z 
Action: RIGHT-REDUCE 
Z 
Input tokens Stack 
Current arcs: X      Y     Z 
Figure 2: Example of how the basic algorithm 
builds dependencies with multiple heads. 
755
and, as a second step, it pops the top item on the 
stack (newly made a dependent), and places it 
back on the list of input words.  This second step 
is necessary because of the constraint that words 
can only be made dependents once all of its own 
dependents have been found.  The behavior of 
the algorithm is illustrated in figure 2, where (a) 
shows an application of left-attach, and (b) 
shows an application of right-attach.  In (b), we 
note that without placing the dependent in the 
right-attach action (Z) back on the input list, the 
dependency between X and Y could not be 
created.  If we abandon the algorithm?s bottom-
up behavior, it is possible to modify the parser 
actions so that it is not necessary to place items 
back in the input list.  This is discussed in section 
2.2. 
In summary, the algorithm has each of the 
three actions from the tree-based algorithm (shift, 
right-reduce, and left-reduce), and two additional 
actions that allow words to be dependents of 
more than one head (right-attach and left-attach).  
Although the algorithm as described so far builds 
unlabeled structures, the extension to labeled 
structures is straightforward: any action that re-
sults in a new arc being created must also choose 
a label for the arc.  Another way to accomplish 
the same goal is to have a copy of each arc-
producing action for each possible arc label.  
This is the same labeling extension as in the al-
gorithm for trees.  Finally, we note that the algo-
rithm does not explicitly prevent multiple arcs 
(with the same direction) from being created be-
tween the same two words.  In the unlabeled 
case, such a constraint can be easily placed on 
arc-producing actions.  In the labeled case, how-
ever, it is useful to allow arcs with different la-
bels to link the same two words2. 
2.2 Arc-eager shift-reduce parsing with 
multiple heads 
Nivre?s arc-eager algorithm was designed to 
build dependencies at the first opportunity, 
avoiding situations where items that form a chain 
of right arcs all have to be placed on the stack 
before any structure is built, as in figure 2(b) for 
example.  This is done by creating dependencies 
not between the top two items on the stack, but 
between the single top item on the stack and the 
next word on the input list, resulting in a hybrid 
                                               
2 This means that the structures produced by the algo-
rithm are technically not limited to projective DAGs, 
since they can also be projective labeled multi-
digraphs. 
bottom-up/top-down strategy.  A similar idea can 
result in an algorithm for dependencies that allow 
multiple heads per word, but in this case the re-
sulting algorithm is not as similar to the arc-
eager algorithm for trees as the algorithm in sec-
tion 2.1 is to its tree-based counterpart. 
The projective DAG arc-eager algorithm has 
four actions, each corresponding to one action of 
the tree-based algorithm, but only the shift action 
is the same as in the tree based algorithm.  The 
four actions in the new algorithm are: (1) shift, 
which removes the next token from the input 
string and pushes it onto the top of the stack; (2) 
reduce, which pops the stack, removing only its 
top item, as long as that item has at least one 
head (unlike in the tree-based algorithm, howev-
er, the algorithm may not reduce immediately 
when an item that has a head is on the top of the 
stack); (3) left-arc, which creates a left depen-
dency arc between the word on top of the stack 
and the next token in the input string, where the 
token in the input string is the head and the item 
on the stack is the dependent (the stack and input 
list are left untouched), as long as a right arc does 
not already exist between the two words; and (4) 
right-arc, which creates a right dependency arc 
between the word on top of the stack and the 
next token in the input list, where the item on the 
stack is the head and the token in the input list is 
the dependent (again, the stack and input list are 
left untouched), as long as a left arc does not al-
ready exist between the two words. 
Like the algorithm in section 2.1, this algo-
rithm can easily be extended to produce labeled 
structures, and it also allows multiple edges (with 
the same direction) between the same two words. 
2.3 Graph transformations for DAGs 
Although the algorithms presented in sections 2.1 
and 2.2 can produce dependency structures 
where a word may have more than one head, 
they are of limited interest on their own, since 
they can only produce projective structures, and 
many of the interesting linguistic phenomena that 
can be represented with DAGs cannot be 
represented with projective DAGs.  Fortunately, 
the pseudo-projective transformations (Nivre and 
Nilsson, 2006) used in tree-based dependency 
parsing can easily be applied to DAGs.  These 
transformations consist of identifying specific 
non-projective arcs, and moving their heads up 
towards the root, making them projective.  The 
process also involves creating markings on the 
labels of the edges involved, so that the trans-
formations are (mostly) reversible.  Because non-
756
projectivity is more common in linguistically 
interesting DAGs, however, the trans-
form/detransform process may be more lossy 
than it is when applied to trees.  This, of course, 
varies according to specific DAGs used for 
representing specific phenomena.  For pseudo-
transformations to work well, we must allow 
multiple differently labeled arcs between the 
same two words (which, as mentioned before, the 
algorithms do).  Combining the algorithm in sec-
tions 2.1 or 2.2 with pseudo-projective parsing, 
we can use DAG training data and produce DAG 
output in the overall parsing framework. 
An alternative to using pseudo-projective 
transformations is to develop an algorithm for 
DAG parsing based on the family of algorithms 
described by Covington (2001), in the same way 
the algorithms in sections 2.1 and 2.2 were de-
veloped based on the algorithms described by 
Nivre (2004).  Although this may be straightfor-
ward, a potential drawback of such an approach 
is that the number of parse actions taken in a Co-
vington-style algorithm is always quadratic on 
the length of the input sentence, resulting in 
parsers that are more costly to train and to run 
(Nivre, 2007).  The algorithms presented here, 
however, behave identically to their linear run-
time tree counterparts when they are trained with 
graphs that are limited to tree structures.  Addi-
tional actions are necessary only when words 
with more than one head are encountered.  For 
data sets where most words have only one head, 
the performance the algorithms described in sec-
tions 2.1 and 2.2 should be close to that of shift-
reduce projective parsing for dependency trees.  
In data sets where most words have multiple 
heads (resulting in higher arc density), the use of 
a Covington-style algorithm may be advanta-
geous, but this is left as an area of future investi-
gation. 
In addition to pseudo-projective transforma-
tions, an additional transformation that is useful 
in DAG parsing is arc reversal.  This consists of 
simply reversing the direction of an edge, adding 
a special mark to its label to indicate that its di-
rection has been reversed.  Detransformation is 
trivial and can be done with perfect accuracy, 
since it can be accomplished by simply reversing 
the arcs marked as reversed.  This transformation 
is useful in cases where structures are mostly in 
DAG form, but may sometimes contain cycles.  
Arc reversal can be used to change the direction 
of an arc in the cycle, making the previously cyc-
lic structure a DAG, which can be handled in the 
framework presented here. 
3 Experiments 
To investigate the efficacy of our DAG parsing 
framework on natural language data annotated 
with dependency DAGs, we conducted two expe-
riments.  The first uses predicate-argument de-
pendencies taken from the HPSG Treebank built 
by Miyao et al (2004) from the WSJ portion of 
the Penn Treebank.  These predicate-argument 
structures are, in general, dependency graphs that 
do contain cycles (although infrequently), and 
also contain a large number of words with mul-
tiple heads.  Since the predicate-argument de-
pendencies are annotated explicitly in the HPSG 
Treebank, extracting a corpus of gold-standard 
dependency graphs is trivial.  The second expe-
riment uses the Danish Dependency Treebank, 
developed by Kromann (2003).  This treebank 
follows a dependency scheme that includes, in 
addition to standard grammatical relations com-
monly used in dependency parsing, long-distance 
dependencies, gapping dependents, and anaphor-
ic reference links.  As with the HPSG predicate 
argument data, a few structures in the data con-
tain cycles, but most of the structures in the tree-
bank are DAGs.  In the experiments presented 
below, the algorithm described in section 2.1 was 
used.  We believe the use of the arc-eager algo-
rithm described in section 2.2 would produce 
similar results, but this is left as future work. 
3.1 Learning component 
The DAG parsing framework, as described so 
far, must decide when to apply each appropriate 
parser action.  As with other data-driven depen-
dency parsing approaches with shift-reduce algo-
rithms, we use a classifier to make these deci-
sions.  Following the work of Sagae and Tsujii 
(2007), we use maximum entropy models for 
classification.  During training, the DAGs are 
first projectivized with pseudo-projective trans-
formations.  They are then processed by the pars-
ing algorithm, which records each action neces-
sary to build the correct structure in the training 
data, along with their corresponding parser con-
figurations (stack and input list contents).  From 
each of these parser configurations, a set of fea-
tures is extracted and used with the correct pars-
ing action as a training example for the maxi-
mum entropy classifier.  The specific features we 
used in both experiments are the same features 
described by Sagae and Tsujii, with the follow-
ing two changes: (1) the addition of a feature that 
indicates whether an arc already exists between 
the top two items on the stack, or the top item on 
757
the stack and the next item on the input list, and 
if so, what type of arc (direction and label); and 
(2) we did not use lemmas, morphological in-
formation or coarse grained part-of-speech tags.  
For the complete list of features used, please see 
(Sagae and Tsujii, 2007). 
During run-time, the classifier is used to de-
termine the parser action according to the current 
parser configuration.  Like Sagae and Tsujii, we 
use a beam search instead of running the algo-
rithm in deterministic mode, although we also 
report deterministic parsing results. 
3.2 Predicate-argument analysis 
The predicate-argument dependencies extracted 
from the HPSG Treebank include information 
such as extraction, raising, control, and other 
long-distance dependencies.  Unlike in structures 
from PropBank, predicate-argument information 
is provided for nearly all words in the data.  Fol-
lowing previous experiments with Penn Tree-
bank WSJ data, or data derived from it, we used 
sections 02-21 as training material, section 22 for 
development, and section 23 for testing.  Only 
the predicate-argument dependencies were used, 
not the phrase structures or other information 
from the HPSG analyses.  Part-of-speech tagging 
was done separately using a maximum entropy 
tagger (Tsuruoka and Tsujii, 2005) with accuracy 
of 97.1%. 
Cycles were eliminated from the dependency 
structures using the arc reversal transform in the 
following way: for each cycle detected in the 
data, the shortest arc in the cycle was reversed 
until no cycles remained.  We applied pseudo-
projective transformation and detransformation 
to determine how much information is lost in this 
process.  By detransforming the projective 
graphs generated from gold-standard dependen-
cies, we obtain labeled precision of 98.1% and 
labeled recall of 97.7%, which is below the accu-
racy expected for detransformation of syntactic 
dependency trees, but still within a range we 
considered acceptable.  This represents an upper-
bound for the accuracy of the DAG parser (in-
cluding the arc-reversal and pseudo-projective 
transformations, and the algorithm described in 
section 2.1). 
Table 1 shows the results obtained with our 
DAG parsing framework in terms of labeled pre-
cision, recall and F-score (89.0, 88.5 and 88.7, 
respectively).  For comparison, we also show 
previously published results obtained by Miyao 
and Tsujii (2005), and Sagae et al (2007), which 
used the same data, but obtained the predicate-
argument analyses using an HPSG parser.  Our 
results are very competitive, at roughly the same 
level as the best previously published results on 
this data set, but obtained with significantly 
higher speed.  The parser took less than four mi-
nutes to process the test set, and pseudo-
projective and arc-reversal detransformation took 
less than one minute in standard hardware (a Li-
nux workstation with a Pentium 4 processor and 
4Gb of RAM).  Sagae et al (2007) reported that 
an HPSG parser took about 20 minutes to parse 
the same data.  Our results were obtained with a 
beam width of 150 parser states.  Running the 
parser with a beam width of 1 (a single parser 
state), emulating the deterministic search used by 
Nivre (2004), resulted in numerous parse failures 
(the end of the input string is reached, and no 
further dependency arcs are created) in the de-
velopment set, and therefore very low dependen-
cy recall (90.1 precision and 36.2 recall on de-
velopment data).  Finally, in table 1 we also 
show results obtained with standard bottom-up 
shift-reduce dependency parsing for trees, using 
the parser described in (Sagae and Tsujii, 2007).  
To train the dependency tree parser, we trans-
formed the DAG predicate-argument structures 
into trees by removing arcs.  Arcs were selected 
for removal as follows: for each word that had 
more than one head, only the arc between the 
word and its closest head (in linear distance in 
the sentence) was kept.  Although this strategy 
still produces dependency analyses with relative-
ly high F-score (87.0), recall is far lower than 
when DAG parsing is used, and the tree parser 
has no mechanism for capturing some of the 
structures captured by the DAG parser.  
 
Parser Precision Recall F-score 
DAG-beam 89.0 88.5 88.7 
Tree only 89.8 84.3 87.0 
Sagae et al 88.5 88.0 88.2 
Miyao & Tsujii 85.0 84.3 84.6 
 
Table 1: Results from experiments with HPSG 
predicate-argument dependencies (labeled preci-
sion, recall and F-score).  Our results are denoted 
by DAG-beam and tree only, and others are pre-
viously published results using the same data. 
3.3 Danish Dependency Treebank experi-
ments 
Our experiments with the Danish Dependency 
Treebank followed the same setup as described 
for the HPSG predicate-argument structures.  
The accuracy of pseudo-projective transforma-
758
tion and detransformation was higher, at 99.4% 
precision and 98.8% recall.  To divide the data 
into training, development and test sections, we 
followed the same procedure as McDonald et al 
(2006), who used the same data, so our results 
could be compared directly (a small number of 
graphs that contained cycles was discarded, as 
done by McDonald et al). 
Our results are shown in table 2 (unlabeled 
precision and recall are used, for comparison 
with previous work, in addition to labeled preci-
sion and recall), along with the results obtained 
by McDonald et al, who used an approximate 
inference strategy in a graph-based dependency 
parsing framework, where a dependency tree is 
computed first, and arcs that improve on the 
overall graph score are added one by one.  As in 
the previous section, we also include results ob-
tained with tree-only parsing.  Obtaining tree 
structures from the Danish Dependency Tree-
bank is straightforward, since anaphoric refer-
ence and long-distance dependency arcs are 
marked as such explicitly and can be easily re-
moved. 
In addition to overall results, we also meas-
ured the parser?s precision and recall on long-
distance dependencies and anaphoric reference.  
On long-distance dependencies the parser had 
83.2 precision and 82.0 recall.  On anaphoric 
reference links the parser has 84.9 precision and 
84.4 recall.  Although these are below the pars-
er?s overall accuracy figures, they are encourag-
ing results.  Finally, unlike with the HPSG predi-
cate-argument structures, using a beam width of 
1 reduces precision and recall by only about 1.5. 
 
Parser Precision Recall F-score 
DAG-beam 87.3 87.1 87.2 
Tree only 87.5 82.7 85.0 
McDonald et al 86.2 84.9 85.6 
DAG-labeled 82.7 82.2 82.4 
 
Table 2: Results from experiments with the 
Danish Dependency Treebank.  Precision, recall 
and F-score for the first three rows are for unla-
beled dependencies. The last row, DAG-labeled, 
shows our results in labeled precision, recall and 
F-score (not directly comparable to other rows). 
 
4 Related work 
The work presented here builds on the dependen-
cy parsing work of Nivre (2004), as discussed in 
section 2, on the work of Nivre and Nilsson 
(2006) on pseudo-projective transformations, and 
on the work of Sagae and Tsujii (2007) in using a 
beam search in shift-reduce dependency parsing 
using maximum entropy classifiers.  As men-
tioned before, McDonald et al (2006) presented 
an approach to DAG parsing (that could also eas-
ily be applied to cyclic structures) using approx-
imate inference in an edge-factored dependency 
model starting from dependency trees.  In their 
model, the addition of extra arcs to the tree was 
learned with the parameters to build the initial 
tree itself, which shows the power and flexibility 
of approximate inference in graph-based depen-
dency models. 
Other parsing approaches that produce depen-
dency graphs that are not limited to tree struc-
tures include those based on linguistically-
motivated lexicalized grammar formalisms, such 
as HPSG, CCG and LFG.  In particular, Clark et 
al. (2002) use a probabilistic model of dependen-
cy DAGs extracted from the CCGBank (Hock-
enmeier and Steedman, 2007) in a CCG parser 
that builds the CCG predicate-argument depen-
dency structures following the CCG derivation, 
not directly through DAG parsing.  Similarly, the 
HPSG parser of Miyao and Tsujii (2005) builds 
the HPSG predicate-argument dependency struc-
ture following unification operations during 
HPSG parsing.  Sagae et al (2007) use a depen-
dency parsing combined with an HPSG parser to 
produce predicate-argument dependencies.  
However, the dependency parser is used only to 
produce a dependency tree backbone, which the 
HPSG parser then uses to produce the more gen-
eral dependency graph.  A similar strategy is 
used in the RASP parser (Briscoe et al, 2006), 
which builds a dependency graph through unifi-
cation operations performed during a phrase 
structure tree parsing process. 
5 Conclusion 
We have presented a framework for dependency 
DAG parsing, using a novel algorithm for projec-
tive DAGs that extends existing shift-reduce al-
gorithms for parsing with dependency trees, and 
pseudo-projective transformations applied to 
DAG structures. 
We have demonstrated that the parsing ap-
proach is effective in analysis of predicate-
argument structure in English using data from the 
HPSG Treebank (Miyao et al, 2004), and in 
parsing of Danish using a rich dependency repre-
sentation (Kromann, 2003). 
759
Acknowledgements 
We thank Yusuke Miyao and Takuya Matsuzaki 
for insightful discussions.  This work was partial-
ly supported by Grant-in-Aid for Specially Pro-
moted Research (MEXT, Japan). 
References 
Briscoe, T., Carroll, J. and Watson, R. 2006. The 
second release of the RASP system. In Proceed-
ings of the COLING/ACL-06 Demo Session.  
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X 
Shared Task on Multilingual Dependency Parsing.  
In Proceedings of the 10th Conference on Compu-
tational Natural Language Learning (CoNLL-X) 
Shared Task session. 
Clark, Stephen, Julia Hockenmaier, and Mark Steed-
man. 2002. Building Deep Dependency Structures 
using a Wide-Coverage CCG Parser. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL). 
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing.  In Proceedings of 
the Annual ACM Southeast Conference, 95-102. 
Erkan, Gunes, Arzucan Ozgur, and Dragomir R. Ra-
dev. 2007. Semisupervised classification for ex-
tracting protein interaction sentences using depen-
dency parsing. In Proceedings of  CoNLL-EMNLP. 
Hudson, Richard. 1984. Word Grammar. Oxford: 
Blackwell. 
Hudson, Richard. 2005. Word Grammar. In K. Brown 
(Ed.), Encyclopedia of Language and Linguistics 
(second ed., pp 633-642). Elsevier. 
Hockenmaier, Julia and Mark Steedman. 2007. 
CCGbank: a corpus of CCG derivations and de-
pendency structures extracted from the Penn Tree-
bank. In Computational Linguistics 33(3), pp 355-
396, MIT press. 
Kromann, Matthias T. 2003. The Danish dependency 
treebank and the underlying linguistic theory. In 
Proceedings of the Second Workshop on Treebanks 
and Linguistic Theories (TLT). 
McDonald, Ryan and Fernando Pereira. 2006. Online 
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of 
the European Chapter of the Association for Com-
putational Linguistics (EACL). 
McDonald, Ryan, Fernando Pereira, Kiril Ribarov and 
Jan Hajic. 2005. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of the Human Language Technology Confe-
rence and Conference on Empirical Methods in 
Natural Language Processing (HLT-EMNLP). 
Miyao, Yusuke, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2004. Corpus-oriented grammar development 
for acquiring a Head-driven Phrase Structure 
Grammar from the Penn Treebank. In Proceedings 
of the International Joint Conference on Natural 
Language Processing (IJCNLP).  
Miyao Yusuke and Jun'ichi Tsujii. 2005. Probabilistic 
disambiguation models for wide-coverage HPSG 
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics. 
Nivre, Joakim. 2004. Incrementality in Deterministic 
Dependency Parsing. In Incremental Parsing: 
Bringing Engineering and Cognition Together 
(Workshop at ACL-2004).  
Nivre, Joakim, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson, Sebastian Riedel, Deniz 
Yuret. 2007. In Proceedings of the CoNLL 2007 
Shared Task in the Joint Conference on Empirical 
Methods in Natural Language Processing and 
Computational Natural Language Learning.  
Nivre, Joakim. and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL), pp. 99-106. 
Nivre, Joakim. 2007. Incremental non-projective de-
pendency parsing.  In Proceedings of Human Lan-
guage Technologies: The Annual Conference of the 
North American Chapter of the Association for 
Computational Linguistics (NAACL-HLT?07). 
Saetre, R., Sagae, K., and Tsujii, J. 2007. Syntactic 
features for protein-protein interaction extraction.  
In Proceedings of the International Symposium on 
Languages in Biology and Medicine (LBM short 
oral presentations). 
Sagae, Kenji., Yusuke Miyao Jun?ichi and Tsujii. 
2007. HPSG Parsing with shallow dependency 
constraints. In Proceedings of the 44th Meeting of 
the Association for Computational Linguistics. 
Sagae, K., Tsujii, J. 2007. Dependency parsing and 
domain adaptation with LR models and parser en-
sembles. In Proceedings of the CoNLL 2007 
Shared Task. in EMNLP-CoNLL. 
Tsuruoka, Yoshimasa and Tsujii, Jun?ichi. 2005. Bidi-
rectional inference with the easiest-first strategy for 
tagging sequence data. In Proceedings of the Hu-
man Language Technology Conference and Confe-
rence on Empirical Methods in Natural Language 
Processing (HLT-EMNLP), pp. 523-530. 
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy Model? A Quasi-
Synchronous Grammar for QA. In Proceedings of 
the Joint Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL). 
760
Proceedings of NAACL HLT 2009: Short Papers, pages 53?56,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Natural Language Understanding of Partial Speech Recognition
Results in Dialogue Systems
Kenji Sagae and Gwen Christian and David DeVault and David R. Traum
Institute for Creative Technologies, University of Southern California
13274 Fiji Way, Marina del Rey, CA 90292
{sagae,gchristian,devault,traum}@ict.usc.edu
Abstract
We investigate natural language understand-
ing of partial speech recognition results to
equip a dialogue system with incremental lan-
guage processing capabilities for more realis-
tic human-computer conversations. We show
that relatively high accuracy can be achieved
in understanding of spontaneous utterances
before utterances are completed.
1 Introduction
Most spoken dialogue systems wait until the user
stops speaking before trying to understand and re-
act to what the user is saying. In particular, in a
typical dialogue system pipeline, it is only once the
user?s spoken utterance is complete that the results
of automatic speech recognition (ASR) are sent on
to natural language understanding (NLU) and dia-
logue management, which then triggers generation
and synthesis of the next system prompt. While
this style of interaction is adequate for some appli-
cations, it enforces a rigid pacing that can be un-
natural and inefficient for mixed-initiative dialogue.
To achieve more flexible turn-taking with human
users, for whom turn-taking and feedback at the sub-
utterance level is natural and common, the system
needs to engage in incremental processing, in which
interpretation components are activated, and in some
cases decisions are made, before the user utterance
is complete.
There is a growing body of work on incremen-
tal processing in dialogue systems. Some of this
work has demonstrated overall improvements in sys-
tem responsiveness and user satisfaction; e.g. (Aist
et al, 2007; Skantze and Schlangen, 2009). Several
research groups, inspired by psycholinguistic mod-
els of human processing, have also been exploring
technical frameworks that allow diverse contextual
information to be brought to bear during incremen-
tal processing; e.g. (Kruijff et al, 2007; Aist et al,
2007).
While this work often assumes or suggests it is
possible for systems to understand partial user ut-
terances, this premise has generally not been given
detailed quantitative study. The contribution of this
paper is to demonstrate and explore quantitatively
the extent to which one specific dialogue system can
anticipate what an utterance means, on the basis of
partial ASR results, before the utterance is complete.
2 NLU for spontaneous spoken utterances
in a dialogue system
For this initial effort, we chose to look at incremental
processing of natural language understanding in the
SASO-EN system (Traum et al, 2008), a complex
spoken dialog system for which we have a corpus
of user data that includes recorded speech files that
have been transcribed and annotated with a semantic
representation. The domain of this system is a nego-
tiation scenario involving the location of a medical
clinic in a foreign country. The system is intended as
a negotiation training tool, where users learn about
negotiation tactics in the context of the culture and
social norms of a particular community.
2.1 The natural language understanding task
The NLU module must take the output of ASR as
input, and produce domain-specific semantic frames
as output. These frames are intended to capture
much of the meaning of the utterance, although a
53
dialogue manager further enriches the frame rep-
resentations with pragmatic information (Traum,
2003). NLU output frames are attribute-value ma-
trices, where the attributes and values are linked to a
domain-specific ontology and task model.
Complicating the NLU task of is the relatively
high word error rate (0.54) in ASR of user speech
input, given conversational speech in a complex do-
main and an untrained broad user population.
The following example, where the user attempts
to address complaints about lack of power in the pro-
posed location for the clinic, illustrates an utterance-
frame pair.
? Utterance (speech): we are prepared to give
you guys generators for electricity downtown
? ASR (NLU input): we up apparently give you
guys generators for a letter city don town
? Frame (NLU output):
<s>.mood declarative
<s>.sem.agent kirk
<s>.sem.event deliver
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
<s>.sem.theme power-generator
<s>.sem.type event
The original NLU component for this system was
described in (Leuski and Traum, 2008). For the pur-
poses of this experiment, we have developed a new
NLU module and tested on several different data
sets as described in the next section. Our approach
is to use maximum entropy models (Berger et al,
1996) to learn a suitable mapping from features de-
rived from the words in the ASR output to semantic
frames. Given a set of examples of semantic frames
with corresponding ASR output, a classifier should
learn, for example, that when ?generators? appears
in the output of ASR, the value power-generators is
likely to be present in the output frame. The specific
features used by the classifier are: each word in the
input string (bag-of-words representation of the in-
put), each bigram (consecutive words), each pair of
any two words in the input, and the number of words
in the input string.
0102030405060 1
23
45
67
89
1011
1213
1415
1617
1819
2021
2223
24
Length
 
n (word
s)
Number of utterances (bars)
050100150200250300350400450
Cumulative number of 
utterances (line)
Exact
ly n wo
rds
At mo
st n w
ords
Figure 1: Length of utterances in the development set.
2.2 Data
Our corpus consists of 4,500 user utterances spread
across a number of different dialogue sessions. Ut-
terances that were out-of-domain (13.7% of the cor-
pus) were assigned a ?garbage? frame, with no se-
mantic content. Approximately 10% of the utter-
ances were set aside for final testing, and another
10% was designated the development corpus for the
NLU module. The development and test sets were
chosen so that all the utterances in a session were
kept in the same set, but sessions were chosen at ran-
dom for inclusion in the development and test sets.
The training set contains 136 distinct frames,
each of which is composed of several attribute-value
pairs, called frame elements. Figure 1 shows the ut-
terance length distribution in the development set.
2.3 NLU results on complete ASR output
To evaluate NLU results, we look at precision, re-
call and f-score of frame elements. When the NLU
module is trained on complete ASR utterances in
the training set, and tested on complete ASR utter-
ances in the development set, f-score of frame ele-
ments is 0.76, with precision at 0.78 and recall at
0.74. To gain insight on what the upperbound on
the accuracy of the NLU module might be, we also
trained the classifier using features extracted from
gold-standard manual transcription (instead of ASR
output), and tested the accuracy of analyses of gold-
standard transcriptions (which would not be avail-
able at run-time in the dialogue system). Under
these ideal conditions, NLU f-score is 0.87. Training
on gold-standard transcriptions and testing on ASR
output produces results with a lower f-score, 0.74.
54
3 NLU on partial ASR results
Roughly half of the utterances in our training data
contain six words or more, and the average utter-
ance length is 5.9 words. Since the ASR module is
capable of sending partial results to the NLU mod-
ule even before the user has finished an utterance, in
principle the dialogue system can start understand-
ing and even responding to user input as soon as
enough words have been uttered to give the system
some indication of what the user means, or even
what the user will have said once the utterance is
completed. To measure the extent to which our NLU
module can predict the frame for an input utterance
when it sees only a partial ASR result with the first
n words, we examine two aspects of NLU with par-
tial ASR results. The first is correctness of the NLU
output with partial ASR results of varying lengths, if
we take the gold-standard manual annotation for the
entire utterance as the correct frame for any of the
partial ASR results for that utterance. The second is
stability: how similar the NLU output with partial
ASR results of varying lengths is to what the NLU
result would have been for the entire utterance.
3.1 Training the NLU module for analysis of
partial ASR results
The simplest way to performNLU of partial ASR re-
sults is simply to process the partial utterances using
the NLU module trained on complete ASR output.
However, better results may be obtained by train-
ing separate NLU models for analysis of partial ut-
terances of different lengths. To train these sepa-
rate NLU models, we first ran the audio of the utter-
ances in the training data through our ASR module,
recording all partial results for each utterance. Then,
to train a model to analyze partial utterances con-
taining n words, we used only partial utterances in
the training set containing n words (unless the entire
utterance contained less than n words, in which case
we simply used the complete utterance). In some
cases, multiple partial ASR results for a single utter-
ance contained the same number of words, and we
used the last partial result with the appropriate num-
ber of words 1. We trained separate NLU models for
1At run-time, this can be closely approximated by taking
the partial utterance immediately preceding the first partial ut-
terance of length n+ 1.
01020304050607080
1
2
3
4
5
6
7
8
9
10
all
Length
 
n (word
s)
F-score
Traine
d on a
ll data
Traine
d on p
artials
 up to
length
 
n
Traine
d on p
artials
 up to
length
 
n + c
ontex
t
Figure 2: Correctness for three NLU models on partial
ASR results up to n words.
n varying from one to ten.
3.2 Results
Figure 2 shows the f-score for frames obtained by
processing partial ASR results up to length n using
three NLU models. The dashed line is our baseline
NLU model, trained on complete utterances only
(model 1). The solid line shows the results obtained
with length-specific NLU models (model 2), and the
dotted line shows results for length-specific models
that also use features that capture dialogue context
(model 3). Models 1 and 2 are described in the previ-
ous sections. The additional features used in model
3 are unigram and bigram word features extracted
from the most recent system utterance.
As seen in Figure 2, there is a clear benefit to
training NLU models specifically tailored for partial
ASR results. Training a model on partial utterances
with four or five words allows for relatively high f-
score of frame elements (0.67 and 0.71, respectively,
compared to 0.58 and 0.66 when the same partial
ASR results are analyzed using model 1). Consider-
ing that half of the utterances are expected to have
more than five words (based on the length of the ut-
terances in the training set), allowing the system to
start processing user input when four or five-word
partial ASR results are available provides interesting
opportunities. Targeting partial results with seven
words or more is less productive, since the time sav-
ings are reduced, and the gain in accuracy is modest.
The context features used in model 3 did not pro-
vide substantial benefits in NLU accuracy. It is pos-
55
0102030405060708090100
1
2
3
4
5
6
7
8
9
10
Length
 
n of pa
rtial A
SR ou
tput us
ed in m
odel 2
Stability F-score
Figure 3: Stability of NLU results for partial ASR results
up to length n.
sible that other ways of representing context or di-
alogue state may be more effective. This is an area
we are currently investigating.
Finally, figure 3 shows the stability of NLU re-
sults produced by model 2 for partial ASR utter-
ances of varying lengths. This is intended to be an
indication of how much the frame assigned to a par-
tial utterance differs from the ultimate NLU output
for the entire utterance. This ultimate NLU output
is the frame assigned by model 1 for the complete
utterance. Stability is then measured as the F-score
between the output of model 2 for a particular partial
utterance, and the output of model 1 for the corre-
sponding complete utterance. A stability F-score of
1.0 would mean that the frame produced for the par-
tial utterance is identical to the frame produced for
the entire utterance. Lower values indicate that the
frame assigned to a partial utterance is revised sig-
nificantly when the entire input is available. As ex-
pected, the frames produced by model 2 for partial
utterances with at least eight words match closely
the frames produced by model 1 for the complete ut-
terances. Although the frames for partial utterances
of length six are almost as accurate as the frames for
the complete utterances (figure 2), figure 3 indicates
that these frames are still often revised once the en-
tire input utterance is available.
4 Conclusion
We have presented experiments that show that it
is possible to obtain domain-specific semantic rep-
resentations of spontaneous speech utterances with
reasonable accuracy before automatic speech recog-
nition of the utterances is completed. This allows for
interesting opportunities in dialogue systems, such
as agents that can interrupt the user, or even finish
the user?s sentence. Having an estimate of the cor-
rectness and stability of NLU results obtained with
partial utterances allows the dialogue system to es-
timate how likely its initial interpretation of an user
utterance is to be correct, or at least agree with its
ultimate interpretation. We are currently working on
the extensions to the NLU model that will allow for
the use of different types of context features, and in-
vestigating interesting ways in which agents can take
advantage of early interpretations.
Acknowledgments
The work described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
References
G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,
M. Swift, and M. K. Tanenhaus. 2007. Incremental
dialogue system faster than and preferred to its non-
incremental counterpart. In Proc. of the 29th Annual
Conference of the Cognitive Science Society.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and
N. Hawes. 2007. Incremental, multi-level processing
for comprehending situated dialogue in human-robot
interaction. In Language and Robots: Proc. from the
Symposium (LangRo?2007). University of Aveiro, 12.
A. Leuski and D. Traum. 2008. A statistical approach
for text processing in virtual humans. In 26th Army
Science Conference.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proc. of the
12th Conference of the European Chapter of the ACL.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negotia-
tion for multi-modal virtual agents. In Proc. of Intelli-
gent Virtual Agents Conference IVA-2008.
D. Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics,
pages 380?394, January.
56
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 81?84,
Paris, October 2009. c?2009 Association for Computational Linguistics
Analysis of Discourse Structure with Syntactic Dependencies and Data-Driven Shift-Reduce Parsing 
  Kenji Sagae USC Institute for Creative Technologies Marina del Rey, CA 90292 USA sagae@ict.usc.edu 
      Abstract We present an efficient approach for dis-course parsing within and across sen-tences, where the unit of processing is an entire document, and not a single sen-tence.  We apply shift-reduce algorithms for dependency and constituent parsing to determine syntactic dependencies for the sentences in a document, and subse-quently a Rhetorical Structure Theory (RST) tree for the entire document.  Our results show that our linear-time shift-reduce framework achieves high accu-racy and a large improvement in effi-ciency compared to a state-of-the-art ap-proach based on chart parsing with dy-namic programming.  1 Introduction Transition-based dependency parsing using shift-reduce algorithms is now in wide use for de-pendency parsing, where the goal is to determine the syntactic structure of sentences.  State-of-the-art results have been achieved for syntactic analysis in a variety of languages (Bucholz and Marsi, 2006).  In contrast to graph-based ap-proaches, which use edge-factoring to allow for global optimization of parameters over entire tree structures using dynamic programming or maxi-mum spanning tree algorithms (McDonald et al, 2005) transition-based models are usually opti-mized at the level of individual shift-reduce ac-tions, and can be used to drive parsers that pro-duce competitive accuracy using greedy search strategies in linear time. Recent research in data-driven shift-reduce parsing has shown that the basic algorithms used for determining dependency trees  (Nivre, 2004) can be extended to produce constituent structures (Sagae and Lavie, 2005), and more general de-
pendency graphs, where words can be linked to more than one head (Henderson et al, 2008; Sa-gae and Tsujii, 2008).  A remarkably similar parsing approach, which predates the current wave of interest in data-driven shift-reduce pars-ing sparked by Yamada and Matsumoto (2003) and Nivre and Scholz (2004), was proposed by Marcu (1999) for data-driven discourse parsing, where the goal is to determine the rhetorical structure of a document, including relationships that span multiple sentences.  The linear-time shift-reduce framework is particularly well suited for discourse parsing, since the length of the in-put string depends on document length, not sen-tence length, making cubic run-time chart pars-ing algorithms often impractical. Soricut and Marcu (2003) presented an ap-proach to discourse parsing that relied on syntac-tic information produced by the Charniak (2000) parser, and used a standard bottom-up chart pars-ing algorithm with dynamic programming to determine discourse structure.  Their approach greatly improved on the accuracy of Marcu?s shift-reduce approach, showing the value of us-ing syntactic information in discourse analysis, but recovered only discourse relations within sentences.   We present an efficient approach to discourse parsing using syntactic information, inspired by Marcu?s application of a shift-reduce algorithm for discourse analysis with Rhetorical Structure Theory (RST), and Soricut and Marcu?s use of syntactic structure to help determine discourse structure.  Our transition-based discourse parsing framework combines elements from Nivre (2004)?s approach to dependency parsing, and Sagae and Lavie (2005)?s approach to constituent parsing.  Our results improve on accuracy over existing approaches for data-driven RST parsing, while also improving on speed over Soricut and Marcu?s chart parsing approach, which produces state-of-the-art results for RST discourse rela-tions within sentences. 
81
2 Discourse analysis with the RST Dis-course Treebank The discourse parsing approach presented here is based on the formalization of Rhetorical Struc-ture Theory (RST) (Mann and Thompson, 1988) used in the RST Discourse Treebank (Carlson et al, 2003).  In this scheme, the discourse structure of a document is represented as a tree, where the leaves are contiguous spans of text, called ele-mentary discourse units, or EDUs.  Each node in the tree corresponds to a contiguous span of text formed by concatenation of the spans corre-sponding to the node?s children, and represents a rhetorical relation (attribution, enablement, elaboration, consequence, etc.) between these text segments.  In addition, each node is marked as a nucleus or as a satellite, depending on whether its text span represents an essential unit of information, or a supporting or background unit of information, respectively.  While the no-tions of nucleus and satellite are in some ways analogous to head and dependent in syntactic dependencies, RST allows for multi-nuclear rela-tions, where two nodes marked as nucleus can be linked into one node. Our parsing framework includes three compo-nents: (1) syntactic dependency parsing, where standard techniques for sentence-level parsing are applied; (2) discourse segmentation, which uses syntactic and lexical information to segment text into EDUs; and (3) discourse parsing, which produces a discourse structure tree from a string of EDUs, also benefiting from syntactic informa-tion.  In contrast to the approach of Soricut and Marcu (2003), which also includes syntactic parsing, discourse segmentation and discourse parsing, our approach assumes that the unit of processing for discourse parsing is an entire document, and that discourse relations may exist within sentences as well as across sentences, while Soricut and Marcu?s processes one sen-tence at a time, independently, finding only dis-course relations within individual sentences.  Parsing entire documents at a time is made pos-sible in our approach through the use of linear-time transition-based parsing.  An additional mi-nor difference is that in our approach syntactic information is represented using dependencies, while Soricut and Marcu used constituent trees. 2.1 Syntactic parsing and discourse seg-mentation Assuming the document has been segmented into sentences, a task for which there are approaches 
with very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sen-tence.  This includes part-of-speech (POS) tag-ging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Nivre, 2004) trained with the averaged perceptron (Collins, 2002).  The dependency parser is also trained with the WSJ Penn Treebank, converted to de-pendencies using the head percolation rules of Yamada and Matsumoto (2003). Discourse segmentation is performed as a bi-nary classification task on each word, where the decision is whether or not to insert an EDU boundary between the word and the next word.  In a sentence of length n, containing the words w1, w2 ? wn, we perform one classification per word, in order.  For word wi, the binary choice is whether to insert an EDU boundary between wi and wi+1.  The EDUs are then the words between EDU boundaries (assuming boundaries exist in the beginning and end of each sentence).   The features used for classification are: the current word, its POS tag, its dependency label, and the direction to its head (whether the head appears before or after the word); the previous two words, their POS tags and dependency la-bels; the next two words, their POS tags and de-pendency labels; the direction from the previous word to its head; the leftmost dependent to the right of the current word, and its POS tag; the rightmost dependent to the left of the current word, and its POS tag; whether the head of the current word is between the previous EDU boundary and the current word; whether the head of the next word is between the previous EDU boundary and the current word.  In addition, we used templates that combine these features (in pairs or triples).  Classification was done with the averaged perceptron. 2.2 Transition-based discourse parsing RST trees can be represented in a similar way as constituent trees in the Penn Treebank, with a few differences: the trees represent entire docu-ments, instead of single sentences; the leaves of the trees are EDUs consisting of one or more contiguous words; and the node labels contain nucleus/satellite status, and possibly the name of a discourse relation.  Once the document has been segmented into a sequence of EDUs, we use a transition-based constituent parsing ap-proach (Sagae and Lavie, 2005) to build an RST tree for the document. 
82
Sagae and Lavie?s constituent parsing algo-rithm uses a stack that holds subtrees, and con-sumes the input string (in our case, a sequence of EDUs) from left to right, using four types of ac-tions: (1) shift, which removes the next token from the input string, and pushes a subtree con-taining exactly that token onto the stack; (2) re-duce-unary-LABEL, which pops the stack, and push onto it a new subtree where a node with label LABEL dominates the subtree that was popped (3) reduce-left-LABEL, and (4) reduce-right-LABEL, which each pops two items from the stack, and pushes onto it a new subtree with root LABEL, which has as right child the subtree previously on top of the stack, and as left child the subtree previously immediately below the top of the stack.  The difference between reduce-left and reduce-right is whether the head of the new subtree comes from the left or right child.  The algorithm assumes trees are lexicalized, and in our use of the algorithm for discourse parsing, heads are entire EDUs, and not single words. Our process for lexicalization of discourse trees, which is required for the parsing algorithm to function properly, is a simple percolation of ?head EDUs,? performed in the same way as lexical heads can be assigned in Penn Treebank-style trees using a head percolation table (Collins, 1999).  To determine head EDUs, we use the nucleus/satellite status of nodes, as fol-lows: for each node, the leftmost child with nu-cleus status is the head; if no child is a nucleus, the leftmost satellite is the head.  Most nodes have exactly two children, one nucleus and one satellite.  The parsing algorithm deals only with binary trees.  We use the same binarization trans-form as Sagae and Lavie, converting the trees in the training set to binary trees prior to training the parser, and converting the binary trees pro-duced by the parser at run-time into n-ary trees.   As with the dependency parser and discourse segmenter, learning is performed using the aver-aged perceptron.  We use similar features as Sa-gae and Lavie, with one main difference: since there is usually no single head-word associated with each node, but a EDU that contains a se-quence of words, we use the dependency struc-ture of the EDU to determine what lexical fea-tures and POS tags should be used as features associated with each RST tree node.  In place of the head-word and POS tag of the top four items on the stack, and the next four items in the input, we use subsets of the words and POS tags in the EDUs for each of those items.  The subset of words (and POS tags) that represent an EDU 
contain the first two and last words in the EDU, and each word in the EDU whose head is outside of the EDU.  In the vast majority of EDUs, this subset of words with heads outside the EDU (the EDU head set) contains a single word.  In addi-tion, we extract these features for the top three (not four) items on the stack, and the next three (not four) words in the input.  For the top two items on the stack, in addition to subsets of words and POS tags described above, we also take the words and POS tags for the leftmost and rightmost children of each word in the EDU head set.  Finally, we use feature templates that com-bine these and other individual features from Sa-gae and Lavie, who used a polynomial kernel and had no need for such templates (at the cost of increased time for both training and running). 3 Results To test our discourse parsing approach, we used the standard training and testing sections of the RST Discourse Treebank and the compacted 18-label set described by Carlson et al (2003).  We used approximately 5% of the standard training set as a development set. Our part-of-speech tagger and syntactic parser were not trained using the standard splits of the Penn Treebank for those tasks, since there are documents in the RST Discourse Treebank test section that are included in the usual training sets for POS taggers and parsers.  The POS tagger and syntactic parser were then trained on sec-tions 2 to 21 of the WSJ Penn Treebank, exclud-ing the specific documents used in the test sec-tion of the RST Discourse Treebank. Table 1 shows the precision, recall and f-score of our discourse segmentation approach on the test set, compared to that of Soricut and Marcu (2003) and Marcu (1999).  In all cases, results were obtained with automatically produced syn-tactic structures.  We also include the total time required for syntactic parsing (required in our 
 Prec. Recall F-score Time Marcu99  83.3 77.1 80.1 - S&M03 83.5 82.7 83.1 361s this work 87.4 86.0 86.7 40s  Table 1: Precision, recall, f-score and time for discourse segmenters, tested on the RST Discourse Treebank.  Time includes syntactic parsing, Charniak (2000) for S&M03, and our implemetation of Nivre arc-standard for our segmenter. 
83
segmentation approach and Soricut and Marcu?s) and segmentation.  For comparison with previous results, we include only segmentation within sen-tences (if all discourse boundaries are counted, including sentence boundaries, our f-score is 92.9). Using our discourse segmentation and transi-tion-based discourse parsing approach, we obtain 42.9 precision and 46.2 recall (44.5 f-score) for all discourse structures in the test set.  Table 2 shows f-score of labeled bracketing for discourse relations within sentences only, for comparison with previously published results.  We note that human performance on this task has f-score 77.0. While our f-score is still far below that of hu-man performance, we have achieved a large gain in speed of processing compared to a state-of-the-art approach. 4 Conclusion We have presented an approach to discourse analysis based on transition-based algorithms for dependency and constituent trees.  Dependency parsing is used to determine the syntactic struc-ture of text, which is then used in discourse seg-mentation and parsing.  A simple discriminative approach to segmentation results in an overall improvement in discourse parsing f-score, and the use of a linear-time algorithm results in an a large improvement in speed over a state-of-the-art approach. Acknowledgments The work described here has been sponsored by the U.S. Army Research, Development, and En-gineering Command (RDECOM). Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred. 
References Buchholz, S. and Marsi, E. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL 2006 Shared Task. Carlson, L., Marcu, D., and Okurowski, M. E. 2003. Building a discourse-tagged corpus in the frame-work of Rhetorical Structure Theory. In J. van Kuppevelt and R. W. Smith, editors, Current and New Directions in Discourse and Dialogue. Klu-wer Academic Publishers. Charniak, E. 2000. A maximum-entropy-inspired parser.  In Proc. of NAACL. Collins, M. 1999. Head-driven statistical models for natural language processing.  PhD dissertation, University of Pennsylvania. Collins, M. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms. In Proc. of EMNLP.  Philadelphia, PA. Gillick, D. 2009. Sentence Boundary Detection and the Problem with the U.S. In Proc. of the NAACL HLT: Short Papers. Boulder, Colorado. Henderson, J., Merlo, P., Musillo, G., Titov, I. 2008. A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies. In Proc. of CoNLL 2008 Shared Task, Manchester, UK. Mann, W. C. and Thompson, S. A. 1988. Rhetorical Structure Theory: toward a functional theory of text organization. Text, 8(3):243-281.  Marcu, D. 1999. A decision-based approach to rhe-torical parsing. In Proc. of the Annual Meeting of the Association for Computational Linguistics. McDonald, R., Pereira, F., Ribarov, K., and Hajic, J. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLT/EMNLP. Nivre, J. 2004. Incrementality in Deterministic De-pendency Parsing. In Incremental Parsing: Bring-ing Engineering and Cognition Together (work-shop at ACL-2004).  Barcelona, Spain.  Nivre, J. and Scholz, M. 2004. Deterministic Depend-ency Parsing of English Text. In Proc. of COLING. Sagae, K. and Lavie, A. 2005.  A classifier-based parser with linear run-time complexity.  In Proc. of IWPT. Sagae, K. and Tsujii, J. 2008. Shift-reduce depend-ency DAG parsing. In Proc. of COLING. Soricut, R. and Marcu, D. 2003. Sentence level dis-course parsing using syntactic and lexical informa-tion. In Proc. of NAACL. Edmonton, Canada. Yamada, H. and Matsumoto, Y. 2003. Statistical de-pendency analysis with support vector machines. In Proc. of IWPT. 
  F-score Time Marcu99  37.2 - S&M03 49.0 481s this work 52.9 69s human 77.0 -  Table 2: F-score for bracketing of RST dis-course trees on the test set of the RST Dis-course Treebank, and total time (syntactic parsing, segmentation and discourse parsing) required to parse the test set (S&M03 and our approach were run on the same hardware).  
84
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 192?201,
Paris, October 2009. c?2009 Association for Computational Linguistics
Clustering Words by Syntactic Similarity Improves Dependency   Parsing of Predicate-Argument Structures 
 Kenji Sagae and Andrew S. Gordon Institute for Creative Technologies University of Southern California 13274 Fiji Way Marina del Rey, CA 90292 {sagae,gordon}@ict.usc.edu     Abstract 
We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntac-tic contexts.  We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a dif-ferent level of sentence structure.  In our ex-periments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Introduction Syntactic parsing of natural language has ad-vanced greatly in recent years, in large part due to data-driven techniques (Collins, 1999; Charniak, 2000; Miyao and Tsujii, 2005; McDonald et al, 2005; Nivre et al, 2007) cou-pled with the availability of large treebanks. Sev-eral recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available.  Two research directions that have produced promising improvements on the accu-racy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al, 2007) and stacking (Nivre and McDonald, 2008; Martins et al, 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a 
treebank (McClosky et al, 2006; Koo et al, 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure.  In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al, 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al, 2008), we use a large corpus of constituent trees (previously gen-erated by an accurate constituent parser), which we use to produce syntactically derived clusters that are then used to improve a transition-based parser that outputs dependency graphs that re-flect predicate-argument structure where words may be dependents of more than one parent.  This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al, 2008), and is suitable for repre-senting both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al, 1992) or by immediate dependency context (Lin, 1998).  Un-like in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic characteristics of lexical items, we use unlexicalized syntactic context, so that words are clustered based only on their syntactic behavior.  This way, we at-tempt to generate clusters that are more concep-tually similar to part-of-speech tags or supertags 
192
(Bangalore and Joshi, 1999), but organized hier-archically to provide tagsets with varying levels of granularity. Our second contribution is a methodology for leveraging a high-accuracy parser to improve the accuracy of a parser that uses a different formal-ism (that represents different structural informa-tion), without the need to process the input with both parsers at run-time.  In our experiments, we show that we can improve the accuracy of a fast dependency parser for predicate-argument struc-tures by using a corpus which was previously automatically annotated using a highly accurate but considerably slower phrase-structure tree parser.  This is accomplished by using the slower parser only to parse the data used to create the syntactic word clusters.  During run-time, the dependency parser uses these clusters, which encapsulate syntactic knowledge from the phrase-structure parser.  Although our experi-ments focus on the use of phrase-structure and dependency parsers, the same framework can be easily applied to data-driven parsing using other syntactic formalisms, such as CCG or HPSG. 2 Clustering by Syntactic Similarity We developed a new approach to clustering words according to their syntactic similarity. Our method involves the use of hierarchical agglom-erate clustering techniques using the calculated syntactic distance between words. Syntactic dis-tance between words is computed as the cosine distance between vector representations of the frequency of unique parse tree paths emanating from the word in a corpus of parse trees. In this research, we employ a novel encoding of syntac-tic parse tree paths that includes direction infor-mation and non-terminal node labels, but does not include lexical information or part-of-speech tags. Consequently, the resulting hierarchy groups words that appear in similar places in similar parse trees, regardless of its assigned part-of-speech tag.  In this section we describe our approach in detail. 2.1 Parse tree path representation Gordon and Swanson (2007) first described a corpus-based method for calculating a measure of syntactic similarity between words, and dem-onstrated its utility in improving the performance of a syntax-based Semantic Role Labeling sys-tem. The central idea behind their approach was that parse tree paths could be used as features for describing a word?s grammatical behavior. 
Parse tree paths are descriptions of tree transi-tions from a terminal (e.g. a verb) to a different node in a constituent parse tree of a sentence. Parse tree paths gained popularity in early Se-mantic Role Labeling research (Gildea and Juraf-sky, 2002), where they were used as features de-scribing the relationship between a verb and a particular semantic role label. For example, Fig-ure 1 illustrates a parse tree path between a verb and a semantically related noun phrase. Gordon and Swanson viewed parse tree paths as features that could be used to describe the syn-tactic contexts of words in a corpus. In their ap-proach, all of the possible parse tree paths that begin at a given word were identified in a large set of automatically generated constituent parse trees. The normalized frequency counts of unique parse tree paths were combined into a feature vector that describes the location that the given word appears in the set of parse trees. This syntactic profile was then compared with other profiles using a cosine distance function, produc-ing a quantitative value of word similarity. In this manner, the syntactic similarity between the verb ?pluck? and the verb ?whisk? was calcu-lated as 0.849. One drawback of the approach of Gordon and Swanson was the inclusion of part-of-speech tags in the encoding of the parse tree paths. As a con-sequence, the cosine distance between words of different classes was always zero, regardless of their similarities in the remainder of the paths. To correct this problem in our current research, we removed part-of-speech tags from the encod-ing of parse tree paths, deleting the tag that be-gins each path and replacing tags when they ap-pear at the end of a path with a generic terminal label.  A second drawback of the approach of Gordon and Swanson is that the path directionality is un-derspecified. Consider the parse tree paths that 
 Figure 1: An example parse tree path from the verb ate to the argument NP He, repre-sented as ?VBD?VP?S?NP.  
193
emanate from each of the words ?some? and ?pancakes? in Figure 1. In the original encoding, the paths for each of these words would be iden-tical (if the part of speech tags were removed), despite their unique locations in this parse tree. To correct this problem in our current research, we elaborated the original set of two path identi-fiers (? and ?) to a set of six tags that included information about the direction of the transition. Up-Right (?) and Down-Left (?) transition are used to and from nodes that are the first constitu-ent of a non-terminal. Up-Left (?) and Down-Right (?) transitions are used to and from nodes that are the last constituent of a non-terminal. Transitions to and from all other constituent nodes are labeled Up-Middle (?) or Down-Middle (?), accordingly. For example, we repre-sent the parse tree path depicted in Figure 1 as: ?VP?S?NP. 2.2 Profiles for BLLIP WSJ Corpus words As in the previous work of Gordon and Swanson (2007), we characterize the syntactic properties of words as the normalized frequency of unique parse tree paths emanating from the word in a large corpus of syntactic parse trees.  In our research, we used the Brown Labora-tory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1 (Charniak et al, 2000), which contains approxi-mately 30 million words of Wall Street Journal news articles, parsed with Charniak (2000) parser.  Although the trees in the BLLIP corpus are enriched with function tags and empty nodes, we remove this information, leaving only the trees produced by the Charniak parser.  We iden-tified the top five thousand most frequent words (or, more generally, types, since these also in-clude other sequences of characters, such as numbers and punctuation) in this corpus, treating words that differed in capitalization or in as-signed part-of-speech tag as separate types. These five thousand types correspond to ap-proximately 85% of the tokens in the BLLIP corpus.  For each token instance of each of these five thousand types, we identified every occur-ring parse tree path emanating from the token in each of the sentences in which it appeared. The most frequent type was the comma, which ap-peared 2.2 million times and produced 118 mil-lion parse tree paths. The least frequent token in this set was the singular noun ?pollution,? with 731 instances producing 35,185 parse tree paths. To generate syntactic profiles for a given type, the frequency of unique parse tree paths was ta-
tabulated, and then normalized by dividing this frequency by the number of tokens of that type in the corpus. To reduce the dimensionality of these normalized frequency vectors, parse tree paths that appeared in less than 0.2% of the instances were ignored. This threshold value produced vectors with dimensionality that was comparable across all five thousand types, and small enough to process given our available computational re-sources.  The mean vector size was 2,228 dimen-sions, with a standard deviation of 734. 2.3 Distance calculation and clustering Pairwise distances between each of the five thou-sand types were computed as the cosine distance between their profile vectors. We then grouped similar types using hierarchical agglomerate clustering techniques, where distance between clusters is calculated as mean distance between elements of each cluster (average link cluster-ing).  The three most similar types (the first 2 clus-tering steps) consisted of the capitalized subordi-nating conjunctions ?Although,? ?While,? and ?Though.? The two most dissimilar types (the last to be included in any existing cluster) were the symbol ?@? and the question mark. 2.4 Cluster label selection Hierarchical agglomerate clustering produces a binary-branching tree structure, where each branch point is ordered according to a similarity value between 0 and 1. In our clustering of the top five thousand most frequent types in the BLLIP corpus, there are five thousand leaf nodes representing individual tokens, and 4999 branch points that cluster these types into a single tree. We label each of these 4999 branch points, and treat these cluster labels as features of the types that they dominate. For example, the singular noun ?house? participates in 114 clusters of in-creasing size. The syntactic features of this type can therefore be characterized by 114 cluster la-bels, which overlap with varying degrees with other tokens in the set. We view these cluster labels as conceptually similar to traditional part-of-speech tags in that they are indicative of the syntactic contexts in which words are likely to appear.  Because words are clustered based on their unlexicalized syntactic contexts, the resulting clusters are more likely to reflect purely syntactic information than are clusters derived from lexical context, such as adjacent words (Brown et al, 1992) or immedi-ate head-word (Lin, 1998).  However, the extent 
194
to which these syntactic contexts are specified can vary from a more general to a more fine-grained level than that of parts-of-speech.  As clusters become more fine-grained, they become more similar to supertags (Bangalore and Joshi, 1999).  Clusters that represent more specific syn-tactic contexts can encode information about, for example, subcategorization.  As these labels are derived empirically from a large corpus of syn-tactic parse trees, they accurately represent syn-tactic distinctions in real discourse at different granularities, in contrast to the single arbitrary granularity of theoretically derived part-of-speech tags used in existing treebanks (Marcus et al, 1993).  While it is sometimes useful to view types as having multiple part-of-speech tags at different levels of granularity (e.g. the 114 tags for the token ?house?), it is often useful to select a sin-gle level of granularity to use across all tokens. For example, it is useful to know which one of the 114 cluster labels for ?house? to use if ex-actly 100 part-of-speech distinctions are to be made among tokens in the set. These cluster la-bels can be identified by slicing the tree at the level for which there are exactly 100 branches, then using the label of the first branch point in each branch as the label for all of its leaf-node types, or the leaf-node itself in the case where no further branching exists. Given our hierarchical clustering, there are five thousand different ways to slice the tree in this manner, yielding sets of cluster labels (and un-clustered types) that vary in size from 1 to 5000. We identified these sets for use in the experiments described in the next sections. Figure 2 shows a dendrogram representation of the cluster tree when it is sliced to produce exactly 60 clusters, 19 of which are individual types. For the other 41 clusters, we show only the most frequent word in the cluster and the number of additional words in the cluster.  The scale line in the lower left of Figure 2 indicates the horizontal length of a calculated similarity between clusters of 0.1. 3 Transition-based dependency parsing with word clusters The clusters obtained with the approach de-scribed in section 2 provide sets of syntactic tags with varying levels of granularity.  Previous work by Koo et al (2008) and Miller et al (2004) suggests that different levels of cluster granularity may be useful in natural language 
   Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 
195
processing tasks with discriminative training.  We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision.  This transi-tion-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al, 2004; Hall et al, 2007) and predicate-argument parsing (Hender-son et al, 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementa-tion of Sagae and Tsujii (2008)?s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument de-pendencies extracted from the HPSG Treebank developed by Miyao et al (2004).  Using this data set alows for a comparison of our results with those obtained in previous work on data-driven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvement, since the clusters were derived from syntactic trees in Penn Treebank format (as produced by the Char-niak parser, without empty nodes, co-indexation or function tags), and used in the identification of HPSG Treebank predicate-argument dependencies.  Figure 3 shows a predicate-argument dependency structure following the annotation standard of the HPSG Treebank, where arrows point from head to modifier.  We note that unlike in the widely known PropBank (Palmer et al, 2005) predicate-argument struc-tures, argument labels start from ARG1 (not ARG0), and predicate-argument relationships are annotated for all words.  One difference between in our implementation is that, instead of maxi-mum entropy classification used by Sagae and Tsujii, we perform parser action classification using the averaged perceptron (Freund and 
Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii?s fea-tures, in addition to a set of cluster-based fea-tures, while retaining fast training times. We now describe the parsing approach, start-ing with the dependency DAG parser that we use as a baseline, followed by how the syntactic clus-ter features were added to the baseline parser. 3.1 Arc-standard parsing for dependency DAGs Sagae and Tsujii (2008) describe two algorithms for dependency parsing with words that have multiple heads.  Each corresponds to extensions of Nivre (2004)?s arc-standard and arc-eager al-gorithms for dependency (tree) parsing.  In our experiments, we used an implementation of the arc-standard extension.   Nivre?s arc-standard dependency parsing algo-rithm uses a stack to process the input string one word at a time, from left to right, using two gen-eral types of parser action: shift (push the next input token onto the stack), and reduce (create a dependency arc between the top two items on the stack, and pop the item marked as the depend-ent).  Reduce actions are subdivided into reduce-right and reduce-left, indicating which of the two items on the top of the stack is the head, and which is the dependent in the newly formed de-pendency arc.  These two reduce actions can be further subdivided to reflect what type of de-pendency arc is created, in the case of labeled dependency parsing.  The extension for allowing multiple heads per word consists of the addition a new type of parser action: attach, which creates a dependency arc without removing anything from the stack.  As with reduce actions, there are two types of attach: attach-left which creates a dependency arc between the top two items on the stack such that the item on top is the head, and 
           Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. 
196
right-attach, which creates a dependency arc be-tween the top two items on the stack such that the top item is the dependent, then pops it from the stack and unshifts it back into the input.  Fi-nally, this algorithm for unlabeled graphs can be extended to produce labeled dependencies in the same way as Nivre?s algorithm, by replacing the reduce and attach actions with sets of actions that perform the reduce or attach operation and also name the label of the arc created.  Sagae and Tsujii (2008) provide a more detailed description of the algorithm, including an example that illus-trates the new attach actions. This basic algorithm is only capable of pro-ducing labeled directed acyclic graphs where, if the nodes (which correspond to words) are placed on a left to right sequence on a horizontal line in the order in which the words appear in the input sentence, all arcs can be drawn above the nodes without crossing.  This corresponds to the notion of projectivity that similarly limits the types of trees produced by Nivre?s algorithm.  Just as in dependency parsing with tree struc-tures, a way to effectively remove this limitation is the use of pseudo-projective transformations (Nivre and Nilsson, 2005), where arcs that cross have their heads moved towards the root and have their labels edited to reflect this change, often making it reversible.  Once crossing arcs have been ?lifted? so that no crossing arcs re-main, the ?projectivized? structures are used to train a parsing model.  Projective structures pro-duced by this model can be ?deprojectivized? through the use of the edits in the arc labels, in an attempt to produce structures that conform to the scheme in the original data.  Sagae and Tsujii also propose a simple arc reversal transform, which simply reverses the direction of a depend-ency arc (editing the arc label to note this change).  This transformation, which can be re-versed trivially, makes it possible to remove cy-cles in dependency graphs. 3.2 Baseline features  To create output graph structures for an input sentence, the algorithm described in section 3.1 relies on an oracle that tells it what action to take at each parser state, where the state is the con-tents of the stack, remaining words in the input, and the dependency arcs formed so far.  In grammar-based shift-reduce parsing, this oracle may take the form of a look-up table derived from grammar rules.  In our data-driven setting, where the parser learns to choose actions based on examples of correctly parsed data, the (likely 
imperfect) substitute for the oracle is a classifier that takes features that represent the parser state as input, and produces a matching parser action as output.  These features should represent as-pects of the parser state that may be informative as to what the corresponding appropriate action is.  Our baseline model uses the averaged percep-tron with a core set of features derived from the following templates, where S(n) denotes the n-th item from the top of the stack (for example, S(1) is the item on top of the stack), and I(n) denotes the next n-th input token: 1. For the items S(1) and S(2): a. the total number of dependents; b. the number of dependents to the right of the item; c. the number of dependents to the left of the item; d. the part-of-speech tag of the right-most dependent of the item; e. the part-of-speech tag of the leftmost dependent of the item; f. the arc label of the rightmost de-pendent of the item; g. the arc label of the leftmost depend-ent of the item; 2. the words in items S(1), S(2), S(3), I(1) and I(2); 3. the part-of-speech tags in items S(1), S(2), S(3), I(1), I(2) and I(3); 4. the part-of-speech tag of the word i mmedi-aely to the right of S(2); 5. the part-of-speech tag of the word immedi-ately to the left of S(1); 6. whether an arc exists between S(1) and S(2); 7. whether an arc exists between S(1) and I(1); 8. the direction of the arc between S(1) and S(2), if any; 9. the label of the arc between S(1) and S(2), if any; 10. the label of the arc between S(1) and I(1), if any; 11. the distance, in linear sequence of words, between S(1) and S(2); 12. the distance, in linear sequence of words, between S(1) and I(1); 
197
13. the previous parser action. In addition to the core set of features, we also use features obtained by concatenating the part-of-speech tags in S(1), S(2) and I(1) with the fea-tures derived from templates 1-6, and additional features derived from selected concatenation of two or three core features. 3.3 Cluster-based features To take advantage of the clusters that reflect syn-tactic similarity between words, we assign arbi-trary unique labels to each of the hierarchical clusters obtained using the procedure described in section 2.  These cluster labels are used to generate additional features that help the parser make its decisions base on the syntactic profile of words.  As explained in section 2.4, each there may be several cluster labels (corresponding to clusters of different granularities) associated with each word.  To select the set of cluster labels to be used to generate features, we first select a de-sired granularity for the clusters, and use the set of labels resulting from slicing the cluster tree at the appropriate level, as discussed in section 2.4.  We experimented with several levels of cluster granularity using development data, and follow-ing Koo et al (2008), we also experimented with using two sets of cluster labels with different levels of granularity at the same time.  Given a specific level of granularity, the cluster-based features we used are: 14. the cluster labels for the words in items S(1), S(2), S(3), I(1), I(2), I(3); 15. the cluster labels for the words in the right-most and leftmost dependents of S(1) and S(2); 16. the concatenation of the cluster labels for the words in S(1), S(2) and I(1), and the features derived from feature templates 1-15. In experiments where we used two sets of cluster labels corresponding to different levels of granularity, we added all the cluster-based fea-tures in 14 and 15 for both sets of labels, and the features in 16 only for the set corresponding to the coarser-grained clusters. 4 Experiments Following previous experiments with Penn Tree-bank WSJ data, or annotations derived from it, we used sections 02-21 of the HPSG Treebank as training material, section 22 for development, and section 23 for testing. Only the predicate-
argument dependencies were used, not the phrase structures or other information from the HPSG analyses. For all experiments described here, part-of-speech tagging was done separately using a CRF tagger with accuracy of 97.3% on sections 22-24.  Our evaluation is based on labeled preci-sion and recall of predicate-argument dependen-cies.  Although accuracy is commonly used for evaluation of dependency parsers, in our task the parser is not restricted to output a fixed number of dependencies.  Labeled precision and recall of predicate-argument pairs are also the standard evaluation metrics for data-driven HPSG and CCG parsers (although the predicate-argument pairs extracted from the HPSG Treebank and the CCGBank are specific to their formalisms and not quantitatively comparable). We started by eliminating cycles from the de-pendency graphs extracted from the HPSG Tree-bank by using the arc reversal transform in the following way: for each cycle detected in the data, the shortest arc in the cycle was reversed until no cycles remained. We then applied pseudo-projective transformation to create data that can be used to train our parser, described in section 3.  By detransforming the projective graphs generated from gold-standard dependen-cies, we obtain labeled precision of 98.1% and labeled recall of 97.7%, which is below the accu-racy expected for detransformation of syntactic dependency trees.  This is expected, since arc crossing occurs more frequently in predicate-argument graphs in the HPSG Treebank than in surface syntactic dependencies. We first trained a parsing model without clus-ter-based features, using only the baseline set of features, which was the product of experimenta-tion using the development set.  On the test set, this baseline model has labeled precision and recall of 88.7 and 88.2, respectively, slightly be-low the precision and recall obtained by Sagae and Tsujii on the same data (89.0 precision and 88.5 recall). We then used the development set to explore the effects of cluster sets with different levels of granularity.  The baseline model has precision and recall of 88.6 and 88.0 on the development set.  We found that by slicing the cluster tree relatively close to the root, resulting in a set of 50 to 100 distinct cluster labels (corresponding to relatively coarse clusters), we obtain small (0.3 to 0.4), but statistically significant (p < 0.005) improvements on precision and recall over the baseline model on the development set.  By in-creasing the number of cluster labels (making the 
198
distinctions among members of different clusters more fine-grained) in steps of 100, we observed improvements in precision and recall until the point where there were 600 distinct cluster la-bels.  This set of 600 cluster labels produced the highest values of precision and recall (89.5 and 89.0) that we obtained for the development set using only one set of cluster labels.  Figure 4 shows how precision, recall and F-score on the development set varied with the number of clus-ter labels used. Following Koo et al (2008), we also experi-mented with using two sets of cluster labels with different levels of granularity.  We found that using the set of 600 labels and an additional set with fewer than 600 labels did not improve or hurt precision and recall.  Finer grained clusters with more than 1,000 labels (combined with the set of 600 labels) improved results further.  The highest precision and recall figures of 90.1 and 89.6 were obtained with the sets of 600 and 1,400 labels. We parsed the test set using the best configu-ration of cluster-based features as determined using the development set (the sets with 600 and 1,400 cluster labels) and obtained 90.2 precision, 89.8 recall and 90.0 f-score, a 13.8% reduction in error over a strong baseline.  Table 1 summarizes our results on the test set.  For comparison, we also shows results published by Sagae and Tsujii (2008), to our knowledge the highest f-score re-ported for this test set, and Miyao and Tsujii (2005), who first reported results on this data set. 
4.1 Surface dependency parsing with clus-ter-based features The parser used in our experiments with HPSG Treebank predicate-argument structures can as-sign more than one head for a single word, but when the parser is trained using only dependency trees, it behaves in exactly the same way as a parser based on Nivre?s arc-standard algorithm, since it never sees examples of attach actions during training.  To see whether our clusters can improve surface dependency parsing, and to al-low for comparison of our results to a larger body of research on surface dependency parsing, we used dependency trees extracted from the Penn Treebank using the Yamada and Matsu-moto (2003) version of the Penn Treebank head-percolation rules to train parsing models that produce dependency trees.  However, no tuning of the features or metaparameters was per-formed; the parser was trained as-is on depend-ency trees. We used the standard train, development and test sets splits to train two models, as in our ex-periments with predicate-argument dependen-cies: a baseline that uses no cluster information, and a model that uses two sets of clusters that were found to improve results in the develop-ment set.  The unlabeled accuracy of our baseline model on the test set is 89.96%, which is consid-erably lower than the best current results.  Koo et al (2008) report 90.84% for a first-order edge-factored model, and 92.02% for a second-order model (and as high as 93.16% with a second-order model enriched with cluster features de-rived from plain text).  Using two sets of clus-ters, one with 600 and one with 1,200 labels, ac-curacy improves by 1.32%, to reach 91.28% (a 13.15% reduction in error compared to our base-line).  While still below the level of the strongest results for this dataset, it is interesting to see that 
 Precision Recall F-score Baseline 88.7 88.2 88.4 Clusters 90.2 89.8 90.0 S & T 89.9 88.5 88.7 Miyao et al 85.0 84.3 84.6  Table 1: Results obtained on the test set us-ing our baseline model and our best cluster-based features.  The results in the bottom two rows are from Sagae and Tsujii (2008) and Miyao and Tsujii (2005).  Figure 4: Effect of cluster granularity on labeled the precision and recall of predicate-argument pairs in the development set.  The improvement in precision and recall between the baseline (zero cluster labels, where no cluster information is added) and 600 cluster labels is statistically significant (p < 0.0005).  
199
the improvement in accuracy over the baseline observed for surface dependency trees is similar to the improvement observed for predicate-argument dependency graphs. 5 Related work Many aspects of this research were inspired by the recent work of Koo et al (2008), who re-ported impressive results on improving depend-ency parsing accuracy using a second order edge-factored model and word clusters derived from plain text using the Brown et al (1992) al-gorithm.  Our clustering approach is significantly different, focusing on the use of parsed data to produce strictly syntactic clusters.  It is possible that using both types of clusters may be benefi-cial. McClosky et al (2006) used a large corpus of parsed text to obtain improved parsing results through self-training.  A key difference in our general framework is that it allows for a parser with one type of syntactic representation to im-prove the accuracy of a different parser with a different type of formalism.  In this regard, our work is related to that of Sagae et al (2007), who used a stacking-like framework to allow a sur-face dependency parser to improve an HPSG parser.  In that work, however, as in other work that combines different parsers through stacking (Martins et al, 2008; Nivre and McDonald, 2008) or voting (Henderson and Brill, 1999), multiple parsers need to process new text at run-time.  In our approach for leveraging diverse parsers, one of the parsers is used only to create a parsed corpus from which we extract clusters of words that have similar syntactic behaviors, and only one parser is needed at run-time. 6 Conclusion We have presented a novel approach for deriving word clusters based on syntactic similarity, and shown how these word clusters can be applied in a transition-based dependency parser. Our experiments focused on predicate-argument structures extracted from the HPSG Treebank, which demonstrates that the syntactic clusters are effective in leveraging cross-framework parser representations to improve parsing accuracy.  However, we expect that simi-lar accuracy improvements can be obtained in parsing using other frameworks and formalisms, and possibly in other natural language processing tasks. 
Acknowledgments The project or effort described here has been sponsored by the U.S. Army Research, Devel-opment, and Engineering Command (RDE-COM). Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred. References  Srinivas Bangalore and Aravind K. Joshi. 1999. Su-pertagging: an approach to almost parsing. Compu-tational Linguistics 25, 2 (Jun. 1999), 237-265. Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-Based n-gram Models of Natural Lan-guage. Computational Linguistics, 18(4):467?479. Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the First Meet-ing of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL), pages 132?139. Charniak, E., Blaheta, D., Ge, N., Hall, K., Hale, J., and Johnson, M. (2000) BLLIP 1987-89 WSJ Cor-pus Release 1. Philadelphia, PA: Linguistic Data Consortium. Michael Collins. 1999. Head-Driven Statistical Mod-els for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania. Michael Collins. 2002. Discriminative Training Me-thods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Pro-ceedings of EMNLP, pages 1?8. Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algo-rithm. Machine Learning, 37(3):277?296. Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Lin-guistics 28(3): 245-288. Andrew Gordon and Reid Swanson. 2007. Generaliz-ing semantic role annotations across syntactically similar verbs. Proceedings of the 2007 meeting of the Association for Computational Linguistics (ACL-07), Prague, Czech Republic, June 23-30, 2007. Johan Hall, Jens Nilsson, Joakim Nivre, Gulsen Ery-igit, Beata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? A study in multilingual parser optimization. In Proceedings of EMNLP-CoNLL. James Henderson, Paola Merlo, G. Musillo, and Ivan Titov. 2008. A latent variable model of synchro-nous parsing for syntactic and semantic dependen-
200
cies. In Proceedings of the Shared Task of the Con-ference on Computational Natural Language Learning (CoNLL), pages 178-182. Manchester, UK. John Henderson and Eric Brill. 1999. Exploiting di-versity in natural language processing: combining parsers. In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Proc-essing (EMNLP). Terry Koo, Xavier Carreras and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), pages 595-603.  Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th inter-national Conference on Computational Linguistics - Volume 2. Montreal, Quebec, Canada. Mitchell P. Marcus, Mary A. Marcinkiewicz, Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank, Computational Linguistics, 19(2), June 1993. Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking Dependency Parsers.  In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing, Waikiki, HI. David McClosky, Eugene Charniak, and Mark John-son. 2006. Effective Self-Training for Parsing. In Proceedings of HLT-NAACL, pages 152?159. Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of de-pendency parsers. In Proceedings of ACL, pages 91?98. Scott Miller, Jethran Guinness and Alex Zamanian. 2004. Name Tagging withWord Clusters and Dis-criminative Training. In Proceedings of HLT-NAACL, pages 337?342. Miyao, Yusuke, Takashi Ninomiya, and Jun?ichi Tsu-jii. 2004. Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP).  Miyao Yusuke and Jun'ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics. Joakim Nivre.2004. Incrementality in Deterministic Dependency Parsing. In Incremental Parsing: Bringing Engineering and Cognition Together (Workshop at ACL-2004). 
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceed-ings of CoNLL, pages 49?56. Joakim Nivre. and Jens Nilsson. 2005. Pseudo-Projective Dependency Parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. 99-106. Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of EMNLP-CoNLL, pages 915-932. Nivre, J. and McDonald, R. (2008) Integrating Graph-Based and Transition-Based Dependency Parsers. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), 950-958. Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The Proposition Bank: A Corpus Annotated with Semantic Roles. Computational Linguistics, 31:1. Kenji Sagae and Alon Lavie. 2006. Parser combina-tion by reparsing. In Proceedings of NAACL: Short Papers, pages 129?132. Kenji Sagae, Yusuke Miyao Jun?ichi and Tsujii. 2007. HPSG Parsing with shallow dependency con-straints. In Proceedings of the 44th Meeting of the Association for Computational Linguistics. Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce dependency DAG parsing. In Proceedings of the International Conference on Computational Lin-guistics (COLING 2008). Hiroyasu Yamada and Y. Matsumoto. 2003. Statisti-cal Dependency Analysis With Support Vector Machines. In Proceedings of the Eighth Interna-tional Workshop on Parsing Technologies (IWPT), pages 195?206.  
201
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 11?20,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Can I finish? Learning when to respond to incremental interpretation
results in interactive dialogue
David DeVault and Kenji Sagae and David Traum
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,sagae,traum}@ict.usc.edu
Abstract
We investigate novel approaches to re-
sponsive overlap behaviors in dialogue
systems, opening possibilities for systems
to interrupt, acknowledge or complete a
user?s utterance while it is still in progress.
Our specific contributions are a method for
determining when a system has reached a
point of maximal understanding of an on-
going user utterance, and a prototype im-
plementation that shows how systems can
use this ability to strategically initiate sys-
tem completions of user utterances. More
broadly, this framework facilitates the im-
plementation of a range of overlap behav-
iors that are common in human dialogue,
but have been largely absent in dialogue
systems.
1 Introduction
Human spoken dialogue is highly interactive, in-
cluding feedback on the speech of others while
the speech is progressing (so-called ?backchan-
nels? (Yngve, 1970)), monitoring of addressees
and other listener feedback (Nakano et al, 2003),
fluent turn-taking with little or no delays (Sacks et
al., 1974), and overlaps of various sorts, including
collaborative completions, repetitions and other
grounding moves, and interruptions. Interrup-
tions can be either to advance the new speaker?s
goals (which may not be related to interpreting the
other?s speech) or in order to prevent the speaker
from finishing, which again can be for various rea-
sons. Few of these behaviors can be replicated by
current spoken dialogue systems. Most of these
behaviors require first an ability to perform in-
cremental interpretation, and second, an ability to
predict the final meaning of the utterance.
Incremental interpretation enables more rapid
response, since most of the utterance can be inter-
preted before utterance completion (Skantze and
Schlangen, 2009). It also enables giving early
feedback (e.g., head nods and shakes, facial ex-
pressions, gaze shifts, and verbal backchannels) to
signal how well things are being perceived, under-
stood, and evaluated (Allwood et al, 1992).
For some responsive behaviors, one must go be-
yond incremental interpretation and predict some
aspects of the full utterance before it has been
completed. For behaviors such as comply-
ing with the evocative function (Allwood, 1995)
or intended perlocutionary effect (Sadek, 1991),
grounding by demonstrating (Clark and Schaefer,
1987), or interrupting to avoid having the utter-
ance be completed, one must predict the semantic
content of the full utterance from a partial prefix
fragment. For other behaviors, such as timing a
reply to have little or no gap, grounding by saying
the same thing at the same time (called ?chanting?
by Hansen et al (1996)), performing collaborative
completions (Clark and Wilkes-Gibbs, 1986), or
some corrections, it is important not only to pre-
dict the meaning, but also the form of the remain-
ing part of the utterance.
We have begun to explore these issues in the
context of the dialogue behavior of virtual human
(Rickel and Johnson, 1999) or embodied conver-
sational agent (Cassell et al, 2000) characters for
multiparty negotiation role-playing (Traum et al,
2008b). In these kinds of systems, human-like be-
havior is a goal, since the purpose is to allow a user
to practice this kind of dialogue with the virtual
humans in training for real negotiation dialogues.
The more realistic the characters? dialogue behav-
ior is, the more kinds of negotiation situations can
be adequately trained for. We discuss these sys-
11
tems further in Section 2.
In Sagae et al (2009), we presented our first re-
sults at prediction of semantic content from partial
speech recognition hypotheses, looking at length
of the speech hypothesis as a general indicator of
semantic accuracy in understanding. We summa-
rize this previous work in Section 3.
In the current paper, we incorporate additional
features of real-time incremental interpretation to
develop a more nuanced prediction model that can
accurately identify moments of maximal under-
standing within individual spoken utterances (Sec-
tion 4). We demonstrate the value of this new
ability using a prototype implementation that col-
laboratively completes user utterances when the
system becomes confident about how the utter-
ance will end (Section 5). We believe such pre-
dictive models will be more broadly useful in im-
plementing responsive overlap behaviors such as
rapid grounding using completions, confirmation
requests, or paraphrasing, as well as other kinds of
interruptions and multi-modal displays. We con-
clude and discuss future work in Section 6.
2 Domain setting
The case study we present in this paper is taken
from the SASO-EN scenario (Hartholt et al, 2008;
Traum et al, 2008b). This scenario is designed
to allow a trainee to practice multi-party negoti-
ation skills by engaging in face to face negotia-
tion with virtual humans. The scenario involves
a negotiation about the possible re-location of a
medical clinic in an Iraqi village. A human trainee
plays the role of a US Army captain, and there are
two virtual humans that he negotiates with: Doctor
Perez, the head of the NGO clinic, and a local vil-
lage elder, al-Hassan. The doctor?s main objective
is to treat patients. The elder?s main objective is to
support his village. The captain?s main objective
is to move the clinic out of the marketplace, ide-
ally to the US base. Figure 1 shows the doctor and
elder in the midst of a negotiation, from the per-
spective of the trainee. Figure A-1 in the appendix
shows a sample dialogue from this domain.
The system has a fairly typical set of pro-
cessing components for virtual humans or dia-
logue systems, including ASR (mapping speech
to words), NLU (mapping from words to semantic
frames), dialogue interpretation and management
(handling context, dialogue acts, reference and de-
ciding what content to express), NLG (mapping
Figure 1: SASO-EN negotiation in the cafe: Dr.
Perez (left) looking at Elder al-Hassan.
2
6
6
6
6
6
6
6
4
mood : declarative
sem :
2
6
6
6
6
6
4
type : event
agent : captain? kirk
event : deliver
theme : power ? generator
modal :
?
possibility : can
?
speech? act :
?
type : offer
?
3
7
7
7
7
7
5
3
7
7
7
7
7
7
7
5
Figure 2: AVM utterance representation.
frames to words), non-verbal generation, and syn-
thesis and realization. The doctor and elder use
the same ASR and NLU components, but have dif-
ferent modules for the other processing, including
different models of context and goals, and differ-
ent output generators. In this paper, we will often
refer to the characters with various terms, includ-
ing ?virtual humans?, ?agents?, or ?the system?.
In this paper, we are focusing on the NLU
component, looking at incremental interpretation
based on partial speech recognition results, and
the potential for using this information to change
the dialogue strategy where warranted, and pro-
vide responses before waiting for the final speech
result. The NLU output representation is an
attribute-value matrix (AVM), where the attributes
and values represent semantic information that
is linked to a domain-specific ontology and task
model (Hartholt et al, 2008). Figure 2 shows an
example representation, for an utterance such as
?we can provide you with power generators?. The
AVMs are linearized, using a path-value notation,
as shown in Figure 3.
To develop and test the new incremen-
tal/prediction models, we are using a corpus of
12
<s>.mood declarative
<s>.sem.type event
<s>.sem.agent captain-kirk
<s>.sem.event deliver
<s>.sem.theme power-generator
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
Figure 3: Example NLU frame.
utterances collected from people playing the role
of captain and negotiating with the virtual doctor
and elder. In contrast with Figure A-1, which
is a dialogue with one of the system designers
who knows the domain well, dialogues with naive
users are generally longer, and often have a fairly
high word error rate (average 0.54), with many
out of domain utterances. The system is robust to
these kinds of problems, both in terms of the NLU
approach (Leuski and Traum, 2008; Sagae et al,
2009) as well as the dialogue strategies (Traum
et al, 2008a). This is accomplished in part by
approximating the meaning of utterances. For
example, the frame in Figure 3 is also returned for
an utterance of we are prepared to give you guys
generators for electricity downtown as well as the
ASR output for this utterance, we up apparently
give you guys generators for a letter city don town.
3 Predicting interpretations from partial
recognition hypotheses
Our NLU module, mxNLU (Sagae et al, 2009), is
based on maximum entropy classification (Berger
et al, 1996), where we treat entire individual
frames as classes, and extract input features from
ASR. The training data for mxNLU is a corpus
of approximately 3,500 utterances, each annotated
with the appropriate frame. These utterances were
collected from user sessions with the system, and
the corresponding frames were assigned manually.
Out-of-domain utterances (about 15% of all utter-
ances in our corpus) could not be mapped to con-
cepts in our ontology and task model, and were
assigned a ?garbage? frame. For each utterance
in our corpus, we have both a manual transcrip-
tion and the output of ASR, although only ASR
is used by mxNLU (both at training and at run-
time). Each training instance for mxNLU consists
of a frame, paired with a set of features that rep-
resent the ASR output for user utterances. The
specific features used by the classifier are: each
word in the input string (bag-of-words representa-
tion of the input), each bigram (pairs of consec-
utive words), each pair of any two words in the
input, and the number of words in the input string.
In the 3,500-utterance training set, there are 136
unique frames (135 that correspond to the seman-
tics of different utterances in the domain, plus one
frame for out-of-domain utterances).1 The NLU
task is then framed as a multiclass classification
approach with 136 classes, and about 3,500 train-
ing examples.
Although mxNLU produces entire frames as
output, we evaluate NLU performance by look-
ing at precision and recall of the attribute-value
pairs (or frame elements) that compose frames.
Precision represents the portion of frame elements
produced by mxNLU that were correct, and re-
call represents the portion of frame elements in
the gold-standard annotations that were proposed
by mxNLU. By using precision and recall of
frame elements, we take into account that certain
frames are more similar than others and also al-
low more meaningful comparative evaluation with
NLU modules that construct a frame from sub-
elements or for cases when the actual frame is not
in the training set. The precision and recall of
frame elements produced by mxNLU using com-
plete ASR output are 0.78 and 0.74, respectively,
for an F-score (harmonic mean of precision and
recall) of 0.76.
3.1 NLU with partial ASR results
The simplest way to perform NLU of partial ASR
results is simply to process the partial utterances
using the NLU module trained on complete ASR
output. However, better results may be obtained
by training separate NLU models for analysis of
partial utterances of different lengths. To train
these separate NLU models, we first ran the au-
dio of the utterances in the training data through
our ASR module, recording all partial results for
each utterance. Then, to train a model to ana-
lyze partial utterances containing N words, we
used only partial utterances in the training set con-
taining N words (unless the entire utterance con-
tained less than N words, in which case we sim-
ply used the complete utterance). In some cases,
multiple partial ASR results for a single utterance
1In a separate development set of 350 utterances, anno-
tated in the same way as the training set, we found no frames
that had not appeared in the training set.
13
0
10
20
30
40
50
60
70
80
1 2 3 4 5 6 7 8 9 10 allLength n (words)
F
-
s
c
o
r
e
Trained on all data
Trained on partials up tolength nTrained on partials up tolength n + context
Figure 4: F-score for three NLU models on partial
ASR results up to N words.
contained the same number of words, and we used
the last partial result with the appropriate number
of words.2 We trained ten separate partial NLU
models for N varying from one to ten.
Figure 4 shows the F-score for frames obtained
by processing partial ASR results up to length N
using three variants of mxNLU. The dashed line is
our baseline NLU model, trained on complete ut-
terances only, and the solid line shows the results
obtained with length-specific NLU models. The
dotted line shows results for length-specific mod-
els that also use features that capture aspects of di-
alogue context. In these experiments, we used uni-
gram and bigram word features extracted from the
most recent system utterance to represent context,
but found that these context features did not im-
prove NLU performance. Our final NLU approach
for partial ASR hypotheses is then to train separate
models for specific lengths, using hypotheses of
that length during training (solid line in figure 4).
4 How well is the system understanding?
In this section, we present a strategy that uses
machine learning to more closely characterize the
performance of a maximum entropy based incre-
mental NLU module, such as the mxNLU mod-
ule described in Section 3. Our aim is to iden-
tify strategic points in time, as a specific utterance
is occurring, when the system might react with
confidence that the interpretation will not signif-
2At run-time, this can be closely approximated by taking
the partial utterance immediately preceding the first partial
utterance of length N + 1.
Utterance time (ms)
NL
U F
?sc
ore
(emp
ty)
(emp
ty)
 
all 
 
elde
r 
 
elde
r do
 you
 
 
elde
r to
 you
 d 
 
elde
r do
 you
 agr
ee 
 
elde
r do
 you
 agr
ee t
o 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic to
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
200 400 600 800 100
0
120
0
140
0
160
0
180
0
200
0
220
0
240
0
260
0
280
0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Partial ASR result
Figure 5: Incremental interpretation of a user ut-
terance.
icantly improve during the rest of the utterance.
This reaction could take several forms, including
providing feedback, or, as described in Section 5
an agent might use this information to opportunis-
tically choose to initiate a completion of a user?s
utterance.
4.1 Motivating example
Figure 5 illustrates the incremental output of
mxNLU as a user asks, elder do you agree to move
the clinic downtown? Our ASR processes cap-
tured audio in 200ms chunks. The figure shows
the partial ASR results after the ASR has pro-
cessed each 200ms of audio, along with the F-
14
score achieved by mxNLU on each of these par-
tials. Note that the NLU F-score fluctuates some-
what as the ASR revises its incremental hypothe-
ses about the user utterance, but generally in-
creases over time.
For the purpose of initiating an overlapping re-
sponse to a user utterance such as this one, the
agent needs to be able (in the right circumstances)
to make an assessment that it has already under-
stood the utterance ?well enough?, based on the
partial ASR results that are currently available. We
have implemented a specific approach to this as-
sessment which views an utterance as understood
?well enough? if the agent would not understand
the utterance any better than it currently does even
if it were to wait for the user to finish their utter-
ance (and for the ASR to finish interpreting the
complete utterance).
Concretely, Figure 5 shows that after the entire
2800ms utterance has been processed by the ASR,
mxNLU achieves an F-score of 0.91. However,
in fact, mxNLU already achieves this maximal F-
score at the moment it interprets the partial ASR
result elder do you agree to move the at 1800ms.
The agent therefore could, in principle, initiate an
overlapping response at 1800ms without sacrific-
ing any accuracy in its understanding of the user?s
utterance.
Of course the agent does not automatically re-
alize that it has achieved a maximal F-score at
1800ms. To enable the agent to make this assess-
ment, we have trained a classifier, which we call
MAXF, that can be invoked for any specific par-
tial ASR result, and which uses various features of
the ASR result and the current mxNLU output to
estimate whether the NLU F-score for the current
partial ASR result is at least as high as the mxNLU
F-score would be if the agent were to wait for the
entire utterance.
4.2 Machine learning setup
To facilitate the construction of our MAXF clas-
sifier, we identified a range of potentially useful
features that the agent could use at run-time to as-
sess its confidence in mxNLU?s output for a given
partial ASR result. These features are exempli-
fied in the appendix in Figure A-2, and include:
K, the number of partial results that have been re-
ceived from the ASR; N , the length (in words) of
the current partial ASR result; Entropy, the en-
tropy in the probability distribution mxNLU as-
signs to alternative output frames (lower entropy
corresponds to a more focused distribution); Pmax,
the probability mxNLU assigns to the most prob-
able output frame; NLU, the most probable output
frame (represented for convenience as fI , where
I is an integer index corresponding to a specific
complete frame). We also define MAXF (GOLD),
a boolean value giving the ground truth about
whether mxNLU?s F-score for this partial is at
least as high as mxNLU?s F-score for the final par-
tial for the same utterance. In the example, note
that MAXF (GOLD) is true for each partial where
mxNLU?s F-score (F (K)) is ? 0.91, the value
achieved for the final partial (elder do you agree to
move the clinic downtown). Of course, the actual
F-score F (K) is not available at run-time, and so
cannot serve as an input feature for the classifier.
Our general aim, then, is to train a classifier,
MAXF, whose output predicts the value of MAXF
(GOLD) as a function of the input features. To
create a data set for training and evaluating this
classifier, we observed and recorded the values of
these features for the 6068 partial ASR results in
a corpus of ASR output for 449 actual user utter-
ances.3
We chose to train a decision tree using Weka?s
J48 training algorithm (Witten and Frank, 2005).4
To assess the trained model?s performance, we car-
ried out a 10-fold cross-validation on our data set.5
We present our results in the next section.
4.3 Results
We will present results for a trained decision
tree model that reflects a specific precision/recall
tradeoff. In particular, given our aim to enable
an agent to sometimes initiate overlapping speech,
while minimizing the chance of making a wrong
assumption about the user?s meaning, we selected
a model with high precision at the expense of
lower recall. Various precision/recall tradeoffs are
possible in this framework; the choice of a spe-
cific tradeoff is likely to be system and domain-
dependent and motivated by specific design goals.
We evaluate our model using several features
which are exemplified in the appendix in Fig-
ure A-3. These include MAXF (PREDICTED),
the trained MAXF classifier?s output (TRUE or
3This corpus was not part of the training data for mxNLU.
4Of course, other classification models could be used.
5All the partial ASR results for a given utterance were
constrained to lie within the same fold, to avoid training and
testing on the same utterance.
15
FALSE) for each partial; KMAXF, the first par-
tial number for which MAXF (PREDICTED) is
TRUE; ?F (K) = F (K) ? F (Kfinal), the ?loss?
in F-score associated with interpreting partial K
rather than the final partialKfinal for the utterance;
T (K), the remaining length (in seconds) in the
user utterance at each partial.
We begin with a high level summary of the
trained MAXF model?s performance, before dis-
cussing more specific impacts of interest in the di-
alogue system. We found that our trained model
predicts that MAXF = TRUE for at least one
partial in 79.2% of the utterances in our cor-
pus. For the remaining utterances, the trained
model predicts MAXF = FALSE for all partials.
The precision/recall/F-score of the trained MAXF
model are 0.88/0.52/0.65 respectively. The high
precision means that 88% of the time that the
model predicts that F-score is maximized at a spe-
cific partial, it really is. On the other hand, the
lower recall means that only 52% of the time that
F-score is in fact maximized at a given partial does
the model predict that it is.
For the 79.2% of utterances for which the
trained model predicts MAXF = TRUE at some
point, Figure 6 shows the amount of time in sec-
onds, T (KMAXF), that remains in the user utter-
ance at the time partialKMAXF becomes available
from the ASR. The mean value is 1.6 seconds; as
the figure shows, the time remaining varies from 0
to nearly 8 seconds per utterance. This represents
a substantial amount of time that an agent could
use strategically, for example by immediately ini-
tiating overlapping speech (perhaps in an attempt
to improve communication efficiency), or by ex-
ploiting this time to plan an optimal response to
the user?s utterance.
However, it is also important to understand the
cost associated with interpreting partial KMAXF
rather than waiting to interpret the final ASR result
Kfinal for the utterance. We therefore analyzed
the distribution in ?F (KMAXF) = F (KMAXF)?
F (Kfinal). This value is at least 0.0 if mxNLU?s
output for partial KMAXF is no worse than its out-
put for Kfinal (as intended). The distribution is
given in Figure 7. As the figure shows, 62.35% of
the time (the median case), there is no difference
in F-score associated with interpreting KMAXF
rather than Kfinal. 10.67% of the time, there is
a loss of -1, which corresponds to a completely
incorrect frame at KMAXF but a completely cor-
Utterance time remaining (seconds)
F
r
e
q
u
e
n
c
y
0 2 4 6 80
10
20
30
Figure 6: Distribution of T (KMAXF).
?F (KMAXF) range Percent of
utterances
-1 10.67%
(?1, 0) 17.13%
0 62.35%
(0, 1) 7.30%
1 2.52%
mean(?F (KMAXF)) -0.1484
median(?F (KMAXF)) 0.0000
Figure 7: The distribution in ?F (KMAXF), the
?loss? associated with interpreting partial KMAXF
rather than Kfinal.
rect frame at Kfinal. The converse also happens
2.52% of the time: mxNLU?s output frame is com-
pletely correct at the early partial but completely
incorrect at the final partial. The remaining cases
are mixed. While the median is no change in F-
score, the mean case is a loss in F-score of -0.1484.
This is the mean penalty in NLU performance that
could be paid in exchange for the potential gain in
communication efficiency suggested by Figure 6.
5 Prototype implementation
To illustrate one use of the techniques described in
the previous sections, we have implemented a pro-
totype module that performs user utterance com-
pletion. This allows an agent to jump in during a
user?s utterance, and say a completion of the utter-
ance before it is finished, at a point when the agent
16
thinks it understands what the user means. This
type of completion is often encountered in human-
human dialogue, and may be used, for example,
for grounding or for bringing the other party?s turn
to a conclusion.
We have equipped one of our virtual humans,
Doctor Perez, with an ability to perform comple-
tions as follows. The first step is for the agent to
recognize when it understands what the user wants
to say. As discussed in Sections 3 and 4, this often
happens before the user has completed the utter-
ance. NLU is performed on partial ASR hypothe-
ses as they become available, and MAXF decides
whether the agent?s understanding of the current
partial hypothesis is likely to improve given more
time. Once MAXF indicates that the agent?s un-
derstanding is likely to be already maximized for
the utterance, we take the current partial ASR hy-
pothesis and attempt to generate text to complete it
in a way that is fluent and agrees with the meaning
of the utterance the user has in mind.
The generation of the surface text for comple-
tions takes advantage of the manual transcriptions
in the corpus of utterances used to train the NLU
module. For each frame that the agent under-
stands, our training set contains several user utter-
ances that correspond to the meaning in that frame.
At the point where the agent is ready to formu-
late a completion, mxNLU has already predicted
a frame for the user?s utterance (even though it is
still incomplete). We then consider only the set
of known utterances that correspond to that frame
as possible sources of completions. As a simple
distance metric, we compute the word error rate
(WER) between the current partial hypothesis for
the user?s utterance and a prefix of each of these
known utterances. In our prototype, these prefixes
have the same length as the current partial ASR
hypothesis. We then select the utterance whose
prefix has the lowest WER against the current par-
tial ASR hypothesis. As a final step, we look in
the prefix of our selected utterance for the last oc-
currence of the last word in the partial ASR, and
if such a word is found, we take the remainder of
the utterance as the agent?s completion. Consid-
ering only the set of utterances that correspond to
the frame predicted by mxNLUmakes it likely that
the completion will have the appropriate meaning.
Since the completion is a suffix of a transcript of a
previous user utterance, and this suffix follows the
last word uttered by the user, it is likely to form a
fluent completion of the user?s partial utterance.
For 356 utterances in our development set, the
procedure above generated completions contain-
ing at least one word for 177 utterances. We list
below four representative examples of user utter-
ance completions generated by Doctor Perez:
1. Partial utterance: we need to
Predicted completion: move your clinic
Actual user completion: move the clinic
2. Partial utterance: I have orders
Predicted completion: to move you and this
clinic
Actual user completion: to help you in mov-
ing the clinic to a new location
3. Partial utterance: the market
Predicted completion: is not safe
Actual user completion: is not a safer loca-
tion
4. Partial utterance: we can also
Predicted completion: give you medical sup-
plies
Actual user completion: build you a well
Although agent completions for user utterances
are often acceptably accurate (examples 1-3), oc-
casionally they are incorrect (example 4). The
frequency of appropriate completions depends on
the accuracy of mxNLU and the chosen preci-
sion/recall tradeoff in MAXF. Finally, although
the agent has the ability to generate these com-
pletions, clearly it should not complete the user?s
utterance at every opportunity. Determining a pol-
icy that results in natural behavior with respect to
the frequency of completions for different types of
agents is a topic under current investigation.
6 Summary and future work
We have presented a framework for interpretation
of partial ASR hypotheses of user utterances, and
high-precision identification of points within user
utterances where the system already understands
the intended meaning. Our initial implementa-
tion of an utterance completion ability for a vir-
tual human serves to illustrate the capabilities of
this framework, but only scratches the surface of
the new range of dialogue behaviors and strategies
it allows.
Immediate future work includes the design of
policies for completions and interruptions that re-
17
sult in natural conversational behavior. Other ap-
plications of this work include the generation of
paraphrases that can be used for grounding, in ad-
dition to extra-linguistic behavior during user ut-
terances, such as head nods and head shakes.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily
reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred. We would also like to thank
Anton Leuski for facilitating the use of incremen-
tal speech results, and David Schlangen and the
ICT dialogue group, for helpful discussions.
References
Jens Allwood, Joakim Nivre, and Elisabeth Ahlsen.
1992. On the semantics and pragmatics of linguistic
feedback. Journal of Semantics, 9.
Jens Allwood. 1995. An activity based approach to
pragmatics. Technical Report (GPTL) 75, Gothen-
burg Papers in Theoretical Linguistics, University of
G?teborg.
Adam L. Berger, Stephen D. Della Pietra, and Vincent
J. D. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Justine Cassell, Joseph Sullivan, Scott Prevost, and
Elizabeth Churchill, editors. 2000. Embodied Con-
versational Agents. MIT Press, Cambridge, MA.
Herbert H. Clark and Edward F. Schaefer. 1987. Col-
laborating on contributions to conversation. Lan-
guage and Cognitive Processes, 2:1?23.
Herbert H. Clark and DeannaWilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22:1?
39. Also appears as Chapter 4 in (Clark, 1992).
Herbert H. Clark. 1992. Arenas of Language Use.
University of Chicago Press.
B. Hansen, D. Novick, and S. Sutton. 1996. Prevention
and repair of breakdowns in a simple task domain.
In Proceedings of the AAAI-96 Workshop on De-
tecting, Repairing, and Preventing Human-Machine
Miscommunication, pages 5?12.
A. Hartholt, T. Russ, D. Traum, E. Hovy, and S. Robin-
son. 2008. A common ground for virtual humans:
Using an ontology in a natural language oriented
virtual human architecture. In Language Resources
and Evaluation Conference (LREC), May.
A. Leuski and D. Traum. 2008. A statistical approach
for text processing in virtual humans. In 26th Army
Science Conference.
Yukiko I. Nakano, Gabe Reinstein, Tom Stocky, and
Justine Cassell. 2003. Towards a model of face-to-
face grounding. In ACL, pages 553?561.
Jeff Rickel and W. Lewis Johnson. 1999. Virtual hu-
mans for team training in virtual reality. In Proceed-
ings of the Ninth International Conference on Artifi-
cial Intelligence in Education, pages 578?585. IOS
Press.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50:696?735.
M. D. Sadek. 1991. Dialogue acts are rational
plans. In Proceedings of the ESCA/ETR workshop
on multi-modal dialogue.
K. Sagae, G. Christian, D. DeVault, and D. R. Traum.
2009. Towards natural language understanding of
partial speech recognition results in dialogue sys-
tems. In Short Paper Proceedings of NAACL HLT.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of EACL 2009, pages 745?753.
D. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008a. A virtual human dialogue model for non-
team interaction. In L. Dybkjaer and W. Minker,
editors, Recent Trends in Discourse and Dialogue.
Springer.
D. R. Traum, S. Marsella, J. Gratch, J. Lee, and
A. Hartholt. 2008b. Multi-party, multi-issue, multi-
strategy negotiation for multi-modal virtual agents.
In Helmut Prendinger, James C. Lester, and Mitsuru
Ishizuka, editors, IVA, volume 5208 of Lecture Notes
in Computer Science, pages 117?130. Springer.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
Victor H. Yngve. 1970. On getting a word in edgewise.
In Papers from the Sixth Regional Meeting, pages
567?78. Chicago Linguistic Society.
18
A Appendix
1 C Hello Doctor Perez.
2 D Hello captain.
3 E Hello captain.
4 C Thank you for meeting me.
5 E How may I help you?
6 C I have orders to move this clinic to a camp near the US base.
7 E We have many matters to attend to.
8 C I understand, but it is imperative that we move the clinic out of this area.
9 E This town needs a clinic.
10 D We can?t take sides.
11 C Would you be willing to move downtown?
12 E We would need to improve water access in the downtown area, captain.
13 C We can dig a well for you.
14 D Captain, we need medical supplies in order to run the clinic downtown.
15 C We can deliver medical supplies downtown, Doctor.
16 E We need to address the lack of power downtown.
17 C We can provide you with power generators.
18 E Very well captain, I agree to have the clinic downtown.
19 E Doctor, I think you should run the clinic downtown.
20 D Elder, the clinic downtown should be in an acceptable condition before
we move.
21 E I can renovate the downtown clinic, Doctor.
22 D OK, I agree to run the clinic downtown, captain.
23 C Excellent.
24 D I must go now.
25 E I must attend to other matters.
26 C Goodbye.
26 D Goodbye.
26 E Farewell, sir.
Figure A-1: Successful negotiation dialogue between C, a captain (human trainee), D, a doctor (virtual
human), and E, a village elder (virtual human).
19
MAXF model training features
Partial ASR result F (K) K N Entropy Pmax NLU MAXF (GOLD)
(empty) 0.00 1 0 2.96 0.48 f82 FALSE
(empty) 0.00 2 0 2.96 0.48 f82 FALSE
all 0.00 3 1 0.82 0.76 f72 FALSE
elder 0.00 4 1 0.08 0.98 f39 FALSE
elder do you 0.83 5 3 1.50 0.40 f68 FALSE
elder to you d 0.50 6 3 1.31 0.75 f69 FALSE
elder do you agree 0.83 7 4 1.84 0.35 f68 FALSE
elder do you agree to 0.83 8 5 1.40 0.61 f68 FALSE
elder do you agree to move the 0.91 9 7 0.94 0.49 f10 TRUE
elder do you agree to move the 0.91 10 7 0.94 0.49 f10 TRUE
elder do you agree to move the clinic to 0.83 11 9 1.10 0.58 f68 FALSE
elder do you agree to move the clinic down 0.83 12 9 1.14 0.66 f68 FALSE
elder do you agree to move the clinic downtown 0.91 13 9 0.50 0.89 f10 TRUE
elder do you agree to move the clinic downtown 0.91 14 9 0.50 0.89 f10 TRUE
Figure A-2: Features used to train the MAXF model.
MAXF model evaluation features
K F (K) ?F (K) T (K) MAXF (PREDICTED)
1 0.00 -0.91 2.6 FALSE
2 0.00 -0.91 2.4 FALSE
3 0.00 -0.91 2.2 FALSE
4 0.00 -0.91 2.0 FALSE
5 0.83 -0.08 1.8 FALSE
6 0.50 -0.41 1.6 FALSE
7 0.83 -0.08 1.4 FALSE
8 0.83 -0.08 1.2 FALSE
9 (= KMAXF) 0.91 0.00 (=?F (KMAXF)) 1.0 TRUE
10 0.91 0.00 0.8 TRUE
11 0.83 -0.08 0.6 FALSE
12 0.83 -0.08 0.4 FALSE
13 0.91 0.00 0.2 TRUE
14 0.91 0.00 0.0 TRUE
Figure A-3: Features used to evaluate the MAXF model.
20
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1044?1050,
Prague, June 2007. c?2007 Association for Computational Linguistics
Dependency Parsing and Domain Adaptation with LR Models and 
Parser Ensembles 
Kenji Sagae1 and Jun?ichi Tsujii1,2,3 
1Department of Computer Science 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan 
2School of Computer Science, University of Manchester 
3National Center for Text Mining 
{sagae,tsujii}@is.s.u-tokyo.ac.jp 
 
 
Abstract 
We present a data-driven variant of the LR 
algorithm for dependency parsing, and ex-
tend it with a best-first search for probabil-
istic generalized LR dependency parsing.  
Parser actions are determined by a classifi-
er, based on features that represent the cur-
rent state of the parser.  We apply this pars-
ing framework to both tracks of the CoNLL 
2007 shared task, in each case taking ad-
vantage of multiple models trained with 
different learners.  In the multilingual track, 
we train three LR models for each of the 
ten languages, and combine the analyses 
obtained with each individual model with a 
maximum spanning tree voting scheme.  In 
the domain adaptation track, we use two 
models to parse unlabeled data in the target 
domain to supplement the labeled out-of-
domain training set, in a scheme similar to 
one iteration of co-training. 
1 Introduction 
There are now several approaches for multilingual 
dependency parsing, as demonstrated in the 
CoNLL 2006 shared task (Buchholz and Marsi, 
2006).  The dependency parsing approach pre-
sented here extends the existing body of work 
mainly in four ways:  
1. Although stepwise 1  dependency parsing has 
commonly been performed using parsing algo-
                                               
1
 Stepwise parsing considers each step in a parsing algo-
rithm separately, while all-pairs parsing considers entire 
rithms designed specifically for this task, such 
as those described by Nivre (2003) and Yamada 
and Matsumoto (2003), we show that this can 
also be done using the well known LR parsing 
algorithm (Knuth, 1965), providing a connec-
tion between current research on shift-reduce 
dependency parsing and previous parsing work 
using LR and GLR models;  
2. We generalize the standard deterministic step-
wise framework to probabilistic parsing, with 
the use of a best-first search strategy similar to 
the one employed in constituent parsing by Rat-
naparkhi (1997) and later by Sagae and Lavie 
(2006);  
3. We provide additional evidence that the parser 
ensemble approach proposed by Sagae and La-
vie (2006a) can be used to improve parsing ac-
curacy, even when only a single parsing algo-
rithm is used, as long as variation can be ob-
tained, for example, by using different learning 
techniques or changing parsing direction from 
forward to backward (of course, even greater 
gains may be achieved when different algo-
rithms are used, although this is not pursued 
here); and, finally, 
4. We present a straightforward way to perform 
parser domain adaptation using unlabeled data 
in the target domain. 
 
We entered a system based on the approach de-
scribed in this paper in the CoNLL 2007 shared 
                                                                          
trees.  For a more complete definition, see the CoNLL-
X shared task description paper (Buchholz and Marsi, 
2006). 
1044
task (Nivre et al, 2007), which differed from the 
2006 edition by featuring two separate tracks, one 
in multilingual parsing, and a new track on domain 
adaptation for dependency parsers.  In the multi-
lingual parsing track, participants train dependency 
parsers using treebanks provided for ten languages: 
Arabic (Hajic et al, 2004), Basque (Aduriz et al 
2003), Catalan (Mart? et al, 2007), Chinese (Chen 
et al, 2003), Czech (B?hmova et al, 2003), Eng-
lish (Marcus et al, 1993; Johansson and Nugues, 
2007), Greek (Prokopidis et al, 2005), Hungarian 
(Czendes et al, 2005), Italian (Montemagni et al, 
2003), and Turkish (Oflazer et al,  2003).  In the 
domain adaptation track, participants were pro-
vided with English training data from the Wall 
Street Journal portion of the Penn Treebank (Mar-
cus et al, 1993) converted to dependencies (Jo-
hansson and Nugues, 2007) to train parsers to be 
evaluated on material in the biological (develop-
ment set) and chemical (test set) domains (Kulick 
et al, 2004), and optionally on text from the 
CHILDES database (MacWhinney, 2000; Brown, 
1973). 
 Our system?s accuracy was the highest in the 
domain adaptation track (with labeled attachment 
score of 81.06%), and only 0.43% below the top 
scoring system in the multilingual parsing track 
(our average labeled attachment score over the ten 
languages was 79.89%).  We first describe our ap-
proach to multilingual dependency parsing, fol-
lowed by our approach for domain adaptation.  We 
then provide an analysis of the results obtained 
with our system, and discuss possible improve-
ments. 
2 A Probabilistic LR Approach for De-
pendency Parsing 
Our overall parsing approach uses a best-first 
probabilistic shift-reduce algorithm based on the 
LR algorithm (Knuth, 1965).  As such, it follows a 
bottom-up strategy, or bottom-up-trees, as defined 
in Buchholz and Marsi (2006), in contrast to the 
shift-reduce dependency parsing algorithm de-
scribed by Nivre (2003), which is a bottom-up/top-
down hybrid, or bottom-up-spans.  It is unclear 
whether the use of a bottom-up-trees algorithm has 
any advantage over the use of a bottom-up-spans 
algorithm (or vice-versa) in practice, but the avail-
ability of different algorithms that perform the 
same parsing task could be advantageous in parser 
ensembles.  The main difference between our pars-
er and a traditional LR parser is that we do not use 
an LR table derived from an explicit grammar to 
determine shift/reduce actions.  Instead, we use a 
classifier with features derived from much of the 
same information contained in an LR table: the top 
few items on the stack, and the next few items of 
lookahead in the remaining input string.  Addition-
ally, following Sagae and Lavie (2006), we extend 
the basic deterministic LR algorithm with a best-
first search, which results in a parsing strategy sim-
ilar to generalized LR parsing (Tomita, 1987; 
1990), except that we do not perform Tomita?s 
stack-merging operations.   
The resulting algorithm is projective, and non-
projectivity is handled by pseudo-projective trans-
formations as described in (Nivre and Nilsson, 
2005).  We use Nivre and Nilsson?s PATH 
scheme2. 
For clarity, we first describe the basic variant of 
the LR algorithm for dependency parsing, which is 
a deterministic stepwise algorithm.  We then show 
how we extend the deterministic parser into a best-
first probabilistic parser. 
2.1 Dependency Parsing with a Data-Driven 
Variant of the LR Algorithm 
The two main data structures in the algorithm are a 
stack S and a queue Q.  S holds subtrees of the fi-
nal dependency tree for an input sentence, and Q 
holds the words in an input sentence.  S is initia-
lized to be empty, and Q is initialized to hold every 
word in the input in order, so that the first word in 
the input is in the front of the queue.3 
The parser performs two main types of actions: 
shift and reduce.  When a shift action is taken, a 
word is shifted from the front of Q, and placed on 
the top of S (as a tree containing only one node, the 
word itself).  When a reduce action is taken, the 
                                               
2
 The PATH scheme was chosen (even though Nivre and 
Nilsson report slightly better results with the HEAD 
scheme) because it does not result in a potentially qua-
dratic increase in the number of dependency label types, 
as observed with the HEAD and HEAD+PATH 
schemes.  Unfortunately, experiments comparing the 
use of the different pseudo-projectivity schemes were 
not performed due to time constraints. 
3
 We append a ?virtual root? word to the beginning of 
every sentence, which is used as the head of every word 
in the dependency structure that does not have a head in 
the sentence. 
1045
two top items in S (s1 and s2) are popped, and a 
new item is pushed onto S.  This new item is a tree 
formed by making the root s1 of a dependent of the 
root of s2, or the root of s2 a dependent of the root 
of s1.  Depending on which of these two cases oc-
cur, we call the action reduce-left or reduce-right, 
according to whether the head of the new tree is to 
the left or to the right its new dependent.  In addi-
tion to deciding the direction of a reduce action, 
the label of the newly formed dependency arc must 
also be decided.  
Parsing terminates successfully when Q is emp-
ty (all words in the input have been processed) and 
S contains only a single tree (the final dependency 
tree for the input sentence).  If Q is empty, S con-
tains two or more items, and no further reduce ac-
tions can be taken, parsing terminates and the input 
is rejected.  In such cases, the remaining items in S 
contain partial analyses for contiguous segments of 
the input. 
2.2 A Probabilistic LR Model for Dependen-
cy Parsing 
In the traditional LR algorithm, parser states are 
placed onto the stack, and an LR table is consulted 
to determine the next parser action.  In our case, 
the parser state is encoded as a set of features de-
rived from the contents of the stack S and queue Q, 
and the next parser action is determined according 
to that set of features.  In the deterministic case 
described above, the procedure used for determin-
ing parser actions (a classifier, in our case) returns 
a single action.  If, instead, this procedure returns a 
list of several possible actions with corresponding 
probabilities, we can then parse with a model simi-
lar to the probabilistic LR models described by 
Briscoe and Carroll (1993), where the probability 
of a parse tree is the product of the probabilities of 
each of the actions taken in its derivation. 
To find the most probable parse tree according 
to the probabilistic LR model, we use a best-first 
strategy.  This involves an extension of the deter-
ministic shift-reduce into a best-first shift-reduce 
algorithm.  To describe this extension, we first in-
troduce a new data structure Ti that represents a 
parser state, which includes a stack Si, a queue Qi, 
and a probability Pi.  The deterministic algorithm 
is a special case of the probabilistic algorithm 
where we have a single parser state T0 that contains 
S0 and Q0, and the probability of the parser state is 
1.  The best-first algorithm, on the other hand, 
keeps a heap H containing multiple parser states 
T0... Tm.  These states are ordered in the heap ac-
cording to their probabilities, which are determined 
by multiplying the probabilities of each of the 
parser actions that resulted in that parser state.  The 
heap H is initialized to contain a single parser state 
T0, which contains a stack S0, a queue Q0 and prob-
ability P0 = 1.0.  S0 and Q0 are initialized in the 
same way as S and Q in the deterministic algo-
rithm.  The best-first algorithm then loops while H 
is non-empty.  At each iteration, first a state Tcurrent 
is popped from the top of H.  If Tcurrent corresponds 
to a final state (Qcurrent is empty and Scurrent contains 
a single item), we return the single item in Scurrent 
as the dependency structure corresponding to the 
input sentence.  Otherwise, we get a list of parser 
actions act0...actn (with associated probabilities 
Pact0...Pactn) corresponding to state Tcurrent.  For 
each of these parser actions actj, we create a new 
parser state Tnew by applying actj to Tcurrent, and set 
the probability Tnew to be Pnew = Pcurrnet * Pactj.  
Then, Tnew is inserted into the heap H.  Once new 
states have been inserted onto H for each of the n 
parser actions, we move on to the next iteration of 
the algorithm. 
3 Multilingual Parsing Experiments 
For each of the ten languages for which training 
data was provided in the multilingual track of the 
CoNLL 2007 shared task, we trained three LR 
models as follows.  The first LR model for each 
language uses maximum entropy classification 
(Berger et al, 1996) to determine possible parser 
actions and their probabilities4.  To control overfit-
ting in the MaxEnt models, we used box-type in-
equality constraints (Kazama and Tsujii, 2003). 
The second LR model for each language also uses 
MaxEnt classification, but parsing is performed 
backwards, which is accomplished simply by re-
versing the input string before parsing starts.  Sa-
gae and Lavie (2006a) and Zeman and ?abokrtsk? 
(2005) have observed that reversing the direction 
of stepwise parsers can be beneficial in parser 
combinations. The third model uses support vector 
machines 5  (Vapnik, 1995) using the polynomial 
                                               
4
 Implementation by Yoshimasa Tsuruoka, available at 
http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/ 
5
 Implementation by Taku Kudo, available at 
http://chasen.org/~taku/software/TinySVM/ and all vs. 
all was used for multi-class classification. 
1046
kernel with degree 2. Probabilities were estimated 
for SVM outputs using the method described in 
(Platt, 1999), but accuracy improvements were not 
observed during development when these esti-
mated probabilities were used instead of simply the 
single best action given by the classifier (with 
probability 1.0), so in practice the SVM parsing 
models we used were deterministic. 
At test time, each input sentence is parsed using 
each of the three LR models, and the three result-
ing dependency structures are combined according 
to the maximum-spanning-tree parser combination 
scheme6 (Sagae and Lavie, 2006a) where each de-
pendency proposed by each of the models has the 
same weight (it is possible that one of the more 
sophisticated weighting schemes proposed by Sa-
gae and Lavie may be more effective, but these 
were not attempted).  The combined dependency 
tree is the final analysis for the input sentence. 
Although it is clear that fine-tuning could pro-
vide accuracy improvements for each of the mod-
els in each language, the same set of meta-
parameters and features were used for all of the ten 
languages, due to time constraints during system 
development.  The features used were7:  
 
? For the subtrees in S(1) and S(2) 
? the number of children of the root word of 
the subtrees; 
? the number of children of the root word of 
the subtree to the right of the root word; 
? the number of children of the root word of 
the subtree to the left of the root word; 
? the POS tag and DEPREL of the rightmost 
and leftmost children;  
? The POS tag of the word immediately to the 
right of the root word of S(2); 
? The POS tag of the word immediately to the 
left of S(1); 
                                               
6
 Each dependency tree is deprojectivized before the 
combination occurs. 
7
 S(n) denotes the nth item from the top of the stack 
(where S(1) is the item on top of the stack), and Q(n) 
denotes the nth item in the queue.  For a description of 
the features names in capital letters, see the shared task 
description (Nivre et al, 2007). 
? The previous parser action; 
? The features listed for the root words of the 
subtrees in table 1.   
In addition, the MaxEnt models also used selected 
combinations of these features.  The classes used 
to represent parser actions were designed to encode 
all aspects of an action (shift vs. reduce, right vs. 
left, and dependency label) simultaneously. 
Results for each of the ten languages are shown 
in table 2 as labeled and unlabeled attachment 
scores, along with the average labeled attachment 
score and highest labeled attachment score for all 
participants in the shared task.  Our results shown 
in boldface were among the top three scores for 
those particular languages (five out of the ten lan-
guages). 
 
 
 S(1) S(2) S(3) Q(0) Q(1) Q(3) 
WORD x x x x x  
LEMMA x x  x   
POS x x x x x x 
CPOS x x  x   
FEATS x x  x   
Table 1: Additional features. 
 
 
 
Language LAS UAS Avg 
LAS 
Top 
LAS 
Arabic 74.71 84.04 68.34 76.52 
Basque 74.64 81.19 68.06 76.94 
Catalan 88.16 93.34 79.85 88.70 
Chinese 84.69 88.94 76.59 84.69 
Czech 74.83 81.27 70.12 80.19 
English 89.01 89.87 80.95 89.61 
Greek 73.58 80.37 70.22 76.31 
Hungarian 79.53 83.51 71.49 80.27 
Italian 83.91 87.68 78.06 84.40 
Turkish 75.91 82.72 70.06 79.81 
ALL 79.90 85.29 65.50 80.32 
Table 2: Multilingual results. 
 
 
4 Domain Adaptation Experiments 
In a similar way as we used multiple LR models in 
the multilingual track, in the domain adaptation 
track we first trained two LR models on the out-of-
1047
domain labeled training data.  The first was a for-
ward MaxEnt model, and the second was a back-
ward SVM model.  We used these two models to 
perform a procedure similar to a single iteration of 
co-training, except that selection of the newly (au-
tomatically) produced training instances was done 
by selecting sentences for which the two models 
produced identical analyses.  On the development 
data we verified that sentences for which there was 
perfect agreement between the two models had 
labeled attachment score just above 90 on average, 
even though each of the models had accuracy be-
tween 78 and 79 over the entire development set. 
Our approach was as follows:  
 
1. We trained the forward MaxEnt and backward 
SVM models using the out-of-domain labeled 
training data;  
2. We then used each of the models to parse the 
first two of the three sets of domain-specific 
unlabeled data that were provided (we did not 
use the larger third set) 
3. We compared the output for the two models, 
and selected only identical analyses that were 
produced by each of the two separate models;  
4. We added those analyses (about 200k words in 
the test domain) to the original (out-of-
domain) labeled training set;  
5. We retrained the forward MaxEnt model with 
the new larger training set; and finally  
6. We used this model to parse the test data. 
Following this procedure we obtained a labeled 
attachment score of 81.06, and unlabeled attach-
ment score of 83.42, both the highest scores for 
this track.  This was done without the use of any 
additional resources (closed track), but these re-
sults are also higher than the top score for the open 
track, where the use of certain additional resources 
was allowed.  See (Nivre et al, 2007). 
5 Analysis and Discussion 
One of the main assumptions in our use of differ-
ent models based on the same algorithm is that 
while the output generated by those models may 
often differ, agreement between the models is an 
indication of correctness.  In our domain adapta-
tion approach, this was clearly true.  In fact, the 
approach would not have worked if this assump-
tion was false.  Experiments on the development 
set were encouraging.  As stated before, when the 
parsers agreed, labeled attachment score was over 
90, even though the score of each model alone was 
lower than 79.  The domain-adapted parser had a 
score of 82.1, a significant improvement.  Interes-
tingly, the ensemble used in the multilingual track 
also produced good results on the development set 
for the domain adaptation data, without the use of 
the unlabeled data at all, with a score of 81.9 (al-
though the ensemble is more expensive to run). 
The different models used in each track were 
distinct in a few ways: (1) direction (forward or 
backward); (2) learner (MaxEnt or SVM); and (3) 
search strategy (best-first or deterministic).  Of 
those differences, the first one is particularly inter-
esting in single-stack shift-reduce models, as ours.  
In these models, the context to each side of a (po-
tential) dependency differs in a fundamental way.  
To one side, we have tokens that have already been 
processed and are already in subtrees, and to the 
other side we simply have a look-ahead of the re-
maining input sentence.  This way, the context of 
the same dependency in a forward parser may dif-
fer significantly from the context of the same de-
pendency in a backward parser.  Interestingly, the 
accuracy scores of the MaxEnt backward models 
were found to be generally just below the accuracy 
of their corresponding forward models when tested 
on development data, with two exceptions: Hunga-
rian and Turkish.  In Hungarian, the accuracy 
scores produced by the forward and backward 
MaxEnt LR models were not significantly differ-
ent, with both labeled attachment scores at about 
77.3 (the SVM model score was 76.1, and the final 
combination score on development data was 79.3).  
In Turkish, however, the backward score was sig-
nificantly higher than the forward score, 75.0 and 
72.3, respectively. The forward SVM score was 
73.1, and the combined score was 75.8.   In expe-
riments performed after the official submission of 
results, we evaluated a backward SVM model 
(which was trained after submission) on the same 
development set, and found it to be significantly 
more accurate than the forward model, with a score 
of 75.7.  Adding that score to the combination 
raised the combination score to 77.9 (a large im-
provement from 75.8).  The likely reason for this 
difference is that over 80% of the dependencies in 
the Turkish data set have the head to the right of 
1048
the dependent, while only less than 4% have the 
head to the left.  This means that the backward 
model builds much more partial structure in the 
stack as it consumes input tokens, while the for-
ward model must consume most tokens before it 
starts making attachments.  In other words, context 
in general in the backward model has more struc-
ture, and attachments are made while there are still 
look-ahead tokens, while the opposite is generally 
true in the forward model. 
6 Conclusion  
Our results demonstrate the effectiveness of even 
small ensembles of parsers that are relatively 
similar (using the same features and the same 
algorithm).  There are several possible extensions 
and improvements to the approach we have 
described.  For example, in section 3 we mention 
the use of different weighting schemes in 
dependency voting.  We list additional ideas that 
were not attempted due to time constraints, but that 
are likely to produce improved results. 
One of the simplest improvements to our ap-
proach is simply to train more models with no oth-
er changes to our set-up.  As mentioned in section 
5, the addition of a backward SVM model did im-
prove accuracy on the Turkish set significantly, 
and it is likely that improvements would also be 
obtained in other languages.  In addition, other 
learning approaches, such as memory-based lan-
guage processing (Daelemans and Van den Bosch, 
2005), could be used.  A drawback of adding more 
models that became obvious in our experiments 
was the increased cost of both training (for exam-
ple, the SVM parsers we used required significant-
ly longer to train than the MaxEnt parsers) and 
run-time (parsing with MBL models can be several 
times slower than with MaxEnt, or even SVM).  A 
similar idea that may be more effective, but re-
quires more effort, is to add parsers based on dif-
ferent approaches.  For example, using MSTParser 
(McDonald and Pereira, 2005), a large-margin all-
pairs parser, in our domain adaptation procedure 
results in significantly improved accuracy (83.2 
LAS).  Of course, the use of different approaches 
used by different groups in the CoNLL 2006 and 
2007 shared tasks represents great opportunity for 
parser ensembles. 
Acknowledgements 
We thank the shared task organizers and treebank 
providers.  We also thank the reviewers for their 
comments and suggestions, and Yusuke Miyao for 
insightful discussions.  This work was supported in 
part by Grant-in-Aid for Specially Promoted Re-
search 18002007.      
References 
A. Abeill?, editor. 2003. Treebanks: Building and Using 
Parsed Corpora. Kluwer.  
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 
1996. A maximum entropy approach to 
naturallanguage processing. Computational 
Linguistics, 22(1):39?71. 
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), pages 201?204. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 2003. 
The PDT: a 3-level annotation scenario. In Abeill? 
(2003), chapter 7, 103?127. 
E. Briscoe and J. Carroll. 1993. Generalized Probabilis-
tic LR Parsing of Natural Language (Corpora) with 
Unification-Based Grammars. In Computational Lin-
guistics, 19(1), pages 25-59. 
R. Brown. 1973. A First Language: The Early Stages. 
Harvard University Press. 
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task 
on Multilingual Dependency Parsing. In Proc. of the 
Tenth Conference on Computational Natural 
Language Learning (CoNLL-X). New York, NY. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementation. 
In Abeill? (2003), chapter 13, pages 231?248. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
W. Daelemans and A. Van den Bosch.  2005.  Memory-
based language processing.  Cambridge University 
Press.  
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, pages 110?117. 
1049
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. In 
Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
J. Kazama, and J. Tsujii. 2003. Evaluation and 
extension of maximum entropy models with ine-
quality constraints.  In Proceedings of EMNLP 2003. 
D. Knuth. 1965. On the translation of languages from 
left to right, Information and Control 8, 607-639. 
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc- 
Donald, M. Palmer, A. Schein, and L. Ungar. 2004. 
Integrated annotation for biomedical information ex- 
traction. In Proc. of the Human Language 
Technology Conference and the Annual Meeting of 
the North American Chapter of the Association for 
Computational Linguistics (HLT/NAACL).  
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
R. McDonald, K.Crammer, and F. Pereira. 2005.  On-
line large-margin training of dependency parsers. In 
Proc. of the 43rd Annual Meeting of the Association 
for Computational Linguistics, 2005 
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
pages 189?210. 
J. Nivre. 2003. An efficient algorithm for dependency 
parsing.  In Proc. of the Eighth International 
Workshop on Parsing Technologies (IWPT?03). 
Nancy, France. 
J. Nivre, and J. Nilsson. 2005. Pseudo-Projective 
Dependency Parsing. In Proceedings of the 43rd 
Annual Meeting of the Association for Computational 
Linguistics (ACL), 99-106. Ann Arbor, MI. 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 shared 
task on dependency parsing. In Proc. of the CoNLL 
2007 Shared Task. Joint Conf. on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r. 
2003. Building a Turkish treebank. In Abeill? (2003), 
chapter 15, pages 261?277.  
J. Platt. 1999. Probabilistic Outputs for Support Vector 
Machines and Comparisons to Regularized 
Likelihood Methods. In Advances in Large Margin 
Classiers, MIT Press. 
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th Workshop 
on Treebanks and Linguistic Theories (TLT), pages 
149?160. 
A. Ratnaparkhi. 1997. A linear observed time statistical 
parser based on maximum entropy models. In 
Proceedings of the Second Conference on Empirical 
Methods in Natural Language Processing. Prov-
idence, RI 
K. Sagae, and A. Lavie. 2006. A best-first probabilistic 
shift-reduce parser. Proceedings of the 43rd Meeting 
of the Association for Computational Linguistics - 
posters (ACL'06). Sydney, Australia. 
K. Sagae, and A. Lavie. 2006a. Parser combination by 
reparsing. Proceedings of the 2006 Human Language 
Technology Conference of the North American 
Chapter of the Association for Computational 
Linguistics - short papers (HLT-NAACL'06). New 
York, NY. 
M. Tomita. 1987. An efficient augmented context-free 
parsing algorithm. Computational Linguistics, 13:31?
46. 
M. Tomita. 1990. The generalized LR parser/compiler - 
version 8.4. In Proceedings of the International 
Conference on Computational Linguistics 
(COLING?90), pages 59?63. Helsinki, Finland. 
V. N. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer-Verlag. 
H. Yamada, and Y. Matsumoto. 2003.  Statistical 
dependency analysis with support vector machines. 
In Proceedings of the Eighth International Workshop 
on Parsing Technologies (IWPT?03). Nancy, France. 
D. Zeman, Z. ?abokrtsk?. 2005. Improving Parsing Ac-
curacy by Combining Diverse Dependency Parsers. 
In Proceedings of the International Workshop on 
Parsing Technologies (IWPT 2005). Vancouver, Brit-
ish Columbia. 
 
1050
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624?631,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
HPSG Parsing with Shallow Dependency Constraints
Kenji Sagae1 and Yusuke Miyao1 and Jun?ichi Tsujii1,2,3
1Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
2School of Computer Science, University of Manchester
3National Center for Text Mining
{sagae,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We present a novel framework that com-
bines strengths from surface syntactic pars-
ing and deep syntactic parsing to increase
deep parsing accuracy, specifically by com-
bining dependency and HPSG parsing. We
show that by using surface dependencies to
constrain the application of wide-coverage
HPSG rules, we can benefit from a num-
ber of parsing techniques designed for high-
accuracy dependency parsing, while actu-
ally performing deep syntactic analysis. Our
framework results in a 1.4% absolute im-
provement over a state-of-the-art approach
for wide coverage HPSG parsing.
1 Introduction
Several efficient, accurate and robust approaches to
data-driven dependency parsing have been proposed
recently (Nivre and Scholz, 2004; McDonald et al,
2005; Buchholz and Marsi, 2006) for syntactic anal-
ysis of natural language using bilexical dependency
relations (Eisner, 1996). Much of the appeal of these
approaches is tied to the use of a simple formalism,
which allows for the use of efficient parsing algo-
rithms, as well as straightforward ways to train dis-
criminative models to perform disambiguation. At
the same time, there is growing interest in pars-
ing with more sophisticated lexicalized grammar
formalisms, such as Lexical Functional Grammar
(LFG) (Bresnan, 1982), Lexicalized Tree Adjoin-
ing Grammar (LTAG) (Schabes et al, 1988), Head-
driven Phrase Structure Grammar (HPSG) (Pollard
and Sag, 1994) and Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000), which represent deep
syntactic structures that cannot be expressed in a
shallower formalism designed to represent only as-
pects of surface syntax, such as the dependency
formalism used in current mainstream dependency
parsing.
We present a novel framework that combines
strengths from surface syntactic parsing and deep
syntactic parsing, specifically by combining depen-
dency and HPSG parsing. We show that, by us-
ing surface dependencies to constrain the applica-
tion of wide-coverage HPSG rules, we can bene-
fit from a number of parsing techniques designed
for high-accuracy dependency parsing, while actu-
ally performing deep syntactic analysis. From the
point of view of HPSG parsing, accuracy can be im-
proved significantly through the use of highly ac-
curate discriminative dependency models, without
the difficulties involved in adapting these models
to a more complex and linguistically sophisticated
formalism. In addition, improvements in depen-
dency parsing accuracy are converted directly into
improvements in HPSG parsing accuracy. From the
point of view of dependency parsing, the applica-
tion of HPSG rules to structures generated by a sur-
face dependency model provides a principled and
linguistically motivated way to identify deep syntac-
tic phenomena, such as long-distance dependencies,
raising and control.
We begin by describing our dependency and
HPSG parsing approaches in section 2. In section
3, we present our framework for HPSG parsing with
shallow dependency constraints, and in section 4 we
624
Figure 1: HPSG parsing
evaluate this framework empirically. Sections 5 and
6 discuss related work and conclusions.
2 Fast dependency parsing and
wide-coverage HPSG parsing
2.1 Data-driven dependency parsing
Because we use dependency parsing as a step in
deep parsing, it is important that we choose a pars-
ing approach that is not only accurate, but also effi-
cient. The deterministic shift/reduce classifier-based
dependency parsing approach (Nivre and Scholz,
2004) has been shown to offer state-of-the-art accu-
racy (Nivre et al, 2006) with high efficiency due to
a greedy search strategy. Our approach is based on
Nivre and Scholz?s approach, using support vector
machines for classification of shift/reduce actions.
2.2 Wide-coverage HPSG parsing
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemas explain general
construction rules, and a large number of lexical en-
tries express word-specific syntactic/semantic con-
straints. Figure 1 shows an example of the process
of HPSG parsing. First, lexical entries are assigned
to each word in a sentence. In Figure 1, lexical
entries express subcategorization frames and pred-
icate argument structures. Parsing proceeds by ap-
plying schemas to lexical entries. In this example,
the Head-Complement Schema is applied to the lex-
ical entries of ?tried? and ?running?. We then obtain
a phrasal structure for ?tried running?. By repeat-
edly applying schemas to lexical/phrasal structures,
Figure 2: Extracting HPSG lexical entries from the
Penn Treebank
we finally obtain an HPSG parse tree that covers the
entire sentence.
In this paper, we use an HPSG parser developed
by Miyao and Tsujii (2005). This parser has a wide-
coverage HPSG lexicon which is extracted from the
Penn Treebank. Figure 2 illustrates their method
for extraction of HPSG lexical entries. First, given
a parse tree from the Penn Treebank (top), HPSG-
style constraints are added and an HPSG-style parse
tree is obtained (middle). Lexical entries are then ex-
tracted from the terminal nodes of the HPSG parse
tree (bottom). This way, in addition to a wide-
coverage lexicon, we also obtain an HPSG treebank,
which can be used as training data for disambigua-
tion models.
The disambiguation model of this parser is based
on a maximum entropy model (Berger et al, 1996).
The probability p(T |W ) of an HPSG parse tree T
for the sentence W = ?w1, . . . , wn? is given as:
p(T |W ) = p(T |L,W )p(L|W )
=
1
Z
exp
(
?
i
?ifi(T )
)
?
j
p(lj |W ),
where L = ?l1, . . . , ln? are lexical entries and
625
p(li|W ) is the supertagging probability, i.e., the
probability of assignining the lexical entry li to wi
(Ninomiya et al, 2006). The probability p(T |L,W )
is a maximum entropy model on HPSG parse trees,
where Z is a normalization factor, and feature func-
tions fi(T ) represent syntactic characteristics, such
as head words, lengths of phrases, and applied
schemas. Given the HPSG treebank as training data,
the model parameters ?i are estimated so as to maxi-
mize the log-likelihood of the training data (Malouf,
2002).
3 HPSG parsing with dependency
constraints
While a number of fairly straightforward models can
be applied successfully to dependency parsing, de-
signing and training HPSG parsing models has been
regarded as a significantly more complex task. Al-
though it seems intuitive that a more sophisticated
linguistic formalism should be more difficult to pa-
rameterize properly, we argue that the difference in
complexity between HPSG and dependency struc-
tures can be seen as incremental, and that the use
of accurate and efficient techniques to determine the
surface dependency structure of a sentence provides
valuable information that aids HPSG disambigua-
tion. This is largely because HPSG is based on a lex-
icalized grammar formalism, and as such its syntac-
tic structures have an underlying dependency back-
bone. However, HPSG syntactic structures includes
long-distance dependencies, and the underlying de-
pendency structure described by and HPSG structure
is a directed acyclic graph, not a dependency tree (as
used by mainstream approaches to data-driven de-
pendency parsing). This difference manifests itself
in words that have multiple heads. For example, in
the sentence I tried to run, the pronoun I is a depen-
dent of tried and of run. This makes it possible to
represent that I is the subject of both verbs, precisely
the kind of information that cannot be represented in
dependency parsing. If we ignore long-distance de-
pendencies, however, HPSG structures can be seen
as lexicalized trees that can be easily converted into
dependency trees.
Given that for an HPSG representation of the syn-
tactic structure of a sentence we can determine a
dependency tree by removing long-distance depen-
dencies, we can use dependency parsing techniques
(such as the deterministic dependency parsing ap-
proach mentioned in section 2.1) to determine the
underlying dependency trees in HPSG structures.
This is the basis for the parsing framework presented
here. In this approach, deep dependency analysis
is done in two stages. First, a dependency parser
determines the shallow dependency tree for the in-
put sentence. This shallow dependency tree corre-
sponds to the underlying dependency graph of the
HPSG structure for the input sentence, without de-
pendencies that roughly correspond to deep syntax.
The second step is to perform HPSG parsing, as
described in section 2.2, but using the shallow de-
pendency tree to constrain the application of HPSG
rules. We now discuss these two steps in more detail.
3.1 Determining shallow dependencies in
HPSG structures using dependency parsing
In order to apply a data-driven dependency ap-
proach to the task of identifying the shallow de-
pendency tree in HPSG structures, we first need a
corpus of such dependency trees to serve as train-
ing data. We created a dependency training corpus
based on the Penn Treebank (Marcus et al, 1993),
or more specifically on the HPSG Treebank gener-
ated from the Penn Treebank (see section 2.2). For
each HPSG structure in the HPSG Treebank, a de-
pendency tree is extracted in two steps. First, the
HPSG tree is converted into a CFG-style tree, sim-
ply by removing long-distance dependency links be-
tween nodes. A dependency tree is then extracted
from the resulting lexicalized CFG-style tree, as is
commonly done for converting constituent trees into
dependency trees after the application of a head-
percolation table (Collins, 1999).
Once a dependency training corpus is available,
it is used to train a dependency parser as described
in section 2.1. This is done by training a classifier
to determine parser actions based on local features
that represent the current state of the parser (Nivre
and Scholz, 2004; Sagae and Lavie, 2005). Train-
ing data for the classifier is obtained by applying the
parsing algorithm over the training sentences (for
which the correct dependency structures are known)
and recording the appropriate parser actions that re-
sult in the formation of the correct dependency trees,
coupled with the features that represent the state of
626
the parser mentioned in section 2.1. An evaluation
of the resulting dependency parser and its efficacy in
aiding HPSG parsing is presented in section 4.
3.2 Parsing with dependency constraints
Given a set of dependencies, the bottom-up process
of HPSG parsing can be constrained so that it does
not violate the given dependencies. This can be
achieved by a simple extension of the parsing algo-
rithm, as follows. During parsing, we store the lex-
ical head of each partial parse tree. In each schema
application, we can determine which child is the
head; for example, the left child is the head when
we apply the Head-Complement Schema. Given this
information and lexical heads, the parser can iden-
tify the dependency produced by this schema appli-
cation, and can therefore judge whether the schema
application violates the dependency constraints.
This method forces the HPSG parser to produce
parse trees that strictly conform to the output of
the dependency parser. However, this means that
the HPSG parser outputs no successful parse results
when it cannot find the parse tree that is completely
consistent with the given dependencies. This situ-
ation may occur when the dependency parser pro-
duces structures that are not covered in the HPSG
grammar. This is especially likely with a fully data-
driven dependency parser that uses local classifica-
tion, since its output may not be globally consistent
grammatically. In addition, the HPSG grammar is
extracted from the HPSG Treebank using a corpus-
based procedure, and it does not necessarily cover
all possible grammatical phenomena in unseen text
(Miyao and Tsujii, 2005).
We therefore propose an extension of this ap-
proach that uses predetermined dependencies as soft
constraints. Violations of schema applications are
detected in the same way as before, but instead of
strictly prohibiting schema applications, we penal-
ize the log-likelihood of partial parse trees created
by schema applications that violate the dependen-
cies constraints. Given a negative value ?, we add
? to the log-probability of a partial parse tree when
the schema application violates the dependency con-
straints. That is, when a parse tree violates n depen-
dencies, the log-probability of the parse tree is low-
ered by n?. The meta parameter ? is determined so
as to maximize the accuracy on the development set.
Soft dependency constraints can be implemented
as explained above as a straightforward extension of
the parsing algorithm. In addition, it is easily inte-
grated with beam thresholding methods of parsing.
Because beam thresholding discards partial parse
trees that have low log-probabilities, we can ex-
pect that the parser would discard partial parse trees
based on violation of the dependency constraints.
4 Experiments
We evaluate the accuracy of HPSG parsing with de-
pendency constraints on the HPSG Treebank (Miyao
et al, 2003), which is extracted from the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993)1. Sections 02-21 were used for training
(for HPSG and dependency parsers), section 22 was
used as development data, and final testing was per-
formed on section 23. Following previous work on
wide-coverage parsing with lexicalized grammars
using the Penn Treebank, we evaluate the parser by
measuring the accuracy of predicate-argument rela-
tions in the parser?s output. A predicate-argument
relation is defined as a tuple ??,wh, a, wa?, where
? is the predicate type (e.g. adjective, intransitive
verb), wh is the head word of the predicate, a is the
argument label (MODARG, ARG1, ... , ARG4), and
wa is the head word of the argument. Labeled pre-
cision (LP)/labeled recall (LR) is the ratio of tuples
correctly identified by the parser. These predicate-
argument relations cover the full range of syntactic
dependencies produced by the HPSG parser (includ-
ing, long-distance dependencies, raising and control,
in addition to surface dependencies).
In the experiments presented in this section, in-
put sentences were automatically tagged with parts-
of-speech with about 97% accuracy, using a max-
imum entropy POS tagger. We also report results
on parsing text with gold standard POS tags, where
explicitly noted. This provides an upper-bound on
what can be expected if a more sophisticated multi-
tagging scheme (James R. Curran and Vadas, 2006)
is used, instead of hard assignment of single tags in
a preprocessing step as done here.
1The extraction software can be obtained from http://www-
tsujii.is.s.u-tokyo.ac.jp/enju.
627
4.1 Baseline
HPSG parsing results using the same HPSG gram-
mar and treebank have recently been reported by
Miyao and Tsujii (2005) and Ninomia et al (2006).
By running the HPSG parser described in section 2.2
on the development data without dependency con-
straints, we obtain similar values of LP (86.8%) and
LR (85.6%) as those reported by Miyao and Tsu-
jii (Miyao and Tsujii, 2005). Using the extremely
lexicalized framework of (Ninomiya et al, 2006) by
performing supertagging before parsing, we obtain
similar accuracy as Ninomiya et al (87.1% LP and
85.9% LR).
4.2 Dependency constraints and the penalty
parameter
Parsing the development data with hard dependency
constraints confirmed the intuition that these con-
straints often describe dependency structures that do
not conform to HPSG schema used in parsing, re-
sulting in parse failures. To determine the upper-
bound on HPSG parsing with hard dependency con-
straints, we set the HPSG parser to disallow the ap-
plication of any rules that result in the creation of
dependencies that violate gold standard dependen-
cies. This results in high precision (96.7%), but re-
call is low (82.3%) due to parse failures caused by
lack of grammatical coverage 2. Using dependen-
cies produced by the shift-reduce SVM parser, we
obtain 91.5% LP and 65.7% LR. This represents a
large gain in precision over the baseline, but an even
greater loss in recall, which limits the usefulness of
the parser, and severely hurts the appeal of hard con-
straints.
We focus the rest of our experiments on parsing
with soft dependency constraints. As explained in
section 3, this involves setting the penalty parame-
ter ?. During parsing, we subtract ? from the log-
probability of applying any schema that violates the
dependency constraints given to the HPSG parser.
Figure 3 illustrates the effect of ? when gold stan-
dard dependencies (and gold standard POS tags) are
used. We note that setting ? = 0 causes the parser
2Although the HPSG grammar does not have perfect cov-
erage of unseen text, it supports complete and mostly correct
analyses for all sentences in the development set. However,
when we require completely correct analyses by using hard con-
straints, lack of coverage may cause parse failures.
8990919293949596 0
5
10
15
20
25
30
35
Penal
ty
Accuracy
Precis
ion
Recal
l
F-sco
re
Figure 3: The effect of ? on HPSG parsing con-
strained by gold standard dependencies.
to ignore dependency constraints, providing base-
line performance. Conversely, setting a high enough
value (? = 30 is sufficient, in practice) causes any
substructures that violate the dependency constraints
to be used only when they are absolutely neces-
sary to produce a valid parse for the input sentence.
In figure 3, this corresponds to an upper-bound on
the accuracy of parsing with soft dependency con-
straints (94.7% f-score), since gold standard depen-
dencies are used.
We set ? empirically with simple hill climbing on
the development set. Because it is expected that the
optimal value of ? depends on the accuracy of the
surface dependency parser, we set separate values
for parsing with a POS tagger or with gold standard
POS tags. Figure 4 shows the accuracy of HPSG
predicate-argument relations obtained with depen-
dency constraints determined by dependency pars-
ing with gold standard POS tags. With both au-
tomatically assigned and gold standard POS tags,
we observe an improvement of about 0.6% in pre-
cision, recall and f-score, when the optimal ? value
is used in each case. While this corresponds to a rel-
ative error reduction of over 6% (or 12%, if we con-
sider the upper-bound dictated by imperfect gram-
matical coverage), a more interesting aspect of this
framework is that it allows techniques designed for
improving dependency accuracy to improve HPSG
parsing accuracy directly, as we illustrate next.
628
89.489.689.89090.290.490.690.891 0
0.5
1
1.5
2
2.5
3
3.5
Penal
ty
Accuracy
Precis
ion
Recal
l
F-sco
re
Figure 4: The effect of ? on HPSG parsing con-
strained by the output of a dependency parser using
gold standard POS tags.
4.3 Determining constraints with dependency
parser combination
Parser combination has been shown to be a power-
ful way to obtain very high accuracy in dependency
parsing (Sagae and Lavie, 2006). Using dependency
constraints allows us to improve HPSG parsing ac-
curacy simply by using an existing parser combina-
tion approach. As a first step, we train two addi-
tional parsers with the dependencies extracted from
the HPSG Treebank. The first uses the same shift-
reduce framework described in section 2.1, but it
process the input from right to left (RL). This has
been found to work well in previous work on depen-
dency parser combination (Zeman and Z?abokrtsky?,
2005; Sagae and Lavie, 2006). The second parser
is MSTParser, the large-margin maximum spanning
tree parser described in (McDonald et al, 2005)3.
We examine the use of two combination schemes:
one using two parsers, and one using three parsers.
The first combination approach is to keep only de-
pendencies for which there is agreement between the
two parsers. In other words, dependencies that are
proposed by one parser but not the other are simply
discarded. Using the left-to-right shift-reduce parser
and MSTParser, we find that this results in very high
precision of surface dependencies on the develop-
ment data. In the second approach, combination of
3Downloaded from http://sourceforge.net/projects/mstparser
the three dependency parsers is done according to
the maximum spanning tree combination scheme of
Sagae and Lavie (2006), which results in high accu-
racy of surface dependencies. For each of the com-
bination approaches, we use the resulting dependen-
cies as constraints for HPSG parsing, determining
the optimal value of ? on the development set in
the same way as done for a single parser. Table 1
summarizes our experiments on development data
using parser combinations to produce dependency
constraints 4. The two combination approaches are
denoted as C1 and C2.
Parser Dep ? HPSG Diff
none (baseline) ? ? 86.5 ?
LR shift-reduce 91.2 1.5 87.1 0.6
RL shift-reduce 90.1 ? ?
MSTParser 91.0 ? ?
C1 (agreement) 96.8* 2.5 87.4 0.9
C2 (MST) 92.4 2.5 87.4 0.9
Table 1: Summary of results on development data.
* The shallow accuracy of combination C1 corre-
sponds to the dependency precision (no dependen-
cies were reported for 8% of all words in the devel-
opment set).
4.4 Results
Having determined ? values on development data
for the shift-reduce dependency parser, the two-
parser agreement combination, and the three-parser
maximum spanning tree combination, we parse the
test data (section 23) using these three different
sources of dependency constraints for HPSG pars-
ing. Our final results are shown in table 2, where
we also include the results published in (Ninomiya
et al, 2006) for comparison purposes, and the result
of using dependency constraints obtained with gold
standard POS tags.
By using two unlabeled dependency parsers to
provide soft dependency constraints, we obtain a
1% absolute improvement in precision and recall of
predicate-argument identification in HPSG parsing
over a strong baseline. Our baseline approach out-
performed previously published results on this test
4The accuracy figures for the dependency parsers is ex-
pressed as unlabeled accuracy of the surface dependencies only,
and are not comparable to the HPSG parsing accuracy figures
629
Parser LP LR F-score
HPSG Baseline 87.4 87.0 87.2
Shift-Reduce + HPSG 88.2 87.7 87.9
C1 + HPSG 88.5 88.0 88.2
C2 + HPSG 88.4 87.9 88.1
Baseline(gold) 89.8 89.4 89.6
Shift-Reduce(gold) 90.62 90.23 90.42
C1+HPSG(gold) 90.9 90.4 90.6
C2+HPSG(gold) 90.8 90.4 90.6
Miyao and Tsujii, 2005 85.0 84.3 84.6
Ninomiya et al, 2006 87.4 86.3 86.8
Table 2: Final results on test set. The first set of
results show our HPSG baseline and HPSG with soft
dependency constraints using three different sources
of dependency constraints. The second set of results
show the accuracy of the same parsers when gold
part-of-speech tags are used. The third set of results
is from existing published models on the same data.
set, and our best performing combination scheme
obtains an absolute improvement of 1.4% over the
best previously published results using the HPSG
Treebank. It is interesting to note that the results ob-
tained with dependency parser combinations C1 and
C2 were very similar, even though in C1 only two
parsers were used, and constraints were provided for
about 92% of shallow dependencies (with accuracy
higher than 96%). Clearly, precision is crucial in de-
pendency constraints.
Finally, although it is necessary to perform de-
pendency parsing to pre-compute dependency con-
straints, the total time required to perform the en-
tire process of HPSG parsing with dependency con-
straints is close to that of the baseline HPSG ap-
proach. This is due to two reasons: (1) the de-
pendency parsing approaches used to pre-compute
constraints are several times faster than the baseline
HPSG approach, and (2) the HPSG portion of the
process is significantly faster when dependency con-
straints are used, since the constraints help sharpen
the search space, making search more efficient. Us-
ing the baseline HPSG approach, it takes approx-
imately 25 minutes to parse the test set. The to-
tal time required to parse the test set using HPSG
with dependency constraints generated by the shift-
reduce parser is 27 minutes. With combination C1,
parsing time increases to 30 minutes, since two de-
pendency parsers are used sequentially.
5 Related work
There are other approaches that combine shallow
processing with deep parsing (Crysmann et al,
2002; Frank et al, 2003; Daum et al, 2003) to im-
prove parsing efficiency. Typically, shallow parsing
is used to create robust minimal recursion seman-
tics, which are used as constraints to limit ambigu-
ity during parsing. Our approach, in contrast, uses
syntactic dependencies to achieve a significant im-
provement in the accuracy of wide-coverage HPSG
parsing. Additionally, our approach is in many
ways similar to supertagging (Bangalore and Joshi,
1999), which uses sequence labeling techniques as
an efficient way to pre-compute parsing constraints
(specifically, the assignment of lexical entries to in-
put words).
6 Conclusion
We have presented a novel framework for taking ad-
vantage of the strengths of a shallow parsing ap-
proach and a deep parsing approach. We have
shown that by constraining the application of rules
in HPSG parsing according to results from a depen-
dency parser, we can significantly improve the ac-
curacy of deep parsing by using shallow syntactic
analyses.
To illustrate how this framework allows for im-
provements in the accuracy of dependency parsing
to be used directly to improve the accuracy of HPSG
parsing, we showed that by combining the results of
different dependency parsers using the search-based
parsing ensemble approach of (Sagae and Lavie,
2006), we obtain improved HPSG parsing accuracy
as a result of the improved dependency accuracy.
Although we have focused on the use of HPSG
and dependency parsing, the general framework pre-
sented here can be applied to other lexicalized gram-
mar formalisms, such as LTAG, CCG and LFG.
Acknowledgements
This research was partially supported by Grant-in-
Aid for Specially Promoted Research 18002007.
630
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
Amaximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Joan Bresnan. 1982. The mental representation of gram-
matical relations. MIT Press.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Natural Language
Learning. New York, NY.
M. Collins. 1999. Head-Driven Models for Natural Lan-
guage Parsing. Phd thesis, University of Pennsylva-
nia.
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan
Mueller, Guenter Neumann, Jakub Piskorski, Ulrich
Schaefer, Melanie Siegel, Hans Uszkoreit, Feiyu Xu,
Markus Becker, and Hans-Ulrich Krieger. 2002. An
integrated architecture for shallow and deep process-
ing. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002).
Michael Daum, Kilian A. Foth, and Wolfgang Menzel.
2003. Constraint-based integration of deep and shal-
low parsing techniques. In Proceedings of the 10th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2003).
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of the International Conference on Computational Lin-
guistics (COLING?96). Copenhagen, Denmark.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Schaefer. 2003. Integrated
shallow and deep parsing: TopP meets HPSG. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2003), pages
104?111.
Stephen Clark James R. Curran and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In
Proceedings of COLING/ACL 2006. Sydney, Aus-
tralia.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the 2002 Conference on Natural Language
Learning.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewics.
1993. Building a large annotated corpus of english:
The penn treebank. Computational Linguistics, 19.
Ryan McDonald, Fernando Pereira, K. Ribarov, and
J. Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technolo-
gies/Empirical Methods in Natural Language Process-
ing (HLT-EMNLP). Vancouver, Canada.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proceedings of the 42nd Meeting of the Associ-
ation for Computational Linguistics. Ann Arbor, MI.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Corpus oriented grammar development for
aquiring a head-driven phrase structure grammar from
the penn treebank. In Proceedings of the Tenth Con-
ference on Natural Language Learning.
T. Ninomiya, T. Matsuzaki, Y. Tsuruoka, Y. Miyao, and
J. Tsujii. 2006. Extremely lexicalized models for ac-
curate and fast hpsg parsing. In Proceedings of the
2006 Conference on Empirical Methods for Natural
Language Processing (EMNLP 2006).
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 64?70. Geneva, Switzerland.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
the Tenth Conference on Natural Language Learning.
New York, NY.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies. Vancouver, BC.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of the 2006 Meeting of
the North American ACL. New York, NY.
Yves Schabes, Anne Abeille, and Aravind Joshi. 1988.
Parsing strategies with lexicalized grammars: Appli-
cation to tree adjoining grammars. In Proceedings of
12th COLING.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Zeman and Zdenek Z?abokrtsky?. 2005. Improving
parsing accuracy by combining diverse dependency
parsers. In Proceedings of the International Workshop
on Parsing Technologies. Vancouver, Canada.
631
Proceedings of ACL-08: HLT, pages 46?54,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Task-oriented Evaluation of Syntactic Parsers and Their Representations
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a comparative evalua-
tion of several state-of-the-art English parsers
based on different frameworks. Our approach
is to measure the impact of each parser when it
is used as a component of an information ex-
traction system that performs protein-protein
interaction (PPI) identification in biomedical
papers. We evaluate eight parsers (based on
dependency parsing, phrase structure parsing,
or deep parsing) using five different parse rep-
resentations. We run a PPI system with several
combinations of parser and parse representa-
tion, and examine their impact on PPI identi-
fication accuracy. Our experiments show that
the levels of accuracy obtained with these dif-
ferent parsers are similar, but that accuracy
improvements vary when the parsers are re-
trained with domain-specific data.
1 Introduction
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame-
works (Charniak, 2000; Klein and Manning, 2003;
Charniak and Johnson, 2005; Petrov and Klein,
2007), but also include dependency parsers (Mc-
Donald and Pereira, 2006; Nivre and Nilsson, 2005;
Sagae and Tsujii, 2007) and deep parsers (Kaplan
et al, 2004; Clark and Curran, 2004; Miyao and
Tsujii, 2008). However, efforts to perform extensive
comparisons of syntactic parsers based on different
frameworks have been limited. The most popular
method for parser comparison involves the direct
measurement of the parser output accuracy in terms
of metrics such as bracketing precision and recall, or
dependency accuracy. This assumes the existence of
a gold-standard test corpus, such as the Penn Tree-
bank (Marcus et al, 1994). It is difficult to apply
this method to compare parsers based on different
frameworks, because parse representations are often
framework-specific and differ from parser to parser
(Ringger et al, 2004). The lack of such comparisons
is a serious obstacle for NLP researchers in choosing
an appropriate parser for their purposes.
In this paper, we present a comparative evalua-
tion of syntactic parsers and their output represen-
tations based on different frameworks: dependency
parsing, phrase structure parsing, and deep pars-
ing. Our approach to parser evaluation is to mea-
sure accuracy improvement in the task of identify-
ing protein-protein interaction (PPI) information in
biomedical papers, by incorporating the output of
different parsers as statistical features in a machine
learning classifier (Yakushiji et al, 2005; Katrenko
and Adriaans, 2006; Erkan et al, 2007; S?tre et al,
2007). PPI identification is a reasonable task for
parser evaluation, because it is a typical information
extraction (IE) application, and because recent stud-
ies have shown the effectiveness of syntactic parsing
in this task. Since our evaluation method is applica-
ble to any parser output, and is grounded in a real
application, it allows for a fair comparison of syn-
tactic parsers based on different frameworks.
Parser evaluation in PPI extraction also illu-
minates domain portability. Most state-of-the-art
parsers for English were trained with the Wall Street
Journal (WSJ) portion of the Penn Treebank, and
high accuracy has been reported for WSJ text; how-
ever, these parsers rely on lexical information to at-
tain high accuracy, and it has been criticized that
these parsers may overfit to WSJ text (Gildea, 2001;
46
Klein and Manning, 2003). Another issue for dis-
cussion is the portability of training methods. When
training data in the target domain is available, as
is the case with the GENIA Treebank (Kim et al,
2003) for biomedical papers, a parser can be re-
trained to adapt to the target domain, and larger ac-
curacy improvements are expected, if the training
method is sufficiently general. We will examine
these two aspects of domain portability by compar-
ing the original parsers with the retrained parsers.
2 Syntactic Parsers and Their
Representations
This paper focuses on eight representative parsers
that are classified into three parsing frameworks:
dependency parsing, phrase structure parsing, and
deep parsing. In general, our evaluation methodol-
ogy can be applied to English parsers based on any
framework; however, in this paper, we chose parsers
that were originally developed and trained with the
Penn Treebank or its variants, since such parsers can
be re-trained with GENIA, thus allowing for us to
investigate the effect of domain adaptation.
2.1 Dependency parsing
Because the shared tasks of CoNLL-2006 and
CoNLL-2007 focused on data-driven dependency
parsing, it has recently been extensively studied in
parsing research. The aim of dependency pars-
ing is to compute a tree structure of a sentence
where nodes are words, and edges represent the re-
lations among words. Figure 1 shows a dependency
tree for the sentence ?IL-8 recognizes and activates
CXCR1.? An advantage of dependency parsing is
that dependency trees are a reasonable approxima-
tion of the semantics of sentences, and are readily
usable in NLP applications. Furthermore, the effi-
ciency of popular approaches to dependency pars-
ing compare favorable with those of phrase struc-
ture parsing or deep parsing. While a number of ap-
proaches have been proposed for dependency pars-
ing, this paper focuses on two typical methods.
MST McDonald and Pereira (2006)?s dependency
parser,1 based on the Eisner algorithm for projective
dependency parsing (Eisner, 1996) with the second-
order factorization.
1http://sourceforge.net/projects/mstparser
Figure 1: CoNLL-X dependency tree
Figure 2: Penn Treebank-style phrase structure tree
KSDEP Sagae and Tsujii (2007)?s dependency
parser,2 based on a probabilistic shift-reduce al-
gorithm extended by the pseudo-projective parsing
technique (Nivre and Nilsson, 2005).
2.2 Phrase structure parsing
Owing largely to the Penn Treebank, the mainstream
of data-driven parsing research has been dedicated
to the phrase structure parsing. These parsers output
Penn Treebank-style phrase structure trees, although
function tags and empty categories are stripped off
(Figure 2). While most of the state-of-the-art parsers
are based on probabilistic CFGs, the parameteriza-
tion of the probabilistic model of each parser varies.
In this work, we chose the following four parsers.
NO-RERANK Charniak (2000)?s parser, based on a
lexicalized PCFG model of phrase structure trees.3
The probabilities of CFG rules are parameterized on
carefully hand-tuned extensive information such as
lexical heads and symbols of ancestor/sibling nodes.
RERANK Charniak and Johnson (2005)?s rerank-
ing parser. The reranker of this parser receives n-
best4 parse results from NO-RERANK, and selects
the most likely result by using a maximum entropy
model with manually engineered features.
BERKELEY Berkeley?s parser (Petrov and Klein,
2007).5 The parameterization of this parser is op-
2http://www.cs.cmu.edu/?sagae/parser/
3http://bllip.cs.brown.edu/resources.shtml
4We set n = 50 in this paper.
5http://nlp.cs.berkeley.edu/Main.html#Parsing
47
Figure 3: Predicate argument structure
timized automatically by assigning latent variables
to each nonterminal node and estimating the param-
eters of the latent variables by the EM algorithm
(Matsuzaki et al, 2005).
STANFORD Stanford?s unlexicalized parser (Klein
and Manning, 2003).6 Unlike NO-RERANK, proba-
bilities are not parameterized on lexical heads.
2.3 Deep parsing
Recent research developments have allowed for ef-
ficient and robust deep parsing of real-world texts
(Kaplan et al, 2004; Clark and Curran, 2004; Miyao
and Tsujii, 2008). While deep parsers compute
theory-specific syntactic/semantic structures, pred-
icate argument structures (PAS) are often used in
parser evaluation and applications. PAS is a graph
structure that represents syntactic/semantic relations
among words (Figure 3). The concept is therefore
similar to CoNLL dependencies, though PAS ex-
presses deeper relations, and may include reentrant
structures. In this work, we chose the two versions
of the Enju parser (Miyao and Tsujii, 2008).
ENJU The HPSG parser that consists of an HPSG
grammar extracted from the Penn Treebank, and
a maximum entropy model trained with an HPSG
treebank derived from the Penn Treebank.7
ENJU-GENIA The HPSG parser adapted to
biomedical texts, by the method of Hara et al
(2007). Because this parser is trained with both
WSJ and GENIA, we compare it parsers that are
retrained with GENIA (see section 3.3).
3 Evaluation Methodology
In our approach to parser evaluation, we measure
the accuracy of a PPI extraction system, in which
6http://nlp.stanford.edu/software/lex-parser.
shtml
7http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
This study demonstrates that IL-8 recognizes and
activates CXCR1, CXCR2, and the Duffy antigen
by distinct mechanisms.
The molar ratio of serum retinol-binding protein
(RBP) to transthyretin (TTR) is not useful to as-
sess vitamin A status during infection in hospi-
talised children.
Figure 4: Sentences including protein names
ENTITY1(IL-8) SBJ?? recognizes OBJ?? ENTITY2(CXCR1)
Figure 5: Dependency path
the parser output is embedded as statistical features
of a machine learning classifier. We run a classi-
fier with features of every possible combination of a
parser and a parse representation, by applying con-
versions between representations when necessary.
We also measure the accuracy improvements ob-
tained by parser retraining with GENIA, to examine
the domain portability, and to evaluate the effective-
ness of domain adaptation.
3.1 PPI extraction
PPI extraction is an NLP task to identify protein
pairs that are mentioned as interacting in biomedical
papers. Because the number of biomedical papers is
growing rapidly, it is impossible for biomedical re-
searchers to read all papers relevant to their research;
thus, there is an emerging need for reliable IE tech-
nologies, such as PPI identification.
Figure 4 shows two sentences that include pro-
tein names: the former sentence mentions a protein
interaction, while the latter does not. Given a pro-
tein pair, PPI extraction is a task of binary classi-
fication; for example, ?IL-8, CXCR1? is a positive
example, and ?RBP, TTR? is a negative example.
Recent studies on PPI extraction demonstrated that
dependency relations between target proteins are ef-
fective features for machine learning classifiers (Ka-
trenko and Adriaans, 2006; Erkan et al, 2007; S?tre
et al, 2007). For the protein pair IL-8 and CXCR1
in Figure 4, a dependency parser outputs a depen-
dency tree shown in Figure 1. From this dependency
tree, we can extract a dependency path shown in Fig-
ure 5, which appears to be a strong clue in knowing
that these proteins are mentioned as interacting.
48
(dep_path (SBJ (ENTITY1 recognizes))
(rOBJ (recognizes ENTITY2)))
Figure 6: Tree representation of a dependency path
We follow the PPI extraction method of S?tre et
al. (2007), which is based on SVMs with SubSet
Tree Kernels (Collins and Duffy, 2002; Moschitti,
2006), while using different parsers and parse rep-
resentations. Two types of features are incorporated
in the classifier. The first is bag-of-words features,
which are regarded as a strong baseline for IE sys-
tems. Lemmas of words before, between and after
the pair of target proteins are included, and the linear
kernel is used for these features. These features are
commonly included in all of the models. Filtering
by a stop-word list is not applied because this setting
made the scores higher than S?tre et al (2007)?s set-
ting. The other type of feature is syntactic features.
For dependency-based parse representations, a de-
pendency path is encoded as a flat tree as depicted in
Figure 6 (prefix ?r? denotes reverse relations). Be-
cause a tree kernel measures the similarity of trees
by counting common subtrees, it is expected that the
system finds effective subsequences of dependency
paths. For the PTB representation, we directly en-
code phrase structure trees.
3.2 Conversion of parse representations
It is widely believed that the choice of representa-
tion format for parser output may greatly affect the
performance of applications, although this has not
been extensively investigated. We should therefore
evaluate the parser performance in multiple parse
representations. In this paper, we create multiple
parse representations by converting each parser?s de-
fault output into other representations when possi-
ble. This experiment can also be considered to be
a comparative evaluation of parse representations,
thus providing an indication for selecting an appro-
priate parse representation for similar IE tasks.
Figure 7 shows our scheme for representation
conversion. This paper focuses on five representa-
tions as described below.
CoNLL The dependency tree format used in the
2006 and 2007 CoNLL shared tasks on dependency
parsing. This is a representation format supported by
several data-driven dependency parsers. This repre-
Figure 7: Conversion of parse representations
Figure 8: Head dependencies
sentation is also obtained from Penn Treebank-style
trees by applying constituent-to-dependency conver-
sion8 (Johansson and Nugues, 2007). It should be
noted, however, that this conversion cannot work
perfectly with automatic parsing, because the con-
version program relies on function tags and empty
categories of the original Penn Treebank.
PTB Penn Treebank-style phrase structure trees
without function tags and empty nodes. This is the
default output format for phrase structure parsers.
We also create this representation by converting
ENJU?s output by tree structure matching, although
this conversion is not perfect because forms of PTB
and ENJU?s output are not necessarily compatible.
HD Dependency trees of syntactic heads (Fig-
ure 8). This representation is obtained by convert-
ing PTB trees. We first determine lexical heads of
nonterminal nodes by using Bikel?s implementation
of Collins? head detection algorithm9 (Bikel, 2004;
Collins, 1997). We then convert lexicalized trees
into dependencies between lexical heads.
SD The Stanford dependency format (Figure 9).
This format was originally proposed for extracting
dependency relations useful for practical applica-
tions (de Marneffe et al, 2006). A program to con-
vert PTB is attached to the Stanford parser. Although
the concept looks similar to CoNLL, this representa-
8http://nlp.cs.lth.se/pennconverter/
9http://www.cis.upenn.edu/?dbikel/software.
html
49
Figure 9: Stanford dependencies
tion does not necessarily form a tree structure, and is
designed to express more fine-grained relations such
as apposition. Research groups for biomedical NLP
recently adopted this representation for corpus anno-
tation (Pyysalo et al, 2007a) and parser evaluation
(Clegg and Shepherd, 2007; Pyysalo et al, 2007b).
PAS Predicate-argument structures. This is the de-
fault output format for ENJU and ENJU-GENIA.
Although only CoNLL is available for depen-
dency parsers, we can create four representations for
the phrase structure parsers, and five for the deep
parsers. Dotted arrows in Figure 7 indicate imper-
fect conversion, in which the conversion inherently
introduces errors, and may decrease the accuracy.
We should therefore take caution when comparing
the results obtained by imperfect conversion. We
also measure the accuracy obtained by the ensem-
ble of two parsers/representations. This experiment
indicates the differences and overlaps of information
conveyed by a parser or a parse representation.
3.3 Domain portability and parser retraining
Since the domain of our target text is different from
WSJ, our experiments also highlight the domain
portability of parsers. We run two versions of each
parser in order to investigate the two types of domain
portability. First, we run the original parsers trained
with WSJ10 (39832 sentences). The results in this
setting indicate the domain portability of the original
parsers. Next, we run parsers re-trained with GE-
NIA11 (8127 sentences), which is a Penn Treebank-
style treebank of biomedical paper abstracts. Accu-
racy improvements in this setting indicate the pos-
sibility of domain adaptation, and the portability of
the training methods of the parsers. Since the parsers
listed in Section 2 have programs for the training
10Some of the parser packages include parsing models
trained with extended data, but we used the models trained with
WSJ section 2-21 of the Penn Treebank.
11The domains of GENIA and AImed are not exactly the
same, because they are collected independently.
with a Penn Treebank-style treebank, we use those
programs as-is. Default parameter settings are used
for this parser re-training.
In preliminary experiments, we found that de-
pendency parsers attain higher dependency accuracy
when trained only with GENIA. We therefore only
input GENIA as the training data for the retraining
of dependency parsers. For the other parsers, we in-
put the concatenation of WSJ and GENIA for the
retraining, while the reranker of RERANK was not re-
trained due to its cost. Since the parsers other than
NO-RERANK and RERANK require an external POS
tagger, a WSJ-trained POS tagger is used with WSJ-
trained parsers, and geniatagger (Tsuruoka et al,
2005) is used with GENIA-retrained parsers.
4 Experiments
4.1 Experiment settings
In the following experiments, we used AImed
(Bunescu and Mooney, 2004), which is a popular
corpus for the evaluation of PPI extraction systems.
The corpus consists of 225 biomedical paper ab-
stracts (1970 sentences), which are sentence-split,
tokenized, and annotated with proteins and PPIs.
We use gold protein annotations given in the cor-
pus. Multi-word protein names are concatenated
and treated as single words. The accuracy is mea-
sured by abstract-wise 10-fold cross validation and
the one-answer-per-occurrence criterion (Giuliano
et al, 2006). A threshold for SVMs is moved to
adjust the balance of precision and recall, and the
maximum f-scores are reported for each setting.
4.2 Comparison of accuracy improvements
Tables 1 and 2 show the accuracy obtained by using
the output of each parser in each parse representa-
tion. The row ?baseline? indicates the accuracy ob-
tained with bag-of-words features. Table 3 shows
the time for parsing the entire AImed corpus, and
Table 4 shows the time required for 10-fold cross
validation with GENIA-retrained parsers.
When using the original WSJ-trained parsers (Ta-
ble 1), all parsers achieved almost the same level
of accuracy ? a significantly better result than the
baseline. To the extent of our knowledge, this is
the first result that proves that dependency parsing,
phrase structure parsing, and deep parsing perform
50
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 53.2/56.5/54.6 N/A N/A N/A N/A
KSDEP 49.3/63.0/55.2 N/A N/A N/A N/A
NO-RERANK 50.7/60.9/55.2 45.9/60.5/52.0 50.6/60.9/55.1 49.9/58.2/53.5 N/A
RERANK 53.6/59.2/56.1 47.0/58.9/52.1 48.1/65.8/55.4 50.7/62.7/55.9 N/A
BERKELEY 45.8/67.6/54.5 50.5/57.6/53.7 52.3/58.8/55.1 48.7/62.4/54.5 N/A
STANFORD 50.4/60.6/54.9 50.9/56.1/53.0 50.7/60.7/55.1 51.8/58.1/54.5 N/A
ENJU 52.6/58.0/55.0 48.7/58.8/53.1 57.2/51.9/54.2 52.2/58.1/54.8 48.9/64.1/55.3
Table 1: Accuracy on the PPI task with WSJ-trained parsers (precision/recall/f-score)
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 49.1/65.6/55.9 N/A N/A N/A N/A
KSDEP 51.6/67.5/58.3 N/A N/A N/A N/A
NO-RERANK 53.9/60.3/56.8 51.3/54.9/52.8 53.1/60.2/56.3 54.6/58.1/56.2 N/A
RERANK 52.8/61.5/56.6 48.3/58.0/52.6 52.1/60.3/55.7 53.0/61.1/56.7 N/A
BERKELEY 52.7/60.3/56.0 48.0/59.9/53.1 54.9/54.6/54.6 50.5/63.2/55.9 N/A
STANFORD 49.3/62.8/55.1 44.5/64.7/52.5 49.0/62.0/54.5 54.6/57.5/55.8 N/A
ENJU 54.4/59.7/56.7 48.3/60.6/53.6 56.7/55.6/56.0 54.4/59.3/56.6 52.0/63.8/57.2
ENJU-GENIA 56.4/57.4/56.7 46.5/63.9/53.7 53.4/60.2/56.4 55.2/58.3/56.5 57.5/59.8/58.4
Table 2: Accuracy on the PPI task with GENIA-retrained parsers (precision/recall/f-score)
WSJ-trained GENIA-retrained
MST 613 425
KSDEP 136 111
NO-RERANK 2049 1372
RERANK 2806 2125
BERKELEY 1118 1198
STANFORD 1411 1645
ENJU 1447 727
ENJU-GENIA 821
Table 3: Parsing time (sec.)
equally well in a real application. Among these
parsers, RERANK performed slightly better than the
other parsers, although the difference in the f-score
is small, while it requires much higher parsing cost.
When the parsers are retrained with GENIA (Ta-
ble 2), the accuracy increases significantly, demon-
strating that the WSJ-trained parsers are not suffi-
ciently domain-independent, and that domain adap-
tation is effective. It is an important observation that
the improvements by domain adaptation are larger
than the differences among the parsers in the pre-
vious experiment. Nevertheless, not all parsers had
their performance improved upon retraining. Parser
CoNLL PTB HD SD PAS
baseline 424
MST 809 N/A N/A N/A N/A
KSDEP 864 N/A N/A N/A N/A
NO-RERANK 851 4772 882 795 N/A
RERANK 849 4676 881 778 N/A
BERKELEY 869 4665 895 804 N/A
STANFORD 847 4614 886 799 N/A
ENJU 832 4611 884 789 1005
ENJU-GENIA 874 4624 895 783 1020
Table 4: Evaluation time (sec.)
retraining yielded only slight improvements for
RERANK, BERKELEY, and STANFORD, while larger
improvements were observed for MST, KSDEP, NO-
RERANK, and ENJU. Such results indicate the dif-
ferences in the portability of training methods. A
large improvement from ENJU to ENJU-GENIA shows
the effectiveness of the specifically designed do-
main adaptation method, suggesting that the other
parsers might also benefit from more sophisticated
approaches for domain adaptation.
While the accuracy level of PPI extraction is
the similar for the different parsers, parsing speed
51
RERANK ENJU
CoNLL HD SD CoNLL HD SD PAS
KSDEP CoNLL 58.5 (+0.2) 57.1 (?1.2) 58.4 (+0.1) 58.5 (+0.2) 58.0 (?0.3) 59.1 (+0.8) 59.0 (+0.7)
RERANK CoNLL 56.7 (+0.1) 57.1 (+0.4) 58.3 (+1.6) 57.3 (+0.7) 58.7 (+2.1) 59.5 (+2.3)
HD 56.8 (+0.1) 57.2 (+0.5) 56.5 (+0.5) 56.8 (+0.2) 57.6 (+0.4)
SD 58.3 (+1.6) 58.3 (+1.6) 56.9 (+0.2) 58.6 (+1.4)
ENJU CoNLL 57.0 (+0.3) 57.2 (+0.5) 58.4 (+1.2)
HD 57.1 (+0.5) 58.1 (+0.9)
SD 58.3 (+1.1)
Table 5: Results of parser/representation ensemble (f-score)
differs significantly. The dependency parsers are
much faster than the other parsers, while the phrase
structure parsers are relatively slower, and the deep
parsers are in between. It is noteworthy that the
dependency parsers achieved comparable accuracy
with the other parsers, while they are more efficient.
The experimental results also demonstrate that
PTB is significantly worse than the other represen-
tations with respect to cost for training/testing and
contributions to accuracy improvements. The con-
version from PTB to dependency-based representa-
tions is therefore desirable for this task, although it
is possible that better results might be obtained with
PTB if a different feature extraction mechanism is
used. Dependency-based representations are com-
petitive, while CoNLL seems superior to HD and SD
in spite of the imperfect conversion from PTB to
CoNLL. This might be a reason for the high per-
formances of the dependency parsers that directly
compute CoNLL dependencies. The results for ENJU-
CoNLL and ENJU-PAS show that PAS contributes to a
larger accuracy improvement, although this does not
necessarily mean the superiority of PAS, because two
imperfect conversions, i.e., PAS-to-PTB and PTB-to-
CoNLL, are applied for creating CoNLL.
4.3 Parser ensemble results
Table 5 shows the accuracy obtained with ensembles
of two parsers/representations (except the PTB for-
mat). Bracketed figures denote improvements from
the accuracy with a single parser/representation.
The results show that the task accuracy significantly
improves by parser/representation ensemble. Inter-
estingly, the accuracy improvements are observed
even for ensembles of different representations from
the same parser. This indicates that a single parse
representation is insufficient for expressing the true
Bag-of-words features 48.2/54.9/51.1
Yakushiji et al (2005) 33.7/33.1/33.4
Mitsumori et al (2006) 54.2/42.6/47.7
Giuliano et al (2006) 60.9/57.2/59.0
S?tre et al (2007) 64.3/44.1/52.0
This paper 54.9/65.5/59.5
Table 6: Comparison with previous results on PPI extrac-
tion (precision/recall/f-score)
potential of a parser. Effectiveness of the parser en-
semble is also attested by the fact that it resulted in
larger improvements. Further investigation of the
sources of these improvements will illustrate the ad-
vantages and disadvantages of these parsers and rep-
resentations, leading us to better parsing models and
a better design for parse representations.
4.4 Comparison with previous results on PPI
extraction
PPI extraction experiments on AImed have been re-
ported repeatedly, although the figures cannot be
compared directly because of the differences in data
preprocessing and the number of target protein pairs
(S?tre et al, 2007). Table 6 compares our best re-
sult with previously reported accuracy figures. Giu-
liano et al (2006) and Mitsumori et al (2006) do
not rely on syntactic parsing, while the former ap-
plied SVMs with kernels on surface strings and the
latter is similar to our baseline method. Bunescu and
Mooney (2005) applied SVMs with subsequence
kernels to the same task, although they provided
only a precision-recall graph, and its f-score is
around 50. Since we did not run experiments on
protein-pair-wise cross validation, our system can-
not be compared directly to the results reported
by Erkan et al (2007) and Katrenko and Adriaans
52
(2006), while S?tre et al (2007) presented better re-
sults than theirs in the same evaluation criterion.
5 Related Work
Though the evaluation of syntactic parsers has been
a major concern in the parsing community, and a
couple of works have recently presented the com-
parison of parsers based on different frameworks,
their methods were based on the comparison of the
parsing accuracy in terms of a certain intermediate
parse representation (Ringger et al, 2004; Kaplan
et al, 2004; Briscoe and Carroll, 2006; Clark and
Curran, 2007; Miyao et al, 2007; Clegg and Shep-
herd, 2007; Pyysalo et al, 2007b; Pyysalo et al,
2007a; Sagae et al, 2008). Such evaluation requires
gold standard data in an intermediate representation.
However, it has been argued that the conversion of
parsing results into an intermediate representation is
difficult and far from perfect.
The relationship between parsing accuracy and
task accuracy has been obscure for many years.
Quirk and Corston-Oliver (2006) investigated the
impact of parsing accuracy on statistical MT. How-
ever, this work was only concerned with a single de-
pendency parser, and did not focus on parsers based
on different frameworks.
6 Conclusion and Future Work
We have presented our attempts to evaluate syntac-
tic parsers and their representations that are based on
different frameworks; dependency parsing, phrase
structure parsing, or deep parsing. The basic idea
is to measure the accuracy improvements of the
PPI extraction task by incorporating the parser out-
put as statistical features of a machine learning
classifier. Experiments showed that state-of-the-
art parsers attain accuracy levels that are on par
with each other, while parsing speed differs sig-
nificantly. We also found that accuracy improve-
ments vary when parsers are retrained with domain-
specific data, indicating the importance of domain
adaptation and the differences in the portability of
parser training methods.
Although we restricted ourselves to parsers
trainable with Penn Treebank-style treebanks, our
methodology can be applied to any English parsers.
Candidates include RASP (Briscoe and Carroll,
2006), the C&C parser (Clark and Curran, 2004),
the XLE parser (Kaplan et al, 2004), MINIPAR
(Lin, 1998), and Link Parser (Sleator and Temperley,
1993; Pyysalo et al, 2006), but the domain adapta-
tion of these parsers is not straightforward. It is also
possible to evaluate unsupervised parsers, which is
attractive since evaluation of such parsers with gold-
standard data is extremely problematic.
A major drawback of our methodology is that
the evaluation is indirect and the results depend
on a selected task and its settings. This indicates
that different results might be obtained with other
tasks. Hence, we cannot conclude the superiority of
parsers/representations only with our results. In or-
der to obtain general ideas on parser performance,
experiments on other tasks are indispensable.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Grant-in-Aid for Young Scientists (MEXT, Japan).
References
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
T. Briscoe and J. Carroll. 2006. Evaluating the accu-
racy of an unlexicalized statistical parser on the PARC
DepBank. In COLING/ACL 2006 Poster Session.
R. Bunescu and R. J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL 2004, pages 439?446.
R. C. Bunescu and R. J. Mooney. 2005. Subsequence
kernels for relation extraction. In NIPS 2005.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
ACL 2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL-2000, pages 132?139.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In 42nd ACL.
S. Clark and J. R. Curran. 2007. Formalism-independent
parser evaluation with CCG and DepBank. In ACL
2007.
A. B. Clegg and A. J. Shepherd. 2007. Benchmark-
ing natural-language parsers for biological applica-
tions using dependency graphs. BMC Bioinformatics,
8:24.
53
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL 2002.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In 35th ACL.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COLING
1996.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interac-
tion sentences using dependency parsing. In EMNLP
2007.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP 2001, pages 167?202.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In EACL 2006.
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating im-
pact of re-training a lexical disambiguation model on
domain adaptation of an HPSG parser. In IWPT 2007.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA 2007.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell, and
A. Vasserman. 2004. Speed and accuracy in shallow
and deep stochastic parsing. In HLT/NAACL?04.
S. Katrenko and P. Adriaans. 2006. Learning relations
from biomedical corpora using dependency trees. In
KDECB, pages 61?80.
J.-D. Kim, T. Ohta, Y. Teteisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19:i180?182.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL 2003.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In LREC Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL 2005.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In EACL
2006.
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with SVM. IEICE - Trans.
Inf. Syst., E89-D(8):2464?2466.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Y. Miyao, K. Sagae, and J. Tsujii. 2007. Towards
framework-independent evaluation of deep linguistic
parsers. In Grammar Engineering across Frameworks
2007, pages 238?258.
A. Moschitti. 2006. Making tree kernels practical for
natural language processing. In EACL 2006.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In ACL 2005.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL 2007.
S. Pyysalo, T. Salakoski, S. Aubin, and A. Nazarenko.
2006. Lexical adaptation of link grammar to the
biomedical sublanguage: a comparative evaluation of
three approaches. BMC Bioinformatics, 7(Suppl. 3).
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007a. BioInfer: a cor-
pus for information extraction in the biomedical do-
main. BMC Bioinformatics, 8(50).
S. Pyysalo, F. Ginter, V. Laippala, K. Haverinen, J. Hei-
monen, and T. Salakoski. 2007b. On the unification of
syntactic annotations under the Stanford dependency
scheme: A case study on BioInfer and GENIA. In
BioNLP 2007, pages 25?32.
C. Quirk and S. Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical ma-
chine translation. In EMNLP 2006.
E. K. Ringger, R. C. Moore, E. Charniak, L. Vander-
wende, and H. Suzuki. 2004. Using the Penn Tree-
bank to evaluate non-treebank parsers. In LREC 2004.
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic
features for protein-protein interaction extraction. In
LBM 2007 short papers.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In EMNLP-CoNLL 2007.
K. Sagae, Y. Miyao, T. Matsuzaki, and J. Tsujii. 2008.
Challenges in mapping of syntactic representations
for framework-independent parser evaluation. In the
Workshop on Automated Syntatic Annotations for In-
teroperable Language Resources.
D. D. Sleator and D. Temperley. 1993. Parsing English
with a Link Grammar. In 3rd IWPT.
Y. Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. Develop-
ing a robust part-of-speech tagger for biomedical text.
In 10th Panhellenic Conference on Informatics.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In First International
Symposium on Semantic Mining in Biomedicine.
54
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 25?32,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
High-accuracy Annotation and Parsing of CHILDES Transcripts
Kenji Sagae
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
sagae@is.s.u-tokyo.ac.jp
Eric Davis
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
dhdavis@cs.cmu.edu
Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
alavie@cs.cmu.edu
Brian MacWhinney
Department of Psychology
Carnegie Mellon University
Pittsburgh, PA 15213
macw@cmu.edu
Shuly Wintner
Department of Computer Science
University of Haifa
31905 Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
Corpora of child language are essential for
psycholinguistic research. Linguistic anno-
tation of the corpora provides researchers
with better means for exploring the develop-
ment of grammatical constructions and their
usage. We describe an ongoing project that
aims to annotate the English section of the
CHILDES database with grammatical re-
lations in the form of labeled dependency
structures. To date, we have produced a cor-
pus of over 65,000 words with manually cu-
rated gold-standard grammatical relation an-
notations. Using this corpus, we have devel-
oped a highly accurate data-driven parser for
English CHILDES data. The parser and the
manually annotated data are freely available
for research purposes.
1 Introduction
In order to investigate the development of child lan-
guage, corpora which document linguistic interac-
tions involving children are needed. The CHILDES
database (MacWhinney, 2000), containing tran-
scripts of spoken interactions between children at
various stages of language development with their
parents, provides vast amounts of useful data for lin-
guistic, psychological, and sociological studies of
child language development. The raw information in
CHILDES corpora was gradually enriched by pro-
viding a layer of morphological information. In par-
ticular, the English section of the database is aug-
mented by part of speech (POS) tags for each word.
However, this information is usually insufficient for
investigations dealing with the syntactic, semantic
or pragmatic aspects of the data.
In this paper we describe an ongoing effort aim-
ing to annotate the English portion of the CHILDES
database with syntactic information based on gram-
matical relations represented as labeled dependency
structures. Although an annotation scheme for syn-
tactic information in CHILDES data has been pro-
posed (Sagae et al, 2004), until now no significant
amount of annotated data had been made publicly
available. In the process of manually annotating sev-
eral thousands of words, we updated the annotation
scheme, mostly by extending it to cover syntactic
phenomena that occur in real data but were unac-
counted for in the original annotation scheme.
The contributions of this work fall into three main
categories: revision and extension of the annota-
tion scheme for representing syntactic information
in CHILDES data; creation of a manually annotated
65,000 word corpus with gold-standard syntactic
analyses; and implementation of a complete parser
that can automatically annotate additional data with
high accuracy. Both the gold-standard annotated
data and the parser are freely available. In addi-
tion to introducing the parser and the data, we re-
port on many of the specific annotation issues that
we encountered during the manual annotation pro-
25
cess, which should be helpful for those who may
use the annotated data or the parser. The anno-
tated corpora and the parser are freely available from
http://childes.psy.cmu.edu/.
We describe the annotation scheme in the next
section, along with issues we faced during the pro-
cess of manual annotation. Section 3 describes the
parser, and an evaluation of the parser is presented in
section 4. We analyze the remaining parsing errors
in section 5 and conclude with some applications of
the parser and directions for future research in sec-
tion 6.
2 Syntactic annotation
The English section of the CHILDES database is
augmented with automatically produced ambiguous
part-of-speech and morphological tags (MacWhin-
ney, 2000). Some of these data have been manually
disambiguated, but we found that some annotation
decisions had to be revised to facilitate syntactic an-
notation. We discuss below some of the revisions we
introduced, as well as some details of the syntactic
constructions that we account for.
2.1 The morphological annotation scheme
The English morphological analyzer incorporated
in CHILDES produces various part-of-speech tags
(there are 31 distinct POS tags in the CHILDES
tagset), including ADJective, ADVerb, COmmuni-
cator, CONJunction, DETerminer, FILler, Noun,
NUMeral, ONomatopoeia, PREPosition, PROnoun,
ParTicLe, QuaNtifier, RELativizer and Verb1. In
most cases, the correct annotation of a word is obvi-
ous from the context in which the word occurs, but
sometimes a more subtle distinction must be made.
We discuss some common problematic issues below.
Adverb vs. preposition vs. particle The words
about, across, after, away, back, down, in, off, on,
out, over, up belong to three categories: ADVerb,
PREPosition and ParTicLe. To correctly annotate
them in context, we apply the following criteria.
First, a preposition must have a prepositional ob-
ject, which is typically realized as a noun phrase
(which may be topicalized, or even elided). Sec-
ond, a preposition forms a constituent with its noun
1We use capital letters to denote the actual tag names in the
CHILDES tagset.
phrase object. Third, a prepositional object can be
fronted (for example, he sat on the chair becomes
the chair on which he sat), whereas a particle-NP
sequence cannot (*the phone number up which he
looked cannot be obtained from he looked up the
phone number). Finally, a manner adverb can be
placed between the verb and a preposition, but not
between a verb and a particle.
To distinguish between an adverb and a particle,
the meaning of the head verb is considered. If the
meaning of the verb and the target word, taken to-
gether, cannot be predicted from the meanings of the
verb and the target word separately, then the target
word is a particle. In all other cases it is an adverb.
Verbs vs. auxiliaries Distinguishing between
Verb and AUXiliary is often straightforward, but
special attention is given when tagging the verbs be,
do and have. If the target word is accompanied by an
non-finite verb in the same clause, as in I have had
enough or I do not like eggs, it is an auxiliary. Ad-
ditionally, in interrogative sentences, the auxiliary is
moved to the beginning of the clause, as in have I
had enough? and do I like eggs?, whereas the main
verb is not. However, this test does not always work
for the verb be, which may head a non-verbal pred-
icate, as in John is a teacher, vs. John is smiling. In
verb-participle constructions headed by the verb be,
if the participle is in the progressive tense, then the
head verb is labeled as auxiliary.
Communicators vs. locative adverbs COmmu-
nicators can be hard to distinguish from locative ad-
verbs, especially at the beginning of a sentence. Our
convention is that CO must modify an entire sen-
tence, so if a word appears by itself, it cannot be a
CO. For example, utterances like here or there are
labeled as ADVerb. However, if these words appear
at the beginning of a sentence, are followed by a
break or pause, and do not clearly express a location,
then they are labeled CO. Additionally, in here/there
you are/go, here and there are labeled CO.
2.2 The syntactic annotation scheme
Our annotation scheme for representing grammati-
cal relations, or GRs (such as subjects, objects and
adjuncts), in CHILDES transcripts is a slightly ex-
tended version of the scheme proposed by Sagae et
al. (2004), which was inspired by a general annota-
26
tion scheme for grammatical relations (Carroll et al,
1998), but adapted specifically for CHILDES data.
Our scheme contains 37 distinct GR types. Sagae
et al reported 96.5% interannotator agreement, and
we do not believe our minor updates to the annota-
tion scheme should affect interannotator agreement
significantly.
The scheme distinguishes among SUBJects, (fi-
nite) Clausal SUBJects2 (e.g., that he cried moved
her) and XSUBJects (eating vegetables is impor-
tant). Similarly, we distinguish among OBJects,
OBJect2, which is the second object of a ditran-
sitive verb, and IOBjects, which are required verb
complements introduced by prepositions. Verb com-
plements that are realized as clauses are labeled
COMP if they are finite (I think that was Fraser) and
XCOMP otherwise (you stop throwing the blocks).
Additionally, we mark required locative adjectival
or prepositional phrase arguments of verbs as LOCa-
tives, as in put the toys in the box/back.
PREDicates are nominal, adjectival or prepo-
sitional complements of verbs such as get, be
and become, as in I?m not sure. Again, we
specifically mark Clausal PREDicates (This is
how I drink my coffee) and XPREDicates (My goal
is to win the competition).
Adjuncts (denoted by JCT) are optional modi-
fiers of verbs, adjectives or adverbs, and we dis-
tinguish among non-clausal ones (That?s much bet-
ter; sit on the stool), finite clausal ones (CJCT, Mary
left after she saw John) and non-finite clausal ones
(XJCT, Mary left after seeing John).
MODifiers, which modify or complement nouns,
again come in three flavors: MOD (That?s a nice
box); CMOD (the movie that I saw was good ); and
XMOD (the student reading a book is tall ).
We then identify AUXiliary verbs, as in did you
do it? ; NEGation (Fraser is not drinking his coffee);
DETerminers (a fly); QUANTifiers (some juice); the
objects of prepositions (POBJ, on the stool); verb
ParTicLes (can you get the blocks out? ); ComPle-
mentiZeRs (wait until the noodles are cool ); COM-
municators (oh, I took it); the INfinitival to; VOCa-
tives (Thank you, Eve); and TAG questions (you
know how to count, don?t you? ).
2As with the POS tags, we use capital letters to represent the
actual GR tags used in the annotation scheme.
Finally, we added some specific relations for han-
dling problematic issues. For example, we use
ENUMeration for constructions such as one, two,
three, go or a, b, c. In COORDination construc-
tions, each conjunct is marked as a dependent of the
conjunction (e.g., go and get your telephone). We
use TOPicalization to indicate an argument that is
topicalized, as in tapioca, there is no tapioca. We
use SeRiaL to indicate serial verbs as in come see
if we can find it or go play with your toys. Finally,
we mark sequences of proper names which form the
same entity (e.g., New York ) as NAME.
The format of the grammatical relation (GR) an-
notation, which we use in the examples that follow,
associates with each word in a sentence a triple i|j|g,
where i is the index of the word in the sentence, j the
index of the word?s syntactic head, and g is the name
of the grammatical relation represented by the syn-
tactic dependency between the i-th and j-th words.
If the topmost head of the utterance is the i-th word,
it is labeled i|0|ROOT. For example, in:
a cookie .
1|2|DET 2|0|ROOT 3|2|PUNCT
the first word a is a DETerminer of word 2 (cookie),
which is itself the ROOT of the utterance.
2.3 Manual annotation of the corpus
We focused our manual annotation on a set of
CHILDES transcripts for a particular child, Eve
(Brown, 1973), and we refer to these transcripts,
distributed in a set of 20 files, as the Eve corpus.
We hand-annotated (including correcting POS tags)
the first 15 files of the Eve corpus following the
GR scheme outlined above. The annotation pro-
cess started with purely manual annotation of 5,000
words. This initial annotated corpus was used to
train a data-driven parser, as described later. This
parser was then used to label an additional 20,000
words automatically, followed by a thorough manual
checking stage, where each syntactic annotation was
manually verified and corrected if necessary. We re-
trained the parser with the newly annotated data, and
proceeded in this fashion until 15 files had been an-
notated and thoroughly manually checked.
Annotating child language proved to be challeng-
ing, and as we progressed through the data, we no-
ticed grammatical constructions that the GRs could
27
not adequately handle. For example, the original GR
scheme did not differentiate between locative argu-
ments and locative adjuncts, so we created a new GR
label, LOC, to handle required verbal locative argu-
ments such as on in put it on the table. Put licenses
a prepositional argument, and the existing JCT rela-
tion could not capture this requirement.
In addition to adding new GRs, we also faced
challenges with telegraphic child utterances lack-
ing verbs or other content words. For instance,
Mommy telephone could have one of several mean-
ings: Mommy this is a telephone, Mommy I want
the telephone, that is Mommy?s telephone, etc. We
tried to be as consistent as possible in annotating
such utterances and determined their GRs from con-
text. It was often possible to determine the VOC
reading vs.the MOD (Mommy?s telephone) reading
by looking at context. If it was not possible to deter-
mine the correct annotation from context, we anno-
tated such utterances as VOC relations.
After annotating the 15 Eve files, we had 18,863
fully hand-annotated utterances, 10,280 adult
and 8,563 child. The utterances consist of 84,226
GRs (including punctuation) and 65,363 words.
The average utterance length is 5.3 words (in-
cluding punctuation) for adult utterances, 3.6 for
child, 4.5 overall. The annotated Eve corpus
is available at http://childes.psy.cmu.
edu/data/Eng-USA/brown.zip. It was used
for the Domain adaptation task at the CoNLL-2007
dependency parsing shared task (Nivre, 2007).
3 Parsing
Although the CHILDES annotation scheme pro-
posed by Sagae et al (2004) has been used in prac-
tice for automatic parsing of child language tran-
scripts (Sagae et al, 2004; Sagae et al, 2005), such
work relied mainly on a statistical parser (Char-
niak, 2000) trained on the Wall Street Journal por-
tion of the Penn Treebank, since a large enough cor-
pus of annotated CHILDES data was not available
to train a domain-specific parser. Having a corpus
of 65,000 words of CHILDES data annotated with
grammatical relations represented as labeled depen-
dencies allows us to develop a parser tailored for the
CHILDES domain.
Our overall parsing approach uses a best-first
probabilistic shift-reduce algorithm, working left-to-
right to find labeled dependencies one at a time. The
algorithm is essentially a dependency version of the
data-driven constituent parsing algorithm for prob-
abilistic GLR-like parsing described by Sagae and
Lavie (2006). Because CHILDES syntactic annota-
tions are represented as labeled dependencies, using
a dependency parsing approach allows us to work
with that representation directly.
This dependency parser has been shown to have
state-of-the-art accuracy in the CoNLL shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre, 2007)3. Sagae and Tsujii (2007) present a
detailed description of the parsing approach used in
our work, including the parsing algorithm. In sum-
mary, the parser uses an algorithm similar to the LR
parsing algorithm (Knuth, 1965), keeping a stack of
partially built syntactic structures, and a queue of
remaining input tokens. At each step in the pars-
ing process, the parser can apply a shift action (re-
move a token from the front of the queue and place
it on top of the stack), or a reduce action (pop the
two topmost stack items, and push a new item com-
posed of the two popped items combined in a sin-
gle structure). This parsing approach is very similar
to the one used successfully by Nivre et al (2006),
but we use a maximum entropy classifier (Berger et
al., 1996) to determine parser actions, which makes
parsing extremely fast. In addition, our parsing ap-
proach performs a search over the space of possible
parser actions, while Nivre et al?s approach is de-
terministic. See Sagae and Tsujii (2007) for more
information on the parser.
Features used in classification to determine
whether the parser takes a shift or a reduce action
at any point during parsing are derived from the
parser?s current configuration (contents of the stack
and queue) at that point. The specific features used
are:4
? Word and its POS tag: s(1), q(2), and q(1).
? POS: s(3) and q(2).
3The parser used in this work is the same as the probabilistic
shift-reduce parser referred to as ?Sagae? in the cited shared
task descriptions. In the 2007 shared task, an ensemble of shift-
reduce parsers was used, but only a single parser is used here.
4s(n) denotes the n-th item from the top of the stack (where
s(1) is the item on the top of the stack), and q(n) denotes the
n-th item from the front of the queue.
28
? The dependency label of the most recently at-
tached dependent of: s(1) and s(2).
? The previous parser action.
4 Evaluation
4.1 Methodology
We first evaluate the parser by 15-fold cross-
validation on the 15 manually curated gold-standard
Eve files (to evaluate the parser on each file, the re-
maining 14 files are used to train the parser). Single-
word utterances (excluding punctuation) were ig-
nored, since their analysis is trivial and their inclu-
sion would artificially inflate parser accuracy mea-
surements. The size of the Eve evaluation corpus
(with single-word utterances removed) was 64,558
words (or 59,873 words excluding punctuation). Of
these, 41,369 words come from utterances spoken
by adults, and 18,504 come from utterances spo-
ken by the child. To evaluate the parser?s portabil-
ity to other CHILDES corpora, we also tested the
parser (trained only on the entire Eve set) on two ad-
ditional sets, one taken from the MacWhinney cor-
pus (MacWhinney, 2000) (5,658 total words, 3,896
words in adult utterances and 1,762 words in child
utterances), and one taken from the Seth corpus (Pe-
ters, 1987; Wilson and Peters, 1988) (1,749 words,
1,059 adult and 690 child).
The parser is highly efficient: training on the en-
tire Eve corpus takes less that 20 minutes on stan-
dard hardware, and once trained, parsing the Eve
corpus takes 18 seconds, or over 3,500 words per
second.
Following recent work on dependency parsing
(Nivre, 2007), we report two evaluation measures:
labeled accuracy score (LAS) and unlabeled accu-
racy score (UAS). LAS is the percentage of tokens
for which the parser predicts the correct head-word
and dependency label. UAS ignores the dependency
labels, and therefore corresponds to the percentage
of words for which the correct head was found. In
addition to LAS and UAS, we also report precision
and recall of certain grammatical relations.
For example, compare the parser output of go buy
an apple to the gold standard (Figure 1). This se-
quence of GRs has two labeled dependency errors
and one unlabeled dependency error. 1|2|COORD
for the parser versus 1|2|SRL is a labeled error be-
cause the dependency label produced by the parser
(COORD) does not match the gold-standard anno-
tation (SRL), although the unlabeled dependency is
correct, since the headword assignment, 1|2, is the
same for both. On the other hand, 5|1|PUNCT ver-
sus 5|2|PUNCT is both a labeled dependency error
and an unlabeled dependency error, since the head-
word assignment produced by the parser does not
match the gold-standard.
4.2 Results
Trained on domain-specific data, the parser per-
formed well on held-out data, even though the train-
ing corpus is relatively small (about 60,000 words).
The results are listed in Table 1.
LAS UAS
Eve cross-validation 92.0 93.8
Table 1: Average cross-validation results, Eve
The labeled dependency error rate is about 8%
and the unlabeled error rate is slightly over 6%. Per-
formance in individual files ranged between the best
labeled error rate of 6.2% and labeled error rate of
4.4% for the fifth file, and the worst error rates of
8.9% and 7.8% for labeled and unlabeled respec-
tively in the fifteenth file. For comparison, Sagae et
al. (2005) report 86.9% LAS on about 2,000 words
of Eve data, using the Charniak (2000) parser with
a separate dependency-labeling step. Part of the rea-
son we obtain levels of accuracy higher than usu-
ally reported for dependency parsers is that the aver-
age sentence length in CHILDES transcripts is much
lower than in, for example, newspaper text. The av-
erage sentence length for adult utterances in the Eve
corpus is 6.1 tokens, and 4.3 tokens for child utter-
ances5.
Certain GRs are easily identifiable, such as DET,
AUX, and INF. The parser has precision and recall
of nearly 1.00 for those. For all GRs that occur more
than 1,000 times in the Eve corpus (which contrains
more than 60,000 tokens), precision and recall are
above 0.90, with the exception of COORD, which
5This differs from the figures in section 2.3 because for the
purpose of parser evaluation we ignore sentences composed
only of a single word plus punctuation.
29
go buy an apple .
parser: 1|2|COORD 2|0|ROOT 3|4|DET 4|2|OBJ 5|1|PUNCT
gold: 1|2|SRL 2|0|ROOT 3|4|DET 4|2|OBJ 5|2|PUNCT
Figure 1: Example output: parser vs. gold annotation
occurs 1,163 times in the gold-standard data. The
parser?s precision for COORD is 0.73, and recall
is 0.84. Other interesting GRs include SUBJ, OBJ,
JCT (adjunct), COM, LOC, COMP, XCOMP, CJCT
(subordinate clause acting as an adjunct), and PTL
(verb particle, easily confusable with prepositions
and adverbs). Their precision and recall is shown
in table 2.
GR Precision Recall F-score
SUBJ 0.96 0.96 0.96
OBJ 0.93 0.94 0.93
JCT 0.91 0.90 0.90
COM 0.96 0.95 0.95
LOC 0.95 0.90 0.92
COMP 0.83 0.86 0.84
XCOMP 0.86 0.87 0.87
CJCT 0.61 0.59 0.60
PTL 0.97 0.96 0.96
COORD 0.73 0.84 0.78
Table 2: Precision, recall and f-score of selected
GRs in the Eve corpus
We also tested the accuracy of the parser on child
utterances and adult utterances separately. To do
this, we split the gold standard files into child and
adult utterances, producing gold standard files for
both child and adult utterances. We then trained
the parser on 14 of the 15 Eve files with both child
and adult utterances, and parsed the individual child
and adult files. Not surprisingly, the parser per-
formed slightly better on the adult utterances due to
their grammaticality and the fact that there was more
adult training data than child training data. The re-
sults are listed in Table 3.
LAS UAS
Eve - Child 90.0 91.7
Eve - Adult 93.1 94.8
Table 3: Average child vs. adult results, Eve
Our final evaluation of the parser involved test-
ing the parser on data taken from a different parts of
the CHILDES database. First, the parser was trained
on all gold-standard Eve files, and tested on man-
ually annotated data taken from the MacWhinney
transcripts. Although accuracy was lower for adult
utterances (85.8% LAS) than on Eve data, the accu-
racy for child utterances was slightly higher (92.3%
LAS), even though child utterances were longer on
average (4.7 tokens) than in the Eve corpus.
Finally, because a few aspects of the many tran-
script sets in the CHILDES database may vary in
ways not accounted for in the design of the parser
or the annotation of the training data, we also re-
port results on evaluation of the Eve-trained parser
on a particularly challenging test set, the Seth cor-
pus. Because the Seth corpus contains transcriptions
of language phenomena not seen in the Eve corpus
(see section 5), parser performance is expected to
suffer. Although accuracy on adult utterances is high
(92.2% LAS), accuracy on child utterances is very
low (72.7% LAS). This is due to heavy use of a GR
label that does not appear at all in the Eve corpus
that was used to train the parser. This GR is used to
represent relations involving filler syllables, which
appear in nearly 45% of the child utterances in the
Seth corpus. Accuracy on the sentences that do not
contain filler syllables is at the same level as in the
other corpora (91.1% LAS). Although we do not ex-
pect to encounter many sets of transcripts that are as
problematic as this one in the CHILDES database, it
is interesting to see what can be expected from the
parser under unfavorable conditions.
The results of the parser on the MacWhinney and
Seth test sets are summarized in table 4, where Seth
(clean) refers to the Seth corpus without utterances
that contain filler sylables.
5 Error Analysis
A major source for parser errors on the Eve cor-
pus (112 out of 5181 errors) was telegraphic speech,
30
LAS UAS
MacWhinney - Child 92.3 94.8
MacWhinney - Adult 85.8 89.4
MacWhinney - Total 88.0 91.2
Seth - Child 72.7 82.0
Seth - Adult 92.2 94.4
Seth - Total 84.6 89.5
Seth (clean) - Child 91.1 92.7
Seth (clean) - Total 92.0 93.9
Table 4: Training on Eve, testing on MacWhinney
and Seth
as in Mommy telephone or Fraser tape+recorder
floor. Telegraphic speech may be the most chal-
lenging, since even for a human annotator, deter-
mining a GR is difficult. The parser usually labeled
such utterances with the noun as the ROOT and the
proper noun as the MOD, while the gold annotation
is context-dependent as described above.
Another category of errors, with about 150 in-
stances, is XCOMP errors. The majority of the er-
rors in this category revolve around dropped words
in the main clause, for example want eat cookie. Of-
ten, the parser labels such utterances with COMP
GRs, because of the lack of to. Exclusive training on
utterances of this type may resolve the issue. Many
of the errors of this type occur with want : the parser
could be conditioned to assign an XCOMP GR with
want as the ROOT of an utterance.
COORD and PRED errors would both benefit
from more data as well. The parser performs ad-
mirably on simple coordination and predicate con-
structions, but has troubles with less common con-
structions such as PRED GRs with get, e.g., don?t
let your hands get dirty (69 errors), and coordina-
tion of prepositional objects, as in a birthday cake
with Cathy and Becky (154 errors).
The performance drop on the Seth corpus can be
explained by a number of factors. First and fore-
most, Seth is widely considered in the literature to
be the child who is most likely to invalidate any the-
ory (Wilson and Peters, 1988). He exhibits false
starts and filler syllables extensively, and his syn-
tax violates many ?universal? principles. This is
reflected in the annotation scheme: the Seth cor-
pus, following the annotation of Peters (1983), is
abundant with filler syllables. Because there was
no appropriate GR label for representing the syn-
tactic relationships involving the filler syllables, we
annotated those with a special GR (not used during
parser training), which the parser is understandably
not able to produce. Filler syllables usually occur
near the start of the sentence, and once the parser
failed to label them, it could not accurately label the
remaining GRs. Other difficulties in the Seth cor-
pus include the usage of dates, of which there were
no instances in the Eve corpus. The parser had not
been trained on the new DATE GR and subsequently
failed to parse it.
6 Conclusion
We described an annotation scheme for represent-
ing syntactic information as grammatical relations
in CHILDES data, a manually curated gold-standard
corpus of 65,000 words annotated according to this
GR scheme, and a parser that was trained on the an-
notated corpus and produces highly accurate gram-
matical relations for both child and adult utterances.
These resources are now freely available to the re-
search community, and we expect them to be in-
strumental in psycholinguistic investigations of lan-
guage acquisition and child language.
Syntactic analysis of child language transcripts
using a GR scheme of this kind has already been
shown to be effective in a practical setting, namely
in automatic measurement of syntactic development
in children (Sagae et al, 2005). That work relied on
a phrase-structure statistical parser (Charniak, 2000)
trained on the Penn Treebank, and the output of that
parser had to be converted into CHILDES grammat-
ical relations. Despite the obvious disadvantage of
using a parser trained on a completely different lan-
guage genre, Sagae et al (2005) demonstrated how
current natural language processing techniques can
be used effectively in child language work, achiev-
ing results that are close to those obtained by man-
ual computation of syntactic development scores for
child transcripts. Still, the use of tools not tailored
for child language and extra effort necessary to make
them work with community standards for child lan-
guage transcription present a disincentive for child
language researchers to incorporate automatic syn-
tactic analysis into their work. We hope that the GR
31
representation scheme and the parser presented here
will make it possible and convenient for the child
language community to take advantage of some of
the recent developments in natural language parsing,
as was the case with part-of-speech tagging when
CHILDES specific tools were first made available.
Our immediate plans include continued improve-
ment of the parser, which can be achieved at least in
part by the creation of additional training data from
other English CHILDES corpora. We also plan to
release automatic syntactic analyses for the entire
English portion of CHILDES.
Although we have so far focused exclusively on
English CHILDES data, dependency schemes based
on functional relationships exist for a number of lan-
guages (Buchholz and Marsi, 2006), and the general
parsing techniques used in the present work have
been shown to be effective in several of them (Nivre
et al, 2006). As future work, we plan to adapt
existing dependency-based annotation schemes and
apply our current syntactic annotation and pars-
ing framework to other languages in the CHILDES
database.
Acknowledgments
We thank Marina Fedner for her help with annota-
tion of the Eve corpus. This work was supported in
part by the National Science Foundation under grant
IIS-0414630.
References
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
Amaximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Roger Brown. 1973. A first language: the early stages.
George Allen & Unwin Ltd., London.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new proposal.
In Proceedings of the 1st International Conference on
Language Resources and Evaluation, pages 447?454,
Granada, Spain.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
D. Knuth. 1965. On the translation of languages from
left to right. Information and Control, 8(6):607?639.
Brian MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates,
Mahwah, NJ, third edition.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of the Tenth Conference on
Computational Natural Language Learning.
Joakim Nivre, editor. 2007. CoNLL-XI Shared Task on
Multilingual Dependency Parsing, Prague, June. As-
sociation for Computational Linguistics.
Ann M. Peters. 1983. The Units of Language Acquisi-
tion. Monographs in Applied Psycholinguistics. Cam-
bridge University Press, New York.
Ann M. Peters. 1987. The role of immitation in the de-
veloping syntax of a blind child. Text, 7:289?311.
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 691?698, Sydney, Australia, July. Association
for Computational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with lr models and parser
ensembles. In Proceedings of the Eleventh Conference
on Computational Natural Language Learning.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2004.
Adding syntactic annotations to transcripts of parent-
child dialogs. In Proceedings of the Fourth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2004), Lisbon, Portugal.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 197?204, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
B. Wilson and Ann M. Peters. 1988. What are you
cookin? on a hot?: A three-year-old blind child?s ?vi-
olation? of universal constraints on constituent move-
ment. Language, 64:249?273.
32
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evaluating the Effects of Treebank Size in a Practical Application for 
Parsing 
Kenji Sagae1, Yusuke Miyao1, Rune S?tre1 and Jun'ichi Tsujii1,2,3 
1Department of Computer Science, Univerisity of Tokyo, Japan 
2School of Computer Science, University of Manchester 
3National Center for Text Mining, Manchester, UK 
{sagae,yusuke,rune.saetre,tsujii@is.s.u-tokyo.ac.jp} 
 
 
 
 
 
Abstract 
Natural language processing modules such as 
part-of-speech taggers, named-entity recog-
nizers and syntactic parsers are commonly 
evaluated in isolation, under the assumption 
that artificial evaluation metrics for individual 
parts are predictive of practical performance 
of more complex language technology sys-
tems that perform practical tasks. Although 
this is an important issue in the design and en-
gineering of systems that use natural language 
input, it is often unclear how the accuracy of 
an end-user application is affected by parame-
ters that affect individual NLP modules.  We 
explore this issue in the context of a specific 
task by examining the relationship between 
the accuracy of a syntactic parser and the 
overall performance of an information extrac-
tion system for biomedical text that includes 
the parser as one of its components.  We 
present an empirical investigation of the rela-
tionship between factors that affect the accu-
racy of syntactic analysis, and how the 
difference in parse accuracy affects the overall 
system.   
1 Introduction 
Software systems that perform practical tasks with 
natural language input often include, in addition to 
task-specific components, a pipeline of basic natu-
ral language processing modules, such as part-of-
speech taggers, named-entity recognizers, syntactic 
parsers and semantic-role labelers.  Although such 
building blocks of larger language technology so-
lutions are usually carefully evaluated in isolation 
using standard test sets, the impact of improve-
ments in each individual module on the overall 
performance of end-to-end systems is less well 
understood.  While the effects of the amount of 
training data, search beam widths and various ma-
chine learning frameworks have been explored in 
detail with respect to speed and accuracy in basic 
natural language processing tasks, how these trade-
offs in individual modules affect the performance 
of the larger systems they compose is an issue that 
has received relatively little attention.  This issue, 
however, is of great practical importance in the 
effective design and engineering of complex soft-
ware systems that deal with natural language.   
In this paper we explore some of these issues 
empirically in an information extraction task in the 
biomedical domain, the identification of protein- 
protein interactions (PPI) mentioned in papers ab-
stracts from MEDLINE, a large database of bio-
medical papers.  Due in large part to the creation of 
biomedical treebanks (Kulick et al, 2004; Tateisi 
et al, 2005) and rapid progress of data-driven 
parsers (Lease and Charniak, 2005; Nivre et al, 
2007), there are now fast, robust and accurate syn-
tactic parsers for text in the biomedical domain.  
Recent research shows that parsing accuracy of 
biomedical corpora is now between 80% and 90% 
(Clegg and Shepherd, 2007; Pyysalo et al, 2007; 
Sagae et al, 2008).  Intuitively, syntactic relation-
ships between words should be valuable in deter-
mining possible interactions between entities 
present in text.  Recent PPI extraction systems 
have confirmed this intuition (Erkan et al, 2007; 
S?tre et al, 2007; Katrenko and Adriaans, 2006).     
While it is now relatively clear that syntactic 
parsing is useful in practical tasks that use natural 
language corpora in bioinformatics, several ques-
14
tions remain as to research issues that affect the 
design and testing of end-user applications, includ-
ing how syntactic analyses should be used in a 
practical setting, whether further improvements in 
parsing technologies will result in further im-
provements in practical systems, whether it is im-
portant to continue the development of treebanks 
and parser adaptation techniques for the biomedi-
cal domain, and how much effort should be spent 
on comparing and benchmarking parsers for bio-
medical data.  We attempt to shed some light on 
these matters by presenting experiments that show 
the relationship of the accuracy of a dependency 
parser and the accuracy of the larger PPI system 
that includes the parser.  We investigate the effects 
of domain-specific treebank size (the amount of 
available manually annotated training data for syn-
tactic parsers) and final system performance, and 
obtain results that should be informative to re-
searchers in bioinformatics who rely on existing 
NLP resources to design information extraction 
systems, as well as to members of the parsing 
community who are interested in the practical im-
pact of parsing research. 
In section 2 we discuss our motivation and re-
lated efforts.  Section 3 describes the system for 
identification of protein-protein interactions used 
in our experiments, and in section 4 describes the 
syntactic parser that provides the analyses for the 
PPI system, and the data used to train the parser.  
We describe our experiments, results and analysis 
in section 5, and conclude in section 6.  
2 Motivation and related work 
While recent work has addressed questions relating 
to the use of different parsers or different types of 
syntactic representations in the PPI extraction task 
(S?tre et al, 2007, Miyao et al, 2008), little con-
crete evidence has been provided for potential ben-
efits of improved parsers or additional resources 
for training syntactic parsers.  In fact, although 
there is increasing interest in parser evaluation in 
the biomedical domain in terms of precision/recall 
of brackets and dependency accuracy (Clegg and 
Shepherd, 2007; Pyysalo et al, 2007; Sagae et al, 
2008), the relationship between these evaluation 
metrics and the performance of practical informa-
tion extraction systems remains unclear.  In the 
parsing community, relatively small accuracy gains 
are often reported as success stories, but again, the 
precise impact of such improvements on practical 
tasks in bioinformatics has not been established. 
One aspect of this issue is the question of do-
main portability and domain adaptation for parsers 
and other NLP modules.  Clegg and Shepherd 
(2007) mention that available statistical parsers 
appear to overfit to the newswire domain, because 
of their extensive use of the Wall Street Journal 
portion of the Penn Treebank (Marcus et al, 1994) 
during development and training.  While this claim 
is supported by convincing evaluations that show 
that parsers trained on the WSJ Penn Treebank 
alone perform poorly on biomedical text in terms 
of accuracy of dependencies or bracketing of 
phrase structure, the benefits of using domain-
specific data in terms of practical system perfor-
mance have not been quantified.  These expected 
benefits drive the development of domain-specific 
resources, such as the GENIA treebank (Tateisi et 
al., 2005), and parser domain adaption (Hara et al, 
2007), which are of clear importance in parsing 
research, but of largely unconfirmed impact on 
practical systems. 
Quirk and Corston-Oliver (2006) examine a 
similar issue, the relationship between parser accu-
racy and overall system accuracy in syntax-
informed machine translation.  Their research is 
similar to the work presented here, but they fo-
cused on the use of varying amounts of out-of-
domain training data for the parser, measuring how 
a translation system for technical text performed 
when its syntactic parser was trained with varying 
amounts of Wall Street Journal text.  Our work, in 
contrast, investigates the use of domain-specific 
training material in parsers for biomedical text, a 
domain where significant amounts of effort are 
allocated for development of domain-specific NLP 
resources in hope that such resources will result in 
better overall performance in practical systems.  
3 A PPI extraction system based on syn-
tactic parsing 
PPI extraction is an NLP task to identify protein 
pairs that are mentioned as interacting in biomedi-
cal papers.  Figure 2 shows two sentences that in-
clude protein names: the former sentence mentions 
a protein interaction, while the latter does not.  
Given a protein pair, PPI extraction is a task of 
binary classification; for example, <IL-8, CXCR1> 
15
is a positive example, and <RBP, TTR> is a ne-
gative example. 
Following recent work on using dependency 
parsing in systems that identify protein interactions 
in biomedical text (Erkan et al, 2007; S?tre et al, 
2007; Katrenko and Adriaans, 2006), we have built 
a system for PPI extraction that uses dependency 
relations as features. As exemplified, for the pro-
tein pair IL-8 and CXCR1 in the first sentence of 
Figure 2, a dependency parser outputs a dependen-
cy tree shown in Figure 1.  From this dependency 
tree, we can extract a dependency path between 
IL-8 and CXCR1 (Figure 3), which appears to be 
a strong clue in knowing that these proteins are 
mentioned as interacting. 
The system we use in this paper is similar to the 
one described in S?tre et al (2007), except that it 
uses syntactic dependency paths obtained with a 
dependency parser, but not predicate-argument 
paths based on deep-parsing.  This method is based 
on SVM with SubSet Tree Kernels (Collins, 2002; 
Moschitti, 2006).  A dependency path is encoded 
as a flat tree as depicted in Figure 4. Because a tree 
kernel measures the similarity of trees by counting 
common subtrees, it is expected that the system 
finds effective subsequences of dependency paths.   
In addition to syntactic dependency features, we 
incorporate bag-of-words features, which are re-
garded as a strong baseline for IE systems.  We use 
lemmas of words before, between and after the pair 
of target proteins. 
In this paper, we use Aimed (Bunescu and 
Mooney, 2004), which is a popular benchmark for 
the evaluation of PPI extraction systems.  The 
Aimed corpus consists of 225 biomedical paper 
abstracts (1970 sentences), which are sentence-
split, tokenized, and annotated with proteins and 
PPIs.  
4 A data-driven dependency parser for 
biomedical text 
The parser we used as component of our PPI ex-
traction system was a shift-reduce dependency 
parser that uses maximum entropy models to de-
termine the parser?s actions.  Our overall parsing 
approach uses a best-first probabilistic shift-reduce 
algorithm, working left-to right to find labeled de-
pendencies one at a time. The algorithm is essen-
tially a dependency version of the constituent 
parsing algorithm for probabilistic parsing with 
LR-like data-driven models described by Sagae 
and Lavie (2006).  This dependency parser has 
been shown to have state-of-the-art accuracy in the 
CoNLL shared tasks on dependency parsing 
(Buchholz and Marsi, 2006; Nivre, 2007). Sagae 
and Tsujii (2007) present a detailed description of 
the parsing approach used in our work, including 
the parsing algorithm and the features used to clas-
sify parser actions.  In summary, the parser uses an 
algorithm similar to the LR parsing algorithm 
(Knuth, 1965), keeping a stack of partially built 
syntactic structures, and a queue of remaining in-
put tokens.  At each step in the parsing process, the 
parser can apply a shift action (remove a token 
from the front of the queue and place it on top of 
the stack), or a reduce action (pop the two topmost 
This study demonstrates that IL-8 recognizes 
and activates CXCR1, CXCR2, and the Duf-
fy antigen by distinct mechanisms. 
 
The molar ratio of serum retinol-binding pro-
tein (RBP) to transthyretin (TTR) is not 
useful to assess vitamin A status during infec-
tion in hospitalized children. 
Figure 2: Example sentences with protein names 
Figure 1: A dependency tree 
ROOT  IL-8  recognizes  and  activates  CXCR1 
ROOT 
SBJ 
OBJ 
COORD 
CC 
ENTITY1(IL-8)    recognizes   ENTITY2(CXCR1) 
Figure 3: A dependency path between protein names 
SBJ OBJ 
16
stack items, and push a new item composed of the 
two popped items combined in a single structure). 
This parsing approach is very similar to the one 
used successfully by Nivre et al (2006), but we 
use a maximum entropy classifier (Berger et al, 
1996) to determine parser actions, which makes 
parsing considerably faster. In addition, our pars-
ing approach performs a search over the space of 
possible parser actions, while Nivre et al?s ap-
proach is deterministic. 
The parser was trained using 8,000 sentences 
from the GENIA Treebank (Tateisi et al, 2005), 
which contains abstracts of papers taken from 
MEDLINE, annotated with syntactic structures.  
To determine the effects of training set size on the 
parser, and consequently on the PPI extraction sys-
tem, we trained several parsing models with differ-
ent amounts of GENIA Treebank data.  We started 
with 100 sentences, and increased the training set 
by 100 sentence increments, up to 1,000 sentences.  
From that point, we increased the training set by 
1,000 sentence increments.  Figure 5 shows the 
labeled dependency accuracy for the varying sizes 
of training sets.  The accuracy was measured on a 
portion of the GENIA Treebank reserved as devel-
opment data.  The result clearly demonstrates that 
the increase in the size of the training set contri-
butes to increasing parse accuracy.  Training the 
parser with only 100 sentences results in parse ac-
curacy of about 72.5%.  Accuracy rises sharply 
with additional training data until the size of the 
training set reaches about 1,000 sentences (about 
82.5% accuracy).  From there, accuracy climbs 
consistently, but slowly, until 85.6% accuracy is 
reached with 8,000 sentences of training data. 
It should be noted that parser accuracy on the 
Aimed data used in our PPI extraction experiments 
may be slightly lower, since the domain of the 
GENIA Treebank is not exactly the same as the 
Aimed corpus.  Both of them were extracted from 
MEDLINE, but the criteria for data selection were 
not the same in the two corpora, creating possible 
differences in sub-domains.  We also note that the 
accuracy of a parser trained with more than 40,000 
sentences from the Wall Street Journal portion of 
the Penn Treebank is under 79%, a level equivalent 
to that obtained by training the parser with only 
500 sentences of GENIA data. 
 
 
Figure 5: Data size vs. parse accuracy 
 
5 Experiments and Results 
In this section we present our PPI extraction expe-
riments applying the dependency parsers trained 
with the different amounts of the GENIA Treebank 
in our PPI system.  As we mentioned, the GENIA 
Treebank is used for training the parser, while the 
Aimed is used for training and evaluation of PPI 
extraction.  A part-of-speech tagger trained with 
GENIA and PennBioIE was used.  We do not ap-
ply automatic protein name detection, and instead 
use the gold-standard protein annotations in the 
Aimed corpus.  Before running a parser, multiword 
protein names are concatenated and treated as sin-
gle words. As described in Section 3, bag-of-words 
and syntactic dependency paths are fed as features 
to the PPI classifier. The accuracy of PPI extrac-
tion is measured by the abstract-wise 10-fold cross 
validation (S?tre et al 2007). 
When we use the part-of-speech tagger and the 
dependency parser trained with WSJ, the accuracy 
(F-score) of PPI extraction on this data set is 55.2.  
The accuracy increases to 56.9 when we train the 
part-of-speech tagger with GENIA and Penn BioIE, 
while using the WSJ-trained parser.  This confirms 
the claims by Lease and Charniak (2005) that sub-
sentential lexical analysis alone is helpful in adapt-
ing WSJ parsers to the biomedical domain.  While 
Lease and Charniak looked only at parse accuracy, 
70
75
80
85
90
0 2000 4000 6000 8000
Figure 4: A tree kernel representation of the dependency 
path 
(dep_path (SBJ (ENTITY1 ecognizes)) 
(rOBJ (recognizes ENTITY2))) 
17
our result shows that the increase in parse accuracy 
is, as expected, beneficial in practice. 
Figure 6 shows the relationship between the 
amount of parser training data and the F-score for 
the PPI extraction.  The result shows that the accu-
racy of PPI extraction increases with the use of 
more sentences to train the parser.    The best accu-
racy was obtained when using 4,000 sentences, 
where parsing accuracy is around 84.3.  Although 
it may appear that further increasing the training 
data for the parser may not improve the PPI extrac-
tion accuracy (since only small and inconsistent 
variations in F-score are observed in Figure 6), 
when we plot the curves shown in Figures 5 and 6 
in a single graph (Figure 7), we see that the two 
curves match each other to a large extent.  This is 
supported by the strong correlation between parse 
accuracy and PPI accuracy observed in Figure 8.  
While this suggests that training the parser with a 
larger treebank may result in improved accuracy in 
PPI extraction, we observe that a 1% absolute im-
provement in parser accuracy corresponds roughly 
to a 0.25 improvement in PPI extraction F-score.  
Figure 5 indicates that to obtain even a 1% im-
provement in parser accuracy by using more train-
ing data, the size of the treebank would have to 
increase significantly. 
Although the results presented so far seem to 
suggest the need for a large data annotation effort 
to achieve a meaningful improvement in PPI ex-
traction accuracy, there are other ways to improve 
the overall accuracy of the system without an im-
provement in parser accuracy.  One obvious alter-
native is to increase the size of the PPI-annotated 
corpus (which is distinct from the treebank used to 
train the parser).  As mentioned in section 3, our 
system is trained using the Aimed corpus, which 
contains 225 abstracts from biomedical papers with 
manual annotations indicating interactions between 
proteins.  Pairs of proteins with no interaction de-
scribed in the text are used as negative examples, 
and pairs of proteins described as interacting are 
used as positive examples.  The corpus contains a 
total of roughly 9,000 examples.  Figure 9 shows 
how the overall system accuracy varies when dif-
ferent amounts of training data (varying amounts 
of training examples) are used to train the PPI sys-
tem (keeping the parse accuracy constant, using all 
of the available training data in the GENIA tree-
bank to train the parser).  While Figure 5 indicates 
that a significant improvement in parse accuracy 
requires a large increase in the treebank used to 
train the parser, and Figure 7 shows that improve-
ments in PPI extraction accuracy may require a 
sizable improvement in parse accuracy, Figure 9 
suggests that even a relatively small increase in the 
PPI corpus may lead to a significant improvement 
in PPI extraction accuracy. 
 
Figure 6: Parser training data size vs. PPI extraction 
accuracy 
 
 
 
Figure 7: Parser training data size vs. parser accuracy 
and PPI extraction accuracy 
 
 
 
Figure 8: Parse accuracy vs. PPI extraction accuracy 
 
53
54
55
56
57
58
0 2000 4000 6000 8000
53
54
55
56
57
58
68
72
76
80
84
88
0 5000 10000
Parser
PPI F-score
53
54
55
56
57
58
70 75 80 85 90
18
 
 
Figure 9: Number of PPI training examples vs. PPI ex-
traction accuracy 
 
While some of the conclusions that can be 
drawn from these results may be somewhat sur-
prising, most are entirely expected.  However, even 
in these straightforward cases, our experiments 
provide some empirical evidence and concrete 
quantitative analysis to complement intuition.  We 
see that using domain-specific training data for the 
parsing component for the PPI extraction system 
produces superior results, compared to using train-
ing data from the WSJ Penn Treebank.  When the 
parser trained on WSJ sentences is used, PPI ex-
traction accuracy is about 55, compared to over 57 
when sentences from biomedical papers are used.  
This corresponds fairly closely to the differences in 
parser accuracy: the accuracy of the parser trained 
on 500 sentences from GENIA is about the same 
as the accuracy of the parser trained on the entire 
WSJ Penn Treebank, and when these parsers are 
used in the PPI extraction system, they result in 
similar overall task accuracy.  However, the results 
obtained when a domain-specific POS tagger is 
combined with a parser trained with out-of-domain 
data, overall PPI results are nearly at the same lev-
el as those obtained with domain-specific training 
data (just below 57 with a domain-specific POS 
tagger and out-of-domain parser, and just above 57 
for domain-specific POS tagger and parser).  At 
the same time, the argument against annotating 
domain-specific data for parsers in new domains is 
not a strong one, since higher accuracy levels (for 
both the parser and the overall system) can be ob-
tained with a relatively small amount of domain-
specific data. 
Figures 5, 6 and 7 also suggest that additional 
efforts in improving parser accuracy (through the 
use of feature engineering, other machine learning 
techniques, or an increase in the size of its training 
set) could improve PPI extraction accuracy, but a 
large improvement in parser accuracy may be re-
quired.  When we combine these results with the 
findings obtained by Miyao et al (2008), they sug-
gest that a better way to improve the overall sys-
tem is to spend more effort in designing a specific 
syntactic representation that addresses the needs of 
the system, instead of using a generic representa-
tion designed for measuring parser accuracy.  
Another potentially fruitful course of action is to 
design more sophisticated and effective ways for 
information extraction systems to use NLP tools, 
rather than simply extracting features that corres-
pond to small fragments of syntactic trees.  Of 
course, making proper use of natural language 
analysis is a considerable challenge, but one that 
should be kept in mind through the design of prac-
tical systems that use NLP components. 
6 Conclusion 
This paper presented empirical results on the rela-
tionship between the amount of training data used 
to create a dependency parser, and the accuracy of 
a system that performs identification of protein-
protein interactions using the dependency parser.  
We trained a dependency parser with different 
amounts of data from the GENIA Treebank to es-
tablish how the improvement in parse accuracy 
corresponds to improvement in practical task per-
formance in this information extraction task.  
While parsing accuracy clearly increased with 
larger amounts of data, and is likely to continue 
increasing with additional annotation of data for 
the GENIA Treebank, the trend in the accuracy of 
PPI extraction indicates that a sizable improvement 
in parse accuracy may be necessary for improved 
detection of protein interactions. 
When combined with recent findings by Miyao 
et al (2008), our results indicate that further work 
in designing PPI extraction systems that use syn-
tactic dependency features would benefit from 
more adequate syntactic representations or more 
sophisticated use of NLP than simple extraction of 
syntactic subtrees.  Furthermore, to improve accu-
racy in this task, efforts on data annotation should 
focus on task-specific data (manual annotation of 
40
45
50
55
60
65
0 5000 10000 15000
19
protein interactions in biomedical papers), rather 
than on additional training data for syntactic pars-
ers.  While annotation of parser training data might 
seems like a cost-effective choice, since improved 
parser results might be beneficial in a number of 
systems where the parser can be used, our results 
show that, in this particular task, efforts should be 
focused elsewhere, such as the annotation of addi-
tion PPI data.  
Acknowledgements 
We thank the anonymous reviewers for their in-
sightful comments.  This work was partially sup-
ported by Grant-in-Aid for Specially Promoted 
Research (MEXT, Japan), Genome Network 
Project (MEXT, Japan), and Grant-in-Aid for 
Young Scientists (MEXT, Japan). 
References 
 
Berger, A., S. A. Della Pietra, and V. J. Della Pietra. 
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 
22(1):39?71. 
Clegg, A. and Shepherd, A. 2007. Benchmarking natu-
ral-language parsers for biological applications using 
dependency graphs. BMC Bioinformatics, 8:24. 
Erkan, G., A. Ozgur, and D. R. Radev. 2007. Semisu-
pervised classification for extracting protein interac-
tion sentences using dependency parsing. In 
Proceedings of CoNLL-EMNLP 2007. 
Hara, T., Miyao, Y and Tsujii, J. 2007. Evaluating Im-
pact of Re-training a Lexical Disambiguation Model 
on Domain Adaptation of an HPSG Parser. In Pro-
ceedings of the International Conference on Parsing 
Technologies (IWPT). 
Katrenko, S. and P. W. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency 
trees. In Proceedings of the first workshop on Know-
ledge Discovery and Emergent Complexity in BioIn-
formatics (KDECB), pages 61?80. 
Kulick, S., A. Bies, M. Liberman, M. Mandel, R. 
McDonald, M. Palmer, A. Schein and L. Ungar. 2004. 
Integrated Annotation for Biomedical Information 
Extraction. In Proceedings of Biolink 2004: Linking 
Biological Literature, Ontologies and Databases 
(HLT-NAACL workshop). 
Lease, M. and Charniak, E. 2005. Parsing Biomedical 
Literature. In R. Dale, K.-F. Wong, J. Su, and O. 
Kwong, editors, Proceedings of the 2nd International 
Joint Conference on Natural Language Processing 
(IJCNLP'05), volume 3651 of Lecture Notes in 
Computer Science, pages 58 ? 69. 
Miyao, Y., S?tre, R., Sagae, K., Matsuzaki, T. and Tsu-
jii, J. 2008. Task-Oriented Evaluation of Syntactic 
Parsers and Their Representations.  In Proceedings of 
the 46th Annual Meeting of the Association for Com-
putational Linguistics. 
Nivre, J., Hall, J., Kubler, S., McDonald, R., Nilsson, J., 
Riedel, S. and Yuret, D. 2007. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proceedings 
the CoNLL 2007 Shared Task in EMNLP-CoNLL. 
Nivre, Joakim, Johan Hall, Jens Nilsson, Gulsen Eryi-
git,and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector 
machines. In Proceedings of the Tenth Conference on 
Computational Natural Language Learning, shared 
task session. 
Pyysalo S., Ginter F., Haverinen K., Heimonen J., Sala-
koski T. and Laippala V. 2007. On the unification of 
syntactic annotations under the Stanford dependency 
scheme: A case study on BioInfer and GENIA.  In 
Proceedings of BioNLP 2007: Biological, Transla-
tional and Clinical Language Processing. 
Quirk, C. and Corston-Oliver S. 2006. The impact of 
parse quality on syntactically-informed statistical 
machine translation. In Proceedings of EMNLP 2007. 
S?tre, R., Sagae, K., and Tsujii, J. 2007. Syntactic fea-
tures for protein-protein interaction extraction.  In 
Proceedings of the International Symposium on Lan-
guages in Biology and Medicine (LBM short oral 
presentations). 
Sagae, K. and Lavie, A. 2006. A best-first probabilistic 
shift-reduce parser. In Proceedings of the 
COLING/ACL 2006 Main Conference Poster Ses-
sions, pages 691?698, Sydney, Australia, July. Asso-
ciation for Computational Linguistics. 
Sagae, K., Miyao, Y. and Tsujii, J. 2008. Challenges in 
Mapping of Syntactic Representations for Frame-
work-Independent Parser Evaluation. In Proceedings 
of the Workshop on Automated Syntatic Annotations 
for Interoperable Language Resources at the First 
International Conference on Global Interoperability 
for Language Resources (ICGL'08). 
Tateisi, Y., Yakushiji, A., Ohta, T., and Tsujii, J. 2005. 
Syntax annotation for the GENIA corpus. In Pro-
ceedings Second International Joint Conference on 
Natural Language Processing: Companion Volume 
including Posters/Demos and tutorial abstracts. 
20
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 860?868,
Beijing, August 2010
Latent Mixture of Discriminative Experts
for Multimodal Prediction Modeling
Derya Ozkan, Kenji Sagae and Louis-Philippe Morency
USC Institute for Creative Technologies
{ozkan,sagae,morency}@ict.usc.edu
Abstract
During face-to-face conversation, people
naturally integrate speech, gestures and
higher level language interpretations to
predict the right time to start talking or
to give backchannel feedback. In this
paper we introduce a new model called
Latent Mixture of Discriminative Experts
which addresses some of the key issues
with multimodal language processing: (1)
temporal synchrony/asynchrony between
modalities, (2) micro dynamics and (3) in-
tegration of different levels of interpreta-
tion. We present an empirical evaluation
on listener nonverbal feedback prediction
(e.g., head nod), based on observable be-
haviors of the speaker. We confirm the im-
portance of combining four types of mul-
timodal features: lexical, syntactic struc-
ture, eye gaze, and prosody. We show
that our Latent Mixture of Discriminative
Experts model outperforms previous ap-
proaches based on Conditional Random
Fields (CRFs) and Latent-Dynamic CRFs.
1 Introduction
Face-to-face communication is highly interactive.
Even when only one person speaks at a time,
other participants exchange information continu-
ously amongst themselves and with the speaker
through gestures, gaze and prosody. These differ-
ent channels contain complementary information
essential to interpretation and understanding of
human behaviors (Oviatt, 1999). Psycholinguistic
studies also suggest that gesture and speech come
from a single underlying mental process, and they
Pitch
Words
Gaze
TimeP
(no
d)
Look  at listener
Speaker
Listener
Prediction
Figure 1: Example of multimodal prediction
model: listener nonverbal backchannel prediction
based on speaker?s speech and eye gaze. As the
speaker says the word her, which is the end of the
clause (her is also the object of the verb bother-
ing), and lowers the pitch while looking back at
the listener and eventually pausing, the listener
is then very likely to head nod (i.e., nonverbal
backchannel).
are related both temporally and semantically (Mc-
Neill, 1992; Cassell and Stone, 1999; Kendon,
2004).
A good example of such complementarity is
how people naturally integrate speech, gestures
and higher level language to predict when to give
backchannel feedback. Building computational
models of such a predictive process is challeng-
ing since it involves micro dynamics and temporal
relationship between cues from different modali-
ties (Quek, 2003). Figure 1 shows an example of
backchannel prediction where a listener head nod
860
is more likely. For example, a temporal sequence
from the speaker where he/she reaches the end of
segment (syntactic feature) with a low pitch and
looks at the listener before pausing is a good op-
portunity for the listener to give nonverbal feed-
back (e.g., head nod). These prediction models
have broad applicability, including the improve-
ment of nonverbal behavior recognition, the syn-
thesis of natural animations for robots and virtual
humans, the training of cultural-specific nonver-
bal behaviors, and the diagnoses of social disor-
ders (e.g., autism spectrum disorder).
In this paper we introduce a new model
called Latent Mixture of Discriminative Experts
(LMDE) which addresses some of the key issues
with multimodal language processing: (1) tempo-
ral synchrony/asynchrony between modalities, (2)
micro dynamics and (3) integration of different
levels of interpretation. We present an empirical
evaluation on nonverbal feedback prediction (e.g.,
head nod) confirming the importance of combin-
ing different types of multimodal features. We
show that our LMDE model outperforms previ-
ous approaches based Conditional Random Fields
(CRFs) and Latent-Dynamic CRFs.
2 Related Work
Earlier work in multimodal language processing
focused on multimodal dialogue systems where
the gestures and speech may be constrained (John-
ston, 1998; Jurafsky et al, 1998). Most of
the research in multimodal language processing
over the past decade fits within two main trends
that have emerged: (1) recognition of individ-
ual multimodal actions such as speech and ges-
tures (e.g, (Eisenstein et al, 2008; Frampton et
al., 2009; Gravano et al, 2007)), and (2) recog-
nition/summarization of the social interaction be-
tween more than one participants (e.g., meeting
analysis (Heylen and op den Akker, 2007; Moore,
2007; Murray and Carenini, 2009; Jovanovic et
al., 2006)).
The work described in this paper can be seen
from a third intermediate category where multi-
modal cues from one person is used to predict
the social behavior of another participant. This
type of predictive models has been mostly stud-
ied in the context of embodied conversational
agents (Nakano et al, 2003; Nakano et al, 2007).
In particular, backchannel feedback (the nods and
paraverbals such as ?uh-hu? and ?mm-hmm? that
listeners produce as someone is speaking) has re-
ceived considerable interest due to its pervasive-
ness across languages and conversational contexts
and this paper addresses the problem of how to
predict and generate this important class of dyadic
nonverbal behavior.
Several researchers have developed models to
predict when backchannel should happen. In gen-
eral, these results are difficult to compare as they
utilize different corpora and present varying eval-
uation metrics. Ward and Tsukahara (2000) pro-
pose a unimodal approach where backchannels
are associated with a region of low pitch last-
ing 110ms during speech. Models were pro-
duced manually through an analysis of English
and Japanese conversational data. Nishimura
et al (2007) present a unimodal decision-tree
approach for producing backchannels based on
prosodic features. Cathcart et al (2003) propose a
unimodal model based on pause duration and tri-
gram part-of-speech frequency. The model was
constructed by identifying, from the HCRC Map
Task Corpus (Anderson et al, 1991), trigrams
ending with a backchannel. Fujie et al (2004)
used Hidden Markov Models to perform head nod
recognition. In their paper, they combined head
gesture detection with prosodic low-level features
from the same person to determine strongly pos-
itive, weak positive and negative responses to
yes/no type utterances.
In recent years, great research has shown the
strength of latent variable models for natural lan-
guage processing (Blunsom et al, 2008). One of
the most relevant works is that of Eisenstein and
Davis (2007), which presents a latent conditional
model for fusion of multiple modalities (speech
and gestures). One of the key difference of our
work is that we are explicitly modeling the mi-
cro dynamics and temporal relationship between
modalities.
3 Multimodal Prediction Models
Human face-to-face communication is a little like
a dance, in that participants continuously adjust
their behaviors based on verbal and nonverbal dis-
861
plays and signals. A topic of central interest in
modeling such behaviors is the patterning of in-
terlocutor actions and interactions, moment-by-
moment, and one of the key challenges is iden-
tifying the patterns that best predict specific ac-
tions. Thus we are interested in developing pre-
dictive models of communication dynamics that
integrate previous and current actions from all in-
terlocutors to anticipate the most likely next ac-
tions of one or all interlocutors. Humans are good
at this: they have an amazing ability to predict, at
a micro-level, the actions of an interlocutor (Bave-
las et al, 2000); and we know that better predic-
tions can correlate with more empathy and better
outcomes (Goldberg, 2005; Fuchs, 1987).
With turn-taking being perhaps the best-known
example, we now know a fair amount about some
aspects of communication dynamics, but much
less about others. However, recent advances in
machine learning and experimental methods, and
recent findings from a variety of perspectives, in-
cluding conversation analysis, social signal pro-
cessing, adaptation, corpus analysis and model-
ing, perceptual experiments, and dialog systems-
building and experimentation, mean that the time
is ripe to start working towards more comprehen-
sive predictive models.
The study of multimodal prediction models
bring a new series of research challenges:
MULTIMODAL ASYNCHRONY While speech
and gestures seem to come from a single under-
lying mental process (McNeill, 1992), they not
always happen at the same time, making it hard
for earlier multimodal fusion approaches based
on synchrony. A multimodal prediction model
needs to be able to learn automatically the tempo-
ral relationship (and relative importance) between
modalities.
MICRO DYNAMICS The dynamic between mul-
timodal signals should be taken at a micro level
since many of the interactions between speech and
gesture happen at the sub-gesture level or sub-
word level (Quek, 2003). Typical word-based
sampling may not be sufficient and instead a
higher sampling rate should be used.
LIMITED ANNOTATED DATA Given the time re-
quirement to correctly annotate multimodal data,
Figure 2: Latent Mixture of Discriminative Ex-
perts: a new dynamic model for multimodal fu-
sion. In this graphical model, xj represents the
jth multimodal observation, hj is a hidden state
assigned to xj , and yj the class label of xj . Gray
circles are latent variables. The micro dynamics
and multimodal temporal relationships are auto-
matically learned by the hidden states hj during
the learning phase.
most multimodal datasets contain only a limited
number of labeled examples. Since many ma-
chine learning algorithms rely on a large training
corpus, effective training of a predictive model on
multimodal datasets is challenging.
4 Latent Mixture of Discriminative
Experts
In this paper we present a multimodal fusion al-
gorithm, called Latent Mixture of Discriminative
Experts (shown in Figure 2), that addresses the
three challenges discussed in the previous section.
The hidden states of LMDE automatically learn
the temporal asynchrony between modalities. By
using a constant sample rate of 30Hz in our ex-
periments, we can model the micro dynamics of
speech and prosody (e.g., change of intonation
in the middle of a word). And finally, by train-
ing separate experts for each modalities, we im-
prove the prediction performance even with lim-
ited datasets.
The task of our LMDE model is to learn a map-
ping between a sequence of multimodal observa-
tions x = {x1, x2, ..., xm} and a sequence of la-
bels y = {y1, y2, ..., ym}. Each yj is a class la-
bel for the jth frame of a video sequence and is a
member of a set Y of possible class labels, for ex-
ample, Y = {head-nod,other-gesture}.
862
Each frame observation xj is represented by a fea-
ture vector ?(xj) ? Rd, for example, the prosodic
features at each sample. For each sequence, we
also assume a vector of ?sub-structure? variables
h = {h1, h2, ..., hm}. These variables are not ob-
served in the training examples and will therefore
form a set of hidden variables in the model.
Following Morency et al (2007), we define our
LMDE model as follows:
P (y | x, ?) = ?
h
P (y | h, x, ?)P (h | x, ?) (1)
where ? is the model parameters that is to be esti-
mated from training data.
To keep training and inference tractable,
Morency et al (2007) restrict the model to have
disjoint sets of hidden states associated with each
class label. Each hj is a member of a set Hyj
of possible hidden states for the class label yj .
H, the set of all possible hidden states, is defined
to be the union of all Hy sets. Since sequences
which have any hj /? Hyj will by definition have
P (y | h, x, ?) = 0, latent conditional model be-
comes:
P (y | x, ?) = ?
h:?hj?Hyj
P (h | x, ?). (2)
What differentiates our LMDE model from the
original work of Morency et al is the definition of
P (h|x, ?):
P (h| x, ?) =
exp
( ?
l ?l ? Tl(h, x)+?
? ?? ? P?(y|x, ??)
)
Z(x, ?) ,
(3)
whereZ is the partition function and P?(y|x) is
the conditional distribution of the expert indexed
by ?. The expert conditional distributions are de-
fined P?(y|x, ??) using the usual conditional ran-
dom field formulation:
P?(y| x, ??) = exp (
?
k ??,k ? F?,k(y, x))
Z?(x, ??) , (4)
F?,k is defined as
F?,k(y, x) =
m?
j=1
f?,k(yj?1, yj , x, j),
and each feature function f?,k(yj?1, yj , x, j) is
either a state function sk(yj , x, j) or a transition
function tk(yj?1, yj , x, j). State functions sk de-
pend on a single hidden variable in the model
while transition functions tk can depend on pairs
of hidden variables. Tl(h, x), defined in Equa-
tion 3, is a special case, summing only over
the transition feature functions tl(hl?1, hl, x, l).
Each expert ? contains a different subset of
f?,k(yj?1, yj , x, j). These feature functions are
defined in Section 5.2.
4.1 Learning Model Parameters
Given a training set consisting of n labeled se-
quences (xi,yi) for i = 1...n, training is done in
a two step process. First each expert ? is trained
following (Kumar and Herbert., 2003; Lafferty et
al., 2001) objective function to learn the parame-
ter ???:
L(??) =
n?
i=1
logP?(yi | xi, ??)? 12?2 ||??||
2
(5)
The first term in Eq. 5 is the conditional log-
likelihood of the training data. The second term
is the log of a Gaussian prior with variance ?2,
i.e., P (??) ? exp
( 1
2?2 ||??||2
).
Then the marginal probabilities P?(yj =
a | y, x, ???), are computed using belief prop-
agation and used as input for Equation 3. The
optimal parameter ?? was learned using the log-
likelyhood of the conditional probability defined
in Equation 2 (i.e., no regularization).
4.2 Inference
For testing, given a new test sequence x, we want
to estimate the most probable sequence of labels
y? that maximizes our LMDE model:
y? = argmaxy
?
h:?hi?Hyi
P (h | x, ??) (6)
5 Experimental Setup
We evaluate our Latent Mixture of Discrimina-
tive Experts on the multimodal task of predicting
listener nonverbal backchannel (i.e., head nods).
Backchannel feedback (the nods and paraverbals
such as ?uh-hu? and ?mm-hmm? that listeners
863
produce as some is speaking) has received con-
siderable interest due to its pervasiveness across
languages and conversational contexts.
5.1 Dataset
We are using the RAPPORT dataset from (Maat-
man et al, 2005), which contains 47 dyadic inter-
actions between a speaker and a listener. Data is
drawn from a study of face-to-face narrative dis-
course (?quasi-monologic? storytelling). In this
dataset, participants in groups of two were told
they were participating in a study to evaluate a
communicative technology. Subjects were ran-
domly assigned the role of speaker and listener.
The speaker viewed a short segment of a video
clip taken from the Edge Training Systems, Inc.
Sexual Harassment Awareness video. After the
speaker finished viewing the video, the listener
was led back into the computer room, where the
speaker was instructed to retell the stories por-
trayed in the clips to the listener. The listener
was asked to not talk during the story retelling.
Elicited stories were approximately two minutes
in length on average. Participants sat approxi-
mately 8 feet apart. Video sequences were manu-
ally annotated to determine the ground truth head
nod labels. A total of 587 head nods occured over
all video sequences.
5.2 Multimodal Features
This section describes the different multimodal
features used to create our five experts.
PROSODY Prosody refers to the rhythm, pitch and
intonation of speech. Several studies have demon-
strated that listener feedback is correlated with a
speaker?s prosody (Nishimura et al, 2007; Ward
and Tsukahara, 2000; Cathcart et al, 2003). For
example, Ward and Tsukahara (2000) show that
short listener backchannels (listener utterances
like ?ok? or ?uh-huh? given during a speaker?s ut-
terance) are associated with a lowering of pitch
over some interval. Listener feedback often fol-
lows speaker pauses or filled pauses such as
?um? (see (Cathcart et al, 2003)). Using openS-
MILE (Eyben et al, 2009) toolbox, we extract the
following prosodic features, including standard
linguistic annotations and the prosodic features
suggested by Ward and Tsukhara: downslopes in
pitch continuing for at least 40ms, regions of pitch
lower than the 26th percentile continuing for at
least 110ms (i.e., lowness), drop or rise in energy
of speech (i.e., energy edge), Fast drop or rise in
energy of speech (i.e., energy fast edge), vowel
volume (i.e., vowels are usually spoken softer)
and Pause in speech (i.e., no speech).
VISUAL GESTURES Gestures performed by the
speaker are often correlated with listener feed-
back (Burgoon et al, 1995). Eye gaze, in particu-
lar, has often been implicated as eliciting listener
feedback. Thus, we manually annotate the follow-
ing contextual feature: speaker looking at the lis-
tener.
LEXICAL Some studies have suggested an asso-
ciation between lexical features and listener feed-
back (Cathcart et al, 2003). Using the transcrip-
tions, we included all individual words (i.e., uni-
grams) spoken by the speaker during the interac-
tions.
SYNTACTIC STRUCTURE Finally, we attempt
to capture syntactic information that may pro-
vide relevant cues by extracting four types of fea-
tures from a syntactic dependency structure cor-
responding to the utterance. The syntactic struc-
ture is produced automatically using a CRF part-
of-speech (POS) tagger and a data-driven left-to-
right shift-reduce dependency parser (Sagae and
Tsujii, 2007), both trained on POS tags and de-
pendency trees extracted from the Switchboard
section of the Penn Treebank (Marcus et al,
1994), converted to dependency trees using the
Penn2Malt tool1. The four syntactic features are:
? Part-of-speech tags for each word (e.g. noun,
verb, etc.), taken from the output of the POS
tagger
? Grammatical function for each word (e.g.
subject, object, etc.), taken directly from the
dependency labels produced by the parser
? Part-of-speech of the syntactic head of each
word, taken from the dependency links pro-
duced by the parser
? Distance and direction from each word to its
syntactic head, computed from the depen-
dency links produced by the parser
1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
864
Figure 3: Baseline Models: a) Conditional Random Fields (CRF), b) Latent Dynamic Conditional
Random Fields(LDCRF), c) CRF Mixture of Experts (no latent variable)
Although our current method for extracting
these features requires that the entire utterance
be available for processing, this provides us with
a first step towards integrating information about
syntactic structure in multimodal prediction mod-
els. Many of these features could in principle be
computed incrementally with only a slight degra-
dation in accuracy, with the exception of features
that require dependency links where a word?s syn-
tactic head is to the right of the word itself. We
leave an investigation that examines only syntac-
tic features that can be produced incrementally in
real time as future work.
5.3 Baseline Models
INDIVIDUAL EXPERTS Our first baseline model
consists of a set of CRF chain models, each
trained with different set of multimodel features
(as described in the previous section). In other
words, only visual, prosodic, lexical or syntactic
features are used to train a single CRF expert. In
one CRF chain model, each gesture class corre-
sponds to a state label. (See Figure 3a).
MULTIMODAL CLASSIFIERS (EARLY FUSION)
Our second baseline consists of two models: CRF
and LDCRF (Morency et al, 2007). To train these
models, we concatenate all multimodal features
(lexical, syntactic, prosodic and visual) in one in-
put vector. Graphical representation of these base-
line models are given in Figure 3.
CRF MIXTURE OF EXPERTS To show the im-
portance of latent variable in our LMDE model,
we trained a CRF-based mixture of discriminative
experts. This model is similar to the Logarithmic
Opinion Pool (LOP) CRF suggested by Smith et
al. (2005). The training is performed in two steps.
A graphical representation of a CRF Mixture of
experts is given in the last graph of Figure 3.
5.4 Methodology
We performed held-out testing by randomly se-
lecting a subset of 11 interactions (out of 47) for
the test set. The training set contains the remain-
ing 36 dyadic interactions. All models in this pa-
per were evaluated with the same training and test
sets. Validation of all model parameters (regular-
ization term and number of hidden states) was per-
formed using a 3-fold cross-validation strategy on
the training set. The regularization term was vali-
dated with values 10k, k = ?1..3. Three different
number of hidden states were tested for the LMDE
models: 2, 3 and 4.
The performance is measured by using the F-
measure. This is the weighted harmonic mean
of precision and recall. Precision is the proba-
bility that predicted backchannels correspond to
actual listener behavior. Recall is the probabil-
ity that a backchannel produced by a listener in
our test set was predicted by the model. We use
the same weight for both precision and recall, so-
called F1. During validation we find all the peaks
(i.e., local maxima) from the marginal probabil-
ities. These backchannel hypotheses are filtered
using the optimal threshold from the validation
set. A backchannel (i.e., head nod) is predicted
correctly if a peak happens during an actual lis-
tener backchannel with high enough probability.
The same evaluation measurement is applied to all
models.
The training of all CRFs and LDCRFs were
done using the hCRF library2. The LMDE model
was implemented in Matlab3 based on the hCRF
2http://sourceforge.net/projects/hrcf/
3The source code is available at:
http://projects.ict.usc.edu/multicomp/.
865
Table 1: Comparison of individual experts with
our Latent Mixture of Discriminative Experts
(LMDE).
Expert Precision Recall f1
Lexical 0.1647 0.3305 0.2198
Prosody 0.1396 0.9112 0.2421
Syntactic 0.1833 0.4663 0.2632
POS 0.1935 0.4514 0.2709
Eye Gaze 0.1573 0.1741 0.1653
LMDE 0.2295 0.5677 0.3268
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Recall
Pre
cis
ion
Latent Mixture
Lexical
Prosody
Syntactic
POS
EyeGaze
Figure 4: Comparison of individual experts with
our LMDE model.
library.
6 Results and Discussion
In this section we present the results of our empiri-
cal evaluation designed to test the three main char-
acteristics of the LMDE model: (1) integration of
multiple sources of information, (2) late fusion ap-
proach and (3) latent variable which models the
hidden dynamic between experts. We also present
an analysis of the output probabilities from the
LMDE model and individual experts.
INDIVIDUAL EXPERTS We trained one individ-
ual expert for each feature types: visual, prosodic,
lexical and syntactic features (both part-of speech
and syntactic structure). Precision, recall and F1
values for each individual expert and our LMDE
model are shown in Table 1 and Figure 4.
Pairwise two-tailed t-test comparison between
our LMDE model and individual experts shows a
Table 2: Comparison of our Latent Mixture of
Discriminative Experts (LMDE) with two early
fusion technique (CRF vs LDCRF) and the CRF
Mixture of Experts (Smith et al, 2005).
model Precision Recall f1
LMDE 0.2295 0.5677 0.3268
Early CRF 0.13958 0.9245 0.2425
Early LDCRF 0.1826 0.2484 0.2105
Mixture CRF 0.1502 0.2712 0.1934
significant difference for Lexical, Prosody, Syn-
tactic and Eye gaze, with respective p-values of
0.0037, 0.0379, 0.0400 and 0.0233. Even though
some experts may not perform well individually
(e.g., eye gaze), they can bring important informa-
tion once merged with others. Table 1 shows that
our LMDE model was able to take advantage of
the complementary information from each expert.
LATE FUSION We compare our approach with
two early fusion models: CRF and Latent-
dynamic CRF (see Figure 3). Table 2 summarizes
the results. The CRF model learns direct weights
between input features and the gesture labels. The
LDCRF is able to model more complex dynam-
ics between input features with the latent variable.
We can see that our LMDE model outperforms
both early fusion approaches because of its late
fusion approach. Pairwise two-tailed t-test analy-
sis gives p-values of 0.0481 and 0.0748, for CRF
and LDCRF respectively.
LATENT VARIABLE The CRF Mixture of Ex-
perts (2005) directly merges the expert outputs
while our model uses a latent variable to model the
hidden dynamic between experts (see Figure 3).
Table 2 summarizes the results. Pairwise two-
tailed t-test comparison between these two mod-
els shows a significant difference with a p-value
of 0.0062. This result is important since it shows
that our LMDE model does learn the hidden inter-
action between experts.
MODEL ANALYSIS To understand the multi-
modal integration which happens at the latent
variable level in our LMDE model, Figure 5
shows the output probabilities for all five individ-
ual experts as well as our model. The strength of
the latent variable is to enable different weigting
866
Speak
er inp
ut fea
tures
Exper
ts
Laten
t
Mixtu
re Ground truth labels
Word
s
Gaze ...
Time
(a) (b)
5.6s 7.7s 10.3s
...
15.6s 17.0s 18.7s 20.5s
Figure 5: Output probabilities from LMDE and individual experts for two different sub-sequences. The
gray areas in the graph corresponds to ground truth backchannel feedbacks of the listener.
of the experts at different point in time.
By analyzing the sequence (a), we observe that
both the POS and Syntactic experts learned that
when no words are present (i.e., pause) there is
a high likelihood of backchennel feedback from
the listener (shown at 5.6s and 10.3s). These two
experts are highly weighted (by one of the hid-
den state) during this part of the sequence. Also,
both the Lexical and POS experts learned that the
word ??that?? (and its part-of-speech) are impor-
tant but since the speaker is not looking at the
listener when saying it, the output from LMDE
model is low (see Figure 5, Sequence (a), 7.7s).
By analyzing sequence (b), we see that the Lex-
ical and POS experts learned the importance of the
??and?? at 15.6s and 20.5s. More importantly, we
can see at 17.0s and 18.7s that the influence of
the POS and Syntactic experts have been reduced
in the LMDE output probability. This difference
of weighting shows that a different hidden state is
active during Sequence (b).
7 Conclusion
In this paper we introduced a new model
called Latent Mixture of Discriminative Experts
(LMDE) for learning predictive models of human
communication behaviors. Many of the interac-
tions between speech and gesture happen at the
sub-gesture or sub-word level. LMDE learns au-
tomatically the temporal relationship between dif-
ferent modalities. Since, we train separate experts
for each modality, LMDE is capable of improv-
ing the prediction performance even with limited
datasets.
We evaluated our model on the task of non-
verbal feedback prediction (e.g., head nod). Our
experiments confirm the importance of combin-
ing the four types of multimodal features: lexical,
syntactic structure, eye gaze, and prosody. LMDE
is a generic model that can be applied to a wide
range of problems. As future work, we are plan-
ning to test our model on dialog act classification
and multimodal behavior recognition tasks.
Acknowledgements
This material is based upon work supported by
the National Science Foundation under Grant No.
0917321 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDECOM).
The content does not necessarily reflect the posi-
tion or the policy of the Government, and no offi-
cial endorsement should be inferred.
References
Anderson, H., M. Bader, E.G. Bard, G. Doherty, S. Garrod,
S. Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo,
867
H. Thompson, and R. Weinert. 1991. The mcrc map task
corpus. Language and Speech, 34(4):351?366.
Bavelas, J.B., L. Coates, and T. Johnson. 2000. Listeners as
co-narrators. JPSP, 79(6):941?952.
Blunsom, P., T. Cohn, and M. Osborne. 2008. A discrimi-
native latent variable model for statistical machine trans-
lation. In ACL: HLT, pages 200?208.
Burgoon, Judee K., Lesa A. Stern, and Leesa Dillman. 1995.
Interpersonal adaptation: Dyadic interaction patterns.
Cambridge University Press, Cambridge.
Cassell, J. and M. Stone. 1999. Living hand to mouth: Psy-
chological theories about speech and gesture in interactive
dialogue systems. In AAAI.
Cathcart, N., Jean Carletta, and Ewan Klein. 2003. A shal-
low model of backchannel continuers in spoken dialogue.
In EACL, pages 51?58.
Eisenstein, J., R. Barzilay, and R. Davis. 2008. Gestural co-
hesion for topic segmentation. In ACL: HLT, pages 852?
860.
Eisentein, J. and R. Davis. 2007. Conditional modality fu-
sion for coreference. In ACL, pages 352?359.
Eyben, Florian, Martin Wo?llmer, and Bjo?rn Schuller. 2009.
openEAR - Introducing the Munich Open-Source Emo-
tion and Affect Recognition Toolkit. In ACII, pages 576?
581.
Frampton, M., J. Huang, T. Bui, and S. Peters. 2009.
Real-time decision detection in multi-party dialogue. In
EMNLP, pages 1133?1141.
Fuchs, D. 1987. Examiner familiarity effects on test perfor-
mance: implications for training and practice. Topics in
Early Childhood Special Education, 7:90?104.
Fujie, Shinya, Yasuhi Ejiri, Kei Nakajima, Yosuke Mat-
susaka, and Tetsunori Kobayashi. 2004. A conversation
robot using head gesture recognition as para-linguistic in-
formation. In RO-MAN, pages 159?164.
Goldberg, S.B. 2005. The secrets of successful mediators.
Negotiation Journal, 21(3):365?376.
Gravano, A., S. Benus, H. Chavez, J. Hirschberg, and
L. Wilcox. 2007. On the role of context and prosody
in the interpretation and ?okay?. In ACL, pages 800?807.
Heylen, D. and R. op den Akker. 2007. Computing
backchannel distributions in multi-party conversations. In
ACL:EmbodiedNLP, pages 17?24.
Johnston, M. 1998. Multimodal language processing. In
ICSLP.
Jovanovic, N., R. op den Akker, and A. Nijholt. 2006.
Adressee identification in face-to-face meetings. In
EACL.
Jurafsky, D., E. Shriberg, B. Fox, and T. Curl. 1998. Lexical,
prosodic and syntactic cures for dialog acts. In Workshop
on Discourse Relations, pages 114?120.
Kendon, A. 2004. Gesture: Visible Action as Utterance.
Cambridge University Press.
Kumar, S. and M. Herbert. 2003. Discriminative random
fields: A framework for contextual interaction in classifi-
cation. In ICCV.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditional
random fields: probabilistic models for segmenting and
labelling sequence data. In ICML.
Maatman, M., J. Gratch, and S. Marsella. 2005. Natural
behavior of a listening agent. In IVA.
Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The penn treebank: anno-
tating predicate argument structure. In ACL:HLT, pages
114?119.
McNeill, D. 1992. Hand and Mind: What Gestures Reveal
about Thought. Univ. Chicago Press.
Moore, P.-Y. Hsueh J. 2007. What decisions have you made:
Automatic decision detection in conversational speech. In
NAACL-HLT, pages 25?32.
Morency, Louis-Philippe, Ariadna Quattoni, and Trevor Dar-
rell. 2007. Latent-dynamic discriminative models for
continuous gesture recognition. In CVPR.
Murray, G. and G. Carenini. 2009. Predicting subjectivity in
multimodal conversations. In EMNLP, pages 1348?1357.
Nakano, Reinstein, Stocky, and Justine Cassell. 2003. To-
wards a model of face-to-face grounding. In ACL.
Nakano, Y., K. Murata, M. Enomoto, Y. Arimoto, Y. Asa,
and H. Sagawa. 2007. Predicting evidence of understand-
ing by monitoring user?s task manipulation in multimodal
conversations. In ACL, pages 121?124.
Nishimura, Ryota, Norihide Kitaoka, and Seiichi Nakagawa.
2007. A spoken dialog system for chat-like conversations
considering response timing. LNCS, 4629:599?606.
Oviatt, S. 1999. Ten myths of multimodal interaction. Com-
munications of the ACM.
Quek, F. 2003. The catchment feature model for multimodal
language analysis. In ICCV.
Sagae, Kenji and Jun?ichi Tsujii. 2007. Dependency parsing
and domain adaptation with LRmodels and parser ensem-
bles. In ACL, pages 1044?1050.
Smith, A., T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL,
pages 18?25.
Ward, N. and W. Tsukahara. 2000. Prosodic features which
cue back-channel responses in english and japanese.
Journal of Pragmatics, 23:1177?1207.
868
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2151?2160, Dublin, Ireland, August 23-29 2014.
Data-driven Measurement of Child Language Development
with Simple Syntactic Templates
Shannon Lubetich
Pomona College
Claremont, CA 91711
shannon.lubetich@pomona.edu
Kenji Sagae
Institute for Creative Technologies
University of Southern California
Los Angeles, CA 90089
sagae@ict.usc.edu
Abstract
When assessing child language development, researchers have traditionally had to choose be-
tween easily computable metrics focused on superficial aspects of language, and more expressive
metrics that are carefully designed to cover specific syntactic structures and require substantial
and tedious labor. Recent work has shown that existing expressive metrics for child language
development can be automated and produce accurate results. We go a step further and pro-
pose that measurement of syntactic development can be performed automatically in a completely
data-driven way without the need for definition of language-specific inventories of grammatical
structures. As a crucial step in that direction, we show that four simple feature templates are
as expressive of language development as a carefully crafted standard inventory of grammatical
structures that is commonly used and has been validated empirically.
1 Introduction
Although child language has been the focus of much study, our understanding of first language acquisi-
tion is still limited. In attempts to measure child language development over time, several metrics have
been proposed. The most commonly used metric is Mean Length of Utterance, or MLU (Brown, 1973),
which is based on the number of morphemes per utterance. The main appeal of MLU is that it can be
easily computed automatically, given machine-readable transcripts. Although MLU values may not be
meaningful across languages, the general approach is suitable for analysis within different languages.
However, MLU?s ability to track language development from age four has been questioned (Klee and
Fitzgerald, 1985; Scarborough, 1990), and its usefulness is still the subject of debate (Rice et al., 2010).
Several metrics based on the usage of grammatical structure have been proposed as more sensitive to
changes in language over a wider range of ages (Scarborough, 1990; Lee and Canter, 1971; Fletcher and
Garman, 1988). These metrics continue to show score increases where MLU plateaus, but their increased
expressivity is typically associated with two severe drawbacks. The first is that their use for computa-
tion of language development scores involves identification of several specific grammatical structures in
child language transcripts, a process that requires linguistic expertise and is both time-consuming and
error-prone. This issue has been addressed by recent work that shows that current natural language pro-
cessing techniques can be applied to automate the computation of these metrics, removing the bottleneck
of manual labor (Sagae et al., 2005; Roark et al., 2007; Sahakian and Snyder, 2012). The second draw-
back is that these measures are language-specific, and development of a measure for a specific language
requires deep expertise and careful design of an inventory of grammatical structures that researchers be-
lieve to be indicative of language development. Going beyond previous work, which addressed the first
drawback of traditional metrics for child language development, we address the second, paving the way
for a language-independent methodology for tracking child language development that is as expressive
as current language-specific alternatives, but without the need for carefully constructed inventories of
grammatical structures.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2151
The specific hypothesis we address in this paper is whether a fully-data driven approach that uses only
a few simple feature templates applied to syntactic dependency trees can capture the same information as
the well-known Index of Productive Syntax, or IPSyn (Scarborough, 1990). In contrast to previous work
that showed that the computation of IPSyn scores can be performed automatically by encoding each of
the 60 language structures in a language-specific inventory (e.g. wh-questions with auxiliary inversion,
propositional complements, conjoined sentences) as complex patterns over parse trees, we propose that
child language development can instead be measured automatically in a way that is fully data-driven and
can be applied to many languages for which accurate dependency parsers are available, without relying
on carefully constructed lists of grammatical structures or complex syntactic patterns in each language.
Specifically, we examine two hypotheses: (1) counts of features extracted from syntactic parse trees
using only simple templates are at least as expressive of changes in language development as the Index
of Productive Syntax (Scarborough, 1990), an empirically validated metric based on an inventory of
grammatical structures derived from the child language literature; and (2) these parse tree features can
be used to model language development without the use of an inventory of specific structures, assuming
only the knowledge that in typically developing children the level of language development is correlated
with age. We emphasize that the goal of this work is not to develop yet one more way to compute IPSyn
scores automatically, but to show empirically that lists of grammatical structures such as those used to
compute IPSyn are not essential to measure syntactic development in children.
In this paper, we start by reviewing IPSyn and previous work on automatic IPSyn scoring based on
manually crafted syntactic patterns in section 2. Using a similar approach, we validate the language
development curves observed by Scarborough (1990) in the original IPSyn study. In section 3 we show
how IPSyn scores can be computed in an entirely different, fully data-driven way, using a support vector
regression. In section 4 we examine how this data-driven framework can be used to track language
development in the absence of a metric such as IPSyn, which allows for application of this approach to
languages other than English. We discuss related work in section 5, and conclude in section 6.
2 Index of Productive Syntax (IPSyn)
The Index of Productive Syntax (Scarborough, 1990) evaluates a child?s linguistic development by ana-
lyzing a transcript of utterances and awarding points when certain syntactic and morphological structures
are encountered. The end result is a number score ranging from 0 to 120, with a higher score correspond-
ing to the presence of more complex grammatical structures, and thus further linguistic development.
IPSyn was designed to be more sensitive to language changes after age 3 than the more common Mean
Length of Utterance (MLU) (Brown, 1973), which fails to account for the fact that children?s speech
increases in complexity even after utterances stop increasing in length.
IPSyn scores are calculated by analyzing a transcript of 100 utterances of a child?s speech, and award-
ing points to specific language structures encountered. There are 60 forms in total from four categories
of noun phrases, verb phrases, questions and negations, and sentence structures. Each form is awarded
0 points if not encountered, 1 point if found once in a transcript, and 2 points if found at least twice.
This sums to a total ranging between 0 and 120 points. Scarborough (1990) motivates the use of this
specific inventory of 60 forms by stating that they ?have been shown to occur in preschool language
production in innumerable studies of language acquisition during the past 25 years,? highlighting that
the task of generating such an inventory and performing empirical validation for additional languages
requires considerable expertise and is far from trivial.
2.1 Automating IPSyn
In support of empirical testing of our first hypothesis?that features extracted from parse trees using only
simple feature templates are as expressive of child language development as the carefully constructed
inventory of grammatical structures in IPSyn?we first implemented an automated version of IPSyn fol-
lowing Sagae et al. (2005), who showed that this task can be performed nearly at the level of trained
human experts. This allows us to generate IPSyn scores for a large set of child language transcripts. Our
implementation differs from previous work mainly in that it uses only the tools provided in the CLAN
2152
software suite (MacWhinney, 2000), which were designed specifically for analysis of child language
transcripts, instead of the Charniak (2000) parser, which was used by Sagae et al. and later by Hassanali
et al. (2014) in a more recent implementation of the same general approach.
We evaluated our implementation using the set of 20 manually scored transcripts described by Sagae
et al. as Set A, and subsequently used to evaluate the implementation of Hassanali et al. Three transcripts
were used as development data, following Sagae et al. The mean absolute difference between manually
generated and automatically generated scores was 3.6, which is very similar to what has been reported
by Hassanali et al. and by Sagae et al. (3.05 and 3.7, respectively) for the same set of transcripts.
Given the possible score differences in manual scoring reported by Scarborough (1990) and the small
number of transcripts used for testing, the differences observed among the automatic systems are not
meaningful. In fact, in examining our development data, we found multiple errors in the manual coding,
causing point discrepancies when our system produced correct results. This highlights the difficulty of
performing this scoring task manually, and raises the question of whether automatic scoring has in fact
surpassed the reliability of manual scoring. That three different implementations of IPSyn appear to
perform comparably suggests this might be the case. We leave an empirical investigation of this question
to future work.
3 From automatic IPSyn to data-driven IPSyn
The fully automatic way of computing IPSyn scores described above in section 2.1, paired with a suffi-
ciently large amount of child language transcript data, gives us a way to test the hypothesis mentioned in
the beginning of section 2.1, that simple features of parse trees are as expressive as the hand-crafted IPSyn
language structure inventory. We did this by first creating several 100-utterance transcripts from existing
child language transcripts, then automatically assigning them IPSyn scores, and using these scores as
targets to be learned from features extracted from the corresponding 100-utterance transcripts. Details of
the data and learning approach used for this experiment, as well as empirical results, are described in the
remainder of this section.
3.1 Generating IPSyn data
To obtain enough child language transcripts in a wide range of ages to test our hypothesis, we turned
to the CHILDES database. To generate training and development sets for our experiments, we used
transcripts from CHILDES of 14 different children with ages ranging from 1 year 1 month to 8 years.
Because each application of IPSyn requires only 100 child utterances, transcripts were split, producing
a total of 593 transcripts, each containing 100 utterances. The 14 children in our dataset came from the
following CHILDES corpora: Brown, MacWhinney, Sachs and Warren. The reason for choosing these
corpora is that they were quickly identified as containing spontaneous natural interactions, as opposed
to reading or specific games and activities designed to elicit a certain kind of language production. It is
likely that other corpora in CHILDES would also suit our purposes, but the data in these four corpora was
sufficient for our experiments. Each of the 593 transcripts was assigned an IPSyn score automatically.
From the Brown, MacWhinney and Sachs corpora, we used transcripts from a total of four children
(Adam from Brown, Mark and Ross from MacWhinney, and Naomi from Sachs), from whom language
data was collected over several years. Transcripts from these three corpora, 572 in total, served as our
training set. The Warren corpus includes data from ten children with ages ranging from 1;6 to 6;2 (that
is, 1 year and 6 months to 6 years and 2 months, using the commonly accepted age notation for this type
of data), from which we created 21 transcripts that served as our development set.
The complete set of 593 transcripts with IPSyn scores gives us the opportunity to verify whether the
language development curves observed by Scarborough (1990) averaged over 75 transcripts in the orig-
inal IPSyn study matches curves produced from averaging results from 593 transcripts from entirely
different subjects. Figure 1 shows a side-by-side comparison between the original figure from (Scarbor-
ough, 1990) and a corresponding figure generated with our automatically scored transcripts. Although
not identical, the two figures are remarkably similar, reflecting that aspects of the emergence of grammar
in child language development are shared across children, and that IPSyn captures some of these aspects.
2153
(a) Original IPSyn study.
(b) Automatically generated.
Figure 1: Comparison between the IPSyn development curves for the four subscales in (a) the 75 tran-
scripts in the original IPSyn study (reproduced from (Scarborough, 1990)), and (b) our set of 593 tran-
scripts scored automatically.
Finally, we used the Garvey corpus to generate a test set. This corpus includes data from 48 different
children with ages ranging from 2;10 to 5;7, from which we extracted 60 transcripts covering all 48
children and the full range of ages in the corpus. No data from the 48 children in the Garvey corpus,
which we used as a test set, were used for training or development of the models used in our experiments.
3.2 A regression model for IPSyn
Given 593 pairs of transcript and IPSyn score, we approached the task of learning a data-driven model
for IPSyn scoring as one of regression. For each transcript, a set of features is extracted, and the IPSyn
score is associated with that feature vector. The features extracted from the transcripts followed four
templates, described in the next subsection. If an accurate function for predicting IPSyn scores from
these feature vectors can be learned, our hypothesis that these features are at least expressive enough to
track child language development as well as the inventory of IPSyn structures is confirmed. To learn our
model, we used the SVM Light
1
implementation of support vector regression (Drucker et al., 1997).
3.3 Features
An important step in learning a regression model for IPSyn is choosing what features to use. To support
our goal of language independence, we decided not to consider language specific features that have been
shown to be useful in this task but are language dependent
2
, and opted instead to see whether the use of
only simple parse tree features would be sufficient. The only prerequisite for extraction of our feature set
is that each transcript must be parsed to produce a syntactic dependency tree. We used the CLAN tools
for morphology analysis (MOR), part-of-speech tagging (POST) and parsing (MEGRASP)
3
, since it is
straightforward to process CHILDES transcripts using those, and they provide high-accuracy analyses
for child language transcripts. The accuracy of the MEGRASP dependency parser for child utterances in
English is estimated to be close to 93% (Sagae et al., 2010).
All of the features used in our model are extracted from parse trees according to four simple classes
that target the following information:
1
http://svmlight.joachims.org/
2
This is in contrast to, for example, the related work of Sahakian and Snyder (2012), which we discuss in section 5.
3
Models for MOR and POST are available for a wide variety of languages. Models for MEGRASP are available only for
English and Japanese, but our data-driven approach is not tied to any specific tagger or parser.
2154
n:prop v n n
Eve eat dust mop.
SUBJ
OBJ
MOD
Figure 2: A dependency tree generated with part-of-speech and grammatical relation information.
Part-of-speech tags: The first type of feature we used is simply the part-of-speech tag of each word. This
can be thought of as a bag of part-of-speech tags. We intentionally avoided the commonly used bag-
of-words, because our goal is to obtain a model that tracks changes in syntax structure, not content.
Although it is highly likely that lexical features would be very informative in this learning task, they
would be useful for the wrong reason: our model is intended to target the emergence of syntax, and
not what children talk about at different ages. We note, however, that as with the Penn Treebank
tagset, the tags used by MOR and POST also reflect morphology, so that information is accounted
for. The full tag set is listed in (MacWhinney, 2000).
Grammatical relations: The second feature class in our model is a bag of dependency labels, where each
label correspond to a grammatical relation that holds between two words in the dependency tree (the
head word and the dependent word). The full set of grammatical relations is listed in (Sagae et al.,
2010).
Head-dependent part-of-speech pairs: Our third feature class is based on pairs of part-of-speech tags,
where each pair corresponds to a bilexical dependency relation in the parse tree, and one of the tags
comes from the head in the dependency, and the other tag comes from the dependent.
Head-relation-dependent triples: The last feature class is similar to the head-dependent pairs described
above, but also including the dependency label that indicates the grammatical relation that holds
between the head and dependent words. Features in this class are then triples composed of a head
part-of-speech tag, a dependent part-of-speech tag, and a dependency label.
As an example, given the parse tree shown in Figure 2, the following features would be extracted:
n:prop v n n
SUBJ OBJ MOD
v_n:prop v_n n_n
v_n:prop_SUBJ v_n_OBJ n_n_MOD
Features are extracted for every tree in each transcript. Because our goal is to measure grammatical
development in child language, these four feature templates were designed to capture the grammatical
relations represented in dependency trees, while leaving out the content reflected in specific lexical items.
While the content of what is said may be related to language development, our features are intended to
focus on syntactic information, covering exactly each of the labeled arcs and the part-of-speech tags in
a dependency tree (Figure 2) with the words removed. We also experimented with part-of-speech tag
bigrams (pairs of adjacent part-of-speech tags), and dependency chains formed by two dependency arcs.
The final choice of the four templates described above was based on results obtained on development
data.
3.4 Data-driven IPSyn evaluation
We trained a support vector regression model using our training set of 572 transcripts, using a polynomial
kernel and tuning the degree d and the regularization metaparameter C on the development set. While
the default C and d values resulted in a mean absolute error of 6.6 points in the score predictions in the
development set, setting C = 1 and d = 3 resulted in a mean absolute error of 4.1 on the development
set. We used these values for the rest of our experiments. The mean absolute error obtained on our
2155
test set of 48 children (60 transcripts) not used in training or tuning of the system was 3.9. When
applying our regression model to the manually scored set of 20 transcripts used by Sagae et al. (2005),
the mean absolute difference was 4.2 from the scores computed automatically using the approach in
section 2.1, and 5.4 from the manually computed scores, which we consider our gold standard target.
Compared to these manually computed scores, the absolute difference of 5.4 is higher than what we
obtained using carefully designed templates based on the IPSyn inventory, but still within the range of
variability expected for trained human scorers (Scarborough, 1990). It is important to keep in mind that
the goal of this experiment was not to improve on the accuracy of previous automatic scoring programs,
which work quite differently by listing manually crafted patterns over parse trees, but to show that a
scoring function can be learned in a data-driven way, without manually crafted patterns. The results
obtained with our regression model do confirm our hypothesis that simple features extracted from parse
trees are enough for tracking child language development in the same way as the much more complex
patterns included in IPSyn.
4 Age prediction
Given the ability of our data-driven approach to approximate IPSyn scores, confirming that a regression
approach with parse tree features is capable of capturing the progression of language development, we
now turn to the question of whether the same type of data-driven framework can be used to track child
language development without the need for a metric such as IPSyn.
Assuming only that language acquisition progresses monotonically over time, we can apply the same
data-driven regression approach to predict a child?s age given a language sample. This task was ap-
proached recently by Sahakian and Snyder (2012), who used an ensemble of existing metrics with a few
additional features. Unlike in our approach, Sahakian and Snyder do include lexical features and hand-
selected patterns in the form of an existing metric (D-level). They make the reasonable argument that
the task of age prediction is child-dependent, and that prediction across children would not make sense
due to individual variation in the rate of language development. Following Sahakian and Snyder, we first
approach age prediction as a child-specific task, but then discuss the application of our regression models
for other children than those used for training.
4.1 Child-specific age prediction
To determine whether our data-driven regression approach can model the development of individual
children at the level where accurate age predictions can be made, we used the same feature templates
described in section 3.3, but trained a regression model to predict age in months, rather than IPSyn scores.
Because this is a child-specific prediction task, we train separate regression models for each child. We
tested our age predictions using 10-fold cross-validation for three children from three different CHILDES
corpora (Adam from Brown, Ross from MacWhinney and Naomi from Sachs) for whom enough data
was available over a wide enough range of ages. In each case the regression approach performed well.
Table 1 shows the mean absolute error in months for each child, and the Pearson r for the correlation
between predicted age and actual age.
Child (corpus) Mean Abs Err Pearson (r)
Adam (Brown) 2.5 0.93
Ross (MacWhinney) 3.7 0.84
Naomi (Sachs) 3.1 0.91
Table 1: Regression results for single corpus age prediction (p < 0.0001 for all r values.)
Perhaps more interesting than the strong correlations between actual age and predicted age for each of
the individual corpora is a comparison of these correlations to correlations between age and MLU, and
age and IPSyn score. One main general criticism of MLU is that it fails to correlate well with age for
older children (around three to four years old). More detailed metrics such as IPSyn are believed to have
better correlation with age after that point. We do observe this situation in our data. Interestingly, our
2156
predicted age scores have much stronger correlations to actual age for older children, which suggests that
our regression approach with simple syntactic features is more expressive in tracking syntactic develop-
ment in older children than either MLU or IPSyn. This is shown in Table 2, which contains Pearson r
correlation coefficients for age and MLU, age and IPSyn, and age and predicted age using our regression
approach.
Child (corpus) MLU r IPSyn r Regression r
Adam (Brown) 0.37
?
0.53
?
0.85
?
Ross (MacW) 0.19 0.34
?
0.79
?
Naomi (Sachs) 0.27 0.52 0.82
?
Table 2: Pearson correlation coefficients between actual age and MLU, actual age and IPSyn score,
and actual age and predicted age, for children at least three years and four months old.
?
p < 0.0001.
?
p < 0.05.
The results shown in Table 2 confirm that features extracted from parse trees alone can offer sub-
stantially better prediction of age for individual children than MLU or even IPSyn scores. This is not
surprising, given that weights for these features are optimized to predict age using data from the specific
child and discriminative learning, but it does show that these features offer enough resolution to track
syntactic development in child language, confirming our second hypothesis.
4.2 Pilot experiment with Japanese language data
A great advantage of using a data-driven framework based on simple feature templates rather than a
traditional approach for measuring syntactic development with manually crafted lists of grammatical
structures is that the data-driven approach is, in principle, language-independent. The same features de-
scribed in section 2.1 could be extracted from dependency parse trees in any language, assuming only that
these dependency trees can be produced automatically. Syntactic dependency parsers and treebanks are
in fact available for a variety of languages (Buchholz and Marsi, 2006; Nivre et al., 2007). Although the
availability of treebanks that include child language samples is certainly desirable, it is not clear whether
it is strictly required in order to generate the syntactic structures used in our approach. While Sagae et
al. (2005) and Hassanali et al. (2014) obtained high levels of accuracy in IPSyn scoring using the Char-
niak (2000) parser with a model trained on the Wall Street Journal portion of the Penn Treebank (Marcus
et al., 1993), we have not verified the effects of parser errors in our data-driven approach. Of course, the
language independence claim applies only to the ability to measure syntactic development within differ-
ent languages, and direct numerical comparisons across languages are not meaningful, since the available
syntactic annotations for different languages follow different conventions and syntactic theories.
Although a full empirical validation of our regression approach in other languages is left as future
work, we performed a pilot experiment with a single Japanese child that suggests our findings may be
robust across languages. We used transcripts from the child Ryo, from the Miyata corpus of the Japanese
section of the CHILDES database
4
. We extracted 80 transcripts of 100 utterances each, covering ages
1;10 (22 months) to 3;0 (36 months). These transcripts were analyzed with the Japanese version of
the MEGRASP parser for CHILDES transcripts at an estimated accuracy of 93% (Miyata et al., 2013).
Using the exact same experimental settings and feature templates as for English, we performed a 10-fold
cross-validation for age prediction using the Japanese data. We obtained a strong correlation between
predicted age and actual age, with r = 0.82 (p < 0.0001). Although this value is slightly lower than the
values in Table 1 for English, the range of target values (age in months) is more compressed. Although
this experiment included only one child, it does suggest that our approach may work well for Japanese.
5 Related work
Within the literature on assessment of child language development, the metric most closely related to
our work is the Index of Productive Syntax (Scarborough, 1990), which we discussed in more detail in
4
http://childes.psy.cmu.edu/data/EastAsian/Japanese/Miyata/
2157
section 2, and used as a target for data-driven learning. Other traditional metrics include Developmental
Sentence Scoring (Lee and Canter, 1971), Language Assessment Remediation and Screening Proce-
dure (Fletcher and Garman, 1988), and D-level (Parisse and Le Normand, 1987) all of which share with
IPSyn the reliance on a hand-crafted inventory of grammatical structures meant to be identified manually
in transcribed child language samples.
Each of these metrics for child language development, along with the Mean Length of Utter-
ance (Brown, 1973), can be computed semi-automatically using the Computerized Profiling sys-
tem (Long et al., 2004). Although fully automatic computation with Computerized Profiling produces
levels of reliability lower than that of manual scoring, the system can be used with human intervention to
produce results of higher quality. More closely related is the work of Sagae et al. (2005) on automating
IPSyn using patterns extracted from automatic parse trees. The work we describe in section 2.1 is closely
based on that of Sagae et al., which we use as a way to validate our data-driven approach.
Roark et al. (2007) examined the ability of several automatically computed syntactic complexity met-
rics to discriminate between healthy and language impaired subjects. Among other metrics, Roark et al.
used Frazier scoring (Frazier, 1985) and Yngve scoring (Yngve, 1960), which are more commonly asso-
ciated with processing difficulty than with emergence of syntax in child language development, but are
related to our approach in that they are based on straightforward generic features of parse trees (depth,
node count), like our counts of grammatical relation labels. Finally, Sahakian and Snyder (Sahakian
and Snyder, 2012) have also approached the problem of learning automatic metrics for child language
development using a regression approach. Their focus, however, was on the combination of the existing
metrics MLU, mean depth of tree (similar to Yngve scoring mentioned above) and D-level, along with a
few hand-picked features (counts of certain closed-class words, ratio of function words to content words,
and average word frequency), to achieve better discrimination than any of these metrics or features alone.
A key difference between our approach and that of Sahakian and Snyder is that their approach builds on
and assumes the existence of a metric such as D-level, which, like IPSyn, includes a carefully designed
language-dependent inventory of language structures, while we use only simple feature templates applied
to parse trees. In addition, they include vocabulary-centric features, while we explicitly avoid vocabu-
lary features, focusing on structural features. It is possible that Sahakian and Snyder?s approach would
benefit from the parse tree features of our approach, either by using the features directly, or by taking a
score obtained by our approach as an additional feature in theirs.
6 Conclusion and future work
We presented a framework for assessment of syntactic development in child language that is com-
pletely data-driven, and unlike traditional metrics such as IPSyn, LARSP and D-level, does not rely
on a language-dependent inventory of language structures chosen specifically for the task. Instead, our
approach is based on the application of support vector regression with simple features extracted from
syntactic parse trees. In our experiments we used dependency parses produced by the MEGRASP parser
for CHILDES transcripts, but it is likely that other modern dependency and constituent parsers would
provide similar results. We showed that our framework is capable of learning IPSyn scores, and that for
individual children it can model syntactic development well after MLU and IPSyn scores fail to correlate
with age.
Having shown that the feature templates described in section 2.1 are as expressive as the inventory
of grammatical structures in IPSyn at tracking language development, and that syntactic development of
individual children can be modeled using our data-driven framework in complete absence of an existing
metric such as IPSyn, it is interesting to consider the applicability of this framework to different lan-
guages for which child language development metrics have not been developed or are not widely used.
One possible way to do this is to train several age regression models representing different development
profiles. In most practical scenarios, the child?s age is known and would not need to be predicted by a
model. By predicting age with several different models and selecting the one that most closely predicts
the child?s actual age, a language development profile matching the child can be found. This could be
used, for example, in diagnosis of language impairment. In this paper we established only the expressive
2158
power of regression using simple syntactic features, and the application of this approach to practical tasks
is left as an interesting direction for future work.
A related direction for future work is the application of this method for assessment of syntactic
development in languages other than English. Given the availability of child language data in vari-
ous languages (MacWhinney, 2000) and recent progress in syntactic analysis for many of these lan-
guages (Buchholz and Marsi, 2006; Nivre et al., 2007), we are optimistic about the applicability of our
approach to other languages. Preliminary results using data from one Japanese child suggest that the
same set of simple feature templates can be used to track language development in Japanese.
Acknowledgments
We thank the anonymous reviewers for insightful suggestions. This work was partly supported by the
National Science Foundation under grants 1263386 and 1 219253 and by the U.S. Army. Any opinion,
content or information presented does not necessarily reflect the position or the policy of the United
States Government, and no official endorsement should be inferred.
References
Roger Brown. 1973. A first language: The early stages. George Allen & Unwin.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149?164, New York
City, June. Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the first conference on North
American chapter of the Association for Computational Linguistics, pages 132?139, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Harris Drucker, Chris, Burges L. Kaufman, Alex Smola, and Vladimir Vapnik. 1997. Support vector regression
machines. In Advances in Neural Information Processing Systems 9, volume 9, pages 155?161.
Paul Fletcher and Michael Garman. 1988. LARSPing by numbers. British Journal of Disorders of Communica-
tion, 23(3):309?321.
L. Frazier. 1985. Syntactic complexity. In D. R. Dowty, L. Karttunen, and A. M. Zwicky, editors, Natural
Language Parsing: Psychological, Computational, and Theoretical Perspectives, pages 129?189. Cambridge
University Press, Cambridge.
Khairun-Nisa Hassanali, Yang Liu, Aquiles Iglesias, Thamar Solorio, and Christine Dollaghan. 2014. Auto-
matic generation of the index of productive syntax for child language transcripts. Behavior Research Methods,
46:254?262.
Thomas Klee and Martha Deitz Fitzgerald. 1985. The relation between grammatical development and mean length
of utterance in morphemes. Journal of Child Language, 12:251?269, 6.
Laura L. Lee and Susan M. Canter. 1971. Developmental sentence scoring: A clinical procedure for estimating
syntactic development in children?s spontaneous speech. Journal of Speech and Hearing Disorders, 36(3):315?
340.
Steven H. Long, Marc E. Fey, and Ron W. Channell. 2004. Computerized profiling (version 9.6.0).
Brian MacWhinney. 2000. The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum Associates, 3rd
edition edition.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
english: The penn treebank. Comput. Linguist., 19(2):313?330, June.
Susanne Miyata, Kenji Sagae, and Brian MacWhinney. 2013. The syntax parser GRASP for CHILDES (in
Japanese). Journal of Health and Medical Science, 3:45?62.
Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Republic, June. Association for Computational
Linguistics.
2159
Christophe Parisse and Marie-Thrse Le Normand. 1987. Indicators of linguistic competence in the peer group
conversational behavior of mildly retarded adults. Applied Psycholinguistics, 8:19?32.
Mabel L. Rice, Filip Smolik, Denise Perpich, Travis Thompson, Nathan Rytting, and Megan Blossom. 2010.
Mean length of utterance levels in 6-month intervals for children 3 to 9 years with and without language impair-
ments. Journal of Speech, Language, and Hearing Research, 53:1?17.
Brian Roark, Margaret Mitchell, and Kristy Hollingshead. 2007. Syntactic complexity measures for detecting
mild cognitive impairment. In Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and
Clinical Language Processing, BioNLP ?07, pages 1?8, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005. Automatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL
?05, pages 197?204, Stroudsburg, PA, USA. Association for Computational Linguistics.
Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhinney, and Shuly Wintner. 2010. Morphosyntactic annotation
of CHILDES transcripts. Journal of Child Language, 37:705?729.
Sam Sahakian and Benjamin Snyder. 2012. Automatically learning measures of child language development. In
ACL (2), pages 95?99. The Association for Computer Linguistics.
Hollis S. Scarborough. 1990. Index of productive syntax. Applied Psycholinguistics, 11:1?22, 3.
Victor H. Yngve. 1960. A model and an hypothesis for language structure. Proceedings of the American Philo-
sophical Society, 104:444?466.
2160
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 33?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interpretation of Partial Utterances in Virtual Human Dialogue Systems
Kenji Sagae and David DeVault and David R. Traum
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA 90292, USA
{sagae,devault,traum}@ict.usc.edu
Abstract
Dialogue systems typically follow a rigid pace
of interaction where the system waits until the
user has finished speaking before producing
a response. Interpreting user utterances be-
fore they are completed allows a system to
display more sophisticated conversational be-
havior, such as rapid turn-taking and appropri-
ate use of backchannels and interruptions. We
demonstrate a natural language understanding
approach for partial utterances, and its use in a
virtual human dialogue system that can often
complete a user?s utterances in real time.
1 Introduction
In a typical spoken dialogue system pipeline, the
results of automatic speech recognition (ASR) for
each user utterance are sent to modules that per-
form natural language understanding (NLU) and di-
alogue management only after the utterance is com-
plete. This results in a rigid and often unnatural pac-
ing where the system must wait until the user stops
speaking before trying to understand and react to
user input. To achieve more flexible turn-taking with
human users, for whom turn-taking and feedback at
the sub-utterance level is natural, the system needs
the ability to start interpretation of user utterances
before they are completed.
We demonstrate an implementation of techniques
we have developed for partial utterance understand-
ing in virtual human dialogue systems (Sagae et al,
2009; DeVault et al, 2009) with the goal of equip-
ping these systems with sophisticated conversational
behavior, such as interruptions and non-verbal feed-
back. Our demonstration highlights the understand-
ing of utterances before they are finished. It also
includes an utterance completion capability, where a
virtual human can make a strategic decision to dis-
play its understanding of an unfinished user utter-
ance by completing the utterance itself.
The work we demonstrate here is part of a grow-
ing research area in which new technical approaches
to incremental utterance processing are being de-
veloped (e.g. Schuler et al (2009), Kruijff et al
(2007)), new possible metrics for evaluating the per-
formance of incremental processing are being pro-
posed (e.g. Schlangen et al (2009)), and the ad-
vantages for dialogue system performance and us-
ability are starting to be empirically quantified (e.g.
Skantze and Schlangen (2009), Aist et al (2007)).
2 NLU for partial utterances
In previous work (Sagae et al, 2009), we presented
an approach for prediction of semantic content from
partial speech recognition hypotheses, looking at
length of the speech hypothesis as a general indi-
cator of semantic accuracy in understanding. In
subsequent work (DeVault et al, 2009), we incor-
porated additional features of real-time incremen-
tal interpretation to develop a more nuanced predic-
tion model that can accurately identify moments of
maximal understanding within individual spoken ut-
terances. This research was conducted in the con-
text of the SASO-EN virtual human dialogue sys-
tem (Traum et al, 2008), using a corpus of approxi-
mately 4,500 utterances from user sessions. The cor-
pus includes a recording of each original utterance, a
33
??
?
?
?
?
?
?
?
?
mood : declarative
sem :
?
?
?
?
?
?
?
?
type : event
agent : captain? kirk
event : deliver
theme : power ? generator
modal :
[
possibility : can
]
speech? act :
[
type : offer
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: AVM utterance representation.
manual transcription, and a gold-standard semantic
frame, allowing us to develop and evaluate a data-
driven NLU approach.
2.1 NLU in SASO-EN Virtual Humans
Our NLU module for the SASO-EN system,
mxNLU (Sagae et al, 2009), is based on maxi-
mum entropy classification (Berger et al, 1996) ,
where we treat entire individual semantic frames as
classes, and extract input features from ASR. The
NLU output representation is an attribute-value ma-
trix (AVM), where the attributes and values repre-
sent semantic information that is linked to a domain-
specific ontology and task model (Figure 1). The
AVMs are linearized, using a path-value notation, as
seen in the NLU input-output example below:
? Utterance (speech): we are prepared to give
you guys generators for electricity downtown
? ASR (NLU input): we up apparently give you
guys generators for a letter city don town
? Frame (NLU output):
<s>.mood declarative
<s>.sem.type event
<s>.sem.agent captain-kirk
<s>.sem.event deliver
<s>.sem.theme power-generator
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
When mxNLU is trained on complete ASR out-
put for approximately 3,500 utterances, and tested
on a separate set of 350 complete ASR utterances,
the F-score of attribute-value pairs produced by the
NLU is 0.76 (0.78 precision and 0.74 recall). These
figures reflect the use of ASR at run-time, and most
errors are caused by incorrect speech recognition.
2.2 NLU with partial ASR results (Sagae et al,
2009)
To interpret utterances before they are complete,
we use partial recognition hypotheses produced by
ASR every 200 milliseconds while the user is speak-
ing. To process these partial utterances produced by
ASR, we train length-specific models for mxNLU.
These models are trained using the partial ASR re-
sults we obtain by running ASR on the audio corre-
sponding to the utterances in the training data. The
NLU task is then to predict the meaning of the en-
tire utterance based only on a (noisy) prefix of the
utterance. On average, the accuracy of mxNLU on a
six-word prefix of an utterance (0.74 F-score) is al-
most as the same as the accuracy of mxNLU on en-
tire utterances. Approximately half of the utterances
in our corpus contain more than six words, creating
interesting opportunities for conversational behavior
that would be impossible under a model where each
utterance must be completed before it is interpreted.
2.3 Detecting points of maximal
understanding (DeVault et al, 2009)
Although length-specific NLU models produce ac-
curate results on average, more effective use of the
interpretation provided by these models might be
achieved if we could automatically gauge their per-
formance on individual utterances at run-time. To
that end, we have developed an approach (DeVault et
al., 2009) that aims to detect those strategic points in
time, as specific utterances are occurring, when the
system reaches maximal understanding of the utter-
ance, in the sense that its interpretation will not sig-
nificantly improve during the rest of the utterance.
Figure 2 illustrates the incremental output of
mxNLU as a user asks, elder do you agree to move
the clinic downtown? Our ASR processes captured
audio in 200ms chunks. The figure shows the par-
tial ASR results after the ASR has processed each
200ms of audio, along with the F-score achieved by
mxNLU on each of these partials. Note that the NLU
F-score fluctuates somewhat as the ASR revises its
incremental hypotheses about the user utterance, but
generally increases over time.
For the purpose of initiating an overlapping re-
sponse to a user utterance such as this one, the agent
needs to be able (in the right circumstances) to make
34
Utterance time (ms)
NL
U F
?sc
ore
(emp
ty)
(emp
ty)
 
all 
 
elde
r 
 
elde
r do
 you
 
 
elde
r to
 you
 d 
 
elde
r do
 you
 agr
ee 
 
elde
r do
 you
 agr
ee t
o 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic to
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
200 400 600 800 100
0
120
0
140
0
160
0
180
0
200
0
220
0
240
0
260
0
280
0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Partial ASR result
Figure 2: Incremental interpretation of a user utterance.
an assessment that it has already understood the ut-
terance ?well enough?, based on the partial ASR re-
sults that are currently available. We have imple-
mented a specific approach to this assessment which
views an utterance as understood ?well enough? if
the agent would not understand the utterance any
better than it currently does even if it were to wait
for the user to finish their utterance (and for the ASR
to finish interpreting the complete utterance).
Concretely, Figure 2 shows that after the entire
2800ms utterance has been processed by the ASR,
mxNLU achieves an F-score of 0.91. However, in
fact, mxNLU already achieves this maximal F-score
at the moment it interprets the partial ASR result el-
der do you agree to move the at 1800ms. The agent
therefore could, in principle, initiate an overlapping
response at 1800ms without sacrificing any accuracy
in its understanding of the user?s utterance.
Of course the agent does not automatically realize
that it has achieved a maximal F-score at 1800ms.
To enable the agent to make this assessment, we
have trained a classifier, which we call MAXF, that
can be invoked for any specific partial ASR result,
and which uses various features of the ASR result
and the current mxNLU output to estimate whether
the NLU F-score for the current partial ASR result
is at least as high as the mxNLU F-score would be if
the agent were to wait for the entire utterance.
To facilitate training of a MAXF classifier, we
identified a range of potentially useful features that
the agent could use at run-time to assess its confi-
dence in mxNLU?s output for a given partial ASR
result. These features include: the number of par-
tial results that have been received from the ASR;
the length (in words) of the current partial ASR
result; the entropy in the probability distribution
mxNLU assigns to alternative output frames (lower
entropy corresponds to a more focused distribution);
the probability mxNLU assigns to the most probable
output frame; and the most probable output frame.
Based on these features, we trained a decision tree
to make the binary prediction that MAXF is TRUE
or FALSE for each partial ASR result. DeVault et al
(2009) include a detailed evaluation and discussion
of the classifier. To briefly summarize our results,
the precision/recall/F-score of the trained MAXF
model are 0.88/0.52/0.65 respectively. The high pre-
cision means that 88% of the time that the model
predicts that F-score is maximized at a specific par-
tial, it really is. Our demonstration, which we out-
line in the next section, highlights the utility of a
high-precision MAXF classifier in making the deci-
sion whether to complete a user?s utterance.
3 Demo script outline
We have implemented the approach for partial utter-
ance understanding described above in the SASO-
EN system (Traum et al, 2008), a virtual human
dialogue system with speech input and output (Fig-
ure 3), allowing us to demonstrate both partial utter-
ance understanding and some of the specific behav-
iors made possible by this capability. We divide this
demonstration in two parts: visualization of NLU
for partial utterances and user utterance completion.
35
Figure 3: SASO-EN: Dr. Perez and Elder al-Hassan.
Partial ASR result Predicted completion
we can provide transportation to move the patient there
the market is not safe
there are supplies where we are going
Table 1: Examples of user utterance completions.
3.1 Visualization of NLU for partial utterances
Because the demonstration depends on usage of the
system within the domain for which it was designed,
the demo operator provides a brief description of the
system, task and domain. The demo operator (or
a volunteer user) then speaks normally to the sys-
tem, while a separate window visualizes the sys-
tem?s evolving understanding. This display is up-
dated every 200 milliseconds, allowing attendees to
see partial utterance understanding in action. For
ease of comprehension, the display will summarize
the NLU state using an English paraphrase of the
predicted meaning (rather than displaying the struc-
tured frame that is the actual output of NLU). The
display will also visualize the TRUE or FALSE state
of the MAXF classifier, highlighting the moment the
system thinks it reaches maximal understanding.
3.2 User utterance completion
The demo operator (or volunteer user) starts to speak
and pauses briefly in mid-utterance, at which point,
if possible, one of the virtual humans jumps in and
completes the utterance (DeVault et al, 2009). Ta-
ble 1 includes a few examples of the many utterances
that can be completed by the virtual humans.
4 Conclusion
Interpretation of partial utterances, combined with
a way to predict points of maximal understanding,
opens exciting possibilities for more natural conver-
sational behavior in virtual humans. This demon-
stration showcases the NLU approach and a sample
application of the basic techniques.
Acknowledgments
The work described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
References
G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,
M. Swift, and M. K. Tanenhaus. 2007. Incremental
dialogue system faster than and preferred to its non-
incremental counterpart. In Proc. of the 29th Annual
Conference of the Cognitive Science Society.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
D. DeVault, K. Sagae, and D. Traum. 2009. Can I finish?
Learning when to respond to incremental interpreta-
tion results in interactive dialogue. In Proc. SIGDIAL.
G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and
N. Hawes. 2007. Incremental, multi-level processing
for comprehending situated dialogue in human-robot
interaction. In Proc. LangRo?2007.
K. Sagae, G. Christian, D. DeVault, and D. R. Traum.
2009. Towards natural language understanding of par-
tial speech recognition results in dialogue systems. In
Short Paper Proceedings of NAACL HLT.
D. Schlangen, T. Baumann, and M. Atterer. 2009. In-
cremental reference resolution: The task, metrics for
evaluation, and a Bayesian filtering model that is sen-
sitive to disfluencies. In Proc. SIGDIAL, page 30?37.
W. Schuler, S. Wu, and L. Schwartz. 2009. A frame-
work for fast incremental interpretation during speech
decoding. Computational Linguistics, 35(3):313?343.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proc. EACL.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negoti-
ation for multi-modal virtual agents. In Proc. of the
Eighth International Conference on Intelligent Virtual
Agents.
36
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Dynamic Programming for Linear-Time Incremental Parsing
Liang Huang
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
lhuang@isi.edu
Kenji Sagae
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
sagae@ict.usc.edu
Abstract
Incremental parsing techniques such as
shift-reduce have gained popularity thanks
to their efficiency, but there remains a
major problem: the search is greedy and
only explores a tiny fraction of the whole
space (even with beam search) as op-
posed to dynamic programming. We show
that, surprisingly, dynamic programming
is in fact possible for many shift-reduce
parsers, by merging ?equivalent? stacks
based on feature values. Empirically, our
algorithm yields up to a five-fold speedup
over a state-of-the-art shift-reduce depen-
dency parser with no loss in accuracy. Bet-
ter search also leads to better learning, and
our final parser outperforms all previously
reported dependency parsers for English
and Chinese, yet is much faster.
1 Introduction
In terms of search strategy, most parsing al-
gorithms in current use for data-driven parsing
can be divided into two broad categories: dy-
namic programming which includes the domi-
nant CKY algorithm, and greedy search which in-
cludes most incremental parsing methods such as
shift-reduce.1 Both have pros and cons: the for-
mer performs an exact search (in cubic time) over
an exponentially large space, while the latter is
much faster (in linear-time) and is psycholinguis-
tically motivated (Frazier and Rayner, 1982), but
its greedy nature may suffer from severe search er-
rors, as it only explores a tiny fraction of the whole
space even with a beam.
Can we combine the advantages of both ap-
proaches, that is, construct an incremental parser
1McDonald et al (2005b) is a notable exception: the MST
algorithm is exact search but not dynamic programming.
that runs in (almost) linear-time, yet searches over
a huge space with dynamic programming?
Theoretically, the answer is negative, as Lee
(2002) shows that context-free parsing can be used
to compute matrix multiplication, where sub-cubic
algorithms are largely impractical.
We instead propose a dynamic programming al-
ogorithm for shift-reduce parsing which runs in
polynomial time in theory, but linear-time (with
beam search) in practice. The key idea is to merge
equivalent stacks according to feature functions,
inspired by Earley parsing (Earley, 1970; Stolcke,
1995) and generalized LR parsing (Tomita, 1991).
However, our formalism is more flexible and our
algorithm more practical. Specifically, we make
the following contributions:
? theoretically, we show that for a large class
of modern shift-reduce parsers, dynamic pro-
gramming is in fact possible and runs in poly-
nomial time as long as the feature functions
are bounded and monotonic (which almost al-
ways holds in practice);
? practically, dynamic programming is up to
five times faster (with the same accuracy) as
conventional beam-search on top of a state-
of-the-art shift-reduce dependency parser;
? as a by-product, dynamic programming can
output a forest encoding exponentially many
trees, out of which we can draw better and
longer k-best lists than beam search can;
? finally, better and faster search also leads to
better and faster learning. Our final parser
achieves the best (unlabeled) accuracies that
we are aware of in both English and Chi-
nese among dependency parsers trained on
the Penn Treebanks. Being linear-time, it is
also much faster than most other parsers,
even with a pure Python implementation.
1077
input: w0 . . . wn?1
axiom 0 : ?0, ??: 0
sh
? : ?j, S? : c
? + 1 : ?j + 1, S|wj? : c + ?
j < n
rex
? : ?j, S|s1|s0? : c
? + 1 : ?j, S|s1xs0? : c + ?
rey
? : ?j, S|s1|s0? : c
? + 1 : ?j, S|s1ys0? : c + ?
goal 2n? 1 : ?n, s0?: c
where ? is the step, c is the cost, and the shift cost ?
and reduce costs ? and ? are:
? = w ? fsh(j, S) (1)
? = w ? frex (j, S|s1|s0) (2)
? = w ? frey (j, S|s1|s0) (3)
Figure 1: Deductive system of vanilla shift-reduce.
For convenience of presentation and experimen-
tation, we will focus on shift-reduce parsing for
dependency structures in the remainder of this pa-
per, though our formalism and algorithm can also
be applied to phrase-structure parsing.
2 Shift-Reduce Parsing
2.1 Vanilla Shift-Reduce
Shift-reduce parsing performs a left-to-right scan
of the input sentence, and at each step, choose one
of the two actions: either shift the current word
onto the stack, or reduce the top two (or more)
items at the end of the stack (Aho and Ullman,
1972). To adapt it to dependency parsing, we split
the reduce action into two cases, rex and rey, de-
pending on which one of the two items becomes
the head after reduction. This procedure is known
as ?arc-standard? (Nivre, 2004), and has been en-
gineered to achieve state-of-the-art parsing accu-
racy in Huang et al (2009), which is also the ref-
erence parser in our experiments.2
More formally, we describe a parser configura-
tion by a state ?j, S? where S is a stack of trees
s0, s1, ... where s0 is the top tree, and j is the
2There is another popular variant, ?arc-eager? (Nivre,
2004; Zhang and Clark, 2008), which is more complicated
and less similar to the classical shift-reduce algorithm.
input: ?I saw Al with Joe?
step action stack queue
0 - I ...
1 sh I saw ...
2 sh I saw Al ...
3 rex Ixsaw Al ...
4 sh Ixsaw Al with ...
5a rey IxsawyAl with ...
5b sh Ixsaw Al with Joe
Figure 2: A trace of vanilla shift-reduce. After
step (4), the parser branches off into (5a) or (5b).
queue head position (current word q0 is wj). At
each step, we choose one of the three actions:
1. sh: move the head of queue, wj , onto stack S
as a singleton tree;
2. rex: combine the top two trees on the stack,
s0 and s1, and replace them with tree s1xs0.
3. rey: combine the top two trees on the stack,
s0 and s1, and replace them with tree s1ys0.
Note that the shorthand notation txt? denotes a
new tree by ?attaching tree t? as the leftmost child
of the root of tree t?. This procedure can be sum-
marized as a deductive system in Figure 1. States
are organized according to step ?, which denotes
the number of actions accumulated. The parser
runs in linear-time as there are exactly 2n?1 steps
for a sentence of n words.
As an example, consider the sentence ?I saw Al
with Joe? in Figure 2. At step (4), we face a shift-
reduce conflict: either combine ?saw? and ?Al? in
a rey action (5a), or shift ?with? (5b). To resolve
this conflict, there is a cost c associated with each
state so that we can pick the best one (or few, with
a beam) at each step. Costs are accumulated in
each step: as shown in Figure 1, actions sh, rex,
and rey have their respective costs ?, ?, and ?,
which are dot-products of the weights w and fea-
tures extracted from the state and the action.
2.2 Features
We view features as ?abstractions? or (partial) ob-
servations of the current state, which is an im-
portant intuition for the development of dynamic
programming in Section 3. Feature templates
are functions that draw information from the fea-
ture window (see Tab. 1(b)), consisting of the
top few trees on the stack and the first few
words on the queue. For example, one such fea-
ture templatef100 = s0.w ? q0.t is a conjunction
1078
of two atomic features s0.w and q0.t, capturing
the root word of the top tree s0 on the stack, and
the part-of-speech tag of the current head word q0
on the queue. See Tab. 1(a) for the list of feature
templates used in the full model. Feature templates
are instantiated for a specific state. For example, at
step (4) in Fig. 2, the above template f100 will gen-
erate a feature instance
(s0.w = Al) ? (q0.t = IN).
More formally, we denote f to be the feature func-
tion, such that f(j, S) returns a vector of feature
instances for state ?j, S?. To decide which action
is the best for the current state, we perform a three-
way classification based on f(j, S), and to do so,
we further conjoin these feature instances with the
action, producing action-conjoined instances like
(s0.w = Al) ? (q0.t = IN) ? (action = sh).
We denote fsh(j, S), frex (j, S), and frey (j, S) to
be the conjoined feature instances, whose dot-
products with the weight vector decide the best ac-
tion (see Eqs. (1-3) in Fig. 1).
2.3 Beam Search and Early Update
To improve on strictly greedy search, shift-reduce
parsing is often enhanced with beam search
(Zhang and Clark, 2008), where b states develop
in parallel. At each step we extend the states in
the current beam by applying one of the three ac-
tions, and then choose the best b resulting states
for the next step. Our dynamic programming algo-
rithm also runs on top of beam search in practice.
To train the model, we use the averaged percep-
tron algorithm (Collins, 2002). Following Collins
and Roark (2004) we also use the ?early-update?
strategy, where an update happens whenever the
gold-standard action-sequence falls off the beam,
with the rest of the sequence neglected.3 The intu-
ition behind this strategy is that later mistakes are
often caused by previous ones, and are irrelevant
when the parser is on the wrong track. Dynamic
programming turns out to be a great fit for early
updating (see Section 4.3 for details).
3 Dynamic Programming (DP)
3.1 Merging Equivalent States
The key observation for dynamic programming
is to merge ?equivalent states? in the same beam
3As a special case, for the deterministic mode (b=1), up-
dates always co-occur with the first mistake made.
(a) Features Templates f(j, S) qi = wj+i
(1) s0.w s0.t s0.w ? s0.t
s1.w s1.t s1.w ? s1.t
q0.w q0.t q0.w ? q0.t
(2) s0.w ? s1.w s0.t ? s1.t
s0.t ? q0.t s0.w ? s0.t ? s1.t
s0.t ? s1.w ? s1.t s0.w ? s1.w ? s1.t
s0.w ? s0.t ? s1.w s0.w ? s0.t ? s1 ? s1.t
(3) s0.t ? q0.t ? q1.t s1.t ? s0.t ? q0.t
s0.w ? q0.t ? q1.t s1.t ? s0.w ? q0.t
(4) s1.t ? s1.lc.t ? s0.t s1.t ? s1.rc.t ? s0.t
s1.t ? s0.t ? s0.rc.t s1.t ? s1.lc.t ? s0
s1.t ? s1.rc.t ? s0.w s1.t ? s0.w ? s0.lc.t
(5) s2.t ? s1.t ? s0.t
(b) ? stack queue?
... s2
...
s1
s1.lc
...
... s1.rc
...
s0
s0.lc
...
... s0.rc
...
q0 q1 ...
(c) Kernel features for DP
ef(j, S) = (j, f2(s2), f1(s1), f0(s0))
f2(s2) s2.t
f1(s1) s1.w s1.t s1.lc.t s1.rc.t
f0(s0) s0.w s0.t s0.lc.t s0.rc.t
j q0.w q0.t q1.t
Table 1: (a) feature templates used in this work,
adapted from Huang et al (2009). x.w and x.t de-
notes the root word and POS tag of tree (or word)
x. and x.lc and x.rc denote x?s left- and rightmost
child. (b) feature window. (c) kernel features.
(i.e., same step) if they have the same feature
values, because they will have the same costs as
shown in the deductive system in Figure 1. Thus
we can define two states ?j, S? and ?j?, S?? to be
equivalent, notated ?j, S? ? ?j?, S??, iff.
j = j? and f(j, S) = f(j?, S?). (4)
Note that j = j? is also needed because the
queue head position j determines which word to
shift next. In practice, however, a small subset of
atomic features will be enough to determine the
whole feature vector, which we call kernel fea-
tures f?(j, S), defined as the smallest set of atomic
templates such that
f?(j, S) = f?(j?, S?) ? ?j, S? ? ?j?, S??.
For example, the full list of 28 feature templates
in Table 1(a) can be determined by just 12 atomic
features in Table 1(c), which just look at the root
words and tags of the top two trees on stack, as
well as the tags of their left- and rightmost chil-
dren, plus the root tag of the third tree s2, and fi-
nally the word and tag of the queue head q0 and the
1079
state form ? : ?i, j, sd...s0?: (c, v, ?) ?: step; c, v: prefix and inside costs; ?: predictor states
equivalence ? : ?i, j, sd...s0? ? ? : ?i, j, s?d...s?0? iff. f?(j, sd...s0) = f?(j, s?d...s?0)
ordering ? : : (c, v, ) ? ? : : (c?, v?, ) iff. c < c? or (c = c? and v < v?).
axiom (p0) 0 : ?0, 0, ??: (0, 0, ?)
sh
state p:
? : ? , j, sd...s0?: (c, , )
? + 1 : ?j, j + 1, sd?1...s0, wj? : (c + ?, 0, {p})
j < n
rex
state p:
: ?k, i, s?d...s?0?: (c?, v?, ??)
state q:
? : ?i, j, sd...s0?: ( , v, ?)
? + 1 : ?k, j, s?d...s?1, s?0
xs0? : (c? + v + ?, v? + v + ?, ??)
p ? ?
goal 2n? 1 : ?0, n, sd...s0?: (c, c, {p0})
where ? = w ? fsh(j, sd...s0), and ? = ?? + ?, with ?? = w ? fsh(i, s?d...s?0) and ? = w ? frex (j, sd...s0).
Figure 3: Deductive system for shift-reduce parsing with dynamic programming. The predictor state set ?
is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995).
The rey case is similar, replacing s?0
xs0 with s?0
ys0, and ? with ? = w ? frey (j, sd...s0). Irrelevant
information in a deduction step is marked as an underscore ( ) which means ?can match anything?.
tag of the next word q1. Since the queue is static
information to the parser (unlike the stack, which
changes dynamically), we can use j to replace fea-
tures from the queue. So in general we write
f?(j, S) = (j, fd(sd), . . . , f0(s0))
if the feature window looks at top d + 1 trees
on stack, and where fi(si) extracts kernel features
from tree si (0 ? i ? d). For example, for the full
model in Table 1(a) we have
f?(j, S) = (j, f2(s2), f1(s1), f0(s0)), (5)
where d = 2, f2(x) = x.t, and f1(x) = f0(x) =
(x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).
3.2 Graph-Structured Stack and Deduction
Now that we have the kernel feature functions, it
is intuitive that we might only need to remember
the relevant bits of information from only the last
(d + 1) trees on stack instead of the whole stack,
because they provide all the relevant information
for the features, and thus determine the costs. For
shift, this suffices as the stack grows on the right;
but for reduce actions the stack shrinks, and in or-
der still to maintain d + 1 trees, we have to know
something about the history. This is exactly why
we needed the full stack for vanilla shift-reduce
parsing in the first place, and why dynamic pro-
gramming seems hard here.
To solve this problem we borrow the idea
of ?graph-structured stack? (GSS) from Tomita
(1991). Basically, each state p carries with it a set
?(p) of predictor states, each of which can be
combined with p in a reduction step. In a shift step,
if state p generates state q (we say ?p predicts q?
in Earley (1970) terms), then p is added onto ?(q).
When two equivalent shifted states get merged,
their predictor states get combined. In a reduction
step, state q tries to combine with every predictor
state p ? ?(q), and the resulting state r inherits
the predictor states set from p, i.e., ?(r) = ?(p).
Interestingly, when two equivalent reduced states
get merged, we can prove (by induction) that their
predictor states are identical (proof omitted).
Figure 3 shows the new deductive system with
dynamic programming and GSS. A new state has
the form
? : ?i, j, sd...s0?
where [i..j] is the span of the top tree s0, and
sd..s1 are merely ?left-contexts?. It can be com-
bined with some predictor state p spanning [k..i]
?? : ?k, i, s?d...s?0?
to form a larger state spanning [k..j], with the
resulting top tree being either s1xs0 or s1ys0.
1080
This style resembles CKY and Earley parsers. In
fact, the chart in Earley and other agenda-based
parsers is indeed a GSS when viewed left-to-right.
In these parsers, when a state is popped up from
the agenda, it looks for possible sibling states
that can combine with it; GSS, however, explicitly
maintains these predictor states so that the newly-
popped state does not need to look them up.4
3.3 Correctness and Polynomial Complexity
We state the main theoretical result with the proof
omitted due to space constraints:
Theorem 1. The deductive system is optimal and
runs in worst-case polynomial time as long as the
kernel feature function satisfies two properties:
? bounded: f?(j, S) = (j, fd(sd), . . . , f0(s0))
for some constant d, and each |ft(x)| also
bounded by a constant for all possible tree x.
? monotonic: ft(x) = ft(y) ? ft+1(x) =
ft+1(y), for all t and all possible trees x, y.
Intuitively, boundedness means features can
only look at a local window and can only extract
bounded information on each tree, which is always
the case in practice since we can not have infinite
models. Monotonicity, on the other hand, says that
features drawn from trees farther away from the
top should not be more refined than from those
closer to the top. This is also natural, since the in-
formation most relevant to the current decision is
always around the stack top. For example, the ker-
nel feature function in Eq. 5 is bounded and mono-
tonic, since f2 is less refined than f1 and f0.
These two requirements are related to grammar
refinement by annotation (Johnson, 1998), where
annotations must be bounded and monotonic: for
example, one cannot refine a grammar by only
remembering the grandparent but not the parent
symbol. The difference here is that the annotations
are not vertical ((grand-)parent), but rather hori-
zontal (left context). For instance, a context-free
rule A ? B C would become DA ? DB BC
for some D if there exists a rule E ? ?DA?.
This resembles the reduce step in Fig. 3.
The very high-level idea of the proof is that
boundedness is crucial for polynomial-time, while
monotonicity is used for the optimal substructure
property required by the correctness of DP.
4In this sense, GSS (Tomita, 1988) is really not a new in-
vention: an efficient implementation of Earley (1970) should
already have it implicitly, similar to what we have in Fig. 3.
3.4 Beam Search based on Prefix Cost
Though the DP algorithm runs in polynomial-
time, in practice the complexity is still too high,
esp. with a rich feature set like the one in Ta-
ble 1. So we apply the same beam search idea
from Sec. 2.3, where each step can accommodate
only the best b states. To decide the ordering of
states in each beam we borrow the concept of pre-
fix cost from Stolcke (1995), originally developed
for weighted Earley parsing. As shown in Fig. 3,
the prefix cost c is the total cost of the best action
sequence from the initial state to the end of state p,
i.e., it includes both the inside cost v (for Viterbi
inside derivation), and the cost of the (best) path
leading towards the beginning of state p. We say
that a state p with prefix cost c is better than a state
p? with prefix cost c?, notated p ? p? in Fig. 3, if
c < c?. We can also prove (by contradiction) that
optimizing for prefix cost implies optimal inside
cost (Nederhof, 2003, Sec. 4). 5
As shown in Fig. 3, when a state q with costs
(c, v) is combined with a predictor state p with
costs (c?, v?), the resulting state r will have costs
(c? + v + ?, v? + v + ?),
where the inside cost is intuitively the combined
inside costs plus an additional combo cost ? from
the combination, while the resulting prefix cost
c? + v + ? is the sum of the prefix cost of the pre-
dictor state q, the inside cost of the current state p,
and the combo cost. Note the prefix cost of q is ir-
relevant. The combo cost ? = ?? + ? consists of
shift cost ?? of p and reduction cost ? of q.
The cost in the non-DP shift-reduce algorithm
(Fig. 1) is indeed a prefix cost, and the DP algo-
rithm subsumes the non-DP one as a special case
where no two states are equivalent.
3.5 Example: Edge-Factored Model
As a concrete example, Figure 4 simulates an
edge-factored model (Eisner, 1996; McDonald et
al., 2005a) using shift-reduce with dynamic pro-
gramming, which is similar to bilexical PCFG
parsing using CKY (Eisner and Satta, 1999). Here
the kernel feature function is
f?(j, S) = (j, h(s1), h(s0))
5Note that using inside cost v for ordering would be a
bad idea, as it will always prefer shorter derivations like in
best-first parsing. As in A* search, we need some estimate
of ?outside cost? to predict which states are more promising,
and the prefix cost includes an exact cost for the left outside
context, but no right outside context.
1081
sh
? : ? , h
...j
? : (c, )
? + 1 : ?h, j? : (c, 0) j < n
rex
: ?h??, h?
k...i
? : (c?, v?) ? : ?h?, h
i...j
? : ( , v)
? + 1 : ?h??, h
h?
k...i
i...j
? : (c? + v + ?, v? + v + ?)
where rex cost ? = w ? frex(h?, h)
Figure 4: Example of shift-reduce with dynamic
programming: simulating an edge-factored model.
GSS is implicit here, and rey case omitted.
where h(x) returns the head word index of tree x,
because all features in this model are based on the
head and modifier indices in a dependency link.
This function is obviously bounded and mono-
tonic in our definitions. The theoretical complexity
of this algorithm is O(n7) because in a reduction
step we have three span indices and three head in-
dices, plus a step index ?. By contrast, the na??ve
CKY algorithm for this model is O(n5) which can
be improved to O(n3) (Eisner, 1996).6 The higher
complexity of our algorithm is due to two factors:
first, we have to maintain both h and h? in one
state, because the current shift-reduce model can
not draw features across different states (unlike
CKY); and more importantly, we group states by
step ? in order to achieve incrementality and lin-
ear runtime with beam search that is not (easily)
possible with CKY or MST.
4 Experiments
We first reimplemented the reference shift-reduce
parser of Huang et al (2009) in Python (hence-
forth ?non-DP?), and then extended it to do dy-
namic programing (henceforth ?DP?). We evalu-
ate their performances on the standard Penn Tree-
bank (PTB) English dependency parsing task7 us-
ing the standard split: secs 02-21 for training, 22
for development, and 23 for testing. Both DP and
non-DP parsers use the same feature templates in
Table 1. For Secs. 4.1-4.2, we use a baseline model
trained with non-DP for both DP and non-DP, so
that we can do a side-by-side comparison of search
6Or O(n2) with MST, but including non-projective trees.
7Using the head rules of Yamada and Matsumoto (2003).
quality; in Sec. 4.3 we will retrain the model with
DP and compare it against training with non-DP.
4.1 Speed Comparisons
To compare parsing speed between DP and non-
DP, we run each parser on the development set,
varying the beam width b from 2 to 16 (DP) or 64
(non-DP). Fig. 5a shows the relationship between
search quality (as measured by the average model
score per sentence, higher the better) and speed
(average parsing time per sentence), where DP
with a beam width of b=16 achieves the same
search quality with non-DP at b=64, while being 5
times faster. Fig. 5b shows a similar comparison
for dependency accuracy. We also test with an
edge-factored model (Sec. 3.5) using feature tem-
plates (1)-(3) in Tab. 1, which is a subset of those
in McDonald et al (2005b). As expected, this dif-
ference becomes more pronounced (8 times faster
in Fig. 5c), since the less expressive feature set
makes more states ?equivalent? and mergeable in
DP. Fig. 5d shows the (almost linear) correlation
between dependency accuracy and search quality,
confirming that better search yields better parsing.
4.2 Search Space, Forest, and Oracles
DP achieves better search quality because it ex-
pores an exponentially large search space rather
than only b trees allowed by the beam (see Fig. 6a).
As a by-product, DP can output a forest encoding
these exponentially many trees, out of which we
can draw longer and better (in terms of oracle) k-
best lists than those in the beam (see Fig. 6b). The
forest itself has an oracle of 98.15 (as if k ? ?),
computed a` la Huang (2008, Sec. 4.1). These can-
didate sets may be used for reranking (Charniak
and Johnson, 2005; Huang, 2008).8
4.3 Perceptron Training and Early Updates
Another interesting advantage of DP over non-DP
is the faster training with perceptron, even when
both parsers use the same beam width. This is due
to the use of early updates (see Sec. 2.3), which
happen much more often with DP, because a gold-
standard state p is often merged with an equivalent
(but incorrect) state that has a higher model score,
which triggers update immediately. By contrast, in
non-DP beam search, states such as p might still
8DP?s k-best lists are extracted from the forest using the
algorithm of Huang and Chiang (2005), rather than those in
the final beam as in the non-DP case, because many deriva-
tions have been merged during dynamic programming.
1082
 2370
 2373
 2376
 2379
 2382
 2385
 2388
 2391
 2394
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
av
g.
 m
od
el
 sc
or
e
b=16 b=64
DP
non-DP
 92.2
 92.3
 92.4
 92.5
 92.6
 92.7
 92.8
 92.9
 93
 93.1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
de
pe
nd
en
cy
 a
cc
ur
ac
y
b=16 b=64
DP
non-DP
(a) search quality vs. time (full model) (b) parsing accuracy vs. time (full model)
 2290
 2295
 2300
 2305
 2310
 2315
 2320
 2325
 2330
 2335
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
av
g.
 m
od
el
 sc
or
e
b=16
b=64
DP
non-DP
 88.5
 89
 89.5
 90
 90.5
 91
 91.5
 92
 92.5
 93
 93.5
 2280  2300  2320  2340  2360  2380  2400
de
pe
nd
en
cy
 a
cc
ur
ac
y
full, DP
full, non-DP
edge-factor, DP
edge-factor, non-DP
(c) search quality vs. time (edge-factored model) (d) correlation b/w parsing (y) and search (x)
Figure 5: Speed comparisons between DP and non-DP, with beam size b ranging 2?16 for DP and 2?64
for non-DP. Speed is measured by avg. parsing time (secs) per sentence on x axis. With the same level
of search quality or parsing accuracy, DP (at b=16) is ?4.8 times faster than non-DP (at b=64) with the
full model in plots (a)-(b), or ?8 times faster with the simplified edge-factored model in plot (c). Plot (d)
shows the (roughly linear) correlation between parsing accuracy and search quality (avg. model score).
100
102
104
106
108
1010
1012
 0  10  20  30  40  50  60  70
nu
m
be
r o
f t
re
es
 e
xp
lo
re
d
sentence length
DP forest
non-DP (16)
 93
 94
 95
 96
 97
 98
 99
 64 32 16 8 4 1
or
ac
le
 p
re
ci
sio
n
 k
DP forest (98.15)
DP k-best in forest
non-DP k-best in beam
(a) sizes of search spaces (b) oracle precision on dev
Figure 6: DP searches over a forest of exponentially many trees, which also produces better and longer
k-best lists with higher oracles, while non-DP only explores b trees allowed in the beam (b = 16 here).
1083
 90.5
 91
 91.5
 92
 92.5
 93
 93.5
 0  4  8  12  16  20  24
ac
cu
ra
cy
 o
n 
de
v 
(e
ac
h 
ro
un
d)
hours
17th
18th
DP
non-DP
Figure 7: Learning curves (showing precision on
dev) of perceptron training for 25 iterations (b=8).
DP takes 18 hours, peaking at the 17th iteration
(93.27%) with 12 hours, while non-DP takes 23
hours, peaking at the 18th (93.04%) with 16 hours.
survive in the beam throughout, even though it is
no longer possible to rank the best in the beam.
The higher frequency of early updates results
in faster iterations of perceptron training. Table 2
shows the percentage of early updates and the time
per iteration during training. While the number of
updates is roughly comparable between DP and
non-DP, the rate of early updates is much higher
with DP, and the time per iteration is consequently
shorter. Figure 7 shows that training with DP is
about 1.2 times faster than non-DP, and achieves
+0.2% higher accuracy on the dev set (93.27%).
Besides training with gold POS tags, we also
trained on noisy tags, since they are closer to the
test setting (automatic tags on sec 23). In that
case, we tag the dev and test sets using an auto-
matic POS tagger (at 97.2% accuracy), and tag
the training set using four-way jackknifing sim-
ilar to Collins (2000), which contributes another
+0.1% improvement in accuracy on the test set.
Faster training also enables us to incorporate more
features, where we found more lookahead features
(q2) results in another +0.3% improvement.
4.4 Final Results on English and Chinese
Table 3 presents the final test results of our DP
parser on the Penn English Treebank, compared
with other state-of-the-art parsers. Our parser
achieves the highest (unlabeled) dependency ac-
curacy among dependency parsers trained on the
Treebank, and is also much faster than most other
parsers even with a pure Python implementation
it update early% time update early% time
1 31943 98.9 22 31189 87.7 29
5 20236 98.3 38 19027 70.3 47
17 8683 97.1 48 7434 49.5 60
25 5715 97.2 51 4676 41.2 65
Table 2: Perceptron iterations with DP (left) and
non-DP (right). Early updates happen much more
often with DP due to equivalent state merging,
which leads to faster training (time in minutes).
word L time comp.
McDonald 05b 90.2 Ja 0.12 O(n2)
McDonald 05a 90.9 Ja 0.15 O(n3)
Koo 08 base 92.0 ? ? O(n4)
Zhang 08 single 91.4 C 0.11 O(n)?
this work 92.1 Py 0.04 O(n)
?Charniak 00 92.5 C 0.49 O(n5)
?Petrov 07 92.4 Ja 0.21 O(n3)
Zhang 08 combo 92.1 C ? O(n2)?
Koo 08 semisup 93.2 ? ? O(n4)
Table 3: Final test results on English (PTB). Our
parser (in pure Python) has the highest accuracy
among dependency parsers trained on the Tree-
bank, and is also much faster than major parsers.
?converted from constituency trees. C=C/C++,
Py=Python, Ja=Java. Time is in seconds per sen-
tence. Search spaces: ?linear; others exponential.
(on a 3.2GHz Xeon CPU). Best-performing con-
stituency parsers like Charniak (2000) and Berke-
ley (Petrov and Klein, 2007) do outperform our
parser, since they consider more information dur-
ing parsing, but they are at least 5 times slower.
Figure 8 shows the parse time in seconds for each
test sentence. The observed time complexity of our
DP parser is in fact linear compared to the super-
linear complexity of Charniak, MST (McDonald
et al, 2005b), and Berkeley parsers. Additional
techniques such as semi-supervised learning (Koo
et al, 2008) and parser combination (Zhang and
Clark, 2008) do achieve accuracies equal to or
higher than ours, but their results are not directly
comparable to ours since they have access to ex-
tra information like unlabeled data. Our technique
is orthogonal to theirs, and combining these tech-
niques could potentially lead to even better results.
We also test our final parser on the Penn Chi-
nese Treebank (CTB5). Following the set-up of
Duan et al (2007) and Zhang and Clark (2008), we
split CTB5 into training (secs 001-815 and 1001-
1084
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 0  10  20  30  40  50  60  70
pa
rs
in
g 
tim
e 
(s
ec
s)
sentence length
Cha
Berk
MST
DP
Figure 8: Scatter plot of parsing time against sen-
tence length, comparing with Charniak, Berkeley,
and the O(n2) MST parsers.
word non-root root compl.
Duan 07 83.88 84.36 73.70 32.70
Zhang 08? 84.33 84.69 76.73 32.79
this work 85.20 85.52 78.32 33.72
Table 4: Final test results on Chinese (CTB5).
?The transition parser in Zhang and Clark (2008).
1136), development (secs 886-931 and 1148-
1151), and test (secs 816-885 and 1137-1147) sets,
assume gold-standard POS-tags for the input, and
use the head rules of Zhang and Clark (2008). Ta-
ble 4 summarizes the final test results, where our
work performs the best in all four types of (unla-
beled) accuracies: word, non-root, root, and com-
plete match (all excluding punctuations). 9,10
5 Related Work
This work was inspired in part by Generalized LR
parsing (Tomita, 1991) and the graph-structured
stack (GSS). Tomita uses GSS for exhaustive LR
parsing, where the GSS is equivalent to a dy-
namic programming chart in chart parsing (see
Footnote 4). In fact, Tomita?s GLR is an in-
stance of techniques for tabular simulation of non-
deterministic pushdown automata based on deduc-
tive systems (Lang, 1974), which allow for cubic-
time exhaustive shift-reduce parsing with context-
free grammars (Billot and Lang, 1989).
Our work advances this line of research in two
aspects. First, ours is more general than GLR in
9Duan et al (2007) and Zhang and Clark (2008) did not
report word accuracies, but those can be recovered given non-
root and root ones, and the number of non-punctuation words.
10Parser combination in Zhang and Clark (2008) achieves
a higher word accuracy of 85.77%, but again, it is not directly
comparable to our work.
that it is not restricted to LR (a special case of
shift-reduce), and thus does not require building an
LR table, which is impractical for modern gram-
mars with a large number of rules or features. In
contrast, we employ the ideas behind GSS more
flexibly to merge states based on features values,
which can be viewed as constructing an implicit
LR table on-the-fly. Second, unlike previous the-
oretical results about cubic-time complexity, we
achieved linear-time performance by smart beam
search with prefix cost inspired by Stolcke (1995),
allowing for state-of-the-art data-driven parsing.
To the best of our knowledge, our work is the
first linear-time incremental parser that performs
dynamic programming. The parser of Roark and
Hollingshead (2009) is also almost linear time, but
they achieved this by discarding parts of the CKY
chart, and thus do achieve incrementality.
6 Conclusion
We have presented a dynamic programming al-
gorithm for shift-reduce parsing, which runs in
linear-time in practice with beam search. This
framework is general and applicable to a large-
class of shift-reduce parsers, as long as the feature
functions satisfy boundedness and monotonicity.
Empirical results on a state-the-art dependency
parser confirm the advantage of DP in many as-
pects: faster speed, larger search space, higher ora-
cles, and better and faster learning. Our final parser
outperforms all previously reported dependency
parsers trained on the Penn Treebanks for both
English and Chinese, and is much faster in speed
(even with a Python implementation). For future
work we plan to extend it to constituency parsing.
Acknowledgments
We thank David Chiang, Yoav Goldberg, Jonathan
Graehl, Kevin Knight, and Roger Levy for help-
ful discussions and the three anonymous review-
ers for comments. Mark-Jan Nederhof inspired the
use of prefix cost. Yue Zhang helped with Chinese
datasets, and Wenbin Jiang with feature sets. This
work is supported in part by DARPA GALE Con-
tract No. HR0011-06-C-0022 under subcontract to
BBN Technologies, and by the U.S. Army Re-
search, Development, and Engineering Command
(RDECOM). Statements and opinions expressed
do not necessarily reflect the position or the policy
of the United States Government, and no official
endorsement should be inferred.
1085
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume I: Parsing of Series in Automatic Computation.
Prentice Hall, Englewood Cliffs, New Jersey.
S. Billot and B. Lang. 1989. The structure of shared
forests in ambiguous parsing. In Proceedings of the
27th ACL, pages 143?151.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the 43rd ACL, Ann Ar-
bor, MI.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of ICML,
pages 175?182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/PKDD.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Communications of the ACM, 13(2):94?
102.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of ACL.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of COLING.
Lyn Frazier and Keith Rayner. 1982. Making and cor-
recting errors during sentence comprehension: Eye
movements in the analysis of structurally ambigu-
ous sentences. Cognitive Psychology, 14(2):178 ?
210.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT-2005).
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24:613?632.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
B. Lang. 1974. Deterministic techniques for efficient
non-deterministic parsers. In Automata, Languages
and Programming, 2nd Colloquium, volume 14 of
Lecture Notes in Computer Science, pages 255?269,
Saarbru?cken. Springer-Verlag.
Lillian Lee. 2002. Fast context-free grammar parsing
requires fast Boolean matrix multiplication. Journal
of the ACM, 49(1):1?15.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computational Linguis-
tics, pages 135?143.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop
at ACL-2004, Barcelona.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proceedings of HLT-NAACL.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165?201.
Masaru Tomita. 1988. Graph-structured stack and nat-
ural language parsing. In Proceedings of the 26th
annual meeting on Association for Computational
Linguistics, pages 249?257, Morristown, NJ, USA.
Association for Computational Linguistics.
Masaru Tomita, editor. 1991. Generalized LR Parsing.
Kluwer Academic Publishers.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
1086
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 95?100,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Identification and Segmentation of Domain-Specific Dialogue Acts for
Conversational Dialogue Systems
Fabrizio Morbini and Kenji Sagae
Institute for Creative Technologies
University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094
{morbini,sagae}@ict.usc.edu
Abstract
Individual utterances often serve multiple
communicative purposes in dialogue. We
present a data-driven approach for identifica-
tion of multiple dialogue acts in single utter-
ances in the context of dialogue systems with
limited training data. Our approach results in
significantly increased understanding of user
intent, compared to two strong baselines.
1 Introduction
Natural language understanding (NLU) at the level
of speech acts for conversational dialogue systems
can be performed with high accuracy in limited do-
mains using data-driven techniques (Bender et al,
2003; Sagae et al, 2009; Gandhe et al, 2008, for
example), provided that enough training material is
available. For most systems that implement novel
conversational scenarios, however, enough exam-
ples of user utterances, which can be annotated as
NLU training data, only become available once sev-
eral users have interacted with the system. This situ-
ation is typically addressed by bootstrapping from a
relatively small set of hand-authored utterances that
perform key dialogue acts in the scenario or from
utterances collected from wizard-of-oz or role-play
exercises, and having NLU accuracy increase over
time as more users interact with the system and more
utterances are annotated for NLU training.
While this can be effective in practice for ut-
terances that perform only one of several possible
system-specific dialogue acts (often several dozens),
longer utterances that include multiple dialogue acts
pose a greater challenge: the many available combi-
nations of dialogue acts per utterance result in sparse
coverage of the space of possibilities, unless a very
large amount of data can be collected and anno-
tated, which is often impractical. Users of the dia-
logue system, whose utterances are collected for fur-
ther NLU improvement, tend to notice that portions
of their longer utterances are ignored and that they
are better understood when they express themselves
with simpler sentences. This results in generation of
data heavily skewed towards utterances that corre-
spond to a single dialogue act, making it difficult to
collect enough examples of utterances with multiple
dialogue acts to improve NLU, which is precisely
what would be needed to make users feel more com-
fortable with using longer utterances.
We address this chicken-and-egg problem with a
data-driven NLU approach that segments and iden-
tifies multiple dialogue acts in single utterances,
even when only short (single dialogue act) utter-
ances are available for training. In contrast to previ-
ous approaches that assume the existence of enough
training data for learning to segment utterances,
e.g. (Stolcke and Shriberg, 1996), or to align spe-
cific words to parts of the formal representation,
e.g. (Bender et al, 2003), our framework requires a
relatively small dataset, which may not contain any
utterances with multiple dialogue acts. This makes it
possible to create new conversational dialogue sys-
tem scenarios that allow and encourage users to ex-
press themselves with fewer restrictions, without an
increased burden in the collection and annotation of
NLU training data.
2 Method
Given (1) a predefined set of possible dialogue acts
for a specific dialogue system, (2) a set of utterances
95
each annotated with a single dialogue act label, and
(3) a classifier trained on this annotated utterance-
label set, which assigns for a given word sequence a
dialogue act label with a corresponding confidence
score, our task is to find the best sequence of dia-
logue acts that covers a given input utterance. While
short utterances are likely to be covered entirely by a
single dialogue act that spans all of its words, longer
utterances may be composed of spans that corre-
spond to different dialogue acts.
bestDialogueActEndingAt(Text,pos) begin
if pos < 0 then
return ?pos, ?null, 1??;
end
S = {};
for j = 0 to pos do
?c, p? = classify(words(Text, j, pos));
S = S ? {?j, ?c, p??};
end
return argmax
?k,?c,p???S
{p ? p
?
: ?h, ?c
?
, p
?
?? =
bestDialogueActEndingAt(Text, k ? 1)};
end
Algorithm 1: The function classify(T ) calls the
single dialogue act classifier subsystem on the in-
put text T and returns the highest scoring dia-
logue act label c with its confidence score p. The
function words(T, i, j) returns the string formed
by concatenating the words in T from the ith to
the jth included. To obtain the best segmenta-
tion of a given text, one has to work its way back
from the end of the text: start by calling ?k, ?c, p??
= bestDialogueActEndingAt(Text, numWords),
where numWords is the number of words
in Text. If k > 0 recursively call
bestDialogueActEndingAt(Text, k ? 1) to obtain
the optimal dialogue act ending at k ? 1.
Algorithm 1 shows our approach for using a sin-
gle dialogue act classifier to extract the sequence of
dialogue acts with the highest overall score from a
given utterance. The framework is independent of
the particular subsystem used to select the dialogue
act label for a given segment of text. The constraint
is that this subsystem should return, for a given se-
quence of words, at least one dialogue act label and
its confidence level in a normalized range that can
be used for comparisons with subsequent runs. In
the work reported in this paper, we use an existing
data-driven NLU module (Sagae et al, 2009), de-
veloped for the SASO virtual human dialogue sys-
tem (Traum et al, 2008b), but retrained using the
data described in section 3. This NLU module per-
forms maximum entropy multiclass classification,
using features derived from the words in the input
utterance, and using dialogue act labels as classes.
The basic idea is to find the best segmentation
(that is, the one with the highest score) of the portion
of the input text up to the ith word. The base case Si
would be for i = 1 and it is the result of our classi-
fier when the input is the single first word. For any
other i > 1 we construct all word spans Tj,i of the
input text, containing the words from j to i, where
1 ? j ? i, then we classify each of the Tj,i and
pick the best returned class (dialogue act label) Cj,i
(and associated score, which in the case of our maxi-
mum entropy classifier is the conditional probability
Score(Cj,i) = P (Cj,i|Tj,i)). Then we assign to the
best segmentation ending at i, Si, the label Ck,i iff:
k = argmax
1?h?i
(
Score(Ch,i) ? Score(Sh?1)
)
(1)
Algorithm 1 calls the classifier O(n2) where n
is the number of words in the input text. Note
that, as in the maximum entropy NLU of Bender et
al. (2003), this search uses the ?maximum approxi-
mation,? and we do not normalize over all possible
sequences. Therefore, our scores are not true proba-
bilities, although they serve as a good approximation
in the search for the best overall segmentation.
We experimented with two other variations of
the argument of the argmax in equation 1: (1) in-
stead of considering Score(Sh?1), consider only
the last segment contained in Sh?1; and (2) instead
of using the product of the scores of all segments,
use the average score per segment: (Score(Ch,i) ?
Score(Sh?1))1/(1+N(Sh?1)) where N(Si) is the
number of segments in Si. These variants produce
similar results; the results reported in the next sec-
tion were obtained with the second variant.
3 Evaluation
3.1 Data
To evaluate our approach we used data collected
from users of the TACQ (Traum et al, 2008a) dia-
96
logue system, as described by Artstein et al (2009).
Of the utterances in that dataset, about 30% are an-
notated with multiple dialogue acts. The annotation
also contains for each dialogue act the correspond-
ing segment of the input utterance.
The dataset contains a total of 1,579 utterances.
Of these, 1,204 utterances contain only a single di-
alogue act, and 375 utterances contain multiple dia-
logue acts, according to manual dialogue act anno-
tation. Within the set of utterances that contain mul-
tiple dialogue acts, the average number of dialogue
acts per utterance is 2.3.
The dialogue act annotation scheme uses a total
of 77 distinct labels, with each label corresponding
to a domain-specific dialogue act, including some
semantic information. Each of these 77 labels is
composed at least of a core speech act type (e.g.
wh-question, offer), and possibly also attributes that
reflect semantics in the domain. For example, the
dialogue act annotation for the utterance What is
the strange man?s name? would be whq(obj:
strangeMan, attr: name), reflecting that
it is a wh-question, with a specific object and at-
tribute. In the set of utterances with only one speech
act, 70 of the possible 77 dialogue act labels are
used. In the remaining utterances (which contain
multiple speech acts per utterance), 59 unique dia-
logue act labels are used, including 7 that are not
used in utterances with only a single dialogue act
(these 7 labels are used in only 1% of those utter-
ances). A total of 18 unique labels are used only
in the set of utterances with one dialogue act (these
labels are used in 5% of those utterances). Table 1
shows the frequency information for the five most
common dialogue act labels in our dataset.
The average number of words in utterances with
only a single dialogue act is 7.5 (with a maximum
of 34, and minimum of 1), and the average length of
utterances with multiple dialogue acts is 15.7 (max-
imum of 66, minimum of 2). To give a better idea of
the dataset used here, we list below two examples of
utterances in the dataset, and their dialogue act an-
notation. We add word indices as subscripts in the
utterances for illustration purposes only, to facilitate
identification of the word spans for each dialogue
act. The annotation consists of a word interval and a
Single DA Utt. [%] Multiple DA Utt. [%]
Wh-questions 51 Wh-questions 31
Yes/No-questions 14 Offers to agent 24
Offers to agent 9 Yes answer 11
Yes answer 7 Yes/No-questions 8
Greeting 7 Thanks 7
Table 1: The frequency of the dialogue act classes most
used in the TACQ dataset (Artstein et al, 2009). The
left column reports the statistics for the set of utterances
annotated with a single dialogue act the right those for the
utterances annotated with multiple dialogue acts. Each
dialogue act class typically contains several more specific
dialogue acts that include domain-specific semantics (for
example, there are 29 subtypes of wh-questions that can
be performed in the domain, each with a separate domain-
specific dialogue act label).
dialogue act label1.
1. ? 0 his 1 name, 2 any 3 other 4 informa-
tion 5 about 6 him, 7 where 8 he 9 lives
10? is labeled with: [0 2] whq(obj:
strangeMan, attr: name), [2 7]
whq(obj: strangeMan) and [7 10]
whq(obj: strangeMan, attr:
location).
2. ? 0 I 1 can?t 2 offer 3 you 4 money 5 but 6 I 7 can
8 offer 9 you 10 protection 11? is labeled with:
[0 5] reject, [5 11] offer(safety).
3.2 Setup
In our experiments, we performed 10-fold cross-
validation using the dataset described above. For
the training folds, we use only utterances with a sin-
gle dialogue act (utterances containing multiple dia-
logue acts are split into separate utterances), and the
training procedure consists only of training a max-
imum entropy text classifier, which we use as our
single dialogue act classifier subsystem.
For each evaluation fold we run the procedure de-
scribed in Section 2, using the classifier obtained
from the corresponding training fold. The segments
present in the manual annotation are then aligned
with the segments identified by our system (the
1Although the dialogue act labels could be thought of as
compositional, since they include separate parts, we treat them
as atomic labels.
97
alignment takes in consideration both the word span
and the dialogue act label associated to each seg-
ment). The evaluation then considers as correct only
the subset of dialogue acts identified automatically
that were successfully aligned with the same dia-
logue act label in the gold-standard annotation.
We compared the performance of our proposed
approach to two baselines; both use the same max-
imum entropy classifier used internally by our pro-
posed approach.
1. The first baseline simply uses the single dia-
logue act label chosen by the maximum entropy
classifier as the only dialogue act for each ut-
terance. In other words, this baseline corre-
sponds to the NLU developed for the SASO di-
alogue system (Traum et al, 2008b) by Sagae
et al (2009)2. This baseline is expected to have
lower recall for those utterances that contain
multiple dialogue acts, but potentially higher
precision overall, since most utterances in the
dataset contain only one dialogue act label.
2. For the second baseline, we treat multiple dia-
logue act detection as a set of binary classifica-
tion tasks, one for each possible dialogue act la-
bel in the domain. We start from the same train-
ing data as above, and create N copies, where
N is the number of unique dialogue acts labels
in the training set. Each utterance-label pair in
the original training set is now present in all N
training sets. If in the original training set an ut-
terance was labeled with the ith dialogue act la-
bel, now it will be labeled as a positive example
in the ith training set and as a negative exam-
ple in all other training sets. Binary classifiers
for each N dialogue act labels are then trained.
During run-time, each utterance is classified by
all N models and the result is the subset of di-
alogue acts associated with the models that la-
beled the example as positive. This baseline is
excepted to be much closer in performance to
our approach, but it is incapable of determining
what words in the utterance correspond to each
dialogue act3.
2We do not use the incremental processing version of the
NLU described by Sagae et al, only the baseline NLU, which
consist only of a maximum entropy classifier.
3This corresponds to the transformation of a multi-label
P [%] R [%] F [%]
Single this 73 77 75
2ndbl 86 71 78
1stbl 82 77 80
Multiple this 87 66 75
2ndbl 85 55 67
1stbl 91 39 55
Overall this 78 72 75
2ndbl 86 64 73
1stbl 84 61 71
Table 2: Performance on the TACQ dataset obtained by
our proposed approach (denoted by ?this?) and the two
baseline methods. Single indicates the performance when
tested only on utterances annotated with a single dialogue
act. Multiple is for utterances annotated with more than
one dialogue act, and Overall indicates the performance
over the entire set. P stands for precision, R for recall,
and F for F-score.
3.3 Results
Table 2 shows the performance of our approach and
the two baselines. All measures show that the pro-
posed approach has considerably improved perfor-
mance for utterances that contain multiple dialogue
acts, with only a small increase in the number of er-
rors for the utterances containing only a single dia-
logue act. In fact, even though more than 70% of
the utterances in the dataset contain only a single di-
alogue act, our approach for segmenting and iden-
tifying multiple dialogue acts increases overall F-
score by about 4% when compared to the first base-
line and by about 2% when compared to the sec-
ond (strong) baseline, which suffers from the addi-
tional deficiency of not identifying what spans cor-
respond to what dialogue acts. The differences in
F-score over the entire dataset (shown in the Over-
all portion of Table 2) are statistically significant
(p < 0.05). As a drawback of our approach, it
is on average 25 times slower than our first base-
line, which is incapable of identifying multiple di-
alogue acts in a utterance4. Our approach is still
about 15% faster than our second baseline, which
classification problem into several binary classifiers, described
as PT4 by Tsoumakas and Katakis (?).
4In our dataset, our method takes on average about 102ms
to process an utterance that was originally labeled with multiple
dialogue acts, and 12ms to process one annotated with a single
dialogue act.
98
0100
200
300
400
500
0 10 20 30 40 50 60 70
Ex
ecu
tio
nt
im
e[
ms
]
Hi
sto
gra
m
(nu
mb
er
of
utt
era
nc
es)
Number of words in input text
this
1stbl
2ndbl
histogram
Figure 1: Execution time in milliseconds of the classifier
with respect to the number of words in the input text.
identifies multiple speech acts, but without segmen-
tation, and with lower F-score. Figure 1 shows the
execution time versus the length of the input text. It
also shows a histogram of utterance lengths in the
dataset, suggesting that our approach is suitable for
most utterances in our dataset, but may be too slow
for some of the longer utterances (with 30 words or
more).
Figure 2 shows the histogram of the average error
(absolute value of word offset) in the start and end
of the dialogue act segmentation. Each dialogue act
identified by Algorithm 1 is associated with a start-
ing and ending index that corresponds to the por-
tion of the input text that has been classified with
the given dialogue act. During the evaluation, we
find the best alignment between the manual annota-
tion and the segmentation we computed. For each
of the aligned pairs (i.e. extracted dialogue act and
dialogue act present in the annotation) we compute
the absolute error between the starting point of the
extracted dialogue act and the starting point of the
paired annotation. We do the same for the ending
point and we average the two error figures. The
result is binned to form the histogram displayed in
figure 2. The figure also shows the average error
and the standard deviation. The largest average er-
ror happens with the data annotated with multiple
dialogue acts. In that case, the extracted segments
have a starting and ending point that in average are
misplaced by about ?2 words.
4 Conclusion
We described a method to segment a given utter-
ance into non-overlapping portions, each associated
0 1 2 3 4 5 6 7 8 9 10
Average error in the starting and ending indexes of each speech act segment
All data: ?=1.07 ?=1.69
Single speech act: ?=0.72 ?=1.12
Multiple speech acts: ?=1.64 ?=2.22
Figure 2: Histogram of the average absolute error in the
two extremes (i.e. start and end) of segments correspond-
ing to the dialogue acts identified in the dataset.
with a dialogue act. The method addresses the prob-
lem that, in development of new scenarios for con-
versational dialogue systems, there is typically not
enough training data covering all or most configu-
rations of how multiple dialogue acts appear in sin-
gle utterances. Our approach requires only labeled
utterances (or utterance segments) corresponding to
a single dialogue act, which tends to be the easiest
type of training data to author and to collect.
We performed an evaluation using existing data
annotated with multiple dialogue acts for each utter-
ance. We showed a significant improvement in over-
all performance compared to two strong baselines.
The main drawback of the proposed approach is the
complexity of the segment optimization that requires
calling the dialogue act classifier O(n2) times with
n representing the length of the input utterance. The
benefit, however, is that having the ability to identify
multiple dialogue acts in utterances takes us one step
closer towards giving users more freedom to express
themselves naturally with dialogue systems.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred. We would also like to thank the anonymous
reviewers for their helpful comments.
99
References
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009. Viability of a simple dialogue
act scheme for a tactical questioning dialogue system.
In DiaHolmia 2009: Proceedings of the 13th Work-
shop on the Semantics and Pragmatics of Dialogue,
page 43?50, Stockholm, Sweden, June.
Oliver Bender, Klaus Macherey, Franz Josef Och, and
Hermann Ney. 2003. Comparison of alignment tem-
plates and maximum entropy models for natural lan-
guage understanding. In Proceedings of the tenth
conference on European chapter of the Association
for Computational Linguistics - Volume 1, EACL ?03,
pages 11?18, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Sudeep Gandhe, David DeVault, Antonio Roque, Bilyana
Martinovski, Ron Artstein, Anton Leuski, Jillian
Gerten, and David R. Traum. 2008. From domain
specification to virtual humans: An integrated ap-
proach to authoring tactical questioning characters.
In Proceedings of Interspeech, Brisbane, Australia,
September.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language
understanding of partial speech recognition results in
dialogue systems. In Short Paper Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT) 2009 conference.
Andreas Stolcke and Elizabeth Shriberg. 1996. Au-
tomatic linguistic segmentation of conversational
speech. In Proc. ICSLP, pages 1005?1008.
David R. Traum, Anton Leuski, Antonio Roque, Sudeep
Gandhe, David DeVault, Jillian Gerten, Susan Robin-
son, and Bilyana Martinovski. 2008a. Natural lan-
guage dialogue architectures for tactical questioning
characters. In Army Science Conference, Florida,
12/2008.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008b. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In IVA, pages 117?130.
100
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 43?51,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Open-domain Commonsense Reasoning Using Discourse Relations from a
Corpus of Weblog Stories
Matt Gerber
Department of Computer Science
Michigan State University
gerberm2@msu.edu
Andrew S. Gordon and Kenji Sagae
Institute for Creative Technologies
University of Southern California
{gordon,sagae}@ict.usc.edu
Abstract
We present a method of extracting open-
domain commonsense knowledge by apply-
ing discourse parsing to a large corpus of per-
sonal stories written by Internet authors. We
demonstrate the use of a linear-time, joint syn-
tax/discourse dependency parser for this pur-
pose, and we show how the extracted dis-
course relations can be used to generate open-
domain textual inferences. Our evaluations
of the discourse parser and inference models
show some success, but also identify a num-
ber of interesting directions for future work.
1 Introduction
The acquisition of open-domain knowledge in sup-
port of commonsense reasoning has long been a
bottleneck within artificial intelligence. Such rea-
soning supports fundamental tasks such as textual
entailment (Giampiccolo et al, 2008), automated
question answering (Clark et al, 2008), and narra-
tive comprehension (Graesser et al, 1994). These
tasks, when conducted in open domains, require vast
amounts of commonsense knowledge pertaining to
states, events, and their causal and temporal relation-
ships. Manually created resources such as FrameNet
(Baker et al, 1998), WordNet (Fellbaum, 1998), and
Cyc (Lenat, 1995) encode many aspects of com-
monsense knowledge; however, coverage of causal
and temporal relationships remains low for many do-
mains.
Gordon and Swanson (2008) argued that the
commonsense tasks of prediction, explanation, and
imagination (collectively called envisionment) can
be supported by knowledge mined from a large cor-
pus of personal stories written by Internet weblog
authors.1 Gordon and Swanson (2008) identified
three primary obstacles to such an approach. First,
stories must be distinguished from other weblog
content (e.g., lists, recipes, and reviews). Second,
stories must be analyzed in order to extract the im-
plicit commonsense knowledge that they contain.
Third, inference mechanisms must be developed that
use the extracted knowledge to perform the core en-
visionment tasks listed above.
In the current paper, we present an approach to
open-domain commonsense inference that addresses
each of the three obstacles identified by Gordon and
Swanson (2008). We built on the work of Gordon
and Swanson (2009), who describe a classification-
based approach to the task of story identification.
The authors? system produced a corpus of approx-
imately one million personal stories, which we used
as a starting point. We applied efficient discourse
parsing techniques to this corpus as a means of ex-
tracting causal and temporal relationships. Further-
more, we developed methods that use the extracted
knowledge to generate textual inferences for de-
scriptions of states and events. This work resulted
in an end-to-end prototype system capable of gen-
erating open-domain, commonsense inferences us-
ing a repository of knowledge extracted from un-
structured weblog text. We focused on identifying
1We follow Gordon and Swanson (2009) in defining a story
to be a ?textual discourse that describes a specific series of
causally related events in the past, spanning a period of time
of minutes, hours, or days, where the author or a close associate
is among the participants.?
43
strengths and weaknesses of the system in an effort
to guide future work.
We structure our presentation as follows: in Sec-
tion 2, we present previous research that has inves-
tigated the use of large web corpora for natural lan-
guage processing (NLP) tasks. In Section 3, we de-
scribe an efficient method of automatically parsing
weblog stories for discourse structure. In Section 4,
we present a set of inference mechanisms that use
the extracted discourse relations to generate open-
domain textual inferences. We conclude, in Section
5, with insights into story-based envisionment that
we hope will guide future work in this area.
2 Related work
Researchers have made many attempts to use the
massive amount of linguistic content created by
users of the World Wide Web. Progress and chal-
lenges in this area have spawned multiple workshops
(e.g., those described by Gurevych and Zesch (2009)
and Evert et al (2008)) that specifically target the
use of content that is collaboratively created by In-
ternet users. Of particular relevance to the present
work is the weblog corpus developed by Burton et
al. (2009), which was used for the data challenge
portion of the International Conference on Weblogs
and Social Media (ICWSM). The ICWSM weblog
corpus (referred to here as Spinn3r) is freely avail-
able and comprises tens of millions of weblog en-
tries posted between August 1st, 2008 and October
1st, 2008.
Gordon et al (2009) describe an approach to
knowledge extraction over the Spinn3r corpus using
techniques described by Schubert and Tong (2003).
In this approach, logical propositions (known as fac-
toids) are constructed via approximate interpreta-
tion of syntactic analyses. As an example, the sys-
tem identified a factoid glossed as ?doors to a room
may be opened?. Gordon et al (2009) found that
the extracted factoids cover roughly half of the fac-
toids present in the corresponding Wikipedia2 arti-
cles. We used a subset of the Spinn3r corpus in
our work, but focused on discourse analyses of en-
tire texts instead of syntactic analyses of single sen-
tences. Our goal was to extract general causal and
temporal propositions instead of the fine-grained
2http://en.wikipedia.org
properties expressed by many factoids extracted by
Gordon et al (2009).
Clark and Harrison (2009) pursued large-scale
extraction of knowledge from text using a syntax-
based approach that was also inspired by the work
of Schubert and Tong (2003). The authors showed
how the extracted knowledge tuples can be used
to improve syntactic parsing and textual entailment
recognition. Bar-Haim et al (2009) present an ef-
ficient method of performing inference with such
knowledge.
Our work is also related to the work of Persing
and Ng (2009), in which the authors developed a
semi-supervised method of identifying the causes of
events described in aviation safety reports. Simi-
larly, our system extracts causal (as well as tem-
poral) knowledge; however, it does this in an open
domain and does not place limitations on the types
of causes to be identified. This greatly increases
the complexity of the inference task, and our results
exhibit a corresponding degradation; however, our
evaluations provide important insights into the task.
3 Discourse parsing a corpus of stories
Gordon and Swanson (2009) developed a super-
vised classification-based approach for identifying
personal stories within the Spinn3r corpus. Their
method achieved 75% precision on the binary task
of predicting story versus non-story on a held-out
subset of the Spinn3r corpus. The extracted ?story
corpus? comprises 960,098 personal stories written
by weblog users. Due to its large size and broad
domain coverage, the story corpus offers unique op-
portunities to NLP researchers. For example, Swan-
son and Gordon (2008) showed how the corpus can
be used to support open-domain collaborative story
writing.3
As described by Gordon and Swanson (2008),
story identification is just the first step towards com-
monsense reasoning using personal stories. We ad-
dressed the second step - knowledge extraction -
by parsing the corpus using a Rhetorical Structure
Theory (Carlson and Marcu, 2001) parser based on
the one described by Sagae (2009). The parser
performs joint syntactic and discourse dependency
3The system (called SayAnything) is available at
http://sayanything.ict.usc.edu
44
parsing using a stack-based, shift-reduce algorithm
with runtime that is linear in the input length. This
lightweight approach is very efficient; however, it
may not be quite as accurate as more complex, chart-
based approaches (e.g., the approach of Charniak
and Johnson (2005) for syntactic parsing).
We trained the discourse parser over the causal
and temporal relations contained in the RST corpus.
Examples of these relations are shown below:
(1) [cause Packages often get buried in the load]
[result and are delivered late.]
(2) [before Three months after she arrived in L.A.]
[after she spent $120 she didn?t have.]
The RST corpus defines many fine-grained rela-
tions that capture causal and temporal properties.
For example, the corpus differentiates between re-
sult and reason for causation and temporal-after and
temporal-before for temporal order. In order to in-
crease the amount of available training data, we col-
lapsed all causal and temporal relations into two
general relations causes and precedes. This step re-
quired normalization of asymmetric relations such
as temporal-before and temporal-after.
To evaluate the discourse parser described above,
we manually annotated 100 randomly selected we-
blog stories from the story corpus produced by Gor-
don and Swanson (2009). For increased efficiency,
we limited our annotation to the generalized causes
and precedes relations described above. We at-
tempted to keep our definitions of these relations
in line with those used by RST. Following previous
discourse annotation efforts, we annotated relations
over clause-level discourse units, permitting rela-
tions between adjacent sentences. In total, we an-
notated 770 instances of causes and 1,009 instances
of precedes.
We experimented with two versions of the RST
parser, one trained on the fine-grained RST rela-
tions and the other trained on the collapsed relations.
At testing time, we automatically mapped the fine-
grained relations to their corresponding causes or
precedes relation. We computed the following ac-
curacy statistics:
Discourse segmentation accuracy For each pre-
dicted discourse unit, we located the reference
discourse unit with the highest overlap. Accu-
racy for the predicted discourse unit is equal to
the percentage word overlap between the refer-
ence and predicted discourse units.
Argument identification accuracy For each dis-
course unit of a predicted discourse relation,
we located the reference discourse unit with the
highest overlap. Accuracy is equal to the per-
centage of times that a reference discourse rela-
tion (of any type) holds between the reference
discourse units that overlap most with the pre-
dicted discourse units.
Argument classification accuracy For the subset
of instances in which a reference discourse re-
lation holds between the units that overlap most
with the predicted discourse units, accuracy is
equal to the percentage of times that the pre-
dicted discourse relation matches the reference
discourse relation.
Complete accuracy For each predicted discourse
relation, accuracy is equal to the percentage
word overlap with a reference discourse rela-
tion of the same type.
Table 1 shows the accuracy results for the fine-
grained and collapsed versions of the RST discourse
parser. As shown in Table 1, the collapsed version
of the discourse parser exhibits higher overall ac-
curacy. Both parsers predicted the causes relation
much more often than the precedes relation, so the
overall scores are biased toward the scores for the
causes relation. For comparison, Sagae (2009) eval-
uated a similar RST parser over the test section of
the RST corpus, obtaining precision of 42.9% and
recall of 46.2% (F1 = 44.5%).
In addition to the automatic evaluation described
above, we also manually assessed the output of the
discourse parsers. One of the authors judged the
correctness of each extracted discourse relation, and
we found that the fine-grained and collapsed ver-
sions of the parser performed equally well with a
precision near 33%; however, throughout our exper-
iments, we observed more desirable discourse seg-
mentation when working with the collapsed version
of the discourse parser. This fact, combined with the
results of the automatic evaluation presented above,
45
Fine-grained RST parser Collapsed RST parser
Accuracy metric causes precedes overall causes precedes overall
Segmentation 36.08 44.20 36.67 44.36 30.13 43.10
Argument identification 25.00 33.33 25.86 26.15 23.08 25.87
Argument classification 66.15 50.00 64.00 79.41 83.33 79.23
Complete 22.20 28.88 22.68 31.26 21.21 30.37
Table 1: RST parser evaluation. All values are percentages.
led us to use the collapsed version of the parser in
all subsequent experiments.
Having developed and evaluated the discourse
parser, we conducted a full discourse parse of the
story corpus, which comprises more than 25 million
sentences split into nearly 1 million weblog entries.
The discourse parser extracted 2.2 million instances
of the causes relation and 220,000 instances of the
precedes relation. As a final step, we indexed the
extracted discourse relations with the Lucene infor-
mation retrieval engine.4 Each discourse unit (two
per discourse relation) is treated as a single docu-
ment, allowing us to query the extracted relations
using information retrieval techniques implemented
in the Lucene toolkit.
4 Generating textual inferences
As mentioned previously, Gordon and Swan-
son (2008) cite three obstacles to performing com-
monsense reasoning using weblog stories. Gordon
and Swanson (2009) addressed the first (story col-
lection). We addressed the second (story analysis)
by developing a discourse parser capable of extract-
ing causal and temporal relations from weblog text
(Section 3). In this section, we present a prelimi-
nary solution to the third problem - reasoning with
the extracted knowledge.
4.1 Inference method
In general, we require an inference method that takes
as input the following things:
1. A description of the state or event of interest.
This is a free-text description of any length.
2. The type of inference to perform, either causal
or temporal.
4Available at http://lucene.apache.org
3. The inference direction, either forward or back-
ward. Forward causal inference produces the
effects of the given state or event. Backward
causal inference produces causes of the given
state or event. Similarly, forward and back-
ward temporal inferences produce subsequent
and preceding states and events, respectively.
As a simple baseline approach, we implemented the
following procedure. First, given a textual input de-
scription d, we query the extracted discourse units
using Lucene?s modified version of the vector space
model over TF-IDF term weights. This produces a
ranked list Rd of discourse units matching the input
description d. We then filterRd, removing discourse
units that are not linked to other discourse units by
the given relation and in the given direction. Each el-
ement of the filtered Rd is thus linked to a discourse
unit that could potentially satisfy the inference re-
quest.
To demonstrate, we perform forward causal infer-
ence using the following input description d:
(3) John traveled the world.
Below, we list the three top-ranked discourse units
that matched d (left-hand side) and their associated
consequents (right-hand side):
1. traveling the world? to murder
2. traveling from around the world to be there ?
even though this crowd was international
3. traveled across the world? to experience it
In a na??ve way, one might simply choose the top-
ranked clause in Rd and select its associated clause
as the answer to the inference request; however, in
the example above, this would incorrectly generate
?to murder? as the effect of John?s traveling (this is
46
more appropriately viewed as the purpose of trav-
eling). The other effect clauses also appear to be
incorrect. This should not come as much of a sur-
prise because the ranking was generated soley from
the match score between the input description and
the causes in Rd, which are quite relevant.
One potential problem with the na??ve selection
method is that it ignores information contained in
the ranked list R?d of clauses that are associated with
the clauses in Rd. In our experiments, we often
observed redundancies in R?d that captured general
properties of the desired inference. Intuitively, con-
tent that is shared across elements ofR?d could repre-
sent the core meaning of the desired inference result.
In what follows, we describe various re-rankings
of R?d using this shared content. For each model
described, the final inference prediction is the top-
ranked element of R?d.
Centroid similarity To approximate the shared
content of discourse units in R?d, we treat each
discourse unit as a vector of TF scores. We then
compute the average vector and re-rank all dis-
course units in R?d based on their cosine simi-
larity with the average vector. This favors infer-
ence results that ?agree? with many alternative
hypotheses.
Description score scaling In this approach, we in-
corporate the score from Rd into the centroid
similarity score, multiplying the two and giving
equal weight to each. This captures the intu-
ition that the top-ranked element of R?d should
represent the general content of the list but
should also be linked to an element of Rd that
bears high similarity to the given state or event
description d.
Log-length scaling When working with the cen-
troid similarity score, we often observed top-
ranked elements of R?d that were only a few
words in length. This was typically the case
when components from sparse TF vectors in
R?d matched well with components from the
centroid vector. Ideally, we would like more
lengthy (but not too long) descriptions. To
achieve this, we multiplied the centroid simi-
larity score by the logarithm of the word length
of the discourse unit in R?d.
Description score/log-length scaling In this ap-
proach, we combine the description score scal-
ing and log-length scaling, multiplying the cen-
troid similarity by both and giving equal weight
to all three factors.
4.2 Evaluating the generated textual inferences
To evaluate the inference re-ranking models de-
scribed above, we automatically generated for-
ward/backward causal and temporal inferences for
five documents (265 sentences) drawn randomly
from the story corpus. For simplicity, we gener-
ated an inference for each sentence in each docu-
ment. Each inference re-ranking model is able to
generate four textual inferences (forward/backward
causal/temporal) for each sentence. In our experi-
ments, we only kept the highest-scoring of the four
inferences generated by a model. One of the authors
then manually evaluated the final predictions for cor-
rectness. This was a subjective process, but it was
guided by the following requirements:
1. The generated inference must increase the lo-
cal coherence of the document. As described
by Graesser et al (1994), readers are typically
required to make inferences about the text that
lead to a coherent understanding thereof. We
required the generated inferences to aid in this
task.
2. The generated inferences must be globally
valid. To demonstrate global validity, consider
the following actual output:
(4) I didn?t even need a jacket (until I got
there).
In Example 4, the system-generated forward
temporal inference is shown in parentheses.
The inference makes sense given its local con-
text; however, it is clear from the surround-
ing discourse (not shown) that a jacket was not
needed at any point in time (it happened to be
a warm day). As a result, this prediction was
tagged as incorrect.
Table 2 presents the results of the evaluation. As
shown in the table, the top-performing models are
those that combine centroid similarity with one or
both of the other re-ranking heuristics.
47
Re-ranking model Inference accuracy (%)
None 10.19
Centroid similarity 12.83
Description score scaling 17.36
Log-length scaling 12.83
Description score/log-length scaling 16.60
Table 2: Inference generation evaluation results.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Confidence-ordered percentage of all 
inferences
In
fe
re
nc
e 
ac
cu
ra
cy None
Centroid similarity
Description score scaling
Log-length scaling
Combined scaling
Figure 1: Inference rate versus accuracy. Values along the x-axis indicate that the top-scoring x% of all inferences
were evaluated. Values along the y-axis indicate the prediction accuracy.
The analysis above demonstrates the relative per-
formance of the models when making inferences for
all sentences; however it is probably the case that
many generated inferences should be rejected due to
their low score. Because the output scores of a single
model can be meaningfully compared across predic-
tions, it is possible to impose a threshold on the in-
ference generation process such that any prediction
scoring at or below the threshold is withheld. We
varied the prediction threshold from zero to a value
sufficiently large that it excluded all predictions for
a model. Doing so demonstrates the trade-off be-
tween making a large number of textual inferences
and making accurate textual inferences. Figure 1
shows the effects of this variable on the re-ranking
models. As shown in Figure 1, the highest infer-
ence accuracy is reached by the re-ranker that com-
bines description score and log-length scaling with
the centroid similarity measure. This accuracy is at-
tained by keeping the top 25% most confident infer-
ences.
5 Conclusions
We have presented an approach to commonsense
reasoning that relies on (1) the availability of a large
corpus of personal weblog stories and (2) the abil-
ity to analyze and perform inference with these sto-
ries. Our current results, although preliminary, sug-
gest novel and important areas of future exploration.
We group our observations according to the last two
problems identified by Gordon and Swanson (2008):
story analysis and envisioning with the analysis re-
sults.
5.1 Story analysis
As in other NLP tasks, we observed significant per-
formance degradation when moving from the train-
ing genre (newswire) to the testing genre (Internet
48
weblog stories). Because our discourse parser relies
heavily on lexical and syntactic features for classi-
fication, and because the distribution of the feature
values varies widely between the two genres, the
performance degradation is to be expected. Recent
techniques in parser adaptation for the Brown corpus
(McClosky et al, 2006) might be usefully applied to
the weblog genre as well.
Our supervised classification-based approach to
discourse parsing could also be improved with ad-
ditional training data. Causal and temporal relations
are instantiated a combined 2,840 times in the RST
corpus, with a large majority of these being causal.
In contrast, the Penn Discourse TreeBank (Prasad et
al., 2008) contains 7,448 training instances of causal
relations and 2,763 training instances of temporal
relations. This represents a significant increase in
the amount of training data over the RST corpus. It
would be informative to compare our current results
with those obtained using a discourse parser trained
on the Penn Discourse TreeBank.
One might also extract causal and temporal rela-
tions using traditional semantic role analysis based
on FrameNet (Baker et al, 1998) or PropBank
(Kingsbury and Palmer, 2003). The former defines a
number of frames related to causation and temporal
order, and roles within the latter could be mapped to
standard thematic roles (e.g., cause) via SemLink.5
5.2 Envisioning with the analysis results
We believe commonsense reasoning based on we-
blog stories can also be improved through more so-
phisticated uses of the extracted discourse relations.
As a first step, it would be beneficial to explore alter-
nate input descriptions. As presented in Section 4.2,
we make textual inferences at the sentence level for
simplicity; however, it might be more reasonable to
make inferences at the clause level, since clauses are
the basis for RST and Penn Discourse TreeBank an-
notation. This could result in the generation of sig-
nificantly more inferences due to multi-clause sen-
tences; thus, more intelligent inference filtering will
be required.
Our models use prediction scores for the tasks
of rejecting inferences and selecting between mul-
tiple candidate inferences (i.e., forward/backward
5Available at http://verbs.colorado.edu/semlink
causal/temporal). Instead of relying on prediction
scores for these tasks, it might be advantageous to
first identify whether or not envisionment should be
performed for a clause, and, if it should, what type
and direction of envisionment would be best. For
example, consider the following sentence:
(5) [clause1 John went to the store] [clause2
because he was hungry].
It would be better - from a local coherence perspec-
tive - to infer the cause of the second clause instead
of the cause of the first. This is due to the fact that a
cause for the first clause is explicitly stated, whereas
a cause for the second clause is not. Inferences made
about the first clause (e.g., that John went to the store
because his dog was hungry), are likely to be unin-
formative or in conflict with explicitly stated infor-
mation.
Example 5 raises the important issue of context,
which we believe needs to be investigated further.
Here, context refers to the discourse that surrounds
the clause or sentence for which the system is at-
tempting to generate a textual inference. The con-
text places a number of constraints on allowable in-
ferences. For example, in addition to content-based
constraints demonstrated in Example 5, the context
limits pronoun usage, entity references, and tense.
Violations of these constraints will reduce local co-
herence.
Finally, the story corpus, with its vast size, is
likely to contain a significant amount of redundancy
for common events and states. Our centroid-based
re-ranking heuristics are inspired by this redun-
dancy, and we expect that aggregation techniques
such as clustering might be of some use when ap-
plied to the corpus as a whole. Having identified
coherent clusters of causes, it might be easier to find
a consequence for a previously unseen cause.
In summary, we have presented preliminary re-
search into the task of using a large, collaboratively
constructed corpus as a commonsense knowledge
repository. Rather than relying on hand-coded on-
tologies and event schemas, our approach relies on
the implicit knowledge contained in written natu-
ral language. We have demonstrated the feasibility
of obtaining the discourse structure of such a cor-
pus via linear-time parsing models. Furthermore,
49
we have introduced inference procedures that are ca-
pable of generating open-domain textual inferences
from the extracted knowledge. Our evaluation re-
sults suggest many opportunities for future work in
this area.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their helpful comments and sugges-
tions. The project or effort described here has
been sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDECOM).
Statements and opinions expressed do not necessar-
ily reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Christian Boitet
and PeteWhitelock, editors, Proceedings of the Thirty-
Sixth Annual Meeting of the Association for Computa-
tional Linguistics and Seventeenth International Con-
ference on Computational Linguistics, pages 86?90,
San Francisco, California. MorganKaufmann Publish-
ers.
Roy Bar-Haim, Jonathan Berant, and Ido Dagan. 2009.
A compact forest for scalable inference over entail-
ment and paraphrase rules. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1056?1065, Singapore, Au-
gust. Association for Computational Linguistics.
K. Burton, A. Java, and I. Soboroff. 2009. The icwsm
2009 spinn3r dataset. In Proceedings of the Third An-
nual Conference on Weblogs and Social Media.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, ISI, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Peter Clark and Phil Harrison. 2009. Large-scale extrac-
tion and use of knowledge from text. In K-CAP ?09:
Proceedings of the fifth international conference on
Knowledge capture, pages 153?160, New York, NY,
USA. ACM.
Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, Phil
Harrison, William R. Murray, and John Thompson.
2008. Augmenting WordNet for Deep Understanding
of Text. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, volume 1 of Research in Computational
Semantics, pages 45?57. College Publications.
Stefan Evert, Adam Kilgarriff, and Serge Sharoff, edi-
tors. 2008. 4th Web as Corpus Workshop Can we beat
Google?
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communi-
cation). The MIT Press, May.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2008. The
fourth fascal recognizing textual entailment challenge.
In Proceedings of the First Text Analysis Conference.
Andrew Gordon and Reid Swanson. 2008. Envision-
ing with weblogs. In International Conference on New
Media Technology.
Andrew Gordon and Reid Swanson. 2009. Identifying
personal stories in millions of weblog entries. In Third
International Conference on Weblogs and Social Me-
dia.
Jonathan Gordon, Benjamin Van Durme, and Lenhart
Schubert. 2009. Weblogs as a source for extracting
general world knowledge. In K-CAP ?09: Proceed-
ings of the fifth international conference on Knowledge
capture, pages 185?186, New York, NY, USA. ACM.
A. C. Graesser, M. Singer, and T. Trabasso. 1994. Con-
structing inferences during narrative text comprehen-
sion. Psychological Review, 101:371?395.
Iryna Gurevych and Torsten Zesch, editors. 2009. The
Peoples Web Meets NLP: Collaboratively Constructed
Semantic Resources.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the
next level of treebank. In Proceedings of Treebanks
and Lexical Theories.
Douglas B. Lenat. 1995. Cyc: a large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL-44: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 337?344, Morristown, NJ,
USA. Association for Computational Linguistics.
Isaac Persing and Vincent Ng. 2009. Semi-supervised
cause identification from aviation safety reports. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 843?851, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-
sakaki, Geraud Campion, Aravind Joshi, and Bonnie
50
Webber. 2008. Penn discourse treebank version 2.0.
Linguistic Data Consortium, February.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT?09), pages
81?84, Paris, France, October. Association for Com-
putational Linguistics.
Lenhart Schubert and Matthew Tong. 2003. Extract-
ing and evaluating general world knowledge from the
brown corpus. In Proceedings of the HLT-NAACL
2003 workshop on Text meaning, pages 7?13, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Reid Swanson and AndrewGordon. 2008. Say anything:
A massively collaborative open domain story writing
companion. In First International Conference on In-
teractive Digital Storytelling.
51
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 37?44,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Self-Training without Reranking for Parser Domain Adaptation and Its Impact on Semantic Role Labeling 
 Kenji Sagae Institute for Creative Technologies University of Southern California Marina del Rey, CA 90292 sagae@ict.usc.edu      Abstract We compare self-training with and with-out reranking for parser domain adapta-tion, and examine the impact of syntactic parser adaptation on a semantic role la-beling system.  Although self-training without reranking has been found not to improve in-domain accuracy for parsers trained on the WSJ Penn Treebank, we show that it is surprisingly effective for parser domain adaptation.  We also show that simple self-training of a syntactic parser improves out-of-domain accuracy of a semantic role labeler. 1 Introduction Improvements in data-driven parsing approaches, coupled with the development of treebanks that serve as training data, have resulted in accurate parsers for several languages.  However, port-ability across domains remains a challenge: pars-ers trained using a treebank for a specific domain generally perform comparatively poorly in other domains.  In English, the most widely used train-ing set for parsers comes from the Wall Street Journal portion of the Penn Treebank (Marcus et al, 1993), and constituent parsers trained on this set are now capable of labeled bracketing preci-sion and recall of over 90% (Charniak and John-son, 2005; Huang, 2008) on WSJ testing sen-tences.  When applied without adaptation to the Brown portion of the Penn Treebank, however, an absolute drop of over 5% in precision and re-call is typically observed (McClosky et al, 2006b).  In pipelined NLP applications that in-clude a parser, this drop often results in severely degraded results downstream. 
We present experiments with a simple self-training approach to semi-supervised parser do-main adaptation that produce results that contra-dict the commonly held assumption that im-proved parser accuracy cannot be obtained by self-training a generative parser without rerank-ing (Charniak, 1997; Steedman et al, 2003; McClosky et al, 2006b, 2008).1  We compare this simple self-training approach to the self-training with reranking approach proposed by McClosky et al (2006b), and show that although McClosky et al?s approach produces better la-beled bracketing precision and recall on out-of-domain sentences, higher F-score on syntactic parses may not lead to an overall improvement in results obtained in NLP applications that include parsing, contrary to our expectations.  This is evidenced by results obtained when different ad-aptation approaches are applied to a parser that serves as a component in a semantic role labeling (SRL) system.  This is, to our knowledge, the first attempt to quantify the benefits of semi-supervised parser domain adaptation in semantic role labeling, a task in which parsing accuracy is crucial. 2 Semi-supervised parser domain adap-tation with self-training Because treebanks are expensive to create, while plain text in most domains is easily obtainable, semi-supervised approaches to parser domain adaptation are a particularly attractive solution to the domain portability problem.  This usually involves a manually annotated training set (a                                                 1 Reichart and Rappoport (2007) show that self-training without reranking is effective when the manually annotated training set is small.  We show that this is true even for a large training set (the standard WSJ Penn Treebank training set, with over 40k sentences). 
37
treebank), and a larger set of unlabeled data (plain text).   Bacchiani and Roark (2003) obtained positive results in unsupervised domain adaptation of language models by using a speech recognition system with an out-of-domain language model to produce an automatically annotated training cor-pus that is used to adapt the language model us-ing a maximum a posteriori (MAP) adaptation strategy.  In subsequent work (Roark and Bac-chiani, 2003), this MAP adaptation approach was applied to PCFG adaptation, where an out-of-domain parser was used to annotate an in-domain corpus automatically with multiple candidate trees per sentence.  A substantial improvement was achieved in out-of-domain parsing, although the obtained accuracy level was still far below that obtained with domain-specific training data. More recent work in unsupervised domain ad-aptation for state-of-the-art parsers has achieved accuracy levels on out-of-domain text that is comparable to that achieved with domain-specific training data (McClosky et al, 2006b).  This is done in a self-training setting, where a parser trained on a treebank (in a seed domain) is used to parse a large amount of unlabeled data in the target domain (assigning only one parse per sentence).  The automatically parsed corpus is then used as additional training data for the parser.  Although initial attempts to improve in-domain parsing accuracy with self-training were unsuccessful (Charniak, 1997; Steedman et al, 2003), recent work has shown that self-training can work in specific conditions (McClosky et al, 2006b), and in particular it can be used to im-prove parsing accuracy on out-of-domain text (Reichart and Rappoport, 2007). 2.1 Self-training with reraking McClosky et al (2006b) presented the most suc-cessful semi-supervised approach to date for ad-aptation of a WSJ-trained parser to Brown data containing several genres of text (such as relig-ion, mystery, romance, adventure, etc.), obtain-ing a substantial accuracy improvement using only unlabeled data.  Their approach involves the use of a first-stage n-best parser and a reranker, which together produce parses for the unlabeled dataset.  The automatically parsed in-domain corpus is then used as additional training mate-rial.  In light of previous failed attempts to im-prove generative parsers through self-training (Charniak, 1997; Steedman et al, 2003), McClosky et al (2006a) argue that the use of a reranker is an important factor in the success of 
their approach.  That work used text from the LA Times (taken from the North American News Corpus, or NANC), which is presumably more similar to the parser?s training material than to text in the Brown corpus, and resulted not only in an improvement of parser accuracy on out-of-domain text (from the Brown corpus), but also in an improvement in accuracy on in-domain text (the standard WSJ test set of the Penn Treebank). It can be argued that the McClosky et al ap-proach is not a pure instance of self-training, since two parsing models are used: the first-stage generative model, and a discriminative model for reranking.  The generative parser is improved based on the output of the discriminative model, but McClosky et al found that the discriminative model does not improve when retrained with its own output. 2.2 Self-training without reraking Although there have been instances of self-training (or similar) approaches that produced improved parser accuracy without reranking, the success of these efforts are often attributed to other specific factors. Reichart and Rappoport (2007) obtained posi-tive results in in-domain and out-of-domain sce-narios with self-training without reranking, but under the constant condition that only a rela-tively small set of manually labeled data is used as the seed training set.  Sagae and Tsujii (2007) improved the out-of-domain accuracy of a de-pendency parser trained on the entire WSJ train-ing set (40k sentences) by using unlabeled data in the same domain as the out-of-domain test data (biomedical text).  However, they used agreement between different parsers to estimate the quality of automatically generated training instances and selected only sentences with high estimated accuracy.  Although the parser im-proves when trained with its own output, the training instances are selected through the use of a separate dependency parsing model. 2.3 Simple self-training without reranking for domain adaptation It is now commonly assumed that the simplest form of self-training, where a single parsing model is retrained with its own output (a single parse tree per sentence, without reranking or other means of training instance selection or es-timation of parse quality), does not improve the 
38
model?s accuracy.2  This assumption, however, is largely based on previous attempts to improve in-domain accuracy through self-training (Steedman et al, 2003; Charniak, 1997; McClosky et al, 2006a, 2008).  We will refer to this type of self-training as simple self-training, to avoid confusion with other self-training set-tings, such as McClosky et al?s, where a reranker is involved. We propose a simple self-training framework for domain adaptation, as follows:  1. A generative parser is trained using a tree-bank in a specific source domain. 2. The parser is used to generate parse trees from text in a target domain, different from the source domain. 3. The parser is retrained using the original treebank, augmented with the parse trees generated in step 2. There are intuitive reasons that may lead one to assume that simple self-training should not work.  One is that no additional information is provided to the model.  In self-training with reranking, the generative model can be enriched with information produced by the discriminative model.  When two parsers are used for training instance selection, one parser informs the other.  In simple self-training, however, there is no addi-tional source of syntactic knowledge with which the self-trained model would be enriched. Another possible reason is that the output of the self-trained parser should be expected to in-clude the same errors found in the automatically generated training material.  If the initial parser has poor accuracy on the target domain, the training data it generates will be of poor quality, resulting in no improvement in the resulting trained model.  The self-trained model may sim-ply learn to make the same mistakes as the origi-nal model. Conversely, there are also intuitive reasons for why it might work.  A possible source of poor performance in new domains is that the model lacks coverage.  Specific lexical items and syn-tactic structures in a new domain appear in a va-riety of contexts, accompanied by different words and structures.  The parser trained on the source domain may analyze some of these new                                                 2 Except for in cases where the initial model is trained using a very small treebank. 
items and structures correctly, and it may also make mistakes.  As long as errors in the auto-matically generated training material are not all systematic, the benefits of adding target-domain information could outweigh the addition of noise in the model. Naturally, it may be that these conditions hold for some pairs of source and target domains but not others.  In the next section, we present ex-periments that investigate whether simple self-training is effective for one particular set of train-ing (WSJ) and testing (Brown) corpora, which are widely used in parsing research for English. 3 Domain adaptations experiments In our experiments we use primarily the Charniak (2000) parser.  In a few specific ex-periments we also use the Charniak and Johnson (2005) reranker; such cases are noted explicitly and are not central to the paper, serving mostly for comparisons.  We follow the three steps de-scribed in section 2.3.  The manually labeled training corpus is the standard WSJ training sec-tions of the Penn Treebank (sections 02 to 21).  Sections 22 and 23 are used as in-domain devel-opment and testing sets, respectively.  The out-of-domain material is taken from the Brown por-tion of the Penn Treebank.  We use the same Brown test set as McClosky et al (2006b), every tenth sentence in the corpus.  Another tenth of the corpus is used as a development set, and the rest of the Brown corpus is not used.  The out-of-domain text then contains not one but several genres of text.  The larger set of unlabeled data is composed of approximately 5.3 million words (320k sentences) of 20th century novels available from Project Gutenberg3, which do not match exactly the target domain, but is closer to it in general than to the source domain (WSJ).  3.1 Simple self-training results The precision, recall and F-score of labeled brackets of the initial parser, trained only on the WSJ Penn Treebank, are shown in the first row of results in Table 1 for the WSJ (in-domain) test set and the Brown (out-of-domain) test set.  These figures serve as our baseline.  The second row of results in Table 1 shows the results ob-tained with a model produced using simple self-training.  The baseline model is used to parse the entire unlabeled dataset (320k sentences), and 
                                                3 http://www.gutenberg.org 
39
the resulting parse trees are added to the WSJ training set to produce the self-trained model.   A substantial improvement is observed for the target test set (Brown), close to an absolute im-provement of 2% in precision, recall and F-score. Table 1 also shows that parser accuracy fell by 1% on WSJ.  Although we do not see this as a problem, since the our goal is to produce an im-proved model for parsing Brown, it is interesting that, unlike in the work of McClosky et al (2006a, 2006b) where self-training includes reranking, simple self-training is effective spe-cifically for domain adaptation, but not for im-proving the accuracy of the parser on in-domain data.  At least in this case, simple self-training does not result in an absolutely improved parsing model (as appears to be the case with McClosky et al?s self-training), although it does result in an improved model for the target data. Finally, the last row in Table 1 shows the re-sults on WSJ and Brown obtained by McClosky et al (2006a, 2006b) using self-training with reranking.  As they have shown, the discrimina-tive reranker can be used to provide further im-provements, as discussed in the next subsection. Unlike McClosky et al (2006a), we did not give different weights to the original and auto-matically generated training instances.  In our experiments with the Brown development data, varying the weight of the gold-standard WSJ training data from 1 to 7, we observed only small differences in F-score (Table 2).  The highest F-score, obtained when the WSJ training corpus is given a relative weight of 3, was only 0.07 higher than the F-score obtained when the WSJ training corpus is given a relative weight of 1. 
 WSJ relative weight Brown dev F-score 1 84.51 2 84.52 3 84.58 4 84.53 5 84.51 6 84.55 7 84.57 Baseline (WSJ only) 82.91 Table 2: Brown development set F-scores ob-tained with self-trained models with different relative weights given to the gold-standard WSJ training data.  The last row shows the F-score for the original model (without adaptation).   Table 3 shows results on the Brown develop-ment set when different amounts of unlabeled data are used to create the self-trained model.  Although F-score generally increases with more unlabeled data, the effect is not monotonic.  McClosky et al observed a similar effect in their self-training experiments, and hypothesized that this may be due to differences between portions of the unlabeled data and the target corpus, and to varying parsing difficulty in portions of the unlabeled data, which results in varying quality of the parse trees produced automatically for training.  A large improvement in F-score over the baseline is observed when adding only 30k sentences.  Additional improvement is observed when additional sentences are added, but these are small in comparison.  One interesting note is 
 WSJ Brown  Precision Recall F-score Precision Recall F-score Baseline 89.49 88.78 89.13 83.93 83.19 83.56 Self-trained 88.26 87.86 88.06 85.78 85.05 85.42 MCJ   91.0   87.1  Table 1. Labeled constituent precision, recall and F-score for the WSJ and Brown test sets, ob-tained with the baseline model (trained only on the WSJ training set) and with the self-trained model.  Results on Brown show an absolute improvement of almost 2%, while results on WSJ show a drop of about 1%.  The last row shows the results obtained by McClosky et al (2006a, 2006b) using self-training with reranking (denoted as MCJ), for comparison purposes. 
40
that, although self-training produced improved bracketing precision and recall, part-of-speech tagging accuracy of Brown remained largely un-changed from the baseline, in the range of 94.42% to 94.50% accuracy.  It is possible that separate adaption for part-of-speech tagging may improve parsing F-score further. The results in this section show that simple self-training is effective in adapting WSJ-trained parser to Brown, but more experiments are needed to determine if the same effects observed in our simple self-training experiments would also be observed with other pairs of seed training data and target datasets, and what characteristics of the datasets may affect domain adaptation. 3.2 Self-training with reranking results To provide a more informative comparison be-tween the results obtained with simple self-training and other work, we also performed McClosky et al?s self-training with reranking using our unlabeled dataset.  In this experiment, intended to provide a better understanding of the role of the unlabeled data (20th century novels vs. LA Times articles), we parse the unlabeled dataset with the Charniak (2000) parser and the Charniak and Johnson (2005) discriminative reranker to produce additional training material for the generative parser.  The resulting genera-tive parser produces slightly improved F-scores compared to the simple self-training setting (88.78% on WSJ and 86.01 on Brown), although a slight drop in WSJ F-score is still observed, indicating that the use of news text is likely an 
important factor in McClosky et al?s superior F-score figures. All of these models can be used to produce n-best parses with the Charniak parser, and these can be reranked with the Charniak and Johnson reranker, whether or not the self-training proce-dure that created the generative model involved reranking.  McClosky et al found that although their self-training procedure involves reranking, the gains in accuracy are orthogonal to those provided by a final reranking step, applied to the output of the self-trained model.   As in their case, applying the WSJ-trained reranker to our self-trained model improves its accuracy.  In the case of our simple self-trained model, the im-provement is of about 1.7%, which means that if a reranker is used at run-time (but not during self-training), F-score goes up to 87.12%.  Inter-estingly, applying a final pass of reranking to the model obtained with self-training with reranking brings F-score up only by less than 1.2%, to 87.17%.  So at least in our case, improvements provided by the use the reranker appear not to be completely orthogonal. 4 Semantic Role Labeling with syntactic parser adaptation To investigate the impact of parser domain adap-tation through self-training on applications that depend on parser output, we use an existing se-mantic role labeling (SRL) system, the Illinois Semantic Role Labeler4, replacing the provided parsing component with our (WSJ) baseline and (adapted) self-trained parsers. We tested the SRL system using the datasets of the CoNLL 2005 shared task (Carreras and M?rquez, 2005).  The system is trained on the WSJ domain using PropBank (Palmer et al 2005), and the shared task includes WSJ and Brown evaluation sets.  Using the baseline WSJ syntactic parser, the SRL system has an F-score of 77.49 on WSJ, which is a competitive result for systems using a single syntactic analysis per sentence.  The highest scoring system (also a UIUC system) in the shared task has 79.44 F-score, and used multiple parse trees, which has been shown to improve results (Punyakanok et al, 2005).  On the Brown evaluation, F-score is 64.75, a steep drop from the performance of the system on WSJ, which reflects that not just the syntactic parser, but also other system compo-nents, were trained with WSJ material.  The                                                 4 http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=SRL 
Sentences added Brown dev. F-score 0 (baseline) 82.91 10k 83.76 20k 84.02 30k 84.29 50k 84.26 100k 84.19 150k 84.38 200k 84.51 250k 84.42 300k 84.51 Table 3: Brown development set F-scores obtained with self-trained models created with different amounts of unlabeled data. 
41
highest scoring system on the Brown evaluation in the CoNLL 2005 shared task had 67.75 F-score. Table 4 shows the results on the Brown evaluation set using the baseline WSJ SRL sys-tem and the results obtained under three self-training parser domain adaptation schemes: sim-ple self-training using novels as unlabeled data (section 3.1), the self-trained model of McClosky et al5, and the reranked results of the McClosky et al self-trained model (which has F-score com-parable to that of a parser trained on the Brown corpus). As expected, the contributions of the three adapted parsing models allowed the system to produce overall SRL results that are better than those produced with the baseline setting.  Sur-prisingly, however, the use of the model created using simple self-training and sentences from novels (sections 2.3 and 3.1) resulted in better SRL results than the use of McClosky et al?s reranking-based self-trained model (whether its results go through one additional step of rerank-ing or not), which produces substantially higher syntactic parsing F-score.  Our self-trained pars-ing model results in an absolute increase of 4% in SRL F-score, outscoring all participants in the shared task (of course, systems in the shared task did not use adapted parsing models or external resources, such as unlabeled data). The im-provement in the precision of the SRL system                                                 5 http://www.cs.brown.edu/~dmcc/selftraining.html 
using simple self-training is particularly large.  Improvements in the precision of the core argu-ments Arg0, Arg1, Arg2 contributed heavily to the improvement of overall scores.  We note that other parts of the SRL system remained constant, and the difference in the re-sults shown in Table 4 come solely from the use of different (adapted) parsers. 5 Conclusion We explored the use of simple self-training, where no reranking or confidence measurements are used, for parser domain adaptation.  We found that self-training can in fact improve the accuracy of a parser in a different domain from the domain of its training data (even when the training data is the entire standard WSJ training material from the Penn Treebank), and that this improvement can be carried on to modules that may use the output of the parser.  We demon-strated that a semantic role labeling system trained with WSJ training data can improve sub-stantially (4%) on Brown just by having its parser be adapted using unlabeled data. Although the fact that self-training produces improved parsing results without reranking does not necessarily conflict with previous work, it does contradict the widely held assumption that this type of self-training does not improve parser accuracy.  One way to reconcile expectations based on previous attempts to improve parsing accuracy with self-training (Charniak, 1997; 
 Precision Recall F-score Baseline (WSJ parser) 66.57 63.02 64.75 
Simple self-trained parser (this paper) 71.66 66.10 68.77 MCJ self-trained parser 69.18 65.37 67.22 
MCJ self-train and rerank 68.62 65.78 67.17 
 Table 4. Semantic role labeling results using the Illinois Semantic Role Labeler (trained on WSJ material from PropBank) using four different parsing models: (1) a model trained on WSJ, (2) a model built from the WSJ training data and 320k sentences from novels as unla-beled data, using the simple self-training procedure described in sections 2.3 and 3.1, (3) the McClosky et al (2006a) self-trained model, and (4) the McClosky et al self-trained model, reranked with the Charniak and Johnson (2005) reranker. 
42
Steedman et al, 2003) and the results observed in our experiments is that we focus specifically on domain adaptation.  In fact, the in-domain accuracy of our adapted model is slightly inferior to that of the baseline, more in line with previous findings. This work represents only one additional step towards understanding of how and when self-training works for parsing and for domain adap-tation.  Additional analysis and experiments are needed to understand under what conditions and in what domains simple self-training can be ef-fective.   One question that seems particularly interest-ing is why the models adapted using self-training with reranking and news text, which produce substantially higher parsing F-scores, did not outperform our model built with simple self-training in contribution to the SRL system.  Al-though we do not have an answer to this ques-tion, two factors that may play a role are the do-main of the training data and the use of the reranker, which may provide improvements in parse quality that are of a different kind of those most needed by the SRL system.  This points to another interesting direction, where adapted parsers can be combined.  Having different ways to perform semi-supervised parser adaptation may result in the creation of adapted models with improved accuracy on a target domain but differ-ent characteristics.  The output of these parsers could then be combined in a voting scheme (Henderson and Brill, 1999) for additional im-provements on the target domain. Acknowledgments We thank Andrew S. Gordon and the anonymous reviewers for valuable comments and sugges-tions.  The work described here has been spon-sored by the U.S. Army Research, Development, and Engineering Command (RDECOM).  State-ments and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred. References Michiel Bacchiani and Brian Roark. 2003. Unsuper-vised language model adaptation. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). Xavier Carreras and Llu?s M?rquez. 2005. Introduc-tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of the CoNLL 2005 shared task. Eugene Charniak. 1997. Statistical parsing with a con-text-free grammar and word statistics. In Proceed-ings of AAAI, pages 598?603. Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the First Meet-ing of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL). Pages 132-139. Seattle, WA. Eugene Charniak and Mark Johnson. 2005. Coarse- to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 2005 Meeting of the Association for Computational Linguistics (ACL), pages 173?180.  John C. Henderson, Eric Brill. 1999. Exploiting Di-versity in Natural Language Processing: Combin-ing Parsers. In Proceedings of the Fourth Confer-ence on Empirical Methods in Natural Language Processing (EMNLP-99), pp. 187?194. College Park, Maryland. Liang Huang (2008). Forest Reranking: Discrimina-tive Parsing with Non-Local Features.  In Proceed-ings of the 2008 Meeting of the Association for Computational Linguistics (ACL). Columbus, OH. Mitchell P. Marcus, Mary Ann Marcinkiewicz and Beatrice Santorini. 1993. Building a large anno-tated corpus of English: the Penn Treebank. In Computational Linguistics 19(2), 313-330.  David McClosky, Eugene Charniak and Mark John-son. 2006a. Effective self-training for parsing. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computa-tional Linguistics (NAACL).  New York, NY. David McClosky, Eugene Charniak and Mark John-son. 2006b. Reranking and self-training for parser adaptation. In Proceedings of the 21st international Conference on Computational Linguistics and the 44th Annual Meeting of the Association For Com-putational Linguistics (ACL).  Sydney, Australia. David McClosky, Eugene Charniak and Mark John-son. 2008. When is self-training effective for pars-ing? In Proceedings of the 22nd international Con-ference on Computational Linguistics (COLING) - Volume 1. Manchester, United Kingdom. Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).  Vasin Punyakanok, Peter Koomen, Dan Roth and Wen-tau Yih. 2005. Generalized Inference with Multiple Semantic Role Labeling Systems.  In Pro-ceedings of the CoNLL 2005 shared task. 
43
Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statisti-cal parsers trained on small datasets.  In Proceed-ings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL). Pages 616-623.  Prague, Czech Republic. Brian Roark and Michiel Bacchiani. 2003. Supervised and unsupervised PCFG adaptation to novel do-mains. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology ? Volume 1 (NAACL-HLT). Kenji Sagae and Jun?ichi Tsujii. 2007.  Multilingual dependency parsing and domain adaptation with data-driven LR models and parser ensembles.  In Proceedings of the CoNLL 2007 shared task. Pra-gue, Czech Republic. Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Osborne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen, Steven Baker and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of Tenth Conference of the European Chapter of the Association for Computa-tional Linguistics (EACL) ? Volume 1. Budapest, Hungary. 
44
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 39?48,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Toward Learning and Evaluation of Dialogue Policies with Text Examples
David DeVault and Anton Leuski and Kenji Sagae
Institute for Creative Technologies
University of Southern California
Playa Vista, CA 90094
{devault,leuski,sagae}@ict.usc.edu
Abstract
We present a dialogue collection and enrich-
ment framework that is designed to explore
the learning and evaluation of dialogue poli-
cies for simple conversational characters us-
ing textual training data. To facilitate learning
and evaluation, our framework enriches a col-
lection of role-play dialogues with additional
training data, including paraphrases of user ut-
terances, and multiple independent judgments
by external referees about the best policy re-
sponse for the character at each point. As
a case study, we use this framework to train
a policy for a limited domain tactical ques-
tioning character, reaching promising perfor-
mance. We also introduce an automatic policy
evaluation metric that recognizes the validity
of multiple conversational responses at each
point in a dialogue. We use this metric to ex-
plore the variability in human opinion about
optimal policy decisions, and to automatically
evaluate several learned policies in our exam-
ple domain.
1 Introduction
There is a large class of potential users of dialogue
systems technology who lack the background for
many of the formal modeling tasks that typically
are required in the construction of a dialogue sys-
tem. The problematic steps include annotating the
meaning of user utterances in some semantic formal-
ism, developing a formal representation of informa-
tion state, writing detailed rules that govern dialogue
management, and annotating the meaning of system
utterances in support of language generation, among
other tasks.
In this paper, we explore data collection and ma-
chine learning techniques that enable the implemen-
tation of domain-specific conversational dialogue
policies through a relatively small data collection ef-
fort, and without any formal modeling. We present
a case study, which serves to illustrate some of
the possibilities in our framework. In contrast to
recent work on data-driven dialogue policy learn-
ing that learns dialogue behavior from existing data
sources (Gandhe and Traum, 2007; Jafarpour et al,
2009; Ritter et al, 2010), we address the task of au-
thoring a dialogue policy from scratch with a spe-
cific purpose, task and scenario in mind. We exam-
ine the data collection, learning and evaluation steps.
The contributions of this work include a data col-
lection and enrichment framework without formal
modeling, and the creation of dialogue policies from
the collected data. We also propose a framework for
evaluating learned policies. We show, for the sce-
nario in our case study, that these techniques deliver
promising levels of performance, and point to possi-
ble future developments in data-driven dialogue pol-
icy creation and evaluation.
2 Case study
For our case study we selected an existing dialogue
system scenario designed for Tactical Questioning
training (Traum et al, 2008). The character targeted
in our study, Amani, is modeled closely after the
Amani Tactical Questioning character described by
Gandhe et al (2009) and Artstein et al (2009). Tac-
tical Questioning dialogues are those in which small
unit military personnel, usually on patrol, hold con-
versations with individuals to produce information
of military value. A tactical questioning dialogue
39
system is a simulation training environment where
virtual characters play the role of a person being
questioned. Tactical questioning characters are de-
signed to be non-cooperative at times. They may
answer some of the interviewers questions in a coop-
erative manner, but may refuse to answer other ques-
tions, or intentionally provide incorrect answers.
Therefore the interviewer is encouraged to conduct
the interview in a manner that induces cooperation
from the character: building rapport with the char-
acter, addressing their concerns, making promises
and offers, as well as threatening or intimidating the
character; the purpose of the dialogue system is to
allow trainees to practice these strategies in a realis-
tic setting (Gandhe et al, 2009).
This type of scenario is a good testbed for our
proposed learning and evaluation framework, since
it involves both flexible conversational choices and
well-defined constraints regarding the disclosure of
specific information. In the Amani scenario, the user
plays the role of a commander of a small military
unit in Iraq whose unit had been attacked by sniper
fire. The user interviews a character named Amani
who was a witness to the incident and is thought to
have some information about the identity of the at-
tackers. Amani is willing to tell the interviewer ev-
erything she knows provided that the user promises
her safety, secrecy, and small monetary compensa-
tion for the information (Artstein et al, 2009).
An exhaustive formal definition of Amani?s ideal
dialogue policy might include a large number of
rules covering a wide range of user utterance types.
The key constraints for the training simulation, how-
ever, can be stated simply with a few rules governing
the release of five pieces of information that Amani
knows. Amani will only reveal one of these pieces of
information if a precondition is met. Table 1 shows
how certain information relates to each of the pre-
conditions in Amani?s dialogue policy. Amani can
only reveal a fact from the first column if the user
promised her an item from the second column. For
example, Amani can only tell the user the shooter?s
name if the user promised her safety. If the user
has not promised safety, Amani will ask him for
safety. If the user refuses to promise safety, Amani
will either decline to answer the question or lie to
the interviewer. Amani does keep track of the user?s
promises and once she is promised safety, she would
information precondition
about shooter?s name safety
about shooter?s description safety
about shooter?s location secrecy
about the occupant of the shop secrecy
about shooter?s daily routine money
Table 1: Amani?s dialogue policy.
not ask for it again.
While the key constraints for Amani?s policy, as
summarized in Table 1, may be easily expressed
in terms of rules involving dialogue-acts, the rest
of Amani?s behavior is more open-ended and un-
derspecified. Ideally, the system designers would
like for the character to obey conversational conven-
tions (such as responding appropriately to greetings,
thankings, etc.). Her responses to other user utter-
ances should match human intuition about what a
good response would be, but specific responses are
not generally dictated by the goals for the training
simulation. There is therefore room for some flex-
ibility, and also for the character to reply that she
does not understand. Of course, her conversational
repertoire is inevitably limited by the available au-
thoring and development effort as well as language
processing challenges.
3 Data collection
The exponential number of possible utterances and
dialogue paths in even a simple conversational dia-
logue scenario such as the Amani scenario suggests
that learning acceptable dialogue behavior from sur-
face text examples without annotation or formal
modeling would require a seemingly insurmount-
able quantity of dialogues to serve as training data.
We address this problem in a data collection frame-
work with four main characteristics: (1) we sidestep
the problem of learning natural language generation
by using a fixed predefined set of utterances for the
Amani character. This so-called ?utterance selec-
tion? approach has been used in a number of dia-
logue systems (Zukerman and Marom, 2006; Sell-
berg and Jnsson, 2008; Kenny et al, 2007, for ex-
ample) and often serves as a reasonable approxima-
tion to generation (Gandhe and Traum, 2010); (2)
we collect dialogues from human participants who
40
play the parts of Amani and the commander in a
structured role play framework (Section 3.1); (3) we
enrich the dialogues collected in the structured role
play step with additional paraphrases for the utter-
ances of the commander, in an attempt to deal with
large variability of natural language input, even for
a limited domain conversational dialogue scenario
(Section 3.2); (4) we further augment the existing
dialogue data by adding acceptable alternatives to
the dialogue acts of the Amani role through the use
of external referees (Section 3.3).
Our data collection procedure is designed to cap-
ture the necessary information for learning dialogue
policies and evaluating their quality by approxi-
mating the exponentially large dialogue variability
while keeping the data collection effort tractable.
3.1 Structured role play
To examine the hypothesis that dialogue policies
such as Amani?s can be learned from examples with-
out explicit rules or any kind of formal modeling,
we collected dialogue data through a constrained
form of role play, which we call structured role play,
where the person playing the role of Amani is en-
couraged, whenever possible, to only use utterances
from a fixed set. Each utterance in the available set
of Amani replies corresponds roughly to one of the
dialogue acts (consisting of an illocutionary force
and some semantic content) described by Artstein et
al. (2009) for their version of the Amani character.
The players in the roles of Amani and the com-
mander take turns producing one utterance at a time,
each in a separate terminal. The commander player,
who receives a natural language description of the
scenario and the goal of the commander, enters utter-
ances through a teletype (chat) interface. The Amani
player, who receives a natural language descrip-
tion of the scenario and of Amani?s dialogue policy,
chooses an utterance from a list for each dialogue
turn. The Amani player is encouraged to use an ut-
terance from this list whenever possible; however,
for user utterances that the Amani player judges can-
not possibly be handled by any existing response, a
new response can be authored (as English text) and
immediately used in the role play. Each player sees
the other?s utterance as text in their own terminal.
This closely resembles a Wizard-of-Oz setup, with
they key difference being that both dialogue partic-
ipants believe they are interacting with another per-
son, which is in fact the case, and the idea of a wiz-
ard controlling a system is not part of the exercise.
However, because the Amani player is encouraged
to limit Amani?s responses to a fixed utterance set,
and the dialogue is constrained to a strict turn-taking
setup that interleaves utterances from each partici-
pant, the situation also differs from conventional role
play.
We collected a total of 19 dialogues and 296 ut-
terances for Amani, for an average of 15.6 Amani
utterances per dialogue.
3.2 Paraphrase generation
The dialogues collected through structured role play
are intended for serving as training data from which
Amani?s dialogue policy can be learned. However,
to cover the natural language variability with which
dialogue acts from the commander can be expressed
would require a much larger number of dialogues
than it would be practical to collect, since a learned
system that deals only with the surface text in the
dialogues would need to deal both with the dia-
logue policy and natural language understanding for
the scenario. Instead, we require only that the di-
alogues collected cover the desired dialogue acts
for the player role in the scenario. To address the
language understanding problem (indirectly), we at-
tempt to cover the variability of expression of these
dialogue acts through the collection of paraphrases
for the commander utterances in the set of dialogues.
For each commander utterance in the 19 dialogues
we collected, we had annotators create a set of para-
phrases. In creating paraphrases, annotators were
asked to take not just the original utterance into ac-
count, but also its context in the dialogue. We did
not specify a fixed number of paraphrases per utter-
ance, but instead asked for as many paraphrases as
the annotator could quickly think of.
Figure 1 exemplifies the paraphrases created dur-
ing this process, for a target user utterance of can
you tell me what you know of the incident?. For this
utterance, a total of 6 paraphrases were generated.
We used a total of 9 annotators, who created para-
phrases for the 296 utterances in the 19 dialogues.
Most annotators were responsible for no more than
two dialogues, and took on average less than 30
minutes per dialogue. The average number of para-
41
Previous Dialogue History
Lieutenant: hi amani
Amani: hello.
Lieutenant: how are you doing?
Amani: fine thank you.
Lieutenant: thank you for meeting with me
Amani: you?re welcome.
Target User Utterance
Lieutenant: can you tell me what you know of the incident?
Paraphrases:
please tell me what information you have about the incident
could you please tell me what you saw?
what can you tell me about the incident?
can you tell me about the incident?
please, tell me what you know about the incident
tell me what you saw, please
System Response
Amani: i saw all the shooting from my window. what do you want to know about it?
External Referees:
(3 referees) i saw all the shooting from my window. what do you want to know about it?
(2 referees) i remember that the gun fire was coming from the window on the second floor of assad?s
shop. the shop is only one story but there are apartments on top of the shop.
(1 referee) what is it you want to know about the incident?
Figure 1: An enriched dialogue turn from an Amani structured role play.
phrases collected per user utterance was 5.5.
Our 9 annotators had differing backgrounds, rang-
ing from transcribers and summer interns to experi-
enced NLP researchers. It should be noted that all
had at least some experience working with natural
language processing technologies. In future work,
we would like to explore using less experienced an-
notators for paraphrasing.
3.3 External referee annotation
Although the paraphrase generation step helps with
coverage of the language used by the commander
in our scenario, the combination of the original di-
alogues collected through structured role play and
the paraphrases do not address one crucial issue in
learning of data-driven dialogue policies, and their
automated evaluation: at each turn, a dialogue par-
ticipant has multiple valid dialogue acts that can be
performed, not a single correct one. In other words,
given the same dialogue history up to a given point,
multiple human dialogue participants following the
same underspecified policy may choose different di-
alogue acts to continue the dialogue, and each of
these different choices may be perfectly acceptable
and coherent. This is one of main challenges in cre-
ation and evaluation of data-driven policies, since
the exponentially many acceptable dialogue paths
are both difficult to model explicitly, and difficult
to recognize automatically when performed during
testing. Of course, the degree to which this is a prac-
tical problem in a specific dialogue scenario depends
on several factors, including how underspecified the
targeted dialogue policy is. In our case study, the
policy has a high level of underspecification, since
only behaviors related to the information in Table 1
are mentioned directly, and even those are only de-
scribed in natural language, without formal rigor.
The rest of the policy dictates only that human play-
ers in the part of Amani act according to their com-
monsense in playing the role of the Amani character.
However, we limit the otherwise potentially infinite
possibilities for dialogue behavior by strongly en-
couraging the Amani player to perform only one of a
set of predefined utterances corresponding to certain
dialogue acts in the scenario. In our experiments, the
number of utterances available for Amani was 96.
We first investigate this issue by attempting to
characterize the amount of human variation in the
choice of one of the 96 available dialogue acts at
any given point in a dialogue. To this end, we intro-
duce the idea of the external referee, who essentially
provides a ?second opinion? for dialogue acts per-
formed by the original role player. The external ref-
eree annotation task works as follows: (1) Starting
with an existing dialogue containing n utterances
42
?u1, u2, ..., un? for the participant whose utterances
will be externally refereed (one of the dialogues
collected through structured role play, in our case
study, where we externally referee the Amani utter-
ances), produce n dialogue histories h1, h2, ..., hn,
with each hi consisting of every utterance from each
dialogue participant from the beginning of the dia-
logue down to, but not including, the ith utterance in
the dialogue. (2) For each dialogue history hi, the
external referee (who must not be the person who
played a part in the original dialogue) chooses an
utterance u?i from the choices available for the sce-
nario, without knowledge of the original utterance
ui in the dialogue from which the history was pro-
duced.
Figure 1 provides an example of the choices made
by 6 external referees for a single target user ut-
terance. Given the previous dialogue history and
the target user utterance (can you tell me what you
know of the incident?), each external referee inde-
pendently chose a single best utterance for the char-
acter to respond with. In the example in the figure,
it can be seen that 3 of the 6 external referees chose
the same response as the original Amani player, as-
serting that Amani did indeed witness the incident
and asking what the commander would like to know.
The other three chose alternative responses; two
of these selected a response asserting information
about where the gun fire was coming from, while
a third referee chose a response simply asking what
the commander would like to know. It is important
to note that all three of these alternative responses
would be acceptable from a design and training per-
spective.
In this annotation task, the task is not to pro-
vide alternative dialogues, but simply one charac-
ter response to each individual utterance, assuming
the fixed history of the original dialogue. In other
words, the annotator has no control or impact over
the dialogue history at any point, and provides only
additional reference utterances for possible immedi-
ate continuations for each dialogue history. It is for
this reason we call the annotator an external referee.
Annotations from multiple external referees for
the dialogues collected through structured role play
do not result in a representation of the lattice of the
many possible dialogue paths in the scenario, but
rather an approximation that represents the possible
options in the immediate future of a given dialogue
history. The main difference is that the available his-
tories are limited to those in the original dialogues
from structured role play. While this may be a lim-
iting factor if one attempts to model dialogue be-
havior based on entire dialogue histories, since the
available histories represent only a very sparse sam-
ple of the space of valid histories, it is possible that
good approximate models can be achieved with fac-
torization of dialogues by sequences of a fixed num-
ber of consecutive turns, e.g. a model that makes a
second-order Markov assumption, considering only
the previous two turns in the dialogue as an approx-
imation of the entire history (Gandhe and Traum,
2007). This is in a way the same approximation used
in n-gram language models, but at the level of gran-
ularity of sentences, rather than words.
We collected annotations from 6 different exter-
nal referees, with each individual referee annotating
the entire set of 19 dialogues, and taking on average
about two hours to complete the annotation of the
entire set. All of our external referees were very fa-
miliar with the design of the Amani character, and
most had natural language processing expertise.
4 Evaluation of dialogue policies with
multiple external referees
4.1 External referee agreement
The dialogues and external referee annotations col-
lected using the procedure described in Section 3
provide a way to characterize the targeted policy
with respect to human variability in choosing utter-
ances from a fixed set, since the annotations include
the choices made by multiple external referees.
From the annotations of utterances chosen for
Amani in our 19 dialogues, we see that human an-
notators agree only 49.2% of the time when choos-
ing an utterance in the external referee framework.
That is, given the same dialogue history, we expect
that two human role players would agree on average
slightly less than 50% of the time on what the next
utterance should be1.
Based on this level of pairwise agreement, one
might conclude that using these data for either policy
learning or policy evaluation is a lost cause. How-
1This represents the averaged agreement over all pairs of
external referees.
43
010
20
30
1 2 3 4 5 6
U
tte
ra
n
ce
co
u
n
t(
%
)
Number of distinct utterance choices
Figure 2: Distribution in number of distinct choices by
external referees
ever, this result does not necessarily indicate that hu-
man raters disagree on what the correct choice is; it
is more likely to reflect that there are in fact mul-
tiple ?correct? (acceptable) choices, which we can
capture through multiple annotators.
The annotations from multiple external referees
in our case study support this view: Figure 2 shows
the number of distinct utterance choices made by
each of the six external referees for each specific ut-
terance in the 19 dialogues collected through struc-
tured role play. Each external referee chooses only
one utterance (out of 96 options) per Amani turn in
the 19 dialogues. Over the 296 Amani utterances
in the entire set of dialogues, all six referees agreed
unanimously on their utterance choice only 23.3%
of the time. The most frequent case, totaling almost
30% of all utterances, was that the set composed by
the single choice from each of the six wizards for
an utterance had exactly two distinct elements. For
only 1.3% of the 296 utterances did that set contain
the maximum number of distinct elements (six), in-
dicating complete disagreement among the external
referees. We note that, in this case, very low agree-
ment to complete disagreement reflects a situation
in dialogue where it is likely that there are many di-
alogue act choices considered acceptable by the col-
lective body of external referees. In our scenario,
there were at most two choices from the six referees
for more than 50% of the Amani turns, indicating
that in the majority of the cases there is only a small
set of acceptable dialogue acts (from the 296 avail-
able), while five or more options were chosen for
less than 10% of all Amani turns.
For a more direct characterization of dialogue sce-
narios, and also for the purposes of evaluation, we
40
50
60
70
80
90
1 2 3 4 5 6 7
W
ea
k
ag
re
em
en
t(
%
)
Number of external referees
bc
bc
bc
bc
bc
Figure 3: Weak agreement between external referees
now define a metric that reflects overall agreement
in a group of external referees. Instead of compar-
ing one choice from a single referee to another single
choice, we instead check for membership of a single
choice cij from a single referee Ri for utterance uj
in the set of choices {ckj |k 6= i} from all of the
other referees {Rk|k 6= i}. In the positive case, we
say that Ri weakly agrees with the rest of the raters
{Rk|k 6= i} on the annotation of utterance uj . We
define the weak agreement agrn for a set of N ex-
ternal referees over a set of m utterances to be rate
at which each rater Ri weakly agrees with the n? 1
raters {Rk|k 6= i}, for all integer values of i ranging
from 1 to N , inclusive. Intuitively, weak agreement
reflects two important questions: (1) how often is
the choice of a referee supported by the choice of
at least one more referee? and (2) given a set of
n ? 1 referees, how much new information (in the
form of unseen choices) should I expect to see from
a new nth referee? Figure 3 addresses these ques-
tions for the scenario in our case study by showing
the weak agreement figures obtained for sets of in-
creasing numbers of external referees, from 2 to 6.
Each point in the graph corresponds to the average
of the weak agreement values obtained for all possi-
ble ways of holding out one external referee Ri, and
computing the weak agreement between Ri and the
other referees, assuming an overall pool containing
the given number of external referees.
We note that with the dialogue act choices of a
single person, coverage of the possible acceptable
options is quite poor, corresponding only to an aver-
age of 50% of the choices made by another person.
44
The coverage increases rapidly as two more external
referees are added, and more slowly, although still
steadily from there. The rightmost point in Figure 3
indicates that with a set of five external referee we
should expect to cover almost 80% of the choices of
a sixth referee.
4.2 Dialogue policy evaluation with multiple
external referees
The weak agreement metric defined in the previ-
ous section can be used to measure the quality of
automatically learned policies, and to provide in-
sight into how a learned policy compares to human-
level performance. Because it recognizes the valid-
ity of multiple responses, the weak agreement metric
can help distinguish true policy errors from policy
choices that are consistent with the intuitions of at
least some human referees about what the character
should say.
In particular, given the choices made by five exter-
nal referees for our 19 Amani dialogues, we can ex-
pect their choices to cover about 80% of the choices
a sixth person would make for what Amani should
say at each turn in these dialogues. (I.e., we know
that the weak agreement among a group of six hu-
man referees is about 80% for this Amani scenario.)
We proceed to rate the quality of an automatic
policy by computing a one-vs-others version of
weak agreement?intuitively treating our policy as
if it were such a ?sixth person?, and comparing it
to the other five. Instead of computing the average
weak agreement for referees randomly selected from
an entire group, as in the previous section, to eval-
uate a policy, we compute its weak agreement com-
pared to the combined set of human external refer-
ees, as follows. For every system utterance uj in our
set of role play dialogues, a given automatic policy
P is used to select a response c?j (corresponding to
a dialogue act in the domain). We then check for
membership of c?j in the set that contains only and
all dialogue act choices ckj for k ranging from 1 to
N , inclusive, where N is the number of external ref-
erees and ckj corresponds to the kth referee?s choice
for the jth utterance. Another way to interpret this
evaluation metric is to consider it a form of accuracy
that computes the number of correct choices made
by the policy divided by the total number of choices
made by the policy, where a choice is considered
?correct? if it matches any of the external referees?
choices for a specific utterance. For this reason, we
refer to this evaluation-focused one-vs-all version of
weak agreement as weak accuracy.
Based on the definition above, an automatic pol-
icy with quality indistinguishable from that of a
person choosing utterances for the Amani character
would have a weak accuracy of about 80% or higher
when measured using a set of five external referees.
We see then that this metric is far from perfect, since
it cannot rank two policies with weak accuracy lev-
els of, say, 80% and 90%. It is also possible for a
policy that results in dialogue behavior noticeably
inferior to that of a human referee to be rated at
the same weak accuracy value for a human referee
(80%). In practice, however, weak accuracy with
five or six external referees has far greater power for
discriminating between policies of varying quality,
and ranking them correctly, than a naive version of
accuracy, which corresponds to weak accuracy us-
ing a single referee. Furthermore, the addition of
only a few more external referees would very likely
increase the efficacy of the weak agreement metric.
Despite the shortcomings of weak accuracy as a
metric for evaluation of quality of dialogue poli-
cies, it opens up a wide range of opportunities for
development of learned policies. Without an auto-
mated metric, development of such techniques can
be only vaguely incremental, relying on either costly
or, more likely, infrequent human evaluations with
results that are difficult to optimize toward with cur-
rent machine learning techniques. The use of im-
perfect automated metrics in situations where ideal
metrics are unavailable or are impractical to deploy
is fairly common in natural language processing.
PARSEVAL (Abney et al, 1991), commonly used
for parser evaluation, and BLEU (Papineni et al,
2002), commonly used in machine translation, are
two examples of well-known imperfect metrics that
have been the subject of much criticism, but that are
widely agreed to have been necessary for much of
the progress enjoyed by their respective fields. Un-
like BLEU, however, which has been shown to cor-
relate with certain types of human judgment on the
quality of machine translation systems, our notion
of weak accuracy has not yet been demonstrated to
correlate with human judgments on the quality of di-
alogue policies, and as such it is only hypothesized
45
to have this property. We leave this important step
of validation as future work.
5 Learning dialogue policies from
examples without formal modeling
Equipped with a dataset with 19 dialogues in the
Amani scenario (including paraphrases for the un-
constrained commander utterances, and external ref-
eree annotations for the constrained Amani utter-
ances), and an automatic evaluation framework for
distinguishing quality differences in learned poli-
cies, we now describe our experiments on learning
dialogue policies from data collected in structured
role play sessions, and enriched with paraphrases
and external referee annotations.
In each of our experiments we attempt to learn
a dialogue policy as a maximum entropy classi-
fier (Berger et al, 1996) that chooses one utterance
out of the 96 possible utterances for Amani after
each commander utterance, given features extracted
from the dialogue history. This policy could be in-
tegrated in a dialogue system very easily, since it
chooses system utterances directly given previous
user and system utterances. We evaluate the dia-
logue policies learned in each experiment through
19-fold cross-validation of our set of 19 dialogues:
in each fold, we hold out one dialogue (and all of its
related information, such as external referee anno-
tations and user utterance paraphrases) and use the
remaining 18 dialogues as training data.
5.1 Learning from examples
Using only the dialogues collected in structured role
play sessions, and no additional information from
external referees or paraphrases, we train the maxi-
mum entropy classifier to choose a system utterance
si based on features extracted from the two previous
user utterances ui and ui?1 and the previous system
utterance si?1. The features extracted from these ut-
terances are the words present in each user utterance,
and the complete text of each system utterance. Low
frequency words occurring fewer than 5 times in the
corpus are excluded.
The weak accuracy for this simple policy is 43%,
a low value that indicates that for more than half its
turns the policy chooses an utterance that was not
chosen by any of the referees, giving us a reasonable
level of confidence that this policy is of poor quality.
5.2 Enhanced training with external referees
The next experiment expands the training set avail-
able to the maximum entropy classifier by adding
training instances based on the utterances chosen by
the external referees. For each of the training in-
stances (target utterance coupled with features from
ui, si?1 and ui?1) we add six new training instances,
each using the same features as the original train-
ing instance, but replacing the target class with the
choice made by an external referee. Note that this
creates identical training instances for cases when
the same utterance is chosen by multiple annotators,
which has the effect of weighting training examples.
With the additional information, weak accuracy for
this policy improves to 56%, which is a large gain
that still results in a mediocre dialogue policy.
5.3 Expanding training examples with
paraphrases
To help determine how much of difficulty in our
policy learning task is due to the related problem
of natural language understanding (NLU), and how
much is due to modeling dialogue behavior regard-
less of NLU, we performed manual annotation of
dialogue acts for the user utterances, and trained a
policy as in the previous section, but using manu-
ally assigned dialogue acts instead of the words for
user utterances in the dialogue history. With this
gold-standard NLU, weak accuracy improves from
56% to 67%, approaching the level of human perfor-
mance, and already at a level where two out of every
three choices made by the learned policy matches
the choice of a human referee.
To bridge the gap between learning purely from
surface text (with no formal modeling) and learn-
ing from manually assigned dialogue acts specifi-
cally designed to capture important information in
the scenario, we turn to the paraphrases collected
for user utterances in our 19 dialogues. These para-
phrases are used to create additional synthetic train-
ing material for the classifier, as follows: for each
training instance produced from a chosen system ut-
terance si and previous utterances ui, si?1 and ui?1
(see previous section), we create additional training
instances keeping the target system utterance si and
previous system utterance si?1 the same, but using
46
a paraphrase u?i in the place of ui, and a paraphrase
u?i?1 in the place of ui?1. Training instances are
added for all possible combinations of the available
paraphrases for ui and ui?1, providing some (arti-
ficial) coverage for parts of the space of possible
dialogue paths that would be otherwise completely
ignored during training.
Training the classifier with material from the ex-
ternal referees (see previous section) and additional
synthetic training examples from paraphrases as de-
scribed above produces a dialogue policy with weak
accuracy of 66%, at the same level as the policy
learned with manually assigned speech acts. It is
noteworthy that this was achieved through a very
simple and intuitive paraphrase annotation task that
requires no technical knowledge about dialogue sys-
tems, dialogue acts or domain modeling. As men-
tioned in section 3.2, paraphrases for each of the 19
dialogues were generated in less than 30 minutes on
average.
6 Conclusion and future work
We introduced a framework for collection and en-
richment of scenario-specific dialogues based only
on tasks that require no technical knowledge. Data
collected in this framework support novel ap-
proaches not just for learning dialogue policies,
but perhaps more importantly for evaluating learned
policies, which allows us to examine different tech-
niques using an objective automatic metric.
Although research on both learning and evalu-
ating dialogue policies is still in early stages, this
case study and proof-of-concept experiments serve
to illustrate the basic ideas of external referee and
paraphrase annotation, and the use of multiple refer-
ence dialogue act choices in evaluation of dialogue
policies, in a way similar to how multiple reference
translations are used in evaluation of machine trans-
lation systems. We do not consider this line of re-
search a replacement for or an alternative to for-
mal modeling of domains and dialogue behavior,
but rather as an additional tool in the community?s
collective arsenal. There are many unexplored av-
enues for including data-driven techniques within
rule-based frameworks and vice-versa.
In future work we intend to further validate the
ideas presented in this paper by performing addi-
tional collection of dialogues in the Amani domain
to serve as a virgin test set, and applying these
techniques to other dialogue domains and scenar-
ios. We also plan to refine the weak accuracy and
weak agreement metrics to take into account the
level of agreement within utterances to reflect that
some parts of dialogues may be more open-ended
than others. Finally, we will conduct human evalu-
ations of different policies to begin validating weak
accuracy as an automatic metric for evaluation of di-
alogue policies.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred. We would also like to thank Ron Artstein,
Sudeep Gandhe, Fabrizio Morbini, Angela Nazar-
ian, Susan Robinson, Michael Rushforth, and David
Traum.
References
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In E. Black, editor, Proceedings of the
workshop on Speech and Natural Language, HLT ?91,
pages 306?311, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009. Viability of a simple dialogue
act scheme for a tactical questioning dialogue system.
In DiaHolmia 2009: Proceedings of the 13th Work-
shop on the Semantics and Pragmatics of Dialogue,
page 43?50, Stockholm, Sweden, June.
Adam L. Berger, Stephen D. Della Pietra, and Vincent
J. D. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computational
Linguistics, 22(1):39?71.
Sudeep Gandhe and David R. Traum. 2007. Creating
spoken dialogue characters from corpora without an-
notations. In Proceedings of Interspeech-07, 08/2007.
Sudeep Gandhe and David R. Traum. 2010. I?ve said it
before, and i?ll say it again: An empirical investigation
47
of the upper bound of the selection approach to dia-
logue. In 11th annual SIGdial Meeting on Discourse
and Dialogue.
Sudeep Gandhe, Nicolle Whitman, David R. Traum, and
Ron Artstein. 2009. An integrated authoring tool for
tactical questioning dialogue systems. In 6th Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, Pasadena, California, July.
Sina Jafarpour, Chris Burges, and Alan Ritter. 2009. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
In Proceedings of the NIPS Workshop on Advances in
Ranking.
Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, An-
ton Leuski, and Albert A. Rizzo. 2007. Virtual pa-
tients for clinical therapist skills training. In Proceed-
ings of the 7th international conference on Intelligent
Virtual Agents, IVA ?07, pages 197?210, Berlin, Hei-
delberg. Springer-Verlag.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Linus Sellberg and Arne Jnsson. 2008. Using ran-
dom indexing to improve singular value decomposi-
tion for latent semantic analysis. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), Marrakech, Morocco, may.
David R. Traum, Anton Leuski, Antonio Roque, Sudeep
Gandhe, David DeVault, Jillian Gerten, Susan Robin-
son, and Bilyana Martinovski. 2008. Natural lan-
guage dialogue architectures for tactical questioning
characters. In Army Science Conference, Florida,
12/2008.
Ingrid Zukerman and Yuval Marom. 2006. A corpus-
based approach to help-desk response generation.
Computational Intelligence for Modelling, Control
and Automation, International Conference on, 1:23.
48
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 137?139,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Mixed-Initiative Conversational Dialogue System for Healthcare
Fabrizio Morbini and Eric Forbell and David DeVault and Kenji Sagae and
David R. Traum and Albert A. Rizzo
Institute for Creative Technologies
University of Southern California
Los Angeles, CA 90094, USA
{morbini,forbell,devault,sagae,traum,rizzo}@ict.usc.edu
Abstract
We present a mixed initiative conversational
dialogue system designed to address primar-
ily mental health care concerns related to
military deployment. It is supported by a
new information-state based dialogue man-
ager, FLoReS (Forward-Looking, Reward
Seeking dialogue manager), that allows both
advanced, flexible, mixed initiative interac-
tion, and efficient policy creation by domain
experts. To easily reach its target population
this dialogue system is accessible as a web ap-
plication.
1 Introduction
The SimCoach project is motivated by the challenge
of empowering troops and their significant others in
regard to their healthcare, especially with respect to
issues related to the psychological toll of military
deployment. SimCoach virtual humans are not de-
signed to act as therapists, but rather to encourage
users to explore available options and seek treatment
when needed by fostering comfort and confidence in
a safe and anonymous environment where users can
express their concerns to an artificial conversational
partner without fear of judgment or possible reper-
cussions.
SimCoach presents a rich test case for all compo-
nents of a dialogue system. The interaction with the
virtual human is delivered via the web for easy ac-
cess. As a trade-off between performance and qual-
ity, the virtual human has access to a limited set of
pre-rendered animations.
The Natural Language Understanding (NLU)
module needs to cope with both chat and military
Figure 1: Bill Ford, a SimCoach character. SimCoach
virtual humans are accessible through a web browser.
The user enters natural language input in the text field
on the bottom of the screen. The simcoach responds with
text, speech and character animation. The text area to the
right shows a transcript of the dialogue.
slang and a broad conversational domain. The dia-
logue policy authoring module needs to support non-
dialogue experts given that important parts of the di-
alogue policy are contributed by experts in psycho-
metrics and mental health issues in the military, and
others with familiarity with the military domain.
The dialogue manager (DM) must be able to take
initiative when building rapport or collecting the in-
formation it needs, but also respond appropriately
when the user takes initiative.
2 Supporting Mixed Initiative Dialogues
There is often a tension between system initiative
and performance of the system?s decision-making
for understanding and actions. A strong system-
initiative policy reduces the action state space since
137
user actions are only allowed at certain points in
the dialogue. System initiative also usually makes
it easier for a domain expert to design a dialogue
policy that will behave as desired.1 Such systems
can work well if the limited options available to the
user are what the user wants to do, but can be prob-
lematic otherwise, especially if the user has a choice
of whether or not to use the system. In particular,
this approach may not be well suited to an appli-
cation like SimCoach. At the other extreme, some
systems allow the user to say anything at any time,
but have fairly flat dialogue policies, e.g., (Leuski et
al., 2006). These systems can work well when the
user is naturally in charge, such as in interviewing
a character, but may not be suitable for situations
in which a character is asking the user questions, or
mixed initiative is desired.
True mixed initiative is notoriously difficult for a
manually constructed call-flow graph, in which the
system might want to take different actions in re-
sponse to similar stimuli, depending on local utili-
ties. Reinforcement learning approaches (Williams
and Young, 2007; English and Heeman, 2005) can
be very useful at learning local policy optimizations,
but they require large amounts of training data and a
well-defined global reward structure, are difficult to
apply to a large state-space and remove some of the
control, which can be undesirable (Paek and Pierac-
cini, 2008).
Our approach to this problem is a forward-looking
reward seeking agent, similar to that described in
(Liu and Schubert, 2010), though with support for
complex dialogue interaction and its authoring. Au-
thoring involves design of local subdialogue net-
works with pre-conditions and effects, and also qual-
itative reward categories (goals), which can be in-
stantiated with specific reward values. The dialogue
manager, called FLoReS, can locally optimize pol-
icy decisions, by calculating the highest overall ex-
pected reward for the best sequence of subdialogues
from a given point. Within a subdialogue, authors
can craft the specific structure of interaction.
Briefly, the main modules that form FLoReS are:
? The information state, a propositional knowl-
1Simple structures, such as a call flow graph (Pieraccini and
Huerta, 2005) and branching narrative for interactive games
(Tavinor, 2009) will suffice for authoring.
edge base that keeps track of the current state
of the conversation. The information state sup-
ports missing or unknown information by al-
lowing atomic formulas to have 3 possible val-
ues: true, false and null.
? A set of inference rules that allows the sys-
tem to add new knowledge to its information
state, based on logical reasoning. Forward in-
ference facilitates policy authoring by provid-
ing a mechanism to specify information state
updates that are independent of the specific di-
alogue context.2
? An event handling system, that allows the in-
formation state to be updated based on user in-
put, system action, or other classes of author-
defined events (such as system timeouts).
? A set of operators. Operators represent lo-
cal dialogue structure (trees), and can also be
thought of as reusable subdialogues. Each state
within the subdialogue can include a reward
for reaching that state. Rewards are functions
of the goals of the system, and are the main
method used to decide what to do when there is
more than one applicable operator. Operators
have preconditions and effects. Effects specify
changes to the information state. The precondi-
tions define when an operator can be activated.
3 Sample Dialogue
In this demo, the user will interact with the Sim-
Coach character Bill Ford, using a standard web
browser and typing text. The virtual human, driven
by FLoReS, will respond using pre-rendered anima-
tions encoded as H.264 video, delivered via a stan-
dard web server. Table 1 shows an excerpt from a
sample conversation with Bill Ford that illustrates
some of the features of this dialogue manager.
The excerpt starts from a rapport building
smalltalk sub-dialogue on the topic of barbecuing
which is interrupted by a user question about con-
fidentiality. The system responds to the user inter-
ruption and then re-starts the interrupted smalltalk
because it is still the most valuable conversation con-
tinuation available at that moment.
2For example: every time the user says that s/he has night-
mares we want to update the information state to include that
s/he also has sleeping problems.
138
Dialogue transcript Notes
Ask anybody about me, and
they?ll tell you that I love to
BBQ
BBQ Smalltalk The character is equipped
with a few operators for
smalltalk about a few topics.
BBQ is one of them.
Is this conversation se-
cret?
We don?t share your info
with anyone who can per-
sonally identify you. The
techs can see what we say,
but just to tell that the site is
working. But they have no
idea who said it, just what
was said
Did that help you?
Yes it did.
Great.
Confidentiality QA
Here the system is inter-
rupted by a user question and
it decides that answering it is
the best course of action.
Like I was saying, I love to
BBQ
BBQ Smalltalk After answering the question,
the best course of action is to
awaken the paused operator
about the BBQ smalltalk.
What is PTSD?
PTSD, or post-traumatic
stress disorder is an anxiety
condition associated with
serious traumatic events.
It can come with survivor
guilt, reliving the trauma in
dreams, numbness, and lack
of involvement with reality.
What is PTSD QA
Again the BBQ smalltalk is
interrupted by another ques-
tion from the user.
So, is PTSD something
you?re worried about. I only
ask, because you?ve been
asking about it. ...
PTSD Topic Interest QA
After answering the second
question the system decides
to ignore the paused operator
and load a follow-up operator
related to the important topic
raised by the user?s question.
The selection is based on the
expected reward that talking
about PTSD can bring to the
system.
Table 1: An excerpt of a conversation with Bill Ford that
shows opportunistic mixed initiative behavior.
Next, the user asks a question about the impor-
tant topic of post-traumatic stress disorder (PTSD).
That allows operators related to the PTSD topic to
become available and at the next chance the most
rewarding operator is no longer the smalltalk sub-
dialogue but one that stays on the PTSD topic.
4 Conclusion
We described the SimCoach dialogue system which
is designed to facilitate access to difficult health con-
cerns faced by military personnel and their fami-
lies. To easily reach its target population, the sys-
tem is available on the web. The dialogue is driven
by FLoReS, a new information-state and plan-based
DM with opportunistic action selection based on ex-
pected rewards that supports non-expert authoring.
Acknowledgments
The effort described here has been sponsored by the
U.S. Army. Any opinions, content or information
presented does not necessarily reflect the position or
the policy of the United States Government, and no
official endorsement should be inferred.
References
M.S. English and P.A. Heeman. 2005. Learning mixed
initiative dialogue strategies by using reinforcement
learning on both conversants. In HLT-EMNLP.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
dial Workshop on Discourse and Dialogue, pages 18?
27.
Daphne Liu and Lenhart K. Schubert. 2010. Combin-
ing self-motivation with logical planning and inference
in a reward-seeking agent. In Joaquim Filipe, Ana
L. N. Fred, and Bernadette Sharp, editors, ICAART (2),
pages 257?263. INSTICC Press.
Tim Paek and Roberto Pieraccini. 2008. Automating
spoken dialogue management design using machine
learning: An industry perspective. Speech Commu-
nication, 50(89):716 ? 729. Evaluating new methods
and models for advanced speech-based interactive sys-
tems.
Roberto Pieraccini and Juan Huerta. 2005. Where do we
go from here? Research and commercial spoken dia-
log systems. In Proceedings of the 6th SIGdial Work-
shop on Discourse and Dialogue, Lisbon, Portugal,
September.
Grant Tavinor. 2009. The art of videogames. New Di-
rections in Aesthetics. Wiley-Blackwell, Oxford.
J.D. Williams and S. Young. 2007. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
139
Proceedings of the SIGDIAL 2013 Conference, pages 372?374,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Roundtable: An Online Framework for Building Web-based         Conversational Agents 
 Eric Forbell and Nicolai Kalisch and Fabrizio Morbini and Kelly Christoffersen  and Kenji Sagae and David Traum and Albert A. Rizzo Institute for Creative Technologies University of Southern California Los Angeles, CA 90094, USA {lastname}@ict.usc.edu 
   
Abstract 
We present an online system that provides a complete web-based sandbox for creating, testing and publishing embodied conversation-al agents. The tool, called Roundtable, em-powers many different types of authors and varying team sizes to create flexible interac-tions by automating many editing workflows while limiting complexity and hiding architec-tural concerns. Finished characters can be pub-lished directly to web servers, enabling highly interactive applications.  1 Introduction To support the creation of a virtual guide system called SimCoach (Rizzo et al 2011) designed to help military service personnel and their families understand behavioral healthcare issues and learn about support resources, a core virtual human architecture that included a new dialogue man-agement approach was developed (Morbini et al, 2012b). SimCoach is an embodied, conversa-tional virtual human guide delivered via the web and is supported by a flexible information state dialogue manager called FLoReS designed to support mixed initiative dialogue with conversa-tional systems. Morbini et al (2012a) provide a detailed description of the dialogue manager. Although FLoReS supports a wide variety of virtual human character behaviors, these must be specified in dialogue policies that must be au-thored manually. Initially, authoring for this dia-logue manager required coding of policies using a custom programming language. Therefore sig-nificant training for content authors was neces-sary, as well as substantial support from dialogue 
system developers in managing resources such as training data for the language understanding sys-tem. To improve the accessibility of the system to non-technical subject matter experts and other creative staff, it became clear that additional tools were necessary. In this demonstration, we present Roundtable: a web-based authoring envi-ronment for virtual human characters that is de-signed for use by subject matter experts who are qualified for content authoring in targeted do-mains, but who may not possess technical skills in programming or experience in dialogue sys-tem design.  2 Supporting rapid authoring of dia-logue agents for the web Roundtable is a complete web-based authoring system enabling the end-to-end creation, valida-tion, testing and web publishing of virtual human characters using the SimCoach virtual human architecture. The system provides features that empower many types of authors, team sizes and makeups. The system allows an author to select from a set of preconfigured 3D character models, model the dialogue policy through behavior tem-plates and more direct subdialogue editing, train and test the natural language understanding com-ponent, render animation performances associat-ed with character behaviors and utterances, and test both text-based and fully animated interac-tions. Finally, the complete character dataset can be exported and deployed to a live, highly avail-able server environment, where interaction data can be monitored and periodically collected for analysis and refinement, all from within the same browser environment (Figure 1). The entire sys-tem, from authoring to end-user interaction with 
372
the virtual human character, is web-based and requires only a current web browser for content authors and end users.   (a)
 (b)
  (c)
  (d)
  Figure 1:  Selected modules from the Roundtable character authoring system (a) character project browser; (b) dialogue policy editor; (c) training data manager (d) action and animation asset man-ager 
At the core of the authoring application is an object-oriented information model and set of management systems that span the following roles: ? Dialogue content management, respon-sible for persistence, search, validation and retrieval operations of all dialogue el-ements including subdialogue networks; information state variables and effects; goals and effects; and dialogue action an-notations that provide the mapping to the action database. ? Training data management, concerned with managing training items for a data-driven natural language understanding module, as well as providing support for running regressions when updating the training set.   ? Action management, provides data op-erations for managing potentially large sets of virtual human performance-related assets, including utterance text, speech au-dio when not system-generated, annotated nonverbal behavior schedules, as well as non-performance actions which include web-hosted videos, digested web articles, or any arbitrary HTML effect. ? Deployment management, enabling rapid deployment of locally tested charac-ters to highly available web servers as well as review and data warehousing functions for both analytic and refinement purposes. The information model is implemented in a re-lational database that fully specifies, relates and allows inquiry and validation of authored infor-mation. Additionally, a complete web application programming interface (API) powers the Roundtable application, providing a transactional framework for data operations as well as user privilege enforcement, but which also allows application expansion. The information model also serves to decouple the authoring representation from the data struc-tures necessary to drive dialogue behavior at runtime. Prior to realizing an authored character in the FLoReS engine, project dialogue data ele-ments are exported into the format expected by the runtime target, a process that we expect to expand in the future to support different dialogue managers and language understanding configura-tions.  
373
 Figure 2: The interactive virtual human character published to the web, accessible by current brows-ers.  3 Demo script This demonstration will show how to build a simple conversational virtual human character using Roundtable, from acquiring an account (http://authoring.simcoach.org, free for academic research) to obtaining the URL for the newly created character, and all of the steps in between. The workflow to build a character is as follows: 1. In the project module (Figure 1a) we create a new character by providing a unique name and selecting an existing 3D character mod-el.  2. Opening the newly created project brings up the interaction module (Figure 1b) where we choose from a list of available subdialogue templates that can be used for common dia-logue behaviors (question-answer, greeting, etc.). The provided Greeting and Goodbye templates are used to define the character?s conversational behavior when initiating and ending an interaction, respectively. Invoking the Question-Answer template, we can quickly define how the character will re-spond to a specific question or statement. Each template requires a name and sample text for any user or system utterance.  3. Following the template-based subdialogue generation, we create training data for the natural language understanding component by providing possible user utterances associ-ated with each user dialogue act in the tem-plates used (Figure 1c).  4. The last task is to refine system utterances, which are generated automatically during the step of policy authoring, and generate anima-tion data. From the action module, we can search and inspect all system actions. For any system action, with a single button click, 
we can synthesize audio and render anima-tions (Figure 1d).  5. Finally, we navigate to the test module, compile our character project, and are then able to chat with the new character to ensure expected behavior. At this point, the charac-ter is ready to be deployed, with its unique URL, and is immediately accessible on the web (Figure 2). 4 Conclusion  We described the Roundtable online authoring framework that has been designed to support non-expert users in rapidly creating embodied, conversational virtual characters of varying complexities.  The tool, being web-based, re-quires zero configuration to get started and au-thored virtual characters can be deployed to In-ternet-facing web servers immediately, expand-ing the reach of many dialogue-driven applica-tions.  Acknowledgments The effort described here has been sponsored by the U.S. Army. Any opinions, content or infor-mation presented does not necessarily reflect the position or the policy of the United States Gov-ernment, and no official endorsement should be inferred. References  A. Rizzo, B. Lange, J.G. Buckwalter, E. Forbell, J. Kim, K. Sagae, J. Williams, B.O. Rothbaum, J. Difede, G. Reger, T. Parsons, and P. Kenny. An in-telligent virtual human system for providing healthcare information and support. In J.D. West-wood et al, editor, Technology and Informatics. IOS Press, 2011. Fabrizio Morbini, David Devault, Kenji Sagae, Jillian Gerten, Angela Nazarian and David Traum FLo-ReS: A Forward Looking, Reward Seeking, Dia-logue Manager in proceedings of International Workshop on Spoken Dialog Systems (IWSDS-2012), Ermenonville, France, November 2012b. Fabrizio Morbini, Eric Forbell, David DeVault, Kenji Sagae, David Traum and Albert Rizzo. A Mixed-Initiative Conversational Dialogue System for Healthcare. Demonstration in SIGdial 2012, the 13th Annual SIGdial meeting on Discourse and Dialogue, Seoul, South Korea, 2012a.    
374
Proceedings of the SIGDIAL 2013 Conference, pages 394?403,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Which ASR should I choose for my dialogue system?
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Artstein,
Dog?an Can, Panayiotis Georgiou, Shri Narayanan, Anton Leuski and David Traum
University of Southern California
Los Angeles, California, USA
{morbini,sagae,artstein,leuski,traum}@ict.usc.edu
{audhkhas,dogancan}@usc.edu {georgiou,shri}@sipi.usc.edu
Abstract
We present an analysis of several pub-
licly available automatic speech recogniz-
ers (ASRs) in terms of their suitability for
use in different types of dialogue systems.
We focus in particular on cloud based
ASRs that recently have become available
to the community. We include features
of ASR systems and desiderata and re-
quirements for different dialogue systems,
taking into account the dialogue genre,
type of user, and other features. We then
present speech recognition results for six
different dialogue systems. The most in-
teresting result is that different ASR sys-
tems perform best on the data sets. We
also show that there is an improvement
over a previous generation of recognizers
on some of these data sets. We also inves-
tigate language understanding (NLU) on
the ASR output, and explore the relation-
ship between ASR and NLU performance.
1 Introduction
Dialogue system developers who are not also
speech recognition experts are in a better posi-
tion than ever before in terms of the ease of in-
tegrating existing speech recognizers in their sys-
tems. While there have been commercial solutions
and toolkits for a number of years, there were a
number of problems in getting these systems to
work. For example, early toolkits relied on spe-
cific machine hardware, software, and firmware
to function properly, often had a difficult instal-
lation process, and moreover often didn?t work
well for complex dialogue domains, or challeng-
ing acoustic environments. Fortunately the situ-
ation has greatly improved in recent years. Now
there are a number of easy to use solutions, in-
cluding open-source systems (like PocketSphinx),
as well as cloud-based approaches.
While this increased choice of quality recogniz-
ers is of great benefit to dialogue system develop-
ers, it also creates a dilemma ? which recognizer
to use? Unfortunately, the answer is not simple ?
it depends on a number of issues, including the
type of dialogue domain, availability and amount
of training data, availability of internet connectiv-
ity for the runtime system, and speed of response
needed. In this paper we assess several freely
available speech recognition engines, and exam-
ine their suitability and performance in several di-
alogue systems. Here we extend the work done in
Yao et al (2010) focusing in particular on cloud
based freely available ASR systems. We include
2 local ASRs for reference, one of which was also
used in the earlier work for easy comparison.
2 Speech Recognizer Features and
Engines
The following are some of the major criteria for
selection of a speech recognizer.
Customization Some of the available speech
recognizers allow the users to tune the recognizer
to the environment it will operate in, by providing
a specialized lexicon, trained language models or
acoustic models. Customization is especially im-
portant for dialogue systems whose input contains
specialized vocabulary (see section 4).
Output options A basic recognizer will output
a string of text, representing its best hypothesis
about the transcription of the speech input. Some
recognizers offer additional outputs which are use-
ful for dialogue systems: ranked n-best hypothe-
ses allow later processing to use context for dis-
ambiguation, and incremental results allow the
system to react while the user is still speaking.
Performance characteristics Dialogue systems
differ in their requirements for response speed; a
394
System Customization Output options Open Source PerformanceN-best Incremental Speed Installation
Pocketsphinx Full Yes Yes Yes realtime Local
Apple No Noa No No network Cloud
Google No Yes Yesb No network Cloud
AT&T Partialc Yes No No network Cloud
Otosense-Kaldi Full Yes No Yesd variablee Local
aSingle output annotated with alternative hypotheses. bOnly for web-delivered applications in a Google Chrome browser.
cCustom language models. dRelease scheduled for Fall 2013. eUser controls trade-off between speed and output quality.
Table 1: Speech recognizer features important for use in dialogue systems
speech recognizer that runs locally can help by
avoiding network latencies.
Output quality Typically, a dialogue system
would want the best recognition accuracy pos-
sible given the constraints. Ultimately, dialogue
systems want the output that would yield the best
performance for Natural Language Understand-
ing and other downstream processes. As a rule,
better speech recognition leads to better language
understanding, though this is not necessarily the
case for specific applications (see section 5).
We evaluated 5 freely available speech recog-
nizers. Their features are summarized in Table 1.
We did not include the MIT WAMI toolkit1 as we
are focused on speech services that can directly
be used by stand alone applications as opposed to
web delivered ones. We did not include commer-
cial recognizers such as Nuance, because licensing
terms can be difficult for research institutions, and
in particular, disallow publishing benchmarks.
Pocketsphinx is a version of the CMU Sphinx
ASR system optimized to run also on embedded
systems (Huggins-Daines et al, 2006). Pocket-
sphinx is fast, runs locally, and requires relatively
modest computational resources. It provides n-
best lists and lattices, and supports incremental
output. It also provides a voice activity detec-
tion functionality for continuous ASR. This ASR
is fully customizable and trainable, but users are
expected to provide language models suitable for
their applications. A few acoustic models are pro-
vided, and can be adapted using the CMUSphinx
tools.2
1http://wami.csail.mit.edu/
2http://cmusphinx.sourceforge.net/wiki/tutorialadapt
Apple Dictation is the OS level feature in both
MacOSX and iOS.3 It is integrated into the text in-
put system pipeline so a user can replace her key-
board with a microphone for entering text in any
application. Dictation is often associated with the
Siri personal assistant feature of iOS. While it is
likely that Dictation and Siri share the same ASR
technology, Dictation only does speech recogni-
tion. Apple states that Dictation learns the charac-
teristics of the user?s voice and adapts to her accent
(Apple Inc, 2012). Dictation requires an internet
connection to send recorded user speech to Ap-
ple?s servers and receive ASR results. Processing
starts as soon as the user starts speaking so the de-
lay of getting the recognition results after the user
finishes speaking is minimal.
To integrate Dictation into a dialogue system,
a system designer needs to include any system de-
fined text input control into her application and use
the control APIs to observe text changes. The user
would need to press a key when starting to speak
and push the key again once she is done speak-
ing. The ASR result is a text string annotated with
alternative interpretations of individual words or
phrases in the text. There is an API for extract-
ing those interpretations from the result. While the
Dictation feature is reasonably fast and easy to in-
tegrate, dialogue system developers have no con-
trol over the ASR process, which must be treated
as a black box. Apple dictation is limited in that
no customization is possible, no partial recogni-
tion results are provided, and there is an unspeci-
fied limit on the number of utterances dictated for
a period of time, which is not a problem for inter-
action between a single user and a dialogue sys-
tem, but may be an issue in dialogue systems that
support multiple concurrent users.
3Dictation was introduced in iOS 5.0 and MacOSX 10.8.
395
Google Speech API provides support for the
HTML 5 speech input feature.4 It is a cloud based
service in which a user submits audio data using
an HTML POST request and receives as reply the
ASR output in the form of an n-best list. The au-
dio data is limited to roughly 10 seconds in length,
longer clips are rejected and return no ASR results.
The user can (1) customize the number of hy-
potheses returned by the ASR, (2) specify which
language the audio file contains and (3) enable a
filter to remove profanities from the output text.
As is the case with Apple Dictation, ASR must be
treated as a black box, and no task customization
is possible for dialogue system developers. Users
cannot specify or provide custom language models
or acoustic models. The service returns only the fi-
nal hypothesis, there is no incremental output.5 In
addition, results for the same inputs may change
unpredictably, since Google may update or other-
wise change its service and models, and models
may be adapted using specific audio data supplied
by users. In our experiments, we observed accu-
racy improvements when submitting the same au-
dio files over repeated trials over two weeks.
AT&T Watson is the ASR engine available
through the AT&T Speech Mashup service.6 It is
a cloud based service that can be accessed through
HTML POST requests, like the Google Speech
API. AT&T Watson is designed to support the de-
mands of online spoken dialogue systems, and can
be customized with data specific to a dialogue sys-
tem. Additionally, in our tests we did not observe
any limitation in the maximum length of the in-
put audio data. However, AT&T does not provide
a default general-purpose language model, and
application-specific models must be built within
the Speech Mashup service using user-provided
text data. The acoustic model must be selected
from a list provided by the AT&T service, and
acoustic models can be further customized within
the Speech Mashup service. The ASR returns an
n-best list of hypotheses but does not provide in-
cremental output.
Otosense-Kaldi Another ASR we employed
was the Kaldi-based OtoSense-Kaldi engine de-
4https://www.google.com/speech-api/v1/recognize
5The demo page shows continuous speech understanding
with incremental results but requires Google Chrome to run
and is specific to web delivered applications:
http://www.google.com/intl/en/chrome/demos/speech.html
6https://service.research.att.com/smm
veloped at SAIL.7 OtoSense-Kaldi8 is an on-line,
multi-threaded architecture based on the Kaldi
toolkit (Povey et al, 2011) that allows for dynam-
ically configurable and distributed ASR.
3 Dialogue Systems, Users, and Data
All spoken dialogue systems are similar in some
respects, in that there is speech by a user (or users)
that needs to be recognized, and this speech is
punctuated by speech from the system. More-
over, the speech is not fully independent, but ut-
terances are connected to other utterances, e.g. an-
swers to questions, or clarifications. There are,
however many ways in which systems can differ,
that have implications for which speech recogniz-
ers are most appropriate. Some of the dimensions
to consider are:
Type of microphone(s) One of the biggest im-
pacts on ASR is the acoustic environment. Will
the audio be clean, coming from a close-talking
head or lapel-mounted microphone, or will it need
to be picked up from a broader directional micro-
phone or microphone array?
Number of speakers/microphones Will there
be one designated microphone per person, or will
speaker identification need to be performed? Will
audio from the system confuse the ASR?
Push to talk or continuous speech Will the
user clearly identify the start and end of speech,
or will the system need to detect speech acousti-
cally?
Type of Users Will there be designated long-
term users, where user-training or system model
adaptation is feasible, or will there be many un-
known users, where training is not feasible? See
also section 3.1 for more on user types.
Genre What kinds of things will people be say-
ing to the system? Is it mostly commands or short
answers to questions, or more open-ended conver-
sation? See section 3.2 for more on genre issues.
Training Data Is within-domain training data
available, and if so how much?
3.1 Types of Users
The type of user is important for the overall
design of the system and has implications for
7http://sail.usc.edu
8OtoSense-Kaldi will be released (BSD license) in 2013.
396
ASR performance as well. One important as-
pect is the broad physical differences among
speakers, such as male vs female, adult vs child
(e.g. Bell and Gustafson, 2003), or language pro-
ficiency/accent, that will have implications for the
acoustics of what is said, and ASR results. Other
aspects of users have implications for what will
be said, and how successful the interface may
be, overall. Many (e.g. Hassel and Hagen, 2006;
Jokinen and Kanto, 2004) have looked at the dif-
ferences between novice and expert users. Ai et
al. (2007a) also points out a difference between
real users and recruited subjects. Real users also
come in many different flavors, depending on their
purposes. E.g. are they interacting with the system
for fun, to do a specific task that they need to get
done, to learn something (specific or general), or
with some other purpose in mind?
We considered the following classes of users,
ordered from easiest to hardest to get to acceptable
performance and robustness levels:
Demonstrators are generally the easiest for a sys-
tem to understand ? a demonstrator is trained in
use of the system, knows what can and can?t be
said, is motivated toward success, and is gener-
ally interested in showing off the most impres-
sive/successful aspects of the system to an audi-
ence rather than using it for its own sake.
Trained/Expert Users are similar to demonstra-
tors, but use the system to achieve specific results
rather than just to show off its capabilities. This
means that users may be forced down lines that
are not ideal for the system, if these are necessary
to accomplish the task.
Motivated Users do not have the training of ex-
pert users, and may say many things that the sys-
tem can not handle as opposed to equivalent ex-
pressions that could be handled. However moti-
vated users do want the system to succeed, and in
general are willing to do whatever they think is
necessary to improve system performance. Unlike
expert users, motivated users might be incorrect
about what will help the system (e.g. hyperarticu-
lation in response to system misunderstanding).
Casual Users are interested in finding out what
the system can do, but do not have particular moti-
vations to help or hinder the system. Casual Users
may also leave in the middle of an interaction, if it
is not engaging enough.
Red Teams are out to test or ?break? the system,
or show it as not-competent, and may try to do
things the system can?t understand or react well
to, even when an alternative formulation is known
to work.
3.2 Types of Dialogue System Genres
Dialogue Genres can be distinguished along many
lines, e.g. the number and relationship of partic-
ipants, specific conversational rules, purposes of
the participants, etc. We distinguish here four gen-
res of dialogue system that have been in use at
the Institute for Creative Technologies and that we
have available corpora for (there are many other
types of dialogue genres, including tutoring, ca-
sual conversation, interviewing,. . . ). Each genre
has implications for the internal representations
and system architectures needed to engage in that
genre of dialogue.
Simple Question-answering This genre in-
volves strong user-initiative and weak global di-
alogue coherence. The user can ask any ques-
tion to the system at any time, and the system
should respond, with an appropriate answer if
able, or with some other reply indicating either
inability or unwillingness to provide the answer.
This genre allows modeling dialogue at a surface-
text level (Gandhe, 2013), without internal se-
mantic representations of the input, and where
the result of ?understanding? input is the system?s
expected output. The NCPEditor9 (Leuski and
Traum, 2011) is a toolkit that provides an author-
ing environment, classification, and dialogue ca-
pability for simple question-answering characters.
The SGT Blackwell, SGT Star, and Twins systems
described below are all systems in this genre.
Advanced Question-answering This genre is
similar to the simple question-answering charac-
ters, in that the main task of the user is to elicit
information from the system character. The differ-
ence is that there is more long-range and interme-
diate dialogue coherence, in that questions can be
answered several utterances after they have been
asked, there can be intervening sub-dialogues, and
characters sometimes take the initiative to pursue
their own goals rather than just responding to the
user. Because of the requirements for somewhat
deeper understanding, and relation of input to con-
9Available free for academic research purposes from
https://confluence.ict.usc.edu/display/VHTK/Home
397
text and character goals and policies, there is a
need of at least a shallow semantic representa-
tion and representation of the dialogue informa-
tion state, and the character must distinguish un-
derstanding of the input from the character out-
put (since the latter will depend on the dialogue
policy and information state, not just the under-
standing of input). The tactical questioning archi-
tecture (Gandhe et al, 2009)10 provides author-
ing and run-time support for advanced question-
answering characters, and has been used to build
over a dozen characters for purposes such as train-
ing tactical questioning, training culture, and psy-
chology experiments (Gandhe et al, 2011). The
Amani character described below is in this genre.
Slot-filling Probably the most common type of
dialogue system (at least in the research commu-
nity) is slot-filling. Here the dialogue is fairly
structured, with an initial greeting phase, then one
or more tasks, which all start with the user se-
lecting the task, and the system taking over ini-
tiative to ?fill? and possibly confirm the needed
slots, before retrieving some information from a
database, or performing a simple service.11 This
genre also requires a semantic representation, at
least of the slots and acceptable values. Gener-
ally, the set of possible values is large enough, that
some form of NLG is needed (at least template
filling), rather than authoring of all full sentences.
There are a number of toolkits and development
frameworks that are well suited to slot-filling sys-
tems, e.g. Ravenclaw (Bohus and Rudnicky, 2003)
or Trindikit (Larsson and Traum, 2000). The Ra-
diobots system, described below is in this genre.
Negotiation and Planning In this genre, the
system is more of an equal partner with the user,
than a servant, as in the slot-filling systems. The
system must not merely understand user requests,
but must also evaluate whether they meet the sys-
tem goals, what the consequences and precondi-
tions of requests are, and whether there are better
alternatives. For this kind of inference, a more de-
tailed semantic representation is required than just
filling in slots. While we are not aware of publicly
available software that makes this kind of system
easy to construct, there have been several built us-
ing an information-state approach, or the soar cog-
10Soon to be released as part of the virtual human toolkit.
11Mixed-initiative versions of this genre exist, where the
user can also provide unsolicited information, which reduces
the number of system queries needed.
nitive architecture. The TRIPS system (Allen et
al., 2001) also has many similarities.
3.3 ICT Dialogue Systems Tested
We tested the recognizers described in section 2
on data sets collected from six different dialogue
domains. Five are the same ones tested in Yao et
al. (2010), to which we added the Twins set. De-
tails on the size of the training and development
sets may be found in Yao et al (2010), here we
report only the numbers relevant to the Twins do-
main and to the NLU analysis, which are not in
Yao et al (2010).
SGT Blackwell was created as a virtual human
technology demonstration for the 2004 Army Sci-
ence Conference. This is a question-answering
character, with no internal semantic representation
and the primary NLU task merged with Dialogue
management as selecting the best response.
The original users were ICT demonstrators.
However, there were also some experiments with
recruited participants (Leuski et al, 2006a; Leuski
et al, 2006b). Later SGT Blackwell became a part
of the ?best design in America? triennial at the
Cooper-Hewitt Museum in New York City, and
the data set here is from visitors to the museum,
who are mostly casual users, but range from expert
to red-team. Users spoke into a mounted direc-
tional microphone (see Robinson et al, 2008 for
more details).
SGT STAR (Artstein et al, 2009a) is a question-
answering character similar to SGT Blackwell, al-
though designed to talk about Army careers rather
than general knowledge. The users are Army per-
sonnel who went to job fairs and visited schools in
the mobile Army adventure vans, speaking using
headset microphones, and performing for an audi-
ence. The users are somewhere between demon-
strators and expert users. They are speaking to
SGT STAR for the benefit of an audience, but their
primary purpose is to convey information to the
audience in a memorable way (through dialogue
with SGT STAR) rather than to show off the high-
lights of the character.
The Twins are two life-size virtual characters
who serve as guides at the Museum of Science
in Boston (Swartout et al, 2010). The charac-
ters promote interest in Science, Technology, En-
gineering and Mathematics (STEM) in children
between the ages of 7 and 14. They are question-
398
answering characters, but unlike SGTs Blackwell
and Star, the response is a whole dialogue se-
quence, potentially involving interchange from
both characters, rather than a single character turn.
There are two types of users for the Twins:
demonstrators, who are museum staff members,
using head-mounted microphones, and museum
visitors, who use a Shure 522 table-top mounted
microphone (Traum et al, 2012). More on analy-
sis of the museum data can be found in (Aggarwal
et al, 2012). We also investigated speech recog-
nition and NLU performance in this domain in
Morbini et al (2012).
This dataset contains 14K audio files each an-
notated with one of the 168 possible response se-
quences. The division in training development and
test is the same used in Morbini et al (2012) (10K
for training, the rest equally divided between de-
velopment and test).
Amani (Artstein et al, 2009b; Artstein et al,
2011) is an advanced question-answering char-
acter used as a prototype for systems meant to
train soldiers to perform tactical questioning. The
users are in between real users and test subjects:
they were cadets at the U.S. Military Academy in
April 2009, who interacted with Amani as a uni-
versity course exercise on negotiation techniques.
They used head-mounted microphones to talk with
Amani.
This dataset comprises of 1.8K audio files each
annotated with one of the 105 possible NLU se-
mantic classes.
Radiobots (Roque et al, 2006) is a training pro-
totype that responds to military calls for artillery
fire in a virtual reality urban combat environment.
This is a domain in the slot-filling genre, where
there is a preferred protocol for the order in which
information is provided and confirmed. Users are
generally trainees, learning how to do calls for fire,
they are motivated users with some training. The
semantic processing involved tagging each word
with the dialogue act and parameter that it was as-
sociated with (Ai et al, 2007b).
This data set was collected during the develop-
ment of the system in 2006 at Fort Sill, Oklahoma,
during two evaluation sessions from recruited vol-
unteer trainees who performed calls for specific
missions (Robinson et al, 2006). These subjects
used head-mounted microphones rather than the
ASTI simulated radios from later data collection.
SASO-EN (Traum et al, 2008) is a negotiation
training prototype in which two virtual characters
negotiate with a human ?trainee? about moving a
medical clinic. The genre is negotiation and plan-
ning, where the human participant must try to form
a coalition, and the characters reason about utili-
ties of different proposals, as well as causes and
effects. The output of NLU is a frame represen-
tation including both semantic elements, like the-
matic argument structure, and pragmatic elements,
such as addressee and referring expressions. Fur-
ther contextual interpretation is performed by each
of the virtual characters to match the (possibly par-
tial) representation to actions and states in their
task model, resolve other referring expressions,
and determine a full set of dialogue acts (Traum,
2003). Speech was collected at the USC Insti-
tute for Creative Technologies (ICT) during 2006?
2009, mostly from visitors and new hires, who
acted as test subjects.
This dataset has 4K audio files each anno-
tated with one of the 117 different NLU semantic
classes.
4 ASR Performance
We tested each of the Datasets described in Sec-
tion 3.3 with some of the recognizers described
in Section 2. All recognizers were tested on the
Amani, SASO-EN, and Twins domains, and we
also tested a natural language understanding com-
ponent on these data sets (Section 5). For SGT
Blackwell, SGT STAR, and Radiobots, we report
the performance on the same development set used
in Yao et al (2010). For Amani and SASO-EN
(where we also report the NLU performance), we
run a 10-fold cross-validation in which 9 folds
where used to train the NLU and ASR language
model and the 10th was used for testing. For the
Twins dialogue system, we used the same partition
into training, development and testing reported in
Morbini et al (2012) and the results reported here
are from the development set. Due to differences
in training/testing regimens, performance of sys-
tems are only comparable within each domain.
Table 2 summarizes the performance of the var-
ious ASR engines on the evaluation data sets. Per-
formance is measured as Word Error Rate and was
obtained using the NIST SCLITE tool.12
Note that only Otosense-Kaldi in the Twins do-
main had adapted acoustic models. In the remain-
12http://www.itl.nist.gov/iad/mig/tools/
399
Speech recognizer Evaluation data setAmani Radiobots SASO-EN SGT Blackwell SGT Star Twins
Pocketsphinx 39.7 11.8 28.4 51 28.6 81
Apple 28 ? 30.9 ? ? 29
AT&T 29 12.1 16.3 27.3 21.7 28.8
Google 23.8 36.3 20 18 26 20.6
Otosense-Kaldi 33.7 ? 22.1 ? ? 18.7
Table 2: Word Error Rates (%) for the various dialogue systems and ASR systems tested.
ing cases only the language model was adapted.
Looking at the results on the development set re-
ported in Yao et al (2010), we have improvements
in 3 out of 5 domains: Amani (?11.8% Google),
SASO-EN (?11.7% AT&T) and SGT Blackwell
(?13% Google). In Radiobots and SGT Star the
performance achieved with just language model
adaptation, when permitted, is worse: +4.8% and
+1.7% respectively.
We find that there is no single best performing
speech recognizer: results vary greatly between
the evaluation test sets. In 4 of the 6 datasets over-
all, and 2 of the 3 datatests tested with Otosense-
Kaldi, the best performer is a cloud-based ser-
vice (Google or AT&T). There are two datasets
for which a local, fully customizable recognizer
performs better than the cloud-based services. Ra-
diobots, consisting of military calls for artillery
fire, has a fairly limited and very specialized vo-
cabulary, and indeed the two recognizers with cus-
tom language models (Pocketsphinx and AT&T)
perform much better than the non-customizable
recognizer (Google).
The Twins dataset is unique in that for the
Otosense-Kaldi system we custom-trained acous-
tic and language models, while standard WSJ
acoustic models and adapted language models
were used for the other dialogue systems. In
both cases the models were triphone based with
a Linear Discriminant Analysis (LDA) front end,
and Maximum Likelihood Linear Transforma-
tion (MLLT) and Maximum Mutual Information
(MMI) training. This reflects on the very good
performance in the Twins domain, decent perfor-
mance on the SASO-EN domain (reasonable mis-
match of WSJ and SASO-EN) and very degraded
performance in Amani (highly mismatched Amani
and WSJ domains). The observed degradation in
performance is accentuated by the MMI discrim-
inative training on the mismatched-WSJ data. As
with PocketSphinx and Watson, and unlike with
Apple Dictation and Google Speech API, with
Kaldi we fully control experimental conditions
and can guarantee no contamination of the train-
test data.
In summary, our evaluation shows that cus-
tomizable recognizers are useful when the ex-
pected speech is highly specialized, or when sub-
stantial resources are available for tuning the rec-
ognizer.
5 NLU Accuracy & Relation between
ASR and NLU
While the different genres of system have different
types of output for NLU: response text, dialogue
act and parameter tags, speech acts, or semantic
frames, many of them can be coerced into a se-
lection task, in which the NLU selects the right
output from a set of possible outputs. This allows
any multiclass classification algorithm to be used
for NLU. A possible drawback is that for some
inputs, the right output might not be available in
the set considered by the training data, even if it
might easily be constructed from known parts us-
ing a generative approach.
A second issue is that even though we can cast
the problem as multi-class classification, classifi-
cation accuracy is not always the most appropriate
metric of NLU quality. For question-answering
characters, getting an appropriate and relevant re-
ply is more important than picking the exact re-
ply selected by a human domain designer or an-
notator: there might be multiple good answers, or
even the best available answer might not be very
good. For that reason, the question-answering
characters allow an ?off-topic? answer and Error-
return plots (Artstein, 2011) might be necessary
to choose an optimal threshold. For the SASO-EN
system, slot-filler metrics such as precision, recall,
and f-score are more appropriate than frame accu-
400
racy, because some frames may have many slots
in common and few that are different (e.g. just a
different addressee). Nonetheless, we begin our
analysis within this common framework. For sim-
plicity, we start with just three domains: Twins,
Amani, and SASO-EN. SGT STAR and Blackwell
are very similar to Twins in terms of NLU. Ra-
diobots is more challenging to coerce to multiclass
classification.
Conventional wisdom in the speech and lan-
guage processing community is that performance
of ASR and NLU are closely tied: improved
speech recognition leads to better language under-
standing, while deficiencies in speech recognition
cause difficulty in understanding. This conven-
tional wisdom is borne out by decades of experi-
ence with speech and dialogue systems, though we
are not aware of attempts to systematically demon-
strate it. The present study shows that the expected
relation between speech recognition and language
understanding holds for the systems we tested.
Accepted assumptions about the relation be-
tween speech recognition and language under-
standing have been repeatedly challenged. Direct
challenges are typically limited to specific appli-
cations. Wang et al (2003) show that for a slot-
filling NLU, ASR can be specifically tuned to rec-
ognize those words that are relevant to the slot-
filling task, resulting in improved understanding
despite a decrease in performance on overall word
recognition. However, Boros et al (1996) found
that when not optimizing the ASR for the specific
slot filling task there is a nearly linear correlation
between word accuracy and NLU accuracy. Al-
shawi (2003) and Huang and Cox (2006) show that
in call-routing applications the word level can be
dispensed with altogether and calls routed based
on phonetic information alone without noticeable
loss in performance. These challenges suggest that
the speech-language divide is not as clean as the
theory suggests.
To investigate the relation between ASR and
NLU, we ran each ASR output from each of
the 5 recognizers through an understanding com-
ponent to obtain an NLU output (each dataset
had a separate NLU component, which was held
constant for all speech recognizers). ASR and
NLU performance are conventionally measured on
scales of opposite polarity: better performance
shows up as lower word error rates but higher
NLU accuracies. For the correlations we invert the
conventional ASR scale and use word accuracy, so
that higher numbers signify better performance on
both scales.13
Figure 1 shows the results obtained in the 3 di-
alogue systems by the various ASR systems. The
figures plot ASR performance against NLU per-
formance; NLU results on manual transcriptions
are included for comparison. There are too few
data points for the correlations between ASR and
NLU performance to be significant, but the trends
are positive, as expected.
Our experiments lend supporting evidence to
the claim that in general, ASR performance is pos-
itively linked to NLU performance (special cases
notwithstanding). The 3 datasets exhibit posi-
tive correlations between speech recognition and
language understanding performance. Thus, we
claim that the basis of the conventional wisdom
is sound: speech recognition directly affects lan-
guage understanding. This conclusion holds when
the speech recognizer has been optimized to pro-
duce the most accurate transcript, rather than for a
specific NLU.
6 Conclusion and Future Work
We have extended here the ASR system evaluation
published in Yao et al (2010) including some new
cloud based ASR services that achieve very good
performance showing an improvement of around
12%. We also showed that ASR and NLU perfor-
mance are correlated.
One possible avenue of future work is to ex-
tract importance weights for each word from the
learnt NLU models and use these weights to try
to explain those cases that diverge from the corre-
lation between ASR and NLU performance. This
may also give us a better measure than WER for
assessing ASR performance in dialogue systems.
Another avenue of future work involves examin-
ing different types of NLU engines, and different
metrics for the different dialogue system genres,
which, again, may lead to a more relevant assess-
ment of ASR performance.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
13We define ?accuracy? as 1 minus WER, so this number
can in principle dip below zero if there are more errors than
words.
401
Amani
r = 0.54, df = 3, p = 0.345
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
50 60 70 80 90 100
51
54
57
60
63
66
69
?
?
?
? ?
?
SASO-EN
r = 0.77, df = 3, p = 0.130
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
60 70 80 90 100
30
40
50
60
70
80
90
?
???
?
?
Twins
r = 0.99, df = 3, p = 0.002
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
0 20 40 60 80 100
40
50
60
70
80
90
100
?
?
?
?
?
Figure 1: Relation between ASR and NLU performance (red dots are manual transcriptions)
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athana-
sios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David Traum. 2012. The Twins cor-
pus of museum visitor questions. In LREC-2012,
Istanbul, Turkey, May.
Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi,
and Diane Litman. 2007a. Comparing spoken dia-
log corpora collected with recruited subjects versus
real users. In SIGdial 2007.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007b. Using information state to improve
dialogue move identification in a spoken dialogue
system. In Proceedings of the 10th Interspeech Con-
ference, Antwerp, Belgium, August.
James F. Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional systems. In IUI, pages 1?8.
Hiyan Alshawi. 2003. Effective utterance classifica-
tion with unsupervised phonotactic models. In HLT-
NAACL 2003, pages 1?7, Edmonton, Alberta, May.
Apple Inc. 2012. Mac basics: Dictation (Technote
HT5449), November.
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and
D. Traum. 2009a. Semi-formal evaluation of con-
versational characters. In O. Grumberg, M. Kamin-
ski, S. Katz, and S. Wintner, editors, Languages:
From Formal to Natural. Essays Dedicated to Nis-
sim Francez on the Occasion of His 65th Birthday,
volume 5533 of Lecture Notes in Computer Science,
pages 22?35. Springer, Berlin.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009b. Viability of a simple dia-
logue act scheme for a tactical questioning dialogue
system. In DiaHolmia 2009: Proceedings of the
13th Workshop on the Semantics and Pragmatics of
Dialogue, page 43?50, Stockholm, Sweden, June.
Ron Artstein, Michael Rushforth, Sudeep Gandhe,
David Traum, and MAJ Aram Donigian. 2011.
Limits of simple dialogue acts for tactical question-
ing dialogues. In 7th IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Barcelona, Spain, July.
Ron Artstein. 2011. Error return plots. In 12th SIG-
dial Workshop on Discourse and Dialogue, Port-
land, OR, June.
Linda Bell and Joakim Gustafson. 2003. Child and
adult speaker adaptation during error resolution in a
publicly available spoken dialogue system. In IN-
TERSPEECH 2003.
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: dialog management using hierarchical task de-
composition and an expectation agenda. In INTER-
SPEECH 2003.
M. Boros, W. Eckert, F. Gallwitz, G. Grz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In In Proceedings of (ICSLP 96), pages
1009?1012.
Sudeep Gandhe, Nicolle Whitman, David R. Traum,
and Ron Artstein. 2009. An integrated authoring
tool for tactical questioning dialogue systems. In 6th
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Pasadena, California, July.
Sudeep Gandhe, Michael Rushforth, Priti Aggarwal,
and David R. Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In Proceedings of
Interspeech-11, Florence, Italy, 08/2011.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
402
Liza Hassel and Eli Hagen. 2006. Adaptation of an
automotive dialogue system to users? expertise and
evaluation of the system. Language Resources and
Evaluation, 40(1):67?85.
Quiang Huang and Stephen Cox. 2006. Task-
independent call-routing. Speech Communication,
48(3?4):374?389.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006
Proceedings. 2006 IEEE International Conference
on, volume 1, pages I?I.
Kristiina Jokinen and Kari Kanto. 2004. User ex-
pertise modeling and adaptivity in a speech-based
e-mail system. In Donia Scott, Walter Daelemans,
and Marilyn A. Walker, editors, ACL, pages 87?94.
ACL.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Anton Leuski and David R. Traum. 2011. NPCEditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32:42?56.
Anton Leuski, Brandon Kennedy, Ronakkumar Patel,
and David Traum. 2006a. Asking questions to
limited domain virtual characters: How good does
speech recognition have to be? In 25th Army Sci-
ence Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006b. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27.
Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,
Maarten Van Segbroeck, Kenji Sagae, Panayio-
tis S. Georgiou, David R. Traum, and Shrikanth S.
Narayanan. 2012. A reranking approach for recog-
nition and classification of speech input in conversa-
tional dialogue systems. In SLT, pages 49?54. IEEE.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka?s?
Burget, Ondr?ej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motl??c?ek, Yanmin Qian, Petr
Schwarz, Jan Silovsky?, Georg Stemmer, and Karel
Vesely?. 2011. The Kaldi speech recognition
toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding, December.
S.M. Robinson, A. Roque, A. Vaswani, D. Traum,
C. Hernandez, and B. Millspaugh. 2006. Evalua-
tion of a spoken dialogue system for virtual reality
call for fire training. In 25th Army Science Confer-
ence, Orlando, Florida, USA.
S. Robinson, D. Traum, M. Ittycheriah, and J. Hen-
derer. 2008. What would you ask a conversational
agent? Observations of human-agent dialogues in
a museum setting. In Proc. of Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
A. Roque, A. Leuski, V. Rangarajan, S. Robinson,
A. Vaswani, S. Narayanan, and D. Traum. 2006.
Radiobot-CFF: A spoken dialogue system for mil-
itary training. In Proc. of Interspeech, Pittsburgh,
Pennsylvania, USA.
W. Swartout, D. Traum, R. Artstein, D. Noren, P. De-
bevec, K. Bronnenkant, J. Williams, A. Leuski,
S. Narayanan, D. Piepol, C. Lane, J. Morie, P. Ag-
garwal, M. Liewer, J. Chiang, J. Gerten, S. Chu,
and K. White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In
J. Allbeck, N. Badler, T. Bickmore, C. Pelachaud,
and A. Safonova, editors, Intelligent Virtual Agents:
10th International Conference, IVA 2010, Philadel-
phia, PA, USA, September 20?22, 2010 Proceed-
ings, volume 6356 of Lecture Notes in Artificial In-
telligence, pages 286?300. Springer, Heidelberg.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In IVA, pages 117?130.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
David Traum. 2003. Semantics and pragmatics of
questions and answers for dialogue agents. In pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 380?394.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is
word error rate a good indicator for spoken lan-
guage understanding accuracy. In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU ?03), pages 577?582.
Xuchen Yao, Pravin Bhutada, Kallirroi Georgila, Kenji
Sagae, Ron Artstein, and David R. Traum. 2010.
Practical evaluation of speech recognizers for vir-
tual human dialogue systems. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, LREC. European Language
Resources Association.
403
Proceedings of the SIGDIAL 2014 Conference, pages 69?73,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Improving Classification-Based Natural Language Understanding with
Non-Expert Annotation
Fabrizio Morbini and Eric Forbell and Kenji Sagae
Institute for Creative Technologies
University of Southern California
Los Angeles, CA 90094, USA
{morbini,forbell,sagae}@ict.usc.edu
Abstract
Although data-driven techniques are com-
monly used for Natural Language Under-
standing in dialogue systems, their effi-
cacy is often hampered by the lack of ap-
propriate annotated training data in suffi-
cient amounts. We present an approach
for rapid and cost-effective annotation of
training data for classification-based lan-
guage understanding in conversational di-
alogue systems. Experiments using a web-
accessible conversational character that in-
teracts with a varied user population show
that a dramatic improvement in natural
language understanding and a substantial
reduction in expert annotation effort can
be achieved by leveraging non-expert an-
notation.
1 Introduction
Robust Natural Language Understanding (NLU)
remains a challenge in conversational dialogue
systems that allow arbitrary natural language input
from users. Although data-driven approaches are
now commonly used to address the NLU problem
as one of classification, e.g. (Heintze et al., 2010;
Leuski and Traum, 2010; Moreira et al., 2011),
where input utterances are mapped automatically
into system-specific categories, the dependence of
such approaches on training data annotated with
semantic classes or dialogue acts creates a chicken
and egg problem: user utterances are needed to
create the annotated training data necessary for
NLU by classification, but these cannot be col-
lected without a working system that users can in-
teract with.
Common solutions to this problem include the
use of Wizard-of-Oz data collection, where a hu-
man expert manually provides the functionality of
data-driven modules while data is collected from
users, or the use of scenario authors who attempt
to anticipate user input to create an initial set of
training data. While these options offer practical
ways around the training data acquisition prob-
lem, they typically require substantial work from
system experts and provide suboptimal solutions:
data-driven approaches work best when utterances
in the training data are drawn from the same distri-
bution as those encountered in actual system use,
but the conditions under which training data is col-
lected (a human expert filling in for systems mod-
ules, or a human expert generating possible user
utterances) are quite different from those where
users interact with the final system. High qual-
ity results are often obtained through an iterative
process where an initial training set is authored
by a scenario designer, but NLU resources are
gradually updated based on real user data over
time (Gandhe et al., 2011). Although this can ulti-
mately produce training data composed primarily
of real user utterances, and therefore result in bet-
ter performance from data-driven models, an ex-
pert annotator is required to perform manual clas-
sification of user utterances. This is a laborious
process that assumes availability and willingness
of the annotator for as long as it takes to collect
enough user utterances, which may range from
weeks to months or even years, depending on the
size of the domain and the number and type of ut-
terance categories.
The main question we address is whether an-
notation by non-experts can be leveraged to speed
up utterance classification and lower its cost. We
present a technique that frames the annotation of
training data as a human intelligence task suit-
able for crowdsourcing. Although there are sim-
ilarities between our technique and active learning
(e.g. see (Gambck et al., 2011)), an important dif-
ference is that our technique does not reduce the
annotation effort by reducing the size of the data
to be labeled, but by casting the annotation task
into a simpler problem. This allows us to take ad-
vantage of the entire data generated by the users.
Through an experiment with a conversational dia-
69
logue system deployed on the web, we show that a
dramatic improvement in the quality of NLU can
be achieved with non-expert data annotation, re-
ducing the time required of an expert annotator by
70%.
2 Improving understanding with data
Our approach for creating accurate utterance clas-
sifiers for NLU in conversational dialogue systems
is based on a simple strategy, which we describe
next in general terms. NLU is assumed to be per-
formed through multiclass classification.
The first step is to create a small initial train-
ing dataset T
0
either through Wizard-of-Oz data
collection or by generation of utterances by a sys-
tem developer or content author. This training set
is used to train a NLU model M
0
. Although this
model is likely to be inadequate, it allows users
to interact with an initial version of the system.
As input utterances are collected from real users,
these utterances are annotated with their desired
NLU output labels. Periodically, at time i, we add
to the initial training dataset T
0
the annotated user
utterances accumulated up to that point. We train
a new NLU model M
i
using this augmented train-
ing set, T
i
.
1
We also keep aside a small fraction
of utterances to test the performance of the NLU
models, that is, at each time i we also have an eval-
uation set E
i
and the union of E
i
and T
i
is the en-
tire set of user utterances collected up to time i. As
more utterances are added and annotated, an NLU
model M
i
is expected to surpass the initial model
M
0
. In general, we replace the running NLU
model M
r
whenever we have a better perform-
ing M
i
model. This straightforward process can
be used to obtain increasingly more accurate lan-
guage understanding, at the cost of data annotation
in the form of labelling utterances with categories
that are defined according to the needs of the spe-
cific system and the specific domain. The cate-
gories may be based on dialogue acts, e.g. (Core
and Allen, 1997; Bunt et al., 2010), user informa-
tion needs, e.g. (Moreira et al., 2011), or stand
in for entire semantic frames, e.g. (DeVault and
Traum, 2013). The technical nature of the task of
categorizing utterances in schemes such as these
usually means that substantial time is required of
an expert annotator.
2.1 Annotation as a human intelligence task
Although the task of annotating NLU training data
involves assigning categories with technical defi-
1
For every time i and j with i < j it holds that T
i
? T
j
.
nitions to utterances, and therefore would appear
to require knowledge of these technical defini-
tions, in fact the task requires primarily the type
of language understanding that is common to all
native speakers of a given language. Our main hy-
pothesis is that this annotation can be structured
as a trivial task that requires no specific exper-
tise, and that annotations performed this way can
have a substantial impact on the quality of utter-
ance classification. We define the NLU annotation
task as follows.
Before annotation begins, each utterance cate-
gory in the system is associated with one or more
canonical utterance(s) that capture the meaning
and communicative intent of that category. These
canonical utterances allow for trivial generation
of surface text from NLU categories by simple
lookup.
Given a set of dialogues between the system
and individual users, each dialogue is split into
segments. Segments are typically composed of
a user utterance and the preceding system utter-
ance. In cases where users produce multiple utter-
ances without intervening system utterances, seg-
ments may consist of a system utterance followed
by consecutive user utterances, so that there is ex-
actly one segment corresponding to each user ut-
terance u in the dialogue and each segment starts
with a system utterance and ends with u.
Annotators work on one dialogue at a time, and
within a dialogue they are presented with one seg-
ment at a time, following the order in which the
utterances appear in the dialogue. Assignment
of a system category to the utterance targeted in
a segment is a multiple choice task, involving n
choices, one of which is none of the above. The re-
maining n?1 choices are generated automatically
by running the current NLU utterance classifier on
the target utterance, and generating the canonical
surface realizations of each of the n ? 1 highest
scoring categories. The annotator is then asked to
choose the option containing language that means
the same as the targeted utterance in the segment.
The choice of n depends on the quality of the cur-
rent NLU model, and must be made to balance the
length of the list of options with the expected num-
ber of times the appropriate choice is not in the list
of options. The more none of the above annota-
tions are performed, the less effective the annota-
tion exercise will be in reducing the effort required
by an annotation expert.
The utterances annotated as none of the above
are given to an expert for annotation. The impact
70
System Hello and welcome.
...
User Are you married?
System Yeah, forty-four years this September. I can?t be-
lieve Linda?s stuck with me this long.
System Okay, let?s get to it. Why are you here today?
User I?ve been having trouble sleeping lately.
System This must be hard for you.
System Are you feeling stressed at all?
User Yes, quite a bit
System So, I?ve got some questions I need to ask you, to get
to know you better.
System Just double checking, you haven?t been sleeping
good. Is that right?
User That?s right
System Have you been having any nightmares?
User Every once in a while, but mostly I can?t stay asleep
...
Figure 1: Excerpt of a sample interaction with a
Simcoach character over the web. User input is
typed into a text box on a web page.
of this procedure on how quickly user utterances
are annotated as training examples for the NLU
and how much expert time is saved depends on
the quality of the initial classification model and
the choice of n. These are practical factors that are
expected to vary greatly from one dialogue system
to another. In the next section, we present one ap-
plication of this procedure to an existing conversa-
tional dialogue system deployed on the web, and
show examples of dialogue segments and annota-
tion options.
3 Experiment
To test our hypothesis that language understand-
ing can be improved with much reduced expert ef-
fort, we applied the framework described above to
a system that implements a conversational char-
acter that talks with users about issues relating
to mental and behavioral disorders and presents
health care options. The system is publicly ac-
cessible at http://www.simcoach.org, and receives
traffic on the order of one hundred users per week.
Of these, about one quarter engage the system in
a meaningful dialogue with multiple turns, with
the dialogues containing on average 16 user utter-
ances. Because our process depends crucially on
user traffic to generate data for annotation, a web-
accessible system is ideally suited for it. An ex-
cerpt from a typical interaction with the system is
shown in Figure 1. The system and the NLU clas-
sifier based on Maximum Entropy models (Berger
et al., 1996) are described respectively in (Rizzo et
al., 2011) and (Sagae et al., 2009).
3.1 Data collection
Starting with an initial system deployed with an
NLU model trained with data generated by an au-
thor attempting to anticipate user behavior, we ap-
plied the approach described in section 2 to im-
prove NLU accuracy over a period of approxi-
mately five months. The initial accuracy of the
NLU classifier was 62%, measured as the number
of utterances classified correctly divided by the to-
tal number of user utterances. This accuracy fig-
ure was obtained only after the five months of data
annotation, using the heldout set of manually an-
notated dialogues.
Although the data annotation procedure as de-
scribed in section 2 could in principle be per-
formed continuously as user data come in, we
instead performed all of our annotation in three
rounds, the first consisting of approximately 2,000
user utterances, the second one month later, con-
sisting of an additional 1,000 utterances. The last
round, collected about two months later, contained
about 2,000 utterances. We used five annotators
2
working in parallel, and the average speed of each
annotator exceeded 500 utterances per hour.
The total number of NLU utterance classes in
the system is 378, although only 120 classes were
used by annotators in all rounds of annotation to
cover all of the utterances collected
3
. In our an-
notation exercise we set the number of multiple
choice items at n = 6, including 5 choices gener-
ated from categories chosen by the NLU classifier,
and one none of the above choice. Figure 2 shows
a sample dialogue segment with the corresponding
multiple choice items. During annotation, clicking
on a multiple choice item advances the annotation
by presenting the next segment containing a user
utterance to be annotated.
3.2 Results
Of the utterances in the three rounds of data col-
lection, respectively 29%, 34% and 17% were
marked by annotators as none of the above. These
were given to a developer of the NLU system who
assigned a category to each of them. In this ex-
pert annotation step the choice is not restricted to
a small set of options, and may be any of the cat-
egories in the system. Given this rate of use of
2
The non-expert annotators belonged to the same team
that developed the system but did not participate in the de-
velopment of the NLU module and the NLU classes used in
the particular dialogue system used.
3
This difference is a further evidence of the difficulty of
correctly anticipating how the end users will interact with the
dialogue system.
71
System Okay, let?s get to it. Why are you here today?
User I?ve been having trouble sleeping lately.
Which of the following options correspond most
closely to the last user utterance? If none of them have
the same general meaning as the user utterance, select
?none of the above.?
(a) I have been in a bad mood lately
(b) I have nightmares often
(c) I haven?t been sleeping well
(d) My family is worried about me
(e) I eat too much
(f) None of the above
Figure 2: Example of a dialogue segment with cor-
responding multiple choice items. The annotation
task consists of choosing the item that has approx-
imately the same meaning and communicative in-
tent as the targeted utterance (the user utterance).
the none of the above category, the need for ex-
pert annotation is not eliminated, but the amount
of expert effort necessary is reduced by over 70%.
The NLU classification accuracy figures ob-
tained after each round of annotation are shown in
Table 1. In the table, Our Approach represents the
results obtained by the technique described here.
A large improvement is observed after the first
round of annotation, with a more modest improve-
ment observed after the other two rounds. The ini-
tial jump in accuracy after round 1 is explained
by the fact that the initial model based on a sys-
tem author?s expectation of what users may say to
the system (approximately 3,000 utterances) is im-
proved using utterances that users did in fact pro-
duce in real interactions with the system. Clearly,
a more well-matched distribution of utterances in
the training data produces higher accuracy.
To assess the value of our approach, we com-
pare it with two other reasonable experimental
conditions: a baseline where only expert annota-
tion is used (Expert Only), and a condition where
no expert annotation is used (No Expert). The Ex-
pert Only condition is meant to represent what can
be achieved with the same workload for the expert
used in Our Approach. This is achieved by random
selection of user utterances to create a set with
the same number of utterances set aside for ex-
pert annotation in Our Approach. The expert then
annotates each of these utterances to create train-
ing data. For the No Expert condition, we used
only utterances annotated by non-experts, leaving
out completely utterances labeled as none of the
NLU accuracy after
each annotation round [%]
Base 1st 2nd 3rd
round round round
Our Approach 62 70 73 78
Expert Only 62 64 68 70
No Expert 62 64 65 71
Table 1: NLU accuracy obtained using the initial
training dataset T
0
, after one round of annotation
with T
1
(2,013 utterances), after two rounds of an-
notation with T
2
(additional 948 utterances), and
after three rounds with T
3
(additional 1806 utter-
ances). Accuracy is estimated on the same heldout
set of dialogues E
3
for all conditions, accounting
for roughly 10% of the annotated data.
above. Both Expert Only and No Expert condi-
tions achieve significantly lower performance than
the approach described here. This indicates that
expert annotation is important, but also that cheap
and fast non-expert annotation can provide sub-
stantial improvements to NLU.
4 Conclusion
We described a framework for annotation of train-
ing data by non-experts that can provide dramatic
improvements to natural language understanding
in dialogue systems that perform NLU through ut-
terance classification. Our approach transforms
the annotation NLU training data into a task that
can be performed by anyone with language profi-
ciency. Annotation is structured as a simple mul-
tiple choice task, easily delivered over the web.
Using our approach with a conversational char-
acter on the web, we improved NLU accuracy
from 62% to 78% using only less than 30% of the
effort it would be required of an expert to annotate
data without non-expert annotation.
Acknowledgments
We thank Kelly Christoffersen, Nicolai Kalisch
and Tomer Mor-Barak for data annotation and up-
dates to the SimCoach system, David Traum for
insightful discussions, and the anonymous review-
ers. The effort described here has been sponsored
by the U.S. Army. Any opinions, content or infor-
mation presented does not necessarily reflect the
position or the policy of the United States Govern-
ment, and no official endorsement should be in-
ferred.
72
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39?71, March.
Harry Bunt, Jan Alexandersson, Jean Carletta, Jae-
Woong Choe, Alex Chengyu Fang, Koiti Hasida,
Kiyong Lee, Volha Petukhova, Andrei Popescu-
Belis, Laurent Romary, Claudia Soria, and David
Traum. 2010. Towards an iso standard for dia-
logue act annotation. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike
Rosner, and Daniel Tapias, editors, Proceedings
of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta, may. European Language Resources
Association (ELRA).
Mark G. Core and James F. Allen. 1997. Coding di-
alogues with the DAMSL annotation scheme. In
David Traum, editor, Working Notes: AAAI Fall
Symposium on Communicative Action in Humans
and Machines, pages 28?35, Menlo Park, Califor-
nia. AAAI, American Association for Artificial In-
telligence.
David DeVault and David Traum. 2013. A method
for the approximation of incremental understanding
of explicit utterance meaning using predictive mod-
els in nite domains. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, GA, June.
Bjrn Gambck, Fredrik Olsson, and Oscar Tckstrm.
2011. Active learning for dialogue act classification.
In INTERSPEECH, pages 1329?1332. ISCA.
Sudeep Gandhe, Michael Rushforth, Priti Aggar-
wal, and David Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In 12th Annual Con-
ference of the International Speech Communication
Association (InterSpeech 2011), Florence, Italy, Au-
gust.
Silvan Heintze, Timo Baumann, and David Schlangen.
2010. Comparing local and sequential models
for statistical incremental natural language under-
standing. In Raquel Fern?andez, Yasuhiro Kata-
giri, Kazunori Komatani, Oliver Lemon, and Mikio
Nakano, editors, SIGDIAL Conference, pages 9?16.
The Association for Computer Linguistics.
Anton Leuski and David R. Traum. 2010. Practical
language processing for virtual humans. In Twenty-
Second Annual Conference on Innovative Applica-
tions of Artificial Intelligence (IAAI-10).
Catarina Moreira, Ana Cristina Mendes, Lu??sa Coheur,
and Bruno Martins. 2011. Towards the rapid devel-
opment of a natural language understanding mod-
ule. In Proceedings of the 10th International Con-
ference on Intelligent Virtual Agents, IVA?11, pages
309?315, Berlin, Heidelberg. Springer-Verlag.
Albert A. Rizzo, Belinda Lange, John G. Buckwalter,
E. Forbell, Julia Kim, Kenji Sagae, Josh Williams,
Barbara O. Rothbaum, JoAnn Difede, Greg Reger,
Thomas Parsons, and Patrick Kenny. 2011. An in-
telligent virtual human system for providing health-
care information and support. In Studies in Health
Technology and Informatics.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language
understanding of partial speech recognition results
in dialogue systems. In Short Paper Proceedings of
the North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nologies (NAACL HLT) 2009 conference.
73
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 50?58,
Dublin, Ireland, August 24 2014.
 Verbal Behaviors and Persuasiveness in Online Multimedia Content 
 Moitreya Chatterjee, Sunghyun Park*, Han Suk Shim*, Kenji Sagae and Louis-Philippe Morency USC Institute for Creative Technologies Los Angeles, CA 90094 metro.smiles@gmail.com,  { park, hshim, sagae, morency }@ict.usc.edu  Abstract Persuasive communication is an essential component of our daily lives, whether it is negotiat-ing, reviewing a product, or campaigning for the acceptance of a point of view. With the rapid expansion of social media websites such as YouTube, Vimeo and ExpoTV, it is becoming ev-er more important and useful to understand persuasiveness in social multimedia content. In this paper we present a novel analysis of verbal behavior, based on lexical usage and para-verbal markers of hesitation, in the context of predicting persuasiveness in online multimedia content. Toward the end goal of predicting perceived persuasion, this work also explores the potential differences in verbal behavior of people expressing a positive opinion (e.g., a posi-tive movie review) versus a negative one. The analysis is performed on a multimedia corpus of 1,000 movie review videos annotated for persuasiveness. Our results show that verbal be-havior can be a significant predictor of persuasiveness in such online multimedia content.  1 Introduction A message that is ?intended to shape, reinforce or change the responses of another or others? is cate-gorized as persuasive communication (Miller, 1980), and it is particularly important for the role it plays in creating social influence and altering other people?s opinions (Reardon, 1991; Zimbardo and Leippe, 1991). For instance, a persuasive advertisement could be a potential profit churner. The growth of social networking sites on the Internet has resulted in an explosion of online content with the purpose of delivering persuasive messages. Websites such as YouTube, Vimeo and ExpoTV are examples of online media in which these messages propagate mainly in the form of videos. ExpoTV, in particular, is a repository of a large number of videos dedicated for product reviews in which people try to convince others in favor of or against the use of various products. This raises an interesting research problem as to what it is that makes certain speakers have a substantial impact on others? opinions while other speakers are ignored. In this paper, we present a novel analysis of spoken persuasion in online multimedia content. Our work is motivated by prior research findings in psychology indicating that verbal behavior is a prom-ising indicator for persuasive communication (Chaiken and Eagly, 1979; Werner, 1982). Such prior findings allow us to hypothesize that two primary types of verbal features will be predictive of per-suasion: lexical features and paraverbal markers of hesitation. Additionally we explore the relation-ship of the sentiment of the content and perceived persuasion, by hypothesizing that speakers? exhibit different verbal behavior when expressing a positive opinion versus a negative one and taking into account these differences will improve prediction performance. We conduct several experiments in order to validate these hypotheses using a multimedia corpus of 1,000 movie review videos obtained from ExpoTV.com, which is a great source of online reviews. Our experiments followed by a detailed analysis also reveal a set of predictive features which characterize persuasive online presentations. In the following section, we present an overview of related work. Section 3 elaborates on our re-
* Both authors contributed equally to this work. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/  
50
search hypotheses. In Section 4, we present our multimedia corpus. The details of the experiments with computational descriptors and methodology are described in Section 5. We discuss the results and findings in Section 6, and finally we conclude our paper and present some future directions of research in Section 7.  2 Related Work Content in the form of written text are omnipresent in our society. Starting from books, magazines and newspapers to the now prevalent emails and blog posts, text-based content are an invaluable component for effective communication. Prior research reports possibly greater persuasiveness in written messages compared to visual or acoustic modalities in certain situations (Chaiken and Eagly, 1979; Werner, 1982). Past research has also revealed that for sophisticated messages, such as those used in a martial setting, written messages are more persuasive (Chaiken and Eagly, 1979).  Although the importance of studying verbal behavior for determining persuasiveness has been un-derscored in prior work in the field of communication sciences (O?Keefe, 2002) and this line of re-search gives us useful pointers to the factors that contribute to persuasiveness in text or verbal com-munication, they present no computational aspect, which is where we put our emphasis in the paper. In the field of natural language processing, text classification based on bag-of-words has been a long standing approach (Lewis and Gale, 1994; Mitchell, 1997; Dave et al., 2003). In fact, Young et al. (2011) have explored lexical features in the specific context of predicting persuasion, but they fo-cus their attention on studying persuasion in dialogue. Our work draws inspiration from such ap-proaches but explores it in the specific context of predicting persuasiveness in online multimedia con-tent using lexical and paraverbal features. 3 Research Hypotheses Motivated by prior works and theoretical background, we designed our experiments to validate three hypotheses. Since multiple prior works point to the usefulness of the text modality in persuasive communication and also to the power of text classification with lexical features in various tasks, we explored the fea-sibility of capturing the difference in verbal behavior between persuasive and unpersuasive expres-sions of opinions in online social multimedia content (specifically, movie reviews). The following is the hypothesis that we specifically tested with our experiments: Hypothesis 1: Verbal behavior, as captured by lexical usage, is indicative of persuasiveness in online social multimedia content, irrespective of whether the opinion expressed is positive or negative. Paraverbal behaviors indicative of hesitation can constitute important information for predicting persuasiveness. For instance, a speaker?s stuttering or breaking his/her speech with filled pauses (such as um and uh) has influence on how other people perceive his/her persuasiveness. Although previous work (DeVault et al, 2013) suggests paraverbal behavior may be indicative of depression, another work on emotion prediction however, (Devillers et al., 2006) raised questions about its predictive power when compared to using standard cues derived from lexical usage. This leads us to our second hypothesis on paraverbal behaviors in the context of predicting persuasiveness: Hypothesis 2: Paraverbal behaviors related to hesitation are indicative of persuasiveness in online social multimedia content.  Past research highlights the importance of the knowledge of the affective state of a document towards its perceived persuasiveness (Murphy, 2001). We therefore hypothesize the following: Hypothesis 3: Knowledge of the sentiment polarity of a movie review improves classification of the speaker?s perceived persuasiveness. 4 Dataset ExpoTV.com is a popular website housing videos of product reviews. Each product review has a vid-eo of a speaker talking about a particular product as well as the speaker?s direct rating of the product on an integral scale from 1 star (for most negative review) to 5 stars (for most positive review). This direct rating is useful for the purpose of our study because this allows us to study perceived persua-sion under different directions of persuasion (in favor of or against). For instance, the speaker in a 5-
51
star movie review video would most likely try to persuade his/her audience in favor of watching the movie while the speaker in a 1-star movie review video would argue against watching the movie. We therefore collected a total of 1,000 movie review videos that were either highly positive or negative. The dataset consists of the following:  ? Positive Reviews: 500 movie review videos with 5-star rating (315 males and 185 females). ? Negative Reviews: 500 movie review videos with 1 or 2-star rating, consisting of 216 1-star videos (151 males and 65 females) and 284 2-star videos (212 males and 72 females). We included 2-star videos due to the lack of enough 1-star videos on the website.  Each video in the corpus has a frontal view of one person talking about a particular movie, and the average length of the videos is about 94 seconds. The corpus contains 372 unique speakers and 600 unique movie titles and is available to the community for purposes of academic research1. 4.1 Evaluation of Persuasiveness Amazon Mechanical Turk (AMT), which is a popular online crowdsourcing platform, was used to obtain subjective evaluation of each speaker?s perceived persuasiveness, following a similar annota-tion scheme as (Mohammadi et al., 2013). For each video in the corpus, we obtained 3 repeated eval-uations on the level of persuasiveness of the speaker by asking the workers to give direct rating on each speaker?s persuasiveness on a Likert scale from 1 (very unpersuasive) to 7 (very persuasive). A total of 50 native English-speaking workers based in the United States participated in the evaluation process online, and the task was evenly distributed among the 50 workers. To minimize gender influ-ence, the task was distributed such that the workers only evaluated speakers of the same gender. The correlation between the mean score of every movie and the individual ratings was found to be 0.7 on the average (Pearson?s Correlation Coefficient). Once the evaluation was complete, we used the mean persuasiveness score for each video as the ground-truth measure of the speaker?s perceived persuasiveness. In this initial effort, we focused on videos that were extremely persuasive or not persuasive at all. Hence, videos with a mean score of equal to or greater than 5.5 were taken as persuasive while those with a mean score of equal to or less than 2.5 were taken as unpersuasive. After this, we ended up with a total of 300 videos, specifically 157 videos of positive reviews (75 persuasive and 82 unpersuasive) and 143 videos of negative re-views (62 persuasive and 81 unpersuasive). 4.2 Transcriptions Using AMT and 18 participants from the same worker pool for persuasiveness evaluation, we ob-tained verbatim transcriptions of these filtered 300 videos, including transcriptions for filled pauses and stutters. Each transcription was reviewed and edited by multiple in-house experienced transcribers for accuracy. We do not use automatic speech recognition techniques in order to avoid noisy tran-scriptions. 5 Experiments In this section, we give details on the design of our computational descriptors followed by the experi-mental methodology. 5.1 Computational Descriptors In our experiments, our main focus was on devising computational descriptors for verbal behaviors in terms of lexical usage and also in terms of paraverbal markers of hesitation that can capture indica-tions of persuasiveness of the speaker.  Verbal (Lexical) Descriptors: As in many text classification tasks, we designed our verbal de-scriptors based on the bag-of-words representation using term frequency of both unigrams and bi-                                                1 Dataset available online: http://multicomp.ict.usc.edu/  
52
grams. Using the 300 filtered videos (see Section 4.1) and without feature selection, the numbers of unigrams reach around 4,500 and bigrams around 24,000. We did not proceed further with higher or-der n-grams because empirical evidence has shown that trigrams and other higher order n-grams do not always show improvement because they introduce problems related to the sparsity of features (Dave et al., 2003).   Paraverbal Descriptors of Hesitation: From the verbatim transcriptions of our corpus, we observed a set of frequent paraverbal cues that could potentially be associated with the level of persuasiveness. The set of descriptors is inspired from the findings of DeVault et al. (2013), who explored a similar set of generic paraverbal features in an interactive dialogue setting. However, we are interested specifically in the ones that capture signs of hesitation. The following were the descriptors that were used:  ? Pause-Fillers: The verbal behaviors of reviewers are often characterized with various pause-fillers, such as um or uh. In order to account for the varying length of each review, we normalized the count of all instances of filled pauses by the number of words spoken in the video. ? Disfluency Markers: A prominent marker of disfluency in human speech is stuttering. To capture this disfluency, we counted all instances of stuttering in each video and normalized them by the number of words spoken in the video. ? Articulation Rate: Articulation rate is defined as the rate of speaking in which all pauses are excluded from calculation (Dankovicova, 1999). This descriptor was computed by taking the ratio of the number of spoken words in each video to the actual time spent speaking.  ? Mean Span of Silence: Human speech is often interspersed with pauses. We therefore computed this descriptor, by measuring the total duration of silence during speech, normalized by the total length of the video. 5.2 Methodology We processed all the videos in our dataset and automatically extracted the indicated lexical and paraverbal features. The extracted features were then used for several classification experiments under three different settings to test our hypotheses: only positive reviews, only negative reviews (called  the sentiment-dependent classifiers) and a combined set of positive and negative reviews (called  the sentiment-independent classifiers). For each such setting, we divided the set of samples (transcription of movie reviews) into 5 balanced folds that were both speaker-independent and movie-independent. In other words, in all our experiments, no 2 folds contained samples from the same speaker or movie title. This was done to remove any form of bias in the classifier based on either the speaker or the movie.  We then performed classification experiments using 5-fold cross-validation using the lexical fea-tures (unigrams and bigrams) on this combined set of reviews (positive and negative reviews togeth-er), each time leaving 1 fold for hold-out testing. Here, we note that for constructing the dictionary, only data from the training set was used. On average across 5-fold cross-validation, the number of unigrams was around 4,560 and bigrams around 23,701 for the combined set of movie reviews. However, since such a feature design typically suffers from problems arising out of the sparsity of the entries of the dictionary in the dataset, we employed a feature selection step. For feature selection and analysis, we used Information Gain (IG), which is a measure of the number of bits of information obtained for category prediction by knowing the presence of a term in a document. Prior evaluation of feature-selection methods for text classification has revealed the superiority of IG as a metric over other ones such as Mutual Information, Term Strength or a simple Document Frequency thresholding for document classification tasks (Yang and Pedersen, 1997). This serves as an inspiring basis for us-ing IG as a metric for feature selection. The gain score G(t) obtained from IG is a non-zero positive value for features that are strongly in-dicative of the extent of persuasiveness of the document, while ones that are not so informative have a value of 0. We therefore select only those lexical features (unigrams and bigrams) which have an IG > 0 based on the distribution obtained from the training set. This allows us to trim the dictionary signifi-cantly and use only meaningful features for classification. 
53
This was then followed up by a 5-fold cross-validation using only the paraverbal features (no fea-ture selection was used here since they were too few in number). The accuracy of classification based on paraverbal features was then compared with that obtained by classification using only the lexical descriptors and by a majority baseline classifier. Furthermore, we also tried an early-fusion approach, where we simply use both lexical and para-verbal features together. Such an approach to fusion seemed more promising here than a decision-level fusion approach because of the few categories of features used (just lexical and paralinguistic, as motivated by the findings of (Gunes and Piccardi, 2005)). 5.3  Classification Model For performing classification experiments we used the Na?ve Bayes classifier. A well-known issue with using the Na?ve Bayes classifier is its incapability of handling new features, which is handled by performing a conditional uniform smoothing (Puurula, 2012). 6 Results and Discussion Table 1 shows the results for our classification experiments, which confirm the predictive power of lexical features. Hypothesis 1: The lexical features (unigrams and bigrams) are predictive of persuasiveness. This is manifested by the fact that they perform significantly better than a majority baseline, which is only 51.04% accurate on the combined set of positive and negative reviews, while the lexical features achieved an accuracy of around 77% (Figure 1). Considering the positive and the negative reviews individually, we note that the lexical features were accurate for nearly 82% of the test samples for the positive reviews and for 86% of the test samples for the negative reviews, again outperforming a sim-ple majority baseline classifier (Table 1). An analysis of the features (Table 2) reveals that certain lexical features contribute to the predicta-bility of the persuasiveness of a speaker. The presence of unigrams such as character or make or bi-grams such as to make or this movie for instance, contributes to the predictability of persuasiveness of the speaker, even though they are not emotionally salient terms. The high IG scores of such features irrespective of the setting we conduct our experiments in (positive reviews only, negative reviews on-ly or a combined set of positive and negative reviews), highlights their importance. Moreover, a (+) sign for most of these unigrams or bigrams show that their presence contributes favorably to the speaker being perceived as persuasive. On the other hand a (-) sign for an informative bigram such as it says is indicative of lack of speaker?s persuasiveness. This can be explained by the context of the usage of such features. For instance, the bigram it says in it says that the movie duration is? is a bi-
 Feature Group Sentiment Dependent Classifier Sentiment Independent Classifier Mean Positive Reviews Negative Reviews Lexical Features (Unigrams and Bigrams) 83.92% 81.74% 86.09% 76.73% ? Unigrams Only 77.70% 74.78% 80.62% 73.77% ? Bigrams  Only 84.05% 81.64% 86.46% 75.81% Para-Linguistic Features 64.23% 65.22% 63.23% 63.04% Early Fusion 84.54% 82.61% 86.46% 78.56% Majority Baseline 52.14% 50.43% 53.85% 51.09%  Table 1: Accuracies for our experiments using a Na?ve Bayes classifier. The scores in bold indi-cate the dominance of the sentiment-dependent classifier under all circumstances. 
54
gram that is uttered by the reviewers when they refer to the DVD cover of the movie to give some more detailed information about it. This is identified as a sign of an unpersuasive reviewer. Such re-sults confirm that the verbal behaviors, as captured by lexical usage, are extremely predictive of per-suasiveness irrespective of whether the opinion expressed is positive or negative, which validates Hy-pothesis 1.  Hypothesis 2: Moreover, our experiments show that while the designed paraverbal features that are markers of hesitation can classify only about 63% of the speakers correctly (see Table 1), however 
  Figure 1: Bar graph visualization of the classification accuracies using different types of fea-tures on the combined set of reviews (i.e. sentiment-independent classifier). ** indicates 2-samples t-test results with p < 0.01 and *** indicates p < 0.001. The error bars show 1 SD.   Feature Positive Reviews Negative Reviews Both Combined Word IG Score Word IG Score Word IG Score 
Unigrams 
The (+) 0.1183 Even (+) 0.11 Make  (+) 0.1117 Make  (+) 0.0816 Make  (-) 0.1082 Just  (+) 0.0728 Everything  (+) 0.0806 Movie  (+) 0.0969 Very  (+) 0.0669 Just  (+) 0.0806 Real  (+) 0.0873 Character  (+) 0.0573 Dollars (+) 0.0722 Not  (+) 0.0867 Becomes  (+) 0.0558 Character  (+) 0.0685 Big  (+) 0.0858 Even (+) 0.0524 Can  (+) 0.0685 One  (+) 0.0817 One  (+) 0.051 Product  (+) 0.0685 Avoid  (+) 0.079 Yourself  (+) 0.05 Famous (+) 0.0609 Feel  (+) 0.079 You  (+) 0.04571 Enjoy  (+) 0.0566 Character  (+) 0.0773 Lot  (+) 0.0456 
Bigrams       
There are  (+) 0.1183 This movie  (+) 0.1083 To make  (+) 0.0905 This movie  (+) 0.0816 Do not  (+) 0.1032 A lot  (+) 0.0617 I can?t  (+) 0.0806 I think  (+) 0.1032 This movie  (+) 0.0578 To make  (+) 0.0806 To make  (+) 0.0989 Lot of  (+) 0.0443 Good movie  (+) 0.0722 Not even  (+) 0.091 It says (-) 0.0417 Buy it  (+) 0.0685 Don?t even  (+) 0.091 You will (-) 0.0417 Really a  (+) 0.0685 The story  (+) 0.079 Twenty dollars (+) 0.0368 Definitely one  (+) 0.0685 The film  (+) 0.0672 The character (+) 0.0386 Best movies  (+) 0.0609 At all  (+) 0.0672 So many (+) 0.033 It?s awesome  (+) 0.0566 It?s so (+) 0.0672 See it (+) 0.033  Table 2: Important unigrams and bigrams when they are used individually as lexical features. (+) indicates that it increases persuasiveness while (-) indicates it contributes to the lack of per-suasiveness.   
55
they are statistically significant features, in terms of their p-values (Figure 2). While classification performance is lower than that obtained with purely lexical features, it is still far above a majority baseline, and thus confirms our second hypothesis. Additionally, it is interesting to note from Table 1 that, although a feature-level fusion of the lexical features and paraverbal features gives us an improvement in classification performance, the difference between the results obtained with fusion and those with lexical features alone was minor and was not statistically significant.  Hypothesis 3: We also observe that a sentiment-dependent classifier trained individually on positive reviews or on negative reviews outperforms one that is trained on a combined set of reviews. This is supported by our empirical results in Table 1 which show that when classification is performed with any of the lexical features, the accuracies are significantly higher for the classifier trained only on the positive or only on the negative reviews (sentiment-dependent classifiers) than for the classifier trained on the combined set of reviews (sentiment-independent classifiers). For instance, when unigrams and bigrams were both used as our lexical features, we observed that for a sentiment-dependent classifier the classification accuracy jumps to over 84% on average. This is significantly better than the scenario where the classifier is not aware of the sentiment of the review. Figure 3 demonstrates this phenomenon.     We resort to feature analysis for an explanation of such an observation (Table 2). The analysis re-veals that certain sentiment-based lexical features, i.e. emotionally salient terms, assume an important role in magnifying the discriminative power of language use in persuasiveness prediction, when prior knowledge about the speaker?s opinion is known. For instance, in the case of a classifier trained only on the positive reviews, unigrams such as enjoy and famous and bigrams such as good movie or it?s awesome become significant. In the context of persuading against watching the movie prominent sen-timent-based unigrams are not and avoid while bigrams are do not, don?t even and at all. This pro-vides empirical support for our third hypothesis. 7 Conclusion and Future Work This work presents several interesting findings about perceived persuasiveness prediction in online social multimedia content by analyzing the verbal behavior of the speaker, modeled using lexical fea-tures and paraverbal features of hesitation. We conducted experiments and showed that verbal behav-ior as captured by lexical descriptors is a strong indicator of persuasiveness, irrespective of whether we persuade in favor of or against something. Much of this is due to the presence of certain unigrams and bigrams that are either indicative of strong persuasiveness or of lack of persuasiveness. Our ex-periments further reveal the superiority of classifying with lexical features as compared to with para-
  Figure 2: Boxplots for the paralinguistic hesitation markers for a classifier trained on the para-linguistic features only. * and *** indicate p <= 0.05 and 0.001, respectively.   
56
verbal features alone.  Moreover we empirically validate the hypothesis that a sentiment-aware classi-fier outperforms a sentiment-independent one. As future work, we intend to explore more paraverbal features for persuasiveness prediction and also try more sophisticated prediction models which explic-itly model the temporal dynamic.  Acknowledgments This work was supported by the National Science Foundation under Grant IIS-1118018 and the U.S. Army. The content does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. References Shelly Chaiken and Alice H. Eagly. 1979. Communication modality as a determinant of message persuasiveness and message comprehensibility. Journal of Personality and Social Psychology, 37:1387-1397. Jana Dankovicova. 1999. Articulation rate variation within the intonation phrase in Czech and English. 14th Int. Congress of Phonetic Sciences, San Francisco, Vol. 1, pp. 269-272.  Kushal Dave, Steve Lawrence, and David M. Pennck. Mining the Peanut Gallery: Opinion Extraction and semantic Classification of Product Reviews, 2003. 2003 Association for Computational Linguistics (ACL ?03). David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum, Stefan, Scherer, Albert (Skip) Rizzo, and Louis-Philippe Morency. 2013. Verbal indicators of psychological distress in interactive dialogue with a virtual human. SIGDIAL 2013 Conf, 2013 Association for Computational Linguistics (ACL ?13). Laurence Devillers and Laurence Vidrascu. 2006. Real-life emotions detection with lexical and paralinguistric cues on Human-Human call center dialogs. Interspeech 2006. Hatice Gunes, and Massimo Piccardi. 2005. Affect Recognition from face and body: Early fusion vs. Late fusion. IEEE Int?l Conf. on Systems, Man and Cybernnetic. Daniel  J. O?Keefe. 2002. Persuasion: Theory and research. (2nd Edition).  Sage Publications, Thousand Oaks, CA. David D. Lewis and William A. Gale. 1994. A Sequential Algorithm for Training Text Classifiers. Special Interest Group in Information Retieval (SIGIR?94 ). Gerald R. Miller (1980). On being persuaded: Some basic distinctions. In M. Roloff, & G. R. Miller (Eds.), Persuasion: New directions in theory and research, 11?28. Beverly Hills, CA: Sage. 
  Figure 3: Bar graph visualization of the classification accuracies of lexical features using a senti-ment-dependent classifier (mean) and a sentiment independent one. ** indicates 2-sample t-test results with p < 0.01 and the error bars show 1 SD.  
57
Tom M. Mitchell. 1997. Machine Learning. McGraw-Hill. Gelareh Mohammadi, Sunghyun Park, Kenji Sagae, Alessandro Vinciarelli, and Lois-Phillippe Morency. 2013. Who is persuasive? The role of perceived personality and Communication modality in social multimedia. Int?l Conf. on Multimodal Interfaces (ICMI ?13). P. Karen Murphy. 2001. What makes a text persuasive? Comparing students? and experts? conceptions of persuasiveness. Int?l Journal of Education Research, 35 (2001) 675-698. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. Conf. on Empirical Methods in Natural Language Processing. (EMNLP ?02). Antti Puurula. 2012. Combining Modifications to Multinomial Naive Bayes for Text Classification. Springer, LNCS. Kathleen Kelley Reardon. 1991. Persuasion in practice. Sage Publication, Inc. Carol Werner. 1982. Intrusiveness and persuasive impact of three communication media. Journal of Applied Social Psychology, 89:155-181. Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. Int?l Conf. on Machine Learning (ICML ?97). Joel Young, Craig Martell, Pranav Anand, Pedro Ortiz and Henry T. Gilbert IV. 2011. A Microtext Corpus for Persuasion Detection in Dialog. Analyzing Microtext: AAAI Workshop (AAAI-Workshop ?11). Phillip G. Zimbardo and Michael R. Leippe. 1991. The psychology of attitude change and social influence. McGrew-Hill New York.   
58
