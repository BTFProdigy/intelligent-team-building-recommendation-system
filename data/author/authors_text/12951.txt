Using Syntactic Information to Extract Relevant Terms for Multi-Document
Summarization
Enrique Amigo? Julio Gonzalo V??ctor Peinado Anselmo Pen?as Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/ Juan del Rosal, 16 - 28040 Madrid - Spain
http://nlp.uned.es
Abstract
The identification of the key concepts in a set of
documents is a useful source of information for
several information access applications. We are
interested in its application to multi-document
summarization, both for the automatic genera-
tion of summaries and for interactive summa-
rization systems.
In this paper, we study whether the syntactic po-
sition of terms in the texts can be used to predict
which terms are good candidates as key con-
cepts. Our experiments show that a) distance
to the verb is highly correlated with the proba-
bility of a term being part of a key concept; b)
subject modifiers are the best syntactic locations
to find relevant terms; and c) in the task of auto-
matically finding key terms, the combination of
statistical term weights with shallow syntactic
information gives better results than statistical
measures alone.
1 Introduction
The fundamental question addressed in this article
is: can syntactic information be used to find the
key concepts of a set of documents? We will pro-
vide empirical answers to this question in a multi-
document summarization environment.
The identification of key terms out of a set of doc-
uments is a common problem in information access
applications and, in particular, in text summariza-
tion: a fragment containing one or more key con-
cepts can be a good candidate to be part of a sum-
mary.
In single-document summarization, key terms are
usually obtained from the document title or head-
ing (Edmundson, 1969; Preston, 1994; Kupiec et
al., 1995). In multi-document summarization, how-
ever, some processing is needed to identify key con-
cepts (Lin and Hovy, 2002; Kraaij et al, 2002;
Schlesinger et al, 2002). Most approaches are
based on statistical criteria.
Criteria to elaborate a manual summary depend,
by and large, on the user interpretation of both the
information need and the content of documents.
This is why this task has also been attempted from
an interactive perspective (Boguraev et al, 1998;
Buyukkokten et al, 1999; Neff and Cooper, 1999;
Jones et al, 2002; Leuski et al, 2003). A standard
feature of such interactive summarization assistants
is that they offer a list of relevant terms (automati-
cally extracted from the documents) which the user
may select to decide or refine the focus of the sum-
mary.
Our hypothesis is that the key concepts of a doc-
ument set will tend to appear in certain syntactic
functions along the sentences and clauses of the
texts. To confirm this hypothesis, we have used
a test bed with manually produced summaries to
study:
? which are the most likely syntactic functions
for the key concepts manually identified in the
document sets.
? whether this information can be used to auto-
matically extract the relevant terms from a set
of documents, as compared to standard statis-
tical term weights.
Our reference corpus is a set of 72 lists of key
concepts, manually elaborated by 9 subjects on
8 different topics, with 100 documents per topic.
It was built to study Information Synthesis tasks
(Amigo et al, 2004) and it is, to the best of
our knowledge, the multi-document summarization
testbed with a largest number of documents per
topic. This feature enables us to obtain reliable
statistics on term occurrences and prominent syn-
tactic functions.
The paper is organized as follows: in Section 2
we review the main approaches to the evaluation
of automatically extracted key concepts for summa-
rization. In Section 3 we describe the creation of the
reference corpus. In Section 4 we study the correla-
tion between key concepts and syntactic function in
texts, and in Section 5 we discuss the experimental
results of syntactic function as a predictor to extract
key concepts. Finally, in Section 6 we draw some
conclusions.
2 Evaluation of automatically extracted
key concepts
It is necessary, in the context of an interactive sum-
marization system, to measure the quality of the
terms suggested by the system, i.e., to what extent
they are related to the key topics of the document
set.
(Lin and Hovy, 1997) compared different strate-
gies to generate lists of relevant terms for summa-
rization using Topic Signatures. The evaluation was
extrinsic, comparing the quality of the summaries
generated by a system using different term lists as
input. The results, however, cannot be directly ex-
trapolated to interactive summarization systems, be-
cause the evaluation does not consider how informa-
tive terms are for a user.
From an interactive point of view, the evaluation
of term extraction approaches can be done, at least,
in two ways:
? Evaluating the summaries produced in the in-
teractive summarization process. This option
is difficult to implement (how do we evaluate
a human produced summary? What is the ref-
erence gold standard?) and, in any case, it is
too costly: every alternative approach would
require at least a few additional subjects per-
forming the summarization task.
? Comparing automatically generated term lists
with manually generated lists of key concepts.
For instance, (Jones et al, 2002) describes a
process of supervised learning of key concepts
from a training corpus of manually generated
lists of phrases associated to a single docu-
ment.
We will, therefore, use the second approach,
evaluating the quality of automatically generated
term lists by comparing them to lists of key con-
cepts which are generated by human subjects after a
multi-document summarization process.
3 Test bed: the ISCORPUS
We have created a reference test bed, the ISCOR-
PUS1 (Amigo et al, 2004) which contains 72 man-
ually generated reports summarizing the relevant in-
formation for a given topic contained in a large doc-
ument set.
For the creation of the corpus, nine subjects per-
formed a complex multi-document summarization
1Available at http://nlp.uned.es/ISCORPUS.
task for eight different topics and one hundred rele-
vant documents per topic. After creating each topic-
oriented summary, subjects were asked to make a
list of relevant concepts for the topic, in two cate-
gories: relevant entities (people, organizations, etc.)
and relevant factors (such as ?ethnic conflicts? as
the origin of a civil war) which play a key role in
the topic being summarized.
These are the relevant details of the ISCORPUS
test bed:
3.1 Document collection and topic set
We have used the Spanish CLEF 2001-2003 news
collection testbed (Peters et al, 2002), and selected
the eight topics with the largest number of docu-
ments manually judged as relevant from the CLEF
assessment pools. All the selected CLEF topics
have more than one hundred documents judged as
relevant by the CLEF assessors; for homogeneity,
we have restricted the task to the first 100 docu-
ments for each topic (using a chronological order).
This set of eight CLEF topics was found to have
two differentiated subsets: in six topics, it is neces-
sary to study how a situation evolves in time: the
importance of every event related to the topic can
only be established in relation with the others. The
invasion of Haiti by UN and USA troops is an ex-
ample of such kind of topics. We refer to them as
?Topic Tracking? (TT) topics, because they are suit-
able for such a task. The other two questions, how-
ever, resemble ?Information Extraction? (IE) tasks:
essentially, the user has to detect and describe in-
stances of a generic event (for instance, cases of
hunger strikes and campaigns against racism in Eu-
rope in this case); hence we will refer to them as IE
summaries.
3.2 Generation of manual summaries
Nine subjects between 25 and 35 years-old were re-
cruited for the manual generation of summaries. All
subjects were given an in-place detailed description
of the task, in order to minimize divergent interpre-
tations. They were told they had to generate sum-
maries with a maximum of information about ev-
ery topic within a 50 sentence space limit, using a
maximum of 30 minutes per topic. The 50 sentence
limit can be temporarily exceeded and, once the 30
minutes have expired, the user can still remove sen-
tences from the summary until the sentence limit is
reached back.
3.3 Manual identification of key concepts
After summarizing every topic, the following ques-
tionnaire was filled in by users:
? Who are the main people involved in the topic?
? What are the main organizations participating in the topic?
? What are the key factors in the topic?
Users provided free-text answers to these ques-
tions, with their freshly generated summary at hand.
We did not provide any suggestions or constraints
at this point, except that a maximum of eight slots
were available per question (i.e., a maximum of
8X3 = 24 key concepts per topic, per user).
This is, for instance, the answer of one user for
a topic about the invasion of Haiti by UN and USA
troops:
People Organizations
Jean Bertrand Aristide ONU (UN)
Clinton EEUU (USA)
Raoul Cedras OEA (OAS)
Philippe Biambi
Michel Josep Francois
Factors
militares golpistas (coup attempting soldiers)
golpe militar (coup attempt)
restaurar la democracia (reinstatement of democracy)
Finally, a single list of key concepts is generated
for each topic, joining all the answers given by the
nine subjects. These lists of key concepts constitute
the gold standard for all the experiments described
below.
3.4 Shallow parsing of documents
Documents are processed with a robust shallow
parser based in finite automata. The parser splits
sentences in chunks and assigns a label to every
chunk. The set of labels is:
? [N]: noun phrases, which correspond to
names or adjectives preceded by a determiner,
punctuation sign, or beginning of a sentence.
? [V]: verb forms.
? [Mod]: adverbial and prepositional phrases,
made up of noun phrases introduced by an ad-
verb or preposition. Note that this is the mech-
anism to express NP modifiers in Spanish (as
compared to English, where noun compound-
ing is equally frequent).
? [Sub]: words introducing new subordinate
clauses within a sentence (que, cuando, mien-
tras, etc.).
? [P]: Punctuation marks.
This is an example output of the chunker:
Previamente [Mod] ,[P]el presidente Bill Clinton [N] hab??a di-
cho [V] que [Sub] tenemos [V] la obligacion [N] de cambiar la
pol??tica estadounidense [Mod] que [Sub] no ha funcionado [V] en
Hait?? [Mod].[P]
Although the precision of the parser is limited,
the results are good enough for the statistical mea-
sures used in our experiments.
4 Distribution of key concepts in syntactic
structures
We have extracted empirical data to answer these
questions:
? Is the probability of finding a key concept cor-
related with the distance to the verb in a sen-
tence or clause?
? Is the probability of finding a key concept in a
noun phrase correlated with the syntactic func-
tion of the phrase (subject, object, etc.)?
? Within a noun phrase, where is it more likely
to find key concepts: in the noun phrase head,
or in the modifiers?
We have used certain properties of Spanish syn-
tax (such as being an SVO language) to decide
which noun phrases play a subject function, which
are the head and modifiers of a noun phrase, etc. For
instance, NP modifiers usually appear after the NP
head in Spanish, and the specification of a concept
is usually made from left to right.
4.1 Distribution of key concepts with verb
distance
Figure 1 shows, for every topic, the probability of
finding a word from the manual list of key con-
cepts in fixed distances from the verb of a sen-
tence. Stop words are not considered for computing
word distance. The broader line represents the aver-
age across topics, and the horizontal dashed line is
the average probability across all positions, i.e., the
probability that a word chosen at random belongs to
the list of key concepts.
The plot shows some clear tendencies in the data:
the probability gets higher when we get close to the
verb, falls abruptly after the verb, and then grows
steadily again. For TT topics, the probability of
finding relevant concepts immediately before the
verb is 56% larger than the average (0.39 before the
verb, versus 0.25 in any position). This is true not
only as an average, but also for all individual TT
topics. This can be an extremely valuable result: it
shows a direct correlation between the position of a
term in a sentence and the importance of the term
in the topic. Of course, this direct distance to the
verb should be adapted for languages with different
syntactic properties, and should be validated for dif-
ferent domains.
The behavior of TT and IE topics is substantially
different. IE topics have smaller probabilities over-
all, because there are less key concepts common to
all documents. For instance, if the topic is ?cases of
hunger strikes?, there is little in common between
Figure 1: Probability of finding key concepts at fixed distances from verb
all cases of hunger strikes found in the collection;
each case has its own relevant people and organiza-
tions, for instance. Users try to make abstraction of
individual cases to write key concepts, and then the
number of key concepts is smaller. The tendency
to have larger probabilities just before the verb and
smaller probabilities just after the verb, however,
can also be observed for IE topics.
Figure 2: Probability of finding key concepts in sub-
ject NPs versus other NPs
4.2 Key Concepts and Noun Phrase Syntactic
Function
We wanted also to confirm that it is more likely to
find a key concept in a subject noun phrase than
in general NPs. For this, we have split compound
sentences in chunks, separating subordinate clauses
([Sub] type chunks). Then we have extracted se-
quences with the pattern [N][Mod]*. We assume
that the sentence subject is a sequence [N][Mod]*
occurring immediately before the verb. For in-
stance:
El presidente [N] en funciones [Mod] de
Hait?? [Mod] ha afirmado [V] que [Sub]...
The rest of [N] and [Mod] chunks are consid-
ered as part of the sentence verb phrase. In a ma-
jority of cases, these assumptions lead to a correct
identification of the sentence subject. We do not
capture, however, subjects of subordinate sentences
or subjects appearing after the verb.
Figure 2 shows how the probability of finding a
key concept is always larger in sentence subjects.
This result supports the assumption in (Boguraev
et al, 1998), where noun phrases receive a higher
weight, as representative terms, if they are syntactic
subjects.
4.3 Distribution of key concepts within noun
phrases
Figure 3: Probability of finding key concepts in NP
head versus NP modifiers
For this analysis, we assume that, in
[N][Mod]* sequences identified as subjects,
[N] is the head and [Mod]* are the modifiers.
Figure 3 shows that the probability of finding a
key concept in the NP modifiers is always higher
than in the head (except for topic TT3, where it is
equal). This is not intuitive a priori; an examination
of the data reveals that the most characteristic con-
cepts for a topic tend to be in the complements: for
instance, in ?the president of Haiti?, ?Haiti? carries
more domain information than ?president?. This
seems to be the most common case in our news
collection. Of course, it cannot be guaranteed that
these results will hold in other domains.
5 Automatic Selection of Key Terms
We have shown that there is indeed a correlation be-
tween syntactic information and the possibility of
finding a key concept. Now, we want to explore
whether this syntactic information can effectively
be used for the automatic extraction of key concepts.
The problem of extracting key concepts for sum-
marization involves two related issues: a) What
kinds of terms should be considered as candidates?
and b) What is the optimal weighting criteria for
them?
There are several possible answers to the first
question. Previous work includes using noun
phrases (Boguraev et al, 1998; Jones et al, 2002),
words (Buyukkokten et al, 1999), n-grams (Leuski
et al, 2003; Lin and Hovy, 1997) or proper
nouns, multi-word terms and abbreviations (Neff
and Cooper, 1999).
Here we will focus, however, in finding appro-
priate weighting schemes on the set of candidate
terms. The most common approach in interactive
single-document summarization is using tf.idf mea-
sures (Jones et al, 2002; Buyukkokten et al, 1999;
Neff and Cooper, 1999), which favour terms which
are frequent in a document and infrequent across
the collection. In the iNeast system (Leuski et al,
2003), the identification of relevant terms is ori-
ented towards multi-document summarization, and
they use a likelihood ratio (Dunning, 1993) which
favours terms which are representative of the set of
documents as opposed to the full collection.
Other sources of information that have been used
as complementary measures consider, for instance,
the number of references of a concept (Boguraev
et al, 1998), its localization (Jones et al, 2002)
or the distribution of the term along the document
(Buyukkokten et al, 1999; Boguraev et al, 1998).
5.1 Experimental setup
A technical difficulty is that the key concepts in-
troduced by the users are intellectual elaborations,
which result in complex expressions which might
even not be present (literally) in the documents.
Hence, we will concentrate on extracting lists of
terms, checking whether these terms are part of
some key concept. We will assume that, once key
terms are found, it is possible to generate full nomi-
nal expressions using, for instance, phrase browsing
strategies (Pen?as et al, 2002).
We will then compare different weighting criteria
to select key terms, using two evaluation measures:
a recall measure saying how well manually selected
key concepts are covered by the automatically gen-
erated term list; and a noise measure counting the
number of terms which do not belong to any key
concept. An optimal list will reach maximum recall
with a minimum of noise. Formally:
R =
|Cl|
|C|
Noise = |Ln|
where C is the set of key concepts manually se-
lected by users; L is a (ranked) list of terms gen-
erated by some weighting schema; Ln is the subset
of terms in L which do not belong to any key con-
cept; and Cl is the subset of key concepts which are
represented by at least one term in the ranked list L.
Here is a (fictitious) example of how R and
Noise are computed:
C = {Haiti, reinstatement of democracy, UN and USA troops}
L = {Haiti, soldiers, UN, USA, attempt}
?
Cl = {Haiti, UN and USA troops} R = 2/3
Ln = {soldiers,attempt} Noise = 2
We will compare the following weighting strate-
gies:
TF The frequency of a word in the set of documents
is taken as a baseline measure.
Likelihood ratio This is taken from (Leuski et al,
2003) and used as a reference measure. We
have implemented the procedure described in
(Rayson and Garside, 2000) using unigrams
only.
OKAPImod We have also considered a measure
derived from Okapi and used in (Robertson et
al., 1992). We have adapted the measure to
consider the set of 100 documents as one single
document.
TFSYNTAX Using our first experimental result,
TFSYNTAX computes the weight of a term
as the number of times it appears preceding a
verb.
Figure 4: Comparison of weighting schemes to ex-
tract relevant terms
5.2 Results
Figure 4 draws Recall/Noise curves for all weight-
ing criteria. They all give similar results except our
TFSYNTAX measure, which performs better than
the others for TT topics. Note that the TFSYN-
TAX measure only considers 10% of the vocabu-
lary, which are the words immediately preceding
verbs in the texts.
In order to check whether this result is consistent
across topics (and not only the effect on an average)
we have compared recall for term lists of size 50 for
individual topics. We have selected 50 as a number
which is large enough to reach a good coverage and
permit additional filtering in an interactive summa-
rization process, such as the iNeast terminological
clustering described in (Leuski et al, 2003).
Figure 5 shows these results by topic. TFSYN-
TAX performs consistently better for all topics ex-
cept one of the IE topics, where the maximum like-
lihood measure is slightly better.
Apart from the fact that TFSYNTAX performs
better than all other methods, it is worth noticing
that sophisticated weighting mechanisms, such as
Okapi and the likelihood ratio, do not behave bet-
ter than a simple frequency count (TF).
6 Conclusions
The automatic extraction of relevant concepts for
a set of related documents is a part of many mod-
els of automatic or interactive summarization. In
this paper, we have analyzed the distribution of rel-
evant concepts across different syntactic functions,
and we have measured the usefulness of detecting
key terms to extract relevant concepts.
Our results suggest that the distribution of key
concepts in sentences is not uniform, having a max-
imum in positions immediately preceding the sen-
tence main verb, in noun phrases acting as subjects
and, more specifically, in the complements (rather
than the head) of noun phrases acting as subjects.
This evidence has been collected using a Spanish
news collection, and should be corroborated outside
the news domain and also adapted to be used for non
SVO languages.
We have also obtained empirical evidence that
statistical weights to select key terms can be im-
proved if we restrict candidate words to those which
precede the verb in some sentence. The combi-
nation of statistical measures and syntactic criteria
overcomes pure statistical weights, at least for TT
topics, where there is certain consistency in the key
concepts across documents.
Acknowledgments
This research has been partially supported by a re-
search grant of the Spanish Government (project
Hermes) and a research grant from UNED. We are
indebted to J. Cigarra?n who calculated the Okapi
weights used in this work.
References
E. Amigo, J. Gonzalo, V. Peinado, A. Pen?as, and
F. Verdejo. 2004. Information synthesis: an em-
pirical study. In Proceedings of the 42th Annual
Meeting of the ACL, Barcelona, July.
B. Boguraev, C. Kennedy, R. Bellamy, S. Brawer,
Y. Wong, and J. Swartz. 1998. Dynamic Presen-
tation of Document Content for Rapid On-line
Skimming. In Proceedings of the AAAI Spring
Figure 5: Comparison of weighting schemes by topic
1998 Symposium on Intelligent Text Summariza-
tion, Stanford, CA.
O. Buyukkokten, H. Garc??a-Molina, and
A. Paepcke. 1999. Seeing the Whole in
Parts: Text Summarization for Web Browsing
on Handheld Devices. In Proceedings of 10th
International WWW Conference.
T. Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1):61?74.
H. P. Edmundson. 1969. New Methods in Auto-
matic Extracting. Journal of the Association for
Computing Machinery, 16(2):264?285.
S. Jones, S. Lundy, and G. W. Paynter. 2002. In-
teractive Document Summarization Using Auto-
matically Extracted Keyphrases. In Proceedings
of the 35th Hawaii International Conference on
System Sciences, Big Island, Hawaii.
W. Kraaij, M. Spitters, and A. Hulth. 2002.
Headline Extraction based on a Combination of
Uni- and Multi-Document Summarization Tech-
niques. In Proceedings of the DUC 2002 Work-
shop on Multi-Document Summarization Evalua-
tion, Philadelphia, PA, July.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of SI-
GIR?95.
A. Leuski, C. Y. Lin, and S. Stubblebine. 2003.
iNEATS: Interactive Multidocument Summariza-
tion. In Proceedings of the 4lst Annual Meeting
of the ACL (ACL 2003), Sapporo, Japan.
C.-Y. Lin and E.H. Hovy. 1997. Identifying Top-
ics by Position. In Proceedings of the 5th Con-
ference on Applied Natural Language Processing
(ANLP), Washington, DC.
C. Lin and E. Hovy. 2002. NeATS in DUC
2002. In Proceedings of the DUC 2002 Work-
shop on Multi-Document Summarization Evalu-
ation, Philadelphia, PA, July.
M. S. Neff and J. W. Cooper. 1999. ASHRAM: Ac-
tive Summarization and Markup. In Proceedings
of HICSS-32: Understanding Digital Documents.
A. Pen?as, F. Verdejo, and J. Gonzalo. 2002. Ter-
minology Retrieval: Towards a Synergy be-
tween Thesaurus and Free Text Searching. In IB-
ERAMIA 2002, pages 684?693, Sevilla, Spain.
C. Peters, M. Braschler, J. Gonzalo, and M. Kluck,
editors. 2002. Evaluation of Cross-Language
Information Retrieval Systems, volume 2406 of
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
S. Preston, K.and Williams. 1994. Managing the
Information Overload. Physics in Business, June.
P. Rayson and R. Garside. 2000. Comparing Cor-
pora Using Frequency Profiling. In Proceedings
of the workshop on Comparing Corpora, pages
1?6, Honk Kong.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conference, pages 21?30.
J. D. Schlesinger, M. E. Okurowski, J. M. Conroy,
D. P. O?Leary, A. Taylor, J. Hobbs, and H. Wil-
son. 2002. Understanding Machine Performance
in the Context of Human Performance for Multi-
Document Summarization. In Proceedings of the
DUC 2002 Workshop on Multi-Document Sum-
marization Evaluation, Philadelphia, PA, July.
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 534?542,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
The role of named entities in Web People Search
Javier Artiles
UNED NLP & IR group
Madrid, Spain
javart@bec.uned.es
Enrique Amig
?
o
UNED NLP & IR group
Madrid, Spain
enrique@lsi.uned.es
Julio Gonzalo
UNED NLP & IR group
Madrid, Spain
julio@lsi.uned.es
Abstract
The ambiguity of person names in the Web
has become a new area of interest for NLP
researchers. This challenging problem has
been formulated as the task of clustering
Web search results (returned in response
to a person name query) according to the
individual they mention. In this paper we
compare the coverage, reliability and in-
dependence of a number of features that
are potential information sources for this
clustering task, paying special attention to
the role of named entities in the texts to
be clustered. Although named entities are
used in most approaches, our results show
that, independently of the Machine Learn-
ing or Clustering algorithm used, named
entity recognition and classification per se
only make a small contribution to solve the
problem.
1 Introduction
Searching the Web for names of people is a highly
ambiguous task, because a single name tends to
be shared by many people. This ambiguity has
recently become an active research topic and, si-
multaneously, in a relevant application domain for
web search services: Zoominfo.com, Spock.com,
123people.com are examples of sites which per-
form web people search, although with limited
disambiguation capabilities.
A study of the query log of the AllTheWeb and
Altavista search sites gives an idea of the relevance
of the people search task: 11-17% of the queries
were composed of a person name with additional
terms and 4% were identified as person names
(Spink et al, 2004). According to the data avail-
able from 1990 U.S. Census Bureau, only 90,000
different names are shared by 100 million people
(Artiles et al, 2005). As the amount of informa-
tion in the WWW grows, more of these people are
mentioned in different web pages. Therefore, a
query for a common name in the Web will usually
produce a list of results where different people are
mentioned.
This situation leaves to the user the task of find-
ing the pages relevant to the particular person he
is interested in. The user might refine the original
query with additional terms, but this risks exclud-
ing relevant documents in the process. In some
cases, the existence of a predominant person (such
as a celebrity or a historical figure) makes it likely
to dominate the ranking of search results, compli-
cating the task of finding information about other
people sharing her name. The Web People Search
task, as defined in the first WePS evaluation cam-
paign (Artiles et al, 2007), consists of grouping
search results for a given name according to the
different people that share it.
Our goal in this paper is to study which doc-
ument features can contribute to this task, and in
particular to find out which is the role that can be
played by named entities (NEs): (i) How reliable
is NEs overlap between documents as a source of
evidence to cluster pages? (ii) How much recall
does it provide? (iii) How unique is this signal?
(i.e. is it redundant with other sources of informa-
tion such as n-gram overlap?); and (iv) How sen-
sitive is this signal to the peculiarities of a given
NE recognition system, such as the granularity of
its NE classification and the quality of its results?
Our aim is to reach conclusions which are are
not tied to a particular choice of Clustering or Ma-
chine Learning algorithms. We have taken two de-
cisions in this direction: first, we have focused on
the problem of deciding whether two web pages
refer to the same individual or not (page corefer-
ence task). This is the kind of relatedness measure
that most clustering algorithms use, but in this way
we can factor out the algorithm and its parameter
settings. Second, we have developed a measure,
Maximal Pairwise Accuracy (PWA) which, given
534
an information source for the problem, estimates
an upper bound for the performance of any Ma-
chine Learning algorithm using this information.
We have used PWA as the basic metric to study the
role of different document features in solving the
coreference problem, and then we have checked
the predictive power of PWA with a Decision Tree
algorithm.
The remainder of the paper is organised as fol-
lows. First, we examine the previous work in Sec-
tion 2. Then we describe the our experimental set-
tings (datasets and features we have used) in Sec-
tion 3 and our empirical study in Section 4. The
paper ends with some conclusions in Section 5.
2 Previous work
In this section we will discuss (i) the state of the
art in Web People Search in general, focusing on
which features are used to solve the problem; and
(ii) lessons learnt from the WePS evaluation cam-
paign where most approaches to the problem have
been tested and compared.
The disambiguation of person names in Web
results is usually compared to two other Natu-
ral Language Processing tasks: Word Sense Dis-
ambiguation (WSD) (Agirre and Edmonds, 2006)
and Cross-document Coreference (CDC) (Bagga
and Baldwin, 1998). Most of early research work
on person name ambiguity focuses on the CDC
problem or uses methods found in the WSD litera-
ture. It is only recently that the web name ambigu-
ity has been approached as a separate problem and
defined as an NLP task - Web People Search - on
its own (Artiles et al, 2005; Artiles et al, 2007).
Therefore, it is useful to point out some crucial
differences between WSD, CRC and WePS:
? WSD typically concentrates in the disam-
biguation of common words (nouns, verbs,
adjectives) for which a relatively small num-
ber of senses exist, compared to the hun-
dreds or thousands of people that can share
the same name.
? WSD can rely on dictionaries to define the
number of possible senses for a word. In the
case of name ambiguity no such dictionary
is available, even though in theory there is an
exact number of people that can be accounted
as sharing the same name.
? The objective of CDC is to reconstruct the
coreference chain for every mention of a per-
son. In Web person name disambiguation it
suffices to group the documents that contain
at least one mention to the same person.
Before the first WePS evaluation campaign in
2007 (Artiles et al, 2007), research on the topic
was not based on a consistent task definition, and
it lacked a standard manually annotated testbed.
In the WePS task, systems were given the top web
search results produced by a person name query.
The expected output was a clustering of these re-
sults, where each cluster should contain all and
only those documents referring to the same indi-
vidual.
2.1 Features for Web People Search
Many different features have been used to repre-
sent documents where an ambiguous name is men-
tioned. The most basic is a Bag of Words (BoW)
representation of the document text. Within-
document coreference resolution has been applied
to produce summaries of text surrounding occur-
rences of the name (Bagga and Baldwin, 1998;
Gooi and Allan, 2004). Nevertheless, the full
document text is present in most systems, some-
times as the only feature (Sugiyama and Okumura,
2007) and sometimes in combination with others -
see for instance (Chen and Martin, 2007; Popescu
and Magnini, 2007)-. Other representations use
the link structure (Malin, 2005) or generate graph
representations of the extracted features (Kalash-
nikov et al, 2007).
Some researchers (Cucerzan, 2007; Nguyen and
Cao, 2008) have explored the use of Wikipedia
information to improve the disambiguation pro-
cess. Wikipedia provides candidate entities that
are linked to specific mentions in a text. The obvi-
ous limitation of this approach is that only celebri-
ties and historical figures can be identified in this
way. These approaches are yet to be applied to the
specific task of grouping search results.
Biographical features are strongly related to
NEs and have also been proposed for this task
due to its high precision. Mann (2003) extracted
these features using lexical patterns to group pages
about the same person. Al-Kamha (2004) used a
simpler approach, based on hand coded features
(e.g. email, zip codes, addresses, etc). In Wan
(2005), biographical information (person name, ti-
tle, organisation, email address and phone num-
ber) improves the clustering results when com-
bined with lexical features (words from the doc-
535
ument) and NE (person, location, organisation).
The most used feature for the Web People
Search task, however, are NEs. Ravin (1999) in-
troduced a rule-based approach that tackles both
variation and ambiguity analysing the structure of
names. In most recent research, NEs (person, lo-
cation and organisations) are extracted from the
text and used as a source of evidence to calculate
the similarity between documents -see for instance
(Blume, 2005; Chen and Martin, 2007; Popescu
and Magnini, 2007; Kalashnikov et al, 2007)-
. For instance, Blume (2005) uses NEs coocur-
ring with the ambiguous mentions of a name as a
key feature for the disambiguation process. Sag-
gion (2008) compared the performace of NEs ver-
sus BoW features. In his experiments a only a
representation based on Organisation NEs outper-
formed the word based approach. Furthermore,
this result is highly dependent on the choice of
metric weighting (NEs achieve high precision at
the cost of a low recall and viceversa for BoW).
In summary, the most common document repre-
sentations for the problem include BoW and NEs,
and in some cases biographical features extracted
from the text.
2.2 Named entities in the WePS campaign
Among the 16 teams that submitted results for the
first WePS campaign, 10 of them
1
used NEs in
their document representation. This makes NEs
the second most common type of feature; only
the BoW feature was more popular. Other fea-
tures used by the systems include noun phrases
(Chen and Martin, 2007), word n-grams (Popescu
and Magnini, 2007), emails and URLs (del Valle-
Agudo et al, 2007), etc. In 2009, the second
WePS campaign showed similar trends regarding
the use of NE features (Artiles et al, 2009).
Due to the complexity of systems, the results
of the WePS evaluation do not provide a direct
answer regarding the advantages of using NEs
over other computationally lighter features such as
BoW or word n-grams. But the WePS campaigns
did provide a useful, standardised resource to per-
form the type of studies that were not possible be-
fore. In the next Section we describe this dataset
and how it has been adapted for our purposes.
1
By team ID: CU-COMSEM, IRST-BP, PSNUS, SHEF,
FICO, UNN, AUG, JHU1, DFKI2, UC3M13
3 Experimental settings
3.1 Data
We have used the testbeds from WePS-1 (Artiles et
al., 2007)
2
and WePS-2 (Artiles et al, 2009) eval-
uation campaigns
3
.
Each WePS dataset consists of 30 test cases: a
random sample of 10 names from the US Cen-
sus, 10 names from Wikipedia, and 10 names from
Programme Committees in the Computer Science
domain (ACL and ECDL). Each test case consists
of, at most, 100 web pages from the top search
results of a web search engine, using a (quoted)
person name as query.
For each test case, annotators were asked to or-
ganise the web pages in groups where all docu-
ments refer to the same person. In cases where
a web page refers to more than one person us-
ing the same ambiguous name (e.g. a web page
with search results from Amazon), the document
is assigned to as many groups as necessary. Doc-
uments were discarded when they did not contain
any useful information about the person being re-
ferred.
Both the WePS-1 and WePS-2 testbeds have
been used to evaluate clustering systems by WePS
task participants, and are now the standard testbed
to test Web People Search systems.
3.2 Features
The evaluated features can be grouped in four
main groups: token-based, n-grams, phrases and
NEs. Wherever possible, we have generated lo-
cal versions of these features that only consider
the sentences of the text that mention the ambigu-
ous person name
4
. Token-based features consid-
ered include document full text tokens, lemmas
(using the OAK analyser, see below), title, snip-
pet (returned in the list of search results) and URL
(tokenised using non alphanumeric characters as
boundaries) tokens. English stopwords were re-
moved, including Web specific stopwords, as file
and domain extensions, etc.
We generated word n-grams of length 2 to 5,
2
The WePS-1 corpus includes data from the Web03
testbed (Mann, 2006) which follows similar annotation
guidelines, although the number of document per ambiguous
name is more variable.
3
Both corpora are available from the WePS website
http://nlp.uned.es/weps
4
A very sparse feature might never occur in a sentence
with the person name. In that cases there is no local version
of the feature.
536
using the sentences found in the document text.
Punctuation tokens (commas, dots, etc) were gen-
eralised as the same token. N-grams were dis-
carded when they were composed only of stop-
words or when they did not contain at least one
token formed by alphanumeric characters (e.g. n-
grams like ?at the? or ?# @?). Noun phrases (us-
ing OAK analyser) were detected in the document
and filtered in a similar way.
Named entities were extracted using two dif-
ferent tools: the Stanford NE Recogniser and the
OAK System
5
.
Stanford NE Recogniser
6
is a high-performance
Named Entity Recognition (NER) system based
on Machine Learning. It provides a general im-
plementation of linear chain Conditional Ran-
dom Field sequence models and includes a model
trained on data from CoNLL, MUC6, MUC7, and
ACE newswire. Three types of entities were ex-
tracted: person, location and organisation.
OAK
7
is a rule based English analyser that in-
cludes many functionalities (POS tagger, stemmer,
chunker, Named Entity (NE) tagger, dependency
analyser, parser, etc). It provides a fine grained
NE recognition covering 100 different NE types
(Sekine, 2008). Given the sparseness of most of
these fine-grained NE types, we have merged them
in coarser groups: event, facility, location, person,
organisation, product, periodx, timex and numex.
We have also used the results of a baseline
NE recognition for comparison purposes. This
method detects sequences of two or more upper-
cased tokens in the text, and discards those that are
found lowercased in the same document or that are
composed solely of stopwords.
Other features are: emails, outgoing links found
in the web pages and two boolean flags that in-
dicate whether a pair of documents is linked or
belongs to the same domain. Because of their
low impact in the results these features haven?t re-
ceived an individual analysis, but they are included
in the ?all features? combination in Figure 7.
5
From the output of both systems we have discarded per-
son NEs made of only one token (these are often first names
that significantly deteriorate the quality of the comparison be-
tween documents).
6
http://nlp.stanford.edu/software/CRF-NER.shtml
7
http://nlp.cs.nyu.edu/oak . OAK was also used to detect
noun phrases and extract lemmas from the text.
4 Experiments and results
4.1 Reformulating WePS as a classification
task
As our goal is to study the impact of different fea-
tures (information sources) in the task, a direct
evaluation in terms of clustering has serious disad-
vantages. Given the output of a clustering system
it is not straightforward to assess why a document
has been assigned to a particular cluster. There are
at least three different factors: the document sim-
ilarity function, the clustering algorithm and its
parameter settings. Features are part of the doc-
ument similarity function, but its performance in
the clustering task depends on the other factors as
well. This makes it difficult to perform error anal-
ysis in terms of the features used to represent the
documents.
Therefore we have decided to transform the
clustering problem into a classification problem:
deciding whether two documents refer to the same
person. Each pair of documents in a name dataset
is considered a classification instance. Instances
are labelled as coreferent (if they share the same
cluster in the gold standard) or non coreferent (if
they do not share the same cluster). Then we
can evaluate the performance of each feature sep-
arately by measuring its ability to rank coreferent
pairs higher and non coreferent pairs lower. In the
case of feature combinations we can study them by
training a classifier or using the maximal pairwise
accuracy methods (explained in Section 4.3).
Each instance (pair of documents) is repre-
sented by the similarity scores obtained using dif-
ferent features and similarity metrics. We have
calculated for each feature three similarity met-
rics: Dice?s coefficient, cosine (using standard
tf.idf weighting) and a measure that simply counts
the size of the intersection set for a given feature
between both documents. After testing these met-
rics we found that Dice provides the best results
across different feature types. Differences be-
tween Dice and cosine were consistent, although
they were not especially large. A possible expla-
nation is that Dice does not take into account the
redundancy of an n-gram or NE in the document,
and the cosine distance does. This can be a cru-
cial factor, for instance, in the document retrieval
by topic; but it doesn?t seem to be the case when
dealing with name ambiguity.
The resulting classification testbed consists of
293,914 instances with the distribution shown in
537
Table 1, where each instance is represented by 69
features.
true false total
WePS1 61,290 122,437 183,727
WePS2 54,641 55,546 110,187
WePS1+WePS2 115,931 177,983 293,914
Table 1: Distribution of classification instances
4.2 Analysis of individual features
There are two main aspects related with the use-
fulness of a feature for WePS task. The first one is
its performance. That is, to what extent the simi-
larity between two documents according to a fea-
ture implies that both mention the same person.
The second aspect is to what extent a feature is or-
thogonal or redundant with respect to the standard
token based similarity.
4.2.1 Feature performance
According to the transformation of WePS cluster-
ing problem into a classification task (described
in Section 4.1), we follow the next steps to study
the performance of individual features. First, we
compute the Dice coefficient similarity over each
feature for all document pairs. Then we rank the
document pair instances according to these simi-
larities. A good feature should rank positive in-
stances on top. If the number of coreferent pairs
in the top n pairs is t
n
and the total number of
coreferent pairs is t, then P =
t
n
n
and R =
t
n
t
. We
plot the obtained precision/recall curves in Figures
1, 2, 3 and 4.
From the figures we can draw the following
conclusions:
First, considering subsets of tokens or lemma-
tised tokens does not outperform the basic token
distance (figure 1 compares token-based features).
We see that only local and snippet tokens perform
slightly better at low recall values, but do not go
beyond recall 0.3.
Second, shallow parsing or n-grams longer than
2 do not seem to be effective, but using bi-grams
improves the results in comparison with tokens.
Figure 2 compares n-grams of different sizes with
noun phrases and tokens. Overall, noun phrases
have a poor performance, and bi-grams give the
best results up to recall 0.7. Four-grams give
slightly better precision but only reach 0.3 recall,
and three-grams do not give better precision than
bi-grams.
Figure 1: Precision/Recall curve of token-based
features
Figure 2: Precision/Recall curve of word n-grams
Third, individual types of NEs do not improve
over tokens. Figure 3 and Figure 4 display the
results obtained by the Stanford and OAK NER
tools respectively. In the best case, Stanford per-
son and organisation named entities obtain results
that match the tokens feature, but only at lower
levels of recall.
Finally, using different NER systems clearly
leads to different results. Surprisingly, the base-
line NE system yields better results in a one to
one comparison, although it must be noted that
this baseline agglomerates different types of en-
538
Figure 3: Precision/Recall curve of NEs obtained
with the Stanford NER tool
Figure 4: Precision/Recall curve of NEs obtained
with the OAK NER tool
tities that are separated in the case of Stanford and
OAK, and this has a direct impact on its recall.
The OAK results are below the tokens and NE
baseline, possibly due to the sparseness of its very
fine grained features. In NE types, cases such as
person and organisation results are still lower than
obtained with Stanford.
4.2.2 Redundancy
In addition to performance, named entities (as well
as other features) are potentially useful for the task
only if they provide information that complements
(i.e. that does not substantially overlap) the basic
token based metric. To estimate this redundancy,
let us consider all document tuples of size four <
a, b, c, d >. In 99% of the cases, token similarity is
different for < a, b > than for < c, d >. We take
combinations such that < a, b > are more similar
to each other than < c, d > according to tokens.
That is:
sim
token
(a, b) > sim
token
(c, d)
Then for any other feature similarity
sim
x
(a, b), we will talk about redundant samples
when sim
x
(a, b) > sim
x
(c, d), non redundant
samples when sim
x
(a, b) < sim
x
(c, d), and
non informative samples when sim
x
(a, b) =
sim
x
(c, d). If all samples are redundant or
non informative, then sim
x
does not provide
additional information for the classification task.
Figure 5 shows the proportion of redundant, non
redundant and non informative samples for sev-
eral similarity criteria, as compared to token-based
similarity. In most cases NE based similarities
give little additional information: the baseline NE
recogniser, which has the largest independent con-
tribution, gives additional information in less than
20% of cases.
In summary, analysing individual features, the
NEs do not outperform BoW in terms of the clas-
sification task. In addition, NEs tend to be re-
dundant regarding BoW. However, if we are able
to combine optimally the contributions of the dif-
ferent features, the BoW approach could be im-
proved. We address this issue in the next section.
Figure 5: Independence of similarity criteria with
respect to the token based feature
539
4.3 Analysis of feature combinations
Up to now we have analysed the usefulness of in-
dividual features for the WePS Task. However,
this begs to ask to what extent the NE features can
contribute to the task when they are combined to-
gether and with token and n-gram based features.
First, we use each feature combinations as the in-
put for a Machine Learning algorithm. In particu-
lar, we use a Decision Tree algorithm and WePS-1
data for training and WePS-2 data for testing. The
Decision Tree algorithm was chosen because we
have a small set of features to train (similarity met-
rics) and some of these features output Boolean
values.
Results obtained with this setup, however, can
be dependent on the choice of the ML approach.
To overcome this problem, in addition to the re-
sults of a Decision Tree Machine Learning algo-
rithm, we introduce a Maximal Pairwise Accuracy
(MPA) measure that provides an upper bound for
any machine learning algorithm using a feature
combination.
We can estimate the performance of an individ-
ual similarity feature x in terms of accuracy. It
is considered a correct answer when the similarity
x(a, a
?
) between two pages referring to the same
person is higher than the similarity x(b, c) between
two pages referring to different people. Let us
call this estimation Pairwise Accuracy. In terms
of probability it can be defined as:
PWA = Prob(x(a, a
?
) > x(c, d))
PWA is defined over a single feature (similar-
ity metric). When considering more than one sim-
ilarity measure, the results depend on how mea-
sures are weighted. In that case we assume that
the best possible weighting is applied. When com-
bining a set of features X = {x
1
. . . x
n
}, a per-
fect Machine Learning algorithm would learn to
always ?listen? to the features giving correct in-
formation and ignore the features giving erroneous
information. In other words, if at least one feature
gives correct information, then the perfect algo-
rithm would produce a correct output. This is what
we call the Maximal Pairwise Accuracy estimation
of an upper bound for any ML system using the set
of features X:
MaxPWA(X) =
Prob(?x ? X.x(a, a
?
) > x(c, d))
Figure 6: Estimated PWA upper bound versus the
real PWA of decision trees trained with feature
combinations
Figure 7: Maximal Pairwise Accuracy vs. results
of a Decision Tree
The upper bound (MaxPWA) of feature combi-
nations happens to be highly correlated with the
PWA obtained by the Decision Tree algorithm (us-
ing its confidence values as a similarity metric).
Figure 6 shows this correlation for several features
combinations. This is an indication that the Deci-
sion Tree is effectively using the information in the
feature set.
Figure 7 shows the PWA upper bound estima-
tion and the actual PWA performance of a Deci-
sion Tree ML algorithm for three combinations:
(i) all features; (ii) non linguistic features, i.e.,
features which can be extracted without natural
language processing machinery: tokens, url, title,
snippet, local tokens, n-grams and local n-grams;
and (iii) just tokens. The results show that accord-
ing to both the Decision Tree results and the upper-
bound (MaxPWA), adding new features to tokens
improves the classification. However, taking non-
linguistic features obtains similar results than tak-
ing all features. Our conclusion is that NE features
are useful for the task, but do not seem to offer a
540
competitive advantage when compared with non-
linguistic features, and are more computationally
expensive. Note that we are using NE features in a
direct way: our results do not exclude the possibil-
ity of effectively exploiting NEs in more sophisti-
cated ways, such as, for instance, exploiting the
underlying social network relationships between
NEs in the texts.
4.3.1 Results on the clustering task
In order to validate our results, we have tested
whether the classifiers learned with our feature
sets lead to competitive systems for the full clus-
tering task. In order to do so, we use the output of
the classifiers as similarity metrics for a particu-
lar clustering algorithm, using WePS-1 to train the
classifiers and WePS-2 for testing.
We have used a Hierarchical Agglomerative
Clustering algorithm (HAC) with single linkage,
using the classifier?s confidence value in the nega-
tive answer for each instance as a distance metric
8
between document pairs. HAC is the algorithm
used by some of the best performing systems in the
WePS-2 evaluation. The distance threshold was
trained using the WePS-1 data. We report results
with the official WePS-2 evaluation metrics: ex-
tended B-Cubed Precision and Recall (Amig?o et
al., 2008).
Two Decision Tree models were evaluated: (i)
ML-ALL is a model trained using all the available
features (which obtains 0.76 accuracy in the clas-
sification task) (ii) ML-NON LING was trained
with all the features except for OAK and Stanford
NEs, noun phrases, lemmas and gazetteer features
(which obtains 0.75 accuracy in the classification
task). These are the same classifiers considered in
Figure 7.
Table 2 shows the results obtained in the clus-
tering task by the two DT models, together with
the four top scoring WePS-2 systems and the av-
erage values for all WePS-2 systems. We found
that a ML based clustering using only non linguis-
tic information slightly outperforms the best par-
ticipant in WePS-2. Surprisingly, adding linguis-
tic information (NEs, noun phrases, etc.) has a
small negative impact on the results (0.81 versus
0.83), although the classifier with linguistic infor-
mation was a bit better than the non-linguistic one.
This seems to be another indication that the use of
8
The DT classifier output consists of two confidence val-
ues, one for the positive and one for the negative answer, that
add up to 1.0 .
noun phrases and other linguistic features to im-
prove the task is non-obvious to say the least.
B-Cubed
run F-? =
0.5
Pre. Rec.
ML-NON LING .83 .91 .77
S-1 .82 .87 .79
ML- ALL .81 .89 .76
S-2 .81 .85 .80
S-3 .81 .93 .73
S-4 .72 .82 .66
WePS-2 systems aver. .61 .74 .63
Table 2: Evaluation on the WePS-2 clustering task
5 Conclusions
We have presented an empirical study that tries to
determine the potential role of several sources of
information to solve the Web People Search clus-
tering problem, with a particular focus on studying
the role of named entities in the task.
To abstract the study from the particular choice
of a clustering algorithm and a parameter set-
ting, we have reformulated the problem as a co-
reference classification task: deciding whether
two pages refer to the same person or not. We
have also proposed the Maximal Pairwise Accu-
racy estimation that establish an upper bound for
the results obtained by any Machine Learning al-
gorithm using a particular set of features.
Our results indicate that (i) NEs do not provide a
substantial competitive advantage in the clustering
process when compared to a rich combination of
simpler features that do not require linguistic pro-
cessing (local, global and snippet tokens, n-grams,
etc.); (ii) results are sensitive to the NER system
used: when using all NE features for training, the
richer number of features provided by OAK seems
to have an advantage over the simpler types in
Stanford NER and the baseline NER system.
This is not exactly a prescription against the use
of NEs for Web People Search, because linguistic
knowledge can be useful for other aspects of the
problem, such as visualisation of results and de-
scription of the persons/clusters obtained: for ex-
ample, from a user point of view a network of the
connections of a person with other persons and or-
ganisations (which can only be done with NER)
can be part of a person?s profile and may help as
a summary of the cluster contents. But from the
perspective of the clustering problem per se, a di-
rect use of NEs and other linguistic features does
not seem to pay off.
541
Acknowledgments
This work has been partially supported by the
Regional Government of Madrid, project MAVIR
S0505-TIC0267.
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer.
Reema Al-Kamha and David W. Embley. 2004.
Grouping search-engine returned citations for
person-name queries. In WIDM ?04: Proceedings of
the 6th annual ACM international workshop on Web
information and data management. ACM Press.
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2008. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information Retrieval.
Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2005.
A testbed for people searching strategies in the
www. In SIGIR.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007. The semeval-2007 weps evaluation: Estab-
lishing a benchmark for the web people search task.
In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007).
ACL.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2009. Weps 2 evaluation campaign: overview of
the web people search clustering task. In WePS 2
Evaluation Workshop. WWW Conference 2009.
Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 17th inter-
national conference on Computational linguistics.
ACL.
Matthias Blume. 2005. Automatic entity disambigua-
tion: Benefits to ner, relation extraction, link anal-
ysis, and inference. In International Conference on
Intelligence Analysis.
Ying Chen and James H. Martin. 2007. Cu-comsem:
Exploring rich features for unsupervised web per-
sonal name disambiguation. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ations. ACL.
Silviu Cucerzan. 2007. Large scale named entity
disambiguation based on wikipedia data. In The
EMNLP-CoNLL-2007.
David del Valle-Agudo, C?esar de Pablo-S?anchez, and
Mar??a Teresa Vicente-D??ez. 2007. Uc3m-13: Dis-
ambiguation of person names based on the compo-
sition of simple bags of typed terms. In Proceedings
of the Fourth International Workshop on Semantic
Evaluations. ACL.
Chung Heong Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
HLT-NAACL.
Dmitri V. Kalashnikov, Stella Chen, Rabia Nuray,
Sharad Mehrotra, and Naveen Ashish. 2007. Dis-
ambiguation algorithm for people search on the web.
In Proc. of IEEE International Conference on Data
Engineering (IEEE ICDE).
Bradley Malin. 2005. Unsupervised name disam-
biguation via social network similarity. In Workshop
on Link Analysis, Counterterrorism, and Security.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceed-
ings of the seventh conference on Natural Language
Learning (CoNLL) at HLT-NAACL 2003. ACL.
Gideon S. Mann. 2006. Multi-Document Statistical
Fact Extraction and Fusion. Ph.D. thesis, Johns
Hopkins University.
Hien T. Nguyen and Tru H. Cao, 2008. Named En-
tity Disambiguation: A Hybrid Statistical and Rule-
Based Incremental Approach. Springer.
Octavian Popescu and Bernardo Magnini. 2007. Irst-
bp: Web people search using name entities. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations. ACL.
Y. Ravin and Z. Kazi. 1999. Is hillary rodham clinton
the president? disambiguating names across docu-
ments. In Proceedings of the ACL ?99 Workshop
on Coreference and its Applications Association for
Computational Linguistics.
Horacio Saggion. 2008. Experiments on semantic-
based clustering for cross-document coreference. In
International Joint Conference on Natural language
Processing.
Satoshi Sekine. 2008. Extended named entity on-
tology with attribute information. In Proceedings
of the Sixth International Language Resources and
Evaluation (LREC?08).
Amanda Spink, Bernard Jansen, and Jan Pedersen.
2004. Searching for people on web search engines.
Journal of Documentation, 60:266 ? 278.
Kazunari Sugiyama and Manabu Okumura. 2007.
Titpi: Web people search task using semi-supervised
clustering approach. In Proceedings of the Fourth
International Workshop on Semantic Evaluations.
ACL.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong
Ding. 2005. Person resolution in person search re-
sults: Webhawk. In CIKM ?05: Proceedings of the
14th ACM international conference on Information
and knowledge management. ACM Press.
542
An Empirical Study of Information Synthesis Tasks
Enrique Amigo? Julio Gonzalo V??ctor Peinado Anselmo Pen?as Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,victor,anselmo,felisa}@lsi.uned.es
Abstract
This paper describes an empirical study of the ?In-
formation Synthesis? task, defined as the process of
(given a complex information need) extracting, or-
ganizing and inter-relating the pieces of information
contained in a set of relevant documents, in order to
obtain a comprehensive, non redundant report that
satisfies the information need.
Two main results are presented: a) the creation
of an Information Synthesis testbed with 72 reports
manually generated by nine subjects for eight com-
plex topics with 100 relevant documents each; and
b) an empirical comparison of similarity metrics be-
tween reports, under the hypothesis that the best
metric is the one that best distinguishes between
manual and automatically generated reports. A met-
ric based on key concepts overlap gives better re-
sults than metrics based on n-gram overlap (such as
ROUGE) or sentence overlap.
1 Introduction
A classical Information Retrieval (IR) system helps
the user finding relevant documents in a given text
collection. In most occasions, however, this is only
the first step towards fulfilling an information need.
The next steps consist of extracting, organizing and
relating the relevant pieces of information, in or-
der to obtain a comprehensive, non redundant report
that satisfies the information need.
In this paper, we will refer to this process as In-
formation Synthesis. It is normally understood as
an (intellectually challenging) human task, and per-
haps the Google Answer Service1 is the best gen-
eral purpose illustration of how it works. In this ser-
vice, users send complex queries which cannot be
answered simply by inspecting the first two or three
documents returned by a search engine. These are a
couple of real, representative examples:
a) I?m looking for information concerning the history of text
compression both before and with computers.
1http://answers.google.com
b) Provide an analysis on the future of web browsers, if
any.
Answers to such complex information needs are
provided by experts which, commonly, search the
Internet, select the best sources, and assemble the
most relevant pieces of information into a report,
organizing the most important facts and providing
additional web hyperlinks for further reading. This
Information Synthesis task is understood, in Google
Answers, as a human task for which a search engine
only provides the initial starting point. Our mid-
term goal is to develop computer assistants that help
users to accomplish Information Synthesis tasks.
From a Computational Linguistics point of view,
Information Synthesis can be seen as a kind of
topic-oriented, informative multi-document sum-
marization, where the goal is to produce a single
text as a compressed version of a set of documents
with a minimum loss of relevant information. Un-
like indicative summaries (which help to determine
whether a document is relevant to a particular topic),
informative summaries must be helpful to answer,
for instance, factual questions about the topic. In
the remainder of the paper, we will use the term
?reports? to refer to the summaries produced in an
Information Synthesis task, in order to distinguish
them from other kinds of summaries.
Topic-oriented multi-document summarization
has already been studied in other evaluation ini-
tiatives which provide testbeds to compare alterna-
tive approaches (Over, 2003; Goldstein et al, 2000;
Radev et al, 2000). Unfortunately, those stud-
ies have been restricted to very small summaries
(around 100 words) and small document sets (10-
20 documents). These are relevant summarization
tasks, but hardly representative of the Information
Synthesis problem we are focusing on.
The first goal of our work has been, therefore,
to create a suitable testbed that permits qualitative
and quantitative studies on the information synthe-
sis task. Section 2 describes the creation of such a
testbed, which includes the manual generation of 72
reports by nine different subjects across 8 complex
topics with 100 relevant documents per topic.
Using this testbed, our second goal has been to
compare alternative similarity metrics for the Infor-
mation Synthesis task. A good similarity metric
provides a way of evaluating Information Synthe-
sis systems (comparing their output with manually
generated reports), and should also shed some light
on the common properties of manually generated re-
ports. Our working hypothesis is that the best metric
will best distinguish between manual and automati-
cally generated reports.
We have compared several similarity metrics, in-
cluding a few baseline measures (based on docu-
ment, sentence and vocabulary overlap) and a state-
of-the-art measure to evaluate summarization sys-
tems, ROUGE (Lin and Hovy, 2003). We also intro-
duce another proximity measure based on key con-
cept overlap, which turns out to be substantially bet-
ter than ROUGE for a relevant class of topics.
Section 3 describes these metrics and the experi-
mental design to compare them; in Section 4, we an-
alyze the outcome of the experiment, and Section 5
discusses related work. Finally, Section 6 draws the
main conclusions of this work.
2 Creation of an Information Synthesis
testbed
We refer to Information Synthesis as the process
of generating a topic-oriented report from a non-
trivial amount of relevant, possibly interrelated doc-
uments. The first goal of our work is the generation
of a testbed (ISCORPUS) with manually produced
reports that serve as a starting point for further em-
pirical studies and evaluation of information synthe-
sis systems. This section describes how this testbed
has been built.
2.1 Document collection and topic set
The testbed must have a certain number of features
which, altogether, differentiate the task from current
multi-document summarization evaluations:
Complex information needs. Being Informa-
tion Synthesis a step which immediately follows a
document retrieval process, it seems natural to start
with standard IR topics as used in evaluation con-
ferences such as TREC2, CLEF3 or NTCIR4. The
title/description/narrative topics commonly used in
such evaluation exercises are specially well suited
for an Information Synthesis task: they are complex
2http://trec.nist.gov
3http://www.clef-campaign.org
4http://research.nii.ac.jp/ntcir/
and well defined, unlike, for instance, typical web
queries.
We have selected the Spanish CLEF 2001-2003
news collection testbed (Peters et al, 2002), be-
cause Spanish is the native language of the subjects
recruited for the manual generation of reports. Out
of the CLEF topic set, we have chosen the eight
topics with the largest number of documents man-
ually judged as relevant from the assessment pools.
We have slightly reworded the topics to change the
document retrieval focus (?Find documents that...?)
into an information synthesis wording (?Generate a
report about...?). Table 1 shows the eight selected
topics.
C042: Generate a report about the invasion of Haiti by UN/US
soldiers.
C045: Generate a report about the main negotiators of the
Middle East peace treaty between Israel and Jordan, giving
detailed information on the treaty.
C047: What are the reasons for the military intervention of
Russia in Chechnya?
C048: Reasons for the withdrawal of United Nations (UN)
peace- keeping forces from Bosnia.
C050: Generate a report about the uprising of Indians in
Chiapas (Mexico).
C085: Generate a report about the operation ?Turquoise?, the
French humanitarian program in Rwanda.
C056: Generate a report about campaigns against racism in
Europe.
C080: Generate a report about hunger strikes attempted in
order to attract attention to a cause.
Table 1: Topic set
This set of eight CLEF topics has two differenti-
ated subsets: in a majority of cases (first six topics),
it is necessary to study how a situation evolves in
time; the importance of every event related to the
topic can only be established in relation with the
others. The invasion of Haiti by UN and USA troops
(C042) is an example of such a topic. We will refer
to them as ?Topic Tracking? (TT) reports, because
they resemble the kind of topics used in such task.
The last two questions (56 and 80), however, re-
semble Information Extraction tasks: essentially,
the user has to detect and describe instances of
a generic event (cases of hunger strikes and cam-
paigns against racism in Europe); hence we will re-
fer to them as ?IE? reports.
Topic tracking reports need a more elaborated
treatment of the information in the documents, and
therefore are more interesting from the point of view
of Information Synthesis. We have, however, de-
cided to keep the two IE topics; first, because they
also reflect a realistic synthesis task; and second, be-
cause they can provide contrastive information as
compared to TT reports.
Large document sets. All the selected CLEF
topics have more than one hundred documents
judged as relevant by the CLEF assessors. For ho-
mogeneity, we have restricted the task to the first
100 documents for each topic (using a chronologi-
cal order).
Complex reports. The elaboration of a com-
prehensive report requires more space than is al-
lowed in current multi-document summarization ex-
periences. We have established a maximum of fifty
sentences per summary, i.e., half a sentence per doc-
ument. This limit satisfies three conditions: a) it
is large enough to contain the essential information
about the topic, b) it requires a substantial compres-
sion effort from the user, and c) it avoids defaulting
to a ?first sentence? strategy by lazy (or tired) users,
because this strategy would double the maximum
size allowed.
We decided that the report generation would be
an extractive task, which consists of selecting sen-
tences from the documents. Obviously, a realistic
information synthesis process also involves rewrit-
ing and elaboration of the texts contained in the doc-
uments. Keeping the task extractive has, however,
two major advantages: first, it permits a direct com-
parison to automatic systems, which will typically
be extractive; and second, it is a simpler task which
produces less fatigue.
2.2 Generation of manual reports
Nine subjects between 25 and 35 years-old were re-
cruited for the manual generation of reports. All
of them self-reported university degrees and a large
experience using search engines and performing in-
formation searches.
All subjects were given an in-place detailed de-
scription of the task in order to minimize divergent
interpretations. They were told that, in a first step,
they had to generate reports with a maximum of in-
formation about every topic within the fifty sentence
space limit. In a second step, which would take
place six months afterwards, they would be exam-
ined from each of the eight topics. The only docu-
mentation allowed during the exam would be the re-
ports generated in the first phase of the experiment.
Subjects scoring best would be rewarded.
These instructions had two practical effects: first,
the competitive setup was an extra motivation for
achieving better results. And second, users tried to
take advantage of all available space, and thus most
reports were close to the fifty sentences limit. The
time limit per topic was set to 30 minutes, which is
tight for the information synthesis task, but prevents
the effects of fatigue.
We implemented an interface to facilitate the gen-
eration of extractive reports. The system displays a
list with the titles of relevant documents in chrono-
logical order. Clicking on a title displays the full
document, where the user can select any sentence(s)
and add them to the final report. A different frame
displays the selected sentences (also in chronolog-
ical order), together with one bar indicating the re-
maining time and another bar indicating the remain-
ing space. The 50 sentence limit can be temporarily
exceeded and, when the 30 minute limit has been
reached, the user can still remove sentences from
the report until the sentence limit is reached back.
2.3 Questionnaires
After summarizing every topic, the following ques-
tionnaire was filled in by every user:
? Who are the main people involved in the topic?
? What are the main organizations participating in the
topic?
? What are the key factors in the topic?
Users provided free-text answers to these ques-
tions, with their freshly generated summary at hand.
We did not provide any suggestions or constraints
at this point, except that a maximum of eight slots
were available per question (i.e. a maximum of
8X3 = 24 key concepts per topic, per user).
This is, for instance, the answer of one user for
the topic 42 about the invasion of Haiti by UN and
USA troops in 1994:
People Organizations
Jean Bertrand Aristide ONU (UN)
Clinton EEUU (USA)
Raoul Cedras OEA (OAS)
Philippe Biambi
Michel Josep Francois
Factors
militares golpistas (coup attempting soldiers)
golpe militar (coup attempt)
restaurar la democracia (reinstatement of democracy)
Finally, a single list of key concepts is gener-
ated for each topic, joining all the different answers.
Redundant concepts (e.g. ?war? and ?conflict?)
were inspected and collapsed by hand. These lists
of key concepts constitute the gold standard for the
similarity metric described in Section 3.2.5.
Besides identifying key concepts, users also filled
in the following questionnaire:
? Were you familiarized with the topic?
? Was it hard for you to elaborate the report?
? Did you miss the possibility of introducing annotations
or rewriting parts of the report by hand?
? Do you consider that you generated a good report?
? Are you tired?
Out of the answers provided by users, the most
remarkable facts are that:
? only in 6% of the cases the user missed ?a lot?
the possibility of rewriting/adding comments
to the topic. The fact that reports are made ex-
tractively did not seem to be a significant prob-
lem for our users.
? in 73% of the cases, the user was quite or very
satisfied about his summary.
These are indications that the practical con-
straints imposed on the task (time limit and extrac-
tive nature of the summaries) do not necessarily
compromise the representativeness of the testbed.
The time limit is very tight, but the temporal ar-
rangement of documents and their highly redundant
nature facilitates skipping repetitive material (some
pieces of news are discarded just by looking at the
title, without examining the content).
2.4 Generation of baseline reports
We have automatically generated baseline reports in
two steps:
? For every topic, we have produced 30 tentative
baseline reports using DUC style criteria:
? 18 summaries consist only of picking the
first sentence out of each document in 18
different document subsets. The subsets
are formed using different strategies, e.g.
the most relevant documents for the query
(according to the Inquery search engine),
one document per day, the first or last 50
documents in chronological order, etc.
? The other 12 summaries consist of a)
picking the first n sentences out of a set
of selected documents (with different val-
ues for n and different sets of documents)
and b) taking the full content of a few doc-
uments. In both cases, document sets are
formed with similar criteria as above.
? Out of these 30 baseline reports, we have se-
lected the 10 reports which have the highest
sentence overlap with the manual summaries.
The second step increases the quality of the base-
lines, making the task of differentiating manual and
baseline reports more challenging.
3 Comparison of similarity metrics
Formal aspects of a summary (or report), such
as legibility, grammatical correctness, informative-
ness, etc., can only be evaluated manually. How-
ever, automatic evaluation metrics can play a useful
role in the evaluation of how well the information
from the original sources is preserved (Mani, 2001).
Previous studies have shown that it is feasible to
evaluate the output of summarization systems au-
tomatically (Lin and Hovy, 2003). The process is
based in similarity metrics between texts. The first
step is to establish a (manual) reference summary,
and then the automatically generated summaries are
ranked according to their similarity to the reference
summary.
The challenge is, then, to define an appropriate
proximity metric for reports generated in the infor-
mation synthesis task.
3.1 How to compare similarity metrics without
human judgments? The QARLA
estimation
In tasks such as Machine Translation and Summa-
rization, the quality of a proximity metric is mea-
sured in terms of the correlation between the rank-
ing produced by the metric, and a reference ranking
produced by human judges. An optimal similarity
metric should produce the same ranking as human
judges.
In our case, acquiring human judgments about
the quality of the baseline reports is too costly, and
probably cannot be done reliably: a fine-grained
evaluation of 50-sentence reports summarizing sets
of 100 documents is a very complex task, which
would probably produce different rankings from
different judges.
We believe there is a cheaper and more robust
way of comparing similarity metrics without using
human assessments. We assume a simple hypothe-
sis: the best metric should be the one that best dis-
criminates between manual and automatically gen-
erated reports. In other words, a similarity metric
that cannot distinguish manual and automatic re-
ports cannot be a good metric. Then, all we need
is an estimation of how well a similarity metric sep-
arates manual and automatic reports. We propose
to use the probability that, given any manual report
Mref , any other manual report M is closer to Mref
than any other automatic report A:
QARLA(sim) = P (sim(M,Mref ) > sim(A,Mref ))
where M,Mref ?M, A ? A
where M is the set of manually generated re-
ports, A is the set of automatically generated re-
ports, and ?sim? is the similarity metric being eval-
uated.
We refer to this value as the QARLA5 estimation.
QARLA has two interesting features:
? No human assessments are needed to compute
QARLA. Only a set of manually produced
summaries and a set of automatic summaries,
for each topic considered. This reduces the
cost of creating the testbed and, in addition,
eliminates the possible bias introduced by hu-
man judges.
? It is easy to collect enough data to achieve sta-
tistically significant results. For instance, our
testbed provides 720 combinations per topic
to estimate QARLA probability (we have
nine manual plus ten automatic summaries per
topic).
A good QARLA value does not guarantee that
a similarity metric will produce the same rankings
as human judges, but a good similarity metric must
have a good QARLA value: it is unlikely that
a measure that cannot distinguish between manual
and automatic summaries can still produce high-
quality rankings of automatic summaries by com-
parison to manual reference summaries.
3.2 Similarity metrics
We have compared five different metrics using the
QARLA estimation. The first three are meant as
baselines; the fourth is the standard similarity met-
ric used to evaluate summaries (ROUGE); and the
last one, introduced in this paper, is based on the
overlapping of key concepts.
3.2.1 Baseline 1: Document co-selection metric
The following metric estimates the similarity of two
reports from the set of documents which are repre-
sented in both reports (i.e. at least one sentence in
each report belongs to the document).
DocSim(Mr,M) =
|Doc(Mr) ?Doc(M)|
|Doc(Mr)|
where Mr is the reference report, M a second re-
port and Doc(Mr), Doc(M) are the documents to
which the sentences in Mr,M belong to.
5Quality criterion for reports evaluation metrics
3.2.2 Baselines 2 and 3: Sentence co-selection
The more sentences in common between two re-
ports, the more similar their content will be. We can
measure Recall (how many sentences from the ref-
erence report are also in the contrastive report) and
Precision (how many sentences from the contrastive
report are also in the reference report):
SentenceSimR(Mr,M) =
|S(Mr) ? S(M)|
|S(Mr)|
SentenceSimP (Mr,M) =
|S(Mr) ? S(M)|
|S(M)|
where S(Mr), S(M) are the sets of sentences in
the reports Mr (reference) and M (contrastive).
3.2.3 Baseline 4: Perplexity
A language model is a probability distribution over
word sequences obtained from some training cor-
pora (see e.g. (Manning and Schutze, 1999)). Per-
plexity is a measure of the degree of surprise of a
text or corpus given a language model. In our case,
we build a language model LM(Mr) for the refer-
ence report Mr, and measure the perplexity of the
contrastive report M as compared to that language
model:
PerplexitySim(Mr,M) =
1
Perp(LM(Mr),M)
We have used the Good-Turing discount algo-
rithm to compute the language models (Clarkson
and Rosenfeld, 1997). Note that this is also a base-
line metric, because it only measures whether the
content of the contrastive report is compatible with
the reference report, but it does not consider the cov-
erage: a single sentence from the reference report
will have a low perplexity, even if it covers only a
small fraction of the whole report. This problem
is mitigated by the fact that we are comparing re-
ports of approximately the same size and without
repeated sentences.
3.2.4 ROUGE metric
The distance between two summaries can be estab-
lished as a function of their vocabulary (unigrams)
and how this vocabulary is used (n-grams). From
this point of view, some of the measures used in the
evaluation of Machine Translation systems, such as
BLEU (Papineni et al, 2002), have been imported
into the summarization task. BLEU is based in the
precision and n-gram co-ocurrence between an au-
tomatic translation and a reference manual transla-
tion.
(Lin and Hovy, 2003) tried to apply BLEU as
a measure to evaluate summaries, but the results
were not as good as in Machine Translation. In-
deed, some of the characteristics that define a good
translation are not related with the features of a good
summary; then Lin and Hovy proposed a recall-
based variation of BLEU, known as ROUGE. The
idea is the same: the quality of a proposed sum-
mary can be calculated as a function of the n-grams
in common between the units of a model summary.
The units can be sentences or discourse units:
ROUGEn =
?
C?{MU}
?
n-gram?C Countm
?
C?{MU}
?
n-gram?C Count
where MU is the set of model units, Countm is
the maximum number of n-grams co-ocurring in a
peer summary and a model unit, and Count is the
number of n-grams in the model unit. It has been
established that unigram and bigram based metrics
permit to create a ranking of automatic summaries
better (more similar to a human-produced ranking)
than n-grams with n > 2.
For our experiment, we have only considered un-
igrams (lemmatized words, excluding stop words),
which gives good results with standard summaries
(Lin and Hovy, 2003).
3.2.5 Key concepts metric
Two summaries generated by different subjects may
differ in the documents that contribute to the sum-
mary, in the sentences that are chosen, and even in
the information that they provide. In our Informa-
tion Synthesis settings, where topics are complex
and the number of documents to summarize is large,
it is likely to expect that similarity measures based
on document, sentence or n-gram overlap do not
give large similarity values between pairs of man-
ually generated summaries.
Our hypothesis is that two manual reports, even if
they differ in their information content, will have the
same (or very similar) key concepts; if this is true,
comparing the key concepts of two reports can be a
better similarity measure than the previous ones.
In order to measure the overlap of key concepts
between two reports, we create a vector ~kc for every
report, such that every element in the vector repre-
sents the frequency of a key concept in the report in
relation to the size of the report:
kc(M)i =
freq(Ci,M)
|words(M)|
being freq(Ci,M) the number of times the
key concept Ci appears in the report M , and
|words(M)| the number of words in the report.
The key concept similarity NICOS (Nuclear In-
formative Concept Similarity) between two reports
M and Mr can then be defined as the inverse of the
Euclidean distance between their associated concept
vectors:
NICOS(M,Mr) =
1
| ~kc(Mr)? ~kc(M)|
In our experiment, the dimensions of kc vectors
correspond to the list of key concepts provided by
our test subjects (see Section 2.3). This list is our
gold standard for every topic.
4 Experimental results
Figure 1 shows, for every topic (horizontal axis),
the QARLA estimation obtained for each similarity
metric, i.e., the probability of a manual report being
closer to other manual report than to an automatic
report. Table 2 shows the average QARLA measure
across all topics.
Metric TT topics IE topics
Perplexity 0.19 0.60
DocSim 0.20 0.34
SentenceSimR 0.29 0.52
SentenceSimP 0.38 0.57
ROUGE 0.54 0.53
NICOS 0.77 0.52
Table 2: Average QARLA
For the six TT topics, the key concept similarity
NICOS performs 43% better than ROUGE, and all
baselines give poor results (all their QARLA proba-
bilities are below chance, QARLA < 0.5). A non-
parametric Wilcoxon sign test confirms that the dif-
ference between NICOS and ROUGE is highly sig-
nificant (p < 0.005). This is an indication that the
Information Synthesis task, as we have defined it,
should not be studied as a standard summarization
problem. It also confirms our hypothesis that key
concepts tend to be stable across different users, and
may help to generate the reports.
The behavior of the two Information Extraction
(IE) topics is substantially different from TT topics.
While the ROUGE measure remains stable (0.53
versus 0.54), the key concept similarity is much
worse with IE topics (0.52 versus 0.77). On the
other hand, all baselines improve, and some of them
(SentenceSim precision and perplexity) give better
results than both ROUGE and NICOS.
Of course, no reliable conclusion can be obtained
from only two IE topics. But the observed differ-
ences suggest that TT and IE may need different
approaches, both to the automatic generation of re-
ports and to their evaluation.
Figure 1: Comparison of similarity metrics by topic
One possible reason for this different behavior is
that IE topics do not have a set of consistent key
concepts; every case of a hunger strike, for instance,
involves different people, organizations and places.
The average number of different key concepts is
18.7 for TT topics and 28.5 for IE topics, a differ-
ence that reveals less agreement between subjects,
supporting this argument.
5 Related work
Besides the measures included in our experiment,
there are other criteria to compare summaries which
could as well be tested for Information Synthesis:
Annotation of relevant sentences in a corpus.
(Khandelwal et al, 2001) propose a task, called
?Temporal Summarization?, that combines summa-
rization and topic tracking. The paper describes the
creation of an evaluation corpus in which the most
relevant sentences in a set of related news were an-
notated. Summaries are evaluated with a measure
called ?novel recall?, based in sentences selected by
a summarization system and sentences manually as-
sociated to events in the corpus. The agreement rate
between subjects in the identification of key events
and the sentence annotation does not correspond
with the agreement between reports that we have
obtained in our experiments. There are, at least, two
reasons to explain this:
? (Khandelwal et al, 2001) work on an average
of 43 documents, half the size of the topics in
our corpus.
? Although there are topics in both experiments,
the information needs in our testbed are more
complex (e.g. motivations for the invasion of
Chechnya)
Factoids. One of the problems in the evalua-
tion of summaries is the versatility of human lan-
guage. Two different summaries may contain the
same information. In (Halteren and Teufel, 2003),
the content of summaries is manually represented,
decomposing sentences in factoids or simple facts.
They also annotate the composition, generalization
and implication relations between extracted fac-
toids. The resulting measure is different from un-
igram based similarity. The main problem of fac-
toids, as compared to other metrics, is that they re-
quire a costly manual processing of the summaries
to be evaluated.
6 Conclusions
In this paper, we have reported an empirical study
of the ?Information Synthesis? task, defined as the
process of (given a complex information need) ex-
tracting, organizing and relating the pieces of infor-
mation contained in a set of relevant documents, in
order to obtain a comprehensive, non redundant re-
port that satisfies the information need.
We have obtained two main results:
? The creation of an Information Synthesis
testbed (ISCORPUS) with 72 reports manually
generated by 9 subjects for 8 complex topics
with 100 relevant documents each.
? The empirical comparison of candidate metrics
to estimate the similarity between reports.
Our empirical comparison uses a quantitative cri-
terion (the QARLA estimation) based on the hy-
pothesis that a good similarity metric will be able to
distinguish between manual and automatic reports.
According to this measure, we have found evidence
that the Information Synthesis task is not a standard
multi-document summarization problem: state-of-
the-art similarity metrics for summaries do not per-
form equally well with the reports in our testbed.
Our most interesting finding is that manually
generated reports tend to have the same key con-
cepts: a similarity metric based on overlapping key
concepts (NICOS) gives significantly better results
than metrics based on language models, n-gram co-
ocurrence and sentence overlapping. This is an in-
dication that detecting relevant key concepts is a
promising strategy in the process of generating re-
ports.
Our results, however, has also some intrinsic lim-
itations. Firstly, manually generated summaries are
extractive, which is good for comparison purposes,
but does not faithfully reflect a natural process of
human information synthesis. Another weakness is
the maximum time allowed per report: 30 minutes
seems too little to examine 100 documents and ex-
tract a decent report, but allowing more time would
have caused an excessive fatigue to users. Our vol-
unteers, however, reported a medium to high satis-
faction with the results of their work, and in some
occasions finished their task without reaching the
time limit.
ISCORPUS is available at:
http://nlp.uned.es/ISCORPUS
Acknowledgments
This research has been partially supported by a
grant of the Spanish Government, project HERMES
(TIC-2000-0335-C03-01). We are indebted to E.
Hovy for his comments on an earlier version of
this paper, and C. Y. Lin for his assistance with the
ROUGE measure. Thanks also to our volunteers for
their valuable cooperation.
References
P. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
toolkit. In Proceeding of Eurospeech ?97,
Rhodes, Greece.
J. Goldstein, V. O. Mittal, J. G. Carbonell, and
J. P. Callan. 2000. Creating and Evaluating
Multi-Document Sentence Extract Summaries.
In Proceedings of Ninth International Confer-
ences on Information Knowledge Management
(CIKM?00), pages 165?172, McLean, VA.
H. V. Halteren and S. Teufel. 2003. Examin-
ing the Consensus between Human Summaries:
Initial Experiments with Factoids Analysis. In
HLT/NAACL-2003 Workshop on Automatic Sum-
marization, Edmonton, Canada.
V. Khandelwal, R. Gupta, and J. Allan. 2001. An
Evaluation Corpus for Temporal Summarization.
In Proceedings of the First International Confer-
ence on Human Language Technology Research
(HLT 2001), Tolouse, France.
C. Lin and E. H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-ocurrence
Statistics. In Proceeding of the 2003 Language
Technology Conference (HLT-NAACL 2003), Ed-
monton, Canada.
I. Mani. 2001. Automatic Summarization, vol-
ume 3 of Natural Language Processing. John
Benjamins Publishing Company, Amster-
dam/Philadelphia.
C. D. Manning and H. Schutze. 1999. Foundations
of statistical natural language processing. MIT
Press, Cambridge Mass.
P. Over. 2003. Introduction to DUC-2003: An In-
trinsic Evaluation of Generic News Text Summa-
rization Systems. In Proceedings of Workshop on
Automatic Summarization (DUC 2003).
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?
318, Philadelphia.
C. Peters, M. Braschler, J. Gonzalo, and M. Kluck,
editors. 2002. Evaluation of Cross-Language
Information Retrieval Systems, volume 2406 of
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
D. R. Radev, J. Hongyan, and M. Budzikowska.
2000. Centroid-Based Summarization of Mul-
tiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceed-
ings of the Workshop on Automatic Summariza-
tion at the 6th Applied Natural Language Pro-
cessing Conference and the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics, Seattle, WA, April.
Proceedings of the 43rd Annual Meeting of the ACL, pages 280?289,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
QARLA:A Framework for the Evaluation of Text Summarization Systems
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,anselmo,felisa}@lsi.uned.es
Abstract
This paper presents a probabilistic
framework, QARLA, for the evaluation
of text summarisation systems. The in-
put of the framework is a set of man-
ual (reference) summaries, a set of base-
line (automatic) summaries and a set of
similarity metrics between summaries.
It provides i) a measure to evaluate the
quality of any set of similarity metrics,
ii) a measure to evaluate the quality of
a summary using an optimal set of simi-
larity metrics, and iii) a measure to eval-
uate whether the set of baseline sum-
maries is reliable or may produce biased
results.
Compared to previous approaches, our
framework is able to combine different
metrics and evaluate the quality of a set
of metrics without any a-priori weight-
ing of their relative importance. We pro-
vide quantitative evidence about the ef-
fectiveness of the approach to improve
the automatic evaluation of text sum-
marisation systems by combining sev-
eral similarity metrics.
1 Introduction
The quality of an automatic summary can be es-
tablished mainly with two approaches:
Human assessments: The output of a number of
summarisation systems is compared by hu-
man judges, using some set of evaluation
guidelines.
Proximity to a gold standard: The best auto-
matic summary is the one that is closest to
some reference summary made by humans.
Using human assessments has some clear ad-
vantages: the results of the evaluation are inter-
pretable, and we can trace what a system is do-
ing well, and what is doing poorly. But it also
has a couple of serious drawbacks: i) different hu-
man assessors reach different conclusions, and ii)
the outcome of a comparative evaluation exercise
is not directly reusable for new techniques, i.e., a
summarisation strategy developed after the com-
parative exercise cannot be evaluated without ad-
ditional human assessments made from scratch.
Proximity to a gold standard, on the other hand,
is a criterion that can be automated (see Section 6),
with the advantages of i) being objective, and ii)
once gold standard summaries are built for a com-
parative evaluation of systems, the resulting test-
bed can iteratively be used to refine text summari-
sation techniques and re-evaluate them automati-
cally.
This second approach, however, requires solv-
ing a number of non-trivial issues. For instance,
(i) How can we know whether an evaluation met-
ric is good enough for automatic evaluation?, (ii)
different users produce different summaries, all of
them equally good as gold standards, (iii) if we
have several metrics which test different features
of a summary, how can we combine them into an
optimal test?, (iv) how do we know if our test bed
280
Figure 1: Illustration of some of the restrictions on Q,K
is reliable, or the evaluation outcome may change
by adding, for instance, additional gold standards?
In this paper, we introduce a probabilistic
framework, QARLA, that addresses such issues.
Given a set of manual summaries and another set
of baseline summaries per task, together with a set
of similarity metrics, QARLA provides quantita-
tive measures to (i) select and combine the best
(independent) metrics (KING measure), (ii) apply
the best set of metrics to evaluate automatic sum-
maries (QUEEN measure), and (iii) test whether
evaluating with that test-bed is reliable (JACK
measure).
2 Formal constraints on any evaluation
framework based on similarity metrics
We are looking for a framework to evaluate au-
tomatic summarisation systems objectively using
similarity metrics to compare summaries. The in-
put of the framework is:
? A summarisation task (e.g. topic oriented, in-
formative multi-document summarisation on
a given domain/corpus).
? A set T of test cases (e.g. topic/document set
pairs for the example above)
? A set of summaries M produced by humans
(models), and a set of automatic summaries
A (peers), for every test case.
? A set X of similarity metrics to compare
summaries.
An evaluation framework should include, at
least:
? A measure QM,X(a) ? [0, 1] that estimates
the quality of an automatic summary a, us-
ing the similarity metrics in X to compare
the summary with the models in M . With
Q, we can compare the quality of automatic
summaries.
? A measure KM,A(X) ? [0, 1] that estimates
the suitability of a set of similarity metrics X
for our evaluation purposes. With K, we can
choose the best similarity metrics.
Our main assumption is that all manual sum-
maries are equally optimal and, while they are
likely to be different, the best similarity metric is
the one that identifies and uses the features that are
common to all manual summaries, grouping and
separating them from the automatic summaries.
With these assumption in mind, it is useful to
think of some formal restrictions that any evalua-
tion framework Q,K must hold. We will consider
the following ones (see illustrations in Figure 1):
(1) Given two automatic summaries a, a? and a
similarity measure x, if a is more distant to all
manual summaries than a?, then a cannot be better
281
than a?. Formally: ?m ? M.x(a,m) < x(a?,m) ?
QM,x(a) ? QM,x(a?)
(2) A similarity metric x is better when it is able
to group manual summaries more closely, while
keeping them more distant from automatic sum-
maries: (?m,m? ? M.x(m,m?) > x?(m,m?) ? ?m ?
M,a ? Ax(a,m) < x?(a,m)) ? KM,A(x) > KM,A(x?)
(3) If x is a perfect similarity metric, the quality of
a manual summary cannot be zero: KM,A(x) = 1 ?
?m ?M.QM,x(m) > 0
(4) The quality of a similarity metric or a summary
should not be dependent on scale issues. In gen-
eral, if x? = f(x) with f being a growing mono-
tonic function, then KM,A(x) = KM,A(x?) and
QM,x(a) = QM,x?(a) .
(5) The quality of a similarity metric should
not be sensitive to repeated elements in A, i.e.
KM,A?{a}(x) = KM,A?{a,a}(x).
(6) A random metric x should have KM,A(x) = 0.
(7) A non-informative (constant) metric x should
have KM,A(x) = 0.
3 QARLA evaluation framework
3.1 QUEEN: Estimation of the quality of an
automatic summary
We are now looking for a function QM,x(a) that
estimates the quality of an automatic summary a ?
A, given a set of models M and a similarity metric
x.
An obvious first attempt would be to compute
the average similarity of a to all model summaries
in M in a test sample. But such a measure depends
on scale properties: metrics producing larger sim-
ilarity values will produce larger Q values; and,
depending on the scale properties of x, this cannot
be solved just by scaling the final Q value.
A probabilistic measure that solves this problem
and satisfies all the stated formal constraints is:
QUEENx,M (a) ? P (x(a,m) ? x(m?,m??))
which defines the quality of an automatic sum-
mary a as the probability over triples of manual
summaries m,m?,m?? that a is closer to a model
than the other two models to each other. This mea-
sure draws from the way in which some formal re-
strictions on Q are stated (by comparing similarity
values), and is inspired in the QARLA criterion
introduced in (Amigo et al, 2004).
Figure 2: Summaries quality in a similarity metric
space
Figure 2 illustrates some of the features of the
QUEEN estimation:
? Peers which are very far from the set of
models all receive QUEEN = 0. In other
words, QUEEN does not distinguish between
very poor automatic summarisation strate-
gies. While this feature reduces granularity
of the ranking produced by QUEEN, we find
it desirable, because in such situations, the
values returned by a similarity measure are
probably meaningless.
? The value of QUEEN is maximised for the
peers that ?merge? with the models. For
QUEEN values between 0.5 and 1, peers are
effectively merged with the models.
? An ideal metric (that puts all models to-
gether) would give QUEEN(m) = 1 for all
models, and QUEEN(a) = 0 for all peers
that are not put together with the models.
This is a reasonable boundary condition say-
ing that, if we can distinguish between mod-
els and peers perfectly, then all peers are
poor emulations of human summarising be-
haviour.
3.2 Generalisation of QUEEN to metric sets
It is desirable, however, to have the possibility of
evaluating summaries with respect to several met-
rics together. Let us imagine, for instance, that
the best metric turns out to be a ROUGE (Lin and
Hovy, 2003a) variant that only considers unigrams
to compute similarity. Now consider a summary
282
which has almost the same vocabulary as a hu-
man summary, but with a random scrambling of
the words which makes it unreadable. Even if the
unigram measure is the best hint of similarity to
human performance, in this case it would produce
a high similarity value, while any measure based
on 2-grams, 3-grams or on any simple syntactic
property would detect that the summary is useless.
The issue is, therefore, how to find informative
metrics, and then how to combine them into an op-
timal single quality estimation for automatic sum-
maries. The most immediate way of combining
metrics is via some weighted linear combination.
But our example suggests that this is not the op-
timal way: the unigram measure would take the
higher weight, and therefore it would assign a fair
amount of credit to a summary that can be strongly
rejected with other criteria.
Alternatively, we can assume that a summary is
better if it is closer to the model summaries ac-
cording to all metrics. We can formalise this idea
by introducing a universal quantifier on the vari-
able x in the QUEEN formula. In other words,
QUEENX,M (a) can be defined as the probability,
measured over M ?M ?M , that for every metric
in X the automatic summary a is closer to a model
than two models to each other.
QUEENX,M (a) ? P (?x ? X.x(a,m) ? x(m?,m??))
We can think of the generalised QUEEN mea-
sure as a way of using a set of tests (every simi-
larity metric in X) to falsify the hypothesis that a
given summary a is a model. If, for every compar-
ison of similarities between a,m,m?,m??, there is
at least one test that a does not pass, then a is re-
jected as a model.
This generalised measure is not affected by the
scale properties of every individual metric, i.e. it
does not require metric normalisation and it is not
affected by metric weighting. In addition, it still
satisfies the properties enumerated for its single-
metric counterpart.
Of course, the quality ranking provided by
QUEEN is meaningless if the similarity metric x
does not capture the essential features of the mod-
els. Therefore, we need to estimate the quality of
similarity metrics in order to use QUEEN effec-
tively.
3.3 KING: estimation of the quality of a
similarity metric
Now we need a measure KM,A(x) that estimates
the quality of a similarity metric x to evaluate
automatic summaries (peers) by comparison to
human-produced models.
In order to build a suitable K estimation, we
will again start from the hypothesis that the best
metric is the one that best characterises human
summaries as opposed to automatic summaries.
Such a metric should identify human summaries
as closer to each other, and more distant to peers
(second constraint in Section 2). By analogy with
QUEEN, we can try (for a single metric):
KM,A(x) ? P (x(a,m) < x(m
?,m??)) =
1? (QUEENx,M (a))
which is the probability that two models are
closer to each other than a third model to a peer,
and has smaller values when the average QUEEN
value of peers decreases. The generalisation of K
to metric sets would be simply:
KM,A(X) ? 1? (QUEENX,M (a)))
This measure, however, does not satisfy formal
conditions 3 and 5. Condition 3 is violated be-
cause, given a limited set of models, the K mea-
sure grows with a large number of metrics in X ,
eventually reaching K = 1 (perfect metric set).
But in this situation, QUEEN(m) becomes 0 for
all models, because there will always exist a met-
ric that breaks the universal quantifier condition
over x.
We have to look, then, for an alternative for-
mulation for K. The best K should minimise
QUEEN(a), but having the quality of the models
as a reference. A direct formulation can be:
KM,A(X) = P (QUEEN(m) > QUEEN(a))
According to this formula, the quality of a met-
ric set X is the probability that the quality of a
283
model is higher than the quality of a peer ac-
cording to this metric set. This formula satisfies
all formal conditions except 5 (KM,A?{a}(x) =
KM,A?{a,a}(x)), because it is sensitive to repeated
peers. If we add a large set of identical (or very
similar peers), K will be biased towards this set.
We can define a suitable K that satisfies condi-
tion 5 if we apply a universal quantifier on a. This
is what we call the KING measure:
KINGM,A(X) ?
P (?a ? A.QUEENM,X(m) > QUEENM,X(a))
KING is the probability that a model is better
than any peer in a test sample. In terms of a qual-
ity ranking, it is the probability that a model gets a
better ranking than all peers in a test sample. Note
that KING satisfies all restrictions because it uses
QUEEN as a quality estimation for summaries; if
QUEEN is substituted for a different quality mea-
sure, some of the properties might not hold any
longer.
Figure 3: Metrics quality representation
Figure 3 illustrates the behaviour of the KING
measure in boundary conditions. The left-
most figure represents a similarity metric which
mixes models and peers randomly. Therefore,
P (QUEEN(m) > QUEEN(a)) ? 0.5. As there
are seven automatic summaries, KING = P (?a ?
A,QUEEN(m) > QUEEN(a)) ? 0.57 ? 0
The rightmost figure represents a metric which
is able to group models and separate them from
peers. In this case, QUEEN(a) = 0 for all peers,
and then KING(x) = 1.
3.4 JACK:Reliability of the peers set
Once we detect a difference in quality between
two summarisation systems, the question is now
whether this result is reliable. Would we get the
same results using a different test set (different ex-
amples, different human summarisers (models) or
different baseline systems)?
The first step is obviously to apply statistical
significance tests to the results. But even if they
give a positive result, it might be insufficient. The
problem is that the estimation of the probabilities
in KING,QUEEN assumes that the sample sets
M,A are not biased. If M,A are biased, the re-
sults can be statistically significant and yet un-
reliable. The set of examples and the behaviour
of human summarisers (models) should be some-
how controlled either for homogeneity (if the in-
tended profile of examples and/or users is narrow)
or representativity (if it is wide). But how to know
whether the set of automatic summaries is repre-
sentative and therefore is not penalising certain au-
tomatic summarisation strategies?
Our goal is, therefore, to have some estimation
JACK(X,M,A) of the reliability of the test set to
compute reliable QUEEN,KING measures. We
can think of three reasonable criteria for this es-
timation:
1. All other things being equal, if the elements
of A are more heterogeneous, we are enhanc-
ing the representativeness of A (we have a
more diverse set of (independent) automatic
summarization strategies represented), and
therefore the reliability of the results should
be higher. Reversely, if all automatic sum-
marisers employ similar strategies, we may
end up with a biased set of peers.
2. All other things being equal, if the elements
of A are closer to the model summaries in M ,
the reliability of the results should be higher.
3. Adding items to A should not reduce its reli-
ability.
A possible formulation for JACK which satis-
fies that criteria is:
JACK(X,M,A) ? P (?a, a? ? A.QUEEN(a) >
0 ? QUEEN(a?) > 0 ? ?x ? X.x(a, a?) ? x(a,m))
i.e. the probability over all model summaries m
of finding a couple of automatic summaries a, a?
284
which are closer to each other than to m according
to all metrics.
This measure satisfies all three constraints: it
can be enlarged by increasing the similarity of the
peers to the models (the x(m,a) factor in the in-
equality) or decreasing the similarity between au-
tomatic summaries (the x(a, a?) factor in the in-
equality). Finally, adding elements to A can only
increase the chances of finding a pair of automatic
summaries satisfying the condition in JACK.
Figure 4: JACK values
Figure 4 illustrates how JACK works: in the
leftmost part of the figure, peers are grouped to-
gether and far from the models, giving a low JACK
value. In the rightmost part of the figure, peers are
distributed around the set of models, closely sur-
rounding them, receiving a high JACK value.
4 A Case of Study
In order to test the behaviour of our evaluation
framework, we have applied it to the ISCORPUS
described in (Amigo et al, 2004). The ISCOR-
PUS was built to study an Information Synthesis
task, where a (large) set of relevant documents has
to be studied to give a brief, well-organised answer
to a complex need for information. This corpus
comprises:
? Eight topics extracted from the CLEF Span-
ish Information Retrieval test set, slightly re-
worded to move from a document retrieval
task (find documents about hunger strikes
in...) into an Information Synthesis task
(make a report about major causes of hunger
strikes in...).
? One hundred relevant documents per topic
taken from the CLEF EFE 1994 Spanish
newswire collection.
? M : Manual extractive summaries for every
topic made by 9 different users, with a 50-
sentence upper limit (half the number of rel-
evant documents).
? A: 30 automatic reports for every topic made
with baseline strategies. The 10 reports with
highest sentence overlap with the manual
summaries were selected as a way to increase
the quality of the baseline set.
We have considered the following similarity
metrics:
ROUGESim: ROUGE is a standard measure
to evaluate summarisation systems based on
n-gram recall. We have used ROUGE-1
(only unigrams with lemmatization and stop
word removal), which gives good results with
standard summaries (Lin and Hovy, 2003a).
ROUGE can be turned into a similarity met-
ric ROUGESim simply by considering only
one model when computing its value.
SentencePrecision: Given a reference and a con-
trastive summary, the number of fragments of
the contrastive summary which are also in the
reference summary, in relation to the size of
the reference summary.
SentenceRecall: Given a reference and a con-
trastive summary, the number of fragments of
the reference summary which are also in the
contrastive summary, in relation to the size of
the contrastive summary.
DocSim: The number of documents used to select
fragments in both summaries, in relation to
the size of the contrastive summary.
VectModelSim: Derived from the Euclidean dis-
tance between vectors of relative word fre-
quencies representing both summaries.
NICOS (key concept overlap): Same as Vect-
ModelSim, but using key-concepts (manually
identified by the human summarisers after
producing the summary) instead of all non-
empty words.
285
TruncatedVectModeln: Same as VectModelSim,
but using only the n more frequent terms
in the reference summary. We have used
10 variants of this measure with n =
1, 8, 64, 512.
4.1 Quality of Similarity Metric Sets
Figure 5 shows the quality (KING values averaged
over the eight ISCORPUS topics) of every individ-
ual metric. The rightmost part of the figure also
shows the quality of two metric sets:
? The first one ({ROUGESim, VectModelSim,
TruncVectModel.1}) is the metric set that
maximises KING, using only similarity met-
rics that do not require manual annotation
(i.e. excluding NICOS) or can only be ap-
plied to extractive summaries (i.e. DocSim,
SentenceRecall and SentencePrecision).
? The second one ({ TruncVectModel.1, ROU-
GESim, DocSim, VectModelSim }) is the best
combination considering all metrics.
The best result of individual metrics is obtained
by ROUGESim (0.39). All other individual met-
rics give scores below 0.31. Both metric sets, on
the other, are better than ROUGESim alone, con-
firming that metric combination is feasible to im-
prove system evaluation. The quality of the best
metric set (0.47) is 21% better than ROUGESim.
4.2 Reliability of the test set
The 30 automatic summaries (baselines) per topic
were built with four different classes of strategies:
i) picking up the first sentence from assorted sub-
sets of documents, ii) picking up first and second
sentences from assorted documents, iii) picking
up first, second or third sentences from assorted
documents, and iv) picking up whole documents
with different algorithms to determine which are
the most representative documents.
Figure 6 shows the reliability (JACK) of every
subset, and the reliability of the whole set of au-
tomatic summaries, computed with the best met-
ric set. Note that the individual subsets are all
below 0.2, while the reliability of the full set of
peers goes up to 0.57. That means that the con-
dition in JACK is satisfied for more than half of
the models. This value would probably be higher
if state-of-the-art summarisation techniques were
represented in the set of peers.
5 Testing the predictive power of the
framework
The QARLA probabilistic framework is designed
to evaluate automatic summarisation systems and,
at the same time, similarity metrics conceived as
well to evaluate summarisation systems. There-
fore, testing the validity of the QARLA proposal
implies some kind of meta-meta-evaluation, some-
thing which seems difficult to design or even to
define.
It is relatively simple, however, to perform some
simple cross-checkings on the ISCORPUS data to
verify that the qualitative information described
above is reasonable. This is the test we have im-
plemented:
If we remove a model m from M , and pretend it
is the output of an automatic summariser, we can
evaluate the peers set A and the new peer m using
M ? = M\{m} as the new model set. If the evalu-
ation metric is good, the quality of the new peer m
should be superior to all other peers inA. What we
have to check, then, is whether the average quality
of a human summariser on all test cases (8 topics
in ISCORPUS) is superior to the average quality
of any automatic summariser. We have 9 human
subjects in the ISCORPUS test bed; therefore, we
can repeat this test nine times.
With this criterion, we can compare our quality
measure Q with state-of-the-art evaluation mea-
sures such as ROUGE variants. Table 1 shows
the results of applying this test on ROUGE-
1, ROUGE-2, ROUGE-3, ROUGE-4 (as state-
of-the-art references) and QUEEN(ROUGESim),
QUEEN(Best Metric Combination) as representa-
tives of the QARLA framework. Even if the test is
very limited by the number of topics, it confirms
the potential of the framework, with the highest
KING metric combination doubling the perfor-
mance of the best ROUGE measure (6/9 versus 3/9
correct detections).
286
Figure 5: Quality of similarity metrics
Figure 6: Reliability of ISCORPUS peer sets
Evaluation criterion human summarisers ranked first
ROUGE-1 3/9
ROUGE-2 2/9
ROUGE-3 1/9
ROUGE-4 1/9
QUEEN(ROUGESim) 4/9
QUEEN(Best Metric Combination) 6/9
Table 1: Results of the test of identifying the manual summariser
287
6 Related work and discussion
6.1 Application of similarity metrics to
evaluate summaries
Both in Text Summarisation and Machine Trans-
lation, the automatic evaluation of systems con-
sists of computing some similarity metric between
the system output and a human model summary.
Systems are then ranked in order of decreasing
similarity to the gold standard. When there are
more than one reference items, similarity is calcu-
lated over a pseudo-summary extracted from every
model. BLEU (Papineni et al, 2001) and ROUGE
(Lin and Hovy, 2003a) are the standard similar-
ity metrics used in Machine Translation and Text
Summarisation. Generating a pseudo-summary
from every model, the results of a evaluation met-
ric might depend on the scale properties of the
metric regarding different models; our QUEEN
measure, however, does not depend on scales.
Another problem of the direct application of a
single evaluation metric to rank systems is how to
combine different metrics. The only way to do
this is by designing an algebraic combination of
the individual metrics into a new combined met-
ric, i.e. by deciding the weight of each individual
metric beforehand. In our framework, however, it
is not necessary to prescribe how similarity met-
rics should be combined, not even to know which
ones are individually better indicators.
6.2 Meta-evaluation of similarity metrics
The question of how to know which similar-
ity metric is best to evaluate automatic sum-
maries/translations has been addressed by
? comparing the quality of automatic items
with the quality of manual references (Culy
and Riehemann, 2003; Lin and Hovy,
2003b). If the metric does not identify that
the manual references are better, then it is not
good enough for evaluation purposes.
? measuring the correlation between the values
given by different metrics (Coughlin, 2003).
? measuring the correlation between the rank-
ings generated by each metric and rank-
ings generated by human assessors. (Joseph
P. Turian and Melamed, 2003; Lin and Hovy,
2003a).
The methodology which is closest to our frame-
work is ORANGE (Lin, 2004), which evaluates a
similarity metric using the average ranks obtained
by reference items within a baseline set. As in
our framework, ORANGE performs an automatic
meta-evaluation, there is no need for human as-
sessments, and it does not depend on the scale
properties of the metric being evaluated (because
changes of scale preserve rankings). The OR-
ANGE approach is, indeed, closely related to the
original QARLA measure introduced in (Amigo et
al., 2004).
Our KING,QUEEN, JACK framework, how-
ever, has a number of advantages over ORANGE:
? It is able to combine different metrics, and
evaluate the quality of metric sets, without
any a-priori weighting of their relative impor-
tance.
? It is not sensitive to repeated (or very similar)
baseline elements.
? It provides a mechanism, JACK, to check
whether a set X,M,A of metrics, manual
and baseline items is reliable enough to pro-
duce a stable evaluation of automatic sum-
marisation systems.
Probably the most significant improvement over
ORANGE is the ability of KING,QUEEN, JACK
to combine automatically the information of dif-
ferent metrics. We believe that a comprehensive
automatic evaluation of a summary must neces-
sarily capture different aspects of the problem with
different metrics, and that the results of every indi-
vidual metric should not be combined in any pre-
scribed algebraic way (such as a linear weighted
combination). Our framework satisfies this con-
dition. An advantage of ORANGE, however, is
that it does not require a large number of gold stan-
dards to reach stability, as in the case of QARLA.
Finally, it is interesting to compare the rankings
produced by QARLA with the output of human
assessments, even if the philosophy of QARLA
is not considering human assessments as the gold
standard for evaluation. Our initial tests on DUC
288
Figure 7: KING vs Pearson correlation with manual rankings in DUC for 1024 metrics combinations
test beds are very promising, reaching Pearson
correlations of 0.9 and 0.95 between human as-
sessments and QUEEN values for DUC 2004 tasks
2 and 5 (Over and Yen, 2004), using metric sets
with highest KING values. The figure 7 shows
how Pearson correlation grows up with higher
KING values for 1024 metric combinations.
Acknowledgments
We are indebted to Ed Hovy, Donna Harman, Paul
Over, Hoa Dang and Chin-Yew Lin for their in-
spiring and generous feedback at different stages
in the development of QARLA. We are also in-
debted to NIST for hosting Enrique Amigo? as a
visitor and for providing the DUC test beds. This
work has been partially supported by the Spanish
government, project R2D2 (TIC-2003-7180).
References
E. Amigo, V. Peinado, J. Gonzalo, A. Pen?as, and
F. Verdejo. 2004. An empirical study of informa-
tion synthesis task. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Barcelona, July.
Deborah Coughlin. 2003. Correlating Automated and
Human Assessments of Machine Translation Qual-
ity. In In Proceedings of MT Summit IX, New Or-
leans,LA.
Christopher Culy and Susanne Riehemann. 2003. The
Limits of N-Gram Translation Evaluation Metrics.
In Proceedings of MT Summit IX, New Orleans,LA.
Luke Shen Joseph P. Turian and I. Dan Melamed.
2003. Evaluation of Machine Translation and its
Evaluation. In In Proceedings of MT Summit IX,
New Orleans,LA.
C. Lin and E. H. Hovy. 2003a. Automatic Evaluation
of Summaries Using N-gram Co-ocurrence Statis-
tics. In Proceeding of 2003 Language Technology
Conference (HLT-NAACL 2003).
Chin-Yew Lin and Eduard Hovy. 2003b. The Poten-
tial and Limitations of Automatic Sentence Extrac-
tion for Summarization. In Dragomir Radev and Si-
mone Teufel, editors, HLT-NAACL 2003 Workshop:
Text Summarization (DUC03), Edmonton, Alberta,
Canada, May 31 - June 1. Association for Computa-
tional Linguistics.
C. Lin. 2004. Orange: a Method for Evaluating Au-
tomatic Metrics for Machine Translation. In Pro-
ceedings of the 36th Annual Conference on Compu-
tational Linguisticsion for Computational Linguis-
tics (Coling?04), Geneva, August.
P. Over and J. Yen. 2004. An introduction to duc 2004
intrinsic evaluation of generic new text summariza-
tion systems. In Proceedings of DUC 2004 Docu-
ment Understanding Workshop, Boston.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, jul.
289
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
MT Evaluation: Human-like vs. Human Acceptable
Enrique Amigo?
 
, Jesu?s Gime?nez  , Julio Gonzalo   , and Llu??s Ma`rquez 
 
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
Juan del Rosal, 16, E-28040, Madrid

enrique,julio  @lsi.uned.es
 TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado, 1?3, E-08034, Barcelona
 jgimenez,lluism  @lsi.upc.edu
Abstract
We present a comparative study on Ma-
chine Translation Evaluation according to
two different criteria: Human Likeness
and Human Acceptability. We provide
empirical evidence that there is a relation-
ship between these two kinds of evalu-
ation: Human Likeness implies Human
Acceptability but the reverse is not true.
From the point of view of automatic eval-
uation this implies that metrics based on
Human Likeness are more reliable for sys-
tem tuning.
Our results also show that current evalua-
tion metrics are not always able to distin-
guish between automatic and human trans-
lations. In order to improve the descrip-
tive power of current metrics we propose
the use of additional syntax-based met-
rics, and metric combinations inside the
QARLA Framework.
1 Introduction
Current approaches to Automatic Machine Trans-
lation (MT) Evaluation are mostly based on met-
rics which determine the quality of a given transla-
tion according to its similarity to a given set of ref-
erence translations. The commonly accepted crite-
rion that defines the quality of an evaluation metric
is its level of correlation with human evaluators.
High levels of correlation (Pearson over 0.9) have
been attained at the system level (Eck and Hori,
2005). But this is an average effect: the degree of
correlation achieved at the sentence level, crucial
for an accurate error analysis, is much lower.
We argue that there is two main reasons that ex-
plain this fact:
Firstly, current MT evaluation metrics are based
on shallow features. Most metrics work only at the
lexical level. However, natural languages are rich
and ambiguous, allowing for many possible differ-
ent ways of expressing the same idea. In order to
capture this flexibility, these metrics would require
a combinatorial number of reference translations,
when indeed in most cases only a single reference
is available. Therefore, metrics with higher de-
scriptive power are required.
Secondly, there exists, indeed, two different
evaluation criteria: (i) Human Acceptability, i.e.,
to what extent an automatic translation could be
considered acceptable by humans; and (ii) Human
Likeness, i.e., to what extent an automatic transla-
tion could have been generated by a human trans-
lator. Most approaches to automatic MT evalu-
ation implicitly assume that both criteria should
lead to the same results; but this assumption has
not been proved empirically or even discussed.
In this work, we analyze this issue through em-
pirical evidence. First, in Section 2, we inves-
tigate to what extent current evaluation metrics
are able to distinguish between human and auto-
matic translations (Human Likeness). As individ-
ual metrics do not capture such distinction well, in
Section 3 we study how to improve the descrip-
tive power of current metrics by means of met-
ric combinations inside the QARLA Framework
(Amigo? et al, 2005), including a new family of
metrics based on syntactic criteria. Second, we
claim that the two evaluation criteria (Human Ac-
ceptability and Human Likeness) are indeed of a
different nature, and may lead to different results
(Section 4). However, translations exhibiting a
high level of Human Likeness obtain good results
in human judges. Therefore, automatic evaluation
metrics based on similarity to references should be
17
optimized over their capacity to represent Human
Likeness. See conclusions in Section 5.
2 Descriptive Power of Standard Metrics
In this section we perform a simple experiment in
order to measure the descriptive power of current
state-of-the-art metrics, i.e., their ability to capture
the features which characterize human translations
with respect to automatic ones.
2.1 Experimental Setting
We use the data from the Openlab 2006 Initiative1
promoted by the TC-STAR Consortium2. This
test suite is entirely based on European Parlia-
ment Proceedings3, covering April 1996 to May
2005. We focus on the Spanish-to-English transla-
tion task. For the purpose of evaluation we use the
development set which consists of 1008 sentences.
However, due to lack of available MT outputs for
the whole set we used only a subset of 504 sen-
tences corresponding to the first half of the devel-
opment set. Three human references per sentence
are available.
We employ ten system outputs; nine are based
on Statistical Machine Translation (SMT) sys-
tems (Gime?nez and Ma`rquez, 2005; Crego et al,
2005), and one is obtained from the free Sys-
tran4 on-line rule-based MT engine. Evalua-
tion results have been computed by means of the
IQMT5 Framework for Automatic MT Evaluation
(Gime?nez and Amigo?, 2006).
We have selected a representative set of 22 met-
ric variants corresponding to six different fami-
lies: BLEU (Papineni et al, 2001), NIST (Dodding-
ton, 2002), GTM (Melamed et al, 2003), mPER
(Leusch et al, 2003), mWER (Nie?en et al, 2000)
and ROUGE (Lin and Och, 2004a).
2.2 Measuring Descriptive Power of
Evaluation Metrics
Our main assumption is that if an evaluation met-
ric is able to characterize human translations, then,
human references should be closer to each other
than automatic translations to other human refer-
ences. Based on this assumption we introduce two
measures (ORANGE and KING) which analyze
1http://tc-star.itc.it/openlab2006/
2http://www.tc-star.org/
3http://www.europarl.eu.int/
4http://www.systransoft.com.
5The IQMT Framework may be freely downloaded at
http://www.lsi.upc.edu/?nlp/IQMT.
the descriptive power of evaluation metrics from
diferent points of view.
ORANGE Measure
ORANGE compares automatic and manual
translations one-on-one. Let  and  be the sets
of automatic and reference translations, respec-
tively, and 	
 an evaluation metric which out-
puts the quality of an automatic translation 

by comparison to  . ORANGE measures the de-
scriptive power as the probability that a human ref-
erence  is more similar than an automatic transla-
tion 
 to the rest of human references:

Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 306?314,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
The Contribution of Linguistic Features to Automatic Machine
Translation Evaluation
Enrique Amigo?1 Jesu?s Gime?nez2 Julio Gonzalo 1 Felisa Verdejo1
1UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
2UPC, Barcelona
jgimenez@lsi.upc.edu
Abstract
A number of approaches to Automatic
MT Evaluation based on deep linguistic
knowledge have been suggested. How-
ever, n-gram based metrics are still to-
day the dominant approach. The main
reason is that the advantages of employ-
ing deeper linguistic information have not
been clarified yet. In this work, we pro-
pose a novel approach for meta-evaluation
of MT evaluation metrics, since correla-
tion cofficient against human judges do
not reveal details about the advantages and
disadvantages of particular metrics. We
then use this approach to investigate the
benefits of introducing linguistic features
into evaluation metrics. Overall, our ex-
periments show that (i) both lexical and
linguistic metrics present complementary
advantages and (ii) combining both kinds
of metrics yields the most robust meta-
evaluation performance.
1 Introduction
Automatic evaluation methods based on similarity
to human references have substantially accelerated
the development cycle of many NLP tasks, such
as Machine Translation, Automatic Summariza-
tion, Sentence Compression and Language Gen-
eration. These automatic evaluation metrics allow
developers to optimize their systems without the
need for expensive human assessments for each
of their possible system configurations. However,
estimating the system output quality according to
its similarity to human references is not a trivial
task. The main problem is that many NLP tasks
are open/subjective; therefore, different humans
may generate different outputs, all of them equally
valid. Thus, language variability is an issue.
In order to tackle language variability in the
context of Machine Translation, a considerable ef-
fort has also been made to include deeper linguis-
tic information in automatic evaluation metrics,
both syntactic and semantic (see Section 2 for de-
tails). However, the most commonly used metrics
are still based on n-gram matching. The reason is
that the advantages of employing higher linguistic
processing levels have not been clarified yet.
The main goal of our work is to analyze to what
extent deep linguistic features can contribute to the
automatic evaluation of translation quality. For
that purpose, we compare ? using four different
test beds ? the performance of 16 n-gram based
metrics, 48 linguistic metrics and one combined
metric from the state of the art.
Analyzing the reliability of evaluation met-
rics requires meta-evaluation criteria. In this re-
spect, we identify important drawbacks of the
standard meta-evaluation methods based on cor-
relation with human judgements. In order to
overcome these drawbacks, we then introduce six
novel meta-evaluation criteria which represent dif-
ferent metric reliability dimensions. Our analysis
indicates that: (i) both lexical and linguistic met-
rics have complementary advantages and different
drawbacks; (ii) combining both kinds of metrics
is a more effective and robust evaluation method
across all meta-evaluation criteria.
In addition, we also perform a qualitative analy-
sis of one hundred sentences that were incorrectly
evaluated by state-of-the-art metrics. The analysis
confirms that deep linguistic techniques are neces-
sary to avoid the most common types of error.
Section 2 examines the state of the art Section 3
describes the test beds and metrics considered in
our experiments. In Section 4 the correlation be-
tween human assessors and metrics is computed,
with a discussion of its drawbacks. In Section 5
different quality aspects of metrics are analysed.
Conclusions are drawn in the last section.
306
2 Previous Work on Machine
Translation Meta-Evaluation
Insofar as automatic evaluation metrics for ma-
chine translation have been proposed, different
meta-evaluation frameworks have been gradually
introduced. For instance, Papineni et al (2001)
introduced the BLEU metric and evaluated its re-
liability in terms of Pearson correlation with hu-
man assessments for adequacy and fluency judge-
ments. With the aim of overcoming some of the
deficiencies of BLEU, Doddington (2002) intro-
duced the NIST metric. Metric reliability was
also estimated in terms of correlation with human
assessments, but over different document sources
and for a varying number of references and seg-
ment sizes. Melamed et al (2003) argued, at the
time of introducing the GTM metric, that Pearson
correlation coefficients can be affected by scale
properties, and suggested, in order to avoid this
effect, to use the non-parametric Spearman corre-
lation coefficients instead.
Lin and Och (2004) experimented, unlike pre-
vious works, with a wide set of metrics, including
NIST, WER (Nie?en et al, 2000), PER (Tillmann
et al, 1997), and variants of ROUGE, BLEU and
GTM. They computed both Pearson and Spearman
correlation, obtaining similar results in both cases.
In a different work, Banerjee and Lavie (2005) ar-
gued that the measured reliability of metrics can
be due to averaging effects but might not be robust
across translations. In order to address this issue,
they computed the translation-by-translation cor-
relation with human judgements (i.e., correlation
at the segment level).
All that metrics were based on n-gram over-
lap. But there is also extensive research fo-
cused on including linguistic knowledge in met-
rics (Owczarzak et al, 2006; Reeder et al, 2001;
Liu and Gildea, 2005; Amigo? et al, 2006; Mehay
and Brew, 2007; Gime?nez and Ma`rquez, 2007;
Owczarzak et al, 2007; Popovic and Ney, 2007;
Gime?nez and Ma`rquez, 2008b) among others. In
all these cases, metrics were also evaluated by
means of correlation with human judgements.
In a different research line, several authors
have suggested approaching automatic evalua-
tion through the combination of individual metric
scores. Among the most relevant let us cite re-
search by Kulesza and Shieber (2004), Albrecht
and Hwa (2007). But finding optimal metric
combinations requires a meta-evaluation criterion.
Most approaches again rely on correlation with
human judgements. However, some of them mea-
sured the reliability of metric combinations in
terms of their ability to discriminate between hu-
man translations and automatic ones (human like-
ness) (Amigo? et al, 2005). .
In this work, we present a novel approach to
meta-evaluation which is distinguished by the use
of additional easily interpretable meta-evaluation
criteria oriented to measure different aspects of
metric reliability. We then apply this approach to
find out about the advantages and challenges of in-
cluding linguistic features in meta-evaluation cri-
teria.
3 Metrics and Test Beds
3.1 Metric Set
For our study, we have compiled a rich set of met-
ric variants at three linguistic levels: lexical, syn-
tactic, and semantic. In all cases, translation qual-
ity is measured by comparing automatic transla-
tions against a set of human references.
At the lexical level, we have included several
standard metrics, based on different similarity as-
sumptions: edit distance (WER, PER and TER),
lexical precision (BLEU and NIST), lexical recall
(ROUGE), and F-measure (GTM and METEOR). At
the syntactic level, we have used several families
of metrics based on dependency parsing (DP) and
constituency trees (CP). At the semantic level, we
have included three different families which op-
erate using named entities (NE), semantic roles
(SR), and discourse representations (DR). A de-
tailed description of these metrics can be found in
(Gime?nez and Ma`rquez, 2007).
Finally, we have also considered ULC, which
is a very simple approach to metric combina-
tion based on the unnormalized arithmetic mean
of metric scores, as described by Gime?nez and
Ma`rquez (2008a). ULC considers a subset of met-
rics which operate at several linguistic levels. This
approach has proven very effective in recent eval-
uation campaigns. Metric computation has been
carried out using the IQMT Framework for Auto-
matic MT Evaluation (Gime?nez, 2007)1. The sim-
plicity of this approach (with no training of the
metric weighting scheme) ensures that the poten-
tial advantages detected in our experiments are not
due to overfitting effects.
1http://www.lsi.upc.edu/?nlp/IQMT
307
2004 2005
AE CE AE CE
#references 5 5 5 4
#systemsassessed 5 10 5+1 5
#casesassessed 347 447 266 272
Table 1: NIST 2004/2005 MT Evaluation Cam-
paigns. Test bed description
3.2 Test Beds
We use the test beds from the 2004 and 2005
NIST MT Evaluation Campaigns (Le and Przy-
bocki, 2005)2. Both campaigns include two dif-
ferent translations exercises: Arabic-to-English
(?AE?) and Chinese-to-English (?CE?). Human as-
sessments of adequacy and fluency, on a 1-5 scale,
are available for a subset of sentences, each eval-
uated by two different human judges. A brief nu-
merical description of these test beds is available
in Table 1. The corpus AE05 includes, apart from
five automatic systems, one human-aided system
that is only used in our last experiment.
4 Correlation with Human Judgements
4.1 Correlation at the Segment vs. System
Levels
Let us first analyze the correlation with human
judgements for linguistic vs. n-gram based met-
rics. Figure 1 shows the correlation obtained by
each automatic evaluation metric at system level
(horizontal axis) versus segment level (vertical
axis) in our test beds. Linguistic metrics are rep-
resented by grey plots, and black plots represent
metrics based on n-gram overlap.
The most remarkable aspect is that there exists
a certain trade-off between correlation at segment
versus system level. In fact, this graph produces
a negative Pearson correlation coefficient between
system and segment levels of 0.44. In other words,
depending on how the correlation is computed,
the relative predictive power of metrics can swap.
Therefore, we need additional meta-evaluation cri-
teria in order to clarify the behavior of linguistic
metrics as compared to n-gram based metrics.
However, there are some exceptions. Some
metrics achieve high correlation at both levels.
The first one is ULC (the circle in the plot), which
combines both kind of metrics in a heuristic way
(see Section 3.1). The metric nearest to ULC is
2http://www.nist.gov/speech/tests/mt
Figure 1: Averaged Pearson correlation at system
vs. segment level over all test beds.
DP-Or-?, which computes lexical overlapping but
on dependency relationships. These results are a
first evidence of the advantages of combining met-
rics at several linguistic processing levels.
4.2 Drawbacks of Correlation-based
Meta-evaluation
Although correlation with human judgements is
considered the standard meta-evaluation criterion,
it presents serious drawbacks. With respect to
correlation at system level, the main problem is
that the relative performance of different metrics
changes almost randomly between testbeds. One
of the reasons is that the number of assessed sys-
tems per testbed is usually low, and then correla-
tion has a small number of samples to be estimated
with. Usually, the correlation at system level is
computed over no more than a few systems.
For instance, Table 2 shows the best 10 met-
rics in CE05 according to their correlation with
human judges at the system level, and then the
ranking they obtain in the AE05 testbed. There
are substantial swaps between both rankings. In-
deed, the Pearson correlation of both ranks is only
0.26. This result supports the intuition in (Baner-
jee and Lavie, 2005) that correlation at segment
level is necessary to ensure the reliability of met-
rics in different situations.
However, the correlation values of metrics at
segment level have also drawbacks related to their
interpretability. Most metrics achieve a Pearson
coefficient lower than 0.5. Figure 2 shows two
possible relationships between human and metric
308
Table 2: Metrics rankings according to correlation
with human judgements using CE05 vs. AE05
Figure 2: Human judgements and scores of two
hypothetical metrics with Pearson correlation 0.5
produced scores. Both hypothetical metrics A and
B would achieve a 0.5 correlation. In the case
of Metric A, a high score implies a high human
assessed quality, but not the reverse. This is the
tendency hypothesized by Culy and Riehemann
(2003). In the case of Metric B, the high scored
translations can achieve both low or high quality
according to human judges but low scores ensure
low quality. Therefore, the same Pearson coeffi-
cient may hide very different behaviours. In this
work, we tackle these drawbacks by defining more
specific meta-evaluation criteria.
5 Alternatives to Correlation-based
Meta-evaluation
We have seen that correlation with human judge-
ments has serious limitations for metric evalua-
tion. Therefore, we have focused on other aspects
of metric reliability that have revealed differences
between n-gram and linguistic based metrics:
1. Is the metric able to accurately reveal im-
provements between two systems?
2. Can we trust the metric when it says that a
translation is very good or very bad?
Figure 3: SIP versus SIR
3. Are metrics able to identify good translations
which are dissimilar from the models?
We now discuss each of these aspects sepa-
rately.
5.1 Ability of metrics to Reveal System
Improvements
We now investigate to what extent a significant
system improvement according to the metric im-
plies a significant improvement according to hu-
man assessors, and viceversa. In other words: are
the metrics able to detect any quality improve-
ment? Is a metric score improvement a strong ev-
idence of quality increase? Knowing that a metric
has a 0.8 Pearson correlation at the system level or
0.5 at the segment level does not provide a direct
answer to this question.
In order to tackle this issue, we compare met-
rics versus human assessments in terms of pre-
cision and recall over statistically significant im-
provements within all system pairs in the test
beds. First, Table 3 shows the amount of signif-
icant improvements over human judgements ac-
cording to the Wilcoxon statistical significant test
(? ? 0.025). For instance, the testbed CE2004
consists of 10 systems, i.e. 45 system pairs; from
these, in 40 cases (rightmost column) one of the
systems significantly improves the other.
Now we would like to know, for every metric, if
the pairs which are significantly different accord-
ing to human judges are also the pairs which are
significantly different according to the metric.
Based on these data, we define two meta-
metrics: Significant Improvement Precision (SIP)
and Significant Improvement Recall (SIR). SIP
309
Systems System pairs Sig. imp.
CE2004 10 45 40
AE2004 5 10 8
CE2005 5 10 4
AE2005 5 10 6
Total 25 75 58
Table 3: System pairs with a significant difference
according to human judgements (Wilcoxon test)
(precision) represents the reliability of improve-
ments detected by metrics. SIR (recall) represents
to what extent the metric is able to cover the sig-
nificant improvements detected by humans. Let
Ih be the set of significant improvements detected
by human assessors and Im the set detected by the
metric m. Then:
SIP =
|Ih ? Im|
|Im|
SIR =
|Ih ? Im|
|Ih|
Figure 3 shows the SIR and SIP values obtained
for each metric. Linguistic metrics achieve higher
precision values but at the cost of an important re-
call decrease. Given that linguistic metrics require
matching translation with references at additional
linguistic levels, the significant improvements de-
tected are more reliable (higher precision or SIP),
but at the cost of recall over real significant im-
provements (lower SIR).
This result supports the behaviour predicted in
(Gime?nez and Ma`rquez, 2009). Although linguis-
tic metrics were motivated by the idea of model-
ing linguistic variability, the practical effect is that
current linguistic metrics introduce additional re-
strictions (such as dependency tree overlap, for in-
stance) for accepting automatic translations. Then
they reward precision at the cost of recall in the
evaluation process, and this explains the high cor-
relation with human judgements at system level
with respect to segment level.
All n-gram based metrics achieve SIP and SIR
values between 0.8 and 0.9. This result suggests
that n-gram based metrics are reasonably reliable
for this purpose. Note that the combined met-
ric, ULC (the circle in the figure), achieves re-
sults comparable to n-gram based metrics with
this test3. That is, combining linguistic and n-
gram based metrics preserves the good behavior
of n-gram based metrics in this test.
3Notice that we just have 75 significant improvement
samples, so small differences in SIP or SIR have no relevance
5.2 Reliability of High and Low Metric
Scores
The issue tackled in this section is to what extent
a very low or high score according to the metric
is reliable for detecting extreme cases (very good
or very bad translations). In particular, note that
detecting wrong translations is crucial in order to
analyze the system drawbacks.
In order to define an accuracy measure for the
reliability of very low/high metric scores, it is nec-
essary to define quality thresholds for both the
human assessments and metric scales. Defining
thresholds for manual scores is immediate (e.g.,
lower than 4/10). However, each automatic evalu-
ation metric has its own scale properties. In order
to solve scaling problems we will focus on equiva-
lent rank positions: we associate the ith translation
according to the metric ranking with the quality
value manually assigned to the ith translation in
the manual ranking.
Being Qh(t) and Qm(t) the human and met-
ric assessed quality for the translation t, and being
rankh(t) and rankm(t) the rank of the translation
t according to humans and the metric, the normal-
ized metric assessed quality is:
QNm(t) = Qh(t
?)| (rankh(t
?) = rankm(t))
In order to analyze the reliability of metrics
when identifying wrong or high quality transla-
tions, we look for contradictory results between
the metric and the assessments. In other words,
we look for metric errors in which the quality es-
timated by the metric is low (QNm(t) ? 3) but the
quality assigned by assessors is high (Qh(t) ? 5)
or viceversa (QNm(t) ? 7 and Qh(t) ? 4).
The vertical axis in Figure 4 represents the ra-
tio of errors in the set of low scored translations
according to a given metric. The horizontal axis
represents the ratio of errors over the set of high
scored translations. The first observation is that
all metrics are less reliable when they assign low
scores (which corresponds with the situation A de-
scribed in Section 4.2). For instance, the best met-
ric erroneously assigns a low score in more than
20% of the cases. In general, the linguistic met-
rics do not improve the ability to capture wrong
translations (horizontal axis in the figure). How-
ever, again, the combining metric ULC achieves
the same reliability as the best n-gram based met-
ric.
310
In order to check the robustness of these results,
we computed the correlation of individual metric
failures between test beds, obtaining 0.67 Pearson
for the lowest correlated test bed pair (AE2004 and
CE2005) and 0.88 for the highest correlated pair
(AE2004 and CE2004).
Figure 4: Counter sample ratio for high vs low
metric scored translations
5.2.1 Analysis of Evaluation Samples
In order to shed some light on the reasons for the
automatic evaluation failures when assigning low
scores, we have manually analyzed cases in which
a metric score is low but the quality according to
humans is high (QNm ? 3 and Qh ? 7). We
have studied 100 sentence evaluation cases from
representatives of each metric family including: 1-
PER, BLEU, DP-Or-?, GTM (e = 2), METEOR
and ROUGEL. The evaluation cases have been ex-
tracted from the four test beds. We have identified
four main (non exclusive) failure causes:
Format issues, e.g. ?US ? vs ?United States?).
Elements such as abbreviations, acronyms or num-
bers which do not match the manual translation.
Pseudo-synonym terms, e.g. ?US Scheduled the
Release? vs. ?US set to Release?). ) In most of
these cases, synonymy can only be identified from
the discourse context. Therefore, terminological
resources (e.g., WordNet) are not enough to tackle
this problem.
Non relevant information omissions, e.g.
?Thank you? vs. ?Thank you very much? or
?dollar? vs. ?US dollar?)). The translation
system obviates some information which, in
context, is not considered crucial by the human
assessors. This effect is specially important in
short sentences.
Incorrect structures that change the meaning
while maintaining the same idea (e.g., ?Bush
Praises NASA ?s Mars Mission? vs ? Bush praises
nasa of Mars mission? ).
Note that all of these kinds of failure - except
formatting issues - require deep linguistic process-
ing while n-gram overlap or even synonyms ex-
tracted from a standard ontology are not enough to
deal with them. This conclusion motivates the in-
corporation of linguistic processing into automatic
evaluation metrics.
5.3 Ability to Deal with Translations that are
Dissimilar to References.
The results presented in Section 5.2 indicate that a
high score in metrics tends to be highly related to
truly good translations. This is due to the fact that
a high word overlapping with human references is
a reliable evidence of quality. However, in some
cases the translations to be evaluated are not so
similar to human references.
An example of this appears in the test bed
NIST05AE which includes a human-aided sys-
tem, LinearB (Callison-Burch, 2005). This system
produces correct translations whose words do not
necessarily overlap with references. On the other
hand, a statistics based system tends to produce
incorrect translations with a high level of lexical
overlapping with the set of human references. This
case was reported by Callison-Burch et al (2006)
and later studied by Gime?nez and Ma`rquez (2007).
They found out that lexical metrics fail to pro-
duce reliable evaluation scores. They favor sys-
tems which share the expected reference sublan-
guage (e.g., statistical) and penalize those which
do not (e.g., LinearB).
We can find in our test bed many instances in
which the statistical systems obtain a metric score
similar to the assisted system while achieving a
lower mark according to human assessors. For in-
stance, for the following translations, ROUGEL
assigns a slightly higher score to the output of a
statistical system which contains a lot of grammat-
ical and syntactical failures.
Human assisted system: The Chinese President made un-
precedented criticism of the leaders of Hong Kong after
political failings in the former British colony on Mon-
day . Human assessment=8.5.
Statistical system: Chinese President Hu Jintao today un-
precedented criticism to the leaders of Hong Kong
wake political and financial failure in the former
British colony. Human assessment=3.
311
Figure 5: Maximum translation quality decreasing
over similarly scored translation pairs.
In order to check the metric resistance to be
cheated by translations with high lexical over-
lapping, we estimate the quality decrease that
we could cause if we optimized the human-aided
translations according to the automatic metric. For
this, we consider in each translation case c, the
worse automatic translation t that equals or im-
proves the human-aided translation th according
to the automatic metric m. Formally the averaged
quality decrease is:
Quality decrease(m) =
Avgc(maxt(Qh(th)?Qh(t)|Qm(th) ? Qm(t)))
Figure 5 illustrates the results obtained. All
metrics are suitable to be cheated, assigning sim-
ilar or higher scores to worse translations. How-
ever, linguistic metrics are more resistant. In addi-
tion, the combined metric ULC obtains the best re-
sults, better than both linguistic and n-gram based
metrics. Our conclusion is that including higher
linguistic levels in metrics is relevant to prevent
ungrammatical n-gram matching to achieve simi-
lar scores than grammatical constructions.
5.4 The Oracle System Test
In order to obtain additional evidence about the
usefulness of combining evaluation metrics at dif-
ferent processing levels, let us consider the follow-
ing situation: given a set of reference translations
we want to train a combined system that takes
the most appropriate translation approach for each
text segment. We consider the set of translations
system presented in each competition as the trans-
lation approaches pool. Then, the upper bound on
the quality of the combined system is given by the
Metric OST
maxOST 6.72
ULC 5.79
ROUGEW 5.71
DP-Or-? 5.70
CP-Oc-? 5.70
NIST 5.70
randOST 5.20
minOST 3.67
Table 4: Metrics ranked according to the Oracle
System Test
predictive power of the employed automatic eval-
uation metric. This upper bound is obtained by se-
lecting the highest scored translation t according
to a specific metric m for each translation case c.
The Oracle System Test (OST) consists of com-
puting the averaged human assessed quality Qh
of the selected translations according to human as-
sessors across all cases. Formally:
OST(m) = Avgc(Qh(Argmaxt(Qm(t))|t ? c))
We use the sum of adequacy and fluency, both
in a 1-5 scale, as a global quality measure. Thus,
OST scores are in a 2-10 range. In summary,
the OST represents the best combined system that
could be trained according to a specific automatic
evaluation metric.
Table 4 shows OST values obtained for the best
metrics. In the table we have also included a ran-
dom, a maximum (always pick the best transla-
tion according to humans) and a minimum (al-
ways pick the worse translation according to hu-
man) OST for all 4. The most remarkable result
in Table 4 is that metrics are closer to the random
baseline than to the upperbound (maximum OST).
This result confirms the idea that an improvement
on metric reliability could contribute considerably
to the systems optimization process. However, the
key point is that the combined metric, ULC, im-
proves all the others (5.79 vs. 5.71), indicating
the importance of combining n-gram and linguis-
tic features.
6 Conclusions
Our experiments show that, on one hand, tradi-
tional n-gram based metrics are more or equally
4In all our experiments, the meta-metric values are com-
puted over each test bed independently before averaging in
order to assign equal relevance to the four possible contexts
(test beds)
312
reliable for estimating the translation quality at the
segment level, for predicting significant improve-
ment between systems and for detecting poor and
excellent translations.
On the other hand, linguistically motivated met-
rics improve n-gram metrics in two ways: (i) they
achieve higher correlation with human judgements
at system level and (ii) they are more resistant to
reward poor translations with high word overlap-
ping with references.
The underlying phenomenon is that, rather
than managing the linguistics variability, linguis-
tic based metrics introduce additional restrictions
for assigning high scores. This effect decreases
the recall over significant system improvements
achieved by n-gram based metrics and does not
solve the problem of detecting wrong translations.
Linguistic metrics, however, are more difficult to
cheat.
In general, the greatest pitfall of metrics is the
low reliability of low metric values. Our qualita-
tive analysis of evaluated sentences has shown that
deeper linguistic techniques are necessary to over-
come the important surface differences between
acceptable automatic translations and human ref-
erences.
But our key finding is that combining both kinds
of metrics gives top performance according to ev-
ery meta-evaluation criteria. In addition, our Com-
bined System Test shows that, when training a
combined translation system, using metrics at sev-
eral linguistic processing levels improves substan-
tially the use of individual metrics.
In summary, our results motivate: (i) work-
ing on new linguistic metrics for overcoming the
barrier of linguistic variability and (ii) perform-
ing new metric combining schemes based on lin-
ear regression over human judgements (Kulesza
and Shieber, 2004), training models over hu-
man/machine discrimination (Albrecht and Hwa,
2007) or non parametric methods based on refer-
ence to reference distances (Amigo? et al, 2005).
Acknowledgments
This work has been partially supported by the
Spanish Government, project INES/Text-Mess.
We are indebted to the three ACL anonymous re-
viewers which provided detailed suggestions to
improve our work.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for Sentence-Level MT Evaluation with Pseudo Ref-
erences. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 296?303.
Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and
Felisa Verdejo. 2005. QARLA: a Framework for
the Evaluation of Automatic Summarization. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
280?289.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and
Llu??s Ma`rquez. 2006. MT Evaluation: Human-
Like vs. Human Acceptable. In Proceedings of
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL), pages 17?24.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in
Machine Translation Research. In Proceedings of
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL).
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Christopher Culy and Susanne Z. Riehemann. 2003.
The Limits of N-gram Translation Evaluation Met-
rics. In Proceedings of MT-SUMMIT IX, pages 1?8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology, pages 138?145.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319?326.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008b. On the Ro-
bustness of Linguistic Features for Automatic MT
Evaluation. (Under submission).
313
Jesu?s Gime?nez and Llu??s Ma`rquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Jesu?s Gime?nez. 2007. IQMT v 2.0. Technical Manual
(LSI-07-29-R). Technical report, TALP Research
Center. LSI Department. http://www.lsi.
upc.edu/?nlp/IQMT/IQMT.v2.1.pdf.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI), pages 75?84.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Offi-
cial release of automatic evaluation scores for all
submissions, August.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
Evaluation of Machine Translation Quality Using
Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25?32.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evalu-
ation. In Proceedings of the 11th Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technology and the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL).
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation (LREC).
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), pages 148?155.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Labelled Dependencies in Machine
Translation Evaluation. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
104?111.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, RC22176. Technical
report, IBM T.J. Watson Research Center.
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Appli-
cations for Error Analysis. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 48?55, Prague, Czech Republic, June.
Association for Computational Linguistics.
Florence Reeder, Keith Miller, Jennifer Doyon, and
John White. 2001. The Naming of Things and
the Confusion of Tongues: an MT Metric. In Pro-
ceedings of the Workshop on MT Evaluation ?Who
did what to whom?? at Machine Translation Summit
VIII, pages 55?59.
Christoph Tillmann, Stefan Vogel, Hermann Ney,
A. Zubiaga, and H. Sawaf. 1997. Accelerated DP
based Search for Statistical Translation. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology.
314
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 361?364,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
The Impact of Query Refinement in the Web People Search Task
Javier Artiles
UNED NLP & IR group
Madrid, Spain
javart@bec.uned.es
Julio Gonzalo
UNED NLP & IR group
Madrid, Spain
julio@lsi.uned.es
Enrique Amig
?
o
UNED NLP & IR group
Madrid, Spain
enrique@lsi.uned.es
Abstract
Searching for a person name in a Web
Search Engine usually leads to a number
of web pages that refer to several people
sharing the same name. In this paper we
study whether it is reasonable to assume
that pages about the desired person can be
filtered by the user by adding query terms.
Our results indicate that, although in most
occasions there is a query refinement that
gives all and only those pages related to
an individual, it is unlikely that the user is
able to find this expression a priori.
1 Introduction
The Web has now become an essential resource
to obtain information about individuals but, at the
same time, its growth has made web people search
(WePS) a challenging task, because every single
name is usually shared by many different peo-
ple. One of the mainstream approaches to solve
this problem is designing meta-search engines that
cluster search results, producing one cluster per
person which contains all documents referring to
this person.
Up to now, two evaluation campaigns ? WePS 1
in 2007 (Artiles et al, 2007) and WePS 2 in 2009
(Artiles et al, 2009) ? have produced datasets for
this clustering task, with over 15 research groups
submitting results in each campaign. Since the re-
lease of the first datasets, this task is becoming an
increasingly popular research topic among Infor-
mation Retrieval and Natural Language Process-
ing researchers.
For precision oriented queries (for instance,
finding the homepage, the email or the phone num-
ber of a given person), clustered results might help
locating the desired data faster while avoiding con-
fusion with other people sharing the same name.
But the utility of clustering is more obvious for re-
call oriented queries, where the goal is to mine the
web for information about a person. In a typical
hiring process, for instance, candidates are eval-
uated not only according to their cv, but also ac-
cording to their web profile, i.e. information about
them available in the Web.
One question that naturally arises is whether
search results clustering can effectively help users
for this task. Eventually, a query refinement made
by the user ? for instance, adding an affiliation or
a location ? might have the desired disambigua-
tion effect without compromising recall. The hy-
pothesis underlying most research on Web People
Search is that query refinement is risky, because it
can enhance precision but it will usually harm re-
call. Adding the current affiliation of a person, for
instance, might make information about previous
jobs disappear from search results.
This hypothesis has not, up to now, been em-
pirically confirmed, and it is the goal of this pa-
per. We want to evaluate the actual impact of us-
ing query refinements in the Web People Search
(WePS) clustering task (as defined in the frame-
work of the WePS evaluation). For this, we have
studied to what extent a query refinement can suc-
cessfully filter relevant results and which type of
refinements are the most successful. In our ex-
periments we have considered the search results
associated to one individual as a set of relevant
documents, and we have tested the ability of dif-
ferent query refinement strategies to retrieve those
documents. Our results are conclusive: in most
occasions there is a ?near-perfect? refinement that
filters out most relevant information about a given
person, but this refinement is very hard to predict
from a user?s perspective.
In Section 2 we describe the datasets that where
used for our experiments. The experimental
methodology and results are presented in Section
3. Finally we present our conclusions in 4.
361
2 Dataset
2.1 The WePS-2 corpus
For our experiments we have used the WePS-2
testbed (Artiles et al, 2009)
1
. It consists of 30
datasets, each one related to one ambiguous name:
10 names were sampled from the US Census, 10
from Wikipedia, and 10 from the Computer Sci-
ence domain (Programme Committee members of
the ACL 2008 Conference). Each dataset consists
of, at most, 100 web pages written in English and
retrieved as the top search results of a web search
engine, using the (quoted) person name as query
2
.
Annotators were asked to organize the web
pages from each dataset in groups where all docu-
ments refer to the same person. For instance, the
?James Patterson? web results were gruped in four
clusters according to the four individuals men-
tioned with that name in the documents. In cases
where a web page refers to more than one person
using the same ambiguous name (e.g. a web page
with search results from Amazon), the document
is assigned to as many groups as necessary. Doc-
uments were discarded when there wasn?t enough
information to cluster them correctly.
2.2 Query refinement candidates
In order to generate query refinement candidates,
we extracted several types of features from each
document. First, we applied a simple preprocess-
ing to the HTML documents in the corpus, con-
verting them to plain text and tokenizing. Then,
we extracted tokens and word n-grams for each
document (up to four words lenght). A list of En-
glish stopwords was used to remove tokens and n-
grams beginning or ending with a stopword. Using
the Stanford Named Entity Recognition Tool
3
we
obtained the lists of persons, locations and organi-
zations mentioned in each document.
Additionally, we used attributes manually an-
notated for the WePS-2 Attribute Extraction Task
(Sekine and Artiles, 2009). These are person
attributes (affiliation, occupation, variations of
name, date of birth, etc.) for each individual shar-
ing the name searched. These attributes emulate
the kind of query refinements that a user might try
in a typical people search scenario.
1
http://nlp.uned.es/weps
2
We used the Yahoo! search service API.
3
http://nlp.stanford.edu/software/CRF-NER.shtml
field F prec. recall cover.
ae affiliation 0.99 0.98 1.00 0.46
ae award 1.00 1.00 1.00 0.04
ae birthplace 1.00 1.00 1.00 0.09
ae degree 0.85 0.80 1.00 0.10
ae email 1.00 1.00 1.00 0.11
ae fax 1.00 1.00 1.00 0.06
ae location 0.99 0.99 1.00 0.27
ae major 1.00 1.00 1.00 0.07
ae mentor 1.00 1.00 1.00 0.03
ae nationality 1.00 1.00 1.00 0.01
ae occupation 0.95 0.93 1.00 0.48
ae phone 0.99 0.99 1.00 0.13
ae relatives 0.99 0.98 1.00 0.15
ae school 0.99 0.99 1.00 0.15
ae work 0.96 0.95 1.00 0.07
stf location 0.96 0.95 1.00 0.93
stf organization 1.00 1.00 1.00 0.98
stf person 0.98 0.97 1.00 0.82
tokens 1.00 1.00 1.00 1.00
bigrams 1.00 1.00 1.00 0.98
trigrams 1.00 1.00 1.00 1.00
fourgrams 1.00 1.00 1.00 0.98
fivegrams 1.00 1.00 1.00 0.98
Table 1: Results for clusters of size 1
field F prec. recall cover.
ae affiliation 0.76 0.99 0.65 0.40
ae award 0.67 1.00 0.50 0.02
ae birthplace 0.67 1.00 0.50 0.10
ae degree 0.63 0.87 0.54 0.15
ae email 0.74 1.00 0.60 0.16
ae fax 0.67 1.00 0.50 0.09
ae location 0.77 1.00 0.66 0.32
ae major 0.71 1.00 0.56 0.09
ae mentor 0.75 1.00 0.63 0.04
ae nationality 0.67 1.00 0.50 0.01
ae occupation 0.76 0.98 0.65 0.52
ae phone 0.75 1.00 0.63 0.13
ae relatives 0.78 0.96 0.68 0.15
ae school 0.68 0.96 0.56 0.17
ae work 0.81 1.00 0.72 0.17
stf location 0.83 0.97 0.77 0.98
stf organization 0.89 1.00 0.83 1.00
stf person 0.83 0.99 0.74 0.98
tokens 0.96 0.99 0.94 1.00
bigrams 0.95 1.00 0.92 1.00
trigrams 0.94 1.00 0.92 1.00
fourgrams 0.91 1.00 0.86 0.99
fivegrams 0.89 1.00 0.84 0.99
Table 2: Results for clusters of size 2
field F prec. recall cover.
ae affiliation 0.51 0.96 0.39 0.81
ae award 0.26 1.00 0.16 0.20
ae birthplace 0.33 0.99 0.24 0.28
ae degree 0.37 0.90 0.26 0.36
ae email 0.35 0.96 0.23 0.33
ae fax 0.30 1.00 0.19 0.15
ae location 0.34 0.96 0.23 0.64
ae major 0.30 0.97 0.20 0.22
ae mentor 0.23 0.95 0.15 0.22
ae nationality 0.36 0.88 0.26 0.16
ae occupation 0.52 0.93 0.40 0.80
ae phone 0.34 0.96 0.23 0.33
ae relatives 0.32 0.95 0.22 0.16
ae school 0.40 0.95 0.29 0.43
ae work 0.45 0.94 0.34 0.38
stf location 0.62 0.87 0.53 1.00
stf organization 0.67 0.96 0.56 1.00
stf person 0.59 0.95 0.47 1.00
tokens 0.87 0.90 0.86 1.00
bigrams 0.79 0.95 0.70 1.00
trigrams 0.75 0.96 0.65 1.00
fourgrams 0.67 0.97 0.55 1.00
fivegrams 0.62 0.96 0.50 1.00
Table 3: Results for clusters of size >=3
3 Experiments
In our experiments we consider each set of doc-
uments (cluster) related to one individual in the
WePS corpus as a set of relevant documents for
a person search. For instance the James Patter-
362
field F prec. recall cover.
best-ae 1.00 0.99 1.00 0.74
best-all 1.00 1.00 1.00 1.00
best-ner 1.00 1.00 1.00 0.99
best-nl 1.00 1.00 1.00 1.00
Table 4: Results for clusters of size 1
field F prec. recall cover.
best-ae 0.77 1.00 0.65 0.79
best-all 0.95 1.00 0.93 1.00
best-ner 0.92 0.99 0.88 1.00
best-nl 0.96 1.00 0.94 1.00
Table 5: Results for clusters of size 2
field F prec. recall cover.
best-ae 0.60 0.97 0.47 0.92
best-all 0.89 0.96 0.85 1.00
best-ner 0.74 0.95 0.63 1.00
best-nl 0.89 0.95 0.85 1.00
Table 6: Results for clusters of size >=3
son dataset in the WePS corpus contains a total of
100 documents, and 10 of them belong to a British
politician named James Patterson. The WePS-2
corpus contains a total of 552 clusters that were
used to evaluate the different types of QRs.
For each person cluster, our goal is to find the
best query refinements; in an ideal case, an expres-
sion that is present in all documents in the clus-
ter, and not present in documents outside the clus-
ter. For each QR type (affiliation, e-mail, n-grams
of various sizes, etc.) we consider all candidates
found in at least one document from the cluster,
and pick up the one that leads to the best harmonic
mean (F
?=.5
) of precision and recall on the cluster
documents (there might be more than one).
For instance, when we evaluate a set of token
QR candidates for the politician in the James Pat-
terson dataset we find that among all the tokens
that appear in the documents of its cluster, ?repub-
lican? gives us a perfect score, while ?politician?
obtains a low precision (we retrieve documents of
other politicians named James Patterson).
In some cases a cluster might not have any can-
didate for a particular type of QR. For instance,
manual person attributes like phone number are
sparse and won?t be available for every individual,
whereas tokens and ngrams are always present.
We exclude those cases when computing F, and
instead we report a coverage measure which rep-
resents the number of clusters which have at least
one candidate of this type of QR. This way we
know how often we can use an attribute (coverage)
field 1 2 >=3
ae affiliation 20.96 17.88 29.41
ae occupation 20.25 21.79 24.60
ae work 3.23 8.38 8.56
ae location 12.66 12.29 8.02
ae school 7.03 6.70 6.42
ae degree 3.23 3.91 5.35
ae email 5.34 6.15 4.28
ae phone 6.19 5.03 3.21
ae nationality 0.28 0.00 3.21
ae relatives 7.03 5.03 2.67
ae birthplace 4.22 5.03 1.60
ae fax 2.95 1.68 1.60
ae major 3.52 3.91 1.07
ae mentor 1.41 2.23 0.00
ae award 1.69 0.00 0.00
Table 7: Distribution of the person attributes used
for the ?best-ae? strategy
and how useful it is when available (F measure).
These figures represent a ceiling for each type
of query refinement: they represent the efficiency
of the query when the user selects the best possible
refinement for a given QR type.
We have split the results in three groups depend-
ing on the size of the target cluster: (i) rare people,
mentioned in only one document (335 clusters of
size 1); (ii)people that appear in two documents
(92 clusters of size 2), often these documents be-
long to the same domain, or are very similar; and
(iii) all other cases (125 clusters of size >=3).
We also report on the aggregated results for cer-
tain subsets of QR types. For instance, if we want
to know what results will get a user that picks the
best person attribute, we consider all types of at-
tributes (e-mail, affiliation, etc.) for every cluster,
and pick up the ones that lead to the best results.
We consider four groups: (i) best-all selects the
best QR among all the available QR types (ii) best-
ae considers all manually annotated attributes (iii)
best-ner considers automatically annotated NEs;
and (iv) best-ng uses only tokens and ngrams.
3.1 Results
The results of the evaluation for each cluster size
(one, two, more than two) are presented in Ta-
bles 1, 2 and 3. These tables display results for
each QR type. Then Tables 4, 5 and 6 show the
results for aggregated QR types.
Two main results can be highlighted: (i) The
best overall refinement is, in average, very good
(F = .89 for clusters of size ? 3). In other words,
there is usually at least one QR that leads to (ap-
proximately) the desired set of results; (ii) this best
363
refinement, however, is not necessarily an intu-
itive choice for the user. One would expect users
to refine the query with a person?s attribute, such
as his affiliation or location. But the results for
the best (manually extracted) attribute are signifi-
cantly worse (F = .60 for clusters of size ? 3),
and they cannot always be used (coverage is .74,
.79 and .92 for clusters of size 1, 2 and ? 3).
The manually tagged attributes from WePS-2
are very precise, although their individual cover-
age over the different person clusters is generally
low. Affiliation and occupation, which are the
most frequent, obtain the largest coverage (0.81
and 0.80 for sizes ? 3). Also the recall of this
type of QRs is low in clusters of two, three or more
documents. When evaluating the ?best-ae? strat-
egy we found that in many clusters there is at least
one manual attribute that can be used as QR with
high precision. This is the case mostly for clusters
of three or more documents (0.92 coverage) and it
decreases with smaller clusters, probably because
there is less information about the person and thus
less biographical attributes are to be found.
In Table 7 we show the distribution of the actual
QR types selected by the ?best-ae? strategy. The
best type is affiliation, which is selected in 29%
of the cases. Affiliation and occupation together
cover around half of the cases (54%), and the rest
is a long tail where each attribute makes a small
contribution to the total. Again, this is a strong
indication that the best refinement is probably very
difficult to predict a priori for the user.
Automatically recognized named entities in the
documents obtain better results, in general, than
manually tagged attributes. This is probably due
to the fact that they can capture all kinds of related
entities, or simply entities that happen to coocur
with the person name. For instance, the pages of a
university professor that is usually mentioned to-
gether with his PhD students could be refined with
any of their names. This goes to show that a good
QR can be any information related to the person,
and that we might need to know the person very
well in advance in order to choose this QR.
Tokens and ngrams give us a kind of ?upper
boundary? of what is possible to achieve using
QRs. They include almost anything that is found
in the manual attributes and the named entities.
They also frequently include QRs that are not re-
alistic for a human refinement. For instance, in
clusters of only two documents it is not uncom-
mon that both pages belong to the same domain
or that they are near duplicates. In those cases to-
kens and ngram QR will probably include non in-
formative strings. In some cases the QRs found
are neither directly biographical or related NEs,
but topical information (e.g. the term ?soccer? in
the pages of a football player or the ngram ?align-
ment via structured multilabel? that is the title of a
paper written by a Computer Science researcher).
These cases widen even more the range of effec-
tive QRs. The overall results of using tokens and
ngrams are almost perfect for all clusters, but at
the cost of considering every possible bit of infor-
mation about the person or even unrelated text.
4 Conclusions
In this paper we have studied the potential effects
of using query refinements to perform the Web
People Search task. We have shown that although
in theory there are query refinements that perform
well to retrieve the documents of most individuals,
the nature of these ideal refinements varies widely
in the studied dataset, and there is no single in-
tuitive strategy leading to robust results. Even if
the attributes of the person are well known before-
hand (which is hardly realistic, given that in most
cases this is precisely the information needed by
the user), there is no way of anticipating which
expression will lead to good results for a particu-
lar person. These results confirm that search re-
sults clustering might indeed be of practical help
for users in Web people search.
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007. The semeval-2007 weps evaluation: Estab-
lishing a benchmark for the web people search task.
In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007).
ACL.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2009. Weps 2 evaluation campaign: overview of
the web people search clustering task. In WePS 2
Evaluation Workshop. WWW Conference 2009.
Satoshi Sekine and Javier Artiles. 2009. Weps2 at-
tribute extraction task. In 2nd Web People Search
Evaluation Workshop (WePS 2009), 18th WWW
Conference.
364
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 49?56, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating DUC 2004 Tasks with the QARLA Framework
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,anselmo,felisa}@lsi.uned.es
Abstract
This papers reports the application of
the QARLA evaluation framework to the
DUC 2004 testbed (tasks 2 and 5). Our
experiment addresses two issues: how
well QARLA evaluation measures corre-
late with human judgements, and what ad-
ditional insights can be provided by the
QARLA framework to the DUC evalua-
tion exercises.
1 Introduction
QARLA (Amigo? et al, 2005) is a framework that
uses similarity to models as a building block for
the evaluation of automatic summarisation systems.
The input of QARLA is a summarisation task, a set
of test cases, a set of similarity metrics, and sets of
models and automatic summaries (peers) for each
test case. With such a testbed, QARLA provides:
? A measure, QUEEN, which combines assorted
similarity metrics to estimate the quality of au-
tomatic summarisers.
? A measure, KING, to select the best combina-
tion of similarity metrics.
? An estimation, JACK, of the reliability of the
testbed for evaluation purposes.
The QARLA framework does not rely on human
judges. It is interesting, however, to find out how
well an evaluation using QARLA correlates with hu-
man judges, and whether QARLA can provide ad-
ditional insights into an evaluation based on human
assessments.
In this paper, we apply the QARLA framework
(QUEEN, KING and JACK measures) to the out-
put of two different evaluation exercises: DUC 2004
tasks 2 and 5 (Over and Yen, 2004). Task 2 re-
quires short (one-hundred word) summaries for as-
sorted document sets; Task 5 consists of generating
a short summary in response to a ?Who is? question.
In Section 2, we summarise the QARLA evalua-
tion framework; in Section 3, we describe the sim-
ilarity metrics used in the experiments. Section 4
discusses the results of the QARLA framework us-
ing such metrics on the DUC testbeds. Finally, Sec-
tion 5 draws some conclusions.
2 The QARLA evaluation framework
QARLA uses similarity to models for the evalua-
tion of automatic summarisation systems. Here we
summarise its main features; the reader may refer to
(Amigo? et al, 2005) for details.
The input of the framework is:
? A summarisation task (e.g. topic oriented, in-
formative multi-document summarisation on a
given domain/corpus).
? A set T of test cases (e.g. topic/document set
pairs for the example above)
? A set of summaries M produced by humans
(models), and a set of automatic summaries A
(peers), for every test case.
? A set X of similarity metrics to compare sum-
maries.
With this input, QARLA provides three main
measures that we describe below.
49
2.1 QUEEN : Estimating the quality of an
automatic summary
QUEEN operates under the assumption that a sum-
mary is better if it is closer to the model summaries
according to all metrics; it is defined as the probabil-
ity, measured onM ?M ?M , that for every metric
in X the automatic summary a is closer to a model
than two models to each other:
QUEENX,M (a) ? P (?x ? X.x(a,m) ? x(m
?,m??))
where a is the automatic summary being eval-
uated, ?m,m?,m??? are three models in M , and
x(a,m) stands for the similarity ofm to a. QUEEN
is stated as a probability, and therefore its range of
values is [0, 1].
We can think of the QUEEN measure as using a
set of tests (every similarity metric in X) to falsify
the hypothesis that a given summary a is a model.
Given ?a,m,m?,m???, we test x(a,m) ? x(m?,m??)
for each metric x. a is accepted as a model only if
it passes the test for every metric. QUEEN(a) is,
then, the probability of acceptance for a in the sam-
ple space M ?M ?M .
This measure has some interesting properties: (i)
it is able to combine different similarity metrics
into a single evaluation measure; (ii) it is not af-
fected by the scale properties of individual metrics,
i.e. it does not require metric normalisation and
it is not affected by metric weighting. (iii) Peers
which are very far from the set of models all receive
QUEEN=0. In other words, QUEEN does not distin-
guish between very poor summarisation strategies.
(iv) The value of QUEEN is maximised for peers
that ?merge? with the models under all metrics inX .
(v) The universal quantifier on the metric parameter
x implies that adding redundant metrics do not bias
the result of QUEEN.
Now the question is: which similarity metrics
are adequate to evaluate summaries? Imagine that
we use a similarity metric based on sentence co-
selection; it might happen that humans do not agree
on which sentences to select, and therefore emulat-
ing their sentence selection behaviour is both easy
(nobody agrees with each other) and useless. We
need to take into account which are the features that
human summaries do share, and evaluate according
to them. This is provided by the KING measure.
2.2 KING: estimating the quality of similarity
metrics
The measure KINGM,A(X) estimates the quality of
a set of similarity metrics X using a set of models
M and a set of peers A. KING is defined as the
probability that a model has higher QUEEN value
than any peer in a test sample. Formally:
KINGM,A(X) ?
P (?a ? A,QUEENM,X(m) > QUEENM,X(a))
For example, an ideal metric -that puts all models
together-would give QUEEN(m) = 1 for all mod-
els, and QUEEN(a) = 0 for all peers which are not
put together with the models, obtaining KING = 1.
KING satisfies several interesting properties: (i)
KING does not depend on the scale properties of the
metric; (ii) Adding repeated or very similar peers
do not alter the KING measure, which avoids one
way of biasing the measure. (iii) the KING value of
random and constant metrics is zero or close to zero.
2.3 JACK: reliability of the peer set
Once we detect a difference in quality between two
summarisation systems, the question is now whether
this result is reliable. Would we get the same results
using a different test set (different examples, differ-
ent human summarisers (models) or different base-
line systems)?
The first step is obviously to apply statistical sig-
nificance tests to the results. But even if they give a
positive result, it might be insufficient. The problem
is that the estimation of the probabilities in KING
assumes that the sample sets M,A are not biased.
If M,A are biased, the results can be statistically
significant and yet unreliable. The set of examples
and the behaviour of human summarisers (models)
should be somehow controlled either for homogene-
ity (if the intended profile of examples and/or users
is narrow) or representativity (if it is wide). But how
to know whether the set of automatic summaries is
representative and therefore is not penalising certain
automatic summarisation strategies?
This is addressed by the JACK measure:
50
JACK(X,M,A) ? P (?a, a? ? A|
?x ? X.x(a, a?) ? x(a,m) ? x(a?, a) ? x(a?,m)?
QUEEN(a) > 0 ? QUEEN(a?) > 0)
i.e. the probability over all model summariesm of
finding a couple of automatic summaries a, a? which
are closer to m than to each other according to all
metrics. This measure satisfies three desirable prop-
erties: (i) it can be enlarged by increasing the sim-
ilarity of the peers to the models (the x(m,a) fac-
tor in the inequalities), i.e. enhancing the quality of
the peer set; (ii) it can also be enlarged by decreas-
ing the similarity between automatic summaries (the
x(a, a?) factor in the inequality), i.e. augmenting the
diversity of (independent) automatic summarisation
strategies represented in the test bed; (iii) adding el-
ements to A cannot diminish the JACK value, be-
cause of the existential quantifier on a, a?.
3 Selection of similarity metrics
Each different similarity metric characterises differ-
ent features of a summary. Our first objective is
to select the best set of metrics, that is, the metrics
which best characterise the human summaries (mod-
els) as opposed to automatic summaries. The second
objective is to obtain as much information as possi-
ble about the behaviour of automatic summaries.
In this Section, we begin by describing a set of
59 metrics used as a starting point. Some of them
provide overlapping information; the second step is
then to select a subset of metrics that minimises re-
dundancy and, at the same time, maximises quality
(KING values). Finally, we analyse the characteris-
tics of the selected metrics.
3.1 Similarity metrics
For this work, we have considered the following
similarity metrics:
ROUGE based metrics (R): ROUGE (Lin and
Hovy, 2003) estimates the quality of an au-
tomatic summary on the basis of the n-gram
coverage related to a set of human summaries
(models). Although ROUGE is an evaluation
metric, we can adapt it to behave as a sim-
ilarity metric between pairs of summaries if
we consider only one model in the computa-
tion. There are different kinds of ROUGE met-
rics such as ROUGE-W, ROUGE-L, ROUGE-
1, ROUGE-2, ROUGE-3, ROUGE-4, etc. (Lin,
2004b). Each of these metrics has been ap-
plied over summaries with three preprocessing
options: with stemming and stopword removal
(type c); only with stopwords removal (type b);
or without any kind of preprocessing (type a).
All these combinations give 24 similarity met-
rics based on ROUGE.
Inverted ROUGE based metrics (Rpre): ROUGE
metrics are recall oriented. If we reverse the di-
rection of the similarity computation, we obtain
precision oriented metrics (i.e. Rpre(a, b) =
R(b, a)). In this way, we generate another 24
metrics based on inverted ROUGE.
TruncatedVectModel (TVMn): This family of met-
rics compares the distribution of the n most
relevant terms from original documents in the
summaries. The process is the following: (1)
obtaining the n most frequent lemmas ignoring
stopwords; (2) generating a vector with the rel-
ative frequency of each term in the summary;
(3) calculating the similarity between two vec-
tors as the inverse of the Euclidean distance.
We have used 9 variants of this measure with
n = 1, 4, 8, 16, 32, 64, 128, 256, 512.
AveragedSentencelengthSim (AVLS): This is a very
simple metric that compares the average length
of the sentences in two summaries. It can be
useful to compare the degree of abstraction of
the summaries.
GRAMSIM: This similarity metric compares the
distribution of the part-of-speech tags in the
two summaries. The processing is the follow-
ing: (1) part-of-speech tagging of summaries
using TreeTagger ; (2) generation of a vector
with the tags frequency for each summary; (3)
calculation of the similarity between two vec-
tors as the inverse of the Euclidean distance.
This similarity metric is not content oriented,
but syntax-oriented.
51
Figure 1: Similarity Metric Clusters
3.2 Clustering similarity metrics
From the set of metrics described above we have 57
(24+24+9) content oriented metrics, plus two met-
rics based on stylistic features (AVLS and GRAM-
SIM). However, the 57 metrics characterising sum-
mary contents are highly redundant. Thus, cluster-
ing similar metrics seems desirable.
We perform an automatic clustering process us-
ing the following notion of proximity between two
metric sets:
sim(X,X ?) ? Prob[H(X) ? H(X ?)]
where H(X) ? ?x ? X.x(a,m) ? x(m?,m??)
Two metrics sets are similar, according to the for-
mula, if they behave similarly with respect to the
QUEEN condition (H predicate in the formula),
i.e. the probability that the two sets of metrics dis-
criminate the same automatic summaries when they
are compared to the same pair of models.
Figure 1 shows the clustering of similarity met-
rics for the DUC 2004 Task 2. The number of clus-
ters was fixed in 10. After the clustering process, the
48 ROUGE metrics are grouped in 7 sets, and the 9
TVM metrics are grouped in 3 sets. In each clus-
ter, the metric with highest KING has been marked
in boldface. Note that the ROUGE-c metrics (with
stemming) with highest KING are those based on re-
call whereas the ROUGE-a/b metrics (without stem-
ming) are those based on precision. Regarding TVM
clusters, the metrics with highest KING in each clus-
ter are those based on a higher number of terms.
Finally, we select the metric with highest KING
in each group, obtaining the 10 most representative
metrics.
3.3 Best evaluation metric: KING values
Figure 2 shows the KING values for the selected
similarity metrics, which represent how every metric
characterises model summaries as opposed to auto-
matic summaries. These are the main results:
? The last column shows the best metric set,
considering all possible metric combinations.
In both DUC tasks, the best combination is
{Rpre-W-1.2.b,TVM.512. This metric set gets
better KING values than any individual metric
in isolation (17% better than the second best for
task 2, and 23% better for task 5). This is an in-
teresting result confirming that we can improve
our ability to characterise human summaries
just by combining standard similarity metrics
in the QARLA framework. Note also that both
metrics in the best set are content-oriented.
? Rpre-W.1.2.b (inverted ROUGE measure, us-
ing non-contiguous word sequences, remov-
ing stopwords, without stemming) obtains the
highest individual KING for task 2, and is one
of the best in task 5, confirming that ROUGE-
based metrics are a robust way of evaluating
summaries, and indicating that non-contiguous
word sequences can be more useful for evalua-
tion purposes than n-grams.
52
Figure 2: Similarity Metric quality
? TVM metrics get higher values when consid-
ering more terms (TVM.512), confirming that
comparing with just a few terms (e.g. TVM.4)
is not informative enough.
? Overall, KING values are higher for task
5, suggesting that there is more agreement
between human summaries in topic-oriented
tasks.
3.4 Reliability of the results
The JACK measure estimates the reliability of
QARLA results, and is correlated with the diversity
of automatic summarisation strategies included in
the testbed. In principle, the larger the number of au-
tomatic summaries, the higher the JACK values we
should obtain. The important point is to determine
when JACK values tend to stabilise; at this point, it
is not useful to add more automatic summaries with-
out introducing new summarisation strategies.
Figure 3 shows how JACKRpre-W,TVM.512 values
grow when adding automatic summaries. For more
than 10 systems, JACK values grow slower in both
tasks. Absolute JACK values are higher in Task 2
than in task 5, indicating that systems tend to pro-
duce more similar summaries in Task 5 (perhaps be-
cause it is a topic-oriented task). This result suggests
that we should incorporate more diverse summarisa-
tion strategies in Task 5 to enhance the reliability of
the testbed for evaluation purposes with QARLA.
4 Evaluation of automatic summarisers:
QUEEN values
The QUEEN measure provides two kinds of infor-
mation to compare automatic summarisation sys-
tems: which are the best systems -according to the
best metric set-, and which are the individual fea-
tures of every automatic summariser -according to
individual similarity metrics-.
4.1 System ranking
The best metric combination for both tasks was
{Rpre-W,TVM.512}; therefore, our global system
evaluation uses this combination of content-oriented
metrics. Figure 4 shows the QUEEN{Rpre-W,TVM.512}
values for each participating system in DUC 2004,
also including the model summaries. As expected,
model summaries obtain the highest QUEEN values
in both DUC tasks, with a significant distance with
respect to the automatic summaries.
4.2 Correlation with human judgements
The manual ranking generated in DUC is based on a
set of human-produced evaluation criteria, whereas
the QARLA framework gives more weight to the as-
pects that characterise model summaries as opposed
to automatic summaries. It is interesting, however,
to find out whether both evaluation methodologies
are correlated. Indeed, this is the case: the Pearson
correlation between manual and QUEEN rankings is
0.92 for the Task 2 and 0.96 for the Task 5.
Of course, QUEEN values depend on the chosen
metric set X; it is also interesting to check whether
53
Figure 3: JACK vs. Number of Automatic Summaries
Figure 4: QUEEN system ranking for the best metric set (A-H are models)
Figure 5: Correlation Between DUC and QARLA results
54
Figure 6: QUEEN values over GRAMSIM
metrics with higher KING values lead to QUEEN
rankings more similar to human judgements. Fig-
ure 5 shows the Pearson correlation between man-
ual and QUEEN rankings for 1024 metric combina-
tions with different KING values. The figure con-
firms that higher KING values are associated with
rankings closer to human judgements.
4.3 Stylistic features
The best metric combination leaves out similarity
metrics based on stylistic features. It is interesting,
however, to see how automatic summaries behave
with respect to this kind of features. Perhaps the
most remarkable fact about stylistic similarities is
that, in the case of the GRAMSIM metric, task 2
and task 5 exhibit a rather different behaviour (see
Figure 6). In task 2, systems merge with the models,
while in task 5 the QUEEN values of the systems
are inferior to the models. This suggests that there
is some stylistic component in models that systems
are not capturing in the topic-oriented task.
5 Related work
The methodology which is closest to our frame-
work is ORANGE (Lin, 2004a), which evaluates a
similarity metric using the average ranks obtained
by reference items within a baseline set. As in
our framework, ORANGE performs an automatic
meta-evaluation, there is no need for human assess-
ments, and it does not depend on the scale properties
of the metric being evaluated (because changes of
scale preserve rankings). The ORANGE approach
is, indeed, intimately related to the original QARLA
measure introduced in (Amigo et al, 2004).
There are several approaches to the automatic
evaluation of summarisation and Machine Transla-
tion systems (Culy and Riehemann, 2003; Coughlin,
2003). Probably the most significant improvement
over ORANGE is the ability to combine automati-
cally the information of different metrics. Our im-
pression is that a comprehensive automatic evalua-
tion of a summary must necessarily capture different
aspects of the problem with different metrics, and
that the results of every individual checking (metric)
should not be combined in any prescribed algebraic
way (such as a linear weighted combination). Our
framework satisfies this condition.
ORANGE, however, has also an advantage over
the QARLA framework, namely that it can be used
for evaluation metrics which are not based on sim-
ilarity between model/peer pairs. For instance,
ROUGE can be applied directly in the ORANGE
framework without any reformulation.
6 Conclusions
The application of the QARLA evaluation frame-
work to the DUC testbed provides some useful in-
sights into the problem of evaluating text summari-
sation systems:
? The results show that a combination of simi-
larity metrics behaves better than any metric in
isolation. The best metric set is {Rpre-W, TVM.512},
a combination of content-oriented metrics. Un-
55
surprisingly, stylistic similarity is less useful
for evaluation purposes.
? The evaluation provided by QARLA correlates
well with the rankings provided by DUC hu-
man judges. For both tasks, metric sets with
higher KING values slightly outperforms the
best ROUGE evaluation measure.
? QARLA measures show that DUC tasks 2 and
5 are quite different in nature. In Task 5, human
summaries are more similar, and the automatic
summarisation strategies evaluated are less di-
verse.
Acknowledgements
We are indebted to Ed Hovy, Donna Harman, Paul
Over, Hoa Dang and Chin-Yew Lin for their inspir-
ing and generous feedback at different stages in the
development of QARLA. We are also indebted to
NIST for hosting Enrique Amigo? as a visitor and
for providing the DUC test beds. This work has
been partially supported by the Spanish government,
project R2D2 (TIC-2003-7180).
References
E. Amigo?, J. Gonzalo, A. Pen?as, and F. Verdejo. 2005.
QARLA: a Framework for the Evaluation of Text
Summarization Systems. In Proceedings of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL 2005).
E. Amigo, V. Peinado, J. Gonzalo, A. Pen?as, and
F. Verdejo. 2004. An Empirical Study of Information
Synthesis Tasks. In Proceedings of the 42th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Barcelona, July.
Deborah Coughlin. 2003. Correlating Automated and
Human Assessments of Machine Translation Quality.
In In Proceedings of MT Summit IX, New Orleans,LA.
Christopher Culy and Susanne Riehemann. 2003. The
Limits of N-Gram Translation Evaluation Metrics. In
Proceedings of MT Summit IX, New Orleans,LA.
C. Lin and E. H. Hovy. 2003. Automatic Evaluation of
Summaries Using N-gram Co-ocurrence Statistics. In
Proceeding of 2003 Language Technology Conference
(HLT-NAACL 2003).
C. Lin. 2004a. Orange: a Method for Evaluating Au-
tomatic Metrics for Machine Translation. In Pro-
ceedings of the 36th Annual Conference on Compu-
tational Linguisticsion for Computational Linguistics
(Coling?04), Geneva, August.
Chin-Yew Lin. 2004b. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
P. Over and J. Yen. 2004. An introduction to DUC 2004
Intrinsic Evaluation of Generic New Text Summariza-
tion Systems. In Proceedings of DUC 2004 Document
Understanding Workshop, Boston.
56
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 455?466,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Corroborating Text Evaluation Results with Heterogeneous Measures
Enrique Amigo? ? Julio Gonzalo ? Jesu?s Gime?nez ? Felisa Verdejo?
? UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
? UPC, Barcelona
{jgimenez}@lsi.upc.edu
Abstract
Automatically produced texts (e.g. transla-
tions or summaries) are usually evaluated with
n-gram based measures such as BLEU or
ROUGE, while the wide set of more sophisti-
cated measures that have been proposed in the
last years remains largely ignored for practical
purposes. In this paper we first present an in-
depth analysis of the state of the art in order
to clarify this issue. After this, we formalize
and verify empirically a set of properties that
every text evaluation measure based on simi-
larity to human-produced references satisfies.
These properties imply that corroborating sys-
tem improvements with additional measures
always increases the overall reliability of the
evaluation process. In addition, the greater the
heterogeneity of the measures (which is mea-
surable) the higher their combined reliability.
These results support the use of heterogeneous
measures in order to consolidate text evalua-
tion results.
1 Introduction
The automatic evaluation of textual outputs is a
core issue in many Natural Language Processing
(NLP) tasks such as Natural Language Generation,
Machine Translation (MT) and Automatic Sum-
marization (AS). State-of-the-art automatic evalu-
ation methods all operate by rewarding similari-
ties between automatically-produced candidate out-
puts and manually-produced reference solutions, so-
called human references or models.
Over the last decade, a wide variety of measures,
based on different quality assumptions, have been
proposed. Recent work suggests exploiting exter-
nal knowledge sources and/or deep linguistic an-
notation, and measure combination (see Section 2).
However, original measures based on lexical match-
ing, such as BLEU (Papineni et al, 2001a) and
ROUGE (Lin, 2004) are still preferred as de facto
standards in MT and AS, respectively. There are,
in our opinion, two main reasons behind this fact.
First, the use of a common measure certainly allows
researchers to carry out objective comparisons be-
tween their work and other published results. Sec-
ond, the advantages of novel measures are not easy
to demonstrate in terms of correlation with human
judgements.
Our goal is not to answer which is the most re-
liable metric or to propose yet another novel mea-
sure. Rather than this, we first analyze in depth the
state of the art, concluding that it is not easy to de-
termine the reliability of a measure. In absence of a
clear proof of the advantages of novel measures, sys-
tem developers naturally tend to prefer well-known
standard measures. Second, we formalize and check
empirically two intrinsic properties that any evalua-
tion measure based on similarity to human-produced
references satisfies. Assuming that a measure satis-
fies a set of basic formal constraints, these properties
imply that corroborating a system comparison with
additional measures always increases the overall re-
liability of the evaluation process, even when the
added measures have a low correlation with human
judgements. In most papers, evaluation results are
corroborated with similar n-gram based measures
(eg. BLEU and ROUGE). However, according to
our second property, the greater the heterogeneity of
455
the measures (which is measurable) the higher their
reliability. The practical implication is that, corrob-
orating evaluation results with measures based on
higher linguistic levels increases the heterogeneity,
and therefore, the reliability of evaluation results.
2 State of the Art
2.1 Individual measures
Among NLP disciplines, MT probably has the
widest set of automatic evaluation measures. The
dominant approach to automatic MT evaluation is,
today, based on lexical metrics (also called n-gram
based metrics). These metrics work by rewarding
lexical similarity between candidate translations and
a set of manually-produced reference translations.
Lexical metrics can be classified according to how
they compute similarity. Some are based on edit dis-
tance, e.g., WER (Nie?en et al, 2000), PER (Till-
mann et al, 1997), and TER (Snover et al, 2006).
Other metrics are based on computing lexical preci-
sion, e.g., BLEU (Papineni et al, 2001b) and NIST
(Doddington, 2002), lexical recall, e.g., ROUGE
(Lin and Och, 2004a) and CDER (Leusch et al,
2006), or a balance between the two, e.g., GTM
(Melamed et al, 2003; Turian et al, 2003b), ME-
TEOR (Banerjee and Lavie, 2005), BLANC (Lita et
al., 2005), SIA (Liu and Gildea, 2006), MAXSIM
(Chan and Ng, 2008), and Ol (Gime?nez, 2008).
The lexical measure BLEU has been criticized in
many ways. Some drawbacks of BLEU are the lack
of interpretability (Turian et al, 2003a), the fact that
it is not necessary to increase BLEU to improve sys-
tems (Callison-burch and Osborne, 2006), the over-
scoring of statistical MT systems (Le and Przybocki,
2005), the low reliability over rich morphology lan-
guages (Homola et al, 2009), or even the fact that a
poor system translation of a book can obtain higher
BLEU results than a manually produced translation
(Culy and Riehemann, 2003).
The reaction to these criticisms has been focused
on the development of more sophisticated measures
in which candidate and reference translations are
automatically annotated and compared at different
linguistic levels. Some of the features employed
include parts of speech (Popovic and Ney, 2007;
Gime?nez and Ma`rquez, 2007), syntactic dependen-
cies (Liu and Gildea, 2005; Gime?nez and Ma`rquez,
2007; Owczarzak et al, 2007a; Owczarzak et al,
2007b; Owczarzak et al, 2008; Chan and Ng,
2008; Kahn et al, 2009), CCG parsing (Mehay and
Brew, 2007), syntactic constituents (Liu and Gildea,
2005; Gime?nez and Ma`rquez, 2007), named entities
(Reeder et al, 2001; Gime?nez and Ma`rquez, 2007),
semantic roles (Gime?nez and Ma`rquez, 2007), dis-
course representations (Gime?nez, 2008), and textual
entailment features (Pado? et al, 2009). In general,
when a higher linguistic level is incorporated, lin-
guistic features at lower levels are preserved.
The proposals for summarization evaluation are
less numerous. Some proposals for AS tasks are
based on syntactic units (Tratz and Hovy, 2008), de-
pendency triples (Owczarzak, 2009) or convolution
kernels (Hirao et al, 2005) which reported some re-
liability improvement over ROUGE in terms of cor-
relation with human judgements.
In general, however, it is not easy to determine
clearly the contribution of deeper linguistic knowl-
edge in those proposals. In the case of MT, im-
provements versus BLEU have been reported (Liu
and Gildea, 2005; Kahn et al, 2009), but not over
a more elaborated metric such as METEOR (Mehay
and Brew, 2007; Chan and Ng, 2008). Besides, con-
troversial results on their performance at sentence vs
system level have been reported in shared evaluation
tasks (Callison-Burch et al, 2008; Callison-Burch et
al., 2009; Callison-Burch et al, 2010).
2.2 Combined measures
Several researchers have suggested integrating het-
erogeneous measures. Some of them optimize the
measure combination function according to the met-
ric?s ability to emulate the behavior of human as-
sessors (i.e., correlation with human assessments).
For instance, using linear combinations (Pado? et al,
2009; Liu and Gildea, 2007; Gime?nez and Ma`rquez,
2008), Decision Trees (Akiba et al, 2001; Quirk,
2004), regression based algorithms (Paul et al,
2007; Albrecht and Hwa, 2007a; Albrecht and Hwa,
2007b) or a variety of supervised machine learn-
ing algorithms(Quirk et al, 2005; Corston-Oliver et
al., 2001; Kulesza and Shieber, 2004; Gamon et al,
2005; Amigo? et al, 2005).
Some of these works report evidence on the con-
tribution of combining heterogeneous measures. For
instance, Albrecht and Hwa included syntax-based
456
measures together with lexical measures, outper-
forming other combination schemes (Albrecht and
Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and
Gildea, after examining the contribution of each
component metric, found that ?metrics showing dif-
ferent properties of a sentence are more likely to
make a good combined metric?(Liu and Gildea,
2007). Akiba et al, which combined multiple edit-
distance features based on lexical, morphosyntac-
tic and lexical semantic information, observed that
their approach improved single editing distance for
several data sets (Akiba et al, 2001). More evi-
dence was provided by Corston and Oliver. They
showed that results on the task of discriminating be-
tween manual and automatic translations improve
when combining linguistic and n-gram based fea-
tures. In addition, they showed that this mixed com-
bination improved over the combination of linguistic
or n-gram based measures alone (Corston-Oliver et
al., 2001). (Pado? et al, 2009) reported a reliability
improvement by including measures based on tex-
tual entailment in the set. In (Gime?nez and Ma`rquez,
2008), a simple arithmetic mean of scores for com-
bining measures at different linguistic levels was ap-
plied with remarkable results in recent shared evalu-
ation tasks (Callison-Burch et al, 2010).
2.3 Meta-evaluation criteria
Meta-evaluation methods have been gradually intro-
duced together with evaluation measures. For in-
stance, Papineni et al (2001b) evaluated the reliabil-
ity of the BLEU metric according to its ability to em-
ulate human assessors, as measured in terms of Pear-
son correlation with human assessments of adequacy
and fluency at the document level. The measure
NIST (Doddington, 2002) was meta-evaluated also
in terms of correlation with human assessments, but
over different document sources and for a varying
number of references and segment sizes. Melamed
et al (2003) argued, at the time of introducing the
GTM metric, that Pearson correlation coefficients
can be affected by scale properties. They suggested
using the non-parametric Spearman correlation co-
efficients instead. Lin and Och meta-evaluated
ROUGE over both Pearson and Spearman correla-
tion over a wide set of metrics, including NIST,
WER, PER, and variants of ROUGE, BLEU and
GTM. They obtained similar results in both cases
(Lin and Och, 2004a). Banerjee and Lavie (2005)
argued that the reliability of metrics at the document
level can be due to averaging effects but might not
be robust across sentence translations. In order to
address this issue, they computed the translation-by-
translation correlation with human assessments (i.e.,
correlation at the sentence level).
However, correlation with human judgements is
not enough to determine the reliability of measures.
First, correlation at sentence level (unlike correla-
tion at system level) tends to be low and difficult to
interpret. Second, correlation at system and segment
levels can produce contradictory results. In (Amigo?
et al, 2009) it is observed that higher linguistic lev-
els in measures increases the correlation with human
judgements at the system level at the cost of corre-
lation at the segment level. As far as we know, a
clear explanation for these phenomena has not been
provided yet.
Third, a high correlation at system level does
not ensure a high reliability. Culy and Rieheman
observed that, although BLEU can achieve a high
correlation at system level in some test suites, it
over-scores a poor automatic translation of ?Tom
Sawyer? against a human produced translation (Culy
and Riehemann, 2003). This meta-evaluation crite-
rion based on the ability to discern between man-
ual and automatic translations have been referred to
as human likeness (Amigo? et al, 2006), in contrast
to correlation with human judgements which is re-
ferred to as human acceptability. Examples of meta-
measures based on this criterion are ORANGE (Lin
and Och, 2004b) and KING (Amigo? et al, 2005).
In addition, many of the approaches to metric com-
bination described in Section 2.2 take human like-
ness as the optimization criterion (Corston-Oliver
et al, 2001; Kulesza and Shieber, 2004; Gamon et
al., 2005). The main advantage of meta-evaluation
based on human likeness is that, since human as-
sessments are not required, metrics can be evaluated
over larger test beds. However, the meta-evaluation
in terms of human likeness is difficult to interpret.
2.4 The use of evaluation measures
In general, the state of the art includes a wide set
of results that show the drawbacks of n-gram based
measures as BLEU, and a wide set of proposals for
new single and combined measures which are meta-
457
evaluated in terms of human acceptability (i.e., their
ability to emulate human judges, typically measured
in terms of correlation with human judgements) or
human-likeness (i.e., their ability to discern between
automatic and human translations) (Amigo? et al,
2006). However, the original measures BLEU and
ROUGE are still preferred.
We believe that one of the reasons is the lack of
an in-depth study on to what extent providing ad-
ditional evaluation results with other metrics con-
tributes to the reliability of such results. The state of
the art suggests that the use of heterogeneous mea-
sures can improve the evaluation reliability. How-
ever, as far as we know, there is no comprehen-
sive analysis on the contribution of novel measures
when corroborating evaluation results with addi-
tional measures.
3 Similarity Based Evaluation Measures
In general, automatic evaluation measures applied
in tasks like MT or AS are similarity measures be-
tween system outputs and human references. These
measures are related with precision, recall or overlap
over specific types of linguistic units. For instance,
ROUGE measures n-gram recall. Other measures
that work at higher linguistic levels apply precision,
recall or overlap of linguistic components such as
dependency relations, grammatical categories, se-
mantic roles, etc.
In order to delimit our hypothesis, let us first de-
fine what is a similarity measure in this context. Un-
fortunately, as far as we know, there is no formal
concept covering the properties of current evaluation
similarity measures. A close concept is that of ?met-
ric? or ?distance function?. But, actually, measures
such as ROUGE or BLEU are not proper ?metrics?,
because they do not satisfy the symmetry and the tri-
angle inequality properties. Therefore, we need a
new definition.
Being ? the universe of system outputs s and
gold-standards g, we assume that a similarity mea-
sure, in our context, is a function x : ?2 ?? < such
that there exists a decomposition function f : ? ??
{e1..en} (e.g., words or other linguistic units or
relationships) satisfying the following constraints:
(i) maximum similarity is achieved only when then
the decomposition of the system output resembles
exactly the gold-standard decomposition; and (ii)
growing overlap or removing non overlapped ele-
ments implies growing x. Formally, if x ranges from
0 to 1:
f(s) = f(g)? x(s, g) = 1
(f(s) = f(s?) ? {e ? f(g) \ f(s?)})? x(s, g) > x(s?, g)
(f(s) = f(s?)? {e ? f(s?) \ f(g)})? x(s, g) > x(s?, g)
For instance, a random function and the reversal
of a similarity funtion (f ?(s) = 1f(s) ) do not satisfythese constraints. While the F measure over Pre-
cision and Recall satisfies these constraints1, pre-
cision and recall in isolation do not satisfy all of
them: maximum recall can be achieved without re-
sembling the goldstandard text decomposition; and
maximum precision can be achieved with only a few
overlapped elements.
BLEU (Papineni et al, 2001a) computes the n-
gram precision while the metric ROUGE (Lin and
Och, 2004a) computes the n-gram recall. How-
ever, in general, both metrics satisfy all the con-
straints, given that BLEU includes a brevity penalty
and ROUGE penalizes or limits the system output
length. The measure METEOR creates an align-
ment between the two strings (Banerjee and Lavie,
2005). This overlap-based measure satisfies also the
previous constraints. Measures based on edit dis-
tance over n-grams (Tillmann et al, 1997; Nie?en
et al, 2000) or other linguistic units (Akiba et al,
2001; Popovic and Ney, 2007) match also our def-
inition of similarity measure. The editing distance
is minimum when the two compared text are equal.
The more the evaluated text contains elements from
the gold-standard the more the editing distance is re-
duced (higher similarity). The word ordering can be
also expressed in terms of a decomposition function.
A similar reasoning applies to every relevant mea-
sure in the state-of-the art.
4 Data Sets and Measures
4.1 Data sets
In this paper, we provide empirical results for
MT and AS. For MT, we use the data sets from
the Arabic-to-English (AE) and Chinese-to-English
(CE) NIST MT Evaluation campaigns in 2004 and
1There is an exception. In an extreme case, when recall is
zero, removing non overlapped elements does not modify the F
measure.
458
AE2004 CE2004 AE2005 CE2005
#human-references 5 5 5 4
#systems 5 10 7 10
#system-outputs-assessed 5 10 6 5
#system-outputs 1,353 1,788 1,056 1,082
#outputs-assessed per-system 347 447 266 272
Table 1: Description of the test beds from 2004 and 2005 NIST MT evaluation campaigns used in the experiments
throughout the paper.
DUC 2005 DUC 2006
#human-references 3-4 3-4
#systems 32 35
#system-outputs-assessed 32 35
#system-outputs 50 50
#outputs-assessed per-system 50 50
Table 2: Description of the test beds from 2005 and 2006 DUC evaluation campaigns used in the experiments through-
out the paper.
20052. Both include two translations exercises: for
the 2005 campaign we contacted each participant
individually and asked for permission to use their
data3. In our experiments, we take the sum of ad-
equacy and fluency, both in a 1-5 scale, as a global
measure of quality (LDC, 2005). Thus, human as-
sessments are in a 2-10 scale. For AS, we have used
the AS test suites developed in the DUC 2005 and
DUC 2006 evaluation campaigns4. This AS task
was to generate a question focused summary of 250
words from a set of 25-50 documents to a complex
question. Summaries were evaluated according to
several criteria. Here, we will consider the respon-
siveness judgements, in which the quality score was
an integer between 1 and 5. See Tables 1 and 2 for a
brief quantitative description of these test beds.
2http://www.nist.gov/speech/tests/mt
3We are grateful to a number of groups and companies who
responded positively: University of Southern California Infor-
mation Sciences Institute (ISI), University of Maryland (UMD),
Johns Hopkins University & University of Cambridge (JHU-
CU), IBM, University of Edinburgh, University of Aachen
(RWTH), National Research Council of Canada (NRC), Chi-
nese Academy of Sciences Institute of Computing Technology
(ICT), Instituto Trentino di Cultura - Centro per la Ricerca Sci-
entifica e Tecnologica(ITC-IRST), MITRE.
4http://duc.nist.gov/
4.2 Measures
As for evaluation measures, for MT we have used a
rich set of 64 measures provided within the ASIYA
Toolkit (Gime?nez and Ma`rquez, 2010)5. This in-
cludes measures operating at different linguistic lev-
els: lexical, syntactic, and semantic. At the lexical
level this set includes variants of 8 measures em-
ployed in the state of the art: BLEU, NIST, GTM,
METEOR, ROUGE, WER, PER and TER. In addi-
tion, we have included a basic measure Ol that com-
putes the lexical overlap without considering word
ordering. All these measures have similar granular-
ity. They use n-grams of a varying length as the ba-
sic unit with additional information provided by lin-
guistic tools. The underlying similarity criteria in-
clude precision, recall, overlap, or edit rate, and the
decomposition functions include words, dependency
tree nodes (DP HWC, DP-Or, etc.), constituency
parsing (CP-STM), discourse roles (DR-Or), seman-
tic roles (SR-Or), named entities, etc. Further details
on the measure set may be found in the ASIYA tech-
nical manual (Gime?nez and Ma`rquez, 2010).
According to our computations, our measures
cover high and low correlations at both levels. Cor-
relation at system level spans between 0.63 and 0.95.
Correlations at sentence level ranges from 0.18 up to
0.54. We will discriminate between two subsets of
5http://www.lsi.upc.edu/?nlp/Asiya
459
measures. The first one includes those that decom-
pose the text into words, n-grams, stems or lexical
semantic tags. This set includes BLEU, ROUGE,
NIST, GTM, PER and WER families. We will re-
fer to them as ?lexical? measures. The second set
are those that consider deeper linguistic levels such
as parts of speech, syntactic dependencies, syntactic
constituents, etc. We will refer to them as ?linguis-
tic? measures.
In the case of automatic summarization (AS), we
have employed the standard variants of ROUGE
(Lin, 2004). These 7 measures are ROUGE-{1..4},
ROUGE-SU, ROUGE-L and ROUGE-W. In addi-
tion we have included the reversed precision version
for each variant and the F measure of both. Notice
that the original ROUGE measures are oriented to
recall. In total, we have 21 measures for the sum-
marization task. All of them are based on n-gram
overlap.
5 Additive reliability
As discussed in Section 2, a number of recent pub-
lications address the problem of measure combi-
nation with successful results, specially when het-
erogeneous measures are combined. The following
property clarifies this issue and justifies the use of
heterogeneous measures when corroborating evalu-
ation results. It asserts that the reliability of system
improvements always increases when the evaluation
result is corroborated by an additional similarity
measure, regardless of the correlation achieved by
the additional measure in isolation.
For the sake of clarity, in the rest of the paper,
we will denote the similarity x(s, g) between sys-
tem output s and human reference g by x(s). The
quality of a system output s will be referred to as
Q(s). Let us define the reliability R(X) of a mea-
sure set as the probability of a real improvement (as
measured by human judges) when a score improve-
ment is observed simultaneously for all measures in
the set X. :
R(X) ? P (Q(s) ? Q(s?)|x(s) ? x(s?) ?x ? X)
According to this definition, we may not be able
to predict the quality of any system output (i.e. a
translation) with a highly reliable measure set, but
we can ensure a system improvement when all mea-
sures corroborate the result. Then the additive relia-
bility property can be stated as:
R(X ? {x}) ? R(X)
We could think of violating this property by
adding, for instance, a measure consisting of a ran-
dom function (x?(s) = rand(0..1)) or a reversal of
the original measure (x?(s) = 1/x(s)). These kind
of measures, however, would not satisfy the con-
straints defined in Section 3.
This property is based on the idea that similar-
ity with human references according to any aspect
should not imply statistically a quality decrease. Al-
though our test suites includes measures with low
correlation at segment and system level, we can con-
firm empirically that all of them satisfy this property.
We have developed the following experiment:
taking all possible measure pairs in the test suites,
we have compared their reliability as a set versus the
maximal reliability of any of them (by computing
the difference R(X)?max(R(x1), R(x2)). Figure
1 shows the obtained distribution of this difference
for our MT and AS test suites. Remarkably, in al-
most every case this difference is positive.
This result has a key implication: Corroborating
evaluation results with a new measure, even when
it has lower correlation with human judgements, in-
creases the reliability of results. Therefore, if the
correlation with judgements is not determinant, the
question is now what factor determines the contri-
bution of the new measures. According to the fol-
lowing property, this factor is the heterogeneity of
measures.
6 Heterogeneity
This property states that the reliability of any mea-
sure combination is lower bounded by the hetero-
geneity of the measure set. In other words, a single
measure can be more or less reliable, but a system
improvement according to all measures in an het-
erogeneous set is reliable.
Let us define the heterogeneity H(X) of a set of
measures X as, given two system outputs s and s?
such that g 6= s 6= s? 6= g (g is the reference
text), the probability that there exist two measures
that contradict each other. That is:
H(X) ? P (?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))
460
Figure 1: Additive reliability for metric pairs.
Thus, given a set X of measures, the property
states that there exists a strict growing function F
such that:
R(X) ? F (H(X)) and H(X) = 1? R(X) = 1
In other words, the more the similarity measures
tend to contradict each other, the more a unanimous
improvement over all similarity measures is reliable.
Clearly, the harder it is that measures agree, the more
meaningful it is when they do.
The first part is derived from the Additive Re-
liability property. Intuitively, any individual mea-
sure has zero heterogeneity. Increasing the hetero-
geneity implies joining measures or measure sets
progressively. According to the Additive Reliabil-
ity property, this joining implies a reliability in-
crease. Therefore, the higher the heterogeneity, the
higher the minimum Reliability achieved by the cor-
responding measure sets.
The second part is derived from the Heterogeneity
definition. If H(X) = 1 then, for any distinct pair
of outputs that differ from the reference, there exist
at least two measures in the set contradicting each
other. That is, H(X) = 1 implies that:
?s 6= s? 6= g(?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))
Therefore, if one output improves the other ac-
cording to all measures, then the output must be
equal than the reference.
?(?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))?
Figure 2: Heterogeneity vs. reliability in MT test suites.
?(g 6= s 6= s? 6= g)? g = s ? g = s?
According to the first constraint of similarity mea-
sures, a text that is equal to the reference achieves
the maximum score:
g = s? f(g) = g(s)? ?x.x(s) ? x(s?)
Finally, if we assume that the reference (human pro-
duced texts) has a maximum quality, then it will
have equal or higher quality than the other output.
g = s? Q(s) ? Q(s?)
Therefore, the reliability of the measure set is maxi-
mal. In summary, if H(X) = 1 then:
R(X) = P (Q(s) ? Q(s?)|x(s) ? x(s?) ?x ? X) =
= P (Q(s) ? Q(s?)|s = g) = 1
Figures 2 and 3 show the relationship between the
heterogeneity of randomly selected measure sets and
their reliability for the MT and summarization test
suites. As the figures show, the higher the hetero-
geneity, the higher the reliability of the measure set.
The results in AS are less pronounced due to the re-
dundancy in ROUGE measure.
Notice that the heterogeneity property does not
necessarily imply a high correlation between reli-
ability and heterogeneity. For instance, an ideal
single measure would have zero heterogeneity and
461
Figure 3: Heterogeneity vs. reliability in summarization
test suites.
achieve maximum reliability, appearing in the top
left area. The property rather brings us to the fol-
lowing situation: let us suppose that we have a set
of single measures available which achieve a certain
range of reliability. We can improve our system ac-
cording to any of these measures. Without human
assessments, we do not know what is the most re-
liable measure. But if we combine them, increas-
ing the heterogeneity, the minimal reliability of the
selected measures will be higher. This implies that
combining heterogeneous measures (e.g. at high lin-
guistic levels) that do not achieve high correlation
in isolation, is better than corroborating results with
any individual measure alone, such as ROUGE and
BLEU, which is the common practice in the state of
the art.
The main drawback of this property is that in-
creasing the heterogeneity implies a sensitivity re-
duction. For instance, if H(X) = 0.9, then only
for 10% of output pairs in the corpus there exists
an improvement according to all measures. In other
words, unanimous evaluation results from heteroge-
neous measures are reliable but harder to achieve for
the system developer. The next section investigates
on this issue.
Finally, Figure 4 shows that linguistic measures
increase the heterogeneity of measure sets. We have
generated sets of metrics of size 1 to 10 made up
by lexical or lexical and linguistic metrics. As the
figure shows, in the second case, the measure sets
achieve a higher heterogeneity.
Figure 4: Heterogeneity of lexical measures vs. lexical
and linguistic measures.
7 Score thresholds vs. Additive Reliability
According to the previous properties, corroborating
evaluation results with several measures increases
the reliability of evaluation results at the cost of sen-
sitivity. On the other hand, increasing the score
threshold of a single measure should have a similar
effect. Which is then the best methodology to im-
prove reliability? In this section we provide exper-
imental evidence on the relationship between both
ways of increasing reliability: we have found that,
corroborating evaluation results over single texts
with additional measures is more reliable than re-
quiring higher score differences according to any in-
dividual measure in the set. More specifically, we
have found that the reliability of a measure set is
higher than the reliability of each of the individual
measures at a similar level of sensitivity.
Formally, we define the sensitivity S(X) of a met-
ric set X as the probability of finding a score im-
provement within text pairs with a real (i.e. human
assessed) quality improvement:
S(X) = P (x(s) ? x(s?)?x ? X|Q(s) ? Q(s?))
Being Rth(x) and Sth(x) the reliability and sen-
sitivity of a single measure x for a certain increase
score threshold th:
462
Figure 5: Heterogeneity vs. reliability Gain for MT test
suites.
Rth(x) = P (Q(s) ? Q(s?)|x(s)? x(s?) ? th)
Sth(x) = P (x(s)? x(s?) ? th|Q(s) ? Q(s?))
The property that we want to check is that, at the
same sensitivity level, combining measures is more
reliable than increasing the score threshold of single
measures:
S(X) = Sth(x).x ? X ?? R(X) ? Rth(x)
Note that if we had a perfect measure xp such that
R(xp) = S(xp) = 1, then combining this measure
with a low reliability measure xl would produce a
lower sensitivity, but the maximal reliability would
be preserved.
In order to confirm empirically this property, we
have developed the following experiment: (i) We
compute the reliability and sensitivity of randomly
chosen measure sets over single text pairs. We have
generated sets of 2,3,5,10,20 and 40 measures. In
the case of summarization corpora we have com-
bined up to 20 measures. In addition, we com-
pute also the heterogeneity H(X) of each measure
set; (ii) Experimenting with different values for the
threshold th, we compute the reliability of single
measures for all potential sensitivity levels; (iii) For
each measure set, we compare the reliability of the
measure set versus the reliability of single measures
at the same sensitivity level. We will refer to this as
the Reliability Gain:
Figure 6: Heterogeneity vs. reliability Gain for MT test
suites.
Reliability Gain =
R(X)?max{Rth(x)/x ? X ? Sth(x) = S(X)}
If there are several reliability values with the same
sensitivity for a given single measures, we choose
the highest reliability value for the single measure.
Figures 5 and 6 illustrate the results for the MT
and AS corpora. The horizontal axis represents the
Heterogeneity of measure sets, while the vertical
axis represents the reliability gain. Remarkably, the
reliability gain is positive for all cases in our test
suites. The maximum reliability gain is 0.34 in the
case of MT and 0.08 for AS (note that summariza-
tion measures are more redundant in our corpora).
In both test suites, the largest information gains are
obtained with highly heterogeneous measure sets.
In summary, given comparable measures in terms
of reliability, corroborating evaluation results with
several measures is more effective than optimizing
systems according to the best measure in the set.
This empirical property provides an additional ev-
idence in favour of the use of heterogeneous mea-
sures and, in particular, of the use of linguistic mea-
sures in combination with standard lexical measures.
8 Conclusions
In this paper, we have analyzed the state of the art in
order to clarify why novel text evaluation measures
463
are not exploited by the community. Our first con-
clusion is that it is not easy to determine the reliabil-
ity of measures, which is highly corpus-dependent
and often contradictory when comparing correlation
with human judgements at segment vs. system lev-
els.
In order to tackle this issue, we have studied a
number of properties that suggest the convenience of
using heterogeneous measures to corroborate eval-
uation results. According to these properties, we
can ensure that, even when if we can not determine
the reliability of individual measures, corroborating
a system improvement with additional measures al-
ways increases the reliability of the results. In ad-
dition, the more heterogeneous the measures em-
ployed (which is measurable), the higher the relia-
bility of the results. But perhaps the most impor-
tant practical finding is that the reliability at similar
sensitivity levels by corroborating evaluation results
with several measures is always higher than improv-
ing systems according to any of the combined mea-
sures in isolation.
These properties point to the practical advantages
of considering linguistic knowledge (beyond lexi-
cal information) in measures, even if they do not
achieve a high correlation with human judgements.
Our experiments show that linguistic knowledge in-
creases the heterogeneity of measure sets, which
in turn increases the reliability of evaluation results
when corroborating system comparisons with sev-
eral measures.
Acknowledgements
This work has been partially funded by the Spanish
Government (Holopedia, TIN2010-21128-C02 and
OpenMT-2, TIN2009-14675-C03) and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement number
247762 (FAUST project, FP7-ICT-2009-4-247762).
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using Multiple Edit Distances to Automatically
Rank Machine Translation Output. In Proceedings of
Machine Translation Summit VIII, pages 15?20.
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 880?887.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 296?303.
Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Summarization. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 280?289.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of the Joint 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 17?
24.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Fe-
lisa Verdejo. 2009. The contribution of linguis-
tic features to automatic machine translation evalua-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1 - Volume 1, ACL ?09,
pages 306?314, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53. Revised August 2010.
464
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM:
A maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to the
Automatic Evaluation of Machine Translation. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 140?147.
Christopher Culy and Susanne Z. Riehemann. 2003. The
Limits of N-gram Translation Evaluation Metrics. In
Proceedings of MT-SUMMIT IX, pages 1?8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd Inter-
national Conference on Human Language Technology,
pages 138?145.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-Level MT evaluation without refer-
ence translations: beyond language modeling. In Pro-
ceedings of EAMT, pages 103?111.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Workshop on
Statistical Machine Translation, pages 256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319?326.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, 1(94):77?86.
Jesu?s Gime?nez. 2008. Empirical Machine Transla-
tion and its Evaluation. Ph.D. thesis, Universitat
Polite`cnica de Catalunya.
Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.
2005. Kernel-based approach for automatic evaluation
of natural language generation technologies: Applica-
tion to automatic summarization. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 145?152, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Petr Homola, Vladislav Kubon?, and Pavel Pecina. 2009.
A simple automatic mt evaluation metric. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, StatMT ?09, pages 33?36, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2009. Expected Dependency Pair Match: Predicting
translation quality with expected syntactic structure.
Machine Translation.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), pages 75?84.
LDC. 2005. Linguistic Data Annotation Spec-
ification: Assessment of Adequacy and Flu-
ency in Translations. Revision 1.5. Tech-
nical report, Linguistic Data Consortium.
http://www.ldc.upenn.edu/Projects/
TIDES/Translation/TransAssess04.pdf.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Official
release of automatic evaluation scores for all submis-
sions, August.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL), pages 241?248.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL).
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of the 20th
International Conference on Computational Linguis-
tics (COLING).
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005.
BLANC: Learning Evaluation Metrics for MT. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing (HLT-EMNLP), pages 740?747.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization, pages
25?32.
Ding Liu and Daniel Gildea. 2006. Stochastic Iter-
ative Alignment for Machine Translation Evaluation.
In Proceedings of the Joint 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics (COLING-ACL), pages 539?546.
465
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of the
2007 Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 41?48.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evaluation.
In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL).
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. An Evaluation Tool for Machine
Translation: Fast Evaluation for MT Research. In Pro-
ceedings of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC).
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-Based Automatic Evalua-
tion for Machine Translation. In Proceedings of SSST,
NAACL-HLT/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation, pages 80?87.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007b. Labelled Dependencies in Machine Transla-
tion Evaluation. In Proceedings of the ACL Workshop
on Statistical Machine Translation, pages 104?111.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2008. Evaluating machine translation with lfg depen-
dencies. Machine Translation, 21(2):95?119.
Karolina Owczarzak. 2009. Depeval(summ):
dependency-based evaluation for automatic sum-
maries. In ACL-IJCNLP ?09: Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
190?198, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Sebastian Pado?, Michael Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Robust machine
translation evaluation with entailment features. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 297?305.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001a.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001b. Bleu: a method for automatic evalu-
ation of machine translation, RC22176. Technical re-
port, IBM T.J. Watson Research Center.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2007.
Reducing Human Assessments of Machine Transla-
tion Quality to Binary Classifiers. In Proceedings of
the 11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation (TMI).
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Applica-
tions for Error Analysis. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
48?55, Prague, Czech Republic, June. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 271?279.
Chris Quirk. 2004. Training a Sentence-Level Machine
Translation Confidence Metric. In Proceedings of the
4th International Conference on Language Resources
and Evaluation (LREC), pages 825?828.
Florence Reeder, Keith Miller, Jennifer Doyon, and John
White. 2001. The Naming of Things and the Confu-
sion of Tongues: an MT Metric. In Proceedings of
the Workshop on MT Evaluation ?Who did what to
whom?? at Machine Translation Summit VIII, pages
55?59.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 223?231.
Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zu-
biaga, and H. Sawaf. 1997. Accelerated DP based
Search for Statistical Translation. In Proceedings of
European Conference on Speech Communication and
Technology.
Stephen Tratz and Eduard Hovy. 2008. Summarization
evaluation using transformed basic elements. In In
Proceedings of TAC-08. Gaithersburg, Maryland.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003a.
Evaluation of machine translation and its evaluation.
In In Proceedings of MT Summit IX, pages 386?393.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003b. Evaluation of Machine Translation and its
Evaluation. In Proceedings of MT SUMMIT IX.
466
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 454?460,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNED: Evaluating Text Similarity Measures without Human Assessments?
Enrique Amigo? ? Julio Gonzalo ? Jesu?s Gime?nez ? Felisa Verdejo?
? UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
? Google, Dublin
jesgim@gmail.com
Abstract
This paper describes the participation of
UNED NLP group in the SEMEVAL 2012 Se-
mantic Textual Similarity task. Our contribu-
tion consists of an unsupervised method, Het-
erogeneity Based Ranking (HBR), to combine
similarity measures. Our runs focus on com-
bining standard similarity measures for Ma-
chine Translation. The Pearson correlation
achieved is outperformed by other systems,
due to the limitation of MT evaluation mea-
sures in the context of this task. However,
the combination of system outputs that partici-
pated in the campaign produces three interest-
ing results: (i) Combining all systems without
considering any kind of human assessments
achieve a similar performance than the best
peers in all test corpora, (ii) combining the 40
less reliable peers in the evaluation campaign
achieves similar results; and (iii) the correla-
tion between peers and HBR predicts, with a
0.94 correlation, the performance of measures
according to human assessments.
1 Introduction
Imagine that we are interested in developing com-
putable measures that estimate the semantic simi-
larity between two sentences. This is the focus of
the STS workshop in which this paper is presented.
In order to optimize the approaches, the organizers
?This work has been partially funded by the Madrid gov-
ernment, grant MA2VICMR (S-2009/TIC- 1542), the Span-
ish government, grant Holopedia (TIN2010-21128-C02-01) and
the European Community?s Seventh Framework Programme
(FP7/ 2007-2013) under grant agreement nr. 288024 (LiMo-
SINe project).
provide a training corpus with human assessments.
The participants must improve their approaches and
select three runs to participate. Unfortunately, we
can not ensure that systems will behave similarly
in both the training and test corpora. For instance,
some Pearson correlations between system achieve-
ments across test corpora in this competition are:
0.61 (MSRpar-MSRvid), 0.34 (MSRvid-SMTeur),
or 0.49 (MSRpar-SMTeur). Therefore, we cannot
expect a high correlation between the system per-
formance in a specific corpus and the test corpora
employed in the competition.
Now, imagine that we have a magic box that,
given a set of similarity measures, is able to pre-
dict which measures will obtain the highest corre-
lation with human assessments without actually re-
quiring those assessments. For instance, suppose
that putting all system outputs in the magic box, we
obtain a 0.94 Pearson correlation between the pre-
diction and the system achievements according to
human assessments, as in Figure 1. The horizontal
axis represents the magic box ouput, and the vertical
axis represents the achievement in the competition.
Each dot represents one system. In this case, we
could decide which system or system combination
to employ for a certain test set.
Is there something like this magic box? The
answer is yes. Indeed, what Figure 1 shows is
the results of an unsupervised method to combine
measures, the Heterogeneity Based Ranking (HBR).
This method is grounded on a generalization of the
heterogeneity property of text evaluation measures
proposed in (Amigo? et al, 2011), which states that
the more a set of measures is heterogeneous, the
454
Figure 1: Correspondence between the magic box infor-
mation and the (unknown) correlation with human assess-
ments, considering all runs in the evaluation campaign.
more a score increase according to all the mea-
sures is reliable. In brief, the HBR method consists
of computing the heterogeneity of the set of mea-
sures (systems) for which a similarity instance (pair
of texts) improves each of the rest of similarity in-
stances in comparison. The result is that HBR tends
to achieve a similar or higher correlation with human
assessments than the single measures. In order to
select the most appropriate single measure, we can
meta-evaluate measures in terms of correlation with
HBR, which is what the previous figure showed.
We participated in the STS evaluation campaign
employing HBR over automatic text evaluation mea-
sures (e.g. ROUGE (Lin, 2004)), which are not actu-
ally designed for this specific problem. For this rea-
son our results were suboptimal. However, accord-
ing to our experiments this method seem highly use-
ful for combining and evaluating current systems.
In this paper, we describe the HBR method and we
present experiments employing the rest of partici-
pant methods as similarity measures.
2 Definitions
2.1 Similarity measures
In (Amigo? et al, 2011) a novel definition of sim-
ilarity is proposed in the context of automatic text
evaluation measures. Here we extend the definition
for text similarity problems in general.
Being ? the universe of texts d, we assume that
a similarity measure, is a function x : ?2 ??
< such that there exists a decomposition function
f : ? ?? {e1..en} (e.g., words or other linguis-
tic units or relationships) satisfying the following
constraints; (i) maximum similarity is achieved only
when the text decomposition resembles exactly the
other text; (ii) adding one element from the second
text increases the similarity; and (iii) removing one
element that does not appear in the second text also
increases the similarity.
f(d1) = f(d2)? x(d1, d2) = 1
(f(d1) = f(d?1) ? {e ? f(d2) \ f(d1)})
? x(d?1, d2) > x(d1, d2)
(f(d1) = f(d?1)? {e ? f(d1) \ f(d2)})
? x(d?1, d2) > x(d1, d2)
According to this definition, a random function,
or the inverse of a similarity function (e.g. 1x(d1d2) ),
do not satisfy the similarity constraints, and there-
fore cannot be considered as similarity measures.
However, this definition covers any kind of overlap-
ping or precision/recall measure over words, syntac-
tic structures or semantic units, which is the case of
most systems here.
Our definition assumes that measures are granu-
lated: they decompose text in a certain amount of
elements (e.g. words, grammatical tags, etc.) which
are the basic representation and comparison units to
estimate textual similarity.
2.2 Heterogeneity
Heterogeneity (Amigo? et al, 2011) represents to
what extent a set of measures differ from each other.
Let us refer to a pair of texts i = (i1, i2) with a
certain degree of similarity to be computed as a sim-
ilarity instance. Then we estimate the Heterogene-
ity H(X ) of a set of similarity measures X as the
probability over similarity instances i = (i1, i2) and
j = (j1, j2) between distinct texts, that there exist
two measures in X that contradict each other. For-
mally:
H(X ) ? Pi1 6=i2
j1 6=j2
(?x, x? ? X|x(i) > x(j) ? x?(j) < x?(i))
where x(i) stands for the similarity, according to
measure x, between the texts i1, i2.
455
3 Proposal: Heterogeneity-Based
Similarity Ranking
The heterogeneity property of text evaluation mea-
sures (in fact, text similarity measures to human ref-
erences) introduced in (Amigo? et al, 2011) states
that the quality difference between two texts is lower
bounded by the heterogeneity of the set of evalua-
tion measures that corroborate the quality increase.
Based on this, we define the Heterogeneity Principle
which is applied to text similarity in general as: the
probability of a real similarity increase between ran-
dom text pairs is correlated with the Heterogeneity
of the set of measures that corroborate this increase:
P (h(i) ? h(j)) ? H({x|x(i) ? x(j)})
where h(i) is the similarity between i1, i2 accord-
ing to human assessments (gold standard). In addi-
tion, the probability is maximal if the heterogeneity
is maximal:
H({x|x(i) ? x(j)}) = 1? P (h(i) ? h(j)) = 1
The first part is derived from the fact that increas-
ing Heterogeneity requires additional diverse mea-
sures corroborating the similarity increase. The di-
rect relationship is the result of assuming that a sim-
ilarity increase according to any aspect is always a
positive evidence of true similarity. In other words,
a positive match between two texts according to any
feature can never be a negative evidence of similar-
ity.
As for the second part, if the heterogeneity of a
measure set is maximal, then the condition of the
heterogeneity definition holds for any pair of dis-
tinct documents (i1 6= i2 and j1 6= j2). Given that
all measures corroborate the similarity increase, the
heterogeneity condition does not hold. Then, the
compared texts in (i1, i2) are not different. There-
fore, we can ensure that P (h(i) ? h(j)) = 1.
The proposal in this paper consists of rank-
ing similarity instances by estimating, for each in-
stance i, the average probability of its texts (i1, i2)
being closer to each other than texts in a different
instance j:
R(i) = Avgj(P (h(i) ? h(j)))
Applying the heterogeneity principle we can esti-
mate this as:
HBRX (i) = Avgj(H({x|x(i) ? x(j)}))
We refer to this ranking function as the Heterogene-
ity Based Ranking (HBR). It satisfies three crucial
properties for a measure combining function:
1. HBR is independent from measure scales and
it does not require relative weighting schemes
between measures. Formally, being f any strict
growing function:
HBRx1..xn(i) = HBRx1..f(xn)(i)
2. HBR is not sensitive to redundant measures:
HBRx1..xn(i) = HBRx1..xn,xn(i)
3. Given a large enough set of similarity
instances, HBR is not sensitive to non-
informative measures. Being xr a random
function such that P (xr(i) > xr(j)) = 12 ,
then:
HBRx1..xn(i) ? HBRx1..xn,xr(i)
The first two properties are trivially satisfied: the
? operator in H and the score comparisons are not af-
fected by redundant measures nor their scales prop-
erties. Regarding the third property, the heterogene-
ity of a set of measures plus a random function xr
is:
H(X ? {xr}) ?
Pi1 6=i2
j1 6=j2
(?x, x? ? X ? {xr}|x(i) > x(j) ? x?(j) < x?(i)) =
H(X ) + (1?H(X )) ?
1
2
=
H(X ) + 1
2
That is, the heterogeneity grows proportionally
when including a random function. Assuming that
the random function corroborates the similarity in-
crease in a half of cases, the result is a proportional
relationship between HBR and HBR with the addi-
tional measure. Note that we need to assume a large
enough amount of data to avoid random effects.
456
4 Official Runs
We have applied the HBR method with excellent
results in different tasks such as Machine Transla-
tion and Summarization evaluation measures, Infor-
mation Retrieval and Document Clustering. How-
ever, we had not previously applied our method to
semantic similarity. Therefore, we decided to ap-
ply directly automatic evaluation measures for Ma-
chine Translation as single similarity measures to be
combined by means of HBR. We have used 64 auto-
matic evaluation measures provided by the ASIYA
Toolkit (Gime?nez and Ma`rquez, 2010)1. This set in-
cludes measures operating at different linguistic lev-
els (lexical, syntactic, and semantic) and includes all
popular measures (BLEU, NIST, GTM, METEOR,
ROUGE, etc.) The similarity formal constraints in
this set of measures is preserved by considering lex-
ical overlap when the target linguistic elements (i.e.
named entities) do not appear in the texts.
We participated with three runs. The first one con-
sisted of selecting the best measure according to hu-
man assessments in the training corpus. It was the
INIST measure (Doddington, 2002). The second run
consisted of selecting the best 34 measures in the
training corpus and combining them with HBR, and
the last run consisted of combining all evaluation
measures with HBR. The heterogeneity of measures
was computed over 1000 samples of similarity in-
stance pairs (pairs of sentences pairs) extracted from
the five test sets. Similarity instances were ranked
over each test set independently.
In essence, the main contribution of these runs is
to corroborate that Machine Translation evaluation
measures are not enough to solve this task. Our runs
appear at the Mean Rank positions 42, 28 and 77.
Apart of this, our results corroborate our main hy-
pothesis: without considering human assessment or
any kind of supervised tunning, combining the mea-
sures with HBR resembles the best measure (INIST)
in the combined measure set. However, when in-
cluding all measures the evaluation result decreases
(rank 77). The reason is that some Machine Trans-
lation evaluation measures do not represent a posi-
tive evidence of semantic similarity in this corpus.
Therefore, the HBR assumptions are not satisfied
and the final correlation achieved is lower. In sum-
1http://www.lsi.upc.edu/ nlp/Asiya
mary, our approach is suitable if we can ensure that
all measures (systems) combined are at least a posi-
tive (high or low) evidence of semantic similarity.
But let us focus on the HBR behavior when com-
bining participant measures, which are specifically
designed to address this problem.
5 Experiment with Participant Systems
5.1 Combining System Outputs
We can confirm empirically in the official results
that all participants runs are positive evidence of se-
mantic similarity. That is, they achieve a correlation
with human assessments higher than 0. Therefore,
the conditions to apply HBR are satisfied. Our goal
now is to resemble the best performance without ac-
cessing human assessments neither from the training
nor the test corpora. Figure 2 illustrates the Pear-
son correlation (averaged across test sets) achieved
by single measures (participants) and all peers com-
bined in an unsupervised manner by HBR (black
column). As the figure shows, HBR results are com-
parable with the best systems appearing in the ninth
position. In addition, Figure 4 shows the differences
over particular test sets between HBR and the best
system. The figure shows that there are not con-
sistent differences between these approaches across
test beds.
The next question is why HBR is not able to im-
prove the best system. Our intuition is that, in this
test set, average quality systems do not contribute
with additional information. That is, the similarity
aspects that the average quality systems are able to
capture are also captured by the best system.
However, the best system within the combined set
is not a theoretical upper bound for HBR. We can
prove it with the following experiment. We apply
HBR considering only the 40 less predictive systems
in the set (the rest of measures are not considered
when computing HBR). Then we compare the re-
sults of HBR regarding the considered single sys-
tems. As Figure 3 shows, HBR improves substan-
tially all single systems achieving the same result
than when combining all systems (0.61). The rea-
son is that all these systems are positive evidences
but they consider partial similarity aspects. But the
most important issue here is that combining the 40
less predictive systems in the evaluation campaign
457
Figure 2: Measures (runs) and HBR sorted by average correlation with human assessments.
Figure 3: 40 less predictive measures (runs) and HBR
sorted by average correlation with human assessments.
is enough to achieve high final scores. This means
that the drawback of these measures as a whole is
not what information is employed but how this in-
formation is scaled and combined. This drawback is
solved by the HBR approach.
In summary, the main conclusion that we can ex-
tract from these results is that, in the absence of hu-
man assessments, HBR ensures a high performance
without the risk derived from employing potentially
biased training corpora or measures based on partial
similarity aspects.
6 An Unsupervised Meta-evaluation
Method
But HBR has an important drawback: its computa-
tional cost, which isO(n4 ?m), being n the number
Figure 4: Average correlation with human assessments
for the best runs and HBR.
of texts involved in the computation and m the num-
ber of measures. The reason is that computing H is
quadratic with the number of texts, and the method
requires to compute H for every pair of texts. In
addition, HBR does not improve the best systems.
However, HBR can be employed as an unsuper-
vised evaluation method. For this, it is enough to
compute the Pearson correlation between runs and
HBR. This is what Figure 1 showed at the beginning
of this article. For each dot (participant run), the
horizontal axis represent the correlation with HBR
(magic box) and the vertical axis represent the cor-
relation with human assessments. This graph has a
Pearson correlation of 0.94 between both variables.
In other words, without accessing human assess-
ments, this method is able to predict the quality of
458
Figure 5: Predicting the quality of measures over a single test set.
textual similarity system with a 0.94 of accuracy in
this test bed.
In this point, we have two options for optimiz-
ing systems. First, we can optimize measures ac-
cording to the results achieved in an annotated train-
ing corpus. The other option consists of considering
the correlation with HBR in the test corpus. In or-
der to compare both approaches we have developed
the following experiment. Given a test corpus t, we
compute the correlation between system scores in t
versus a training corpus t?. This approach emulates
the scenario of training systems over a (training) set
and evaluating over a different (test) set. We also
compute the correlation between system scores in
all corpora vs. the scores in t. Finally, we compute
the correlation between system scores in t and our
predictor in t (which is the correlation system/HBR
across similarity instances in t). This approach em-
ulates the use of HBR as unsupervised optimization
method.
Figure 5 shows the results. The horizontal axis
represents the test set t. The black columns rep-
resent the prediction over HBR in the correspond-
ing test set. The grey columns represent the predic-
tion by using the average correlation across test sets.
The light grey columns represents the prediction us-
ing the correlation with humans in other single test
set. Given that there are five test sets, the figure in-
cludes four grey columns for each test set. The fig-
ure clearly shows the superiority of HBR as measure
quality predictor, even when it does not employ hu-
man assessments.
7 Conclusions
The Heterogeneity Based Ranking provides a mech-
anism to combine similarity measures (systems)
without considering human assessments. Interest-
ingly, the combined measure always improves or
achieves similar results than the best single measure
in the set. The main drawback is its computational
cost. However, the correlation between single mea-
sures and HBR predicts with a high confidence the
accuracy of measures regarding human assessments.
Therefore, HBR is a very useful tool when optimiz-
ing systems, specially when a representative training
corpus is not available. In addition, our results shed
some light on the contribution of measures to the
task. According to our experiments, the less reliable
measures as a whole can produce reliable results if
they are combined according to HBR.
The HBR software is available at
http://nlp.uned.es/?enrique/
References
Enrique Amigo?, Julio Gonzalo, Jesus Gimenez, and Fe-
lisa Verdejo. 2011. Corroborating text evaluation re-
sults with heterogeneous measures. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 455?466, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd Inter-
459
national Conference on Human Language Technology,
pages 138?145.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94):77?86.
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
460
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 36?43,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The Heterogeneity Principle in Evaluation Measures for Automatic
Summarization?
Enrique Amigo? Julio Gonzalo Felisa Verdejo
UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
Abstract
The development of summarization systems
requires reliable similarity (evaluation) mea-
sures that compare system outputs with hu-
man references. A reliable measure should
have correspondence with human judgements.
However, the reliability of measures depends
on the test collection in which the measure
is meta-evaluated; for this reason, it has not
yet been possible to reliably establish which
are the best evaluation measures for automatic
summarization. In this paper, we propose
an unsupervised method called Heterogeneity-
Based Ranking (HBR) that combines summa-
rization evaluation measures without requiring
human assessments. Our empirical results in-
dicate that HBR achieves a similar correspon-
dence with human assessments than the best
single measure for every observed corpus. In
addition, HBR results are more robust across
topics than single measures.
1 Introduction
In general, automatic evaluation metrics for summa-
rization are similarity measures that compare system
outputs with human references. The typical develop-
ment cycle of a summarization system begins with
selecting the most predictive metric. For this, evalu-
ation metrics are compared to each other in terms
?This work has been partially funded by the Madrid gov-
ernment, grant MA2VICMR (S-2009/TIC- 1542), the Spanish
Government, grant Holopedia (TIN2010-21128-C02-01) and
the European Community?s Seventh Framework Programme
(FP7/ 2007-2013) under grant agreement nr. 288024 (LiMo-
SINe project).
of correlation with human judgements. The sec-
ond step consists of tuning the summarization sys-
tem (typically in several iterations) in order to maxi-
mize the scores according to the selected evaluation
measure.
There is a wide set of available measures beyond
the standard ROUGE: for instance, those comparing
basic linguistic elements (Hovy et al, 2005), depen-
dency triples (Owczarzak, 2009) or convolution ker-
nels (Hirao et al, 2005) which reported some relia-
bility improvement with respect to ROUGE in terms
of correlation with human judgements. However,
in practice ROUGE is still the preferred metric of
choice. The main reason is that the superiority of a
measure with respect to other is not easy to demon-
strate: the variability of results across corpora, ref-
erence judgements (Pyramid vs responsiveness) and
correlation criteria (system vs. summary level) is
substantial. In the absence of a clear quality crite-
rion, the de-facto standard is usually the most rea-
sonable choice.
In this paper we rethink the development cy-
cle of summarization systems. Given that the best
measure changes across evaluation scenarios, we
propose using multiple automatic evaluation mea-
sures, together with an unsupervised method to com-
bine measures called Heterogeneity Based Rank-
ing (HBR). This method is grounded on the gen-
eral Heterogeneity property proposed in (Amigo? et
al., 2011), which states that the more a measure
set is heterogeneous, the more a score increase ac-
cording to all the measures simultaneously is reli-
able. In brief, the HBR method consists of com-
puting the heterogeneity of measures for which a
36
system-produced summary improves each of the rest
of summaries in comparison.
Our empirical results indicate that HBR achieves
a similar correspondence with human assessments
than the best single measure for every observed cor-
pus. In addition, HBR results are more robust across
topics than single measures.
2 Definitions
We consider here the definition of similarity mea-
sure proposed in (Amigo? et al, 2011):
Being ? the universe of system outputs (sum-
maries) s and gold-standards (human references) g,
we assume that a similarity measure is a function
x : ?2 ?? < such that there exists a decompo-
sition function f : ? ?? {e1..en} (e.g., words
or other linguistic units or relationships) satisfying
the following constraints; (i) maximum similarity is
achieved only when the summary decomposition re-
sembles exactly the gold standard; (ii) adding one
element from the gold standard increases the simi-
larity; and (iii) removing one element that does not
appear in the gold standard also increases the simi-
larity. Formally:
f(s) = f(g)?? x(s, g) = 1
(f(s) = f(s?) ? {eg ? f(g) \ f(s)}) =?
x(s, g) > x(s?, g)
(f(s) = f(s?)? {e?g ? f(s) \ f(g)}) =?
x(s, g) > x(s?, g)
This definition excludes random functions, or the
inverse of any similarity function (e.g. 1f(s) ). It
covers, however, any overlapping or precision/recall
measure over words, n-grams, syntactic structures or
any kind of semantic unit. In the rest of the paper,
given that the gold standard g in summary evaluation
is usually fixed, we will simplify the notation saying
that x(s, g) ? x(s).
We consider also the definition of heterogeneity
of a measure set proposed in (Amigo? et al, 2011):
The heterogeneity H(X ) of a set of measures X is
defined as, given two summaries s and s? such that
g 6= s 6= s? 6= g (g is the reference text), the proba-
bility that there exists two measures that contradict
each other.
H(X ) ?
Ps,s? 6=g(?x, x
? ? X/x(s) > x(s?) ? x?(s) < x?(s?))
3 Proposal
The proposal in this paper is grounded on the hetero-
geneity property of evaluation measures introduced
in (Amigo? et al, 2011). This property establishes
a relationship between heterogeneity and reliability
of measures. However, this work does not provide
any method to evaluate and rank summaries given a
set of available automatic evaluation measures. We
now reformulate the heterogeneity property in order
to define a method to combine measures and rank
systems.
3.1 Heterogeneity Property Reformulation
The heterogeneity property of evaluation measures
introduced in (Amigo? et al, 2011) states that, as-
suming that measures are based on similarity to hu-
man references, the real quality difference between
two texts is lower bounded by the heterogeneity of
measures that corroborate the quality increase. We
reformulate this property in the following way:
Given a set of automatic evaluation measures
based on similarity to human references, the prob-
ability of a quality increase in summaries is corre-
lated with the heterogeneity of the set of measures
that corroborate this increase:
P (Q(s) ? Q(s?)) ? H({x|x(s) ? x(s?)})
where Q(s) is the quality of the summary s accord-
ing to human assessments. In addition, the proba-
bility is maximal if the heterogeneity is maximal:
H({x|x(s) ? x(s?)}) = 1? P (Q(s) ? Q(s?)) = 1
The first part is derived from the fact that
increasing heterogeneity requires additional di-
verse measures corroborating the similarity increase
(H({x|x(s) ? x(s?)}))). The correlation is the re-
sult of assuming that a similarity increase accord-
ing to any aspect is always a positive evidence of
true similarity to human references. In other words,
37
a positive match between the automatic summary
and the human references, according to any feature,
should never be a negative evidence of quality.
As for the second part, if the heterogeneity of a
measure set X is maximal, then the condition of
the heterogeneity definition (?x, x? ? X .x(s) >
x(s?) ? x?(s) < x?(s?)) holds for any pair of sum-
maries that are different from the human references.
Given that all measures in X corroborate the simi-
larity increase (X = {x|x(s) ? x(s?)}), the hetero-
geneity condition does not hold. Then, at least one
of the evaluated summaries is not different from the
human reference and we can ensure that P (Q(s) ?
Q(s?)) = 1.
3.2 The Heterogeneity Based Ranking
The main goal in summarization evaluation is rank-
ing systems according to their quality. This can be
seen as estimating, for each system-produced sum-
mary s, the average probability of being ?better?
than other summaries:
Rank(s) = Avgs?(P (Q(s) ? Q(s
?)))
Applying the reformulated heterogeneity property
we can estimate this as:
HBRX (s) = Avgs?(H({x|x(s) ? x(s
?)}))
We refer to this ranking function as the Heterogene-
ity Based Ranking (HBR). It satisfies three crucial
properties for a measure combining function. Note
that, assuming that any similarity measure over hu-
man references represents a positive evidence of
quality, the measure combining function must be
at least robust with respect to redundant or random
measures:
1. HBR is independent from measure scales and
it does not require relative weighting schemes
between measures. Formally, being f any strict
growing function:
HBRx1..xn(s) = HBRx1..f(xn)(s)
2. HBR is not sensitive to redundant measures:
HBRx1..xn(s) = HBRx1..xn,xn(s)
3. Given a large enough set of similarity
instances, HBR is not sensitive to non-
informative measures. In other words, being
xr a random function such that P (xr(s) >
xr(s?)) = 12 , then:
HBRx1..xn(s) ? HBRx1..xn,xr(s)
The first two properties are trivially satisfied: the
? operator in H and the score comparisons are not af-
fected by redundant measures nor their scale proper-
ties. Regarding the third property, the Heterogeneity
of a set of measures plus a random function xr is:
H(X ? {xr}) ?
Ps,s?(?x, x
? ? X?{xr}|x(s) > x(s
?)?x?(s) < x?(s?)) =
H(X ) + (1?H(X )) ?
1
2
=
H(X ) + 1
2
That is, the Heterogeneity grows proportionally
when including a random function. Assuming that
the random function corroborates the similarity in-
crease in a half of cases, the result is a proportional
relationship between HBR and HBR with the addi-
tional measure. Note that we need to assume a large
enough amount of data to avoid random effects.
4 Experimental Setting
4.1 Test Bed
We have used the AS test collections used in the
DUC 2005 and DUC 2006 evaluation campaigns1
(Dang, 2005; Dang, 2006). The task was to gener-
ate a question focused summary of 250 words from a
set of 25-50 documents to a complex question. Sum-
maries were evaluated according to several criteria.
Here, we will consider the responsiveness judge-
ments, in which the quality score was an integer be-
tween 1 and 5. See Table 1 for a brief numerical
description of these test beds.
In order to check the measure combining method,
we have employed standard variants of ROUGE
(Lin, 2004), including the reversed precision version
for each variant 2. We have considered also the F
1http://duc.nist.gov/
2Note that the original ROUGE measures are oriented to re-
call
38
DUC 2005 DUC 2006
#human-references 3-4 3-4
#systems 32 35
#system-outputs-assessed 32 35
#system-outputs 50 50
#outputs-assessed per-system 50 50
Table 1: Test collections from 2005 and 2006 DUC evaluation campaigns used in our experiments.
measure between recall and precision oriented mea-
sures. Finally, our measure set includes also BE or
Basic Elements (Hovy et al, 2006).
4.2 Meta-evaluation criterion
The traditional way of meta-evaluating measures
consists of computing the Pearson correlation be-
tween measure scores and quality human assess-
ments. But the main goal of automatic evaluation
metrics is not exactly to predict the real quality of
systems; rather than this, their core mission is de-
tecting system outputs that improve the baseline sys-
tem in each development cycle. Therefore, the issue
is to what extent a quality increase between two sys-
tem outputs is reflected by the output ranking pro-
duced by the measure.
According to this perspective, we propose meta-
evaluating measures in terms of an extended version
of AUC (Area Under the Curve). AUC can be seen
as the probability of observing a score increase when
observing a real quality increase between two sys-
tem outputs (Fawcett, 2006).
AUC(x) = P (x(s) > x(s?)|Q(s) > Q(s?))
In order to customize this measure to our scenario,
two special cases must be handled:
(i) For cases in which both summaries obtain the
same value, we assume that the measure rewards
each instance with equal probability. That is, if
x(s) = x(s?),P (x(s) > x(s?)|Q(s) > Q(s?)) = 12 .
(ii) Given that in the AS evaluation scenarios there
are multiple quality levels, we still apply the same
probabilistic AUC definition, considering pairs of
summaries in which one of them achieves more
quality than the other according to human assessors.
Figure 1: Correlation between probability of quality in-
crease and Heterogeneity of measures that corroborate
the increase
5 Experiments
5.1 Measure Heterogeneity vs. Quality
Increase
We hypothesize that the probability of a real similar-
ity increase to human references (as stated by human
assessments) is directly related to the heterogeneity
of the set of measures that confirm such increase. In
order to verify whether this principle holds in prac-
tice, we need to measure the correlation between
both variables. Therefore, we compute, for each pair
of summaries in the same topic the heterogeneity of
the set of measures that corroborate a score increase
between both:
H({x ? X |x(s) ? x(s?)})
The Heterogeneity has been estimated by counting
cases over 10,000 samples (pairs of summaries) in
both corpora.
Then, we have sorted each pair ?s, s?? according
to its related heterogeneity. We have divided the re-
sulting rank in 100 intervals of the same size. For
39
Figure 2: AUC comparison between HBR and single measures in DUC 2005 and DUC 2006 corpora.
each interval, we have computed the average hetero-
geneity of the set and the probability of real quality
increase (P (Q(s) ? Q(s?))).
Figure 1 displays the results. Note that the direct
relation between both variables is clear: a key for
predicting a real quality increase is how heteroge-
neous is the set of measures corroborating it.
5.2 HBR vs. Single Measures
In the following experiment, we compute HBR and
we compare the resulting AUC with that of single
measures. The heterogeneity of measures is esti-
mated over samples in both corpora (DUC 2005 and
DUC 2006), and HBR ranking is computed to rank
summaries for each topic. For the meta-evaluation,
the AUC probability is computed over summary
pairs from the same topic.
Figure 2 shows the resulting AUC values of sin-
gle measures and HBR. The black bar represents the
HBR approach. The light grey bars are ROUGE
measures oriented to precision. The dark grey bars
include ROUGE variants oriented to recall and F,
and the measure BE. As the Figure shows, recall-
based measures achieve in general higher AUC val-
ues than precision-oriented measures. The HBR
measure combination appears near the top. It is im-
proved by some measures such as ROUGE SU4 R,
although the difference is not statistically significant
(p = 0.36 for a t-test between ROUGE SU4 R and
HBR, for instance). HBR improves the 10 worst
single measures with statistical significance (p <
0.025).
5.3 Robustness
The next question is why using HBR instead of the
?best? measure (ROUGE-SU4-R in this case). As
we mentioned, the reliability of measures can vary
across scenarios. For instance, in DUC scenarios
most systems are extractive, and exploit the maxi-
mum size allowed in the evaluation campaign guide-
lines. Therefore, the precision over long n-grams is
not crucial, given that the grammaticality of sum-
maries is ensured. In this scenario the recall over
words or short n-grams over human references is a
clear signal of quality. But we can not ensure that
these characteristics will be kept in other corpora, or
even when evaluating new kind of summarizers with
the same corpora.
Our hypothesis is that, given that HBR resembles
the best measure without using human assessments,
it should have a more stable performance in situa-
tions where the best measure changes.
In order to check empirically this assertion, we
have investigated the lower bound performance of
measures in our test collections. First, we have
ranked measures for each topic according to their
AUC values; Then, we have computed, for every
measure, its rank regarding the rest of measures
(scaled from 0 to 1). Finally, we average each mea-
sure across the 10% of topics in which the measure
40
Figure 3: Average rank of measures over the 10% of topics with lowest results for the measure.
gets the worst ranks. Figure 3 shows the results: the
worst performance of HBR across topics is better
than the worst performance of any single measure.
This confirms that the combination of measures us-
ing HBR is indeed more robust than any measure in
isolation.
5.4 Consistent vs. Inconsistent Topics
The Heterogeneity property is grounded on the as-
sumption that any similarity criteria represents a
positive evidence of similarity to human references.
In general, we can assert that this assumption holds
over a large enough random set of texts. However,
depending on the distribution of summaries in the
corpus, this assumption may not always hold. For
instance, we can assume that, given all possible sum-
maries, improving the word precision with respect to
the gold standard can never be a negative evidence
of quality. However, for a certain topic, it could hap-
pen that the worst summaries are also the shortest,
and have high precision and low recall. In this case,
precision-based similarity could be correlated with
negative quality. Let us refer to these as inconsis-
tent topics vs. consistent topics. In terms of AUC,
a measure represents a negative evidence of quality
when AUC is lower than 0.5. Our test collections
contain 100 topics, out of which 25 are inconsis-
tent (i.e., at least one measure achieves AUC values
lower than 0.5) and 75 are consistent with respect to
our measure set (all measures achieve AUC values
higher than 0.5).
Figure ?? illustrates the AUC achieved by mea-
sures when inconsistent topics are excluded. As with
the full set of topics, recall-based measures achieve
higher AUC values than precision-based measures;
but, in this case, HBR appears at the top of the rank-
ing. This result illustrates that (i) HBR behaves par-
ticularly well when our assumptions on similarity
measures hold in the corpus; and that (ii) in prac-
tice, there may be topics for which our assumptions
do not hold.
6 Conclusions
In this paper, we have confirmed that the heterogene-
ity of a set of summary evaluation measures is cor-
related with the probability of finding a real quality
improvement when all measures corroborate it. The
HBR measure combination method is based on this
principle, which is grounded on the assumption that
any similarity increase with respect to human refer-
ences is a positive signal of quality.
Our empirical results indicate that the Hetero-
geneity Based Ranking achieves a reliability simi-
lar to the best single measure in the set. In addi-
41
Figure 4: AUC comparison between HBR and single measures in corpora DUC2005 and DUC 2006 over topics in
which all measures achieve AUC bigger than 0.5.
tion, HBR results are more robust across topics than
single measures. Our experiments also suggest that
HBR behaves particularly well when the assump-
tions of the heterogeneity property holds in the cor-
pus. These assumptions are conditioned by the dis-
tribution of summaries in the corpus (in particular,
on the amount and variability of the summaries that
are compared with human references), and in prac-
tice 25% of the topics in our test collections do not
satisfy them for our set of measures.
The HBR (Heterogeneity Based Ranking) method
proposed in this paper does not represent the ?best
automatic evaluation measure?. Rather than this, it
promotes the development of new measures. What
HBR does is solving ?or at least palliating? the prob-
lem of reliability variance of measures across test
beds. According to our analysis, our practical rec-
ommendations for system refinement are:
1. Compile an heterogenous set of measures, cov-
ering multiple linguistic aspects (such as n-
gram precision, recall, basic linguistic struc-
tures, etc.).
2. Considering the summarization scenario, dis-
card measures that might not always represent
a positive evidence of quality. For instance,
if very short summaries are allowed (e.g. one
word) and they are very frequent in the set of
system outputs to be compared to each other,
precision oriented measures may violate HBR
assumptions.
3. Evaluate automatically your new summariza-
tion approach within this corpus according to
the HBR method.
Our priority for future work is now developing
a reference benchmark containing an heterogenous
set of summaries, human references and measures
satisfying the heterogeneity assumptions and cover-
ing multiple summarization scenarios where differ-
ent measures play different roles.
The HBR software is available at
http://nlp.uned.es/?enrique/
References
Enrique Amigo?, Julio Gonzalo, Jesus Gimenez, and Fe-
lisa Verdejo. 2011. Corroborating text evaluation re-
sults with heterogeneous measures. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 455?466, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Hoa Trang Dang. 2005. Overview of DUC 2005. In Pro-
ceedings of the 2005 Document Understanding Work-
shop.
Hoa Trang Dang. 2006. Overview of DUC 2006. In Pro-
ceedings of the 2006 Document Understanding Work-
shop.
42
Tom Fawcett. 2006. An introduction to roc analysis.
Pattern Recogn. Lett., 27:861?874, June.
Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.
2005. Kernel-based approach for automatic evaluation
of natural language generation technologies: Applica-
tion to automatic summarization. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 145?152, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Eduard Hovy, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating DUC 2005 using Basic Elements. Proceed-
ings of Document Understanding Conference (DUC).
Vancouver, B.C., Canada.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated Summarization Evalu-
ation with Basic Elements. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation (LREC), pages 899?902.
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Karolina Owczarzak. 2009. Depeval(summ):
dependency-based evaluation for automatic sum-
maries. In ACL-IJCNLP ?09: Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
190?198, Morristown, NJ, USA. Association for Com-
putational Linguistics.
43
