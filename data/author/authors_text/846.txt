Bagging and Boost ing a Treebank Parser 
John C. Henderson 
The MITRE Corporat ion  
202 Bur l ington  Road 
Bedford ,  MA 01730 
jhndrsn@mit re .o rg  
Eric Br i l l  
M ic roso f t  Research  
1 M ic roso f t  Way 
Redmond,  WA 98052 
br i l l@microso f t . com 
Abstract  
Bagging and boosting, two effective machine learn- 
ing techniques, are applied to natural anguage pars- 
ing. Experiments using these techniques with a 
trainable statistical parser are described. The best 
resulting system provides roughly as large of a gain 
in F-measure as doubling the corpus size. Error 
analysis of the result of the boosting technique re- 
veals some inconsistent annotations in the Penn 
Treebank, suggesting a semi-automatic method for 
finding inconsistent treebank annotations. 
1 Int roduct ion 
Henderson and Brill (1999) showed that independent 
human research efforts produce parsers that can be 
combined for an overall boost in accuracy. Finding 
an ensemble of parsers designed to complement each 
other is clearly desirable. The parsers would need 
to be the result of a unified research effort, though, 
in which the errors made by one parser are targeted 
with priority by the developer of another parser. 
A set of five parsers which each achieve only 40% 
exact sentence accuracy would be extremely valu- 
able if they made errors in such a way that at least 
two of the five were correct on any given sentence 
(and the others abstained or were wrong in different 
ways). 100% sentence accuracy could be achieved 
by selecting the hypothesis that was proposed by 
the two parsers that agreed completely. 
In this paper, the task of automatically creating 
complementary parsers is separated from the task of 
creating a single parser. This facilitates tudy of the 
ensemble creation techniques in isolation. The result 
is a method for increasing parsing performance by 
creating an ensemble of parsers, each produced from 
data using the same parser induction algorithm. 
2 Bagging and Pars ing 
2.1 Background 
The work of Efron and Tibshirani (1993) enabled 
Breiman's refinement and application of their tech- 
niques for machine learning (Breiman, 1996). His 
technique is called bagging, short for "bootstrap ag- 
gregating". In brief, bootstrap techniques and bag- 
ging in particular educe the systematic biases many 
estimation techniques introduce by aggregating es- 
timates made from randomly drawn representative 
resamplings of those datasets. 
Bagging attempts to find a set of classifiers which 
are consistent with the training data, different from 
each other, and distributed such that the aggregate 
sample distribution approaches the distribution of 
samples in the training set. 
Algorithm: Bagging Predictors 
(Breiman, 1996) (1) 
Given: training set  = {(yi ,x~), i  E {1. . .m}} 
drawn from the set A of possible training sets where 
Yi is the label for example x~, classification i duction 
algorithm q2 : A --* ? with classification algorithm 
Ce ? and ? :  X- -~Y.  
1. Create k bootstrap replicates o f / :  by sampling 
m items from E with replacement. Call them 
L1. . .Lk .  
2. For each j e {1. . .k},  Let Cj = ~(? j )  be the 
classifier induced using Lj as the training set. 
3. If Y is a discrete set, then for each x~ observed 
in the test set, yi = mode(?j (x i ) . . .  Cj(x~)). y~ 
is the value predicted by the most predictors, 
the majority vote. 
2.2 Bagging for Parsing 
An algorithm that applies the technique of bagging 
to parsing is given in Algorithm 2. Previous work on 
combining independent parsers is leveraged to pro- 
duce the combined parser. The rest of the algorithm 
is a straightforward transformation of bagging for 
classifiers. Exploratory work in this vein was de- 
scribed by HajiC et al (1999). 
Algorithm: Bagging A Parser (2) 
Given: A corpus (again as a funct ion)C :S?T ~ N, 
S is the set of possible sentences, and T is the set 
of trees, with size m = \[C\] = ~s, t  C(s, t) and parser 
induction algorithm g. 
1. Draw k bootstrap replicates C1 ... Ck of C each 
containing m samples of (s,t) pairs randomly 
34 
picked from the domain of C according to the 
distribution D(s , t )  = C(s,t)/\]C\]. Each boot- 
strap replicate is a bag of samples, where each 
sample in a bag is drawn randomly with replace- 
ment from the bag corresponding to C. 
2. Create parser f~ = g(Ci) for each i. 
3. Given a novel sentence 8test E Ctest ,  combine 
the collection of hypotheses ti = fi(Stest) us- 
ing the unweighted constituent voting scheme 
of Henderson and Brill (1999). 
2.3 Exper iment  
The training set for these experiments was sections 
01-21 of the Penn Treebank (Marcus et al, 1993). 
The test set was section 23. The parser induction 
algorithm used in all of the experiments in this pa- 
per was a distribution of Collins's model 2 parser 
(Collins, 1997). All comparisons made below refer 
to results we obtained using Collins's parser. 
The results for bagging are shown in Figure 2 and 
Table 1. The row of figures are (from left-to-right) 
training set F-measure ~,test set F-measure, percent 
perfectly parsed sentences in training set, and per- 
cent perfectly parsed sentences in test set. An en- 
semble of bags was produced one bag at a time. In 
the table, the In i t ia l  row shows the performance 
achieved when the ensemble contained only one bag, 
F inal(X)  shows the performance when the ensem- 
ble contained X bags, BestF gives the performance 
of the ensemble size that gave the best F-measure 
score. Tra inBestF and TestBestF give the test set 
performance for the ensemble size that performed 
the best on the training and test sets, respectively. 
On the training set al of the accuracy measures 
are improved over the original parser, and on the 
test set there is clear improvement in precision and 
recall. The improvement on exact sentence accuracy 
for the test set is significant, but only marginally so. 
The overall gain achieved on the test set by bag- 
ging was 0.8 units of F-measure, but because the 
entire corpus is not used in each bag the initial per- 
formance is approximately 0.2 units below the best 
previously reported result. The net gain using this 
technique is 0.6 units of F-measure. 
3 Boosting 
3.1 Background 
The AdaBoost algorithm was presented by Fre- 
und and Schapire in 1996 (Freund and Schapire, 
1996; Freund and Schapire, 1997) and has become a
widely-known successful method in machine learn- 
ing. The AdaBoost algorithm imposes one con- 
straint on its underlying learner: it may abstain from 
making predictions about labels of some samples, 
1This is the balanced version ofF-measure, where precision 
and recall are weighted equally. 
but it must consistently be able to get more than 
50?-/o accuracy on the samples for which it commits 
to a decision. That accuracy is measured accord- 
ing to the distribution describing the importance of 
samples that it is given. The learner must be able 
to get more correct samples than incorrect samples 
by mass of importance on those that it labels. This 
statement of the restriction comes from Schapire and 
Singer's study (1998). It is called the weak learning 
criterion. 
Schapire and Singer (1998) extended AdaBoost by 
describing how to choose the hypothesis mixing co- 
efficients in certain circumstances and how to incor- 
porate a general notion of confidence scores. They 
also provided a better characterization f its theo- 
retical performance. The version of AdaBoost used 
in their work is shown in Algorithm 3, as it is the 
version that most amenable to parsing. 
Algorithm: AdaBoost 
(F reund and Schapire, 1997") (3) 
Given: Training set /: as in bagging, except yi E 
{-1,  1 } is the label for example xi. Initial uniform 
distribution D1 (i) = 1/m. Number of iterations, T. 
Counter t = 1. tI,, ?~, and ? are as in Bagging. 
1. Create Lt by randomly choosing with replace- 
ment m samples from L: using distribution Dt. 
2. Classifier induction: Ct ~- ~(Lt)  
3. Choose at E IR. 
4. Adjust and normalize the distribution. Zt is a 
normalization coefficient. 
1 
D, + , ( i) = -~- Dt ( i ) exp(-c~tYiCt( xi ) ) 
5. Increment t. Quit if t > T. 
6. Repeat from step 1. 
7. The final hypothesis i
~)boost(:g) ~- sign Z ~t?,(x) 
t 
The value of at should generally be chosen to min- 
imize 
Z Dt (i) exp(-a~ Yi Ct (x,)) 
i 
in order to minimize the expected per-sample train- 
ing error of the ensemble, which Schapire and Singer 
show can be concisely expressed by I-\] Zt. They also 
give several examples for how to pick an appropriate 
a, and selection generally depends on the possible 
outputs of the underlying learner. 
Boosting has been used in a few NLP systems. 
Haruno et al (1998) used boosting to produce more 
accurate classifiers which were embedded as control 
35
Set Instance P R F Gain Exact Gain 
Training Original Parser 96.25 96.31 96.28 NA 64.7 NA 
Initial 93.61 93.63 93.62 0.00 55.5 0.0 
BestF(15) 96.16 95.86 96.01 2.39 62.1 6.6 
Final(15) 96.16 95.86 96.01 2.39 62.1 6.6 
Test Original Parser 88.73 88.54 88.63 NA 34.9 NA 
Initial 88.43 88.34 88.38 0.00 33.3 0.0 
TrainBestF(15) 89.54 88.80 89.17 0.79 34.6 1.3 
TestBestF(13) 89.55 88.84 89.19 0.81 34.7 1.4 
Final(15) 89.54 88.80 89.17 0.79 34.6 1.3 
Table 1: Bagging the Treebank 
mechanisms of a parser for Japanese. The creators 
of AdaBoost used it to perform text classification 
(Schapire and Singer, 2000). Abney et al (1999) 
performed part-of-speech tagging and prepositional 
phrase attachment using AdaBoost as a core compo- 
nent. They found they could achieve accuracies on 
both tasks that were competitive with the state of 
the art. As a side effect, they found that inspecting 
the samples that were consistently given the most 
weight during boosting revealed some faulty anno- 
tations in the corpus. In all of these systems, Ad- 
aBoost has been used as a traditional classification 
system. 
3.2 Boosting for Parsing 
Our goal is to recast boosting for parsing while con- 
sidering a parsing system as the embedded learner. 
The formulation is given in Algorithm 4. The in- 
tuition behind the additive form is that the weight 
placed on a sentence should be the sum of the weight 
we would like to place on its constituents. The 
weight on constituents that are predicted incorrectly 
are adjusted by a factor of 1 in contrast o a factor 
of ~ for those that are predicted incorrectly. 
Algorithm: Boosting A Parser (4) 
Given corpus C with size m = IC I = ~s.~C(s, t )  
and parser induction algorithm g. Initial uniform 
distribution Dl(i) = 1/m. Number of iterations, T. 
Counter t = 1. 
1. Create Ct by randomly choosing with replace- 
ment m samples from C using distribution Dr. 
2. Create parser ft ~ g(Ct). 
3. Choose at E R (described below). 
4. Adjust and normalize the distribution. Zt is 
a normalization coefficient. For all i, let parse 
tree ~-~' -- ft(s,). Let ~(T,c) be a function indi- 
cating that c is in parse tree r, and ITI is the 
number of constituents in tree T. T(s) is the set 
of constituents that are found in the reference 
or hypothesized annotation for s. 
Dt+l ( i )  : 
1 - , 
cET(s i )  
5. Increment . Quit if t > T. 
6. Repeat from step 1. 
7. The final hypothesis is computed by combin- 
ing the individual constituents. Each parser Ct 
in the ensemble gets a vote with weight at for 
the constituents they predict. Precisely those 
constituents with weight strictly larger than 
1 ~--~t at are put into the final hypothesis. 
A potential constituent can be considered correct 
if it is predicted in the hypothesis and it exists in 
the reference, or it is not predicted and it is not in 
the reference. Potential constituents that do not ap- 
pear in the hypothesis or the reference should not 
make a big contribution to the accuracy computa- 
tion. There are many such potential constituents, 
and if we were maximizing a function that treated 
getting them incorrect the same as getting a con- 
stituent that appears in the reference correct, we 
would most likely decide not to predict any con- 
stituents. 
Our model of constituent accuracy is thus sim- 
ple. Each prediction correctly made over T(s) will be 
given equal weight. That is, correctly hypothesizing 
a constituent in the reference will give us one point, 
but a precision or recall error will cause us to miss 
one point. Constituent accuracy is then a/(a+b+c),  
where a is the number of constituents correctly hy- 
pothesized, b is the number of precision errors and c 
is the number of recall errors. 
In Equation 1, a computation of aca as described 
is shown. 
Otca = 
D( i )  
i c6T(si) 
D( i )  
i cCT(s i )  
Boosting algorithms were developed that at- 
tempted to maximize F-measure, precision, and re- 
call by varying the computation of a, giving results 
too numerous to include here. The algorithm given 
here performed the best of the lot, but was only 
marginally better for some metrics. 
(1: 
36 
Set Instance P R F Gain Exact Gain 
Training Original Parser 96.25 96.31 96.28 NA 64.7 NA 
Initial 93.54 93.61 93.58 0.00 54.8 0.0 
BestF(15) 96.21 95.79 96.00 2.42 57.3 2.5 
Final(15) 96.21 95.79 96.00 2.42 57.3 2.5 
Test Original Parser 88.73 88.54 88.63 NA 34.9 NA 
Initial 88.05 88.09 88.07 0.00 33.3 0.0 
TrainBestF(15) 89.37 88.32 88.84 0.77 33.0 -0.3 
TestBestF(14) 89.39 88.41 88.90 0.83 33.4 0.1 
Final(15) 89.37 88.32 88.84 0.77 33.0 -0.3 
Table 2: Boosting the Treebank 
3.3 Experiment 
The experimental results for boosting are shown in 
Figure 3 and Table 2. There is a large plateau in 
performance from iterations 5 through 12. Because 
of their low accuracy and high degree of specializa- 
tion, the parsers produced in these iterations had 
little weight during voting and had little effect on 
the cumulative decision making. 
As in the bagging experiment, it appears that 
there would be more precision and recall gain to 
be had by creating a larger ensemble. In both the 
bagging and boosting experiments ime and resource 
constraints dictated our ensemble size. 
In the table we see that the boosting algorithm 
equaled bagging's test set gains in precision and re- 
call. The In i t ia l  performance for boosting was 
lower, though. We cannot explain this, and expect 
it is due to unfortunate resampling of the data dur- 
ing the first iteration of boosting. Exact sentence 
accuracy, though, was not significantly improved on 
the test set. 
Overall, we prefer bagging to boosting for this 
problem when raw performance is the goal. There 
are side effects of boosting that are useful in other 
respects, though, which we explore in Section 4.2. 
3.3.1 Weak Learning Criterion Violations 
It was hypothesized in the course of investigating the 
failures of the boosting algorithm that the parser in- 
duction system did not satisfy the weak learning cri- 
terion. It was noted that the distribution of boosting 
weights were more skewed in later iterations. Inspec- 
tion of the sentences that were getting much mass 
placed upon them revealed that their weight was be- 
ing boosted in every iteration. The hypothesis was 
that the parser was simply unable to learn them. 
39832 parsers were built to test this, one for each 
sentence in the training set. Each of these parsers 
was trained on only a single sentence 2 and evaluated 
on the same sentence. It was discovered that a full 
4764 (11.2%) of these sentences could not be parsed 
completely correctly by the parsing system. 
2The sentence was replicated 10 times to avoid threshold- 
ing effects in the learner. 
3.3.2 Corpus Trimming 
In order to evaluate how well boosting worked with 
a learner that better satisfied the weak learning cri- 
terion, the boosting experiment was run again on 
the Treebank minus the troublesome sentences de- 
scribed above. The results are in Table 3. This 
dataset produces a larger gain in comparison to the 
results using the entire Treebank. The initial ac- 
curacy, however, is lower. We hypothesize that the 
boosting algorithm did perform better here, but the 
parser induction system was learning useful informa- 
tion in those sentences that it could not memorize 
(e.g. lexical information) that was successfully ap- 
plied to the test set. 
In this manner we managed to clean our dataset o 
the point that the parser could learn each sentence 
in isolation. The corpus-makers cannot necessarily 
be blamed for the sentences that could not be mem- 
orized. All that can be said about those sentences 
is that for better or worse, the parser's model would 
not accommodate hem. 
4 Corpus Analys is  
4.1 Noisy Corpus: Empirical Investigation 
To acquire experimental evidence of noisy data, dis- 
tributions that were used during boosting the sta- 
ble corpus were inspected. The distribution was ex- 
pected to be skewed if there was noise in the data, or 
be uniform with slight fluctuations if it fit the data 
well. 
We see how the boosting weight distribution 
changes in Figure 1. The individual curves are in- 
dexed by boosting iteration in the key of the figure. 
This training run used a corpus of 5000 sentences. 
The sentences are ranked by the weight they are 
given in the distribution, and sorted in decreasing or- 
der by weight along the x-axis. The distribution was 
smoothed by putting samples into equal weight bins, 
and reporting the average mass of samples in the bin 
as the y-coordinate. Each curve on this graph cor- 
responds to a boosting iteration. We used 1000 bins 
for this graph, and a log scale on the x-axis. Since 
there were 5000 samples, all samples initially had a 
y-value of 0.0002. 
37
Set Instance P R F Gain Exact Gain 
Training Original Parser 96.25 96.31 96.28 NA 64.7 NA 
Initial 94.60 94.68 94.64 0.00 62.2 0.0 
BestF(8) 97.38 97.00 97.19 2.55 63.1 0.9 
Final(15) 97.00 96.17 96.58 1.94 55.0 -7.2 
Test Original Parser 88.73 88.54 88.63 NA 34.9 NA 
Initial 87.43 87.21 87.32 0.00 32.6 0.0 
TrainBestF(8) 89.12 87.62 88.36 1.04 32.8 0.2 
TestBestF(6) 89.07 87.77 88.42 1.10 32.9 0.4 
Final(15) 89.18 87.19 88.18 0.86 31.7 -0.8 
Table 3: Boosting the Stable Corpus 
0.05 
0.045 
0.04 
0035 
0.03 
~' o.o2s I 
0,02 
0.015 
0.01 
0.005 
0 
, , . . . , i " . 
2 . . . . . . .  
3 . . . . . . .  
4 ? 
5 . . . . .  
6 . . . .  
7 . . . . . . . .  
8 . . . . . . .  
9 . . . . . . . . .  
1 0 - -  
11  . . . . . . .  
i 
Figure 1: Weight Change During Boosting 
Notice first that the left endpoints of the lines 
move from bottom to top in order of boosting it- 
eration. The distribution becomes monotonically 
more skewed as boosting progresses. Secondly we 
see by the last iteration that most of the weight is 
concentrated on less than 100 samples. This graph 
shows behavior consistent with noise in the corpus 
on which the boosting algorithm is focusing. 
4.2 T reebank  Inconsistencies 
There are sentences in the corpus that can be learned 
by the parser induction algorithm in isolation but 
not in concert because they contain conflicting in- 
formation. Finding these sentences leads to a better 
understanding of the quality of our corpus, and gives 
an idea for where improvements in annotation qual- 
ity can be made. Abney et al (1999) showed a 
similar corpus analysis technique for part of speech 
tagging and prepositional phrase tagging, but for 
parsing we must remove errors introduced by the 
parser as we did in Section 3.3.2 before questioning 
the corpus quality. A particular class of errors, in- 
consistencies, can then be investigated. Inconsistent 
annotations are those that appear plausible in iso- 
lation, but which conflict with annotation decisions 
made elsewhere in the corpus. 
In Figure 5 we show a set of trees selected from 
within the top 100 most heavily weighted trees at 
the end of 15 iterations of boosting the stable cor- 
pus.Collins's parser induction system is able to learn 
to produce any one of these structures in isolation, 
but the presence of conflicting information in differ- 
ent sentences prevents it from achieving 100% accu- 
racy on the set. 
5 Training Corpus Size Effects 
We suspect our best parser diversification techniques 
gives performance gain approximately equal to dou- 
bling the size of the training set. While this cannot 
be directly tested without hiring more annotators, 
an expected performance bound for a larger train- 
ing set can be produced by extrapolating from how 
well the parser performs using smaller training sets. 
There are two characteristics of training curves for 
large corpora that can provide such a bound: train- 
ing curves generally increase monotonically in the 
absence of over-training, and their first derivatives 
generally decrease monotonically. 
Set Sentences P R 
50 
100 
500 
1000 
5000 
10000 
20000 
39832 
50 
100 
500 
1000 
5000 
10000 
20000 
39832 
F Exact 
67.57 32.15 43.57 5.4 
69,03 56.23 61.98 8,5 
78,12 75.46 76.77 18,2 
81.36 80.70 81.03 22.9 
87.28 87.09 87.19 34.1 
89.74 89.56 89.65 41.0 
92.42 92.40 92.41 50.3 
96.25 96.31 96.28 64.7 
68.13 32.24 43.76 4.7 
69.90 54.19 61.05 7.8 
78.72 75.33 76.99 19.1 
81.61 80.68 81.14 22.2 
86.03 85.43 85.73 28.6 
87.29 86.81 87.05 30.8 
87.99 87.87 87.93 32.7 
88.73 88.54 88.63 34.9 
Table 4: Effects of Varying Training Corpus Size 
The training curves we present in Figure 4 and Ta- 
ble 4 suggest hat roughly doubling the corpus size 
38 
in the range of interest (between 10000 and 40000 
sentences) gives a test set F-measure gain of approx- 
imately 0.70. 
Bagging achieved significant gains of approxi- 
mately 0.60 over the best reported previous F- 
measure without adding any new data. In this re- 
spect, these techniques how promise for making 
performance gains on large corpora without adding 
more data or new parsers. 
6 Conc lus ion  
We have shown two methods, bagging and boosting, 
for automatically creating ensembles of parsers that 
produce better parses than any individual in the en- 
semble. Neither of the algorithms exploit any spe- 
cialized knowledge of the underlying parser induc- 
tion algorithm, and the data used in creating the 
ensembles has been restricted to a single common 
training set to avoid issues of training data quantity 
affecting the outcome. 
Our best bagging system performed consistently 
well on all metrics, including exact sentence accu- 
racy. It resulted in a statistically significant F- 
measure gain of 0.6 over the performance of the base- 
line parser. That baseline system is the best known 
Treebank parser. This gain compares favorably with 
a bound on potential gain from increasing the corpus 
size. 
Even though it is computationally expensive to 
create and evaluate a small (15-30) ensemble of 
parsers, the cost is far outweighed by the opportu- 
nity cost of hiring humans to annotate 40000 more 
sentences. The economic basis for using ensemble 
methods will continue to improve with the increasing 
value (performance p r price) of modern hardware. 
Our boosting system, although dominated by the 
bagging system, also performed significantly better 
than the best previously known individual parsing 
result. We have shown how to exploit the distri- 
bution created as a side-effect of the boosting al- 
gorithm to uncover inconsistencies in the training 
corpus. A semi-automated technique for doing this 
as well as examples from the Treebank that are in- 
consistently annotated were presented. Perhaps the 
biggest advantage ofthis technique is that it requires 
no a priori notion of how the inconsistencies can be 
characterized. 
7 Acknowledgments  
We would like to thank Michael Collins for enabling 
all of this research by providing us with his parser 
and helpful comments. 
This work was funded by NSF grant IRI-9502312. 
The views expressed in this paper are those of the 
authors and do not necessarily reflect the views of 
the MITRE Corporation. This work was done while 
both authors were at Johns Hopkins University. 
Re ferences  
Steven Abney, Robert E. Schapire, and Yoram 
Singer. 1999. Boosting applied to tagging and PP 
attachment. In Proceedings of the Joint SIGDAT 
Conference on Empirical Methods in Natural Lan- 
guage Processing and Very Large Corpora, pages 
38-45, College Park, Maryland. 
L. Breiman. 1996. Bagging predictors. In Machine 
Learning, volume 24, pages 123-140. 
Michael Collins. 1997. Three generative, lexicalised 
models for statistical parsing. In Proceedings of 
the Annual Meeting of the Association for Com- 
putational Linguistics, volume 35, Madrid. 
B. Efron and R. Tibshirani. 1993. An Introduction 
to the Bootstrap. Chapman and Hall. 
Y. Freund and R.E. Schapire. 1996. Experiments 
with a new boosting algorithm. In Proceedings of 
the International Conference on Machine Learn- 
ing. 
Y. Freund and R.E. Schapire. 1997. A decision- 
theoretic generalization f on-line learning and an 
application to boosting. Journal of Computer and 
Systems Sciences, 55(1):119-139, Aug. 
Jan Haji~, E. Brill, M. Collins, B. Hladka, D. Jones, 
C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann, 
and D. Zeman. 1999. Core natural language 
processing technology applicable to multiple lan- 
guages. Prague Bulletin of Mathematical Linguis- 
tics, 70. 
Masahiko Haruno, Satoshi Shirai, and Yoshifumi 
Ooyama. 1998. Using decision trees to construct 
a practical parser. In Proceedings of the 36th 
Annual Meeting of the Association for Compu- 
tational Linguistics and 17th International Con- 
ference on Computational Linguistics, volume 1, 
pages 505-511, Montreal, Canada. 
John C. Henderson and Eric Brill. 1999. Exploiting 
diversity in natural language processing: Combin- 
ing parsers. In Proceedings of the Fourth Confer- 
ence on Empirical Methods in Natural Language 
Processing, College Park, Maryland. 
Mitchell P. Marcus, Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building a large 
annotated corpus of english: The Penn Treebank. 
Computational Linguistics, 19(2):313-330. 
Robert E. Schapire and Yoram Singer. 1998. Im- 
proved boosting algorithms using confidence-rated 
predictions. In Proceedings of the Eleventh An- 
nual Conference on Computational Learning The- 
ory, pages 80-91. 
Robert E. Schapire and Yoram Singer. 2000. Boos- 
texter: A boosting-based system for text catego- 
rization. Machine Learning, 39(2/3):1-34, May. 
To appear. 
"4Q 39
\ 
j J  
i 
i ?/ 
'...\]~'.,.. 
AN 
b~ 
.=. 
O O 
O 
b~ 
..= 
b~ 
40
, , z _~ 
/~ ~j 
~_. ~g 
0 
O 
41 
Mitigating the Paucity-of-Data Problem: Exploring the
Effect of Training Corpus Size on Classifier Performance
for Natural Language Processing
Michele Banko and Eric Brill
Microsoft Research
1 Microsoft Way
Redmond, WA 98052 USA
{mbanko, brill}@microsoft.com
ABSTRACT
In this paper, we discuss experiments applying machine learning
techniques to the task of confusion set disambiguation, using three
orders of magnitude more training data than has previously been
used for any disambiguation-in-string-context problem. In an
attempt to determine when current learning methods will cease to
benefit from additional training data, we analyze residual errors
made by learners when issues of sparse data have been
significantly mitigated. Finally, in the context of our results, we
discuss possible directions for the empirical natural language
research community.
Keywords
Learning curves, data scaling, very large corpora, natural language
disambiguation.
1. INTRODUCTION
A significant amount of work in empirical natural language
processing involves developing and refining machine learning
techniques to automatically extract linguistic knowledge from on-
line text corpora. While the number of learning variants for
various problems has been increasing, the size of training sets
such learning algorithms use has remained essentially unchanged.
For instance, for the much-studied problems of part of speech
tagging, base noun phrase labeling and parsing, the Penn
Treebank, first released in 1992, remains the de facto training
corpus. The average training corpus size reported in papers
published in the ACL-sponsored Workshop on Very Large
Corpora was essentially unchanged from the 1995 proceedings to
the 2000 proceedings. While the amount of available on-line text
has been growing at an amazing rate over the last five years (by
some estimations, there are currently over 500 billion readily
accessible words on the web), the size of training corpora used by
our field has remained static.
Confusable word set disambiguation, the problem of choosing the
correct use of a word given a set of words with which it is
commonly confused, (e.g. {to, too, two}, {your, you?re}), is a
prototypical problem in NLP. At some level, this task is identical
to many other natural language problems, including word sense
disambiguation, determining lexical features such as pronoun case
and determiner number for machine translation, part of speech
tagging, named entity labeling, spelling correction, and some
formulations of skeletal parsing. All of these problems involve
disambiguating from a relatively small set of tokens based upon a
string context. Of these disambiguation problems, lexical
confusables possess the fortunate property that supervised training
data is free, since the differences between members of a confusion
set are surface-apparent within a set of well-written text.
To date, all of the papers published on the topic of confusion set
disambiguation have used training sets for supervised learning of
less than one million words. The same is true for most if not all of
the other disambiguation-in-string-context problems. In this
paper we explore what happens when significantly larger training
corpora are used. Our results suggest that it may make sense for
the field to concentrate considerably more effort into enlarging
our training corpora and addressing scalability issues, rather than
continuing to explore different learning methods applied to the
relatively small extant training corpora.
2. PREVIOUS WORK
2.1 Confusion Set Disambiguation
Several methods have been presented for confusion set
disambiguation. The more recent set of techniques includes
multiplicative weight-update algorithms [4], latent semantic
analysis [7], transformation-based learning [8], differential
grammars [10], decision lists [12], and a variety of Bayesian
classifiers [2,3,5]. In all of these papers, the problem is
formulated as follows: Given a specific confusion set (e.g. {to,
two, too}), all occurrences of confusion set members in the test
set are replaced by some marker. Then everywhere the system
sees this marker, it must decide which member of the confusion
set to choose. Most learners that have been applied to this
problem use as features the words and part of speech tags
appearing within a fixed window, as well as collocations
surrounding the ambiguity site; these are essentially the same
features as those used for the other disambiguation-in-string-
context problems.
2.2 Learning Curves for NLP
A number of learning curve studies have been carried out for
different natural language tasks. Ratnaparkhi [12] shows a
learning curve for maximum-entropy parsing, for up to roughly
one million words of training data; performance appears to be
asymptoting when most of the training set is used. Henderson [6]
showed similar results across a collection of parsers.
Figure 1 shows a learning curve we generated for our task of
word-confusable disambiguation, in which we plot test
classification accuracy as a function of training corpus size using
a version of winnow, the best-performing learner reported to date
for this well-studied task [4]. This curve was generated by training
on successive portions of the 1-million word Brown corpus and
then testing on 1-million words of Wall Street Journal text for
performance averaged over 10 confusion sets. The curve might
lead one to believe that only minor gains are to be had by
increasing the size of training corpora past 1 million words.
While all of these studies indicate that there is likely some (but
perhaps limited) performance benefit to be obtained from
increasing training set size, they have been carried out only on
relatively small training corpora. The potential impact to be felt by
increasing the amount of training data by any signifcant order has
yet to be studied.
0.70
0.72
0.74
0.76
0.78
0.80
0.82
100,000 400,000 700,000 1,000,000
Training Corpus Size (words)
Te
st
Ac
cu
ra
cy
Figure 1: An Initial Learning Curve for Confusable
Disambiguation
3. EXPERIMENTS
This work attempts to address two questions ? at what point will
learners cease to benefit from additional data, and what is the
nature of the errors which remain at that point. The first question
impacts how best to devote resources in order to improve natural
language technology. If there is still much to be gained from
additional data, we should think hard about ways to effectively
increase the available training data for problems of interest. The
second question allows us to study failures due to inherent
weaknesses in learning methods and features rather than failures
due to insufficient data.
Since annotated training data is essentially free for the problem of
confusion set disambiguation, we decided to explore learning
curves for this problem for various machine learning algorithms,
and then analyze residual errors when the learners are trained on
all available data. The learners we used were memory-based
learning, winnow, perceptron,1 transformation-based learning, and
decision trees. All learners used identical features2 and were used
out-of-the-box, with no parameter tuning. Since our point is not
to compare learners we have refrained from identifying the
learners in the results below.
We collected a 1-billion-word training corpus from a variety of
English texts, including news articles, scientific abstracts,
government transcripts, literature and other varied forms of prose.
Using this collection, which is three orders of magnitude greater
than the largest training corpus previously used for this task, we
trained the five learners and tested on a set of 1 million words of
Wall Street Journal text.3
In Figure 2 we show learning curves for each learner, for up to
one billion words of training data.4 Each point in the graph
reflects the average performance of a learner over ten different
confusion sets which are listed in Table 1. Interestingly, even out
to a billion words, the curves appear to be log-linear. Note that
the worst learner trained on approximately 20 million words
outperforms the best learner trained on 1 million words. We see
that for the problem of confusable disambiguation, none of our
learners is close to asymptoting in performance when trained on
the one million word training corpus commonly employed within
the field.
Table 1: Confusion Sets
{accept, except} {principal, principle}
{affect, effect} {then, than}
{among, between} {their, there}
{its, it?s} {weather, whether}
{peace, piece} {your, you?re}
The graph in Figure 2 demonstrates that for word confusables, we
can build a system that considerably outperforms the current best
results using an incredibly simplistic learner with just slightly
more training data. In the graph, Learner 1 corresponds to a
trivial memory-based learner. This learner simply keeps track of
all <wi-1, wi+1>, < wi-1> and <wi+1> counts for all occurrences of
the confusables in the training set. Given a test set instance, the
learner will first check if it has seen <wi-1,wi+1> in the training set.
If so, it chooses the confusable word most frequently observed
with this tuple. Otherwise, the learner backs off to check for the
frequency of <wi-1>; if this also was not seen then it will back off
to <wi+1>, and lastly, to the most frequently observed confusion-
1 Thanks to Dan Roth for making both Winnow and Perceptron
available.
2 We used the standard feature set for this problem. For details
see [4].
3 The training set contained no text from WSJ.
4 Learner 5 could not be run on more than 100 million words of
training data.
set member as computed from the training corpus. Note that with
10 million words of training data, this simple learner outperforms
all other learners trained on 1 million words.
Many papers in empirical natural language processing involve
showing that a particular system (only slightly) outperforms
others on one of the popular standard tasks. These comparisons
are made from very small training corpora, typically less than a
million words. We have no reason to believe that any
comparative conclusions drawn on one million words will hold
when we finally scale up to larger training corpora. For instance,
our simple memory based learner, which appears to be among the
best performers at a million words, is the worst performer at a
billion. The learner that performs the worst on a million words of
training data significantly improves with more data.
Of course, we are fortunate in that labeled training data is easy to
locate for confusion set disambiguation. For many natural
language tasks, clearly this will not be the case. This reality has
sparked interest in methods for combining supervised and
unsupervised learning as a way to utilize the relatively small
amount of available annotated data along with much larger
collections of unannotated data [1,9]. However, it is as yet
unclear whether these methods are effective other than in cases
where we have relatively small amounts of annotated data
available.
4. RESIDUAL ERRORS
After eliminating errors arising from sparse data and examining
the residual errors the learners make when trained on a billion
words, we can begin to understand inherent weaknesses in
ourlearning algorithms and feature sets. Sparse data problems can
always be reduced by buying additional data; the remaining
problems truly require technological advances to resolve them.
We manually examined a sample of errors classifiers made when
trained on one billion words and classified them into one of four
categories: strongly misleading features, ambiguous context,
sparse context and corpus error. In the paragraphs that follow, we
define the various error types, and discuss what problems remain
even after a substantial decrease in the number of errors attributed
to the problem of sparse data.
Strongly Misleading Features
Errors arising from strongly misleading features occur when
features which are strongly associated with one class appear in the
context of another. For instance, in attempting to characterize the
feature set of weather (vs. its commonly-confused set member
whether), according to the canonical feature space used for this
problem we typically expect terms associated with atmospheric
conditions, temperature or natural phenomena to favor use of
weather as opposed to whether. Below is an example which
illustrates that such strong cues are not always sufficient to
accurately disambiguate between these confusables. In such cases,
a method for better weighing features based upon their syntactic
context, as opposed to using a simple bag-of-words model, may
be needed.
Example: On a sunny day whether she swims or not depends on
the temperature of the water.
0.75
0.80
0.85
0.90
0.95
1.00
1 10 100 1000
Sizeof TrainingCorpus (Millions of Words)
Test Accuracy
Learner 1
Learner 2
Learner 3
Learner 4
Learner 5
Figure 2. Learning Curves for Confusable Disambiguation
Ambiguous Context
Errors can also arise from ambiguous contexts. Such errors are
made when feature sets derived from shallow local contexts are
not sufficient to disambiguate among members of a confusable
set. Long-range, complex dependencies, deep semantic
understanding or pragmatics may be required in order to draw a
distinction among classes. Included in this class of problems are
so-called ?garden-path? sentences, in which ambiguity causes an
incorrect parse of the sentence to be internally constructed by the
reader until a certain indicator forces a revision of the sentence
structure.
Example 1: It's like you're king of the hill.
Example 2: The transportation and distribution departments
evaluate weather reports at least four times a day to determine if
delivery schedules should be modified.
Sparse Context
Errors can also be a result of sparse contexts. In such cases, an
informative term appears, but the term was not seen in the training
corpus. Sparse contexts differ from ambiguous contexts in that
with more data, such cases are potentially solvable using the
current feature set. Sparse context problems may also be lessened
by attributing informative lexical features to a word via clustering
or other analysis.
Example: It's baseball's only team-owned spring training site.
Corpus Error
Corpus errors are attributed to cases in which the test corpus
contains an incorrect use of a confusable word, resulting in
incorrectly evaluating the classification made by a learner. In a
well-edited test corpus such as the Wall Street Journal, errors of
this nature will be minimal.
Example: If they don't find oil, its going to be quite a letdown.
Table 2 shows the distribution of error types found after learning
with a 1-billion-word corpus. Specifically, the sample of errors
studied included instances that one particular learner, winnow,
incorrectly classified when trained on one billion words. It is
interesting that more than half of the errors were attributed to
sparse context. Such errors could potentially be corrected were
the learner to be trained on an even larger training corpus, or if
other methods such as clustering were used.
The ambiguous context errors are cases in which the feature space
currently utilized by the learners is not sufficient for
disambiguation; hence, simply adding more data will not help.
Table 2: Distribution of Error Types
Error Type Percent Observed
Ambiguous Context 42%
Sparse Context 57%
Misleading Features 0%
Corpus Error 1%
5. A BILLION-WORD TREEBANK?
Our experiments demonstrate that for confusion set
disambiguation, system performance improves with more data, up
to at least one billion words. Is it feasible to think of ever having
a billion-word Treebank to use as training material for tagging,
parsing, named entity recognition, and other applications?
Perhaps not, but let us run through some numbers.
To be concrete, assume we want a billion words annotated with
part of speech tags at the same level of accuracy as the original
million word corpus.5 If we train a tagger on the existing corpus,
the na?ve approach would be to have a person look at every single
tag in the corpus, decide whether it is correct, and make a change
if it is not. In the extreme, this means somebody has to look at
one billion tags. Assume our automatic tagger has an accuracy of
95% and that with reasonable tools, a person can verify at the rate
of 5 seconds per tag and correct at the rate of 15 seconds per tag.
This works out to an average of 5*.95 + 15*.05 = 5.5 seconds
spent per tag, for a total of 1.5 million hours to tag a billion
words. Assuming the human tagger incurs a cost of $10/hour, and
assuming the annotation takes place after startup costs due to
development of an annotation system have been accounted for, we
are faced with $15 million in labor costs. Given the cost and labor
requirements, this clearly is not feasible. But now assume that we
could do perfect error identification, using sample selection
techniques. In other words, we could first run a tagger over the
billion-word corpus and using sample selection, identify all and
only the errors made by the tagger. If the tagger is 95% accurate,
we now only have to examine 5% of the corpus, at a correction
cost of 15 seconds per tag. This would reduce the labor cost to $2
million for tagging a billion words. Next, assume we had a way
of clustering errors such that correcting one tag on average had
the effect of correcting 10. This reduces the total labor cost to
$200k to annotate a billion words, or $20k to annotate 100
million. Suppose we are off by an order of magnitude; then with
the proper technology in place it might cost $200k in labor to
annotate 100 million additional words.
As a result of the hypothetical analysis above, it is not absolutely
infeasible to think about manually annotating significantly larger
corpora. Given the clear benefit of additional annotated data, we
should think seriously about developing tools and algorithms that
would allow us to efficiently annotate orders of magnitude more
data than what is currently available.
6. CONCLUSIONS
We have presented learning curves for a particular natural
language disambiguation problem, confusion set disambiguation,
training with more than a thousand times more data than had
previously been used for this problem. We were able significantly
reduce the error rate, compared to the best system trained on the
standard training set size, simply by adding more training data.
5 We assume an annotated corpus such as the Penn Treebank
already exists, and our task is to significantly grow it.
Therefore, we are only taking into account the marginal cost of
additional annotated data, not start-up costs such as style
manual design.
We see that even out to a billion words the learners continue to
benefit from additional training data.
It is worth exploring next whether emphasizing the acquisition of
larger training corpora might be the easiest route to improved
performance for other natural language problems as well.
7. REFERENCES
[1] Brill, E. Unsupervised Learning of Disambiguation Rules
for Part of Speech Tagging. In Natural Language Processing
Using Very Large Corpora, 1999.
[2] Gale, W. A., Church, K. W., and Yarowsky, D. (1993). A
method for disambiguating word senses in a large corpus.
Computers and the Humanities, 26:415--439.
[3] Golding, A. R. (1995). A Bayesian hybrid method for
context-sensitive spelling correction. In Proc. 3rd Workshop
on Very Large Corpora, Boston, MA.
[4] Golding, A. R. and Roth, D. (1999), A Winnow-Based
Approach to Context-Sensitive Spelling Correction. Machine
Learning, 34:107--130.
[5] Golding, A. R. and Schabes, Y. (1996). Combining trigram-
based and feature-based methods for context-sensitive
spelling correction. In Proc. 34th Annual Meeting of the
Association for Computational Linguistics, Santa Cruz, CA.
[6] Henderson, J. Exploiting Diversity for Natural Language
Parsing. PhD thesis, Johns Hopkins University, August 1999.
[7] Jones, M. P. and Martin, J. H. (1997). Contextual spelling
correction using latent semantic analysis. In Proc. 5th
Conference on Applied Natural Language Processing,
Washington, DC.
[8] Mangu, L. and Brill, E. (1997). Automatic rule acquisition
for spelling correction. In Proc. 14th International
Conference on Machine Learning. Morgan Kaufmann.
[9] Nigam, K, McCallum, A, Thrun, S and Mitchell, T. Text
Classification from Labeled and Unlabeled Documents using
EM. Machine Learning. 39(2/3). pp. 103-134. 2000.
[10] Powers, D. (1997). Learning and application of differential
grammars. In Proc. Meeting of the ACL Special Interest
Group in Natural Language Learning, Madrid.
[11] Ratnaparkhi, Adwait. (1999) Learning to Parse Natural
Language with Maximum Entropy Models. Machine
Learning, 34, 151-175.
[12] Yarowsky, D. (1994). Decision lists for lexical ambiguity
resolution: Application to accent restoration in Spanish and
French. In Proc. 32nd Annual Meeting of the Association for
Computational Linguistics, Las Cruces, NM.
Automatic Question Answering: Beyond the Factoid 
 
 
Radu Soricut 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292, USA 
radu@isi.edu 
Eric Brill 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
brill@microsoft.com  
Abstract 
In this paper we describe and evaluate a Ques-
tion Answering system that goes beyond an-
swering factoid questions. We focus on FAQ-
like questions and answers, and build our sys-
tem around a noisy-channel architecture which 
exploits both a language model for answers 
and a transformation model for an-
swer/question terms, trained on a corpus of 1 
million question/answer pairs collected from 
the Web. 
1 Introduction 
The Question Answering (QA) task has received a great 
deal of attention from the Computational Linguistics 
research community in the last few years (e.g., Text RE-
trieval Conference TREC 2001-2003). The definition of 
the task, however, is generally restricted to answering 
factoid questions: questions for which a complete answer 
can be given in 50 bytes or less, which is roughly a few 
words. Even with this limitation in place, factoid ques-
tion answering is by no means an easy task. The chal-
lenges posed by answering factoid question have been 
addressed using a large variety of techniques, such as 
question parsing (Hovy et al, 2001; Moldovan et al, 
2002), question-type determination (Brill et al, 2001; 
Ittycheraih and Roukos, 2002;   Hovy et al, 2001; 
Moldovan et al, 2002), WordNet exploitation (Hovy et 
al., 2001; Pasca and Harabagiu, 2001; Prager et al, 
2001), Web exploitation (Brill et al, 2001; Kwok et al, 
2001), noisy-channel transformations (Echihabi and 
Marcu, 2003), semantic analysis (Xu et al, 2002; Hovy 
et al, 2001; Moldovan et al, 2002), and inferencing 
(Moldovan et al, 2002). 
 The obvious limitation of any factoid QA system is 
that many questions that people want answers for are not 
factoid questions. It is also frequently the case that non-
factoid questions are the ones for which answers cannot 
as readily be found by simply using a good search en-
gine. It follows that there is a good economic incentive 
in moving the QA task to a more general level: it is 
likely that a system able to answer complex questions of 
the type people generally and/or frequently ask has 
greater potential impact than one restricted to answering 
only factoid questions. A natural move is to recast the 
question answering task to handling questions people 
frequently ask or want answers for, as seen in Frequently 
Asked Questions (FAQ) lists. These questions are some-
times factoid questions (such as, ?What is Scotland's 
national costume??), but in general are more complex 
questions (such as, ?How does a film qualify for an 
Academy Award??, which requires an answer along the 
following lines: ?A feature film must screen in a Los 
Angeles County theater in 35 or 70mm or in a 24-frame 
progressive scan digital format suitable for exhibiting in 
existing commercial digital cinema sites for paid admis-
sion for seven consecutive days. The seven day run must 
begin before midnight, December 31, of the qualifying 
year.  [?]?). 
 In this paper, we make a first attempt towards solv-
ing a QA problem more generic than factoid QA, for 
which there are no restrictions on the type of questions 
that are handled, and there is no assumption that the an-
swers to be provided are factoids. In our solution to this 
problem we employ learning mechanisms for question-
answer transformations (Agichtein et al, 2001; Radev et 
al., 2001), and also exploit large document collections 
such as the Web for finding answers (Brill et al, 2001; 
Kwok et al, 2001). We build our QA system around a 
noisy-channel architecture which exploits both a lan-
guage model for answers and a transformation model for 
answer/question terms, trained on a corpus of 1 million 
question/answer pairs collected from the Web. Our 
evaluations show that our system achieves reasonable 
performance in terms of answer accuracy for a large va-
riety of complex, non-factoid questions.   
2 Beyond Factoid Question Answering 
One of the first challenges to be faced in automatic ques-
tion answering is the lexical and stylistic gap between 
the question string and the answer string. For factoid 
questions, these gaps are usually bridged by question 
reformulations, from simple rewrites (Brill et al, 2001), 
to more sophisticated paraphrases (Hermjakob et al, 
2001), to question-to-answer translations (Radev et al, 
2001). We ran several preliminary trials using various 
question reformulation techniques. We found out that in 
general, when complex questions are involved, reformu-
lating the question (using either simple rewrites or ques-
tion-answer term translations) more often hurts the 
performance than improves on it.  
 Another widely used technique in factoid QA is 
sentence parsing, along with question-type determina-
tion. As mentioned by Hovy et al (2001), their hierar-
chical QA typology contains 79 nodes, which in many 
cases can be even further differentiated.   While we ac-
knowledge that QA typologies and hierarchical question 
types have the potential to be extremely useful beyond 
factoid QA, the volume of work involved is likely to 
exceed by orders of magnitude the one involved in the 
existing factoid QA typologies. We postpone such work 
for future endeavors. 
 The techniques we propose for handling our ex-
tended QA task are less linguistically motivated and 
more statistically driven. In order to have access to the 
right statistics, we first build a question-answer pair 
training corpus by mining FAQ pages from the Web, as 
described in Section 3. Instead of sentence parsing, we 
devise a statistical chunker that is used to transform a 
question into a phrase-based query (see Section 4). After 
a search engine uses the formulated query to return the N 
most relevant documents from the Web, an answer to the 
given question is found by computing an answer lan-
guage model probability (indicating how similar the pro-
posed answer is to answers seen in the training corpus), 
and an answer/question translation model probability 
(indicating how similar the proposed answer/question 
pair is to pairs seen in the training corpus). In Section 5 
we describe the evaluations we performed in order to 
assess our system?s performance, while in Section 6 we 
analyze some of the issues that negatively affected our 
system?s performance.  
3 A Question-Answer Corpus for FAQs 
In order to employ the learning mechanisms described in 
the previous section, we first need to build a large train-
ing corpus consisting of question-answer pairs of a broad 
lexical coverage. Previous work using FAQs as a source 
for finding an appropriate answer (Burke et al, 1996) or 
for learning lexical correlations (Berger et al, 2000) 
focused on using the publicly available Usenet FAQ 
collection and other non-public FAQ collections, and 
reportedly worked with an order of thousands of ques-
tion-answer pairs. 
 Our approach to question/answer pair collection 
takes a different path. If one poses the simple query 
?FAQ? to an existing search engine, one can observe that 
roughly 85% of the returned URL strings corresponding 
to genuine FAQ pages contain the substring ?faq?, while 
virtually all of the URLs that contain the substring ?faq? 
are genuine FAQ pages. It follows that, if one has access 
to a large collection of the Web?s existent URLs, a sim-
ple pattern-matching for ?faq? on these URLs will have 
a recall close to 85% and precision close to 100% on 
returning FAQ URLs from those available in the collec-
tion. Our URL collection contains approximately 1 bil-
lion URLs, and using this technique we extracted 
roughly 2.7 million URLs containing the (uncased) 
string ?faq?, which amounts to roughly 2.3 million FAQ 
URLs to be used for collecting question/answer pairs. 
 The collected FAQ pages displayed a variety of for-
mats and presentations. It seems that the variety of ways 
questions and answers are usually listed in FAQ pages 
does not allow for a simple high-precision high-recall 
solution for extracting question/answer pairs: if one 
assumes that only certain templates are used when 
presenting FAQ lists, one can obtain clean ques-
tion/answer pairs at the cost of losing many other such 
pairs (which happen to be presented in different tem-
plates); on the other hand, assuming very loose con-
straints on the way information is presented on such 
pages, one can obtain a bountiful set of question/answer 
pairs, plus other pairs that do not qualify as such. We 
settled for a two-step approach: a first recall-oriented 
pass based on universal indicators such as punctuation 
and lexical cues allowed us to retrieve most of the ques-
tion/answer pairs, along with other noise data; a second 
precision-oriented pass used several filters, such as lan-
guage identification, length constrains, and lexical cues 
to reduce the level of noise of the question/answer pair 
corpus. Using this method, we were able to collect a total 
of roughly 1 million question/answer pairs, exceeding by 
orders of magnitude the amount of data previously used 
for learning question/answer statistics.      
4 A QA System Architecture 
The architecure of our QA system is presented in Figure 
1. There are 4 separate modules that handle various 
stages in the system?s pipeline: the first module is called 
Question2Query, in which questions posed in natural 
language are transformed into phrase-based queries be-
fore being handed down to the SearchEngine module. 
The second module is an Information Retrieval engine 
which takes a query as input and returns a list of docu-
ments deemed to be relevant to the query in a sorted 
manner. A third module, called Filter, is in charge of 
filtering out the returned list of documents, in order to 
provide acceptable input to the next module. The forth 
module, AnswerExtraction, analyzes the content pre-
sented and chooses the text fragment deemed to be the 
best answer to the posed question. 
 
Figure 1: The QA system architecture 
 
 This architecture allows us to flexibly test for vari-
ous changes in the pipeline and evaluate their overall 
effect. We present next detailed descriptions of how each 
module works, and outline several choices that present 
themselves as acceptable options to be evaluated. 
4.1 The Question2Query Module 
A query is defined to be a keyword-based string that 
users are expected to feed as input to a search engine. 
Such a string is often thought of as a representation for a 
user?s ?information need?, and being proficient in ex-
pressing one?s ?need? in such terms is one of the key 
points in successfully using a search engine. A natural 
language-posed question can be thought of as such a 
query. It has the advantage that it forces the user to pay 
more attention to formulating the ?information need? 
(and not typing the first keywords that come to mind). It 
has the disadvantage that it contains not only the key-
words a search engine normally expects, but also a lot of 
extraneous ?details? as part of its syntactic and discourse 
constraints, plus an inherently underspecified unit-
segmentation problem, which can all confuse the search 
engine.  
 To counterbalance some of these disadvantages, we 
build a statistical chunker that uses a dynamic program-
ming algorithm to chunk the question into 
chunks/phrases. The chunker is trained on the answer 
side of the Training corpus in order to learn 2 and 3-
word collocations, defined using the likelihood ratio of 
Dunning (1993).  Note that we are chunking the question 
using answer-side statistics, precisely as a measure for 
bridging the stylistic gap between questions and answers.  
  Our chunker uses the extracted collocation statistics 
to make an optimal chunking using a Dijkstra-style dy-
namic programming algorithm. In Figure 2 we present 
an example of the results returned by our statistical 
chunker. Important cues such as ?differ from? and 
?herbal medications? are presented as phrases to the 
search engine, therefore increasing the recall of the 
search. Note that, unlike a segmentation offered by a 
parser (Hermjakob et al, 2001), our phrases are not nec-
essarily syntactic constituents. A statistics-based chunker 
also has the advantage that it can be used ?as-is? for 
question segmentation in languages other than English, 
provided training data (i.e., plain written text) is avail-
able. 
 
Figure 2: Question segmentation into query using a 
statistical chunker  
4.2 The SearchEngine Module 
This module consists of a configurable interface with 
available off-the-shelf search engines. It currently sup-
ports MSNSearch and Google. Switching from one 
search engine to another allowed us to measure the im-
pact of the IR engine on the QA task. 
4.3 The Filter Module 
This module is in charge of providing the AnswerExtrac-
tion module with the content of the pages returned by the 
search engine, after certain filtering steps. One first step 
is to reduce the volume of pages returned to only a man-
ageable amount. We implement this step as choosing to 
return the first N hits provided by the search engine. 
Other filtering steps performed by the Filter Module 
include tokenization and segmentation of text into sen-
tences. 
 One more filtering step was needed for evaluation 
purposes only: because both our training and test data 
were collected from the Web (using the procedure de-
scribed in Section 3), there was a good chance that ask-
ing a question previously collected returned its already 
available answer, thus optimistically biasing our evalua-
tion. The Filter Module therefore had access to the refer-
ence answers for the test questions as well, and ensured 
that, if the reference answer matched a string in some 
retrieved page, that page was discarded. Moreover, we 
found that slight variations of the same answer could 
defeat the purpose of the string-matching check. For the 
purpose of our evaluation, we considered that if the 
question/reference answer pair had a string of 10 words 
or more identical with a string in some retrieved page, 
that page was discarded as well. Note that, outside the 
Question2Query 
Module 
Q Search Engine 
Module 
Filter 
Module 
Answer Extraction 
Module 
A 
Query 
Documents
Answer 
List 
 Training 
Corpus 
    Web 
Query 
How do herbal medications differ from  
conventional drugs? 
 
"How do" "herbal medications" "differ from"  
"conventional" "drugs" 
evaluation procedure, the string-matching filtering step 
is not needed, and our system?s performance can only 
increase by removing it. 
4.4 The AnswerExtraction Module 
Authors of previous work on statistical approaches to 
answer finding (Berger et al, 2000) emphasized the need 
to ?bridge the lexical chasm? between the question terms 
and the answer terms. Berger et al showed that tech-
niques that did not bridge the lexical chasm were likely 
to perform worse than techniques that did.  
 For comparison purposes, we consider two different 
algorithms for our AnswerExtraction module: one that 
does not bridge the lexical chasm, based on N-gram co-
occurrences between the question terms and the answer 
terms; and one that attempts to bridge the lexical chasm 
using Statistical Machine Translation inspired techniques 
(Brown et al, 1993) in order to find the best answer for a 
given question. 
 For both algorithms, each 3 consecutive sentences 
from the documents provided by the Filter module form 
a potential answer. The choice of 3 sentences comes 
from the average number of sentences in the answers 
from our training corpus. The choice of consecutiveness 
comes from the empirical observation that answers built 
up from consecutive sentences tend to be more coherent 
and contain more non-redundant information than an-
swers built up from non-consecutive sentences. 
4.4.1 N-gram Co-Occurrence Statistics for Answer 
Extraction  
N-gram co-occurrence statistics have been successfully 
used in automatic evaluation (Papineni et al 2002, Lin 
and Hovy 2003), and more recently as training criteria in 
statistical machine translation (Och 2003).  
 We implemented an answer extraction algorithm 
using the BLEU score of Papineni et al (2002) as a 
means of assessing the overlap between the question and 
the proposed answers. For each potential answer, the 
overlap with the question was assessed with BLEU (with 
the brevity penalty set to penalize answers shorter than 3 
times the length of the question). The best scoring poten-
tial answer was presented by the AnswerExtraction 
Module as the answer to the question. 
4.4.2 Statistical Translation for Answer Extraction 
As proposed by Berger et al (2000), the lexical gap be-
tween questions and answers can be bridged by a statis-
tical translation model between answer terms and 
question terms. Their model, however, uses only an An-
swer/Question translation model (see Figure 3) as a 
means to find the answer.   
 A more complete model for answer extraction can 
be formulated in terms of a noisy channel, along the 
lines of Berger and Lafferty (2000) for the Information 
Retrieval task, as illustrated in Figure 3: an answer gen-
eration model proposes an answer A according to an an-
swer generation probability distribution; answer A is 
further transformed into question Q by an an-
swer/question translation model according to a question-
given-answer conditional probability distribution. The 
task of the AnswerExtraction algorithm is to take the 
given question q and find an answer a in the potential 
answer list that is most likely both an appropriate and 
well-formed answer. 
 
Figure 3: A noisy-channel model for answer  
extraction 
 
 The AnswerExtraction procedure employed depends 
on the task T we want it to accomplish. Let the task T be 
defined as ?find a 3-sentence answer for a given ques-
tion?. Then we can formulate the algorithm as finding 
the a-posteriori most likely answer given question and 
task, and write it as p(a|q,T). We can use Bayes? law to 
write this as: 
)|(
)|(),|(),|(
Tqp
TapTaqpTqap ?=  (1) 
Because the denominator is fixed given question and 
task, we can ignore it and find the answer that maxi-
mizes the probability of being both a well-formed and an 
appropriate answer as: 
4342143421
dependentquestiontindependenquestion
a
TaqpTapa
??
?= ),|()|(maxarg  (2) 
The decomposition of the formula into a question-
independent term and a question-dependent term allows 
us to separately model the quality of a proposed answer 
a with respect to task T, and to determine the appropri-
ateness of the proposed answer a with respect to ques-
tion q to be answered in the context of task T.  
 Because task T fits the characteristics of the ques-
tion-answer pair corpus described in Section 3, we can 
use the answer side of this corpus to compute the prior 
probability p(a|T). The role of the prior is to help down-
grading those answers that are too long or too short, or 
are otherwise not well-formed. We use a standard tri-
gram language model to compute the probability distri-
bution p(?|T). 
 The mapping of answer terms to question terms is 
modeled using Black et al?s (1993) simplest model, 
called IBM Model 1. For this reason, we call our model 
Answer 
Generation 
Model
A Q
Answer Extraction 
Algorithm 
q a 
Answer/Question 
Translation 
Model 
Model 1 as well. Under this model, a question is gener-
ated from an answer a of length n according to the fol-
lowing steps: first, a length m is chosen for the question, 
according to the distribution ?(m|n) (we assume this 
distribution is uniform); then, for each position j in q, a 
position i in a is chosen from which qj is generated, ac-
cording to the distribution t(?| ai ). The answer is as-
sumed to include a NULL word, whose purpose is to 
generate the content-free words in the question (such as 
in ?Can you please tell me???). The correspondence 
between the answer terms and the question terms is 
called an alignment, and the probability p(q|a) is com-
puted as the sum over all possible alignments. We ex-
press this probability using the following formula: 
))|(
1
1
))|()|((
1
()|()|(
11
NULLqt
n
aacaqt
n
nnmaqp
j
ii
n
i
j
m
j
++
+?+= ?? ==? (3) 
where t(qj| ai ) are the probabilities of ?translating? an-
swer terms into question terms, and c(ai|a) are the rela-
tive counts of the answer terms. Our parallel corpus of 
questions and answers can be used to compute the trans-
lation table t(qj| ai ) using the EM algorithm, as described 
by Brown et al (1993). Note that, similarly with the 
statistical machine translation framework, we deal here 
with ?inverse? probabilities, i.e. the probability of a 
question term given an answer, and not the more intui-
tive probability of answer term given question.  
 Following Berger and Lafferty (2000), an even sim-
pler model than Model 1 can be devised by skewing the 
translation distribution t(?| ai ) such that all the probabil-
ity mass goes to the term ai. This simpler model is called 
Model 0. In Section 5 we evaluate the proficiency of 
both Model 1 and Model 0 in the answer extraction task. 
5 Evaluations and Discussions 
We evaluated our QA system systematically for each 
module, in order to assess the impact of various algo-
rithms on the overall performance of the system. The 
evaluation was done by a human judge on a set of 115 
Test questions, which contained a large variety of non-
factoid questions. Each answer was rated as either cor-
rect(C), somehow related(S), wrong(W), or cannot 
tell(N). The somehow related option allowed the judge 
to indicate the fact that the answer was only partially 
correct (for example, because of missing information, or 
because the answer was more general/specific than re-
quired by the question, etc.). The cannot tell option was 
used in those cases when the validity of the answer could 
not be assessed. Note that the judge did not have access 
to any reference answers in order to asses the quality of a 
proposed answer. Only general knowledge and human 
judgment were involved when assessing the validity of 
the proposed answers. Also note that, mainly because 
our system?s answers were restricted to a maximum of 3 
sentences, the evaluation guidelines stated that answers 
that contained the right information plus other extrane-
ous information were to be rated correct.  
 For the given set of Test questions, we estimated the 
performance of the system using the formula 
(|C|+.5|S|)/(|C|+|S|+|W|). This formula gives a score of 1 
if the questions that are not ?N? rated are all considered 
correct, and a score of 0 if they are all considered wrong. 
A score of 0.5 means that, in average, 1 out of 2 ques-
tions is answered correctly.   
5.1 Question2Query Module Evaluation 
We evaluated the Question2Query module while keeping 
fixed the configuration of the other modules 
(MSNSearch as the search engine, the top 10 hits in the 
Filter module), except for the AnswerExtraction module, 
for which we tested both the N-gram co-occurrence 
based algorithm (NG-AE) and a Model 1 based algo-
rithm (M1e-AE, see Section 5.4). 
 The evaluation assessed the impact of the statistical 
chunker used to transform questions into queries, against 
the baseline strategy of submitting the question as-is to 
the search engine. As illustrated in Figure 4, the overall 
performance of the QA system significantly increased 
when the question was segmented before being submit-
ted to the SearchEngine module, for both AnswerExtrac-
tion algorithms. The score increased from 0.18 to 0.23 
when using the NG-AE algorithm, and from 0.34 to 0.38 
when using the M1e-AE algorithm.  
0
0.1
0.2
0.3
0.4
NG-AE M1e-AE
As-is
Segmented
Figure 4: Evaluation of the Question2Query  
module 
5.2 SearchEngine Module Evaluation 
The evaluation of the SearchEngine module assessed the 
impact of different search engines on the overall system 
performance. We fixed the configurations of the other 
modules (segmented question for the Question2Query 
module, top 10 hits in the Filter module), except for the 
AnswerExtraction module, for which we tested the per-
formance while using for answer extraction the NG-AE, 
M1e-AE, and ONG-AE algorithms. The later algorithm 
works exactly like NG-AE, with the exception that the 
potential answers are compared with a reference answer 
available to an Oracle, rather than against the question. 
The performance obtained using the ONG-AE algorithm 
can be thought of as indicative of the ceiling in the per-
formance that can be achieved by an AE algorithm given 
the potential answers available.  
 As illustrated in Figure 5, both the MSNSearch and 
Google search engines achieved comparable perform-
ance accuracy. The scores were 0.23 and 0.24 when us-
ing the NG-AE algorithm, 0.38 and 0.37 when using the 
M1e-AE algorithm, and 0.46 and 0.46 when using the 
ONG-AE algorithm, for MSNSearch and Google, re-
spectively. As a side note, it is worth mentioning that 
only 5% of the URLs returned by the two search engines 
for the entire Test set of questions overlapped. There-
fore, the comparable performance accuracy was not due 
to the fact that the AnswerExtraction module had access 
to the same set of potential answers, but rather to the fact 
that the 10 best hits of both search engines provide simi-
lar answering options. 
 
0
0.1
0.2
0.3
0.4
0.5
NG-AE M1e-AE ONG-AE
MSNSearch
Google
Figure 5: MSNSearch and Google give similar 
performance both in terms of realistic AE 
algorithms and oracle-based AE algorithms 
5.3 Filter Module Evaluation 
As mentioned in Section 4, the Filter module filters out 
the low score documents returned by the search engine 
and provides a set of potential answers extracted from 
the N-best list of documents. The evaluation of the Filter 
module therefore assessed the trade-off between compu-
tation time and accuracy of the overall system: the size 
of the set of potential answers directly influences the 
accuracy of the system while increasing the computation 
time of the AnswerExtraction module. The ONG-AE 
algorithm gives an accurate estimate of the performance 
ceiling induced by the set of potential answers available 
to the AnswerExtraction Module. 
 As illustrated in Figure 6, there is a significant per-
formance ceiling increase from considering only the 
document returned as the first hit (0.36) to considering 
the first 10 hits (0.46). There is only a slight increase in 
performance ceiling, however, from considering the first 
10 hits to considering the first 50 hits (0.46 to 0.49). 
0
0.1
0.2
0.3
0.4
0.5
First Hit First 10
Hits
First 50
Hits
ONG-AE
Figure 6: The scores obtained using the ONG-AE 
answer extraction algorithm for various N-best lists 
5.4 AnswerExtraction Module Evaluation 
The Answer-Extraction module was evaluated while 
fixing all the other module configurations (segmented 
question for the Question2Query module, MSNSearch as 
the search engine, and top 10 hits in the Filter module). 
 The algorithm based on the BLEU score, NG-AE, 
and its Oracle-informed variant ONG-AE, do not depend 
on the amount of training data available, and therefore 
they performed uniformly at 0.23 and 0.46, respectively 
(Figure 7). The score of 0.46 can be interpreted as a per-
formance ceiling of the AE algorithms given the avail-
able set of potential answers. 
 The algorithms based on the noisy-channel architec-
ture displayed increased performance with the increase 
in the amount of available training data, reaching as high 
as 0.38. An interesting observation is that the extraction 
algorithm using Model 1 (M1-AE) performed poorer 
than the extraction algorithm using Model 0 (M0-AE), 
for the available training data.  Our explanation is that 
the probability distribution of question terms given an-
swer terms learnt by Model 1 is well informed (many 
mappings are allowed) but badly distributed, whereas the 
probability distribution learnt by Model 0 is poorly in-
formed (indeed, only one mapping is allowed), but better 
distributed. Note the steep learning curve of Model 1, 
whose performance gets increasingly better as the distri-
bution probabilities of various answer terms (including 
the NULL word) become more informed (more map-
pings are learnt), compared to the gentle learning curve 
of Model 0, whose performance increases slightly only 
as more words become known as self-translations to the 
system (and the distribution of the NULL word gets bet-
ter approximated). 
 From the above analysis, it follows that a model 
whose probability distribution of question terms given 
answer terms is both well informed and well distributed 
is likely to outperform both M1-AE and M0-AE. Such a 
model was obtained when Model 1 was trained on both 
the question/answer parallel corpus from Section 3 and 
an artificially created parallel corpus in which each ques-
tion had itself as its ?translation?.  This training regime 
allowed the model to assign high probabilities to identity 
mappings (and therefore be better distributed), while also 
distributing some probability mass to other question-
answer term pairs (and therefore be well informed). We 
call the extraction algorithm that uses this model M1e-
AE, and the top score of 0.38 was obtained by M1e-AE 
when trained on 1 million question/answer pairs. Note 
that the learning curve of algorithm M1e-AE in Figure 7 
indeed indicates that this answer extraction procedure is 
well informed about the distribution probabilities of vari-
ous answer terms (it has the same steepness in the 
learning curve as for M1-AE), while at the same time 
uses a better distribution of the probability mass for each 
answer term compared to M1-AE (it outperforms M1-
AE by roughly a constant amount for each training set 
size in the evaluation).  
104 105 106
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Training size (QA pairs)
Ac
cu
ra
cy
ONG?AE 
M0?AE 
M1e?AE 
NG?AE 
M1?AE 
 
Figure 7: The performance of our QA system with 
various answer extraction algorithms and different 
training set sizes 
6 Performance issues 
In building our system, we have demonstrated that a 
statistical model can capitalize on large amounts of read-
ily available training data to achieve reasonable per-
formance on answering non-factoid questions. Our 
system was able to successfully answer a large variety of 
complex, non-factoid questions, such as: 
 
Q: Can I travel with the Ameripass in Mexico?  
A: The Ameripass includes all of USA excluding Alaska. 
In Canada, it is valid for travel to Montreal, Quebec, To-
ronto, Ontario, Vancouver BC, and New Westminster BC. 
In Mexico it is valid for travel to Matamoros, Nuevo 
Laredo, Tamaulipas and Tijuana Baja California. 
 
Q: What is a PIC Freeze? 
A: How to Prevent Slamming: Institute a PIC Freeze The 
best way to keep from being slammed, or having a long dis-
tance company change your service without your permis-
sion, is to request a "PIC freeze" for your line. PIC, or pre-
subscribed interexchange carrier, is the telecom industry 's 
term for a long distance company.  
 
   For those questions which were not answered cor-
rectly, we identified some of the most frequent causes 
which led to erroneous answers: 
? answer was not in the retrieved pages (see the 46% 
performance ceiling given by the Oracle) 
? answer was of the wrong ?type? (e.g., an answer for 
?how-to? instead of ?what-is?): 
Q: What are best graduate schools for AI?  
A: If you are applying for grad school in AI, and you did 
some research for an AI person, even if it was long ago 
and you don't consider it to be relevant, get a recommen-
dation if you think that the person likes you at all. [?] 
? it pointed to where an answer might be instead of 
answering the question: 
Q: What do research studies say about massage therapy?  
A: It supports research on the efficacy of therapeutic 
massage through the public, charitable AMTA Founda-
tion. Additional information about massage therapy and 
about AMTA is available via the Web at 
www.amtamassage.org. 
? the translation model overweighed the answer lan-
guage model (too good a "translation", too bad an 
answer) 
Q: What are private and public keys? 
A: Private and public keys Private and public keys Algo-
rithms can use two types of keys: private and public. 
? did not pick up the key content word (in the exam-
ple below, eggs) 
Q: What makes eggs have thin, brittle shells? 
A: The soft-shelled clams, such as steamer, razor, and 
geoduck clams, have thin brittle shells that can't com-
pletely close. Cod - A popular lean, firm, white meat 
fish from the Pacific and the North Atlantic. 
It is worth pointing out that most of these errors do not 
arise from within a single module, but rather they are the 
result of various interactions between modules that miss 
on some relevant information. 
7 Conclusions 
Previous work on question answering has focused almost 
exclusively on building systems for handling factoid 
questions. These systems have recently achieved impres-
sive performance (Moldovan et al, 2002). The world 
beyond the factoid questions, however, is largely unex-
plored, with few notable exceptions (Berger et al, 2001; 
Agichtein et al, 2002; Girju 2003). The present paper 
attempts to explore the portion related to answering 
FAQ-like questions, without restricting the domain or 
type of the questions to be handled, or restricting the 
type of answers to be provided. While we still have a 
long way to go in order to achieve robust non-factoid 
QA, this work is a step in a direction that goes beyond 
restricted questions and answers. 
 We consider the present QA system as a baseline on 
which more finely tuned QA architectures can be built. 
Learning from the experience of factoid question an-
swering, one of the most important features to be added 
is a question typology for the FAQ domain. Efforts to-
wards handling specific question types, such as causal 
questions, are already under way (Girju 2003). A care-
fully devised typology, correlated with a systematic ap-
proach to fine tuning, seem to be the lessons for success 
in answering both factoid and beyond factoid questions.   
References 
Eugene Agichten, Steve Lawrence, and Luis Gravano. 
2002. Learning to Find Answers to Questions on the 
Web. ACM Transactions on Internet Technology. 
Adam L. Berger, John D. Lafferty. 1999. Information 
Retrieval as Statistical Translation. Proceedings of 
the SIGIR 1999, Berkeley, CA. 
Adam Berger, Rich Caruana, David Cohn, Dayne 
Freitag, Vibhu Mittal. 2000. Bridging the Lexical 
Chasm: Statistical Approaches to Answer-Finding. 
Research and Development in Information Retrieval, 
pages 192--199. 
Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais, 
Andrew Ng. 2001. Data-Intensive Question Answer-
ing. Proceedings of the TREC-2001Conference, NIST. 
Gaithersburg, MD. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics, 
19(2):263--312. 
Robin Burke, Kristian Hammond, Vladimir Kulyukin, 
Steven Lytinen, Noriko Tomuro, and Scott Schoen-
berg. 1997. Question Answering from Frequently-
Asked-Question Files: Experiences with the FAQ 
Finder System. Tech. Rep. TR-97-05, Dept. of Com-
puter Science, University of Chicago. 
Ted Dunning. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational Linguis-
tics, Vol. 19, No. 1. 
Abdessamad Echihabi and Daniel Marcu. 2003. A Noisy-
Channel Approach to Question Answering. Proceed-
ings of the ACL 2003. Sapporo, Japan. 
Roxana Garju. 2003. Automatic Detection of Causal 
Relations for Question Answering. Proceedings of the 
ACL 2003, Workshop on "Multilingual Summariza-
tion and Question Answering - Machine Learning and 
Beyond", Sapporo, Japan. 
Ulf Hermjakob, Abdessamad Echihabi, and Daniel 
Marcu. 2002. Natural Language Based Reformulation 
Resource and Web Exploitation for Question Answer-
ing. Proceedings of the TREC-2002 Conference, 
NIST. Gaithersburg, MD. 
Abraham Ittycheriah and Salim Roukos. 2002. IBM's 
Statistical Question Answering System-TREC 11. Pro-
ceedings of the TREC-2002 Conference, NIST. 
Gaithersburg, MD. 
Cody C. T. Kwok, Oren Etzioni, Daniel S. Weld. Scaling 
Question Answering to the Web. 2001. WWW10. 
Hong Kong. 
Chin-Yew Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence Sta-
tistics. Proceedings of the HLT/NAACL 2003. 
Edmonton, Canada. 
Dan Moldovan, Sanda Harabagiu, Roxana Girju, Paul 
Morarescu, Finley Lacatusu, Adrian Novischi, Adri-
ana Badulescu, Orest Bolohan. 2002. LCC Tools for 
Question Answering. Proceedings of the TREC-2002 
Conference, NIST. Gaithersburg, MD. 
Franz Joseph Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. Proceedings of the 
ACL 2003. Sapporo, Japan. 
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. Bleu: a Method for Automatic Evaluation 
of Machine Translation. Proceedings of the ACL 
2002. Philadephia, PA. 
Marius Pasca, Sanda Harabagiu, 2001. The Informative 
Role of WordNet in Open-Domain Question Answer-
ing. Proceedings of the NAACL 2001 Workshop on 
WordNet and Other Lexical Resources, Carnegie 
Mellon University. Pittsburgh, PA. 
John M. Prager, Jennifer Chu-Carroll, Krysztof Czuba. 
2001. Use of WordNet Hypernyms for Answering 
What-Is Questions. Proceedings of the TREC-2002 
Conference, NIST. Gaithersburg, MD. 
Dragomir Radev, Hong Qi, Zhiping Zheng, Sasha Blair-
Goldensohn, Zhu Zhang, Weiguo Fan, and John 
Prager. 2001. Mining the Web for Answers to Natural 
Language Questions. Tenth International Conference 
onInformation and Knowledge Management. Atlanta, 
GA. 
Jinxi Xu, Ana Licuanan, Jonathan May, Scott Miller, 
Ralph Weischedel. 2002. TREC 2002 QA at BBN: 
Answer Selection and Confidence Estimation. Pro-
ceedings of the TREC-2002 Conference, NIST. 
Gaithersburg, MD. 
Web Search Intent Induction via Automatic Query Reformulation
Hal Daume? III
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
hdaume@isi.edu
Eric Brill
Microsoft Research
One Microsoft Way
Seattle, WA 98052
brill@microsoft.com
Abstract
We present a computationally efficient method
for automatic grouping of web search results
based on reformulating the original query to al-
ternative queries the user may have intended.
The method requires no data other than query
logs and the standard inverted indices used by
most search engines. Our method outperforms
standard web search in the task of enabling
users to quickly find relevant documents for in-
formational queries.
1 Introduction and Motivation
In a study of web search query logs, Broder (2002) ob-
served that most queries fall into one of three basic cate-
gories: navigational, informational and transactional. A
navigational query is one where the user has a particu-
lar URL they are attempting to find. An informational
query is one where the user has a particular information
need to satisfy. A transactional query is one in which the
user seeks to perform some sort of web-mediated activity
(such as purchasing a product).
In that paper, Broder (2002) also confirms that most
queries are very short: on the order of two words. For in-
formational queries this is often inadequate. The brevity
of queries is often due to the fact that the user does not
know exactly what he is looking for. This makes it dif-
ficult for him to formulate enough, or even correct, key-
words. These types of queries make up anywhere be-
tween 39 and 48 percent of all web queries, according to
Broder, making them a prime target for research.
2 Prior Work
Our interest is in informational queries. The general ap-
proach we explore to assist users find what they want is
to present structured results. Dumais et. al. (2001) have
shown that displaying structured results improves a user?s
ability to find relevant documents quickly.
There are three general techniques for presenting web
search results in a structured manner, ranging from to-
tally supervised methods to totally unsupervised meth-
ods. The first approach, manual classication, is typi-
fied by a system like Yahoo!, where humans have cre-
ated a hierarchical structure describing the web and man-
ually classify web pages into this hierarchy. The second
approach, automatic classication (see, for instance, the
classification system reported by Dumais (2000)) builds
on the hierarchies constructed for manual classification
systems, but web pages are categorized by a (machine-
learned) text classification system. The third approach,
typified by systems such as Vivisimo and the system of
Zamir et al (1999), look at the text of the returned docu-
ments and perform document clustering.
A related unsupervised approach to this problem is
from Beeferman and Berger (2000). Their approach
leverages click-through data to cluster related queries.
The intuition behind their method is that if two differ-
ent queries lead to users clicking on the same URL, then
these queries are related (and vice-versa). They per-
form agglomerative clustering to group queries, based on
click-through data.
Our approach is most closely related to this agglom-
erative clustering approach, but does not require click-
through data. Moreover, the use of click-through data
can result in query clusters with low user utility (see Sec-
tion 3.2). Furthermore, our approach does not suffer from
the computation cost of document clustering by text and
produces structured results with meaningful names with-
out the economic cost of building hierarchies.
3 Methodology
Our goal is to provide a range of possible needs to a
user whose query is underspecified. Suppose a naive
user John enters a query for ?fly fishing.? This query
will retrieve a large set of documents. We assume that
John?s search need (information about flies for catching
trout) is somewhere in or near this set, but we do not
know exactly where. However, we can attempt to iden-
tify other queries, made by other people, that are relevant
to John?s need. We refer to this process as Query Driven
Search Expansion and henceforth refer to our system as
the QDSE system.
3.1 Formal Specification
Formally, if Q is the set of queries to our search engine
and D is the set of indexed documents, let R be a binary
relation on Q?D where qRd if and only if d is in the re-
turn set for the query q. It is likely that the set of related
queries is quite large for a given q (in practice the size
is on the order of ten thousand; for our dataset, ?fly fish-
ing? has 29, 698 related queries). However, some of these
queries will be only tangentially related to q. Moreover,
some of them will be very similar to each other. In order
to measure these similarities, we define a distance met-
ric between two queries q and q? based on their returned
document sets, ignoring the text of the query:
?q, q?? = 1 ? |R[q] ? R[q
?]|
|R[q] ? R[q?]| (1)
One could then sort the set of related queries according
to ?q, q?? and present the top few to the user. Unfortu-
nately, this is insufficient: the top few are often too sim-
ilar to each other to provide any new useful information.
To get around this problem, we use the maximal marginal
relevance (MMR) scheme originally introduced by Car-
bonell et. al. (1998). In doing so, we order alternative
quereies according to:
argmin
q?
[
? ?q, q?? ? (1 ? ?) min
q??
?q?, q???
]
(2)
where q?s are drawn from unreturned query expansions
and q??s are drawn from the previously returned set.1
3.2 Alternative Distance Metrics
One particular thing to note in Equation 1 is that we do
not take relative rankings into account in calculating dis-
tance. One could define a distance metric weighted by
each document?s position in the return list.
We ran experiments using PageRank to weight the dis-
tance (calculated based on a recent full web crawl). Sys-
tem output was observed to be significantly inferior to the
standard ranking. We attribute this degradation to the fol-
lowing: if two queries agree only on their top documents,
they are too similar to be worth presenting to the user as
alternatives. This is the same weakness as is found in the
Beeferman and Berger (2000) approach.
4 System
The system described above functions in a completely
automatic fashion and responds in real-time to users
queries. Across the top of the return results, the query
1Queries that appear to be URLs, and strings with a very
small edit distance to the original are discarded.
is listed, as are the top ranked alternative queries. Each
of these query suggestions is a link to a heading, which
are shown below. Below this list are the top five search
result links from MSN Search under the original query2.
After the top five results from MSN Search, we display
each header with a +/- toggle to expand or collapse it.
Under each expanded query we list its top 4 results.
5 Evaluation Setup
Evaluating the results of search engine algorithms with-
out embedding these algorithms in an on-line system is
a challenge. We evaluate our system against a standard
web search algorithm (in our case, MSN Search). Ide-
ally, since our system is focused on informational queries,
we would like a corpus of ?query, intent? pairs, where the
query is underspecified. One approach would be to create
this corpus ourselves. However, doing so would bias the
results. An alternative would be to use query logs; unfor-
tunately, these do not include intents. In the next section,
we explain how we create such pairs.
5.1 Deriving Query/Intent Pairs
We have a small collection of click-through data, based
on experiments run at Microsoft Research over the past
year. Given this data, for a particular user and query,
we look for the last URL they clicked on and viewed
for at least two minutes3. We consider all of these doc-
uments to be satisfactory solutions for the user?s search
need. We further discard pairs that were in the top five
because we intend to use these pairs to evaluate our sys-
tem against vanilla MSN Search. Since the first five re-
sults our system returns are identical to the first five re-
sults MSN Search returns, it is not worthwhile annotating
these data-points (this resulted in a removal of about 20%
of the data, most of which were navigational queries).
These ?query, URL? pairs give us a hint at how to get to
the desired ?query, intent? pairs. For each ?query, URL?
pair, we looked at the query itself and the web page at the
URL. Given the query, the relevant URL and the top five
MSN Search results, we attempted to create a reasonable
search intent that was (a) consistent with the query and
the URL, but (b) not satisfied by any of the top five re-
sults. There were a handful of cases (approximately an
additional 5%) where we could not think of a reasonable
intent for which (b) held ? in these cases, we discarded
that pair.4 In all, we created 52 such pairs; four randomly
2The top five queries originally returned by MSN Search are
included because there is a chance the user knew what he was
doing and actually entered a good query.
3It may be the case that the users found an earlier URL also
to be relevant. This does not concern us, as we do not actually
use these URLs for evaluation purposes ? we simply use them
to gain insight into intents.
4We make no claim that the intents we derive were neces-
sarily the original intent in the mind of the user. We only go
through this process to get a sense of the sorts of information
chosen ?query, URL, intent? triples are shown in Table 1.
Once the intents have been derived, the original URLs are
thrown away: they are not used in any of our experiments.
5.2 Relevance Annotation
Our evaluation now consists of giving human annotators
?query, intent? pairs and having them mark the first rele-
vant URL in the return set (if there is one). However, in
order to draw an unbiased comparison between our sys-
tem and vanilla MSN Search, we need to present the out-
put from both as a simple ordered list. This requires first
converting our system?s output to a list.
5.2.1 Linearization of QDSE Output
We wish to linearize our results in such a way that
the position of the first relevant URL enables us to draw
meaningful inferences. In vanilla MSN search, we can
ascribe a cost of 1 to reading each URL in the list: having
a relevant URL as the 8th position results in a cost of 8.
Similarly, we wish to ascribe a cost to each item in our
results. We do this by making the assumption that the
user is able to guess (with 100% accuracy) which sub-
category a relevant URL will be in (we will evaluate this
assumption later). Given this assumption, we say that the
cost of a link in the top 5 vanilla MSN links is simply its
position on the page. Further down, we assume there is a
cost for reading each of the MSN links, as well as a cost
for reading each header until you get to the one you want.
Finally, there is a cost for reading down the list of links
under that header. Given this cost model, we can linearize
our results by simply sorting them by cost (in this model,
several links will have the same cost ? in this case, we fall
back to the original ordering).
5.2.2 Annotation
We divided the 52 ?query, intent? pairs into two sets of
32 (12 common pairs). Each set of 32 was then scrambled
and half were assigned to class System 1 and half were
assigned to class System 2. It was ensured that the 12
overlapping pairs were evenly distributed.
Four annotators were selected. The first two were pre-
sented with the first 32 pairs and the second two were
presented with the second 32 pairs, but with the sys-
tems swapped.5 Annotators were given a query, the in-
tent, and the top 100 documents returned from the search
according to the corresponding system (in the case of
QDSE, enough alternate queries were selected so that
there would be exactly 100 total documents listed). The
annotator selected the first link which answered the in-
tent. If there was no relevant link, they recorded that.
people really are looking for, so that we need not invent queries
off the tops of our heads.
5The interface used for evaluation converted the QDSE re-
sults into a linear list using our linearization technique so that
the interface was consistent for both systems.
5.3 Predictivity Annotation
Our cost function for the linearization of the hierarchical
results (see Section 5.2.1) assumes that users are able to
predict which category will contain a relevant link. In or-
der to evaluate this assumption, we took our 52 queries
and the automatically generated category names for each
using the QDSE system. We then presented four new an-
notators with the queries, intents and categories. They se-
lected the first category which they thought would contain
a relevant link. They also were able to select a ?None?
category if they did not think any would contain relevant
links. Each of the four annotators performed exactly the
same annotation ? it was done four times so agreement
could be calculated.
6 Results and Analysis
Our results are calculated on two metrics: relevance and
predictivity, as described in the previous section.
6.1 Relevance Results
The results of the evaluation are summarized in Table 2.
The table reports four statistics for each of the systems
compared. In the table, MSN is vanilla MSN search and
QDSE is the system described in this paper.
The first row is probability of success using this sys-
tem (number of successful searches divided by the num-
ber of total searches). The second line is the probability
of success, given that you are only allowed to read the
first 20 results. Next, Avg. Success Cost, is the average
cost of the relevant URL for that system. This cost aver-
ages only over the successes (queries for which a relevant
URL was found). The next statistic, Avg. Cost, is the av-
erage cost including failures, where the cost of a failure
is, in the case of vanilla MSN, the number of returned
results and, in the case of QDSE, the cost of reading the
top five results, all the labels and one category expansion6
The last statistic, Avg. Mutual Cost, is the average cost
for all pairs where both systems found a relevant docu-
ment. The last line reports inter-annotator agreement as
calculated over the 12 pairs, which is low due partly to
the small sample size and partly to the fact that the in-
tents themselves were still somewhat underspecified.7
6.2 Predictivity Results
We performed two calculations on the results of the pre-
dictivity annotations. In the first calculation, we consider
the relevance judgments on the QDSE system to be the
gold standard. We calculated accuracy of choosing the
correct first category. This measures the extent to which
6The user may have been able to determine his search had
failed having only read the categories, yielding a lower cost.
7We intend to run timed user studies in our future work;
however, it has been observed (Dumais et al, 2001) that pre-
senting users with structured results enables them to find rel-
evant documents more quickly; to do timed studies in the lin-
earization is an unrealistic scenario, since one would never de-
ploy the system in this configuration.
Query: Soldering iron URL: www.siliconsolar.com/accessories.htm
Intent: looking for accessories for soldering irons (but not soldering irons themselves)
Query: Whole Foods URL: www.wholefoodsmarket.com/company/communitygiving.html
Intent: looking for the Whole Foods Market?s community giving policy
Query: final fantasy URL: www.playonline.com/ff11/home/
Intent: looking for a webforum for final fantasy games
Query: online computer course URL: www.microsoft.com/traincert/
Intent: looking for information on Microsoft Certified Technical Education centers
Table 1: Four random ?query, URL, intent? triples
MSN QDSE
Prob. Success 88.0% 67.7%
Prob. Success 20 68.7% 62.6%
Avg. Success Cost 12.4 4.7
Avg. Cost 22.9 9.0
Avg. Mutual Cost 23.0 9.0
kappa 0.57 0.45
Table 2: Results of the evaluation
the oracle system is correct. On this task, accuracy was
0.54. The second calculation we made was to determine
whether a user can predict, looking at the headers only,
whether their search has been successful. In the task
of simply identifying failed searches, accuracy was 0.70.
Inter-annotator agreement for predictivity was somewhat
low, with a kappa value of only 0.49.
6.3 Analysis
As can be seen from Table 2, a user is less likely to find a
relevant query in the top 100 documents using the QDSE
system than using the MSN system. However, this is an
artificial task: very few users will actually read through
the top 100 returned documents before giving up. At a
cutoff of 20 documents, the user is still more likely to suc-
ceed using MSN, but the difference is not nearly so large
(note, however, that by cutting off at 20 in the QDSE lin-
earization, the user will typically see only one result from
each alternate query, thus heavily relying on the under-
lying search engine to do a good job). The rest of the
numbers (not included for brevity) are consistent at 20.
Moreover, as seen in the evaluation of the predictiv-
ity results, users can decide, with 70% accuracy, whether
their search has failed having read only the category la-
bels. This is in stark contrast to the vanilla MSN search
where they could not know without reading all the results
whether their search had succeeded.
If one does not wish to give up on recall at all, we could
simply list all the MSN search results immediately after
the QDSE results. By doing this, we ensure that the prob-
ability of success is at least as high for the QDSE system.
We can upper-bound the additional cost this would incur
to the QDSE system by 4.15, yielding an upper bound of
13.2, still superior to vanilla MSN.
If one is optimistic and is willing to assume that a user
will know based only on the category labels whether or
not their search has succeeded, then the relevant com-
parison from Table 2 is between Avg. Success Cost for
QDSE and Avg. Cost for MSN. In this case, our cost
of 4.7 is a factor of 5 better than the MSN cost. If, on
the other hand, one is pessimistic and believes that a user
will not be able to identify based on the category names
whether or not their search has succeeded in the QDSE
system, then the interesting comparison is between the
Avg. Costs for MSN and QDSE. Both favor QDSE.
Lastly, the reciprocal rank statistic at 20 results confirm
that the QDSE system is more able to direct the user to
relevant documents than vanilla MSN search.
7 Conclusion
We have presented a method for providing useful sug-
gested queries for underspecified informational queries.
We evaluated our system using an unbiased metric
against a standard web search system and found that
our system enables users to more quickly find relevant
pages. This conclusion is based on an ?oracle? assump-
tion, which we also evaluate. Based on these evaluations,
we can show that even under a pessimistic view point, our
system outperforms the vanilla search engine.
There is still room for improvement, especially in the
predictivity results. We would like users to be able to
more readily identify the class into which a relevant doc-
ument (if one exists) would be found. We are investi-
gating multi-document summarization techniques which
might allow users to better pinpoint the category in which
a relevant document might be found.
References
D. Beeferman and A. Berger. 2000. Agglomerative clus-
tering of a search engine query log. In KDD.
S. Brin and L Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems.
A. Broder. 2002. A taxonomy of web search. In SIGIR.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Research and Develop-
ment in Information Retrieval.
S. Dumais and H. Chen. 2000. Hierarchical classifica-
tion of Web content. In Proc. of SIGIR-00.
S. Dumais, E. Cutrell, and H. Chen. 2001. Optimizing
search by showing results in context. In CHI.
O. Zamir and O. Etzioni. 1999. Grouper: a dynamic
clustering interface to Web search results. In Com-
puter Networks.
An Improved Error Model for Noisy Channel Spelling Correction
Abstract
The noisy channel model has been applied
to a wide range of problems, including
spelling correction.  These models consist
of two components: a source model and a
channel model.  Very little research has
gone into improving the channel model
for spelling correction.  This paper
describes a new channel model for
spelling correction, based on generic
string to string edits.  Using this model
gives significant performance
improvements compared to previously
proposed models.
Introduction
The noisy channel model (Shannon 1948)
has been successfully applied to a wide
range of problems, including spelling
correction.  These models consist of two
components: a source model and a channel
model.  For many applications, people have
devoted considerable energy to improving
both components, with resulting
improvements in overall system accuracy.
However, relatively little research has gone
into improving the channel model for
spelling correction.  This paper describes an
improvement to noisy channel spelling
correction via a more powerful model of
spelling errors, be they typing mistakes or
cognitive errors, than has previously been
employed.  Our model works by learning
generic string to string edits, along with the
probabilities of each of these edits.  This
more powerful model gives significant
improvements in accuracy over previous
approaches to noisy channel spelling
correction.
1 Noisy Channel Spelling Correction
This paper will address the problem of
automatically training a system to correct
generic single word spelling errors.1  We do
not address the problem of correcting
specific word set confusions such as
{to,too,two} (see (Golding and Roth 1999)).
We will define the spelling correction
problem abstractly as follows: Given an
alphabet ? , a dictionary D consisting of
strings in ? * and a string s, where
Ds ? and *??s , find the word Dw ?  that
is most likely to have been erroneously input
as s.  The requirement that Ds ? can be
dropped, but it only makes sense to do so in
the context of a sufficiently powerful
language model.
In a probabilistic system, we want to
find )|(argmax   w swP .  Applying Bayes?
Rule and dropping the constant
denominator, we get the unnormalized
posterior: )(*)|(argmax   w wPwsP .  We now
have a noisy channel model for spelling
correction, with two components, the source
model P(w) and the channel model P(s | w).
The model assumes that natural language
text is generated as follows: First a person
chooses a word to output, according to the
probability distribution P(w).  Then the
person attempts to output the word w, but
the noisy channel induces the person to
output string s instead, according to the
                                                     
1
 Two very nice overviews of spelling correction can
be found in (Kukich 1992) and (Jurafsky and Martin
2000).
Eric Brill and Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, Wa. 98052
{brill,bobmoore}@microsoft.com
distribution P(s | w).  For instance, under
typical circumstances we would expect
P(the | the) to be very high, P(teh | the) to be
relatively high and P(hippopotamus | the) to
be extremely low.  In this paper, we will
refer to the channel model as the error
model.
Two seminal papers first posed a
noisy channel model solution to the spelling
correction problem.  In (Mayes, Damerau et
al. 1991), word bigrams are used for the
source model.  For the error model, they first
define the confusion set of a string s to
include s, along with all words w in the
dictionary D such that s can be derived from
w by a single application of one of the four
edit operations:
(1) Add a single letter.
(2) Delete a single letter.
(3) Replace one letter with another.
(4) Transpose two adjacent letters.
Let C be the number of words in the
confusion set of d.  Then they define the
error model, for all s in the confusion set of
d, as:
??
??
?
?
=
=
otherwise )1(
)-(1
d  s if  
)|(
C
dsP ?
?
7KLVLVDYHU\VLPSOHHUURUPRGHOZKHUH LV
the prior on a typed word being correct, and
the remaining probability mass is distributed
evenly among all other words in the
confusion set.
Church and Gale (1991) propose a
more sophisticated error model.  Like
Mayes, Damerau, et al (1991), they
consider as candidate source words only
those words that are a single basic edit away
from s, using the same edit set as above.
However, two improvements are made.
First, instead of weighing all edits equally,
each unique edit has a probability associated
with it. Second, insertion and deletion
probabilities are conditioned on context.
The probability of inserting or deleting a
character is conditioned on the letter
appearing immediately to the left of that
character.
The error probabilities are derived by
first assuming all edits are equiprobable.
They use as a training corpus a set of space-
delimited strings that were found in a large
collection of text, and that (a) do not appear
in their dictionary and (b) are no more than
one edit away from a word that does appear
in the dictionary.  They iteratively run the
spell checker over the training corpus to find
corrections, then use these corrections to
update the edit probabilities. Ristad and
Yianilos (1997) present another algorithm
for deriving these edit probabilities from a
training corpus, and show that for the
problem of word pronunciation, using the
learned string edit distance gives one fourth
the error rate compared to using unweighted
edits.
2 An Improved Error Model
Previous error models have all been based
on Damerau-Levenshtein distance measures
(Damerau 1964; Levenshtein 1966), where
the distance between two strings is the
minimum number of single character
insertions, substitutions and deletions (and
in some cases, character pair transpositions)
necessary to derive one string from another.
Improvements have been made by
associating probabilities with individual edit
operations.
We propose a much more generic
HUURU PRGHO  /HW  EH DQ DOSKDEHW  2XU
model allows all edit operations of the form
?  ZKHUH *??, .  3 ?  LV WKH
probability that when users intends to type
WKH VWULQJ  WKH\ W\SH  LQVWHDG 1RWH WKDW
the edit operations allowed in Church and
Gale (1991), Mayes, Damerau et al (1991)
and Ristad and Yianilos (1997), are properly
subsumed by our generic string to string
substitutions.
In addition, we condition on the
position in the string that the edit occurs in,
3 ?  _ 361 ZKHUH 361   ^VWDUW RI
word, middle of word, end of word}.2  The
position is determined by the location of
VXEVWULQJ  LQ WKH VRXUFH GLFWLRQDU\ZRUG
Positional information is a powerful
conditioning feature for rich edit operations.
For instance, P(e | a) does not vary greatly
between the three positions mentioned
above.  However, P(ent | ant) is highly
dependent upon position.  People rarely
mistype antler as entler, but often mistype
reluctant as reluctent.
Within the noisy channel framework,
we can informally think of our error model
as follows.  First, a person picks a word to
generate.  Then she picks a partition of the
characters of that word.  Then she types
each partition, possibly erroneously.  For
example, a person might choose to generate
the word physical.  She would then pick a
partition from the set of all possible
partitions, say: ph y s i c al.  Then she
would generate each partition, possibly with
errors.  After choosing this particular word
and partition, the probability of generating
the string fisikle with the partition f i s i k le
would be P(f | ph) *P(i | y) * P(s | s) *P(i | i)
* P(k | c) *P(le | al).3
The above example points to
advantages of our model compared to
previous models based on weighted
Damerau-Levenshtein distance.  Note that
neither P(f | ph) nor P(le | al) are modeled
directly in the previous approaches to error
modeling.  A number of studies have
pointed out that a high percentage of
misspelled words are wrong due to a single
letter insertion, substitution, or deletion, or
from a letter pair transposition (Damerau
1964; Peterson 1986).  However, even if this
is the case, it does not imply that nothing is
                                                     
2
 Another good PSN feature would be morpheme
boundary.
3
 We will leave off the positional conditioning
information for simplicity.
to be gained by modeling more powerful
edit operations.  If somebody types the
string confidant, we do not really want to
model this error as P(a | e), but rather P(ant |
ent).  And anticedent can more accurately be
modeled by P(anti | ante), rather than P(i | e).
By taking a more generic approach to error
modeling, we can more accurately model the
errors people make.
A formal presentation of our model
follows.  Let Part(w) be the set of all
possible ways of partitioning string w into
adjacent (possibly null) substrings.  For a
particular partition R?Part(w), where |R|=j
(R consists of j contiguous segments), let Ri
be the ith segment.  Under our model,
P(s | w) =
? ??
? =
=
?)(
||
1|||| )(
)|()|(
wPartR
R
i
ii
RT
sPartT
RTPwRP
One particular pair of alignments for
s and w induces a set of edits that derive s
from w.  By only considering the best
partitioning of s and w, we can simplify this
to:
P(s | w) =
max R ?Part(w),T?Part(s) P(R|w)?
=
||
1
R
i
P(Ti|Ri)
We do not yet have a good way to
derive P(R | w), and in running experiments
we determined that poorly modeling this
distribution gave slightly worse performance
than not modeling it at all, so in practice we
drop this term.
3 Training the Model
To train the model, we need a training set
consisting of {si, wi} string pairs,
representing spelling errors si paired with
the correct spelling of the word wi.  We
begin by aligning the letters in si with those
in wi based on minimizing the edit distance
between si and wi, based on single character
insertions, deletions and substitutions.   For
instance, given the training pair <akgsual,
actual>, this could be aligned as:
a     c       t     u     a      l
a      k     g     s     u     a     l
This corresponds to the sequence of edit
operations:
a?a    c?N ?g   t?s   u?u   a?a   l?l
To allow for richer contextual
information, we expand each nonmatch
substitution to incorporate up to N additional
adjacent edits. For example, for the first
nonmatch edit in the example above, with
N=2, we would generate the following
substitutions:
c ? k
ac ? ak
c ? kg
ac ? akg
ct ? kgs
We would do similarly for the other
nonmatch edits, and give each of these
substitutions a fractional count.
We can then calculate the probability
RI HDFK VXEVWLWXWLRQ ?  DV FRXQW ?
FRXQW FRXQW ? LVVLPSO\WKHVXP
of the counts derived from our training data
as explained above.  Estimating FRXQW LVD
bit tricky.  If we took a text corpus, then
extracted all the spelling errors found in the
corpus and then used those errors for
training, FRXQW  ZRXOG VLPSO\ EH WKH
number of times VXEVWULQJ  RFFXUV LQ WKH
text corpus.  But if we are training from a set
of {si, wi} tuples and not given an associated
corpus, we can do the following:
(a) From a large collection of representative
WH[WFRXQWWKHQXPEHURIRFFXUUHQFHVRI 
(b) Adjust the count based on an estimate of
the rate with which people make typing
errors.
Since the rate of errors varies widely
and is difficult to measure, we can only
crudely approximate it.  Fortunately, we
have found empirically that the results are
not very sensitive to the value chosen.
Essentially, we are doing one
iteration of the Expectation-Maximization
algorithm (Dempster, Laird et al 1977).
The idea is that contexts that are useful will
accumulate fractional counts across multiple
instances, whereas contexts that are noise
will not accumulate significant counts.
4 Applying the Model
Given a string s, where Ds ? , we want to
return )|()|(argmax   w contextwPswP .  Our
approach will be to return an n-best list of
candidates according to the error model, and
then rescore these candidates by taking into
account the source probabilities.
We are given a dictionary D and a
set of parameters P, where each parameter is
3 ?  IRU VRPH *??, , meaning the
SUREDELOLW\WKDWLIDVWULQJ  LV LQWHQGHG WKH
QRLV\FKDQQHOZLOOSURGXFH  LQVWHDG )LUVW
note that for a particular pair of strings {s,
w} we can use the standard dynamic
programming algorithm for finding edit
distance by filling a |s|*|w| weight matrix
(Wagner and Fisher 1974; Hall and Dowling
1980), with only minor changes.  For
computing the Damerau-Levenshtein
distance between two strings, this can be
done in O(|s|*|w|) time.  When we allow
generic edit operations, the complexity
increases to O(|s|2*|w|2).  In filling in a cell
(i,j) in the matrix for computing Damerau-
Levenshtein distance we need only examine
cells (i,j-1), (i-1,j) and (i-1,j-1).  With
generic edits, we have to examine all cells
(a,b) where a ? i and b ? j.
We first precompile the dictionary
into a trie, with each node in the trie
corresponding to a vector of weights.  If we
think of the x-axis of the standard weight
matrix for computing edit distance as
corresponding to w (a word in the
dictionary), then the vector at each node in
the trie corresponds to a column in the
weight matrix associated with computing the
distance between s and the string prefix
ending at that trie node.
:HVWRUHWKH ? SDUDPHWHUVDVDtrie
of tries. We have one trie corresponding to
DOOVWULQJV WKDWDSSHDURQWKHOHIWKDQGVLGH
of some substitution in our parameter set.
At every node in this trie, corresponding to a
VWULQJ ZHSRLQW WR D trie consisting of all
VWULQJV  WKDW DSSHDURQ WKH ULJKW KDQG VLGH
RIDVXEVWLWXWLRQLQRXUSDUDPHWHUVHWZLWK
on the left hand side.  We store the
substitution probabilities at the terminal
QRGHVRIWKH WULHV
%\ VWRULQJ ERWK  DQG  VWULQJV LQ
reverse order, we can efficiently compute
edit distance over the entire dictionary.  We
process the dictionary trie from the root
downwards, filling in the weight vector at
each node.  To find the substitution
parameters that are applicable, given a
particular node in the trie and a particular
position in the input string s (this
corresponds to filling in one cell in one
vector of a dictionary trie node) we trace up
from the node to the root, while tracing
GRZQ WKH  trie from the root. As we trace
GRZQ WKH  trie, if we encounter a terminal
node, we follow the pointer to the
FRUUHVSRQGLQJ  trie, and then trace
backwards from the position in s while
WUDFLQJGRZQWKH trie.
Note that searching through a static
dictionary D is not a requirement of our
error model.  It is possible that with a
different search technique, we could apply
our model to languages such as Turkish for
which a static dictionary is inappropriate
(Oflazer 1994).
Given a 200,000-word dictionary, and
using our best error model, we are able to
spell check strings not in the dictionary in
approximately 50 milliseconds on average,
running on a Dell 610 500mhz Pentium III
workstation.
5 Results
5.1  Error Model in Isolation
We ran experiments using a 10,000-
word corpus of common English spelling
errors, paired with their correct spelling.
We used 80% of this corpus for training and
20% for evaluation.  Our dictionary
contained approximately 200,000 entries,
including all words in the test set.  The
results in this section are obtained with a
language model that assigns uniform
probability to all words in the dictionary.  In
Table 1 we show K-best results for different
maximum context window sizes, without
using positional information.  For instance,
the 2-best accuracy is the percentage of time
the correct answer is one of the top two
answers returned by the system. Note that a
maximum window of zero corresponds to
the set of single character insertion, deletion
and substitution edits, weighted with their
probabilities.  We see that, up to a point,
additional context provides us with more
accurate spelling correction and beyond that,
additional context neither helps nor hurts.
Max
Window 1-Best 2-Best 3-Best
0 87.0 93.9 95.9
CG 89.5 94.9 96.5
1 90.9 95.6 96.8
2 92.9 97.1 98.1
3 93.6 97.4 98.5
4 93.6 97.4 98.5
Table 1 Results without positional
information
In Table 1, the row labelled CG
shows the results when we allow the
equivalent set of edit operations to those
used in (Church and Gale 1991).  This is a
proper superset of the set of edits where the
maximum window is zero and a proper
subset of the edits where the maximum
window is one.  The CG model is essentially
equivalent to the Church and Gale error
model, except (a) the models above can
posit an arbitrary number of edits and (b) we
did not do parameter reestimation (see
below).
Next, we measured how much we
gain by conditioning on the position of the
edit relative to the source word.  These
results are shown in Table 2.  As we
expected, positional information helps more
when using a richer edit set than when using
only single character edits.  For a maximum
window size of 0, using positional
information gives a 13% relative
improvement in 1-best accuracy, whereas
for a maximum window size of 4, the gain is
22%.  Our full strength model gives a 52%
relative error reduction on 1-best accuracy
compared to the CG model (95.0%
compared to 89.5%).
Max Window 1-Best 2-Best 3-Best
0 88.7 95.1 96.6
1 92.8 96.5 97.4
2 94.6 98.0 98.7
3 95.0 98.0 98.8
4 95.0 98.0 98.8
5 95.1 98.0 98.8
Table 2 Results with positional
information.
We experimented with iteratively
reestimating parameters, as was done in the
original formulation in (Church and Gale
1991).  Doing so resulted in a slight
degradation in performance.  The data we
are using is much cleaner than that used in
(Church and Gale 1991) which probably
explains why reestimation benefited them in
their experiments and did not give any
benefit to the error models in our
experiments.
5.2  Adding a Language Model
Next, we explore what happens to
our results as we add a language model.  In
order to get errors in context, we took the
Brown Corpus and found all occurrences of
all words in our test set.  Then we mapped
these words to the incorrect spellings they
were paired with in the test set, and ran our
spell checker to correct the misspellings.
We used two language models.  The first
assumed all words are equally likely, i.e. the
null language model used above.    The
second used a trigram language model
derived from a large collection of on-line
text (not including the Brown Corpus).
Because a spell checker is typically applied
right after a word is typed, the language
model only used left context.
We show the results in Figure 1,
where we used the error model with
positional information and with a maximum
context window of four, and used the
language model to rescore the 5 best word
candidates returned by the error model.
Note that for the case of no language model,
the results are lower than the results quoted
above (e.g. a 1-best score above of 95.0%,
compared to 93.9% in the graph).  This is
because the results on the Brown Corpus are
computed per token, whereas above we were
computing results per type.
One question we wanted to ask is whether
using a good language model would obviate
the need for a good error model.  In Figure
2, we applied the trigram model to resort the
5-best results of the CG model.  We see that
while a language model improves results,
using the better error model (Figure 1) still
gives significantly better results.  Using a
language model with our best error model
gives a 73.6% error reduction compared to
using a language model with the CG error
model.  Rescoring the 20-best output of the
CG model instead of the 5-best only
improves the 1-best accuracy from 90.9% to
91.0%.
93
94
95
96
97
98
99
100
1 2 3 4 5
N-Best
A
cc
ur
ac
y
No
Language
Model
Trigram
Language
Model
Figure 1 Spelling Correction
Improvement When Using a Language
Model
84
86
88
90
92
94
96
1 2 3 4 5
N-Best
A
cc
ur
ac
y
No
Language
Model
Trigram
Language
Model
Figure 2 Using the CG Error Model with
a Trigram Language Model
Conclusion
We have presented a new error model for
noisy channel spelling correction based on
generic string to string edits, and have
demonstrated that it results in a significant
improvement in performance compared to
previous approaches.  Without a language
model, our error model gives a 52%
reduction in spelling correction error rate
compared to the weighted Damerau-
Levenshtein distance technique of Church
and Gale.  With a language model, our
model gives a 74% reduction in error.
One exciting future line of research
is to explore error models that adapt to an
individual or subpopulation.  With a rich set
of edits, we hope highly accurate
individualized spell checking can soon
become a reality.
References
Church, K. and W. Gale (1991). ?Probability Scoring
for Spelling Correction.? Statistics and Computing
1: 93-103.
Damerau, F. (1964). ?A technique for computer
detection and correction of spelling errors.?
Communications of the ACM 7(3): 659-664.
Dempster, A., N. Laird, et al (1977). ?Maximum
likelihood from incomplete data via the EM
algorithm.? Journal of the Royal Statistical Society
39(1): 1-21.
Golding, A. and D. Roth (1999). ?A Winnow-Based
Approach to Spelling Correction.? Machine
Learning 34: 107-130.
Hall, P. and G. Dowling (1980). ?Approximate string
matching.? ACM Computing Surveys 12(4): 17-38.
Jurafsky, D. and J. Martin (2000). Speech and
Language Processing, Prentice Hall.
Kukich, K. (1992). ?Techniques for Automatically
Correcting Words in Text.? ACM Computing
Surveys 24(4): 377-439.
Levenshtein, V. (1966). ?Binary codes capable of
correcting deletions, insertions and reversals.?
Soviet Physice -- Doklady 10: 707-710.
Mayes, E., F. Damerau, et al (1991). ?Context Based
Spelling Correction.? Information Processing and
Management 27(5): 517-522.
Oflazer, K. (1994). Spelling Correction in
Agglutinative Languages. Applied Natural
Language Processing, Stuttgart, Germany.
Peterson, J. (1986). ?A note on undetected typing
errors.? Communications of the ACM 29(7): 633-
637.
Ristad, E. and P. Yianilos (1997). Learning String
Edit Distance. International Conference on
Machine Learning, Morgan Kaufmann.
Shannon, C. (1948). ?A mathematical theory of
communication.? Bell System Technical Journal
27(3): 379-423.
Wagner, R. and M. Fisher (1974). ?The string to
string correction problem.? JACM 21: 168-173.
Scaling to Very Very Large Corpora for  
Natural Language Disambiguation 
Michele Banko and Eric Brill 
Microsoft Research 
1 Microsoft Way 
Redmond, WA 98052 USA 
{mbanko,brill}@microsoft.com 
 
Abstract 
The amount of readily available on-line 
text has reached hundreds of billions of 
words and continues to grow.  Yet for 
most core natural language tasks, 
algorithms continue to be optimized, 
tested and compared after training on  
corpora consisting of only one million 
words or less.  In this paper, we 
evaluate the performance of different 
learning methods on a prototypical 
natural language disambiguation task, 
confusion set disambiguation, when 
trained on orders of magnitude more 
labeled data than has previously been 
used.  We are fortunate that for this 
particular application, correctly labeled 
training data is free. Since this will 
often not be the case, we examine 
methods for effectively exploiting very 
large corpora when labeled data comes 
at a cost. 
1 Introduction 
Machine learning techniques, which 
automatically learn  linguistic information from 
online text corpora, have been applied to a 
number of natural language problems 
throughout the last decade.  A large percentage 
of papers published in this area involve 
comparisons of different learning approaches 
trained and tested with commonly used corpora.  
While the amount of available online text has 
been increasing at a dramatic rate, the size of 
training corpora typically used for learning has 
not.  In part, this is due to the standardization of 
data sets used within the field, as well as the 
potentially large cost of annotating data for 
those learning methods that rely on labeled text. 
The empirical NLP community has put 
substantial effort into evaluating performance of 
a large number of machine learning methods 
over fixed, and relatively small, data sets.  Yet 
since we now have access to significantly more 
data, one has to wonder what conclusions that 
have been drawn on small data sets may carry 
over when these learning methods are trained 
using much larger corpora.   
In this paper, we present a study of the 
effects of data size on machine learning for 
natural language disambiguation. In particular, 
we study the problem of selection among 
confusable words, using orders of magnitude 
more training data than has ever been applied to 
this problem.  First we show learning curves for 
four different machine learning algorithms.  
Next, we consider the efficacy of voting, sample 
selection and partially unsupervised learning 
with large training corpora, in hopes of being 
able to obtain the benefits that come from 
significantly larger training corpora without 
incurring too large a cost. 
2 Confusion Set Disambiguation 
Confusion set disambiguation is the problem of 
choosing the correct use of a word, given a set 
of words with which it is commonly confused.  
Example confusion sets include: {principle , 
principal}, {then, than}, {to,two,too}, and 
{weather,whether}. 
 Numerous methods have been presented 
for confusable disambiguation. The more recent 
set of techniques includes mult iplicative weight-
update algorithms (Golding and Roth, 1998), 
latent semantic analysis (Jones and Martin, 
1997), transformation-based learning (Mangu 
and Brill, 1997), differential grammars (Powers, 
1997), decision lists (Yarowsky, 1994), and a 
variety of Bayesian classifiers (Gale et al, 1993, 
Golding, 1995, Golding and Schabes, 1996).  In 
all of these approaches, the problem is 
formulated as follows:  Given a specific 
confusion set (e.g. {to,two,too}), all occurrences 
of confusion set members in the test set are 
replaced by a marker;  everywhere the system 
sees this marker, it must decide which member 
of the confusion set to choose.   
 Confusion set disambiguation is one of a 
class of natural language problems involving 
disambiguation from a relatively small set of 
alternatives based upon the string context in 
which the ambiguity site appears.  Other such 
problems include word sense disambiguation, 
part of speech tagging and some formulations of 
phrasal chunking.  One advantageous aspect of 
confusion set disambiguation, which allows us 
to study the effects of large data sets on 
performance, is that labeled training data is 
essentially free, since the correct answer is 
surface apparent in any collection of reasonably 
well-edited text.  
 
3 Learning Curve Expe riments 
This work was partially motivated by the desire 
to develop an improved grammar checker.  
Given a fixed amount of time, we considered 
what would be the most effective way to focus 
our efforts in order to attain the greatest 
performance improvement.  Some possibilities 
included modifying standard learning 
algorithms, exploring new learning techniques, 
and using more sophisticated features.  Before 
exploring these somewhat expensive paths, we 
decided to first see what happened if we simply 
trained an existing method with much more 
data.  This led to the exploration of learning 
curves for various machine learning algorithms : 
winnow1, perceptron, na?ve Bayes, and a very 
simple memory-based learner.  For the first 
three learners, we used the standard collection of 
features employed for this problem: the set of 
words within a window of the target word, and 
collocations containing words and/or parts of 
                                                                 
1 Thanks to Dan Roth for making both Winnow and 
Perceptron available. 
speech.  The memory-based learner used only 
the word before and word after as features. 
 
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0.1 1 10 100 1000
Millions of Words
T
e
s
t
 
A
c
c
u
r
a
c
y
Memory-Based
Winnow
Perceptron
Na?ve Bayes
 
Figure 1. Learning Curves for Confusion Set 
Disambiguation 
 
 We collected a 1-billion-word training 
corpus from a variety of English texts, including 
news articles, scientific abstracts, government 
transcripts, literature and other varied forms of 
prose.  This training corpus is three orders of 
magnitude greater than the largest training 
corpus previously used for this problem.  We 
used 1 million words of Wall Street Journal text 
as our test set, and no data from the Wall Street 
Journal was used when constructing the training 
corpus. Each learner was trained at several 
cutoff points in the training corpus, i.e. the first 
one million words, the first five million words, 
and so on, until all one billion words were used 
for training. In order to avoid training biases that 
may result from merely concatenating the 
different data sources to form a larger training 
corpus, we constructed each consecutive 
training corpus by probabilistically sampling 
sentences from the different sources weighted 
by the size of each source. 
 In Figure 1, we show learning curves for 
each learner, up to one billion words of training 
data.  Each point in the graph is the average 
performance over ten confusion sets for that size 
training corpus.  Note that the curves appear to 
be log-linear even out to one billion words. 
 Of course for many problems, additional 
training data has a non-zero cost.  However, 
these results suggest that we may want to 
reconsider the trade-off between spending time 
and money on algorithm development versus 
spending it on corpus development.  At least for 
the problem of confusable disambiguation, none 
of the learners tested is close to asymptoting in 
performance at the training corpus size 
commonly employed by the field. 
 Such gains in accuracy, however, do not 
come for free.  Figure 2 shows the size of 
learned representations as a function of training 
data size.  For some applications, this is not 
necessarily a concern.  But for others, where 
space comes at a premium, obtaining the gains 
that come with a billion words of training data 
may not be viable without an effort made to 
compress information.  In such cases, one could 
look at numerous methods for compressing data 
(e.g. Dagan and Engleson, 1995, Weng, et al 
1998). 
4 The Efficacy of Voting 
Voting has proven to be an effective technique 
for improving classifier accuracy for many 
applications, including part-of-speech tagging 
(van Halteren, et al 1998), parsing (Henderson 
and Brill, 1999), and word sense disambiguation 
(Pederson, 2000).  By training a set of classifiers 
on a single training corpus and then combining 
their outputs in classification, it is often possible 
to achieve a target accuracy with less labeled 
training data than would be needed if only one 
classifier  was being used.  Voting can be 
effective in reducing both the bias of a particular 
training corpus and the bias of a specific learner.  
When a training corpus is very small, there is 
much more room for these biases to surface and 
therefore for voting to be effective.  But does 
voting still offer performance gains when 
classifiers are trained on much larger corpora? 
 The complementarity between two 
learners was defined by Brill and Wu (1998) in 
order to quantify the percentage of time when 
one system is wrong, that another system is 
correct, and therefore providing an upper bound 
on combination accuracy. As training size 
increases significantly, we would expect 
complementarity between classifiers to decrease.  
This is due in part to the fact that a larger 
training corpus will reduce the data set variance 
and any bias arising from this.  Also, some of 
the differences between classifiers might be due 
to how they handle a sparse training set.   
1
10
100
1000
10000
100000
1000000
1 10 100 1000
Millions of Words
Winnow
Memory-Based
 
Figure 2. Representation Size vs. Training 
Corpus Size 
 
 
As a result of comparing a sample of 
two learners as a function of increasingly large 
training sets, we see in Table 1 that 
complementarity does indeed decrease as 
training size increases. 
 
Training Size (words) Complementarity(L1,L2) 
106 0.2612 
107 0.2410 
108 0.1759 
109 0.1612 
Table 1. Complementarity 
 
 Next we tested whether this decrease in 
complementarity meant that voting loses its 
effectiveness as the training set increases.  To 
examine the impact of voting when using a 
significantly larger training corpus, we ran 3 out 
of the 4 learners on our set of 10 confusable 
pairs, excluding the memory-based learner.  
Voting was done by combining the normalized 
score each learner assigned to a classification 
choice.  In Figure 3, we show the accuracy 
obtained from voting, along with the single best 
learner accuracy at each training set size.  We 
see that for very small corpora, voting is 
beneficial, resulting in better performance than 
any single classifier.  Beyond 1 million words, 
little is gained by voting, and indeed on the 
largest training sets voting actually hurts 
accuracy. 
 
0.80
0.85
0.90
0.95
1.00
0.1 1 10 100 1000
Millions of words
T
e
s
t
 
A
c
c
u
r
a
c
y
Best
Voting
 
Figure 3. Voting Among Classifiers 
5 When Annotated Data Is Not Free 
While the observation that learning curves are 
not asymptoting even with orders of magnitude 
more training data than is currently used is very 
exciting, this result may have somewhat limited 
ramifications.  Very few problems exist for 
which annotated data of this size is available for 
free.  Surely we cannot reasonably expect that 
the manual annotation of one billion words 
along with corresponding parse trees will occur 
any time soon (but see (Banko and Brill 2001) 
for a discussion that this might not be 
completely infeasible).  Despite this pitfall, there 
are techniques one can use to try to obtain the 
benefits of considerably larger training corpora 
without incurring significant additional costs.  In 
the sections that follow, we study two such 
solutions: active learning and unsupervised 
learning. 
5.1    Active Learning 
Active learning involves intelligently selecting a 
portion of samples for annotation from a pool of  
as-yet unannotated training samples.  Not all 
samples in a training set are equally useful.  By 
concentrating human annotation efforts on the 
samples of greatest utility to the machine 
learning algorithm, it may be possible to attain 
better performance for a fixed annotation cost 
than if samples were chosen randomly for 
human annotation. 
Most active learning approaches work 
by first training a seed learner (or family of 
learners) and then running the learner(s) over a 
set of unlabeled samples.   A sample is 
presumed to be more useful for training the 
more uncertain its classification label is.  
Uncertainty can be judged by the relative 
weights assigned to different labels by a single 
classifier (Lewis and Catlett, 1994).  Another 
approach, committee-based sampling, first 
creates a committee of classifie rs and then 
judges classification uncertainty according to 
how much the learners differ among label 
assignments. For example, Dagan and Engelson 
(1995) describe a committee-based sampling 
technique where a part of speech tagger is 
trained using an annotated seed corpus.  A 
family of taggers is then generated by randomly 
permuting the tagger probabilities, and the 
disparity among tags output by the committee 
members is used as a measure of classification 
uncertainty.   Sentences for human annotation 
are drawn, biased to prefer those containing high 
uncertainty instances. 
 While active learning has been shown to 
work for a number of tasks, the majority of 
active learning experiments in natural language 
processing have been conducted using very 
small seed corpora and sets of unlabeled 
examples.  Therefore, we wish to explore 
situations where we have, or can afford, a non-
negligible sized training corpus (such as for 
part-of-speech tagging) and have access to very 
large amounts of unlabeled data.  
 We can use bagging (Breiman, 1996), a 
technique for generating a committee of 
classifiers, to assess the label uncertainty of a 
potential training instance.  With bagging, a 
variant of the original training set is constructed 
by randomly sampling sentences with 
replacement from the source training set in order 
to produce N new training sets of size equal to 
the original. After the N models have been 
trained and run on the same test set, their 
classifications for each test sentence can be 
compared for classification agreement.  The 
higher the disagreement between classifiers, the 
more useful it would be to have an instance
0%
1%
10%
100%
0.95 0.96 0.97 0.98 0.99 1.00
Test Accuracy
T
r
a
i
n
i
n
g
 
D
a
t
a
 
U
s
e
d
Sequential
Sampling from 5M
Sampling from 10M
Sampling from 100M
 
Figure 4.  Active Learning with Large Corpora 
manually labeled. 
 We used the na?ve Bayes classifier, 
creating 10 classifiers each trained on bags 
generated from an initial one million words of 
labeled training data.  We present the active 
learning algorithm we used below.  
 
 
Initialize: Training data consists of X words 
correctly labeled 
Iterate : 
1) Generate a committee of classifiers using 
bagging on the training set  
 
2) Run the committee on unlabeled portion of 
the training set 
3) Choose M instances from the unlabeled set 
for labeling - pick the M/2 with the greatest 
vote entropy and then pick another M/2 
randomly ? and add to training set 
  
 
We initially tried selecting the M most 
uncertain examples, but this resulted in a sample 
too biased toward the difficult instances.  
Instead we pick half of our samples for 
annotation randomly and the other half from 
those whose labels we are most uncertain of, as 
judged by the entropy of the votes assigned to 
the instance by the committee.  This is, in effect, 
biasing our sample toward instances the 
classifiers are most uncertain of. 
We show the results from sample 
selection for confusion set disambiguation in 
Figure 4.  The line labeled "sequential" shows 
test set accuracy achieved for different 
percentages of the one billion word training set, 
where training instances are taken at random.  
We ran three active learning experiments, 
increasing the size of the total unlabeled training 
corpus from which we can pick samples to be 
annotated.  In all three cases, sample selection 
outperforms sequential sampling.  At the 
endpoint of each training run in the graph, the 
same number of samples has been annotated for 
training.  However, we see that the larger the 
pool of candidate instances for annotation is, the 
better the resulting accuracy.  By increasing the 
pool of unlabeled training instances for active 
learning, we can improve accuracy with only a 
fixed additional annotation cost. Thus it is 
possible to benefit from the availability of 
extremely large corpora without incurring the 
full costs of annotation, training time, and 
representation size.  
5.2 Weakly Supervised Learning 
While the previous section shows that we can 
benefit from substantially larger training corpora 
without needing significant additional manual 
annotation, it would be ideal if we could 
improve classification accuracy using only our 
seed annotated corpus and the large unlabeled 
corpus, without requiring any additional hand 
labeling.  In this section we turn to unsupervised 
learning in an attempt to achieve this goal. 
Numerous approaches have been explored for 
exploiting situations where some amount of 
annotated data is available  and a much larger 
amount of data exists unannotated, e.g. 
Marialdo's HMM part-of-speech tagger training 
(1994), Charniak's parser retraining experiment 
(1996), Yarowsky's seeds for word sense 
disambiguation (1995) and Nigam et als (1998) 
topic classifier learned in part from unlabelled 
documents.  A nice discussion of this general 
problem can be found in Mitchell (1999). 
 The question we want to answer is 
whether there is something to be gained by 
combining unsupervised and supervised learning 
when we scale up both the seed corpus and the 
unlabeled corpus significantly.  We can again 
use a committee of bagged classifiers, this time 
for unsupervised learning.  Whereas with active 
learning we want to choose the most uncertain 
instances for human annotation, with 
unsupervised learning we want to choose the 
instances that have the highest probability of 
being correct for automatic labeling and 
inclusion in our labeled training data. 
In Table 2, we show the test set 
accuracy (averaged over the four most 
frequently occurring confusion pairs) as a 
function of the number of classifiers that agree 
upon the label of an instance. For this 
experiment, we trained a collection of 10 na?ve 
Bayes classifiers, using bagging on a 1-million-
word seed corpus.  As can be seen, the greater 
the classifier agreement, the more likely it is that 
a test sample has been correctly labeled. 
 
Classifiers 
 In Agreement 
Test  
Accuracy 
10 0.8734 
9 0.6892 
8 0.6286 
7 0.6027 
6 0.5497 
5 0.5000 
Table 2. Committee Agreement vs. Accuracy 
 
Since the instances in which all bags agree have 
the highest probability of being correct, we 
attempted to automatically grow our labeled 
training set using the 1-million-word labeled 
seed corpus along with the collection of na?ve 
Bayes classifiers described above. All instances 
from the remainder of the corpus on which all 
10 classifiers agreed were selected, trusting the 
agreed-upon label. The classif iers were then 
retrained using the labeled seed corpus plus the 
new training material collected automatically 
during the previous step.  
 In Table 3 we show the results from 
these unsupervised learning experiments for two 
confusion sets.  In both cases we gain from 
unsupervised training compared to using only 
the seed corpus, but only up to a point.  At this 
point, test set accuracy begins to decline as 
additional training instances are automatically 
harvested.  We are able to attain improvements 
in accuracy for free using unsupervised learning, 
but unlike our learning curve experiments using 
correctly labeled data, accuracy does not 
continue to improve with additional data.   
 {then, than} {among, between} 
 Test  
Accuracy 
% Total 
Training Data 
Test  
Accuracy 
% Total  
Training Data 
106-wd labeled seed corpus 0.9624 0.1 0.8183 0.1 
seed+5x106 wds, unsupervised 0.9588 0.6 0.8313 0.5 
seed+107 wds, unsupervised 0.9620 1.2 0.8335 1.0 
seed+108 wds, unsupervised 0.9715 12.2 0.8270 9.2 
seed+5x108 wds, unsupervised 0.9588 61.1 0.8248 42.9 
109 wds, supervised 0.9878 100 0.9021 100 
Table 3. Committee-Based Unsupervised Learning 
 
 Charniak (1996) ran an experiment in 
which he trained a parser on one million words 
of parsed data, ran the parser over an additional 
30 million words, and used the resulting parses 
to reestimate model probabilities.  Doing so 
gave a small improvement over just using the 
manually parsed data.  We repeated this 
experiment with our data, and show  the 
outcome in Table 4.  Choosing only the labeled 
instances most likely to be correct as judged by 
a committee of classifiers results in higher 
accuracy than using all instances classified by a 
model trained with the labeled seed corpus. 
 
 Unsupervised:    
All Labels 
Unsupervised: 
Most Certain Labels 
 {then, than} 
107  words 0.9524 0.9620 
108  words 0.9588 0.9715 
5x108 words 0.7604 0.9588 
 {among, between} 
107  words 0.8259 0.8335 
108  words 0.8259 0.8270 
5x108 words 0.5321 0.8248 
Table 4. Comparison of Unsupervised Learning 
Methods 
 In applying unsupervised learning to 
improve upon a seed-trained method, we 
consistently saw an improvement in 
performance followed by a decline. This is 
likely due to eventually having reached a point 
where the gains from additional training data are 
offset by the sample bias in mining these 
instances.  It may be possible to combine active 
learning with unsupervised learning as a way to 
reduce this sample bias and gain the benefits of 
both approaches. 
6 Conclusions  
In this paper, we have looked into what happens 
when we begin to take advantage of the large 
amounts of text that are now readily available. 
We have shown that for a prototypical natural 
language classification task,  the performance of 
learners can benefit significantly from much 
larger training sets.  We have also shown that 
both active learning and unsupervised learning 
can be used to attain at least some of the 
advantage that comes with additional training 
data, while minimizing the cost of additional 
human annotation. We propose that a logical 
next step for the research community would be 
to direct efforts towards increasing the size of 
annotated training collections, while 
deemphasizing the focus on comparing different 
learning techniques trained only on small 
training corpora.  While it is encouraging that 
there is a vast amount of on-line text, much 
work remains to be done if we are to learn how 
best to exploit this resource to improve natural 
language processing. 
References 
Banko, M. and Brill, E. (2001). Mitigating the 
Paucity of Data Problem. Human Language 
Technology. 
Breiman L., (1996). Bagging Predictors, Machine 
Learning 24 123-140. 
Brill, E. and Wu, J. (1998). Classifier combination 
for improved lexical disambiguation. In 
Proceedings of the 17th International Conference 
on Computational Linguistics. 
Charniak, E. (1996). Treebank Grammars , 
Proceedings AAAI-96 , Menlo Park, Ca. 
Dagan, I. and Engelson, S. (1995). Committee-based 
sampling for training probabilistic classifiers. In 
Proc. ML-95, the 12th Int. Conf. on Machine 
Learning. 
Gale, W. A., Church, K. W., and Yarowsky, D. 
(1993). A method for disambiguating word senses 
in a large corpus. Computers and the Humanities, 
26:415--439. 
Golding, A. R. (1995). A Bayesian hybrid method for 
context-sensitive spelling correction. In Proc. 3rd 
Workshop on Very Large Corpora, Boston, MA. 
Golding, A. R. and Roth, D.(1999),  A Winnow-
Based Approach to Context-Sensitive Spelling 
Correction.  Machine Learning, 34:107--130. 
Golding, A. R. and Schabes, Y. (1996). Combining 
trigram-based and feature-based methods for 
context-sensitive spelling correction. In Proc. 34th 
Annual Meeting of the Association for 
Computational Linguistics, Santa Cruz, CA. 
Henderson, J. C. and Brill, E. (1999). Exploiting 
diversity in natural language processing: 
combining parsers. In 1999 Joint Sigdat 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora. 
ACL, New Brunswick NJ. 187-194. 
Jones, M. P. and Martin, J. H. (1997). Contextual 
spelling correction using latent semantic analysis. 
In Proc. 5th Conference on Applied Natural 
Language Processing, Washington, DC. 
Lewis , D. D., & Catlett, J. (1994). Heterogeneous 
uncertainty sampling. Proceedings of the Eleventh 
International Conference on Machine Learning 
(pp. 148--156). New Brunswick, NJ: Morgan 
Kaufmann. 
Mangu, L. and Brill, E. (1997). Automatic rule 
acquisition for spelling correction. In Proc. 14th 
International Conference on Machine Learning. 
Morgan Kaufmann. 
Merialdo, B. (1994). Tagging English text with a 
probabilistic model. Computational Linguistics, 
20(2):155--172. 
Mitchell, T. M. (1999), The role of unlabeled data in 
supervised learning , in Proceedings of the Sixth 
International Colloquium on Cognitive Science, 
San Sebastian, Spain. 
Nigam, N., McCallum, A., Thrun, S., and  Mitchell, 
T. (1998). Learning to classify text from labeled 
and unlabeled documents. In Proceedings of the 
Fifteenth National Conference on Artificial 
Intelligence. AAAI Press.. 
Pedersen, T. (2000). A simple approach to building 
ensembles of naive bayesian classifiers for word 
sense disambiguation. In Proceedings of the First 
Meeting of the North American Chapter of the 
Association for Computational Linguistics May 1-
3, 2000, Seattle, WA 
Powers, D. (1997). Learning and application of 
differential grammars. In Proc. Meeting of the 
ACL Special Interest Group in Natural Language 
Learning, Madrid. 
van Halteren, H. Zavrel, J. and Daelemans, W. 
(1998). Improving data driven wordclass tagging 
by system combination. In COLING-ACL'98, 
pages 491497, Montreal, Canada. 
Weng, F., Stolcke, A, & Sankar, A (1998). Efficient 
lattice representation and generation . Proc. Intl. 
Conf. on Spoken Language Processing, vol. 6, pp. 
2531-2534. Sydney, Australia. 
Yarowsky, D. (1994). Decision lists for lexical 
ambiguity resolution: Application to accent 
restoration in Spanish and French . In Proc. 32nd 
Annual Meeting of the Association for 
Computational Linguistics, Las Cruces, NM.  
Yarowsky, D. (1995) Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, pp. 189-196, 1995. 
 
A Unified Framework for Automatic Evaluation using  
N-gram Co-Occurrence Statistics 
 
Radu SORICUT 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292, USA 
radu@isi.edu 
Eric BRILL 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
brill@microsoft.com 
 
Abstract 
In this paper we propose a unified framework 
for automatic evaluation of NLP applications 
using N-gram co-occurrence statistics. The 
automatic evaluation metrics proposed to date 
for Machine Translation and Automatic 
Summarization are particular instances from 
the family of metrics we propose. We show 
that different members of the same family of 
metrics explain best the variations obtained 
with human evaluations, according to the 
application being evaluated (Machine 
Translation, Automatic Summarization, and 
Automatic Question Answering) and the 
evaluation guidelines used by humans for 
evaluating such applications. 
1 Introduction 
With the introduction of the BLEU metric for 
machine translation evaluation (Papineni et al 
2002), the advantages of doing automatic 
evaluation for various NLP applications have 
become increasingly appreciated: they allow for 
faster implement-evaluate cycles (by by-passing 
the human evaluation bottleneck), less variation in 
evaluation performance due to errors in human 
assessor judgment, and, not least, the possibility of 
hill-climbing on such metrics in order to improve 
system performance (Och 2003). Recently, a 
second proposal for automatic evaluation has come 
from the Automatic Summarization community 
(Lin and Hovy, 2003), with an automatic 
evaluation metric called ROUGE, inspired by 
BLEU but twisted towards the specifics of the 
summarization task. 
An automatic evaluation metric is said to be 
successful if it is shown to have high agreement 
with human-performed evaluations. Human 
evaluations, however, are subject to specific 
guidelines given to the human assessors when 
performing the evaluation task; the variation in 
human judgment is therefore highly influenced by 
these guidelines. It follows that, in order for an 
automatic evaluation to agree with a human-
performed evaluation, the evaluation metric used 
by the automatic method must be able to account, 
at least to some degree, for the bias induced by the 
human evaluation guidelines. None of the 
automatic evaluation methods proposed to date, 
however, explicitly accounts for the different 
criteria followed by the human assessors, as they 
are defined independently of the guidelines used in 
the human evaluations. 
In this paper, we propose a framework for 
automatic evaluation of NLP applications which is 
able to account for the variation in the human 
evaluation guidelines. We define a family of 
metrics based on N-gram co-occurrence statistics, 
for which the automatic evaluation metrics 
proposed to date for Machine Translation and 
Automatic Summarization can be seen as particular 
instances. We show that different members of the 
same family of metrics explain best the variations 
obtained with human evaluations, according to the 
application being evaluated (Machine Translation, 
Automatic Summarization, and Question 
Answering) and the guidelines used by humans 
when evaluating such applications. 
2 An Evaluation Plane for NLP 
In this section we describe an evaluation plane 
on which we place various NLP applications 
evaluated using various guideline packages. This 
evaluation plane is defined by two orthogonal axes 
(see Figure 1): an Application Axis, on which we 
order NLP applications according to the 
faithfulness/compactness ratio that characterizes 
the application?s input and output; and a Guideline 
Axis, on which we order various human guideline 
packages, according to the precision/recall ratio 
that characterizes the evaluation guidelines. 
2.1 An Application Axis for Evaluation 
When trying to define what translating and 
summarizing means, one can arguably suggest that 
a translation is some ?as-faithful-as-possible? 
rendering of some given input, whereas a summary 
is some ?as-compact-as-possible? rendering of 
some given input. As such, Machine Translation 
(MT) and Automatic Summarization (AS) are on 
the extremes of a faithfulness/compactness (f/c) 
ratio between inputs and outputs. In between these 
two extremes lie various other NLP applications: a 
high f/c ratio, although lower than MT?s, 
characterizes Automatic Paraphrasing (paraphrase: 
To express, interpret, or translate with latitude); 
close to the other extreme, a low f/c ratio, although 
higher than AS?s, characterizes Automatic 
Summarization with view-points (summarization 
which needs to focus on a given point of view, 
extern to the document(s) to be summarized). 
Another NLP application, Automatic Question 
Answering (QA), has arguably a close-to-1 f/c 
ratio: the task is to render an answer about the 
thing(s) inquired for in a question (the faithfulness 
side), in a manner that is concise enough to be 
regarded as a useful answer (the compactness 
side). 
2.2 An Guideline Axis for Evaluation 
Formal human evaluations make use of various 
guidelines that specify what particular aspects of 
the output being evaluated are considered 
important, for the particular application being 
evaluated. For example, human evaluations of MT 
(e.g., TIDES 2002 evaluation, performed by NIST) 
have traditionally looked at two different aspects 
of a translation: adequacy (how much of the 
content of the original sentence is captured by the 
proposed translation) and fluency (how correct is 
the proposed translation sentence in the target 
language). In many instances, evaluation 
guidelines can be linearly ordered according to the 
precision/recall (p/r) ratio they specify. For 
example, evaluation guidelines for adequacy 
evaluation of MT  have a low p/r ratio, because of 
the high emphasis on recall (i.e., content is 
rewarded) and low emphasis on precision (i.e., 
verbosity is not penalized); on the other hand, 
evaluation guidelines for fluency of MT have a 
high p/r ratio, because of the low emphasis on 
recall (i.e., content is not rewarded) and high 
emphasis on wording (i.e., extraneous words are 
penalized). Another evaluation we consider in this 
paper, the DUC 2001 evaluation for Automatic 
Summarization (also performed by NIST), had 
specific guidelines for coverage evaluation, which 
means a low p/r ratio, because of the high 
emphasis on recall (i.e., content is rewarded). Last 
but not least, the QA evaluation for correctness we 
discuss in Section 4 has a close-to-1 p/r ratio for 
evaluation guidelines (i.e., both correct content and 
precise answer wording are rewarded). 
When combined, the application axis and the 
guideline axis define a plane in which particular 
evaluations are placed according to their 
application/guideline coordinates. In Figure 1 we 
illustrate this evaluation plane, and the evaluation 
examples mentioned above are placed in this plane 
according to their coordinates.  
3 A Unified Framework for Automatic 
Evaluation 
In this section we propose a family of evaluation 
metrics based on N-gram co-occurrence statistics. 
Such a family of evaluation metrics provides 
flexibility in terms of accommodating both various 
NLP applications and various values of 
precision/recall ratio in the human guideline 
packages used to evaluate such applications. 
3.1 A Precision-focused Family of Metrics 
Inspired by the work of Papineni et al (2002) on 
BLEU, we define a precision-focused family of 
metrics, using as parameter a non-negative integer 
N. Part of the definition includes a list of stop-
words (SW) and a function for extracting the stem 
of a given word (ST).    
Suppose we have a given NLP application for 
which we want to evaluate the candidate answer 
set Candidates for some input sequences, given a 
Figure 1: Evaluation plane for NLP applications 
adequacy evaluation
TIDES?MT(2002)
precision
recall
precision
recall
faithfulness
compactness
low faithfulness
compactness AS
MT
fluency evaluation
TIDES?MT(2002)
QA(2004)
correctness evaluation
coverage evaluation
DUC?AS (2001)
Guideline Axis
  
QA
low high
high
Application
Axis
reference answer set References. For each 
individual candidate answer C, we define S(C,n)  
as the multi-set of n-grams obtained from the 
candidate answer C after stemming the unigrams 
using ST and eliminating the unigrams found in 
SW. We therefore define a precision score: 
? ?
? ?
? ?
? ?=
}{ ),(
}{ ),(
)(
)(
)(
CandidatesC nCSngram
CandidatesC nCSngram
clip
ngramCount
ngramCount
nP
 
where Count(ngram) is the number of n-gram 
counts, and Countclip(ngram) is the maximum 
number of co-occurrences of ngram in the 
candidate answer and its reference answer. 
Because the denominator in the P(n) formula 
consists of a sum over the proposed candidate 
answers, this formula is a precision-oriented 
formula, penalizing verbose candidates.  This 
precision score, however, can be made artificially 
higher when proposing shorter and shorter 
candidate answers. This is offset by adding a 
brevity penalty, BP: 
??
?
<?
??= ? ||||,
||||,1
|)|/||1( rcBife
rcBif
BP cBr
 
where |c| equals the sum of the lengths of the 
proposed answers, |r| equals the sum of the lengths 
of the reference answers, and B is a brevity 
constant.  
We define now a precision-focused family of 
metrics, parameterized by a non-negative integer 
N, as: 
)))(log(exp()(
1
nPwBPNPS
N
n
n?
=
?=
 
This family of metrics can be interpreted as a 
weighted linear average of precision scores for 
increasingly longer n-grams.  As the values of the 
precision scores decrease roughly exponentially 
with the increase of N, the logarithm is needed to 
obtain a linear average. Note that the metrics of 
this family are well-defined only for N?s small 
enough to yield non-zero P(n) scores. For test 
corpora of reasonable size, the metrics are usually 
well-defined for N?4. 
The BLEU proposed by Papineni et al (2002) 
for automatic evaluation of machine translation is 
part of the family of metrics PS(N), as the 
particular metric obtained when N=4, wn?s are 1/N, 
the brevity constant B=1, the list of stop-words SW 
is empty, and the stemming function ST is the 
identity function.  
3.2 A Recall-focused Family of Metrics 
As proposed by Lin and Hovy (2003), a 
precision-focused metric such as BLEU can be 
twisted such that it yields a recall-focused metric. 
In a similar manner, we define a recall-focused 
family of metrics, using as parameter a non-
negative integer N, with a list of stop-words (SW) 
and a function for extracting the stem of a given 
word (ST) as part of the definition. 
As before, suppose we have a given NLP 
application for which we want to evaluate the 
candidate answer set Candidates for some input 
sequences, given a reference answer set 
References. For each individual reference answer 
R, we define S(R,n)  as the multi-set of n-grams 
obtained from the reference answer R after 
stemming the unigrams using ST and eliminating 
the unigrams found in SW. We therefore define a 
recall score as: 
? ?
? ?
? ?
? ?=
}{Re ),(
}{Re ),(
)(
)(
)(
ferencesR nRSngram
ferencesR nRSngram
clip
ngramCount
ngramCount
nR  
where, as before, Count(ngram) is the number of 
n-gram counts, and Countclip(ngram) is the 
maximum number of co-occurrences of ngram in 
the reference answer and its corresponding 
candidate answer. Because the denominator in the 
R(n) formula consists of a sum over the reference 
answers, this formula is essentially a recall-
oriented formula, which penalizes incomplete 
candidates. This recall score, however, can be 
made artificially higher when proposing longer and 
longer candidate answers. This is offset by adding 
a wordiness penalty, WP: 
??
?
>?
??= ? ||||,
||||,1
|)|/||1( rcWife
rcWif
WP rcW  
where |c| and |r| are defined as before, and W is a 
wordiness constant. 
We define now a recall-focused family of 
metrics, parameterized by a non-negative integer 
N, as: 
)))(log(exp()(
1
nRwWPNRS
N
n
n?
=
?=  
This family of metrics can be interpreted as a 
weighted linear average of recall scores for 
increasingly longer n-grams.  For test corpora of 
reasonable size, the metrics are usually well-
defined for N?4. 
The ROUGE metric proposed by Lin and Hovy 
(2003) for automatic evaluation of machine-
produced summaries is part of the family of 
metrics RS(N), as the particular metric obtained 
when N=1, wn?s are 1/N, the wordiness constant 
W=?, the list of stop-words SW is their own , and 
the stemming function ST is the one defined by the 
Porter stemmer (Porter 1980). 
3.3 A Unified Framework for Automatic 
Evaluation 
The precision-focused metric family PS(N) and 
the recall-focused metric family RS(N) defined in 
the previous sections are unified under the metric 
family AEv(?,N), defined as: 
)()1()(
)()(),(
NPSNRS
NPSNRSNAEv ??+?= ???  
This formula extends the well-known F-measure 
that combines recall and precision numbers into a 
single number (van Rijsbergen, 1979), by 
combining recall and precision metric families into 
a single metric family. For ?=0, AEv(?,N) is the 
same as the recall-focused family of metrics 
RS(N); for ?=1, AEv(?, ?N) is the same as the 
precision-focused family of metrics PS(N). For ? 
in between 0 and 1, AEv(?,N) are metrics that 
balance recall and precision according to ?. For the 
rest of the paper, we restrict the parameters of the 
AEv(?,N) family as follows: ? varies continuously 
in [0,1], N varies discretely in {1,2,3,4}, the linear 
weights wn are 1/N, the brevity constant is 1, the 
wordiness constant is 2, the list of stop-words SW 
is our own 626 stop-word list, and the stemming 
function ST is the one defined by the Porter 
stemmer (Porter 1980). 
We establish a correspondence between the 
parameters of the family of metrics AEv(?,N) and 
the evaluation plane in Figure 1 as follows: ? 
parameterizes the guideline axis (x-axis) of the 
plane, such that ?=0 corresponds to a low 
precision/recall (p/r) ratio, and ?=1 corresponds to 
a high p/r ratio; N parameterizes the application 
axis (y-axis) of the plane, such that N=1 
corresponds to a low faithfulness/compactness (f/c) 
ratio (unigram statistics allow for a low 
representation of faithfulness, but a high 
representation of compactness), and N=4 
corresponds to a high f/c ratio (n-gram statistics up 
to 4-grams allow for a high representation of 
faithfulness, but a low representation of 
compactness). 
This framework enables us to predict that a 
human-performed evaluation is best approximated 
by metrics that have similar f/c ratio as the 
application being evaluated and similar p/r ratio as 
the evaluation package used by the human 
assessors. For example, an application with a high 
f/c ratio, evaluated using a low p/r ratio evaluation 
guideline package (an example of this is the 
adequacy evaluation for MT in TIDES 2002), is 
best approximated by the automatic evaluation 
metric defined by a low ? and a high N; an 
application with a close-to-1 f/c ratio, evaluated 
using an evaluation guideline package 
characterized by a close-to-1 p/r ratio (such as the 
correctness evaluation for Question Answering in 
Section 4.3) is best approximated by an automatic 
metric defined by a median ? and a median N. 
4 Evaluating the Evaluation Framework 
In this section, we present empirical results 
regarding the ability of our family of metrics to 
approximate human evaluations of various 
applications under various evaluation guidelines.  
We measure the amount of approximation of a 
human evaluation by an automatic evaluation as 
the value of the coefficient of determination R2 
between the human evaluation scores and the 
automatic evaluation scores for various systems 
implementing Machine Translation, 
Summarization, and Question Answering 
applications. In this framework, the coefficient of 
determination R2 is to be interpreted as the 
percentage from the total variation of the human 
evaluation (that is, why some system?s output is 
better than some other system?s output, from the 
human evaluator?s perspective) that is captured by 
the automatic evaluation (that is, why some 
system?s output is better than some other system?s 
output, from the automatic evaluation perspective). 
The values of R2 vary between 0 and 1, with a 
value of 1 indicating that the automatic evaluation 
explains perfectly the human evaluation variation, 
and a value of 0 indicating that the automatic 
evaluation explains nothing from the human 
evaluation variation. All the results for the values 
of R2 for the family of metrics AEv(?,N) are 
reported with ? varying from 0 to 1 in 0.1 
increments, and N varying from 1 to 4. 
 
4.1 Machine Translation Evaluation 
The Machine Translation evaluation carried out 
by NIST in 2002 for DARPA?s TIDES programme 
involved 7 systems that participated in the 
Chinese-English track. Each system was evaluated 
by a human judge, using one reference extracted 
from a list of 4 available reference translations. 
Each of the 878 test sentences was evaluated both 
for adequacy (how much of the content of the 
original sentence is captured by the proposed 
translation) and fluency (how correct is the 
proposed translation sentence in the target 
language). From the publicly available data for this 
evaluation (TIDES 2002), we compute the values 
of R2 for 7 data points (corresponding to the 7 
systems participating in the Chinese-English 
track), using as a reference set one of the 4 sets of 
reference translations available. 
In Table 1, we present the values of the 
coefficient of determination R2 for the family of 
metrics AEv(?,N), when considering only the 
fluency scores from the human evaluation. As 
mentioned in Section 2, the evaluation guidelines 
for fluency have a high precision/recall ratio, 
whereas MT is an application with a high 
faithfulness/compactness ratio. In this case, our 
evaluation framework predicts that the automatic 
evaluation metrics that explain most of the 
variation in the human evaluation must have a high 
? and a high N. As seen in Table 1, our evaluation 
framework correctly predicts the automatic 
evaluation metrics that explain most of the 
variation in the human evaluation: metrics 
AEv(1,3), AEv(0.9,3), and AEv(1,4) capture most 
of the variation: 79.04%, 78.94%, and 78.87%, 
respectively. Since metric AEv(1,4) is almost the 
same as the BLEU metric (modulo stemming and 
stop word elimination for unigrams), our results 
confirm the current practice in the Machine 
Translation community, which commonly uses 
BLEU for automatic evaluation. For comparison 
purposes, we also computed the value of R2 for 
fluency using the BLEU score formula given in 
(Papineni et al, 2002), for the 7 systems using the 
same one reference, and we obtained a similar 
value, 78.52%; computing the value of R2 for 
fluency using the BLEU scores computed with all 4 
references available yielded a lower value for R2, 
64.96%, although BLEU scores obtained with 
multiple references are usually considered more 
reliable.  
In Table 2, we present the values of the 
coefficient of determination R2 for the family of 
metrics AEv(?,N), when considering only the 
adequacy scores from the human evaluation. As 
mentioned in Section 2, the evaluation guidelines 
for adequacy have a low precision/recall ratio, 
whereas MT is an application with high 
faithfulness/compactness ratio. In this case, our 
evaluation framework predicts that the automatic 
evaluation metrics that explain most of the 
variation in the human evaluation must have a low 
? and a high N. As seen in Table 2, our evaluation 
framework correctly predicts the automatic 
evaluation metric that explains most of the 
variation in the human evaluation: metric AEv(0,4) 
captures most of the variation, 83.04%. For 
comparison purposes, we also computed the value 
of R2 for adequacy using the BLEU score formula 
given in (Papineni et al, 2002), for the 7 systems 
using the same one reference, and we obtain a 
similar value, 83.91%; computing the value of R2 
for adequacy using the BLEU scores computed 
with all 4 references available also yielded a lower 
value for R2, 62.21%. 
4.2 Automatic Summarization Evaluation 
The Automatic Summarization evaluation 
carried out by NIST for the DUC 2001 conference 
involved 15 participating systems. We focus here 
on the multi-document summarization task, in 
which 4 generic summaries (of 50, 100, 200, and 
400 words) were required for a given set of 
documents on a single subject. For this evaluation 
30 test sets were used, and each system was 
evaluated by a human judge using one reference 
extracted from a list of 2 reference summaries.  
One of the evaluations required the assessors to 
judge the coverage of the summaries. The 
coverage of a summary was measured by 
comparing a system?s units versus the units of a 
reference summary, and assessing whether each 
system unit expresses all, most, some, hardly any, 
or none of the current reference unit. A final 
evaluation score for coverage was obtained using a 
coverage score computed as a weighted recall 
score (see (Lin and Hovy 2003) for more 
information on the human summary evaluation). 
From the publicly available data for this evaluation 
(DUC 2001), we compute the values of R2 for 15 
data points available (corresponding to the 15 
participating systems). 
In Tables 3-4 we present the values of the 
coefficient of determination R2 for the family of 
metrics AEv(?,N), when considering the coverage 
4 76.10 76.45 76.78 77.10 77.40 77.69 77.96 78.21 78.45 78.67 78.87 
3 76.11 76.6 77.04 77.44 77.80 78.11 78.38 78.61 78.80 78.94 79.04 
2 73.19 74.21 75.07 75.78 76.32 76.72 76.96 77.06 77.03 76.87 76.58 
1 31.71 38.22 44.82 51.09 56.59 60.99 64.10 65.90 66.50 66.12 64.99 
N/? 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Table 1: R2 values for the family of metrics AEv(?,N), for fluency scores in MT evaluation 
 
4 83.04 82.58 82.11 81.61 81.10 80.56 80.01 79.44 78.86 78.26 77.64 
3 81.80 81.00 80.16 79.27 78.35 77.39 76.40 75.37 74.31 73.23 72.11 
2 80.84 79.46 77.94 76.28 74.51 72.63 70.67 68.64 66.55 64.42 62.26 
1 62.16 66.26 69.18 70.59 70.35 68.48 65.24 60.98 56.11 50.98 45.88 
N/? 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Table 2: R2 values for the family of metrics AEv(?,N), for adequacy scores in MT evaluation 
scores from the human evaluation, for summaries 
of 200 and 400 words, respectively (the values of 
R2 for summaries of 50 and 100 words show 
similar patterns). As mentioned in Section 2, the 
evaluation guidelines for coverage have a low 
precision/recall ratio, whereas AS is an application 
with low faithfulness/compactness ratio.  In this 
case, our evaluation framework predicts that the 
automatic evaluation metrics that explain most of 
the variation in the human evaluation must have a 
low ? and a low N. As seen in Tables 3-4, our 
evaluation framework correctly predicts the 
automatic evaluation metric that explain most of 
the variation in the human evaluation: metric 
AEv(0,1) explains 90.77% and 92.28% of the 
variation in the human evaluation of summaries of 
length 200 and 400, respectively. Since metric 
AEv(0, 1) is almost the same as the ROUGE metric 
proposed by Lin and Hovy (2003) (they only differ 
in the stop-word list they use), our results also 
confirm the proposal for such metrics to be used 
for automatic evaluation by the Automatic 
Summarization community. 
4.3 Question Answering Evaluation 
One of the most common approaches to 
automatic question answering (QA) restricts the 
domain of questions to be handled to so-called 
factoid questions. Automatic evaluation of factoid 
QA is often straightforward, as the number of 
correct answers is most of the time limited, and 
exhaustive lists of correct answers are available. 
When removing the factoid constraint, however, 
the set of possible answer to a (complex, beyond-
factoid) question becomes unfeasibly large, and 
consequently automatic evaluation becomes a 
challenge.  
In this section, we focus on an evaluation carried 
out in order to assess the performance of a QA 
system for answering questions from the 
Frequently-Asked-Question (FAQ) domain 
(Soricut and Brill, 2004). These are generally 
questions requiring a more elaborated answer than 
a simple factoid (e.g., questions such as: ?How 
does a film qualify for an Academy Award??). 
In order to evaluate such a system a human-
performed evaluation was performed, in which 11 
versions of the QA system (various modules were 
implemented using various algorithms) were 
separately evaluated. Each version was evaluated 
by a human evaluator, with no reference answer 
available. For this evaluation 115 test questions 
were used, and the human evaluator was asked to 
assess whether the proposed answer was correct, 
somehow related, or wrong. A unique ranking 
number was achieved using a weighted average of 
the scored answers. (See (Soricut and Brill, 2004) 
for more details concerning the QA task and the 
evaluation procedure.) 
One important aspect in the evaluation procedure 
was devising criteria for assigning a rating to an 
answer which was not neither correct nor wrong. 
One of such cases involved so-called flooded 
answers: answers which contain the correct 
information, along with several other unrelated 
pieces of information. A first evaluation has been 
carried with a guideline package asking the human 
assessor to assign the rating correct to flooded 
answers. In Table 5, we present the values of the 
coefficient of determination R2 for the family of 
metrics AEv(?,N) for this first QA evaluation. On 
the guideline side, the guideline package used in 
this first QA evaluation has a low precision/recall 
ratio, because the human judge is asked to evaluate 
based on the content provided by a given answer 
(high recall), but is asked to disregard the 
conciseness (or lack thereof) of the answer (low 
precision); consequently, systems that focus on 
4 67.10 66.51 65.91 65.29 64.65 64.00 63.34 62.67 61.99 61.30 60.61
3 69.55 68.81 68.04 67.24 66.42 65.57 64.69 63.79 62.88 61.95 61.00
2 74.43 73.29 72.06 70.74 69.35 67.87 66.33 64.71 63.03 61.30 59.51
1 90.77 90.77 90.66 90.42 90.03 89.48 88.74 87.77 86.55 85.05 83.21
N/? 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Table 3: R2 for the family of metrics AEv(?,N), for coverage scores in AS evaluation (200 words) 
 
4 81.24 81.04 80.78 80.47 80.12 79.73 79.30 78.84 78.35 77.84 77.31
3 84.72 84.33 83.86 83.33 82.73 82.08 81.39 80.65 79.88 79.07 78.24
2 89.54 88.56 87.47 86.26 84.96 83.59 82.14 80.65 79.10 77.53 75.92
1 92.28 91.11 89.70 88.07 86.24 84.22 82.05 79.74 77.30 74.77 72.15
N/? 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Table 4: R2 for the family of metrics AEv(?,N), for coverage scores in AS evaluation (400 words) 
giving correct and concise answers are not 
distinguished from systems that give correct 
answers, but have no regard for concision. On the 
application side, as mentioned in Section 2, QA is 
arguably an application characterized by a close-
to-1 faithfulness/compactness ratio. In this case, 
our evaluation framework predicts that the 
automatic evaluation metrics that explain most of 
the variation in the human evaluation must have a 
low ? and a median N. As seen in Table 5, our 
evaluation framework correctly predicts the 
automatic evaluation metric that explain most of 
the variation in the human evaluation: metric 
AEv(0,2) explains most of the human variation, 
91.72%. Note that other members of the AEv( ??,N) 
family do not explain nearly as well the variation 
in the human evaluation. For example, the 
ROUGE-like metric AEv(0,1) explains only 
61.61% of the human variation, while the BLEU-
like metric AEv(1,4) explains a mere 17.7% of the 
human variation (to use such a metric in order to 
automatically emulate  the human QA evaluation is 
close to performing an evaluation assigning 
random ratings to the output answers). 
In order to further test the prediction power of 
our evaluation framework, we carried out a second 
QA evaluation, using a different evaluation 
guideline package: a flooded answer was rated 
only somehow-related. In Table 6, we present the 
values of the coefficient of determination R2 for 
the family of metrics AEv(?,N) for this second QA 
evaluation. Instead of performing this second 
evaluation from scratch, we actually simulated it 
using the following methodology: 2/3 of the output 
answers rated correct of the systems ranked 1st, 2nd, 
3rd, and 6th by the previous human evaluation have 
been intentionally over-flooded using two long and 
out-of-context sentences, while their ratings were 
changed from correct to somehow-related. Such a 
change simulated precisely the change in the 
guideline package, by downgrading flooded 
answers. This means that, on the guideline side, the 
guideline package used in this second QA 
evaluation has a close-to-1 precision/recall ratio, 
because the human judge evaluates now based both 
on the content and the conciseness of a given 
answer. At the same time, the application remains 
unchanged, which means that on the application 
side we still have a close-to-1 
faithfulness/compactness ratio. In this case, our 
evaluation framework predicts that the automatic 
evaluation metrics that explain most of the 
variation in the human evaluation must have a 
median ? and a median N. As seen in Table 6, our 
evaluation framework correctly predicts the 
automatic evaluation metric that explain most of 
the variation in the human evaluation: metric 
AEv(0.3,2) explains most of the variation in the 
human evaluation, 86.26%. Also note that, while 
the R2 values around AEv(0.3,2) are still 
reasonable, evaluation metrics that are further and 
further away from it have increasingly lower R2 
values, meaning that they are more and more 
unreliable for this task. The high correlation of 
metric AEv(0.3,2) with human judgment, however, 
suggests that such a metric is a good candidate for 
performing automatic evaluation of  QA systems 
that go beyond answering factoid questions. 
5 Conclusions 
In this paper, we propose a unified framework 
for automatic evaluation based on N-gram co-
occurrence statistics, for NLP applications for 
which a correct answer is usually an unfeasibly 
large set (e.g., Machine Translation, Paraphrasing, 
Question Answering, Summarization, etc.). The 
success of BLEU in doing automatic evaluation of 
machine translation output has often led 
researchers to blindly try to use this metric for 
evaluation tasks for which it was more or less 
4 63.40 57.62 51.86 46.26 40.96 36.02 31.51 27.43 23.78 20.54 17.70 
3 81.39 76.38 70.76 64.76 58.61 52.51 46.63 41.09 35.97 31.33 27.15 
2 91.72 89.21 85.54 80.78 75.14 68.87 62.25 55.56 49.04 42.88 37.20 
1 61.61 58.83 55.25 51.04 46.39 41.55 36.74 32.12 27.85 23.97 20.54 
N/? 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Table 5: R2 for the family of metrics AEv(?,N), for correctness scores, first QA evaluation 
 
4 79.94 79.18 75.80 70.63 64.58 58.35 52.39 46.95 42.11 37.87 34.19 
3 76.15 80.44 81.19 78.45 73.07 66.27 59.11 52.26 46.08 40.68 36.04 
2 67.76 77.48 84.34 86.26 82.75 75.24 65.94 56.65 48.32 41.25 35.42 
1 56.55 60.81 59.60 53.56 45.38 37.40 30.68 25.36 21.26 18.12 15.69 
N/? 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Table 6: R2 for the family of metrics AEv(?,N), for correctness scores, second QA evaluation 
appropriate (see, e.g., the paper of Lin and Hovy 
(2003), in which the authors start with the 
assumption that BLEU might work for 
summarization evaluation, and discover after 
several trials a better candidate). 
 Our unifying framework facilitates the 
understanding of when various automatic 
evaluation metrics are able to closely approximate 
human evaluations for various applications. Given 
an application app and an evaluation guideline 
package eval, the faithfulness/compactness ratio of 
the application and the precision/recall ratio of the 
evaluation guidelines determine a restricted area in 
the evaluation plane in Figure 1 which best 
characterizes the (app, eval) pair. We have 
empirically demonstrated that the metrics from the 
AEv( ??,N) family that best approximate human 
judgment are those that have the ? and N 
parameters in the determined restricted area. To 
our knowledge, this is the first proposal regarding 
automatic evaluation in which the automatic 
evaluation metrics are able to account for the 
variation in human judgment due to specific 
evaluation guidelines. 
References  
DUC. 2001. The Document Understanding 
Conference. http://duc.nist.gov. 
C.Y. Lin and E. H. Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
Occurrence Statistics. In Proceedings of the 
HLT/NAACL 2003: Main Conference, 150-156. 
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 
2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In 
Proceedings of the ACL 2002, 311-318. 
M. F. Porter. 1980. An algorithm for Suffix 
Stripping. Program, 14: 130-137. 
F. J. Och. 2003. Minimum Error Rate Training for 
Statistical Machine Translation. In Proceedings 
of the ACL 2003, 160-167. 
R. Soricut and E. Brill. 2004. Automatic Question 
Answering: Beyond the Factoid. In Proceedings 
of the HLT/NAACL 2004: Main Conference, 57-
64. 
TIDES. 2002. The Translingual Information 
Detection, Extraction, and Summarization 
programme. http://tides.nist.gov. 
C. J. van Rijsbergen. 1979. Information Retrieval. 
London: Butterworths. Second Edition. 
 
Eric Brill 
Microsoft Research 
One Microsoft Way 
Redmond, Wa. 98052 
brill@microsoft.com 
'C 
1 
Abstract 
A wide range of natural anguage problems 
can be viewed as disambiguating between a
small set of alternatives based upon the 
string context surrounding the ambiguity 
site. In this paper we demonstrate that 
classification accuracy can be improved by 
invoking a more descriptive feature set than 
what is typically used. We present a 
technique that disambiguates by learning 
regular expressions describing the stnng 
contexts in which the ambiguity sites 
appear. 
Introduction 
Many natural language tasks are essentially n- 
way classification problems, where classification 
decisions are made from a small set of choices, 
based upon the linguistic context in which the 
ambiguity site occurs. Examples of such tasks 
include: confusable word set disambiguation; 
word sense disambiguation; determining such 
lexical features as pronoun case and determiner 
number for machine translation; part of speech 
tagging; named entity labeling; spelling 
correction; and some formulations of skeletal 
parsing. Very similar feature sets have been 
used across machine learning algorithms and 
across classification problems. For example, in 
confusable word set disambiguation, systems 
typically use as features the occurrence of a 
particular word within a window of +/- n words 
of the target, and collocations based on the 
words and part of speech tags of up to two 
words to the left and two words to the fight of 
the target. 
Below we present a machine learning 
algorithm that learns from a much richer feature 
set than that typically used for classification in 
natural language. Our algorithm learns rule 
sequences for n-way classification, where the 
condition of a rule can be a restricted regular 
expression on the string context in which the 
ambiguity site appears. We demonstrate that 
using this more powerful feature space leads to 
an improvement in disambiguation performance 
on confusable words. 
1 Previous Work: Richer Features 
Most previous work applying machine learning 
to linguistic disambiguafion has used as features 
very local collocational information as well as 
the presence of a word within a fixed window of 
an ambiguity site. Indeed, one of the great 
insights in both speech recognition and natural 
language processing is the realization that fixed 
local cues provide a great deal of useful 
information. 
While the n-gram reins supreme in 
language modeling, there has been some 
interesting work done building language models 
based on linguistically richer features. Bahl, 
Brown et al (1989) describe a language model 
that builds a decision tree that is allowed to ask 
questions about the history up to twenty words 
back. Saul and Pereira (1997) describe a 
language model that can in essence skip over 
uninformative words in the history. Della Pietra 
et al (1994) discuss an approach to language 
modeling based on link grammars, where the 
model can look beyond the two previous words 
to condition on linguistically relevant words in 
the history. The language model described by 
Chelba and Jelinek (1998) similarly conditions 
on linguistically relevant words by assigning 
partial phrase structure to the history and 
percolating headwords. 
Samuellson, Tapanainen et al (1996) 
describe a method for learning a particular 
Pattern-Based Disambiguation for Natural Language Processing 
useful type of pattern, which they call a barrier. 
Given two symbols X and Y, and a set of 
symbols S, they learn conditions of the form: 
take an action if there is an X preceded by a Y, 
with no intervening symbols from S. In their 
paper they demonstrate how such patterns can be 
useful for part of speech tagging. Even-Zohar 
and Roth (2000) show that by including 
linguistic features based on relations such as 
subject and object, they can better disambiguate 
between verb pairs. 
2 Definitions 
Below we provide the standard definition for 
regular expressions, and then define a less 
expressive language formafism, which we will 
refer to as reduced regular expressions. The 
learning method we describe herein can learn 
rules conditioned on any reduced regular 
expression. 
Regular Expression (RE): 1 Given a finite 
alphabet E ,  the set of regular expressions over 
that alphabet is defined as (Hopcroft and Ullman 
1979): 
(1) Va~ E, a is a regular expression and 
denotes the set {a} 
(2) if r and s are regular expressions denoting the 
languages R and S, respectively, then (r+s), (rs), 
and (r*) are regular expressions that denote the 
sets R ~_~ S, RS and R* respectively. 
Reduced Regular Expression (R.RE): Given a 
finite alphabet ~,  the set of reduced regular 
expressions over that alphabet is defined as: 
(1) Va~ E: 
a is an RRE and denotes the set {a} 
a+ is an RRE and denotes the positive 
closure of the set {a} 
a* is an RRE and denotes the Kleene 
closure of the set {a} 
-a is an RRE and denotes the set ~ - a 
~a+ is an RRE and denotes the positive 
closure of the set Z - a 
~a* is an RRE and denotes the Kleene 
closure of the set E - a 
(2) . is an RRE denoting the set E 
(3) .+ is an RRE denoting the positive closure 
of the set E 
(4) .* is an RRE denoting the Kleene closure of 
the set 
(5) if r and s are RREs denoting the languages R 
and S, respectively, then rs is an RRE denoting 
the set RS. 
Some examples of strings that are 
regular expressions but not reduced regular 
expressions include: (ab)*, a(b\]c)d, (a (bc)+)* 
Next, we need some definitions to allow 
us to make reference to particular positions in a 
collection of strings. 
Corpus: A corpus is an ordered set of strings. 
We will notate the jth String of a corpus C as 
C\[j\]. \]C\] is the number of strings in the corpus. 
IqJ\]l is the number of symbols in the jt~ string of 
the corpus. 
Corpus Position.'. A corpus position for a 
corpus C is a tuple (j,k), meaning the k th symbol 
in the jtb string in the corpus, with the 
restrictions: 1_< j _<\[ C\ [and 0 _< k _<\[ CU\] \[. 
A Corpus Position Set is a set of corpus 
positions. 
Next, we define an RRE-Tree, the data 
structure we will use in learning RREs. 
RRE-Tree: An RRE-Tree over E is a tree 
(V,E), where V is a set of tuples <v,S>, v being 
a unique vertex identifier and S being a Corpus 
Position Set, and E is a set of labeled directed 
edges <vi,vj,label>, where vi and vj are vertex 
identifiers, label ~ LABEL_SET and 
LABEL SET = {dot, dot+, dot*} U 
{a,a+,a*,~a,~a+,~a* I 'Va~ E }.2 
3 Rule Sequence Learning 
Our implemented learner is based upon the 
transformation-based l arning paradigm (Bnll 
1995). In this section we briefly review 
transformation-based learning. 
I In all of our formulations, we ignore expressions 
denoting the empty set O and the set {e}. 2 We use "dot" for "." 
2 
(1) A start-state annotator, which assigns an 
initial label to a string. 
(2) A sequence of rules of the form: Change the 
label of a string from m to n if C(string), where 
C is a predicate over strings and m,n ~ L. 
A string is labelled by first applying the 
start-state annotator to it, and then applying each 
rule, in order. 
To learn a transformation sequence, the 
system begins with a properly labelled training 
set. It then removes the labels and applies the 
start-state annotator to each string. Then the 
learner iteratively does the following: 
(1) Find the best rule to apply to the training set. 
(2) Append that rule to the end of the learned 
transformation sequence. 
(3) Apply that rule to the training set. 
4.1 RRE-Tree Construction 
: until the stopping criterion is met. 
4 Learning RRE Rules 
Below we will demonstrate how to learn 
transformation sequences where the predicate 
C(stfing) is of the form "Does RRE R apply to 
the stnng?" We will show this for the binary 
classification case (where ILl = 2). 
In each learning iteration, we will 
construct an RRE-Tree in a particular way, find 
the best node in that RRE-Tree, and then return 
the edge labels on the path from root to best 
node as the learned RRE. The learner will learn 
a sequence of rules of the form: 
Change the label of a string from li to lj if the 
string matches reduced regular expression R. 
Before proceeding, we need to specify 
two things: the start-state annotator and the 
goodness measure for determining what rule is 
best. The system will use a start-state annotator 
that initially labels all strings with the most 
frequent label in the training set, and the 
goodness measure will simply be the number of 
good label changes minus the number of bad 
label changes when a rule is applied to the 
training set. 
Take the following training set: 
String # String True Label Init. Guess 
1 abc  0 1 
2 abb  1 1 
3 baa  1 1 
Since 1 is the most frequent label in the 
training set, the start-state annotator would 
initially assign all three training set strings the 
label 1, meaning stnng 1 would be incorrectly 
labelled and strings 2 and 3 would be correct. 
Now we want to learn a rule whose appfication 
will best improve our labelling of the training 
set. 
We will first present an algorithm for 
constructing an RRE-Tree for a training corpus, 
and then trace through the appficadon of this 
algorithm to our example training corpus above. 
To simplify the presentation, we will limit 
ourselves to learning rules for a weaker language 
type, which we call Very Reduced Regular 
Expressions (VRREs). The extension to RRE 
learning is straightforward. 
Very Reduced Regular Expression (VRRE): 
Given a finite alphabet E,  the set of very 
reduced regular expressions over that alphabet is
defined as: 
(1) 'v'a~ E: a is a VRRE and denotes the set 
{a} 
(2) . is a VRRE denoting the set g 
(3) .* is a VRRE denoting the Kleene closure of 
the set E 
(4) if r and s are VRREs denoting the languages 
R and S, respectively, then rs is a VRRE 
denoting the set RS. 
Say we have a training corpus C. For 
every string C\[j\]~ C, Tmth\[C\[j\]\] ~ {0,1 } is the 
true label of C\[j\] and Guess\[C\[j\]\] is the current 
guess of the label of C\[j\]. The algorithm for one 
iteration of rule learning follows. 
Main() { 
3 
In string classification, the goal is to 
assign the proper label to a string, from a 
prespecified set of labels L. A transformation- 
based system consists of: 
(1) Create root node with corpus position set S = 
{0,0) \ [ j  = 1 .. ICI). Push this node onto 
processing stack (STACK). 
(2) While (STACK not empty) { 
STATE = pop(STACK); 
Push(dotexpand(STATE),STACK); 
Push(dotstarexpand (STATE), STACK); 
Va~ Z 
Push(atomexpand(a, STATE),STACK) 
} 
(3) Find best state S in the RRE-tree. Let R be 
the RRE obtained by following the edges from 
the root to S, outputting each edge label as the 
edge is traversed. Return either the rule "0--~ 1 if 
R" or "1--)0 if R" depending on which is 
appropriate for state S. 
} 
dotexpand(STATE) { 
create new state STATE" 
let P be the corpus position set of STATE 
P' = {0,k) I (j,k-1) E P and k-1 ~ ICorpusfj\]l} 
If (P' not empty) { 
Make P' the corpus position set of 
STATE' 
Add (STATE,STATE' ,DOT) to tree 
edges 
return STATE' 
} 
Else return NULL 
} 
dotstarexpand(STATE) { 
create new state STATE' 
let P be the corpus position set of STATE 
P' = {(j,k) \[ (j,m) ~ P, m_< k, and k _< 
ICorpusU\]l} 
I f (P '?  P) { 
Make P' the corpus position set of STATE' 
Add (STATE,STATE',DOT*) to tree edges 
return STATE' 
} 
Else return NULL 
} 
atomexpand(a, STATE) { 
create new state STATE' 
let P be the corpus position set of STATE 
P' = {(j,k) I ( j ,k-1) E P, k-1 ? Icorpusfj\]l, and 
the k-1 st symbol in Corpus\[j\] is a} 
If (P' not empty) { 
Make P' the corpus position set of 
STATE 
Add (STATE,STATE',a) to tree edges 
return STATE' 
} 
Else return NULL 
} 
Each state S in the RRE-tree represents he RRE 
corresponding to the edge labels on the path 
from root to S. For a state S with corpus 
position set P and corresponding RRE R, the 
goodness of the rule: 0~1 if R, is computed asp 
Goodness_0_to_l (S) = 
(J,*~v Score_0to  1 ((j,k)) 
where 
Score_0_to_ l  ((j ,k)) = 
1 i f  k = Icfj\]l 
^ Guess \ [ j \ ]  = 0 
^ Truth\[ j \ ]  = 1 
-1 if  k = IC\[j\]l 
^ Guess \ [ j \ ]  = 0 
A Truth\[ j \ ]  = 0 
0 o therwise  
Similarly, we can compute the score for the 
rule : 1@0 if R. We then define Goodness(S) =
max(Goodness0_to  l(S),Goodness_l_to_0(S)) 
Returning to our example, for the 3 
strings in this training corpus, the root node of 
the RRE-Tree would have the corpus position 
set: {(1,0),(2,0),(3,0)}. The root node 
corresponds to the null RRE, and so the position 
set consists of the beginning of each string in the 
training set. In figure 1 (at the end of the paper) 
we show a partial RRE-Tree. If we follow the 
edge labelled "dot" from the root node, we see it 
leads to a state with position set 
{(1,1),(2,1),(3,1)}, as a dot advances all 
positions by one. 
3 This assumes an RRE must match the entire string 
in order to accept it. 
4 
The square state in figure 1 represents 
the RRE: "dot* c" and the triangular state 
represents "a dot c". Both the square and 
triangular states have a corpus position set 
consisting of only one corpus position, namely 
the end of stnng 1, and both would have a 
goodness core of 1 for the corresponding l&0 
rule. If We prefer shorter ules, we will learn as 
our first rule in the rule list: l&0  if dot* c. 
After applying this rule to the training corpus, all 
strings will be correctly labelled and training 
will terminate. If the stopping criterion were not 
met, we would apply the learned rule to change 
the values of our Guess array, then create a new 
RRE-tree, find the best state in that tree, and so 
on.  
It is easy to extend the above algorithm 
to learn RREs instead of VRREs. Note, for 
instance, that he corpus position set for a state S 
with incoming edge labelled ~a can be found by 
taking the position set for the sibling of S with 
incoming edge labelled dot and deleting those 
corpus positions that are found in the position 
set for the sibling of S with incoming edge 
labelled a. 
..? 
5 Optimizations 
The algorithm above is exponential. There are 
some opfirnizations we can perform that make it 
feasible to apply the learning algorithm. 
Optimization 1: Pruning states we know 
cannot be on the path from root to the best state. 
Define GoodPotential 0 to I(S) as the number 
of sentences s in the training corpus for which 
Guess\[s\]=0, Truth\[s\]= 1 and 
3k : (s, k) ~ corpus_position_set(S). We can 
similarly define GoodPotential 1 to0(S), and 
then define 
GoodPotential(S)= 
max(GoodPotential 0 to_l(S), 
GoodPotential 1 to O(S)) 
As we construct the RRE-tree, we keep 
track of the largest Goodness(S) we have 
encountered. If that value is X, then for a state 
S', if GoodPotential(S')_<X, it is impossible for 
any path through S' to reach a state with a better 
goodness score than the best found thus far. We 
can check this condition when pushing states 
onto the stack, and when popping off the stack 
to be processed, and if the pruning condition is 
met, the state is discarded. 
Optimization 2: Merging states with identical 
corpus position sets. If we are going to push a 
state onto the stack when a state already exists 
with an identical corpus position set, we do not 
need to retain both states. We may use 
heuristics to decide which of the states with 
identical corpus position sets we should keep 
(such as choosing the one with the shortest path 
to the root). 
6 Experiments 
To test whether learning RREs can improve 
disambiguafion accuracy, we explored the task 
of confusion set disambiguation (Golding and 
Roth 1999). We trained and applied two 
different rule sequence l arners, one which used 
the standard feature set for this problem (e.g. the 
identical feature set to that used in (Golding and 
Roth 1999) and (Mangu and Brill 1997) and 
described in the introduction, and one which 
learned RR.Es. 4 Because we wanted to 
deterinine what could be gained by using RREs, 
we ran an ablation study where we kept 
everything else constant across the two runs, and 
did not use performance enhancing techniques 
such as parameter tuning on held out data or 
classifier combination. 
Both learners were given a window of 
+/- 5 words surrounding the ambiguity site. 
Context was not allowed to cross sentence 
boundaries. The training and test set were 
derived by finding all instances of the 
confusable words in the Brown Corpus, using 
the Penn Treebank parts of speech and 
tokenization (Marcus, Santorini et al 1993), and 
then dividing this set into 80% for training and 
20% for testing. 
For the RRE-based system, we mapped 
the +/- 5 word window of context into a string as 
follows (where wi is a word and ti is a part of 
speech tag): 
4 The set of RREs is a superset of what can be 
learned using the standard feature set. 
5 
Wi. 5 ti. 5 Wi-4 ti. 4 Wi. 3 ti. 3 Wi. 2 ti.2. Wi. I ti. 1 M IDDLE 
Wi+l ti+l wi+2 ti+2 wi+3 ti+3 wi+4 ti+4 wi+5 ti+5 
where MIDDLE is the ambiguity site. 
Both for execution time and space 
considerations for the learner and for fear of 
overtraining, we put a bound on the length of the 
RRE that could be learned, s We define an 
atomic RRE as any RRE derived without any 
concatenation perations. Then the length of an 
RRE is defined as the number of atomic RREs 
which that RRE is made up of. The atom 
"MIDDLE" is not counted in length. 
Below we give two examples of rules 
that were learned for one confusion set:  6 
(1) past ~ passed if .* ~DT MIDDLE DOT IN 
(2) past ~ passed if (~to)* NN MIDDLE 
The first rule says to change the 
disambiguation guess to << passed >> if the word 
before is not a determiner and the word after is a 
preposition. This matches contexts uch as : << 
... they passed by ... >> while not matching 
contexts uch as : << ... made in the past by ... >> 
The  second rule captures contexts uch as : << ... 
the hike passed the campground ... >~ while not 
matching contexts uch as : << ... want to take a 
hike past the campground... >> 
In Table 1, we show test set results from 
running the rule sequence learner with both the 
standard set of features and with RRE-based 
features. 7 The results are sorted by training 
corpus size, with the raise/rise training corpus 
being the smallest and the then/than training 
corpus being the largest. Baseline accuracy is 
5 Note that this does not imply a bound on the length 
of a string to which an RRE can apply. 
6 DT= determiner, IN = preposition, biN = singular 
noun. 
7 While these results look worse than those achieved 
by other systems, as reported in (Golding and Roth, 
1999), we used different data splits and tokenization. 
Our baseline accuracies are significantly lower than 
the baselines for their test sets. If we account for this 
by instead measuring percent error reduction 
compared to baseline accuracy, then our average 
reduction is better than that reported for the BaySpell 
system, but worse than that of WinSpell. If we add 
voting to our system (WinSpell employs voting), then 
we attain results on par with WinSpell. 
the accuracy attained on the test set by always 
picking the word that appears more frequently in 
the training set. 
Conf. Pair Baseline Standard 
Raise/Rise 53.6 75.0 
Pnncipal/Pfinciple 
Accept/Except 
Affect/Effect 
64.5 80.6 
60.0 94.5 
86.8 94.3 
Lead/Led 53.6 89.3 
Piece/Peace 51.1 83.0 
Weather/Whether 79.7 84.6 
Quiet/Quite 83.1 100 
County/Country 75.6 78.2 
Past/Passed 63.7 88.1 
Amount/Number 74.1 83.3 
96.1 Begin/Being 
Among/Between 
Then/Than 
90.8 
69.2 76.8 
62.8 93.1 
RRE 
78.6 
83.9 
90.9 
94.3 
89.3 
83.0 
89.2 
98.5 
83.3 
89.3 
87.0 
96.7 
80.8 
93.4 
Table 1 Test Set Results: Standard vs RRE- 
Based Features 
In Table 2 we see that the RRE-based 
system outperforms the standard system on 9 of  
the confusion sets, the standard system 
outperforms the RRE-based system on 2 and the 
two systems attain identical results on 3. We see 
that the relative performance of the RRE-based 
learner is better overall on the larger training 
sets than on the smaller sets. This is to be 
expected, as more data is needed to support 
learning the more expressive RRE-based rules. 
RRE Standard IdenticM 
Be~er Better 
All 9 2 3 
Confusables 
1 3 7 Smallest 
Sets 
7 Largest 
Sets 
1 0 
Table 2 Performance Analysis Across Different 
Sets 
Pooling all of the test sets into one big 
set, the RRE-based system achieves an overall 
accuracy of 89.9%, compared to 88.5% for the 
standard learner. Weighting each confusion pair 
equally, the RRE-based system achieves an 
6 
overall accuracy of 88.4%, compared to 86.9% 
for the standard learner. 
Conclusions 
The RRE-based rule sequence l arner presented 
above is able to learn rules using more 
expressive conditions than what is typically used 
for disambiguation tasks in natural language 
processing. These regular-expression based 
conditions lead to higher accuracy than what is 
achieved when using the same learning 
paradigm with the traditionally used feature set. 
We hope that other learning algorithms can 
benefit from the ideas presented here and that 
the idea of learning RREs can be generalized to
allow other learners to incorporate more 
powerful features as well. 
References 
Bahl, L., P. Brown, et al (1989). "'A Tree-Based 
Language Model for Natural Language Speech 
Recognition." IEEE Transactions on Acoustics, 
Speech and Signal Processing 37: 1001-1008. 
Brill, E. (1995). "Transformation-Based error- 
dnven learning and natural language processing: 
a case study in part of speech tagging." 
Computational Linguistics. 
Chelba, C. and F. Jelinek (1998). Exploiting 
Syntactic Structure for Language Modeling. 
Proceedings of Coling/ACL, Montreal, Canada. 
Even-Zohar, Y. and D. Roth (2000). A 
Classification Approach to Word Prediction. 
Proceedings ofNAACL, Seattle, Wa. 
Golding, A. and D. Roth (1999). "A Winnow- 
Based Approach to Context-Sensitive Spelling 
Correction." Machine Learning. 
Hopcroft, J. and J. Ullman (1979). Introduction 
to Automata Theory, Languages and 
Computation, Addison-Wesley. 
Mangu, L. and E. Brill (1997). Automatic Rule 
Acquisition for Spelling Correction. Proceedings 
of the International Conference on Machine 
Learning, Nashville, Tn. 
Marcus, M., B. Santofini, et al (1993). 
"Building a large annotated corpus of English: 
the Penn Treebank." Computational Linguistics. 
Pietra, S. D., V. D. Pietra, et al (1994). 
Inference and Estimation of a Long-Range 
Trigram Model. Proceedings of the Second 
International Colloquium on Grammatical 
Inference, Alicante, Spain. 
Samuellson, C., P. Tapanainen, et al (1996). 
Inducing Constraint Grammars. Grammatical 
Inference: Learning Syntax from Sentences. L.
Miclet and C. D. 1. Huguera, Springer. 1147. 
Saul, L. and F. Pereira (1997). Aggregate and 
mixed-order Markov models for statistical 
language processing. Proceedings of the Second 
Conference on EMNLP. 
7 
(2,1) 
(2,0) 
(1,O),OA), 
(1,2),(1,3), 
(2,o),(2,1), 
(2,2),(2,3), 
(3,0),(3,1), 
(3,2),(3,3) 
C D 
a 
~ ~  ~ (2,2) 
b 
-@ 
Figure I : A Partial RRE-Tree 
8 
An Analysis of the AskMSR Question-Answering System 
Eric Brill, Susan Dumais and Michele Banko 
Microsoft Research 
One Microsoft Way 
Redmond, Wa. 98052 
{brill,sdumais,mbanko}@microsoft.com 
 
 
Abstract 
We describe the architecture of the 
AskMSR question answering system and 
systematically evaluate contributions of 
different system components to accuracy.    
The system differs from most question 
answering systems in its dependency on 
data redundancy rather than sophisticated 
linguistic analyses of either questions or 
candidate answers.    Because a wrong an-
swer is often worse than no answer, we 
also explore strategies for predicting 
when the question answering system is 
likely to give an incorrect answer. 
1 Introduction 
Question answering has recently received attention 
from the information retrieval, information extrac-
tion, machine learning, and natural language proc-
essing communities (AAAI, 2002; ACL-ECL, 
2002; Voorhees and Harman, 2000, 2001).   The 
goal of a question answering system is to retrieve 
answers to questions rather than full documents or 
best-matching passages, as most information re-
trieval systems currently do.   The TREC Question 
Answering Track, which has motivated much of 
the recent work in the field, focuses on fact-based, 
short-answer questions such as ?Who killed Abra-
ham Lincoln?? or ?How tall is Mount Everest??   
In this paper we describe our approach to short 
answer tasks like these, although the techniques we 
propose are more broadly applicable. 
Most question answering systems use a va-
riety of linguistic resources to help in understand-
ing the user?s query and matching sections in 
documents.  The most common linguistic resources 
include: part-of-speech tagging, parsing, named 
entity extraction, semantic relations, dictionaries, 
WordNet, etc. (e.g., Abney et al, 2000; Chen et al 
2000; Harabagiu et al, 2000; Hovy et al, 2000; 
Pasca et al, 2001; Prager et al, 2000).  We chose 
instead to focus on the Web as a gigantic data re-
pository with tremendous redundancy that can be 
exploited for question answering.  We view our 
approach as complimentary to more linguistic ap-
proaches, but have chosen to see how far we can 
get initially by focusing on data per se as a key 
resource available to drive our system design.  Re-
cently, other researchers have also looked to the 
web as a resource for question answering (Buch-
holtz, 2001; Clarke et al, 2001; Kwok et al, 
2001). These systems typically perform complex 
parsing and entity extraction for both queries and 
best matching Web pages, and maintain local 
caches of pages or term weights.  Our approach is 
distinguished from these in its simplicity and effi-
ciency in the use of the Web as a large data re-
source. 
Automatic QA from a single, small infor-
mation source is extremely challenging, since there 
is likely to be only one answer in the source to any 
user?s question.   Given a source, such as the 
TREC corpus, that contains only a relatively small 
number of formulations of answers to a query, we 
may be faced with the difficult task of mapping 
questions to answers by way of uncovering com-
plex lexical, syntactic, or semantic relationships 
between question string and answer string.  The 
need for anaphor resolution and synonymy, the 
presence of alternate syntactic formulations and 
indirect answers all make answer finding a poten-
tially challenging task.  However, the greater the 
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 257-264.
                         Proceedings of the Conference on Empirical Methods in Natural
     
Question Rewrite Query <Search Engine>
Collect Summaries, 
Mine N-grams
Filter N-GramsTile N-Grams N-Best Answers
Where is the Louvre
Museum located?
?+the Louvre Museum +is located?
?+the Louvre Museum +is +in?
?+the Louvre Museum +is near?
?+the Louvre Museum +is?
Louvre AND Museum AND nearin Paris France 59%
museums          12%
hostels              10%
Figure 1. System Architecture
answer redundancy in the source data collection, 
the more likely it is that we can find an answer that 
occurs in a simple relation to the question.  There-
fore, the less likely it is that we will need to solve 
the aforementioned difficulties facing natural lan-
guage processing systems. 
In this paper, we describe the architecture of 
the AskMSR Question Answering System and 
evaluate contributions of different system compo-
nents to accuracy.    Because a wrong answer is 
often worse than no answer, we also explore 
strategies for predicting when the question answer-
ing system is likely to give an incorrect answer. 
2 System Architecture 
As shown in Figure 1, the architecture of our sys-
tem can be described by four main steps: query-
reformulation, n-gram mining, filtering, and n-
gram tiling. In the remainder of this section, we 
will briefly describe these components. A more 
detailed description can be found in [Brill et al, 
2001]. 
2.1 Query Reformulation 
Given a question, the system generates a number 
of weighted rewrite strings which are likely sub-
strings of declarative answers to the question. For 
example, ?When was the paper clip invented?? is 
rewritten as ?The paper clip was invented?.  We 
then look through the collection of documents in 
search of such patterns.  Since many of these string 
rewrites will result in no matching documents, we 
also produce less precise rewrites that have a much 
greater chance of finding matches.  For each query, 
we generate a rewrite which is a backoff to a sim-
ple ANDing of all of the non-stop words in the 
query.   
The rewrites generated by our system are 
simple string-based manipulations. We do not use 
a parser or part-of-speech tagger for query refor-
mulation, but do use a lexicon for a small percent-
age of rewrites, in order to determine the possible 
parts-of-speech of a word as well as its morpho-
logical variants.   Although we created the rewrite 
rules and associated weights manually for the cur-
rent system, it may be possible to learn query-to-
answer reformulations and their weights (e.g., 
Agichtein et al, 2001; Radev et al, 2001). 
2.2 N-Gram Mining 
Once the set of query reformulations has been gen-
erated, each rewrite is formulated as a search en-
gine query and sent to a search engine from which 
page summaries are collected and analyzed.  From 
the page summaries returned by the search engine, 
n-grams are collected as possible answers to the 
question.   For reasons of efficiency, we use only 
the page summaries returned by the engine and not 
the full-text of the corresponding web page.   
The returned summaries contain the query 
terms, usually with a few words of surrounding 
context.  The summary text is processed in accor-
dance with the patterns specified by the rewrites. 
Unigrams, bigrams and trigrams are extracted and 
subsequently scored according to the weight of the 
query rewrite that retrieved it.  These scores are 
summed across all summaries containing the n-
gram (which is the opposite of the usual inverse 
document frequency component of docu-
ment/passage ranking schemes).  We do not count 
frequency of occurrence within a summary (the 
usual tf component in ranking schemes).  Thus, the 
final score for an n-gram is based on the weights 
associated with the rewrite rules that generated it 
and the number of unique summaries in which it 
occurred. 
2.3 N-Gram Filtering 
Next, the n-grams are filtered and reweighted ac-
cording to how well each candidate matches the 
expected answer-type, as specified by a handful of 
handwritten filters.  The system uses filtering in 
the following manner. First, the query is analyzed 
and assigned one of seven question types, such as 
who-question, what-question, or how-many-
question.  Based on the query type that has been 
assigned, the system determines what collection of 
filters to apply to the set of potential answers found 
during the collection of n-grams.  The candidate n-
grams are analyzed for features relevant to the fil-
ters, and then rescored according to the presence of 
such information.   
A collection of 15 simple filters were devel-
oped based on human knowledge about question 
types and the domain from which their answers can 
be drawn.  These filters used surface string fea-
tures, such as capitalization or the presence of dig-
its, and consisted of handcrafted regular expression 
patterns. 
2.4 N-Gram Tiling 
Finally, we applied an answer tiling algorithm, 
which both merges similar answers and assembles 
longer answers from overlapping smaller answer 
fragments.  For example, "A B C" and "B C D" is 
tiled into "A B C D." The algorithm proceeds 
greedily from the top-scoring candidate - all sub-
sequent candidates (up to a certain cutoff) are 
checked to see if they can be tiled with the current 
candidate answer. If so, the higher scoring candi-
date is replaced with the longer tiled n-gram, and 
the lower scoring candidate is removed. The algo-
rithm stops only when no n-grams can be further 
tiled. 
 
3 Experiments 
For experimental evaluations we used the first 500 
TREC-9 queries (201-700) (Voorhees and Harman, 
2000).  We used the patterns provided by NIST for 
automatic scoring.  A few patterns were slightly 
modified to accommodate the fact that some of the 
answer strings returned using the Web were not 
available for judging in TREC-9.   We did this in a 
very conservative manner allowing for more spe-
cific correct answers (e.g., Edward J. Smith vs. 
Edward Smith) but not more general ones (e.g., 
Smith vs. Edward Smith), and also allowing for 
simple substitutions (e.g., 9 months vs. nine 
months).  There also are substantial time differ-
ences between the Web and TREC databases (e.g., 
the correct answer to Who is the president of Bo-
livia? changes over time), but we did not modify 
the answer key to accommodate these time differ-
ences, because it would make comparison with 
earlier TREC results impossible.  These changes 
influence the absolute scores somewhat but do not 
change relative performance, which is our focus 
here.   
All runs are completely automatic, starting 
with queries and generating a ranked list of 5 can-
didate answers.  For the experiments reported in 
this paper we used Google as a backend because it 
provides query-relevant summaries that make our 
n-gram mining efficient.  Candidate answers are a 
maximum of 50 bytes long, and typically much 
shorter than that.  We report the Mean Reciprocal 
Rank (MRR) of the first correct answer, the Num-
ber of Questions Correctly Answered (NAns), and 
the proportion of Questions Correctly Answered 
(%Ans).   
3.1 Basic System Performance 
Using our current system with default settings we 
obtain a MRR of 0.507 and answers 61% of the 
queries correctly (Baseline, Table 1).  The average 
answer length was 12 bytes, so the system is re-
turning short answers, not passages.   Although it 
is impossible to compare our results precisely with 
TREC-9 groups, this is very good performance and 
would place us near the top of 50-byte runs for 
TREC-9.   
3.2 Contributions of Components 
Table 1 summarizes the contributions of the differ-
ent system components to this overall perform-
ance.  We report summary statistics as well as 
percent change in performance when components 
are removed (%Drop MRR). 
 
Query Rewrites: 
As described earlier, queries are transformed to 
successively less precise formats, with a final 
backoff to simply ANDing all the non-stop query 
terms.  More precise queries have higher weights 
associated with them, so n-grams found in these 
responses are given priority.   If we set al the re-
write weights to be equal, MRR drops from 0.507 
to 0.489, a drop of 3.6%.   Another way of looking 
at the importance of the query rewrites is to exam-
ine performance where the only rewrite the system 
uses is the backoff AND query.   Here the drop is 
more substantial, down to 0.450 which represents a 
drop of 11.2%.    
Query rewrites are one way in which we 
capitalize on the tremendous redundancy of data 
on the web ? that is, the occurrence of multiple 
linguistic formulations of the same answers in-
creases the chances of being able to find an answer 
that occurs within the context of a simple pattern 
match with the query.   Our simple rewrites help 
compared to doing just AND matching. Soubbotin 
and Soubbotin (2001) have used more specific 
regular expression matching to good advantage and 
we could certainly incorporate some of those ideas 
as well. 
MRR NAns %Ans
%Drop
MRR
Baseline 0.507 307 61.4% 0.0%
Query Rewrite:
  Same Weight All Rewrites 0.489 298 59.6% 3.6%
  AND-only query 0.450 281 56.2% 11.2%
Filter N-Gram:
  Base, NoFiltering 0.416 268 53.6% 17.9%
  AND, NoFiltering 0.338 226 45.2% 33.3%
Tile N-Gram:
  Base, NoTiling 0.435 277 55.4% 14.2%
  AND, NoTiling 0.397 251 50.2% 21.7%
Combinations:
  Base, NoTiling NoFiltering 0.319 233 46.6% 37.1%
  AND, NoTiling NoFiltering 0.266 191 38.2% 47.5%
Table 1.  Componential analysis of the AskMSR QA system.
 
N-Gram Filtering: 
Unigrams, bigrams and trigrams are extracted from 
the (up to) 100 best-matching summaries for each 
rewrite, and scored according the weight of the 
query rewrite that retrieved them.  The score as-
signed to an n-gram is a weighted sum across the 
summaries containing the n-grams, where the 
weights are those associated with the rewrite that 
retrieved a particular summary.   The best-scoring 
n-grams are then filtered according to seven query 
types.   For example the filter for the query How 
many dogs pull a sled in the Iditarod? prefers a 
number, so candidate n-grams  like dog race, run, 
Alaskan, dog racing, many mush move down the 
list and pool of 16 dogs (which is a correct answer) 
moves up.  Removing the filters decreases MRR 
by 17.9% relative to baseline (down to 0.416).  Our 
simple n-gram filtering is the most important indi-
vidual component of the system. 
 
N-Gram Tiling: 
Finally, n-grams are tiled to create longer answer 
strings.   This is done in a simple greedy statistical 
manner from the top of the list down.   Not doing 
this tiling decreases performance by 14.2% relative 
to baseline (down to 0.435).    The advantages 
gained from tiling are two-fold.  First, with tiling 
substrings do not take up several answer slots, so 
the three answer candidates: San, Francisco, and 
San Francisco, are conflated into the single answer 
candidate: San Francisco.  In addition, longer an-
swers can never be found with only trigrams, e.g., 
light amplification by stimulted emission of radia-
tion can only be returned by tiling these shorter n-
grams into a longer string.   
 
Combinations of Components: 
Not surprisingly, removing all of our major com-
ponents except the n-gram accumulation (weighted 
sum of occurrences of unigrams, bigrams and tri-
grams) results in substantially worse performance 
than our full system, giving an MRR of 0.266, a 
decrease of 47.5%.    The simplest entirely statisti-
cal system with no linguistic knowledge or proc-
essing employed, would use only AND queries, do 
no filtering, but do statistical tiling.   This system 
uses redundancy only in summing n-gram counts 
across summaries.  This system has MRR 0.338, 
which is a 33% drop from the best version of our 
system, with all components enabled.   Note, how-
ever, that even with absolutely no linguistic proc-
essing, the performance attained is still very rea-
sonable performance on an absolute scale, and in 
fact only one TREC-9 50-byte run achieved higher 
accuracy than this. 
To summarize, we find that all of our process-
ing components contribute to the overall accuracy 
of the question-answering system.  The precise 
weights assigned to different query rewrites seems 
relatively unimportant, but the rewrites themselves 
do contribute considerably to overall accuracy.    
N-gram tiling turns out to be extremely effective, 
serving in a sense as a ?poor man?s named-entity 
recognizer?.  Because of the effectiveness of our 
tiling algorithm over large amounts of data, we do 
not need to use any named entity recognition com-
ponents.  The component that identifies what filters 
to apply over the harvested n-grams, along with the 
actual regular expression filters themselves, con-
tributes the most to overall performance. 
4 Component Problems 
Above we described how components contributed 
to improving the performance of the system.  In 
this section we look at what components errors are 
attributed to.  In Table 2, we show the distribution 
of error causes, looking at those questions for 
which the system returned no correct answer in the 
top five hypotheses. 
 
Problem % of Errors 
Units 23 
Time 20 
Assembly 16 
Correct 14 
Beyond Paradigm 12 
Number Retrieval 5 
Unknown Problem 5 
Synonymy 2 
Filters  2 
Table 2.  Error Attribution 
 
The biggest error comes from not knowing 
what units are likely to be in an answer given a 
question (e.g. How fast can a Corvette go ? xxx 
mph).  Interestingly, 34% of our errors (Time and 
Correct) are not really errors, but are due to time 
problems or cases where the answer returned is 
truly correct but not present in the TREC-9 answer 
key.  16% of the failures come from the inability of 
our n-gram tiling algorithm to build up the full 
string necessary to provide a correct answer.   
Number retrieval problems come from the fact 
that we cannot query the search engine for a num-
ber without specifying the number.  For example, a 
good rewrite for the query How many islands does 
Fiji have would be ? Fiji has <NUM> islands ?, 
but we are unable to give this type of query to the 
search engine.  Only 12% of the failures we clas-
sify as being truly outside of the system?s current 
paradigm, rather than something that is either al-
ready correct or fixable with minor system en-
hancements. 
5 Knowing When We Don?t Know 
Typically, when deploying a question answering 
system, there is some cost associated with return-
ing incorrect answers to a user.  Therefore, it is 
important that a QA system has some idea as to 
how likely an answer is to be correct, so it can 
choose not to answer rather than answer incor-
rectly.  In the TREC QA track, there is no distinc-
tion made in scoring between returning a wrong 
answer to a question for which an answer exists 
and returning no answer.  However, to deploy a 
real system, we need the capability of making a 
trade-off between precision and recall, allowing 
the system not to answer a subset of questions, in 
hopes of attaining high accuracy for the questions 
which it does answer. 
Most question-answering systems use 
hand-tuned weights that are often combined in an 
ad-hoc fashion into a final score for an answer hy-
pothesis (Harabagiu et al, 2000; Hovy et al, 2000; 
Prager et al, 2000; Soubbotin & Soubbotin, 2001; 
Brill et. al., 2001).   Is it still possible to induce a 
useful precision-recall (ROC) curve when the sys-
tem is not outputting meaningful probabilities for 
answers?  We have explored this issue within the 
AskMSR question-answering system.   
Ideally, we would like to be able to deter-
mine the likelihood of answering correctly solely 
from an analysis of the question.  If we can deter-
mine we are unlikely to answer a question cor-
rectly, then we need not expend the time, cpu 
cycles and network traffic necessary to try to an-
swer that question.   
We built a decision tree to try to predict 
whether the system will answer correctly, based on 
a set of features extracted from the question string: 
word unigrams and bigrams, sentence length 
(QLEN), the number of capitalized words in the 
sentence, the number of stop words in the sentence 
(NUMSTOP), the ratio of the number of nonstop 
words to stop words, and the length of longest 
word (LONGWORD).  We use a decision tree be-
cause we also wanted to use this as a diagnostic 
tool to indicate what question types we need to put 
further developmental efforts into.  The decision 
tree built from these features is shown in Figure 2. 
The first split of the tree asks if the word ?How? 
appears in the question.  Indeed, the system per-
forms worst on ?How? question types.  We do best 
on short ?Who? questions with a large number of 
stop words. 
 
 
Figure 2.  Learning When We Don't Know -- Us-
ing Only Features from Query 
 
We can induce an ROC curve from this 
decision tree by sorting the leaf nodes from the 
highest probability of being correct to the lowest.  
Then we can gain precision at the expense of recall 
by not answering questions in the leaf nodes that 
have the highest probability of error.  The result of 
doing this can be seen in Figures 3 and 4, the line 
labeled ?Question Features?.  The decision tree 
was trained on Trec 9 data.  Figure 3 shows the 
results when applied to the same training data, and 
Figure 4 shows the results when testing on Trec 10 
data.  As we can see, the decision tree overfits the 
training data and does not generalize sufficiently to 
give useful results on the Trec 10 (test) data. 
Next, we explored how well answer cor-
rectness correlates with answer score in our sys-
tem.  As discussed above, the final score assigned 
to an answer candidate is a somewhat ad-hoc score 
based upon the number of retrieved passages the n-
gram occurs in, the weight of the rewrite used to 
retrieve each passage, what filters apply to the n-
gram, and the effects of merging n-grams in an-
swer tiling.  In Table 3, we show the correlation 
coefficient calculated between whether a correct 
answer appears in the top 5 answers output by the 
system and (a) the score of the system?s first 
ranked answer and (b) the score of the first ranked 
answer minus the score of the second ranked an-
swer.  A correlation coefficient of 1 indicates 
strong positive association, whereas a correlation 
of ?1 indicates strong negative association. We see 
that there is indeed a correlation between the 
scores output by the system and the answer accu-
racy, with the correlation being tighter when just 
considering the score of the first answer. 
 
 Correlation 
Coefficient 
Score #1 .363 
Score #1 ? Score #2 .270 
Table 3 . Do answer scores correlate with correct-
ness? 
 
Because a number of answers returned by 
our system are correct but scored wrong according 
to the TREC answer key because of time mis-
matches, we also looked at the correlation, limiting 
ourselves to Trec 9 questions that were not time-
sensitive.  Using this subset of questions, the corre-
lation coefficient between whether a correct an-
swer appears in the system?s top five answers, and 
the score of the #1 answer, increases from .363 to 
.401.  In Figure 3 and 4, we show the ROC curve 
induced by deciding when not to answer a question 
based on the score of the first ranked answer (the 
line labeled ?score of #1 answer?).  Note that the 
score of the top ranked answer is a significantly 
better predictor of accuracy than what we attain by 
considering features of the question string, and 
gives consistent results across two data sets. 
Finally, we looked into whether other at-
tributes were indicative of the likelihood of answer 
correctness.  For every question, a set of snippets is 
gathered.  Some of these snippets come from AND 
queries and others come from more refined exact 
string match rewrites.  In Table 4, we show MRR 
as a function of the number of non-AND snippets 
retrieved.  For instance, when all of the snippets 
come from AND queries, the resulting MRR was 
found to be only .238.  For questions with 100 to 
400 snippets retrieved from exact string match re-
writes, the MRR was .628. 
 
NumQ MRR
0 91 0.238
1 to 10 80 0.405
11 to 100 153 0.612
100 to 400 175 0.628
NumNon-AND 
Passages
 
Table 4 . Accuracy vs. Number of Passages Re-
trieved From Non-AND Rewrites 
 
We built a decision tree to predict whether 
a correct answer appears in the top 5 answers, 
based on all of the question-derived features de-
scribed earlier, the score of the number one rank-
ing answer, as well as a number of additional 
features describing the state of the system in proc-
essing a particular query.  Some of these features 
include: the total number of matching passages 
retrieved, the number of non-AND matching pas-
sages retrieved, whether a filter applied, and the 
weight of the best rewrite rule for which matching 
passages were found.   We show the resulting deci-
sion tree in Figure 5, and resulting ROC curve con-
structed from this decision tree, in Figure 3 and 4 
(the line labeled ?All Features?).  In this case, the 
decision tree does give a useful ROC curve on the 
test data (Figure 4), but does not outperform the 
simple technique of using the ad hoc score of the 
best answer returned by the system.  Still, the deci-
sion tree has proved to be a useful diagnostic in 
helping us understand the weaknesses of our sys-
tem. 
 
ROC Curve for QA
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.5 1
Recall
Pr
ec
isi
on
All Features
Score of #1
Answer
Question
Features
 
 
Figure 3. Three different precision/recall trade-
offs, trained on Trec 9 and tested on Trec 9. 
 
Trec 10 ROC Curve
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.5 1
Recall
Pr
ec
isi
on
Score of #1
Answer
Question
Features
All Features
 
Figure 4. Three different precision/recall trade-
offs, trained on Trec 9 and tested on Trec 10. 
6 Conclusions 
We have presented a novel approach to question-
answering and carefully analyzed the contributions 
of each major system component, as well as ana-
lyzing what factors account for the majority of er-
rors made by the AskMSR question answering 
system.   In addition, we have demonstrated an 
approach to learning when the system is likely to 
answer a question incorrectly, allowing us to reach 
any desired rate of accuracy by not answering 
some portion of questions.  We are currently ex-
ploring whether these techniques can be extended 
beyond short answer QA to more complex cases of 
information access.   
 
  
Figure 5.  Learning When We Don't Know -- Us-
ing All Features 
 
References 
AAAI Spring Symposium Series Mining answers from 
text and knowledge bases (2002). 
S. Abney, M. Collins and A. Singhal (2000).  Answer 
extraction.  In Proceedings of ANLP 2000. 
ACL-EACL Workshop on Open-domain question an-
swering.  (2002). 
E. Agichtein, S. Lawrence and L. Gravano (2001).  
Learning search engine specific query transforma-
tions for question answering.  In Proceedings of 
WWW10. 
E. Brill, J. Lin, M. Banko, S. Dumais and A. Ng (2001). 
Data-intensive question answering.  In Proceedings 
of the Tenth Text Retrieval Conference (TREC 2001). 
S. Buchholz (2001).  Using grammatical relations, an-
swer frequencies and the World Wide Web for TREC 
question answering.   To appear in Proceedings of 
the Tenth Text REtrieval Conference (TREC 2001). 
J. Chen, A. R. Diekema, M. D. Taffet, N. McCracken, 
N. E. Ozgencil, O. Yilmazel, E. D. Liddy (2001).  
Question answering: CNLP at the TREC-10 question 
answering track.  To appear in Proceedings of the 
Tenth Text REtrieval Conference (TREC 2001). 
C. Clarke, G. Cormack and T. Lyman (2001).  Exploit-
ing redundancy in question answering.   In Proceed-
ings of SIGIR?2001. 
C. Clarke, G. Cormack and T. Lynam (2001).   Web 
reinforced question answering.  To appear in Pro-
ceedings of the Tenth Text REtrieval Conference 
(TREC 2001). 
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. 
Surdeanu, R. Bunescu, R. Girju, V. Rus and P. 
Morarescu (2000).  FALCON: Boosting knowledge 
for question answering.  In Proceedings of the Ninth 
Text REtrieval Conference (TREC-9). 
E. Hovy, L. Gerber, U. Hermjakob, M. Junk and C. Lin 
(2000).  Question answering in Webclopedia.  In 
Proceedings of the Ninth Text REtrieval Conference 
(TREC-9). 
E. Hovy, U. Hermjakob and C. Lin (2001).  The use of 
external knowledge in factoid QA.  To appear in 
Proceedings of the Tenth Text REtrieval Conference 
(TREC 2001). 
C. Kwok, O. Etzioni and D. Weld (2001).  Scaling ques-
tion answering to the Web.  In Proceedings of 
WWW?10. 
M. A. Pasca and S. M. Harabagiu (2001).  High per-
formance question/answering.  In Proceedings of 
SIGIR?2001. 
J. Prager, E. Brown, A. Coden and D. Radev (2000).  
Question answering by predictive annotation.  In 
Proceedings of SIGIR?2000. 
D. R. Radev, H. Qi, Z. Zheng, S. Blair-Goldensohn, Z. 
Zhang, W. Fan and J. Prager (2001). Mining the web 
for answers to natural language questions. In ACM 
CIKM 2001: Tenth International Conference on In-
formation and Knowledge Management.  
M. M. Soubbotin and S. M. Soubbotin (2001).  Patterns 
and potential answer expressions as clues to the right 
answers.  To appear in Proceedings of the Tenth Text 
REtrieval Conference (TREC 2001). 
E. Voorhees and D. Harman, Eds. (2000).  Proceedings 
of the Ninth Text REtrieval Conference (TREC-9). 
E. Voorhees and D. Harman, Eds. (2001).  Proceedings 
of the Tenth Text REtrieval Conference (TREC 
2001). 
 
Spelling correction as an iterative process 
that exploits the collective knowledge of web users 
 
Silviu Cucerzan and Eric Brill 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{silviu,brill}@microsoft.com 
 
 
 
Abstract 
Logs of user queries to an internet search engine pro-
vide a large amount of implicit and explicit informa-
tion about language. In this paper, we investigate 
their use in spelling correction of search queries, a 
task which poses many additional challenges beyond 
the traditional spelling correction problem. We pre-
sent an approach that uses an iterative transformation 
of the input query strings into other strings that corre-
spond to more and more likely queries according to 
statistics extracted from internet search query logs. 
1 Introduction 
The task of general purpose spelling correction has 
a long history (e.g. Damerau, 1964; Rieseman and 
Hanson, 1974; McIlroy, 1982), traditionally focus-
ing on resolving typographical errors such as in-
sertions, deletions, substitutions, and 
transpositions of letters that result in unknown 
words (i.e. words not found in a trusted lexicon of 
the language). Typical word processing spell 
checkers compute for each unknown word a small 
set of in-lexicon alternatives to be proposed as 
possible corrections, relying on information about 
in-lexicon-word frequencies and about the most 
common keyboard mistakes (such as typing m in-
stead of n) and phonetic/cognitive mistakes, both 
at word level (e.g. the use of acceptible instead of 
acceptable) and at character level (e.g. the misuse 
of f instead of ph). Very few spell checkers attempt 
to detect and correct word substitution errors, 
which refer to the use of in-lexicon words in inap-
propriate contexts and can also be the result of 
both typographical mistakes (such as typing coed 
instead of cord) and cognitive mistakes (e.g. prin-
cipal and principle). Some research efforts to 
tackle this problem have been made; for example 
Heidorn et al (1982) and Garside et al (1987) de-
veloped systems that rely on syntactic patterns to 
detect substitution errors, while Mays et al (1991) 
employed word co-occurrence evidence from a 
large corpus to detect and correct such errors.   
The former approaches were based on the imprac-
tical assumption that all possible syntactic uses    
of all words (i.e. part-of-speech) are known, and 
presented both recall and precision problems be-
cause many of the substitution errors are not syn-
tactically anomalous and many unusual syntactic 
constructions do not contain errors. The latter ap-
proach had very limited success under the assump-
tions that each sentence contains at most one 
misspelled word, each misspelling is the result of a 
single point change (insertion, deletion, substitu-
tion, or transposition), and the defect rate (the rela-
tive number of errors in the text) is known. A 
different body of work (e.g. Golding, 1995; Gold-
ing and Roth, 1996; Mangu and Brill, 1997) fo-
cused on resolving a limited number of cognitive 
substitution errors, in the framework of context 
sensitive spelling correction (CSSC). Although 
promising results were obtained (92-95% accu-
racy), the scope of this work was very limited as it 
only addressed known sets of commonly confused 
words, such as {peace, piece}). 
1.1 Spell Checking of Search Engine Queries 
The task of web-query spelling correction ad-
dressed in this work has many similarities to tradi-
tional spelling correction but also poses additional 
challenges. Both the frequency and severity of 
spelling errors for search queries are significantly 
greater than in word processing.  Roughly 10-15% 
of the queries sent to search engines contain errors. 
Typically, the validity of a query cannot be de-
cided by lexicon look-up or by checking its gram-
maticality. Because web queries are very short (on 
average, less than 3 words), techniques that use a 
multitude of features based on relatively wide con-
text windows, such as those investigated in CSSC, 
are difficult to apply. Rather than being well-
formed sentences, most queries consist of one con-
cept or an enumeration of concepts, many times 
containing legitimate words that are not found in 
any traditional lexicon. 
 Just defining what a valid web query is represents 
a difficult enterprise. We clearly cannot use only a 
static trusted lexicon, as many new names and 
concepts (such as aznar, blog, naboo, nimh, nsync, 
and shrek) become popular every day and it would 
be extremely difficult if not impossible to maintain 
a high-coverage lexicon. In addition, employing 
very large lexicons can result in more errors sur-
facing as word substitutions, which are very diffi-
cult to detect, rather than as unknown words. 
 One alternative investigated in this work is to ex-
ploit the continuously evolving expertise of mil-
lions of people that use web search engines, as 
collected in search query logs (seen as histograms 
over the queries received by a search engine). In 
some sense, we could say that the validity of a 
word can be inferred from its frequency in what 
people are querying for, similarly to Wittgen-
stein?s (1968) observation that ?the meaning of a 
word is its use in the language?. Such an approach 
has its own caveats. For example, it would be er-
roneous to simply extract from web-query logs all 
the queries whose frequencies are above a certain 
value and consider them valid. Misspelled queries 
such as britny spears are much more popular than 
correctly spelled queries such as bayesian nets and 
amd processors. Our challenge is to try to utilize 
query logs to learn what queries are valid, and to 
build a model for valid query probabilities, despite 
the fact that a large percentage of the logged que-
ries are misspelled and there is no trivial way to 
determine the valid from invalid queries. 
2 Problem Formulation. Prior Work 
Comprehensive reviews of the spelling correction 
literature were provided by Peterson (1980), 
Kukich (1992), and Jurafsky and Martin (2000). In 
this section, we survey a few lexicon-based spell-
ing correction approaches by using a series of for-
mal definitions of the task and presenting concrete 
examples showing the strengths and the limits cor-
responding to each situation. We iteratively rede-
fine the problem, starting from an approach purely 
based on a trusted lexicon and ending up with an 
approach in which the role of the trusted lexicon is 
greatly diminished. While doing so, we also make 
concrete forward steps in our attempt to provide a 
definition of valid web queries.  
 Let ?  be the alphabet of a language and *??L a 
broad-coverage lexicon of the language. The sim-
plest and historically the first definition of lexicon-
based spelling correction (Damerau, 1964) is: 
Given an unknown word Lw \*?? , find Lw ?'  
such that ),(min)',( vwdistwwdist
Lv?
= . 
i.e. for any out-of-lexicon word in a text, find the 
closest word form(s) in the available lexicon and 
hypothesize it as the correct spelling alternative. 
dist  can be any string-based function; for exam-
ple, it can be the ratio between the number of let-
ters two words do not have in common and the 
number of letters they share.1 The two most used 
classes of distances in spelling correction are edit 
distances, as proposed by Damerau (1964) and 
Levenshtein (1965), and correlation matrix dis-
tances (Cherkassky et al, 1974). In our study, we 
use a modified version of the Damerau-Lev-
enshtein edit distance, as presented in Section 3. 
 One flaw of the preceding formulation is that it 
does not take into account the frequency of words 
in a language. A simple solution to this problem is 
to compute the probability of words in the target 
language as maximum likelihood estimates (MLE) 
over a large corpus and reformulate the general 
spelling-correction problem as follows: 
Given Lw \*?? , find Lw ?'  such that 
??)',( wwdist and )(max)'(
),(:
vPwP
vwdistLv ???
= . 
 In this formulation, all in-lexicon words that are 
within some ?reasonable? distance ?  of the un-
known word are considered as good candidates, 
the correction being chosen based on its prior 
probability in the language. While there is an im-
plicit conditioning on the original spelling because 
of the domain on which the best correction is 
searched, this objective function only uses the 
prior probability of words in the language and not 
the actual distances between each candidate and 
the input word  
 One solution that allows using a probabilistic edit 
distance is to condition the probability of a correc-
tion on the original spelling )|( wvP :  
Given Lw \*?? , find Lw ?'  such that 
??)',( wwdist and )|(max)|'(
),(:
wvPwwP
vwdistLv ???
= . 
  In a noisy channel model framework, as em-
ployed for spelling correction by Kernigham et al 
(1990), the objective function can be written by 
using Bayesian inversion as the product between 
the prior probability of words in a language )(vP  
(the language model), and the likelihood of mis-
spelling a word v as w, )|( vwP  (which models the 
noisy channel and will be called the error model). 
In the above formulations, unknown words are   
corrected in isolation. This is a rather major flaw 
because context is extremely important for spelling 
correction, as illustrated in the following example: 
power crd  power cord 
video crd  video card 
                                                          
1
 Note that the function does not have to be symmetric; thus, 
the notation dist(w,w?) is used with a loose sense. 
 The misspelled word crd should be corrected to 
two different words depending on its contexts.2   
 A formulation of the spelling correction problem 
that takes into account context is the following: 
Given a string *??s , rl wccs = , with Lw \
*??  
and *, Lcc rl ? , find Lw ?'  such that ??)',( wwdist  
and )|(max)|'(
),(: rlvwdistLvrl
wccvPwccwP
???
= . 
 Spaces and other word delimiters are ignored in 
this formulation and the subsequent formulations 
for simplicity, although text tokenization repre-
sents an important part of the spelling-correction 
process, as discussed in Sections 5 and 6. 
 The task definitions enumerated up to this point 
(on which most traditional spelling correction sys-
tems are based) ignore word substitution errors. In 
the case of web searches, it is extremely important 
to provide correction suggestions for valid words 
when they are more meaningful as a search query 
than the original query, for example: 
golf war  gulf war 
sap opera  soap opera 
 This problem is partially addressed by the task of 
CSSC, which can be formalized as follows: 
Given a set of confusable valid word forms          
in a language },...,,{ 21 nwwwW =  and a string 
ril cwcs = , choose Ww j ?  such that 
)|(max)|(
..1 rilknkrilj
cwcwPcwcwP
=
= . 
 In the CSSC literature, the sets of confusables are 
presumed known, but they could also be built for 
each in-lexicon word w as all words 'w  with 
??)',( wwdist , similarly to the approach investi-
gated by Mays et al (1991), in which they chose a 
1=?  and employed an edit distance with all point 
changes having the same cost 1. 
 The generalized problem of phrasal spelling cor-
rection can then be formulated as follows: 
Given *??s , find *' Ls ?  such that ??)',( ssdist  
and )|(max)|'(
),(:*
stPssP
tsdistLt ???
= . 
 Typically, a correction is desirable when *Ls ?  
(i.e. at least one of the component words is un-
known) but, as shown above, there are frequent 
cases (e.g. golf war) when sequences of valid 
words should be changed to other word sequences. 
Note that word boundaries are hidden in this latter 
                                                          
2
 To simplify the exposition, we only consider two highly 
probable corrections, but other valid alternatives exist, e.g. 
video cd. 
formulation, making it more general and allowing 
it to cover two other important spelling error 
classes, concatenation and splitting, e.g.: 
power point slides  powerpoint slides 
chat inspanich   chat in spanish 
 Yet, it still does not account for another important 
class of cases in web query correction which is 
represented by out-of-lexicon words that are valid 
in certain contexts (therefore, *' Ls ? ), for example: 
amd processors  amd processors (no change) 
 The above phrase represents a legitimate query, 
despite the fact that it may contain unknown words 
when employing a traditional English lexicon.  
 Some even more interesting cases not handled by 
traditional spellers and also not covered by the 
latter formulation are those in which in-lexicon 
words should be changed to out-of-lexicon words, 
as in the following examples, where two valid 
words must be concatenated into an out of lexicon 
word: 
gun dam planet  gundam planet 
limp biz kit  limp bizkit 
 These observations lead to an even more general 
formulation of the spelling-correction problem: 
Given *??s , find *' ??s  such that ??)',( ssdist  
and )|(max)|'(
),(:*
stPssP
tsdistt ????
= . 
 For the first time, the formulation no longer 
makes explicit use of a lexicon of the language.3 In 
some sense, the actual language in which the web 
queries are expressed becomes less important than 
the query-log data from which the string probabili-
ties are estimated. This probability model can be 
seen as a substitute for a measure of the meaning-
fulness of strings as web-queries. For example, an 
implausible random noun phrase in any of the tra-
ditional corpora such as sad tomatoes is meaning-
ful in the context of web search (being the name of 
a somewhat popular music band). 
3 The Error Model. String Edit Functions 
All formulations of the spelling correction task 
given in the previous section used a string distance 
function and a threshold to restrict the space in 
which alternative spellings are searched. Various 
previous work has addressed the problem of 
choosing appropriate functions (e.g. Kernigham et 
al. 1990, Brill and Moore, 2002; Toutanova and 
Moore, 2003). 
                                                          
3
 A trusted lexicon may still be used in the estimation of the 
language model probability for the computation of )|( stP . 
 The choice of distance function d and threshold ? 
could be extremely important for the accuracy of a 
speller. At one extreme, the use of a too restrictive 
function/threshold combination can result in not 
finding the best correction for a given query. For 
example, using the vanilla Damerau-Levenshtein 
edit distance (defined as the minimum number of 
point changes required to transform a string into 
another, where a point change is one of the follow-
ing operations: insertion of a letter, deletion of a 
letter, and substitution of one letter with another 
letter) and a threshold 1=? , the correction donadl 
duck  donald duck would not be possible. At the 
other extreme, the use of a less limiting function 
might have as consequence suggesting very 
unlikely corrections. For example, using the same 
classical Levenshtein distance and 2=?  would 
allow the correction of the string donadl duck, but 
will also lead to bad corrections such as log wood 
 dog food (based on the frequency of the queries, 
as incorporated in )(sP ).  Nonetheless, large dis-
tance corrections are still desirable in a diversity of 
situations, for example: 
platnuin rings   platinum rings 
ditroitigers   detroit tigers 
 The system described in this paper makes use of a 
modified context-dependent weighted Damerau-
Levenshtein edit function which allows insertion, 
deletion, substitution, immediate transposition, and 
long-distance movement of letters as point 
changes, for which the weights were interactively 
refined using statistics from query logs. 
4 The Language Model. Exploiting Large 
Web Query Logs 
A misspelling such as ditroitigers is far from the 
correct alternative and thus, it might be extremely 
difficult to find its correct spelling based solely on 
edit distance. Nonetheless, the correct alternative 
could be reached by allowing intermediate valid 
correction steps, such as ditroitigers  detroitti-
gers  detroit tigers. But what makes detroittigers 
a valid correction step? Recall that the last formu-
lation of spelling correction in Section 3 did not 
explicitly use a lexicon of the language. Rather, 
any string that appears in the query log used for 
training can be considered a valid correction and 
can be suggested as an alternative to the current 
web query based on the relative frequency of the 
query and the alternative spelling. Thus, a spell 
checker built according to this formulation could 
suggest the correction detroittigers because this 
alternative occurs frequently enough in the em-
ployed query log. However, detroittigers itself 
could be corrected to detroit tigers if presented as 
a stand-alone query to this spell checker, based on 
similar query-log frequency facts, which naturally 
leads to the idea of an iterative correction ap-
proach. 
 
albert einstein 4834 
albert einstien 525 
albert einstine 149 
albert einsten 27 
albert einsteins 25 
albert einstain 11 
albert einstin 10 
albert eintein 9 
albeart einstein 6 
aolbert einstein 6 
alber einstein 4 
albert einseint 3 
albert einsteirn 3 
albert einsterin 3 
albert eintien 3 
alberto einstein 3 
albrecht einstein 3 
alvert einstein 3 
Table 1. Counts of different (mis)spellings of Albert          
Einstein?s name in a web query log.  
 Essential to such an approach are three typical 
properties of the query logs (e.g. see Table 1): 
? words in the query logs are misspelled in vari-
ous ways, from relatively easy-to-correct mis-
spellings to very-difficult-to-correct ones, that 
make the user?s intent almost impossible to 
recognize;  
? the less malign (difficult to correct) a misspell-
ing is the more frequent it is; 
? the correct spellings tend to be more frequent 
than misspellings. 
 In this context, the spelling correction problem 
can be given the following iterative formulation: 
Given a string *0 ??s , find a sequence    
*
21 ,..., ??nsss   such that  ??+ ),( 1ii ssdist , 
)|(max)|(
),(:1 * itsdisttii
stPssP
i ????
+ = , 1..0 ??? ni , 
and )|(max)|(
),(:* ntsdisttnn
stPssP
n ????
= . 
 An example of correction that can be made by   
iteratively applying the base spell checker is: 
anol scwartegger   arnold schwarzenegger 
Misspelled query: anol scwartegger 
First iteration: arnold schwartnegger 
Second iteration: arnold schwarznegger 
Third iteration: arnold schwarzenegger 
Fourth iteration: no further correction 
 Up to this point, we underspecified the notion of 
string in the task formulations given. One possibil-
ity is to consider whole queries as the strings to be 
corrected and iteratively search for better logged 
queries according to the agreement between their 
relative frequencies and the character error model. 
This is equivalent to identifying all queries in the 
query log that are misspellings of other queries and 
for any new query, find a correction sequence of 
logged queries. While such an approach exploits 
the vast information available in web-query logs, it 
only covers exact matches of the queries that ap-
pear in these logs and provides a low coverage of 
infrequent queries. For example, a query such as 
britnet spear inconcert could not be corrected if 
the correction britney spears in concert does not 
appear in the employed query log, although the 
substring britnet spear could be corrected to brit-
ney spears. 
 To address the shortcomings of such an approach, 
we propose a system based on the following for-
mulation, which uses query substrings: 
Given *0 ??s , find a sequence 
*
21 ,..., ??nsss , 
such that for each 1..0 ?? ni  there exist the de-
compositions ii lii
l
iii wwwws 1,1
1
1,11i0,
1
0, ...s ,... +++ == , 
where k hjw ,  are words or groups of words such that 
??+ ),( 1,10, kiki wwdist , ilkni ..1  ,1..0 ?????  and 
)|(max)|(
** ),(:1 itsdisttii
stPssP
i ????
+ = , 1..0 ??? ni , 
and )|(max)|(
** ),(: ntsdisttnn
stPssP
n ????
= . 
Note that the length of the string decomposition 
may vary from one iteration to the next one, for 
example: 
 
 In the implementation evaluated in this paper, we 
allowed decompositions of query strings into 
words and word bigrams. The tokenization process 
uses space and punctuation delimiters in addition 
to the information provided about multi-word 
compounds (e.g. add-on and back-up) by a trusted 
English lexicon with approximately 200k entries. 
By using the tokenization process described above, 
we extracted word unigram and bigram statistics 
from query logs to be used as the system?s lan-
guage model. 
5 Query Correction 
An input query is tokenized using the same space 
and word-delimiter information in addition to the 
available lexical information as used for process-
ing the query log. For each token, a set of alterna-
tives is computed using the weighted Levenshtein 
distance function described in Section 3 and two 
different thresholds for in-lexicon and out-of-
lexicon tokens 
 Matches are searched in the space of word uni-
grams and bigrams extracted from query logs in 
addition to the trusted lexicon. Unigrams and bi-
grams are stored in the same data structure on 
which the search for correction alternatives is 
done. Because of this, the proposed system han-
dles concatenation and splitting of words in ex-
actly the same manner as it handles 
transformations of words to other words. 
 Once the sets of all possible alternatives are com-
puted for each word form in the query, a modified 
Viterbi search (in which the transition probabilities 
are computed using bigram and unigram query-log 
statistics and output probabilities are replaced with 
inverse distances between words) is employed to 
find the best possible alternative string to the input 
query under the following constraint: no two adja-
cent in-vocabulary words are allowed to change 
simultaneously. This constraint prevents changes 
such as log wood  dog food. An algorithmic con-
sequence of this constraint is that there is no need 
to search all the possible paths in the trellis, which 
makes the modified search procedure much faster, 
as described further. We assume that the list of 
alternatives for each word is randomly ordered but 
the input word is on the first position of the list 
when the word is in the trusted lexicon. In this 
case, the searched paths form what we call fringes. 
Figure 1 presents an example of a trellis in which 
w1, w2 and w3 are in-lexicon word forms. Observe 
that instead of computing the cost of k1k2 possible 
paths between the alternatives corresponding to w1 
and w2, we only need to compute the cost of k1+k2 
paths. 
31 =l  
42 =l  
20 =l  0s  britenetspear   inconcert 
 
1s  britneyspears  in concert 
 
2s  britney spears in concert 
 
3s  britney spears in concert 
11
2
1
1
1
1ka
a
a
w

2
2
2
2
1
2
2ka
a
a
w

3
3
2
3
1
3
3ka
a
a
w

4
4
2
4
1
4
4ka
a
a
w

5
5
2
5
1
5
5ka
a
a
w

6
6
2
6
1
6
6ka
a
a
w

7
7
2
7
1
7
7ka
a
a
w

sto
p w
or
d
un
kno
wn
 
wo
rd
 
Figure 1. Example of trellis of the modified Viterbi search 
 Because we use word-bigram statistics, stop 
words such as prepositions and conjunctions may 
interfere negatively with the best path search. For 
example, in correcting a query such as platunum 
and rigs, the language model based on word bi-
grams would not provide a good context for the 
word form rigs. 
 To avoid this type of problems, stop words and 
their most likely misspelling are given a special 
treatment. The search is done by first ignoring 
them, as in Figure 1, where w4 is presumed to be 
such a word. Once a best path is found by ignoring 
stop words, the best alternatives for the skipped 
stop words (or their misspellings) are computed in 
a second Viterbi search with fringes in which the 
extremities are fixed, as presented in Figure 2. 
 
1
1
2
1
1
1
1ka
a
a
w

2
2
2
2
1
2
2ka
a
a
w

3
3
2
3
1
3
3ka
a
a
w

4
4
2
4
1
4
4ka
a
a
w

5
5
2
5
1
5
5ka
a
a
w

6
6
2
6
1
6
6ka
a
a
w

7
7
2
7
1
7
7ka
a
a
w

sto
p w
or
d
 
Figure 2. Modified Viterbi search ? stop-word treatment 
 The approach of search with fringes coupled with 
an iterative correction process is both very effi-
cient and very effective. In each iteration, the 
search space is much reduced. Changes such as log 
wood  dog food are avoided because they can not 
be made in one iteration and there are no interme-
diate corrections conditionally more probable than 
the left-hand-side query (log wood) and less prob-
able than the right-hand-side query (dog food). 
 An iterative process is prone to other types of 
problems. Short queries can be iteratively trans-
formed into other un-related queries; therefore, 
changing such queries is restricted additionally in 
our system. Another restriction we imposed is to 
not allow changes of in-lexicon words in the first 
iteration, so that easy-to-fix unknown-word errors 
are handled before any word substitution error. 
6 Evaluation 
For this work, we are concerned primarily with 
recall because providing good suggestions for mis-
spelled queries can be viewed as more important 
than abstaining to provide alternative query sug-
gestions for valid queries as long as these sugges-
tions are reasonable (for example, suggesting 
cowboy ropes for cowboy robes may not have ma-
jor cost to a user). A real system would have a 
component that decides whether to surface a spell-
ing suggestion based on where we want to be on 
the ROC curve, thus negotiating between precision 
and recall. 
 One problem with evaluating a spell checker de-
signed to correct search queries is that evaluation 
data is hard to get. Even if the system were used 
by a search engine and click-through information 
were available, such information would provide 
only a crude measure of precision and would not 
allow us to measure recall, by capturing only cases 
in which the corrections proposed by that particu-
lar speller are clicked on by the users. 
 We performed two different evaluations of the 
proposed system.4 The first evaluation was done 
on a test set comprising 1044 unique randomly 
sampled queries from a daily query log, which 
were annotated by two annotators. Their inter-
agreement rate was 91.3%. 864 of these queries 
were considered valid by both annotators; for the 
other 180, the annotators provided spelling correc-
tions. The overall agreement of our system with 
the annotators was 81.8%. The system suggested 
131 alternative queries for the valid set, counted as 
false positives, and 156 alternative queries for the 
misspelled set. Table 2 shows the accuracy ob-
tained by the proposed system and results from an 
ablation study where we disabled various compo-
nents of the system, to measure their influence on 
performance. 
                                                          
4
 The test data sets can be downloaded from 
http://research.microsoft.com/~silviu/Work 
  All queries Valid Misspelled 
Nr. queries 1044 864 180 
Full system 81.8 84.8 67.2 
No lexicon 70.3 72.2 61.1 
No query log 77.0 82.1 52.8 
All edits equal 80.4 83.3 66.1 
Unigrams only 54.7 57.4 41.7 
1 iteration only 80.9 88.0 47.2 
2 iterations only 81.3 84.4 66.7 
No fringes 80.6 83.3 67.2 
Table 2. Accuracy of various instantiations of the system 
 By completely removing the trusted lexicon, the 
accuracy of the system on misspelled queries 
(61.1%) was higher than in the case of only using 
a trusted lexicon and no query log data (52.8%). It 
can also be observed that the language model built 
using query logs is by far more important than the 
channel model employed: using a poorer character 
error model by setting all edit weights equal did 
not have a major impact on performance (66.1% 
recall), while using a poorer language model that 
only employs unigram statistics from the query 
logs crippled the system (41.7% recall). Another 
interesting aspect is related to the number of itera-
tions. Because the first iteration is more conserva-
tive than the following iterations, using only one 
iteration led to fewer false positives but also to a 
much lower recall (47.2%). Two iterations were 
sufficient to correct most of the misspelled queries 
that the full system could correct. While fringes 
did not have a major impact on recall, they helped 
avoid false positives (and had a major impact on 
speed). 
81.2
81.681.880.7
69.468.9
67.2
66.1
65
70
75
80
85
1 month 2 months 3 months 4 months
All queries
Mispelled queries
 
Figure 3. Accuracy and recall as functions of the number of  
 monthly query logs used to train the language model 
 Figure 3 shows the performance of the full system 
as a function of the number of monthly query logs 
employed. While both the total accuracy and the 
recall increased when using 2 months of data in-
stead of 1 month, by using more query log data (3 
and 4 month), the recall (or accuracy on mis-
spelled queries) still improves but at the expense 
of having more false positives for valid queries, 
which leads to an overall slightly smaller accuracy.  
 A post-analysis of the results showed that the sys-
tem suggested in many cases reasonable correc-
tions but different from the gold standard ones. 
Many false positives could be considered reason-
able suggestions, although it is not clear whether 
they would have been helpful to the users (e.g. 
2002 kawasaki ninja zx6e  2002 kawasaki ninja 
zx6r was counted as an error, although the sugges-
tion represents a more popular motorcycle model). 
In the case of misspelled queries in which the 
user?s intent was not clear, the suggestion made by 
the system could be considered valid despite the 
fact that it disagreed with the annotators? choice 
(e.g. gogle  google instead of the gold standard 
correction goggle). 
 To address the problems generated by the fact that 
the annotators could only guess the user intent, we 
performed a second evaluation, on a set of queries 
randomly extracted from query log data, by sam-
pling pairs of successive queries ),( 21 qq  sent by 
the same users in which the queries differ from 
one another by an un-weighted edit distance of at 
most 1+(len( 1q )+len( 2q ))/10 (i.e. allow a point 
change for every 5 letters). We then presented the 
list to human annotators who had the option to re-
ject a pair, choose one of the queries as a valid cor-
rection of the other, or propose a correction for 
both when none of them were valid but the in-
tended valid query was easy to guess from the se-
quence, as in example 3 below: 
(audio flie, audio file)  audio file 
(bueavista, buena vista)  buena vista 
(carrabean nooms, carrabean rooms)  caribbean rooms 
 Table 3 shows the performance obtained by dif-
ferent instantiations of the system on this set.  
 
Full system 73.1 
No lexicon 59.2 
No query log 44.9 
All edits equal 69.9 
Unigrams only 43.0 
1 iteration only 45.5 
2 iterations only 68.2 
No fringes 71.0 
Table 3. Accuracy of the proposed system on a set which  
     contains misspelled queries that the users had reformulated 
 The main system disagreed 99 times with the gold 
standard, in 80 of these cases suggesting a differ-
ent correction. 40 of the corrections were not ap-
propriate (e.g. porat was corrected by our system 
to pirate instead of port in chinese porat also 
called xiamen), 15 were functionally equivalent 
corrections given our target search engine (e.g. 
audio flie  audio files instead of audio file), 17 
were different valid suggestions (e.g. bellsouth 
lphone isting  bellsouth phone listings instead of 
bellsouth telephone listing), while 8 represented 
gold standard errors (e.g. the speller correctly sug-
gested brandy sniffters  brandy snifters instead 
of brandy sniffers). Out of 19 cases in which the 
system did not make a suggestion, 13 were genu-
ine errors (e.g. paul waskiewiscz with the correct 
spelling paul waskiewicz), 4 were cases in which 
the original input was correct, although different 
from the user?s intent (e.g. cooed instead of coed) 
and 2 were gold standard errors (e.g. commandos 3 
walkthrough had the wrong correction commando 
3 walkthrough, as this query refers to a popular 
videogame called ?commandos 3?). 
 
Differences Gold std errors Format  Diff. valid Real Errors 
80+19 8+2 15+0 17+4 40+13 
 The above table shows a synthesis of this error 
analysis on the second evaluation set. The first 
number in each column refers to a precision error 
(i.e. the speller suggested something different than 
the gold standard), while the second refers to a 
recall error (i.e. no suggestion). 
 As a result of this error analysis, we could argua-
bly consider that while the agreement with the 
gold standard experiments are useful for measur-
ing the relative importance of components, they do 
not give us an absolute measure of  system useful-
ness/accuracy. 
 
Agreement Correctness Precision Recall 
73.1 85.5 88.4 85.4 
 In the above table, we consider correctness as the 
relative number of times the suggestion made by 
the speller was correct or reasonable; precision 
measures the number of correct suggestions in the 
total number of spelling suggestions made by the 
system; recall is computed as the relative number 
of correct/reasonable suggestions made when such 
suggestions were needed.  
 As an additional verification and to confirm the 
difficulty of the test queries, we sent a set of them 
to Google and observed that Google speller?s 
agreement with the gold standard was slightly 
lower than our system?s agreement. 
7 Conclusion 
To our knowledge, this paper is the first to show a 
successful attempt of using the collective knowl-
edge stored in search query logs for the spelling 
correction task. We presented a technique to mine 
this extremely informative but very noisy resource 
that actually exploits the errors made by people as 
a way to do effective query spelling correction. A 
direction that we plan to investigate is the adapta-
tion of such a technique to the general purpose 
spelling correction, by using statistics from both 
query-logs and large office document collections. 
Acknowledgements 
We wish to thank Robert Ragno and Robert Roun-
thwaite for helpful comments and discussions.  
References 
Brill, E. and R. Moore. 2000. An improved error model for 
noisy channel spelling correction. In Proceedings of the ACL 
2000, pages 286-293. 
Cherkassky, V., N. Vassilas, G.L. Brodt, R.A. Wagner, and 
M.J. Fisher. 1974. The string to string correction problem. In 
Journal of ACM, 21(1):168-178. 
Damerau, F.J. 1964. A technique for computer detection and 
correction of spelling errors. In Communications of ACM, 
7(3):171-176. 
Garside, R., G. Leech and G. Sampson. 1987. Computational 
analysis of English: a corpus-based approach, Longman. 
Golding, A.R. 1995. A Bayesian hybrid method for context-
sensitive spelling correction. In Proceedings of the Work-
shop on Very Large Corpora, pages 39-53. 
Golding, A.R. and D. Roth. 1996. Applying winnow to con-
text-sensitive spelling correction. In Proceedings of ICML 
1996, pages 182-190. 
Heidorn, G.E., K. Jensen, L.A. Miller, R.J. Byrd and M.S. 
Chodorow. 1982. The EPISTLE text-critiquing system. In 
IBM Systems Journal, 21(3):305-326. 
Jurafsky, D. and J.H. Martin. 2000. Speech and language 
processing. Prentice-Hall. 
Kernighan, M., K. Church, and W. Gale. 1990. A spelling 
correction program based on a noisy channel model. In Pro-
ceedings of COLING 1990. 
Kukich, K. 1992. Techniques for automatically correcting 
words in a text. In Computing Surveys, 24(4):377-439. 
Mays, E., F.J. Damerau and R.L. Mercer. 1991. Context-
based spelling correction. In Information Processing and 
Management, 27(5):517-522. 
Mangu, L. and E. Brill. 1997. Automatic rule acquisition for 
spelling correction. In Proceedings of the ICML 1997, pages 
734-741. 
McIlroy, M.D. 1982. Development of a spelling list. In J-
IEEE-TRANS-COMM, 30(1);91-99. 
Peterson, J.L. 1980. Computer programs for spelling correc-
tion: an experiment in program design. Springer-Verlag. 
Rieseman, E.M. and A.R. Hanson. 1974. A contextual post-
processing system for error correction using binary n-grams. 
In IEEE Transactions on Computers, 23(5):480-493. 
Toutanova, K. and R. C. Moore. 2002. Pronunciation Model-
ing for Improved Spelling Correction. In Proceedings of the 
ACL 2002.pages 141-151. 
Wittgenstein, L. 1968. Philosophical Investigations. Basil 
Blackwell, Oxford. 
