Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 37?45,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Construction of a German HPSG grammar from a detailed treebank
Bart Cramer? and Yi Zhang??
Department of Computational Linguistics & Phonetics, Saarland University, Germany?
LT-Lab, German Research Center for Artificial Intelligence, Germany?
{bcramer,yzhang}@coli.uni-saarland.de
Abstract
Grammar extraction in deep formalisms
has received remarkable attention in re-
cent years. We recognise its value, but try
to create a more precision-oriented gram-
mar, by hand-crafting a core grammar, and
learning lexical types and lexical items
from a treebank. The study we performed
focused on German, and we used the Tiger
treebank as our resource. A completely
hand-written grammar in the framework of
HPSG forms the inspiration for our core
grammar, and is also our frame of refer-
ence for evaluation. 1
1 Introduction
Previous studies have shown that treebanks can
be helpful when constructing grammars. The
most well-known example is PCFG-based statis-
tical parsing (Charniak and Johnson, 2005), where
a PCFG is induced from, for instance, the Penn
Treebank. The underlying statistical techniques
have been refined in the last decade, and previ-
ous work indicates that the labelled f-score of this
method converges to around 91%.
An alternative to PCFGs, with more linguistic
relevance, is formed by deeper formalisms, such
as TAG (Joshi and Schabes, 1997), CCG (Steed-
man, 1996), LFG (Kaplan and Bresnan, 1995)
and HPSG (Pollard and Sag, 1994). For LFG
(Butt et al, 2002) and HPSG (Flickinger, 2000;
Mu?ller, 2002), large hand-written grammars have
been developed. In the case of HPSG, the gram-
mar writers found the small number of principles
too restrictive, and created more rules (approxi-
mately 50 to 300) to accommodate for phenomena
1The research reported in this paper has been carried out
with financial support from the Deutsche Forschungsgemein-
schaft and the German Excellence Cluster of Multimodal
Computing & Interaction.
that vanilla HPSG cannot describe correctly. The
increased linguistic preciseness comes at a cost,
though: such grammars have a lower out-of-the-
box coverage, i.e. they will not give an analysis on
a certain portion of the corpus.
Experiments have been conducted, where a
lexicalised grammar is learnt from treebanks, a
methodology for which we coin the name deep
grammar extraction. The basic architecture of
such an experiment is to convert the treebank to
a format that is compatible with the chosen lin-
guistic formalism, and read off the lexicon from
that converted treebank. Because all these for-
malisms are heavily lexicalised, the core gram-
mars only consist of a small number of principles
or operators. In the case of CCG (Hockenmaier
and Steedman, 2002), the core grammar consists
of the operators that CCG stipulates: function ap-
plication, composition and type-raising. Standard
HPSG defines a few schemata, but these are usu-
ally adapted for a large-scale grammar. Miyao et
al. (2004) tailor their core grammar for optimal use
with the Penn Treebank and the English language,
for example by adding a new schema for relative
clauses.
Hockenmaier and Steedman (2002), Miyao et
al. (2004) and Cahill et al (2004) show fairly good
results on the Penn Treebank (for CCG, HPSG and
LFG, respectively): these parsers achieve accura-
cies on predicate-argument relations between 80%
and 87%, which show the feasibility and scalabil-
ity of this approach. However, while this is a sim-
ple method for a highly configurational language
like English, it is more difficult to extend to lan-
guages with more complex morphology or with
word orders that display more freedom. Hocken-
maier (2006) is the only study known to the au-
thors that applies this method to German, a lan-
guage that displays these properties.
This article reports on experiments where the
advantages of hand-written and derived grammars
37
are combined. Compared to previous deep gram-
mar extraction approaches, a more sophisticated
core grammar (in the framework of HPSG) is cre-
ated. Also, more detailed syntactic features are
learnt from the resource treebank, which leads to
a more precise lexicon. Parsing results are com-
pared with GG (German Grammar), a previously
hand-written German HPSG grammar (Mu?ller,
2002; Crysmann, 2003; Crysmann, 2005).
2 Core grammar
2.1 Head-driven phrase structure grammar
This study has been entirely embedded in the
HPSG framework (Pollard and Sag, 1994). This
is a heavily lexicalised, constraint-based theory of
syntax, and it uses typed feature structures as its
representation. HPSG introduces a small num-
ber of principles (most notably, the Head Feature
Principle) that guide the construction of a few Im-
mediate Dominance schemata. These schemata
are meant to be the sole basis to combine words
and phrases. Examples of schemata are head-
complement, head-subject, head-specifier, head-
filler (for long-distance dependencies) and head-
modifier.
In this study, the core grammar is an extension
of the off-the-shelf version of HPSG. The type hi-
erarchy is organised by a typed feature structure
hierarchy (Carpenter, 1992), and can be read by
the LKB system (Copestake, 2002) and the PET
parser (Callmeier, 2000). The output is given in
Minimal Recursion Semantics (Copestake et al,
2005) format, which can be minimally described
as a way to include scope information in depen-
dency output.
2.2 The German language
Not unlike English, German uses verb position
to distinguish between different clause types. In
declarative sentences, verbs are positioned in the
second position, while subordinate classes are
verb-final. Questions and imperatives are verb-
initial. However, German displays some more
freedom with respect to the location of subjects,
complements and adjuncts: they can be scram-
bled rather freely. The following sentences are
all grammatical, and have approximately the same
meaning:
(1) a. Der
The.NOM
Pra?sident
President.NOM
hat
has
gestern
yesterday
das
the.ACC
Buch
book.ACC
gelesen.
read.PERF.
?The president read the book yester-
day?
b. Gestern hat der Pra?sident das Buch
gelesen.
c. Das Buch hat der Pra?sident gestern
gelesen.
As can be seen, the main verb is placed at sec-
ond position (the so-called ?left bracket?), but all
other verbs remain at the end of the sentence,
in the ?right bracket?. Most linguistic theories
about German recognise the existence of topolog-
ical fields: the Vorfeld before the left bracket, the
Mittelfeld between both brackets, and the Nach-
feld after the right bracket. The first two are
mainly used for adjuncts and arguments, whereas
the Nachfeld is typically, but not necessarily, used
for extraposed material (e.g. relative clauses or
comparative phrases) and some VPs. Again, the
following examples mean roughly the same:
(2) a. Er
He
hat
has
das
the.ACC
Buch,
Book.ACC,
das
that
sie
she
empfohlen
recommended
hat,
has,
gelesen.
read.PERF.
He has read the book that she recom-
mended.
b. Er hat das Buch gelesen, das sie emp-
fohlen hat.
c. Das Buch hat er gelesen, das sie emp-
fohlen hat.
Another distinctive feature of German is its rela-
tively rich morphology. Nominals are marked with
case, gender and number, and verbs with number,
person, tense and mood. Adjectives and nouns
have to agree with respect to gender, number and
declension type, the latter being determined by
the (non-)existence and type of determiner used
in the noun phrase. Verbs and subjects have to
agree with respect to number and person. Ger-
man also displays highly productive noun com-
pounding, which amplifies the need for effective
unknown word handling. Verb particles can ei-
ther be separated from or concatenated to the verb:
compare ?Er schla?ft aus? (?He sleeps in?) and ?Er
38
Amerikaner
?
?
no-det
VAL
[
SPEC ??
SUBCAT ??
]
?
?
?
?
noun
VAL
[
SPEC ?det?
SUBCAT ??
]
?
?
mu?ssen
?
?
?
?
?
?
?
?
verb
VAL 1
SLASH 2
XCOMP
?
?
verb
VAL 1
SLASH 2
?
?
?
?
?
?
?
?
?
?
hart
[
adverb
MOD verb
]
arbeiten
?
?
verb-inf
VAL
[
SUBJ ?np-nom?
]
SLASH ??
?
?
?
?
slash-subj
VAL
[
SUBJ ??
]
SLASH ?np-nom?
?
?
?
?
mod-head
VAL
[
SUBJ ??
]
SLASH ?np-nom?
?
?
?
?
head-cluster
VAL
[
SUBJ ??
]
SLASH ?np-nom?
?
?
?
?
filler-head
VAL
[
SUBJ ??
]
SLASH ??
?
?
Figure 1: This figure shows a (simplified) parse tree of the sentence ?Amerikaner mu?ssen hart arbeiten?
(?Americans have to work hard?).
wird ausschlafen? (?He will sleep in?). In such
verbs, the word ?zu? (which translates to the En-
glish ?to? in ?to sleep?) can be infixed as well: ?er
versucht auszuschlafen? (?He tries to sleep in?).
These characteristics make German a compar-
atively complex language to parse with CFGs:
more variants of the same lemma have to be mem-
orised, and the expansion of production rules will
be more diverse, with a less peaked statistical dis-
tribution. Efforts have been made to adapt existing
CFG models to German (Dubey and Keller, 2003),
but the results still don?t compare to state-of-the-
art parsing of English.
2.3 Structure of the core grammar
The grammar uses the main tenets from Head-
driven Phrase Structure Grammar (Pollard and
Sag, 1994). However, different from earlier deep
grammar extraction studies, more sophisticated
structures are added. Mu?ller (2002) proposes a
new schema (head-cluster) to account for verb
clusters in the right bracket, which includes the
possibility to merge subcategorisation frames of
e.g. object-control verbs and its dependent verb.
Separate rules for determinerless NPs, genitive
modification, coordination of common phrases,
relative phrases and direct speech are also created.
The free word order of German is accounted for
by scrambling arguments with lexical rules, and
by allowing adjuncts to be a modifier of unsat-
urated verb phrases. All declarative phrases are
considered to be head-initial, with an adjunct or
argument fronted using the SLASH feature, which
is then discharged using the head-filler schema.
The idea put forward by, among others, (Kiss and
Wesche, 1991) that all sentences should be right-
branching is linguistically pleasing, but turns out
be computationally very expensive (Crysmann,
2003), and the right-branching reading should be
replaced by a left-branching reading when the
right bracket is empty (i.e. when there is no auxil-
iary verb present).
An example of a sentence is presented in fig-
ure 1. It receives a right-branching analysis, be-
cause the infinitive ?arbeiten? resides in the right
bracket. The unary rule slash-subj moves the re-
quired subject towards the SLASH value, so that it
can be discharged in the Vorfeld by the head-filler
schema. ?mu?ssen? is an example of an argument
attraction verb, because it pulls the valence fea-
ture (containing SUBJ, SUBCAT etc; not visible
in the diagram) to itself. The head-cluster rule as-
sures that the VAL value then percolates upwards.
Because ?Amerikaner? does not have a specifier, a
separate unary rule (no-det) takes care of discharg-
ing the SPEC feature, before it can be combined
with the filler-head rule.
As opposed to (Hockenmaier, 2006), this study
39
(a)
teure Detektive kann sich der Supermarkt nicht leisten
NP
MO HD
NP
DET HD
VP
HDNGOA DA
S
SBOCHD
(b)
teure Detektive kann sich der Supermarkt nicht leisten
NP
MO HD
NP
DET HD
S
OA HD REFL SB MO VC
Figure 2: (a) shows the original sentence, whereas (b) shows the sentence after preprocessing. Note that
NP is now headed, that the VP node is deleted, and that the verbal cluster is explicitly marked in (b). The
glossary of this sentence is ?Expensive.ACC detectives.ACC can REFL the.NOM supermarket.NOM not
afford?
employs a core lexicon for words that have marked
semantic behaviour. These are usually closed
word classes, and include items such as raising
and auxiliary verbs, possessives, reflexives, arti-
cles, complementisers etc. The size of this core
lexicon is around 550 words. Note that, because
the core lexicon only contains function words, its
coverage is negligible without additional entries.
3 Derivation of the lexicon
3.1 The Tiger treebank
The Tiger treebank (Brants et al, 2002) is a tree-
bank that embraces the concept of constituency,
but can have crossing branches, i.e. the tree might
be non-projective. This allowed the annotators to
capture the German free word order. Around one-
third of the sentences received a non-projective
analysis. An example can be found in figure 2.
Additionally, it annotates each branch with a syn-
tactic function.
The text comes from a German newspaper
(Frankfurter Rundschau). It was annotated semi-
automatically, using a cascaded HMM model. Af-
ter each phase of the HMM model, the output was
corrected by human annotators. The corpus con-
sists of over 50,000 sentences, with an average
sentence length of 17.6 tokens (including punc-
tuation). The treebank employs 26 phrase cate-
gories, 56 PoS tags and 48 edge labels. It also en-
codes number, case and gender at the noun termi-
nals, and tense, person, number and mood at verbs.
Whether a verb is finite, an infinitive or a partici-
ple is encoded in the PoS tag. A peculiarity in the
annotation of noun phrases is the lack of headed-
ness, which was meant to keep the annotation as
theory-independent as reasonably possible.
3.2 Preprocessing
A number of changes had to be applied to the tree-
bank to facilitate the read-off procedure:
? A heuristic head-finding procedure is applied
in the spirit of (Magerman, 1995). We use
priority lists to find the NP?s head, deter-
miner, appositions and modifiers. PPs and
CPs are also split into a head and its depen-
dent.
? If a verb has a separated verb particle, this
particle is attached to the lemma of the verb.
For instance, if the verb ?schlafen? has a parti-
cle ?aus?, the lemma will be turned into ?auss-
chlafen? (?sleep in?). If this is not done, sub-
categorisation frames will be attributed to the
wrong lemma.
? Sentences with auxiliaries are non-projective,
if the adjunct of the embedded VP is in the
Vorfeld. This can be solved by flattening the
tree (removing the VP node), and marking
the verbal cluster (VC) explicitly. See fig-
ure 2 for an example. 67.6% of the origi-
nal Tiger treebank is projective, and with this
procedure, this is lifted to 80.1%.
? The Tiger treebank annotates reflexive pro-
nouns with the PoS tag PRF, but does not
distinguish the syntactic role. Therefore, if a
verb has an object that has PRF as its part-of-
speech, the label of that edge is changed into
REFL, so that reflexive verbs can be found.
40
(a)
0 10 20 30 40 50
0
10000
20000
30000
40000
50000
(b)
0 10 20 30 40 50
0
200
400
600
800
1000
(c)
0 10 20 30 40 50
0
0.1
0.2
0.3
0.4
0.5
Figure 3: These graphs show learning curves of the algorithm on the first 45,000 sentences of the Tiger
treebank. Graph (a) indicates the amount of lemmas learnt (from top to bottom: nouns, names, adjec-
tives, verbs and adverbs). The graph in (b) shows the number of distinct lexical types for verbs that are
learnt. Graph (c) shows the average proportion of morphological forms that is observed per verb lemma,
assuming that each verb has 28 different forms: infinitive, zu (to) + infinitive, participle, imperative and
24 finite forms (3 (person) * 2 (number) * 2 (tense) * 2 (mood)).
The preprocessing stage failed in 1.1% of the
instances.
3.3 Previous work
The method described in Hockenmaier (2006) first
converts the Tiger analysis to a tree, after which
the lexical types were derived. Because it was
the author?s goal to convert all sentences, some
rather crude actions had to be taken to render
non-projective trees projective: whenever a cer-
tain node introduces non-projectivity, some of its
daughters are moved to the parent tree, until that
node is projective. Below, we give two examples
where this will lead to incorrect semantic compo-
sition, with the consequence of flawed lexicon en-
tries. We argue that it is questionable whether the
impressive conversion scores actually represent a
high conversion quality. It would be interesting to
see how this grammar performs in a real parsing
task, but no such study has been carried out so far.
The first case deals with extraposed material in
the Nachfeld. Typical examples include relative
phrases, comparatives and PH/RE constructions2.
2NPs, AVPs and PPs can, instead of their usual headed
structure, be divided in two parts: a ?placeholder? and
a ?repeated element?. These nodes often introduce non-
projectivity, and it is not straightforward to create a valid lin-
guistic analysis for these phenomena. Example sentences of
these categories (NPs, AVPs and PPs, respectively) are:
(1) [ PH Es ] ist wirklich schwer zu sagen, [ RE welche
Positionen er einnimmt ]
(2) Man mu? sie also [ PH so ] behandeln , [ RE wie man
eine Weltanschauungsbewegung behandelt ]
(3) Alles deutet [ PH darauf ] hin [ RE da? sie es nicht
schaffen wird ]
These examples all have the RE in the Nachfeld, but their
placement actually has a large variety.
The consequence is that the head of the extraposed
material will be connected to the verb, instead of
to the genuine head.
Another example where Hockenmaier?s algo-
rithm will create incorrect lexical entries is when
the edge label is PAR (for ?parentheses?) or in the
case of appositions. Consider the following sen-
tence:
(3) mit
with
160
160
Planstellen
permanent posts
(etliche
(several
sind
are
allerdings
however
noch
still
unbesetzt)
unoccupied)
The conclusion that will be drawn from this sen-
tence is that ?sind? can modify nouns, which is
only true due to the parentheses, and has no re-
lation with the specific characteristics of ?sind?.
Similarly, appositions will act as modifiers of
nouns. Although one might argue that this is the
canonical CCG derivation for these phenomena, it
is not in the spirit of the HPSG grammars, and we
believe that these constructions are better handled
in rules than in the lexicon.
3.4 Procedure
In our approach, we will be more conservative,
and the algorithm will only add facts to its knowl-
edge base if the evidence is convincing. That
means that less Tiger graphs will get projective
analyses, but that doesn?t have to be a curse: we
can derive lexical types from non-projective anal-
yses just as well, and leave the responsibility for
solving the more complex grammatical phenom-
ena to the core grammar. For example, lexical
rules will deal with fronting and Mittelfeld scram-
bling, as we have stated before. This step of the
41
procedure has indeed strong affinity with deep lex-
ical acquisition, except for the fact that in DLA all
lexical types are known, and this is not the case in
this study: the hand-written lexical type hierarchy
is still extended with new types that are derived
from the resource treebank, mostly for verbs.
The basic procedure is as follows:
? Traverse the graph top-down.
? For each node:
? Identify the node?s head (or the deepest
verb in the verb cluster3);
? For each complement of this node, add
this complement to the head?s subcate-
gorisation frame.
? For each modifier, add this head to the
possible MOD values of the modifier?s
head.
? For each lexical item, a mapping of (lemma,
morphology)? word form is created.
After this procedure, the following information
is recorded for the verb lemma ?leisten? from fig-
ure 2:
? It has a subcategorisation frame ?npnom-refl-
npacc?.
? Its infinitive form is ?leisten?.
The core grammar defines that possible sub-
jects are nominative NPs, expletive ?es? and CPs.
Expletives are considered to be entirely syntac-
tic (and not semantic), so they will not receive a
dependency relation. Complements may include
predicative APs, predicative NPs, genitive, dative
and accusative NPs, prepositional complements,
CPs, reflexives, separable particles (also purely
syntactic), and any combination of these. For non-
verbs, the complements are ordered (i.e. it is a
list, and not a verb). Verb complementation pat-
terns are sets, which means that duplicate com-
plements are not allowed. For verbs, it is also
recorded whether the auxiliary verb to mark the
perfect tense should be either ?haben? (default) or
?sein? (mostly verbs that have to do with move-
ment). Nouns are annotated with whether they can
have appositions or not.
3That means that the head of a S/VP-node is assumed
to be contained in the lexicon, as it must be some sort of
auxiliary.
Results from the derivation procedure are
graphed in figure 3. The number of nouns and
names is still growing after 45,000 sentences,
which is an expected result, given the infinite na-
ture of names and frequent noun compounding.
However, it appears that verbs, adjectives and ad-
verbs are converging to a stable level. On the other
hand, lexical types are still learnt, and this shows a
downside of our approach: the deeper the extrac-
tion procedure is, the more data is needed to reach
the same level of learning.
The core grammar contains a little less than 100
lexical types, and on top of that, 636 lexical types
are learnt, of which 579 are for verbs. It is inter-
esting to see that the number of lexical types is
considerably lower than in (Hockenmaier, 2006),
where around 2,500 lexical types are learnt. This
shows that our approach has a higher level of gen-
eralisation, and is presumably a consequence of
the fact that the German CCG grammar needs dis-
tinct lexical types for verb-initial and verb-final
constructions, and for different argument scram-
blings in the Mittelfeld, whereas in our approach,
hand-written lexical rules are used to do the scram-
bling.
The last graph shows that the number of word
forms is still insufficient. We assume that each
verb can have 28 different word forms. As can be
seen, it is clear that only a small part of this area
is learnt. One direction for future research might
be to find ways to automatically expand the lexi-
con after the derivation procedure, or to hand-code
morphological rules in the core grammar.
4 Parsing
4.1 Methodology
All experiments in this article use the first 45,000
sentences as training data, and the consecutive
5,000 sentences as test data. The remaining 472
sentences are not used. We used the PET parser
(Callmeier, 2000) to do all parsing experiments.
The parser was instructed to yield a parse error af-
ter 50,000 passive edges were used. Ambiguity
packing (Oepen and Carroll, 2000) and selective
unpacking (Zhang et al, 2007) were used to re-
duce memory footprint and speed up the selection
of the top-1000 analyses. The maximum entropy
model, used for selective unpacking, was based on
200 treebanked sentences of up to 20 words from
the training set. Part-of-speech tags delivered by
the stock version of the TnT tagger (Brants, ) were
42
Tiger T.+TnT GG
Out of vocabulary 71.9 % 5.2 % 55.6 %
Parse error 0.2 % 1.5 % 0.2 %
Unparsed 7.9 % 37.7 % 28.2 %
Parsed 20.0 % 55.6 % 16.0 %
Total 100.0 % 100.0 % 100.0 %
Avg. length 8.6 12.8 8.0
Avg. nr. of parses 399.0 573.1 19.2
Avg. time (s) 9.3 15.8 11.6
Table 1: This table shows coverage results on the held-out test set. The first column denotes how the
extracted grammar performs without unknown word guessing. The second column uses PoS tags and
generic types to guide the grammar when an unknown word is encountered. The third column is the
performance of the fully hand-written HPSG German grammar by (Mu?ller, 2002; Crysmann, 2003).
OOV stands for out-of-vocabulary. A parse error is recorded when the passive edge limit (set to 50,000)
has been reached. The bottom three rows only gives information about the sentences where the grammar
actually returns at least one parse.
Training set Test set
All 100.0 % 100.0 %
Avg. length 14.2 13.5
Coverage 79.0 % 69.0 %
Avg. length 13.2 12.8
Correct (top-1000) 52.0% 33.5 %
Avg. length 10.4 8.5
Table 2: Shown are the treebanking results, giv-
ing an impression of the quality of the parses.
The ?training set? and ?test set? are subsets of 200
sentences from the training and test set, respec-
tively. ?Coverage? means that at least one analysis
is found, and ?correct? indicates that the perfect
solution was found in the top-1000 parses.
used when unknown word handling was turned
on. These tags were connected to generic lexical
types by a hand-written mapping. The version of
GG that was employed (Mu?ller, 2002; Crysmann,
2003) was dated October 20084.
4.2 Results
Table 1 shows coverage figures in three different
settings. It is clear that the resulting grammar has
a higher coverage than the GG, but this comes at a
cost: more ambiguity, and possibly unnecessary
ambiguity. Remarkably, the average processing
time is lower, even when the sentence lengths and
4It should be noted that little work has gone in to provid-
ing unknown word handling mechanisms, and that is why we
didn?t include it in our results. However, in a CoNLL-2009
shared task paper (Zhang et al, 2009), a coverage of 28.6%
was reported when rudimentary methods were used.
ambiguity rates are higher. We attribute this to
the smaller feature structure geometry that is in-
troduced by the core grammar (compared to the
GG). Using unknown word handling immediately
improved the coverage, by a large margin. Larger
ambiguity rates were recorded, and the number of
parser errors slightly increased.
Because coverage does not imply quality, we
wanted to look at the results in a qualitative fash-
ion. We took a sample of 200 sentences from
both the training and the test set, where the ones
from the training set did not overlap with the set
used to train the MaxEnt model, so that both set-
tings were equally influenced by the rudimentary
MaxEnt model. We evaluated for how many sen-
tences the exactly correct parse tree could be found
among the top-1000 parses (see table 2). The dif-
ference between the performance on the training
and test set give an idea of how well the gram-
mar performs on unknown data: if the difference
is small, the grammar extends well to unseen data.
Compared to evaluating on lexical coverage, we
believe this is a more empirical estimation of how
close the acquisition process is to convergence.
Based on the kind of parse trees we observed,
the impression was that on both sets, performance
was reduced due to the limited predictive power
of the disambiguation model. There were quite
a few sentences for which good parses could be
expected, because all lexical entries were present.
This experiment also showed that there were sys-
tematic ambiguities that were introduced by in-
consistent annotation in the Tiger treebank. For in-
43
stance, the word ?ein? was learnt as both a number
(the English ?one?) and as an article (?a?), leading
to spurious ambiguities for each noun phrase con-
taining the word ?ein?, or one of its morphological
variants. These two factors reinforced each other:
if there is spurious ambiguity, it is even harder for
a sparsely trained disambiguation model to pull
the correct parse inside the top-1000.
The difference between the two ?correct? num-
bers in table 2 is rather large, meaning that the
?real? coverage might seem disappointingly low.
Not unexpectedly, we found that the generic lex-
ical types for verbs (transitive verb, third person
singular) and nouns (any gender, no appositions
allowed) was not always correct, harming the re-
sults considerably.
A quantitative comparison between deep gram-
mars is always hard. Between DELPH-IN gram-
mars, coverage has been the main method of eval-
uation. However, this score does not reward rich-
ness of the semantic output. Recent evidence from
the ERG (Ytrest?l et al, 2009) suggests that the
ERG reaches a top-500 coverage of around 70%
on an unseen domain, a result that this experiment
did not approximate. The goal of GG is not com-
putational, but it serves as a testing ground for lin-
guistic hypotheses. Therefore, the developers have
never aimed at high coverage figures, and crafted
the GG to give more detailed analyses and to be
suited for both parsing and generation. We are
happy to observe that the coverage figures in this
study are higher than GG?s (Zhang et al, 2009),
but we realise the limited value of this evaluation
method. Future studies will certainly include a
more granular evaluation of the grammar?s perfor-
mance.
5 Conclusion and discussion
We showed how a precise, wide-coverage HPSG
grammar for German can be created successfully,
by constructing a core grammar by hand, and ap-
pending it with linguistic information from the
Tiger treebank. Although this extracted gram-
mar suffers considerably more from overgenera-
tion than the hand-written GG, we argue that our
conservative derivation procedure delivers a more
detailed, compact and correct compared to pre-
vious deep grammar extraction efforts. The use
of the core lexicon allows us to have more lin-
guistically motivated analyses of German than ap-
proaches where the core lexicon only comprises
the textbook principles/operators. We compared
our lexicon extraction results to those from (Hock-
enmaier, 2006). Also, preliminary parsing exper-
iments are reported, in which we show that this
grammar produces reasonable coverage on unseen
text.
Although we feel confident about the successful
acquisition of the grammar, there still remain some
limiting factors in the performance of the grammar
when actually parsing. Compared to coverage fig-
ures of around 80%, reported by (Riezler et al,
2001), the proportion of parse forests containing
the correct parse in this study is rather low. The
first limit is the constructional coverage, mean-
ing that the core grammar is not able to construct
the correct analysis, even though all lexical en-
tries have been derived correctly before. The most
frequent phenomena that are not captured yet are
PH/RE constructions and extraposed clauses, and
we plan to do an efficient implementation (Crys-
mann, 2005) of these in a next version of the gram-
mar. Second, as shown in figure 3, data scarcity in
the learning of the surface forms of lemmas neg-
atively influences the parser?s performance on un-
seen text.
In this paper, we focused mostly on the cor-
rectness of the derivation procedure. We would
like to address the real performance of the gram-
mar/parser combination in future work, which can
only be done when parses are evaluated according
to a more granular method than we have done in
this study. Furthermore, we ran into the issue that
there is no straightforward way to train larger sta-
tistical models automatically, which is due to the
fact that our approach does not convert the source
treebank to the target formalism?s format (in our
case HPSG), but instead reads off lexical types
and lexical entries directly. We plan to investigate
possibilities to have the annotation be guided auto-
matically by the Tiger treebank, so that the disam-
biguation model can be trained on a much larger
amount of training data.
Acknowledgements
We would like to thank Rebecca Dridan, Antske
Fokkens, Stephan Oepen and the anonymous re-
viewers for their valuable contributions to this pa-
per.
44
References
T. Brants. TnT: a statistical part-of-speech tagger. In
Proceedings of the Sixth Conference on Applied Nat-
ural Language Processing.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, pages 24?41.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project.
In International Conference On Computational Lin-
guistics, pages 1?7.
A. Cahill, M. Burke, R. ODonovan, J. Van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of ACL-2004, pages 320?327.
U. Callmeier. 2000. PET?a platform for experimen-
tation with efficient HPSG processing techniques.
Natural Language Engineering, 6(01):99?107.
B. Carpenter. 1992. The Logic of Typed Feature Struc-
tures: With Applications to Unification Grammars,
Logic Programs, and Constraint Resolution. Cam-
bridge University Press, Cambridge, UK.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL-2005, pages 173?180.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2005. Minimal Recursion Semantics: An Intro-
duction. Research on Language & Computation,
3(4):281?332.
A. Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA, USA.
B. Crysmann. 2003. On the efficient implementation
of German verb placement in HPSG. In Proceedings
of RANLP-2003, pages 112?116.
B. Crysmann. 2005. Relative Clause Extraposition in
German: An Efficient and Portable Implementation.
Research on Language & Computation, 3(1):61?82.
A. Dubey and F. Keller. 2003. Probabilistic parsing
for German using sister-head dependencies. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 96?103.
D. Flickinger. 2000. On building a more effcient gram-
mar by exploiting types. Natural Language Engi-
neering, 6(1):15?28.
J. Hockenmaier and M. Steedman. 2002. Acquiring
compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of LREC-2002, pages 1974?
1981.
J. Hockenmaier. 2006. Creating a CCGbank and a
Wide-Coverage CCG Lexicon for German. In Pro-
ceedings of ACL-2006, pages 505?512.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. Handbook of formal languages, 3:69?
124.
R.M. Kaplan and J. Bresnan. 1995. Lexical-Functional
Grammar: A formal system for grammatical rep-
resentation. Formal Issues in Lexical-Functional
Grammar, pages 29?130.
T. Kiss and B. Wesche. 1991. Verb order and head
movement. Text Understanding in LILOG, Lecture
Notes in Artificial Intelligence, 546:216?242.
D. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL-1995, pages
276?283.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2004. Corpus-
oriented grammar development for acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank. In Proceedings of IJCNLP-2004.
S. Mu?ller. 2002. Complex Predicates: Verbal
Complexes, Resultative Constructions, and Particle
Verbs in German. CSLI Publications, Stanford, CA,
USA.
S. Oepen and J. Carroll. 2000. Ambiguity packing in
constraint-based parsing: practical results. In Pro-
ceedings of NAACL-2000, pages 162?169.
C.J. Pollard and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University Of Chicago Press,
Chicago, IL, USA.
S. Riezler, T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell III, and M. Johnson. 2001. Parsing
the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques.
In Proceedings of ACL-2001, pages 271?278.
M. Steedman. 1996. Surface structure and interpreta-
tion. MIT Press, Cambridge, MA, USA.
G. Ytrest?l, D. Flickinger, and S. Oepen. 2009. Ex-
tracting and Annotating Wikipedia Sub-Domains.
In Proceedings of the Seventh International Work-
shop on Treebanks and Linguistic Theories, pages
185?197.
Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency
in Unification-Based N-Best Parsing. In Proceed-
ings of the Tenth International Conference on Pars-
ing Technologies, pages 48?59.
Y. Zhang, R. Wang, and S.. Oepen. 2009. Hybrid Mul-
tilingual Parsing with HPSG for SRL. In Proceed-
ings of CoNLL-2009, to appear.
45
Proceedings of the ACL 2007 Student Research Workshop, pages 43?48,
Prague, June 2007. c?2007 Association for Computational Linguistics
Limitations of Current Grammar Induction Algorithms
Bart Cramer
School of Behavioral and Cognitive Neurosciences
University of Groningen
Groningen, the Netherlands
bart.cramer@gmail.com
Abstract
I review a number of grammar induction
algorithms (ABL, Emile, Adios), and test
them on the Eindhoven corpus, resulting in
disappointing results, compared to the usu-
ally tested corpora (ATIS, OVIS). Also, I
show that using neither POS-tags induced
from Biemann?s unsupervised POS-tagging
algorithm nor hand-corrected POS-tags as
input improves this situation. Last, I argue
for the development of entirely incremental
grammar induction algorithms instead of the
approaches of the systems discussed before.
1 Introduction
Grammar induction is a task within the field of nat-
ural language processing that attempts to construct a
grammar of a given language solely on the basis of
positive examples of this language. If a successful
method is found, this will have both practical appli-
cations and considerable theoretical implications.
Concerning the practical side, this will make the
engineering of NLP systems easier, especially for
less widely studied languages. One can conceive
successful GI algorithms as an inspiration for sta-
tistical machine translation systems.
Theoretically, grammar induction is important as
well. One of the main assertions in the nativist?s
position is the Poverty of the Stimulus argument,
which means that the child does not perceive enough
positive examples of language throughout his early
youth to have learned the grammar from his parents,
without the help of innate knowledge (or: Universal
Grammar), that severely constrains the number of
hypotheses (i.e. grammars) that he can learn. Proved
more strictly for formal grammars, Gold?s (1967)
work showed that one cannot learn any type of su-
perfinite grammar (e.g. regular languages, context-
free languages), if one only perceives (an unlim-
ited amount of) positive examples. After, say, n ex-
amples, there is always more than 1 grammar that
would be able to explain the seen examples, thus
these grammar might give different judgments on an
n + 1th example, of which it is impossible to say in
advance which judgment is the correct one.
But, given this is true, isn?t the grammar induction
pursuit deemed to fail? Not really. First, there are
hints that children do receive negative information,
and that they use it for grammar acquisition. Also,
the strictness required by Gold is not needed, and an
approximation in the framework of PAC (Probably
Approximately Correct) or VC (Vapnis and Cher-
vonenkis) could then suffice. This, and other argu-
ments favouring the use of machine learning tech-
niques in linguistic theory testing, are very well re-
viewed in Lappin and Shieber (2007).
Several attempts have been made to create such
systems. The authors of these systems reported
promising results on the ATIS and OVIS treebanks. I
tried to replicate these findings on the more compli-
cated Eindhoven treebank, which turned out to yield
disappointing results, even inferior to very simple
baselines. As an attempt to ameliorate this, and as
an attempt to confirm Klein and Manning?s (2002)
and Bod?s (2006) thesis that good enough unsuper-
vised POS-taggers exist to justify using POS-tags
instead of words in evaluating GI systems, I pre-
43
sented the algorithms with both POS-tags that were
induced from Biemann?s unsupervised POS-tagging
algorithm and hand-corrected POS-tags. This did
not lead to improvement.
2 Current Grammar Induction Models
2.1 Algorithms
Grammar induction models can be split up into two
types: tag-based and word-based grammar induc-
tion. The key feature that distinguishes between
these two is the type of input. Tag-based systems
receive part-of-speech tags as their input (i.e. the
words are already labelled), and only induce rules
using the given tags. This kind of work is done
by, for instance, Klein and Manning (2005). On the
other hand, word-based models accept plain text as
its input, and have to extract both the categories and
the syntactic rules from given input.
Recently, several word-based grammar induction
algorithms have been developed: Alignment-Based
Learning (van Zaanen, 2002), Adios (Solan et al,
2005), Emile (Adriaans, 1992; Adriaans and Ver-
voort, 2002) and GraSp1 (Henrichsen, 2002). Al-
though the means of computation and underlying
aims differ, they all rely to a certain extent on Har-
ris? principle (1951): if two word groups constitute
the same category, then they can be interchanged in
any sentence, without damaging the grammaticality
of that sentence. Hence, these GI system depend on
the inverse: if two word groups appear to occur in
the same contexts, they probably possess the same
syntactic characteristics.
The most prominent example of this principle is
Alignment-Based Learning, or ABL, (van Zaanen,
2002). This algorithm consists of two stages. First,
all sentences are aligned such that it finds a shared
and a distinct part of all pairs of sentences, sug-
gesting that the distinct parts have the same type.
For example, consider the pair ?I saw the man? and
?I saw John?. Here, ?John? and ?the man? are cor-
rectly identified as examples of the same type (NP?s
in this case). The second step, that takes the same
corpus as input, tries to identify the constituents in
that sentence. Because the generated constituents
found in the previous step might overlap, the correct
1As there was no current working version of this system, I
did not include it in this project.
John
(.)
Pat
(.)
Jim
(.)
walks x x
talks x x
smiles x x
Table 1: An example of some context/expression
pairs to show the workings of EMILE. Note that, un-
der standard settings, a rule covering this entire table
will be inferred, causing a phrase like ?John talks? to
be accepted, although there was no such input sen-
tence.
ones have to be selected. Simple heuristics are used
to achieve this, for example to take the constituent
that was generated first (ABL-first) or to take the
constituent with the highest score on some proba-
bilistic function (ABL-leaf). For details, I refer to
van Zaanen (2000). Because ABL compares all sen-
tences in the corpus with all other sentences, the al-
gorithm is quadratic in the number of sentences, but
has low memory demands. Interestingly, ABL does
not come up with an explicit grammar, but generates
just a bracketed version of the corpus instead.
Adios (Solan et al, 2005) uses Harris? principle
as well, although it attempts to create a grammar
(either context-free or context-sensitive) more ex-
plicitly. The algorithm represents language as a di-
rected pseudograph2 , with equivalence classes (ini-
tially single words) as nodes. Input sentences can
be regarded as ?snakes? over the nodes in the graph.
If enough support is found, words are merged into
equivalence classes, or frequently occurring edges
are put in a path (a rule in usual grammatical terms).
This generalisation process is done iteratively, until
convergence is reached.
Emile (Adriaans, 1992; Adriaans and Vervoort,
2002) is the system that to a greater extent tries to
pinpoint its reasons to accept a linguistic hypothe-
sis. Each rule is divided into expressions and types,
where types should be the interchangeable part of
two sentences. Instead of explicitly comparing each
sentence with all other sentences, it incrementally
builds up a table of type/expression pairs, and on the
basis of this table rules are extracted. An example is
given in table 1. This incrementality has two major
2This is a graph that allows for loops and multiple edges.
44
consequences: it makes the system vastly more effi-
cient in terms of time, at the cost of rising memory
demands, and it models time linearly, in contrast to
ABL and Adios.
2.2 Evaluation
Different methods of evaluation are used in GI. One
of them is visual inspection (Henrichsen, 2002).
This is not a reproducible and independent evalua-
tion measure, and it does certainly not suffice as an
assessment of the quality of the results. However,
Roberts and Atwell (2003) argue that this evaluation
should still be included in GI discussions.
A second evaluation method is shown by Solan
et al (2005), in which Adios had to carry out a test
that is available on the Internet: English as a Second
Language (ESL). This test shows three sentences, of
which the examinee has to say which sentence is the
grammatical one. Adios answers around 60% cor-
rect on these questions, which is considered as inter-
mediate for a person who has had 6 years of English
lessons. Although this sounds impressive, no exam-
ples of test sentences are given, and the website is
not available anymore, so we are not able to assess
this result.
A third option is to have sentences generated by
the induced grammar judged on their naturalness,
and compare this average with the average of the
sentences of the original corpus. Solan et al (2005)
showed that the judgments of Adios generated sen-
tences were comparable to the sentences in their cor-
pus. However, the algorithm might just generates
overly simple utterances, and will receive relatively
high scores that it doesn?t deserve.
The last option for evaluation is to compare the
parses with hand-annotated treebanks. This gives
the most quantifiable and detailed view on the per-
formance of a GI system. An interesting compara-
tive study between Emile and ABL using this eval-
uation method is available in van Zaanen and Adri-
aans (2001) where F-scores of 41.4% (Emile) and
61.7% (ABL) are reported on the OVIS (Openbaar
Vervoer Informatie Systeem3; Dutch) corpus, and
25.4% and 39.2% on the ATIS (Air Traffic Informa-
tion System; English) corpus.
3This acronym means Public Transport Information System.
3 Experiment 1
3.1 Motivation
A major choice in evaluating GI systems is to decide
which corpus to train the algorithm on. The cre-
ators of ABL and Emile chose to test on the ATIS
and OVIS corpus, which is, I believe, an unfortu-
nate choice. These corpora contain sentences that
are spoken to a computer, and represent a very lim-
ited subset of language. Deep recursion, one of the
aspects that is hard to catch in grammar induction,
does not occur often. The average sentence lengths
are 7.5 (ATIS) and 4.4 (OVIS). If we want to know
whether a system is truly capable of bootstrapping
knowledge about language, there is only one way to
test it: by using natural language that is unlimited
in its expressive power. Therefore, I will test ABL,
Adios and Emile on the Eindhoven corpus, that con-
tains 7K sentences, with an average length of ap-
proximately 20 tokens. This is, as far as I know, the
first attempt to train and test word-based GI algo-
rithms on such a complicated corpus.
3.2 Method
The Eindhoven corpus has been automatically anno-
tated by Alpino (Bouma et al, 2000; van der Beek
et al, 2002), a wide-coverage hand-written parser
for Dutch, with around 90% dependency triple ac-
curacy. Afterwards, this treebank has been manu-
ally corrected. The treebank does not literally con-
tain trees, but graphs: some nodes can be copied, so
that linguistic structure can be analyzed in more de-
tail. However, by removing all double nodes it is still
possible to retrieve a list of bracket-tuples from these
graphs. The graphs are also non-concatenative,
meaning that a constituent can span word groups that
are not contiguous. Therefore, if a sentence contains
a constituent wi...wjwk...wl, with k ? j > 1, three
bracket-tuples are generated: (i, j), (k, l) and (i, l).
Evaluation of the algorithm is done according to
PARSEVAL, except for a few changes that are also
proposed by Klein and Manning (2002). The set of
bracket-pairs that is found in the Alpino treebank
are called facts, and those from a grammar induc-
tion algorithm predictions. The intersection of the
facts and predictions are called hits. From these we
can compute the unlabeled precision, recall and F-
score. The subtleties adopted from Klein and Man-
45
ning are the following: constituents of length 0 or 1,
constituents that span the whole sentence and con-
stituents just excluding punctuation are not taken
into account, as these are obvious predictions.
Three baselines were created: an algorithm that
always branches left4, idem for right-branching and
an algorithm that performs binary branching on ran-
dom points in the sentence. Note that left-branching
and right-branching yield the maximum number of
predictions.
3.3 Results
From the results in table 2, it can be seen that ABL
scores best: it is the only one that is able to slightly
outperform the random baseline. This is surpris-
ing, because it is the least complicated system of the
three. Adios and Emile performed poorly. It ap-
pears that, with larger sentences, the search space
become too sparse to actually induce any meaning-
ful structure. This is expressed in the low number of
predictions per sentence that Adios (1.5) and Emile
(0.7) make. Adjusting support parameters, to make
the algorithm accept more hypotheses, did not have
the intended effect. Still, notice that Emile has a rel-
atively high precision.
In sum, none of the systems is convincingly able
to outperform the very simple baselines. Neither
did visual inspection give the impression that mean-
ingful information was derived. Therefore, it can
be concluded that current word-based GI algorithms
are not equipped to derive syntactic structure from
corpora as complicated as the Eindhoven corpus.
4 Experiment 2
4.1 Motivation
The second experiment deals with the difference
between tag-based and word-based systems. Intu-
itively, the latter task seems to be more challenging.
Still, Klein and Manning (2002) and Bod (2006)
stick to tag-based models. Their argumentation is
twofold.
First, Bod assumes that unsupervised POS-
tagging can be done successfully, without explic-
itly showing results that can confirm this. Klein
and Manning did tag their text using a simple un-
supervised POS-tagging algorithm, and this mod-
4For example: [ [ [ I saw ] the ] large ] house.
erately harmed their performance: their Context-
Constituent Model?s F-score on Wall Street Journal
text fell from 71.1% to 63.2%.
Second, Klein and Manning created context vec-
tors for a number of non-terminals (NP, VP, PP), and
extracted the two principal components from these
vectors. They did the same with contexts of con-
stituents and distituents. The distribution of these
vectors suggest that the non-terminals were easier
to distinguish from each other than the constituents
from the distituents, suggesting that POS-tagging is
easier than finding syntactic rules. However, this
result would be more convincing if this is true for
POS-tags as well.
4.2 Method
In order to test the argument above, and as an at-
tempt to improve the results from the previous ex-
periment, POS-tags were induced using Biemann?s
unsupervised POS-tagger (Biemann, 2006). Be-
cause that algorithm needs at least 50M words to
work reliably, it was trained on the concatenation of
the Eindhoven corpus and the CLEF corpus (70M
words, also newspaper text). The tags of the Eind-
hoven corpus are then used as input for the GI al-
gorithms, both under same settings as experiment 1.
The evaluation was done the same way as in experi-
ment 1.
The same method was carried out using hand-
corrected tags. Large and equal improvements will
imply the justification for tag-based grammar in-
duction. If the models only improve on the hand-
corrected tags, this will suggest the opposite.
4.3 Results
The results can be found in table 3. Generally, more
predictions were made with respect to experiment 1,
due to the denser search space. Only a convergence
to the baseline was achieved, especially by Adios
and Emile, that were very low in predictions in the
first experiment. Again, none of the tested systems
was able to clearly outperform the baselines.
Because using neither induced nor hand-corrected
made the systems work more reliably, there seems to
be no strong evidence in favor or against Bod?s and
Klein and Manning?s conjecture. Therefore, there is
no sound justification for tag-based grammar induc-
tion yet.
46
Method Hits/Predictions Precision Recall F-score
Left 5.8K / 119K 4.9% 9.2% 6.4%
Right 4.4K / 119K 3.6% 6.9% 4.8%
Random 11K / 93K 11.7% 17.3% 14.0%
ABL-leaf 4.0K / 24K 16.9% 6.4% 9.3%
ABL-first 13K / 113K 11.6% 20.8% 14.9%
Adios 319 / 11K 2.8% 0.5% 0.9%
Emile 912 / 5.2K 17.3% 1.5% 2.7%
Table 2: This table shows the results of experiment 1. Left, Right and Random are baseline scores. The two
variants of ABL differ in the selection phase. 62.9K facts were found in the Alpino treebank.
Induced tags Hand-corrected tags
Method Hits/Pred.?s Precision Recall F-score Hits/Pred.?s Precision Recall F-score
ABL-leaf 5K / 30K 16.8% 8.1% 10.9% 7.0K / 34K 21.0% 11.2% 14.6%
ABL-first 11K / 125K 9.2% 18.2% 12.2% 12.6K / 123K 10.3% 20.0% 13.6%
Adios 2.7K / 24K 11.2% 4.3% 6.3% 2.2K / 20K 11.0% 3.5% 5.3%
Emile 1.8K / 16K 11.2% 2.9% 4.6% 1.7K / 19K 8.9% 2.7% 4.1%
Table 3: This table shows the results of experiment 2. The baseline scores are identical to the ones in
experiment 1.
5 Discussion
The results from experiment 1 and 2 clearly show
that ABL, Adios and Emile have severe shortcom-
ings, and that they cannot derive meaningful struc-
ture from language as complicated as the Eindhoven
corpus. An important reason for this is that a cor-
pus with only 7K sentences is not able to sufficiently
cover the search space. This can be seen from the
very low number of predictions made by Adios and
Emile: there was not enough support to accept hy-
potheses.
But how should we proceed? Any algorithm
based solely on Harris? principle can be either incre-
mental (Emile) or non-incremental (ABL, Adios).
The previous experiments show that very large cor-
pora are needed to mitigate the very sparse search
space, leading me to conclude that non-incremental
systems are not suitable for the problem of gram-
mar induction. Also, incremental systems have the
advantage of an intuitive notion of time: it is al-
ways clear which working hypothesis of a grammar
is maintained.
Emile retains a Boolean table with all combina-
tions of types and expressions it has encountered up
until a given moment. This means that very infre-
quent words demand a disproportionally large part
of the memory. Therefore, all found words and rules
should be divided into three groups: pivotal, nor-
mal and infrequent. Initially, all encountered words
are infrequent. Transitions to the normal and piv-
otal stage occur when an estimator of the relative
frequency is high enough, for example by taking the
lower bound of the confidence interval (Mikheev,
1997). Ultimately, the number of words in the nor-
mal and pivotal stage will converge to a constant.
For example, if the relative frequency of a word
should be larger than 0.01 to become pivotal, there
can only be 100 of these words. Because one can
define upper limits for pivotal and normal words,
the size of the bookkeeping table is limited as well.
Also, when the system starts inducing syntactic cate-
gories of words, very infrequent words should not be
parsed as a separate category initially, but as a mem-
ber of another open-class category. This connects to
the cross-linguistic tendency that infrequent words
generally have simple complementation patterns.
One very important question remains: what in-
tuitions should this imaginary system use to induce
rules? First, all sentences should be sorted by length.
Then, for each sentence, the following steps are
taken:
47
? Update the bookkeeping tables.
? Parse the sentence as deeply as possible.
? If the sentence cannot be parsed completely,
induce all possible rules that would make the
parse complete. Add all these rules to the book-
keeping tables.
The last step deserves some extra attention. If
the algorithm encounters the sentence ?he is such a
(.)?, we can safely infer that the unknown word at
(.) is a noun. Inducing complementation patterns
should be possible as well. Imagine that the algo-
rithm understands NP?s and transitive verbs. Then
consider the following: ?John gave Tim a book?.
It will parse ?John gave Tim? as a sentence, and ?a
book? as a noun phrase. Because these two should
be connected, a number of hypotheses are generated,
for example: ?a book? is a complement of ?Tim?; ?a
book? is a complement of ?John gave Tim?; ?a book?
is a second complement of ?gave?. Naturally, only
the last hypothesis is correct. All three inductions
are included, but only the last is likely to be repro-
duced in later sentences in the corpus, because sen-
tences of the form ?(.) gave (.) (.)? are more likely
than ?John gave Tim (.)? and ?Tim (.)?.
6 Acknowledgments
I would like to thank Jennifer Spenader, Gertjan van
Noord and the anonymous reviewers for providing
me their invaluable comments.
References
Pieter W. Adriaans and Mark R. Vervoort. 2002. The
EMILE 4.1 grammar induction toolbox. In Proceed-
ings of the 6th International Colloquium on Gram-
mar Induction (ICGI), pages 293?295, Amsterdam,
the Netherlands.
Pieter W. Adriaans. 1992. Language learning from a cat-
egorial perspective. Ph.D. thesis, University of Ams-
terdam, NL.
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of ACL/COLING-2006 Students Research Work-
shop, pages 7?12, Sydney, Australia.
Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of ACL/COLING-2006,
pages 865?872, Sydney, Australia.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2000. Alpino: wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands (CLIN), pages 45?59, Tilburg,
the Netherlands.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 16:447?474.
Zellig S. Harris. 1951. Methods in Structural Linguis-
tics. University of Chicago Press, Chicago.
Peter J. Henrichsen. 2002. GraSp: Grammar learning
from unlabelled speech corpora. In Proceedings of
CoNLL-2002, pages 22?28, Pennsylvania, PA, USA.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative Constituent-Context Model for improved gram-
mar induction. In Proceedings of ACL-2001, pages
128?135, Toulouse, France.
Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context model. Pattern Recognition,
9(38):1407?1419.
Shalom Lappin and Stuart M. Shieber. 2007. Machine
learning theory and practice as a source of insight into
universal grammar. Computational Linguistics, 43:1?
34.
Andrei Mikheev. 1997. Automatic rule induction for
unknown-word guessing. Computational Linguistics,
23(3):405?423.
Andrew Roberts and Eric Atwell. 2003. The use of cor-
pora for automatic evaluation of grammar inference
systems. In Proceedings of the Corpus Linguistics
2003 conference, pages 657?661, Lancaster, United
Kingdom.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. Proceedings of the National Academy of Sci-
ences, 102(33):11629?11634.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Proceedings of Computational Lin-
guistics in the Netherlands (CLIN) 2001, pages 8?22,
Enschede, the Netherlands.
Menno van Zaanen and Pieter W. Adriaans. 2001.
Alignment-Based Learning versus EMILE: A compar-
ison. In Proceedings of the 13th Dutch-Belgian Artifi-
cial Intelligence Conference (BNAIC), pages 315?322,
Amsterdam, the Netherlands.
Menno van Zaanen. 2002. Implementing Alignment-
Based Learning. In Proceedings of the 6th Interna-
tional Colloquium on Grammatical Inference (ICGI),
pages 312?314, Amsterdam, the Netherlands.
48
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 223?231,
Beijing, August 2010
Constraining robust constructions for broad-coverage parsing with
precision grammars
Bart Cramer? and Yi Zhang??
Department of Computational Linguistics & Phonetics, Saarland University?
LT-Lab, German Research Center for Artificial Intelligence (DFKI)?
{bcramer,yzhang}@coli.uni-saarland.de
Abstract
This paper addresses two problems that
commonly arise in parsing with precision-
oriented, rule-based models of grammar:
lack of speed and lack of robustness. First,
we show how we can reduce parsing times
by restricting the number of tasks the
parser will carry out, based on a gener-
ative model of rule applications. Sec-
ond, we show that a combination of search
space restriction and radically overgen-
erating robustness rules lead to a more
robust parser, with only a small penalty
in precision. Applying both the robust-
ness rules and a fragment fallback strat-
egy showed better recall than just giving
fragment analyses, with equal precision.
Results are reported on a medium-sized
HPSG grammar for German. 1
1 Introduction
In the field of natural language processing, it
is common wisdom that handwritten, rule-based
models generally perform poorly on complex
problems, mainly due to the knowledge acquisi-
tion bottleneck: it is hard for the human modeller
to conceive of all possible scenarios the model
has to cope with. In parsing, many approaches
have relied on hand-written grammars, and their
fragility is one of their largest weaknesses. Such
models can fail due to insufficiency of lexical en-
tries or grammatical constructions, but also due
1The research reported on in this paper has been carried
out with financial support from the Deutsche Forschungs-
gemeinschaft and the German Excellence Cluster of Multi-
modal Computing & Interaction.
to creative or ungrammatical input. In any case,
the parser should always return a reasonable out-
put. A very simple technique is partial or fragment
parsing (Kiefer et al, 1999; Riezler et al, 2001;
Zhang et al, 2007a): if there is no item in the chart
that both spans the complete sentence and fulfills
the root condition, several chunks that do conform
to a root condition are combined by minimising a
certain cost function (for instance to favour larger
chunks, or more probable chunks).
A second problem with deep parsers is their rel-
atively low efficiency. For online applications, it is
impermissible to wait for longer than a minute be-
fore the system responds. Apart from studies that
were aimed at increasing the efficiency of deep
parsers by using smarter algorithms (e.g. using
left-corner relations (Van Noord, 1997)), several
studies in recent years have suggested that search
space restriction can offer a beneficial balance be-
tween speed and accuracy as well. Techniques
that have been proposed are, among others, su-
pertagging (Clark and Curran, 2007), CFG filter-
ing (Matsuzaki et al, 2007) and beam threshold-
ing (Ninomiya et al, 2005).
A potential disadvantage of the latter technique
is that the unifications have taken place by the
time the value of the resulting chart item is in-
vestigated. One strategy that tries to prevent ex-
ecution of unlikely tasks altogether is presented
by van Noord (2009). In this method, the parser
learns from an unannotated corpus which parse
steps contributed to the solution as preferred by
the disambiguation model (as opposed to a cer-
tain gold standard). Hence, this approach is self-
learning.
Another study that is close to our approach
223
to search space restriction is c-structure pruning
(Cahill et al, 2008). The authors show that a
large, hand-written, unification-based parser (the
XLE LFG parser for English) can perform reason-
ably faster (18%) without losing accuracy, by not
allowing the parser to unify if the resulting item
will have a span that does not conform to a CFG
tree that was generated from the sentence before-
hand by a PCFG parser. Much better results (67%
speed-up) are obtained by pruning chart items lo-
cally, based on their relative probabilities (Cahill
et al, 2008). This is the approach that is closest to
the one we present in this paper.
In this paper, we introduce a method that ad-
dresses robustness and efficiency concurrently.
The search space is restricted by setting a maxi-
mum on the number of tasks per chart cell. Be-
cause tasks are carried out according to a prior-
ity model based on the generative probabilities of
the rule applications, it is unlikely that good read-
ings are dropped. More robustness is achieved by
adding radically overgenerating rules to the gram-
mar, which could cover all sentences, given an dis-
proportionate amount of time and memory. By
strongly restricting the search space, however, the
computation requirements remains within bounds.
Because the robustness rules are strongly dispre-
ferred by both the priority model and the dis-
ambiguation model, all sentences that would be
covered by the ?restricted? grammar remain high-
precision, but sentences that are not covered will
get an additional push from the robustness rules.
1.1 An HPSG grammar for German
The grammar we use (Cramer and Zhang, 2009)
is the combination of a hand-written, constraint-
based grammar in the framework of HPSG and an
open word class lexicon extracted from the Tiger
treebank (Brants et al, 2002) in a deep lexical ac-
quisition step. One of the aims of this grammar
is to be precision-oriented: it tries to give detailed
analyses of the German language, and reject un-
grammatical sentences as much as possible. How-
ever, this precision comes at the cost of lower cov-
erage, as we will see later in this paper.
Along with the grammar, a treebank has been
developed by re-parsing the Tiger treebank, and
including those sentences for which the grammar
was able to reproduce the original Tiger depen-
dencies. The treebank?s size is just over 25k sen-
tences (only selected from the first 45k sentences,
so they don?t overlap with either the development
or test set), and contains the correct HPSG deriva-
tion trees. These (projective) derivation trees will
function as the training set for the statistical mod-
els we develop in this study.
2 Restriction of the search space
2.1 The PET parser
The parser we employ, the PET parser (Callmeier,
2000), is an agenda-driven, bottom-up,
unification-based parser. In order to reduce com-
putational demands, state-of-the-art techniques
such as subsumption-based packing (Oepen
and Carroll, 2000) and the quasi-destructive
unification operator (Tomabechi, 1991) have been
implemented.
A central component in the parser is the agenda,
implemented as a priority queue of parsing tasks
(unifications). Tasks are popped from the agenda,
until no task is left, after which all passive items
spanning the complete sentence are compared
with the root conditions as specified by the gram-
mar writer. The best parse is extracted from the
parse forest by a Maximum Entropy parse disam-
biguation model (Toutanova et al, 2002), using
selective unpacking (Zhang et al, 2007b).
Two different types of items are identified: pas-
sive items and active items. Passive items are
?normal? chart items, in the sense that they can
freely combine with other items. Active items
still need to combine with a passive item to be
complete. Hence, the parser knows two types of
tasks as well (see figure 1): rule+passive and ac-
tive+passive.
Each time a task succeeds, the following hap-
pens:
? For each inserted passive item, add
(rule+passive) tasks that combine the
passive item with each of the rules, and add
(active+passive) tasks that combine with
each of the neighbouring active items.
? For each inserted active item, add (ac-
tive+passive) tasks that combine the remain-
224
unary binary
rule+passive
binary
active+passive
R
+ P ?
R
P
R
+ P ?
R
P
R
P
1
+ P
2
?
R
P
1
P
2
Figure 1: Depicted are the different types of tasks in the PET parser. Not shown are the features
structures imposed by the rules and the chart items.
ing gaps in the active item with existing
neighbouring passive items in the chart.
2.2 Defining priorities
The priorities of the parsing tasks are calculated
based on a generative PCFG model extracted from
the treebank by maximum likelihood estimation,
smoothed by Lidstone smoothing. Each passive
chart item receives a score based on its generative
probability, calculated as the product of all applied
rule probabilities. For active parsing items, we set
the score to be the upper bound of this generative
probability, if the item succeeds later in combin-
ing with other passive edge(s) to build a complete
subtree. This is done by simply assuming the un-
determined subtree in the active item receiving a
generative score of 1.
The priorities that are assigned to both types of
tasks are not yet conditioned on the probability
of the topmost rule application. Hence, they are
computed using the following simple formula:
Pr = p(R) ? p(P )
where Pr is the task?s priority, p(R) the prior
probability of the rule category R; and p(P ) is
the highest possible generative probability of the
resulting passive item P .
2.3 Restriction strategies
It is a natural thought to allocate more computa-
tional resources to longer sentences, and this is
exactly what happens in the restriction strategies
we develop in this study. We define a cap on
the number of tasks for a certain cell/span (i, j),
which means that the number of cells is quadrati-
cally related to the number of words in a sentence:
ncells = n(n + 1)/2.
We define three task restriction strategies: all,
success, and passive. In all, the cap is defined
for all tasks, whether the unification is success-
ful or not. Success only counts tasks that are suc-
cessful (i.e. lead to either an active or a passive
item), and passive only counts tasks that lead to a
passive item. In all strategies, morphological and
lexical tasks are not counted, and hence not re-
stricted. Unary phrasal rules (such as empty-det)
are counted, though.
The implementation uses only one priority
queue. Each time a task is popped from the
agenda, it is checked whether the limit for this
span has been reached or not. If so, the task is
discarded; otherwise, it is executed.
2.4 Methodology
All our experiments are based on the Tiger tree-
bank (Brants et al, 2002). The grammar?s lex-
icon is based on the first 45k sentences in the
treebank, and so are the MaxEnt disambiguation
model (Toutanova et al, 2002) and the genera-
tive model we developed for this study. The de-
velopment set (s45001-s47500) was used to fine-
tune the methods, but all final results presented in
this paper are with respect to the test set (s47501-
s50000). The maximum time for building up the
packed parse forest is 60 seconds, after which un-
packing is started. Unpacking the first reading
usually has negligible computation costs, and is
not reported on. Along with the best reading?s
derivation, the dependencies are output, and com-
225
Strategy exhaustive all success passive
Cap size 3000 200 100
Time (s) 7.20 1.04 0.92 1.06
Coverage 59.4% 60.5% 60.0% 59.0%
Exact 17.6% 17.6% 17.4% 17.4%
Recall 37.6% 39.5% 38.9% 38.0%
Precision 80.7% 80.3% 80.1% 80.4%
F-score 51.3% 52.9% 52.4% 51.6%
Table 1: A more detailed look into some data points from figure 2. ?Coverage? and ?Exact? are sentential
percentages, showing how many sentences receive at least one or the exactly correct reading. Recall,
precision and f-score are on a per-dependency basis.
l
0 2 4 6 8
46
48
50
52
54
Time (s)
F?
sc
or
e
l exhaustive
all
success
passive
Figure 2: This figure shows the tradeoff between
speed and f-score for the standard grammar, using
the restriction strategies with different cap sizes.
pared to the gold standard dependencies from the
Tiger treebank.
2.5 Results
The results of the experiments, with different cap
sizes, are summarized in table 1 and figure 2.
As expected, for all strategies it holds that longer
computation times lead to higher coverage num-
bers. The interesting thing is that the restriction of
the search space doesn?t affect the parses? preci-
sion, indicating that the priorities work well: the
tasks leading to good solutions are indeed given
high priority scores.
A striking observation is that the coverage num-
bers go up by about 1%, with reductions in parse
times of more than 80%. This is due to the use of
the timeout, and the generic tendency of our defi-
nition of the priorities: because less rule applica-
tions lead to higher log probabilities, the agenda
will favour tasks with smaller span size. If the
agenda doesn?t apply too strong a restriction on
those tasks, the parser might not create any items
spanning the whole sentence after the full 60 sec-
onds, and hence produce no parse. This is miti-
gated by stronger restriction, leading to a quicker
path upwards in the chart.
No large differences of success are found be-
tween the different strategies. The intuition be-
hind the success and passive strategies was that
only more effort should be invested into a par-
ticular span if not enough chart items for that
span have been created. However, the time/quality
trade-offs are very similar for all strategies, as
shown in figure 22.
The strategies we have reported on have one
thing in common: their counters are with respect
to one particular span, and therefore, they have
a very local scope. We have tried other strate-
gies that would give the algorithm more flexibil-
ity by defining the caps on more global scale, for
instance per span length or for the entire chart.
However, this degraded the performance severely,
because the parser was not able to divide its atten-
tion properly.
2One might be tempted to consider the all strategy as
the best one. However, the time/f-score tradeoff curves look
slightly different on the development set.
226
3 Increasing robustness
For hand-written deep parsers, efficiency and cov-
erage are often competing factors: allowing more
items to be created might be beneficial for recall,
but the parser will also be too slow. However, be-
cause the search space can be restricted so rigidly,
we can make the grammar more permissive to ac-
cept more sentences, hopefully without a heavy
efficiency penalty. One way to do this is to re-
move constraints from the grammar rules. How-
ever, that would infringe on the precision-oriented
nature of the grammar. Instead, we will keep the
normal grammar rules as they are, and create a
small number of additional, super-accepting ro-
bustness rules. The intuition is that when the re-
stricted part of the grammar can find a solution,
that solution will indeed be found, and preferred
by the statistical models. On the other hand, when
the sentence is extragrammatical, the robustness
rules may be able to overcome the barriers.
Let?s consider the following example, assuming
that the grammar only lists ?to run? as an intransi-
tive verb:
?John ran the marathon yesterday?
A fragment approach would come up with the
following solution:
John ran the marathon yesterday
subj-h
?John? will correctly be identified as the subject
of ?ran?, but that is all. No dependencies are estab-
lished between ?the marathon? and ?ran?, or ?yes-
terday? and ?ran?. The former is hard to establish,
because of the missing lexical item. However, the
latter should be doable: the lexicon knows that
?yesterday? is an adverb that modifies verbs. If
we could create a robustness rule that would ab-
sorb the object (?the marathon?) without assigning
a dependency, it would at least be able to identify
the modifier dependency between ?ran? and ?yes-
terday?.
John
ran the marathon
yesterdaym-robust
h-adjunct
subj-h
In other words, a fragment analysis solely com-
bines items at the top level, whereas a robust
parser would ideally be able to overcome barri-
ers in both the lower and the higher regions of the
chart, meaning that the damage can be localised
and thus minimised. The robustness rules we pro-
pose are intended to achieve that.
How does this idea interact with the restriction
mechanism explained in the previous section? Ro-
bustness rules get an inhibitively large, constant
penalty in both the priority model and the dis-
ambiguation model. That means that at first the
parser will try to build the parse forest with the re-
stricted set of rules, because tasks involving sub-
trees with only rules from the standard grammar
will always have a higher priority than tasks us-
ing an item with a robustness rule application in
its subtree. When this is finished, the robustness
rules try to fill the gaps. Especially in the suc-
cess and passive strategies, tasks with robustness
rules are discarded if already enough chart items
are found for a particular span, meaning that the
parser automatically focusses on those parts of the
chart that haven?t been filled before.
3.1 Defining robustness rules
Defining robustness rules is a sort of grammar
engineering, and it took a bit of experimentation
to find rules that worked well. One of the fac-
tors was the interaction between the subsumption-
based packing and the robustness rules. When the
chart is built up, items that are subsumed by an ex-
isting item are marked as ?frozen?, and the latter
(more general) item functions as the representa-
tive node in the remainder of the parsing process.
When unpacking the best solution, the best deriva-
tion tree is extracted from the packed forest, which
227
might include a frozen node. Because this frozen
node has more constraints than its representative,
this derivation tree is not guaranteed to be free of
unification failures, and hence, before outputting,
this is checked by replaying all the unifications in
the derivation tree. This procedure is repeated un-
til a sound derivation has been found.
So what happens when the representative nodes
are very general? Many nodes will be packed,
and hence the chart will remain compact. How-
ever, the unpacking process will become prob-
lematic, because many of the proposed derivation
trees during unpacking will be incorrect, leading
to excessive computation times (in the order of
minutes).
Therefore, we chose to define robustness rules
such, that the resulting chart items will be equally
constrained as their daughters. They are all bi-
nary, and have one common ancestor in the type
hierarchy:
?
?????????????
structure-robust
SYNSEM 1
ROBUST +
MN-DTR
?
?
sign
SYNSEM 1
[
LOCAL.CAT.HEAD verb
]
ROBUST -
?
?
RB-DTR
?
?
sign
SYNSEM
[
NONLOCAL no-nonlocal
]
ROBUST -
?
?
?
?????????????
All rules have a main daughter and a robust
daughter. The co-indexation of the SYNSEM of
the main daughter and the SYNSEM of the rule
itself has the effect that the resulting chart item
will have the exact same syntactic properties as its
main daughter, whereas the robust daughter does
not contribute to the syntactic properties of the
mother node. The ROBUST feature is used to
prevent the application of two robust rules con-
secutively. Additional constraints (not shown)
make sure that morphological processing is fin-
ished, and that both parts are not involved in a
coordination. Robustness rules do not yield a de-
pendency triple (although they mght be guessed
accurately by a few heuristics).
We define two pairs of robustness rules, each
pair consisting of a rule with MN-DTR first and
RB-DTR second, and one rule in the other order:
+V The robust daughter is a verb, which is still
allowed to have valence, but cannot have any
features in NONLOCAL.
+NV The robust daughter is anything but a verb,
cannot have any non-empty valence list, and
cannot have any features in NONLOCAL.
3.2 Fragment parsing
As a baseline for comparison, we investigate the
existing partial parsing algorithms that pick frag-
mented analyses from the parse forest as a fall-
back strategy when there is no full parse available.
Kiefer et al (1999) took a shortest-path approach
to find a sequence of fragment analysis that min-
imizes a heuristics-based cost function. Another
variation of the algorithm (Riezler et al, 2001)
is to pick fewest chunks that connect the entire
sentence. While these early approaches are based
on simple heuristics, more sophisticated parse se-
lection methods also use the statistical models to
rank the partial analyses. For example, Zhang et
al. (2007a) proposed several ways of integrating
discriminative parse ranking scores with the par-
tial parse selection algorithm.
In this experiment, we first use the shortest
path algorithm to find candidate chunks of par-
tial analysis. All phrasal constituents were given
equal weights, and preferred over input and lex-
ical edges. For each chunk (edges spanning the
same sub-string of the input sentence), the edge
with the highest generative probability is picked.
Consequently, the best partial reading (covering
that edge) is decoded by the selective unpacking
algorithm using the MaxEnt parse ranking model.
With each fragment, the partial semantic represen-
tations were extracted. Similar to the robustness
rules, no cross-fragment dependencies are recov-
ered in this approach. Due to the limited number
of chart items and the use of selective unpacking,
the computation times for the shortest-path algo-
rithm are marginal.
3.3 Results
The results of this experiment are listed in ta-
ble 2. For the robust versions of the grammar,
no exhaustive parsing results are reported, be-
cause they take too long to compute, as can be
expected. Coverage number are on a per-sentence
228
standard +V +NV +V+NV
exhaustive restricted restricted
time (s) 7.20 0.92 4.10 1.42 4.09
no fragment coverage 59.3% 60.0% 72.6% 69.9% 78.6%
recall 37.6% 38.9% 48.4% 47.0% 53.8%
precision 80.7% 80.1% 78.6% 78.2% 77.7%
f-score 51.3% 52.4% 59.9% 58.7% 63.6%
fragment coverage 94.3% 98.3% 98.5% 98.7% 98.5%
recall 50.4% 53.6% 59.5% 56.9% 61.3%
precision 75.4% 75.0% 75.0% 74.5% 74.7%
f-score 60.4% 62.5% 66.3% 64.5% 67.3%
Table 2: Results for experiments with different robustness rules, and with or without fragment fallback
strategy.
basis, whereas the other percentages are on a per-
dependency basis. Time denotes the average num-
ber of seconds it takes to build the parse forest. All
results under ?restricted? are carried out with the
success strategy, with a cap of 200 tasks (success-
200). ?(No) fragment? indicates whether a frag-
ment parse is returned when no results are ob-
tained after selective unpacking.
The robustness rules significantly increase the
sentential coverage, in the case of +V+NV almost
20 percent points. The gains of +V and +NV
are fairly additive: they seem to cover different
sets of extragrammatical sentences. In the most
permissive setting (+V+NV), dependency recall
goes up by 16 percent point, with only a 3 per-
cent point decrease of precision, showing that the
newly-covered sentences still receive fairly accu-
rate parses. Also, it can be seen that the +V pair of
rules is more effective than +NV to increase cov-
erage. The robust grammars are certainly slower
than the standard grammar, but still twice as fast
as the standard grammar in an exhaustive setting.
Coverage numbers are approximating 100%
when the fragment parsing fallback strategy is ap-
plied, in all settings. However, it is interesting
to see that the recall numbers are higher when
the robustness rules are more permissive, but that
no significant effect on the precision is observed.
This suggests that the lumps that are connected by
the fragment parsing mechanism are larger, due
to previous applications of the robustness rules.
From this, we conclude that the connections made
by the robustness rules are of relatively high qual-
ity.
We have also tried the all-3000 and passive-
100 settings (the same as listed in table 1). That
yielded very similar results, except on the gram-
mar with both +V and +NV enabled. With pas-
sive-100, there was a small decrease in cover-
age (76.0%), but this drop was much more pro-
nounced for all-3000: 72.0%. This suggests that,
if the pressure on the generative model is larger
due to heavier overgeneration, counting success-
ful tasks or passive items performs better than just
counting the number of executed tasks.
After manual inspection, we found out that the
kind of constructions the robustness rules created
were very diverse. Most of the rule applications
were not in the top of the tree, as was intended.
There also seemed to be a correlation between the
length of the robust daughter and the quality of the
parse. When the robust daughter of the rule was
large, the application of the robustness rule looked
like an emergency strategy, with a corresponding
quality of the parse. However, when the robust-
ness rule connects a verb to a relatively small con-
stituent (a particle or an NP, for example), the re-
sulting derivation tree was of reasonable quality,
keeping most of the other dependencies intact.
4 Discussion
Achieving broad coverage in deep parsing while
maintaining high precision is difficult. Until now,
most existing hand-written grammar-based pars-
ing systems rely on fragment analyses (or various
ways of putting fragments together to compose
229
partial readings), but we argued (with the exam-
ple in section 3) that such an approach delivers in-
ferior results when the tree falls apart at the very
bottom. The use of robust constructions offers a
way to keep the damage local, but can create an
intractable search space. The proposed pruning
strategies carefully control the bound of overgen-
eration, resulting in improvements on both pars-
ing efficiency and coverage, with a significantly
smaller degradation in f-score than a pure frag-
ment approach. The combination of grammar en-
gineering, statistical modelling and algorithmic
design in the parser brings the parser performance
to a new level.
Although the experiments were carried out on
a specific grammar framework, we consider the
techniques put forward in this paper to be applica-
ble to other linguistic frameworks. The robustness
rules are easy to construct (with the precautions
from section 3.1 in mind), and all modern deep
parsers have a treebank to their disposal, from
which the generative model can be learned.
There are still points that can be improved on.
Currently, there is no way to determine which of
the robust rule applications are more promising
than others, and the decision to try one before the
other is solely based on the the probabilities of the
passive items, and not on the generative model.
This can be inefficient: for instance, all robustness
rules presented in this paper (both +V and +NV)
requires the main daughter to be a verb. It would
be straightforward to learn from a small treebank
that trying to unify the main daughter of a robust-
ness rules (which should have a verbal head) with
a specifier-head rule application does not have a
high chance on succeeding.
Another possible improvement is to differenti-
ate between different robustness rules. We pre-
sented a two-tier system here, but the framework
lends itself naturally to more layers with differing
degrees of specificity, creating a smoother scale
from specific/prioritised to robust/non-prioritised.
References
Brants, S., S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In Pro-
ceedings of the Workshop on Treebanks and Lin-
guistic Theories, pages 24?41.
Cahill, A., J.T. Maxwell III, P. Meurer, C. Rohrer, and
V. Rose?n. 2008. Speeding up LFG parsing using
c-structure pruning. In Proceedings of the Work-
shop on Grammar Engineering Across Frameworks,
pages 33?40. Association for Computational Lin-
guistics.
Callmeier, U. 2000. PET?a platform for experimen-
tation with efficient HPSG processing techniques.
Natural Language Engineering, 6(01):99?107.
Clark, S. and J.R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?
552.
Cramer, B. and Y. Zhang. 2009. Construction of
a German HPSG grammar from a detailed tree-
bank. In Proceedings of the GEAF workshop ACL-
IJCNLP 2009, pages 37?45.
Kiefer, B., H.U. Krieger, J. Carroll, and R. Malouf.
1999. A bag of useful techniques for efficient and
robust parsing. In Proceedings of the 37th annual
meeting of the Association for Computational Lin-
guistics on Computational Linguistics, pages 473?
480. Association for Computational Linguistics.
Matsuzaki, T., Y. Miyao, and J. Tsujii. 2007. Ef-
ficient HPSG parsing with supertagging and CFG-
filtering. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI
2007), pages 1671?1676, Hyderabad, India.
Ninomiya, T., Y. Tsuruoka, Y. Miyao, and J. Tsujii.
2005. Efficacy of beam thresholding, unification
filtering and hybrid parsing in probabilistic HPSG
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 103?114.
Association for Computational Linguistics.
Oepen, S. and J. Carroll. 2000. Ambiguity packing in
constraint-based parsing: practical results. In Pro-
ceedings of the first conference on North American
chapter of the Association for Computational Lin-
guistics, pages 162?169. Morgan Kaufmann Pub-
lishers Inc. San Francisco, CA, USA.
Riezler, S., T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell III, and M. Johnson. 2001. Parsing
the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 271?
278.
Tomabechi, H. 1991. Quasi-destructive graph unifi-
cation. In Proceedings of the 29th annual meet-
ing on Association for Computational Linguistics,
pages 315?322. Association for Computational Lin-
guistics.
230
Toutanova, K., C.D. Manning, S. Shieber,
D. Flickinger, and S. Oepen. 2002. Parse
disambiguation for a rich HPSG grammar. In
Proceedings of the First Workshop on Treebanks
and Linguistic Theories, pages 253?263.
Van Noord, G. 1997. An efficient implementation of
the head-corner parser. Computational Linguistics,
23(3):425?456.
van Noord, G. 2009. Learning efficient parsing. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 817?825,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Zhang, Y., V. Kordoni, and E. Fitzgerald. 2007a. Par-
tial parse selection for robust deep processing. In
Proceedings of ACL 2007 Workshop on Deep Lin-
guistic Processing, pages 128?135, Prague, Czech.
Zhang, Y., S. Oepen, and J. Carroll. 2007b. Effi-
ciency in Unification-Based N-Best Parsing. In Pro-
ceedings of the Tenth International Conference on
Parsing Technologies, pages 48?59. Association for
Computational Linguistics.
231
