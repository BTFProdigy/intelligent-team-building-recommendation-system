Proceedings of the Workshop on BioNLP, pages 162?170,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Bridging the Gap between Domain-Oriented and
Linguistically-Oriented Semantics
Sumire Uematsu Jin-Dong Kim Jun?ich Tsujii
Department of Computer Science
Graduate School of Information Science and Technology
University of Tokyo
7-3-1 Hongo Bunkyo-ku Tokyo 113-0033 Japan
{uematsu,jdkim,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper compares domain-oriented and
linguistically-oriented semantics, based on the
GENIA event corpus and FrameNet. While
the domain-oriented semantic structures are
direct targets of Text Mining (TM), their ex-
traction from text is not straghtforward due
to the diversity of linguistic expressions. The
extraction of linguistically-oriented semactics
is more straghtforward, and has been studied
independentely of specific domains. In or-
der to find a use of the domain-independent
research achievements for TM, we aim at
linking classes of the two types of seman-
tics. The classes were connected by analyz-
ing linguistically-oriented semantics of the ex-
pressions that mention one biological class.
With the obtained relationship between the
classes, we discuss a link between TM and
linguistically-oriented semantics.
1 Introduction
This paper compares the linguistically-oriented and
domain-oriented semantics of the GENIA event cor-
pus, and suggests a factor for utilizing NLP tech-
niques for Text Mining (TM) in the bio-medical do-
main.
The increasing number of scientific articles in the
bio-medical domain has contributed in drawing con-
siderable attention to NLP-based TM. An impor-
tant step in NLP-based TM is obtaining the domain-
oriented semantics of sentences, as shown at the bot-
tom of figure 1. The BioInfer (Pyysalo et al, 2007)
and the GENIA event corpus (Kim et al, 2008) pro-
vide annotations of such semantic structures on col-
lections of bio-medical articles. Domain-oriented
semantic structures are valuable assets because their
representation suits information needs in the do-
main; however, the extraction of such structures is
difficult due to the large gap between the text and
these structures.
On the other hand, the extraction of linguistically-
oriented semantics from text has long been studied
in computational linguistics, and has recently been
formalized as Semantic Role Labeling (Gildea and
Jurafsky, 2002), and semantic structure extraction
(Baker et al, 2007)(Surdeanu et al, 2008). Seman-
tic structures in such tasks are exemplified in the
middle of figure 1. The linguistically-oriented se-
mantic structures are easier to extract, although the
information is not practical to the domain.
We aim at relating linguistically-oriented frames
of semantics with domain-oriented classes, thus
making a step forward in utilizing the computa-
tional linguistic resources for the bio-medical TM.
Of all the differences in the two type of seman-
tics, we focused on the fact that the former frames
are more sensitive to the perspective imposed by
the sentence writer. In the right hand-side exam-
ple of figure 1, the linguistically-oriented structure
treats PBMC, a cell entity, as an agent; however the
bio-medical structure reflects the scientific view that
there are no agents, objects acting with intention, in
bio-molecular phenomena.
As a preliminary investigation, we selected
four representative classes of bio-molecular phe-
nomena; Localization, Binding, Cell adhesion,
and Gene expression, and investigated domain-
oriented annotations for the classes in the GENIA
162
?, whereas in many other cell types, NF-kappa B TRANSLOCATES from cytosol to nucleus as a result of ?
?, both C3a and C3a(desArg) were found to enhance IL-6 RELEASE by PBMC in a dose-dependent manner.
Natural?language
FrameNet?expression?(Linguis?ally?oriented?seman?s) 
Class:?????Mo?n?Theme:?NF?kappa?B?Source:?from?cytosol?Goal:?????to?nucleus?
Class:????Releasing?Theme:?IL?6?Agent:???PBMC?
GENIA?expression?(Biologically?oriented?seman?s) 
Class:???????Localiza?n?Theme:????NF?kappa?B?FromLoc:?cytosol?ToLoc:??????nucleus?
Theme:????IL?6?FromLoc:?(inside?of)?PMBC?ToLoc:??????(outside?of)?PMBC?
Figure 1: A comparison of the linguistically-oriented and biologically-
oriented structure of semantics
event corpus. Expressions mentioning the four
classes were examined and manually classified into
linguistically-oriented frames, represented by those
defined in FrameNet (Baker et al, 1998). FN frames
associated to a bio-molecular event class constitute a
list of possible perspectives in mentioning phenom-
ena of the class.
The rest of this paper is structured in the fol-
lowing way: Section 2 reviews the existing work
on semantic structures and expression varieties in
the bio-medical domain, and provides a compari-
son to our work. In section 3, we describe the GE-
NIA event corpus, and the FrameNet frames used as
linguistically-oriented classes in our investigation.
Sections 4 and 5 explain the methods and results of
the corpus investigation; in particular the sections in-
vestigate how the linguistic frames were associated
to the domain-oriented classes of semantics. Finally,
we provide discussion and conclusion in section 6
and 7.
2 Related Work
Existing work on semantics approached domain-
oriented semantic structures from linguistically-
oriented semantics. In contrast, our approach uses
domain-oriented semantics to find the linguistic se-
mantics that represent them. We believe that the two
different approaches could complement each other.
The PASbio(Wattarujeekrit et al, 2004) pro-
poses Predicate Argument Structures (PASs), a type
of linguistically-oriented semantic structures, for
domain-specific lexical items, based on PASs de-
fined in PropBank(Wattarujeekrit et al, 2004) and
NomBank(Meyers et al, 2004). The PASs are de-
fined per lexical item, and is therefore distinct from a
biologically-oriented representation of events. (Co-
hen et al, 2008) investigated syntactic alternations
of verbs and their nominalized forms which oc-
curred in the PennBioIE corpus(Kulick et al, 2004),
whilst keeping PASs of the PASBio in their minds.
The BioFrameNet(Dolbey et al, 2006) is an at-
tempt to extend the FrameNet with specific frames
to the bio-medical domain, and to apply the frames
to corpus annotation. Our attempts were similar, in
that both were: 1) utilizing the FN frames or their
extensions to classify mentions of biological events,
and 2) relating the frames and the FEs (roles of par-
ticipants) with classes in domain ontologies; e.g. the
Gene Ontology(Ashburner et al, 2000).
As far as the authors know, it is the first at-
tempt to explicitly address the problem of linking
linguistically-oriented and domain-oriented frames
of semantics. However, it has been indirectly stud-
ied through works on TM or Relation Extraction
using linguistically-oriented semantic structures as
features, such as in the case with (Harabagiu et al,
2005).
3 Corpora
?We used domain-oriented annotations of the GE-
NIA event corpus and linguistically-oriented frames
defined in FrameNet (FN), to link domain-oriented
and linguistically-oriented frames of semantics. We
briefly describe these resources next.
163
Mo?n 
Releasing
Ge?g 
A?ching 
Being_located
Becoming
Event StateGENIA?event
Biological_process
Viral_life_cycle
Cellular_process
Physiological_process
Cell_adhesion
Cell_communica?n 
Localiza?n 
Binding
Metabolism
DNA_metabolism
Gene_expression
Crea?g 
Being_a?ched 
Figure 2: The resulting relationship between linguistically-oriented and
biologically-oriented frames.
The GENIA event corpus consists of 1,000 Med-
line abstracts; that is, 9,372 sentences annotated
with domain-oriented semantic structures. The an-
notation was completed for all mentions of biolog-
ical events, and resulted in 6,114 identified events.
Examples of annotated event structures are shown at
the bottom of figure 1. Each structure has attributes
type and themes, which respectively show the bio-
logical class of the mentioned event and phrases ex-
pressing the event participants. The event classes are
defined based on the terms in the Gene Ontology.
For example, the Localization class in the GENIA
event corpus is defined as an equivalent of the GO
term Localization (GO0051179). The event classi-
fications used in the corpus are depicted in the left
hand-side of figure 2. Arrows in the figure depict
the inheritance relations defined in the GENIA event
ontology. For instance, the Localization class is de-
fined as a type of Physiological process. Each of
the annotated structures has additional attributes that
point phrases that the annotator of the structure used
as a clue. Among the attributes, the clueType at-
tribute shows a clue phrase to the event class. In our
investigation, the attribute was treated as a predicate,
or an equivalent of the lexical unit in the FN.
FN is a network of frames that are are
linguistically-oriented classifications of semantics.
A FN frame is defined as ?a script-like conceptual
structure that describes a particular type of situation,
object, or event and the participants and proposi-
tions involved in it,? and is associated with words,
or lexical units, evoking the frame. For instance, the
verbs move, go and fly are lexical units of the Mo-
tion frame, and they share the same semantic struc-
ture. Each FN frame has annotation examples form-
ing an attestation of semantic overlap between the
lexical units. Additionally, FN defines several types
of frame-frame relations; e.g. inheritance, prece-
dence, subframe, etc. The right hand-side of figure
2 shows some FN frames and inheritance relation-
ships between them. The FN provides linguistically-
oriented classifications of event mentions based on
surface expressions, and also shows abstract rela-
tions between the frames.
4 Additional Annotation
Our aim is to link linguistically-oriented and
domain-oriented frames of the bio-medical text?s se-
mantics. A major problem in this task was that there
were no annotated corpora with both types of se-
mantic structures. Therefore, we decided to concen-
trate on the mentions of a few classes of biological
phenomena, and to annotate samples of the mentions
with linguistically-oriented structures conforming to
164
Freq. Keyword Frame
693 binding Attaching
247 bind Attaching
125 interaction Attaching, Being attached
120 complex ?
99 bound Attaching, Being attached
91 interact Attaching, Being attached
61 form Becoming
52 crosslink Attaching
46 formation Becoming
Table 1: The most frequent keywords of the Binding class,
mentioned 2,006 times in total.
Freq. Keyword Frame
131 translocation Motion
81 secretion Releasing
75 release Releasing
32 secrete Releasing
25 mobilization Motion
23 localization Being located
20 uptake Getting
18 translocate Motion
15 expression Creating
9 present Being located
Table 2: The most frequent keywords of the Localization
class, mentioned 582 times in total.
the FrameNet annotations.
The following provides the annotation proce-
dures. First, we collected linguistic expressions that
mention each of the selected GENIA event classes
from the GENIA event corpus. We then sampled
and annotated them with their linguistically-oriented
semantics which conformed to the FrameNet.
4.1 Target Classes and Keywords
We concentrated mainly on the mentions of four GE-
NIA classes; Localization, Binding, Cell adhesion,
and Gene expression. Gene expression, Binding,
and Localization are three of the most frequent four
classes in the GENIA event corpus.1 Binding and
Localization are the two most primitive molecular
events. The Cell adhesion class was included as a
comparison for the Binding class.
Counting keywords for mentioning events was
close to automatic. We extracted phrases pointed
by a clueType attribute from each event structure.
We then tokenized the phrases, performed a simple
stemming on the tokens, and counted the resulting
words. The stemming process simply replaced each
inflected word to its stem by consulting a small list
of inflected words with their stems. Manual work
was only used in making the small list.
4.2 FN Annotation
A major challenge encountered in annotating a sam-
pled expression with a semantic structure conform-
ing to FN, was in the assignment of a FN frame to
1Except correlation and regulation classes which express re-
lational information rather than events.
the mention. Our decision was based on the follow-
ing four points: 1) keywords used in the mention, 2)
description of FN frames, 3) syntactic positions of
the event participants, and 4) frame-frame relations.
The first indicates that a FN frame became a can-
didate frame for the mention, if the keyword in the
mention is a lexical unit of the FN frame. FN frames
and their lexical units could be easily checked by
consulting the FN dictionary. If there were no en-
tries for the keyword in the dictionary, synonyms or
words in the keyword?s definition were used. For ex-
ample, the verb translocate has no entries in the FN
dictionary, and the frames for verbs such as move
were used instead.
For the second point, we discarded FN frames that
are either evoked by a completely different sense of
the keyword, or too specific of a non-biological sit-
uations.
Before we assigned a FN frame to each mention,
we manually examined the syntactic positions of all
event participants present in the sampled GENIA
mentions. Combinations of the syntactic position
and event participants observed for a keyword were
compared with sample annotations of the candidate
FN frames.
We checked frame-frame relations between the
candidate frames, because they can be regarded
as evidence that shows that the conception of the
frames is related. For our aim, it was sufficient to
choose a set of frames that best describes the differ-
ent perspectives for mentioning one type of molecu-
lar phenomena. Even when some keywords seemed
to be dissimilar in the three points mentioned above,
165
Freq. Keyword Frame
98 adhesion Being attached
19 adherence Being attached
16 interaction Being attached, Attaching
15 binding Attaching
8 adherent Being attached
Table 3: The most frequent keywords of the Cell adhesion
class, mentioned 193 times in total.
Freq. Keyword Frame
1513 expression Creating
357 express Creating
239 production Creating
71 overexpression Creating
69 produce Creating
62 synthesis Creating
Table 4: The most frequent keywords of the
Gene expression class, mentioned 2,769 times in
total.
a single frame could be assigned to them if it was
quite clear that they shared a similar perspective.
The frame-frame relations provided in the FN were
treated as clues to the similarity.
Keywords frequently used in each event class are
listed in tables 1, 2, 3, and 4, with the final assign-
ment of FN frames to each keyword.
5 Analysis
After the linguistic annotation was performed, we
compared the GENIA event structure and the frame
structure of each sampled expression, and obtained
relations of the GENIA class-FN frame and GE-
NIA slot-FN participant. The resulting relationships
between FN frames and the four GENIA classes
demonstrate a gap between linguistically-oriented
and domain-oriented classification of events, as
shown in figure 2.
The relations can be explained by decomposing it
into two cases: 1) 1-to-n mappings, and 2) n-to-1
mappings. The n-to-n mapping from GENIA to FN
can then be regarded as a mix of the two cases. In
the following sections, the two cases are described
in detail. Further, we show conversion examples of
a FN structure to a GENIA event structure, which
were supported by the obtained GENIA participant-
FN participant relations.
5.1 1-to-N Mapping: Different Perspectives on
the Same Phenomena
A 1-to-n mapping from GENIA to FN can be ex-
plained as the case where the same molecular phe-
nomena are expressed from different perspectives.
5.1.1 Binding Expressed in Multiple frames
The Binding class in GENIA is defined as
?the selective, often stoichiometric interaction of a
molecule with one or more specific sites on an-
other molecule.? We associated the class with three
frames, and two frames of the three, Attaching and
Becoming frames, represent different perspectives
for mentioning the class. The Being attached frame
shares the same conception as Attaching, but ex-
presses states instead of events. See table 1 for key-
words of the class, and the frames assigned to the
words.
Attaching: In the perspective represented by this
frame, a binding phenomenon was recognized as a
event in which protein molecules were simply at-
tached to one another.
[The 3?-CAGGTG E-boxItem] could BIND
[USF proteinsGoal], ? ? ?
(PubMed ID 10037751, Event IDs E11, E12, E13)
Becoming: In the perspective represented by this
frame, a product of a binding event was treated, on
the surface, as a different entity from the original
parts.
When activated, [glucocorticoid recep-
torsEntity] FORM [a dimerFinal category] ? ? ?
(PubMed ID 10191934, Event ID E5)
This type of expression was possible because a prod-
uct of a binding often obtains a different function-
ality, and can be treated as a different type of en-
tity. Note that this frame was not associated with the
Cell adhesion class described in section 5.2.
166
A CB?
Figure 3: A schematic figure of translocation.
Being attached: Annotators recognized a protein
binding event from the sentence below, which basi-
cally mentions a state of the NF-kB.
In T cells and T cell lines, [NF-kBItem]
is BOUND [to a cytoplasmic proteic in-
hibitor, the IkBGoal].
(PubMed ID 1958222, Event ID E2, E102)
Although this type of expression shares a similar
point of view with the Attaching frame, we classi-
fied these expressions into the Being attached frame
in order to demonstrate cases in which a prerequisite
Binding event was inferred from a state.
5.1.2 Translocation Expressed in Multiple
Frames
The Localization class in the GENIA corpus is de-
fined as a class for ?any process by which a cell, a
substance, or a cellular entity, such as a protein com-
plex or organelle, is transported to, and/or main-
tained in a specific location.? Sampled expressions
of the class separated into mentions of a process, by
which an entity was transported to a specific loca-
tion, and those of the process in which an entity was
maintained in a specific location. We concentrate on
the former in this section, and describe the latter in
section 5.1.3.
We associated the frames: Motion, Releasing and
Getting with what we call translocation events, or
Localization events in which an entity was trans-
ported to a specific location. Figure 3 provides a
schematic representation of a translocation event.
Each of the three frames had a different perspective
in expressing the translocations. See table 2 for key-
words of the frames.
Motion: This group consists of expressions cen-
tered on the translocated entities of the translocation
- namely, B in the figure 3.
[NK cell NFATTheme] ? ? ? MIGRATES [to
the nucleusGoal] upon stimulation,? ? ?
(PubMed ID 7650486, Event ID E33)
Activation of T lymphocytes ? ? ? results
in TRANSLOCATION [of the transcrip-
tion factors NF-kappa B, AP-1, NFAT, and
STATTheme] [from the cytoplasmSource] [into
the nucleusGoal].
(PubMed ID 9834092, Event ID E67)
These expressions are similar to those of the Motion
frame in the FN.
[Her footTheme] MOVED [from the
brakeSource] [to the acceleratorGoal] and the
car glided forward.
Releasing: This group consists of expressions
centered on a starting point of the translocation -
namely, A in the figure 3.
In [unstimulated cells whichAgent] do not
SECRETE [IL-2Theme], only Sp1 binds to
this region, ? ? ?
(PubMed ID 7673240, Event ID E13)
Activation of NF-kappaB is thought to
be required for [cytokineTheme] RELEASE
[from LPS-responsive cellsAgent], ? ? ?
(PubMed ID 1007564, Event ID E14)
The verbal keywords occurred as a transitive in
most cases, and had subjects and objects that ex-
pressed starting points and entities in the transloca-
tions. This is a typical syntactic pattern of the Re-
leasing frame, if we regarded an Agent in the FN as
a starting point of the movement of a Theme.
[The policeAgent] RELEASED [the sus-
pectTheme].
Getting: This group consists of expressions cen-
tered on a goal point of the translocation - namely,
C in figure 3. We assumed that this group has an
opposite point of view from the Releasing frame.
The noun uptake was found to be a keyword in this
group.
The integral membrane ? ? ? appears to play
a physiological role in binding and UP-
TAKE [of Ox LDLTheme] [by monocyte-
macrophagesRecipient], ? ? ?
(PubMed ID 9285527, Event ID E10)
167
To summarize, we observed three groups of ex-
pressions that mention translocation events, and
each group represented different perspectives to
mention the events. Each of the groups and the as-
sociated frame seemed similar, in that they shared
similar keywords and possible syntactic positions to
express the event participant.
5.1.3 Localization excluding Translocation
Expressed in Multiple Frames
Localization events excluding translocations were
expressed in the Being located and Creating frames.
Being located: This group consists of expressions
that simply mention an entity in a specific location.
? ? ? [recombinant NFAT1Theme] LOCAL-
IZES [in the cytoplasm of transiently
transfected T cellsLocation] ? ? ?
(PubMed ID 8668213, Event ID E23)
Creating: A noun expression was observed to be
used by instances mentioning the presence of pro-
teins.
horbol esters are required to induce
[AIM/CD69Created entity] Cell-surface EX-
PRESSION as well as ? ? ?
(PubMed ID 1545132, Event ID E12)
Expressions in these cases indicate an abbrevi-
ation for gene expression, which is a event of
Gene expression class. This type of overlap be-
tween the Localization and Gene expression is ex-
plained in section 5.2.2
5.2 N-to-1 Mapping: Same Conception for
Different Molecular Phenomenon
In contrast to the cases described in section 5.1, the
same conception could be applied to different bio-
logical phenomena.
5.2.1 Shared Conception for Binding and
Cell adhesion
Molecular events classified into Binding and
Cell adhesion shared the conception that two enti-
ties were attached to each other. However, types of
the entities involved are different. They are: the pro-
tein molecule in Binding, and cell in Cell adhesion.
CD36 is a cell surface glycoprotein
? ? ?, which INTERACTS with throm-
bospondin, ? ? ?, and erythrocytes para-
sitized with Plasmodium falciparum.
In the sentence above, an event involving a cell sur-
face glycoprotein and thrombospondin was recog-
nized as a Binding, whereas an event involving a cell
surface glycoprotein and erythrocytes was classified
as a Cell adhesion event.
5.2.2 Shared Expressions of Localization and
Gene expression
Both Localization and Gene expression classes
are connected with the Creating frame. Some
Localization events have a dependency on the
Gene expression event. Protein molecules are made
in events classified into the Gene expression class.
[Th1 cellsCreator] PRODUCE [IL-2 and
IFN-gammaCreated entity], ? ? ?
(PubMed ID 10226884, Event ID E11, E12)
The molecules are then translocated somewhere.
Consequently, localized protein molecules might in-
dicate a Gene expression event, and a phrase ?pro-
tein expression? was occasionally recognized as
mentioning a Localization.
horbol esters are required to induce
[AIM/CD69Created entity] cell-surface EX-
PRESSION as well as ? ? ?
(PubMed ID 1545132, Event ID E12)
5.3 Conversion of FN Structures to GENIA
Events
During the investigation, we compared participant
slots of GENIA and FN structures, in addition to the
structures themselves. Figures 4 and 5 depict con-
version examples from a FN structure and its par-
ticipants to a GENIA structure, with the domain-
oriented type of each participant entity. The conver-
sions were supported by samples, and need quanti-
tative evaluation.
6 Discussion
By annotating sentences of the GENIA event corpus
with semantic structures conforming to FrameNet,
we explicitly compared linguistically-oriented and
168
Class:????Releasing?Theme:?Protein?Agent:???Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(inside?of)?Cell?ToLoc:??????(outside?of)?Cell?
Class:?A?ching?Item:?Protein?A?Goal:?Protein?B?
Class:??Binding?Theme:?Protein?A,?protein?B?
FrameNet?expression
Class:????Mo?n?Theme:?Protein?Source:?Cell?loca?n?A?Goal:?????Cell?loca?n?B?
GENIA?expression
Class:???????Localiza?n?Theme:????Protein?FromLoc:?Cell?loca?n?A?ToLoc:??????Cell?loca?n?B?
Class:????Ge?g?Theme:?Protein?Recipient:?Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(outside?of)?Cell?ToLoc:??????(inside?of)?Cell?
FrameNet?expression
GENIA?expression
Class:?Becoming?En?y:????????????????? Proteins?Final_category:?Pro?n_complex ?
Class:??Binding?Theme:?Proteins?
Figure 4: FN-to-GENIA conversions for Binding
Class:????Releasing?Theme:?Protein?Agent:???Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(inside?of)?Cell?ToLoc:??????(outside?of)?Cell?
Class:?A?ching?Item:?Protein?A?Goal:?Protein?B?
Class:??Binding?Theme:?Protein?A,?protein?B?
FrameNet?expression
Class:????Mo?n?Theme:?Protein?Source:?Cell?loca?n?A?Goal:?????Cell?loca?n?B?
GENIA?expression
Class:???????Localiza?n?Theme:????Protein?FromLoc:?Cell?loca?n?A?ToLoc:??????Cell?loca?n?B?
Class:????Ge?g?Theme:?Protein?Recipient:?Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(outside?of)?Cell?ToLoc:??????(inside?of)?Cell?
FrameNet?expression
GENIA?expression
Class:?Becoming?En?y:????????????????? Proteins?Final_category:?Pro?n_complex ?
Class:??Binding?Theme:?Proteins?
Figure 5: FN-to-GENIA conversions for Localization.
domain-oriented semantics of the bio-molecular ar-
ticles. Our preliminary result illustrates the gap be-
tween the two type of semantics, and a relationship
between them. We discuss development of a Text
Mining (TM) system, in association with the extrac-
tion of linguistically-oriented semantics, which has
been studied independently of TM.
First, our result would show that TM involves at
least two qualitatively different tasks. One task is
related to our results; that is, recognizing equiva-
lent events which are expressed from different per-
spectives, and hence expressed by using different
linguistic frames, and at the same time distinguish-
ing event mentions which share the same linguistic
frame but belong to different domain classes. Our
investigation indicates that this task is mainly depen-
dent on domain knowledge and how a phenomenon
can be conceptualized. Another task of TM is the ex-
traction of linguistically-oriented semantics, which
basically maps various syntactic realizations to the
shared structures. In order to develop a TM system,
we need to solve the two difficult tasks.
Second, TM could benefit from linguistically-
oriented frames by using them as an intermediat-
ing layer between text and domain-oriented infor-
mation. The domain-oriented semantic structures,
which is a target of TM, are inevitably dependent
on the domain. On the other hand, the extraction of
linguistically-oriented semantics from text is less de-
pendent. Therefore, using the linguistically-oriented
structure could be favorable to domain portability of
a TM system.
Our aim was explicitly linking linguistically-
oriented and domain-oriented semantics of the bio-
molecular articles, and the preliminary result show
the possibility of the extraction of linguistically-
oriented semantics contributing to TM. Further in-
v tigation of the relationship would be a important
step forward for TM in the bio-molecular domain.
Our investigation was preliminary. For exam-
ple, conversions from FN structures to GENIA event
structures, depicted in figures 4 and 5, were based
on manual investigation. Further, they were attested
by limited samples in the corpus. For our results to
contribute to a TM system, evaluation of the conver-
sions and automatic extraction of such conversions
must be considered.
7 Conclusion
This paper presents a relationship of domain-
oriented and linguistically-oriented frames of se-
mantics, obtained by an investigation of the GE-
NIA event corpus. In the investigation, we anno-
tated sample sentences from the GENIA event cor-
pus with linguistically-oriented semantic structures
as those of FrameNet, and compared them with
domain-oriented semantic annotations that the cor-
pus originally possesses. The resulting relations
between the domain-oriented and linguistically-
oriented frames suggest that mentions of a bio-
logical phenomenon could be realized in a num-
ber of linguistically-oriented frames, and that
the linguistically-oriented frames represent possible
perspectives in mentioning the phenomenon. The
resulting relations would illustrate a challenge in
developing a Text Mining system, and would indi-
cate importance of linguistically-oriented frames as
an intermediating layer between text and domain-
oriented information. Our future plan includes
evaluation of our conversions from a linguistically-
oriented to a domain-oriented structure, and auto-
matic extraction of such conversions.
169
References
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein,
H. Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S.
Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-
Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E.
Richardson, M. Ringwald, G. M. Rubin, and G. Sher-
lock. 2000. Gene ontology: tool for the unification of
biology. The Gene Ontology Consortium. Nat Genet,
25(1):25?29, May.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
Semeval-2007 task 19: Frame semantic structure ex-
traction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99?104, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9):e3158, 09.
Andrew Dolbey, Michael Ellsworth, and Jan Scheffczyk.
2006. Bioframenet: A domain-specific framenet
extension with links to biomedical ontologies. In
Proceedings of the Second International Workshop
on Formal Biomedical Knowledge Representation:
?Biomedical Ontology in Action? (KR-MED 2006),
volume 222 of CEUR Workshop Proceedings. CEUR-
WS.org, Nov.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sanda M. Harabagiu, Cosmin Adrian Bejan, and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In IJCAI-05, Proceedings of the Nine-
teenth International Joint Conference on Artificial In-
telligence, pages 1061?1066.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1):10.
Seth Kulick, Ann Bies, Mark Liberman, Mark Man-
del, Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In Lynette Hirschman and James Puste-
jovsky, editors, HLT-NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, Ontolo-
gies and Databases, pages 61?68, Boston, Mas-
sachusetts, USA, May 6. Association for Computa-
tional Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In A. Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annota-
tion, pages 24?31, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(1):50.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August. Coling 2008 Organizing Committee.
Tuangthong Wattarujeekrit, Parantu Shah, and Nigel Col-
lier. 2004. Pasbio: predicate-argument structures for
event extraction in molecular biology. BMC Bioinfor-
matics, 5(1):155.
170
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 85?88,
Paris, October 2009. c?2009 Association for Computational Linguistics
Evaluating Contribution of Deep Syntactic Information
to Shallow Semantic Analysis
Sumire Uematsu Jun?ichi Tsujii
Graduate School of Information Science and Technology
The University of Tokyo
{uematsu,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents shallow semantic pars-
ing based only on HPSG parses. An
HPSG-FrameNet map was constructed
from a semantically annotated corpus, and
semantic parsing was performed by map-
ping HPSG dependencies to FrameNet re-
lations. The semantic parsing was evalu-
ated in a Senseval-3 task; the results sug-
gested that there is a high contribution of
syntactic information to semantic analysis.
1 Introduction
This paper presents semantic parsing based only
on HPSG parses, and examines the contribution of
the syntactic information to semantic analysis.
In computational linguistics, many researchers
have studied the relationship between syntax and
semantics. Its quantitative analysis was formal-
ized as semantic parsing, or semantic role label-
ing, and has attracted the attention of researchers.
Recently, an improvement in the accuracy and
robustness of ?deep parsers? has enabled us to di-
rectly map deep syntactic dependencies to seman-
tic relations. Deep parsers are based on linguisti-
cally expressive grammars; e.g. HPSG, LFG, etc,
and less affected by syntactic alternations such as
passivization. Their results are therefore expected
to closely relate to semantic annotations. For ex-
ample, the sentences in figure 1 share the same
set of semantic roles, and the roles have one-to-
one relations to deep syntactic dependencies in the
sentences. However, the results of the deep parsers
are represented in complex structures, shown in
figure 3, and cannot be straightforwardly com-
pared to semantic annotations.
In order to directly map the deep dependencies
to semantic relations, we adapted the corpus anal-
ysis method of (Frank and Semecky?, 2004) for
the semantic parsing using HPSG parses. We per-
formed the semantic parsing by mapping paths in
HPSG parses to semantic predicate-argument re-
lations. The analysis of the HPSG paths for the
predicate-argument pairs, and the preliminary re-
sult of the semantic parsing indicate the contribu-
tion of syntactic analysis to semantic parsing.
2 Related Work
Besides (Frank and Semecky?, 2004)?s work, as
mentioned above, there have been several studies
on the relationship between deep syntax and se-
mantic parsing. Although the studies did not focus
on direct mappings between deep syntax and shal-
low semantics, they suggested a strong relation-
ship between the two. (Miyao and Tsujii, 2004)
evaluated the accuracy of an HPSG parser against
PropBank semantic annotations, and showed that
the HPSG dependants correlated with semantic ar-
guments of the PropBank, particularly with ?core?
arguments. In (Gildea and Hockenmaier, 2003)
and (Zhang et al, 2008), features from deep parses
were used for semantic parsing, together with fea-
tures from CFG or dependency parses. The deep
features were reported to contribute to a perfor-
mance gain.
3 Syntactic and Semantic Parsing
Some semantic relations are easily identified by
using syntactic parsing while others are more diffi-
cult. This section presents easy and difficult cases
in syntax-semantics map construction.
Trivial when using syntactic analysis: Syn-
tactic parsing, including CFG analysis, detects
semantic similarity of sentences sharing similar
phrase structures. For the example sentences a)
and b) in figure 1, the parsing provides similar
phrase structures, and therefore gives the same
syntactic dependency to occurrences of each role.
Trivial when using deep analysis: Deep pars-
ing reveals the semantic similarity of sentences
85
a) ?, ICommunicator praise themEvaluee for being 99 percent perfectReason. 
b) ?, but heCommunicator praised the Irish premierEvaluee for making a ``sensible?? speechReason.
?, HeEvaluee has been particularly praised as an exponent of ?,
d) ?, SheCommunicator was supposed, therefore, to praise himEvaluee and then ? 
c) The childEvaluee is praised for having a dry bedReason and ?
e) ItEvaluee received high praise, ?
f) AliceWearer ?s dress
g) Versace?s dress
Figure 1: Sentences with a set of semantic roles for the predicate praise.
a) ?, ICommunicator praise themEvaluee for being 99 percent perfectReason. 
b) ?, but heCommunicator praised the Irish premierEvaluee for making a ``sensible?? speechReason.
?, HeEvaluee has been particularly praised as an exponent of ?,
d) ?, SheCommunicator was supposed, therefore, to praise himEvaluee and then ? 
c) The childEvaluee is praised for having a dry bedReason and ?
e) ItEvaluee received high praise, ?
f) AliceWearer ?s dress
g) Versace?s dress
Figur 2: Example phr ses
for section 3.
Mary
Head?Complement?schema
Head?Subject?schema
likes
SYNSEM|LOCAL CATCONT|HOOK
HEADVAL 3?
6?
VFORM:??finverbAUX:???????noneSUBJ:???<??????>?1COMP:<???????>2?verb_arg12PRED:???like?ARG1: 4?ARG2: 5?The
SYNSEM: LOCAL CATCONT|HOOKHEAD:??detVAL|?SPEC:?<???????>8det_arg1PRED:???the?ARG1: 4?7? LOCAL2?
noun_arg0PRED:???Mary?
CAT
CONT|HOOK
HEADVAL CASE:??acc
nounAGR:???3sg
5?SUBJ:???<??????>?COMP:<???????>
SYNSEM:?girl noun_arg0PRED:???girl?SPR:?????<???????>?
LOCALCAT
CONT|HOOK
HEADVAL CASE:??nom
nounAGR:???3sg
4?
SUBJ:???<??????>?COMP:<???????>7SYNSEM:8?
Head?Specifier?schema
1?SYNSEM:? LOCALCAT
CONT|HOOK:?
HEADVAL CASE:??nom
nounAGR:???3sg
4?SUBJ:???<??????>?COMP:<???????> SYNSEM|LOCAL CATCONT|HOOK:?
HEAD:VAL 3?
6?
SUBJ:???<??????>?1COMP:<???????>
SYNSEM|LOCAL CATCONT|HOOK:?
HEAD:VAL 3?
6?
SUBJ:???<??????>?COMP:<???????>
Figure 3: An HPSG parse for The girl likes Mary.
containing complex syntactic phenomena, which
is not easily detected by CFG analysis. The sen-
tences c) and d) in figure 1 contain passivization
and object raising, while deep parsing provides
one dependency for each role in the figure.
Not trivial even when using deep analysis:
Some semantic arguments are not direct syntactic
dependants of their predicates - especially of noun
predicates. In sentence e) in figure 2, the Evaluee
phrase depends on the predicate praise, through
the support verb receive. The deep analysis would
be advantageous in capturing such dependencies,
because it provides receive with direct links to the
phrases of the role and the predicate.
Problematic when using only syntactic analy-
sis: Sometimes, the semantic role of a phrase is
strongly dependent on the type of the mentioned
entity, rather than on the syntactic dependency. In
phrases f) and g) in figure 2, the phrases Alice and
Versace, have the same syntactic relation to the
predicate dress. However, the Wearer role is given
only to the former phrase.
4 A Wide-Coverage HPSG Parser
We employed a wide-coverage HPSG parser for
semantic parsing, and used deep syntactic depen-
dencies encoded in a Predicate Argument Struc-
ture (PAS) in each parse node.
In our experiments, the parser results were con-
sidered as graphs, as illustrated by figures 3 and 4,
to extract HPSG dependencies conveniently. The
The               girl                likes            Mary.verb_arg12 noun_arg0noun_arg0det_arg1
ARG1
ARG2ARG1
Figure 4: A simplified representation of figure 3.
graph is obtained by ignoring most of the linguis-
tic information in the original parse nodes, and
by adding edges directing to the PAS dependants.
The PAS information is represented in the graph,
by the terminal nodes? PAS types, e.g. verb arg12,
etc., and by the added edges. Note that the inter-
pretation of the edge labels depends on the PAS
type. If the PAS type is verb arg12, the ARG2 de-
pendant is the object of the transitive verb or its
equivalence (the subject of the passive, etc.). If
the PAS type is prep arg12, then the dependant is
the NP governed by the preposition node.
5 Semantic Parsing Based on FrameNet
We employed FrameNet (FN) as a semantic cor-
pus. Furthermore, we evaluated our semantic pars-
ing on the SRL task data of Senseval-3 (Litkowski,
2004), which consists of FN annotations.
In FN, semantic frames are defined, and each
frame is associated with predicates that evoke the
frame. For instance, the verb and noun praise are
predicates of the Judgment communication frame,
and they share the same set of semantic roles.
The Senseval-3 data is a standard for evaluation
of semantic parsing. The task is defined as identi-
fying phrases and their semantic roles for a given
sentence, predicate, and frame. The data includes
null instantiations of roles1, which are ?conceptu-
ally salient?, but do not appear in the text.
6 Methods
The semantic parsing using an HPSG-FN map
consisted of the processes shown in figure 5.
1An example of a null instantiation is the Communicator
role in the sentence, ?All in all the conference was acclaimed
as a considerable success.?
86
Map?construc?n?
HPSG?parsingRaw?sentences?
Seman??annota?ns?
Training?data?
HPSG?parses?
Phrase?projec?n? 
HPSG?parses?with?seman?ally?marked?nodes?
HPSG?dependency?extrac?n? 
HPSG?dependency?between??predicate1?and?role1?
Map?instances?
HPSG?dependency?between??predicate1?and?role2?
HPSG?parsingRaw?sentences?
Predicate?annota?ns?
Test?data?
HPSG?parses?
Phrase?projec?n? 
HPSG?parses?with?nodes?marked?as?predicates?
Role?node?predic?n 
Feature?filter?
HPSG?parses?with?seman?ally?marked?nodes?
Role?predic?n?rules?
Seman??parsing?(Map?evalua?n)?
Figure 5: Processes in the map construction and evaluation.
It           recieved       high         praise,  ?adj_arg1verb_arg12noun_arg0
ARG2 ARG1ARG1
Evaluee?role
noun_arg0
Figure 6: an HPSG path for a
semantic relation.
Predicate base: The base form of the semantic
predicate word. (praise in the case of figure 6).
Predicate type: The PAS type of the HPSG
terminal node for the predicate - see section 4.
(noun arg0 in figure 6).
Intermediate word base: The base form of the
intermediate word, corresponding to a terminal
passed by the path, and satisfying pre-defined
conditions. The word may be a support verb.
- see figure 6. (receive in figure 6).
Intermediate word type: The PAS type of the
intermediate word. (verb arg12 in figure 6).
Dependency label sequence: The labels of
the path?s edges. We omitted labels presenting
head-child relations, for identifying a phrase with
another phrase sharing the same head word.
(Reverse of ARG2, ARG1 in figure 6).
Table 1: Features used to represent a HPSG path.
Filter Pred. Inter. Dep.
base type base type label
Same ? ? ? ? ?
AllInter ? ? ? ?
AllPred ? ? ? ?
AllPred-AllInter ? ? ?
Table 2: Syntactic features for role prediction.
Phrase projection: Because we used FN anno-
tations, which are independent of any syntactic
framework, role phrases needed to be projected
to appropriate HPSG nodes. We projected the
phrases based on maximal projection, which was
generally employed, with heads defined in the
HPSG.
HPSG dependency extraction: As an HPSG
dependency for a predicate-argument pair, we
used the shortest path between the predicate node
and the argument node in the HPSG parse. The
path was then represented by pre-defined fea-
tures, listed in table 1. The search for the short-
est path was done in the simplified graph of the
HPSG parse (see figure 4), with the edges denot-
ing deep dependencies, and head-child relations.
An instance of the HPSG-FN map consisted of the
path?s features, the FN frame, and the role label.
Role node prediction: The role prediction was
based on simple rules with scores. The rules were
obtained by filtering features of the map instances.
Table 2 shows the feature filters. The score of a
rule was the number of map instances matching
the rule?s features. In the test, for each node of a
HPSG parse, the role label with the highest score
was selected as the result, where the score of a la-
bel was that of the rule providing the label.
7 Experiments
For the experiments, we employed a wide cover-
age HPSG parser, Enju version 2.3.12, and the data
for the Semantic Role Labeling task of Senseval-3.
7.1 Analysis of Map Instances
We extracted 41,193 HPSG-FN map instances
from the training set, the training data apart from
the development set. The instances amounted to
97.7 % (41,193 / 42,163) of all the non-null in-
stantiated roles in the set, and HPSG paths were
short for many instances. Paths to syntactic ar-
guments were almost directly mapped to semantic
roles, while roles for other phrases were more am-
biguous.
The length distribution of HPSG paths: 64 %
(26410 / 41193) of the obtained HPSG paths were
length-one, and 8 % (3390 / 41193) were length-
two, due to the effect of direct links provided by
HPSG parsing. The length of a path was defined
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
87
Pred. Freq. Feature representation Interpretation
Verb 3792 verb arg12/?/?/ARG2 The object of the transitive predicate
3191 verb arg12/?/?/ARG1 The subject of the transitive predicate
Noun 7468 noun arg0/?/?/? NP headed by the predicate
1161 noun arg0/of/prep arg12/Rev-ARG1 The PP headed by ?of?, attaching to the predicate
Adj 1595 adj arg1/?/?/ARG1 The modifiee of the predicate
274 verb arg12/?/?/ARG2 The modifiee of the predicate treated as a verb
Table 3: Most frequent syntactic paths extracted for predicates of each POS.
as the number of the labels in the Dep. label seq.
of the path. Most of the one-length paths were
paths directing to syntactic arguments, and to PPs
attaching to the predicates. The two-length paths
included paths using support verbs (see figure 6).
Most frequent HPSG dependencies: The most
frequent paths are shown in table 3; syntactic de-
pendencies are presented and counted as taples of
Pred. type, Inter. base, Inter. type, and Dep.
label seq. The interpretation column describes
the syntactic dependencies for the taples. Note
that the column denotes normalized dependencies,
in which object indicates objects of active voice
verbs, subjects of passive-voiced verbs, etc.
7.2 Performance of Semantic Parsing
Finally, semantic parsing was evaluated on the test
data. Table 4 shows the overall performance. The
scores were measured by the Senseval-3 official
script, in the restrictive setting, and can be directly
compared to other systems? scores. Since our pre-
liminary system of semantic parsing ignored null
instantiations of roles, it lost around 0.10 point
of the recalls. We believe that such instantia-
tions may be separately treated. Although the sys-
tem was based on only the syntactic information,
and was very na??ve, the system?s performance was
promising, and showed the high contribution of
syntactic dependencies for semantic parsing.
8 Conclusion
This paper presents semantic parsing based on
only HPSG parses, and investigates the contribu-
tion of syntactic information to semantic parsing.
We constructed an HPSG-FN map by finding
the HPSG paths that corresponded to semantic re-
lations, and used it as role prediction rules in se-
mantic parsing. The semantic parsing was evalu-
ated on the SRL task data of Senseval-3. Although
the preliminary system used only the syntactic in-
formation, the performance was promising, and
Rule set Prec. Overlap Recall
Same 0.799 0.783 0.518
AllInter 0.599 0.586 0.589
AllPred 0.472 0.462 0.709
AllPred-AllInter 0.344 0.335 0.712
Senseval-3 best 0.899 0.882 0.772
Senseval-3 4th best 0.802 0.784 0.654
Table 4: Semantic parsing result on the test data.
indicated that syntactic dependencies may make
significant contribution to semantic analysis.
This paper also suggests a limit of the seman-
tic analysis based purely on syntax. A next step
for accurate HPSG-FN mapping could be analy-
sis of the interaction between the HPSG-FN map
and other information, such as named entity types
which were shown to be effective in many studies.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
Anette Frank and Jir??? Semecky?. 2004. Corpus-based
induction of an LFG syntax-semantics interface for
frame semantic processing. In Proc. of International
Workshop on Linguistically Interpreted Corpora.
Daniel Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial
grammar. In Proc. of EMNLP.
Ken Litkowski. 2004. Senseval-3 task: Automatic la-
beling of semantic roles. In Proc. of Senseval-3.
Yusuke Miyao and Jun?ichi Tsujii. 2004. Deep lin-
guistic analysis for the accurate identification of
predicate-argument relations. In Proc. of Coling.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid learning of dependency structures from hetero-
geneous linguistic resources. In Proc. of CoNLL.
88
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1042?1051,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Multiple Dependency Corpora
for Inducing Wide-coverage Japanese CCG Resources
Sumire Uematsu?
uematsu@cks.u-tokyo.ac.jp
Takuya Matsuzaki?
takuya-matsuzaki@nii.ac.jp
Hiroki Hanaoka?
hanaoka@nii.ac.jp
Yusuke Miyao?
yusuke@nii.ac.jp
Hideki Mima?
mima@t-adm.t.u-tokyo.ac.jp
?The University of Tokyo
Hongo 7-3-1, Bunkyo, Tokyo, Japan
?National Institute of Infomatics
Hitotsubashi 2-1-2, Chiyoda, Tokyo, Japan
Abstract
This paper describes a method of in-
ducing wide-coverage CCG resources for
Japanese. While deep parsers with corpus-
induced grammars have been emerging
for some languages, those for Japanese
have not been widely studied, mainly be-
cause most Japanese syntactic resources
are dependency-based. Our method first
integrates multiple dependency-based cor-
pora into phrase structure trees and then
converts the trees into CCG derivations.
The method is empirically evaluated in
terms of the coverage of the obtained lexi-
con and the accuracy of parsing.
1 Introduction
Syntactic parsing for Japanese has been domi-
nated by a dependency-based pipeline in which
chunk-based dependency parsing is applied and
then semantic role labeling is performed on the de-
pendencies (Sasano and Kurohashi, 2011; Kawa-
hara and Kurohashi, 2011; Kudo and Matsumoto,
2002; Iida and Poesio, 2011; Hayashibe et al,
2011). This dominance is mainly because chunk-
based dependency analysis looks most appropriate
for Japanese syntax due to its morphosyntactic ty-
pology, which includes agglutination and scram-
bling (Bekki, 2010). However, it is also true that
this type of analysis has prevented us from deeper
syntactic analysis such as deep parsing (Clark and
Curran, 2007) and logical inference (Bos et al,
2004; Bos, 2007), both of which have been sur-
passing shallow parsing-based approaches in lan-
guages like English.
In this paper, we present our work on induc-
ing wide-coverage Japanese resources based on
combinatory categorial grammar (CCG) (Steed-
man, 2001). Our work is basically an extension of
a seminal work on CCGbank (Hockenmaier and
Steedman, 2007), in which the phrase structure
trees of the Penn Treebank (PTB) (Marcus et al,
1993) are converted into CCG derivations and a
wide-coverage CCG lexicon is then extracted from
these derivations. As CCGbank has enabled a va-
riety of outstanding works on wide-coverage deep
parsing for English, our resources are expected to
significantly contribute to Japanese deep parsing.
The application of the CCGbank method to
Japanese is not trivial, as resources like PTB are
not available in Japanese. The widely used re-
sources for parsing research are the Kyoto corpus
(Kawahara et al, 2002) and the NAIST text corpus
(Iida et al, 2007), both of which are based on the
dependency structures of chunks. Moreover, the
relation between chunk-based dependency struc-
tures and CCG derivations is not obvious.
In this work, we propose a method to integrate
multiple dependency-based corpora into phrase
structure trees augmented with predicate argument
relations. We can then convert the phrase structure
trees into CCG derivations. In the following, we
describe the details of the integration method as
well as Japanese-specific issues in the conversion
into CCG derivations. The method is empirically
evaluated in terms of the quality of the corpus con-
version, the coverage of the obtained lexicon, and
the accuracy of parsing with the obtained gram-
mar. Additionally, we discuss problems that re-
main in Japanese resources from the viewpoint of
developing CCG derivations.
There are three primary contributions of this pa-
per: 1) we show the first comprehensive results for
Japanese CCG parsing, 2) we present a methodol-
ogy for integrating multiple dependency-based re-
1042
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 1: A CCG derivation.
X/Y : f Y : a ? X : fa (>)
Y : a X\Y : a ? X : fa (<)
X/Y : f Y/Z : g ? X/Z : ?x.f(gx) (> B)
Y\Z : g X\Y : f ? X\Z : ?x.f(gx) (< B)
Figure 2: Combinatory rules (used in the current
implementation).
sources to induce CCG derivations, and 3) we in-
vestigate the possibility of further improving CCG
analysis by additional resources.
2 Background
2.1 Combinatory Categorial Grammar
CCG is a syntactic theory widely accepted in the
NLP field. A grammar based on CCG theory con-
sists of categories, which represent syntactic cat-
egories of words and phrases, and combinatory
rules, which are rules to combine the categories.
Categories are either ground categories like S and
NP or complex categories in the form of X/Y or
X\Y , where X and Y are the categories. Cate-
gory X/Y intuitively means that it becomes cat-
egory X when it is combined with another cat-
egory Y to its right, and X\Y means it takes a
category Y to its left. Categories are combined
by applying combinatory rules (Fig. 2) to form
categories for larger phrases. Figure 1 shows a
CCG analysis of a simple English sentence, which
is called a derivation. The verb give is assigned
category S\NP/NP/NP , which indicates that it
takes two NPs to its right, one NP to its left, and fi-
nally becomes S. Starting from lexical categories
assigned to words, we can obtain categories for
phrases by applying the rules recursively.
An important property of CCG is a clear inter-
face between syntax and semantics. As shown in
Fig. 1, each category is associated with a lambda
term of semantic representations, and each com-
binatory rule is associated with rules for semantic
composition. Since these rules are universal, we
can obtain different semantic representations by
switching the semantic representations of lexical
categories. This means that we can plug in a vari-
Sentence S Verb S\$ (e.g. S\NPga)
Noun phrase NP Post particle NPga|o|ni|to\NP
Auxiliary verb S\S
Table 1: Typical categories for Japanese syntax.
Cat. Feature Value Interpretation
NP case ga nominal
o accusative
ni dative
to comitative, complementizer, etc.
nc none
S form stem stem
base base
neg imperfect or negative
cont continuative
vo s causative
Table 2: Features for Japanese syntax (those used
in the examples in this paper).
ety of semantic theories with CCG-based syntactic
parsing (Bos et al, 2004).
2.2 CCG-based syntactic theory for Japanese
Bekki (2010) proposed a comprehensive theory
for Japanese syntax based on CCG. While the the-
ory is based on Steedman (2001), it provides con-
crete explanations for a variety of constructions of
Japanese, such as agglutination, scrambling, long-
distance dependencies, etc. (Fig. 3).
The ground categories in his theory are S, NP,
and CONJ (for conjunctions). Table 1 presents
typical lexical categories. While most of them
are obvious from the theory of CCG, categories
for auxiliary verbs require an explanation. In
Japanese, auxiliary verbs are extensively used to
express various semantic information, such as
tense and modality. They agglutinate to the main
verb in a sequential order. This is explained in
Bekki?s theory by the category S\S combined with
a main verb via the function composition rule
(<B). Syntactic features are assigned to categories
NP and S (Table 2). The feature case represents a
syntactic case of a noun phrase. The feature form
denotes an inflection form, and is necessary for
constraining the grammaticality of agglutination.
Our implementation of the grammar basically
follows Bekki (2010)?s theory. However, as a first
step in implementing a wide-coverage Japanese
parser, we focused on the frequent syntactic con-
structions that are necessary for computing pred-
icate argument relations, including agglutination,
inflection, scrambling, case alternation, etc. Other
details of the theory are largely simplified (Fig. 3),
1043
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 3: A simplified CCG analysis of the sentence ?The ambassador participated in the negotiation.?.
S ? NP/NP (RelExt)
S\NP1 ? NP1/ P1 (RelIn)
S ? S1/S1 (Con)
S\$1\NP1 ? (S1\$1\NP1)/(S1\$1\NP1) (ConCoord)
Figure 4: Type changing rules. The upper two are
for relative clauses and the others for continuous
clauses.
coordination and semantic representation in par-
ticular. The current implementation recognizes
coordinated verbs in continuous clauses (e.g., ??
???????????/he played the pia o and
sang?), but the treatment of other types of coor-
dination is largely simplified. For semantic repre-
sentation, we define predicate argument structures
(PASs) rather than the theory?s formal representa-
tion based on dynamic logic. Sophisticating our
semantic representation is left for future work.
For parsing efficiency, we modified the treat-
ment of some constructions so that empty el-
ements are excluded from the implementation.
First, we define type changing rules to produce
relative and continuous clauses (shown in Fig. 4).
The rules produce almost the same results as the
theory?s treatment, but without using empty ele-
ments (pro, etc.). We also used lexical rules to
treat pro-drop and scrambling. For the sentence in
Fig. 3, the deletion of the nominal phrase (??
?), the dative phrase (???), or both results in
valid sentences, and shuffling the two phrases does
so as well. Lexical entries with the scrambled or
dropped arguments are produced by lexical rules
in our implementation.
2.3 Linguistic resources for Japanese parsing
As described in Sec. 1, dependency-based analysis
has been accepted for Japanese syntax. Research
on Japanese parsing also relies on dependency-
based corpora. Among them, we used the follow-
ing resources in this work.
Kyoto corpus A news text corpus annotated
with morphological information, chunk bound-
Kyoto Corpus 
Chunk 
?? ? government NOM ?? ? ambassador ACC ?? ? negotiation DAT ?? ? ? ? participation do cause PAST 
NAIST Corpus 
Dep. 
Causer ARG-ga ARG-ni 
Figure 5: The Kyoto and NAIST annotations for
?The government had the ambassador participate
in the negotiation.?. Accusatives are labeled as
ARG-ga in causative (see Sec. 3.2).
aries, and dependency relations among chunks
(Fig. 5). The dependencies are classified into four
types: Para (coordination), A (apposition), I (ar-
gument cluster), and Dep (default). Most of the
dependencies are annotated as Dep.
NAIST text corpus A corpus annotated with
anaphora and coreference relations. The same set
as the Kyoto corpus is annotated.1 The corpus
only focuses on three cases: ?ga? (subject), ?o?
(direct object), and ?ni? (indirect object) (Fig. 5).
Japanese particle corpus (JP) (Hanaoka et al,
2010) A corpus annotated with distinct gram-
matical functions of the Japanese particle (postpo-
sition) ?to?. In Japanese, ?to? has many functions,
including a complementizer (similar to ?that?), a
subordinate conjunction (similar to ?then?), a co-
ordination conjunction (similar to ?and?), and a
case marker (similar to ?with?).
2.4 Related work
Research on Japanese deep parsing is fairly lim-
ited. Formal theories of Japanese syntax were
presented by Gunji (1987) based on Head-driven
Phrase Structure Grammar (HPSG) (Sag et al,
2003) and by Komagata (1999) based on CCG, al-
though their implementations in real-world pars-
ing have not been very successful. JACY (Siegel
1In fact, the NAIST text corpus includes additional texts,
but in this work we only use the news text section.
1044
and Bender, 2002) is a large-scale Japanese gram-
mar based on HPSG, but its semantics is tightly
embedded in the grammar and it is not as easy
to systematically switch them as it is in CCG.
Yoshida (2005) proposed methods for extracting
a wide-coverage lexicon based on HPSG from a
phrase structure treebank of Japanese. We largely
extended their work by exploiting the standard
chunk-based Japanese corpora and demonstrated
the first results for Japanese deep parsing with
grammar induced from large corpora.
Corpus-based acquisition of wide-coverage
CCG resources has enjoyed great success for En-
glish (Hockenmaier and Steedman, 2007). In
that method, PTB was converted into CCG-based
derivations from which a wide-coverage CCG lex-
icon was extracted. CCGbank has been used for
the development of wide-coverage CCG parsers
(Clark and Curran, 2007). The same methodology
has been applied to German (Hockenmaier, 2006),
Italian (Bos et al, 2009), and Turkish (C?ak?c?,
2005). Their treebanks are annotated with depen-
dencies of words, the conversion of which into
phrase structures is not a big concern. A notable
contribution of the present work is a method for in-
ducing CCG grammars from chunk-based depen-
dency structures, which is not obvious, as we dis-
cuss later in this paper.
CCG parsing provides not only predicate argu-
ment relations but also CCG derivations, which
can be used for various semantic processing tasks
(Bos et al, 2004; Bos, 2007). Our work consti-
tutes a starting point for such deep linguistic pro-
cessing for languages like Japanese.
3 Corpus integration and conversion
For wide-coverage CCG parsing, we need a)
a wide-coverage CCG lexicon, b) combinatory
rules, c) training data for parse disambiguation,
and d) a parser (e.g., a CKY parser). Since d) is
grammar- and language-independent, all we have
to develop for a new language is a)?c).
As we have adopted the method of CCGbank,
which relies on a source treebank to be converted
into CCG derivations, a critical issue to address is
the absence of a Japanese counterpart to PTB. We
only have chunk-based dependency corpora, and
their relationship to CCG analysis is not clear.
Our solution is to first integrate multiple
dependency-based resources and convert them
into a phrase structure treebank that is independent
ProperNoun 
????? Yeltsin 
NP 
ProperNoun 
??? Russia 
Noun 
???president 
PostP 
? DAT 
PP 
NP 
Aux 
??? not 
VP 
Verb 
?? forgive 
VerbSuffix 
? PASSIVE 
VP Aux 
? PAST 
VP 
?to Russian president Yeltsin? ?(one) was not forgiven? 
Figure 6: Internal structures of a nominal chunk
(left) and a verbal chunk (right).
of CCG analysis (Step 1). Next, we translate the
treebank into CCG derivations (Step 2). The idea
of Step 2 is similar to what has been done with
the English CCGbank, but obviously we have to
address language-specific issues.
3.1 Dependencies to phrase structure trees
We first integrate and convert available Japanese
corpora?namely, the Kyoto corpus, the NAIST
text corpus, and the JP corpus ?into a phrase
structure treebank, which is similar in spirit to
PTB. Our approach is to convert the depen-
dency structures of the Kyoto corpus into phrase
structures and then augment them with syntac-
tic/semantic roles from the other two corpora.
The conversion involves two steps: 1) recogniz-
ing the chunk-internal structures, and (2) convert-
ing inter-chunk dependencies into phrase struc-
tures. For 1), we don?t have any explicit infor-
mation in the Kyoto corpus although, in princi-
ple, each chunk has internal structures (Vadas and
Curran, 2007; Yamada et al, 2010). The lack of
a chunk-internal structure makes the dependency-
to-constituency conversion more complex than a
similar procedure by Bos et al (2009) that con-
verts an Italian dependency treebank into con-
stituency trees since their dependency trees are an-
notated down to the level of each word. For the
current implementation, we abandon the idea of
identifying exact structures and instead basically
rely on the following generic rules (Fig. 6):
Nominal chunks Compound nouns are first
formed as a right-branching phrase and
post-positions are then attached to it.
Verbal chunks Verbal chunks are analyzed as
left-branching structures.
The rules amount to assume that all but the last
word in a compound noun modify the head noun
(i.e., the last word) and that a verbal chunk is typ-
ically in a form V A1 . . . An, where V is a verb
1045
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP 
PP 
Noun 
?? process  
PostPcm  
? ACC  PP 
NP 
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP PP 
Noun 
?? process  
PostPcm  
? ACC  
Para Dep 
?$ proFess froP EirtK to deatK? 
Figure 7: From inter-chunk dependencies to a tree.
(or other predicative word) and Ais are auxiliaries
(see Fig. 6). We chose the left-branching structure
as default for verbal chunks because the semantic
scopes of the auxiliaries are generally in that or-
der (i.e., A1 has the narrowest scope). For both
cases, phrase symbols are percolated upward from
the right-most daughters of the branches (except
for a few cases like punctuation) because in almost
all cases the syntactic head of a Japanese phrase is
the right-most element.
In practice, we have found several patterns of
exceptions for the above rules. We implemented
exceptional patterns as a small CFG and deter-
mined the chunk-internal structures by determin-
istic parsing with the generic rules and the CFG.
For example, two of the rules we came up with are
rule A: Number ? PrefixOfNumber Number
rule B: ClassifierPhrase ? Number Classifier
in the precedence: rule A > B > generic rules.
Using the above, we bracket a compound noun
? ? ? ??
approximately thousand people death
PrefixOfNumber Number Classifier CommonNoun
?death of approximately one thousand people?
as in
(((? ?) ?) ??)
(((approximately thousand) people) death)
We can improve chunk-internal structures to some
extent by refining the CFG rules. A complete solu-
tion like the manual annotation by Vadas and Cur-
ran (2007) is left for future work.
The conversion of inter-chunk dependencies
into phrase structures may sound trivial, but it is
not necessarily easy when combined with chunk-
internal structures. The problem is to which node
in the internal structure of the head the dependent
dep modifier-type precedence
Para ??/PostPcm ??/PostPcm, */(Verb|Aux), ...
Dep */PostPcm */(Verb|Aux), */Noun, ...
Dep */PostPadnom */Noun, */(Verb|Aux), ...
Table 3: Rules to determine adjoin position.
PP 
Noun 
? dog 
PostP 
? DAT 
VP 
NP 
Adj  
?? 
white  
NP 
VP 
Noun 
? 
cat  
Verb 
?? say  
Aux 
? PAST 
VP PP 
Verb 
?? go!  
PostP 
? 
CMP  
ARG - to 
ARG - ni 
ARG - ga 
ARG - ga 
ARG - ga 
ARG - ga ARG - ni 
ARG - CLS  
NAIST 
JP 
Figure 8: Overlay of pred-arg structure annotation
(?The white cat who said ?Go!? to the dog.?).
tree is adjoined (Fig. 7). In the case shown in the
figure, three chunks are in the dependency relation
indicated by arrows on the top. The dotted arrows
show the nodes to which the subtrees are adjoined.
Without any human-created resources, we can-
not always determine the adjoin positions cor-
rectly. Therefore, as a compromise, we wound up
implementing approximate heuristic rules to deter-
mine the adjoin positions. Table 3 shows examples
of such rules. A rule specifies a precedence of the
possible adjoin nodes as an ordered list of patterns
on the lexical head of the subtree under an ad-
join position. The precedence is defined for each
combination of the type of the dependent phrase,
which is determined by its lexical head, and the
dependency type in the Kyoto corpus.
To select the adjoin position for the left-most
subtree in Fig. 7, for instance, we look up the
rule table using the dependency type, ?Para?, and
the lexical head of the modifier subtree, ? ??
/PostPcm?, as the key, and find the precedence ??
?/PostPcm, */(Verb|Aux), ...?. We thus select the
PP-node on the middle subtree indicated by the
dotted arrow because its lexical head (the right-
most word), ? ??/PostPcm?, matches the first
pattern in the precedence list. In general, we seek
for an adjoin node for each pattern p in the prece-
dence list, until we find a first match.
The semantic annotation given in the NAIST
corpus and the JP corpus is overlaid on the phrase
structure trees with slight modifications (Fig. 8).
1046
PP 
Noun 
?? negotiation 
PostPcm  
? DAT VP Noun 
?? participation  
Verb 
? do 
VerbSuffix 
? CAUSE  
Aux 
? PAST 
VP 
VP 
S 
NPni 
NP 
?? negotiation 
T1 
? DAT T4 
T5 
?? participation  
S?S 
? do 
S?S 
? CAUSE  
S?S 
? PAST 
T3 
T2 
S ? 
? 
?  or   ?B   
?  or   ?B   
?  or   ?B   
NPni 
NPnc 
?? negotiation 
NPni?NPnc 
? DAT 
Svo_s?NPni 
Svo_s?NPni 
?? participation  
Svo_s?Svo_s 
? do 
Scont?Svo_s 
? CAUSE  
Sbase?Scont 
? PAST 
Scont?NPni 
Sbase?NPni 
Sbase 
Step 2 - 1  
Step 2 - 2, 2 - 3  
Figure 9: A phrase structure into a CCG deriva-
tion.
In the figure, the annotation given in the two cor-
pora is shown inside the dotted box at the bottom.
We converted the predicate-argument annotations
given as labeled word-to-word dependencies into
the relations between the predicate words and their
argument phrases. The results are thus similar to
the annotation style of PropBank (Palmer et al,
2005). In the NAIST corpus, each pred-arg re-
lation is labeled with the argument-type (ga/o/ni)
and a flag indicating that the relation is medi-
ated by either a syntactic dependency or a zero
anaphora. For a relation of a predicate wp and its
argument wa in the NAIST corpus, the boundary
of the argument phrase is determined as follows:
1. If wa precedes wp and the relation is medi-
ated by a syntactic dep., select the maximum
PP that is formed by attaching one or more
postpositions to the NP headed by wa.
2. If wp precedes wa or the relation is mediated
by a zero anaphora, select the maximum NP
headed by wa that does not include wp.
In the figure, ??/dog?/DAT? is marked as the ni-
argument of the predicate ???/say? (Case 1), and
???/white ?/cat? is marked as its ga-argument
(Case 2). Case 1 is for the most basic construction,
where an argument PP precedes its predicate. Case
VP 
??    ? 
friend- DAT 
PP VP 
?? 
meet - BASE  
NPni ? 
VP 
10 ?    ? 
10 o?clock - TIME  
PP VP 
?? 
meet - BASE  
T/T ? 
X  
S 
??    ? 
friend- DAT 
NPni S?NPni 
?? 
meet - BASE  
S 
10 ?    ? 
10 o?clock - TIME  
S?S S 
?? 
meet - BASE  
?(to) Peet at ten? 
?(to) Peet a friend? 
Figure 10: An argument post particle phrase (PP)
(upper) and an adjunct PP (lower).
2 covers the relative clause construction, where a
relative clause precedes the head NP, the modifi-
cation of a noun by an adjective, and the relations
mediated by zero anaphora.
The JP corpus provides only the function label
to each particle ?to? in the text. We determined
the argument phrases marked by the ?to? particles
labeled as (nominal or clausal) argument-markers
in a similar way to Case 1 above and identified the
predicate words as the lexical heads of the phrases
to which the PPto phrases attach.
3.2 Phrase structures to CCG derivations
This step consists of three procedures (Fig. 9):
1. Add constraints on categories and features
to tree nodes as far as possible and assign a
combinatory rule to each branching.
2. Apply combinatory rules to all branching and
obtain CCG derivations.
3. Add feature constraints to terminal nodes.
3.2.1 Local constraint on derivations
According to the phrase structures, the first proce-
dure in Step 2 imposes restrictions on the resulting
CCG derivations. To describe the restrictions, we
focus on some of the notable constructions and il-
lustrate the restrictions for each of them.
Phrases headed by case marker particles A
phrase of this type must be either an argument
(Fig. 10, upper) or a modifier (Fig. 10, lower) of a
predicative. Distinction between the two is made
based on the pred-arg annotation of the predica-
tive. If a phrase is found to be an argument, 1) cat-
egory NP is assigned to the corresponding node,
2) the case feature of the category is given accord-
ing to the particle (in the case of Fig. 10 (upper),
1047
VP 
Verb 
?? 
Speak - NEG  
Aux 
??? 
not- CONT  
Aux 
? 
PAST- BASE  
VP 
S cont ?S  
S ba s e ?S  
?did not speak? 
? or ?B  
? or ?B  S cont ?NPg a  
Sneg ?NPg a  
?? 
Speak - NEG  
S cont ?Sneg  
??? 
not- CONT  
S b ase ?S cont  
? 
PAST- BASE  
S b ase ?NPg a  
Figure 11: An auxiliary verb and its conversion.
VP 
Verb 
?? inquire - NEG  
VerbSuffix 
??? cause - BASE  
??    ? her - DAT 
PP 
VP 
ARG - ga  
?(to) Kave Ker inTuire? 
? 
S?NPni[1 ] 
S?S 
??? cause - BASE  
??    ? her - DAT 
NPni[1 ] 
S 
S?NPni[1 ] 
?? inquire - NEG  
ga         [1]  
NPni[1 ] 
ga: [1 ] 
Figure 12: A causative construction.
ni for dative), and 3) the combinatory rule that
combines the particle phrase and the predicative
phrase is assigned backward function application
rule (<). Otherwise, a category T/T is assigned to
the corresponding modifier node and the rule will
be forward function application (>).
Auxiliary verbs As described in Sec. 2.2, an
auxiliary verb is always given the category S\S
and is combined with a verbal phrase via < or <B
(Fig. 11). Furthermore, we assign the form feature
value of the returning category S according to the
inflection form of the auxiliary. In the case shown
in the figure, Sbase\S is assigned for ??/PAST-
BASE? and Scont\S for ????/not-CONT?. As
a result of this restriction, we can obtain condi-
tions for every auxiliary agglutination because the
two form values in S\S are both restricted after
applying combinatory rules (Sec. 3.2.2).
Case alternations In addition to the argu-
ment/adjunct distinction illustrated above, a pro-
cess is needed for argument phrases of predicates
involving case alternation. Such predicates are
either causative (see Fig. 12) or passive verbs
and can be detected by voice annotation from the
NAIST corpus. For an argument of that type of
verb, its deep case (ga for Fig. 12) must be used
to construct the semantic representation, namely
the PAS. As well as assigning the shallow case
value (ni in Fig. 12) to the argument?s category
NP, as usual, we assign a restriction to the PAS
S?NPo[1] 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
NP 
? 
book 
NP 
NP[1]?NP[1] 
VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
Noun 
? 
book 
NP 
S?NP[1] 
NP[1]?NP[1] 
Noun 
? 
store 
NP 
?   ? 
book-ACC 
PP VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
VP X 
S 
NP?NP 
NP 
? 
store 
NP 
NP?NP 
?    ? 
book-ACC 
NPo 
S 
S?NPo 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
?a store wKere (,) EougKt tKe EooN? 
?a EooN wKiFK (,) EougKt? 
Figure 13: A relative clause with/without argu-
ment extraction (upper/lower, respectively).
of the verb so that the semantic argument corre-
sponding to the deep case is co-indexed with the
argument NP. These restrictions are then utilized
for PAS construction in Sec. 3.2.3.
Relative clauses A relative clause can be de-
tected as a subtree that has a VP as its left child
and an NP as its right child, as shown in Fig. 13.
The conversion of the subtree consists of 1) in-
serting a node on the top of the left VP (see the
right-hand side of Fig. 13), and 2) assigning the
appropriate unary rule to make the new node. The
difference between candidate rules RelExt and Re-
lIn (see Fig. 4) is whether the right-hand NP is
an obligatory argument of the VP or not, which
can be determined by the pred-arg annotation on
the predicate in the VP. In the upper example in
Fig. 13, RelIn is assigned because the right NP
?book? is annotated as an accusative argument of
the predicate ?buy?. In contrast, RelExt is as-
signed in the lower side in the figure because the
right NP ?store? is not annotated as an argument.
Continuous clauses A continuous clause can be
detected as a subtree with a VP of continuous form
as its left child and a VP as its right child. Its
conversion is similar to that of a relative clause,
and only differs in that the candidate rules are Con
and ConCoord. ConCoord generates a continu-
ous clause that shares arguments with the main
clause while Con produces one without shared ar-
guments. Rule assignment is done by comparing
the pred-arg annotations of the two phrases.
1048
Training Develop. Test
#Sentences 24,283 4,833 9,284
#Chunks 234,685 47,571 89,874
#Words 664,898 136,585 255,624
Table 4: Statistics of input linguistic resources.
3.2.2 Inverse application of rules
The second procedure in Step 2 begins with as-
signing a category S to the root node. A combi-
natory rule assigned to each branching is then ?in-
versely? applied so that the constraint assigned to
the parent transfers to the children.
3.2.3 Constraints on terminal nodes
The final process consists of a) imposing restric-
tions on the terminal category in order to instan-
tiate all the feature values, and b) constructing a
PAS for each verbal terminal. An example of pro-
cess a) includes setting the form features in the
verb category, such as S\NPni, according to the
inflection form of the verb. As for b), arguments
in a PAS are given according to the category and
the partial restriction. For instance, if a category
S\NPni is obtained for ???/inquire? (Fig. 12),
the PAS for ?inquire? is unary because the cate-
gory has one argument category (NPni), and the
category is co-indexed with the semantic argument
ga in the PAS due to the partial restriction depicted
in Sec. 3.2.1. As a result, a lexical entry is ob-
tained as?? ` S\NPni[1]: inquire([1]).
3.3 Lexical entries
Finally, lexical rules are applied to each of the ob-
tained lexical entries in order to reduce them to
the canonical form. Since words in the corpus (es-
pecially verbs) often involve pro-drop and scram-
bling, there are a lot of obtained entries that have
slightly varied categories yet share a PAS. We as-
sume that an obtained entry is a variation of the
canonical one and register the canonical entries in
the lexicon. We treat only subject deletion for pro-
drop because there is not sufficient information to
judge the deletion of other arguments. Scrambling
is simply treated as permutation of arguments.
4 Evaluation
We used the following for the implementation of
our resources: Kyoto corpus ver. 4.02, NAIST text
2http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?
Kyoto\%20University\%20Text\%20Corpus
Training Develop. Test
St.1 St.2 St.1 St.2 St.1 St.2
Sent. 24,283 24,116 4,833 4,803 9,284 9,245
Converted 24,116 22,820 4,803 4,559 9,245 8,769
Con. rate 99.3 94.6 99.4 94.9 99.6 94.9
Table 5: Statistics of corpus conversion.
Sentential Coverage
Covered Uncovered Cov. (%)
Devel. 3,920 639 85.99
Test 7,610 1,159 86.78
Lexical Coverage
Word Known Unknown
combi. cat. word
Devel. 127,144 126,383 682 79 0
Test 238,083 236,651 1,242 145 0
Table 6: Sentential and lexical coverage.
corpus ver. 1.53, and JP corpus ver. 1.04. The
integrated corpus is divided into training, devel-
opment, and final test sets following the standard
data split in previous works on Japanese depen-
dency parsing (Kudo and Matsumoto, 2002). The
details of these resources are shown in Table 4.
4.1 Corpus conversion and lexicon extraction
Table 5 shows the number of successful conver-
sions performed by our method. In total, we ob-
tained 22,820 CCG derivations from 24,283 sen-
tences (in the training set), resulting in the to-
tal conversion rate of 93.98%. The table shows
we lost more sentences in Step 2 than in Step 1.
This is natural because Step 2 imposed more re-
strictions on resulting structures and therefore de-
tected more discrepancies including compounding
errors. Our conversion rate is about 5.5 points
lower than the English counterpart (Hockenmaier
and Steedman, 2007). Manual investigation of the
sampled derivations would be beneficial for the
conversion improvement.
For the lexicon extraction from the CCGbank,
we obtained 699 types of lexical categories from
616,305 word tokens. After lexical reduction, the
number of categories decreased to 454, which in
turn may produce 5,342 categories by lexical ex-
pansion. The average number of categories for a
word type was 11.68 as a result.
4.2 Evaluation of coverage
Following the evaluation criteria in (Hockenmaier
and Steedman, 2007), we measured the coverage
3http://cl.naist.jp/nldata/corpus/
4https://alaginrc.nict.go.jp/resources/tocorpus/
tocorpusabstract.html
1049
of the grammar on unseen texts. First, we obtained
CCG derivations for evaluation sets by applying
our conversion method and then used these deriva-
tions as gold standard. Lexical coverage indicates
the number of words to which the grammar assigns
a gold standard category. Sentential coverage indi-
cates the number of sentences in which all words
are assigned gold standard categories 5.
Table 6 shows the evaluation results. Lexical
coverage was 99.40% with rare word treatment,
which is in the same level as the case of the En-
glish CCG parser C&C (Clark and Curran, 2007).
We also measured coverage in a ?weak? sense,
which means the number of sentences that are
given at least one analysis (not necessarily cor-
rect) by the obtained grammar. This number was
99.12 % and 99.06 % for the development and the
test set, respectively, which is sufficiently high for
wide-coverage parsing of real-world texts.
4.3 Evaluation of parsing accuracy
Finally, we evaluated the parsing accuracy. We
employed the parser and the supertagger of
(Miyao and Tsujii, 2008), specifically, its gen-
eralized modules for lexicalized grammars. We
trained log-linear models in the same way as
(Clark and Curran, 2007) using the training set as
training data. Feature sets were simply borrowed
from an English parser; no tuning was performed.
Following conventions in research on Japanese de-
pendency parsing, gold morphological analysis re-
sults were input to a parser. Following C&C, the
evaluation measure was precision and recall over
dependencies, where a dependency is defined as a
4-tuple: a head of a functor, a functor category, an
argument slot, and a head of an argument.
Table 7 shows the parsing accuracy on the de-
velopment and the test sets. The supertagging ac-
curacy is presented in the upper table. While our
coverage was almost the same as C&C, the perfor-
mance of our supertagger and parser was lower.
To improve the performance, tuning disambigua-
tion models for Japanese is a possible approach.
Comparing the parser?s performance with previ-
ous works on Japanese dependency parsing is dif-
ficult as our figures are not directly comparable
to theirs. Sassano and Kurohashi (2009) reported
the accuracy of their parser as 88.48 and 95.09
5Since a gold derivation can logically be obtained if gold
categories are assigned to all words in a sentence, sentential
coverage means that the obtained lexicon has the ability to
produce exactly correct derivations for those sentences.
Supertagging accuracy
Lex. Cov. Cat. Acc.
Devel. 99.40 90.86
Test 99.40 90.69
C&C 99.63 94.32
Overall performance
LP LR LF UP UR UF
Devel. 82.55 82.73 82.64 90.02 90.22 90.12
Test 82.40 82.59 82.50 89.95 90.15 90.05
C&C 88.34 86.96 87.64 93.74 92.28 93.00
Table 7: Parsing accuracy. LP, LR and LF refer to
labeled precision, recall, and F-score respectively.
UP, UR, and UF are for unlabeled.
in unlabeled chunk-based and word-based F1 re-
spectively. While our score of 90.05 in unlabeled
category dependency seems to be lower than their
word-based score, this is reasonable because our
category dependency includes more difficult prob-
lems, such as whether a subject PP is shared by
coordinated verbs. Thus, our parser is expected to
be capable of real-world Japanese text analysis as
well as dependency parsers.
5 Conclusion
In this paper, we proposed a method to induce
wide-coverage Japanese resources based on CCG
that will lead to deeper syntactic analysis for
Japanese and presented empirical evaluation in
terms of the quality of the obtained lexicon and
the parsing accuracy. Although our work is basi-
cally in line with CCGbank, the application of the
method to Japanese is not trivial due to the fact that
the relationship between chunk-based dependency
structures and CCG derivations is not obvious.
Our method integrates multiple dependency-
based resources to convert them into an integrated
phrase structure treebank. The obtained treebank
is then transformed into CCG derivations. The
empirical evaluation in Sec. 4 shows that our cor-
pus conversion successfully converts 94 % of the
corpus sentences and the coverage of the lexicon
is 99.4 %, which is sufficiently high for analyz-
ing real-world texts. A comparison of the parsing
accuracy with previous works on Japanese depen-
dency parsing and English CCG parsing indicates
that our parser can analyze real-world Japanese
texts fairly well and that there is room for improve-
ment in disambiguation models.
1050
References
Daisuke Bekki. 2010. Formal Theory of Japanese Syn-
tax. Kuroshio Shuppan. (In Japanese).
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING 2004, pages
1240?1246.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cate-
gorial grammar treebank for Italian. In Proceedings
of the Eighth International Workshop on Treebanks
and Linguistic Theories (TLT8), pages 27?38.
Johan Bos. 2007. Recognising textual entailment and
computational semantics. In Proceedings of Seventh
International Workshop on Computational Seman-
tics IWCS-7, page 1.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of ACL Stu-
dent Research Workshop, pages 73?78.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Takao Gunji. 1987. Japanese Phrase Structure Gram-
mar: A Unification-based Approach. D. Reidel.
Hiroki Hanaoka, Hideki Mima, and Jun?ichi Tsujii.
2010. A Japanese particle corpus built by example-
based annotation. In Proceedings of LREC 2010.
Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type.
In Proceedings of IJCNLP 2011, pages 201?209.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of the Joint Conference of COLING/ACL
2006.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of ACL-HLT 2011, pages 804?813.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text
corpus with predicate-argument and coreference re-
lations. In Proceedings of Linguistic Annotation
Workshop, pages 132?139.
Daisuke Kawahara and Sadao Kurohashi. 2011. Gen-
erative modeling of coordination by factoring paral-
lelism and selectional preferences. In Proceedings
of IJCNLP 2011.
Daisuke Kawahara, Sadao Kurohashi, and Koiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 495?498. (In Japanese).
Nobo Komagata. 1999. Information Structure in Texts:
A Computational Analysis of Contextual Appropri-
ateness in English and Japanese. Ph.D. thesis, Uni-
versity of Pennsylvania.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analyisis using cascaded chunking. In
Proceedings of CoNLL 2002.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction, 2nd
Edition. CSLI Publications.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of IJCNLP 2011.
Manabu Sassano and Sadao Kurohashi. 2009. A uni-
fied single scan algorithm for Japanese base phrase
chunking and dependency parsing. In Proceedings
of ACL-IJCNLP 2009.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL 2007, pages 240?247.
Emiko Yamada, Eiji Aramaki, Takeshi Imai, and
Kazuhiko Ohe. 2010. Internal structure of a disease
name and its application for ICD coding. Studies
in health technology and informatics, 160(2):1010?
1014.
Kazuhiro Yoshida. 2005. Corpus-oriented develop-
ment of Japanese HPSG parsers. In Proceedings of
the ACL Student Research Workshop.
1051
