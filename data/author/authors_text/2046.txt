Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101?112,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Design of a hybrid high quality machine translation system 
Kurt Eberle 
Johanna Gei? 
Mireia Ginest?-Rosell 
Bogdan Babych  
Anthony Hartley 
Reinhard Rapp  
Lingenio GmbH Serge Sharoff 
Karlsruher Stra?e 10 
69 126 Heidelberg, Germany 
Martin Thomas 
Centre for Translation Studies 
University of Leeds 
 Leeds, LS2 9JT, UK 
[k.eberle,j.geiss,m.ginesti-rosell] 
@lingenio.de 
[B.Babych,A.Hartley,R.Rapp, 
S.Sharoff,M.Thomas]@leeds.ac.uk  
  
 
 
Abstract 
This paper gives an overview of the 
ongoing FP7 project HyghTra (2010 ? 
2014). The HyghTra project is conducted 
in a partnership between academia and 
industry involving the University of Leeds 
and Lingenio GmbH (company). It adopts a 
hybrid and bootstrapping approach to the 
enhancement of MT quality by applying 
rule-based analysis and statistical 
evaluation techniques to both parallel and 
comparable corpora in order to extract 
linguistic information and enrich the lexical 
and syntactic resources of the underlying 
(rule-based) MT system that is used for 
analysing the corpora. The project places 
special emphasis on the extension of 
systems to new language pairs and 
corresponding rapid, automated creation of 
high quality resources. The techniques are 
fielded and evaluated within an existing 
commercial MT environment. 
1 Motivation 
Statistical Machine Translation (SMT) has been 
around for about 20 years, and for roughly half of 
this time SMT and the 'traditional' Rule-based 
Machine Translation (RBMT) have been seen as 
competing paradigms. During the last decade 
however, there is a trend and growing interest in 
combining the two methodologies. In our approach 
these two approaches are viewed as 
complementary. 
Advantages of SMT are low cost and robustness, 
but definite disadvantages of (pure) SMT are that it 
needs huge amounts of data, which for many 
language pairs are not available and are unlikely to 
become available in the future. Also, SMT tends to 
disregard important classificatory knowledge (such 
as morphosyntactic, categorical and lexical class 
features), which can be provided and used 
relatively easily within non-statistical 
representations.  
On the other hand, advantages of RBMT are that 
its (grammar and lexical) rules and information are 
understandable by humans and can be exploited for 
a lot of applications outside of translation 
(dictionaries, text understanding, dialogue systems, 
etc.).  
The slot grammar approach used in Lingenio 
systems (cf.  McCord 1989, Eberle 2001) is a 
prime example of such linguistically rich 
representations that can be used for a number of 
different applications. Fig.1 shows this by a 
visualization of (an excerpt of) the entry for the 
ambiguous German verb einstellen in the database 
that underlies (a)  the Lingenio translation 
products, where it links up with corresponding set 
of the transfer rules, and (b) Lingenio?s dictionary 
product TranslateDict, which is primarily intended 
for human translators.   
 
101
 
 
Fig 1 a) data base entry einstellen 
('translation' represents links between SL and T entries) 
 
 
 
 
Fig 1 b) product entry einstellen 
 
The obvious disadvantages of RBMT are high cost, 
weaknesses in dealing with incorrect input and in 
making correct choices with respect to ambiguous 
words, structures, and transfer equivalents. 
SMT output is often surprisingly good with respect 
to short distance collocations, but often misses 
correct choices are missed in cases where 
selectional restrictions take effect on distant words. 
RBMT output is generally good if the parser 
assigns the correct analysis to a sentence and  if the 
target words can be correctly chosen from the set 
of alternatives. However, in the presence of 
ambiguous words and structures, and where 
linguistic information is lacking, the decisions may 
be wrong. 
Given the complementarity of SMT and RBMT 
and their very different strengths and weaknesses, 
we take a view that an optimized MT architecture 
must comprise elements of both paradigms. The 
key issue therefore lies in the identification of such 
elements and how to connect them to each other. 
We propose a specific type of hybrid translation ? 
hybrid high quality translation (HyghTra), where 
core RBMT systems are created and enhanced by a 
range of reliable statistical techniques. 
 
2 Development Methodology 
Many hybrid systems described in the literature 
have attempted to put some analytical abstraction 
on top of an SMT kernel.1 In our view this is not 
the best option because, according to the 
underlying philosophy, SMT is linguistically 
ignorant at the beginning and learns all linguistic 
rules automatically from corpora. However, the 
extracted information is typically represented in 
huge data sets which are not readable by humans in 
a natural way. This means that this type of 
architecture does not easily provide interfaces for 
incorporating linguistic knowledge in a canonical 
and simple way. 
Thus we approach the problem from the other end, 
, integrating information derived from corpora 
using statistical methods into RBMT systems. 
Provided the underlying RBMT systems are 
linguistically sound and sufficiently modular in 
structure, we believe this to have greater potential 
for generating high quality output. 
We currently use and carry out the following work 
plan: 
 
(I) Creation of MT systems  
(with rule-based core MT information and 
statistical extension and training): 
(a) We start out with declarative analysis and 
generation components of the considered 
languages, and with basic bilingual dictionaries 
connecting to one another the entries of relatively 
small vocabularies comprising the most frequent 
words of each language in a given translation pair 
(cf. Fig 1 a). 
(b) Having completed this phase, we extend the 
dictionaries and train the analysis-, transfer- and 
generation-components of the rule-based core 
systems using monolingual and bilingual corpora.  
 
                                                           
1 A prominent early example is Frederking and 
colleagues (Frederking & Nirenburg, 1994). For an 
overview of  hybrid MT till the late nineties see Streiter 
et al (1999). More recent  approaches include Groves & 
Way (2006a, 2006b). Commercial implementations 
include AppTek (http://www.apptek.com) and Language 
Weaver (http://www.languageweaver.com). An ongoing 
MT important project investigating hybrid methods is 
EuroMatrixPlus (http://www.euromatrixplus.net/) 
102
(II) Error detection and improvement cycle:  
(a) We automatically discover the most frequent 
problematic grammatical constructions and 
multiword expressions for commercial RBMT and 
SMT systems using automatic construction-based 
evaluation as proposed in (Babych and Hartley, 
2009) and develop a framework for fixing 
corresponding grammar rules and extending 
grammatical coverage of the systems in a semi-
automatic way. This shortens development time for 
commercial MT and contributes to yielding 
significantly higher translation quality. 
 
(III) Extension to other languages: 
Structural similarity and translation by pivot 
languages is used to obtain extension to further 
languages: 
High-quality translation between closely related 
languages (e.g., Russian and Ukrainian or 
Portuguese and Spanish) can be achieved with 
relatively simple resources (using linguistic 
similarity, but also homomorphism assumptions 
with respect to parallel text, if available), while 
greater efforts are put into ensuring better-quality 
translation between more distant languages (e.g. 
German and Russian). According to our prior 
research (Babych et al, 2007b) the pipeline 
between languages of different similarity results in 
improved translation quality for a larger number of 
language pairs (e.g., MT from Portuguese or 
Ukrainian into German is easier if there are high-
quality analysis and transfer modules for Spanish 
and Russian into German (respectively). Of course, 
(III) draws heavily on the detailed analysis and MT 
systems that the industrial partner in HyghTra 
provides for a number of languages. 
 
In the following sections we give more details of 
the work currently done with regard to (I) and with 
regard to parts of (II): the creation of a new MT 
system following the strategy sketched. We cannot 
go further into detail with (II) and (III) here, which 
will become a priority for future research. 
3 Creation of a new system 
Early pilot studies covering some aspects of the 
strategy described here (using information from 
pivot languages and similarity) showed promising 
results (Rapp, 1999; Rapp & Mart?n Vide, 2007; 
see also Koehn & Knight, 2002). 
We expect that the proposed semi-automatic 
creation of a new MT system as sketched above 
will work best if one of the two languages involved 
is already 'known' by modules to which the system 
has access. Against the background of the pipeline 
approach mentioned above in (III), this means that 
we assume an analysis and translation system that 
continuously grows by 'learning' new languages 
where 'learning' is facilitated by information about 
the languages already 'known' and by exploiting 
similarity assumptions ? and, of course, by being 
fed with information prepared and provided by the 
human 'companion' of the system. 
From this perspective, we assume the following 
steps of extending the system (with work done by 
the 'companion' and work done by the system) 
 
1. Acquire parallel and comparable corpora. 
2. Define a core of the morphology of the new 
language and compile a basic dictionary for the 
most frequent words and translations. 
Morphological representations and features for 
new languages are derived both manually and 
automatically, as proposed in (Babych et al, 
2012 (in preparation)). 
3. Using established alignment technology (e.g. 
Giza++) and parallel corpora, generate a first 
extension of this dictionary. 
4. Expand the dictionary of step 3 using 
comparable corpora as proposed in a study by 
Rapp (1999). This is applicable mainly to single 
word units. 
5. Expand coverage of multiword-units using 
novel technology. 
6. Cross-validate the new dictionary with respect 
to available ones by transitivity. 
7. Integrate the new dictionary into the new MT 
system as developing from reusing components 
and adding new components as in 8. 
8. Complete morphology and spell out declarative 
analysis and generation grammar for the new 
language. 
9. Automatically evaluate the translations of the 
most frequent grammatical constructions and 
multiword expressions in a machine-translated 
corpus, prioritising support for these 
constructions with a type of risk-assessment 
framework proposed in Babych and Hartley 
(2008). 
10. Extend support for high-priority constructions 
semi-automatically by mining correct 
103
translations from parallel corpora. 
11. Train and evaluate the new grammar and 
transfer of the new MT system using the new 
dictionary on the basis of available parallel 
corpora. 
 
The following sections give an overview of the 
different steps. 
Step 1: Acquire parallel and comparable 
corpora 
As our parallel corpus, we use the Europarl. The 
size of the current version is up to 40 million 
words per language, and several of the languages 
we are currently considering are covered. Also, we 
make use of other parallel corpora such as the 
Canadian Hansards (Proceedings of the Canadian 
Parliament) for the English?French language pair. 
For non-EU Languages (mainly Russian), we 
intend to conduct a pilot study to establish the 
feasibility of retrieving parallel corpora from the 
web, a problem for which various approaches have 
been proposed (Resnik, 1999; Munteanu & Marcu, 
2005; Wu & Fung, 2005).  
In addition to the parallel corpora, we will need 
large monolingual corpora in the future (at least 
200 million words) for each of the six languages. 
Here, we intend to use newspaper corpora 
supplemented with text collections downloadable 
from the web.  
The corpora are stored in a database that allows 
for assigning analyses of different depth and nature 
to the sentences and for alignment between the 
sentences and their analyses. The architecture of 
this database and the corresponding analysis and 
evaluation frontend is described in (Eberle et al
2010, 2012). Section Results contains examples of 
such representations. 
Step 2: Compile a basic dictionary for the most 
frequent words 
A prerequisite of the suggested hybrid approach 
with rule-based kernel is to define morphological 
classifications for the new language(s). This is 
done exploiting similarities to the classifications as 
available for the existing languages. Currently, this 
has been carried out for Dutch (on the basis of 
German) and for Spanish (on the basis of 
French/other Romance languages). The most 
frequent words (the basic vocabulary of a 
language) are typically also the most ambiguous 
ones. Since the Lingenio systems are lexically 
driven transfer systems (cf. Eberle 2001), we 
define (a) structural conditions,  which inform the 
choice of the possible target words (single words 
or multiword expressions) and (b)restructuring 
conditions, as necessary (cf. Fig 1 a:  attributes 
'transfer conditions' and 'structural change'). In 
order to ensure quality this must be done by human 
lexicographers and therefore costly for a large 
dictionary. However, we manually create only very 
small basic dictionaries and extend these (semi-
automatically) step 3 and those which follow. 
Some important morphosyntactic features of the 
language are derived from a monolingual corpus 
annotated with publicly available part-of-speech 
taggers and lemmatisers. However, these tools 
often do not explicitly represent linguistic features 
needed for the generation stage in RBMT. In 
(Babych et al, 2012) we propose a systematic 
approach to recovering such missing generation-
oriented representations from grammar models and 
statistical combinatorial properties of annotated 
features. 
Step 3: Generating dictionary extensions from 
parallel corpora 
Based on parallel corpora, dictionaries can be 
derived using established techniques of automatic 
sentence alignment and word alignment. For 
sentence alignment, the length-based Gale & 
Church aligner (1993) can be used, or ? 
alternatively ? Dan Melamed?s GSA-algorithm 
(Geometric Sentence Alignment; Melamed, 1999).  
For segmentation of text we use corresponding 
Lingenio-tools (unpublished).2 
For word alignment Giza++ (Och & Ney, 2003) is 
the standard tool. Given a word alignment, the 
extraction of a (SMT) dictionary is relatively 
straightforward. With the exception of sentence 
segmentation, these algorithms are largely 
language independent and can be used for all of the 
languages that we consider. We did this for a 
number of language pairs on the basis of the 
                                                           
2  If these cannot be applied because of  lack of 
information about a language, we intend to use the 
algorithm by Kiss & Strunk (2006). An open-source 
implementation of parts of the Kiss & Strunk algorithm 
is available from Patrick Tschorn at 
http://www.denkselbst.de/sentrick/index.html. 
104
Europarl-texts considered (as stored in our 
database). In order to optimize the results we use 
the dictionaries of step 1 as set of cognates (cf. 
Simard at al 1992, Gough & Way 2004), as well as 
other words easily obtainable from the internet that 
can be used for this purpose (like company names 
and other named entities with cross-language 
identity and terminology translations). Using the 
morphology component of the new language and 
the categorial information from the transfer 
relation, we compute the basic forms of the 
inflected words found. Later, we intend to further 
improve the accuracy of word alignment by 
exploiting chunk type syntactic information of the 
narrow context of the words (cf. Eberle & Rapp 
2008). An early stage variant of this is already used 
in Lingenio products. The corresponding function 
AutoLearn<word> extracts new word relations on 
the basis of existing dictionaries and (partial) 
syntactic analyses. (Fig 2 gives an example). 
 
 
 
 
 
 
 
 
 
 
Fig 2 AutoLearn<word>: new entries using 
transfer links and syntactic analysis 
 
Given the relatively small size of the available 
parallel corpora, we expect that the automatically 
generated dictionaries will comprise about 20,000 
entries each (This corresponds to first results on 
the basis of German?English). This is far too 
small for a serious general purpose MT system. 
Note that, in comparison, the English?German 
dictionary used in the current Lingenio MT 
product comprises more than 480,000 keywords 
and phrases. 
Step 4: Expanding dictionaries using 
comparable corpora (word equations) 
In order to expand the dictionaries using a set of 
monolingual comparable corpora, the basic 
approach pioneered by Fung & McKeown (1997) 
and Rapp (1995, 1999) is to be further developed 
and refined in the second phase of the project as to 
obtain a practical tool that can be used in an 
industrial context. 
The basic assumption underlying the approach 
is that across languages there is a correlation 
between the co-occurrences of words that are 
translations of each other. If ? for example ? in a 
text of one language two words A and B co-occur 
more often than expected by chance, then in a text 
of another language those words that are 
translations of A and B should also co-occur more 
frequently than expected. It is further assumed that 
a small dictionary (as generated in step 2) is 
available at the beginning, and that the aim is to 
expand this basic lexicon. Using a corpus of the 
target language, first a co-occurrence matrix is 
computed whose rows are all word types occurring 
in the corpus and whose columns are all target 
words appearing in the basic lexicon. Next a word 
of the source language is considered whose 
translation is to be determined. Using the source-
language corpus, a co-occurrence vector for this 
word is computed. Then all known words in this 
vector are translated to the target language. As the 
basic lexicon is small, only some of the 
translations are known. All unknown words are 
discarded from the vector and the vector positions 
are sorted in order to match the vectors of the 
target-language matrix. Using standard measures 
for vector similarity, the resulting vector is 
compared to all vectors in the co-occurrence 
matrix of the target language. The vector with the 
highest similarity is considered to be the 
translation of our source-language word. 
From a previous pilot study (Rapp, 1999) it can 
be expected that this methodology achieves an 
accuracy in the order of 70%, which means that 
only a relatively modest amount of manual post-
editing is required.  
The automatically generated results are 
improved and the amount of post-editing is 
reduced by exploiting sense (disambiguation) 
information as available from the analysis 
component for the 'known' language of the new 
language pair.. Also we try to exploit categorial 
and underspecified syntactic information of the 
contexts of the words similar to what has been 
suggested for improving word alignment in the 
previous step (see also Fig.2). Also, as the frequent 
words are already covered by the basic lexicon 
(whose production from parallel corpora on the 
basis of a manually compiled kernel does not show 
 
105
an ambiguity problem of similar significance), and 
as experience shows that most low frequency 
words in a full-size lexicon tend to be 
unambiguous, the ambiguity problem is reduced 
further for the words investigated and extracted by 
this comparison method. 
Step 5: Expanding dictionaries using 
comparable corpora (multiword units) 
In order to account for technical terms, idioms, 
collocations, and typical short phrases, an 
important feature of an MT lexicon is a high 
coverage of multiword units. Very recent work 
conducted at the University of Leeds (Sharoff et 
al., 2006) shows that dictionary entries for such 
multiword units can be derived from comparable 
corpora if a dictionary of single words is available. 
It could even be shown that this methodology can 
be superior to deriving multiword-units from 
parallel corpora (Babych et al, 2007). This is a 
major breakthrough as comparable corpora are far 
easier to acquire than parallel corpora. It even 
opens up the possibility of building domain-
specific dictionaries by using texts from different 
domains. 
The outline of the algorithm is as follows: 
? Extract collocations from a corpus of the 
source language (Smadja, 1993) 
? To translate a collocation, look up all its 
words using any dictionary 
? Generate all possible permutations 
(sequences) of the word translations 
? Count the occurrence frequencies of these 
sequences in a corpus of the target 
language and test for significance 
? Consider the most significant sequence to 
be the translation of the source language 
collocation 
Of course, in later steps of the project, we will 
experiment on filtering these sequences by 
exploiting structural knowledge similarly to what 
was described in the two previous steps. This can 
be obtained on the basis of the declarative analysis 
component of the new language which is 
developed in parallel. 
Step 6: Cross-validate dictionaries 
The combination of the corpus-based methods for 
automatic dictionary generation as described in 
steps 3 to 5 will lead to high coverage dictionaries 
as the availability of very large monolingual 
corpora is no major problem for our languages. 
However, as all steps are error prone, it can be 
expected that a considerable number of dictionary 
entries (e.g. 50%) are not correct. To facilitate (but 
not eliminate) the manual verification of the 
dictionary, we will  perform an automatic cross-
check which utilizes the dictionaries? property of 
transitivity. What we mean by this is that if we 
have two dictionaries, one translating from 
language A to language B, the other from language 
B to language C, then we can also translate from 
language A to C by use of the intermediate 
language (or interlingua) B. That is, the property of 
transitivity, although having some limitations due 
to ambiguity problems, can be exploited to 
automatically generate a raw dictionary for A to C. 
Lingenio  has some experience with this method 
having exploited it for extending and improving its 
English ? French dictionaries using French ? 
German and German ? English. 
As the corpus-based approach (steps 3 to 5) 
allows us to also generate this type of dictionary  
via comparable corpora, we have two different 
ways to generate a dictionary for a particular 
language pair. This means that we can validate one 
with the other. Furthermore, with increasing 
number of language pairs created, there are more 
and more languages that can serve as interlingua or 
'pivot': This, step by step, gives an increasing 
potential for mutual cross-validation.  
Specific attention will be paid to automating as 
far as possible the creation of selectional 
restrictions to be assigned to the transfer relations 
of the new dictionaries in all steps of dictionary 
creation (2?6). We will try to do this on the basis 
of the analysis components as available for the 
languages considered: These are: a completely 
worked out analysis component for the 'old' 
language, a declarative (chunk parsing) component 
for the new one (compare the two following steps 
for this).  
Step 7: Integrate dictionaries in existing 
machine translation systems 
Lingenio has a relatively rich infrastructure for 
automatic importation of various kinds of lexical 
information into the database used by the analyses 
and translation systems. If necessary the 
information on hand (for instance from 
conventional dictionaries of publishing houses) is 
106
completed and normalized during or before 
importation. This may be executed completely 
automatically ? by using the existing analyses 
components and resources respectively as 
databases ? or interactively ? by asking the 
lexicographer for additional information, if needed. 
For example, there may be a list of multiword 
expressions to be imported into the database. In 
order to have available correct syntactic and 
semantic information for these expressions, they 
are analysed by the parser of the corresponding 
language. From the analysis found, the information 
necessary to describe the new lemma in the lexicon 
with respect to semantic type and syntactic 
structure is obtained. The same information is used 
to automatically create correct restructuring 
constraints for translation relations which use the 
new lemma as target. If the parser does not find a 
sound syntactic description, for example because 
some basic information or the expression is 
missing in the lexical database, the lexicographer is 
asked for the missing information or is handed 
over the expression to code it manually.  
Using these tools importation of new lexical 
information, as provided in the previous steps, is 
considerably accelerated.  
Step 8: Compile rule bases for new language 
pairs 
Although experience clearly shows that 
construction and maintenance of the dictionaries is 
by far the most expensive task in (rule-based) 
Machine Translation, the grammars (analysis and 
generation) must of course be developed and 
maintained also. Lingenio has longstanding 
experience with the development of grammars, 
dictionaries and all other components of RBMT.  
The used grammar formalism (slot grammar, 
cf. McCord 1991) is unification based and its 
structuring focuses on dependency, where phrases 
are analysed into heads and grammatical roles ? so 
called (complement and adjunct) slots.  
The grammar formalism and basic rule types 
are designed in a very general way in order to 
allow good portability from one language to 
another such that spelling out the declarative part 
of a grammar does not take very much time (2-4 
person months approx. for relatively similar 
languages like Romance languages according to 
our experience). The portation of linguistic rules to 
new languages is also facilitated by the modular 
design with clearly defined interfaces that make it 
relatively straightforward to integrate information 
from corpora. 
Given a parallel corpus as acquired in step 1, 
the following procedure defines grammar develop-
ment:  
 
1. Define a declarative grammar for the new 
language and train this grammar on the parallel 
-corpus according to the following steps: 
2. Use a chunk parser for the grammar on the 
basis of an efficient part-of-speech tagger for 
the new language.  
3. Combine the chunk analyses of the sentence, 
according to suggestions for packed syntactic 
structures (cf. Schiehlen 2001 and others) and 
underspecified representation structures 
respectively (cf. Eberle, 2004, and others), 
such that the result represents a disjunction of 
the possible analyses of the sentence. 
4. Filter the alternatives of the representation by 
using mapping constraints between source and 
target sentence as can be computed from the 
lexical transfer relations and the structural 
analysis of the sentence. For instance, if we 
know, as in the example of the last section, that 
in the source sentence there is a relative clause 
with lexical elements A, B, . . . modifying a 
head H and that there are translations TH, TA, 
TB, . . . of H, A, B,. . . , in the target sentence 
which, among other possibilities, can be 
supposed to stand in a similar structural 
relation there, then we prefer this relation to 
the competing structural possibilities. (Fig. 3 in 
section results shows the corresponding 
selection for a German-Spanish example in the 
project database). 
5. For each of the remaining structural 
possibilities of the thus revised underspecified 
representation, take its lexical material and 
underspecified structuring as a context for its 
successful firing. For instance, if the 
possibility is left that O is the direct object of 
VP, where VP is an underspecified verbal 
phrase and O an underspecified nominal 
phrase (i.e. where details of the substructuring 
are not spelled out), take the sentence as a 
reference for direct object complementation 
and O and VP as contexts which accept this 
complementation. 
107
6. Develop more abstract conditions from the 
conditions learned according to (5) and 
integrate the different cases. 
7. Tune the results using standard methods of 
corpus-based linguistics. Among other things 
this means: Distinguish between training and 
test corpora, adjust weights according to the 
results of test runs, etc. 
 
The basic idea of the proposed learning procedure 
is similar to that used with respect to learning 
lexical transfer relations: Do not define the 
statistical model for the ?ignorant? state, where the 
surface items of the bilingual corpora are 
considered. Instead, define it for appropriate 
maximally abstract analyses of the sentences 
(which, of course, must be available 
automatically), because, then, much smaller sets of 
data will do. Here, the important question is: What 
is the most abstract level of representation that can 
be reached automatically and which shows reliable 
results? We think that it is the level of 
underspecified syntactic description as used in the 
procedure above. 
The result of training the grammar is a set of 
rules which assign weights and contexts to each 
filler rule of the declarative grammar and thus 
allow to estimate how likely it is that a particular 
rule is applied in a particular context in comparison 
with other rules (Fig. 4 and 5 in section results 
give an overview of the relevance of  grammar 
rules and their triggering conditions w.r.t. 
German).  
We mentioned that the task of translating texts 
into each other does not presuppose that each 
ambiguity in a source sentence is resolved. On the 
contrary, translation should be ambiguity 
preserving (cf. Kay, Gawron & Norvig 1994, 
compare the example above). It is obvious that 
underspecified syntactic representations as 
suggested here are also especially suited for 
preserving ambiguities appropriately.  
Step 9: Automatically evaluate translations of 
the most frequent grammatical constructions 
and multiword expressions in a machine-
translated corpus 
In a later work package of the project, we will run 
a large parallel corpus through available 
(competitive) MT engines, which will be enhanced 
by automatic dictionaries developed during the 
previous stages. On the source-language side of the 
corpus we will automatically generate lists of 
frequent multiword expressions (MWEs) and 
grammatical constructions using the methodology 
proposed in (Sharoff et al, 2006). For each of the 
identified MWEs and constructions we will 
generate a parallel concordance using open-source 
CSAR architecture developed by the Leeds team 
(Sharoff, 2006). The concordance will be 
generated by running queries to the sentence-
aligned parallel corpora and will return lists of 
corresponding sentences from gold-standard 
human translations and corresponding sentences 
generated by MT. Each of these concordances will 
be automatically evaluated using standard MT 
evaluation metrics, such as BLEU. Under these 
settings parallel concordances will be used as 
standard MT evaluation corpora in an automated 
MT evaluation scenario. 
Normally BLEU gives reliable results for MT 
corpora over 7000 words. However, in (Babych 
and Hartley, 2009; Babych and Hartley, 2008) we 
demonstrated that if the corpus is constructed in 
this controlled way, where evaluated fragments of 
sentences are selected as local contexts for specific 
multiword expressions or grammatical 
constructions, then BLEU scores have another 
?island of stability? for much smaller corpora, 
which now may consist of only five or more 
aligned concordance lines. This concordance-based 
evaluation scenario gives correct predictions of 
translation quality for the local context of each of 
the evaluated expressions. 
The scores for the evaluated MWEs and 
constructions will be put in a risk-assessment 
framework, where we will balance the frequency 
of constructions and their translation quality. The 
top priority receive the most frequent expressions 
that are the most problematic ones for a particular 
MT engine, i.e., with queries with lowest BLEU 
scores for their concordances. This framework will 
allow MT developers to work down the priority list 
and correct or extend coverage for those 
constructions which will have the biggest impact 
on MT quality. 
Step 10: Extend support for high-priority 
constructions semi-automatically by mining 
correct translations from parallel corpora 
At this stage we will automate the procedure of 
correcting errors and extending coverage for 
108
problematic MWEs and grammatical 
constructions, identified in Step 9. For this we will 
exploit alignment between source-language 
sentences and gold-standard human translations. In 
the target human translations we will identify 
linguistically-motivated multiword expressions, 
e.g., using part-of-speech patterns or tf-idf 
distribution templates (Babych et al, 2007) and 
run standard alignment tools (e.g., GIZA++) for 
finding the most probable candidate MWEs that 
correspond to the problematic source-language 
expressions. Source and target MWEs paired in 
this way will form the basis for automatically-
generated grammar rules. The rules will normally 
generalise several pairs of MWEs, and may be 
underspecified for certain lexical or morphological 
features. Later such rules will be manually checked 
and corrected by language specialists in MT 
development teams that work on specific 
translation directions. 
This procedure will allow to speed up the grammar 
development procedure for large-scale MT projects 
and will focus on grammatical constructions with 
the highest impact on MT quality, establishing 
them as a top priority for MT developers. In 
HyghTra and with respect to the languages 
considered there, this procedure will be integrated 
into the grammar development and optimization of 
step 8, in particular it will be related to step 4 of 
the procedure sketched there. With regard to 
integration, we aim at an interleaved architecture in 
the long run.  
Step 11: Bootstrap the system 
In Step 11, the new grammar and the transfer of 
the new MT system and the new dictionary may be 
mutually trained further using the steps before and 
applying the system to additional corpora. 
 
4 Results 
Declarative slot grammars for Dutch and Spanish 
have been developed using the patterns of German 
and French ? where declarative  means that there 
has been used no relevant semantic or other 
information in order to spell out weighting or 
filters for rule application -- the only constraint 
being morphosyntactic accessibility. The necessary 
morphological information has been adapted 
similarly from the corresponding model languages. 
The basic dictionaries have been compiled 
manually (Dutch) or extracted from a conventional 
electronic dictionary (translateDict Spanish).  
For a subset of the Spanish corpus (reference 
sentences of the grammar, parts of the open source 
Leeds corpus (Sharoff, 2006), and Europarl), 
syntactic analyses have been computed and stored 
in the database. As the number of analyses grows 
extremely with the length of sentences, only 
relatively short sentences (up to 15 words)  have 
been considered. These analyses are currently 
compared to the analyses of the German 
translations of the corresponding sentences (one 
translation per sentence), which are taken as a kind 
of 'gold' standard as the German analysis 
component (as part of the translation products) has 
proven to be sufficiently reliable. On the basis of 
the comparison a preference on the competitive 
analyses of the Spanish sentence is entailed and 
used for defining a statistical evaluation 
component for the Spanish grammar. Fig.3 shows 
the corresponding representations in the database 
for the sentence Aumenta la demana de energ?a 
el?ctrica por la ola de calor3  and its translation die 
Nachfrage nach Strom steigt wegen der 
Hitzewelle/the demand for electricity increases 
because of the heat-wave. 
 
 
 
 
 
 
 
 
 
 
Fig.3 Selection of analyses via correspondences 
(prefer first Spanish analysis because of subj-congruity) 
 
The analyses are associated with the corresponding 
creation protocols, which are structured lists whose 
items describe, via the identifiers, which rule has 
been applied when and to what structures in the 
process of creating the analysis. From the selection 
of a best analysis for a sentence, we can entail the 
circumstances under which the application of 
particular rules are preferred. This has been carried 
                                                           
3 Sentence taken from the online newspaper El D?a de 
Concepci?n del Uruguay 
 
 
109
out - not yet for the 'new' language Spanish, but for 
the 'known' language German, in order to obtain a 
measure about how correctly the existing grammar 
evaluation component can be replaced by the 
results of the corresponding statistical study.  
 
Fig.4  Frequency of applications of rules 
 
 
 cluster 
applications 
similarity feas  mod feas head 
383, 384,.. 0,86 sent, ... emosentaffv,.. 
557,558,566,.. 0,68 denselb,.. gebv, ... 
 
Fig.5  Preliminary constraints related to grammar 
rule clusters 
 
Fig.4 shows the distribution of rule usages within 
the training set of analyses (of approx.30.000 
sentences). 390 different rules were used with a 
total of 133708 rule applications. The subject rule 
(383) and the noun determiner rule (46) the most 
used rules (35% of all applications). Fig 5. 
illustrates the preliminary results of a clustering 
algorithm where different rule applications are 
grouped into clusters and the key features of the 
head and modifier phrases for each cluster are 
extracted. 
Currently, we try to determine further and tare 
the linguistic features and the weighting which 
models best the evaluation for German. (The gold 
standard that is used in this test is the set of 
analyses mentioned above). The investigations are 
not yet completed, but preliminary results on the 
basis of the morphosyntactic and semantic 
properties of the neighboring elements are 
promising. After consolidation, the findings will be 
transferred to Spanish on the basis of the selection 
procedure illustrated in Fig. 3. The next step of 
grammar training in the immediate future will 
consist of  changing the focus to underspecified 
analyses as described in step 8 
5 Conclusions 
The project tries to make state-of-the-art statistical 
methods available for dictionary development and 
grammar development for a rule-based dominated 
industrial setting and to exploit such methods 
there.  
With regard to SMT dictionary creation, it goes 
beyond the current state of the art as it also aims at 
developing and applying algorithms for the semi-
automatic generation of bilingual dictionaries from 
unrelated monolingual (i.e., comparable) corpora 
of the source and the target language, instead of 
using relatively literally translated (i.e., parallel) 
texts only. Comparable corpora are far easier to 
obtain than parallel corpora. Therefore the 
approach offers a solution to the serious data 
acquisition bottleneck in SMT. This approach is 
also more cognitively plausible than previous 
suggestions on this topic, since human bilinguality 
is normally not based on memorizing parallel texts. 
Our suggestion models human capacity to translate 
texts using linguistic knowledge acquired from 
monolingual data, so it also exemplifies many 
more features of a truly self-learning MT system 
(shared also by a human translator).  
In addition, the proposal suggests a new 
method for spelling out grammars and parsers for 
languages by splitting grammars into declarative 
kernels and trainable decision algorithms and by 
exploiting cross-linguistic knowledge for 
optimizing the results of the corresponding parsers.   
For developing different components and 
dictionaries for the system a bootstrapping 
architecture is suggested that uses the acquired 
lexical information for training the grammar of the 
new language, which in turn uses the 
(underspecified) parser results for optimizing the 
lexical information in the corresponding translation 
dictionaries. We expect that the suggested methods 
significantly improve translation quality and 
reduce the costs of creating new language pairs for 
Machine Translation. The preliminary results 
obtained so far in the project appear promising. 
6 Acknowledgments 
This research is supported by a Marie Curie IAPP 
project taking place within the 7th European 
Community Framework Programme (Grant 
agreement no.: 251534) 
110
7 References 
Armstrong, S.; Kempen, M.; McKelvie, D.; Petitpierre, D.; 
Rapp, R.; Thompson, H. (1998). Multilingual Corpora 
for Cooperation. Proceedings of the 1st International 
Conference on Linguistic Resources and Evaluation 
(LREC), Granada, Vol. 2, 975?980. 
Babych, B., Hartley, A., Sharoff S.; Mudraya, O. (2007). 
Assisting Translators in Indirect Lexical Transfer. 
Proceedings of the 45th Annual Meeting of the ACL.  
Babych, B., Anthony Hartley, & Serge Sharoff (2007b) 
Translating from under-resourced languages: 
comparing direct transfer against pivot translation. 
Proceedings of MT Summit XI, 10-14 September 
2007, Copenhagen, Denmark, 29-35 
Babych, B. & Hartley, A. (2008). Automated MT Evaluation 
for Error Analysis: Automatic Discovery of Potential 
Translation Errors for Multiword Expressions. ELRA 
Workshop on Evaluation ?Looking into the Future of 
Evaluation: When automatic metrics meet task-based  
and performance-based approaches?. Marrakech, 
Morocco 27 May 2008. Proceedings of LREC?08. 
Babych, B. and Hartley, A. (2009). Automated error analysis 
for multiword expressions: using BLEU-type scores 
for automatic discovery of potential translation errors. 
Linguistica Antverpiensia, New Series (8/2009): 
Journal of translation and interpreting studies. Special 
Issue on Evaluation of Translation Technology. 
Babych, B., Babych, S. and Eberle, K. (2012). Deriving 
generation-oriented MT resources from corpora: case 
study and evaluation of de/het classification for Dutch 
Noun (in preparation) 
Baroni, M.; Bernardini, S. (2004). BootCaT: Bootstrapping 
corpora and terms from the web. Proceedings of 
LREC 2004.  
Callison-Burch, C., Miles Osborne, & Philipp Koehn: Re-
evaluating the role of BLEU in machine translation 
research. EACL-2006: 11th Conference of the 
European Chapter of the Association for 
Computational Linguistics, Trento, Italy, April 3-7, 
2006; pp.249-256  
Charniak, E.; Knight, K.; Yamada, K. (2003). Syntax-based 
language models for statistical machine translation". 
Proceedings of MT Summit IX. 
Eberle, Kurt (2001). FUDR-based MT, head switching and the 
lexicon. Proceedings of the the eighth Machine 
Translation Summit, Santiage de Compostela.  
Eberle, Kurt (2004). Flat underspecified representation and its 
meaning for a fragment of German. 
Habilitationsschrift, Universit?t Stuttgart. 
Eberle, K.; Rapp, R. (2008). Rapid Construction of 
Explicative Dictionaries Using Hybrid Machine 
Translation. In: Storrer, A.;  Geyken, A.; Siebert, A.; 
W?rzner, K._M (eds.) Text Resources and Lexical 
Knowledge: Selected Papers from the 9th Conference 
on Natural Language Processing KONVENS 2008. 
Berlin: Mouton de Gruyter..  
Eckart,K., Eberle, K.; Heid, U. (2010) An infrastructure for 
more reliable corpus analysis. Proceedings of the 
Workshop on Web Services and Processing Pipelines 
in HLT of LREC-2010 , Valetta. 
Eberle, K.; Eckart,K., Heid, U.,Haselbach, B. (2012) A 
tool/database interface for multi-level analyses. 
Proceedings of LREC-2012 , Istanbul. 
Frederking, R.; Nirenburg, S.; Farwell, D.;  Helmreich, S.; 
Hovy, E.; Knight, K.; Beale, S.; Domashnev, C.; 
Attardo, D.; Grannes, D.; Brown, R. (1994). Integrated 
Translation from Multiple Sources within the Pangloss 
MARK II Machine Translation System. Proceedings 
of Machine Translation of the Americas, 73?80. 
Frederking, Robert and Sergei Nirenburg (1994). Three heads 
are better than one. In: Proceedings of ANLP-94, 
Stuttgart, Germany.  
Fung, P.; McKeown, K. (1997). Finding terminology 
translations from non-parallel corpora. Proceedings of 
the 5th Annual Workshop on Very Large Corpora, 
Hong Kong: August 1997, 192-202.  
Gale, W.A.; Church, K.W. (1993). A progrm for aligning 
sentences in bilingual corpora. Computational 
Linguistics, 19(1), 75?102. 
Gonz?lez, J.; Antonio L. Lagarda, Jos? R. Navarro, Laura 
Eliodoro, Adri? Gim?nez, Francisco Casacuberta, Joan 
M. de Val and Ferran Fabregat (2004). SisHiTra: A 
Spanish-to-Catalan hybrid machine translation system. 
Berlin: Springer LNCS. 
Gough, N., Way, A. (2004). Example-Based Controlled 
Translation. Proceedings of the Ninth Workshop of the 
European Association for Machine Translation, 
Valetta, Malta.  
Groves, D. & Way, A. (2006b). Hybridity in MT: Experiments 
on the Europarl Corpus. In Proceedings of the 11th 
Conference of the European Association for Machine 
Translation, Oslo, Norway, 115?124. 
Groves, D.; Way, A. (2006a). Hybrid data-driven models of 
machine translation. Machine Translation, 19(3?4). 
Special Issue on Example-Based Machine Translation. 
301?323. 
Habash, N.; Dorr, B. (2002). Handling translation 
divergences: Combining statistical and symbolic 
techniques in generation-heavy machine translation. 
Proceedings of AMTA-2002, Tiburon, California, 
USA. 
Kiss, T.; Strunk, J. (2006): Unsupervised multilingual 
sentence boundary detection. Computational 
Linguistics 32(4), 485?525. 
Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical 
Machine Translation. Proceedings of MT Summit X, 
Phuket, Thailand 
Koehn, P.; Knight, K. (2002). Learning a translation lexicon 
from monolingual corpora. In: Proceedings of ACL-02 
Workshop on Unsupervised Lexical Acquisition, 
Philadelphia PA. 
Language Industry Monitor (1992). Statistical methods 
gaining ground. In: Language Industry Monitor, 
September/October 1992 issue. 
111
McCord, M. (1989). A new version of the machine translation 
system LMT.  Journal of Literary and Linguistic 
Computing, 4, 218?299. 
McCord, M. (1991). The slot grammar system.  In: Wedekind, 
J., Rohrer, C.(eds): Unification in Grammar, MIT-
Press. 
Melamed, I. Dan (1999). Bitext maps and aligment via pattern 
recognition. Computational Linguistics, 25(1), 107?
130. 
Munteanu, D.S.; Marcu, D. (2005). Improving machine 
translation performance by exploiting non-parallel 
corpora. Computational Linguistics, 31(4), 477?504. 
Och, F.J.; Ney, H. (2002). Discriminative trainig and 
maximum entropy models for statistical machine 
translation. Proceedings of the  Annual Meeting of the 
Association for Computational Linguistics, 
Philadelphia, PA, 295?302.  
Och, F.J.; Ney, H. (2003). A systematic comparison of various 
statistical alignment models. Computational 
Linguistics, 29(1), 19?51. 
Papineni, K.; Roukos, S.; Ward, T.; Zhu, W. (2002). BLEU: A 
method for automatic evaluation of machine 
translation. In: Proceedings of the 40th Annual 
Meeting of the ACL, Philadelphia, PA, 311?318. 
Rapp, R. (1995). Identifying word translations in non-parallel 
texts. In: Proceedings of the 33rd Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, 1995, 320?322 
Rapp, R. (1999). Automatic identification of word translations 
from unrelated English and German corpora. In: 
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics 1999, College 
Park, Maryland. 519?526. 
Rapp, R. (2004). A freely available automatically generated 
thesaurus of related words. In: Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation (LREC), Lisbon, Vol. II, 
395?398. 
Rapp, R.; Martin Vide, C. (2007). Statistical machine 
translation without parallel corpora. In: Georg Rehm, 
Andreas Witt, Lothar Lemnitzer (eds.): Data 
Structures for Linguistic Resources and Applications. 
Proceedings of the Biennial GLDV Conference 2007. 
T?bingen: Gunter Narr. 231?240 
Resnik, R. (1999). Mining the web for bilingual text. 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics. 
Sato, S.; Nagao, M. (1990). Toward memory-based 
translation. Proceedings of COLING 1990, 247?252. 
Schiehlen, M. (2001) Syntactic Underspecification. In: Special 
Research Area 340 ? Final report, University of 
Stuttgart.  
Sharoff, S. (2006) Open-source corpora: using the net to fish 
for linguistic data. In International Journal of Corpus 
Linguistics 11(4), 435?462.  
Sharoff, S.; Babych, B.; Hartley, A. (2006). Using comparable 
corpora to solve problems difficult for human 
translators. In: Proceedings of COLING/ACL 2006, 
739?746.  
Sharoff, S. (2006). A uniform interface to large-scale 
linguistic resources. In Proceedings of the Fifth 
Language Resources and Evaluation Conference, 
LREC-2006, Genoa. 
Simard, M., Foster, G., Isabelle, P. (1992). Using Cognates to 
Align Sentences in Bilingual Corpora. Proceeedings of 
the International Conference on Theoretical and 
Methodological Issues, Montr?al. 
Smadja, F. (1993). Retrieving collocations from text: Xtract. 
Computational Linguistics, 19(1), 143?177. 
Streiter, O., Carl, M., Haller, J. (eds)(1999). Hybrid 
Approaches to Machine Translation. IAI working 
papers 36. 
Streiter, O.; Carl, M.; Iomdin, L.L.: 2000, A Virtual 
Translation Machine for Hybrid Machine Translation'. 
In: Proceedings of the Dialogue'2000 International 
Seminar in Computational Linguistics and 
Applications. Tarusa, Russia.  
Streiter, O.; Iomdin, L.L. (2000). Learning Lessons from 
Bilingual Corpora: Benefits for Machine Translation. 
International Journal of Corpus Linguistics, 5(2), 199?
230. 
Thurmair, G. (2005). Hybrid architectures for machine 
translation systems. Language Resources and 
Evaluation, 39 (1), 91?108. 
Thurmair, G. (2006). Using corpus information to improve 
MT quality. Proceedings of the LR4Trans-III 
Workshop, LREC, Genova. 
Thurmair, G. (2007) Automatic evaluation in MT system 
production. MT Summit XI Workshop: Automatic 
procedures in MT evaluation, 11 September 2007, 
Copenhagen, Denmark, 
Veronis, Jean (2006). Technologies du Langue. Actualit?s ? 
Comentaires ? R?flexions. Translation. Systran or 
Reverso? 
http://aixtal.blogspot.com/2006/01/translation-systran-
or-reverso.html  
Wu, D., Fung, P. (2005). Inversion transduction grammar 
constraints for mining parallel sentences from quasi-
comparable corpora. Second International Joint 
Conference on Natural Language Processing 
(IJCNLP-2005). Jeju, Korea. 
 
112
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 1?6,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Workshop on Hybrid Approaches to Translation:
Overview and Developments
Marta R. Costa-jussa`, Rafael E. Banchs
Institute for Infocomm Research1
Patrik Lambert
Barcelona Media3
Kurt Eberle
Lingenio GmbH4
Reinhard Rapp
Aix-Marseille Universite?, LIF2
Bogdan Babych
University of Leeds5
1{vismrc,rembanchs}@i2r.a-star.edu.sg, 2reinhardrapp@gmx.de,
3patrik.lambert@barcelonamedia.org, 4k.eberle@lingenio.de,
5b.babych@leeds.ac.uk
Abstract
A current increasing trend in machine
translation is to combine data-driven and
rule-based techniques. Such combinations
typically involve the hybridization of dif-
ferent paradigms such as, for instance,
the introduction of linguistic knowledge
into statistical paradigms, the incorpora-
tion of data-driven components into rule-
based paradigms, or the pre- and post-
processing of either sort of translation sys-
tem outputs. Aiming at bringing together
researchers and practitioners from the dif-
ferent multidisciplinary areas working in
these directions, as well as at creating a
brainstorming and discussion venue for
Hybrid Translation approaches, the Hy-
Tra initiative was born. This paper gives
an overview of the Second Workshop on
Hybrid Approaches to Translation (HyTra
2013) concerning its motivation, contents
and outcomes.
1 Introduction
Machine translation (MT) has continuously been
evolving from different perspectives. Early sys-
tems were basically dictionary-based. These ap-
proaches were further developed to more complex
systems based on analysis, transfer and genera-
tion. The objective was to climb up (and down)
in the well-known Vauquois pyramid (see Figure
1) to facilitate the transfer phase or to even mini-
mize the transfer by using an interlingua system.
But then, corpus-based approaches irrupted, gen-
erating a turning point in the field by putting aside
the analysis, generation and transfer phases.
Although there had been such a tendency right
from the beginning (Wilks, 1994), in the last
Figure 1: Vauquois pyramid (image from
Wikipedia).
years, the corpus-based approaches have reached
a point where many researchers assume that rely-
ing exclusively on data might have serious limi-
tations. Therefore, research has focused either on
syntactical/hierarchical-based methods or on try-
ing to augment the popular phrase-based systems
by incorporating linguistic knowledge. In addi-
tion, and given the fact that research on rule-based
has never stopped, there have been several propos-
als of hybrid architectures combining both rule-
based and data-driven approaches.
In summary, there is currently a clear trend to-
wards hybridization, with researchers adding mor-
phological, syntactic and semantic knowledge to
statistical systems, as well as combining data-
driven methods with existing rule-based systems.
In this paper we provide a general overview
of current approaches to hybrid MT within the
context of the Second Workshop on Hybrid Ap-
proaches to Translation (HyTra 2013). In our
overview, we classify hybrid MT approaches ac-
cording to the linguistic levels that they address.
We then briefly summarize the contributions pre-
sented and collected in this volume.
1
The paper is organized as follows. First, we mo-
tivate and summarize the main aspects of the Hy-
Tra initiative. Then, we present a general overview
of the accepted papers and discuss them within
the context of other state-of-the-art research in the
area. Finally, we present our conclusions and dis-
cuss our proposed view of future directions for
Hybrid MT research.
2 Overview of the HyTra Initiative
The HyTra initiative started in response to the in-
creasing interest in hybrid approaches to machine
translation, which is reflected on the substantial
amount of work conducted on this topic. An-
other important motivation was the observation
that, up to now, no single paradigm has been able
to successfully solve to a satisfactory extent all of
the many challenges that the problem of machine
translation poses.
The first HyTra workshop took part in conjunc-
tion with the EACL 2012 conference (Costa-jussa`
et al, 2012). The Second HyTra Workshop, which
was co-organized by the authors of this paper, has
been co-located with the ACL 2013 conference
(Costa-jussa` et al, 2013). The workshop has been
supported by an extensive programme committee
comprising members from over 30 organizations
and representing more than 20 countries. As the
outcome of a comprehensive peer reviewing pro-
cess, and based on the recommendations of the
programme committee, 15 papers were finally se-
lected for either oral or poster presentation at the
workshop.
The workshop also had the privilege to be hon-
ored by two exceptional keynote speeches:
? Controlled Ascent: Imbuing Statistical MT
with Linguistic Knowledge by Will Lewis and
Chris Quirk (2013), Microsoft research. The
intersection of rule-based and statistical ap-
proaches in MT is explored, with a particular
focus on past and current work done at Mi-
crosoft Research. One of their motivations
for a hybrid approach is the observation that
the times are over when huge improvements
in translation quality were possible by sim-
ply adding more data to statistical systems.
The reason is that most of the readily avail-
able parallel data has already been found.
? How much hybridity do we have? by Her-
mann Ney, RWTH Aachen. It is pointed
out that after about 25 years the statistical
approach to MT has been widely accepted
as an alternative to the classical approach
with manually designed rules. But in prac-
tice most statistical MT systems make use
of manually designed rules at least for pre-
processing in order to improve MT quality.
This is exemplified by looking at the RWTH
MT systems.
3 Hybrid Approaches Organized by
Linguistic Levels
?Hybridization? of MT can be understood as com-
bination of several MT systems (possibly of very
different architecture) where the single systems
translate in parallel and compete for the best re-
sult (which is chosen by the integrating meta sys-
tem). The workshop and the papers do not fo-
cus on this ?coarse-grained? hybridization (Eisele
et al, 2008), but on a more ?fine grained? one
where the systems mix information from differ-
ent levels of linguistic representations (see Fig-
ure 2). In the past and mostly in the framework
of rule-based machine translation (RBMT) it has
been experimented with information from nearly
every level including phonetics and phonology
for speech recognition and synthesis in speech-
to-speech systems (Wahlster, 2000) and includ-
ing pragmatics for dialog translation (Batliner et
al., 2000a; Batliner et al, 2000b) and text coher-
ence phenomena (Le Nagard and Koehn, 2010).
With respect to work with emphasis on statisti-
cal machine translation (SMT) and derivations of
it mainly those information levels have been used
that address text in the sense of sets of sentences.
As most of the workshop papers relate to this
perspective - i.e. on hybridization which is de-
fined using SMT as backbone, in this introduc-
tion we can do with distinguishing between ap-
proaches focused on morphology, syntax, and se-
mantics. There are of course approaches which
deal with more than one of these levels in an in-
tegrated manner, which are commonly refered to
as multilevel approaches. As the case of treat-
ing syntax and morphology concurrently is espe-
cially common, we also consider morpho-syntax
as a separate multilevel approach.
3.1 Morphological approaches
The main approaches of statistical MT that ex-
ploit morphology can be classified into segmen-
tation, generation, and enriching approaches. The
2
Figure 2: Major linguistic levels (image from
Wikipedia).
first one attempts to minimize the vocabulary of
highly inflected languages in order to symmetrize
the (lexical granularity of the) source and the tar-
get language. The second one assumes that, due
to data sparseness, not all morphological forms
can be learned from parallel corpora and, there-
fore, proposes techniques to learn new morpho-
logical forms. The last one tries to enrich poorly
inflected languages to compensate for their lack of
morphology. In HyTra 2013, approaches treating
morphology were addressed by the following con-
tributions:
? Toral (2013) explores the selection of data to
train domain-specific language models (LM)
from non-domain specific corpora by means
of simplified morphology forms (such as
lemmas). The benefit of this technique is
tested using automatic metrics in the English-
to-Spanish task. Results show an improve-
ment of up to 8.17% of perplexity reduction
over the baseline system.
? Rios Gonzalez and Goehring (2013) propose
machine learning techniques to decide on the
correct form of a verb depending on the con-
text. Basically they use tree-banks to train the
classifiers. Results show that they are able
to disambiguate up to 89% of the Quechua
verbs.
3.2 Syntactic approaches
Syntax had been addressed originally in SMT in
the form of so called phrase-based SMT with-
out any reference to linguistic structures; during
the last decade (or more) the approach evolved
to or, respectively, was complemented by - work
on syntax-based models in the linguistic sense of
the word. Most such approaches can be classi-
fied into three different types of architecture that
are defined by the type of syntactic analysis used
for the source language and the type of generation
aimed at for the target language: tree-to-tree, tree-
to-string and string-to-tree. Additionally, there
are also the so called hierarchical systems, which
combine the phrase-based and syntax-based ap-
proaches by using phrases as translation-units and
automatically generated context free grammars as
rules. Approaches dealing with the syntactic ap-
proach in HyTra 2013 include the following pa-
pers:
? Green and Zabokrtsky? (2013) study three dif-
ferent ways to ensemble parsing techniques
and provide results in MT. They compute cor-
relations between parsing quality and transla-
tion quality, showing that NIST is more cor-
related than BLEU.
? Han et al (2013) provide a framework for
pre-reordering to make Chinese word order
more similar to Japanese. To this purpose,
they use unlabelled dependency structures of
sentences and POS tags to identify verbal
blocks and move them from after-the-object
positions (SVO) to before-the-object posi-
tions (SOV).
? Nath Patel et al (2013) also propose a pre-
reordering technique, which uses a limited
set of rules based on parse-tree modification
rules and manual revision. The set of rules is
specifically listed in detail.
? Saers et al (2013) report an unsupervised
learning model that induces phrasal ITGs by
breaking rules into smaller ones using mini-
mum description length. The resulting trans-
lation model provides a basis for generaliza-
tion to more abstract transduction grammars
with informative non-terminals.
3.3 Morphosyntactical approaches
In linguistic theories, morphology and syntax are
often considered and represented simultaneously
(not only in unification-based approaches) and the
same is true for MT systems.
3
? Laki et al (2013) combine pre-reordering
rules with morphological and factored mod-
els for English-to-Turkish.
? Li et al (2013) propose pre-reordering rules
to be used for alignment-based reordering,
and corresponding POS-based restructuring
of the input. Basically, they focus on tak-
ing advantage of the fact that Korean has
compound words, which - for the purpose of
alignment - are split and reordered similarly
to Chinese.
? Turki Khemakhem et al (2013) present
work about an English-Arabic SMT sys-
tem that uses morphological decomposition
and morpho-syntactic annotation of the target
language and incorporates the correspond-
ing information in a statistical feature model.
Essentially, the statistical feature language
model replaces words by feature arrays.
3.4 Semantic approaches
The introduction of semantics in statistical MT has
been approached to solve word sense disambigua-
tion challenges covering the area of lexical seman-
tics and, more recently, there have been different
techniques using semantic roles covering shallow
semantics, as well as the use of distributional se-
mantics for improving translation unit selection.
Approaches treating the incorporation of seman-
tics into MT in HyTra 2013 include the following
research work:
? Rudnick et al (2013) present a combina-
tion of Maximum Entropy Markov Models
and HMM to perform lexical selection in
the sense of cross-lingual word sense disam-
biguation (i.e. by choice from the set of trans-
lation alternatives). The system is meant to
be integrated into a RBMT system.
? Boujelbane (2013) proposes to build a bilin-
gual lexicon for the Tunisian dialect us-
ing modern standard Arabic (MSA). The
methodology is based on leveraging the large
available annotated MSA resources by ex-
ploiting MSA-dialect similarities and ad-
dressing the known differences. The author
studies morphological, syntactic and lexical
differences by exploiting Penn Arabic Tree-
bank, and uses the differences to develop
rules and to build dialectal concepts.
? Bouillon et al (2013) presents two method-
ologies to correct homophone confusions.
The first one is based on hand-coded rules
and the second one is based on weighted
graphs derived from a pronunciation re-
source.
3.5 Other multilevel approaches
In a number of linguistic theories information
from the morphological, syntactic and semantic
level is considered conjointly and merged in cor-
responding representations (a RBMT example is
LFG (Lexical Functional Grammars) analysis and
the corresponding XLE translation architecture).
In HyTra 2013 there are three approaches dealing
with multilevel information:
? Pal et al (2013) propose a combination of
aligners: GIZA++, Berkeley and rule-based
for English-Bengali.
? Hsieh et al (2013) use comparable corpora
extracted from Wikipedia to extract parallel
fragments for the purpose of extending an
English-Bengali training corpus.
? Tambouratzis et al (2013) describe a hybrid
MT architecture that uses very few bilingual
corpus and a large monolingual one. The
linguistic information is extracted using
pattern recognition techniques.
Table 1 summarizes the papers that have been
presented in the Second HyTra Workshop. The
papers are arranged into the table according to the
linguistic level they address.
4 Conclusions and further work
The success of the Second HyTra Workshop con-
firms that research in hybrid approaches to MT
systems is a very active and promising area. The
MT community seems to agree that pure data-
driven or rule-based paradigms have strong lim-
itations and that hybrid systems are a promising
direction to overcome most of these limitations.
Considerable progress has been made in this area
recently, as demonstrated by consistent improve-
ments for different language pairs and translation
tasks.
The research community is working hard, with
strong collaborations and with more resources at
hand than ever before. However, it is not clear
4
Morphological (Toral, 2013) Hybrid Selection of LM Training Data Using Linguistic Information and Perplexity
(Gonzales and Goehring, 2013) Machine Learning disambiguation of Quechua verb morphology
Syntax (Green and Zabokrtsky?, 2013) Improvements to SBMT using Ensemble Dependency Parser
(Han et al, 2013) Using unlabeled dependency parsing for pre-reordering for Chinese-to-Japanese SMT
(Patel et al, 2013) Reordering rules for English-Hindi SMT
(Saers et al, 2013) Unsupervised transduction grammar induction via MDL
Morpho-syntactic (Laki et al, 2013) English to Hungarian morpheme-based SMT system with reordering rules
(Li et al, 2013) Experiments with POS-based restructuring and alignment based reordering for SMT
(Khemakhem et al, 2013) Integrating morpho-syntactic feature for English Arabic SMT
Semantic (Rudnick and Gasser, 2013) Lexical Selection for Hybrid MT with Sequence Labeling
(Boujelbane et al, 2013) Building bilingual lexicon to create dialect Tunisian corpora and adapt LM
(Bouillon et al, 2013) Two approaches to correcting homophone confusions in a hybrid SMT based system
Multilevels (Pal et al, 2013) A hybrid Word alignment model for PBSMT
(Hsieh et al, 2013) Uses of monolingual in-domain corpora for cross-domain adaptation with hybrid MT approaches
(Tambouratzis et al, 2013) Overview of a language-independent hybrid MT methodology
Table 1: HyTra 2013 paper overview.
whether technological breakthroughs as in the past
are still possible are still possible, or if MT will be
turning into a research field with only incremen-
tal advances. The question is: have we reached
the point at which only refinements to existing ap-
proaches are needed? Or, on the contrary, do we
need a new turning point?
Our guess is that, similar to the inflection point
giving rise to the statistical MT approach during
the last decade of the twentieth century, once again
there might occur a new discovery which will rev-
olutionize further the research on MT. We cannot
know whether hybrid approaches will be involved;
but, in any case, this seems to be a good and smart
direction as it is open to the full spectrum of ideas
and, thus, it should help to push the field forward.
Acknowledgments
This workshop has been supported by the Sev-
enth Framework Program of the European Com-
mission through the Marie Curie actions HyghTra,
IMTraP, AutoWordNet and CrossLingMind and
the Spanish ?Ministerio de Econom??a y Competi-
tividad? and the European Regional Development
Fund through SpeechTech4all. We would like to
thank the funding institution and all people who
contributed towards making the workshop a suc-
cess. For a more comprehensive list of acknowl-
edgments refer to the preface of this volume.
References
Anton Batliner, J. Buckow, Heinrich Niemann, Elmar
No?th, and Volker Warnke, 2000a. The Prosody
Module, pages 106?121. New York, Berlin.
Anton Batliner, Richard Huber, Heinrich Niemann, El-
mar No?th, Jo?rg Spilker, and K. Fischer, 2000b. The
Recognition of Emotion, pages 122?130. New York,
Berlin.
Pierrette Bouillon, Johanna Gerlach, Ulrich Germann,
Barry Haddow, and Manny Rayner. 2013. Two ap-
proaches to correcting homophone confusions in a
hybrid machine translation system. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Rahma Boujelbane, Mariem Ellouze khemekhem, Si-
war BenAyed, and Lamia HadrichBelguith. 2013.
Building bilingual lexicon to create dialect tunisian
corpora and adapt language model. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Marta R. Costa-jussa`, Patrik Lambert, Rafael E.
Banchs, Reinhard Rapp, and Bogdan Babych, edi-
tors. 2012. Proceedings of the Joint Workshop on
Exploiting Synergies between Information Retrieval
and Machine Translation (ESIRMT) and Hybrid Ap-
proaches to Machine Translation (HyTra). As-
sociation for Computational Linguistics, Avignon,
France, April.
Marta R. Costa-jussa`, Patrik Lambert, Rafael E.
Banchs, Reinhard Rapp, Bogdan Babych, and Kurl
Eberle, editors. 2013. Proceedings of the Sec-
ond Workshop on Hybrid Approaches to Translation
(HyTra). Association for Computational Linguis-
tics, Sofia, Bulgaria, August.
Andreas Eisele, Christian Federmann, Hans Uszkoreit,
Herve? Saint-Amand, Martin Kay, Michael Jelling-
haus, Sabine Hunsicker, Teresa Herrmann, and
Yu Chen. 2008. Hybrid machine translation archi-
tectures within and beyond the euromatrix project.
In John Hutchins and Walther v.Hahn, editors, 12th
annual conference of the European Association for
Machine Translation (EAMT), pages 27?34, Ham-
burg, Germany.
Annette Rios Gonzales and Anne Goehring. 2013.
Machine learning disambiguation of quechua verb
morphology. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Nathan Green and Zdenek Zabokrtsky?. 2013. Im-
provements to syntax-based machine translation us-
ing ensemble dependency parsers. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
5
Dan Han, Pascual Martinez-Gomez, Yusuke Miyao,
Katsuhito Sudoh, and Masaaki NAGATA. 2013.
Using unlabeled dependency parsing for pre-
reordering for chinese-to-japanese statistical ma-
chine translation. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation, Sofia.
An-Chang Hsieh, Hen-Hsen Huang, and Hsin-Hsi
Chen. 2013. Uses of monolingual in-domain cor-
pora for cross-domain adaptation with hybrid mt ap-
proaches. In ACL Workshop on Hybrid Machine Ap-
proaches to Translation, Sofia.
Ines Turki Khemakhem, Salma Jamoussi, and Abdel-
majid Ben Hamadou. 2013. Integrating morpho-
syntactic feature in english-arabic statistical ma-
chine translation. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation, Sofia.
La?szlo? Laki, Attila Novak, and Borba?la Siklo?si. 2013.
English to hungarian morpheme-based statistical
machine translation system with reordering rules. In
ACL Workshop on Hybrid Machine Approaches to
Translation, Sofia.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden, July. Association for
Computational Linguistics.
Will Lewis and Chris Quirk. 2013. Controlled ascent:
Imbuing statistical mt with linguistic knowledge. In
ACL Workshop on Hybrid Machine Approaches to
Translation, Sofia.
Shuo Li, Derek F. Wong, and Lidia S. Chao.
2013. Experiments with pos-based restructuring and
alignment-based reordering for statistical machine
translation. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Santanu Pal, Sudip Naskar, and Sivaji Bandyopadhyay.
2013. A hybrid word alignment model for phrase-
based statistical machine translation. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale, and
Sasikumar M. 2013. Reordering rules for english-
hindi smt. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Alex Rudnick and Michael Gasser. 2013. Lexical se-
lection for hybrid mt with sequence labeling. In ACL
Workshop on Hybrid Machine Approaches to Trans-
lation, Sofia.
Markus Saers, Karteek Addanki, and Dekai Wu. 2013.
Unsupervised transduction grammar induction via
minimum description length. In ACL Workshop on
Hybrid Machine Approaches to Translation, Sofia.
George Tambouratzis, Sokratis Sofianopoulos, and
Marina Vassiliou. 2013. Language-independent hy-
brid mt with presemt. In ACL Workshop on Hybrid
Machine Approaches to Translation, Sofia.
Antonio Toral. 2013. Hybrid selection of language
model training data using linguistic information and
perplexity. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer,
Berlin, Heidelberg, New York.
Yorick Wilks. 1994. Stone soup and the french
room: The empiricist-rationalist debate about ma-
chine translation. Current Issues in Computational
Linguistics: in honor of Don Walker, pages 585?
594. Pisa, Italy: Giardini / Dordrecht, The Nether-
lands: Kluwer Academic.
6
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 75?81,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Deriving de/het gender classification for Dutch nouns for rule-based MT generation tasks 
Bogdan Babych Centre for Translation Studies University of Leeds b.babych@leeds.ac.uk 
Jonathan Geiger Lingenio GmbH  geiger@cl.uni-heidelberg.de 
Mireia Ginest? Rosell Centre for Translation Studies University of Leeds mireia.ginesti@gmail.com 
Kurt Eberle Lingenio GmbH  k.eberle@lingenio.de 
 Abstract Linguistic resources available in the pub-lic domain, such as lemmatisers, part-of-speech taggers and parsers can be used for the development of MT systems: as separate processing modules or as anno-tation tools for the training corpus. For SMT this annotation is used for training factored models, and for the rule-based systems linguistically annotated corpus is the basis for creating analysis, generation and transfer dictionaries from corpora. However, the annotation in many cases is insufficient for rule-based MT, especially for the generation tasks. In this paper we analyze a specific case when the part-of-speech tagger does not provide infor-mation about de/het gender of Dutch nouns that is needed for our rule-based MT systems translating into Dutch. We show that this information can be derived from large annotated monolingual corpo-ra using a set of context-checking rules on the basis of co-occurrence of nouns and determiners in certain morphosyntac-tic configurations. As not all contexts are sufficient for disambiguation, we evalu-ate the coverage and the accuracy of our method for different frequency thresholds                                                 ? 2014 European Association for Computational Linguis-tics.  
in the news corpora. Further we discuss possible generalization of our method, and using it to automatically derive other types of linguistic information needed for rule-based MT: syntactic subcategoriza-tion frames, feature agreement rules and contextually appropriate collocates. 1 Introduction This paper evaluates a methodology for deriving gender classification of nouns based on their con-textual features and light-weight linguistic anno-tation of a corpus. We approach the problem as reconstructing an enriched set of linguistic fea-tures for RBMT generation lexicon from com-bining implicit information available in corpora with a set of general linguistic principles imple-mented as a small set of simple hand-crafted con-textual rules.  These rules are specified as configurations of part-of-speech codes and operate over configura-tions of part-of-speech codes designed to capture certain disambiguating linguistic constructions. Theoretically, the rules can be made highly-accurate if the list of disambiguating construc-tions is exhaustive, but there is a well-known trade-off between Precision, Recall and the de-velopment effort for hand-crafted sets of rules. Additional factors to be taken into account are the quality and size of the annotated corpus. In our experiment we take a practical approach, us-ing a minimal set of contextual rules that cover most typical constructions.  
75
We evaluate Precision and coverage for this set of rules for different frequency thresholds of nouns in the corpus. The results indicate the po-tential of the proposed methodology for a larger set of similar tasks, where we intend to enrich linguistic resources for rule-based MT tasks us-ing implicit linguistic information, which can be discovered in annotated corpora. The paper is organised as follows: Section 2 discusses linguistic aspects of the gender disam-biguation task for Dutch nouns; Section 3 de-scribes the set-up of our experiment on automati-cally deriving the lexicon for Dutch nouns en-riched with gender information; Section 4 pre-sents evaluation results for Precision and cover-age for different frequency thresholds; Section 4 gives interpretation of the results; Section 6 dis-cusses the development context, generalisation of our methodology for rule-based MT and some ideas for future work.  2 Linguistic aspects of gender disam-biguation task for Dutch nouns Predicting gender of Dutch nouns from their context is a simple and clearly defined contextual disambiguation task, and we can evaluate three aspects of the performance of our method: (a) what coverage and accuracy can be achieved on this task compared to the gold standard; (b) how do the coverage and accuracy change in different frequency thresholds; (c) what is the proportion of contexts which can be used for disambiguation in different frequency thresholds (since some contexts will not disambiguate the features of interest). Nouns in Dutch belong to one of the two gen-der classes which determine the choice of the definite articles (used with singular nouns) and other determiners: neuter nouns take determiners het, dat, dit, ons, and nouns with the common gender, which historically is the merged mascu-line and feminine, take de, die, deze, onze. Nouns can only be disambiguated when used as singular and take a definite determiner, so not all contexts in corpus which contain nouns can be useful for disambiguation.  The information about het/de classification for nouns is a non-interpretable (in terms of the gen-erative grammar) system-internal morphological feature: it characterises only combinatorial prop-erties of nouns, but does not directly influence their syntactic functions in a structure of a sen-tence or their semantic interpretation (unlike the 
part-of-speech/Noun category, morphological case and number). Therefore, this feature is much more useful for text generation than for analysis, and belongs to the family of other similar sys-tem-internal features, like inflection classes, sub-categorisation frames, lexical functions (colloca-tional restrictions), etc. Interestingly, this feature normally operates in the local context of several words within a limited number of possible part-of-speech sequences. For machine translation task this information needs to be supplied by the target language gen-eration rules, or by the target language model, since it is normally not present in the source text, and cannot be derived from application of trans-fer rules or the translation model. There are several wide-coverage part-of-speech taggers and lemmatisers for Dutch in the public domain (open source and/or freely availa-ble), such as Dutch parameter files for the Tree-Tagger (Schmid, 1994), TiMBL / Frog tagger / lemmatiser / dependency parser (Van den Bosch et al., 2007), Alpino system (Bouma et al., 2001). Some of them provide only plain high-level an-notation of part-of-speech codes, without gender information for nouns. However, some do gener-ate enriched part-of-speech codes for nouns spec-ifying their gender. Because of this we can benchmark our methodology using this enriched information as gold-standard and calculate Preci-sion in addition to coverage.  3 Set-up of the experiment In our experiment TiMBL / Frog was used to automatically annotate a 60-million-word section of the balanced Dutch SoNaR corpus (Oostdijk et al., 2008). TiMBL/Frog provides gold-standard diction-ary-based information about these classes for identified lemmas. For the prediction task we ignored the gold-standard gender class infor-mation, and used only the generic part-of-speech information and the number category for nouns. In the evaluation stage, we compared these au-tomatically predicted gender classes with the gold-standard classes. Prediction of the de/het classes was performed by a set of regular expressions, which cover most typical contexts, where these determiners are distinguished. If both types of determiners were found in different contexts for the same noun, then the class that has the majority of contexts was assigned. Regular expressions covered sim-ple contexts, e.g.: Det (Adj)? Noun:  
76
Table 1. Evaluation of the task of predicting Dutch determiner classes: Number of tokens and proportions in each frequency threshold   (1) de nieuwe geschiedschrijving the.Gend:COM new history.Gend:COM  -- but not more complex ambiguous contexts, e.g., sequences of nominal compounds:  (2) waar is de apparaat-code van mijn ka-mera? Where is the~Gend:COM device~Gend:NEUT ? code~Gend:COM of my camera?  or cases where het is not a determiner, but is mis-tagged as such: we assumed that such contexts are less frequent and error rate will be limited, so we can save the development effort for our hand-crafted rule set relying on the signal being stronger than noise introduced by such complex cases.  The results reported in this paper were gener-ated using the following two multilevel regular expressions (expressions which operate on the levels of lemmas and parts-of-speech:   de/det__art   /(adj|conjcoord)* (.*)/nounsg  het/det__art   /(adj|conjcoord)* (.*)/nounsg     
                      These regular expressions describe configurations that allow several optional adjectives or coordinative conjunctions between the definite determiner and a singular noun. The noun is captured if the configuration matches the piece of text and classified according to the type of the determiner.  4 Evaluation results The results are presented in Table 1 and Charts 1 and 2, which visualise some of the data from Table 1. Rows in Table 1 represent different frequency cut-off points, e.g: None = no frequency cut-off, Frq>1 = noun types with frequency greater than one, etc. Columns represent:  - Gold standard: the number of noun types identified in the gold-standard above the specified frequency - Predicted: the number of noun types for which prediction of the gender on the ba-sis of the context in the corpus was made (for the rest prediction was not possible since no disambiguating contexts were found for those noun types) - Wrong, %/100: the number and the pro-portion of wrongly predicted noun types (of the total number of Predicted types) 
Gold standard Predicted Wrong %:100 Correct %:100 Missed %:100 Contexts %:100
None 157066 74505 2417 0.032 72088 0.968 84978 0.541 0.752
70006 45710 1604 0.035 44106 0.965 25900 0.37 0.573
48002 35766 1229 0.034 34537 0.966 13465 0.281 0.518
38084 30245 1012 0.033 29233 0.967 8851 0.232 0.491
32051 26515 858 0.032 25657 0.968 6394 0.199 0.475
28025 23818 744 0.031 23074 0.969 4951 0.177 0.465
25026 21735 661 0.03 21074 0.97 3952 0.158 0.456
22789 20053 597 0.03 19456 0.97 3333 0.146 0.450
21002 18701 543 0.029 18158 0.971 2844 0.135 0.444
19546 17553 498 0.028 17055 0.972 2491 0.127 0.440
?
12244 11436 279 0.024 11157 0.976 1087 0.089 0.421
6795 6482 123 0.019 6359 0.981 436 0.064 0.410
4297 4116 69 0.017 4047 0.983 250 0.058 0.401
Frq threshold
Frq>1
Frq>2
Frq>3
Frq>4
Frq>5
Frq>6
Frq>7
Frq>8
Frq>9
Frq>=20
Frq>=50
Frq>=100
77
- Correct, %/100: the number and the pro-portion of correctly predicted noun types (of the total number of Predicted types) - Missed, %/100: the number and the pro-portion of noun types where prediction of gender was not possible (of the total number of nouns in the Gold standard). - Contexts, %/100: the proportion of con-texts for noun tokens, which were useful for disambiguation  For instance, the first row shows the figures when no frequency cut-off is applied, e.g.: there were 157066 types labeled as Nouns in our sec-tion of SoNaR corpus, of which 74505 Nouns were found in a specific context with a definite determiner that allowed to disambiguate gender. Out of these, 2417 types (3.2%) were disambigu-ated wrongly for different reasons, 72088 types (96.8%) were disambiguated correctly. However, there still remain 84978 noun types (or 54.1% of the total number of 157066 in the gold standard), which were not disambiguated. In total, in the corpus 75.2% of contexts were useful for de/het disambiguation (contained a definite determiner in the immediate left context, or in a one-word-apart position, being separated by an adjective). The second row in Table 1 presents the subset of 70006 noun types out of the results presented in the first row for 157066 noun types, i.e., the results only for nouns with frequency more than one; the third row ? for noun types with frequen-cies more than two, etc. The intuition is that pre-diction for more frequent nouns should be more accurate since more token contexts become available for disambiguation of a specific noun type.     
Chart 1. Distribution of correctly predicted, missed and wrongly predicted nouns    
Chart 1 visualizes correct, missing and wrong proportion of noun types in the total count of these types for different frequency cut-off points. On the vertical axis there is a number of noun types, on the horizontal axis ? not greater than frequencies. It can be seen from the chart that the propor-tion of non-disambiguated noun types declines with increasing frequency threshold.    
Chart 2. Proportion of context useful for disam-biguation (evidence), not predicted (missing) and wrongly predicted (wrong) de/het classes for nouns.  Chart 2 examines the relation between frequency cut-off points and Evidence (top yellow/light line) ? the proportions of contexts available for disambiguation; Missing (middle red/medium line) ? the proportion of nouns where de/het dis-ambiguation was not possible and Wrong (bot-tom blue/dark line) ? the error rate. 5 Interpretation of the results The following conclusions can be derived from the evaluation data:  1. The Precision even for simple contextual disambiguation rules is surprisingly high: 96.8% for nouns where the prediction was possible. This indicates that simple disambiguation patterns are sufficiently frequent to outweigh more complex patterns which were not covered by the rule and may have lead to errors. 2. For the whole data set (without frequency cut-off) the Recall is much lower: automatic prediction procedure missed 54% of noun tokens that were found in the corpus and a contained 
0 1 2 3 4 5 6 7 8 9 19 49 99
0
20000
40000
60000
80000
100000
120000
140000
160000
180000
correct
missing
wrong
0 1 2 3 4 5 6 7 8 9 ? ? 19 49 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Wrong %
Missing %
Evidence %
78
gold-standard gender class, since no disambiguation context was found for these nouns in corpus. However, since more frequent nouns have more chances of occurring in a disambiguation context, mostly low frequent nouns are missed: if we exclude nouns which occurred only once, the procedure misses 37% of nouns; in frequency threshold Frq> 2 it misses even less ? 28%, etc. 3. Error rate (proportion of wrongly disambiguated nouns) is relatively stable (3.2% on the whole data set), and does not depend too much on the frequency of nouns: it declines very slowly when the frequency increases (much slower that the coverage of the certain threshold). 4. The proportion of contexts which are useful for disambiguation declines slowly with the increase in frequency threshold, but stabilises around 40% for highly frequent nouns. Interestingly, when the proportion of such contexts goes down, the error rate stays the same. In general, the results indicate that for practical purposes of rule-based MT development ? a suf-ficiently large list of gender-disambiguated Dutch nouns (around 75000) can be successfully collected from a medium-size corpus (60MW) with very high Precision (96.8%). The method will provide gender disambiguation information for around 46% of all nouns found in the corpus; and for higher frequency threshold the percent-age of gender-disambiguated nouns goes up rap-idly, flattening at around 90% for Frq>10. This performance reaches the quality standards for creating wide-coverage generation dictionaries for rule-based MT. 6 Development context and generaliza-tion of the methodology The task of predicting gender classes for nouns gives indication how other types of similar mor-phosyntactic resources and representations can be developed and enhanced. Our methodology is part of a larger develop-ment infrastructure for creating a corpus-based development environment for industry-standard rule-based MT systems enhanced with statistical tools and data. The infrastructure uses large monolingual corpora annotated by openly availa-ble part-of-speech taggers and lemmatisers, and semi-automatically derives a set of morphologi-cal and syntactic patterns for the lexical items 
found there. These patterns represent advanced linguistic features for the lexicon, such as classi-fication by inflectional morphological paradigms, derivational classes (e.g., gender for nouns), lex-ical valencies (subcategorisation and case frames), attachment preferences and lexical col-locates. For individual lexical items these patterns do not need to be fully specified from the training corpus: missing forms are reconstructed on the basis of evidence from other lexemes that fit the same pattern, so the system recognises and gen-erates correct output also for unseen forms.  In the context of our hybrid MT development infrastructure this approach particularly targets creation of linguistically-rich resources that gen-erate correct target language forms and phrases. The generation aspect is usually not covered by the annotation tools available in the public do-main, so parsers, part-of-speech taggers and lemmatisers usually work only in the direction of analysis, and do not deal with generation).  In a more general context the described infra-structure develops lexical and morphosyntactic resources in a systematic way, so they can be used in a wider range of applications and tasks. It also attempts to bridge the gap between rule-based and statistical techniques in MT by creat-ing rich and highly accurate linguistic representa-tions using corpus-based statistical techniques and integrating them within processing models for hybrid MT architecture. The central principle of the proposed infra-structure is that advanced morphosyntactic fea-tures and representations are derived from corpo-ra annotated with light-weight linguistic features.  The interpretation of this principle is that the tools like part-of-speech taggers and lemmatisers implement a unidirectional functional perspec-tive on the morphosyntactic system, which only partially covers the network of linguistic rela-tions involved in the analysis and generation as-pects of the language. Rule-based MT applica-tion instead need to rely on the alternative rela-tional perspective of morphosyntactic representa-tions. Our infrastructure aims at reconstructing this perspective by combining large corpora and unidirectional annotation tools. It derives a range of generation-oriented morphosyntactic features and representations using local context and standard analysis-oriented annotation features in corpora. The main motivation is that from the point of view of rule-based MT there is a certain imbal-ance between resources for analysis and annota-
79
tion of texts on the one hand, and resources for language generation on the other hand. Text an-notation resources, such as part-of-speech tag-gers, lemmatisers, parsers, chunkers ? have a longer history of research and development, e.g., (Greene and Rubin, 1971), have created common standards and are more widely available in the public domain, e.g., (Schmid, 1994; Brants, 2000). In their existing form they can be applied to new languages and are more widely used in practical applications. On the other hand, genera-tion-oriented tools are much less accessible, of-ten propitiatory, and lack common standards and shared frameworks for integration of new lan-guages. The predominant unidirectional text-annotation focus might be explained by a historic reason that text annotation was seen as an inter-esting computational problem with a clearly de-fined evaluation procedure, which was much harder to develop for the generation tasks. The idea behind the infrastructure is that if at least some unidirectional annotation tools are available for a certain language, the relational morphosyntactic resources can be automatically developed from large annotated corpora. This will include automatic acquisition of inflectional paradigms for lexical items, attachment prefer-ence detection, automatic acquisition of lexical functions. Our infrastructure aims at developing standards and building openly available re-sources for a number of languages, including under-resourced languages, such as Portuguese, Russian and Ukrainian, in order to carry out the following morphosyntacitc tasks:  1. word form generation: for a given lem-ma, part-of-speech and inflectional fea-ture values to generate the correct word form, e.g.: drive~V + Person(3rd); Num-ber(singular) ? drives  2. generation of paradigms: for a given lemma and part-of-speech to generate a set of all word forms and their inflec-tional feature values, e.g., drive~V ? drive~VV; drives~VVZ; driving~VVG; drove~VVD; driven~VVN 3. feature agreement generation: for a given sequence of lemmas with their part-of-speech codes to generate a correct se-quence of inflected word forms, where inflectional features, e.g., in a language with adjectives and nouns marked for gender to generate a correct gender agreement between the two: in Spanish, e.g., nuestro~A.Gender(_).Number(_) 
En:'our' + profesora~ N.Gender(fem).Number(plur) En:'professors(female)' ? nuestras profesoras 4. lexical feature generation: to select cor-rect lemmas for lexically underspecified structures, e.g., in a language with the gender feature marked on determiners and nouns to select the correct deter-miner to go with a given noun: Dutch: [Determiner.Def(definite)] + beroep~N.Number(singular) ? het beroep 5. subcategorisation frame generation: to generate the correct prepositional phrase and/or morphological case features for a given verb and a noun (or a noun phrase), e.g.: dispence~V + N ? dis-pense with + N; dispose~V + N ? dis-pose of +N 6. collocate / lexical function generation (in terms of Mel??uk, 1998): to select the correct lemma or ranked set of lemmas for a given word and semantic features of its context, e.g., '[not-real/true] + [Noun]': mock trial; false assumption; counterfeit goods; fake name 7. word order generation: to generate cor-rect linear sequence of words for a given dependency structure, e.g.:   I ? find ? issues ? certain   difficult ??  I find certain issues difficult  The first two functions are performed on internal features of a word, while the other five require contextual input in addition. The described func-tionality has applications for rule-based MT and Natural Language Generation, which could both benefit from shared standards and the infrastruc-ture of relation-oriented linguistic resources.  Acknowledgement The work is supported by the FP7 Marie Curie IAPP project HyghTra: A Hybrid High Quality Translation System, grant agreement no 251534.      
80
References Bouma, G., van Noord, G., and R. Malouf. (2001). Alpino: wide coverage computational analysis of Dutch. In W. Daelemans, K. Sima'an, J. Veenstra, and J. Zavrel, editors, Computational Linguistics in the Netherlands 2000, pages 45--59. Rodolpi, Am-sterdam. Brants, T. (2000), TnT - A Statistical Part-of-Speech Tagger. In Proceedings of the Sixth Applied Natu-ral Language Processing Conference ANLP-2000, Seattle, WA. Greene, B. B. & Rubin, G. M. (1971), Automatic Grammatical Tagging of English. Department of Linguistics, Brown University, Providence, Rhode Island Mel??uk, I. A. (1998). Collocations and Lexical Func-tions. In Anthony P. Cowie (ed.) Phraseology. Theory, analysis, and applications, 23?53. Oxford: Clarendon. Oostdijk, N., M. Reynaert, P. Monachesi, G. van Noord, R. Ordelman, I. Schuurman, V. Vandeghinste. From D-Coi to SoNaR: A reference corpus for Dutch. In: LREC 2008. Schmid, H. (1994), Probabilistic Part-of-Speech Tag-ging Using Decision Trees. Proceedings of Interna-tional Conference on New Methods in Language Processing, Manchester, UK. Van den Bosch, A., Busser, G.J., Daelemans, W., and Canisius, S. (2007). An efficient memory-based morphosyntactic tagger and parser for Dutch, In F. van Eynde, P. Dirix, I. Schuurman, and V. Vandeghinste (Eds.), Selected Papers of the 17th Computational Linguistics in the Netherlands Meeting, Leuven, Belgium, pp. 99-114.  
81
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, page 97,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Hybrid Strategies for better products and  shorter time-to-market 
 
Kurt Eberle 
 
Lingenio GmbH 
 
Karlsruher Stra?e 10 
69126 Heidelberg 
Germany 
k.eberle@lingenio.de 
  
 
Abstract 
The main Lingenio MT products are based on rule-based architectures. In the presentation we show 
how knowledge from corpora is integrated into the systems using the language analysis- and 
translation-components in a bootstrapping approach. This relates to the bilingual dictionaries, but 
also to learning decisions concerning the selection of syntactic rules and semantic readings in 
parsing and semantic evaluation. These strategies contribute both to improve the quality of the 
systems and to shorten go-to-market of new products significantly. Also a number of attractive spin-
off functions can be generated from them which, in addition, can be used for designing new types 
of products and as preparatory and postediting features in MT systems whose core is of type SMT. 
97
