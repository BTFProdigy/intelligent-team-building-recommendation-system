Proceedings of the Third Workshop on Statistical Machine Translation, pages 167?170,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
TectoMT: Highly Modular MT System
with Tectogrammatics Used as Transfer Layer?
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, Petr Pajas
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
{zabokrtsky,ptacek,pajas}@ufal.mff.cuni.cz
Abstract
We present a new English?Czech machine
translation system combining linguistically
motivated layers of language description (as
defined in the Prague Dependency Treebank
annotation scenario) with statistical NLP ap-
proaches.
1 Introduction
We describe a new MT system (called Tec-
toMT) based on the conventional analysis-transfer-
synthesis architecture. We use the layers of language
description defined in the Prague Dependency Tree-
bank 2.0 (PDT for short, (Hajic? and others, 2006)),
namely (1) word layer ? raw text, no linguistic
annotation, (2) morphological layer ? sequence of
tagged and lemmatized tokens, (3) analytical layer
? each sentence represented as a surface-syntactic
dependency tree, and (4) tectogrammatical layer ?
each sentence represented as a deep-syntactic de-
pendency tree in which only autosemantic words do
have nodes of their own; prefixes w-, m-, a-, or t-
will be used for denoting these layers.1
We use ?Praguian? tectogrammatics (introduced
in (Sgall, 1967)) as the transfer layer because
we believe that, first, it largely abstracts from
language-specific (inflection, agglutination, func-
tional words. . . ) means of expressing non-lexical
?The research reported in this paper is financially supported
by grants GAAV C?R 1ET101120503 and MSM0021620838.
1In addition, we use also p-layer (phrase structures) as an
a-layer alternative, the only reason for which is that we do not
have a working a-layer parser for English at this moment.
meanings, second, it allows for a natural transfer
factorization, and third, local tree contexts in t-trees
carry more information (esp. for lexical choice) than
local linear contexts in the original sentences.
In order to facilitate separating the transfer of lex-
icalization from the transfer of syntactization, we in-
troduce the concept of formeme. Each t-node?s has
a formeme attribute capturing which morphosyntac-
tic form has been (in the case of analysis) or will
be (synthesis) used for the t-node in the surface sen-
tence shape. Here are some examples of formemes
we use for English: n:subj (semantic noun (sn) in
subject position), n:for+X (sn with preposition for),
n:X+ago (sn with postposition ago), n:poss (posses-
sive form of sn), v:because+fin (semantic verb (sv)
as a subordinating finite clause introduced by be-
cause), v:without+ger (sv as a gerund after without),
adj:attr (semantic adjective (sa) in attributive posi-
tion), adj:compl (sa in complement position).
The presented system intensively uses the PDT
technology (data formats, software tools). Special
attention is paid to modularity: the translation is im-
plemented (in Perl) as a long sequence of processing
modules (called blocks) with relatively tiny, well-
defined tasks, so that each module is independently
testable, improvable, or substitutable. TectoMT al-
lows to easily combine blocks based on different
approaches, from blocks using complex probabilis-
tic solutions (e.g., B2, B6, B35, see the next section),
through blocks applying simpler Machine Learning
techniques (e.g., B69) or empirically based heuris-
tics (e.g., B7, B25, B36, B71), to blocks implementing
?crisp? linguistic rules (e.g., B48-B51, B59). There are
also blocks for trivial technical tasks (e.g., B33, B72).
167
English m-layerSheshe PRP hashave VBZ nevernever RB laughedlaugh VBN inin IN herher PRP$ newnew JJ bossboss NN 's's POS officeoffice NN .. .
NPBShe has
English p-layerS
ADVPnever
VP
laughedin
VP
her new
PP
NPBboss 's
NPB
office .
English a-layer
She has neverlaughedin
her newboss's
office.
English t-layer
#PersPronn:subj neveradv:
laughv:fin
#PersPronn:poss newadj:attr
bossn:poss
officen:in+X
Czech t-layer
#PersPronn:1 nikdyadv:
sm?t_sev:fin ??adn:v+6
#PersPronadj:attr nov?adj:attr
??fn:2
Czech a-layer
NikdyD........1A... seP7
nesm?laVpFS...3..NA..vR??aduN.IS6.....A...
sv?hoP8MS2......... nov?hoAAMS2....1A...
??faN.MS2.....A...
.Z
Figure 1: MT ?pyramid? as implemented in TectoMT. All the representations are rooted with artificial nodes, serving
only as labels. Virtually, the pyramid is bottomed with the input sentence on the source side (She has never laughed in
her new boss?s office.) and its automatic translation on the target side (Nikdy se nesma?la v u?r?adu sve?ho nove?ho s?e?fa.).
2 Translation Procedure
The structure of this section directly renders the se-
quence of blocks currently used for English-Czech
translation in TectoMT. The intermediate stages of
the translation process are illustrated in Figure 1;
identifiers of the blocks affecting on the translation
of the sample sentence are typeset in bold.
2.1 From English w-layer to English m-layer
B1: Segment the source English text into sentences.
B2: Split the sentences into sequences of tokens,
roughly according to Penn Treebank (PTB for short;
(Marcus et al, 1994)) conventions. B3: Tag the
tokens with PTB-style POS tags using a tagger
(Brants, 2000). B4: Fix some tagging errors sys-
tematically made by the tagger using a rule-based
corrector. B5: Lemmatize the tokens using morpha,
(Minnen et al, 2000).
2.2 From English m-layer to English p-layer
B6: Build PTB-style phrase-structure tree for each
sentence using a parser (Collins, 1999).
2.3 From English p-layer to English a-layer
B7: In each phrase, mark the head node (using a set
of heuristic rules). B8: Convert phrase-structure trees
to a-trees. B9: Apply some heuristic rules to fix ap-
position constructions. B10: Apply another heuris-
tic rules for reattaching incorrectly positioned nodes.
B11: Unify the way in which multiword prepositions
(such as because of ) and subordinating conjunctions
(such as provided that) are treated. B12: Assign an-
alytical functions (only if necessary for a correct
treatment of coordination/apposition constructions).
2.4 From English a-layer to English t-layer
B13: Mark a-nodes which are auxiliary (such as
prepositions, subordinating conjunctions, auxiliary
verbs, selected types of particles, etc.) B14: Mark not
as an auxiliary node too (but only if it is connected to
a verb form). B15: Build t-trees. Each a-node cluster
formed by an autosemantic node and possibly sev-
eral associated auxiliary nodes is ?collapsed? into a
single t-node. T-tree dependency edges are derived
from a-tree edges connecting the a-node clusters.
B16: Explicitely distinguish t-nodes that are mem-
bers of coordination (conjuncts) from shared modi-
fiers. It is necessary as they all are attached below
the coordination conjunction t-node. B17: Modify
t-lemmas in specific cases. E.g., all kinds of per-
sonal pronouns are represented by the ?artificial? t-
lemma #PersPron. B18: Assign functors that are nec-
essary for proper treatment of coordination and ap-
position constructions. B19: Distribute shared auxil-
iary words in coordination constructions. B20: Mark
t-nodes that are roots of t-subtrees corresponding to
finite verb clauses. B21: Mark passive verb forms.
B22: Assign (a subset of) functors. B23: Mark t-nodes
corresponding to infinitive verbs. B24: Mark t-nodes
which are roots of t-subtrees corresponding to rel-
ative clauses. B25: Identify coreference links be-
tween relative pronouns (or other relative pronom-
inal word) and their nominal antecedents. B26: Mark
168
t-nodes that are the roots of t-subtrees correspond-
ing to direct speeches. B27: Mark t-nodes that are
the roots of t-subtrees corresponding to parenthe-
sized expressions. B28: Fill the nodetype attribute
(rough classification of t-nodes). B29: Fill the sem-
pos attribute (fine-grained classification of t-nodes).
B30: Fill the grammateme attributes (semantically in-
dispensable morphological categories, such as num-
ber for nouns, tense for verbs). B31: Determine the
formeme of each t-node. B32: Mark personal names,
distinguish male and female first names if possible.
2.5 From English t-layer to Czech t-layer
B33: Initiate the target-side t-trees, simply by cloning
the source-side t-trees. B34: In each t-node, trans-
late its formeme.2 B35: Translate t-lemma in each
t-node as its most probable target-language counter-
part (which is compliant with the previously chosen
formeme), according to a probabilistic dictionary.3
B36: Apply manual rules for fixing the formeme and
lexeme choices, which are otherwise systematically
wrong and are reasonably frequent. B37: Fill the gen-
der grammateme in t-nodes corresponding to deno-
tative nouns (it follows from the chosen t-lemma).4
B38: Fill the aspect grammateme in t-nodes corre-
sponding to verbs. Information about aspect (perfec-
tive/imperfective) is necessary for making decisions
about forming complex future tense in Czech. B39:
Apply rule-based correction of translated date/time
expressions (several templates such as 1970?s, July
1, etc.). B40: Fix grammateme values in places where
the English-Czech grammateme correspondence is
not trivial (e.g., if an English gerund expression
is translated using Czech subordinating clause, the
2The translation mapping from English formemes to Czech
formemes was obtained as follows: we analyzed 10,000 sen-
tence pairs from the WMT?08 training data up to the t-layer
(using a tagger shipped with the PDT and parser (McDonald et
al., 2005) for Czech), added formemes to t-trees on both sides,
aligned the t-trees (using a set of weighted heuristic rules, simi-
larly to (Menezes and Richardson, 2001)), and from the aligned
t-node pairs extracted for each English formeme its most fre-
quent Czech counterpart.
3The dictionary was created by merging the translation dic-
tionary from PCEDT ((Cur???n and others, 2004)) and a trans-
lation dictionary extracted from a part of the parallel corpus
Czeng ((Bojar and Z?abokrtsky?, 2006)) aligned at word-level by
Giza++ ((Och and Ney, 2003)).
4Czech nouns have grammatical gender which is (among
others) important for resolving grammatical agreement.
tense grammateme has to be filled). B41: Negate
verb forms where some arguments of the verbs bear
negative meaning (double negation in Czech). B42:
Verb t-nodes in active voice that have transitive t-
lemma and no accusative object, are turned to re-
flexives. B43: The t-nodes with genitive formeme
or prepositional-group formeme, whose counterpart
English t-nodes are located in pre-modification po-
sition, are moved to post-modification position. B44:
Reverse the dependency orientation between nu-
meric expressions and counted nouns, if the value
of the numeric expression is greater than four and
the noun without the numeral would be expressed in
nominative or accusative case. B45: Find coreference
links from personal pronouns to their antecedents,
if the latter are in subject position (needed later for
reflexivization).
2.6 From Czech t-layer to Czech a-layer
B46: Create initial a-trees by cloning t-trees. B47:
Fill the surface morphological categories (gender,
number, case, negation, etc.) with values derived
from values of grammatemes, formeme, seman-
tic part of speech etc. B48: Propagate the values
of gender and number of relative pronouns from
their antecedents (along the coreference links). B49:
Propagate the values of gender, number and person
according to the subject-predicate agreement (i.e.,
from subjects to the finite verbs). B50: Resolve agree-
ment of adjectivals in attributive positions (copying
gender/number/case from their governing nouns).
B51: Resolve complement agreement (copying gen-
der/number from subject to adjectival complement).
B52: Apply pro-drop ? deletion of personal pronouns
in subject positions. B53: Add preposition a-nodes
(if implied by the t-node?s formeme). B54: Add a-
nodes for subordinating conjunction (if implied by
the t-node?s formeme). B55: Add a-nodes corre-
sponding to reflexive particles for reflexiva tantum
verbs. B56: Add an a-node representing the auxiliary
verb by?t (to be) in the case of compound passive
verb forms. B57: Add a-nodes representing modal
verbs, accordingly to the deontic modality gram-
mateme. B58: Add the auxiliary verb by?t in imperfec-
tive future-tense complex verb forms. B59: Add verb
forms such as by/bys/bychom expressing conditional
verb modality. B60: Add auxiliary verb forms such
as jsem/jste in past-tense complex verb forms. B61:
169
Partition a-trees into finite clauses (a-nodes belong-
ing to the same clause are coindexed). B62: In each
clause, a-nodes which represent clitics are moved to
the so called second position in the clause (accord-
ing to Wackernagel?s law). B63: Add a-nodes cor-
responding to sentence-final punctuation mark. B64:
Add a-nodes corresponding to commas on bound-
aries between governing and subordinated clauses.
B65: Add a-nodes corresponding to commas in front
of conjunction ale and also commas in multiple co-
ordinations. B66: Add pairs of parenthesis a-nodes.
B67: Choose morphological lemmas in a-nodes cor-
responding to personal pronouns. B68: Generate
the resulting word forms (derived from lemmas and
tags) using Czech word form generator described in
(Hajic?, 2004). B69: Vocalize prepositions k, s, v, and
z (accordingly to the prefix of the following word).
B70: Capitalize the first word in each sentence as well
as in each direct speech.
2.7 From Czech a-layer to Czech w-layer
B71: Create the resulting sentences by flattening the
a-trees. Heuristic rules for proper spacing around
punctuation marks are used. B72: Create the resulting
text by concatenating the resulting sentences.
3 Final remarks
We believe that the potential contribution of tec-
togrammatical layer of language representation for
MT is the following: it abstracts from many
language-specific phenomena (which could reduce
the notorious data-sparsity problem) and offers a
natural factorization of the translation task (which
could be useful for formulating independence as-
sumptions when building probabilistic models). Of
course, the question naturally arises whether these
properties can ever outbalance the disadvantages, es-
pecially cumulation and interference of errors made
on different layers, considerable technical complex-
ity, and the need for detailed linguistic insight. In
our opinion, this question still remains open. On
one hand, the translation quality offered now by Tec-
toMT is below the state-of-the-art system according
to the preliminary evaluation of the WMT08 Shared
Task. But on the other hand, the potential of tec-
togrammatics has not been used fully, and more-
over there are still many components with only pilot
heuristic implementation which increase the number
of translation errors and which can be relatively eas-
ily substituted by corpus-based solutions. In the near
future, we plan to focus especially on the transfer
blocks, which are currently based on the naive as-
sumption of isomorphism of the source and target
t-trees and which do not make use of the target lan-
guage model, so far.
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2006. CzEng:
Czech-English Parallel Corpus, Release version 0.5.
Prague Bulletin of Mathematical Linguistics, 86:59?
62.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger . pages 224?231, Seattle.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Jan Cur???n et al 2004. Prague Czech - English Depen-
dency Treebank, Version 1.0. CD-ROM, Linguistics
Data Consortium, LDC Catalog No.: LDC2004T25,
Philadelphia.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceedings
of HTL/EMNLP, pages 523?530, Vancouver, Canada.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the workshop on Data-driven methods in ma-
chine translation, volume 14, pages 1?8.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust Applied Morphological Generation. In Pro-
ceedings of the 1st International Natural Language
Generation Conference, pages 201?208, Israel.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague.
170
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125?129,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
English-Czech MT in 2008 ?
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin Popel,
Jan Pta?c?ek, Jan Rous?, Zdene?k ?Zabokrtsky?
Charles University, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz
{popel,jan.rous}@matfyz.cz
Abstract
We describe two systems for English-to-
Czech machine translation that took part
in the WMT09 translation task. One of
the systems is a tuned phrase-based system
and the other one is based on a linguisti-
cally motivated analysis-transfer-synthesis
approach.
1 Introduction
We participated in WMT09 with two very dif-
ferent systems: (1) a phrase-based MT based
on Moses (Koehn et al, 2007) and tuned for
English?Czech translation, and (2) a complex
system in the TectoMT platform ( ?Zabokrtsky? et
al., 2008).
2 Data
2.1 Monolingual Data
Our Czech monolingual data consist of (1)
the Czech National Corpus (CNC, versions
SYN200[056], 72.6%, Kocek et al (2000)), (2)
a collection of web pages downloaded by Pavel
Pecina (Web, 17.1%), and (3) the Czech mono-
lingual data provided by WMT09 organizers
(10.3%). Table 1 lists sentence and token counts
(see Section 2.3 for the explanation of a- and t-
layer).
Sentences 52 M
with nonempty t-layer 51 M
a-nodes (i.e. tokens) 0.9 G
t-nodes 0.6 G
Table 1: Czech monolingual training data.
? The work on this project was supported by the grants
MSM0021620838, 1ET201120505, 1ET101120503, GAUK
52408/2008, M?SMT ?CR LC536 and FP6-IST-5-034291-STP
(EuroMatrix).
2.2 Parallel Data
As the source of parallel data we use an internal
release of Czech-English parallel corpus CzEng
(Bojar et al, 2008) extended with some additional
texts. One of the added sections was gathered
from two major websites containing Czech sub-
titles to movies and TV series1. The matching of
the Czech and English movies is rather straight-
forward thanks to the naming conventions. How-
ever, we were unable to reliably determine the se-
ries number and the episode number from the file
names. We employed a two-step procedure to au-
tomatically pair the TV series subtitle files. For
every TV series:
1. We clustered the files on both sides to remove
duplicates
2. We found the best matching using a provi-
sional translation dictionary. This proved to
be a successful technique on a small sample
of manually paired test data. The process was
facilitated by the fact that the correct pairs of
episodes usually share some named entities
which the human translator chose to keep in
the original English form.
Table 2 lists parallel corpus sizes and the distri-
bution of text domains.
English Czech
Sentences 6.91 M
with nonempty t-layer 6.89 M
a-nodes (i.e. tokens) 61 M 50 M
t-nodes 41 M 33 M
Distribution: [%] [%]
Subtitles 68.2 Novels 3.3
Software Docs 17.0 Commentaries/News 1.5
EU (Legal) Texts 9.5 Volunteer-supplied 0.4
Table 2: Czech-English data sizes and sources.
1www.opensubtitles.org and titulky.com
125
2.3 Data Preprocessing using TectoMT
platform: Analysis and Alignment
As we believe that various kinds of linguistically
relevant information might be helpful in MT, we
performed automatic analysis of the data. The
data were analyzed using the layered annotation
scheme of the Prague Dependency Treebank 2.0
(PDT 2.0, Hajic? and others (2006)), i.e. we used
three layers of sentence representation: morpho-
logical layer, surface-syntax layer (called analyti-
cal (a-) layer), and deep-syntax layer (called tec-
togrammatical (t-) layer).
The analysis was implemented using TectoMT,
( ?Zabokrtsky? et al, 2008). TectoMT is a highly
modular software framework aimed at creating
MT systems (focused, but by far not limited to
translation using tectogrammatical transfer) and
other NLP applications. Numerous existing NLP
tools such as taggers, parsers, and named entity
recognizers are already integrated in TectoMT, es-
pecially for (but again, not limited to) English and
Czech.
During the analysis of the large Czech mono-
lingual data, we used Jan Hajic??s Czech tagger
shipped with PDT 2.0, Maximum Spanning Tree
parser (McDonald et al, 2005) with optimized set
of features as described in Nova?k and ?Zabokrtsky?
(2007), and a tool for assigning functors (seman-
tic roles) from Klimes? (2006), and numerous other
components of our own (e.g. for conversion of an-
alytical trees into tectogrammatical ones).
In the parallel data, we analyzed the Czech side
using more or less the same scenario as used for
the monolingual data. English sentences were an-
alyzed using (among other tools) Morce tagger
Spoustova? et al (2007) and Maximum Spanning
Tree parser.2
The resulting deep syntactic (tectogrammatical)
Czech and English trees are then aligned using T-
aligner?a feature based greedy algorithm imple-
mented for this purpose (Marec?ek et al, 2008). T-
aligner finds corresponding nodes between the two
given trees and links them. For deciding whether
to link two nodes or not, T-aligner makes use of
a bilingual lexicon of tectogrammatical lemmas,
morphosyntactic similarities between the two can-
didate nodes, their positions in the trees and other
similarities between their parent/child nodes. It
2In some previous experiments (e.g. ?Zabokrtsky? et al
(2008)), we used phrase-structure parser Collins (1999) with
subsequent constituency-dependency conversion.
also uses word alignment generated from surface
shapes of sentences by GIZA++ tool, Och and Ney
(2003). We use acquired aligned tectogrammatical
trees for training some models for the transfer.
As analysis of such amounts of data is obvi-
ously computationally very demanding, we run it
in parallel using Sun Grid Engine3 cluster of 40
4-CPU computers. For this purpose, we imple-
mented a rather generic tool that submits any Tec-
toMT pipeline to the cluster.
3 Factored Phrase-Based MT
We essentially repeat our experiments from last
year (Bojar and Hajic?, 2008): GIZA++ align-
ments4 on a-layer lemmas (a-layer nodes corre-
spond 1-1 to surface tokens), symmetrized using
grow-diag-final (no -and) heuristic5 .
Probably due to the domain difference (the test
set is news), including Subtitles in the parallel data
and Web in the monolingual data did not bring any
improvement that would justify the additional per-
formance costs. For most of the phrase-based ex-
periments, we thus used only 2.2M parallel sen-
tences (27M Czech and 32M English tokens) and
43M Czech sentences (694 M tokens).
In Table 3 below, we report the scores for the
following setups selected from about 50 experi-
ments we ran in total:
Moses T is a simple phrase-based translation (T)
with no additional factors. The translation is
performed on truecased word forms (i.e. sen-
tence capitalization removed unless the first
word seems to be a name). The 4-gram lan-
guage model is based on the 43M sentences.
Moses T+C is a factored setup with form-to-form
translation (T) and target-side morphological
coherence check following Bojar and Hajic?
(2008). The setup uses two language mod-
els: 4-grams of word forms and 7-grams of
morphological tags.
Moses T+C+C&T+T+G 84k is a setup desirable
from the linguistic point of view. Two in-
dependent translation paths are used: (1)
form?form translation with two target-side
checks (lemma and tag generated from the
target-side form) as a fine-grained baseline
3http://gridengine.sunsource.net/
4Default settings, IBM models and iterations: 153343.
5Later, we found out that the grow-diag-final-and heuris-
tic provides insignificantly superior results.
126
with the option to resort to (2) an independent
translation of lemma?lemma and tag?tag
finished by a generation step that combines
target-side lemma and tag to produce the fi-
nal target-side form.
We use three language models in this setup
(3-grams of forms, 3-grams of lemmas, and
10-grams of tags).
Due to the increased complexity of the setup,
we were able to train this model on 84k par-
allel sentences only (the Commentaries sec-
tion) and we use the target-side of this small
training data for language models, too.
For all the setups we perform standard MERT
training on the provided development set.6
4 Translation Setup Based on
Tectogrammatical Transfer
In this translation experiment, we follow the tradi-
tional analysis-transfer-synthesis approach, using
the set of PDT 2.0 layers: we analyze the input
English sentence up to the tectogrammatical layer
(through the morphological and analytical ones),
then perform the tectogrammatical transfer, and
then synthesize the target Czech sentence from its
tectogrammatical representation. The whole pro-
cedure consists of about 80 steps, so the following
description is necessarily very high level.
4.1 Analysis
Each sentence is tokenized (roughly according to
the Penn Treebank conventions), tagged by the En-
glish version of the Morce tagger Spoustova? et al
(2007), and lemmatized by our lemmatizer. Then
the dependency parser (McDonald et al, 2005) is
applied. Then the analytical trees resulting from
the parser are converted to the tectogrammatical
ones (i.e. functional words are removed, only
morphologically indispensable categories are left
with the nodes using a sequence of heuristic proce-
dures). Unlike in PDT 2.0, the information about
the original syntactic form is stored with each t-
node (values such as v:inf for an infinitive verb
form, v:since+fin for the head of a subor-
dinate clause of a certain type, adj:attr for
an adjective in attribute position, n:for+X for a
given prepositional group are distinguished).
6We used the full development set of 2k sentences for
?Moses T? and a subset of 1k sentences for the other two
setups due to time constraints.
One of the steps in the analysis of English is
named entity recognition using Stanford Named
Entity Recognizer (Finkel et al, 2005). The nodes
in the English t-layer are grouped according to the
detected named entities and they are assigned the
type of entity (location, person, or organization).
This information is preserved in the transfer of the
deep English trees to the deep Czech trees to al-
low for the appropriate capitalization of the Czech
translation.
4.2 Transfer
The transfer phase consists of the following steps:
? Initiate the target-side (Czech) t-trees sim-
ply by ?cloning? the source-side (English) t-
trees. Subsequent steps usually iterate over
all t-nodes. In the following, we denote a
source-side t-node as S and the correspond-
ing target-side node as T.
? Translate formemes using
two probabilistic dictionaries
(p(T.formeme|S.formeme, S.parent.lemma)
and p(T.formeme|S.formeme)) and a few
manual rules. The formeme translation
probability estimates were extracted from a
part of the parallel data mentioned above.
? Translate lemmas using a probabilistic dictio-
nary (p(T.lemma|S.lemma)) and a few rules
that ensure compatibility with the previously
chosen formeme. Again, this probabilistic
dictionary was obtained using the aligned
tectogrammatical trees from the parallel cor-
pus.
? Fill the grammatemes (deep-syntactic equiv-
alent of morphological categories) gender
(for denotative nouns) and aspect (for verbs)
according to the chosen lemma. We also
fix grammateme values where the English-
Czech grammateme correspondence is non-
trivial (e.g. if an English gerund expression is
translated to Czech as a subordinating clause,
the tense grammateme has to be filled). How-
ever, the transfer of grammatemes is defi-
nitely much easier task than the transfer of
formemes and lemmas.
4.3 Synthesis
The transfer step yields an abstract deep
syntactico-semantical tree structure. Firstly,
127
we derive surface morphological categories
from their deep counterparts taking care of their
agreement where appropriate and we also remove
personal pronouns in subject positions (because
Czech is a pro-drop language).
To arrive at the surface tree structure, auxil-
iary nodes of several types are added, including
(1) reflexive particles, (2) prepositions, (3) subor-
dinating conjunctions, (4) modal verbs, (5) ver-
bal auxiliaries, and (6) punctuation nodes. Also,
grammar-based node ordering changes (imple-
mented by rules) are performed: e.g. if an English
possessive attribute is translated using Czech gen-
itive, it is shifted into post-modification position.
After finishing the inflection of nouns, verbs,
adjectives and adverbs (according to the values of
morphological categories derived from agreement
etc.), prepositions may need to be vocalized: the
vowel -e or -u is attached to the preposition if the
pronunciation of prepositional group would be dif-
ficult otherwise.
After the capitalization of the beginning of each
sentence (and each named entity instance), we ob-
tain the final translation by flattening the surface
tree.
4.4 Preliminary Error Analysis
According to our observations most errors happen
during the transfer of lemmas and formemes.
Usually, there are acceptable translations of
lemma and formeme in respective n-best lists
but we fail to choose the best one. The sce-
nario described in Section 4.2 uses quite a
primitive transfer algorithm where formemes
and lemmas are translated separately in two
steps. We hope that big improvements could
be achieved with more sophisticated algo-
rithms (optimizing the probability of the whole
tree) and smoothed probabilistic models (such
as p(T.lemma|S.lemma, T.parent.lemma) and
p(T.formeme|S.formeme, T.lemma, T.parent.lemma)).
Other common errors include:
? Analysis: parsing (especially coordinations
are problematic with McDonald?s parser).
? Transfer: the translation of idioms and col-
locations, including named entities. In these
cases, the classical transfer at the t-layer
is not appropriate and utilization of some
phrase-based MT would help.
? Synthesis: reflexive particles, word order.
5 Experimental Results and Discussion
Table 3 reports lowercase BLEU and NIST scores
and preliminary manual ranks of our submissions
in contrast with other systems participating in
English?Czech translation, as evaluated on the
official WMT09 unseen test set. Note that auto-
matic metrics are known to correlate quite poorly
with human judgements, see the best ranking but
?lower scoring? PC Translator this year and also
in Callison-Burch et al (2008).
System BLEU NIST Rank
Moses T 14.24 5.175 -3.02 (4)
Moses T+C 13.86 5.110 ?
Google 13.59 4.964 -2.82 (3)
U. of Edinburgh 13.55 5.039 -3.24 (5)
Moses T+C+C&T+T+G 84k 10.01 4.360 -
Eurotran XP 09.51 4.381 -2.81 (2)
PC Translator 09.42 4.335 -2.77 (1)
TectoMT 07.29 4.173 -3.35 (6)
Table 3: Automatic scores and preliminary human
rank for English?Czech translation. Systems in
italics are provided for comparison only. Best re-
sults in bold.
Unfortunately, this preliminary evaluation sug-
gests that simpler models perform better, partly
because it is easier to tune them properly both
from computational point of view (e.g. MERT
not stable and prone to overfitting with more fea-
tures7), as well as from software engineering point
of view (debugging of complex pipelines of tools
is demanding). Moreover, simpler models run
faster: ?Moses T? with 12 sents/minute is 4.6
times faster than ?Moses T+C?. (Note that we have
not tuned either of the models for speed.)
While ?Moses T? is probably nearly identical
setup as Google and Univ. of Edinburgh use,
the knowledge of correct language-dependent to-
kenization and the use of relatively high quality
large language model data seems to bring moder-
ate improvements.
6 Conclusion
We described our experiments with a complex lin-
guistically motivated translation system and vari-
ous (again linguistically-motivated) setups of fac-
tored phrase-based translation. An automatic eval-
uation seems to suggest that simpler is better, but
we are well aware that a reliable judgement comes
only from human annotators.
7For ?Moses T+C+C&T+T+G?, we observed BLEU
scores on the test set varying by up to five points absolute
for various weight settings yielding nearly identical dev set
scores.
128
References
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
143?146, Columbus, Ohio, June. Association for
Computational Linguistics.
Ondr?ej Bojar, Miroslav Jan??c?ek, Zdene?k ?Zabokrtsky?,
Pavel ?Ces?ka, and Peter Ben?a. 2008. CzEng 0.7:
Parallel Corpus with Community-Supplied Transla-
tions. In Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, May. ELRA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T0 1, Philadelphia.
Va?clav Klimes?. 2006. Analytical and Tectogrammat-
ical Analysis of a Natural Language. Ph.D. thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity, Prague, Czech Rep.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Praha.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clav
Nova?k. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of European Machine Translation Confer-
ence (EAMT 08), pages 102?111, Hamburg, Ger-
many.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT
?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancou-
ver, British Columbia, Canada.
Va?clav Nova?k and Zdene?k ?Zabokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, ed-
itors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th I nternational Conference on
Text, Speech and Dialogue, Lecture Notes in Com-
puter Science, pages 92?98, Pilsen, Czech Repub-
lic. Springer Science+Business Media Deutschland
GmbH.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular Hybrid MT System
with Tectogrammatics Used as Transfer Layer. In
Proc. of the ACL Workshop on Statistical Machine
Translation, pages 167?170, Columbus, Ohio, USA.
129
