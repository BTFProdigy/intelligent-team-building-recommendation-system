Proceedings of NAACL-HLT 2013, pages 348?358,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Dialectal Arabic to English Machine Translation:
Pivoting through Modern Standard Arabic
Wael Salloum and Nizar Habash
Center for Computational Learning Systems
Columbia University
{wael,habash}@ccls.columbia.edu
Abstract
Modern Standard Arabic (MSA) has a wealth
of natural language processing (NLP) tools
and resources. In comparison, resources for
dialectal Arabic (DA), the unstandardized spo-
ken varieties of Arabic, are still lacking. We
present ELISSA, a machine translation (MT)
system for DA to MSA. ELISSA employs a
rule-based approach that relies on morpho-
logical analysis, transfer rules and dictionar-
ies in addition to language models to produce
MSA paraphrases of DA sentences. ELISSA
can be employed as a general preprocessor for
DA when using MSA NLP tools. A man-
ual error analysis of ELISSA?s output shows
that it produces correct MSA translations over
93% of the time. Using ELISSA to produce
MSA versions of DA sentences as part of
an MSA-pivoting DA-to-English MT solution,
improves BLEU scores on multiple blind test
sets between 0.6% and 1.4%.
1 Introduction
Much work has been done on Modern Standard Ara-
bic (MSA) natural language processing (NLP) and
machine translation (MT), especially Statistical MT
(SMT). MSA has a wealth of resources in terms of
morphological analyzers, disambiguation systems,
and parallel corpora. In comparison, research on di-
alectal Arabic (DA), the unstandardized spoken vari-
eties of Arabic, is still lacking in NLP in general and
MT in particular. In this paper we present ELISSA,
our DA-to-MSA MT system, and show how it can
help improve the translation of highly dialectal Ara-
bic text into English by pivoting on MSA.
The ELISSA approach can be summarized as fol-
lows. First, ELISSA uses different techniques to
identify dialectal words and multi-word construc-
tions (phrases) in a source sentence. Then, ELISSA
produces MSA paraphrases for the selected words
and phrase using a rule-based component that de-
pends on the existence of a dialectal morphologi-
cal analyzer, a list of morphosyntactic transfer rules,
and DA-MSA dictionaries. The resulting MSA is in
a lattice form that we pass to a language model for n-
best decoding. The output of ELISSA, whether a top-
1 choice sentence or n-best sentences, is passed to an
MSA-English SMT system to produce the English
translation sentence. ELISSA-based MSA-pivoting
for DA-to-English SMT improves BLEU scores (Pa-
pineni et al, 2002) on three blind test sets between
0.6% and 1.4%. A manual error analysis of trans-
lated words shows that ELISSA produces correct
MSA translations over 93% of the time.
The rest of this paper is structured as follows:
Section 2 motivates the use of ELISSA to improve
DA-English SMT with an example. Section 3 dis-
cusses some of the challenges associated with pro-
cessing Arabic and its dialects. Section 4 presents
related work. Section 5 details ELISSA and its
approach and Section 6 presents results evaluating
ELISSA under a variety of conditions.
2 Motivating Example
Table 1 shows a motivating example of how pivot-
ing on MSA can dramatically improve the transla-
tion quality of a statistical MT system that is trained
on mostly MSA-to-English parallel corpora. In this
example, we use Google Translate?s online Arabic-
English SMT system.1 The table is divided into two
parts. The top part shows a dialectal (Levantine)
sentence, its reference translation to English, and
its Google Translate translation. The Google Trans-
late translation clearly struggles with most of the DA
words, which were probably unseen in the training
data (i.e., out-of-vocabulary ? OOV) and were con-
1The system was used on February 21, 2013.
348
DA source hP AJ??
@ 	???Q.
	
gA? ?
	
K

B HA

J
	
J
???
	??

J?J. K
 ?AK

	
?YK. B? ??J.

K

?J
?
	
j ??@ ?j
	
???@ ?J
j? ???J.

J?J
k A? ?

A?

??Am?'A?E.
. Y?J. ? A? h?QK

bhAlHAlh? hAy mA Hyktbwlw ?HyT AlSfHh Al?xSyh? tb?w wlA bdn yAh yb?tln kwmyntAt l?nw mAxbrhwn AymtA
rH yrwH ?Albld.
Human
Reference
In this case, they will not write on his profile wall and they do not want him to send them comments because he
did not tell them when he will go to the country.
Google
Translate
Bhalhalh Hi Hictpoulo Ahat Profile Tbau not hull Weah Abatln Comintat Anu Mabarhun Oamta welcomed calls
them Aalbuld.
Human ? ?? ??Q.
	
mProceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772?778,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Sentence Level Dialect Identification
for Machine Translation System Selection
Wael Salloum, Heba Elfardy, Linda Alamir-Salloum, Nizar Habash and Mona Diab?
Center for Computational Learning Systems, Columbia University, New York, USA
{wael,heba,habash}@ccls.columbia.edu
?Department of Computer Science, The George Washington University, Washington DC, USA
?mtdiab@email.gwu.edu
Abstract
In this paper we study the use of sentence-
level dialect identification in optimizing
machine translation system selection when
translating mixed dialect input. We test
our approach on Arabic, a prototypical
diglossic language; and we optimize the
combination of four different machine
translation systems. Our best result im-
proves over the best single MT system
baseline by 1.0% BLEU and over a strong
system selection baseline by 0.6% BLEU
on a blind test set.
1 Introduction
A language can be described as a set of dialects,
among which one "standard variety" has a spe-
cial representative status.
1
Despite being increas-
ingly ubiquitous in informal written genres such
as social media, most non-standard dialects are
resource-poor compared to their standard variety.
For statistical machine translation (MT), which re-
lies on the existence of parallel data, translating
from non-standard dialects is a challenge. In this
paper we study the use of sentence-level dialect
identification together with various linguistic fea-
tures in optimizing the selection of outputs of four
different MT systems on input text that includes a
mix of dialects.
We test our approach on Arabic, a prototypi-
cal diglossic language (Ferguson, 1959) where the
standard form of the language, Modern Standard
Arabic (MSA) and the regional dialects (DA) live
side-by-side and are closely related. MSA is the
language used in education, scripted speech and
official settings while DA is the primarily spoken
1
This paper presents work supported by the Defense Ad-
vanced Research Projects Agency (DARPA) contract No.
HR0011-12-C-0014. Any opinions, findings and conclusions
or recommendations expressed in this paper are those of the
authors and do not necessarily reflect the views of DARPA.
native vernacular. We consider two DAs: Egyp-
tian and Levantine Arabic in addition to MSA. Our
best system selection approach improves over our
best baseline single MT system by 1.0% absolute
BLEU point on a blind test set.
2 Related Work
Arabic Dialect Machine Translation. Two ap-
proaches have emerged to alleviate the problem
of DA-English parallel data scarcity: using MSA
as a bridge language (Sawaf, 2010; Salloum and
Habash, 2011; Salloum and Habash, 2013; Sajjad
et al, 2013), and using crowd sourcing to acquire
parallel data (Zbib et al, 2012). Sawaf (2010)
and Salloum and Habash (2013) used hybrid so-
lutions that combine rule-based algorithms and re-
sources such as lexicons and morphological ana-
lyzers with statistical models to map DA to MSA
before using MSA-to-English MT systems. Zbib
et al (2012) obtained a 1.5M word parallel corpus
of DA-English using crowd sourcing. Applied on
a DA test set, a system trained on their 1.5M word
corpus outperformed a system that added 150M
words of MSA-English data, as well as outper-
forming a system with oracle DA-to-MSA pivot.
In this paper we use four MT systems that trans-
late from DA to English in different ways. Similar
to Zbib et al (2012), we use DA-English, MSA-
English and DA+MSA-English systems. Our DA-
English data includes the 1.5M words created by
Zbib et al (2012). Our fourth MT system uses
ELISSA, the DA-to-MSA MT tool by Salloum and
Habash (2013), to produce an MSA pivot.
Dialect Identification. There has been a num-
ber of efforts on dialect identification (Biadsy et
al., 2009; Zaidan and Callison-Burch, 2011; Ak-
bacak et al, 2011; Elfardy et al, 2013; Elfardy
and Diab, 2013). Elfardy et al (2013) performed
token-level dialect ID by casting the problem as
a code-switching problem and treating MSA and
Egyptian as two different languages. They later
772
used features from their token-level system to train
a classifier that performs sentence-level dialect ID
(Elfardy and Diab, 2013). In this paper, we use
AIDA, the system of Elfardy and Diab (2013), to
provide a variety of dialect ID features to train
classifiers that select, for a given sentence, the MT
system that produces the best translation.
System Selection and Combination in Machine
Translation. The most popular approach to MT
system combination involves building confusion
networks from the outputs of different MT sys-
tems and decoding them to generate new transla-
tions (Rosti et al, 2007; Karakos et al, 2008; He
et al, 2008; Xu et al, 2011). Other researchers
explored the idea of re-ranking the n-best output
of MT systems using different types of syntactic
models (Och et al, 2004; Hasan et al, 2006; Ma
and McKeown, 2013). While most researchers
use target language features in training their re-
rankers, others considered source language fea-
tures (Ma and McKeown, 2013).
Most MT system combination work uses MT
systems employing different techniques to train on
the same data. However, in this paper, we use the
same MT algorithms for training, tuning, and test-
ing, but vary the training data, specifically in terms
of the degree of source language dialectness. Our
approach runs a classifier trained only on source
language features to decide which system should
translate each sentence in the test set, which means
that each sentence goes through one MT system
only. Since we do not combine the output of the
MT systems on the phrase level, we call our ap-
proach "system selection" to avoid confusion.
3 Machine Translation Experiments
In this section, we present our MT experimental
setup and the four baseline systems we built, and
we evaluate their performance and the potential of
their combination. In the next section we present
and evaluate the system selection approach.
MT Tools and Settings. We use the open-source
Moses toolkit (Koehn et al, 2007) to build four
Arabic-English phrase-based statistical machine
translation systems (SMT). Our systems use a
standard phrase-based architecture. The parallel
corpora are word-aligned using GIZA++ (Och and
Ney, 2003). The language model for our systems
is trained on English Gigaword (Graff and Cieri,
2003). We use SRILM Toolkit (Stolcke, 2002)
to build a 5-gram language model with modified
Kneser-Ney smoothing. Feature weights are tuned
to maximize BLEU on tuning sets using Mini-
mum Error Rate Training (Och, 2003). Results
are presented in terms of BLEU (Papineni et al,
2002). All evaluation results are case insensi-
tive. The English data is tokenized using simple
punctuation-based rules. The MSA portion of the
Arabic side is segmented according to the Arabic
Treebank (ATB) tokenization scheme (Maamouri
et al, 2004; Sadat and Habash, 2006) using the
MADA+TOKAN morphological analyzer and tok-
enizer v3.1 (Roth et al, 2008), while the DA por-
tion is ATB-tokenized with MADA-ARZ (Habash
et al, 2013). The Arabic text is also Alif/Ya nor-
malized. For more details on processing Arabic,
see (Habash, 2010).
MT Train/Tune/Test Data. We have two par-
allel corpora. The first is a DA-English corpus
of 5M tokenized words of Egyptian (?3.5M)
and Levantine (?1.5M). This corpus is part of
BOLT data. The second is an MSA-English cor-
pus of 57M tokenized words obtained from sev-
eral LDC corpora (10 times the size of the DA-
English data). We work with eight standard MT
test sets: three MSA sets from NIST MTEval with
four references (MT06, MT08, and MT09), four
Egyptian sets from LDC BOLT data with two ref-
erences (EgyDevV1, EgyDevV2, EgyDevV3, and
EgyTestV2), and one Levantine set from BBN
(Zbib et al, 2012) with one reference which we
split into LevDev and LevTest. We used MT08
and EgyDevV3 to tune SMT systems while we di-
vided the remaining sets among classifier training
data (5,562 sentences), dev (1,802 sentences) and
blind test (1,804 sentences) sets to ensure each of
these new sets has a variety of dialects and genres
(weblog and newswire).
MT Systems. We build four MT systems.
(1) DA-Only. This system is trained on the DA-
English data and tuned on EgyDevV3.
(2) MSA-Only. This system is trained on the
MSA-English data and tuned on MT08.
(3) DA+MSA. This system is trained on the
combination of both corpora (resulting in 62M to-
kenized
2
words on the Arabic side) and tuned on
2
Since the DA+MSA system is intended for DA data and
DA morphology, as far as tokenization is concerned, is more
complex, we tokenized the training data with dialect aware-
ness (DA with MADA-ARZ and MSA with MADA) since
MADA-ARZ does a lot better than MADA on DA (Habash
et al, 2013). Tuning and Test data, however, are tokenized
by MADA-ARZ since we do not assume any knowledge of
the dialect of a test sentence.
773
EgyDevV3.
(4) MSA-Pivot. This MSA-pivoting system
uses Salloum and Habash (2013)?s DA-MSA MT
system followed by an Arabic-English SMT sys-
tem which is trained on both corpora augmented
with the DA-English where the DA side is prepro-
cessed with the same DA-MSA MT system then
tokenized with MADA-ARZ. The result is 67M
tokenized words on the Arabic side. EgyDevV3
was similarly preprocessed with the DA-MSA MT
system and MADA-ARZ and used for tuning the
system parameters. Test sets are similarly prepro-
cessed before decoding with the SMT system.
Baseline MT System Results. We report the re-
sults of our dev set on the four MT systems we
built in Table 1. The MSA-Pivot system produces
the best singleton result among all systems. All
differences in BLEU scores between the four sys-
tems are statistically significant above the 95%
level. Statistical significance is computed using
paired bootstrap re-sampling (Koehn, 2004).
System Training Data (TD) BLEU
Name DA-En MSA-En DA
T
-En TD Size
1. DA-Only 5M 5M 26.6
2. MSA-Only 57M 57M 32.7
3. DA+MSA 5M 57M 62M 33.6
4. MSA-Pivot 5M 57M 5M 67M 33.9
Oracle System Selection 39.3
Table 1: Results from the baseline MT systems and their or-
acle system selection. The training data west used in different
MT systems are also indicated. DA
T
(in the fourth column)
is the DA part of the 5M word DA-En parallel data processed
with the DA-MSA MT system.
Oracle System Selection. We also report in Ta-
ble 1 an oracle system selection where we pick, for
each sentence, the English translation that yields
the best BLEU score. This oracle indicates that
the upper bound for improvement achievable from
system selection is 5.4% BLEU. Excluding dif-
ferent systems from the combination lowered the
overall score between 0.9% and 1.8%, suggesting
the systems are indeed complementary.
4 MT System Selection
The approach we take in this paper benefits from
the techniques and conclusions of previous papers
in that we build different MT systems similar to
those discussed above but instead of trying to find
which one is the best, we try to leverage the use
of all of them by automatically deciding what sen-
tences should go to which system. Our hypothesis
is that these systems complement each other in in-
teresting ways where the combination of their se-
lections could lead to better overall performance
stipulating that our approach could benefit from
the strengths while avoiding the weaknesses of
each individual system.
4.1 Dialect ID Binary Classification
For baseline system selection, we use the clas-
sification decision of Elfardy and Diab (2013)?s
sentence-level dialect identification system to de-
cide on the target MT system. Since the deci-
sion is binary (DA or MSA) and we have four MT
systems, we considered all possible configurations
and determined empirically that the best configu-
ration is to select MSA-Only for the MSA tag and
MSA-Pivot for the DA tag. We do not report other
configuration results due to space restrictions.
4.2 Feature-based Four-Class Classification
For our main approach, we train a four-class clas-
sifier to predict the target MT system to select
for each sentence using only source-language fea-
tures. We experimented with different classifiers
in the Weka Data Mining Tool (Hall et al, 2009)
for training and testing our system selection ap-
proach. The best performing classifier was Naive
Bayes (with Weka?s default settings).
Training Data Class Labels. We run the
5,562 sentences of the classification training
data through our four MT systems and produce
sentence-level BLEU scores (with length penalty).
We pick the name of the MT system with the high-
est BLEU score as the class label for that sen-
tence. When there is a tie in BLEU scores, we pick
the system label that yields better overall BLEU
scores from the systems tied.
Training Data Source-Language Features.
We use two sources of features extracted from
untokenized sentences to train our four-class
classifiers: basic and extended features.
A. Basic Features
These are the same set of features that were used
by the dialect ID tool together with the class label
generated by this tool.
i. Token-Level Features. These features rely on
language models, MSA and Egyptian morphologi-
cal analyzers and a Highly Dialectal Egyptian lex-
icon to decide whether each word is MSA, Egyp-
tian, Both, or Out of Vocabulary.
ii. Perplexity Features. These are two features
that measure the perplexity of a sentence against
774
two language models: MSA and Egyptian.
iii. Meta Features. Features that do not di-
rectly relate to the dialectalness of words in the
given sentence but rather estimate how informal
the sentence is and include: percentage of to-
kens, punctuation, and Latin words, number of to-
kens, average word length, whether the sentence
has any words that have word-lengthening effects
or not, whether the sentence has any diacritized
words or not, whether the sentence has emoticons
or not, whether the sentence has consecutive re-
peated punctuation or not, whether the sentence
has a question mark or not, and whether the sen-
tence has an exclamation mark or not.
iv. The Dialect-Class Feature. We run the sen-
tence through the Dialect ID binary classifier and
we use the predicted class label (DA or MSA) as a
feature in our system. Since the Dialect ID system
was trained on a different data set, we think its de-
cision may provide additional information to our
classifiers.
B. Extended Features
We add features extracted from two sources.
i. MSA-Pivoting Features. Salloum and Habash
(2013) DA-MSA MT system produces interme-
diate files used for diagnosis or debugging pur-
poses. We exploit one file in which the sys-
tem identifies (or, "selects") dialectal words and
phrases that need to be translated to MSA. We ex-
tract confidence indicating features. These fea-
tures are: sentence length (in words), percent-
age of selected words and phrases, number of se-
lected words, number of selected phrases, num-
ber of words morphologically selected as dialec-
tal by a mainly Levantine morphological analyzer,
number of words selected as dialectal by the tool?s
DA-MSA lexicons, number of OOV words against
the MSA-Pivot system training data, number of
words in the sentences that appeared less than 5
times in the training data, number of words in the
sentences that appeared between 5 and 10 times
in the training data, number of words in the sen-
tences that appeared between 10 and 15 times
in the training data, number of words that have
spelling errors and corrected by this tool (e.g.,
word-lengthening), number of punctuation marks,
and number of words that are written in Latin
script.
ii. MT Training Data Source-Side LM Perplex-
ity Features. The second set of features uses per-
plexity against language models built from the
source-side of the training data of each of the four
baseline systems. These four features may tell the
classifier which system is more suitable to trans-
late a given sentence.
4.3 System Selection Evaluation
Development Set. The first part of Table 2 re-
peats the best baseline system and the four-system
oracle combination from Table 1 for convenience.
The third row shows the result of running our sys-
tem selection baseline that uses the Dialect ID bi-
nary decision on the Dev set sentences to decide
on the target MT system. It improves over the best
single system baseline (MSA-Pivot) by a statisti-
cally significant 0.5% BLEU. Crucially, we should
note that this is a deterministic process.
System BLEU Diff.
Best Single MT System Baseline 33.9 0.0
Oracle 39.3 5.4
Dialect ID Binary Selection Baseline 34.4 0.5
Four-Class Classification
Basic Features 35.1 1.2
Extended Features 34.8 0.9
Basic + Extended Features 35.2 1.3
Table 2: Results of baselines and system selection systems
on the Dev set in terms of BLEU. The best single MT system
baseline is MSA-Pivot.
The second part of Table 2 shows the results of
our four-class Naive Bayes classifiers trained on
the classification training data we created. The
first column shows the source of sentence level
features employed. As mentioned earlier, we use
the Basic features alone, the Extended features
alone, and then their combination. The classifier
that uses both feature sources simultaneously as
feature vectors is our best performer. It improves
over our best baseline single MT system by 1.3%
BLEU and over the Dialect ID Binary Classifica-
tion system selection baseline by 0.8% BLEU. Im-
provements are statistically significant.
System BLEU Diff.
DA-Only 26.6
MSA-Only 30.7
DA+MSA 32.4
MSA-Pivot 32.5
Four-System Oracle Combination 38.0 5.5
Best Dialect ID Binary Classifier 32.9 0.4
Best Classifier: Basic + Extended Features 33.5 1.0
Table 3: Results of baselines and system selection systems
on the Blind test set in terms of BLEU.
Blind Test Set. Table 3 shows the results on our
Blind Test set. The first part of the table shows
the results of our four baseline MT systems. The
systems have the same rank as on the Dev set and
775
System All Dialect MSA
DA-Only 26.6 19.3 33.2
MSA-Only 32.7 14.7 50.0
DA+MSA 33.6 19.4 46.3
MSA-Pivot 33.9 19.6 46.4
Four-System Oracle Combination 39.3 24.4 52.1
Best Performing Classifier 35.2 19.8 50.0
Table 4: Dialect breakdown of performance on the Dev set
for our best performing classifier against our four baselines
and their oracle combination. Our classifier does not know
of these subsets, it runs on the set as a whole; therefore, we
repeat its results in the second column for convenience.
MSA-Pivot is also the best performer. The differ-
ences in BLEU are statistically significant. The
second part shows the four-system oracle combi-
nation which shows a 5.5% BLEU upper bound
on improvements. The third part shows the re-
sults of the Dialect ID Binary Classification which
improves by 0.4% BLEU. The last row shows
the four-class classifier results which improves by
1.0% BLEU over the best single MT system base-
line and by 0.6% BLEU over the Dialect ID Bi-
nary Classification. Results on the Blind Test set
are consistent with the Dev set results.
5 Discussion and Error Analysis
DA versus MSA Performance. In Table 4, col-
umn All illustrates the results over the entire Dev
set, while columns DA and MSA show system
performance on the DA and MSA subsets of the
Dev set, respectively. The best single baseline MT
system for DA is MSA-Pivot has a large room for
improvement given the oracle upper bound (4.8%
BLEU absolute). However, our best system selec-
tion approach improves over MSA-Pivot by a small
margin of 0.2% BLEU absolute only, albeit a sta-
tistically significant improvement. The MSA col-
umn oracle shows a smaller improvement of 2.1%
BLEU absolute over the best single MSA-Only MT
system. Furthermore, when translating MSA with
our best system selection performer we get the
same results as the best baseline MT system for
MSA even though our system does not know the
dialect of the sentences a priori. If we consider the
breakdown of the performance in our best overall
(33.9% BLEU) single baseline MT system (MSA-
Pivot), we observe that the performance on MSA
is about 3.6% absolute BLEU points below our
best results; this suggests that most of the system
selection gain over the best single baseline is on
MSA selection.
Manual Error Analysis. We performed manual
error analysis on a Dev set sample of 250 sen-
tences distributed among the different dialects and
genres. Our best performing classifier selected the
best system in 48% of the DA cases and 52% of
the MSA cases. We did a detailed manual error
analysis for the cases where the classifier failed to
predict the best MT system. The sources of errors
we found cover 89% of the cases. In 21% of the
error cases, our classifier predicted a better trans-
lation than the one considered gold by BLEU due
to BLEU bias, e.g., severe sentence-level length
penalty due to an extra punctuation in a short sen-
tence. Also, 3% of errors are due to bad refer-
ences, e.g., a dialectal sentence in an MSA set that
the human translators did not understand.
A group of error sources resulted from MSA
sentences classified correctly as MSA-Only; how-
ever, one of the other three systems produced bet-
ter translations for two reasons. First, since the
MSA training data is from an older time span than
the DA data, 10% of errors are due to MSA sen-
tences that use recent terminology (e.g., Egyp-
tian revolution 2011: places, politicians, etc.)
that appear in the DA training data. Also, web
writing styles in MSA sentences such as blog
style (e.g., rhetorical questions), blog punctuation
marks (e.g., "..", "???!!"), and formal MSA forum
greetings resulted in 23%, 16%, and 6% of the
cases, respectively.
Finally, in 10% of the cases our classifier is con-
fused by a code-switched sentence, e.g., a dialec-
tal proverb in an MSA sentence or a weak MSA
literal translation of dialectal words and phrases.
Some of these cases may be solved by adding
more features to our classifier, e.g., blog style writ-
ing features, while others need a radical change to
our technique such as word and phrase level di-
alect identification for MT system combination of
code-switched sentences.
6 Conclusion and Future Work
We presented a sentence-level classification ap-
proach for MT system selection for diglossic lan-
guages. We got a 1.0% BLEU improvement over
the best baseline single MT system. In the future
we plan to add more training data to see the effect
on the accuracy of system selection. We plan to
give different weights to different training exam-
ples based on the drop in BLEU score the exam-
ple can cause if classified incorrectly. We also plan
to explore confusion-network combination and re-
ranking techniques based on target language fea-
tures.
776
References
Murat Akbacak, Dimitra Vergyri, Andreas Stolcke,
Nicolas Scheffer, and Arindam Mandal. 2011.
Effective arabic dialect classification using diverse
phonotactic models. In INTERSPEECH, volume 11,
pages 737?740.
Fadi Biadsy, Julia Hirschberg, and Nizar Habash.
2009. Spoken arabic dialect identification using
phonotactic modeling. In Proceedings of the Work-
shop on Computational Approaches to Semitic Lan-
guages at the meeting of the European Associa-
tion for Computational Linguistics (EACL), Athens,
Greece.
Heba Elfardy and Mona Diab. 2013. Sentence Level
Dialect Identification in Arabic. In Proceedings of
the 51th Annual Meeting of the Association for Com-
putational Linguistics (ACL-13), Sofia, Bulgaria.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in arabic.
In Proceedings of the 18th International Conference
on Application of Natural Language to Information
Systems (NLDB2013), MediaCity, UK.
Charles F Ferguson. 1959. Diglossia. Word,
15(2):325?340.
David Graff and Christopher Cieri. 2003. English Gi-
gaword, LDC Catalog No.: LDC2003T05. Linguis-
tic Data Consortium, University of Pennsylvania.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
S. Hasan, O. Bender, and H. Ney. 2006. Rerank-
ing translation hypotheses using structural proper-
ties. In EACL?06 Workshop on Learning Structured
Information in Natural Language Applications.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 98?107. Associa-
tion for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies: Short Papers, pages 81?84.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain,
July. Association for Computational Linguistics.
Wei-Yun Ma and Kathleen McKeown. 2013. Using
a supertagged dependency language model to select
a good translation in system combination. In Pro-
ceedings of NAACL-HLT, pages 433?438.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109, Cairo, Egypt.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Franz Josef Och. 2004.
A smorgasbord of features for statistical machine
translation. In Meeting of the North American chap-
ter of the Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Proceed-
ings of the 41st Annual Conference of the Associa-
tion for Computational Linguistics, pages 160?167,
Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, PA.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 312?319, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological
Tagging, Diacritization, and Lemmatization Using
Lexeme Models and Feature Ranking. In Proceed-
ings of ACL-08: HLT, Short Papers, pages 117?120,
Columbus, Ohio.
Fatiha Sadat and Nizar Habash. 2006. Combination
of Arabic preprocessing schemes for statistical ma-
777
chine translation. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1?8, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Hassan Sajjad, Kareem Darwish, and Yonatan Be-
linkov. 2013. Translating dialectal arabic to en-
glish. In The 51st Annual Meeting of the Association
for Computational Linguistics - Short Papers (ACL
Short Papers 2013), Sofia, Bulgaria.
Wael Salloum and Nizar Habash. 2011. Dialectal to
Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Pro-
ceedings of the First Workshop on Algorithms and
Resources for Modelling of Dialects and Language
Varieties, pages 10?21, Edinburgh, Scotland.
Wael Salloum and Nizar Habash. 2013. Dialectal
Arabic to English Machine Translation: Pivoting
through Modern Standard Arabic. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT), At-
lanta, GA.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in
the Americas (AMTA), Denver, Colorado.
Andreas Stolcke. 2002. SRILM an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the jhu system combination scheme
for wmt 2011. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 171?176.
Association for Computational Linguistics.
Omar F Zaidan and Chris Callison-Burch. 2011. The
arabic online commentary dataset: an annotated
dataset of informal arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-11), pages 37?41.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine Translation of Arabic Di-
alects. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 49?59, Montr?al, Canada, June. As-
sociation for Computational Linguistics.
778
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 10?21,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Dialectal to Standard Arabic Paraphrasing
to Improve Arabic-English Statistical Machine Translation
Wael Salloum and Nizar Habash
Center for Computational Learning Systems
Columbia University
{wael,habash}@ccls.columbia.edu
Abstract
This paper is about improving the quality
of Arabic-English statistical machine trans-
lation (SMT) on dialectal Arabic text us-
ing morphological knowledge. We present a
light-weight rule-based approach to producing
Modern Standard Arabic (MSA) paraphrases
of dialectal Arabic out-of-vocabulary (OOV)
words and low frequency words. Our ap-
proach extends an existing MSA analyzer with
a small number of morphological clitics, and
uses transfer rules to generate paraphrase lat-
tices that are input to a state-of-the-art phrase-
based SMT system. This approach improves
BLEU scores on a blind test set by 0.56 abso-
lute BLEU (or 1.5% relative). A manual error
analysis of translated dialectal words shows
that our system produces correct translations
in 74% of the time for OOVs and 60% of the
time for low frequency words.
1 Introduction
Much work has been done on Modern Standard Ara-
bic (MSA) natural language processing (NLP) and
machine translation (MT). In comparison, research
on dialectal Arabic (DA), the unstandardized spoken
varieties of Arabic, is still lacking in NLP in general
and MT in particular. In this paper we address the is-
sue of MT out-of-vocabulary (OOV) terms and low
frequency terms in highly dialectal Arabic text.
We present a light-weight rule-based approach to
producing MSA morphological paraphrases of DA
OOV words and low frequency words. However, we
don?t do lexical translation. Our approach extends
an existing MSA analyzer to two DA varieties (Lev-
antine and Egyptian) with less than 40 morphologi-
cal clitics. We use 11 morphological transfer rules
to generate paraphrase lattices that are input to a
state-of-the-art phrase-based statistical MT (SMT)
system. Our system improves BLEU scores on a
blind test set by 0.56 absolute BLEU (or 1.5% rela-
tive). A manual error analysis of translated dialectal
words shows that our system produces correct trans-
lations in 74% of the time for OOVs and 60% of the
time for low frequency words.
The rest of this paper is structured as follows:
Section 2 is related work, Section 3 presents linguis-
tic challenges and motivation, Section 4 details our
approach and Section 5 presents results evaluating
our approach under a variety of conditions.
2 Related Work
Dialectal Arabic NLP Much work has been done
in the context of MSA NLP (Habash, 2010). Specif-
ically for Arabic-to-English SMT, the importance of
tokenization using morphological analysis has been
shown by many researchers (Lee, 2004; Zollmann
et al, 2006; Habash and Sadat, 2006). In contrast,
research on DA NLP is still in its early stages: (Ki-
lany et al, 2002; Kirchhoff et al, 2003; Duh and
Kirchhoff, 2005; Habash and Rambow, 2006; Chi-
ang et al, 2006). Several researchers have explored
the idea of exploiting existing MSA rich resources
to build tools for DA NLP, e.g., Chiang et al (2006)
built syntactic parsers for DA trained on MSA tree-
banks. Such approaches typically expect the pres-
ence of tools/resources to relate DA words to their
MSA variants or translations. Given that DA and
MSA do not have much in terms of parallel cor-
pora, rule-based methods to translate DA-to-MSA
10
or other methods to collect word-pair lists have been
explored. For example, Abo Bakr et al (2008) intro-
duced a hybrid approach to transfer a sentence from
Egyptian Arabic into MSA. This hybrid system con-
sisted of a statistical system for tokenizing and tag-
ging, and a rule-based system for constructing dia-
critized MSA sentences. Moreover, Al-Sabbagh and
Girju (2010) described an approach of mining the
web to build a DA-to-MSA lexicon. In the con-
text of DA-to-English SMT, Riesa and Yarowsky
(2006) presented a supervised algorithm for online
morpheme segmentation on DA that cut the OOVs
by half.
Machine Translation for Closely Related Lan-
guages Using closely related languages has been
shown to improve MT quality when resources are
limited. Hajic? et al (2000) argued that for very close
languages, e.g., Czech and Slovak, it is possible
to obtain a better translation quality by using sim-
ple methods such as morphological disambiguation,
transfer-based MT and word-for-word MT. Zhang
(1998) introduced a Cantonese-Mandarin MT that
uses transformational grammar rules. In the context
of Arabic dialect translation, Sawaf (2010) built a
hybrid MT system that uses both statistical and rule-
based approaches for DA-to-English MT. In his ap-
proach, DA is normalized into MSA using a dialec-
tal morphological analyzer. This use of ?resource-
rich? related languages is a specific variant of the
more general approach of using pivot/bridge lan-
guages (Utiyama and Isahara, 2007; Kumar et al,
2007). In the case of MSA and DA variants, it
is plausible to consider the MSA variants of a DA
phrase as monolingual paraphrases (Callison-Burch
et al, 2006; Habash, 2008; Du et al, 2010).
This paper presents results on a rule-based sys-
tem to generate alternative paraphrases for DA OOV
words and low frequency words to help improve
SMT from highly dialectal Arabic to English. Our
work is most similar to Sawaf (2010)?s approach to
DA normalization into MSA, although we shy away
from the term in our work since we do not produce a
single MSA version of the input to pass on to MSA-
English MT. Instead we pass multiple paraphrases
(or alternative normalizations) as a lattice to an SMT
system, in a manner similar to Du et al (2010). Cer-
tain aspects of our approach are similar to Riesa
and Yarowsky (2006)?s, in that we use morpholog-
ical analysis for DA to help DA-English MT; but
unlike them and similar to Sawaf (2010), we use a
rule-based approach to model DA morphology. Our
morphological analysis implementation is quite sim-
ilar to the approach taken by Abo Bakr et al (2008),
which extend existing MSA analyzers through rules;
however, unlike them, we are not interested in gen-
erating MSA per se, but rather to use it as a bridge
to English MT. Our interest in OOV words is simi-
lar to Habash (2008), who compared multiple tech-
niques for handling MSA OOVs; however, unlike
him, we target dialectal phenomena and we use lat-
tices as input to the SMT system. Also related is the
recent work by Nakov and Ng (2011), who use mor-
phological knowledge to generate paraphrases for a
morphologically rich language, Malay, to extend the
phrase table in a Malay-to-English SMT system.
3 Challenge and Motivation
We are primarily interested in improving Arabic-
English SMT on highly dialectal text. This partic-
ular type of text has many challenges. We discuss
these challenges and motivate our research approach
with an analysis of DA OOV terms in a state-of-the-
art SMT system.
3.1 Arabic Linguistic Challenges
The Arabic language poses many challenges for
NLP. Arabic is a morphologically complex language
which includes rich inflectional morphology, ex-
pressed both templatically and affixationally, and
several classes of attachable clitics. For exam-
ple, the Arabic word A? 	E?J.

J?J
?? w+s+y-ktb-wn+hA
1
?and they will write it? has two proclitics (+? w+
?and? and +? s+ ?will?), one prefix -?


y- ?3rd per-
son?, one suffix 	??- -wn ?masculine plural? and one
pronominal enclitic A?+ +hA ?it/her?. Additionally,
Arabic is written with optional diacritics that primar-
ily specify short vowels and consonantal doubling.
The absence of these diacritics together with the lan-
guage?s complex morphology lead to a high degree
of ambiguity: the Buckwalter Arabic Morphological
1Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007): (in alphabetical or-
der) Abt?jHxd?rzs?SDTD???fqklmnhwy and the additional sym-
bols: ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', h?

?, ? ?.
11
Analyzer (BAMA), for instance, produces an average
of 12 analyses per word. Moreover, some letters in
Arabic are often spelled inconsistently which leads
to an increase in both sparsity (multiple forms of the
same word) and ambiguity (same form correspond-
ing to multiple words), e.g., variants of Hamzated
Alif,

@ ? or @ A?, are often written without their
Hamza (Z ?): @ A; and the Alif-Maqsura (or dotless
Ya) ? ? and the regular dotted Ya ?


y are often used
interchangeably in word final position (Kholy and
Habash, 2010). Arabic complex morphology and
ambiguity are handled using tools for disambigua-
tion and tokenization (Habash and Rambow, 2005;
Diab et al, 2007). For our SMT system, we pre-
process the Arabic text so that it is tokenized in
the Penn Arabic Treebank tokenization (Maamouri
et al, 2004), Alif/Ya normalized and undiacritized.
These measures have an important effect on reduc-
ing overall OOV rate (Habash, 2008).
3.2 Dialectal Arabic Challenges
Contemporary Arabic is in fact a collection of vari-
eties: MSA, which has a standard orthography and
is used in formal settings, and DAs, which are com-
monly used informally and with increasing presence
on the web, but which do not have standard or-
thographies. There are several varieties of DA which
primarily vary geographically, e.g., Levantine Ara-
bic, Egyptian Arabic, etc. DAs differ from MSA
phonologically, morphologically and to some lesser
degree syntactically. The differences between MSA
and DAs have often been compared to Latin and the
Romance languages (Habash, 2006). The morpho-
logical differences are most noticeably expressed in
the use of clitics and affixes that do not exist in
MSA. For instance, the Levantine Arabic equivalent
of the MSA example above is A??J.

J?J
k? w+H+y-
ktb-w+hA ?and they will write it?. The optionality
of vocalic diacritics helps hide some of the differ-
ences resulting from vowel changes; compare the
diacritized forms: Levantine wHayuktubuwhA and
MSA wasayaktubuwnahA.
All of the NLP challenges of MSA described
above are shared by DA. However, the lack of stan-
dard orthographies for the dialects and their numer-
ous varieties pose new challenges. Additionally,
DAs are rather impoverished in terms of available
tools and resources compared to MSA; e.g., there is
very little parallel DA-English corpora and almost
no MSA-DA parallel corpora. The number and so-
phistication of morphological analysis and disam-
biguation tools in DA is very limited in compari-
son to MSA (Duh and Kirchhoff, 2005; Habash and
Rambow, 2006; Abo Bakr et al, 2008). MSA tools
cannot be effectively used to handle DA: Habash and
Rambow (2006) report that less than two-thirds of
Levantine verbs can be analyzed using an MSA mor-
phological analyzer.
3.3 Dialectal Arabic OOVs
We analyzed the types of OOVs in our dev set
against our large system (see Section 5) with an eye
for dialectal morphology. The token OOV rate is
1.51% and the type OOV rate is 7.45%; although the
token OOV rate may seem small, it corresponds to
almost one third of all sentences having one OOV at
least (31.48%). In comparison with MSA test sets,
such as NIST MTEval 2006?s token OOV rate of
0.8% (and 3.42% type OOV rate), these numbers
are very high specially given the size of training
data. Out of these OOVs, 25.9% have MSA read-
ings or are proper nouns. The rest, 74.1%, are di-
alectal words. We classified the dialectal words into
two types: words that have MSA-like stems and di-
alectal affixational morphology (affixes/clitics) and
those that have dialectal stem and possibly dialectal
morphology. The former set accounts for almost half
of all OOVS (49.7%) or almost two thirds of all di-
alectal OOVS. In this paper we only target dialectal
affixational morphology cases as they are the largest
class involving dialectal phenomena that do not re-
quire extension to our stem lexica. The morphologi-
cal coverage of the analyzer we use, ALMOR, which
itself uses the BAMA databases is only 21% of all
the OOV words. Our analyzer, ADAM, presented in
Section 4.2, improves coverage substantially.
It is important to note that a word can be in-
vocabulary (INV) but not have a correct possible
translation in the phrase table. Some of these words
may be of such low frequency that their various pos-
sible translations simply do not appear in the train-
ing data. Others may have a frequent MSA read-
ing and an infrequent/unseen DA reading (or vice
versa).
12
4 Approach
Our basic approach to address the issue of transla-
tional OOVs is to provide rule-based paraphrases of
the source language words into words and phrases
that are INV. The paraphrases are provided as al-
ternatives in an input lattice to the SMT system.
This particular implementation allows this approach
to be easily integrated with a variety of SMT sys-
tems. The alternatives include different analyses
of the same original word and/or translations into
MSA. We focus on the question of Arabic dialects,
although the approach can be extended to handle
low frequency MSA words also that may have been
mis-tokenized by the MSA preprocessing tools. As
mentioned above, we only report in this work on di-
alect morphology translation to MSA and we leave
lemma/word translation to future work. We identify
four distinct operations necessary for this approach
and evaluate different subsets of them in Section 5.
1. Selection. Identify the words to handle, e.g.,
OOVs or low frequency words.
2. Analysis. Produce a set of alternative analyses
for each word.
3. Transfer. Map each analysis into one or more
target analyses.
4. Generation. Generate properly tokenized
forms of the target analyses.
The core steps of analysis-transfer-generation are
similar to generic transfer-based MT (Dorr et al,
1999). In essence our approach can be thought of
as a mini-rule-based system that is used to hybridize
an SMT system (Simard et al, 2007; Sawaf, 2010).
4.1 Selection
The most obvious set of words to select for para-
phrasing is the phrase-table OOV words. We iden-
tify them by comparing each word in the source
text against all phrase-table singletons. Another set
of words to consider includes low frequency words
(DA or MSA), which are less likely to be associated
with good phrase-table translations. We compute the
frequency of such words against the original training
data. We further extend the idea of frequency-based
selection to typed-frequency selection in which we
consider different frequency cut-offs for different
types of words (MSA or DA). Evaluation and more
details are presented in Section 5.3.
4.2 Analysis
Whereas much work has been done on MSA mor-
phological analysis (Al-Sughaiyer and Al-Kharashi,
2004), a small handful of efforts have targeted the
creation of dialectal morphology systems (Kilany et
al., 2002; Habash and Rambow, 2006; Abo Bakr et
al., 2008). In this section, we present a new dialec-
tal morphological analyzer, ADAM, built as an ex-
tension to an already existing MSA analyzer. We
only focus on extensions that address dialectal af-
fixes and clitics, as opposed to stems, which we plan
to address in future work. This approach to extend-
ing an MSA analyzer is similar to work done by
Abo Bakr et al (2008) and it contrasts as rather a
shallow/quick-and-dirty solution compared to other
more demanding efforts on building dialectal an-
alyzers from scratch, such as the MAGEAD sys-
tem (Habash and Rambow, 2006; Altantawy et al,
2011).
4.2.1 ADAM: Analyzer for Dialectal Arabic
Morphology
ADAM is built on the top of BAMA database
(Buckwalter, 2004) as used in the ALMOR morpho-
logical analyzer/generator (Habash, 2007), which is
the rule-based component of the MADA system for
morphological analysis and disambiguation of Ara-
bic (Habash and Rambow, 2005; Roth et al, 2008).
The ALMOR system presents analyses as lemma and
feature-value pairs including clitics.
The BAMA databases contain three tables of
Arabic stems, complex prefixes and complex suf-
fixes2 and three additional tables with constraints
on matching them. MSA, according to the BAMA
databases, has 1,208 complex prefixes and 940 com-
plex suffixes, which correspond to 49 simple pre-
fixes/proclitics and 177 simple suffixes/enclitics, re-
spectively. The number of combinations in prefixes
is a lot bigger than in suffixes, which explains the
different proportions of complex affixes to simple
affixes.
We extended the BAMA database through a
2We define a complex prefix as the full sequence of pre-
fixes/proclitics that may appear at the beginning of a word.
Complex suffixes are defined similarly.
13
Dialect Word ??J.

J?J
kA?? wmAHyktblw ?And he will not write for him?
Analysis Proclitics [ Lemma & Features ] Enclitics
w+ mA+ H+ yktb +l +w
conj+ neg+ fut+ [katab IV subj:3MS voice:act] +prep +pron3MS
and+ not+ will+ he writes +for +him
Transfer Word 1 Word 2 Word 3
Proclitics [ Lemma & Features ] [ Lemma & Features ] [ Lemma & Features ] Enclitic
conj+ [ lan ] [katab IV subj:3MS voice:act] [ li ] +pron3MS
and+ will not he writes for +him
Generation w+ ln yktb l +h
MSA Phrase ?? I.

J?K

	??? wln yktb lh ?And he will not write for him?
Figure 1: An example illustrating the analysis-transfer-generation steps to translate a word with dialectal morphology
into its MSA equivalent phrase.
set of rules that add new Levantine/Egyptian
dialectal affixes and clitics by copying and ex-
tending existing MSA affixes/clitics. For instance,
the dialectal future proclitic +h H+ ?will? has
a similar behavior to the standard Arabic future
particle +? s+. As such, an extension rule would
create a copy of each occurrence of the MSA
prefix and replace it with the dialectal prefix.
The algorithm that uses this rule to extend the
BAMA database adds the prefix Ha/FUT_PART
and many other combinations involving it,
e.g., wa/PART+Ha/FUT_PART+ya/IV3MS, and
fa/CONJ+Ha/FUT_PART+na/IV1P. We reserve
discussion of other more complex mappings with
no exact MSA equivalence to a future publication
on ADAM.
The rules (89 in total) introduce 11 new dialectal
proclitics (plus spelling variants and combinations)
and 27 dialectal enclitics (again, plus spelling vari-
ants and combinations). ADAM?s total of simple pre-
fixes and suffixes increases to 60 (22% increase) and
204 (15% increase) over BAMA, respectively. The
numbers for complex prefixes and suffixes increase
at a faster rate to 3,234 (168% increase) and (142%
increase), respectively.
As an example of ADAM output, consider the sec-
ond set of rows in Figure 1, where a single analysis
is shown.
4.2.2 ADAM performance
We conducted an analysis of ADAM?s behavior
over the OOV set analyzed in Section 3.3. Whereas
ALMOR (before ADAM) only produces analyzes for
21% of all the OOV words, ADAM covers almost
63%. Among words with dialectal morphology,
ADAM?s coverage is 84.4%. The vast majority of the
unhandled dialectal morphology cases involve a par-
ticular Levantine/Egyptian suffix ?+ +? ?not?. We
plan to address these cases in the future. In about
10% of all the analyzed words, ADAM generates
alternative dialectal readings to supplement exist-
ing ALMOR MSA analyses, e.g., I.

J?K. bktb has an
MSA (and coincidentally dialectal) analysis of ?with
books? and ADAM also generates the dialectal only
analysis ?I write?.
4.3 Transfer
In the transfer step, we map ADAM?s dialectal anal-
yses to MSA analyses. This step is implemented
using a set of transfer rules (TR) that operate on
the lemma and feature representation produced by
ADAM. The TRs can change clitics, features or
lemma, and even split up the dialectal word into
multiple MSA word analyses. Crucially the input
and output of this step are both in the lemma and
feature representation (Habash, 2007). A particular
analysis may trigger more than one rule resulting in
multiple paraphrases. This only adds to the fan-out
which started with the original dialectal word having
multiple analyses.
Our current system uses 11 rules only, which were
determined to handle all the dialectal clitics added in
ADAM. As more clitics are added in ADAM, more
TRs will be needed. As examples, two TRs which
lead to the transfer output shown in the third set of
rows in Figure 1 can be described as follows:3
3All of our rules are written in a declarative form, which
14
? if the dialectal analysis shows future and nega-
tion proclitics, remove them from the word and
create a new word, the MSA negative-future
particle 	?? ln, to precede the current word and
which inherits all proclitics preceding the fu-
ture and negation proclitics.
? if the dialectal analysis shows the dialectal in-
direct object enclitic, remove it from the word
and create a new word to follow the current
word; the new word is the preposition +? l+
with an enclitic pronoun that matches the fea-
tures of the indirect object.
In the current version evaluated in this paper, we al-
ways provide a lower-scored back-off analysis that
removes all dialectal clitics as an option.
4.4 Generation
In this step, we generate Arabic words from all anal-
yses produced by the previous steps. The gener-
ation is done using the general tokenizer TOKAN
(Habash, 2007) to produce Arabic Treebank (ATB)
scheme tokenizations. TOKAN is used in the base-
line system to generate tokenizations for MSA from
morphologically disambiguated input in the same
ATB scheme (see Section 5.1). The various gener-
ated forms are added in the lattices, which are then
input to the SMT system.
5 Evaluation on Machine Translation
5.1 Experimental Setup
We use the open-source Moses toolkit (Koehn et
al., 2007) to build two phrase-based SMT systems
trained on two different data conditions: a medium-
scale MSA-only system trained using a newswire
(MSA-English) parallel text with 12M words on
the Arabic side (LDC2007E103) and a large-scale
MSA/DA-mixed system (64M words on the Arabic
side) trained using several LDC corpora including
some limited DA data. Both systems use a stan-
dard phrase-based architecture. The parallel cor-
pus is word-aligned using GIZA++ (Och and Ney,
2003). Phrase translations of up to 10 words are
extracted in the Moses phrase table. The language
model for both systems is trained on the English
may be complicated to explain given the allotted space, as such
we present only the functional description of the TRs.
side of the large bitext augmented with English Gi-
gaword data. We use a 5-gram language model with
modified Kneser-Ney smoothing. Feature weights
are tuned to maximize BLEU on the NIST MTEval
2006 test set using Minimum Error Rate Training
(Och, 2003). This is only done on the baseline sys-
tems.
For all systems, the English data is tokenized us-
ing simple punctuation-based rules. The Arabic side
is segmented according to the Arabic Treebank tok-
enization scheme (Maamouri et al, 2004) using the
MADA+TOKAN morphological analyzer and tok-
enizer (Habash and Rambow, 2005) ? v3.1 (Roth et
al., 2008). The Arabic text is also Alif/Ya normal-
ized (Habash, 2010). MADA-produced Arabic lem-
mas are used for word alignment.
Results are presented in terms of BLEU (Papineni
et al, 2002), NIST (Doddington, 2002) and ME-
TEOR (Banerjee and Lavie, 2005) metrics.4 How-
ever, all optimizations were done against the BLEU
metric. All evaluation results are case insensitive.
All of the systems we present use the lattice input
format to Moses (Dyer et al, 2008), including the
baselines which do not need them. We do not re-
port on the non-lattice baselines, but in initial exper-
iments we conducted, they did not perform as well
as the degenerate lattice version.
The Devtest Set Our devtest set consists of sen-
tences containing at least one non-MSA segment (as
annotated by LDC)5 in the Dev10 audio develop-
ment data under the DARPA GALE program. The
data contains broadcast conversational (BC) seg-
ments (with three reference translations), and broad-
cast news (BN) segments (with only one reference,
replicated three times). The data set contained a
mix of Arabic dialects, with Levantine Arabic be-
ing the most common variety. The particular na-
ture of the devtest being transcripts of audio data
adds some challenges to MT systems trained on pri-
marily written data in news genre. For instance,
each of the source and references in the devtest set
contained over 2,600 uh-like speech effect words
(uh/ah/oh/eh), while the baseline translation system
we used only generated 395. This led to severe
4We use METEOR version 1.2 with four match modules:
exact, stem, wordnet, and paraphrases.
5http://www.ldc.upenn.edu/
15
brevity penalty by the BLEU metric. As such, we re-
moved all of these speech effect words in the source,
references and our MT system output. Another sim-
ilar issue was the overwhelming presence of com-
mas in the English reference compared to the Ara-
bic source: each reference had about 14,200 com-
mas, while the source had only 64 commas. Our
MT system baseline predicted commas in less than
half of the reference cases. Similarly we remove
commas from the source, references, and MT out-
put. We do this to all the systems we compare in this
paper. However, even with all of this preprocess-
ing, the length penalty was around 0.95 on average
in the large system and around 0.85 on average in
the medium system. As such, we report additional
BLEU sub-scores, namely the unigram and bigram
precisions (Prec-1 and Prec-2, respectively), to pro-
vide additional understanding of the nature of our
improvements.
We split this devtest set into two sets: a develop-
ment set (dev) and a blind test set (test). We report
all our analyses and experiments on the dev set and
reserve the test set for best parameter runs at the end
of this section. The splitting is done randomly at
the document level. The dev set has 1,496 sentences
with 32,047 untokenized Arabic words. The test set
has 1,568 sentences with 32,492 untokenized Arabic
words.
5.2 Handling Out-of-Vocabulary Words
In this section, we present our results on handling
OOVs in our baseline MT system following the ap-
proach we described in Section 4. The results are
summarized in Table 1. The table is broken into
two parts corresponding to the large and medium
systems. Each part contains results in BLEU, Prec-
1 (unigram precision), Prec-2 (bigram precision),
NIST and METEOR metrics. The performance of
the large system is a lot better than the medium sys-
tem in all experiments. Some of the difference is
simply due to training size; however, another factor
is that the medium system is trained on MSA only
data while the large system has DA in its training
data.
We compare the baseline system (first row) to two
methods of OOV handling through dialectal para-
phrase into MSA. The first method uses the ADAM
morphological analyzer and generates directly skip-
ping the transfer step to MSA. Although this may
create implausible output for many cases, it is suf-
ficient for some, especially through the system?s
natural addressing of orthographic variations. This
method appears in Table 1 as ADAM Only. The sec-
ond method includes the full approach as discussed
in Section 4, i.e., including the transfer step.
The use of the morphological analyzer only
method (ADAM Only) yields positive improvements
across all metrics and training data size conditions.
In the medium system, the improvement is around
0.42% absolute BLEU (or 2.1% relative). The large
system improves by about 0.34% absolute BLEU (or
almost 1% relative). Although these improvements
are small, they are only accomplished by targeting a
part of the OOV words (about 0.6% of all words).
The addition of transfer rules leads to further
modest improvements in both large and medium
systems according to BLEU; however, the NIST
and METEOR metrics yield negative results in the
medium system. A possible explanation for the
difference in behavior is that paraphrase-based ap-
proaches to MT often suffer in smaller data con-
ditions since the paraphrases they map into may
themselves be OOVs against a limited system. Our
transfer approach also has a tendency to generate
longer paraphrases as options, which may have lead
to more fragmentation in the METEOR score algo-
rithm. In terms of BLEU scores, the full system
(analysis and transfer) improves over the baseline
on the order of 0.5% BLEU absolute. The relative
BLEU score in the large and medium systems are
1.24% and 2.54% respectively.
All the systems in Table 1 do not drop unhan-
dled OOVs, thus differing from the most common
method of ?handling? OOV, which is known to
game popular MT evaluation metrics such as BLEU
(Habash, 2008). In fact, if we drop OOVs in our
baseline system, we get a higher BLEU score of
36.36 in the large system whose reported base-
line gets 36.16 BLEU. That said, our best result
with OOV handling produces a higher BLEU score
(36.61) which is a nice result for doing the right
thing and not just deleting problem words. All dif-
ferences in BLEU scores in the large system are sta-
tistically significant above the 95% level. Statistical
significance is computed using paired bootstrap re-
sampling (Koehn, 2004).
16
Large (64M words) Medium (12M words)
System BLEU Prec-1 Prec-2 NIST METEOR BLEU Prec-1 Prec-2 NIST METEOR
Baseline 36.16 74.56 45.04 8.9958 52.59 20.09 63.69 30.89 6.0039 40.85
ADAM Only 36.50 74.79 45.22 9.0655 52.95 20.51 64.37 31.22 6.1994 41.80
ADAM+Transfer 36.61 74.85 45.37 9.0825 53.02 20.60 64.70 31.48 6.1740 41.77
Table 1: Results for the dev set under large and medium training conditions. The baseline is compared to using
dialectal morphological analysis only and analysis plus transfer to MSA. BLEU and METEOR scores are presented
as percentages.
Large (64M words)
System BLEU Prec-1 Prec-2 NIST METEOR
Baseline 36.16 74.56 45.04 8.9958 52.59
ADAM+Transfer 36.61 74.85 45.37 9.0825 53.02
+ Freq x ? 10 36.71 74.89 45.50 9.0821 52.97
+ Freq xMSA ? 10 36.62 74.86 45.38 9.0816 52.96
+ Freq xDIAMSA ? 13 36.66 74.86 45.43 9.0836 53.01
+ Freq xDIA ? 45 36.73 75.00 45.57 9.0961 53.03
+ Freq xMSA ? 10 + xDIAMSA ? 13 + xDIA ? 45 36.78 74.96 45.61 9.0926 52.96
Table 2: Results for the dev set under large training condition, varying the set of words selected for MSA paraphrasing.
5.3 Extending Word Selection
Following the observation that some dialectal
words may not pose a challenge to SMT since they
appear frequently in training data, while some MSA
words may be challenging since they are infrequent,
we conduct a few experiments that widen the set of
words selected for DA-MSA paraphrasing. We re-
port our results on the large data condition only. Re-
sults are shown in Table 2. The baseline and best
system from Table 1 are repeated for convenience.
We consider two types of word-selection exten-
sions beyond OOVs. First, we consider frequency-
based selection, where all words with less than or
equal to a frequency of x are considered for para-
phrasing in addition to being handled in the system?s
phrase table. Many low frequency words actually
end up being OOVs as far as the phrase table is con-
cerned since they are not aligned properly or at all by
GIZA++. Secondly we consider a typed-frequency
approach, where different frequency values are con-
sidered depending on wether a word is MSA only,
dialect only or has both dialect and MSA readings.
We determine MSA words to be those that have AL-
MOR analyses but no new ADAM analyses. Dialect-
only words are those that have ADAM analyses but
no ALMOR analyses. Finally, dialect/MSA words
are those that have ALMOR analyses and get more
dialect analyses through ADAM. The intuition be-
hind the distinction is that problematic MSA only
words may be much less frequent than problematic
dialectal words.
We conducted a large number of experiments to
empirically determine the best value for x in the
frequency-based approach and xMSA, xDIA, and
xDIAMSA for the typed frequency approach. For
the typed frequency approach, we took a greedy path
to determine optimal values for each case and then
used the best results collectively. Our best values
are presented in Table 2. Both frequency-based ap-
proaches improve over the best results of only tar-
geting OOVs. Further more, the fine-tuned typed
frequency approach even yields further improve-
ments leading to 0.62% absolute BLEU improve-
ment over the baseline (or 1.71% relative). This
score is statistically significant against the baseline
and the ADAM+Transfer system as measured using
paired bootstrap resampling (Koehn, 2004).
5.4 Blind Test Results
We apply our two basic system variants and best re-
sult with typed frequency selection to the blind test
set. The results are shown in Table 3. The test set
overall has slightly higher scores than the dev set,
suggesting it may be easier to translate relatively.
17
Large (64M words)
System BLEU Prec-1 Prec-2 NIST METEOR
Baseline 37.24 75.12 46.40 9.1599 52.93
ADAM Only 37.63 75.40 46.59 9.2414 53.39
ADAM+Transfer 37.71 75.46 46.70 9.2472 53.41
+ Freq xMSA ? 10 + xDIAMSA ? 13 + xDIA ? 45 37.80 75.47 46.82 9.2578 53.44
Table 3: Results for the blind test set under large training condition, comparing our best performing settings.
All of our system variants improve over the baseline
and show the same rank in performance as on the dev
set. Our best performer improves over the baseline
by 0.56 absolute BLEU (or 1.5% relative). The rel-
ative increase in Prec-2 is higher than in Prec-1 sug-
gesting perhaps that some improvements are coming
from better word order.
5.5 Manual Error Analysis
We conduct two manual error analyses comparing
the baseline to our best system. First we compare
the baseline system to our best system applied only
to OOVs. Among all 656 OOV tokens (1.51%) in
our dev set we attempt to handle 417 tokens (0.96%)
(i.e., 63.57% of possible OOVs) which could pos-
sibly affect 320 sentences (21.39%); however, we
only see a change in 247 sentences (16.51%). We
took a 50-sentence sample from these 247 sentences
(our sample is 20%). We classified every occur-
rence of an OOV into not handled (the output has
the OOV word), mistranslated (including deleted),
or corrected (the output contains the correct transla-
tion); we focused on adequacy rather than fluency
in this analysis. Table 4 presents some examples
from the analysis set illustrating different behaviors.
Among the OOVs in the sample (total 68 instances),
22% are not handled. Among the handled cases, we
successfully translate 74% of the cases. Translation
errors are mostly due to spelling errors, lexical am-
biguity or proper names. There are no OOV dele-
tions. This analysis suggests that our results reflect
the correctness of the approach as opposed to ran-
dom BLEU bias due to sentence length, etc.
In the second manual error analysis, we compare
two systems to help us understand the effect of han-
dling low frequency (LF) words: (a) our best system
applied only to OOVs [OOV], and (b) our best sys-
tem applied to OOVs and LF words [OOV+LF]. For
LF words only (as compared to OOVs), we attempt
to handle 669 tokens (1.54%) which could possi-
bly affect 489 sentence (32.69%); however, we see
a change in only 268 sentences (17.91%) (as com-
pared to the OOV handling system). We took a 50-
sentence sample from these sentences in the dev set
where the output of the two systems is different (to-
tal 268 sentences; our sample is 19%). We classified
each LF word into mistranslated or correct, and we
annotated each case as dialectal, MSA, or tokeniza-
tion error. Among the LF words in the sample (total
64 instances), the [OOV+LF] system successfully
translated 55% of the cases while the [OOV] sys-
tem successfully translated 50% of the cases. Over-
all, 11% of all LF words in our sample are due to a
tokenization error, 34% are MSA, and 55% are di-
alectal. Among dialectal cases, the [OOV+LF] sys-
tem successfully translated 60% of the cases while
the [OOV] system successfully translated 42% of
the cases. Among MSA cases, the [OOV+LF] sys-
tem successfully translates 55% of the cases while
the [OOV] system successfully translate 64% of the
cases. The conclusion here is that (a) the majority of
LF cases handled are dialectal and (b) the approach
to handle them is helpful; however (c) the LF han-
dling approach may hurt MSA words overall. Ta-
ble 5 presents some examples from the analysis set
illustrating different behaviors.
6 Conclusion and Future Work
We presented a light-weight rule-based approach
to producing MSA paraphrases of dialectal Arabic
OOV words and low frequency words. The gener-
ated paraphrase lattices result in improved BLEU
scores on a blind test set by 0.56 absolute BLEU
(or 1.5% relative). In the future, we plan to extend
our system?s coverage of phenomena in the handled
dialects and on new dialects. We are interested in
using ADAM to extend the usability of existing mor-
phological disambiguation systems for MSA to the
18
Arabic y?ny ?n AlAzdHAmAt btstxdmwn1 Albn?klAt2?
Reference You mean for traffic jams you use1 the bicycles2?
Baseline I mean, about the traffic btstxdmwn1 Albn?klAt2?
OOV-Handle I mean, about the traffic use1 Albn?klAt2?
Arabic nHnA bntAml3 Anh fy h?A Almwqf tbdA msyrh? jdydh? slmyh? mTlwbh? lAlmnTqh? .
Reference We hope3 in this situation to start a new peace process that the region needs.
Baseline We bntAml3 that in this situation start a new march peaceful needed for the region.
OOV-Handle We hope3 that this situation will start a new march peaceful needed for the region.
Arabic dktwr Anwr mAjd ??qy4 ry?ys mrkz Al?rq AlAwsT lldrAsAt AlAstrAtyjyh? mn AlryAD ...
Reference Dr. Anwar Majid ?Ishqi4 President of the Middle East Center for Strategic Studies from Riyadh ...
Baseline Dr. anwar majed ??qy4 head of middle east center for strategic studies from riyadh ...
OOV-Handle Dr. anwar majed love4, president of the middle east center for strategic studies from riyadh ...
Table 4: Examples of different results of handling OOV words. Words of interest are bolded. Superscript indexes are
used to link the related words within each example. Words with index 1 and 3 are correctly translated; the word with
index 2 is not handled; and the word with index 4 is an incorrectly translated proper name.
Arabic ... wl?lk Ht?Aml m? Aljmy? ?ly hAlAsAs.
Reference ... and I shall therefore deal with everyone on this basis.
OOV ... and therefore dealt with everyone to think.
OOV+LF ... and therefore dealt with everyone on this basis.
Arabic ... t?ydwn nfs Alkrh? An lm ykn AswA ...
Reference ... repeat the same thing if not worse ...
OOV ... to re - the same if not worse ...
OOV+LF ... bring back the same if not worse ...
Table 5: Examples of different results of handling LF words. Words of interest are bolded. Both examples show a LF
word mistranslated in the first system and successfully translated in the second system. The first examples shows a
dialectal word while the second example shows an MSA word.
dialects, e.g., MADA. Furthermore, we want to au-
tomatically learn additional morphological system
rules and transfer rules from limited available data
(DA-MSA or DA-English) or at least use these re-
sources to learn weights for the manually created
rules.
Acknowledgments
This research was supported by the DARPA GALE
program, contract HR0011-06-C-0022. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this work are those of the authors and
do not necessarily reflect the view of DARPA. We
would like to thank Amit Abbi for help with the
MT baseline. We also would like to thank John
Makhoul, Richard Schwartz, Spyros Matsoukas, Ra-
bih Zbib and Mike Kayser for helpful discussions
and feedback and for providing us with the devtest
data.
References
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan.
2008. A Hybrid Approach for Converting Written
Egyptian Colloquial Dialect into Diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008. Cairo University.
Rania Al-Sabbagh and Roxana Girju. 2010. Mining the
Web for the Induction of a Dialectical Arabic Lexicon.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, LREC. Eu-
ropean Language Resources Association.
Imad A. Al-Sughaiyer and Ibrahim A. Al-Kharashi.
2004. Arabic morphological analysis techniques:
A comprehensive survey. Journal of the Ameri-
can Society for Information Science and Technology,
55(3):189?213.
Mohamed Altantawy, Nizar Habash, and Owen Ram-
bow. 2011. Fast Yet Rich Morphological Analysis.
In proceedings of the 9th International Workshop on
Finite-State Methods and Natural Language Process-
ing (FSMNLP 2011), Blois, France.
19
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Arbor,
Michigan.
Tim Buckwalter. 2004. Buckwalter Arabic Morpholog-
ical Analyzer Version 2.0. Linguistic Data Consor-
tium, University of Pennsylvania. LDC Catalog No.:
LDC2004L02, ISBN 1-58563-324-0.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 17?24.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
Dialects. In Proceedings of the European Chapter of
ACL (EACL).
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky, 2007.
Arabic Computational Morphology: Knowledge-
based and Empirical Methods, chapter Automated
Methods for Processing Arabic Text: From Tokeniza-
tion to Base Phrase Chunking. Springer.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Human Language Technology, pages
128?132, San Diego.
Bonnie J. Dorr, Pamela W. Jordan, and John W. Benoit.
1999. A Survey of Current Research in Machine
Translation. In M. Zelkowitz, editor, Advances in
Computers, Vol. 49, pages 1?68. Academic Press, Lon-
don.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facil-
itating translation using source language paraphrase
lattices. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP?10, pages 420?429, Cambridge, Mas-
sachusetts.
Kevin Duh and Katrin Kirchhoff. 2005. POS tagging of
dialectal Arabic: a minimally supervised approach. In
Proceedings of the ACL Workshop on Computational
Approaches to Semitic Languages, Semitic ?05, pages
55?62, Ann Arbor, Michigan.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, Columbus, Ohio.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A Morphological Analyzer and Generator for the Ara-
bic Dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 681?688, Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic Prepro-
cessing Schemes for Statistical Machine Translation.
In Proceedings of the Human Language Technology
Conference of the NAACL, Companion Volume: Short
Papers, pages 49?52, New York City, USA.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash. 2006. On Arabic and its Dialects. Multi-
lingual Magazine, 17(81).
Nizar Habash. 2007. Arabic Morphological Representa-
tions for Machine Translation. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash. 2008. Four Techniques for Online Han-
dling of Out-of-Vocabulary Words in Arabic-English
Statistical Machine Translation. In Proceedings of
ACL-08: HLT, Short Papers, pages 57?60, Columbus,
Ohio.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?, Jan Hric, and Vladislav Kubon. 2000. Ma-
chine Translation of Very Close Languages. In Pro-
ceedings of the 6th Applied Natural Language Pro-
cessing Conference (ANLP?2000), pages 7?12, Seat-
tle.
Ahmed El Kholy and Nizar Habash. 2010. Tech-
niques for Arabic Morphological Detokenization and
Orthographic Denormalization. In Workshop on Lan-
guage Resources and Human Language Technology
for Semitic Languages in the Language Resources and
Evaluation Conference (LREC), Valletta, Malta.
H. Kilany, H. Gadalla, H. Arram, A. Yacoub, A. El-
Habashi, and C. McLemore. 2002. Egyptian
Colloquial Arabic Lexicon. LDC catalog number
LDC99L22.
Katrin Kirchhoff, Jeff Bilmes, Sourin Das, Nicolae Duta,
Melissa Egan, Gang Ji, Feng He, John Henderson,
Daben Liu, Mohamed Noamany, Pat Schone, Richard
Schwartz, and Dimitra Vergyri. 2003. Novel Ap-
proaches to Arabic Speech Recognition: Report from
the 2002 Johns Hopkins Summer Workshop. In IEEE
International Conference on Acoustics, Speech, and
Signal Processing, Hong Kong, China.
20
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of the
Empirical Methods in Natural Language Processing
Conference (EMNLP?04), Barcelona, Spain.
Shankar Kumar, Franz J. Och, and Wolfgang Macherey.
2007. Improving word alignment with bridge lan-
guages. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 42?50, Prague, Czech Re-
public.
Young-Suk Lee. 2004. Morphological Analysis for Sta-
tistical Machine Translation. In Proceedings of the 5th
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics/Human Language
Technologies Conference (HLT-NAACL04), pages 57?
60, Boston, MA.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Preslav Nakov and Hwee Tou Ng. 2011. Translat-
ing from Morphologically Complex Languages: A
Paraphrase-Based Approach. In Proceedings of the
Meeting of the Association for Computational Linguis-
tics (ACL?2011), Portland, Oregon, USA.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of the 41st Annual Conference of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally Su-
pervised Morphological Segmentation with Applica-
tions to Machine Translation. In Proceedings of the
7th Conference of the Association for Machine Trans-
lation in the Americas (AMTA06), pages 185?192,
Cambridge,MA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological Tag-
ging, Diacritization, and Lemmatization Using Lex-
eme Models and Feature Ranking. In Proceedings of
ACL-08: HLT, Short Papers, pages 117?120, Colum-
bus, Ohio.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in the
Americas (AMTA), Denver, Colorado.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with sta-
tistical phrase-based post-editing. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 203?206, Prague, Czech Republic.
Masao Utiyama and Hitoshi Isahara. 2007. A compar-
ison of pivot methods for phrase-based statistical ma-
chine translation. In HLT-NAACL, pages 484?491.
Xiaoheng Zhang. 1998. Dialect MT: a case study be-
tween Cantonese and Mandarin. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, ACL ?98, pages 1460?
1464, Montreal, Canada.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Bridging the Inflection Morphology Gap
for Arabic Statistical Machine Translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, Companion Volume: Short Papers,
pages 201?204, New York City, USA.
21
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 160?164,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The Columbia System in the QALB-2014 Shared Task
on Arabic Error Correction
Alla Rozovskaya Nizar Habash
?
Ramy Eskander Noura Farra Wael Salloum
Center for Computational Learning Systems, Columbia University
?
New York University Abu Dhabi
{alla,ramy,noura,wael}@ccls.columbia.edu
?
nizar.habash@nyu.edu
Abstract
The QALB-2014 shared task focuses on
correcting errors in texts written in Mod-
ern Standard Arabic. In this paper, we
describe the Columbia University entry in
the shared task. Our system consists of
several components that rely on machine-
learning techniques and linguistic knowl-
edge. We submitted three versions of the
system: these share several core elements
but each version also includes additional
components. We describe our underlying
approach and the special aspects of the dif-
ferent versions of our submission. Our
system ranked first out of nine participat-
ing teams.
1 Introduction
The topic of text correction has seen a lot of in-
terest in the past several years, with a focus on
correcting grammatical errors made by learners of
English as a Second Language (ESL). The two
most recent CoNLL shared tasks were devoted to
grammatical error correction for non-native writ-
ers (Ng et al., 2013; Ng et al., 2014).
The QALB-2014 shared task (Mohit et al.,
2014) is the first competition that addresses the
problem of text correction in Modern Standard
Arabic (MSA) texts. The competition makes
use of the recently developed QALB corpus (Za-
ghouani et al., 2014). The shared task covers all
types of mistakes that occur in the data.
Our system consists of statistical models, lin-
guistic resources, and rule-based modules that ad-
dress different types of errors.
We briefly discuss the task in Section 2. Sec-
tion 3 gives an overview of the Columbia system
and describes the system components. In Sec-
tion 4, we evaluate the complete system on the de-
velopment data and show the results obtained on
test. Section 5 concludes.
2 Task Description
The QALB-2014 shared task addresses the prob-
lem of correcting errors in texts written in Modern
Standard Arabic (MSA). The task organizers re-
leased training, development, and test data. All
of the data comes from online commentaries writ-
ten to Aljazeera articles.
1
The training data con-
tains 1.2 million words; the development and the
test data contain about 50,000 words each. The
data was annotated and corrected by native Arabic
speakers. For more detail on the QALB corpus, we
refer the reader to Zaghouani et al. (2014). The re-
sults in the subsequent sections are reported on the
development set.
It should be noted that in the annotation process,
the annotators did not assign error categories but
only specified an appropriate correction. In spite
of this, it is possible, to isolate certain error types
automatically, by using the corrections in coordi-
nation with the input words. The first type con-
cerns punctuation errors. Errors involving punc-
tuation account for about 39% of all errors in the
data. In addition to punctuation mistakes, another
very common source of errors refers to subopti-
mal spelling for two groups of letters ? Alif (and
its Hamzated versions) and Ya (and its undotted or
Alif Maqsura versions). For more detail on this
and other Arabic phenomena, we refer the reader
to Habash (2010; Buckwalter (2007; El Kholy and
Habash (2012). Mistakes associated with Alif and
1
http://www.aljazeera.net/
160
Component System
CLMB-1 CLMB-2 CLMB-3
MADAMIRA
MLE
Na??ve Bayes
GSEC
MLE-unigram
Punctuation
Dialectal
Patterns
Table 1: The three versions of the Columbia sys-
tem and their components.
Ya spelling constitute almost 30% of all errors.
3 System Overview
The Columbia University system consists of sev-
eral components designed to address different
types of errors. We submitted three versions of the
system. We refer to these as CLMB-1, CLMB-2,
and CLMB-3. Table 1 lists all of the components
and indicates which components are included in
each version. The components are applied in the
order shown in the table. Below we describe each
component in more detail.
3.1 MADAMIRA Corrector
MADAMIRA (Pasha et al., 2014) is a tool
designed for morphological analysis and dis-
ambiguation of Modern Standard Arabic.
MADAMIRA performs morphological analysis
in context. This is a knowledge-rich resource
that requires a morphological analyzer and a
large corpus where every word is marked with
its morphological features. The task organizers
provided the shared task data pre-processed
with MADAMIRA, including all of the features
generated by the tool for every word. In addition
to the morphological analysis and contextual
morphological disambiguation, MADAMIRA
also performs Alif and Ya spelling correction
for the phenomena associated with these letters
discussed in Section 2. The corrected form was
included among the features and can be used
for correcting the input. We use the corrections
proposed by MADAMIRA and apply them to the
data. As we show in Section 4, while the form
proposed by MADAMIRA may not necessarily
be correct, MADAMIRA performs at a very high
precision. MADAMIRA corrector is used in the
CLMB-1 and CLMB-2 systems.
3.2 Maximum Likelihood Model
The Maximum Likelihood Estimator (MLE) is a
supervised component that is trained on the train-
ing data of the shared task. Given the annotated
training data, a map is defined that specifies for ev-
ery word n-gram in the source text the most likely
n-gram corresponding to it in the target text. The
MLE model considers source n-grams of lengths
between 1 to 3; the MLE-unigram model that is
part of the CLMB-3 version only considers n-
grams of length 1.
The MLE approach performs well on errors that
have been observed in the training data and can
be unambiguously corrected without using the sur-
rounding context, i.e. do not have many alternative
corrections. Consequently, MLE fails on words
that have many possible corrections, as well as
words not seen in training.
3.3 Na??ve Bayes for Unseen Words
The Na??ve Bayes component addresses errors for
words that were not seen in training. The system
uses the approach proposed in Rozovskaya and
Roth (2011) that proved to be successful for cor-
recting errors made by English as a Second Lan-
guage learners. The model operates at the word
level and targets word replacement errors that in-
volve single tokens. Candidate corrections are
generated using a character confusion table that is
based on the training data. The model is a Na??ve
Bayes classifier trained on the Arabic Gigaword
corpus (Parker et al., 2011) with word n-gram fea-
tures in the 4-word window around the word to be
corrected. The Na??ve Bayes component is used in
the CLMB-1 system.
3.4 The GSEC Model
The CLMB-3 system implements a Generalized
Character-Level Error Correction model (GSEC)
proposed in Farra et al. (2014). GSEC is a super-
vised model that operates at the character level.
Because of this, the source and the target side of
the training data need to be aligned at the charac-
ter level. We use the alignment tool Sclite (Fiscus,
1998). The alignment maps each source charac-
ter to itself, a different character, a pair of char-
acters, or an empty string. For the shared task,
punctuation corrections are ignored since punctu-
ation errors are handled by the punctuation correc-
tor described in the following section. It should
161
also be noted that the model was not trained to
insert missing characters. The model is a multi-
class SVM classifier (Kudo, 2005) that makes use
of character-level features using a window of four
characters that may occur within the word bound-
aries as well as in the surrounding context. Due
to a long training time, GSEC was trained on a
quarter of the training data. The system is post-
processed with a unigram word-level maximum-
likelihood model described in Section 3.2. For
more detail on the GSEC approach, we refer the
reader to Farra et al. (2014).
3.5 Punctuation Corrector
The shared task data contains a large number of
punctuation mistakes. Punctuation errors, such as
missing periods and commas, account for about
30% of all errors in the data. Most of these errors
involve incorrectly omitting a punctuation symbol.
Our punctuation corrector is a statistical model
that inserts periods and commas. The system is
a decision tree model trained on the shared task
training data using WEKA (Hall et al., 2009). For
punctuation insertion, every space that is not fol-
lowed or preceded by a punctuation mark is con-
sidered.
To generate features, we use a window of size
three around the target space. The features are de-
fined as follows:
? The part-of-speech of the previous word
? The existence of a conjunctive or connective
proclitic in the following word; that is a ?w?
or ?f? proclitic that is either a conjunction, a
sub-conjunction or a connective particle
The part-of-speech and proclitic information is
obtained by running MADAMIRA on the text.
We also ran experiments where the model is
trained with a complete list of features produced
by MADAMIRA; that is part-of-speech, gender,
number, person, aspect, voice, case, mood, state,
proclitics and enclitics. This was done for two pre-
ceding words and two following words. However,
this model did not perform as well as the one de-
scribed above, which we used in the final system.
Note that the punctuation model predicts pres-
ence or absence of a punctuation mark in a spe-
cific location and is applied to the source data
from which all punctuation marks have been re-
moved. However, when we apply our punctuation
model in the correction pipeline, we find that it
is always better to keep the already existing peri-
ods and commas in the input text instead of over-
writing them with the model prediction. In other
words, we only attempt to add missing punctua-
tion.
3.6 Dialectal Usage Corrector
Even though the shared task data is written in
MSA, MSA is not a native language for Arabic
speakers. Typically, an Arabic speaker has a native
proficiency in one of the many Arabic dialects and
learns to write and read MSA in a formal setting.
For this reason, even in MSA texts produced by
native Arabic speakers, one typically finds words
and linguistic features specific to the writer?s na-
tive dialect that are not found in the standard lan-
guage.
To address such errors, we use Elissa (Salloum
and Habash, 2012), which is Dialectal to Standard
Arabic Machine Translation System. Elissa uses
a rule-based approach that relies on the existence
of a dialectal morphological analyzer (Salloum
and Habash, 2011), a list of hand-written trans-
fer rules, and dialectal-to-standard Arabic lexi-
cons. Elissa uses different dialect identification
techniques to select dialectal words and phrases
(dialectal multi-word expressions) that need to be
handled. Then equivalent MSA paraphrases of the
selected words/phrases are generated and an MSA
lattice for each input sentence is constructed. The
paraphrases within the lattice are then ranked us-
ing language models and the n-best sentences are
extracted from lattice. We use 5-gram language
models trained using SRILM (Stolcke, 2002) on
about 200 million untokenized, Alif /Ya normal-
ized words extracted from Arabic GigaWord. This
component is employed in the CLMB-2 system.
3.7 Pattern-Based Corrector
We created a set of rules that account for very
common phenomena involving incorrectly split or
merged tokens. The MADAMIRA corrector de-
scribed above does not handle splits and merges;
however, some of the cases are handled in the
MLE method. Note that the MLE method is re-
strictive since it does not correct words not seen
in training, while the pattern-based corrector is
more general. The rules were created through
analysis of samples of the QALB Shared Task
162
training data. Some of the rules use regular ex-
pressions, while others make use of the rule-
based Standard Arabic Morphological Analyzer
(SAMA) (Maamouri et al., 2010), the same out-
of-context analyzer used inside of MADAMIRA.
Rules for splitting words
? All digits are separated from words.
? A space is added after all word medial Ta-
Marbuta characters.
? A space is added after the very common
?ElY? ?at/about/on? preposition if it is at-
tached to the following word.
? If a word has a morphological analysis that
includes ?lmA? (as negation particle, relative
pronoun or pseudo verb), ?hA? (a demonstra-
tive pronoun), or ?Ebd? and ?>bw? in proper
nouns, a space is inserted after those parts of
the analysis.
? If a word has no morphological analysis, but
starts with a set of commonly mis-attached
words, and the rest of the word has an anal-
ysis, the word is split after the mis-attached
word sequence.
Rules for merging words
? All lone occurrences of the conjunction w
?and? are attached to the following word.
? All sequences of the punctuation marks (., ?,
!) that occur between two and six times are
merged: e.g ! ! ! ? !!!.
4 Experimental Results
In Section 3, we described the individual sys-
tem components that address different types of
errors. In this section, we show how the sys-
tem improves when each component is added into
the system. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012), the official
scorer of the shared task.
Table 2 reports performance results of each ver-
sion of the Columbia system on the development
data. Table 3 shows the performance results for the
best-performing system, CLMB-1, as each system
component is added.
System P R F1
CLMB-1 72.22 62.79 67.18
CLMB-2 69.49 61.72 65.38
CLMB-3 69.71 59.42 64.15
Table 2: Performance of the Columbia systems
on the development data.
System P R F1
MADAMIRA 83.33 32.94 47.21
+ MLE 86.52 42.52 57.02
+ NB 85.80 43.27 57.53
+ Punc. 73.66 59.51 65.83
+ Patterns 72.22 62.79 67.18
Table 3: Performance of the CLMB-1 system on
the development data and the contribution of
its components.
System P R F1
CLMB-1 73.34 63.23 67.91
CLMB-2 70.86 62.21 66.25
CLMB-3 71.45 60.00 65.22
Table 4: Performance of the Columbia systems
on the test data.
Finally, Table 4 reports results obtained on the
test data. These results are comparable to the per-
formance observed on the development data. In
particular, CLMB-1 achieves the highest score.
5 Conclusion
We have described the Columbia University sys-
tem that participated in the first shared task
on grammatical error correction for Arabic and
ranked first out of nine participating teams. We
have presented three versions of the system; all of
these incorporate several components that target
different types of mistakes, which we presented
and evaluated in this paper.
Acknowledgments
This material is based on research funded by grant
NPRP-4-1058-1-168 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
The statements made herein are solely the respon-
sibility of the authors. Nizar Habash performed
most of his contribution to this paper while he was
at the Center for Computational Learning Systems
at Columbia University.
163
References
T. Buckwalter. 2007. Issues in Arabic Morphological
Analysis. In A. van den Bosch and A. Soudi, editors,
Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
D. Dahlmeier and H. T. Ng. 2012. Better evaluation
for grammatical error correction. In Proceedings of
NAACL.
A. El Kholy and N. Habash. 2012. Orthographic and
morphological processing for English?Arabic sta-
tistical machine translation. Machine Translation,
26(1-2).
N. Farra, N. Tomeh, A. Rozovskaya, and N. Habash.
2014. Generalized character-level spelling error cor-
rection. In Proceedings of ACL.
J. Fiscus. 1998. Sclite scoring package ver-
sion 1.5. US National Institute of Standard
Technology (NIST), URL http://www. itl. nist.
gov/iaui/894.01/tools.
N. Y. Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies 3.1.
M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA data
mining software: an update. SIGKDD Explorations,
11(1):10?18.
T. Kudo. 2005. YamCha: Yet another multipurpose
chunk annotator. http://chasen.org/ taku/software/.
M. Maamouri, D. Graff, B. Bouziri, S. Krouna, A. Bies,
and S. Kulick. 2010. LDC Standard Arabic Mor-
phological Analyzer (SAMA) Version 3.1. Linguistic
Data Consortium.
B. Mohit, A. Rozovskaya, N. Habash, W. Zaghouani,
and O. Obeid. 2014. The first QALB shared task on
automatic text correction for Arabic. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proceedings of
CoNLL: Shared Task.
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of CoNLL: Shared Task.
R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda.
2011. Arabic Gigaword Fifth Edition. Linguistic
Data Consortium.
A. Pasha, M. Al-Badrashiny, A. E. Kholy, R. Eskan-
der, M. Diab, N. Habash, M. Pooleery, O. Rambow,
and R. Roth. 2014. MADAMIRA: A fast, compre-
hensive tool for morphological analysis and disam-
biguation of arabic. In Proceedings of LREC.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks.
In Proceedings of ACL.
W. Salloum and N. Habash. 2011. Dialectal to stan-
dard arabic paraphrasing to improve arabic-english
statistical machine translation. In Proceedings of
the First Workshop on Algorithms and Resources for
Modelling of Dialects and Language Varieties.
W. Salloum and N. Habash. 2012. Elissa: A dialectal
to standard arabic machine translation system. In
Proceedings of COLING (Demos).
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Proceedings of International Con-
ference on Spoken Language Processing.
W. Zaghouani, B. Mohit, N. Habash, O. Obeid,
N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani,
and K. Oflazer. 2014. Large scale arabic error anno-
tation: Guidelines and framework. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC?14).
164
