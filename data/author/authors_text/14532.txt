Building and using comparable corpora for domain-specific bilingual lexicon extraction 
Darja Fi?er Nikola Ljube?i? University of Ljubljana, Faculty of Arts, Deparment of Translation University of Zagreb, Faculty of Humanities and Social Sciences A?ker?eva 2 Ivana Lu?i?a 3 Ljubljana, Slovenia Zagreb, Croatia darja.fiser@ff.uni-lj.si nikola.ljubesic@ffzg.hr  ?pela Vintar Senja Pollak University of Ljubljana, Faculty of Arts, Deparment of Translation University of Ljubljana, Faculty of Arts, Deparment of Translation A?ker?eva 2 A?ker?eva 2 Ljubljana, Slovenia Ljubljana, Slovenia spela.vintar@ff.uni-lj.si senja.pollak@ff.uni-lj.si  Abstract This paper presents a series of experiments aimed at inducing and evaluating domain-specific bilingual lexica from comparable corpora. First, a small English-Slovene comparable corpus from health magazines was manually constructed and then used to compile a large comparable corpus on health-related topics from web corpora. Next, a bilingual lexicon for the domain was extracted from the corpus by comparing context vectors in the two languages. Evaluation of the results shows that a 2-way translation of context vectors significantly improves precision of the extracted translation equivalents. We also show that it is sufficient to increase the corpus for one language in order to obtain a higher recall, and that the increase of the number of new words is linear in the size of the corpus. Finally, we demonstrate that by lowering the frequency threshold for context vectors, the drop in precision is much slower than the increase of recall. 1 Introduction Research into using comparable corpora in NLP has gained momentum in the past decade largely due to limited availability of parallel data for many 
language pairs and domains. As an alternative to already established parallel approaches (e.g. Och 2000, Tiedemann 2005) the comparable corpus-based approach relies on texts in two or more languages which are not parallel but nevertheless share several parameters, such as topic, time of publication and communicative goal (Fung 1998, Rapp 1999). The main advantage of this approach is the simpler, faster and more time efficient compilation of comparable corpora, especially from the rich web data (Xiao & McEnery 2006). In this paper we describe the compilation process of a large comparable corpus of texts on health-related topics for Slovene and English that were published on the web. Then we report on a set of experiments we conducted in order to automatically extract translation equivalents for terms from the health domain. The parameters we tested and analysed are: 1- and 2-way translations of context vectors with a seed lexicon, the size of the corpus used for bilingual lexicon extraction, and the word frequency threshold for vector construction. The main contribution of this paper is a much-desired language- and domain-independent approach to bootstrapping bilingual lexica with minimal manual intervention as well as minimal reliance on the existing linguistic resources. The paper is structured as follows: in the next section we give an overview of previous work relevant for our research. In Section 3 we present the construction of the corpus. Section 4 describes 
19
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 19?26,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
the experiments for bilingual lexicon extraction the results of which are reported, evaluated and discussed in Section 5. We conclude the paper with final remarks and ideas for future work. 2 Related work Bilingual lexica are the key component of all cross-lingual NLP applications and their compilation remains a major bottleneck in computational linguistics. In this paper we follow the line of research that was inspired by Fung (1998) and Rapp (1999) who showed that texts do not need to be parallel in order to extract translation equivalents from them. Instead, their main assumption is that the term and its translation appear in similar contexts anyhow. The task of finding the appropriate translation equivalent of a term is therefore reduced to finding the word in the target language whose context vector is most similar to the source term?s context vector based on their occurrence in a comparable corpus. This is basically a three-step procedure:  (1) Building context vectors. When representing a word?s context, some approaches look at a simple co-occurrence window of a certain size while others include some syntactic information as well. For example, Otero (2007) proposes binary dependences previously extracted from a parallel corpus, while Yu and Tsujii (2009) use dependency parsers and Marsi and Krahmer (2010) use syntactic trees. Instead of context windows, Shao and Ng (2004) use language models. Next, words in co-occurrence vectors can be represented as binary features, by term frequency or weighted by different association measures, such as TF-IDF (Fung, 1998), PMI (Shezaf and Rappoport, 2010) or, one of the most popular, the log likelihood score. Approaches also exist that weigh co-occurrence terms differently if they appear closer to or further from the nucleus word in the context (e.g. Saralegi et al, 2008). (2) Translating context vectors. Finding the most similar context vectors in the source and target language is not straightforward because a direct comparison of vectors in two different languages is not possible. This is why most researchers first translate features of source context vectors with machine-readable dictionaries and compute similarity measures on those. Koehn and Knight 
(2002) construct the seed dictionary automatically based on identical spelled words in the two languages. Similarly, cognate detection is used by Saralegi et al (2008) by computing the longest common subsequence ratio. D?jean et al (2005), on the other hand, use a bilingual thesaurus instead of a bilingual dictionary. (3) Selecting translation candidates. After source context vectors have been translated, they are ready to be compared to the target context vectors. A number of different vector similarity measures have been investigated. Rapp (1999) applies city-block metric, while Fung (1998) works with cosine similarity. Recent work often uses Jaccard index or Dice coefficient (Saralegi et al, 2008). In addition, some approaches include a subsequent re-ranking of translation candidates based on cognates detection (e.g. Shao and Ng, 2004). 3 Corpus construction A common scenario in the NLP community is a project on a specific language pair in a new domain for which no ready-made resources are available. This is why we propose an approach that takes advantage of the existing general resources, which are then fine-tuned and enriched to be better suited for the task at hand. In this section we describe the construction of a domain-specific corpus that we use for extraction of translation equivalents in the second part of the paper. 3.1 Initial corpus We start with a small part of the Slovene PoS tagged and lemmatized reference corpus FidaPLUS (Arhar et al, 2007) that contains collections of articles from the monthly health and lifestyle magazine called Zdravje1 , which were published between 2003 and 2005 and contain 1 million words. We collected the same amount of text from the most recent issues of the Health Magazine, which is a similar magazine for the English-speaking readers. We PoS-tagged and lemmatized the English part of the corpus with the TreeTagger (Schmid, 1994). 
                                                1 http://www.zdravje.si/category/revija-zdravje [1.4.2010] 
20
3.2 Corpus extension We then extended the initial corpus automatically from the 2 billion-word ukWaC (Ferraresi et al, 2008) and the 380 million-word slWaC (Ljube?i? and Erjavec, 2011), very large corpora that were constructed from the web by crawling the .uk and .si domain respectively. We took into account all the documents from these two corpora that best fit the initial corpora by computing a similarity measure between models of each document and the initial corpus in the corresponding language. The models were built with content words lemmas as their parameters and TF-IDF values as the corresponding parameter values. The inverse document frequency was computed for every language on a newspaper domain of 20 million words. The similarity measure used for calculating the similarity between a document model and a corpus model was cosine with a similarity threshold of 0.2. This way, we were able to extend the Slovene part of the corpus from 1 to 6 million words and the English part to as much as 50 million words. We are aware of more complex methods for building comparable corpora, such as (Li and Gaussier, 2010), but the focus of this paper is on using comparable corpora collected from the web on the bilingual lexicon extraction task, and not the corpus extension method itself. Bilingual lexicon extraction from the extended corpus is described in the following section. 4 Bilingual lexicon extraction In this section we describe the experiments we conducted in order to extract translation equivalents of key terms in the health domain. We ran a series of experiments in which we adjusted the following parameters:  (1) 1- and 2-way translation of context vectors with a seed dictionary; (2) corpus size of the texts between the languages; (3) the word frequency threshold for vector construction.  Although several parameters change in each run of the experiment, the basic algorithm for finding translation equivalents in comparable corpora is always the same: 
 (1) build context vectors for all unknown words in the source language that satisfy the minimum frequency criterion and translate the vectors with a seed dictionary; (2) build context vectors for all candidate translations satisfying the frequency criterion in the target language; (3) compute the similarity of all translated source vectors  with the target vectors and rank translation candidates according to this score.  Previous research (Ljube?i? et al, 2011) has shown that best results are achieved by using content words as features in context vectors and a context window of 7 with encoded position. The highest-scoring combination of vector association and similarity measures turned out to be Log Likelihood (Dunning, 1993) and Jensen-Shannon divergence (Lin, 1991), so we are using those throughout the experiments presented in this paper. 4.1 Translation of context vectors In order to be able to compare two vectors in different languages, a seed dictionary to translate features in context vectors of source words is needed. We tested our approach with a 1-way translation of context features of English vectors into Slovene and a 2-way translation of the vectors from English into Slovene and vice versa where we then take the harmonic mean of the context similarity in both directions for every word pair. A similar 2-way approach is described in (Chiao et al 2004) with the difference that they average on rank values, not on similarity measures. An empirical comparison with their method is given in the automatic evaluation section. A traditional general large-sized English-Slovene dictionary was used for the 1-way translation, which was then complemented with another general large-sized Slovene-English dictionary by the same author in the 2-way translation setting. Our technique relies on the assumption that additional linguistic knowledge is encoded in the independent dictionary in the opposite direction and was indirectly inspired by a common approach to filter out the noise in bilingual lexicon extraction from parallel corpora with source-to-target and target-to-source word-alignment. 
21
Only content-word dictionary entries were taken into account. No multi-word entries were considered either. And, since we do not yet deal with polysemy at this stage of our research, we only extracted the first sense for each dictionary entry. The seed dictionaries we obtained in this way contained 41.405 entries (Eng-Slo) and 30.955 entries (Slo-Eng). 4.2 Corpus size Next, we tested the impact of the extended corpus on the quality and quantity of the extracted translation equivalents by gradually increasing the size of the corpus from 1 to 6 million words. Not only did we increase corpus size for each language equally, we also tested a much more realistic setting in which the amount of data available for one language is much higher than for the other, in our case English for which we were able to compile a 50 million word corpus, which is more than eight times more than for Slovene. 4.3 Word frequency threshold Finally, we tested the precision and recall of the extracted lexica based on the minimum frequency of the words in the corpus from as high as 150 and down to 25 occurrences. This is an important parameter that shows the proportion of the corpus lexical inventory our method can capture and with which quality. 5 Evaluation of the results At this stage of our research we have limited the experiments to nouns. This speeds up and simplifies our task but we believe it still gives an adequate insight into the usefulness of the approach for a particular domain since nouns carry the highest domain-specific terminological load. 5.1 Automatic evaluation Automatic evaluation of the results was performed against a gold standard lexicon of health-related terms that was obtained from the top-ranking nouns in the English health domain model of the initial corpus and that at the same time appeared in the comprehensive dictionary of medical terms mediLexicon2 and were missing from the general bilingual seed dictionary. The gold standard                                                 2 http://www.medilexicon.com [1.4.2010] 
contains 360 English single-word terms with their translations into Slovene. If more than one translation variant is possible for a single English term, all variants appear in the gold standard and any of these translations suggested by the algorithm is considered as correct. Below we present the results of three experiments that best demonstrate the performance and impact of the key parameters for bilingual lexicon extraction from comparable corpora that we were testing in this research. The evaluation measure for precision used throughout this research is mean reciprocal rank (Vorhees, 2001) on first ten translation candidates. Recall is calculated as the percentage of goldstandard entries we were able to calculate translation candidates for. Additionally, a global recall impact of our methods is shown as the overall number of entries for which we were able to calculate translation candidates. Unless stated otherwise, the frequency threshold for the generation of context vectors in the experiments was set to 50. We begin with the results of 1- and 2-way context vector translations that we tested on the initial 1-million-word corpus we constructed from health magazines as well as on a corpus of the same size we extracted from the web. We compared the results of our method with that proposed in (Chiao et al 2004) strengthening our claim that it is the additional information in the reverse dictionary that makes the significant impact, not the reversing itself. As Table 1 shows, using two general dictionaries (2-way two dict) significantly improves the results as a new dictionary brings additional information. That it is the dictionary improving the results is proven by using just one, inverted dictionary in the 2-way manner, which produced worse results than the 1-way approach (2-way inverse dict). The approach of Chiao et al(2004) is also based on new dictionary knowledge since using only one inverted dictionary with their 2-way method yielded results that were almost identical to the 1-way computation. Using rank, not similarity score in averaging results proved to be a good approach (2-way Chiao two dict), but not as efficient as our approach which uses similarity scores (2-way two dict). Our approach yields higher precision and is also easier to compute. Namely, for every candidate pair only the reverse similarity score has 
22
to be computed, and not all similarity scores for every inverse pair to obtain a rank value. Therefore, only the 2-way translation setting averaging on similarity scores is used in the rest of the experiments. It is interesting that the results on the web corpus have a higher precision but a lower recall (0.355 on the initial corpus and 0.198 on the web corpus). Higher precision can be explained with the domain modelling technique that was used to extract web data, which may have contributed to a terminologically more homogenous collection of documents in the health domain. On the other hand, the lower recall can be explained with the extracted web documents being less terminologically loaded than the initial corpus.  
Corpus 1-way 2-way inverse dict 
2-way Chiao two dict 
2-way two dict 
1 M initial 0.591 0.566 0.628 0.641 1 M web 0.626 0.610 0.705 0.710  Table 1: Precision regarding the corpus source and the translation method  The second parameter we tested in our experiments was the impact of corpus size on the quality and amount of the extracted translation equivalents. For the first 6 million words the Slovene and English parts of the corpus were enlarged in equal proportions and after that only the English part of the corpus was increased up to 18 million words.  Corpus size P R No. of translated words Not already in dict 1 0.718 0.198 1246 244 6 0.668 0.565 4535 1546 18 0.691 0.716 9122 4184  Table 2: Precision, recall, number of translated words and number of new words (not found in the dictionary) obtained with different corpus sizes  
 Figure 1: Precision and recall as a function of corpus size  
 Figure 2: The number of new words (not found in the seed dictionary) as a function of corpus size  Figure 1 shows that precision with regard to the gold standard is more or less constant with an average of 0.68 if we disregard the first two measurements that are probably bad estimates since the intersection with the gold standard is small (as shown in Table 1) and evens out as the size of the corpus increases. When analyzing recall against the gold standard we see the typical logarithmic recall behavior when depicted as a function of corpus size. On the other hand, when we consider the number of new translation equivalents (i.e. the number of source words that do not appear in the seed dictionary), the function behaves almost linearly (see Figure 2). This can be explained with the fact that in the dictionary the most frequent words are best represented. Because of that we can observe a steady increase in the number of words not present in the seed lexicon that pass the frequency threshold with the increasing corpus size. Finally, we study the impact of the word frequency threshold for context vector generation on the quality and amount of the extracted translation equivalents on the six million corpora in both languages.   
23
Frequency P No. of translated words 
F1 
25 0.561 7203 0.719 50 0.668 4535 0.648 75 0.711 3435 0.571 100 0.752 2803 0.513 125 0.785 2374 0.464 150 0.815 2062 0.424  Table 3: Precision, number of new words and F1 obtained with different frequency thresholds  As can be seen in Table 3, by lowering the frequency criterion, the F1 measure increases showing greater gain in recall than loss in precision. For calculating recall, the number of new words passing the frequency criterion is normalized with the assumed number of obtainable lexicon entries set to 7.203 (the number of new words obtained with the lowest frequency criterion). This is a valuable insight since the threshold can be set according to different project scenarios. If, for example, lexicographers can be used in order to check the translation candidates and choose the best ones among them, the threshold may well be left low and they will still be able to identify the correct translation very quickly. If, on the other hand, the results will be used directly by another application, the threshold will be raised in order to reduce the amount of noise introduced by the lexicon for the following processing stages. 5.2 Manual evaluation For a more qualitative inspection of the results we performed manual evaluation on a random sample of 100 translation equivalents that are not in the general seed dictionary or present in our gold standard. We were interested in finding out to what extent these translation equivalents belong to the health domain and if their quality is comparable to the results of the automatic evaluation. Manual evaluation was performed on translation equivalents extracted from the comparable corpus containing 18 million English words and 6 million Slovene words, where the frequency threshold was set to 50. 51% of the manually evaluated words belonged to the health domain, 23% were part of general vocabulary, 10% were proper names and 
the rest were acronyms and errors arising from PoS-tagging and lemmatization in the ukWaC corpus. Overall, in 45% the first translation equivalent was correct and additional 11% contained the correct translation among the ten best-ranked candidates. For 44 % of the extracted translation equivalents no appropriate translation was suggested. Among the evaluated health-domain terms, 61% were translated correctly with the first candidate and for the additional 20% the correct translation appeared among the first 10 candidates. Of the 19% health-domain terms with no appropriate translation suggestion, 4 terms, that is 21% of the wrongly translated terms, were translated as direct hypernyms and could loosely be considered as correct (e.g. the English term bacillus was translated as mikroorganizem into Slovene, which means microorganism). Even most other translation candidates were semantically closely related, in fact, there was only one case in the manually inspected sample that provided completely wrong translations. Manual evaluation shows that the quality of translations for out-of-goldstandard terms is consistent with the results of automatic evaluation. A closer look revealed that we were able to obtain translation equivalents not only for the general vocabulary but especially terms relevant for the health domain, and furthermore, that their quality is also considerably higher than for the general vocabulary which is not of our primary interest in this research. The results could be further improved by filtering out the noise obtained from errors in PoS-tagging and lemmatization and, more importantly, by identifying proper names. Multi-word expressions should also be tackled as they present problems, especially in cases of 1:many mappings, such as the English single-word term immunodeficiency that is translated with a multi-word expression in Slovene (imunska pomanjkljivost). 6 Conclusions In this paper we described the compilation process of a domain-specific comparable corpus from already existing general resources. The corpus compiled from general web corpora was used in a set of experiments to extract translation equivalents 
24
for the domain vocabulary by comparing contexts in which terms appear in the two languages. The results show that a 2-way translation of context vectors consistently improves the quality of the extracted translation equivalents by using additional information given from the reverse dictionary. Next, increasing the size of only one part of the comparable corpus brings a slight increase in precision but a very substantial increase in recall.  If we are able to translate less than 20% of the gold standard with a 1 million word corpus, the recall is exceeds 70% when we extend the English part of the corpus to 15 million words. Moreover, the increase of the number of new words we obtain in this way keeps being linear for even large corpus sizes. We can also expect the amount of available text to keep rising in the future. This is a valuable finding because a scenario in which much more data is available for one of the two languages in question is a very common one.  Finally, we have established that the word frequency threshold for building context vectors can be lowered in order to obtain more translation equivalents without a big sacrifice in their quality. For example, a 10% drop in precision yields almost twice as many translation equivalents. Manual evaluation has shown that the quality of health-related terms that were at the center of our research is considerably higher than the rest of the vocabulary but has also revealed some noise in POS-tagging and lemmatization of the ukWaC corpus that consequently lowers the results of our method and should be dealt with in the future.  A straightforward extension of this research is to tackle other parts of speech in addition to nouns. Other shortcomings of our method that will have to be addressed in our future work are multi-word expressions and multiple senses of polysemous words and their translations. We also see potential in using cognates for re-ranking translation candidates as they are very common in the health domain. Acknowledgments Research reported in this paper has been supported by the ACCURAT project within the EU 7th Framework Programme (FP7/2007-2013), grant agreement no. 248347, and by the Slovenian Research Agency, grant no. Z6-3668. 
References  Arhar, ?., Gorjanc, V., and Krek, S. (2007). FidaPLUS corpus of Slovenian - The New Generation of the Slovenian Reference Corpus: Its Design and Tools. In Proceedings of the Corpus Linguistics Conference (CL2007), Birmingham, pp. 95-110. D?jean, H., Gaussier, E., Renders, J.-M. and Sadat, F. (2005). Automatic processing of multilingual medical terminology: Applications to thesaurus enrichment and cross-language information retrieval. Artificial Intelligence in Medicine, 33(2): 111?124.  Doe, J. (2011): Bilingual lexicon extraction from comparable corpora: A comparative study. Doe, J. (2011): Compiling web corpora for Croatian and Slovene. Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics - Special issue on using large corpora, 19(1). Fung, P. (1998). A statistical view on bilingual lexicon extraction: From parallel corpora to nonparallel corpora. In Proc. of the 3rd Conference of the Association for Machine Translation in the Americas, pp. 1?17. Fung, P., Prochasson, E. and Shi, S. (2010). Trillions of Comparable Documents. In Proc. of the 3rd workshop on Building and Using Comparable Corpora (BUCC'10), Language Resource and Evaluation Conference (LREC2010), Malta, May 2010, pp. 26?34. Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell, MA. Koehn, P. and Knight, K. (2002). Learning a translation lexicon from monolingual corpora. In Proc. of the workshop on Unsupervised lexical acquisition (ULA '02) at ACL 2002, Philadelphia, USA, pp. 9?16. Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1): 145-151. Ljube?i?, N. and Erjavec, T. (2011). hrWaC and slWaC: Compiling Web Corpora for Croatian and Slovene. (submitted to International Workshop on Balto-Slavonic Natural Language Processing). Ljube?i?, N., Fi?er D., Vintar ?. and  Pollak S. Bilingual Lexicon Extraction from Comparable Corpora: A Comparative Study. (accepted for WoLeR 2011 at ESSLLI International Workshop on Lexical Resources). 
25
Marsi, E. and Krahmer, E. (2010). Automatic analysis of semantic similarity in comparable text through syntactic tree matching. In Proc. of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752?760. Och, F. J. and Ney, H. (2000). Improved Statistical Alignment Models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, Hongkong, China, pp. 440?447. Otero, P. G. (2007). Learning Bilingual Lexicons from Comparable English and Spanish Corpora. In Proc. of the Machine Translation Summit (MTS 2007), pp. 191?198. Rapp, R. (1999). Automatic identification of word translations from unrelated English and German corpora. In Proc. of the 37th annual meeting of the Association for Computational Linguistics (ACL '99), pp. 519?526. Schmid, H. (1994): Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proc. of International Conference on New Methods in Language Processing. Saralegi, X., San Vicente, I. and Gurrutxaga, A. (2008). Automatic Extraction of Bilingual Terms from Comparable Corpora in a Popular Science Domain. In Proc. of the 1st Workshop on Building and Using Comparable Corpora (BUCC) at LREC 2008. Shao, L. and Ng, H. T. (2004). Mining New Word Translations from Comparable Corpora. In  Proc. 
of the 20th International Conference on Computational Linguistics (COLING '04), Geneva, Switzerland. Shezaf, D. and Rappoport, A. (2010). Bilingual Lexicon Generation Using Non-Aligned Signatures. In Proc.of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, pp. 98?107. Steinberger, R.,  Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufi?, D. and Varga, D. (2006). The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proc. of the 5th International Conference on Language Resources and Evaluation (LREC 2006), pp. 2142?2147. Tiedemann, J. (2005). Optimisation of Word Alignment Clues. Natural Language Engineering, 11(03): 279?293. Vorhees, E. M. (2001). Overview of the TREC-9 Question Answering Track. In Proceedings of the Ninth Text REtrieval Conference (TREC-9), 2001. Xiao, Z., McEnery, A. (2006). Collocation, semantic prosody and near synonymy: a cross-linguistic perspective. Applied Linguistics 27(1): 103?129. Yu, K. and Tsujii, J. (2009). Bilingual dictionary extraction from Wikipedia. In Proc. of the 12th Machine Translation Summit (MTS 2009), Ottawa, Ontario, Canada. 
26
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 87?92,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Were the clocks striking or surprising? Using WSD to improve MT performance 
?pela Vintar University of Ljubljana Dept. of Translation Studies SI - 1000 Ljubljana, A?ker?eva 2 spela.vintar@ff.uni-lj.si 
Darja Fi?er University of Ljubljana Dept. of Translation Studies SI - 1000 Ljubljana, A?ker?eva 2 darja.fiser@ff.uni-lj.si 
Aljo?a Vr??aj University of Ljubljana Dept. of Translation Studies SI - 1000 Ljubljana, A?ker?eva 2 aljosav@gmail.com 
   Abstract 
We report on a series of experiments aimed at improving the machine translation of ambig-uous lexical items by using wordnet-based unsupervised Word Sense Disambiguation (WSD) and comparing its results to three MT systems. Our experiments are performed for the English-Slovene language pair using UKB, a freely available graph-based word sense disambiguation system. Results are evaluated in three ways: a manual evaluation of WSD performance from MT perspective, an analysis of agreement between the WSD-proposed equivalent and those suggested by the three systems, and finally by computing BLEU, NIST and METEOR scores for all translation versions. Our results show that WSD performs with a MT-relevant precision of 71% and that 21% of sense-related MT er-rors could be prevented by using unsuper-vised WSD. 1 Introduction Ambiguity continues to be a tough nut to crack in MT. In most known languages certain lexical items can refer to more than a single concept, meaning that MT systems need to choose be-tween several translation equivalents represent-ing different senses of the source word. Wrong choices often result in grave translation errors, as words often refer to several completely unrelated concepts. The adjective striking can mean beauti-ful, surprising; delivering a hard blow or indicat-ing a certain time, and the noun ?course? can be something we give, take, teach or eat.  Our aim was to assess the performance of three MT systems for the English-Slovene lan-guage pair and to see whether wordnet-based Word Sense Disambiguation (WSD) could im-prove performance and assist in avoiding grave sense-related translation errors.  
For WSD we use UKB (Agirre and Soroa 2009), a graph-based algorithm that uses wordnet (Fellbaum 1998) and computes the probability of each sense of a polysemous word by taking into account the senses of context words. In our ex-periment we use Orwell's notorious novel 1984 as the source and its translation into Slovene by Alenka Puhar as the reference translation. We then disambiguate the English source with UKB, assign each disambiguated English word a Slo-vene equivalent from sloWNet (Fi?er 2009) and compare these with the equivalents proposed by Google, Bing and Presis. Results are evaluated in several ways:  ? By manually evaluating WSD perfor-mance from the MT perspective, ? By analysing the agreement between each of the MT systems and the UKB/wordnet-derived translation, ? By comparing BLEU, NIST and ME-TEOR scores achieved with each transla-tion version.  Our results show that the ad hoc WSD strategies used by the evaluated MT systems can definitely be improved by a proper WSD algorithm, but also that wordnet is not the ideal semantic resource to help resolve translation dilemmas, mainly due to its fine sense granularity.  2 Word Sense Disambiguation and Machine Translation Wordnet-based approaches to improving MT have been successfully employed by numerous authors, on the one hand as a semantic resource to help resolve ambiguity, and on the other hand as a rich source of domain-specific translation equivalents. As early as 1993 (Knight 1993), wordnet was used as the lower ontology within 
87
the PANGLOSS MT system. Yuseop et al (2002) have employed LSA and the semantic similarity of wordnet literals to translate colloca-tions, while Salam et al (2009) used wordnet for disambiguation and the choice of the correct translation equivalent in an English to Bengali SMT system. WSD for machine translation purposes slightly differs from traditional WSD, because distinct source language senses, which share the same translation equivalent, need not be differentiated in WSD (Vickrey et al 2005). This phenomenon is known as parallel ambiguities and is particu-larly common among related languages (Resnik and Yarowsky 2000). Although early experi-ments failed to provide convincing proof that WSD can improve SMT, Carpuat and Wu (2007), Chan et al (2007) and Ali et al (2009) clearly demonstrate that incorporating a word sense disambiguation system on the lexical level brings significant improvement according to all common MT evaluation metrics.  Still, using wordnet as the source of sense in-ventories has been heavily criticized not just in the context of MT (Apidianaki 2009), but also within other language processing tasks. The most notorious arguments against wordnet are its high granularity and - as a consequence - high similar-ity between some senses, but its global availabil-ity and universality seem to be advantages that prevail in many cases (Edmonds and Kilgarriff 2002).  Our experiments lie somewhat in between; on the one hand we demonstrate the potential of WSD in MT, especially for cases where different MT systems disagree, and on the other hand we attribute most WSD errors to the inadequacy of the sense splitting in wordnet (see Discussion).  3 Experimental setup 3.1 Corpus and MT systems Our corpus consists of George Orwell's novel 1984, first published in English in 1949, and its translation into Slovene by Alenka Puhar, first published in 1967. While it may seem unusual to be using a work of fiction for the assessment of MT systems, literary language is usually richer in ambiguity and thus provides a more complex semantic space than non-fiction.  We translated the entire novel into Slovene with Google Translate1, Bing2 and Presis3, the first                                                 1 http://translate.google.com (translation from and into Slovene has been available as of September 2008) 
two belonging to the family of freely available statistical systems and the latter being a rule-based MT system developed by the Slovenian company Amebis. For the purposes of further analysis and com-parison with our disambiguated corpus all texts - original and translations - have been PoS-tagged and lemmatized using the JOS web service (Er-javec et al 2010) for Slovene and ToTaLe (Er-javec et al 2005) for English. Because we can only disambiguate content words, we retained only nouns, verbs, adjectives and adverbs and discarded the rest. After all these preprocessing steps our texts end up looking as follows:  English: It was a bright cold day in April and the clocks were striking thirteen. English-preprocessed:  be bright cold day April clock be strike Slovene-reference:  Bil je jasen, mrzel aprilski dan in ure so bile trinajst. Slovene-reference-preprocessed:  biti biti jasen mrzel aprilski dan ura biti biti Slovene-Google: Bilo je svetlo mrzel dan v aprilu, in ure so bile trinajst presenetljiv. Slovene-Google-preprocessed: biti biti svetlo mrzel dan april ura biti biti presenetljiv Slovene-Bing: Je bil svetlo hladne dan aprila in v ure so bili presenetljivo trinajst. Slovene-Bing-preprocessed: biti biti svetlo hladen dan april ura biti biti presenetljivo Slovene-Presis: Svetal hladen dan v aprilu je bilin so ure udarjale trinajst. Slovene-Presis-preprocessed: svetel hladen dan april biti bilin biti ura udarjati Figure 1. Corpus preprocessing 3.2 Disambiguation with UKB and wordnet The aim of semantic annotation and disambig-uation is to identify polysemous lexical items in the English text and assign them the correct sense in accordance with the context. Once the sense of the word has been determined, we can exploit the cross-language links between word-nets of different languages and propose a Slo-vene translation equivalent from the Slovene wordnet.  We disambiguated the English corpus with UKB, which utilizes the relations between synsets and constructs semantic graphs for each candidate sense of the word. The algorithm then                                                                        2 http://www.microsofttranslator.com/ (available for Slo-vene since 2010) 3 http://presis.amebis.si (available for English-Slovene since 2002) 
88
computes the probability of each graph based on the number and weight of edges between the nodes representing semantic concepts. Disam-biguation is performed in a monolingual context for single- and multiword nouns, verbs, adjec-tives and adverbs, provided they are included in the English wordnet.  Figure 2 shows the result of the disambigua-tion algorithm for the word face, which has as many as 13 possible senses in wordnet. We are given the probability of each sense in the given context (eg. 0.173463) and the ID of the synset (eg. eng-30-05600637-n), and for the purposes of clarity we also added the literals (words) associ-ated with this particular synset ID in the English (face, human face) and Slovene (fris, obraz, fa-ca) wordnet respectively. As can be seen from this example, wordnet is - in most cases - a very fine-grained sense inventory, and looking at the Slovene equivalents clearly shows that many of these senses may partly or entirely overlap, at least in the context of translation.  WSD: ctx_Oen.1.1.2 24    !! face ? W: 0.173463     ID: eng-30-05600637-n   ENGWN: face, human face,  (the front of the human head from the forehead to the chin and ear to ear)    SLOWN: fris, obraz, faca, ?love?ki obraz,  (EMPTYDEF) ? W: 0.116604     ID: eng-30-08510666-n   ENGWN: side, face,  (a surface forming part of the outside of an object)        SLOWN: stranica, ploskev,  (EMPTYDEF) ? W: 0.0956895    ID: eng-30-03313602-n   ENGWN: face,  (the side upon which the use of a thing depends (usually the most prominent surface of an object))        SLOWN: sprednja stran, prava stran, zgornja stran, lice,  (EMPTYDEF) ? W: 0.0761554    ID: eng-30-04679738-n   ENGWN: expression, look, aspect, facial expression, face,  (the feelings expressed on a person's face)  SLOWN: izraz, pogled, obraz, izraz na obrazu,  (EMPTYDEF) ? W: 0.0709513    ID: eng-30-03313456-n   ENGWN: face,  (a vertical surface of a building or cliff)  SLOWN: stena, fasada,  (EMPTYDEF) ? W: 0.0653514    ID: eng-30-06825399-n   ENGWN: font, fount, typeface, face, case,  (a specific size and style of type within a type family)     SLOWN: font, pisava, ?rkovna dru?ina, vrsta ?rk, ?rkovna podoba, ?rkovni slog,  (EMPTYDEF) ? W: 0.0629878    ID: eng-30-04838210-n   ENGWN: boldness, nerve, brass, face, cheek,  (impudent aggressiveness)  SLOWN: predrznost, nesramnost,  (EMPTYDEF) ? W: 0.0610286    ID: eng-30-06877578-n   ENGWN: grimace, face,  (a contorted facial expression)  SLOWN: spaka, grimasa,  (EMPTYDEF) ? W: 0.0605221    ID: eng-30-03313873-n   ENGWN: face,  (the striking or working surface of an implement) SLOWN: ?elo, podplat, udarna povr?ina,  (EMPTYDEF) ? W: 0.0579952    ID: eng-30-05601198-n   ENGWN: face,  (the part of an animal corresponding to the human face)   SLOWN: obraz,  (EMPTYDEF) ? W: 0.0535548    ID: eng-30-05168795-n   ENGWN: face,  (status in the eyes of others)    SLOWN: ugled, dobro ime,  (EMPTYDEF) ? W: 0.05303      ID: eng-30-09618957-n   ENGWN: face,  (a part of a person that is used to refer to a person)    SLOWN: obraz,  (EMPTYDEF) 
? W: 0.0526668    ID: eng-30-04679419-n   ENGWN: face,  (the general outward appearance of something)     SLOWN: podoba,  (EMPTYDEF) Figure 2. Disambiguation result for the word face with probabilities for each of the twelve senses  As can be seen in Table 1, almost half of all the tokens in the corpus are considered to be am-biguous according to the English wordnet. Since the Slovene wordnet is considerably smaller than the English one, almost half of the different am-biguous words occurring in our corpus have no equivalent in sloWNet. This could affect the re-sults of our experiment, because we cannot eval-uate the potential benefit of WSD if we cannot compare the translation equivalent from sloWNet with the solutions proposed by different MT sys-tems. We therefore restricted ourselves to the words and sentences for which an equivalent ex-ists in sloWNet.   Corpus size in tokens 103,769 Corpus size in types 10,982 Ambiguous tokens 48,632 Ambiguous types 7,627 Synsets with no equivalent in sloWNet 3,192 Table 1. Corpus size and number of ambiguous words  One method of evaluating the performance of WSD in the context of Machine Translation is through metrics for automatic evaluation (BLEU, NIST, METEOR etc.). We thus generated our own translation version, in fact a stripped version similar to those in Figure 1 consisting only of content words in their lemmatized form. We translated the disambiguated words with word-net, exploiting the cross-language universality of the synset ID. However, since we can only pro-pose translation equivalents for the words which are included in wordnet, we had to come up with a translation solution for those which were not. Such words include proper names (Winston, Smith, London, Oceania), hyphenated com-pounds (pig-iron, lift-shaft, gorilla-faced) and Orwellian neologisms (Minipax, Newspeak, thoughtcrime). We translated these words with three alternative methods:  ? Using a general bilingual dictionary, ? Using the English-Slovene Wikipedia and Wiktionary, 
89
? Using the automatically constructed bi-lingual lexicon from the English-Slovene parallel Orwell corpus.  The fourth option was to leave them untranslated and simply add them to the generated Slovene version.  4 Evaluation The number of meanings a word can have, the degree of translation equivalence or the quality of the target text are all extremely disputable and vague notions. For this reason we wished to evaluate our results from as many angles as pos-sible, both manually and automatically.  4.1 Manual evaluation of WSD precision in the context of MT Firstly, we were interested in the performance of the UKB disambiguation tool in the context of MT. Since UKB uses wordnet as a sense invento-ry, the algorithm assigns a probability to each sense of a lexical item according to its context in an unsupervised way. The precision of UKB for unsupervised WSD is reported at around 58% for all words and around 72% for nouns, but of course these figures measure the number of cases where the algorithm selected the correct wordnet synset from a relatively fine-grained network of possible senses (Agirre and Soroa 2009). We adjusted the evaluation task to an MT scenar-io by manually checking 200 disambiguated words and their suggested translation equiva-lents, and if the equivalent was acceptable we counted it among the positive instances regard-less of the selected sense. For example, the Eng-lish word breast has four senses in wordnet: (1) the upper frontal part of a human chest, (2) one of the two soft milk-secreting glands of a wom-an, (3) meat carved from the breast of a fowl and (4) the upper front part of an animal correspond-ing to the human chest. For the English sentence Winston nuzzled his chin into his breast... UKB suggested the second sense, which is clearly wrong, but since the ambiguity is preserved in Slovene and the word prsi can be used for all of the four meanings, we consider this a case of successful disambiguation for the purposes of MT.   Translation equivalent correct incorrect borderline Number/ % 142 (71%) 46 (23%) 12 (6%) Table 2: Manual evaluation of WSD perfor-mance for MT 
 The precision of WSD using this relaxed criteri-on was 71%, with 6% so-called borderline cases. These include cases where the equivalent was semantically correct but had the wrong part of speech (eg. glass door -> *steklo instead of steklen).   4.2 Agreement between each of the MT systems and the disambiguated equivalent It is interesting to compare the equivalents we propose through our wordnet-based WSD procedure with those suggested by the three MT systems: Presis, Google and Bing.  Total no. of disambiguated tokens 13,737 WSD = reference 3,933 WSD = Presis 4,290 WSD = Google 4,464 WSD = Bing 4,377 WSD = ref = Presis = Google = Bing 2,681 WSD = ref ? Presis ? Google ? Bing 269 Table 3: Comparison of WSD/wordnet-based equivalent and the translations proposed by Presis, Google, Bing and the reference transla-tion  The comparison was strict in the sense that we only took into account the first Slovene equiva-lent proposed within the same synset. Of the over 48k ambiguous tokens we obviously considered only those which had an equivalent in sloWNet, otherwise comparison with the MT systems would have been impossible. We can see from Table 2 that the WSD/wordnet-based equivalents most often agree with Google translation, and that for approximately every fifth ambiguous word all systems agree with each other and with the reference translation.  If we also look at the number of cases where our WSD-wordnet-based equivalent is the only one to agree with the reference translation, it is safe to assume that these are the cases where WSD could clearly improve MT. Of all the in-stances where WSD agrees with the reference translation we can subtract the instances where all systems agree, because these need no im-provement. Of the remaining 1,252 ambiguous words, 269 or 20% were such that only the WSD/wordnet equivalent corresponded to the reference translation. 
90
4.3 Evaluation with metrics Finally, we wanted to see how the WSD/wordnet-based translation compares with the three MT systems using the BLEU, NIST and METEOR scores. For the purposes of this com-parison we pre-processed all five versions of our corpus - original, reference translation, Presis, Google and Bing translation - by lemmatization, removal of all function words, removal of sen-tences where the alignment was not 1:1, and fi-nally by removal of the sentences which con-tained lexical items for which there was no equivalent in sloWNet. We then generated the sixth version by trans-lating all ambiguous words with sloWNet (see Section 3), and for the words not included in the English wordnet we used four alternative transla-tion strategies; a general bilingual dictionary (dict), wiktionary (wikt), a word-alignment lexi-con (align) and amending untranslated words to the target language version (amend).      BLEU (n=1) NIST METEOR Bing 0.506 3.594 0.455 Google 0.579 4.230 0.481 Presis 0.485 3.333 0.453 WSD 0.440 3.258 0.429 WSD-amend 0.410 3.308 0.430 WSD-dict 0.405 3.250 0.427 WSD-align 0.448 3.588 0.434 WSD-wikt 0.442 3.326 0.429 Table 4: Evaluation with metrics  Table 3 shows the results of automatic evalua-tion; the corpus consisted of 2,428 segments. We can see that our generated version using disam-biguated equivalents does not outperform any of the MT systems on any metric, except once when the WSD-align version outperforms Presis on the NIST score and comes fairly close to the Bing score.  It is possible that the improvement we are try-ing to achieve is difficult to measure with these metrics because our method operates on the level of single words, while the metrics typically eval-uate entire sentences and corpora. We are using a stripped version of the corpus, ie. only content words which can potentially be ambiguous, whereas the metrics are normally used to calcu-late the similarity between two versions of run-ning text. Finally, the corpus we are using for automatic evaluation is very small. 
5 Discussion Although employing WSD and comparing word-net-based translation equivalents to those pro-posed by MT systems scored no significant im-provement with standard MT evaluation metrics, we remain convinced that the other two evalua-tion methods show the potential of using WSD, particularly with truly ambiguous words and not those where sense distinctions are slight or vague. A manual inspection of the examples where MT systems disagreed and our WSD-based equiva-lent was the only one to agree with the reference translation shows that these are indeed examples of grave MT errors. For example, the word hand in the sentence The clock's hands said six mean-ing eighteen can only be translated correctly with a proper WSD strategy and was indeed mistrans-lated as roka (body part) by all three systems. If a relatively simplistic and unsupervised technique such as the one we propose can prevent 20% of these mistakes, it is certainly worth employing at least as a post-processing step.  The fact that we explore the impact of WSD on a work of fiction rather than domain-specific texts may also play a role in the results we ob-tained, although it is not entirely clear in what way. We believe that in general there is more ambiguity in literary texts meaning that a single word will appear in a wider range of senses in a work of fiction than it would in a domain-specific corpus. This might mean that WSD for literary texts is more difficult, however our own experiments so far show no significant difference in WSD performance.  A look at the cases where WSD goes wrong shows that these are typically words with a high number of senses which are difficult to differen-tiate even for a human. The question from the title of this paper is actually a translation blunder made by both Google and Bing, since striking was interpreted in its more expressive sense and translated into Slovene as presenetljiv [surpris-ing]. However, UKB also got it wrong and chose the sense defined as deliver a sharp blow, as with the hand, fist, or weapon instead of indicate a certain time by striking. While these meanings may seem quite easy to tell apart, especially if the preceding word in a sentence is clock, strike as a verb has as many as 20 senses in Princeton WordNet, and many of these seem very similar. In this case the Slovene translation we propose is "less wrong" than the surprising solution offered by Google or Bing, because udarjati may actual-ly be used in the clock sense as well.  
91
We might also assume that statistical MT sys-tems will perform worse on fiction; results in Table 3 show that both statistical systems outper-form the rule-based Presis. Then again, Orwell's 1984 has been freely available as a parallel cor-pus for a very long time and it is therefore possi-ble that both Google and Bing have used it as training data for their SMT model.  6 Conclusion We described an experiment in which we explore the potential of WSD to improve the machine translation of ambiguous words for the English-Slovene language pair. We utilized the output of UKB, a graph-based WSD tool using wordnet, to select the appropriate equivalent from sloWNet. Manual evaluation showed that the correct equivalent was proposed in 71% of the cases. We then compared these equivalents with the output of three MT systems. While the benefit of WSD could not be proven with the BLEU, NIST and METEOR scores, the correspondence of the WSD/wordnet-based equivalent with the refer-ence translation was high. Furthermore it appears that in cases where MT systems disagree WSD can help choose the correct equivalent.  As future work we plan to redesign the exper-iment so as to directly use WSD as a post-processing step to machine translation instead of generating our own stripped translation version. This would provide better comparison grounds. In order to improve WSD precision we intend to combine two different algorithms and use it only in cases where both agree. Also, we intend to experiment with different text types and context lengths to be able to evaluate WSD performance in the context of MT on a larger scale.  References Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. Pro-ceeding of the European Association of Computa-tional Linguistics conference (EACL09). Ola Mohammad Ali, Mahmoud Gad Alla and Mo-hammad Said Abdelwahab. 2009. Improving ma-chine translation using hybrid dictionary-graph based word sense disambiguation with semantic and statistical methods. International Journal of Computer and Electrical Engineering, 1/5. Marianna Apidianaki. 2009. Data-driven semantic analysis for multilingual WSD and lexical selection in translation. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 77?85, 
Athens, Greece, Association for Computational Linguistics. Marine Carpuat and Dekai Wu. 2007. Improving sta-tistical machine translation using word sense dis-ambiguation. Proceedings of Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).  Yee Seng Chan, Hwee Tou Ng and David Chiang. 2007. Word Sense Disambiguation Improves Sta-tistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association of Compu-tational Linguistics (Prague, Czech Republic). 33-40. Philip Edmonds and Adam Kilgarriff. 2002. Introduc-tion to the Special Issue on Evaluating Word Sense Disambiguation Systems. Natural Language Engi-neering 8 (4): 279?291.  Toma? Erjavec, Darja Fi?er, Simon Krek and Nina Ledinek. 2010. The JOS Linguistically Tagged Corpus of Slovene. Proceedings of the 7th Interna-tional Conference on Language Resources and Evaluation (LREC'10), Malta. Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press. Darja Fi?er. 2009. Leveraging Parallel Corpora and Existing Wordnets for Automatic Construction of the Slovene Wordnet. Human language technolo-gy: challenges of the information society, (LNCS 5603). Berlin; Heidelberg: Springer: 359-368. Kevin Knight. 1993. Building a large ontology for machine translation. Proceedings of the ARPA Human Language Technology Workshop, Plains-boro, New Jersey. Philip Resnik and David Yarowsky. 2000. Distin-guishing Systems and Distinguishing Senses: New Evaluation Methods for Word Sense Disambigua-tion. Natural Language Engineering, 5(2): 113-133. Khan Md. Anwarus Salam, Mumit Khan and Tetsuro Nishino. 2009. Example based English-Bengali machine translation using wordnet. Proceedings of TriSA'09, Japan. David Vickrey, Luke Biewald, Marc Teyssier in Daphne Koller. 2005. Word-Sense Disambiguation for Machine Translation. Proceedings of the Con-ference Empirical Methods in Natural Language Processing (EMNLP).  Kim Yuseop, Jeong-Ho Chang in Byoung-Tak Zhang (2002): Target Word Selection Using WordNet and Data-Driven Models in Machine Translation. Pro-ceedings of the Conference PRICAI?02: Trends in Artificial Intelligence.  
92
