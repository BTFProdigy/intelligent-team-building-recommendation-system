An Enhanced Model  
for Chinese Word Segmentation and Part-of-speech Tagging 
Jiang Feng 
Department of  
Computer Science and Engineering  
Shanghai Jiao Tong University 
Shanghai, China, 200030  
f_jiang@sjtu.edu.cn 
 
Liu Hui 
Department of  
Computer Science and Engineering  
Shanghai Jiao Tong University 
Shanghai, China, 200030 
lh_Charles@sjtu.edu.cn 
 
Chen Yuquan 
Department of  
Computer Science and Engineering  
Shanghai Jiao Tong University 
Shanghai, China, 200030 
yqchen@mail.sjtu.edu.cn 
Lu Ruzhan 
Department of  
Computer Science and Engineering  
Shanghai Jiao Tong University 
Shanghai, China, 200030 
rzlu@mail.sjtu.edu.cn 
 
Abstract 
This paper will present an enhanced 
probabilistic model for Chinese word 
segmentation and part-of-speech (POS) 
tagging. The model introduces the information 
of Chinese word length as one of its features 
to reach a more accurate result. And in 
addition, the model also achieves the 
integration of segmentation and POS tagging. 
After presenting the model, this paper will 
give a brief discussion on how to solve the 
problems in statistics and how to further 
integrate Chinese Named Entity Recognition 
into the model. Finally, some figures of 
experiments and comparisons will be reported, 
which shows that the accuracy of word 
segmentation is 97.09%, and the accuracy of 
POS tagging is 98.77%. 
1 Introduction 
Generally, Chinese Lexical Analysis consists of 
two phases; one is word segmentation and the 
other is part-of-speech(POS) tagging. Rule -based 
approach and statistic -based approach are two 
dominant ways in natural language processing, as 
well as Chinese Lexical Analysis. This paper will 
only focus on the later one. Hence, our model is 
called a probabilistic model.  
Scanning through the researches in this field 
before, we have just found two points at which the 
performance of a Chinese word segmentation and 
POS tagging system could get better. One is the on 
the system architecture, and the other is from the 
Machine Learning theory. 
First, the traditional way of Chinese Lexical 
Analysis simply regards the word segmentation 
and POS tagging as two separated phases. Each 
one of them has its own algorithms and models.  
Dividing the whole process into two independent 
parts can lower the complexity of the design of 
system, but decrease the performance as well, 
because the two are fully integrated when a human 
processing a sentence. Fortunately, many 
researchers have already noticed it, and recent 
projects pay more attention on the integration of 
word segmentation and POS tagging, such as [Gao 
Shan, Zhang Yan. 2001]?s pseudo trigram 
integrated model, [Fu Guohong et al 2001]?s 
analyzer which incorporates backward Dynamic 
Programming and A* algorithm, [Sun Maosong, et 
al. 2003]?s ?Divide and Conquer integration?, 
[Zhang Huaping, et al 2003]?s hierarchical hidden 
Markov model and so on. The experiments given 
by these papers also showed a great potential of the 
integrated models. 
Besides the system architecture, another point 
should be noticed. A probabilistic model of word 
segmentation and POS tagging can be regarded as 
an instance of Machine Learning. In Machine 
Learning, the feature extraction is the most 
important aspect, and far more important than a 
learning algorithm. In the models nowadays, it 
seems that the features for Chinese Lexical 
Analysis are a little too simple. Most of them take 
tag sequences, or word frequencies as the 
distinguishing features and ignore the other useful 
information that are provided by Chinese itself. 
In this paper, we will present an enhanced, not 
too complex, model for word segmentation and 
POS tagging, which will not only inherit the merit 
of an integrated model, but also take a new feature 
(word length) into account.  
The second part of this paper will describe the 
model, including the input, output, and some 
assumptions. The third part will give some brief 
discussion about the model on some issues like 
data sparseness and Named Entity Recognition. In 
the final part, the results of our experiments will be 
reported. 
2 The Model 
The first step to establish the model is to make a 
formal description for its input and output. Here, a 
Chinese word segmentation and POS tagging 
system is viewed as with input, 
nCCC ,...,, 21  
where Ci is the i'th Chinese character of the input 
sentence, and with output pairs, ( nm ? ) 
???
?
???
?
???
?
???
?
???
?
???
?
m
m
T
L
T
L
T
L
,...,
2
2
1
1  
where Li is the word length of the i?th word in 
the segmented word sequence, Ti is the word tag, 
and each (Li, Ti) pair is corresponding to a 
segmented and tagged word, and ?
=
=
m
i
ni
1
. 
 
It is easily seen that the distinction between this 
model and other models is that this one introduces 
word length. In fact, word length really works, and 
affects the performance of the system in a great 
deal, of which our later experiments will approve.  
The motivation to introduce word length into 
our model is initially from the classical Chinese 
poems. When we read these poems, we may 
spontaneously obey some laws in where to have a 
pause. For example, in most cases, a 7-character-
lined Jueju(A kind of poem format) is read as 
**/**/***. And the pauses in a sentence are much 
related to the length of words or chunks. Even in 
modern Chinese, word length also plays a part. 
Sometimes we prefer to use disyllabic words rather 
than single one, though both are correct in 
grammar. For example, in our daily lives, we 
always say ? /n /v /n? or ? /n 
/v /n?, but seldom hear ? /n /v /n?, 
where ? ?, ? ? and ? ? have the same 
meaning. So, it is reasonable to assume that the 
occurrence of the word length will obey some 
unwritten laws when human writes or speaks. 
Introducing the word length into the word 
segmentation and POS tagging model may be in 
accord with the needs for processing Chinese. 
Another main characteristic of the model is that 
it is an integrated model, because there is only one 
hop through the input sentence to the output word-
tag sequence.  
 
The following text will introduce how the model 
works. We will also inherit n-gram assumption in 
our model. 
Our destination is to find a sequence of (Li, Ti) 
pairs that maximizes the probability,  
)|),(( CTLP  
i.e. 
)|),((maxarg),(
),(
CTLPTL
TL
R =  ? ? ? ? . 2.1 
(For conveniece, we will use A
r
 to represent a 
squence of A1, A2, A3...) 
And,  
)(
),(*)),(|(
)|),((
CP
TLPTLCP
CTLP = .......2.2 
For )(CP  is a constant given a C , we just need 
to consider )),(|( TLCP and ),( TLP . 
First consider )),(|( TLCP . Suppose W  is the 
vertex of words that ),( TL represents(i.e. the 
segmented word sequence), and the dependency 
assumption is like the following Bayers Network: 
 
Figure 2.1: Dependency assumption among  
length-tag pair, word and character 
 
So, we have, 
)),(|(*)|()),(|( TLWPWCPTLCP = ?2.3 
Because W  is the segmentation of C , 
)|( WCP  is always 1, and by another assumption 
that the occurrence of every word is independent to 
each other, then 
?
=
=
m
i
iii TLWPTLWP
1
)),(|()),(|( ???2. 4 
where )),(|( iii TLWP  means the conditional 
probability of Wi under Li and Ti. For example, 
P(? ?|2, v) is the conditional probability of 
? ? under a 2-charactered verb which may be 
computed as (the number of ? ? appearing as a 
verb) / (the number of all 2-charactered verbs). 
With 2.3 and 2.4, )),(|( TLCP  is ready. 
 
Then consider ),( TLP , which is easy to 
retrieve when we apply n-gram assumption. 
Suppose n is 2, which means that (Li, Ti) only 
depends on (Li-1, Ti-1). 
?
=
--=
m
i
iiii TLTLPTLP
1
11 )),(|),((),( ? ? ..2.5 
Here )),(|),(( 11 -- iiii TLTLP means the 
probability of a Tag Ti with Length Li appearing 
next to Tag Ti-1 with Length Li-1, which may be 
computed as (the number of (Li-1, Ti-1)(Li, Ti) 
appearing in corpus) / (the number of (Li-1,  Ti-1) 
appearing in corpus). So, ),( TLP  is also ready. 
 
Combining formula 2.1, 2.2, 2.3, 2.4 and 2.5, we 
have, 
?
= -
-
???
?
???
?
???
?
???
?
???
?
???
?=
m
i i
i
i
i
i
i
i
TL
R T
L
T
L
P
T
L
WPTL
1 1
1
),(
)|(*)|(maxarg),(
............................................2.6 
 
Now, the enhanced model is complete with 2.6. 
When establishing the model, we have made 
several assumptions.  
1. the dependency assumption between tag-length 
pairs, words and characters like the Bayers 
network of figure 2.1 
2. Word and word are independent. 
3. n-gram assumption on (T,L) pairs. 
The validation of these assumptions is still 
somewhat in doubt, but the computational 
complexity of the model is decreased. 
All the resources required to achieve this model 
are also listed, i.e., a word list with 
probability )|( ???
?
???
?
i
i
i T
L
WP , and an n-gram transition 
network with probability ),...,|(
1
1
1
1
???
?
???
?
???
?
???
?
???
?
???
?
-
-
+-
+-
i
i
ni
ni
i
i
T
L
T
L
T
L
P . 
The algorithm to implement this model is also 
rather simple, and using Dynamic Programming, 
we could finish the algorithm in O(cn), where n is 
the length of input sentence, and c is a constant 
related to the maximum ambiguity in a position. 
 
3 Discussion 
Though the model itself is not difficult to 
implement as we have presented in last section, 
there are still some problems that we will be 
probably encountered with in practice. The first 
one is the data sparseness when we do the statistics. 
Another is how to further integrate Chinese Named 
Entity Recognition into the new, word-length-
introduced model. 
 
3.1 Data Sparseness 
The Data Sparseness happens when we are 
calculating ),...,|(
1
1
1
1
???
?
???
?
???
?
???
?
???
?
???
?
-
-
+-
+-
i
i
ni
ni
i
i
T
L
T
L
T
L
P . After the 
word length is introduced, the need for larger 
corpus is greatly increased. Suppose we are using a 
tri-gram assumption on length-tag pairs, the 
number of tags is 28 as that of our system, and the 
max word length is 6, then the number of patterns 
we should count is, 
 28 * 6 * 28 * 6 * 28 * 6 = 4,741,632.  
To retrieve a reasonable statistical result, the 
scale of the corpus should at least be several times 
larger than that value. It is common that we don?t 
have such a large corpus, and meet the problem so 
called Data Sparseness.  
One way to deal with the problem is to find a 
good smoothing, and another is to make further 
independent assumption between word length and 
word tag. The word length sequence and word tag 
sequence can be considered independent. That 
means, 
),...,|(*),...,|(
),...,|(
1111
1
1
1
1
-+--+-
-
-
+-
+-
=
???
?
???
?
???
?
???
?
???
?
???
?
iniiinii
i
i
ni
ni
i
i
TTTPLLLP
T
L
T
L
T
L
P  
.........................................3.1 
Now, the patterns to count are just as many as 
those of a traditional n-gram assumption that only 
assumes the dependency among tags. 
 
3.2 Named Entity Recognition Integration 
Named Entity Recognition is one of the most 
important parts of word segmentation and POS 
tagging systems, for the words in word list are 
limited while the language seems infinite. There 
are always new words appearing in human 
language, among which human names, place 
names and organization names are most common 
and most valuble  to recognize. The performance of 
Named Entity Recognition will have a deep impact 
on the performance of a whole word segmentation 
and POS tagging system. The research on Named 
Entity Recognition has appeared for many years. 
No matter whether the current performance of 
Named Entity Recognition is ideal or not, we will 
not discuss it here, and instead, we will just show 
how to integrate the existing Name Entity 
Recognition methods into the new model.  
During the integration, more attention should be 
paid to the structural and probabilistic consistency. 
For structural consistency, the original system 
structure does not need modifying when a new 
method of Named Entity Recognition is applied. 
For probabilistic  consistency, the probabilit ies 
outputted by the Named Entity Recognition should 
be compatible with the probabilit ies of the words 
in the original word list.  
Here, we will take the Human Name 
Recognition as an example to show how to do the 
integration. 
[Zheng Jiahen, et al 2000] has presented a 
probabilistic  method for Chinese Human Name 
Recognition, which is easy to understand and 
suitable to be borrowed as a demonstration. 
That paper defined the probability for a Chinese 
Human Name as: 
)(*)()|( kEiFiknsP = ............................3.2 
)(*)(*)()|( kEjMiFijknpP = .............3.3 
Where each one of ?i?, ?j?, ?k? represents a 
single Chinese characters, ?ik?, ?ijk? are the strings 
which may be a human name, ?ns? means a single 
name when ?j? is empty, ?np? means plural name 
when ?j? is not empty, F(i) is the probability of ?i? 
being a family name, M(j) means the probability of 
?j? being the middle character of a human name, 
E(k) means the probability of ?k? being the tailing 
character of a human name, P(ns | ik ) is the 
probability of ?ik? being a single name, and P(np | 
ijk ) is the probability of ?ijk? being a plural name. 
F(i), M(j), and E(k) are easily retrieved from 
corpus, so P(ns | ik ) and P(np | ijk) can be known. 
However, P(ns | ik ) and P(np | ijk) do not satisfy 
the requirements of the word length introduced 
model. The model needs probabilit ies like 
)),(|( tlwP , where w is a word, t is a word tag, and 
l is the word length. Therefore, P(ns | ik) needs to 
be modified into P(ik | nh, 2), for ik is always a 2-
charactered word, and likewise, P(np | ijk) needs to 
be modified into P(ijk | nh, 3), where ?nh? is the 
word tag for human name in our system. 
P(ns | ik ) is equivalent to P(nh, 2 | ik ) and P(np | 
ijk ) is equivalent to P(nh, 3 | ijk). P(ns | ik) can be 
converted into P(ik | nh, 2) through following way, 
)2,(
)()()|2,(
)2,(
)()|2,(
)2,|(
nhP
kPiPiknhP
nhP
ikPiknhP
nhikP
==
 
.........................................3.4 
where ?i?, ?k? have the same meaning with those 
in 3.2 and 3.3. and nh is the tag for human name. 
In this formula, ?i? and ?k? are assumed to be 
independent. P(nh, 2), P(i), P(k) are easy to 
retrieve, which represent the probability of a 2-
charactered human name, the probability of 
character ?i? and the probability of character ?k?. 
P(nh, 2 | ik) is computed from 3.2. Thus, the 
conversion of P(nk  | nh, 2) to P(nh, 2 | ik ) is done. 
In the same way, P(np | ijk) can be converted 
into P(ijk | nh, 3) by: 
)3,(
)()()()|3,(
)3,(
)()|3,(
)3,|(
nhP
kPjPiPiknhP
nhP
ijkPijknhP
nhijkP
==
 
...........................................3.5 
Finally, the Human Name Recognition Module  
is integrated into the whole system. The input 
string C1, C2, ?, Cn first goes through the Human 
Name Recognition module, and the module 
outputs a temporary word list, which consists of a 
column of words that are probably human names 
and a column of probabilities corresponding to the 
words, which can be computed by 3.4 and 3.5. The 
whole system then merges the temporary word list 
and the original word list into a new word list, and 
applies the new word list in segmenting and 
tagging C1, C2, ?, Cn. 
 
4 Conclusion & Experiments 
This paper has presented an enhanced 
probabilistic model of Chinese Lexical Analysis, 
which introduces word length as one of the 
features and achieves the integration of word 
segmentation, Named Entity Recognition and POS 
tagging.  
At last, we will briefly give the results of our 
experiments. In the previous experiments, we have 
compared many simple probabilistic models for 
Chinese word segmentation and POS tagging, and 
found that the system using maximum word 
frequency as segmentation strategy and forward 
tri-gram Markov model as POS tagging model 
(MWF + FTMM) reaches the best performance. 
Our comparisons will be done between the 
MWF+FTMM and the enhance model with tri-
gram assumption. The training corpus is 40MB 
annotated Chinese text from People?s Daily. The 
testing data is about 1MB in size and is from 
People?s Daily, too.  
 
 MWF+FTMM New Model 
WSA 95.24% 97.09% 
PTA 97.12% 98.77% 
Total 92.50% 95.90% 
Table 4.1: The accuracy by word,  
with named entity not considered 
 
 MWF+FTMM New Model 
WSA 93.86% 95.68% 
PTA 93.89% 95.72% 
Total 88.13% 91.59% 
Table 4.2: The accuracy by word, 
with named entity considered 
 
 MWF+FTMM New Model 
WSA 69.46% 82.63% 
PTA 72.58% 80.33% 
Total 50.42% 66.38% 
Table 4.3: The accuracy by sentence, 
with named entity not considered 
 
 MWF+FTMM New Model 
WSA 63.86% 74.78% 
PTA 61.40% 67.41% 
Total 39.21% 50.42% 
Table 4.4: The accuracy by sentence, 
with named entity considered 
 
NOTES: 
MWF: Maximum Word Frequency, a very simple 
strategy in word segmentation disambiguation, 
which chooses the word sequence with max 
probability as its result.  
FTMM: Forward Tri-gram Markov Model, a 
popular model in POS tagging. 
MWF+FTMM: A strategy, which chooses the 
output that makes a balance between the MWF 
and FTMM as its result. 
WSA (by word): Word Segmentation Accuracy, 
measured by recall, i.e. the number of correct 
segments divided by the number of segments 
in corpus.  
(In a problem like word segmentation, the 
result of precision measurement is commonly 
around that of recall measurement.) 
PTA (by word): POS Tagging Accuracy based on 
correct segmentation, the number of words that 
are correctly segmented and tagged divided by 
the number of words that are correctly 
segmented. 
Total (by word): total accuracy of the system, 
measured by recall, i.e. the number of words 
that are correctly segmented and tagged 
divided by the number of words in corpus, or 
simply WSA * PTA. 
WSA (by sentence): the number of correctly 
segmented sentences divided by the number of 
sentences in corpus. A correctly segmented 
sentence is a sentence whose words are all 
correctly segmented.  
PTA (by sentence): the number of correctly tagged 
sentences divided by the number of correctly 
segmented sentences in corpus. A correctly 
tagged sentence is a sentence whose words are 
all correctly segmented and tagged. 
Total (by sentence): WSA * PTA. 
Named entity considered or not: When named 
entity is not considered, all the unknown words 
in corpus are deleted before evaluation. 
Otherwise, nothing is done on the corpus. 
 
According to the results above (Table 4.1, Table 
4.2, Table 4.3, Table 4.4), the new enhanced model 
does better than the MWF + FTMM in every field. 
Introducing the word length into a Chinese word 
segmentation and POS tagging system seems 
effective.  
This paper just focuses on the pure probabilistic 
model for word segmetation and POS tagging. It 
can be predicted that, with more disambiguation 
strategies, such as some rule based approaches, 
being implemented into the new model to achieve 
a multi-engine system, the performance will be 
further improved. 
 
5 Acknowledgements 
Thank Fang Hua and Kong Xianglong for their 
previous work, who have just graduated. 
References  
Sun Maosong, Xu Dongliang, Benjamin K Tsou. 
2003. Integrated Chinese word segmentation and 
part-of-speech tagging based on the divide-and-
conquer strategy. International Conference on 
Natural Language Processing and Knowledge 
Engineering Proceedings, Beijing. 
Zhang Huaping, Liu Qun, et al 2003. Chinese 
lexical analysis using hierarchical hidden 
Markov model. 2nd SIGHAN workshop 
affiliated with 41th ACL, Sapporo Japan 
Fu Guohong, Wang Ping, Wang Xiaolong. 2001. 
Research on the approach of integrating chinese 
wordd segmentation with part-of-speech tagging. 
Application Research of Computer. (In Chinese) 
Gao Shan, Zhang Yan. 2001. The Research on 
Integrated Chinese Word Segmentation and 
Labeling based on trigram statistical model. 
Natural Language Understanding & Machine 
Translation (JSCL-2001), Taiyuan. (In Chinese) 
Zheng Jiahen, Li Xin, et al 2000. The Research of 
Chinese names recognition method based on 
corpus. Journal of Chinese Information 
Processing. (In Chinese) 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 199?207,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Weakly Supervised Learning Approach 
for Spoken Language Understanding 
Wei-Lin Wu, Ru-Zhan Lu, Jian-Yong Duan,  
Hui Liu, Feng Gao, Yu-Quan Chen 
Department of Computer Science and Engineering 
Shanghai Jiao Tong University 
Shanghai, 200030, P. R. China 
{wu-wl,lu-rz,duan-jy,liuhui,gaofeng,chen-yq} 
@cs.sjtu.edu.cn 
 
 
Abstract 
In this paper, we present a weakly super-
vised learning approach for spoken lan-
guage understanding in domain-specific 
dialogue systems. We model the task of 
spoken language understanding as a suc-
cessive classification problem. The first 
classifier (topic classifier) is used to iden-
tify the topic of an input utterance. With 
the restriction of the recognized target 
topic, the second classifier (semantic 
classifier) is trained to extract the corre-
sponding slot-value pairs. It is mainly 
data-driven and requires only minimally 
annotated corpus for training whilst re-
taining the understanding robustness and 
deepness for spoken language. Most im-
portantly, it allows the employment of 
weakly supervised strategies for training 
the two classifiers. We first apply the 
training strategy of combining active 
learning and self-training (Tur et al, 
2005) for topic classifier. Also, we pro-
pose a practical method for bootstrapping 
the topic-dependent semantic classifiers 
from a small amount of labeled sentences. 
Experiments have been conducted in the 
context of Chinese public transportation 
information inquiry domain. The experi-
mental results demonstrate the effective-
ness of our proposed SLU framework 
and show the possibility to reduce human 
labeling efforts significantly. 
1 Introduction 
Spoken Language Understanding (SLU) is one of 
the key components in spoken dialogue systems.  
Its task is to identify the user?s goal and extract 
from the input utterance the information needed 
to complete the query. Traditionally, there are 
mainly two mainstreams in the SLU researches: 
knowledge-based approaches, which are based 
on robust parsing or template matching tech-
niques (Sneff, 1992; Dowding et al, 1993; Ward 
and Issar, 1994); and data-driven approaches, 
which are generally based on stochastic models 
(Pieraccini and Levin, 1993; Miller et al, 1995). 
Both approaches have their drawbacks, however. 
The former approach is cost-expensive to de-
velop since its grammar development is time-
consuming, laboursome and requires linguistic 
skills. It is also strictly domain-dependent and 
hence difficult to be adapted to new domains. On 
the other hand, although addressing such draw-
backs associated with knowledge-based ap-
proaches, the latter approach often suffers the 
data sparseness problem and hence needs a fully 
annotated corpus in order to reliably estimate an 
accurate model. More recently, some new varia-
tion methods are proposed through certain trade-
offs, such as the semi-automatically grammar 
learning approach (Wang and Acero, 2001) and 
Hidden Vector State (HVS) model (He and 
Young, 2005). The two methods require only 
minimally annotated data (only the semantic 
frames are annotated). 
This paper proposes a novel weakly super-
vised spoken language understanding approach. 
Our SLU framework mainly includes two suc-
cessive classifiers: topic classifier and semantic 
classifier. The main advantage of the proposed 
approach is that it is mainly data-driven and re-
quires only minimally annotated corpus for train-
ing whilst retaining the understanding robustness 
and deepness for spoken language. In particular, 
the two classifiers are trained using weakly su-
pervised strategies: the former one is trained 
through the combination of active learning and 
self-training (Tur et al, 2005), and the latter one 
199
is trained using a practical bootstrapping tech-
nique. 
2 The System Architecture 
The semantic representation of an application 
domain is usually defined in terms of the 
semantic frame, which contains a frame type 
representing the topic of the input sentence, and 
some slots representing the constraints the query 
goal has to satisfy. Then, the goal of the SLU 
system is to translate an input utterance into a 
semantic frame. Besides the two key components, 
i.e., topic classifier and semantic classifier, our 
system also contains a preprocessor and a slot-
value merger. Figure 1 illustrates the overall 
system architecture. It also describes the whole 
SLU procedure using an example sentence. 
 
Preprocessor
Please tell me how can I
go from the people's
square to the bund by bus
Topic
classification
Semantic
classification
Slot-value merger
Please tell me how can
I go from [location]1 to
[location]2 by [bus]
Please tell me how can
I go from [location]1 to
[location]2 by [bus]
FRAME:  ShowRoute
FRAME:  ShowRoute
[location]1:  ShowRoute.[route].[origin]
[location]2:  ShowRoute.[route].[destination]
[bus]: ShowRoute.[route].[transport_type]
FRAME: ShowRoute
SLOTS: [route].[origin] = the people's square
[route].[destination] = the bund
              [route].[transport_type] = bus
Inconsistent
slot-values
  
Figure 1: The System architecture1 
2.1 The Preprocessor 
Usually, the preprocessor is to look for the sub-
strings in a sentence that correspond to a seman-
tic class or matching a regular expression and to 
replace them with the class label, e.g., ?Huashan 
Road? and ?1954? are replaced with two class 
labels [road_name] and [number] respectively. In 
our system, the preprocessor can recognize more 
complex word sequences, e.g., ?1954 Huashan 
Road? can be recognized as [address] through 
matching a rule like ?[address] ? [number] 
[road_name]?. The preprocessor is implemented 
with a local chart parser, which is a variation of 
the robust parser introduced in (Wang, 1999). 
The robust local parser can skip noise words in 
the sentence, which ensures that the system has 
the low level robustness. For example, ?1954 of 
the Huashan Road)? can also be recognized as 
                                                 
1 Because the length is limited, in this paper we only illus-
trate all the example sentences in English, which are Chi-
nese sentences, in fact. 
[address] by skipping the words ?of the?. How-
ever, the robust local parser possibly skips the 
words in the sentence by mistake and produces 
an incorrect class label. To avoid this side-effect, 
this local parser exploits an embedded decision 
tree for pruning, of which the details can be seen 
in (Wu et al, 2005). According to our experience, 
it is fairly easy for a general developer with good 
understanding of the application to author the 
small grammar used by the local chart parser and 
annotate the training cases for the embedded de-
cision tree. The work can be finished in several 
hours. 
2.2 Topic Classification 
Given the representation of semantic frame, topic 
classification can be regarded as identifying the 
frame type. It is suited to be dealt using pattern 
recognition techniques. The application of statis-
tical pattern techniques to topic classification can 
improve the robustness of the whole understand-
ing system. Also, in our system, topic classifica-
tion can greatly reduce the search space and 
hence improve the performance of subsequent 
semantic classification. For example, the total 
number of slots into which the concept [location] 
can be filled in all topics is 33 and the corre-
sponding maximum number of slots in a single 
topic is decreased to 10. 
Many statistical pattern recognition techniques 
have been applied to similar tasks, such as Na?ve 
Bayes, N-Gram and Support Vector Machines 
(SVMs) (Wang et al, 2002). According to the 
literature (Wang et al, 2002) and our experi-
ments, the SVMs showed the best performance 
among many other statistical classifiers. Also, it 
has been showed that active learning can be ef-
fectively applied to the SVMs (Schohn and Cohn, 
2000; Tong and Koller, 2000). Therefore, we 
choose the SVMs as the topic classifier. We re-
sorted to the LIBSVM toolkit (Chang and Lin, 
2001) to construct the SVMs for our experiments. 
Following the practice in (Wang et al, 2002), the 
SVMs use a binary valued features vector. If the 
simplest feature (Chinese character) is used, each 
query is converted into a feature vector 
1 | |, , chch ch ch=< >JJK
JJK ?  ( | |ch
JJK  is the total number of 
Chinese characters occur in the corpus) with bi-
nary valued elements: 1 if a given Chinese char-
acter is in this input sentence or 0 otherwise. Due 
to the existence of the preprocessor, we can also 
include semantic class labels (e.g., [location]) as 
features for topic classification. Intuitively, the 
class label features are more informative than the 
200
Chinese character features. At the same time, 
including class labels as features can also relieve 
the data sparseness problem. 
2.3 Topic-dependent Semantic Classifica-
tion 
The job of semantic classification is to assign the 
concepts with the most likely slots. It can also be 
modeled as a classification problem since the 
number of possible slot names for each concept 
is limited. Let?s consider the example sentence in 
Figure 1. After the preprocessing and topic clas-
sification, we get the preprocessed result ?Please 
tell me how can I go from [location]1 to [loca-
tion]2 by [bus]?? and the topic ShowRoute. We 
have to work out which slots are to be filled with 
the values such as [location]2. The first clue is 
the surrounding literal context. Intuitively, we 
can infer that it is a [destination] since a [destina-
tion] indicator ?to? is before it. If [location]1 has 
already been recognized as a [origin], it is an-
other clue to imply that  [location]2  is a [destina 
tion]. Since initially the slot context is not avail-
able, the slot context is only employed for the 
semantic re-classification, which will be de-
scribed in latter section. 
To learn the topic-dependent semantic classi-
fiers, the training sentences need to be annotated 
against the semantic frame. Our annotating sce-
nario is relatively simple and can be performed 
by general developers. For example, for the sen-
tence ?Please tell me how can I go from the peo-
ple?s square to the bund by bus??, the annotated 
results are like the following: 
 
 
 
 
 
 
The corresponding slot names can be automati-
cally extracted from the domain model. A do-
main model is usually a hierarchical structure of 
the relevant concepts in the application domain. 
For every occurrence of a concept in the domain 
model graph, we list all the concept names along 
the path from the root to its occurrence position 
and regard their concatenation as a slot name. 
Thus, the slot name is not flat since it inherits the 
hierarchy from the domain model. 
With provision of the annotated data, we can 
collect all the literal and slot context features re-
lated to each concept. The examples of features 
for the concept [location] are illustrated as fol-
lows:  
(1) to within the ?3 windows 
(2) from _ to  
(3) ShowRoute.[route].[origin] within the 2?  
windows 
The former two are literal context features. Fea-
ture (1) is a context-word that tends to indicate 
ShowRoute.[route].[destination]. Feature (2) is a 
collocation that checks for the pattern ?from? 
and ?to? immediately before and after the con-
cept [location] respectively, and tends to indicate 
ShowRoute.[route].[origin]. The third one is a 
slot context feature, which tends to imply the 
target concept [location] is of type Show-
Route.[route].[destination]. In nature, these fea-
tures are equivalent to the rules in the semantic 
grammar used by the robust rule-based parser. 
For example, the feature (2) has the same func-
tion as the semantic rule ?[origin] ? from [loca-
tion] to?. The advantage of our approach is that 
we can automatically learn the semantic ?rules? 
from the training data rather than manually au-
thoring them. Also, the learned ?rules? are intrin-
sically robust since they may involves gaps, for 
example, feature (1) allows skipping some noise 
words between ?to? and [location]. 
The next problem is how to apply these fea-
tures when predicting a new case since the active 
features for a new case may make opposite pre-
dictions. One simple and effective strategy is 
employed by the decision list (Rivest, 1987), i.e., 
always applying the strongest features. In a deci-
sion list, all the features are sorted in order of 
descending confidence. When a new target con-
cept is classified, the classifier runs down the list 
and compares the features against the contexts of 
the target concept. The first matched feature is 
applied to make a predication. Obviously, how to 
measure the confidence of features is a very im-
portant issue for the decision list. We use the 
metric described in (Yarowsky, 1994; Golding, 
1995). Provided that 1( | ) 0P s f >  for all i : 
( ) max ( | )iiconfidence f P s f=                 (1) 
This value measures the extent to which the con-
text is unambiguously correlated with one par-
ticular slot is . 
2.4 Slot-value merging and semantic re-
classification 
The slot-value merger is to combine the slots 
assigned to the concepts in an input sentence. 
Another simultaneous task of the slot-value 
merger is to check the consistency among the 
identified slot-values. Since the topic-dependent 
classifiers corresponding to different concepts 
FRAME: ShowRoute 
Slots:   [route].[origin].[location].( the people?s square)
[route].[destination].[location].(the bund) 
[route].[transport_type].[by_bus].(bus) 
201
are training and running independently, it possi-
bly results in inconsistent predictions.  Consider-
ing the preprocessed word sequence ?Please tell 
me how can I go from [location]1 to [location]2 
by [bus]? , they are semantically clashed if [loca-
tion]1 and [location]2 are both classified as 
ShowRoute.[route].[origin]. To relieve this prob-
lem, we can use the semantic classifier based on 
the slot context feature. We apply the context 
features like, for example, ?Show-
Route.[route].[origin] within the k?  windows?, 
which tends to imply Show-
Route.[route].[destination]. The literal contexts 
reflect the local lexical semantic dependency. 
The slot contexts, however, are good at capturing 
the long distance dependency. Therefore, when 
the slot-value merger finds that two or more slot-
value pairs clash, it first anchors the one with the 
highest confidence. Then, it extracts the slot con-
texts for the other concepts and passes them to 
the semantic classification module for re-
classification. If the re- classification results still 
clash, the dialog system can involve the user in 
an interactive dialog for clarity. 
The idea of semantic classification and re-
classification can be understood as follows: it 
first finds the concept or slot islands (like partial 
parsing) and then links them together. This 
mechanism is well-suited for SLU since the spo-
ken utterance usually consists of several phrases 
and noises (restart, repeats and filled pauses, etc) 
are most often between them (Ward and Issar, 
1994). Especially, this phenomena and the out-
of-order structures are very frequent in the spo-
ken Chinese utterances. 
3 Weakly Supervised Training of the 
Topic Classifier and Topic-dependent 
Semantic Classifiers 
As stated before, to train the classifiers for topic 
identification and slot-filling, we need to label 
each sentence in the training set against the se-
mantic frame. Although this annotating scenario 
is relatively minimal, the labeling process is still 
time-consuming and costly. Meanwhile unla-
beled sentences are relatively easy to collect. 
Therefore, to reduce the cost of labeling training 
utterances, we employ weakly supervised tech-
niques for training the topic and semantic classi-
fiers. 
The weakly supervised training of the two 
classifiers is successive. Assume that a small 
amount of seed sentences are manually labeled 
against the semantic frame. We first exploit the 
labeled frame types (e.g. ShowRoute) of the 
seed sentences to train a topic classifier through 
the combination of active learning and self-
training. The resulting topic classifier is used to 
label the remaining training sentences with the 
corresponding topic, which are not selected by 
active learning. Then, we use all the sentences 
annotated against the semantic frame (including 
the seed sentences and sentences labeled by ac-
tive learning) and the remaining training 
sentences labeled the topic to train the semantic 
classifiers using a practical bootstrapping tech-
nique. 
3.1 Combining Active Learning and Self-
training for Topic Classification 
We employ the strategy of combining active 
learning and self-training for training the topic 
classifier, which was firstly proposed in (Tur et 
al., 2005) and applied to a similar task.  
One way to reduce the number of labeling ex-
amples is active learning, which have been ap-
plied in many domains (McCallum and Nigam, 
1998; Tang et al, 2002; Tur et al, 2005). Usu-
ally, the classifier is trained by randomly sam-
pling the training examples. However, in active 
learning, the classifier is trained by selectively 
sampling the training examples (Cohn et al, 
1994). The basic idea is that the most informa-
tive ones are selected from the unlabeled exam-
ples for a human to label. That is to say, this 
strategy tries to always select the examples, 
which will have the largest improvement on per-
formance, and hence minimizes the human label-
ing effort whilst keeping performance (Tur et al, 
2005). According to the strategy of determining 
the informative level of an example, the active 
learning approaches can be divided into two 
categories: uncertainty-based and committee-
based. Here, we employ the uncertainty-based 
strategy for selective sampling. It is assumed that 
a small amount of labeled examples is initially 
available, which is used to train a basic classifier. 
Then the classifier is applied to the unannotated 
examples. Typically the most unconfident exam-
ples are selected for a human to label and then 
added to the training set. The classifier is re-
trained and the procedure is repeated until the 
system performance converges. 
Another alternative for reducing human label-
ing effort is self-training. In self-training, an ini-
tial classifier is built using a small amount of 
annotated examples. The classifier is then used to 
label the unannotated training examples. The 
examples with classification confidence scores 
202
over a certain threshold, together with their pre-
dicted labels, are added to the training set to re-
train the classifier. This procedure repeated until 
the system performance converges. 
These two strategies are complementary and 
hence can be combined. The combination strat-
egy is quite straightforward for pool-based train-
ing. At each iteration, the current classifier is 
applied to the examples in the current pool. The 
most unconfident examples in the pool are se-
lected by active learning and labeled by a human. 
The remaining examples in the pool are auto-
matically labeled by the current classifier. Then, 
these two parts of labeled examples are both 
added into the training set and used for retraining 
the classifier. Since the LIBSVM toolkit pro-
vides the class probability, we directly use the 
class probability as the confidence score. Our 
dynamic pool-based (the pool size is n ) algo-
rithm of combining active learning and self-
training for training the topic classifier is as fol-
lows: 
1. Given a small amount of human-labeled 
training set 
tS  ( n  sentences) and a larger 
amount of unlabeled set uS , build the initial 
classifier using tS . 
2. While labelers/ sentences are available 
(a) Get n  unlabeled sentences from uS  
(b) Apply the current classifier to n  unla-
beled sentences 
(c) Select m  examples which are most in-
formative to the current classifier and 
manually label the selected m  exam-
ples 
(d) Add the m  human-labeled examples 
and the remaining n m?  machine-
labeled examples to the training set 
tS  
(e) Train a new classifier on all labeled ex-
amples 
3.2 Bootstrapping the Topic-dependent 
Semantic Classifiers 
Bootstrapping refers to a problem of inducing a 
classifier given a small set of labeled data and a 
large set of unlabeled data (Abney, 2002). It has 
been applied to problems such as word-sense 
disambiguation (Yarowsky, 1995), web-page 
classification (Blum and Mitchell, 1998), named-
entity recognition (Collins and Singer, 1999) and 
automatic construction of semantic lexicon 
(Thelen and Riloff, 2003). The key to the boot-
strapping methods is to exploit the redundancy in 
the unlabeled data (Collins and Singer, 1999). 
Thus, many language processing problems can 
be dealt using the bootstrapping methods since 
language is highly redundant (Yarowsky, 1995). 
The semantic classification problem here also 
exhibits the redundancy. In the example ?Please 
tell me how can I go from [location]1 to [loca-
tion]2 by [bus]??, there are multiple literal con-
text features which all indicate that [location]1 is 
of type ShowRoute.[route].[origin], such as:  
(1) from within the ?1 windows; 
(2) from _ to ; 
(3) to within the +1 windows. 
If the [location]2 has already be recognized as 
ShowRoute.[route].[destination], thus the slot 
context feature ?ShowRoute.[route].[origin] 
within the 2?  windows? is also a strong evi-
dence that [location]1 is of type Show-
Route.[route].[origin]. That is to say, the literal 
context and slot context features above effec-
tively overdetermine the slot of a concept in the 
input sentence. Especially, the literal and slot 
context features can be seen as two natural 
?views? of an example from the respective of 
?Co-Training? (Blum and Mitchell, 1998). Our 
bootstrapping algorithm exploits the property of 
redundancy to incrementally identify the features 
for assigning slots of a concept, given a few an-
notated seed sentences. 
The bootstrapping algorithm is performed on 
each topic iT  ( 1 i n? ? , n  is the number of 
topic) as follows:  
1. For each concept jC  in iT  (1 j m? ? , m is 
the number of concepts appears in the sen-
tences of topic iT ): 
(1.1) Build the two initial classifiers based on 
literal and slot context features respec-
tively using a small amount of labeled 
seed sentences. 
(1.2) Apply the current classifier based on the 
literal context feature to the remaining 
unlabeled concepts in the training sen-
tences belong to topic iT . Keep those 
classified slots with confidence score 
above a certain threshold (In this paper, 
the threshold is fixed on 0.5). 
2. Check the consistency of the classified slots 
in each sentence. If some slots in a sentence 
clashed, take the one with the highest confi-
dence score among them and leave the others 
unlabeled.  
3. For each concept jC in iT , apply the current 
classifier based on the slot context to the re-
sidual unlabeled concepts. Keep those classi-
203
fied slots with confidence score above a cer-
tain threshold. Repeat Step 3. 
4. Augment the new classified cases into the 
training set and retrain the two classifiers 
based on literal and slot context features re-
spectively.  
5. If new slots are classified from the training 
data, return to step 2. Otherwise, repeat 2-5 
to label training data and keep all new classi-
fied slots regardless of the confidence score. 
Train the two final semantic classifiers based 
on the literal and context features respec-
tively using the new labeled training data. 
4 Experiments and Results 
4.1 Data Collection and Experimental Set-
ting 
Our experiments were carried out in the context 
of Chinese public transportation information in-
quiry domain. We collected two kinds of corpus 
for our domain using different ways. Firstly, a 
natural language corpus was collected through a 
specific website which simulated a dialog system. 
The user can conduct some mixed-initiative con-
versational dialogues with it by typing Chinese 
queries. Then we collected 2,286 natural lan-
guage utterances through this way. It was divided 
into two parts: the training set contained 1,800 
sentences (TR), and the test set contained 486 
sentences (TS1). Also, a spoken language corpus 
was collected through the deployment of a pre-
liminary version of telephone-based dialog sys-
tem, of which the speech recognizer is based on 
the speaker-independent Chinese dictation sys-
tem of IBM ViaVoice Telephony and the SLU 
component is a robust rule-based parser. The 
spoken utterances corpus contained 363 spoken 
utterances. Then we obtained two test set from 
this corpus: one consisted of the recognized text 
(TS2); the other consisted of the corresponding 
transcription (TS3). The Chinese character error 
rate and concept error rate of TS2 are 35.6% and 
41.1% respectively. We defined ten types of 
topic for our domain: ListStop, ShowFare, 
ShowRoute, ShowRouteTime, etc. The first 
corpus covers all the ten topic types and the sec-
ond corpus only covers four topic types. The to-
tal number of Chinese characters appear in the 
data set is 923. All the sentences were annotated 
against the semantic frame. In our experiments, 
the topic classifier and semantic classifiers were 
trained on the natural language training set (TR) 
and tested on three test sets (TS1, TS2 and TS3). 
The performance of topic classification and 
semantic classification are measured in terms of 
topic error rate and slot error rate respectively. 
Topic performance is measured by comparing 
the topic of a sentence predicated by the topic 
classifier with the reference topic. The slot error 
rate is measured by counting the insertion, dele-
tion and substitution errors between the slots 
generated by our system and these in the refer-
ence annotation. 
4.2 Supervised Training Experiments 
Firstly, in order to validate the effectiveness of 
our proposed SLU system using successive 
learners, we compared our system with a rule-
based robust semantic parser. The parsing algo-
rithm of this parser is same as the local chart 
parser used by the preprocessor. The handcrafted 
grammar for this semantic parser took a linguis-
tic expert one month to develop, which consists 
of 798 rules (except the lexical rules for named 
entities such as [loc_name]). In our SLU system, 
we first use the SVMs to identify the topic and 
then apply the semantic classifier (decision list) 
related to the identified topic to assign the slots 
to the concepts. The SVMs used the augmented 
binary features (923 Chinese characters and 20 
semantic class labels). A general developer inde-
pendently annotated the TR set against the se-
mantic frame, which took only four days. 
Through feature extraction from the TR set and 
feature pruning, we obtained 2,259 literal context 
features and 369 slot context features for 20 
kinds of concepts in our domain. Table 1 Shows 
that our SLU method has better performance 
than the rule-based robust parser in both topic 
classification and slot identification. Due to the 
high concept error rate of recognized utterances, 
the performance of semantic classification on the 
TS2 is relatively poor. However, if considering 
only the correctly identified concepts on TS2, the 
slot error rate is 9.2%. Note that, since the TS2 
(recognized speech) covers only four types of 
topic but TS1 (typed utterance) covers ten topics, 
the topic error on the TS2 (recognized speech) is 
lower than that on TS1. 
Table 1 also compares our system with the 
two-stage classification with the reversed order. 
Another alternative for our system is to reverse 
the two main processing stages, i.e., finding the 
roles for the concepts prior to identifying the 
topic. For instance, in the example sentence in 
Fig.1, the concept (e.g., [location]) in the pre-
processed sequence is first recognized as slots 
(e.g., [route].[origin]) before topic classification. 
204
Therefore, the slots like [route].[origin] can be 
included as features for topic classification, 
which is deeper than the concepts like [location] 
and potential to achieve improvement on per-
formance of topic classification. This strategy 
was adopted in some previous works (He and 
Young, 2003; Wutiwiwatchai and Furui, 2003). 
However, the results indicate that, at least in our 
two-stage classification formwork, the strategy 
of identifying the topic before assigning the slots 
to the concepts is more optimal. According to 
our error analysis, the unsatisfied performance of 
the reversed two-stage classification system can 
be explained as follows:  (1) Since the semantic 
classification is performed on all topics, the 
search space is much bigger and the ambiguities 
increase. This deteriorates the performance of 
semantic classification. (2) In the case that the 
slots and Chinese characters are included as fea-
tures, the topic classifier relies heavily on the slot 
features. Then, the errors of semantic classifica-
tion have serious negative effect on the topic 
classification. 
 
Table 1: Performance comparsion of the rule-
based robust semantic parser, the reversed two-
stage classification system and our SLU systems 
(TER: Topic Error Rate; SER: Slot Error Rate; 
DL: Decision List) 
TS1 TS2 TS3 
 TER 
(%)   
SER 
( %) 
TER 
(%)   
SER  
( %) 
TER
(%) 
SER  
( %)
Rule-based se-
mantic parser 6.8  11.6 4.1  47.9 3.0 5.4
Reversed two-
stage classifica-
tion system 
4.9 11.1 3.6 47.4 2.5 4.9
Our system 2.9   8.4 2.2   45.6 1.4  4.6
 
4.3 Weakly Supervised Training 
Experiments 
4.3.1 Active Learning and Self-training Ex-
periments for Topic Classification 
In order to evaluate the performance of active 
learning and self-training, we compared three 
sampling strategies: random sampling, active 
learning only, active learning and self-training. 
At each iteration of pool-based active learning 
and self-training, we get 200 sentences (i.e., the 
pool size is set as 200) and select 50 most uncon-
fident sentences from them for manually labeling 
and exploit the remaining sentences using self-
training. All the experiments were repeated ten 
times with different randomly selected seed sen-
tences and the results were averaged. Figure 1 
plots the learning curves of three strategies 
trained on TR and tested on the TS1 set. It is evi-
dent that active learning significantly reduces the 
need for labeled data. For instance, it requires 
1600 examples if they are randomly chosen to 
achieve a topic error rate of 3.2% on TS1, but 
only 600 actively selected examples, a saving of 
62.5%. The strategy of combing active learning 
and self-training can further improve the per-
formance of topic classification compared with 
active learning only with the same amount of 
labeled data. 
2.00%
3.00%
4.00%
5.00%
6.00%
7.00%
8.00%
9.00%
10.00%
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of labeled sentences
To
pi
c 
er
ro
r 
ra
te
Random
Active Learning
Active Learning &
Self-traing
Figure 2: Learning curves using different sam-
pling strategies. 
 
We also evaluated the performance of topic 
classification using active learning and self-
training with the pool size of 200 on the three 
test sets. Table 2 shows that active learning and 
self-training with the pool size of 200 achieves 
almost the same performance on three test sets as 
random sampling, but requires only 33.3% data. 
 
Table 2: The topic error rate using active learn-
ing and self-training with pool size of 200 on the 
three test sets (AL: Active Learning) 
 TS1 (%) 
TS2 
(%) 
TS3 
(%) 
Labeled 
Sent.(#) 
Random 2.9 2.2 1.4 1,800 
AL 3.2 2.5 1.7 600 
AL & self-training 2.9 2.5 1.4 600 
 
4.3.2 Bootstrapping Experiments for Se-
mantic Classification 
As stated before, the bootstrapping procedure 
begins with a small amount of sentences anno-
tated against the semantic frame, which is the 
initial seed sentence or annotated by active learn-
ing, and the remaining training sentences, the 
topics of which are machine-labeled by the re-
sulting topic classifier. For example, in the 
205
weakly supervised training scenario with the 
pool size of 200, the active learning and self-
training procedure ran 8 iterations. At each itera-
tion, 50 sentences were selected by active learn-
ing. So the total number of labeled sentences is 
600. We compared our bootstrapping methods 
with supervised training for semantic classifica-
tion. We tried two bootstrapping methods: using 
only the literal context features (Bootstrapping 1) 
and using the literal and slot context features 
(Bootstrapping 2). If the step 4 of the bootstrap-
ping algorithm in Section 3.2 is canceled, the 
new bootstrapping variation corresponds to 
Bootstrapping 2. Also, we repeated the experi-
ments ten times with different labeled sentences 
and the results were averaged. Figure 3 plots the 
learning curves of bootstrapping and supervised 
training with different number of labeled sen-
tences on the TS1 set. The results indicate that 
bootstrapping methods can effectively make use 
of the unlabeled data to improve the semantic 
classification performance. In particular, the 
learning curve of bootstrapping 1 achieves more 
significant improvement than the curve of boot-
strapping 2. It can be explained as follows: in-
cluding the slot context features further increases 
the redundancy of data and hence corrects the 
initial misclassified cases by the semantic classi-
fier using only literal context features or provides 
new cases. 
6.00%
8.00%
10.00%
12.00%
14.00%
16.00%
18.00%
20.00%
0 100 200 300 400 500 600 700
Number of labeled sentences
Sl
ot
 e
rr
or
 r
at
e
Supervised
training
Bootstrapping 1
Bootstrapping 2
Figure 3: Learning curves of bootstrapping meth-
ods for semantic classification on TS1. 
 
Finally, we compared two SLU systems 
through weakly supervised and supervised 
training respectively. The supervised one was 
trained using all the annotated sentences in TR 
(1800 sentences). In the weakly supervised 
training scenario (the pool size is still 200), The 
topic classifier and semantic classifiers were both 
trained using only 600 labeled sentences. Table 3 
shows that the weakly supervised scenario 
achieves comparable performance to the super-
vised one, but requires only 33.3% labeled data. 
 
Table 3: Performance comparison of two SLU 
systems through weakly supervised and super-
vised training on the three test sets (TER: Topic 
Error Rate; SER: Slot Error Rate) 
TS1 TS2 TS3 
 TER 
(%)  
SER 
(%)
TER  
(%)   
SER 
(%) 
TER 
(%)  
SER
(%)
Supervised 2.9  8.4 2.2   45.6 1.4  4.6
Weakly  
Supervised 2.9 9.7 2.5 44.8 1.4 5.7
 
5 Conclusion and Future work 
We have presented a new SLU framework using 
two successive classifiers. The proposed frame-
work exhibits the advantages as follows. 
z It has good robustness on processing spoken 
language: (1) The preprocessor provides the 
low level robustness. (2) It inherits the ro-
bustness of topic classification using statis-
tical pattern recognition techniques. It can 
also make use of topic classification to 
guide slot filling. (3) The strategy of first 
finding the concepts or slot islands and then 
linking them is suited for processing spoken 
language. 
z It also keeps the understanding deepness: (1) 
The class of semantic classification is the 
slot name, which inherits the hierarchy from 
the domain model. (2) The semantic re-
classification mechanism ensures the consis-
tency among the identified slot-value pairs. 
z It is mainly data-driven and requires only 
minimally annotated corpus for training. 
Most importantly, our proposed SLU 
framework allows the employment of 
weakly supervised strategies for training the 
two classifiers, which can reduce the cost of 
annotating labeled sentences. 
The future work includes further evaluation of 
our approach in other application domains and 
languages. We also plan to integrate this under-
standing system into a whole dialog system. 
Then, high level knowledges, such as the dialog 
context, can also be included as the features of 
topic and semantic classifiers. Moreover, cur-
rently, the topics are manually defined through 
examination of the example sentences by human. 
Then, it is worthwhile to investigate how to ap-
propriately define topics and the probability of 
206
exploiting the sentence clustering techniques to 
facilitate the topic (frame) designment. 
6 Acknowledgements 
The authors would like to thank three anony-
mous reviewers for their careful reading and 
helpful suggestions. This work is supported by 
National Natural Science Foundation of China 
(NSFC, No. 60496326) and 863 project of China 
(No. 2001AA114210-11). 
References 
S. Abney. 2002. Bootstrapping. In Proc. of ACL, pp. 
360-367, Philadelphia, PA. 
A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In Proc. of 
COLT, Madison, WI. 
C. Chang and C. Lin. 2001. LIBSVM: a library for 
support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
D. Cohn, L. Atlas and R. Ladner. 1994. Improving 
generalization with active learning. Machine 
Learning 15, pp.201-221. 
M. Collins and Y. Singer.1999. Unsupervised models 
for named entity classification. In Proc. of EMNLP. 
J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. 
Cherny, R. Moore, and D. Moran. 1993. GEMINI: 
A natural language system for spoken-language 
understanding. In Proc. of  ACL, Columbus, Ohio, 
pp. 54-61.  
R. Golding. 1995. A Bayesian Hybrid Method for 
Context-sensitive Spelling Correction. In Proc. 3rd 
Workshop on Very Large Corpora, Boston, MA. 
Y. He and S.J. Young. 2003. A Data-Driven Spoken 
Language Understanding System. IEEE Workshop 
on Automatic Speech Recognition and Understand-
ing, US Virgin Islands. 
Y. He and S. Young. 2005. Semantic Processing us-
ing the Hidden Vector State Model. Computer 
Speech and Language 19(1): 85-106. 
A. McCallum and K. Nigam.1998. Employing EM 
and pool-based active learning for text classifica-
tion. In Proc. of  ICML. 
S. Miller, R. Bobrow, R. Ingria, and R. Schwartz. 
1994. Hidden Understanding Models of Natural 
Language. In Proc. of ACL, pp. 25-32. 
R. Pieraccini and E. Levin. 1993. A learning ap-
proach to natural language understanding. NATO-
ASI, New Advances & Trends in Speech Recogni-
tion and Coding, Springer-Verlag, Bubion, Spain. 
R. L. Rivest. 1987. Learning decision lists. Machine 
Learning, 2(3):229--246, 1987. 
S. Seneff. 1992. TINA: A natural language system for 
spoken language applications. Computational Lin-
guistics, vol. 18, no. 1., pp. 61-86. 
G. Schohn and D. Cohn. 2000. Less Is More: Active 
Learning with Support Vector Machines. In Proc. 
of ICML, pp. 839-846. 
M. Tang, X. Luo, S. Roukos.2002. Active learning for 
statistical natural language parsing. In Proc. of 
ACL, Philadelphia, Pennsylvania. 
M. Thelen and E. Riloff. 2002. A Bootstrapping 
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts. In Proc. of EMNLP?02. 
S. Tong and D. Koller. 2000. Support Vector Machine 
Active Learning with Applications to Text Classifi-
cation. In Proc. of ICML, pp. 999-1006. 
G. Tur, D. Hakkani-T?r, Robert E. Schapire. Combin-
ing Active and Semi-Supervised Learning for Spo-
ken Language Understanding. Speech Communi-
cation, Vol. 45, No. 2, pp. 171-186, 2005. 
Y. Wang. 1999. A Robust Parser for Spoken Lan-
guage Understanding. In Proc. of EUROSPEECH. 
Budapest, Hungary. 
Y. Wang and A Acero. 2001.Grammar learning for 
spoken language understanding. In Proc. of ASRU 
Workshop, Madonna di Campiglio, Italy. 
Y. Wang, A. Acero, C. Chelba, B. Frey and L. Wong. 
2002. Combination of Statistical and Rule-based 
Approaches for Spoken Language Understanding, 
In ICSLP. Denver, Colorado. 
W. Ward and S. Issar. 1994. Recent Improvements in 
the CMU Spoken Language Understanding System. 
In Proc. of ARPA Workshop on HLT, March, 1994. 
W Wu, J Duan, R Lu, F Gao. 2005. Embedded ma-
chine learning systems for Robust Spoken Lan-
guage Parsing. In Proc. of IEEE NLP-KE, Wuhan, 
China. 
C. Wutiwiwatchai and S. Furui. 2003. Combination of 
Finite State Automata and Neural Network for 
Spoken Language Understanding. In Proc. of EU-
ROSPEECH2003, Geneva, Switzerland. 
D. Yarowsky. 1994. Decision Lists for Lexical Ambi-
guity Resolution: Application to Accent Restora-
tion in Spanish and French. In Proc. of ACL 1994, 
pp. 88-95. 
207
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 73?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic Labeling of Compound Nominalization in Chinese
Jinglei Zhao, Hui Liu & Ruzhan Lu
Department of Computer Science
Shanghai Jiao Tong University
800 Dongchuan Road Shanghai, China
{zjl,lh charles,rzlu}@sjtu.edu.cn
Abstract
This paper discusses the semantic interpre-
tation of compound nominalizations in Chi-
nese. We propose four coarse-grained se-
mantic roles of the noun modifier and use a
Maximum Entropy Model to label such re-
lations in a compound nominalization. The
feature functions used for the model are
web-based statistics acquired via role related
paraphrase patterns, which are formed by a
set of word instances of prepositions, sup-
port verbs, feature nouns and aspect mark-
ers. By applying a sub-linear transformation
and discretization of the raw statistics, a rate
of approximately 77% is obtained for classi-
fication of the four semantic relations.
1 Introduction
A nominal compound (NC) is the concatenation of
any two or more nominal concepts which functions
as a third nominal concept (Finin, 1980). (Leonard,
1984) observed that the amount of NCs had been in-
creasing explosively in English in recent years. NCs
such as satellite navigation system are abundant in
news and technical texts. In other languages such as
Chinese, NCs have been more productive since ear-
lier days as evidenced by the fact that many simple
words in Chinese are actually a result of compound-
ing of morphemes.
Many aspects in Natural Language Processing
(NLP), such as machine translation, information re-
trieval, question answering, etc. call for the auto-
matic interpretation of NCs, that is, making explicit
the underlying semantic relationships between the
constituent concepts. For example, the semantic re-
lations involved in satellite communication system
can be expressed by the conceptual graph (Sowa,
1984) in Figure 1, in which, for instance, the se-
mantic relation between satellite and communica-
tion is MANNER. Due to the productivity of NCs
and the lack of syntactic clues to guide the interpre-
tation process, the automatic interpretation of NCs
has been proven to be a very difficult problem in
NLP.
In this paper, we deal with the semantic interpre-
tation of NCs in Chinese. Especially, we will fo-
cus on a subset of NCs in which the head word is a
verb nominalization. Nominalization is a common
phenomenon across languages in which a predica-
tive expression is transformed to refer to an event
or a property. For example, the English verb com-
municate has the related nominalized form commu-
nication. Different from English, Chinese has little
morphology. Verb nominalization in Chinese has the
same form as the verb predicate.
Nominalizations retain the argument structure of
the corresponding predicates. The semantic relation
between a noun modifier and a verb nominalization
head can be characterized by the semantic role the
modifier can take respecting to the corresponding
verb predicate. Our method uses a Maximum En-
tropy model to label coarse-grained semantic roles
in Chinese compound nominalizations. Unlike most
approaches in compound interpretation and seman-
tic role labeling, we don?t exploit features from
any parsed texts or lexical knowledge sources. In-
stead, features are acquired using web-based statis-
73
[satellite]m(MANNER)m[communication]m(TELIC)m[system] 
Figure 1: The conceptual graph for satellite communication system
tics (PMI-IR) produced from paraphrase patterns of
the compound Nominalization.
The remainder of the paper is organized as fol-
lows: Section 2 describes related works. Section
3 describes the semantic relations for our labeling
task. Section 4 introduces the paraphrase patterns
used. Section 5 gives a detailed description of our
algorithm. Section 6 presents the experimental re-
sult. Finally, in Section 7, we give the conclusions
and discuss future work.
2 Related Works
2.1 Nominal Compound Interpretation
The methods used in the semantic interpretation of
NCs fall into two main categories: rule-based ones
and statistic-based ones. The rule-based approaches
such as (Finin, 1980; Mcdonald, 1982; Leonard,
1984; Vanderwende, 1995) think that the interpreta-
tion of NCs depends heavily on the constituent con-
cepts and model the semantic interpretation as a slot-
filling process. Various rules are employed by such
approaches to determine, for example, whether the
modifier can fill in one slot of the head.
The statistic-based approaches view the seman-
tic interpretation as a multi-class classification prob-
lem. (Rosario and Hearst, 2001; Moldovan et al,
2004; Kim and Baldwin, 2005) use supervised meth-
ods and explore classification features from a simple
structured type hierarchy. (Kim and Baldwin, 2006)
use a set of seed verbs to characterize the semantic
relation between the constituent nouns and explores
a parsed corpus to classify NCs. (Turney, 2005) uses
latent relational analysis to classify NCs. The simi-
larity between two NCs is characterized by the sim-
ilarity between their related pattern set.
(Lauer, 1995) is the first to use paraphrase based
unsupervised statistical models to classify semantic
relations of NCs. (Lapata, 2000; Grover et al, 2005;
Nicholson, 2005) use paraphrase statistics computed
from parsed texts to interpret compound nominaliza-
tion, but the relations used are purely syntactic. La-
pata(2000) only classifies syntactic relations of sub-
ject and object. Grover(2005) and Nicholson (2005)
classify relations of subject, object and prepositional
object.
2.2 Semantic Role Labeling of Nominalization
Most previous work on semantic role labeling of
nominalizations are conducted in the situation where
a verb nominalization is the head of a general noun
phrase. (Dahl et al, 1987; Hull and Gomez, 1996)
use hand-coded slot-filling rules to determine the se-
mantic roles of the arguments of a nominalization.
In such approaches, first, parsers are used to identify
syntactic clues such as prepositional types. Then,
rules are applied to label semantic roles according
to clues and constraints of different roles.
Supervised machine learning methods become
prevalent in recent years in semantic role labeling
of verb nominalizations as part of the resurgence
of research in shallow semantic analysis. (Pradhan
et al, 2004) use a SVM classifier for the semantic
role labeling of nominalizations in English and Chi-
nese based on the FrameNet database and the Chi-
nese PropBank respectively. (Xue, 2006) uses the
Chinese Nombank to label nominalizations in Chi-
nese. Compared to English, the main difficulty of
using supervised method for Chinese, as noted by
Xue (2006), is that the precision of current parsers
of Chinese is very low due to the lack of morphol-
ogy, difficulty in segmentation and lack of sufficient
training materials in Chinese.
2.3 Web as a large Corpus
Data sparseness is the most notorious hinder for ap-
plying statistical methods in natural language pro-
cessing. However, the World Wide Web can be seen
as a large corpus. (Grefenstette and Nioche, 2000;
Jones and Ghani, 2000) use the web to generate cor-
pora for languages for which electronic resources
are scarce. (Zhu and Rosenfeld, 2001) use Web-
based n-gram counts for language modeling. (Keller
and Lapata, 2003) show that Web page counts and
n-gram frequency counts are highly correlated in a
log scale.
74
3 Semantic Relations
Although verb nominalization is commonly con-
sidered to have arguments as the verb predicate,
Xue(2006) finds that there tend to be fewer argu-
ments and fewer types of adjuncts in verb nomi-
nalizations compared to verb predicates in Chinese.
We argue that this phenomenon is more obvious in
compound nominalization. By analyzing a set of
compound nominalizations of length two from a bal-
anced corpus(Jin et al, 2003), we find the semantic
relations between a noun modifier and a verb nomi-
nalization head can be characterized by four coarse-
grained semantic roles: Proto-Agent (PA), Proto-
Patient (PP), Range (RA) and Manner (MA). This
is illustrated by Table1.
Relations Examples
PA ???? (Blood Circulation)
ja[? (Bird Migration)
PP ??+n (Enterprise Management)
???a (Animal Categorization)
MA -1; (Laser Storage)
?(?& (Satellite Communication)
RA ??? (Global Positioning)
?u (Long-time Development)
Table 1: Semantic Relations between Noun Modifier
and Verb Nominalization Head.
Due to the linking between semantic roles and
syntactic roles (Dowty, 1991), the relations above
overlap with syntactic roles, for example, Proto-
Agent with Subject and Proto-Patient with Object,
but they are not the same, as illustrated by the
example ???a(Animal Categorization). Al-
though the predicate ?a(categorize) in Chinese is
an intransitive verb, the semantic relation between
??(animal) and ?a(categorization) is Proto-
Patient.
4 Paraphrase Patterns
4.1 Motivations
Syntactic patterns provide clues for semantic rela-
tions (Hearst, 1992). For example, Hearst(1992)
uses the pattern ?NP such as List? to indicate that
nouns in List are hyponyms of NP. To classify the
four semantic relations listed in section 3, we pro-
pose some domain independent surface paraphrase
patterns to characterize each semantic relation. The
patterns we adopted mainly exploit a set of word in-
stances of prepositions, support verbs, feature nouns
and aspect markers.
Prepositions are strong indicators of semantic
roles in Chinese. For example, in sentence 1), the
preposition r(ba) indicates that the noun ?(door)
and ?n(Zhangsan) is the Proto-Patient and Proto-
Agent of verb?(lock) respectively.
1) a. ?nr???
b. Zhangsan ba door locked.
c. Zhangsan locked the door.
The prepositions we use to characterize each rela-
tion are listed in table 2.
Relations Prepositional Indicators
PP 