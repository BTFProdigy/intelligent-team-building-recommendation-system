NAACL HLT Demonstration Program, pages 1?2,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Demonstration of PLOW: A Dialogue System for One-Shot Task 
Learning
James Allen, Nathanael Chambers, George Ferguson
* 
, Lucian Galescu, Hyuckchul Jung, 
Mary Swift
* 
and William Taysom
Florida Institute for Human and Machine Cognition, Pensacola, FL 32502
*Computer Science Department, University of Rochester, Rochester, NY 14627
Introduction
We describe a system that can learn new 
procedure models effectively from one 
demonstration by the user. Previous work to learn 
tasks through observing a demonstration (e.g., 
Lent & Laird, 2001) has required observing many 
examples of the same task. One-shot learning of 
tasks presents a significant challenge because the 
observed sequence is inherently incomplete ? the 
user only performs the steps required for the 
current situation.  Furthermore, their decision-
making processes, which reflect the control 
structures in the procedure, are not revealed. 
We will demonstrate a system called PLOW 
(Procedural Learning on the Web) that learns task 
knowledge through observation accompanied by a 
natural language ?play-by-play?. Natural 
language (NL) alleviates many task learning 
problems by identifying (i) a useful level of 
abstraction of observed actions; (ii) parameter 
dependencies; (iii) hierarchical structure; (iv) 
semantic relationships between the task and the 
items involved in the actions; and (v) control 
constructs not otherwise observable. Various 
specialized reasoning modules in the system 
communicate and collaborate with each other to 
interpret the user?s intentions, build a task model 
based on the interpretation, and check consistency 
between the learned task and prior knowledge.
The play-by-play approach in NL enables our 
task learning system to build a task with high-
level constructs that are not inferable from 
observed actions alone. In addition to the 
knowledge about task structure, NL also provides 
critical information to transform the observed 
actions into more robust and reliable executable 
forms. Our system learns how to find objects used 
in the task, unifying the linguistic information of 
the objects with the semantic representations of 
the user?s NL descriptions about them.  The 
objects can then be reliably found in dynamic and 
complex environments. See Jung et al(2006) and 
Chambers et al(2006) for more details on the 
PLOW system.
The PLOW System
PLOW learns tasks executable on the web 
involving actions such as navigation, information 
extraction and form filling, and can learn iterative 
steps that operate over lists of objects on pages. 
Figure 1 shows the system during learning a task 
to find publications for a specified author. Upper 
left is the Mozilla browser, in which the user can 
demonstrate action and the system can execute 
actions in a mixed-initiative fashion. The user 
may speak or type to the system (SR output is 
lower right), and PLOW combines knowledge 
from the language and the demonstrated actions to 
produce a parameterized procedure (described in 
generated natural language in the upper right 
corner). Figure 2 shows a complete training 
dialogue in which PLOW learns how to find 
article titles. To save space, simple 
acknowledgments by the system are not shown.
Figure 1: PLOW learning a task
1
Evaluation
The PLOW system was evaluated by independent 
evaluators who considered four task learning 
systems developed in the CALO project. There 
were 16 human subjects who received training on 
each of the systems and who worked through a 
number of successful scripted training sessions 
with each. They were then given ten new 
problems, ranging from slight variations to 
problems they had seen to problems that were 
substantially new. They were free to choose which 
problems to work on and which system to use and 
the resulting tasks learned were tested with 
different settings of the parameters and scored out 
of a total of 4 points based on a complex 
predefined evaluation criteria (not known to the 
developers). The PLOW system did well in the 
test, not only receiving the highest average score 
on tasks learned by a system (figure 3) but also 
was strongly preferred by the users and selected 
more than half the time (figure 4). 
The Demonstration
If we are allowed a presentation we will 
demonstrate PLOW live on a task selected by the 
audience. In addition, we would like to have the 
system available for an extended period of time 
during the conference so that attendees can spend 
time using the system to teach it simple tasks. The 
system runs on a laptop and all that is needed for 
a demo is internet access.
Acknowledgements & References
This work was supported by DARPA grant NBCH-
D-03-0010 under a subcontract from SRI International, 
ONR grant N000140510314, and NSF grant5-28096.
Chambers, N. et al (2006). Using Semantics to Identify 
Web Objects. Proceedings AAAI.
Jung, H., J. Allen, et al (2006). One-Shot Proce-dure 
Learning from Instruction and Observation. FLAIRS, 
Melbourne, FL. 
Lent, M. and Laird, J. (2001)  Learning Procedural 
Knowledge through Observation,  Proc.  of the Intl 
Conf. on Knowledge Capture.
   Figure 3: Average score (out of 4)   Figure 4: User preference for systems (55 trials)
U: Let me show you how to find someone's publications 
U: Go to this webpage 
Navigates to http://scholar.google.com
U: Click on the advanced search link 
Clicks Advanced Scholar Search 
U: Put the author's name here 
Types the name in the Author text field 
U: Click the search button 
Clicks on the Search Scholar button 
U: OK here is the list of results 
Selects the table on page
PLOW: Can you check that I understand the data. 
PLOW: puts up a new window with the results. 
U: OK this is good.
U: this is the article title 
Highlights a title 
PLOW: attempts to find all titles and displays them
U: This is wrong.
PLOW OK
U: This is another article title 
Highlights another correct title 
PLOW: recomputes all the titles and displays them
Switches back to Scholar window
U: OK click next to get more results
Clicks the Next link 
PLOW: When should I stop searching?
U: Get thirty results 
PLOW: OK. I will do that
PLOW: iterates through pages until it gets 30 results
PLOW: I?m done.
U: We're done with the demonstration 
U: Let's practice
PLOW: what is the name of the author?
Enters a new name
PLOW executes the procedure successfully!!
Figure 2: Learning to find titles of an author?s publications (U = user)
2
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 20?24, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ATT1: Temporal Annotation Using Big Windows and Rich Syntactic and
Semantic Features
Hyuckchul Jung and Amanda Stent
AT&T Labs - Research
180 Park Ave
Florham Park, NJ 07932, USA
hjung, stent@research.att.com
Abstract
In this paper we present the results of exper-
iments comparing (a) rich syntactic and se-
mantic feature sets and (b) big context win-
dows, for the TempEval time expression and
event segmentation and classification tasks.
We show that it is possible for models using
only lexical features to approach the perfor-
mance of models using rich syntactic and se-
mantic feature sets.
1 Introduction
TempEval-3 Temporal Annotation Task (UzZaman
et al, 2012) has three subtasks:
A Time expression extraction and classification -
extract time expressions from input text, and de-
termine the type and normalised value for each
extracted time expression.
B Event extraction and classification - extract event
mentions from input text, and determine the class,
tense and aspect features for each extracted event.
C Temporal link identification - identify and cate-
gorise temporal links between events in the same
or consecutive sentences, events and time expres-
sions in the same sentence, and events and the
document creation time of the input text.
Here we report results for the first two tasks.
Previous TempEval competitions have shown that
rich syntactic and semantic feature sets can lead to
good performance on event and time expression ex-
traction and classification tasks (e.g. (Llorens et al,
Type Files EVENT TIMEX
AQUAINT gold 73 4431 579
TimeBank gold 183 6698 1243
TE3-Silver silver 2452 81329 12739
Table 1: Frequency of event and time expressions in the
text portions of the TempEval-3 data sets
2010; UzZaman and Allen, 2010)). In this work, we
show that with large windows of context, it is pos-
sible for models using only lexical features to ap-
proach the performance of models using rich syn-
tactic and semantic feature sets.
2 Data
Using the gold and silver data distributed by the
TempEval-3 task organizers (see Table 1), we pro-
cessed each input file with the Stanford CoreNLP
(Stanford Natural Language Processing Group,
2012) and SENNA (Collobert et al, 2011) open-
source NLP tools. From the Stanford CoreNLP
tools we obtained a tokenization of the input text,
the lemma and part of speech (POS) tag for each
token, and dependency and constituency parses for
each sentence. From SENNA, we obtained a seman-
tic role labelling for each sentence.
3 Approach
We were curious to explore the tradeoff between ad-
ditional context on the one hand, and additional lay-
ers of representation on the other, for the event and
time expression extraction tasks. Researchers have
investigated the impacts of different sets of features
(Adafre and de Rijke, 2005; Angeli et al, 2012;
20
Feature type Features Used in
Lexical 1 token ATT1,
ATT2, ATT3
Lexical 2 lemma ATT1, ATT2
Part of speech POS tag ATT1, ATT2
Dependency governing verb, governing verb POS, governing preposition,
phrase tag, path to root of parse tree, head word, head word lemma,
head word POS
ATT1, ATT2
Constituency
parse
governing verb, governing verb POS, governing preposition,
phrase tag, path to root of parse tree
ATT1, ATT2
Semantic role semantic role label, semantic role labels along path to root of parse
tree
ATT1
Table 2: Features used in our models
Tag type Tags
time expression extraction tags B DATE, B DURATION, B SET, B TIME, I DATE,
I DURATION, I SET, I TIME, O
Event expression extraction tags B ACTION, B ASPECTUAL, B ACTION, B OCCURRENCE,
B PERCEPTION, B REPORTING, B STATE, O
Event tense FUTURE, INFINITIVE, PAST, PASTPART, PRESENT, PRES-
PART, NONE, O
Event aspect PROGRESSIVE, PREFECTIVE PROGRESSIVE, PERFEC-
TIVE, NONE, O
Event polarity NEG, POS
Event modality ?D, CAN, CLOSE, COULD, DELETE, HAVE TO, HAVE TO,
LIKELIHOOD, MAY, MIGHT, MUST, NONE, O, POSSIBLE,
POTENTIAL, SHOULD, SHOULD HAVE TO, TO, UNLIKELY,
UNTIL, WOULD, WOULD HAVE TO
Table 3: Tags assigned by our classifiers for TempEval-3 tasks A and B
Rigo and Lavelli, 2011). In particular, (Rigo and
Lavelli, 2011) also examined performance based on
different sizes of n-grams in a small scale (n=1,3).
In this work, we intended to systematically inves-
tigate the performance of various models with differ-
ent layers of representation (based on much larger
sets of rich syntactic/semantic features) as well as
additional context. For each time expression/event
segmentation/classification task, we trained twelve
models exploring these two dimensions, three of
which we submitted for TempEval-3.
Additional layers of representation We
trained three types of model: (ATT1) STAN-
FORD+SENNA, (ATT2) STANFORD and (ATT3)
WORDS ONLY. The basic features used in each
type of model are given in Table 2: ATT1 models
include lexical, syntactic and semantic features,
ATT2 models include only lexical and syntactic
features, and ATT3 models include only lexical
features. For the ATT1 models we had 18 basic
features per token, for the ATT2 models we had 16
basic features per token, and for the ATT3 models
we had one basic feature per token.
Additional context We experimented with context
windows of 0, 1, 3, and 7 words preceding and fol-
lowing the token to be labeled (i.e. window sizes of
1, 3, 7, and 15). For each window size, we trained
ATT1, ATT2 and ATT3 models. The ATT1 mod-
els had 18 basic features per token in the context
window, for up to 15 tokens, so up to 270 basic fea-
tures for each token to be labeled. The ATT2 mod-
els had 16 basic features per token in the context
21
window, so up to 240 basic features for each token
to be labeled. The ATT3 models had 1 basic feature
per token in the context window, so up to 15 basic
features for each token to be labeled.
Model training For event extraction and classifica-
tion, time expression extraction and classification,
and event feature classification, we used the machine
learning toolkit LLAMA (Haffner, 2006). LLAMA
encodes multiclass classification problems using bi-
nary MaxEnt classifiers to increase the speed of
training and to scale the method to large data sets.
We also used a front-end to LLAMA that builds un-
igram, bigram and trigram extended features from
basic features; for example, from the basic feature
?go there today?, it would build the features ?go?,
?there?, ?today?, ?go there?, ?there today?, and ?go
there today?. We grouped our basic features (see Ta-
ble 2) by type rather than by token, and the LLAMA
front-end then produced ngram features. We chose
LLAMA primarily because of the proven power
of the ngram feature-extraction front-end for NLP
tasks.
4 Event and Time Expression Extraction
For event and time expression extraction, we trained
BIO classifiers. A BIO classifier tags each input to-
ken as either Beginning, In, or Out of an event/time
expression. Our classifier for events simultaneously
assigns a B, I or O to each token, and classifies the
class of the event for tokens that Begin or are In an
event. Our time expression classifier simultaneously
assigns a B, I, or O to each token, and classifies the
type of the time expression for tokens that Begin or
are In a time expression (see Table 3).
A BIO model may sometimes be inconsistent; for
example, a token may be labeled as Inside a segment
of a particular type, while the previous token may
be labeled as Out of any segment. We considered
the two most likely labels for each token (as long as
each had likelihood at least 0.9), choosing the one
most consistent with the context.
5 Event Feature Classification
We determined the event features for each extracted
event using four additional classifiers, one each for
tense, aspect, polarity and modality. These classi-
fiers were trained only on tokens identified as part of
event expressions. Since the event expressions were
single words for all but a few (erroneous) cases in the
silver data, for determining the event features, we
used the same features as before, with the single ad-
dition of the event class (during testing, we used the
dynamically assigned event class from the event seg-
mentation classifier). As before, we experimented
with ATT1, ATT2, and ATT3 models. TempEval-
3 only includes evaluation of tense and aspect fea-
tures, so we only report for those. The tags assigned
by each classifier are listed in Table 3.
6 Time Normalization
To compute TIMEX3 standard based values for
extracted time expressions, we used the TIMEN
(Llorens et al, 2012) and TRIOS (UzZaman and
Allen, 2010) time normalizers. Values from the
normalizers were validated in post-processing (e.g.
?T2445? is invalid) and, when the normalizers re-
turned different non-nil values, TIMEN?s values
were selected without further reasoning. Time nor-
malization was out of scope in our research for this
evaluation, but it remains as part of our future work.
7 Results and Discussion
Our results for event segmentation/classification on
the TempEval-3 test data are provided in Table 4.
The absence of semantic features causes only small
changes in F1. The absence of syntactic features
causes F1 to drop slightly (less than 2.5% for all
but the smallest window size), with recall decreasing
while precision improves somewhat. Attribute F1 is
also impacted minimally by the absence of semantic
features, and about 2-5% by the absence of syntactic
features for all but the smallest window size.1
Our results for time expression extraction and
classification on the TempEval-3 test data are pro-
vided in Table 5. Here, the performance drops more
in the absence of semantic and syntactic features;
however, there is an interaction between length of
time expression and performance drop which we
may be able to ameliorate in future work by han-
dling consistency issues in the BIO time expression
extraction model better.
1In Tables 4 and 5, we present results that are slightly dif-
ferent from our submission due to a minor fix in our models by
removing some redundant feature values used twice.
22
Features Window size F1 P R Class Tense Aspect
STANFORD+SENNA 15 (ATT1) 81.16 81.49 80.83 71.60 59.62 73.76
7 81.08 81.74 80.43 71.49 59.05 73.78
3 80.35 81.23 79.49 71.41 58.67 73.17
1 80.94 80.77 81.10 72.37 58.06 73.71
STANFORD 15 (ATT2) 80.86 81.02 80.70 71.05 59.10 73.34
7 81.30 81.90 80.70 71.57 59.01 74.14
3 80.87 81.58 80.16 71.94 58.96 73.70
1 80.78 80.72 80.83 71.80 57.47 73.41
WORDS ONLY 15 (ATT3) 78.58 81.95 75.47 69.5 55.27 70.76
7 78.40 82.21 74.93 69.14 55.54 70.27
3 78.14 82.44 74.26 69.39 52.75 70.38
1 73.55 79.78 68.23 66.33 44.94 63.15
Table 4: Event extraction results (F1, P and R, strict match); feature classification results (attribute F1)
Features Window size F1 P R Type Value
STANFORD+SENNA 15 (ATT1) 80.17 (85.95) 93.27 (100) 70.29 (75.36) 77.69 65.29
7 76.99 (83.68) 91.09 (99.01) 66.67 (72.46) 75.31 64.44
3 75.52 (83.82) 88.35 (98.06) 65.94 (73.19) 75.52 63.07
1 66.12 (83.27) 75.70 (95.33) 58.70 (73.91) 72.65 59.59
STANFORD 15 (ATT2) 78.69 (85.25) 90.57 (98.11) 69.57 (75.36) 76.23 65.57
7 78.51 (84.30) 91.35 (98.08) 68.84 (73.91) 76.03 63.64
3 78.19 (84.77) 90.48 (98.10) 68.84 (74.64) 75.72 64.20
1 67.48 (83.74) 76.85 (95.37) 60.14 (74.64) 73.17 59.35
WORDS ONLY 15 (ATT3) 72.34 (80.85) 87.63 (97.94) 61.59 (68.84) 74.04 60.43
7 72.34 (80.85) 87.63 (97.94) 61.59 (67.84) 74.04 59.57
3 74.48 (82.85) 88.12 (98.02) 64.49 (71.74) 75.31 61.09
1 44.62 (82.87) 49.56 (92.04) 40.58 (75.36) 70.92 39.84
Table 5: Time expression extraction results (F1, P and R, strict match with relaxed match in parentheses); attribute F1
for type and value features
A somewhat surprising finding is that both event
and time expression extraction are subject to rela-
tively tight constraints from the lexical context. We
were surprised by how well the ATT3 (WORDS
ONLY) models performed, especially in terms of
precision. We were also surprised that the words
only models with window sizes of 3 and 7 performed
as well as the models with a window size of 15. We
think these results are promising for ?big data? text
analytics, where there may not be time to do heavy
preprocessing of input text or to train large models.
8 Future Work
For us, participation in TempEval-3 is a first step
in developing a temporal understanding component
for text analytics and virtual agents. We now in-
tend to appy our best performing models to this task.
In future work, we plan to evaluate our initial re-
sults with larger data sets (e.g., cross validation on
the tempeval training data) and experiment with hy-
brid/ensemble methods for performing time expres-
sion and temporal link extraction.
Acknowledgments
We thank Srinivas Bangalore, Patrick Haffner, and
Sumit Chopra for helpful discussions and for sup-
plying LLAMA and its front-end for our use.
23
References
S. F. Adafre and M. de Rijke. 2005. Feature engineering
and post-processing for temporal expression recogni-
tion using conditional random fields. In Proceedings
of the ACL Workshop on Feature Engineering for Ma-
chine Learning in Natural Language Processing.
G. Angeli, C. D. Manning, and D. Jurafsky. 2012. Pars-
ing time: Learning to interpret time expressions. In
Proceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (HLT-NAACL).
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(3?4).
H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem
(English and Spanish): Evaluating CRFs and semantic
roles in TempEval-2. In Proceedings of the Interna-
tional Workshop on Semantic Evaluation (SemEval).
H. Llorens, L. Derczynski, R. Gaizauskas, and E. Sa-
quete. 2012. Timen: An open temporal expression
normalisation resource. In Proceedings of the Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
S. Rigo and A. Lavelli. 2011. Multisex - a multi-
language timex sequential extractor. In Proceedings
of Temporal Representation and Reasoning (TIME).
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP. http://nlp.stanford.
edu/software/corenlp.shtml.
N. UzZaman and J. F. Allen. 2010. TRIPS and TRIOS
system for TempEval-2: Extracting temporal informa-
tion from text. In Proceedings of the International
Workshop on Semantic Evaluation (SemEval).
N. UzZaman, H. Llorens, J. Allen, L. Derczynski,
M. Verhagen, and J. Pustejovsky. 2012. Tempeval-
3: Evaluating events, time expressions, and tempo-
ral relations. http://arxiv.org/abs/1206.
5333v1.
24
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 109?113,
Dublin, Ireland, August 23-24, 2014.
AT&T: The Tag&Parse Approach to Semantic Parsing of Robot Spatial
Commands
Svetlana Stoyanchev, Hyuckchul Jung, John Chen, Srinivas Bangalore
AT&T Labs Research
1 AT&T Way Bedminster NJ 07921
{sveta,hjung,jchen,srini}@research.att.com
Abstract
The Tag&Parse approach to semantic
parsing first assigns semantic tags to each
word in a sentence and then parses the
tag sequence into a semantic tree. We
use statistical approach for tagging, pars-
ing, and reference resolution stages. Each
stage produces multiple hypotheses which
are re-ranked using spatial validation. We
evaluate the Tag&Parse approach on a cor-
pus of Robotic Spatial Commands as part
of the SemEval Task6 exercise. Our sys-
tem accuracy is 87.35% and 60.84% with
and without spatial validation.
1 Introduction
In this paper we describe a system participating
in the SemEval2014 Task-6 on Supervised Seman-
tic Parsing of Robotic Spatial Commands. It pro-
duces a semantic parse of natural language com-
mands addressed to a robot arm designed to move
objects on a grid surface. Each command directs
a robot to change position of an object given a
current configuration. A command uniquely iden-
tifies an object and its destination, for example
?Move the turquoise pyramid above the yellow
cube?. System output is a Robot Control Lan-
guage (RCL) parse (see Figure 1) which is pro-
cessed by the robot arm simulator. The Robot Spa-
tial Commands dataset (Dukes, 2013) is used for
training and testing.
Our system uses a Tag&Parse approach which
separates semantic tagging and semantic parsing
stages. It has four components: 1) semantic tag-
ging, 2) parsing, 3) reference resolution, and 4)
spatial validation. The first three are trained using
LLAMA (Haffner, 2006), a supervised machine
learning toolkit, on the RCL-parsed sentences.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
For semantic tagging, we train a maximum en-
tropy sequence tagger for assigning a semantic la-
bel and value to each word in a sentence, such as
type cube or color blue. For parsing, we train a
constituency parser on non-lexical RCL semantic
trees. For reference resolution, we train a maxi-
mum entropy model that identifies entities for ref-
erence tags found by previous components. All of
these components can generate multiple hypothe-
ses. Spatial validation re-ranks these hypotheses
by validating them against the input spatial con-
figuration. The top hypothesis after re-ranking is
returned by the system.
Separating tagging and parsing stages has sev-
eral advantages. A tagging stage allows the system
flexibility to abstract from possible grammatical or
spelling errors in a command. It assigns a seman-
tic category to each word in a sentence. Words not
contributing to the semantic meaning are assigned
?O? label by the tagger and are ignored in the fur-
ther processing. Words that are misspelled can po-
tentially receive a correct tag when a word simi-
larity feature is used in building a tagging model.
This will be especially important when process-
ing output of spoken commands that may contain
recognition errors.
The remainder of the paper is organized thusly.
In Section 2 we describe each of the components
used in our system. In Section 3 we describe the
results reported for SemEval2014 and evaluation
of each system component. We summarize our
findings and present future work in Section 4.
2 System
2.1 Sequence Tagging
A sequence tagging approach is used for condi-
tional inference of tags given a word sequence.
It is used for many natural language tasks, such
as part of speech (POS) and named entity tag-
ging (Toutanova and others, 2003; Carreras et al.,
2003). We train a sequence tagger for assign-
109
Figure 1: RCL tree for a sentence Move the turquoise pyramid above the yellow cube.
Word index tag label
Move 1 action move
the 2 O -
turquoise 3 color cyan
pyramid 4 type prism
above 5 relation above
the 6 O -
yellow 7 color yellow
cube 8 type cube
Table 1: Tagging labels for a sentence Move the
turquoise pyramid above the yellow cube.
ing a combined semantic tag and label (such as
type cube) to each word in a command. The tags
used for training are extracted from the leaf-level
nodes of the RCL trees. Table 2 shows tags and
labels for a sample sentence ?Move the turquoise
pyramid above the yellow cube? extracted from
the RCL parse tree (see Figure 1). In some cases,
a label is the same as a word (yellow, cube) while
in other cases, it differs (turquoise - cyan, pyramid
- prism).
We train a sequence tagger using LLAMA max-
imum entropy (maxent) classification (Haffner,
2006) to predict the combined semantic tag and
label of each word. Neighboring words, immedi-
ately neighboring semantic tags, and POS tags are
used as features, where the POS tagger is another
sequence tagging model trained on the Penn Tree-
bank (Marcus et al., 1993). We also experimented
with a tagger that assigns tags and labels in sep-
arate sequence tagging models, but it performed
poorly.
2.2 Parsing
We use a constituency parser for building RCL
trees. The input to the parser is a sequence of
tags assigned by a sequence tagger, such as ?ac-
tion color type relation color type? for the exam-
ple in Figure 1.
The parser generates multiple RCL parse tree
hypotheses sorted in the order of their likelihood.
The likelihood of a tree T given a sequence of tags
T is determined using a probabilistic context free
grammar (PCFG) G:
P (T |S) =
?
r?T
P
G
(r) (1)
The n-best parses are obtained using the CKY
algorithm, recording the n-best hyperedge back-
pointers per constituent along the lines of (Huang
and Chiang, 2005). G was obtained and P
G
was
estimated from a corpus of non-lexical RCL trees
generated by removing all nodes descendant from
the tag nodes (action, color, etc.). Parses may con-
tain empty nodes not corresponding to any tag in
the input sequence. These are hypothesized by the
parser at positions in between input tags and in-
serted as edges according to the PCFG, which has
probabilistic rules for generating empty nodes.
2.3 Reference Resolution
Reference resolution identifies the most prob-
able antecedent for each anaphor within a
text (Hirschman and Chinchor, 1997). It applies
when multiple candidates antecedents are present.
For example, in a sentence ?Pick up the red cube
standing on a grey cube and place it on top of
the yellow one?, the anaphor it has two candidate
antecedents corresponding to entity segments the
red cube and a grey cube. In our system, anaphor
and antecedents are represented by reference tags
occurring in one sentence. A reference tag is ei-
ther assigned by a sequence tagger to one of the
words (e.g. to a pronoun) or is inserted into a
tree by the parser (e.g. ellipsis). We train a bi-
nary maxent model for this task using LLAMA.
The input is a pair consisting of an anaphor and
a candidate antecedent, along with their features.
110
Features that are used include the preceding and
following words as well as the tags/labels of both
the anaphor and candidate antecedent. The refer-
ence resolution component selects the antecedent
for which the model returns the highest score.
2.4 Spatial Validation
SemEval2014 Task6 provided a spatial planner
which takes an RCL command as an input and
determines if that command is executable in the
given spatial context. At each step described in
2.1?2.3, due to the statistical nature of our ap-
proach, multiple hypotheses can be easily com-
puted with different confidence values. We used
the spatial planner to validate the final output RCL
commands from the three steps by checking if the
RCLs are executable or not. We generate multi-
ple tagger output hypotheses. For each tagger out-
put hypothesis, we generate multiple parser out-
put hypotheses. For each parser output hypothe-
sis, we generate multiple reference resolution out-
put hypotheses. The resulting output hypotheses
are ranked in the order of confidence scores with
the highest tagging output scores ranked first, fol-
lowed by the parsing output scores, and, finally,
reference resolution output scores. The system re-
turns the result of the top scored command that is
valid according to the spatial validator.
In many applications, there can be a tool or
method to validate tag/parse/reference outputs
fully or partially. Note that in our system the val-
idation is performed after all output is generated.
Tightly coupled validation, such as checking va-
lidity of a tagged entity or a parse constituent,
could help in computing hypotheses at each step
(e.g., feature values based on possible entities or
actions) and it remains as future work.
3 Results
In this section, we present evaluation results on the
three subsets of the data summarized in Table 3. In
the TEST2500 data set, the models are trained on
the initial 2500 sentences of the Robot Commands
Treebank and evaluated on the last 909 sentences
(this corresponds to the data split of the SemEval
task). In TEST500 data set, the models are trained
on the initial 500 sentences of the training set and
evaluated on the last 909 test sentences. We re-
port these results to analyze the models? perfor-
mance on a reduced training size. In DEV2500
data set, models are trained on 90% of the initial
2500 sentences and evaluated on 10% of the 2500
# Dataset Avg # hyp Accuracy
1 TEST2500 1-best 1 86.0%
2 TEST2500 max-5 3.34 95.2%
3 TEST500 1-best 1 67.9%
4 TEST500 max-5 4.25 83.8%
5 DEV2500 1-best 1 90.8%
6 DEV2500 max-5 2.9 98.0%
Table 3: Tagger accuracy for 1-best and maximum
of 5-best hypotheses (max-5).
sentences using a random data split. We observe
that sentence length and standard deviation of test
sentences in the TEST2500 data set is higher than
on the training sentences while in the DEV2500
data set training and test sentence length and stan-
dard deviation are comparable.
3.1 Semantic Tagging
Table 3 presents sentence accuracy of the seman-
tic tagging stage. Tagging accuracy is evaluated
on 1-best and on max-5 best tagger outputs. In
the max-5 setting the number of hypotheses gen-
erated by the tagger varies for each input with the
average numbers reported in Table 3. Tagging ac-
curacy on TEST2500 using 1-best is 86.0%. Con-
sidering max-5 best tagging sequences, the accu-
racy is 95.2%. On the TEST500 data set tagging
accuracy is 67.9% and 83.8% on 1-best and max-
5 best sequences respectively, approximately 8%
points lower than on TEST2500 data set. On the
DEV2500 data set tagging accuracy is 90.8% and
98.0% on 1-best and max-5 best sequences, 4.8%
and 2.8% points higher than on the TEST2500
data set. The higher performance on DEV2500 in
comparison to the TEST2500 can be explained by
the higher complexity of the test sentences in com-
parison to the training sentences in the TEST2500
data set.
3.2 RCL Parsing
Parsing was evaluated using the EVALB scoring
metric (Collins, 1997). Its 1-best F-measure accu-
racy on gold standard TEST2500 and DEV2500
semantic tag sequences was 96.17% and 95.20%,
respectively. On TEST500, its accuracy remained
95.20%. On TEST2500 with system provided in-
put sequences, its accuracy was 94.79% for 869
out of 909 sentences that were tagged correctly.
3.3 System Accuracy
Table 4 presents string accuracy of automatically
generated RCL parse trees on each data set. The
111
Name Train #sent Train Sent. len. (stdev) Test #sent Test Sent. Len. (stdev)
TEST2500 2500 13.44 (5.50) 909 13.96 (5.59)
TEST500 500 14.62(5.66) 909 13.96 (5.59)
DEV2500 2250 13.43 ( 5.53) 250 13.57 (5.27)
Table 2: Number of sentences, average length and standard deviation of the data sets.
results are obtained by comparing system output
RCL parse string with the reference RCL parse
string. For each data set, we ran the system
with and without spatial validation. We ran RCL
parser and reference resolution on automatically
assigned semantic tags (Auto) and oracle tagging
(Orcl). We observed that some tag labels can be
verified systematically and corrected them with
simple rules: e.g., change ?front? to ?forward?
because relation specification in (Dukes, 2013)
doesn?t have ?front? even though annotations in-
cluded cases with ?front? as relation.
The system performance on TEST2500 data
set using automatically assigned tags and no spa-
tial validation is 60.84%. In this mode, the sys-
tem uses 1-best parser and 1-best tagger output.
With spatial validation, which allows the system to
re-rank parser and tagger hypotheses, the perfor-
mance increases by 27% points to 87.35%. This
indicates that the parser and the tagger component
often produce a correct output which is not ranked
first. Using oracle tags without / with spatial vali-
dation on TEST2500 data set the system accuracy
is 67.55% / 94.83%, 7% points above the accuracy
using predicted tags.
The system performance on TEST500 data set
using automatically assigned tags with / with-
out spatial validation is 48.95% / 74.92%, ap-
proximately 12% points below the performance
on TEST2500 (Row 1). Using oracle tags with-
out / with spatial validation the performance on
TEST500 data set is 63.89% / 94.94%. The per-
formance without spatial validation is only 4% be-
low TEST2500, while with spatial validation the
performance on TEST2500 and TEST500 is the
same. These results indicate that most perfor-
mance degradation on a smaller data set is due to
the semantic tagger.
The system performance on DEV2500 data set
using automatically assigned tags without / with
spatial validation is 68.0% / 96.80% (Row 5), 8%
points above the performance on TEST2500 (Row
1). With oracle tags, the performance is 69.60%
/ 98.0%, which is 2-3% points above TEST2500
(Row 2). These results indicate that most perfor-
mance improvement on a better balanced data set
# Dataset Tag Accuracy without / with
spatial validation
1 TEST2500 Auto 60.84 / 87.35
2 TEST2500 Orcl 67.55 / 94.83
3 TEST500 Auto 48.95 / 74.92
4 TEST500 Orcl 63.89 / 94.94
5 DEV2500 Auto 68.00 / 96.80
6 DEV2500 Orcl 69.60 / 98.00
Table 4: System accuracy with and without spatial
validation using automatically assigned tags and
oracle tags (OT).
DEV2500 is due to better semantic tagging.
4 Summary and Future Work
In this paper, we present the results of semantic
processing for natural language robot commands
using Tag&Parse approach. The system first tags
the input sentence and then applies non-lexical
parsing to the tag sequence. Reference resolution
is applied to the resulting parse trees. We com-
pare the results of the models trained on the data
sets of size 500 (TEST500) and 2500 (TEST2500)
sentences. We observe that sequence tagging
model degrades significantly on a smaller data set.
Parsing and reference resolution models, on the
other hand, perform nearly as well on both train-
ing sizes. We compare the results of the models
trained on more (DEV2500) and less (TEST2500)
homogeneous training/testing data sets. We ob-
serve that a semantic tagging model is more sen-
sitive to the difference between training and test
set than parsing model degrading significantly a
less homogeneous data set. Our results show that
1) both tagging and parsing models will benefit
from an improved re-ranking, and 2) our parsing
model is robust to a data size reduction while tag-
ging model requires a larger training data set.
In future work we plan to explore how
Tag&Parse approach will generalize in other do-
mains. In particular, we are interested in using
a combination of domain-specific tagging models
and generic semantic parsing (Das et al., 2010) for
processing spoken commands in a dialogue sys-
tem.
112
References
Xavier Carreras, Llu??s M`arquez, and Llu??s Padr?o.
2003. A Simple Named Entity Extractor Using Ad-
aBoost. In Proceedings of the CoNLL, pages 152?
157, Edmonton, Canada.
Michael Collins. 1997. Three Generative Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL, pages 16?23.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-
Semantic Parsing. In HLT-NAACL, pages 948?956.
Kais Dukes. 2013. Semantic Annotation of Robotic
Spatial Commands. In Language and Technology
Conference (LTC).
Patrick Haffner. 2006. Scaling large margin classifiers
for spoken language understanding. Speech Com-
munication, 48(3-4):239?261.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 Coreference Task Definition. In Proceedings of
the Message Understanding Conference (MUC-7).
Science Applications International Corporation.
Liang Huang and David Chiang. 2005. Better K-
best Parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
?05, pages 53?64, Stroudsburg, PA, USA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the 2003 Conference of the
NAACL on Human Language Technology - Volume
1, pages 173?180.
113
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 146?154,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Building Timelines from Narrative Clinical Records: Initial Results 
Based-on Deep Natural Language Understanding
Hyuckchul Jung, James Allen, Nate Blaylock, Will de Beaumont, 
Lucian Galescu, Mary Swift
Florida Institute for Human and Machine Cognition
40 South Alcaniz Street, Pensacola, Florida, USA
{hjung,blaylock,jallen,wbeaumont,lgalescu,mswift}@ihmc.us
Abstract
We present an end-to-end system that proc-
esses narrative clinical records, constructs 
timelines for the medical histories of pa-
tients, and visualizes the results. This work 
is motivated by real clinical records and 
our general approach is based on deep se-
mantic natural language understanding.
1 Introduction
It is critical for physicians and other healthcare 
providers to have complete and accurate knowl-
edge of the medical history of patients that  in-
cludes disease/symptom progression over time and 
related tests/treatments in chronological order. 
While various types of clinical records (e.g., dis-
charge summaries, consultation notes, etc.) contain 
comprehensive medical history information, it  can 
be often challenging and time-consuming to com-
prehend the medical history of patients when the 
information is stored in multiple documents in dif-
ferent  formats and the relations among various 
pieces of information is not explicit.
For decades, researchers have investigated tem-
poral information extraction and reasoning in the 
medical domain (Zhou and Hripcsak, 2007). How-
ever, information extraction in the medical domain 
typically relies on shallow NLP techniques (e.g., 
pattern matching, chunking, templates, etc.),  and 
most temporal reasoning techniques are based on 
structured data with temporal tags (Augusto, 2005; 
Stacey and McGregor, 2007).
In this paper, we present our work on develop-
ing an end-to-end system that  (i) extracts interest-
ing medical concepts (e.g., medical conditions/
tests/treatments), related events and temporal ex-
pressions from raw clinical text records, (ii) con-
structs timelines of the extracted information; and 
(iii) visualizes the timelines, all using deep seman-
tic natural language understanding (NLU). 
Our deep NLU system extracts rich semantic 
information from narrative text records and builds 
logical forms that  contain ontology types as well as 
linguistic features. Ontology- and pattern-based 
extraction rules are used on the logical forms to 
retrieve time points/intervals, medical concepts/
events and their temporal/causal relations that are 
pieced together by our system?s temporal reasoning 
component to create comprehensive timelines.
Our system is an extension to a well-proven 
general-purpose NLP system (Allen et  al., 2000) 
rather than a system specialized to the clinical do-
main, and the temporal reasoning in our system is 
tightly integrated into the NLP system?s deep se-
mantic analysis. We believe this approach will al-
low us to process a broader variety of documents 
and complex forms of temporal expressions.
In the coming sections, we first present a moti-
vating example, a real clinical record of a cancer 
patient. Next, we give an overview of our NLU 
system including how medical ontology is inte-
grated into our system. The overview section is 
followed by detailed description of our information 
extraction and temporal reasoning approach. Then, 
we discuss our results and conclude.
2 Motivating Example
Our work is carried out as a collaboration with the 
Moffitt  Cancer Center (part of the NCI Compre-
hensive Cancer Centers), who have provided us 
with access to clinical records for over 1500 pa-
tients. Figure 1 shows a (de-identified) ?History of 
Present Illness? (HPI) section of a Thoracic Con-
sultation Note from this data set. 
146
The text of this section provides a very detailed 
description of what  problems/tests/treatments an 
anonymous cancer patient went  through over a pe-
riod. Such narrative text is common in clinical 
notes and, because such notes are carefully created 
by physicians, they tend to have only relevant in-
formation about patient medical history. 
Nonetheless, there are lots of challenges in con-
structing complete and accurate medical history 
because of complex temporal expressions/
relations, medical language specific grammar/
jargons, implicit  information and domain-specific 
medical knowledge (Zhou and Hripcsak, 2007).
In this paper, as an initial step towards con-
structing complete timelines from narrative text, 
we focus on sentences with explicit  temporal ex-
pressions listed below (tagged as Line 1 ~ 11) plus 
a sentence in the present tense (Line 12):
1
?
Line 1: She had a left radical nephrectomy in  09/
2007; pathological stage at that time  was a T3 
NX MX. 
?
Line 2: Prior to her surgery CT scan in 08/2007 
showed lung nodules. 
?
Line 3: She was placed on Nexavar in 11/2007. 
?
Line 4: She was started on Afinitor on 03/05/08. 
?
Line 5: She states that prior to starting the Afini-
tor she had no shortness of breath or dyspnea on 
exertion and she was quite active. 
?
Line 6: Unfortunately 4 weeks after starting the 
Afinitor she developed a dry cough and progres-
sive shortness of breath with dyspnea on exer-
tion. 
?
Line 7: She received a 5 day dose pack of 
prednisone and was treated with Augmentin in 
05/2008. 
?
Line 8: She subsequently had a CT scan of the 
chest  done on 05/14/08 that  showed interval de-
velopment of bilateral lower lobe infiltrates that 
were not present on the 02/19/08 scan. 
?
Line 9: Because of her respiratory symptoms, the 
Afinitor was stopped on 05/18/2008. 
?
Line 10: Prior to  the Afinitor she was able to 
walk, do gardening, and swim without any 
shortness of breath.  
?
Line 11: She has had a 140 pound weight  since 
10/2007.
?
Line 12: She denies fevers, chills, hemoptysis or 
chest pain. 
In these 12 sentences, there are instances of 10 
treatments (e.g., procedures such as ?nephrectomy? 
and drugs such as ?Nexavar?), 3 tests (e.g., CT-
scan), 13 problems/symptoms (e.g., lung nodules) 
and 2 other types of clinical findings (e.g., the can-
cer stage level ?T3 NX MX?). There are also 23 
events of various types represented with verbs such 
as  ?had?, ?was?, ?showed?, and ?was started?.
While there are simple expressions such as ?on 
03/05/08? in Line 3, there are also temporal ex-
pressions in more complex forms with time rela-
tions (e.g., ?prior to?), time references (e.g., ?at 
that time?) or event references (e.g., ?4 weeks after 
starting Afinitor?). Throughout  this paper, we will 
use Line 1 ~ 12 as a concrete example based on 
which we develop general techniques to construct 
timelines.
1
 For privacy, identities of patients/physicians were concealed and the dates/time-spans in the original sources were 
altered while maintaining their chronological order. Some measurements and geographic names were also modified.
Figure 1: A sample medical record -- Thoracic 
Consultation Note
1
PAST MEDICAL HISTORY: 
1. History of melanoma of the left arm.  She had excision of 3 sentinel lymph nodes in the left axilla 
that were negative.  This was in 07/2007.
2. Status post right hip replacement.
3. Status post cholecystectomy.
4. Status post renal stone removal.
5. Fracture of the right hip and left wrist in a motor vehicle accident.
6. Diabetes.
7. Elevated cholesterol.
8. Hypertension.
9. Spinal stenosis. 
ALLERGIES:
She has no known drug allergies.  She is allergic to IVP dye which causes shortness of breath.  She 
tolerates IV dye when she is pre treated.  
SOCIAL HISTORY: 
She is born and raised in California and she lived in Florida for 30 years.  She has worked as a 
medical billing analyst.  She has never smoked. She does not use alcoholic beverages.
FAMILY HISTORY:
Her father died at age 69 of prostate cancer.  Her mother died at age 72 of emphysema.  She had 1 
sister who died from melanoma.  
REVIEW OF SYSTEMS: 
A complete review of systems was performed.  See the questionnaire.  She has hypothyroidism.  
She has some back pain related to her spinal stenosis.  She suffers from mild depression.  
CURRENT MEDICATIONS:
1. Carvedilol 6.25 mg p.o. daily.
2. Darvocet N100, 1 tablet as needed.
3. Fish oil, 1000 mg three times a day.
4. Glimepiride 4 mg daily in the morning and 2 mg at bedtime.
5. Lipitor 20 mg daily.
6. Metformin 1000 mg twice daily.
7. Paroxetine 20 mg daily.
8. Synthroid 0.112 mg daily.
9. Tylenol as needed.
10. Vitamin B12, 2500 mcg p.o. twice daily.
XXX X XX-XX-XX 
CONSULTATION DATE: 07/06/2008
RE: XXX BIRTH DATE: XX/XX/XXXX
UR#: XX-XX-XX AGE: 75
THORACIC CONSULTATION NOTE
REQUESTING PHYSICIAN:
XXXXXXXXXX, MD.
REASON FOR CONSULTATION:
Shortness of breath and abnormal chest x ray.
HISTORY OF PRESENT ILLNESS:
Ms. XXX is a 75 year old woman who has a history of metastatic renal cancer.  She had a left radical 
nephrectomy in 09/2007; pathological stage at that time was a T3 NX MX.  Prior to her surgery CT 
scan in 08/2007 showed lung nodules.  These nodules have progressed with time.  She was placed 
on Nexavar in 11/2007.  She subsequently was found to have a new mass in her left nephrectomy 
bed.  She was continued on the Nexavar, however, she showed radiographic progression and the 
Nexavar was discontinued.  She was started on Afinitor on 03/05/08.  She states that prior to starting 
the Afinitor she had no shortness of breath or dyspnea on exertion and she was quite active.  
Unfortunately 4 weeks after starting the Afinitor she developed a dry cough and progressive 
shortness of breath with dyspnea on exertion.  She received a 5 day dose pack of prednisone and 
was treated with Augmentin in 05/2008.  This had no impact on her cough or shortness of breath.  
She subsequently had a CT scan of the chest done on 05/14/08 that showed interval development 
of bilateral lower lobe infiltrates that were not present on the 02/19/08 scan.  She had mediastinal 
and right hilar adenopathy that had increased.  She had multiple lung nodules and there was 
recurrent tumor noted in the left renal bed which was thought to be larger.  Because of her 
respiratory symptoms, the Afinitor was stopped on 05/18/08.  She still has a dry cough.  She is short 
of breath after walking 15 to 20 feet.  She has no shortness of breath at rest.  She denies PND or 
orthopnea.  Prior to the Afinitor she was able to walk, do gardening, and swim without any shortness 
of breath.  She has had a 140 pound weight since 10/2007.  She notices anorexia.  She has no 
travel history.
She denies fevers, chills, hemoptysis or chest pain.  She has never smoked.  She denies 
pneumonia, asthma, wheezing, or myocardial infarction, congestion heart failure or heart murmur.  
She has dogs and cats at home and has had them for a long time and this never caused her 
respiratory problems. 
PHYSICAL EXAMINATION: 
VITAL SIGNS:   Blood pressure 131/74, pulse 106, respiratory rate 20, temperature 97.3, weight 
64.0 kg.
HEENT:   Pupils equal, round, reactive to light.  Extraocular muscles were intact.  Nose and mouth 
were clear. 
NECK:  Trachea midline.  Carotids were 2 plus.  No masses, thyromegaly or adenopathy.  
LUNGS:   Respirations were unlabored.  There is no dullness to percussion or tenderness to 
palpation.  She has some bibasilar dry rales.
HEART:   Regular rate and rhythm without murmur.
ABDOMEN:  Soft, positive bowel sounds, nontender. 
EXTREMITIES:   No clubbing or cyanosis.  She had some mild pedal edema. 
DATABASE:
Chest x ray from 06/01/08 was reviewed.  She had bilateral lower lobe patchy densities.  She had 
some nodular densities bilaterally as well.  There is widening of the mediastinum on the right.  CT 
scan of the chest from 05/14/08 also was reviewed.  She had bilateral lower lobe infiltrates that were 
new.  She had mediastinal and right hilar adenopathy.  She had multiple lung nodules.  There is 
recurrent tumor in the left renal bed that was thought to be larger. 
IMPRESSION:
1. Metastatic renal cancer with multiple lung nodules with mediastinal and hilar adenopathy.  
2. Bilateral lower lobe infiltrates.  These infiltrates had developed after starting the Afinitor, as did 
her shortness of breath and dyspnea on exertion.  She recently started on oxygen by her primary 
care physician when she was found to have exercise O2 saturations of 86%.  She is currently taking 
2 liters of oxygen.  I would be concerned that the infiltrates may be related to pneumonitis from the 
Afinitor.  I also think her shortness of breath, cough and hypoxemia are related to the infiltrates as 
well.  
RECOMMENDATIONS:
1. I reviewed my impressions with the patient.
2. I am going to schedule her for a bronchoscopy and bronchoalveolar lavage.  I am going to get 
baseline pulmonary function tests on her. 
3. She will be seen by Dr. XXX on 08/12/08.  I will call and discuss the case with him pending the 
above results.  The options are likely going to be observation off Afinitor or may consider placing her 
on prednisone, if the bronchoalveolar lavage is unremarkable.  
4. Further recommendations will be made after the above.
Do not type or edit below this line. This will cause format damage.
  
Dictated by XXXX, MD
Electronically Signed
FXXXXXXX, MD 07/10/2008 10:15
________________________
XXXXX, MD
DD: 07/10/2008  9:24 A
DT: 07/13/2008 11:46 A
ID: XXXXXXX.LML
CS: XXXXXX
cc: 
??
HISTORY OF PRESENT ILLNESS:
Ms. XXX is a 75 year ld woman who has a history of metastatic renal 
cancer. She had a left adi al nephrectomy in 09/2007; pathological stage 
at that time was a T3 NX MX. Prior to her surgery CT scan in 08/2007 
showed lung nodules. These nodules have progressed with time. She was 
placed on Nexavar in 11/2007. She subsequently was found to have a 
new mass in her left nephrectomy bed. She was continued on the 
N xavar, owever, she showed radiographic progression and the Nexavar 
was disc ntinued. She was started on Afinitor on 03/05/08. She states 
that p i r to starting the Afinitor she had no shortness of breath or 
dys n a on exe tion and she was quite active. Unfortunately 4 weeks 
after starting the Afinitor she developed a dry cough and progressive 
tness f br th with dyspnea on exertion. She received a 5 day dose 
pack of prednisone and was treated with Augmentin in 05/2008. This had 
no impact on her cough or shortness of breath. She subsequently had a 
CT scan of the chest done on 05/14/08 that showed interval development 
of bilateral lower lobe infiltrates that were not present on the 02/19/08 
scan. She had mediastinal and right hilar adenopathy that had increased. 
She had multiple lung nodules and there was recurrent tumor noted in the 
left renal bed which was thought to be larger. Because of her respiratory 
symptoms, the Afinitor was stopped on 05/18/2008. She still has a dry 
cough. She is short of breath after walking 15 to 20 feet. She has no 
shortness of breath at rest. She denies PND or orthopnea. Prior to the 
Afinitor she was able to walk, do gardening, and swim without any 
shortness of breath. She has had a 140 pound weight since 10/2007. She 
notices anorexia. She has no travel history. She denies fevers, chills, 
hemoptysis or chest pain. She has never smoked. She denies pneumonia, 
asthma, wheezing, or myocardial infarction, congestion heart failure or 
heart murmur. She has dogs and cats at home and has had them for a long 
time and this never caused her respiratory problems.
147
3 Natural Language Understanding 
(NLU) System
Our system is an extension to an existing NLU sys-
tem that is the result of a decade-long research ef-
fort  in developing generic natural language tech-
nology. The system uses a ?deep? understanding 
approach, attempting to find a linked, overall 
meaning for all the words in a paragraph. An archi-
tectural view of the system is shown in Figure 2.
3.1 Core NLU Components
At the core of the system is a packed-forest  chart 
parser which builds constituents bottom-up using a 
best-first search strategy. The core grammar is a 
hand-built, lexicalized context-free grammar, aug-
mented with feature structures and feature unifica-
tion. The parser draws on a general purpose seman-
tic lexicon and ontology which define a range of 
word senses and lexical semantic relations. The 
core semantic lexicon was constructed by hand and 
contains more than 7000 lemmas. It  can be also 
dynamically augmented for unknown words by 
consulting WordNet (Miller, 1995). 
To support  more robust processing as well as 
domain configurability, the core system is in-
formed by a variety of statistical and symbolic pre-
processors. These include several off-the-shelf sta-
tisical NLP tools such as the Stanford POS tagger 
(Toutanova and Manning, 2000), the Stanford 
named-entity recognizer (NER) (Finkel et al, 
2005) and the Stanford Parser (Klein and Manning, 
2003). The output of these and other specialized 
preprocessors (such as a street address recognizer) 
are sent  to the parser as advice. The parser then can 
include or not include this advice (e.g., that a cer-
tain phrase is a named entity) as it searches for the 
optimal parse of the sentence.
The result  of parsing is a frame-like semantic 
representation that we call the Logical Form (LF). 
The LF representation includes semantic types, 
semantic roles for predicate arguments, and de-
pendency relations. Figure 3 shows an LF example 
for the sentence ?She had a left radical nephrec-
tomy in 09/2007?. In the representation, elements 
that start  with colons (e.g., :THEME) are semantic 
roles of ontological concepts, and role values can 
be a variable to refer to another LF term.
3.2 UMLS Integration
By far the most  critical aspect  of porting our ge-
neric NLU components to the task of understand-
ing clinical text  is the need for domain-specific 
lexical and ontologic information. One widely used 
comprehensive resource that  can provide both is 
the National Library of Medicine?s Unified Medi-
cal Language System (UMLS) (Bodenreider, 
2004). UMLS was integrated into or system via 
MetaMap (Aronson and Lang, 2010), a tool also 
developed by NLM, that  can identify and rank 
UMLS concepts in text.
Specifically, we added MetaMap as a special 
kind of named entity recognizer feeding advice 
into the Parser?s input chart (see Figure 2). We run 
MetaMap twice on the input  text  to obtain UMLS 
information both for the maximal constituents, and 
for individual words in those constituents (e.g., 
?lung cancer?, as well as ?lung? and ?cancer?).
The lexicon constructs representations for the 
new words and phrases on the fly. Our general ap-
proach for dealing with how the corresponding 
concepts fit  in our system ontology uses an ontol-
Core Lexicon
& Semantic Ontology
Grammar
Parser
Wordnet
Unknown Word 
Processing
New Lexical Entries
Output
Chart
Input
Chart
Statistical Parser
Bracketing Preferences
Input
Named Entity 
Recognizer
Name 
Hypotheses
POS 
Tagging
POS
Hypotheses
Word Hypotheses
MetaMap UMLS
UMLS
POS/sense 
Hypotheses
LF Semantic 
Representation 
for reasoners
Figure 2: Front-end language processing components with MetaMap and UMLS
148
ogy specialization mechanism which we call on-
tology grafting, whereby new branches are created 
from third party ontological sources, and attached 
to appropriate leaf nodes in our ontology.
The UMLS Semantic Network and certain vo-
cabularies included in the UMLS Metathesaurus 
define concept hierarchies along multiple axes. 
First, we established links between the 15 UMLS 
semantic groups and corresponding concepts in our 
ontology. Second, we selected a list of nodes from 
the SNOMED-CT and NCI hierarchies (27 and 11 
nodes, respectively) and formed ontological 
branches rooted in these nodes that  we grafted onto 
our ontology. 
Based on these processes, UMLS information 
gets integrated into our LF representation. In Fig-
ure 3, the 3rd term has a role called :domain-info 
and, in fact, its value is (UMLS :CUI C2222800 
:CONCEPT "left nephrectomy" :PREFERRED 
"nephrectomy of left kidney (treatment)" 
:SEMANTIC-TYPES (TOPP) :SEMANTIC-
GROUPS (PROC) :SOURCES (MEDCIN MTH)) 
that provides detailed UMLS concept information. 
Here, the semantic type ?TOPP? is a UMLS abbre-
viation for ?Therapeutic or Preventive Procedure?. 
More details about complex issues surrounding 
UMLS integration into our system can be found in 
(Swift et al, 2010).
4 Information Extraction (IE) from Clinical 
Text Records
In this section, we describe how to extract basic 
elements that will be used as a foundation to con-
struct timelines. We first describe our general ap-
proach to extracting information from LF graphs. 
Then we give details specific to the various types 
of information we extract in our system: various 
clinical concepts, temporal concepts (points as well 
as intervals), events and temporal relations.
4.1 LF Pattern-based Extraction
Given LF outputs from the NLU system described 
in Section 3, we use LF pattern-based rules for in-
formation extraction. The basic structure of an ex-
traction rule is a list  of LF patterns followed by a 
unique rule ID and the output specification.
Each LF-pattern specifies a pattern against  an 
LF. Variables can appear anywhere except as role 
names in different formats:
?
?x - (unconstrained) match anything 
?
?!x - match any non-null value
?
(? x V1 V2 ...) - (constrained) match one of the 
specified values V1,  V2, ...
As an example, the extraction rule in Figure 4 
will match LFs that mean a person had a treatment 
or a medical-diagnostic with explicit  UMLS in-
formation (i.e., part of LFs in Figure 3 matches). 
The output specification records critical informa-
tion from the extraction to be used by other rea-
soners. 
The extraction rules have all been developed by 
hand. Nevertheless, they are quite general, since a) 
LF patterns abstract away from lexical and syntac-
tic variability in the broad class of expressions of 
interest (however, lexical and syntactic features 
may be used if needed); and b) LF patterns make 
heavy use of ontological categories, which pro-
vides abstraction at the semantic level.
4.2 Clinical Concept Extraction
Among various types of concepts included in clini-
cal records, we focus on concepts related to 
problems/tests/treatments to build a medical his-
(F V1 (:* ONT::HAVE W::HAVE) :AFFECTED V2 :THEME V3 :MOD V4 :TENSE W::PAST) 
(PRO V2 (:* ONT::PERSON W::SHE) :PROFORM ONT::SHE :CO-REFERENCE V0) 
(A V3 (:* ONT::TREATMENT W::LEFT-RADICAL-NEPHRECTOMY) :DOMAIN-INFO (UMLS .....)
(F V4 (:* ONT::TIME-SPAN-REL W::IN) :OF V1 :VAL V5)
(THE V5 ONT::TIME-LOC :YEAR 2007 :MONTH 9)
Figure 3: LF semantic representation for ?She had a left radical nephrectomy in 09/2007?
(?x1 ?y2 (? type1 ONT::HAVE) :AFFECTED ?y2 :THEME ?y3 :MOD ?y4)
(?x2 ?y2 (? type2 ONT::PERSON)))
(?x3 ?y3 (? type3 ONT::TREATMENT ONT::MEDICAL-DIAGNOSTIC) :DOMAIN-INFO ?!info)
List of LF patterns
-extract-person-has-treatment-or-medical-diagnostic>
(EVENT :type ?type1 :class occurrence :subject ?y2 :object ?y3)
Unique rule ID
Output Specification
Figure 4: An example extraction rule
149
tory and extract  them using extraction rules as de-
scribed above. Figure 5 shows a rule to extract 
substances by matching any LF with a substance 
concept (as mentioned already, subclasses such as 
pharmacologic substances, would also match).
The rule in Figure 5 checks the :quantifier role 
and its value (e.g., none) is used to infer the pres-
ence or the absence of concepts. Using similar 
rules, we extract  additional concepts such as 
medical-disorders-and-conditions, physical-
symptom, treatment, medical-diagnostic, medical-
action and clinical-finding. Here, medical-action 
and clinical-finding are to extract concepts in a 
broader sense.
2
 To cover additional concepts, we 
can straightforwardly update extraction rules.
4.3 Temporal Expression Extraction
Temporal expressions are also extracted in the 
same way but using different  LF patterns. We have 
14 rules to extract  dates and time-spans of varying 
levels of complexity; for the example in Figure 1 
six of these rules were applied. Figure 6 shows LF 
patterns for a rule to extract temporal expressions 
of the form ?until X days/months/years ago?; for 
example, here is what the rule extracts for ?until 3 
days ago?:
(extraction :type time-span :context-rel (:* 
ont::event-time-rel w::until) :reference (time-position 
:context-rel (:* ont::event-time-rel w::ago) :amount 3 
:unit (:* ont::time-unit ont::day))) 
From this type of output, other reasoners can 
easily access necessary information about given 
temporal expressions without investigating the 
whole LF representation on their own.
4.4 Event Extraction
To construct timelines, the concepts of interest 
(Section 4.2) and the temporal expressions (Sec-
tion 4.3) should be pieced together. For that pur-
pose, it  is critical to extract events because they not 
only describe situations that  happen or occur but 
also represent states or circumstances where some-
thing holds. Furthermore, event features provide 
useful cues to reason about  situations surrounding 
extracted clinical concepts.
Here, we do not formally define events, but  refer 
to (Sauri et  al., 2006) for detailed discussion about 
events. While events can be expressed by multiple 
means (e.g., verbs, nominalizations, and adjec-
tives), our extraction rules for events focus on 
verbs and their features such as class, tense, aspect, 
and polarity. Figure 7 shows a rule to extract  an 
event  with the verb ?start? like the one in Line 4, 
?She was started on Afinitor on 03/05/08?. The 
output specification from this rule for Line 4 will 
have the :class, :tense, and :passive roles as (aspec-
tual initiation), past, and true respectively.
These event  features play a critical role in con-
structing timelines (Section 5). For instance, the 
event  class (aspectual initiation) from applying the 
rule in Figure 7 to Line 4 implies that  the concept 
?Afinitor? (a pharmacologic-substance) is not  just 
something tried on the given date, 03/05/08,  but 
something that continued from that date.
4.5 Relation Information Extraction
The relations among extracted concepts (namely, 
conjoined relations between events and set rela-
tions between clinical concepts) also play a key 
role in our approach. When events or clinical con-
cepts are closely linked with such relations, heuris-
tically, they tend to share similar properties that are 
exploited in constructing timelines as described in 
Section 5.
5 Building Timelines from Extracted Results
Extracted clinical concepts, temporal expressions, 
events, and relations (Section 4) are used as a 
2
 While concept classification into certain categories is a very important task in the medical domain, sophisticated 
concept categorization like the one specified in the 2010 i2b2/VA Challenge (https://www.i2b2.org/NLP/Relations/) 
is not the primary goal of this paper. We rather focus on how to associate extracted concepts with other events and 
temporal expressions to build timelines.
(?x1 ?y1 (:* ont::event-time-rel w::until) :val ?val) 
(?x2 ?val (? type2 ont::time-loc) :mod ?mod) 
(?x3 ?mod (? type3 ont::event-time-rel) :displacement 
?displacement) 
(?x4 ?displacement (? type4 ont::quantity) :unit ?unit 
:amount ?amount) 
(?x5 ?amount ont::number :value ?num)
Figure 6: LF patterns to extract a time-span
((?x1 ?y1 (? type1 ONT::SUBSTANCE) :domain-info 
?info :quantifier ?quan)
-extract-substance>
(extraction :type substance :concept ?type1 :umlsinfo 
?info :ont-term ?y1 :quantifier ?quan))
Figure 5: A rule to extract substances
150
foundation to construct timelines that  represent 
patients? medical history. In this section, we pre-
sent  timeline construction processes (as shown in 
Figure 8), using example sentences from Section 2.
Step 1: We first  make connections between events 
and clinical concepts. In the current system, events 
and clinical concepts are extracted in separate rules 
and their relations are not always explicit  in the 
output specification of the rules applied. For in-
stance, Figure 9 shows LFs for the sentence in Line 
7 in a graph format, using simplified LF terms for 
illustration. The clinical concept ?prednisone? and 
the event  ?received? get  extracted by different 
rules and the relation between them is not explicit 
in their output specifications.
To address such a case, for a pair of an event 
and a clinical concept, we traverse LF graphs and 
decide that a relation between them exists if there 
is a path that  goes through certain pre-defined con-
cepts that  do not separate them semantically and 
syntactically (e.g., concepts of measure-units, 
evidence/history, development, and some proposi-
tions).
Step 2: Second, we find temporal expressions as-
sociated with events. This step is relatively 
straightforward. While temporal expressions and 
events get  extracted separately, by investigating 
their LFs, we can decide if a given temporal ex-
pression is a modifier of an event. In Figure 9, the 
time-span-relation (i.e., ?in?) in the dotted-line box 
is a direct modifier of the event ?was treated?.
Step 3: Next, we propagate the association be-
tween events and temporal expressions. That is, 
when the relation between an event and a temporal 
expression is found, we check if the temporal ex-
pression can be associated with additional events 
related to the event  (esp. when the related events 
do not have any associated temporal expression). 
In Figure 9, the event  ?received? does not have a 
temporal expression as a modifier. However, it  is 
conjoined with the event  ?was treated? in the same 
past  tense under the same speech act. Thus, we let 
the event  ?received? share the same temporal ex-
pression with its conjoined event. Here, the con-
joined relation was extracted with relation rules 
described in Section 4.5, which allows us to focus 
on only related events.
Step 4: When temporal expressions do not have 
concrete time values within the expressions, we 
need to designate times for them by looking into 
information in their LFs:
?
Event references: The system needs to find the 
referred event  and gets its time value. For in-
stance, in ?4 weeks after starting Afinitor? (Line 
6),  ?starting Afinitor? refers to a previous event 
in Line 4. The system investigates all events with 
a verb with the same- or sub-type of ont::start 
and Afinitor as its object  (active verbs) or its 
subject (passive verbs). After resolving event 
references, additional time reference or relation 
computation may be required (e.g., computation 
for ?4 weeks after?).
?
Time references: Concrete times for expressions 
like the above example ?N weeks after 
<reference-time>? can be easily computed by 
checking the time displacement  information in 
LFs with the reference time. However, expres-
sions such as ?N days ago?  are based on the 
context of clinical records (e.g., record creation 
(?x1 ?ev (? type1 ont::start) :affected ?affected :tense ?tense :passive ?passive :progressive ?progresive 
  :perfective ?perfective :negation ?negation)
-extract-start-event>
(EVENT :type ?type1 :class (aspectual initiation) :subject ?affected :object null :tense ?tense :passive
   ?passive :progressive ?progresive :perfective ?perfective :negation ?negation :ont-term ?ev)
Figure 7: An event extraction rule example
Inputs: Clinical concepts, Temporal 
Expressions, Events, Relations, LFs
Outputs: Clinical concepts with associated dates 
or timespans.
Steps: 
1. Build links between events and clinical 
concepts
2. Find associated temporal expressions for 
events
3. Propagate temporal expressions through 
relations between events when applicable
4. Compute concrete time values for temporal 
expressions, taking into account the context of 
clinical records
5. Compute time values for clinical concepts 
based on their associated events
Figure 8: Pseudocode for Timeline Construction
151
time). Document creation time is usually repre-
sented as metadata attached to the document  it-
self, or it could be retrieved from a database 
where clinical records are stored. In addition, 
previously mentioned dates or time-spans can be 
referred to using pronouns (e.g., ?at  that/this 
time?). For such expressions, we heuristically 
decide that it refers to the most  recent  temporal 
expression.
?
Time relation: Some temporal expressions have 
directional time relations (e.g., ?until?, ?prior 
to?, and ?after?) specifying intervals with open 
ends. When the ending time of a time span is not 
specified (e.g., ?since 10/2007? in Line 10). We 
heuristically set it from the context of the clinical 
record such as the document creation time.
Step 5: Finally, we designate or compute times on 
or during which the presence or the absence of 
each clinical concept is asserted. Since temporal 
expressions are associated with events, to find time 
values for clinical concepts, we first check the rela-
tions between events and clinical concepts. When 
an event with a concrete time is found for a clinical 
concept, the event?s class is examined. For classes 
such as state and occurrence, the concrete time 
value of the event  is used. In contrast, for an aspec-
tual event, we check its feature (e.g., initiation or 
termination) and look for other aspectual events 
related to the clinical concept and compute a time 
span. For instance, regarding ?Afinitor?, Line 4 
and Line 9 have events with classes (aspectual ini-
tiation) and (aspectual termination) respectively, 
which leads to a time span between the two dates 
in Line 4 and Line 9. Currently, we do not resolve 
conflicting hypotheses. 
Assertion of Presence  or Absence  of Clinical 
Concepts: To check if a certain concept is present 
or not, we take into account quantifier information 
(e.g., none), the negation role values of events, and 
the verb types of events (e.g., ?deny? indicates the 
absence assertion). In addition to such information 
readily available in the output  specifications of the 
clinical concept- and event-extraction rules, we 
also check the path (as in Step 1) that relates the 
clinical concepts and the events, and the quantifiers 
of the concepts in the path are used to compute 
negation values. For instance, given ?The scan 
shows no evidence of lung nodules?, the quantifier 
of the concept  ?evidence? indicates the absence of 
the clinical finding ?lung nodules?.
6 Timeline Results and Discussion 
For the example in Section 2 (Line 1 ~ 12), we ex-
tract all the instances of the clinical concepts and 
the temporal expressions. Out of 23 events, 17 
were extracted. While we missed events such as 
state/was (Line 5), done (Line 8), and walk/do/
swim  (Line 10), our event  extraction rules can be 
extended to cover them if need be.
Figure 10 visualizes the extraction results of the 
example. We use a web widget tool called Simile 
Timeline (www.simile-widgets.org/timeline/). 
Some property values (that  were also extracted by 
rules) are shown alongside some concepts (e.g., 
weight  measurement). Note that not  all extracted 
clinical findings are displayed in Figure 10 because 
we visualize clinical concepts only when they are 
associated with temporal expressions in our LFs. 
For instance, the CT-scan on 05/14/08 in Line 8 is 
not shown because the date was not  associated 
with it due to fragmented LFs from the Parser. 
Figure 9: Graph format LFs of the sentence in Line 7 -- ?She received a 5 day dose pack of 
                  prednisone and was treated with Augmentin in 05/2008.?
152
However, we were still able to extract ?no infil-
trates? and ?scan? from a meaningful fragment.
In addition to the fragmented LF issue, we plan 
to work on temporal reasoning for concepts in the 
sentences without explicit temporal expressions, 
and the current limited event reference resolution 
will be improved. We are also working on evalua-
tion with 48 clinical records from 10 patients. An-
notated results will be created as a gold-standard 
and precision/recall will be measured.
7 Related Work
Temporal information is of crucial importance in 
clinical applications, which is why it  has attracted 
a lot  interest over the last  two decades or more 
(Augusto, 2005). Since so much clinical informa-
tion is still residing in unstructured form, in par-
ticular as text  in the patient?s health record, the last 
decade has seen a number of serious efforts in 
medical NLP  in general (Meystre et  al., 2008) and 
in extracting temporal information from clinical 
text in particular. 
Some of this surge in interest  has been spurred 
by dedicated competitions on extraction of con-
cepts and events from clinical text (such as the 
i2b2 NLP challenges). At the same time, the evolu-
tion of temporal markup languages such as Ti-
meML (Sauri et al, 2006), and temporal 
extraction/inference competitions (such as the two 
TempEval challenges,  Verhagen et  al., 2009) in the 
general area of NLP have led to the development 
of tools such as TARSQI (Verhagen et  al., 2005) 
that could be adapted to the clinical domain.
Although the prevailing paradigm in this area is 
to use superficial methods for extracting and clas-
sifying temporal expressions, it has long been rec-
ognized that higher level semantic processing, in-
cluding discourse-level analysis, would have to be 
performed to get  past the limits of the current  ap-
proaches (cf. Zhou and Hripcsak, 2007). 
Recent attempts to use deeper linguistic features 
include the work of  Bethard et  al. (2007), who 
used syntactic structure in addition to lexical and 
some minor semantic features to classify temporal 
relations of the type we discussed in Section 4.3. 
Savova and her team have also expressed interest 
in testing off-the-shelf deep parsers and semantic 
role labelers for aiding in temporal relation identi-
fication and classification (Savova et al, 2009); 
although we are not  aware of any temporal extrac-
tion results yet, we appreciate their effort in ex-
panding the TimeML annotation schema for the 
clinical domain, as well as their efforts in develop-
ing corpora of clinical text  annotated with temporal 
information.
The work of Mulkar-Mehta et  al. (2009) also 
deserves a mention, even though they apply their 
techniques to biomedical text rather than clinical 
text. They obtain a shallow logical form that repre-
sents predicate-argument relations implicit  in the 
syntax by post-processing the results of a statistical 
parser. Temporal relations are obtained from the 
shallow LF based on a set  of hand-built rules by an 
abductive inference engine.
To our knowledge, however, our system is the 
first  general-purpose NLU system that produces a 
full, deep syntactic and semantic analysis of the 
text as a prerequisite to the extraction and analysis 
of relevant clinical and temporal information.
8 Conclusion
In this paper, we presented a prototype deep natu-
ral language understanding system to construct 
timelines for the medical histories of patients. Our 
approach is generic and extensible to cover a vari-
ety of narrative clinical text  records. The results 
from our system are promising and they can be 
used to support medical decision making.
9 Acknowledgement
This work was supported by the National Cancer 
Institute and the H. Lee Moffitt  Cancer Center and 
Research Institute (Award # RC2CA1488332).
Figure 10: Visualization of timeline results
153
References 
James Allen, Donna Byron, Myroslava Dzikovska, 
George Ferguson, Lucian  Galescu, and Amanda 
Stent.  2000. An architecture for a generic dialogue 
shell. Journal of Natural Language Engineering 
6(3):1?16.
Mary Swift, Nate Blaylock, James Allen, Will de 
Beaumont, Lucian Galescu, and Hyuckchul Jung. 
2010. Augmenting a Deep Natural Language Proc-
essing System with UMLS. Proceedings of the 
Fourth International Symposium on Semantic Mining 
in Biomedicine (poster abstract)
Alan R. Aronson and Fran?ois-Michel Lang.  2010. An 
overview of MetaMap: historical perspective and 
recent advances.  Journal of the American Medical 
Informatics Association. 17:229-236.
Juan C. Augusto. 2005.  Temporal reasoning for decision 
support in medicine. Artificial Intelligence in Medi-
cine, 33(1): 1-24.
Steven Bethard, James H.  Martin, and Sara Klingen-
stein. 2007. Timelines from Text: Identification of 
Syntactic Temporal Relations. In Proceedings of the 
International Conference on Semantic Computing 
(ICSC '07), 11-18.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, Vol. 32.
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Information 
into Information Extraction Systems by Gibbs Sam-
pling.  Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning.  2003. Fast Ex-
act Inference with a Factored Model for Natural Lan-
guage Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press.
S.  M. Meystre, G. K. Savova, K. C. Kipper-Schuler, J. 
F. Hurdle. 2008. Extracting information from textual 
documents in the electronic health record: a review 
of recent research. IMIA Yearbook of Medical Infor-
matics.
George A. Miller. 1995. WordNet: A lexical database for 
English. Communications of the ACM, 38(5).
R. Mulkar-Mehta, J.R. Hobbs, C.-C. Liu, and X.J. Zhou. 
2009. Discovering causal and temporal relations in 
biomedical texts. In AAAI Spring Symposium, 74-
80.
Roser Sauri, Jessica Littman, Bob Knippen, Robert Gai-
zauskas, Andrea Setzer, and James Pustejovsky. 
2006. TimeML annotation guidelines. (available at 
http://www.timeml.org/site/publications/time 
MLdocs/annguide_1.2.1.pdf)
G. Savova, S. Bethard, W. Styler, J. Martin, M. Palmer, 
J. Masanz, and W. Ward. 2009. Towards temporal 
relation discovery from the clinical narrative. Pro-
ceedings of the Annual AMIA Symposium, 568-572.
Michael Stacey and Carolyn McGregor. 2007.  Temporal 
abstraction in intelligent clinical data analysis: A sur-
vey. Artificial Intelligence in Medicine, 39.
Kristina Toutanova and Christopher D. Manning. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings 
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large 
Corpora (EMNLP/VLC-2000).
M. Verhagen, I. Mani, R. Sauri, R. Knippen, S.B. Jang, 
J. Littman, A. Rumshisky, J. Phillips, and 
J. Pustejovsky. 2005. Automating temporal annota-
tion with TARSQI. In Proceedings of the ACL 2005 
on Interactive poster and demonstration sessions 
(ACLdemo '05), 81-84. 
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, 
J. Moszkowicz, and J. Pustejovsky. 2009. The Tem-
pEval challenge: identifying temporal relations in 
text . Language Resources and Evaluation 
 43(2):161-179.
Li Zhou, Carol Friedman, Simon Parsons and George 
Hripcsak. 2005. System Architecture for Temporal 
Information Extraction,  Representation and Reason-
ing in Clinical Narrative Reports. Proceedings of the 
Annual AMIA Symposium.
Li Zhou and George Hripcsak. 2007. Temporal reason-
ing with medical data - A review with emphasis on 
medical natural language processing. Journal of 
Biomedical Informatics, 40.
154
Proceedings of the SIGDIAL 2014 Conference, pages 257?259,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
MVA: The Multimodal Virtual Assistant
Michael Johnston
1
, John Chen
1
, Patrick Ehlen
2
, Hyuckchul Jung
1
, Jay Lieske
2
, Aarthi Reddy
1
,
Ethan Selfridge
1
, Svetlana Stoyanchev
1
, Brant Vasilieff
2
, Jay Wilpon
1
AT&T Labs Research
1
, AT&T
2
{johnston,jchen,ehlen,hjung,jlieske,aarthi,
ethan,sveta,vasilieff,jgw}@research.att.com
Abstract
The Multimodal Virtual Assistant (MVA)
is an application that enables users to plan
an outing through an interactive multi-
modal dialog with a mobile device. MVA
demonstrates how a cloud-based multi-
modal language processing infrastructure
can support mobile multimodal interac-
tion. This demonstration will highlight in-
cremental recognition, multimodal speech
and gesture input, contextually-aware lan-
guage understanding, and the targeted
clarification of potentially incorrect seg-
ments within user input.
1 Introduction
With the recent launch of virtual assistant appli-
cations such as Siri, Google Now, S-Voice, and
Vlingo, spoken access to information and services
on mobile devices has become commonplace. The
Multimodal Virtual Assistant (MVA) project ex-
plores the application of multimodal dialog tech-
nology in the virtual assistant landscape. MVA de-
parts from the existing paradigm for dialog-based
mobile virtual assistants that display the unfold-
ing dialog as a chat display. Instead, the MVA
prototype situates the interaction directly within a
touch-based interface that combines a map with
visual information displays. Users can interact
using combinations of speech and gesture inputs,
and the interpretation of user commands depends
on both map and GUI display manipulation and
the physical location of the device.
MVA is a mobile application that allows users
to plan a day or evening out with friends using
natural language and gesture input. Users can
search and browse over multiple interconnected
domains, including music events, movie show-
ings, and places to eat. They can specify multi-
ple parameters in natural language, such as ?Jazz
concerts around San Francisco next Saturday?. As
users find interesting events and places, they can
be collected together into plans and shared with
others. The central components of the graph-
ical user interface are a dynamic map showing
business and event locations, and an information
display showing the current recognition, system
prompts, search result listing, or plans (Figure 1).
Figure 1: MVA User Interface
Spoken input begins when the user taps a micro-
phone button on the display. As the user speaks,
incremental speech recognition results appear. In
addition to enabling voice input, the microphone
button also activates the map as a drawing can-
vas, and enables the user to combine speech with
drawing in coordinated multimodal commands.
For example, a user might say, ?Movies playing
tonight in this area? while simultaneously outlin-
ing a relevant area on the map. Or a user may say,
?Restaurants? while drawing a line down a spe-
cific street. MVA determines the intent and dis-
ambiguates concepts in the input in order to re-
turn relevant results. MVA then responds to user
input multimodally, by updating the display and
using speech synthesis to summarize results, pro-
vide feedback, or make requests for clarification
and additional information.
257
2 Sample Interaction
In Figure 2 we present a sample of interaction
from MVA that illustrates some of its capabilities.
The user starts with a spoken natural language
query where they specify some constraints: the
type of music (jazz), location (San Francisco), and
time (tomorrow). The system gets low confidence
on the location, so it constructs a targeted clarifi-
cation for clarifying only that constraint. The user
repeats the location, and then the system searches
for events meeting the user?s constraints. The user
then reviews the results, and follows on with a
refinement: ?What about blues??. Even though
many parameters in this query are underspecified,
the system applies contextually-aware natural lan-
guage understanding and interprets this as ?Blues
concerts near San Francisco tomorrow?. After
selecting a concert, the user then searches for a
restaurant nearby. The location of the concert re-
mains salient. The user follows up with a mul-
timodal query combining speech and gesture to
search for similar restaurants in an adjoining area.
U: ?Jazz concerts near San Francisco tomorrow.?
S: ?Where did you want to see jazz tomorrow??
U: ?San Francisco.?
S: ?I found 20 jazz concerts in San
Francisco tomorrow.?
[Zooms map to San Francisco and displays
pins on map and list of results]
U: ?What about blues??
S: ?I found 20 blues concerts in
San Francisco tomorrow.?
U: [Clicks on a concert listing and adds it
to the plan]
U: ?Sushi restaurants near there.?
S: ?I found 10 sushi restaurants.?
U: ?What about here??
[Circles adjoining area on map]
S: ?I found 5 sushi restaurants in
the area you indicated?
Figure 2: Sample Interaction
3 System Architecture
Figure 3 shows the underlying multimodal assis-
tant architecture supporting the MVA app. The
user interacts with a native iOS client. When the
user taps the microphone icon, this initiates the
flow of audio interleaved with gesture and context
information streamed over a WebSocket connec-
tion to the platform.
This stream of interleaved data is handled at
the server by a multimodal natural language pro-
cessing pipeline. This fields incoming packets of
Figure 3: MVA Multimodal assistant Architecture
data from the client, demuxes the incoming data
stream, and sends audio, ink traces, and context
information to three modules that operate in par-
allel. The audio is processed using the AT&T
Watson
SM
speech recognition engine (Goffin et
al., 2005). Recognition is performed using a dy-
namic hierarchical language model (Gilbert et al.,
2011) that combines a statistical N-gram language
model with weighted sub-grammars. Ink traces
are classified into gestures using a linear classi-
fier. Speech recognition results serve as input to
two NLUmodules. A discriminative stochastic se-
quence tagger assigns tags to phrases within the
input, and then the overall string with tags is as-
signed by a statistical intent classifier to one of
a number of intents handled by the system e.g.
search(music event), refine(location).
The NLU results are passed along with gesture
recognition results and the GUI and device context
to a multimodal dialog manager. The contextual
resolution component determines if the input is a
query refinement or correction. In either case, it
retrieves the previous command from a user con-
text store and combines the new content with the
context through destructive unification (Ehlen and
Johnston, 2012). A location salience component
then applies to handle cases where a location is
not specified verbally. This component uses a su-
pervised classifier to select from among a series
of candidate locations, including the gesture (if
present), the current device location, or the current
map location (Ehlen and Johnston, 2010).
The resolved semantic interpretation of the ut-
terance is then passed to a Localized Error Detec-
tion (LED) module (Stoyanchev et al., 2012). The
LEDmodule contains two maximum entropy clas-
sifiers that independently predict whether a con-
258
cept is present in the input, and whether a con-
cept?s current interpretation is correct. These clas-
sifiers use word scores, segment length, confu-
sion networks and other recognition and context
features. The LED module uses these classifiers
to produce two probability distributions; one for
presence and one for correctness. These distri-
butions are then used by a Targeted Clarification
component (TC) to either accept the input as is,
reject all of the input, or ask a targeted clarifica-
tion question (Stoyanchev et al., 2013). These de-
cisions are currently made using thresholds tuned
manually based on an initial corpus of user inter-
action withMVA. In the targeted clarification case,
the input is passed to the natural language gen-
eration component for surface realization, and a
prompt is passed back to the client for playback
to the user. Critically, the TC component decides
what to attempt to add to the common ground
by explicit or implicit confirmation, and what to
explicitly query from the user; e.g. ?Where did
you want to see jazz concerts??. The TC com-
ponent also updates the context so that incoming
responses from the user can be interpreted with re-
spect to the context set up by the clarification.
Once a command is accepted by the multimodal
dialog manager, it is passed to the Semantic Ab-
straction Layer (SAL) for execution. The SAL in-
sulates natural language dialog capabilities from
the specifics of any underlying external APIs that
the system may use in order to respond to queries.
A general purpose time normalization component
projects relative time expressions like ?tomorrow
night? or ?next week? onto a reference timeframe
provided by the client context and estimates the
intended time interval. A general purpose location
resolution component maps from natural language
expressions of locations such as city names and
neighborhoods to specific geographic coordinates.
These functions are handled by SAL?rather than
relying on any time and location handling in the
underlying information APIs?to provide consis-
tency across application domains.
SAL also includes mechanisms for category
mapping; the NLU component tags a portion
of the utterance as a concept (e.g., a mu-
sic genre or a cuisine) and SAL leverages
this information to map a word sequence to
generic domain-independent ontological represen-
tations/categories that are reusable across different
backend APIs. Wrappers in SAL map from these
categories, time, and location values to the spe-
cific query language syntax and values for each
specific underlying API. In some cases, a single
natural language query to MVA may require mul-
tiple API calls to complete, and this is captured
in the wrapper. SAL also handles API format dif-
ferences by mapping all API responses into a uni-
fied format. This unified format is then passed to
our natural language generation component to be
augmented with prompts, display text, and instruc-
tions to the client for updating the GUI. This com-
bined specification of a multimodal presentation is
passed to the interaction manager and routed back
to the client to be presented to the user.
In addition to testing the capabilities of our mul-
timodal assistant platform, MVA is designed as a
testbed for running experiments with real users.
Among other topics, we are planning experiments
with MVA to evaluate methods of multimodal in-
formation presentation and natural language gen-
eration, error detection and error recovery.
Acknowledgements
Thanks to Mike Kai and to Deepak Talesra for
their work on the MVA project.
References
Patrick Ehlen and Michael Johnston. 2010. Location
grounding in multimodal local search. In Proceed-
ings of ICMI-MLMI, pages 32?39.
Patrick Ehlen and Michael Johnston. 2012. Multi-
modal dialogue in mobile local search. In Proceed-
ings of ICMI, pages 303?304.
Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Dia-
mantino Caseiro, Vincent Goffin, Andrej Ljolje,
Mike Phillips, Chao Wang, and Jay G. Wilpon.
2011. Your mobile virtual assistant just got smarter!
In Proceedings of INTERSPEECH, pages 1101?
1104. ISCA.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy,
Mazim Rahim, Giuseppe Riccardi, and Murat Sar-
aclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP, pages 1033?1036,
Philadelphia, PA, USA.
Svetlana Stoyanchev, Philipp Salletmayer, Jingbo
Yang, and Julia Hirschberg. 2012. Localized de-
tection of speech recognition errors. In Proceedings
of SLT, pages 25?30.
Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg.
2013. Modelling human clarification strategies. In
Proceedings of SIGDIAL 2013, pages 137?141.
259
