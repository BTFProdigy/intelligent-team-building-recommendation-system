Dialogue Interaction with the DARPA Communicator
Infrastructure: The
Development of Useful Software
Samuel Bayer
The MITRE Corporation
 202 Burlington Rd.
Bedford, MA 01730
sam@mitre.org
Christine Doran
The MITRE Corporation
 202 Burlington Rd.
Bedford, MA 01730
cdoran@mitre.org
Bryan George
The MITRE Corporation
11493 Sunset Hills Rd.
Reston, VA 20190
bgeorge@mitre.org
ABSTRACT
To support engaging human users in robust, mixed-initiative
speech dialogue interactions which reach beyond current
capabilities in dialogue systems, the DARPA Communicator
program [1] is  funding the development of a distributed
message-passing infrastructure for dialogue systems which all
Communicator participants are using. In this presentation, we
describe the features of and requirements for a genuinely
useful software infrastructure for this purpose.
Keywords
Spoken dialogue, speech interfaces
1. INTRODUCTION
Over the last five years, three technological advances have
cooperated to push speech-enabled dialogue systems back
into the limelight: the availability of robust real-time speech
recognition tools, the explosion of Internet-accessible
information sources, and the proliferation of mobile
information access devices such as cell phones. However, the
systems being fielded, and the standards arising from these
efforts, represent only a limited set of capabilities for robust
voice-enabled interaction with knowledge sources. The most
prominent indication of these limitations is the fact that these
systems are overwhelmingly system-directed; the system asks
a question, and the user responds. While this type of
interactions sidesteps a number of problems in speech
recognition and dialogue tracking, it is overwhelmingly likely
that these restrictions are not manageable in the long term.
The DARPA Communicator program [1] is exploring how to
engage human users in robust, mixed-initiative speech
dialogue interactions which reach beyond current capabilities
in dialogue systems. To support this exploration, the
Communicator program has funded the development of a
distributed message-passing infrastructure for dialogue
systems which all Communicator participants are using. In
this presentation, we describe the features of and requirements
for a genuinely useful software infrastructure for this purpose.
2. BUILDING USEFUL SOFTWARE
The Galaxy Communicator software infrastructure (GCSI) is an
elaboration and extension of MIT's Galaxy-II distributed
infrastructure for dialogue interaction [3]. The fact that all
program participants are required to use the GCSI imposes a
somewhat more severe set of requirements on the infrastructure
than usual, and these requirements range far beyond the
straightforward considerations of functionality.
? Flexibility: the infrastructure should be flexible enough
to encompass the range of interaction strategies that the
various Communicator sites might experiment with
? Obtainability: the infrastructure should be easy to get
and to install
? Learnability: the infrastructure should be easy to learn to
use
? Embeddability: the infrastructure should be easy to
embed into other software programs
? Maintenance: the infrastructure should be supported and
maintained for the Communicator program
? Leverage: the infrastructure should support longer-term
program and research goals for distributed dialogue
systems
3. FLEXIBILITY
The GCSI is a distributed hub-and-spoke architecture based on
message-passing. The hub of the GCSI incorporates a scripting
mechanism that allows the programmer to take control of the
message traffic by implementing "hub programs" in a simple
scripting language. The benefits of this sort of infrastructure
are considerable in the context of exploring different
interaction and control strategies for dialogue. For example:
? Because the infrastructure is based on message-passing
instead of APIs, there's no need for the hub to have any
compile-time knowledge of the functional properties of
the servers it communicates with (in contrast to, for
instance, a CORBA infrastructure).
? Because the hub scripting allows the programmer to alter
the flow of control of messages, it's possible to integrate
servers with a variety of implicit interaction paradigms
(e.g., synchronous vs. asynchronous) without modifying
the servers themselves
? Because the hub scripting allows the programmer to alter
the flow of control of messages, it's possible to insert
simple tools and filters to convert data among formats
without modifying the servers themselves.
? Because the hub scripting language fires rules based on
aspects of the hub state, it's easy to write programs which
modify the message flow of control in real time.
4. OBTAINABILITY
We believe that the simplest licensing and distribution model
for software like the GCSI is an open source model. With the
appropriate open source licensing properties, there are no
barriers to freely distributing and redistributing the GCSI, or
to distributing dialogue systems created using the GCSI, or to
building commercial products based on it. The GCSI i s
distributed under a modified version of the MIT X Consortium
license, and we are reasonably certain that the license
simplifies all these tasks. In particular, two Communicator
sites are planning to distribute their entire dialogue systems
as open source, which would not be possible without
appropriate licensing of the GCSI.
It's also important to address the level of complexity of
installing the software once it's obtained. Research software i s
notoriously hard to install, and it's far more useful to ensure
that the software can be used straightforwardly on a small
number of common platforms and operating systems than to
try to make it run on as many platforms as possible. We've
targeted the three platforms which the program participants
were developing on: Windows NT, Intel Linux, and Sparc
Solaris. The GCSI is known to work or to have worked on other
configurations (HP-UX and SGI IRIX, for instance), but these
configurations are not supported in any meaningful way. The
open source model can help here, too: if someone wants to port
the infrastructure to a BSD OS, for instance, they have all the
source (and will hopefully contribute their modifications to
the open source code base).
5. LEARNABILITY
Once the software is installed, it's important to know where to
start and how to proceed. We have offered a series of two-day
intensive training courses on the Communicator infrastructure
which have been attended by the majority of Communicator
participants. In addition, the GCSI comes with extensive
documentation and examples, including a toy end-to-end
dialogue system example which illustrates one possible
configuration of Communicator-compliant servers. Our goal i s
to ensure that it's possible to learn to use the Communicator
infrastructure from the documentation alone, and at least two
sites have succeeded in creating dialogue systems using the
GCSI in a short period of time without attending our training
course.
6. EMBEDDABILITY
The GCSI includes libraries and templates to create
Communicator-compliant servers in C, Java, Python, and
Allegro Common Lisp. However, it's not enough to provide a
software library; this library has to be well-behaved in a
number of ways. In particular, if the GCSI is to be used in
conjunction with CORBA or various windowing systems, i t
must be possible to embed the GCSI server libraries into other
main loops, and to control all the features of the GCSI without
controlling the toplevel flow of control. To enable this goal,
the GCSI is based on a straightforward event-based
programming model, which is used to implement the default
Communicator server main loop, as well as the implementation
of the Python and Allegro server libraries. The GCSI i s
distributed with a number of examples illustrating this
embedding.
7. MAINTENANCE
Finally, GCSI consumers must be able to rely on getting help
when something goes wrong, and expect that design and
implementation problems will be rectified and that desired
complex behaviors will be supported. The importance of
responsiveness and flexibility in maintenance is one of the
reasons we prefer the GCSI for Communicator instead of a
third-party tool such as SRI's Open Agent Architecture [2],
which the Communicator program does not control the
development of.
In addition to maintaining a bug queue for the GCSI, we have
addressed successively more complicated infrastructure
requirements in successive releases of the GCSI. For instance,
in the most recent release (3.0), we addressed infrastructure
support for asynchronous delegation strategies being
explored by the Communicator effort at MIT and issues
relating to consumption of audio input by multiple
recognizers.
8. LEVERAGE
Ultimately, we hope that the GCSI, together with open-source
servers such as recognizers, parsers, synthesizers and dialogue
modules provided by application developers, will foster a
vigorous explosion of work in speech-enabled dialogue
systems. For example:
? The programming-language-independent nature of the
GCSI message-passing paradigm allows the
Communicator program to develop implementation-
independent service standards for recognition, synthesis,
and other better-understood resources.
? The freely available nature of the GCSI allows application
developers to contribute dialogue system modules which
are already configured to work with other components.
? The availability of an "environment" for dialogue system
development will support the development of an open
source "toolkit" of state-of-the art, freely available
modules. A number of Communicator sites are already
releasing such modules.
? A common infrastructure will contribute to the
elaboration of "best practice" in dialogue system
development.
There are certainly a number of emerging and existing
alternatives to the GCSI for dialogue system development
(SRI's Open Agent Architecture, for instance). However, we
believe that the combination of a software package like the
GCSI and the critical mass generated by its use in the DARPA
Communicator program presents a unique opportunity for
progress in this area.
The GCSI is available under an open source license at
http://fofoca.mitre.org/download.
9. ACKNOWLEDGMENTS
This work was funded by the DARPA Communicator program
under contract number DAAB07-99-C201.  ? 2001 The MITRE
Corporation. All rights reserved.
10. REFERENCES
[1] http://www.darpa.mil/ito/research/com/index.html.
[2] D. L. Martin, A. J. Cheyer, and D. B. Moran. The
open agent architecture: A framework for building
distributed software systems. Applied Artificial
Intelligence, vol. 13, pp. 91--128, January-March
1999.
[3] S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. Galaxy-II: A Reference Architecture for
Conversational System Development. Proc. ICSLP
98, Sydney, Australia, November 1998.
Exploring Speech-Enabled Dialogue with the Galaxy
Communicator Infrastructure
Samuel Bayer
The MITRE Corporation
 202 Burlington Rd.
Bedford, MA 01730
sam@mitre.org
Christine Doran
The MITRE Corporation
 202 Burlington Rd.
Bedford, MA 01730
cdoran@mitre.org
Bryan George
The MITRE Corporation
11493 Sunset Hills Rd.
Reston, VA 20190
bgeorge@mitre.org
ABSTRACT
This demonstration will motivate some of the significant
properties of the Galaxy Communicator Software Infrastructure
and show  how  they support the goals of the DARPA
Communicator program.
Keywords
Spoken dialogue, speech interfaces
1. INTRODUCTION
The DARPA Communicator program [1], now in its second
fiscal year, is intended to push the boundaries of speech-
enabled dialogue systems by enabling a freer interchange
between human and machine. A crucial enabling technology
for the DARPA Communicator program is the Galaxy
Communicator software infrastructure (GCSI), which provides
a common software platform for dialogue system development.
This infrastructure was initially designed and constructed by
MIT [2], and is now maintained and enhanced by the MITRE
Corporation. This demonstration will motivate some of the
significant properties of this infrastructure and show how they
support the goals of the DARPA Communicator program.
2. HIGHLIGHTED PROPERTIES
The GCSI is a distributed hub-and-spoke infrastructure which
allows the programmer to develop Communicator-compliant
servers in C, C++, Java, Python, or Allegro Common Lisp. This
system is based on message passing rather than CORBA- or
RPC-style APIs. The hub in this infrastructure supports
routing of messages consisting of key-value pairs, but also
supports logging and rule-based scripting. Such an
infrastructure has the following desirable properties:
? The scripting capabilities of the hub allow the
programmer to weave together servers which may not
otherwise have been intended to work together, by
rerouting messages and their responses and transforming
their keys.
? The scripting capabilities of the hub allow the
programmer to insert simple tools and filters to convert
data among formats.
? The scripting capabilities of the hub make it easy to
modify the message flow of control in real time.
? The scripting capabilities of the hub and the simplicity of
message passing make it simple to build up systems bit
by bit.
? The standard infrastructure allows the Communicator
program to develop platform- and programming-
language-independent service standards for recognition,
synthesis, and other better-understood resources.
? The standard infrastructure allows members of the
Communicator program to contribute generally useful
tools to other program participants.
This demonstration will illustrate a number of these
properties.
3. DEMO CONFIGURATION AND
CONTENT
By way of illustration, this demo will simulate a process of
assembling a Communicator-compliant system, while at the
same time exemplifying some of the more powerful aspects of
the infrastructure. The demonstration has three phases,
representing three successively more complex configuration
steps. We use a graphical display of the Communicator hub to
make it easy to see the behavior of this system.
As you can see in Figure 1, the hub is connected to eight
servers:
? MITRE's Java Desktop Audio Server (JDAS)
? MIT SUMMIT recognizer, using MIT's Mercury travel
domain language model
? CMU Sphinx recognizer, with a Communicator-compliant
wrapper written by the University of Colorado Center for
Spoken Language Research (CSLR), using CSLR's travel
domain language model
? A string conversion server, for managing
incompatibilities between recognizer output and
synthesizer input
? CSLR's concatenative Phrase TTS synthesizer, using their
travel domain voice
? CMU/Edinburgh Festival synthesizer, with a
Communicator-compliant wrapper written by CSLR, using
CMU's travel domain language model for Festival's
concatenative voice
? MIT TINA parser, using MIT's Mercury travel domain
language model
? MIT Genesis paraphraser, using MIT's Mercury travel
domain language model
Figure 1: Initial demo configuration
We will use the flexibility of the GCSI, and the hub scripting
language in particular, to change the path that messages follow
among these servers.
3.1 Phase 1
In phase 1, we establish audio connectivity. JDAS is MITRE's
contribution to the problem of reliable access to audio
resources. It is based on JavaSound 1.0 (distributed with JDK
1.3), and supports barge-in. We show the capabilities of JDAS
by having the system echo the speaker's input; we also
demonstrate the barge-in capabilities of JDAS bye showing
that the speaker can interrupt the playback with a new
utterance/input. The goal in building JDAS is that anyone who
has a desktop microphone and the Communicator
infrastructure will be able to use this audio server to establish
connectivity with any Communicator-compliant recognizer or
synthesizer.
3.2 Changing the message path
The hub maintains a number of information states. The
Communicator hub script which the developer writes can both
access and update these information states, and we can invoke
"programs" in the Communicator hub script by sending
messages to the hub. This demonstration exploits this
capability by using messages sent from the graphical display
to change the path that messages follow, as illustrated in
Figure 2. In phase 1, the hub script routed messages from JDAS
back to JDAS (enabled by the message named "Echo"). In the
next phase, we will change the path of messages from JDAS
and send them to a speech recognizer.
Figure 2: Modifying the hub information state
3.3 Phase 2
Now that we've established audio connectivity, we can add
recognition and synthesis. In this configuration, we will route
the output of the preferred recognizer to the preferred
synthesizer. When we change the path through the hub script
using the graphical display, the preferred servers are
highlighted. Figure 3 shows that the initial configuration of
phase 2 prefers SUMMIT and Festival.
Figure 3:  Initial recognition/synthesis configuration
The SUMMIT recognizer and the Festival synthesizer were not
intended to work together; in fact, while there is a good deal of
activity in the area of establishing data standards for various
aspects of dialogue systems (cf. [3]), there are no
programming-language-independent service definitions for
speech. The hub scripting capability, however, allows these
tools to be incorporated into the same configuration and to
interact with each other. The remaining incompatibilities (for
instance, the differences in markup between the recognizer
output and the input the synthesizer expects) are addressed by
the string server, which can intervene between the recognizer
and synthesizer. So the GCSI makes it easy both to connect a
variety of tools to the hub and make them interoperate, as well
as to insert simple filters and processors to facilitate the
interoperation.
In addition to being able to send general messages to the hub,
the user can use the graphical display to send messages
associated with particular servers. So we can change the
preferred recognizer or synthesizer. (as shown in Figure 4), or
change the Festival voice (as shown in Figure 5). All these
messages are configurable from the hub script.
Figure 4: Preferring a recognizer
Figure 5: Changing the Festival voice
3.4 Phase 3
Now that we've established connectivity with recognition and
synthesis, we can add parsing and generation (or, in this case,
input paraphrase). Figure 6 illustrates the final configuration,
after changing recognizer and synthesizer preferences. In this
phase, the output of the recognizer is routed to the parser,
which produces a structure which is then paraphrased and then
sent to the synthesizer. So for instance, the user might say "I'd
like to fly to Tacoma", and after parsing and paraphrase, the
output from the synthesizer might be "A trip to Tacoma".
Figure 6: Adding parsing and paraphrase
4. CONCLUSION
The configuration at the end of phase 3 is obviously not a
complete dialogue system; this configuration is missing
context management and dialogue control, as well as an
application backend, as illustrated by the remaining
components in white in Figure 7. However, the purpose of the
demonstration is to illustrate the ease of plug-and-play
experiments within the GCSI, and the role of these capabilities
to assemble and debug a complex Communicator interface. The
GCSI is available under an open source license at
http://fofoca.mitre.org/download    .
Figure 7: A sample full dialogue system configuration
5. ACKNOWLEDGMENTS
This work was funded by the DARPA Communicator program
under contract number DAAB07-99-C201.  ? 2001 The MITRE
Corporation. All rights reserved.
6. REFERENCES
[1] http://www.darpa.mil/ito/research/com/index.html.
[2] S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. Galaxy-II: A Reference Architecture for
Conversational System Development. Proc. ICSLP
98, Sydney, Australia, November 1998.
[3] "'Voice Browser' Activity." http://www.w3.org/Voice.
Finding Errors Automatically in
Semantically Tagged Dialogues
John Aberdeen, Christine Doran, Laurie Damianos,
Samuel Bayer and Lynette Hirschman
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730 USA
+1.781.271.2000
{aberdeen,cdoran,laurie,sam,lynette}@mitre.org
ABSTRACT
We describe a novel method for detecting errors in task-based
human-computer (HC) dialogues by automatically deriving
them from semantic tags. We examined 27 HC dialogues from
the DARPA Communicator air travel domain, comparing user
inputs to system responses to look for slot value
discrepancies, both automatically and manually. For the
automatic method, we labeled the dialogues with semantic tags
corresponding to "slots" that would be filled in "frames" in
the course of the travel task. We then applied an automatic
algorithm to detect errors in the dialogues. The same dialogues
were also manually tagged (by a different annotator) to label
errors directly. An analysis of the results of the two tagging
methods indicates that it may be possible to detect errors
automatically in this way, but our method needs further work
to reduce the number of false errors detected. Finally, we
present a discussion of the differing results from the two
tagging methods.
Keywords
Dialogue, Error detection, DARPA Communicator.
1. INTRODUCTION
In studying the contrasts between human-computer (HC) and
human-human (HH) dialogues [1] it is clear that many HC
dialogues are plagued by disruptive errors that are rarely seen
in HH dialogues. A comparison of HC and HH dialogues may
help us understand such errors. Conversely, the ability to
detect errors in dialogues is critical to understanding the
differences between HC and HH communication.
Understanding HC errors is also crucial to improving HC
interaction, making it more robust, trustworthy and efficient.
The goal of the work described in this paper is to provide an
annotation scheme that allows automatic calculation of
misunderstandings and repairs, based on semantic information
presented at each turn. If we represent a dialogue as a sequence
of pairs of partially-filled semantic frames (one for the user?s
utterances, and one for the user?s view of the system state), we
can annotate the accumulation and revision of information in
the paired frames.  We hypothesized that, with such a
representation, it would be straightforward to detect when the
two views of the dialogue differ (a misunderstanding), where
the difference originated (source of error), and when the two
views reconverge (correction). This would be beneficial
because semantic annotation often is used for independent rea-
sons, such as measurements of concepts per turn [8],
information bit rate [9], and currently active concepts [10].
Given this, if our hypothesis is correct, then by viewing
semantic annotation as a representation of filling slots in user
and system frames, it should be possible to detect errors
automatically with little or no additional annotation.
2. SEMANTIC TAGGING
We tagged 27 dialogues from 4 different systems that
participated in a data collection conducted by the DARPA
Communicator  program in the summer of 2000. These are
dialogues between paid subjects and spoken language
dialogue systems operating in the air travel domain. Each
dialogue was labeled with semantic tags by one annotator. We
focused on just the surface information available in the
dialogues, to minimize inferences made by the annotator.
The semantic tags may be described along two basic
dimensions: slot and type. The slot dimension describes the
items in a semantic frame that are filled over the course of a
dialogue, such as DEPART_CITY and AIRLINE (see Table 1 for
the complete list).
The type dimension describes whether the tag is a PROMPT, a
FILL, or an OFFER. This type dimension is critical to semantic
analysis since it allows one to describe the effect a tag has on
slots in the frame. PROMPTs are attempts to gather values to
fill slots, e.g., "what city do you want to fly to". FILLs are
actual slot fills, e.g., "I?d like to fly to San Francisco". OFFERs
represent actual flight information based on previous slot
FILLs, e.g., "there is a 9:45 flight to San Francisco on Delta".
However, OFFERs often do not exactly match slot FILLs (e.g.,
the user requests a flight at 9:30, but the closest match flight
is at 9:45), and thus must be distinguished from FILLs.
In addition to the two basic dimensions of slot and type, each
tag takes a leg attribute to indicate which leg of a trip is being
discussed. There is also an initial USER_ID slot which has two
types (PROMPT_USER_ID and FILL_USER_ID), but no leg
attribute.
Our semantic tag set alo includes two special tags, YES and
NO, for annotating responses to offers and yes/no questions.
Finally, we have two tags, PROMPT_ERASE_ FRAMES and
FILL_ERASE_FRAMES, for annotating situations where the
frames are erased and the dialogue is restarted (e.g., the user
says "start over"). Figure 1 shows part of a sample dialogue
with semantic tags. Our semantic tagset is summarized in Table
1.
Table 1. Semantic Tagset
PROMPT FILL OFFER
DEPART_CITY X X X
ARRIVE_CITY X X X
DEPART_AIRPORT X X X
ARRIVE_AIRPORT X X X
DATE X X X
DEPART_TIME X X X
ARRIVE_TIME X X X
AIRLINE X X X
USER_ID X X
ERASE_FRAMES X X
YES (single bare tag)
NO (single bare tag)
3. ERROR DETECTION
To provide a baseline for comparison to an algorithm that
detects errors automatically, we had an annotator (not the same
person who did the semantic tagging described above)
manually tag the problem areas. This annotator marked four
items:
(1) occurrence: where the problem first occurs in the
dialogue (e.g. where the user says the item which the
system later incorporates incorrectly)
(2) detection: where the user could first be aware that
there is a problem (e.g. where the system reveals its
mistake)
(3) correction attempt: where the user attempts to repair
the error
(4) correction detection: where the user is first able to
detect that the repair has succeeded
We next developed an algorithm for automatically finding
errors in our semantically tagged dialogues. In this phase of
the research, we concentrated on deriving an automatic method
for assigning the first two of the four error categories,
occurrence and detection (in a later phase we plan to develop
automatic methods for correction attempt and correction
detection). First, the algorithm derives the turn-by-turn frame
states for both the user's utterances and the system's utterances
(i.e., what the user heard the system say), paying special
attention to confirmation tags such as YES or deletion tags
like FILL_ERASE_FRAMES. Then, the algorithm compares
patterns of user and system events to hypothesize errors.
Occurences and detections are hypothesized for three types of
errors: hallucinations (system slot fill without user slot fill),
mismatches (system slot fill does not match user slot fill), and
prompts after fills (system prompt after user slot fill).
Figure 2 shows a sample dialogue that illustrates several error
types. Utterance S12 shows a prompt after fill error ? the user
has already supplied (in utterance U11) the information the
system is requesting. In utterance U13 the user supplies
contradictory information, and the system catches this and
tries to resolve it in utterances S14 and S16. Next a mismatch
error is illustrated ? the user specifies ARRIVE_CITY in
utterance U17, and the system shows that it has misrecognized
Figure 1. Portion of a Sample Dialogue Labeled with Semantic Tags
 S0: System said: Welcome to our travel planning system.
<prompt_user_id>Please say your PIN number</prompt_user_id>.
U1: User said: My PIN is <fill_user_id>four four four four</fill_user_id>.
 S2: Sys tem said: What are your travel plans?
U3: User said: I want to go to <fill_arrive_city leg=?1?>San Francisco</fill_arrive_city>.
 S4: System said: Traveling to <fill_arrive_city leg=?1?>San Francisco</fill_arrive_city>.
<prompt_depart_city leg=?1?>Where are you departing from?</prompt_depart_city>
U5: User said: <fill_depart_city leg=?1?>Boston</fill_depart_city>.
 S6: System said: <prompt_date leg=?1?>When would you like to leave</prompt_date>
<fill_depart_city leg=?1?>Boston</fill_depart_city>?
U7: User said: <fill_date leg=?1?>July twenty-fifth</fill_date> at
<fill_depart_time leg=?1?>12 noon</fill_depart_time>.
 S8: System said: I have a <offer_airline leg=?1?>Delta</offer_airline> flight that leaves
<offer_depart_city leg=?1?>Boston</offer_depart_city> at
<offer_depart_time leg=?1?>11:10AM</offer_depart_time> on
<offer_date leg=?1?>July twenty-fifth</offer_date> and arrives in
<offer_arrive_city leg=?1?>San Francisco</offer_arrive_city> at
<offer_arrive_time>5:19PM</offer_arrive_time>. Is that OK?
it in utterance S18. The user attempts to correct this
misrecognition in utterance U21, and as can be seen from
utterance S22, the system again has misrecognized the user?s
utterance.
Below we describe the results from running the automatic
algorithm on our 27 semantically tagged dialogues.
4. RESULTS
In the 27 dialogues considered, a total of 131 items were
flagged by one or both of the methods as error items (60 occur,
71 detect). A breakdown of these errors and which method
found them is in Table 2.
Table 2. Unique Errors Identified
# errors found by: Occur Detect Total
Both Methods 14 23 37
Automatic Only 28 38 66
Manual Only 18 10 28
Totals 60 71 131
As can be seen in Table 2 the automatic method flagged many
more items as errors than the manual method.
Table 3. Error Judgements
Occur Detect
E NE Q E NE Q
Auto 48% 40% 12% 52% 38% 10%
Man 84% 13% 3% 82% 15% 3%
We carefully examined each of the items flagged as errors by
the two methods. Three judges (the semantic tagging
annotator, the manual error tagging annotator, and a third
person who did not participate in the annotation) determined
which of the errors found by each of the two methods were real
errors (E), not real errors (NE), or questionable (Q). For
calculations in the present analysis, we used E as the baseline
of real errors, rather than E+Q. Table 3 shows the judgements
made for both the automatic and manual method, which are
discussed in the next section. It is important to note that
human annotators do not perform this task perfectly, with error
rates of 13% and 15%. This is also shown in the precision and
recall numbers for the two methods in Table 4.
Table 4. Precision & Recall
Occur DetectPrecision
& Recall P R P R
Automatic 0.48 0.57 0.52 0.84
Manual 0.84 0.77 0.82 0.71
5. ANALYSIS
The automatic method flagged 40 items as errors that the
judges determined were not errors (17 occur, 23 detect). These
40 false errors can be classified as follows:
A. 10 were due to bugs in the algorithm or source data
B. 19 were false errors that can be eliminated with non-
trivial changes to the semantic tagset and/or algorithm
C. 3 were false errors that could not be eliminated
without the ability to make inferences about world
knowledge
D. 8 were due to mistakes made by the semantic
annotator
One example of the 19 false errors above in B is when the first
user utterance in a dialogue is a bare location, it is unclear
whether the user intends it to be a departure or arrival location.
Our semantic tagset currently has no tags for ambiguous
situations such as these. Adding underspecified tags to our
tagset (and updating the automatic algorithm appropriately)
would solve this problem. Another example is a situation
where a system was legitimately asking for clarification about
a slot fill, but the algorithm flagged it as prompting for keys
that had already been filled. This could be fixed by adding a
CLARIFY element to the type dimension (currently PROMPT,
FILL, and OFFER). We believe that making these changes
would not compromise the generality of our semantic tagset.
However, as the point of our approach is to derive errors
without much additional annotation, additions to the semantic
tagset should only be made when there is substantial
justification.
There were also 21 errors (15 occur, 6 detect) that were not
detected by the automatic method, but were judged as real
errors. These 21 errors may be categorized as follows:
A. 2 were due to bugs in the algorithm
B. 8 were situations where the algorithm correctly
flagged the detect point of an error, but missed the
associated occur point
C. 6 were situations that could be fixed by
modifications to the semantic tagset
D. 1 was an error that could be fixed either by a
revision to the semantic tagset or a revision to the
algorithm
E. 2 were situations where the system ignored a user
fill, and the automatic algorithm interpreted it as no
confirmation (not an error). Human judgement is
required to detect these errors
F. 2 were due to mistakes made by the semantic
annotator
6. PREVIOUS WORK
In Hirschman & Pao [5], annotation was done by manual
inspection of the exchanges in the dialogue. Each exchange
was evaluated based on the portion of information "visible to
the other party". Errors and problems were identified manually
and traced back to their point of origin. This is quite similar to
our baseline manual annotation described in section 3.
There have been other approaches to detecting and
characterizing errors in HC dialogues. Danieli [2] used
expectations to model future user ut terances, and Levow [6][7]
used utterance and pause duration, as well as pitch variability
to characterize errors and corrections. Dybkj?r, Bernsen &
Dybkj?r [4] developed a set of principles of cooperative HC
dialogue, as well as a taxonomy of errors typed according to
which of the principles are violated. Finally, Walker et. al.
[11][12] have trained an automatic classifier that identifies
and predicts problems in HC dialogues.
7. DISCUSSION
It is clear that our algorithm and semantic tagset, as they stand
now, need improvements to reduce the number of false errors
detected. However, even now the automatic method offers some
advantages over tagging errors manually, the most important
of which is that many researchers already annotate their
dialogues with semantic tags for other purposes and thus
many errors can be detected with no additional annotation.
Also, the automatic method associates errors with particular
slots, enabling researchers to pinpoint aspects of their
dialogue management strategy that need the most work.
Finally, Day et. al. [3] have shown that correcting existing
annotations is more time efficient than annotating from
scratch. In this way, the automatic method may be used to
"seed" an annotation effort, with later hand correction.
8. ACKNOWLEDGMENTS
This work was funded by the DARPA Communicator program
under contract number DAAB07-99-C201.  ? 2001 The MITRE
Corporation. All rights reserved.
9. REFERENCES
[1] Aberdeen, J. and Doran, C. Human-computer and human-
human dialogues. DARPA Communicator Principle
Investigators Meeting (Philadelphia, PA USA 2000).
http://www.dsic-web.net/ito/meetings/communicator
_sep2000/
[2] Danieli, M. On the use of expectations for detecting and
repairing human-machine miscommunication.
Proceedings of AAAI Workshop on Detecting, Repairing
and Preventing Human-Machine Miscommunication
(Portland OR, USA 1996).
[3] Day, D., Aberdeen, J., Hirschman, L., Kozierok, R.,
Robinson, P. and Vilain, M. Mixed-initiative
development of language processing systems. In
Proceedings of the Fifth Conference on Applied Natural
Language Processing (Washington DC, USA 1997).
[4] Dybkj?r, L., Bernsen, N.O. and Dybkj?r, H. Reducing
miscommunication in spoken human-machine dialogue.
Proceedings of AAAI Workshop on Detecting, Repairing
and Preventing Human-Machine Miscommunication
(Portland OR, USA 1996).
[5] Hirschman, L. and Pao, C. The cost of errors in a spoken
language system. Proceedings of the Third European
Figure 2. Portion of a Sample Dialogue Illustrating Several Errors
   U11: User said: I want to fly from <fill_depart_city leg=?1?>Seattle</fill_depart_city> to
<fill_arrive_city leg=?1?>Tokyo</fill_arrive_city>
     S12: System said: <prompt_arrive_city leg="1">to which city would you like to fly</prompt_arrive_city>
from <fill_depart_city leg="1">seattle</fill_depart_city>
U13: User said: <fill_arrive_city leg="1">seattle washington</fill_arrive_city>
    S14: System said: excuse me, i didn't hear you well. I think you want to fly from
<fill_depart_city leg="1">seattle</fill_depart_city> If this is okay, let's continue. Otherwise,
please specify the locations again.
  U15: User said: <yes>that's okay</yes>
    S16: System said: <prompt_arrive_city leg="1">what city would you like to fly to?</prompt_arrive_city>
    U17: User said: <fill_arrive_city leg="1">tokyo japan</fill_arrive_city>
     S18: System said: flying to <fill_arrive_city leg="1">togiak</fill_arrive_city>
<prompt_date leg="1">on what date?</prompt_date>
  U19: User said: <fill_date leg="1">monday September twenty fifth</fill_date>
  S20: System said: there don't seem to be any flights from <fill_depart_city leg="1">seattle</fill_depart_city>
to <fill_arrive_city leg="1">togiak</fill_arrive_city> which depart
<fill_depart_time leg="1">about ten PM</fill_depart_time> on
<fill_date leg="1">september twenty fifth</fill_date>.
  U21: User said:  destination <fill_arrive_city leg="1">tokyo japan</fill_arrive_city>
  S22: System said: did you say you want to fly to
<prompt_arrive_city leg="1">san diego</prompt_arrive_city>?
Conference on Speech Communication and Technology
(Berlin, Germany 1993).
[6] Levow, G.A. Characterizing and recognizing spoken
corrections in human-computer dialogue. Proceedings of
COLING-ACL (Montreal, Canada 1998).
[7] Levow, G.A. Understanding recognition failures in spoken
corrections in human-computer dialogue. Proceedings of
ECSA Workshop on Dialogue and Prosody (Eindhoven,
The Netherlands 1999).
[8] Luo, X. and Papineni, K. IBM DARPA Communicator v1.0.
DARPA Communicator Principle Investigators Meeting
(Philadelphia, PA USA 2000). http://www.dsic-web.net
/ito/meetings/communicator_sep2000/
[9] Polifroni, J. and Seneff, S. Galaxy-II as an architecture for
spoken dialogue evaluation. Proceedings of the Second
International Conference on Language Resources and
Evaluation (Athens, Greece 2000).
[10] Rudnicky, A. CMU Communicator. DARPA Communicator
Principle Investigators Meeting (Philadelphia, PA USA
2000). http://www.dsic-web.net/ito/meetings
/communicator_sep2000/
[11] Walker, M., Langkilde, I., Wright, J., Gorin, A. and Litman,
D. Learning to predict problematic situations in a spoken
dialogue system: experiments with how may I help you?
Proceedings of the Seventeenth International Conference
on Machine Learning (Stanford, CA USA 2000).
[12] Walker, M., Wright, J. and Langkilde, I. Using natural
language processing and discourse features to identify
understanding errors in a spoken dialogue system.
Proceedings of the North American Meeting of the
Association of Computational Linguistics (Seattle, WA
USA 2000).
MiTAP for SARS Detection 
 
 
Laurie E. Damianos, Samuel Bayer, 
Michael A. Chisholm, John Henderson,  
Lynette Hirschman, William Morgan, 
Marc Ubaldino, Guido Zarrella 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730 
{laurie, sam, chisholm, 
jhndrsn, lynette, wmorgan, 
ubaldino,jzarrella}@mitre.org 
James M. Wilson, V, MD and  
Marat G. Polyak 
Division of Integrated Biodefense 
ISIS Center, Georgetown University 
2115 Wisconsin Avenue Suite 603 
Washington, DC 20007 
{wilson, mgp5} 
@isis.imac.georgetown.edu 
 
Abstract 
The MiTAP prototype for SARS detection 
uses human language technology for detect-
ing, monitoring, and analyzing potential indi-
cators of infectious disease outbreaks and 
reasoning for issuing warnings and alerts. Mi-
TAP focuses on providing timely, multi-
lingual information access to analysts, domain 
experts, and decision-makers worldwide. Data 
sources are captured, filtered, translated, 
summarized, and categorized by content. 
Critical information is automatically extracted 
and tagged to facilitate browsing, searching, 
and scanning, and to provide key terms at a 
glance. The processed articles are made avail-
able through an easy-to-use news server and 
cross-language information retrieval system 
for access and analysis anywhere, any time. 
Specialized newsgroups and customizable fil-
ters or searches on incoming stories allow us-
ers to create their own view into the data 
while a variety of tools summarize, indicate 
trends, and provide alerts to potentially rele-
vant spikes of activity. 
1 
2 
Background 
Potentially catastrophic biological events that threaten 
US national security are steadily increasing in fre-
quency. These events pose immediate danger to ani-
mals, plants, and humans. Current disease surveillance 
systems are inadequate for detecting indicators early 
enough to ensure the rapid response needed to combat 
these biological events and corresponding public reac-
tion. Recent examples of outbreaks include both the 
HIV/AIDS and foot and mouth pandemics, the spread of 
West Nile virus to and across the US, the escape of Rift 
Valley Fever from Africa, SARS, and the translocation 
of both mad cow disease (BSE) and monkey pox to the 
United States.  
Biological surveillance systems in the United States 
rely most heavily on human medical data for signs of 
epidemic activity. These systems span multiple organi-
zations and agencies, are often not integrated, and have 
no alerting capability. As a result, responders have an 
insufficient amount of lead time to prepare for biologi-
cal events or catastrophes. 
Indications and Warnings (I&Ws) provide the poten-
tial for early alert of impending biological events, per-
haps weeks to months in advance. Sources of I&Ws 
include transportation data, telecommunication traffic, 
economic indices, Internet news, RSS feeds (RSS) in-
cluding weblogs, commerce, agricultural surveillance, 
weather, and other environmental data. Retrospective 
analyses of major infectious disease outbreaks (e.g., 
West Nile Virus and SARS) show that I&Ws were pre-
sent weeks to months in advance, but these indicators 
were missed because data sources were difficult to ob-
tain and hard to integrate. As a result, the available in-
formation was not utilized for appropriate national and 
international response. This illuminates a critical need in 
biodefense for an integrated system linking I&Ws for 
biological events from multiple and disparate sources 
with the response community. 
Introduction 
MiTAP (Damianos et al 2002) was originally devel-
oped by the MITRE Corporation under the Defense 
Advanced Research Projects Agency (DARPA) 
Translingual Information Detection Extraction and 
Summarization (TIDES) program. TIDES aims to revo-
lutionize the way that information is obtained from hu-
man language by enabling people to find and interpret 
relevant information quickly and effectively, regardless 
of language or medium. MiTAP was initially created for 
tracking and monitoring infectious disease outbreaks 
and other biological threats as part of a DARPA Inte-
grated Feasibility Experiment in biosecurity to explore 
the integration of synergistic TIDES language process-
ing technologies applied to a real world domain. The 
system has since been expanded to other domains such 
as weapons of mass destruction, satellite monitoring, 
and suspect terrorist activity. In addition, researchers 
and analysts are examining hundreds of MiTAP data 
sources for differing perspectives on conflict and hu-
manitarian relief efforts. 
Our newest MiTAP prototype explores the integra-
tion of outputs from operational data mining (anomaly 
detection), human language technology (information 
extraction, temporal tagging, machine translation, cross-
language information retrieval), and visualization tools 
to detect SARS-specific I&Ws in Asia, with relevance 
to pathogen translocation to the United States. Using 
feeds from English and Chinese language newswire, 
weblogs, and other Internet data, the system translates 
Chinese text data and tracks keyword combinations 
thought to represent I&Ws specific to SARS outbreaks 
in China. Analysts can use cross-language information 
retrieval for retrospective analysis and improving the 
I&W model, save searches to use as filters on incoming 
data, view trends, and visualize the data along a time-
line. Figure 1 shows an overview of the prototype. 
Warnings generated by this MiTAP prototype are in-
tended to complement traditional biosurveillance and 
communications already in use by the international pub-
lic health community. This system represents an expan-
sion of current US surveillance capabilities to detect 
biological agents of catastrophic potential.
 
 
Figure 1 Overview of the MiTAP prototype for SARS detection. 
3 Component Technologies 
The MiTAP prototype relies extensively on human 
language technology and expert system reasoning. 
Below, MiTAP capabilities are described briefly 
along with their contributing component 
technologies. 
3.1 
3.2 
3.3 
3.4 
3.5 
Information Processing 
After Internet news sources are captured and 
normalized, they are passed through a zoner using 
human-generated rules to identify source, date, and 
other information such as headline, or title, and 
content. The Alembic natural language analyzer (Ab-
erdeen et al 1995; Vilain and Day 1996) processes 
the zoned messages to identify paragraph, sentence, 
and word boundaries as well as part-of-speech tags. 
The messages then pass through the Alembic named 
entity recognizer for identification and tagging of 
person, organization, location, and disease names. 
Finally, the article is processed by the TempEx 
normalizing time expression tagger (Mani and Wil-
son 2000). 
For Chinese and other non-English sources, the 
CyberTrans machine translation system (Miller et al 
2001) is used to translate articles automatically into 
English. CyberTrans wraps commercial and research 
translation engines to produce a common set of 
interfaces; the current prototype makes use of the 
SYSTRAN Chinese-English system.  
RSS feeds can provide a high volume textual ge-
stalt.  Weblogs, in particular, are a good source of 
timely text, some of which is topical and all of which 
is based on personal observations and experiences. 
Aggregate measurements on these feeds can provide 
indications of public health-related phenom-
ena.  Consider the relative rates of words and phrases 
such as "stay home from" or "pneumonia.?  Geotem-
poral location of non-seasonal spikes in relative rank 
of these strings can establish suspicion for further 
investigation by I&W experts. 
Browsing 
English language data and pairs of foreign language 
documents and their translated versions are made 
available on a news server (INN 2001) for browsing. 
The system categorizes and bins articles into 
newsgroups based on their content. To do this, the 
system relies on a combination of the information 
extraction results as well as human-generated rules 
for pattern matching. Newsgroups are created to 
provide multiple perspectives on the data; analysts 
can subscribe to specific disease tracking 
newsgroups, regional newsgroups, specific data 
source newsgroups, or to customized topic tracking 
newsgroups that may be based on several related 
subjects. 
Tagged entities in each article are color-coded to 
enable rapid scanning of information and easy identi-
fication of key names. The five most frequently men-
tioned locations in each article as well as the top five 
people are presented as a list for quick reference. 
Information Retrieval 
To supplement access to the articles on the news 
server and to allow for retrospective analysis, articles 
are indexed using the Lucene information retrieval 
system (The Jakarta Project 2001) for English 
language documents and using PSE (Darwish 2002) 
for foreign language documents. Web links are 
maintained between foreign language documents and 
their translated versions to allow for more accurate 
human translations of selected documents. 
Analysts can perform full text, source-specific 
queries over the entire set of archived documents and 
view the retrieved results as a relevance-ranked list or 
as a plot across a timeline. A cross-language informa-
tion retrieval interface allows users to search in Eng-
lish across the Chinese language sources. 
Users can also save specific search constraints to 
be used as filters on incoming data. These saved 
searches provide a simple analytic capability as well 
as an alerting feature. (See below.) 
Analysis 
To assist analysts in identifying relevant and related 
articles, we have integrated multi-document summa-
rization and watch lists. Columbia University?s 
Newsblaster (McKeown et al 2002) automatically 
detects daily topics, clusters MiTAP articles around 
those topics, and generates multi-document summari-
zations which are made available on the news server. 
Multiple technologies (e.g., coreference, information 
extraction) from Alias I, Inc. (Baldwin et al 2002) 
produces comprehensive views on specific named 
entities (i.e., people or disease) across MiTAP docu-
ments. These views are summarized through ranked 
lists, highlighting important topics of the day and 
activities which might indicate disease outbreak.  
Finely-tuned searches can be saved and applied as 
filters or topic tracking mechanisms. These saved 
searches are automatically updated at specific inter-
vals and can be aggregated and displayed visually as 
bar graphs to reveal spikes of activity that otherwise 
might go undetected. 
Alerting 
The MiTAP prototype has two separate alerting ca-
pabilities: saved searches and an integrated expert 
system. The saved search functionality allows ana-
lysts to set thresholds for alerting purposes. For ex-
ample, MiTAP can send email when any new article 
arrives, when a specified maximum number of arti-
cles arrives, or when the daily number of new articles 
increases by some percentage of the total or moving 
average. 
The Human Language Indication Detector 
(HLID) performs data fusion on a number of dispa-
rate sources, compressing a large volume of informa-
tion into a smaller but more significant set of alerts. 
HLID monitors a variety of sources including MiTAP 
articles, information events in RSS feeds, and other 
dynamically updated information on the World Wide 
Web. HLID analyzes events from these sources in 
real time and generates an estimate of significance 
for each, complete with an audit trail of supporting 
and negating evidence. This allows an analyst to di-
rect a search for indicators towards interesting data 
while reducing the time spent investigating false 
alarms and insignificant events.  
HLID is composed of four major components. 
The first is an event collector, which monitors a data 
source and triggers action when an event is observed. 
These events are sent to the rule based reasoning en-
gine, an expert system shell (JESS 2004) with hand 
authored rules. The engine performs vetting and ini-
tial investigation of each event by identifying corre-
lated events, corroborating or invalidating evidence, 
and references to supporting information. The engine 
can also supplement its knowledge base by perform-
ing a directed search via the query management sys-
tem, which allows retrieval of information from a 
wide variety of sources including databases and web 
pages. Lastly, the alerting mechanism disseminates 
the conclusions reached by the system and provides 
an interface that allows an analyst to launch a deeper 
search for indicators and warnings. 
4 
5 
Acknowledgments 
This work has been funded, in part, by the Defense 
Advanced Research Projects Agency Translingual 
Information Detection Extraction and Summarization 
program under contract numbers DAAB07-01-C-
C201 and W15P7T-04-C-D001, the Office of the 
Secretary of Defense in support of the Coalition Pro-
visional Authority in Baghdad, and a MITRE Special 
Initiative for Rapid Integration of Novel Indications 
and Warnings for SARS. 
References 
Aberdeen, J., Burger, J., Day, D., Hirschman, L., 
Robinson, P., and Vilain, M. 1995. MITRE: De-
scription of the Alembic System as Used for 
MUC-6. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). 
Baldwin, B,, Moore, M., Ross, A., Shah, D.  2002. 
Trinity Information Access System. Proceedings of 
Human Lanuage Technology Conference, San 
Diego, CA. 
Damianos, L., Ponte, J., Wohlever, S., Reeder, F., 
Day, D., Wilson, G., Hirschman, L. 2002. MiTAP, 
Text and Audio Processing for Bio-Security: A 
Case Study In Proceedings of IAAI-2002: The 
Fourteenth Innovative Applications of Artificial 
Intelligence Conference, Edmonton, Alberta, Can-
ada. 
Darwish, K. PSE: A Small Search Engine written in 
Perl 2002 
http://tides.umiacs.umd.edu/software.html 
INN: InterNetNews, Internet Software Consortium 
2001, http://www.isc.org/products/INN.  
The Jakarta Project, 2001 
http://jakarta.apache.org/lucene/docs/index.html. 
JESS: the Rule Engine for the Java? Platform 2004 
http://herzberg.ca.sandia.gov/jess/  
Mani, I. and Wilson, G. 2000. Robust Temporal 
Processing of News. In Proceedings of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL'2000), 69-76. 
McKeown, K., Barzilay, R., Evan, D., Hatzivassi-
loglou, V., Klavans, J., Sable, C., Schiffman, B., 
Sigelman, S. 2002. Tracking and Summarizing 
News on a Daily Basis with Columbia's Newsblas-
ter. In Proceedings of HLT 2002: Human Lan-
guage Technology Conference. 
Miller, K., Reeder, F., Hirschman, L., Palmer, D. 
2001. Multilingual Processing for Operational 
Users, NATO Workshop on Multilingual Process-
ing at EUROSPEECH. 
RSS RDF Site Summary http://purl.org/rss/1.0/spec 
Vilain, M. and Day, D. 1996. Finite-state phrase 
parsing by rule sequences. In Proceedings of the 
1996 International Conference on Computational 
Linguistics (COLING-96), Copenhagen, Denmark. 
The MITRE Logical Form Generation System
Samuel Bayer and John Burger and Warren Greiff and Ben Wellner
The MITRE Corporation
Bedford, MA 01730
 
sam,john,greiff,wellner  @mitre.org
Abstract
In this paper, we describe MITRE?s contribution to
the logical form generation track of Senseval-3. We
begin with a description of the context of MITRE?s
work, followed by a description of the MITRE sys-
tem and its results. We conclude with a commentary
on the form and structure of this evaluation track.
1 Introduction
The logic form identification track of the 2004 Sen-
seval evaluation requires its participants to produce
a version of each input sentence with each input
word in citation form, annotated with both a scope-
free Davidsonian logic and lexical category infor-
mation for major categories. The output ignores el-
ements like determiners and negation, and features
such as plurals and verb tenses.
This evaluation is of interest to the MITRE Cor-
poration because it has a long-standing interest in
text processing and understanding, in all its various
dimensions. In our current internally funded Read-
ing Comprehension (RC) project, we focus on the
detailed understanding of individual stories, using
the ability to answer comprehension questions as-
sociated with these stories as our evaluation metric.
At the moment, we are interested in getting a sense
of how much inference is routinely needed in order
to answer RC questions; so generation of sentence
meanings is not currently our research focus. How-
ever, in the context of our exploration, we continue
to maintain an automated system for producing sen-
tence meanings from text.
2 The MITRE logic generation system
The system which MITRE employed for the
Senseval-3 logical form evaluation consists of the
following components:
 the Humphreys/Carroll/Minnen morphological
analyzer (Minnen et al, 2001)
 the CMU Link Grammar parser (Sleator and
Temperley, 1991)
 a link interpretation language which is used to
produce a dependency graph
 additional lexical knowledge sources
 an argument canonicalizer based partially on
the principles of Relational Grammar (Perl-
mutter, 1983)
 a task-specific logical form generator
The morphological analyzer is straightforward,
and we will not say more about it. We discuss the
remaining components below.
2.1 The CMU Link Grammar parser
The Link Grammar formalism consists of labeled,
undirected links among pairs of words. Each word
in the Link Grammar dictionary is mapped to a com-
plex logical expression of the link ends the word can
participate in. These link ends have a major compo-
nent (indicated by uppercase letters), a minor com-
ponent (indicated by lowercase letters), and a re-
quired direction (looking leftward (-) or rightward
(+)). Two words can be joined by a link if their link
ends are compatible. The Link Parser provides rea-
sonable performance achieving 75% labeled con-
stituent accuracy on the TreeBank data. There are
a large number of link types some of which pro-
vide very detailed distinctions beyond those found
in phrase structure grammars. For further details,
see (Sleator and Temperley, 1991).
Figure 1 shows the processing of the simple sen-
tence Chris loves Sam. We describe link parser
output as a set of 6-tuples, consisting of the index,
word, and link end for each end of the link; we omit
the direction information from the link, since it can
be inferred from the tuple. For instance, loves at in-
dex 2 is joined to Sam at index 3 via an O link; loves
bears O looking rightward in the lexicon, and Sam
bears O looking leftward, and these link ends are
compatible. As mentioned, ndividual lexical items
may (and often do) have multiple link types associ-
ated with them (e.g. Sam also bears S looking right-
ward for the case when Sam is a subject.)
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
input sentence Chris loves Sam
link parser 1 Chris Ss 2 loves Ss
output 2 loves O 3 Sam Os
rules (1) LINK ( S SF SX ) role[left]: arg:S role[right]: head
(2) LINK O role[left]: head role[right]: arg:O
(3) FEAT ( S- SX- ) category: v
dependency [v [ Chris 1:H]:S loves 2:H,3singular,present [Sam 3:H]:O]
object
logic form Chris (x1) loves:v (e1, x1, x2) Sam (x2)
Figure 1: Processing ?Chris loves Sam?
Link parses contain a great deal of detail, but be-
cause the link parser is a general-purpose tool, ex-
tracting this detail for a particular task may require
further processing. In particular, the category and
head/dependent information that is needed for logi-
cal form generation can be computed to a large de-
gree, but is not explicitly present. Our link interpre-
tation language addresses this issue.
2.2 The link interpretation language
Our link interpretation language operates on the out-
put of the link parser, and assembles a dependency
graph. The link interpretation language can assign
properties and categories to individual link ends via
FEAT rules, and assign head/dependency relations
to links via LINK rules.
Look again at Figure 1. Rule (1) applies to any
link whose ends are compatible with the link ends
S, SF or SX1 . This rule assigns the arg:S role
(i.e., subject argument) to the left end of the link,
and the head role to the right end. In other words,
if two words are linked by an S link, the left element
is the subject of the right element. Rule (2) creates
an analogous dependency for the O link, making the
right element the object of the left element. Rule (3)
says that anything on the leftward-looking end of an
S or SX link) should be assigned the category v; i.e.,
it?s a verb.
The LINK rules can assign a range of roles, in-
cluding:
 head
 argument of a particular type (e.g., S or O)
 modifier of a particular type (e.g., DET)
 merge, which promotes all dependents of the
merged element and constructs a complex lex-
ical head (e.g., for idioms or multi-word proper
names)
1S links are simple subject-verb relations, SF is used for the
special case where the subject is it or there (e.g. It was raining.),
and SX is used whent he subject is the first person pronoun I.
 filler and hole, which establish relationships re-
lated to unbounded dependencies
In addition, LINK and FEAT rules can assign
roles, properties and categories to the parents of the
left and right elements when necessary, and the pro-
cessor postpones these assignments until the appro-
priate parent relationships are established.
The processor which interprets this language be-
gins by assigning a dependency object to each word
in the sentence; the word is the head of the depen-
dency object, and the object has no dependents. The
processor then looks at each of the links, in any or-
der. It applies all relevant FEAT operators to each
link end, and finds the first LINK rule which ap-
plies. If any LINK rules which must be postponed
are found, the processor collects all candidate rules,
and chooses among them after the parent relation-
ships are established.
The output of this procedure as shown in the
fourth row of Figure 1 is a set of interconnected de-
pendency objects. Every dependency object which
has been identified as a non-head link end will have
the object it depends on as its parent. In the ideal
case, this set will have only one parentless object,
which will be the dependency object associated with
the matrix verb. Figure 1 also shows the topmost
dependency object for our example sentence; in this
representation, each word or constituent bears a suf-
fix indicating that it is the head (:H) or the relation-
ship it bears to the head (e.g., :O).
In general the process of adding LINK and FEAT
rules was carried out in a data-driven manner. Cur-
rently, there are 88 LINK rules and 63 FEAT rules.
While the number of potential rules is quite large
due to a large number of link types, catagories, and
properties, we have found that these rules general-
ize reasonably well and expect that the remaining
rules that would be required to represent very spe-
cific cases.
2.3 Additional lexical knowledge sources
For the purposes of deriving logical forms, the link
parser output doesn?t contain quite enough informa-
tion. We rely on two additional sources of lexical
knowledge: a small dictionary, developed in concert
with the link interpretation language, which identi-
fies features such as auxiliary for verbs, and a body
of lexical control information, derived from sub-
categorization classes in Comlex (Macleod et al,
1998). The first source informs the link interpre-
tation process, by identifying which verbs are de-
pendents of other verbs. The second source informs
our next step, the argument canonicalizer.
2.4 The argument canonicalizer
In this step, we construct an argument network for
each dependency object, in the spirit of Relational
Grammar (Perlmutter, 1983). For those predicative
phrases in argument positions which lack a subject,
we determine and assign a subject to control the
phrase. We use the lowest available grammatical
relation (first object, then subject) as the controller,
unless the information we?ve collected from Com-
lex indicates otherwise (e.g., in the case of promise).
We then identify those argument networks to which
Passive has applied, and undo it, and do the same for
Dative Movement, in order to derive the canonical
predicate argument order.
2.5 Deriving the logical forms
At this point, we have all the information we need to
derive the logical forms required for this evaluation
track. We generate logical forms via the following
steps:
1. We eliminate those words for which no output
is required (e.g., determiners).
2. We identify the remaining words which require
a part of speech suffix (e.g., nouns but not
proper nouns).
3. We identify the remaining words which take
arguments (e.g., verbs but not nouns) and those
which add their own instance variable (e.g.,
verbs but not prepositions).
4. We add the appropriate argument structures for
noun-noun compounds, and make other task-
specific adjustments.
5. We collect and format the appropriate predi-
cates and argument lists.
In some cases, a subject argument was required, but
we could not infer the appropriate filler; in these
cases, we insert the string ?MISSING? as the log-
ical subject in the logical form.
3 Results
Table 1 shows the precision and recall over both ar-
guments and predicates. Table 2 includes the pre-
centage of sentences of which all arguments were
identified (SentArg) and all predicates were identi-
fied (SentPred). SentArgPred indicates the percent-
age of sentences for which all arguments were iden-
tified correctly out of sentences that had all pred-
icates identified correctly. SentArgPredSent is the
percentage of sentences for which all arguments and
all predicates were identified correctly (SentArg-
PredSent).
Precision Recall
Arguments 0.74 0.66
Predicates 0.84 0.78
Table 1: Argument and predicate precision and re-
call.
Accuracy
SentArg 0.27
SentPred 0.21
SentArgPred 0.40
SentArgPredSent 0.087
Table 2: Sentence-based accuracy of extracted logic
forms.
Clearly, these results indicate room for improve-
ment in this task.
4 Comments on the evaluation
We found some problems in this evaluation.
4.1 Resolving vagueness in the task
In some cases, the details of the task are vague.
One example is collocations. The task description
clearly allows for collocations (e.g. proud of, at a
loss), but there is little guidance about how to decide
whether some word sequence should be a colloca-
tion. These decisions affect the system scores, and
the absence of clear guidance on this issue clearly
suggests uncertainty about what the scores mean.
Having an official list of collocations is only one
part of the solution, however. Since collocations
obscure internal structure, creating a collocation po-
tentially loses information; so the issue isn?t simply
to know what?s on the list, but to have some guide-
line for deciding what should be on the list.
One way in which to motivate guidelines, define
scoring metrics, etc. is to include a more goal-
directed task description. The last two decades of
research in computational linguistics have cemented
the crucial role of system evaluation, but the sum-
mary in (Hirschman and Thompson, 1996) makes
it clear that the best evaluations are defined with a
specific task in mind. In a previous attempt to define
predicate-argument structure, Semeval, the effort
was abandoned because so many constructs would
require detailed attention and resolution, and be-
cause most information-extraction systems did not
generate full predicate-argument structures (most
likely because the task did not require it) (Grishman
and Sundheim, 1996). While introducing a task cre-
ates its own problems by removing domain indepen-
dence, the constraints it provides are worth consid-
eration. For example, in a task such as Question An-
swering, certain distinctions in the logic-form pre-
sented here may serve no purpose or perhaps finer
grained distinctions are required.
As another example of this issue, the scorer pro-
vided for this task computes the precision and recall
for both predicates and predicate arguments in the
logic forms. In some circumstances, the scorer as-
signs the same score for predication of an incorrect,
independently specified variable (e.g., x2 instead of
x1 as the first argument of loves in Figure 1) as
for predication of an otherwise unspecified variable
(e.g., x3 instead of x1). This may be an informa-
tive scoring strategy, but having a more specific task
would help make this decision.
4.2 Suggested improvements in the logic
In many ways, it?s also impossible to make judg-
ments about the syntax and implied model for the
logic without a more specific task, but it?s still worth
pointing out some inconsistencies.
First, the implied account of noun-noun com-
pounds introduces an nn predicate, but assigns to
the resulting phrase a different variable than either
of the nominal constituents. Adjectival modifica-
tion, on the other hand, is represented by sharing
of variables. (Rus, 2002) argues for this account
of noun-noun compounds (p. 111), but provides
no motivation for treating the noun-noun compound
goat hair as having a separate variable from its
head but not doing the same for the adjective-noun
sequence curly hair.
Second, the account of pronominal possessives
(our, my) would lead to a poor account of full pos-
sessives. The possessive pronoun shares a variable
with its possesseed, which does not allow a paral-
lel or adequate account at all of the full possessives
(e.g., the poor boy?s father could only have boy,
poor, and father assigned to the same index). The
possessive should be treated like noun-noun com-
pounds, with a poss operator.
Finally, adverbs which modify adjectives have
nothing to attach to. In the single example of this
construction in the sample data (Sunshine makes me
very happy) the modifier very is predicated of me,
because happy is predicated of me. This account
leads immediately to problems with examples like
John is very tall but hardly imposing, where all four
modifying elements would end up being predicated
of John, introducing unnecessary ambiguity. In-
troducing properties in the logic as individuals (cf.
(Chierchia and Turner, 1988)) would almost cer-
tainly be an improvement.
References
G. Chierchia and R. Turner. 1988. Semantics
and property theory. Linguistics and Philosophy,
11:261?302.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference - 6: A brief his-
tory. In Papers presented to the Sixteenth Inter-
national Conference On Computational Linguis-
tics (COLING -96), University of Copenhagen.
L. Hirschman and H. Thompson. 1996. Overview
of evaluation in speech and natural language pro-
cessing. In R. Cole, editor, Survey of the State
of the Art in Human Language Technology, chap-
ter 13. Cambridge University Press, Cambridge.
Catherine Macleod, Ralph Grishman, and Adam
Meyers, 1998. COMLEX Syntax Reference Man-
ual. Proteus Project, NYU.
G. Minnen, J. Carroll, and D. Pearce. 2001. Ap-
plied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
David M. Perlmutter, editor. 1983. Studies in Rela-
tional Grammar 1. University of Chicago Press.
Vasile Rus. 2002. Logic Forms for Wordnet
Glosses. Ph.D. thesis, Southern Methodist Uni-
versity.
Daniel Sleator and Davy Temperley. 1991. Pars-
ing English with a Link Grammar. Technical Re-
port CMU-CS-91-196, Carnegie Mellon Univer-
sity Dept. of Computer Science, October.
