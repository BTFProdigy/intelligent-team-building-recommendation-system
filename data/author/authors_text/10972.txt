2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 131?141,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Learning on an Approximate Corpus?
Jason Smith and Jason Eisner
Center for Language and Speech Processing
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218, USA
{jsmith,jason}@cs.jhu.edu
Unsupervised learning techniques can take advan-
tage of large amounts of unannotated text, but the
largest text corpus (the Web) is not easy to use in
its full form. Instead, we have statistics about this
corpus in the form of n-gram counts (Brants and
Franz, 2006). While n-gram counts do not directly
provide sentences, a distribution over sentences can
be estimated from them in the same way that n-
gram language models are estimated. We treat this
distribution over sentences as an approximate cor-
pus and show how unsupervised learning can be
performed on such a corpus using variational infer-
ence. We compare hidden Markov model (HMM)
training on exact and approximate corpora of vari-
ous sizes, measuring speed and accuracy on unsu-
pervised part-of-speech tagging.
1 Introduction
We consider the problem of training generative mod-
els on very large datasets in sublinear time. It is well
known how to train an HMM to maximize the like-
lihood of a corpus of sentences. Here we show how
to train faster on a distribution over sentences that
compactly approximates the corpus. The distribu-
tion is given by an 5-gram backoff language model
that has been estimated from statistics of the corpus.
In this paper, we demonstrate our approach on
a traditional testbed for new structured-prediction
learning algorithms, namely HMMs. We focus on
unsupervised learning. This serves to elucidate the
structure of our variational training approach, which
stitches overlapping n-grams together rather than
treating them in isolation. It also confirms that at
least in this case, accuracy is not harmed by the
key approximations made by our method. In future,
we hope to scale up to the Google n-gram corpus
(Brants and Franz, 2006) and learn a more detailed,
explanatory joint model of tags, syntactic dependen-
cies, and topics. Our intuition here is that web-scale
data may be needed to learn the large number of lex-
ically and contextually specific parameters.
?Work was supported in part by NSF grant No. 0347822.
1.1 Formulation
Let w (?words?) denote an observation sequence,
and let t (?tags?) denote a hidden HMM state se-
quence that may explain w. This terminology is
taken from the literature on inducing part-of-speech
(POS) taggers using a first-order HMM (Merialdo,
1994), which we use as our experimental setting.
Maximum a posteriori (MAP) training of an
HMM p? seeks parameters ? to maximize
N ?
?
w
c(w) log
?
t
p?(w, t) + log Pr prior(?) (1)
where c is an empirical distribution that assigns
probability 1/N to each of the N sentences in a
training corpus. Our technical challenge is to gen-
eralize this MAP criterion to other, structured dis-
tributions c that compactly approximate the corpus.
Specifically, we address the case where c is given
by any probabilistic FSA, such as a backoff lan-
guage model?that is, a variable-order Markov
model estimated from corpus statistics. Similar sen-
tences w share subpaths in the FSA and cannot eas-
ily be disentangled. The support of c is typically infi-
nite (for a cyclic FSA) or at least exponential. Hence
it is no longer practical to compute the tagging distri-
bution p(t | w) for each sentence w separately, as in
traditional MAP-EM or gradient ascent approaches.
We will maximize our exact objective, or a cheaper
variational approximation to it, in a way that cru-
cially allows us to retain the structure-sharing.
1.2 Motivations
Why train from a distribution rather than a corpus?
First, the foundation of statistical NLP is distribu-
tions over strings that are specified by weighted au-
tomata and grammars. We regard parameter estima-
tion from such a distribution c (rather than from a
sample) as a natural question. Previous work on
modeling c with a distribution from another fam-
ily was motivated by approximating a grammar or
131
model rather than generalizing from a dataset, and
hence removed latent variables while adding param-
eters (Nederhof, 2000; Mohri and Nederhof, 2001;
Liang et al, 2008), whereas we do the reverse.
Second, in practice, one may want to incorporate
massive amounts of (possibly out-of-domain) data
in order to get better coverage of phenomena. Mas-
sive datasets usually require a simple model (given a
time budget). We propose that it may be possible to
use a lot of data and a good model by reducing the
accuracy of the data representation instead. While
training will become more complicated, it can still
result in an overall speedup, because a frequent 5-
gram collapses into a single parameter of the esti-
mated distribution that only needs to be processed
once per training iteration. By pruning low-count
n-grams or reducing the maximum n below 5, one
can further increase data volume for the fixed time
budget at the expense of approximation quality.
Third, one may not have access to the original
corpus. If one lacks the resources to harvest the
web, the Google n-gram corpus was derived from
over a trillion words of English web text. Privacy
or copyright issues may prevent access, but one may
still be able to work with n-gram statistics: Michel
et al (2010) used such statistics from 5 million
scanned books. Several systems use n-gram counts
(Bergsma et al, 2009; Lin et al, 2009) or other
web statistics (Lapata and Keller, 2005) as features
within a classifier. A large language model from n-
gram counts yields an effective prior over hypothe-
ses in tasks like machine translation (Brants et al,
2007). We similarly construct an n-gram model, but
treat it as the primary training data whose structure
is to be explained by the generative HMM. Thus our
criterion does not explain the n-grams in isolation,
but rather tries to explain the likely full sentences
w that the model reconstructed from overlapping n-
grams. This is something like shotgun sequencing,
in which likely DNA strings are reconstructed from
overlapping short reads (Staden, 1979); however, we
train an HMM on the resulting distribution rather
than merely trying to find its mode.
Finally, unsupervised HMM training discovers la-
tent structure by approximating an empirical distri-
bution c (the corpus) with a latent-variable distribu-
tion p (the trained HMM) that has fewer parameters.
We show how to do the same where the distribution
c is not a corpus but a finite-state distribution. In
general, this finite-state c could represent some so-
phisticated estimate of the population distribution,
using shrinkage, word classes, neural-net predictors,
etc. to generalize in some way beyond the training
sample before fitting p. For the sake of speed and
clear comparison, however, our present experiments
take c to be a compact approximation to the sample
distribution, requiring only n-grams.
Spectral learning of HMMs (Hsu et al, 2009)
also learns from a collection of n-grams. It has the
striking advantage of converging globally to the true
HMM parameters (under a certain reparameteriza-
tion), with enough data and under certain assump-
tions. However, it does not exploit context beyond
a trigram (it will not maximize, even locally, the
likelihood of a finite sample of sentences), and can-
not exploit priors or structure?e.g., that the emis-
sions are consistent with a tag dictionary or that the
transitions encode a higher-order or factorial HMM.
Our more general technique extends to other latent-
variable models, although it suffers from variational
EM?s usual local optima and approximation errors.
2 A variational lower bound
Our starting point is the variational EM algorithm
(Jordan et al, 1999). Recall that this maximizes a
lower bound on the MAP criterion of equation 1, by
bounding the log-likelihood subterm as follows:
log
?
t p?(w, t) (2)
= log
?
t q(t)(p?(w, t)/q(t))
?
?
t q(t) log(p?(w, t)/q(t))
= Eq(t)[log p?(w, t)? log q(t)] (3)
This use of Jensen?s inequality is valid for any distri-
bution q. As Neal and Hinton (1998) show, the EM
algorithm (Dempster et al, 1977) can be regarded
as locally maximizing the resulting lower bound by
alternating optimization, where q is a free parame-
ter. The E-step optimizes q for fixed ?, and the M-
step optimizes ? for fixed q. These computations are
tractable for HMMs, since the distribution q(t) =
p?(t | w) that is optimal at the E-step (which makes
the inequality tight) can be represented as a lattice
(a certain kind of weighted DFA), and this makes
the M-step tractable via the forward-backward algo-
rithm. However, there are many extensions such as
132
factorial HMMs and Bayesian HMMs in which an
expectation under p?(t | w) involves an intractable
sum. In this setting, one may use variational EM, in
which q is restricted to some parametric family q?
that will permit a tractable M-step. In this case the
E-step chooses the optimal values of the variational
parameters ?; the inequality is no longer tight.
There are two equivalent views of how this pro-
cedure is applied to a training corpus. One view is
that the corpus log-likelihood is just as in (2), where
w is taken to be the concatenation of all training
sentences. The other view is that the corpus log-
likelihood is a sum over many terms of the form (2),
one for each training sentence w, and we bound each
summand individually using a different q?.
However, neither view leads to a practical imple-
mentation in our setting. We can neither concatenate
all the relevant w nor loop over them, since we want
the expectation of (2) under some distribution c(w)
such that {w : c(w) > 0} is very large or infinite.
Our move is to make q be a conditional distribution
q(t | w) that applies to all w at once. The follow-
ing holds by applying Jensen?s inequality separately
to each w in the expectation (this is valid since for
each w, q(t | w) is a distribution):
Ec(w) log
?
t p?(w, t) (4)
= Ec(w) log
?
t q(t | w)(p?(w, t)/q(t | w))
? Ec(w)
?
t q(t | w) log(p?(w, t)/q(t | w))
= Ecq(w,t)[log p?(w, t)? log q(t | w)] (5)
where we use cq(w, t) to denote the joint distribu-
tion c(w) ? q(t | w). Thus, just as c is our approx-
imate corpus, cq is our approximate tagged corpus.
Our variational parameters ? will be used to param-
eterize cq directly. To ensure that cq? can indeed
be expressed as c(w) ? q(t | w), making the above
bound valid, it suffices to guarantee that our varia-
tional family preserves the marginals:
(?w)
?
t cq?(w, t) = c(w)
3 Finite-state encodings and algorithms
In the following, we will show how to maximize
(5) for particular families of p, c, and cq that can
be expressed using finite-state machines (FSMs)?
that is, finite-state acceptors (FSAs) and transducers
(FSTs). This general presentation of our method en-
ables variations using other FSMs.
A path in an FSA accepts a string. In an FST,
each arc is labeled with a ?word : tag? pair, so that a
path accepts a string pair (w, t) obtained by respec-
tively concatenating the words and the tags encoun-
tered along the path. Our FSMs are weighted in the
(+,?) semiring: the weight of any path is the prod-
uct (?) of its arc weights, while the weight assigned
to a string or string pair is the total weight (+) of all
its accepting paths. An FSM is unambiguous if each
string or string pair has at most one accepting path.
Figure 1 reviews how to represent an HMM POS
tagger as an FST (b), and how composing this with
an FSA that accepts a single sentence gives us the
familiar HMM tagging lattice as an FST (c). The
forward-backward algorithm sums over paths in the
lattice via dynamic programming (Rabiner, 1989).
In section 3.1, we replace the straight-line FSA
of Figure 1a with an FSA that defines a more gen-
eral distribution c(w) over many sentences. Note
that we cannot simply use this as a drop-in replace-
ment in the construction of Figure 1. That would
correspond to running EM on a single but uncer-
tain sentence (distributed as c(w)) rather than a col-
lection of observed sentences. For example, in the
case of an ordinary training corpus of N sentences,
the new FSA would be a parallel union (sum) of
N straight-line paths?rather than a serial concate-
nation (product) of those paths as in ordinary EM
(see above). Running the forward algorithm on the
resulting lattice would compute Ec(w)
?
t p(w, t),
whose log is logEc(w)
?
t p(w, t) rather than our
desired Ec(w) log
?
t p(w, t). Instead, we use c in
section 3.2 to construct a variational family cq?. We
then show in sections 3.3?3.5 how to compute and
locally maximize the variational lower bound (5).
3.1 Modeling a corpus with n-gram counts
n-gram backoff language models have been used for
decades in automatic speech recognition and statis-
tical machine translation. We follow the usual FSA
construction (Allauzen et al, 2003). The state of a 5-
gram FSA model c(w) must remember the previous
4-gram. For example, it would include an arc from
state defg (the previous 4-gram) to state efgh with
label h and weight c(h | defg). Then, with appro-
priate handling of boundary conditions, a sentence
w = . . . defghi . . . is accepted along a single path of
weight c(w) = ? ? ? c(h | defg) ? c(i | efgh) ? ? ? . Arcs
133
(a) w Time flies like an arrow
(b) p(w,t) Start Vw:V
StopNw:N
DT
w:DTw:V
w:V
(c) w o p(w,t) Start VTime : V NTime : N
Vflies : V
N
flies : N
flies : V
flies : N
Prep
like : Prep
V
like : V
like : Prep
like : V
DTan : DTan : DT Narrow : N
Figure 1: Ordinary HMM tagging with finite-state machines. An arc?s label may have up to three components:
?word:tag / weight.? (Weights are suppressed for space. State labels are not part of the machine but suggest the history
recorded by each state.) (a) w is an FSA that generates the sentence ?Time flies like an arrow?; all arcs have probability
1. (b) p(w, t) is an FST representing an HMM (many arcs are not shown and words are abbreviated as ?w?). Each arc
w : t is weighted by the product of transition and emission probabilities, p(t | previous t) ? p(w | t). Composing (a)
with (b) yields (c), an FST that encodes the joint probabilities p(w, t) of all possible taggings of the sentence w.
of weight 0 can be omitted from the FSA.1
To estimate a conditional probability like c(h |
defg) above, we simply take an unsmoothed ratio of
two n-gram counts. This ML estimation means that
c will approximate as closely as possible the train-
ing sample from which the counts were drawn. That
gives a fair comparison with ordinary EM, which
trains directly on that sample. (See discussion at the
end of section 1.2 for alternatives.)
Yet we decline to construct a full 5-gram model,
which would not be as compact as desired. A col-
lection of all web 5-grams would be nearly as large
as the web itself (by Zipf?s Law). We may not have
such a collection. For example, the Google n-gram
corpus version 2 contains counts only for 1-grams
that appear at least 40 times and 2-, 3-, 4-, and 5-
grams that appear at least 10 times (Lin et al, 2009).
1The FSA?s initial state is the unigram history #, and its final
states (which have no outgoing arcs) are the other states whose
n-gram labels end in #. Here # is a boundary symbol that falls
between sentences. To compute the weighted transitions, sen-
tence boundaries must be manually or automatically annotated,
either on the training corpus as in our present experiments, or
directly on the training n-grams if we have only those.
To automatically find boundaries in an n-gram collection,
one could apply a local classifier to each n-gram. But in princi-
ple, one could exploit more context and get a globally consistent
annotation by stitching the n-grams together and applying the
methods of this paper?replacing p? with an existing CRF sen-
tence boundary detector, replacing c with a document-level (not
sentence-level) language model, and optimizing cq? to be a ver-
sion of c that is probabilistically annotated with sentence bound-
aries, which yields our desired distribution over sentences.
Instead, we construct a backoff language model.
This FSA has one arc for each n-gram in the col-
lection. Our algorithm?s runtime (per iteration) will
be linear in the number of arcs. If the 5-gram defgh
is not in our collection, then there can be no h arc
leaving defg. When encountering h in state defg, the
automaton will instead take a failure arc (Allauzen
et al, 2003) to the ?backoff state? efg. It may be
able to consume the h from that state, on an arc with
weight c(h | efg); or it may have to back off further
to fg. Each state?s failure arc is weighted such that
the state?s outgoing arcs sum to 1. It is labeled with
the special symbol ?, which does not contribute to
the word string accepted along a path.
We take care never to allow backoff to the empty
state ,2 since we find that c(w) is otherwise too
coarse an approximation to English: sampled sen-
tences tend to be disjointed, with some words gener-
ated in complete ignorance of their left context.
3.2 The variational distribution cq(w, t)
The ?variational gap? between (4) and (5) is
Ec(w)KL(q(t | w) || p?(t | w)). That is, the bound
is good if q does a good job of approximating p??s
tagging distribution on a randomly drawn sentence.
Note that n?1 is the order of our n-gram Markov
2To prevent such backoff, it suffices to include all 2-grams
with count > 0. But where the full collection of 2-grams is
unavailable or too large, one can remove the empty state (and
recursively remove all states that transition only to removed
states), and then renormalize the model locally or globally.
134
model c(w) (i.e., each word is chosen given the pre-
vious n ? 1 words). Let np ? 1 be the order of the
HMM p?(w, t) that we are training: i.e., each tag is
chosen given the previous np ? 1 tags. Our experi-
ments take np = 2 (a bigram HMM) as in Figure 1.
We will take q?(t | w) to be a conditional Markov
model of order nq ? 1.3 It will predict the tag at po-
sition i using a multinomial conditioned on the pre-
ceding nq?1 tags and on the word n-gram ending at
position i (where n is as large as possible such that
this n-gram is in our training collection). ? is the
collection of all multinomial parameters.
If nq = np, then our variational gap can be made 0
as in ordinary non-variational EM (see section 3.5).
In our experiments, however, we save memory by
choosing nq = 1. Thus, our variational gap is tight
to the extent that a word?s POS tag under the model
p? is conditionally independent of previous tags and
the rest of the sentence, given an n-word window.4
This is the assumption made by local classification
models (Punyakanok et al, 2005; Toutanova and
Johnson, 2007). Note that it is milder than the ?one
tagging per n-gram? hypothesis (Dawborn and Cur-
ran, 2009; Lin et al, 2009), which claims that each
5-gram (and therefore each sentence!) is unambigu-
ous as to its full tagging. In contrast, we allow that
a tag may be ambiguous even given an n-word win-
dow; we merely suppose that there is no further dis-
ambiguating information accessible to p?.5
We can encode the resulting cq(w, t) as an FST.
With nq = 1, the states of cq are isomorphic to the
states of c. However, an arc in c from defg with
label h and weight 0.2 is replaced in cq by several
arcs?one per tag t?with label h : t and weight
0.2 ? q?(t | defgh).6 We remark that an encoding of
3A conditional Markov model is a simple case of a
maximum-entropy Markov model (McCallum et al, 2000).
4At present, the word being tagged is the last word in the
window. We do have an efficient modification in which the win-
dow is centered on the word, by using an FST cq that delays the
emission of a tag until up to 2 subsequent words have been seen.
5With difficulty, one can construct English examples that
violate our assumption. (1) ?Some monitor lizards from
Africa . . . ? versus ?Some monitor lizards from a distance . . . ?:
there are words far away from ?monitor? that help disambiguate
whether ?monitor? is a noun or a verb. (?Monitor lizards? are
a species, but some people like to monitor lizards.) (2) ?Time
flies?: ?flies? is more likely to be a noun if ?time? is a verb.
6In the case nq > 1, the states of c would need to be split
in order to remember nq ? 1 tags of history. For example, if
q(t | w) as an FST would be identical except for
dropping the c factor (e.g., 0.2) from each weight.
Composing c ? q would then recover cq.
This construction associates one variational pa-
rameter in ? with each arc in cq?that is, with each
pair (arc in c, tag t), if nq = 1. There would be lit-
tle point in sharing these parameters across arcs of
cq, as that would reduce the expressiveness of the
variational distribution without reducing runtime.7
Notice that maximizing equation (5) jointly learns
not only a compact slow HMM tagger p?, but also a
large fast tagger q? that simply memorizes the likely
tags in each n-gram context. This is reminiscent of
structure compilation (Liang et al, 2008).
3.3 Computing the variational objective
The expectation in equation (5) can now be com-
puted efficiently and elegantly by dynamic program-
ming over the FSMs, for a given ? and ?.
We exploit our representation of cq? as an FSM
over the (+,?) semiring. The path weights repre-
sent a probability distribution over the paths. In gen-
eral, it is efficient to compute the expected value of
a random FSM path, for any definition of value that
decomposes additively over the path?s arcs. The ap-
proach is to apply the forward algorithm to a version
of cq? where we now regard each arc as weighted
by an ordered pair of real numbers. The (+,?) op-
erations for combining weights (section 3) are re-
placed with the operations of an ?expectation semir-
ing? whose elements are such pairs (Eisner, 2002).
Suppose we want to find Ecq?(w,t) log q?(t | w).
To reduce this to an expected value problem, we
must assign a value to each arc of cq? such that the
c is Figure 1a, splitting its states with nq = 2 would yield a
cq with a topology like Figure 1c, but with each arc having an
independent variational parameter.
7One could increase the number of arcs and hence varia-
tional parameters by splitting the states of cq to remember more
history. In particular, one could increase the width nq of the tag
window, or one could increase the width of the word window by
splitting states of c (without changing the distribution c(w)).
Conversely, one could reduce the number of variational pa-
rameters by further restricting the variational family. For exam-
ple, requiring q(t | w) to have entropy 0 (analogous to ?hard
EM? or ?Viterbi EM?) would associate a single deterministic
tag with each arc of c. This is fast, makes cq as compact as c,
and is still milder than ?one tagging per n-gram.? More gener-
ously, one could allow up to 2 tags per arc of c, or use a low-
dimensional representation of the arc?s distribution over tags.
135
total value of a path accepting (w, t) is log q?(t |
w). Thus, let the value of each arc in cq? be the log
of its weight in the isomorphic FST q?(t | w).8
We introduce some notation to make this precise.
A state of cq? is a pair of the form [hc, hq], where hc
is a state of c (e.g., an (n? 1)-word history) and hq
is an (nq ? 1)-tag history. We saw in the previous
section that an arc a leaving this state, and labeled
with w : t where w is a word and t is a tag, will
have a weight of the form ka
def
= c(w | hc)?a where
?a
def
= q?(t | hcw, hq). We now let the value va
def
=
log ?a.9 Then, just as the weight of a path accepting
(w, t) is
?
a ka = cq?(w, t), the value of that path
is
?
a va = log q?(t | w), as desired.
To compute the expected value r? over all paths,
we follow a generalized forward-backward recipe
(Li and Eisner, 2009, section 4.2). First, run the for-
ward and backward algorithms over cq?.10 Now the
expected value is a sum over all arcs of cq?, namely
r? =
?
a ?akava?a, where ?a denotes the forward
probability of arc a?s source state and ?a denotes
the backward probability of arc a?s target state.
Now, in fact, the expectation we need to compute
is not Ecq?(w,t) log q?(t | w) but rather equation (5).
So the value va of arc a should not actually be
log ?a but rather log ?a ? log ?a where ?a
def
= p?(t |
8The total value is then the sum of the logs, i.e., the log
of the product. This works because q? is unambiguous, i.e., it
computes q?(t | w) as a product along a single accepting path,
rather than summing over multiple paths.
9The special case of a failure arc a goes from [hc, hq] to
[h?c, hq], where h
?
c is a backed-off version of hc. It is labeled
with ? : , which does not contribute to the word string or
tag string accepted along a path. Its weight ka is the weight
c(? | hc) of the corresponding failure arc in c from hc to h?c.
We define va
def
= 0, so it does not contribute to the total value.
10Recall that the forward probability of each state is defined
recursively from the forward probabilities of the states that have
arcs leading to it. As our FST is cyclic, it is not possible to visit
the states in topologically sorted order. We instead solve these
simultaneous equations by a relaxation algorithm (Eisner, 2002,
section 5): repeatedly sweep through all states, updating their
forward probability, until the total forward probability of all fi-
nal states is close to the correct total of 1 =
?
w,t cq?(w, t)
(showing that we have covered all high-prob paths). A corre-
sponding backward relaxation is actually not needed yet (we do
need it for ?? in section 3.4): backward probabilities are just 1,
since cq? is constructed with locally normalized probabilities.
When we rerun the forward-backward algorithm after a pa-
rameter update, we use the previous solution as a starting point
for the relaxation algorithm. This greatly speeds convergence.
hp) ? p?(w | t). This is a minor change?except that
va now depends on hp, which is the history of np?1
previous tags. If np > nq, then a?s start state does
not store such a long history. Thus, the value of a
actually depends on how one reaches a! It is prop-
erly written as vza, where za is a path ending with
a and z is sufficiently long to determine hp.11
Formally, let Za be a ?partitioning? set of paths to
a, such that any path in cq? from an initial state to
the start state of a must have exactly one z ? Za as
a suffix, and each z ? Za is sufficiently long so that
vza is well-defined. We can now find the expected
value as r? =
?
a
?
z?Za ?z
(?
z?z kz
)
kavza?a.
The above method permits p? to score the tag se-
quences of length np that are hypothesized by cq?.
One can regard it as implicitly running the general-
ized forward-backward algorithm over a larger FST
that marries the structure of cq? with the np-gram
HMM structure,12 so that each value is again local to
a single arc za. However, it saves space by working
directly on cq? (which has manageable size because
we deliberately kept nq small), rather than material-
izing the larger FST (as bad as increasing nq to np).
TheZa trick usesO(CTnq) rather thanO(CTnp)
space to store the FST, where C is the number of
arcs in c (= number of training n-grams) and T is
the number of tag types. With or without the trick,
runtime isO(CTnp+BCTnq), whereB is the num-
11By concatenating z?s start state?s hq with the tags along z.
Typically z has length np ? nq (and Za consists of the paths
of that length to a?s start state). However, z may be longer if it
contains ? arcs, or shorter if it begins with an initial state.
12Constructed by lazy finite-state intersection of cq? and p?
(Mohri et al, 2000). These do not have to be n-gram taggers,
but must be same-length FSTs (these are closed under inter-
section) and unambiguous. Define arc values in both FSTs such
that for any (w, t), cq? and p? accept (w, t) along unique paths
of total values v = ? log q?(t | w) and v? = log p?(w, t), re-
spectively. We now lift the weights into the expectation semir-
ing (Eisner, 2002) as follows. In cq?, replace arc a?s weight
ka with the semiring weight ?ka, kava?. In p? , replace arc a??s
weight with ?1, v?a??. Then if k = cq?(w, t), the intersected
FST accepts (w, t) with weight ?k, k(v + v?)?. The expecta-
tion of v+v? over all paths is then a sum
?
za ?zarza?za over
arcs za of the intersected FST?we are using za to denote the
arc in the intersected FST that corresponds to ?a in cq? when
reached via path z,? and rza to denote the second component
of its semiring weight. Here ?za and ?za denote the forward
and backward probabilities in the intersected FST, defined from
the first components of the semiring weights. We can get them
more efficiently from the results of running forward-backward
on the smaller cq?: ?za = ?z
?
z?z kz and ?za = ?a = 1.
136
ber of forward-backward sweeps (footnote 10). The
ordinary forward algorithm requires nq = np and
takesO(CTnp) time and space on a length-C string.
3.4 Computing the gradient as well
To maximize our objective (5), we compute its gra-
dient with respect to ? and ?. We follow an efficient
recipe from Li and Eisner (2009, section 5, case 3).
The runtime and space match those of section 3.3,
except that the runtime rises to O(BCTnp).13
First suppose that each va is local to a single arc.
We replace each weight ka with k?a = ?ka, kava?
in the so-called expectation semiring, whose sum
and product operations can be found in Li and Eis-
ner (2009, Table 1). Using these in the forward-
backward algorithm yields quantities ??a and ??a
that also fall in the expectation semiring.14 (Their
first components are the old ?a and ?a.) The
desired gradient15 ??k?,?r?? is
?
a ??a(?k?a)??a,
16
where?k?a = (?ka,?(kava)) = (?ka, (?ka)va+
ka(?va)). Here? gives the vector of partial deriva-
tives with respect to all ? and ? parameters. Yet each
?k?a is sparse, with only 3 nonzero components, be-
cause k?a depends on only one ? parameter (?a) and
two ? parameters (via ?a as defined in section 3.3).
When np > nq, we sum not over arcs a of cq? but
over arcs za of the larger FST (footnote 12). Again
we can do this implicitly, by using the short path za
in cq? in place of the arc za. Each state of cq? must
then store ?? and ?? values for each of the Tnp?nq
states of the larger FST that it corresponds to. (In the
case np ? nq = 1, as in our experiments, this fortu-
nately does not increase the total asymptotic space,
13An alternative would be to apply back-propagation
(reverse-mode automatic differentiation) to section 3.3?s com-
putation of the objective. This would achieve the same runtime
as in section 3.3, but would need as much space as time.
14This also computes our objective r?: summing the ???s of the
final states of cq? gives ?k?, r?? where k? = 1 is the total probabil-
ity of all paths. This alternative computation of the expectation
r?, using the forward algorithm (instead of forward-backward)
but over the expectation semiring, was given by Eisner (2002).
15We are interested in ?r?. ?k? is just a byproduct. We re-
mark that ?k? 6= 0, even though k? = 1 for any valid parameter
vector ? (footnote 14), as increasing ? invalidly can increase k?.
16By a product of pairs we always mean ?k, r??s, t? def=
?ks, kt+ rs?, just as in the expectation semiring, even though
the pair?k?a is not in that semiring (its components are vectors
rather than scalars). See (Li and Eisner, 2009, section 4.3). We
also define scalar-by-pair products as k?s, t? def= ?ks, kt?.
since each state of cq? already has to store T arcs.)
With more cleverness, one can eliminate this
extra storage while preserving asymptotic runtime
(still using sparse vectors). Find ??k?, (?r?)(1)? =
?
a ??a??ka, 0???a. Also find ?r?, (?r?)
(2)? =
?
a
?
z?Za?z
(?
z?z?kz,?kz?
)
?kavza,?(kavza)?
?a. Now our desired gradient ?r? emerges as
(?r?)(1) + (?r?)(2). The computation of (?r?)(1)
uses modified definitions of ??a and ??a that depend
only on (respectively) the source and target states of
a?not za.17 To compute them, initialize ?? (respec-
tively ??) at each state to ?1, 0? or ?0, 0? according to
whether the state is initial (respectively final). Now
iterate repeatedly (footnote 10) over all arcs a: Add
??a?ka, 0? +
?
z?Za ?z
(?
z?z kz
)
?0, kavza? to the
?? at a?s target state. Conversely, add ?ka, 0???a to
the ?? at a?s source state, and for each z ? Za, add(?
z?z kz
)
?0, kavza??a to the ?? at z?s source state.
3.5 Locally optimizing the objective
Recall that cq? associates with each [hc, hq, w] a
block of ? parameters that must be ? 0 and sum to
1. Our optimization method must enforce these con-
straints. A standard approach is to use a projected
gradient method, where after each gradient step on
?, the parameters are projected back onto the prob-
ability simplex. We implemented another standard
approach: reexpress each block of parameters {?a :
a ? A} as ?a
def
= exp ?a/
?
b?A exp ?b, as is possi-
ble iff the ?a parameters satisfy the constraints. We
then follow the gradient of r? with respect to the new
? parameters, given by ?r?/??a = ?a(?r?/??a?EA)
where EA =
?
b ?b(?r?/??b).
Another common approach is block coordinate
ascent on ? and ??this is ?variational EM.? M-
step: Given ?, we can easily find optimal esti-
mates of the emission and transition probabilities ?.
They are respectively proportional to the posterior
expected counts of arcs a and paths za under cq?,
namely N ? ?aka?a and N ? ?z
(?
z?z kz
)
ka?a.
E-step: Given ?, we cannot easily find the opti-
mal ? (even if nq = np).18 This was the rea-
17First components ?a and ?a remain as in cq?. ??a sums
paths to a. ??ka, 0???a can?t quite sum over paths starting with
a (their early weights depend on z), but (?r?)(2) corrects this.
18Recall that cq? must have locally normalized probabilities
(to ensure that its marginal is c). If nq = np, the optimal ?
is as follows: we can reduce the variational gap to 0 by setting
137
son for gradient ascent. However, for any single
sum-to-1 block of parameters {?a : a ? A}, it
is easy to find the optimal values if the others are
held fixed. We maximize LA
def
= r? + ?A
?
a?A ?a,
where ?A is a Lagrange multiplier chosen so that
the sum is 1. The partial derivative ?r?/??a can be
found using methods of section 3.4, restricting the
sums to za for the given a. For example, follow-
ing paragraphs 2?3 of section 3.4, let ??a, ra?
def
=
?
z?Za ??za, rza? where ??za, rza?
def
= ??za??za.19
Setting ?LA/??a = 0 implies that ?a is propor-
tional to exp((ra +
?
z?Za ?za log ?za)/?a).
20
Rather than doing block coordinate ascent by up-
dating one ? block at a time (and then recomputing
ra values for all blocks, which is slow), one can take
an approximate step by updating all blocks in paral-
lel. We find that replacing the E-step with a single
parallel step still tends to improve the objective, and
that this approximate variational EM is faster than
gradient ascent with comparable results.21
4 Experiments
4.1 Constrained unsupervised HMM learning
We follow the unsupervised POS tagging setup of
Merialdo (1994) and many others (Smith and Eis-
ner, 2005; Haghighi and Klein, 2006; Toutanova and
Johnson, 2007; Goldwater and Griffiths, 2007; John-
son, 2007). Given a corpus of sentences, one seeks
the maximum-likelihood or MAP parameters of a bi-
gram HMM (np = 2). The observed sentences, for
q?(t | hcw, hq) to the probability that t begins with t if we
randomly draw a suffix w ? c(? | hcw) and randomly tag ww
with t ? p?(? | ww, hq). This is equivalent to using p? with the
backward algorithm to conditionally tag each possible suffix.
19The first component of ??za??za is ?za?za = ?za ? 1.
20If a is an arc of cq? then ?r?/??a is the second component
of
?
z?Za
??za(?k?za/??a)??za. Then ?LA/??a works out to?
z?Za
ca(rza+?za(log ?za?log ?a?1))+?A. Set to 0 and
solve for ?a, noting that ca, ?a, ?A are constant over a ? A.
21In retrospect, an even faster strategy might be to do a series
of block ? and ?? updates, updating ?? at a state (footnote 10) im-
mediately after updating ? on the arcs leading from that state,
which allows a better block update at predecessor states. On an
acyclic machine, a single backward pass of this sort will reduce
the variational gap to 0 if nq = np (footnote 18). This is be-
cause, thanks to the up-to-date ??, each block of arcs gets new ?
weights in proportion to relative suffix path probabilities under
the new ?. After this backward pass, a single forward pass can
update the ? values and collect expected counts for the M-step
that will update ?. Standard EM is a special case of this strategy.
us, are replaced by the faux sentences extrapolated
from observed n-grams via the language model c.
The states of the HMM correspond to POS tags as
in Figure 1. All transitions are allowed, but not all
emissions. If a word is listed in a provided ?dictio-
nary? with its possible tags, then other tags are given
0 probability of emitting that word. The EM algo-
rithm uses the corpus to learn transition and emis-
sion probabilities that explain the data under this
constraint. The constraint ensures that the learned
states have something to do with true POS tags.
Merialdo (1994) spawned a long line of work
on this task. Ideas have included Bayesian learn-
ing methods (MacKay, 1997; Goldwater and Grif-
fiths, 2007; Johnson, 2007), better initial parame-
ters (Goldberg et al, 2008), and learning how to
constrain the possible parts of speech for a word
(Ravi and Knight, 2008), as well as non-HMM se-
quence models (Smith and Eisner, 2005; Haghighi
and Klein, 2006; Toutanova and Johnson, 2007).
Most of this work has used the Penn Treebank
(Marcus et al, 1993) as a dataset. While this
million-word Wall Street Journal (WSJ) corpus is
one of the largest that is manually annotated with
parts of speech, unsupervised learning methods
could take advantage of vast amounts of unannotated
text. In practice, runtime concerns have sometimes
led researchers to use small subsets of the Penn Tree-
bank (Goldwater and Griffiths, 2007; Smith and Eis-
ner, 2005; Haghighi and Klein, 2006). Our goal is
to point the way to using even larger datasets.
The reason for all this past research is that (Meri-
aldo, 1994) was a negative result: while EM is
guaranteed to improve the model?s likelihood, it de-
grades the match between the latent states and true
parts of speech (if the starting point is a good one
obtained with some supervision). Thus, for the task
of POS induction, there must be something wrong
with the HMM model, the likelihood objective, or
the search procedure. It is clear that the model is far
too weak: there are many latent variables in natural
language, so the HMM may be picking up on some-
thing other than POS tags. Ultimately, fixing this
will require richer models with many more param-
eters. But learning these (lexically specific) param-
eters will require large training datasets?hence our
present methodological exploration on whether it is
possible to scale up the original setting.
138
4.2 Setup
We investigate how much performance degrades
when we approximate the corpus and train approx-
imately with nq = 1. We examine two measures:
likelihood on a held-out corpus and accuracy in POS
tagging. We train on corpora of three different sizes:
?WSJ-big (910k words? 441k n-grams @ cutoff 3),
? Giga-20 (20M words? 2.9M n-grams @ cutoff 10),
? Giga-200 (200M wds? 14.4M n-grams @ cutoff 20).
These were drawn from the Penn Treebank (sections
2?23) and the English Gigaword corpus (Parker et
al., 2009). For held-out evaluation, we use WSJ-
small (Penn Treebank section 0) or WSJ-big.
We estimate backoff language models for these
corpora based on collections of n-grams with n ? 5.
In this work, we select the n-grams by simple count
cutoffs as shown above,22 taking care to keep all 2-
grams as mentioned in footnote 2.
Similar to Merialdo (1994), we use a tag dictio-
nary which limits the possible tags of a word to those
it was observed with in the WSJ, provided that the
word was observed at least 5 times in the WSJ. We
used the reduced tagset of Smith and Eisner (2005),
which collapses the original 45 fine-grained part-of-
speech tags into just 17 coarser tags.
4.3 Results
In all experiments, our method achieves similar ac-
curacy though slightly worse likelihood. Although
this method is meant to be a fast approximation of
EM, standard EM is faster on the smallest dataset
(WSJ-big). This is because this corpus is not much
bigger than the 5-gram language model built from it
(at our current pruning level), and so the overhead
of the more complex n-gram EM method is a net
disadvantage. However, when moving to larger cor-
pora, the iterations of n-gram EM become as fast as
standard EM and then faster. We expect this trend
to continue as one moves to much larger datasets, as
the compression ratio of the pruned language model
relative to the original corpus will only improve.
The Google n-gram corpus is based on 50? more
data than our largest but could be handled in RAM.
22Entropy-based pruning (Stolcke, 2000) may be a better se-
lection method when one is in a position to choose. However,
count cutoffs were already used in the creation of the Google
n-gram corpus, and more complex methods of pruning may not
be practical for very large datasets.
 72
 74
 76
 78
 80
 82
 84
 86
Acc
ura
cy
Time
EM (WSJ-big)N-gram EM (WSJ-big)EM (Giga-20)N-gram EM (Giga-20)EM (Giga-200)N-gram EM (Giga-200)
Like
liho
od
Time
EM (WSJ-big)N-gram EM (WSJ-big)EM (Giga-20)N-gram EM (Giga-20)EM (Giga-200)N-gram EM (Giga-200)
Figure 2: POS-tagging accuracy and log-likelihood af-
ter each iteration, measured on WSJ-big when training
on the Gigaword datasets, else on WSJ-small. Runtime
and log-likelihood are scaled differently for each dataset.
Replacing EM with our method changes runtime per it-
eration from 1.4s? 3.5s, 48s? 47s, and 506s? 321s.
5 Conclusions
We presented a general approach to training genera-
tive models on a distribution rather than on a training
sample. We gave several motivations for this novel
problem. We formulated an objective function simi-
lar to MAP, and presented a variational lower bound.
Algorithmically, we gave nontrivial general meth-
ods for computing and optimizing our variational
lower bound for arbitrary finite-state data distribu-
tions c, generative models p, and variational fami-
lies q, provided that p and q are unambiguous same-
length FSTs. We also gave details for specific useful
families for c, p, and q.
As proof of principle, we used a traditional HMM
POS tagging task to demonstrate that we can train
a model from n-grams almost as accurately as from
full sentences, and do so faster to the extent that the
n-gram dataset is smaller. More generally, we offer
our approach as an intriguing new tool to help semi-
supervised learning benefit from very large datasets.
139
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proc. of ACL, pages 40?47.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In Proc. of IJCAI.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Linguistic Data Consortium, Philadelphia.
LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proc. of EMNLP.
Tim Dawborn and James R. Curran. 2009. CCG
parsing with one syntactic structure per n-gram. In
Australasian Language Technology Association Work-
shop, pages 71?79.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proc. of ACL, pages 1?8.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, pages 746?754.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proc. of ACL, pages 744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proc. of NAACL,
pages 320?327.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A
spectral algorithm for learning hidden Markov models.
In Proc. of COLT.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL, pages
296?305.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. In M. I. Jordan, editor, Learning
in Graphical Models. Kluwer.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of
EMNLP, pages 40?51.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: Trading structure for features.
In International Conference on Machine Learning
(ICML), Helsinki, Finland.
D. Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2009. Unsupervised ac-
quisition of lexical knowledge from n-grams. Sum-
mer workshop technical report, Center for Language
and Speech Processing, Johns Hopkins University.
David J. C. MacKay. 1997. Ensemble learning for hid-
den Markov models. http://www.inference.
phy.cam.ac.uk/mackay/abstracts/
ensemblePaper.html.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov models for
information extraction and segmentation. In Proc. of
ICML, pages 591?598.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2):155?
171.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, W. Brockman, The Google Books Team, J. P.
Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant,
S. Pinker, M. A. Nowak, and E. L. Aiden. 2010.
Quantitative analysis of culture using millions of digi-
tized books. Science, 331(6014):176?182.
Mehryar Mohri and Mark-Jan Nederhof. 2001. Regu-
lar approximation of context-free grammars through
transformation. In Jean-Claude Junqua and Gert-
jan van Noord, editors, Robustness in Language and
Speech Technology, chapter 9, pages 153?163. Kluwer
Academic Publishers, The Netherlands, February.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17?32, January.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view
of the EM algorithm that justifies incremental, sparse,
and other variants. In M.I. Jordan, editor, Learning in
Graphical Models, pages 355?368. Kluwer.
Mark-Jan Nederhof. 2000. Practical experiments
with regular approximation of context-free languages.
Computational Linguistics, 26(1).
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth
edition. Linguistic Data Consortium, Philadelphia.
LDC2009T13.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In
Proc. of IJCAI, pages 1124?1129.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
140
recognition. Proc. of the IEEE, 77(2):257?286, Febru-
ary.
Sujith Ravi and Kevin Knight. 2008. Minimized models
for unsupervised part-of-speech tagging. In Proc. of
ACL, pages 504?512.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. of ACL, pages 354?362.
R. Staden. 1979. A strategy of DNA sequencing em-
ploying computer programs. Nucleic Acids Research,
6(7):2601?2610, June.
Andreas Stolcke. 2000. Entropy-based pruning of back-
off language models. In DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Proc. of NIPS, volume 20.
141
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Review of Hypothesis Alignment Algorithms for MT System Combination
via Confusion Network Decoding
Antti-Veikko I. Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,
Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing Zhangf
aApple Inc., Cupertino, CA 95014
arosti@apple.com
bMicrosoft Research, Redmond, WA 98052
xiaohe@microsoft.com
cJohns Hopkins University, Baltimore, MD 21218
{damianos,yuan.cao,jrsmith}@jhu.edu
dSAIC, Monheimsallee 22, D-52062 Aachen, Germany
gregor.leusch@saic.com
eRWTH Aachen University, D-52056 Aachen, Germany
{freitag,ney}@cs.rwth-aachen.de
fRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,bzhang}@bbn.com
Abstract
Confusion network decoding has proven to
be one of the most successful approaches
to machine translation system combination.
The hypothesis alignment algorithm is a cru-
cial part of building the confusion networks
and many alternatives have been proposed in
the literature. This paper describes a sys-
tematic comparison of five well known hy-
pothesis alignment algorithms for MT sys-
tem combination via confusion network de-
coding. Controlled experiments using identi-
cal pre-processing, decoding, and weight tun-
ing methods on standard system combina-
tion evaluation sets are presented. Transla-
tion quality is assessed using case insensitive
BLEU scores and bootstrapping is used to es-
tablish statistical significance of the score dif-
ferences. All aligners yield significant BLEU
score gains over the best individual system in-
cluded in the combination. Incremental indi-
rect hidden Markov model and a novel incre-
mental inversion transduction grammar with
flexible matching consistently yield the best
translation quality, though keeping all things
equal, the differences between aligners are rel-
atively small.
?The work reported in this paper was carried out while the
authors were at Raytheon BBN Technologies and
?RWTH Aachen University.
1 Introduction
Current machine translation (MT) systems are based
on different paradigms, such as rule-based, phrase-
based, hierarchical, and syntax-based. Due to the
complexity of the problem, systems make various
assumptions at different levels of processing and
modeling. Many of these assumptions may be
suboptimal and complementary. The complemen-
tary information in the outputs from multiple MT
systems may be exploited by system combination.
Availability of multiple system outputs within the
DARPA GALE program as well as NIST Open MT
and Workshop on Statistical Machine Translation
evaluations has led to extensive research in combin-
ing the strengths of diverse MT systems, resulting in
significant gains in translation quality.
System combination methods proposed in the lit-
erature can be roughly divided into three categories:
(i) hypothesis selection (Rosti et al, 2007b; Hilde-
brand and Vogel, 2008), (ii) re-decoding (Frederking
and Nirenburg, 1994; Jayaraman and Lavie, 2005;
Rosti et al, 2007b; He and Toutanova, 2009; De-
vlin et al, 2011), and (iii) confusion network de-
coding. Confusion network decoding has proven to
be the most popular as it does not require deep N -
best lists1 and operates on the surface strings. It has
1N -best lists of around N = 10 have been used in confu-
sion network decoding yielding small gains over using 1-best
191
also been shown to be very successful in combining
speech recognition outputs (Fiscus, 1997; Mangu et
al., 2000). The first application of confusion net-
work decoding in MT system combination appeared
in (Bangalore et al, 2001) where a multiple string
alignment (MSA), made popular in biological se-
quence analysis, was applied to the MT system out-
puts. Matusov et al (2006) proposed an alignment
based on GIZA++ Toolkit which introduced word
reordering not present in MSA, and Sim et al (2007)
used the alignments produced by the translation edit
rate (TER) (Snover et al, 2006) scoring. Extensions
of the last two are included in this study together
with alignments based on hidden Markov model
(HMM) (Vogel et al, 1996) and inversion transduc-
tion grammars (ITG) (Wu, 1997).
System combinations produced via confusion net-
work decoding using different hypothesis alignment
algorithms have been entered into open evalua-
tions, most recently in 2011 Workshop on Statistical
Machine Translation (Callison-Burch et al, 2011).
However, there has not been a comparison of the
most popular hypothesis alignment algorithms us-
ing the same sets of MT system outputs and other-
wise identical combination pipelines. This paper at-
tempts to systematically compare the quality of five
hypothesis alignment algorithms. Alignments were
produced for the same system outputs from three
common test sets used in the 2009 NIST Open MT
Evaluation and the 2011 Workshop on Statistical
Machine Translation. Identical pre-processing, de-
coding, and weight tuning algorithms were used to
quantitatively evaluate the alignment quality. Case
insensitive BLEU score (Papineni et al, 2002) was
used as the translation quality metric.
2 Confusion Network Decoding
A confusion network is a linear graph where all
paths visit all nodes. Two consecutive nodes may be
connected by one or more arcs. Given the arcs repre-
sent words in hypotheses, multiple arcs connecting
two consecutive nodes can be viewed as alternative
words in that position of a set of hypotheses encoded
by the network. A special NULL token represents
a skipped word and will not appear in the system
combination output. For example, three hypotheses
outputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?
may be aligned as follows:
twelve big blue cars
twelve NULL NULL cars
dozen NULL blue cars
This alignment may be represented compactly as the
confusion network in Figure 1 which encodes a total
of eight unique hypotheses.
40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)
Figure 1: Confusion network from three strings ?twelve
big blue cars?, ?twelve cars?, and ?dozen blue cars? us-
ing the first as the skeleton. The numbers in parentheses
represent counts of words aligned to the corresponding
arc.
Building confusion networks from multiple ma-
chine translation system outputs has two main prob-
lems. First, one output has to be chosen as the skele-
ton hypothesis which defines the final word order of
the system combination output. Second, MT system
outputs may have very different word orders which
complicates the alignment process. For skeleton se-
lection, Sim et al (2007) proposed choosing the out-
put closest to all other hypotheses when using each
as the reference string in TER. Alternatively, Ma-
tusov et al (2006) proposed leaving the decision to
decoding time by connecting networks built using
each output as a skeleton into a large lattice. The
subnetworks in the latter approach may be weighted
by prior probabilities estimated from the alignment
statistics (Rosti et al, 2007a). Since different align-
ment algorithm produce different statistics and the
gain from the weights is relatively small (Rosti et al,
2011), weights for the subnetworks were not used
in this work. The hypothesis alignment algorithms
used in this work are briefly described in the follow-
ing section.
The confusion networks in this work were repre-
sented in a text lattice format shown in Figure 2.
Each line corresponds to an arc, where J is the arc
index, S is the start node index, E is the end node in-
dex, SC is the score vector, and W is the word label.
The score vector has as many elements as there are
input systems. The elements correspond to each sys-
tem and indicate whether a word from a particular
192
J=0 S=0 E=1 SC=(1,1,0) W=twelve
J=1 S=0 E=1 SC=(0,0,1) W=dozen
J=2 S=1 E=2 SC=(1,0,0) W=big
J=3 S=1 E=2 SC=(0,1,1) W=NULL
J=4 S=2 E=3 SC=(1,0,1) W=blue
J=5 S=2 E=3 SC=(0,1,0) W=NULL
J=6 S=3 E=4 SC=(1,1,1) W=cars
Figure 2: A lattice in text format representing the con-
fusion network in Figure 1. J is the arc index, S and E
are the start and end node indexes, SC is a vector of arc
scores, and W is the word label.
system was aligned to a given link2. These may be
viewed as system specific word confidences, which
are binary when aligning 1-best system outputs. If
no word from a hypothesis is aligned to a given link,
a NULL word token is generated provided one does
not already exist, and the corresponding element in
the NULL word token is set to one. The system
specific word scores are kept separate in order to
exploit system weights in decoding. Given system
weights wn, which sum to one, and system specific
word scores snj for each arc j (the SC elements), the
weighted word scores are defined as:
sj =
Ns?
n=1
wnsnj (1)
where Ns is the number of input systems. The hy-
pothesis score is defined as the sum of the log-word-
scores along the path, which is linearly interpolated
with a logarithm of the language model (LM) score
and a non-NULL word count:
S(E|F ) =
?
j?J (E)
log sj + ?SLM (E) + ?Nw(E)
(2)
where J (E) is the sequence of arcs generating the
hypothesis E for the source sentence F , SLM (E)
is the LM score, and Nw(E) is the number of
non-NULL words. The set of weights ? =
{w1, . . . , wNs , ?, ?} can be tuned so as to optimize
an evaluation metric on a development set.
Decoding with an n-gram language model re-
quires expanding the lattice to distinguish paths with
2A link is used as a synonym to the set of arcs between two
consecutive nodes. The name refers to the confusion network
structure?s resemblance to a sausage.
unique n-gram contexts before LM scores can be as-
signed the arcs. Using long n-gram context may re-
quire pruning to reduce memory usage. Given uni-
form initial system weights, pruning may remove
desirable paths. In this work, the lattices were ex-
panded to bi-gram context and no pruning was per-
formed. A set of bi-gram decoding weights were
tuned directly on the expanded lattices using a dis-
tributed optimizer (Rosti et al, 2010). Since the
score in Equation 2 is not a simple log-linear inter-
polation, the standard minimum error rate training
(Och, 2003) with exact line search cannot be used.
Instead, downhill simplex (Press et al, 2007) was
used in the optimizer client. After bi-gram decod-
ing weight optimization, another set of 5-gram re-
scoring weights were tuned on 300-best lists gener-
ated from the bi-gram expanded lattices.
3 Hypothesis Alignment Algorithms
Two different methods have been proposed for
building confusion networks: pairwise and incre-
mental alignment. In pairwise alignment, each
hypothesis corresponding to a source sentence is
aligned independently with the skeleton hypothe-
sis. This set of alignments is consolidated using the
skeleton words as anchors to form the confusion net-
work (Matusov et al, 2006; Sim et al, 2007). The
same word in two hypotheses may be aligned with a
different word in the skeleton resulting in repetition
in the network. A two-pass alignment algorithm to
improve pairwise TER alignments was introduced in
(Ayan et al, 2008). In incremental alignment (Rosti
et al, 2008), the confusion network is initialized by
forming a simple graph with one word per link from
the skeleton hypothesis. Each remaining hypothesis
is aligned with the partial confusion network, which
allows words from all previous hypotheses be con-
sidered as matches. The order in which the hypothe-
ses are aligned may influence the alignment qual-
ity. Rosti et al (2009) proposed a sentence specific
alignment order by choosing the unaligned hypoth-
esis closest to the partial confusion network accord-
ing to TER. The following five alignment algorithms
were used in this study.
193
3.1 Pairwise GIZA++ Enhanced Hypothesis
Alignment
Matusov et al (2006) proposed using the GIZA++
Toolkit (Och and Ney, 2003) to align a set of tar-
get language translations. A parallel corpus where
each system output acting as a skeleton appears as
a translation of all system outputs corresponding to
the same source sentence. The IBM Model 1 (Brown
et al, 1993) and hidden Markov model (HMM) (Vo-
gel et al, 1996) are used to estimate the alignment.
Alignments from both ?translation? directions are
used to obtain symmetrized alignments by interpo-
lating the HMM occupation statistics (Matusov et
al., 2004). The algorithm may benefit from the fact
that it considers the entire test set when estimating
the alignment model parameters; i.e., word align-
ment links from all output sentences influence the
estimation, whereas other alignment algorithms only
consider words within a pair of sentences (pairwise
alignment) or all outputs corresponding to a single
source sentence (incremental alignment). However,
it does not naturally extend to incremental align-
ment. The monotone one-to-one alignments are then
transformed into a confusion network. This aligner
is referred to as GIZA later in this paper.
3.2 Incremental Indirect Hidden Markov
Model Alignment
He et al (2008) proposed using an indirect hidden
Markov model (IHMM) for pairwise alignment of
system outputs. The parameters of the IHMM are
estimated indirectly from a variety of sources in-
cluding semantic word similarity, surface word sim-
ilarity, and a distance-based distortion penalty. The
alignment between two target language outputs are
treated as the hidden states. A standard Viterbi al-
gorithm is used to infer the alignment. The pair-
wise IHMM was extended to operate incrementally
in (Li et al, 2009). Sentence specific alignment or-
der is not used by this aligner, which is referred to
as iIHMM later in this paper.
3.3 Incremental Inversion Transduction
Grammar Alignment with Flexible
Matching
Karakos et al (2008) proposed using inversion trans-
duction grammars (ITG) (Wu, 1997) for pairwise
alignment of system outputs. ITGs form an edit
distance, invWER (Leusch et al, 2003), that per-
mits properly nested block movements of substrings.
For well-formed sentences, this may be more nat-
ural than allowing arbitrary shifts. The ITG algo-
rithm is very expensive due to its O(n6) complexity.
The search algorithm for the best ITG alignment, a
best-first chart parsing (Charniak et al, 1998), was
augmented with an A? search heuristic of quadratic
complexity (Klein and Manning, 2003), resulting in
significant reduction in computational complexity.
The finite state-machine heuristic computes a lower
bound to the alignment cost of two strings by allow-
ing arbitrary word re-orderings. The ITG hypothesis
alignment algorithm was extended to operate incre-
mentally in (Karakos et al, 2010) and a novel ver-
sion where the cost function is computed based on
the stem/synonym similarity of (Snover et al, 2009)
was used in this work. Also, a sentence specific
alignment order was used. This aligner is referred
to as iITGp later in this paper.
3.4 Incremental Translation Edit Rate
Alignment with Flexible Matching
Sim et al (2007) proposed using translation edit rate
scorer3 to obtain pairwise alignment of system out-
puts. The TER scorer tries to find shifts of blocks
of words that minimize the edit distance between
the shifted reference and a hypothesis. Due to the
computational complexity, a set of heuristics is used
to reduce the run time (Snover et al, 2006). The
pairwise TER hypothesis alignment algorithm was
extended to operate incrementally in (Rosti et al,
2008) and also extended to consider synonym and
stem matches in (Rosti et al, 2009). The shift
heuristics were relaxed for flexible matching to al-
low shifts of blocks of words as long as the edit dis-
tance is decreased even if there is no exact match in
the new position. A sentence specific alignment or-
der was used by this aligner, which is referred to as
iTER later in this paper.
3.5 Incremental Translation Edit Rate Plus
Alignment
Snover et al (2009) extended TER scoring to con-
sider synonyms and paraphrase matches, called
3http://www.cs.umd.edu/?snover/tercom/
194
TER-plus (TERp). The shift heuristics in TERp
were also relaxed relative to TER. Shifts are allowed
if the words being shifted are: (i) exactly the same,
(ii) synonyms, stems or paraphrases of the corre-
sponding reference words, or (iii) any such combina-
tion. Xu et al (2011) proposed using an incremental
version of TERp for building consensus networks. A
sentence specific alignment order was used by this
aligner, which is referred to as iTERp later in this
paper.
4 Experimental Evaluation
Combination experiments were performed on (i)
Arabic-English, from the informal system combi-
nation track of the 2009 NIST Open MT Evalua-
tion4; (ii) German-English from the system com-
bination evaluation of the 2011 Workshop on Sta-
tistical Machine Translation (Callison-Burch et al,
2011) (WMT11) and (iii) Spanish-English, again
from WMT11. Eight top-performing systems (as
evaluated using case-insensitive BLEU) were used
in each language pair. Case insensitive BLEU scores
for the individual system outputs on the tuning and
test sets are shown in Table 1. About 300 and
800 sentences with four reference translations were
available for Arabic-English tune and test sets, re-
spectively, and about 500 and 2500 sentences with a
single reference translation were available for both
German-English and Spanish-English tune and test
sets. The system outputs were lower-cased and to-
kenized before building confusion networks using
the five hypothesis alignment algorithms described
above. Unpruned English bi-gram and 5-gram lan-
guage models were trained with about 6 billion
words available for these evaluations. Multiple com-
ponent language models were trained after dividing
the monolingual corpora by source. Separate sets
of interpolation weights were tuned for the NIST
and WMT experiments to minimize perplexity on
the English reference translations of the previous
evaluations, NIST MT08 and WMT10. The sys-
tem combination weights, both bi-gram lattice de-
coding and 5-gram 300-best list re-scoring weights,
were tuned separately for lattices build with each hy-
pothesis alignment algorithm. The final re-scoring
4http://www.itl.nist.gov/iad/mig/tests/
mt/2009/ResultsRelease/indexISC.html
outputs were detokenized before computing case in-
sensitive BLEU scores. Statistical significance was
computed for each pairwise comparison using boot-
strapping (Koehn, 2004).
Decode Oracle
Aligner tune test tune test
GIZA 60.06 57.95 75.06 74.47
iTER 59.74 58.63? 73.84 73.20
iTERp 60.18 59.05? 76.43 75.58
iIHMM 60.51 59.27?? 76.50 76.17
iITGp 60.65 59.37?? 76.53 76.05
Table 2: Case insensitive BLEU scores for NIST MT09
Arabic-English system combination outputs. Note, four
reference translations were available. Decode corre-
sponds to results after weight tuning and Oracle corre-
sponds to graph TER oracle. Dagger (?) denotes statisti-
cally significant difference compared to GIZA and double
dagger (?) compared to iTERp and the aligners above it.
The BLEU scores for Arabic-English system
combination outputs are shown in Table 2. The first
column (Decode) shows the scores on tune and test
sets for the decoding outputs. The second column
(Oracle) shows the scores for oracle hypotheses ob-
tained by aligning the reference translations with the
confusion networks and choosing the path with low-
est graph TER (Rosti et al, 2008). The rows rep-
resenting different aligners are sorted according to
the test set decoding scores. The order of the BLEU
scores for the oracle translations do not always fol-
low the order for the decoding outputs. This may be
due to differences in the compactness of the confu-
sion networks. A more compact network has fewer
paths and is therefore less likely to contain signif-
icant parts of the reference translation, whereas a
reference translation may be generated from a less
compact network. On Arabic-English, all incremen-
tal alignment algorithms are significantly better than
the pairwise GIZA, incremental IHMM and ITG
with flexible matching are significantly better than
all other algorithms, but not significantly different
from each other. The incremental TER and TERp
were statistically indistinguishable. Without flexi-
ble matching, iITG yields a BLEU score of 58.85 on
test. The absolute BLEU gain over the best individ-
ual system was between 6.2 and 7.6 points on the
test set.
195
Arabic German Spanish
System tune test tune test tune test
A 48.84 48.54 21.96 21.41 27.71 27.13
B 49.15 48.97 22.61 21.80 28.42 27.90
C 49.30 49.50 22.77 21.99 28.57 28.23
D 49.38 49.59 22.90 22.41 29.00 28.41
E 49.42 49.75 22.90 22.65 29.15 28.50
F 50.28 50.69 22.98 22.65 29.53 28.61
G 51.49 50.81 23.41 23.06 29.89 29.82
H 51.72 51.74 24.28 24.16 30.55 30.14
Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source
languages.
Decode Oracle
Aligner tune test tune test
GIZA 25.93 26.02 37.32 38.22
iTERp 26.46 26.10 38.16 38.76
iTER 26.27 26.39? 37.00 37.66
iIHMM 26.34 26.40? 37.87 38.48
iITGp 26.47 26.50? 37.99 38.60
Table 3: Case insensitive BLEU scores for WMT11
German-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
iTERp and GIZA.
The BLEU scores for German-English system
combination outputs are shown in Table 3. Again,
the graph TER oracle scores do not follow the same
order as the decoding scores. The scores for GIZA
and iTERp are statistically indistinguishable, and
iTER, iIHMM, and iITGp are significantly better
than the first two. However, they are not statistically
different from each other. Without flexible match-
ing, iITG yields a BLEU score of 26.47 on test. The
absolute BLEU gain over the best individual system
was between 1.9 and 2.3 points on the test set.
The BLEU scores for Spanish-English system
combination outputs are shown in Table 4. All align-
ers but iIHMM are statistically indistinguishable and
iIHMM is significantly better than all other align-
ers. Without flexible matching, iITG yields a BLEU
score of 33.62 on test. The absolute BLEU gain over
the best individual system was between 3.5 and 3.9
Decode Oracle
Aligner tune test tune test
iTERp 34.20 33.61 50.45 51.28
GIZA 34.02 33.62 50.23 51.20
iTER 34.44 33.79 50.39 50.39
iITGp 34.41 33.85 50.55 51.33
iIHMM 34.61 34.05? 50.48 51.27
Table 4: Case insensitive BLEU scores for WMT11
Spanish-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
aligners above iIHMM.
points on the test set.
5 Error Analysis
Error analysis was performed to better understand
the gains from system combination. Specifically, (i)
how the different types of translation errors are af-
fected by system combination was investigated; and
(ii) an attempt to quantify the correlation between
the word agreement that results from the different
aligners and the translation error, as measured by
TER (Snover et al, 2006), was made.
5.1 Influence on Error Types
For each one of the individual systems, and for each
one of the three language pairs, the per-sentence er-
rors that resulted from that system, as well as from
each one of the the different aligners studied in this
paper, were computed. The errors were broken
196
down into insertions/deletions/substitutions/shifts
based on the TER scorer.
The error counts at the document level were ag-
gregated. For each document in each collection, the
number of errors of each type that resulted from each
individual system as well as each system combina-
tion were measured, and their difference was com-
puted. If the differences are mostly positive, then
it can be said (with some confidence) that system
combination has a significant impact in reducing the
error of that type. A paired Wilcoxon test was per-
formed and the p-value that quantifies the probabil-
ity that the measured error reduction was achieved
under the null hypothesis that the system combina-
tion performs as well as the best system was com-
puted.
Table 5 shows all conditions under consideration.
All cases where the p-value is below 10?2 are con-
sidered statistically significant. Two observations
are in order: (i) all alignment schemes significantly
reduce the number of substitution/shift errors; (ii)
in the case of insertions/deletions, there is no clear
trend; there are cases where the system combination
increases the number of insertions/deletions, com-
pared to the individual systems.
5.2 Relationship between Word Agreement
and Translation Error
This set of experiments aimed to quantify the rela-
tionship between the translation error rate and the
amount of agreement that resulted from each align-
ment scheme. The amount of system agreement at
a level x is measured by the number of cases (con-
fusion network arcs) where x system outputs con-
tribute the same word in a confusion network bin.
For example, the agreement at level 2 is equal to 2
in Figure 1 because there are exactly 2 arcs (with
words ?twelve? and ?blue?) that resulted from the
agreement of 2 systems. Similarly, the agreement at
level 3 is 1, because there is only 1 arc (with word
?cars?) that resulted from the agreement of 3 sys-
tems. It is hypothesized that a sufficiently high level
of agreement should be indicative of the correctness
of a word (and thus indicative of lower TER). The
agreement statistics were grouped into two values:
the ?weak? agreement statistic, where at most half
of the combined systems contribute a word, and the
?strong? agreement statistic, where more than half
non-NULL words NULL words
weak strong weak strong
Arabic 0.087 -0.068 0.192 0.094
German 0.117 -0.067 0.206 0.147
Spanish 0.085 -0.134 0.323 0.102
Table 6: Regression coefficients of the ?strong? and
?weak? agreement features, as computed with a gener-
alized linear model, using TER as the target variable.
of the combined systems contribute a word. To sig-
nify the fact that real words and ?NULL? tokens
have different roles and should be treated separately,
two sets of agreement statistics were computed.
A regression with a generalized linear model
(glm) that computed the coefficients of the agree-
ment quantities (as explained above) for each align-
ment scheme, using TER as the target variable, was
performed. Table 6 shows the regression coeffi-
cients; they are all significant at p-value < 0.001.
As is clear from this table, the negative coefficient of
the ?strong? agreement quantity for the non-NULL
words points to the fact that good aligners tend to
result in reductions in translation error. Further-
more, increasing agreements on NULL tokens does
not seem to reduce TER.
6 Conclusions
This paper presented a systematic comparison of
five different hypothesis alignment algorithms for
MT system combination via confusion network de-
coding. Pre-processing, decoding, and weight tun-
ing were controlled and only the alignment algo-
rithm was varied. Translation quality was compared
qualitatively using case insensitive BLEU scores.
The results showed that confusion network decod-
ing yields a significant gain over the best individ-
ual system irrespective of the alignment algorithm.
Differences between the combination output using
different alignment algorithms were relatively small,
but incremental alignment consistently yielded bet-
ter translation quality compared to pairwise align-
ment based on these experiments and previously
published literature. Incremental IHMM and a novel
incremental ITG with flexible matching consistently
yield highest quality combination outputs. Further-
more, an error analysis shows that most of the per-
197
Language Aligner ins del sub shft
GIZA 2.2e-16 0.9999 2.2e-16 2.2e-16
iHMM 2.2e-16 0.433 2.2e-16 2.2e-16
Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16
iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16
iTERp 2.2e-16 1 2.2e-16 2.2e-16
GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16
iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16
German iITGp 0.8551 0.2848 2.2e-16 2.2e-16
iTER 0.2491 1.233e-07 2.2e-16 2.2e-16
iTERp 0.9997 0.007489 2.2e-16 2.2e-16
GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16
iHMM 2.2e-16 1 2.2e-16 2.2e-16
Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16
iTER 2.2e-16 1 2.2e-16 2.2e-16
iTERp 3.335e-16 1 2.2e-16 2.2e-16
Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.
formance gains from system combination can be at-
tributed to reductions in substitution errors and word
re-ordering errors. Finally, better alignments of sys-
tem outputs, which tend to cause higher agreement
rates on words, correlate with reductions in transla-
tion error.
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Proc.
Coling, pages 33?40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Proc.
WMT, pages 22?64.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proc.
Sixth Workshop on Very Large Corpora, pages 127?
133. Morgan Kaufmann.
Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-
ishnan, and Spyros Matsoukas. 2011. System combi-
nation using discriminative cross-adaptation. In Proc.
IJCNLP, pages 667?675.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347?354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95?
100.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. EMNLP, pages 1202?1211.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 98?
107.
Almut S. Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis
selection from combined n-best lists. In AMTA, pages
254?261.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL, pages 81?84.
Damianos Karakos, Jason R. Smith, and Sanjeev Khu-
danpur. 2010. Hypothesis ranking and two-pass ap-
proaches for machine translation system combination.
In Proc. ICASSP.
198
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Proc.
NAACL, pages 40?47.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with appli-
cations to machine translation evaluation. In Proc. MT
Summit 2003, pages 240?247, September.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. ACL/IJCNLP, pages 949?957.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In Proc. COLING, pages 219?225.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33?40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311?318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Rirchard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. ACL, pages
312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proc. NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothesis
alignment with flexible matching for building confu-
sion networks: BBN system description for WMT09
system combination task. In Proc. WMT, pages 61?
65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In Proc.
WMT, pages 321?326.
Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,
Necip Fazil Ayan, Jason Eisner, Damianos Karakos,
Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-
ros Matsoukas, Hermann Ney, Richard Schwartz, Bing
Zhang, and Jing Zheng. 2011. Confusion network de-
coding for MT system combination. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language
Exploitation, pages 333?361. Springer.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy or
HTER? exploring different human judgments with a
tunable MT metric. In Proc. WMT, pages 259?268.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. ICCL, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the JHU system combination scheme
for WMT 2011. In Proc. WMT, pages 171?176.
199
