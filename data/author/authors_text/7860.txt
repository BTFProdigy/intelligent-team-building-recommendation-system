Semi-Supervised Training of a Kernel PCA-Based Model
for Word Sense Disambiguation
Weifeng SU Marine CARPUAT Dekai WU1
weifeng@cs.ust.hk marine@cs.ust.hk dekai@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology, Clear Water Bay, Hong Kong
Abstract
In this paper, we introduce a new semi-supervised learning
model for word sense disambiguation based on Kernel Prin-
cipal Component Analysis (KPCA), with experiments showing
that it can further improve accuracy over supervised KPCA
models that have achieved WSD accuracy superior to the best
published individual models. Although empirical results with
supervised KPCA models demonstrate significantly better ac-
curacy compared to the state-of-the-art achieved by either na??ve
Bayes or maximum entropy models on Senseval-2 data, we
identify specific sparse data conditions under which supervised
KPCA models deteriorate to essentially a most-frequent-sense
predictor. We discuss the potential of KPCA for leveraging
unannotated data for partially-unsupervised training to address
these issues, leading to a composite model that combines both
the supervised and semi-supervised models.
1 Introduction
Wu et al (2004) propose an efficient and accurate new
supervised learning model for word sense disambigua-
tion (WSD), that exploits a nonlinear Kernel Principal
Component Analysis (KPCA) technique to make pre-
dictions implicitly based on generalizations over feature
combinations. Experiments performed on the Senseval-
2 English lexical sample data show that KPCA-based
word sense disambiguation method is capable of outper-
forming other widely used WSD models including na??ve
Bayes, maximum entropy, and SVM models.
Despite the excellent performance of the supervised
KPCA-based WSD model on average, though, our fur-
ther error analysis investigations have suggested certain
limitations. In particular, the supervised KPCA-based
model often appears to perform poorly when it encoun-
ters target words whose contexts are highly dissimilar
to those of any previously seen instances in the train-
ing set. Empirically, the supervised KPCA-based model
nearly always disambiguates target words of this kind
to the most frequent sense. As a result, for this partic-
ular subset of test instances, the precision achieved by
the KPCA-based model is essentially no higher than the
precision achieved by the most-frequent-sense baseline
model (which simply always selects the most frequent
sense for the target word). The work reported in this pa-
per stems from a hypothesis that the most-frequent-sense
1The author would like to thank the Hong Kong Research Grants
Council (RGC) for supporting this research in part through grants
RGC6083/99E, RGC6256/00E, and DAG03/04.EG09.
strategy can be bettered for this category of errors.
This is a case of data sparseness, so the observation
should not be very surprising. Such behavior is to be ex-
pected from classifiers in general, and not just the KPCA-
based model. Put another way, even though KPCA is
able to generalize over combinations of dependent fea-
tures, there must be a sufficient number of training in-
stances from which to generalize.
The nature of KPCA, however, suggests a strategy that
is not applicable to many of the other conventional WSD
models. We propose a model in this paper that takes ad-
vantage of unsupervised training using large quantities of
unannotated corpora, to help compensate for sparse data.
Note that although we are using the WSD task to ex-
plain the model, in fact the proposed model is not lim-
ited to WSD applications. We have hypothesized that
the KPCA-based method is likely to be widely applica-
ble to other NLP tasks; since data sparseness is a com-
mon problem in many NLP tasks, a weakly-supervised
approach allowing the KPCA-based method to compen-
sate for data sparseness is highly desirable. The general
technique we describe here is applicable to any similar
classification task where insufficient labeled training data
is available.
The paper is organized as follows. After a brief look
at related work, we review the baseline supervised WSD
model, which is based on Kernel PCA. We then discuss
how data sparseness affects the model, and propose a
new semi-supervised model that takes advantage of un-
labeled data, along with a composite model that com-
bines both the supervised and semi-supervised models.
Finally, details of the experimental setup and compara-
tive results are given.
2 Related work
The long history of WSD research includes numerous
statistically trained methods; space only permits us to
summarize a few key points here. Na??ve Bayes models
(e.g., Mooney (1996), Chodorow et al (1999), Pedersen
(2001), Yarowsky and Florian (2002)) as well as max-
imum entropy models (e.g., Dang and Palmer (2002),
Klein and Manning (2002)) in particular have shown a
large degree of success for WSD, and have established
challenging state-of-the-art benchmarks. The Senseval
series of evaluations facilitates comparing the strengths
and weaknesses of various WSD models on common
data sets, with Senseval-1 (Kilgarriff and Rosenzweig,
1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held
in 1998, 2001, and 2004 respectively.
3 Supervised KPCA baseline model
Our baseline WSD model is a supervised learning model
that also makes use of Kernel Principal Component
Analysis (KPCA), proposed by (Scho?lkopf et al, 1998)
as a generalization of PCA. KPCA has been successfully
applied in many areas such as de-noising of images of
hand-written digits (Mika et al, 1999) and modeling the
distribution of non-linear data sets in the context of shape
modelling for real objects (Active Shape Models) (Twin-
ing and Taylor, 2001). In this section, we first review the
theory of KPCA and explanation of why it is suited for
WSD applications.
3.1 Kernel Principal Component Analysis
The Kernel Principal Component Analysis technique, or
KPCA, is a method of nonlinear principal component ex-
traction. A nonlinear function maps the n-dimensional
input vectors from their original space Rn to a high-
dimensional feature space F where linear PCA is per-
formed. In real applications, the nonlinear function is
usually not explicitly provided. Instead we use a kernel
function to implicitly define the nonlinear mapping; in
this respect KPCA is similar to Support Vector Machines
(Scho?lkopf et al, 1998).
Compared with other common analysis techniques,
KPCA has several advantages:
? As with other kernel methods it inherently takes
combinations of predictive features into account
when optimizing dimensionality reduction. For nat-
ural language problems in general, of course, it is
widely recognized that significant accuracy gains
can often be achieved by generalizing over relevant
feature combinations (e.g., Kudo and Matsumoto
(2003)).
? We can select suitable kernel function according to
the task we are dealing with and the knowledge we
have about the task.
? Another advantage of KPCA is that it is good at
dealing with input data with very high dimension-
ality, a condition where kernel methods excel.
Nonlinear principal components (Diamantaras and
Kung, 1996) may be defined as follows. Suppose we
are given a training set of M pairs (xt, ct) where the
observed vectors xt ? Rn in an n-dimensional input
space X represent the context of the target word being
disambiguated, and the correct class ct represents the
sense of the word, for t = 1, ..,M . Suppose ? is a
nonlinear mapping from the input space Rn to the fea-
ture space F . Without loss of generality we assume the
M vectors are centered vectors in the feature space, i.e.,
?M
t=1 ? (xt) = 0; uncentered vectors can easily be con-
verted to centered vectors (Scho?lkopf et al, 1998). We
wish to diagonalize the covariance matrix in F :
C =
1
M
M?
j=1
? (xj) ?
T (xj) (1)
To do this requires solving the equation ?v = Cv for
eigenvalues ? ? 0 and eigenvectors v ? F . Because
Cv =
1
M
M?
j=1
(?(xj) ? v)? (xj) (2)
we can derive the following two useful results. First,
? (?(xt) ? v) = ? (xt) ? Cv (3)
for t = 1, ..,M . Second, there exist ?i for i = 1, ...,M
such that
v =
M?
i=1
?i? (xi) (4)
Combining (1), (3), and (4), we obtain
M?
M?
i=1
?i (?(xt) ? ?(xi ))
=
M?
i=1
?i(? (xt) ?
M?
j=1
? (xj)) (?(xj) ? ?(xi ))
for t = 1, ..,M . Let K? be the M ?M matrix such that
K?ij = ?(xi) ? ? (xj) (5)
and let ??1 ? ??2 ? . . . ? ??M denote the eigenvalues
of K? and ??1 ,..., ??M denote the corresponding complete
set of normalized eigenvectors, such that ??t(??t ? ??t) = 1
when ??t > 0. Then the lth nonlinear principal compo-
nent of any test vector xt is defined as
ylt =
M?
i=1
??li (?(xi) ? ?(xt )) (6)
where ??li is the lth element of ??l .
3.2 Why is KPCA suited to WSD?
The potential of nonlinear principal components for
WSD can be illustrated by a simplified disambiguation
example for the ambiguous target word ?art?, with the
two senses shown in Table 1. Assume a training cor-
pus of the eight sentences as shown in Table 2, adapted
from Senseval-2 English lexical sample corpus. For each
sentence, we show the feature set associated with that
occurrence of ?art? and the correct sense class. These
eight occurrences of ?art? can be transformed to a binary
vector representation containing one dimension for each
feature, as shown in Table 3.
Extracting nonlinear principal components for the vec-
tors in this simple corpus results in nonlinear generaliza-
tion, reflecting an implicit consideration of combinations
of features. Table 2 shows the first three dimensions of
the principal component vectors obtained by transform-
ing each of the eight training vectors xt into (a) principal
component vectors zt using the linear transform obtained
via PCA, and (b) nonlinear principal component vectors
yt using the nonlinear transform obtained via KPCA as
described below.
Table 1: A tiny corpus for the target word ?art?, adapted from the Senseval-2 English lexical sample corpus (Kilgarriff
2001), together with a tiny example set of features. The training and testing examples can be represented as a set of
binary vectors: each row shows the correct class c for an observed vector x of five dimensions.
TRAINING design/N media/N the/DT entertainment/N world/N Class
x1 He studies art in London. 1
x2 Punch?s weekly guide to the
world of the arts, entertain-
ment, media and more.
1 1 1 1
x3 All such studies have influ-
enced every form of art, de-
sign, and entertainment in
some way.
1 1 1
x4 Among the technical arts cul-
tivated in some continental
schools that began to affect
England soon after the Nor-
man Conquest were those
of measurement and calcula-
tion.
1 2
x5 The Art of Love. 1 2
x6 Indeed, the art of doctor-
ing does contribute to bet-
ter health results and discour-
ages unwarranted malprac-
tice litigation.
1 2
x7 Countless books and classes
teach the art of asserting
oneself.
1 2
x8 Pop art is an example. 1
TESTING
x9 In the world of de-
sign arts particularly, this led
to appointments made for
political rather than academic
reasons.
1 1 1 1
Table 2: The original observed training vectors (showing only the first three dimensions) and their first three principal
components as transformed via PCA and KPCA.
Observed vectors PCA-transformed vectors KPCA-transformed vectors Class
t (x1t , x
2
t , x
3
t ) (z
1
t , z
2
t , z
3
t ) (y
1
t , y
2
t , y
3
t ) ct
1 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1
2 (0, 1, 1) (1.675, -1.132, 0.1049) (1.149, 0.02934, 0.322) 1
3 (1, 0, 0) (-0.367, 1.697, -0.2391) (0.8209, 0.7722, -0.2015) 1
4 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
5 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
6 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
7 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
8 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1
Similarly, for the test vector x9, Table 3 shows the
first three dimensions of the principal component vec-
tors obtained by transforming it into (a) a principal com-
ponent vector z9 using the linear PCA transform ob-
tained from training, and (b) a nonlinear principal com-
ponent vector y9 using the nonlinear KPCA transform
obtained obtained from training. The vector similarities
in the KPCA-transformed space can be quite different
from those in the PCA-transformed space. This causes
the KPCA-based model to be able to make the correct
Table 3: Testing vector (showing only the first three dimensions) and its first three principal components as transformed
via the trained PCA and KPCA parameters. The PCA-based and KPCA-based sense class predictions disagree.
Observed
vectors
PCA-transformed vectors KPCA-transformed vectors Predicted
Class
Correct
Class
t (x1t , x
2
t , x
3
t ) (z
1
t , z
2
t , z
3
t ) (y
1
t , y
2
t , y
3
t ) c?t ct
9 (1, 0, 1) (-0.3671, -0.5658, -0.2392) 2 1
9 (1, 0, 1) (4e-06, 8e-07, 1.111e-18) 1 1
class prediction, whereas the PCA-based model makes
the wrong class prediction.
What permits KPCA to apply stronger generalization
biases is its implicit consideration of combinations of
feature information in the data distribution from the high-
dimensional training vectors. In this simplified illustra-
tive example, there are just five input dimensions; the
effect is stronger in more realistic high dimensional vec-
tor spaces. Since the KPCA transform is computed from
unsupervised training vector data, and extracts general-
izations that are subsequently utilized during supervised
classification, it is possible to combine large amounts of
unsupervised data with reasonable smaller amounts of
supervised data.
Interpreting this example graphically can be illuminat-
ing even though the interpretation in three dimensions is
severely limiting. Figure 1(a) depicts the eight original
observed training vectors xt in the first three of the five
dimensions; note that among these eight vectors, there
happen to be only four unique points when restricting
our view to these three dimensions. Ordinary linear PCA
can be straightforwardly seen as projecting the original
points onto the principal axis, as can be seen for the case
of the first principal axis in Figure 1(b). Note that in this
space, the sense 2 instances are surrounded by sense 1
instances. We can traverse each of the projections onto
the principal axis in linear order, simply by visiting each
of the first principal components z1t along the principle
axis in order of their values, i.e., such that
z11 ? z
1
8 ? z
1
4 ? z
1
5 ? z
1
6 ? z
1
7 ? z
1
2 ? z
1
3 ? z
1
9
It is significantly more difficult to visualize the non-
linear principal components case, however. Note that
in general, there may not exist any principal axis in X ,
since an inverse mapping from F may not exist. If we
attempt to follow the same procedure to traverse each of
the projections onto the first principal axis as in the case
of linear PCA, by considering each of the first principal
components y1t in order of their value, i.e., such that
y14 ? y
1
5 ? y
1
6 ? y
1
7 ? y
1
9 ? y
1
1 ? y
1
8 ? y
1
3 ? y
1
2
then we must arbitrarily select a ?quasi-projection? di-
rection for each y1t since there is no actual principal axis
toward which to project. This results in a ?quasi-axis?
roughly as shown in Figure 1(c) which, though not pre-
cisely accurate, provides some idea as to how the non-
linear generalization capability allows the data points to
be grouped by principal components reflecting nonlin-
ear patterns in the data distribution, in ways that linear
Figure 1: Original vectors, PCA projections, and KPCA
?quasi-projections? (see text).
PCA cannot do. Note that in this space, the sense 1 in-
stances are already better separated from sense 2 data
points. Moreover, unlike linear PCA, there may be up
to M of the ?quasi-axes?, which may number far more
than five. Such effects can become pronounced in the
high dimensional spaces are actually used for real word
sense disambiguation tasks.
3.3 Algorithm
To extract nonlinear principal components efficiently,
note that in both Equations (5) and (6) the explicit form
of ? (xi) is required only in the form of (? (xi) ?? (xj)),
i.e., the dot product of vectors in F . This means that we
can calculate the nonlinear principal components by sub-
stituting a kernel function k(xi, xj) for (?(xi) ??(xj ))
in Equations (5) and (6) without knowing the mapping ?
explicitly; instead, the mapping ? is implicitly defined
by the kernel function. It is always possible to construct
a mapping into a space where k acts as a dot product
so long as k is a continuous kernel of a positive integral
operator (Scho?lkopf et al, 1998).
Thus we train the KPCA model using the following
algorithm:
1. Compute an M ?M matrix K? such that
K?ij = k(xi, xj) (7)
2. Compute the eigenvalues and eigenvectors of matrix
K? and normalize the eigenvectors. Let ??1 ? ??2 ?
. . . ? ??M denote the eigenvalues and ??1,..., ??M de-
note the corresponding complete set of normalized
eigenvectors.
To obtain the sense predictions for test instances, we
need only transform the corresponding vectors using the
trained KPCA model and classify the resultant vectors
using nearest neighbors. For a given test instance vector
x, its lth nonlinear principal component is
ylt =
M?
i=1
??lik(xi, xt) (8)
where ??li is the ith element of ??l.
For our disambiguation experiments we employ a
polynomial kernel function of the form k(xi, xj) =
(xi ? xj)
d
, although other kernel functions such as gaus-
sians could be used as well. Note that the degenerate
case of d = 1 yields the dot product kernel k(xi, xj) =
(xi?xj) which covers linear PCA as a special case, which
may explain why KPCA always outperforms PCA.
4 Semi-supervised KPCA model
4.1 Utilizing unlabeled data
In WSD, as with many NLP tasks, features are often in-
terdependent. For example, the features that represent
words that frequently co-occur are typically highly in-
terdependent. Similarly, the features that represent syn-
onyms tend to be highly interdependent.
It is a strength of the KPCA-based model that it gen-
eralizes over combinations of interdependent features.
This enables the model to predict the correct sense even
when the context surrounding a target word has not been
previously seen, by exploiting the similarity to feature
combinations that have been seen.
However, in practice the labeled training corpus for
WSD is typically relatively small, and does not yield
enough training instances to reliably extract dependen-
cies between features. For example, in the Senseval-
2 English lexical sample data, for each target word
there are only about 120 training instances on average,
whereas on the other hand we typically have thousands
of features for each target word.
The KPCA model can fail when it encounters a target
word whose context contains a combination of features
that may in fact be interdependent, but are not similar to
any combinations that occurred in the limited amounts
of labeled training data. Because of the sparse data, the
KPCA model wrongly considers the context of the tar-
get word to be dissimilar to those previously seen?even
though the contexts may in truth be similar. In the ab-
sence of any contexts it believes to be similar, the model
therefore tends simply to predict the most frequent sense.
The potential solution we propose to this problem is
to add much larger quantities of unannotated data, with
which the KPCA model can first be trained in unsu-
pervised fashion. This provides a significantly broader
dataset from which to generalize over combinations of
dependent features. One of the advantages of our WSD
model is that during KPCA training, the sense class is not
taken into consideration. Thus we can take advantage of
the vast amounts of cheap unannotated corpora, in addi-
tion to the relatively small amounts of labeled training
data. Adding a large quantity of unlabeled data makes
it much likelier that dependent features can be identified
during KPCA training.
4.2 Algorithm
The primary difference of the semi-supervised KPCA
model from the supervised KPCA baseline model de-
scribed above lies in the eigenvector calculation step. As
we mentioned earlier, in KPCA-based model, we need
to calculate the eigenvectors of matrix K, where Kij =
(?(xi) ? ?(xj )). In the supervised KPCA model, train-
ing vectors such as xi and xj are only drawn from the
labeled training corpus. In the semi-supervised KPCA
model, training vectors are drawn from both the labeled
training corpus and a much larger unlabeled training cor-
pus. As a consequence, the maximum number of eigen-
vectors in the supervised KPCA model is the minimum
of the number of features and the number of vectors from
the labeled training corpus, while the maximum number
of eigenvectors for the semi-supervised KPCA model is
the minimum of the number of features and total num-
ber of vectors from the combined labeled and unlabeled
training corpora.
However, one would not want to apply the semi-
supervised KPCA model indiscriminately. While it can
be expected to be valuable in cases where the data was
too sparse for reliable training of the supervised KPCA
model, at the same time it is important to note that the un-
labeled data is typically drawn from quite different dis-
tributions than the labeled data, and may therefore be ex-
pected to introduce a new source of noise.
We therefore define a composite semi-supervised
KPCA model based on the following assumption. If we
are sufficiently confident about the prediction made by
the supervised KPCA model as to the predicted sense
for the target word, we need not resort to the semi-
supervised KPCA method. On the other hand, if we
are not confident about the supervised KPCA model?s
prediction, we then turn to the semi-supervised KPCA
model and take its classification as the predicted sense.
Specifically, the composite model uses the following
algorithm to combine the sense predictions of the super-
vised and semi-supervised KPCA models in order to dis-
ambiguate the target word in a given test instance x:
1. let s1 be the predicted sense of x using the super-
vised KPCA baseline model
2. let c be the similarity between x and its most similar
training instance
3. if c ? t or s1 6= smf (where t is a preset thresh-
old, and smf is the most frequent sense of the target
word):
? then predict the sense of the target word of x
to be s1
? else predict the sense of the target word of
x to be s2, the sense predicted by the semi-
supervised KPCA model
The two conditions checked in step 3 serve to fil-
ter those instances where the supervised KPCA baseline
model is confident enough to skip the semi-supervised
KPCA model. In particular:
? The threshold t specifies a minimum level of the
supervised KPCA baseline model?s confidence, in
terms of similarity. If c ? t, then there were training
instances that were of sufficient similarity to the test
instance so that the model can be confident that a
correct disambiguation can be predicted based only
on those similar training instances. In this case the
semi-supervised KPCA model is not needed.
? If s1 is not the most frequent sense smf of the
target word, then there is strong evidence that the
test instance should be disambiguated as s1 because
this is overriding an otherwise strong tendency to
disambiguate the target word to the most frequent
sense. Again, in this case the semi-supervised
KPCA model should be avoided.
The threshold t is defined to rise as the relative fre-
quency of the most frequent sense falls. Specifically,
t = 1? P (smf) + c where P (smf) is the probability of
most frequent sense in the training corpus and c is a small
constant. This reflects the assumption that the higher the
probability of the most frequent sense, the less likely that
a test instance disambiguated as the most frequent sense
is wrong.
5 Experimental setup
We evaluated the composite semi-supervised KPCA
model using data from the Senseval-2 English lexical
sample task (Kilgarriff, 2001)(Palmer et al, 2001). We
chose to focus on verbs, which have proven particularly
difficult to disambiguate. Our task consists in disam-
biguating several instances of 16 different target verbs.
Table 4: The semi-supervised KPCA model outperforms
supervised na??ve Bayes and maximum entropy models,
as well as the most-frequent-sense and supervised KPCA
baseline models.
Fine-grained
accuracy
Coarse-
grained
accuracy
Most frequent
sense
41.4% 51.7%
Na??ve Bayes 55.4% 64.2%
Maximum entropy 54.9% 64.1%
Supervised KPCA 57.0% 66.6%
Composite semi-
supervised KPCA
57.4% 67.2%
For each target word, training and test instances manu-
ally tagged with WordNet senses are available. There are
an average of about 10.5 senses per target word, rang-
ing from 4 to 19. All our models are evaluated on the
Senseval-2 test data, but trained on different training sets.
We report accuracy, the number of correct predictions
over the total number of test instances, at two different
levels of sense granularity.
The supervised models are trained on the Senseval-
2 training data. On average, 137 annotated training in-
stances per target word are available.
In addition to the small annotated Senseval-2 data
set, the semi-supervised KPCA model can make use of
large amounts of unannotated data. Since most of the
Senseval-2 verb data comes from the Wall Street Journal,
we choose to augment the Senseval-2 data by collecting
additional training instances from the Wall Street Jour-
nal Tipster corpus. In order to minimize the noise during
KPCA learning, we only extract the sentences in which
the target word occurs. For each target word, up to 1500
additional training instances were extracted. The result-
ing training corpus for the semi-supervised KPCA model
is more than 10 times larger than the Senseval-2 training
set, with an average of 1637 training instances per target
word.
The set of features used is as described by Yarowsky
and Florian (2002) in their ?feature-enhanced na??ve
Bayes model?, with position-sensitive, syntactic, and lo-
cal collocational features.
6 Results
Table 4 shows that the composite semi-supervised KPCA
model improves on the high-performance supervised
KPCA model, for both coarse-grained and fined-grained
sense distinctions. The supervised KPCA model signif-
icantly outperforms a na??ve Bayes model, and a max-
imum entropy model, which are among the top per-
forming models for WSD. Note that these results are
consistent with the larger study of supervised models
conducted by Wu et al (2004). The composite semi-
supervised KPCA model outperforms all of the three su-
pervised models, and in particular, it further improves the
Table 5: Semi-supervised KPCA is not necessary when
supervised KPCA is very confident.
Fine-grained
accuracy
Coarse-
grained
accuracy
Supervised KPCA 62.1% 71.3%
Semi-supervised
KPCA
57.1% 67.1%
Table 6: Semi-supervised KPCA outperforms supervised
KPCA when supervised KPCA is not confident: adding
training data helps when there are no similar instances in
the training set.
Fine-grained
accuracy
Coarse-
grained
accuracy
Supervised KPCA 30.8% 44.11%
Semi-supervised
KPCA
38.3% 51.47%
accuracy of the supervised KPCA model.
Overall, with the addition of the semi-supervised
model, the accuracy for disambiguating the verbs in-
creases from 57% to 57.4% on the fine-grained task, and
from 66.6% to 67.2% on the coarse-grained task.
In our composite model, the supervised KPCA model
predicts senses with high confidence for more than 94%
of the test instances. The predictions of the semi-
supervised model are used for the remaining 6% of the
test instances. Table 5 shows that it is not necessary to
use the semi-supervised training model for all the train-
ing instances. In fact, when the supervised model is con-
fident, its predictions are significantly more accurate than
those of the semi-supervised model alone.
When the predictions of the supervised KPCA model
are not accurate, the semi-supervised KPCA model out-
performs the supervised model. This happens when (1)
there is no training instance that is very similar to the test
instance considered and when (2) in the absence of rele-
vant features to learn from in the small annotated train-
ing set, the supervised KPCA model can only predict the
most frequent sense for the current target. In these condi-
tions, our experiment results in Table 6 confirm that the
semi-supervised KPCA model benefits from the large ad-
ditional training data, suggesting it is able to learn useful
feature conjunctions, which help to give better predic-
tions.
The composite semi-supervised KPCA model there-
fore chooses the best model depending on the degree
of confidence of the supervised model. All the KPCA
weights, for both the supervised and the semi-supervised
model, have been pre-computed during training, and it
is therefore inexpensive to switch from one model to the
other at testing time.
7 Conclusion
We have proposed a new composite semi-supervised
WSD model based on the Kernel PCA technique, that
employs both supervised and semi-supervised compo-
nents. This strategy allows us to combine large amounts
of cheap unlabeled data with smaller amounts of labeled
data. Experiments on the hard-to-disambiguate verbs
from the Senseval-2 English lexical sample task confirm
that when the supervised KPCA model is insufficiently
confident in its sense predictions, taking advantage of the
semi-supervised KPCA model trained with the unlabeled
data can help to give a better prediction. The composite
semi-supervised KPCA model exploits this to improve
upon the accuracy of the supervised KPCA model intro-
duced by Wu et al (2004).
References
Martin Chodorow, Claudia Leacock, and George A. Miller. A topical/local clas-
sifier for word sense identification. Computers and the Humanities, 34(1-
2):115?120, 1999. Special issue on SENSEVAL.
Hoa Trang Dang and Martha Palmer. Combining contextual features for word
sense disambiguation. In Proceedings of the SIGLEX/SENSEVAL Workshop
on Word Sense Disambiguation: Recent Successes and Future Directions,
pages 88?94, Philadelphia, July 2002. SIGLEX, Association for Computa-
tional Linguistics.
Konstantinos I. Diamantaras and Sun Yuan Kung. Principal Component Neural
Networks. Wiley, New York, 1996.
Adam Kilgarriff and Joseph Rosenzweig. Framework and results for English
Senseval. Computers and the Humanities, 34(1):15?48, 1999. Special issue
on SENSEVAL.
Adam Kilgarriff. English lexical sample task description. In Proceedings of
Senseval-2, Second International Workshop on Evaluating Word Sense Dis-
ambiguation Systems, pages 17?20, Toulouse, France, July 2001. SIGLEX,
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. Conditional structure versus conditional
estimation in NLP models. In Proceedings of EMNLP-2002, Conference on
Empirical Methods in Natural Language Processing, pages 9?16, Philadel-
phia, July 2002. SIGDAT, Association for Computational Linguistics.
Taku Kudo and Yuji Matsumoto. Fast methods for kernel-based text analysis.
In Proceedings of the 41set Annual Meeting of the Asoociation for Computa-
tional Linguistics, pages 24?31, 2003.
S. Mika, B. Scho?lkopf, A. Smola, K.-R. Mu?ller, M. Scholz, and G. Ra?tsch. Ker-
nel PCA and de-noising in feature spaces. Advances in Neural Information
Processing Systems, 1999.
Raymond J. Mooney. Comparative experiments on disambiguating word senses:
An illustration of the role of bias in machine learning. In Proceedings of the
Conference on Empirical Methods in Natural Language Processing, Philadel-
phia, May 1996. SIGDAT, Association for Computational Linguistics.
Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang
Dang. English tasks: All-words and verb lexical sample. In Proceedings of
Senseval-2, Second International Workshop on Evaluating Word Sense Dis-
ambiguation Systems, pages 21?24, Toulouse, France, July 2001. SIGLEX,
Association for Computational Linguistics.
Ted Pedersen. Machine learning with lexical features: The Duluth approach to
SENSEVAL-2. In Proceedings of Senseval-2, Second International Work-
shop on Evaluating Word Sense Disambiguation Systems, pages 139?142,
Toulouse, France, July 2001. SIGLEX, Association for Computational Lin-
guistics.
Bernhard Scho?lkopf, Alexander Smola, and Klaus-Rober Mu?ller. Nonlinear com-
ponent analysis as a kernel eigenvalue problem. Neural Computation, 10(5),
1998.
C. J. Twining and C. J. Taylor. Kernel principal component analysis and the con-
struction of non-linear active shape models. In Proceedings of BMVC20001,
2001.
Dekai Wu, Weifeng Su, and Marine Carpuat. A Kernel PCA method for superior
word sense disambiguation. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics, Barcelona, July 2004.
David Yarowsky and Radu Florian. Evaluating sense disambiguation across di-
verse parameter spaces. Natural Language Engineering, 8(4):293?310, 2002.
A Kernel PCA Method for Superior Word Sense Disambiguation
Dekai WU1 Weifeng SU Marine CARPUAT
dekai@cs.ust.hk weifeng@cs.ust.hk marine@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
Abstract
We introduce a new method for disambiguating
word senses that exploits a nonlinear Kernel Prin-
cipal Component Analysis (KPCA) technique to
achieve accuracy superior to the best published indi-
vidual models. We present empirical results demon-
strating significantly better accuracy compared to
the state-of-the-art achieved by either na??ve Bayes
or maximum entropy models, on Senseval-2 data.
We also contrast against another type of kernel
method, the support vector machine (SVM) model,
and show that our KPCA-based model outperforms
the SVM-based model. It is hoped that these highly
encouraging first results on KPCA for natural lan-
guage processing tasks will inspire further develop-
ment of these directions.
1 Introduction
Achieving higher precision in supervised word
sense disambiguation (WSD) tasks without resort-
ing to ad hoc voting or similar ensemble techniques
has become somewhat daunting in recent years,
given the challenging benchmarks set by na??ve
Bayes models (e.g., Mooney (1996), Chodorow et
al. (1999), Pedersen (2001), Yarowsky and Flo-
rian (2002)) as well as maximum entropy models
(e.g., Dang and Palmer (2002), Klein and Man-
ning (2002)). A good foundation for comparative
studies has been established by the Senseval data
and evaluations; of particular relevance here are
the lexical sample tasks from Senseval-1 (Kilgarriff
and Rosenzweig, 1999) and Senseval-2 (Kilgarriff,
2001).
We therefore chose this problem to introduce
an efficient and accurate new word sense disam-
biguation approach that exploits a nonlinear Kernel
PCA technique to make predictions implicitly based
on generalizations over feature combinations. The
1The author would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research
in part through grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
technique is applicable whenever vector represen-
tations of a disambiguation task can be generated;
thus many properties of our technique can be ex-
pected to be highly attractive from the standpoint of
natural language processing in general.
In the following sections, we first analyze the po-
tential of nonlinear principal components with re-
spect to the task of disambiguating word senses.
Based on this, we describe a full model for WSD
built on KPCA. We then discuss experimental re-
sults confirming that this model outperforms state-
of-the-art published models for Senseval-related
lexical sample tasks as represented by (1) na??ve
Bayes models, as well as (2) maximum entropy
models. We then consider whether other kernel
methods?in particular, the popular SVM model?
are equally competitive, and discover experimen-
tally that KPCA achieves higher accuracy than the
SVM model.
2 Nonlinear principal components and
WSD
The Kernel Principal Component Analysis tech-
nique, or KPCA, is a nonlinear kernel method
for extraction of nonlinear principal components
from vector sets in which, conceptually, the n-
dimensional input vectors are nonlinearly mapped
from their original space Rn to a high-dimensional
feature space F where linear PCA is performed,
yielding a transform by which the input vectors
can be mapped nonlinearly to a new set of vectors
(Scho?lkopf et al, 1998).
A major advantage of KPCA is that, unlike other
common analysis techniques, as with other kernel
methods it inherently takes combinations of pre-
dictive features into account when optimizing di-
mensionality reduction. For natural language prob-
lems in general, of course, it is widely recognized
that significant accuracy gains can often be achieved
by generalizing over relevant feature combinations
(e.g., Kudo and Matsumoto (2003)). Another ad-
vantage of KPCA for the WSD task is that the
dimensionality of the input data is generally very
Table 1: Two of the Senseval-2 sense classes for the target word ?art?, from WordNet 1.7 (Fellbaum 1998).
Class Sense
1 the creation of beautiful or significant things
2 a superior skill
large, a condition where kernel methods excel.
Nonlinear principal components (Diamantaras
and Kung, 1996) may be defined as follows. Sup-
pose we are given a training set of M pairs (xt, ct)
where the observed vectors xt ? Rn in an n-
dimensional input space X represent the context of
the target word being disambiguated, and the cor-
rect class ct represents the sense of the word, for
t = 1, ..,M . Suppose ? is a nonlinear mapping
from the input space Rn to the feature space F .
Without loss of generality we assume the M vec-
tors are centered vectors in the feature space, i.e.,
?M
t=1 ?(xt) = 0; uncentered vectors can easily
be converted to centered vectors (Scho?lkopf et al,
1998). We wish to diagonalize the covariance ma-
trix in F :
C = 1M
M
?
j=1
?(xj) ?T (xj) (1)
To do this requires solving the equation ?v = Cv
for eigenvalues ? ? 0 and eigenvectors v ? F . Be-
cause
Cv = 1M
M
?
j=1
(?(xj) ? v)? (xj) (2)
we can derive the following two useful results. First,
? (?(xt) ? v) = ? (xt) ? Cv (3)
for t = 1, ..,M . Second, there exist ?i for i =
1, ...,M such that
v =
M
?
i=1
?i?(xi) (4)
Combining (1), (3), and (4), we obtain
M?
M
?
i=1
?i (?(xt) ? ?(xi ))
=
M
?
i=1
?i(? (xt) ?
M
?
j=1
?(xj)) (?(xj) ? ?(xi ))
for t = 1, ..,M . Let K? be the M ? M matrix such
that
K?ij = ?(xi) ? ?(xj) (5)
and let ??1 ? ??2 ? . . . ? ??M denote the eigenval-
ues of K? and ??1 ,..., ??M denote the corresponding
complete set of normalized eigenvectors, such that
??t(??t ? ??t) = 1 when ??t > 0. Then the lth nonlinear
principal component of any test vector xt is defined
as
ylt =
M
?
i=1
??li (?(xi) ? ?(xt )) (6)
where ??li is the lth element of ??l .
To illustrate the potential of nonlinear principal
components for WSD, consider a simplified disam-
biguation example for the ambiguous target word
?art?, with the two senses shown in Table 1. Assume
a training corpus of the eight sentences as shown
in Table 2, adapted from Senseval-2 English lexical
sample corpus. For each sentence, we show the fea-
ture set associated with that occurrence of ?art? and
the correct sense class. These eight occurrences of
?art? can be transformed to a binary vector represen-
tation containing one dimension for each feature, as
shown in Table 3.
Extracting nonlinear principal components for
the vectors in this simple corpus results in nonlinear
generalization, reflecting an implicit consideration
of combinations of features. Table 3 shows the first
three dimensions of the principal component vectors
obtained by transforming each of the eight training
vectors xt into (a) principal component vectors zt
using the linear transform obtained via PCA, and
(b) nonlinear principal component vectors yt using
the nonlinear transform obtained via KPCA as de-
scribed below.
Similarly, for the test vector x9, Table 4 shows the
first three dimensions of the principal component
vectors obtained by transforming it into (a) a princi-
pal component vector z9 using the linear PCA trans-
form obtained from training, and (b) a nonlinear
principal component vector y9 using the nonlinear
KPCA transform obtained obtained from training.
The vector similarities in the KPCA-transformed
space can be quite different from those in the PCA-
transformed space. This causes the KPCA-based
model to be able to make the correct class pre-
diction, whereas the PCA-based model makes the
Table 2: A tiny corpus for the target word ?art?, adapted from the Senseval-2 English lexical sample corpus
(Kilgarriff 2001), together with a tiny example set of features. The training and testing examples can be
represented as a set of binary vectors: each row shows the correct class c for an observed vector x of five
dimensions.
TRAINING design/N media/N the/DT entertainment/N world/N Class
x1 He studies art in London. 1
x2 Punch?s weekly guide to
the world of the arts,
entertainment, media and
more.
1 1 1 1
x3 All such studies have in-
fluenced every form of art,
design, and entertainment
in some way.
1 1 1
x4 Among the techni-
cal arts cultivated in
some continental schools
that began to affect
England soon after the
Norman Conquest were
those of measurement
and calculation.
1 2
x5 The Art of Love. 1 2
x6 Indeed, the art of doc-
toring does contribute to
better health results and
discourages unwarranted
malpractice litigation.
1 2
x7 Countless books and
classes teach the art of
asserting oneself.
1 2
x8 Pop art is an example. 1
TESTING
x9 In the world of de-
sign arts particularly, this
led to appointments made
for political rather than
academic reasons.
1 1 1 1
wrong class prediction.
What permits KPCA to apply stronger general-
ization biases is its implicit consideration of com-
binations of feature information in the data dis-
tribution from the high-dimensional training vec-
tors. In this simplified illustrative example, there
are just five input dimensions; the effect is stronger
in more realistic high dimensional vector spaces.
Since the KPCA transform is computed from unsu-
pervised training vector data, and extracts general-
izations that are subsequently utilized during super-
vised classification, it is quite possible to combine
large amounts of unsupervised data with reasonable
smaller amounts of supervised data.
It can be instructive to attempt to interpret this
example graphically, as follows, even though the
interpretation in three dimensions is severely limit-
ing. Figure 1(a) depicts the eight original observed
training vectors xt in the first three of the five di-
mensions; note that among these eight vectors, there
happen to be only four unique points when restrict-
ing our view to these three dimensions. Ordinary
linear PCA can be straightforwardly seen as pro-
jecting the original points onto the principal axis,
Table 3: The original observed training vectors (showing only the first three dimensions) and their first three
principal components as transformed via PCA and KPCA.
Observed vectors PCA-transformed vectors KPCA-transformed vectors Class
t (x1t , x2t , x3t ) (z1t , z2t , z3t ) (y1t , y2t , y3t ) ct
1 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1
2 (0, 1, 1) (1.675, -1.132, 0.1049) (1.149, 0.02934, 0.322) 1
3 (1, 0, 0) (-0.367, 1.697, -0.2391) (0.8209, 0.7722, -0.2015) 1
4 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
5 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
6 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
7 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
8 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1
Table 4: Testing vector (showing only the first three dimensions) and its first three principal components
as transformed via the trained PCA and KPCA parameters. The PCA-based and KPCA-based sense class
predictions disagree.
Observed
vectors
PCA-transformed vectors KPCA-transformed vec-
tors
Predicted
Class
Correct
Class
t (x1t , x2t , x3t ) (z1t , z2t , z3t ) (y1t , y2t , y3t ) c?t ct
9 (1, 0, 1) (-0.3671, -0.5658, -0.2392) 2 1
9 (1, 0, 1) (4e-06, 8e-07, 1.111e-18) 1 1
as can be seen for the case of the first principal axis
in Figure 1(b). Note that in this space, the sense 2
instances are surrounded by sense 1 instances. We
can traverse each of the projections onto the prin-
cipal axis in linear order, simply by visiting each of
the first principal components z1t along the principle
axis in order of their values, i.e., such that
z11 ? z18 ? z14 ? z15 ? z16 ? z17 ? z12 ? z13 ? z19
It is significantly more difficult to visualize
the nonlinear principal components case, however.
Note that in general, there may not exist any prin-
cipal axis in X , since an inverse mapping from F
may not exist. If we attempt to follow the same pro-
cedure to traverse each of the projections onto the
first principal axis as in the case of linear PCA, by
considering each of the first principal components
y1t in order of their value, i.e., such that
y14 ? y15 ? y16 ? y17 ? y19 ? y11 ? y18 ? y13 ? y12
then we must arbitrarily select a ?quasi-projection?
direction for each y1t since there is no actual prin-
cipal axis toward which to project. This results in a
?quasi-axis? roughly as shown in Figure 1(c) which,
though not precisely accurate, provides some idea
as to how the nonlinear generalization capability al-
lows the data points to be grouped by principal com-
ponents reflecting nonlinear patterns in the data dis-
tribution, in ways that linear PCA cannot do. Note
that in this space, the sense 1 instances are already
better separated from sense 2 data points. More-
over, unlike linear PCA, there may be up to M of
the ?quasi-axes?, which may number far more than
five. Such effects can become pronounced in the
high dimensional spaces are actually used for real
word sense disambiguation tasks.
3 A KPCA-based WSD model
To extract nonlinear principal components effi-
ciently, note that in both Equations (5) and (6) the
explicit form of ?(xi) is required only in the form
of (?(xi) ??(xj)), i.e., the dot product of vectors in
F . This means that we can calculate the nonlinear
principal components by substituting a kernel func-
tion k(xi, xj) for (?(xi) ? ?(xj )) in Equations (5)
and (6) without knowing the mapping ? explicitly;
instead, the mapping ? is implicitly defined by the
kernel function. It is always possible to construct
a mapping into a space where k acts as a dot prod-
uct so long as k is a continuous kernel of a positive
integral operator (Scho?lkopf et al, 1998).
the/DT
4, 5, 6, 7
1, 8
3
2
design/N
media/N
(a)
9
the/DT
4, 5, 6, 7
1, 8 3
2
design/N
media/N
(b)
9
the/DT
4, 5, 6, 7
1, 8 3
2
design/N
media/N
(c)
9
first principal
axis
: training example with sense class 1
: training example with sense class 2
: test example with unknown sense class
: test example with predicted sense
first principal
? quasi-axis?
class 2 (correct sense class=1)
: test example with predicted sense
class 1 (correct sense class=1)
Figure 1: Original vectors, PCA projections, and
KPCA ?quasi-projections? (see text).
Table 5: Experimental results showing that the
KPCA-based model performs significantly better
than na??ve Bayes and maximum entropy models.
Significance intervals are computed via bootstrap
resampling.
WSD Model Accuracy Sig. Int.
na??ve Bayes 63.3% +/-0.91%
maximum entropy 63.8% +/-0.79%
KPCA-based model 65.8% +/-0.79%
Thus we train the KPCA model using the follow-
ing algorithm:
1. Compute an M ? M matrix K? such that
K?ij = k(xi, xj) (7)
2. Compute the eigenvalues and eigenvectors of
matrix K? and normalize the eigenvectors. Let
??1 ? ??2 ? . . . ? ??M denote the eigenvalues
and ??1,..., ??M denote the corresponding com-
plete set of normalized eigenvectors.
To obtain the sense predictions for test instances,
we need only transform the corresponding vectors
using the trained KPCA model and classify the re-
sultant vectors using nearest neighbors. For a given
test instance vector x, its lth nonlinear principal
component is
ylt =
M
?
i=1
??lik(xi, xt) (8)
where ??li is the ith element of ??l.
For our disambiguation experiments we employ a
polynomial kernel function of the form k(xi, xj) =
(xi ? xj)d, although other kernel functions such as
gaussians could be used as well. Note that the de-
generate case of d = 1 yields the dot product kernel
k(xi, xj) = (xi?xj) which covers linear PCA as a
special case, which may explain why KPCA always
outperforms PCA.
4 Experiments
4.1 KPCA versus na??ve Bayes and maximum
entropy models
We established two baseline models to represent
the state-of-the-art for individual WSD models: (1)
na??ve Bayes, and (2) maximum entropy models.
The na??ve Bayes model was found to be the most
accurate classifier in a comparative study using a
subset of Senseval-2 English lexical sample data
by Yarowsky and Florian (2002). However, the
maximum entropy (Jaynes, 1978) was found to
yield higher accuracy than na??ve Bayes in a sub-
sequent comparison by Klein and Manning (2002),
who used a different subset of either Senseval-1 or
Senseval-2 English lexical sample data. To control
for data variation, we built and tuned models of both
kinds. Note that our objective in these experiments
is to understand the performance and characteristics
of KPCA relative to other individual methods. It
is not our objective here to compare against voting
or other ensemble methods which, though known to
be useful in practice (e.g., Yarowsky et al (2001)),
would not add to our understanding.
To compare as evenly as possible, we em-
ployed features approximating those of the ?feature-
enhanced na??ve Bayes model? of Yarowsky and Flo-
rian (2002), which included position-sensitive, syn-
tactic, and local collocational features. The mod-
els in the comparative study by Klein and Man-
ning (2002) did not include such features, and so,
again for consistency of comparison, we experi-
mentally verified that our maximum entropy model
(a) consistently yielded higher scores than when
the features were not used, and (b) consistently
yielded higher scores than na??ve Bayes using the
same features, in agreement with Klein and Man-
ning (2002). We also verified the maximum en-
tropy results against several different implementa-
tions, using various smoothing criteria, to ensure
that the comparison was even.
Evaluation was done on the Senseval 2 English
lexical sample task. It includes 73 target words,
among which nouns, adjectives, adverbs and verbs.
For each word, training and test instances tagged
with WordNet senses are provided. There are an av-
erage of 7.8 senses per target word type. On average
109 training instances per target word are available.
Note that we used the set of sense classes from Sen-
seval?s ?fine-grained? rather than ?coarse-grained?
classification task.
The KPCA-based model achieves the highest ac-
curacy, as shown in Table 5, followed by the max-
imum entropy model, with na??ve Bayes doing the
poorest. Bear in mind that all of these models are
significantly more accurate than any of the other re-
ported models on Senseval. ?Accuracy? here refers
to both precision and recall since disambiguation of
all target words in the test set is attempted. Results
are statistically significant at the 0.10 level, using
bootstrap resampling (Efron and Tibshirani, 1993);
moreover, we consistently witnessed the same level
of accuracy gains from the KPCA-based model over
Table 6: Experimental results comparing the
KPCA-based model versus the SVM model.
WSD Model Accuracy Sig. Int.
SVM-based model 65.2% +/-1.00%
KPCA-based model 65.8% +/-0.79%
many variations of the experiments.
4.2 KPCA versus SVM models
Support vector machines (e.g., Vapnik (1995),
Joachims (1998)) are a different kind of ker-
nel method that, unlike KPCA methods, have al-
ready gained high popularity for NLP applications
(e.g., Takamura and Matsumoto (2001), Isozaki and
Kazawa (2002), Mayfield et al (2003)) including
the word sense disambiguation task (e.g., Cabezas
et al (2001)). Given that SVM and KPCA are both
kernel methods, we are frequently asked whether
SVM-based WSD could achieve similar results.
To explore this question, we trained and tuned
an SVM model, providing the same rich set of fea-
tures and also varying the feature representations to
optimize for SVM biases. As shown in Table 6,
the highest-achieving SVM model is also able to
obtain higher accuracies than the na??ve Bayes and
maximum entropy models. However, in all our ex-
periments the KPCA-based model consistently out-
performs the SVM model (though the margin falls
within the statistical significance interval as com-
puted by bootstrap resampling for this single exper-
iment). The difference in KPCA and SVM perfor-
mance is not surprising given that, aside from the
use of kernels, the two models share little structural
resemblance.
4.3 Running times
Training and testing times for the various model im-
plementations are given in Table 7, as reported by
the Unix time command. Implementations of all
models are in C++, but the level of optimization is
not controlled. For example, no attempt was made
to reduce the training time for na??ve Bayes, or to re-
duce the testing time for the KPCA-based model.
Nevertheless, we can note that in the operating
range of the Senseval lexical sample task, the run-
ning times of the KPCA-based model are roughly
within the same order of magnitude as for na??ve
Bayes or maximum entropy. On the other hand,
training is much faster than the alternative kernel
method based on SVMs. However, the KPCA-
based model?s times could be expected to suffer
in situations where significantly larger amounts of
Table 7: Comparison of training and testing times for the different WSD model implementations.
WSD Model Training time [CPU sec] Testing time [CPU sec]
na??ve Bayes 103.41 16.84
maximum entropy 104.62 59.02
SVM-based model 5024.34 16.21
KPCA-based model 216.50 128.51
training data are available.
5 Conclusion
This work represents, to the best of our knowl-
edge, the first application of Kernel PCA to a
true natural language processing task. We have
shown that a KPCA-based model can significantly
outperform state-of-the-art results from both na??ve
Bayes as well as maximum entropy models, for
supervised word sense disambiguation. The fact
that our KPCA-based model outperforms the SVM-
based model indicates that kernel methods other
than SVMs deserve more attention. Given the theo-
retical advantages of KPCA, it is our hope that this
work will encourage broader recognition, and fur-
ther exploration, of the potential of KPCA modeling
within NLP research.
Given the positive results, we plan next to com-
bine large amounts of unsupervised data with rea-
sonable smaller amounts of supervised data such as
the Senseval lexical sample. Earlier we mentioned
that one of the promising advantages of KPCA is
that it computes the transform purely from unsuper-
vised training vector data. We can thus make use of
the vast amounts of cheap unannotated data to aug-
ment the model presented in this paper.
References
Clara Cabezas, Philip Resnik, and Jessica Stevens.
Supervised sense tagging using support vector
machines. In Proceedings of Senseval-2, Sec-
ond International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 59?62,
Toulouse, France, July 2001. SIGLEX, Associ-
ation for Computational Linguistics.
Martin Chodorow, Claudia Leacock, and George A.
Miller. A topical/local classifier for word sense
identification. Computers and the Humanities,
34(1-2):115?120, 1999. Special issue on SEN-
SEVAL.
Hoa Trang Dang and Martha Palmer. Combining
contextual features for word sense disambigua-
tion. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 88?
94, Philadelphia, July 2002. SIGLEX, Associa-
tion for Computational Linguistics.
Konstantinos I. Diamantaras and Sun Yuan Kung.
Principal Component Neural Networks. Wiley,
New York, 1996.
Bradley Efron and Robert J. Tibshirani. An Intro-
duction to the Bootstrap. Chapman and Hall,
1993.
Hideki Isozaki and Hideto Kazawa. Efficient sup-
port vector classifiers for named entity recogni-
tion. In Proceedings of COLING-2002, pages
390?396, Taipei, 2002.
E.T. Jaynes. Where do we Stand on Maximum En-
tropy? MIT Press, Cambridge MA, 1978.
Thorsten Joachims. Text categorization with sup-
port vector machines: Learning with many rel-
evant features. In Proceedings of ECML-98,
10th European Conference on Machine Learning,
pages 137?142, 1998.
Adam Kilgarriff and Joseph Rosenzweig. Frame-
work and results for English Senseval. Comput-
ers and the Humanities, 34(1):15?48, 1999. Spe-
cial issue on SENSEVAL.
Adam Kilgarriff. English lexical sample task de-
scription. In Proceedings of Senseval-2, Sec-
ond International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 17?20,
Toulouse, France, July 2001. SIGLEX, Associ-
ation for Computational Linguistics.
Dan Klein and Christopher D. Manning. Con-
ditional structure versus conditional estimation
in NLP models. In Proceedings of EMNLP-
2002, Conference on Empirical Methods in Nat-
ural Language Processing, pages 9?16, Philadel-
phia, July 2002. SIGDAT, Association for Com-
putational Linguistics.
Taku Kudo and Yuji Matsumoto. Fast methods
for kernel-based text analysis. In Proceedings of
the 41set Annual Meeting of the Asoociation for
Computational Linguistics, pages 24?31, 2003.
James Mayfield, Paul McNamee, and Christine Pi-
atko. Named entity recognition using hundreds of
thousands of features. In Walter Daelemans and
Miles Osborne, editors, Proceedings of CoNLL-
2003, pages 184?187, Edmonton, Canada, 2003.
Raymond J. Mooney. Comparative experiments on
disambiguating word senses: An illustration of
the role of bias in machine learning. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, Philadelphia, May
1996. SIGDAT, Association for Computational
Linguistics.
Ted Pedersen. Machine learning with lexical fea-
tures: The Duluth approach to SENSEVAL-2.
In Proceedings of Senseval-2, Second Interna-
tional Workshop on Evaluating Word Sense Dis-
ambiguation Systems, pages 139?142, Toulouse,
France, July 2001. SIGLEX, Association for
Computational Linguistics.
Bernhard Scho?lkopf, Alexander Smola, and Klaus-
Rober Mu?ller. Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation,
10(5), 1998.
Hiroya Takamura and Yuji Matsumoto. Feature
space restructuring for SVMs with application to
text categorization. In Proceedings of EMNLP-
2001, Conference on Empirical Methods in Nat-
ural Language Processing, pages 51?57, 2001.
Vladimir N. Vapnik. The Nature of Statistical
Learning Theory. Springer-Verlag, New York,
1995.
David Yarowsky and Radu Florian. Evaluat-
ing sense disambiguation across diverse param-
eter spaces. Natural Language Engineering,
8(4):293?310, 2002.
David Yarowsky, Silviu Cucerzan, Radu Florian,
Charles Schafer, and Richard Wicentowski. The
Johns Hopkins SENSEVAL2 system descrip-
tions. In Proceedings of Senseval-2, Sec-
ond International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 163?166,
Toulouse, France, July 2001. SIGLEX, Associa-
tion for Computational Linguistics.
Augmenting Ensemble Classification for Word Sense Disambiguation with a
Kernel PCA Model
Marine CARPUAT Weifeng SU Dekai WU1
marine@cs.ust.hk weifeng@cs.ust.hk dekai@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology, Clear Water Bay, Hong Kong
Abstract
The HKUST word sense disambiguation systems
benefit from a new nonlinear Kernel Principal
Component Analysis (KPCA) based disambigua-
tion technique. We discuss and analyze results
from the Senseval-3 English, Chinese, and Multi-
lingual Lexical Sample data sets. Among an en-
semble of four different kinds of voted models, the
KPCA-based model, along with the maximum en-
tropy model, outperforms the boosting model and
na??ve Bayes model. Interestingly, while the KPCA-
based model typically achieves close or better ac-
curacy than the maximum entropy model, neverthe-
less a comparison of predicted classifications shows
that it has a significantly different bias. This char-
acteristic makes it an excellent voter, as confirmed
by results showing that removing the KPCA-based
model from the ensemble generally degrades per-
formance.
1 Introduction
Classifier combination has become a standard ar-
chitecture for shared task evaluations in word sense
disambiguation (WSD), named entity recognition,
and similar problems that can naturally be cast as
classification problems. Voting is the most com-
mon method of combination, having proven to be
remarkably effective yet simple.
A key problem in improving the accuracy of such
ensemble classification systems is to find new vot-
ing models that (1) exhibit significantly different
prediction biases from the models already voting,
and yet (2) attain stand-alone classification accura-
cies that are as good or better. When either of these
conditions is not met, adding the new voting model
typically degrades the accuracy of the ensemble in-
stead of helping it.
In this work, we investigate the potential of one
promising new disambiguation model with respect
1The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
to augmenting our existing ensemble combining a
maximum entropy model, a boosting model, and
a na??ve Bayes model?a combination representing
some of the best stand-alone WSD models cur-
rently known. The new WSD model, proposed
by Wu et al (2004), is a method for disambiguat-
ing word senses that exploits a nonlinear Kernel
Principal Component Analysis (KPCA) technique.
That the KPCA-based model could potentially be
a good candidate for a new voting model is sug-
gested by Wu et al?s empirical results showing that
it yielded higher accuracies on Senseval-2 data sets
than other models that included maximum entropy,
na??ve Bayes, and SVM based models.
In the following sections, we begin with a de-
scription of the experimental setup, which utilizes
a number of individual classifiers in a voting en-
semble. We then describe the KPCA-based model
to be added to the baseline ensemble. The accuracy
results of the three submitted models are examined,
and also the individual voting models are compared.
Subsequently, we analyze the degree of difference
in voting bias of the KPCA-based model from the
others, and finally show that this does indeed usu-
ally lead to accuracy gains in the voting ensemble.
2 Experimental setup
2.1 Tasks evaluated
We performed experiments on the following lexical
sample tasks from Senseval-3:
English (fine). The English lexical sample task
includes 57 target words (32 verbs, 20 nouns and
5 adjectives). For each word, training and test in-
stances tagged with WordNet senses are provided.
There are an average of 8.5 senses per target word
type, ranging from 3 to 23. On average, 138 training
instances per target word are available.
English (coarse). This modified evaluation of the
preceding task employs a sense map that groups
fine-grained sense distinctions into the same coarse-
grained sense.
Chinese. The Chinese lexical sample task in-
cludes 21 target words. For each word, several
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
senses are defined using the HowNet knowledge
base. There are an average of 3.95 senses per tar-
get word type, ranging from 2 to 8. Only about 37
training instances per target word are available.
Multilingual (t). The Multilingual (t) task is de-
fined similarly to the English lexical sample task,
except that the word senses are the translations into
Hindi, rather than WordNet senses. The Multilin-
gual (t) task requires finding the Hindi sense for 31
English target word types. There are an average of
7.54 senses per target word type, ranging from 3 to
16. A relatively large training set is provided (more
than 260 training instances per word on average).
Multilingual (ts). The Multilingual (ts) task uses
a different data set of 10 target words and provides
the correct English sense of the target word for both
training and testing. There are an average of 6.2
senses per target word type, ranging from 3 to 11.
The training set for this subtask was smaller, with
about 150 training instances per target word.
2.2 Ensemble classification
The WSD models presented here consist of ensem-
bles utilizing various combinations of four voting
models, as follows. Some of these component mod-
els were also evaluated on other Senseval-3 tasks:
the Basque, Catalan, Italian, and Romanian Lexical
Sample tasks (Wicentowski et al, 2004), as well as
Semantic Role Labeling (Ngai et al, 2004).
The first voting model, a na??ve Bayes model, was
built as Yarowsky and Florian (2002) found this
model to be the most accurate classifier in a compar-
ative study on a subset of Senseval-2 English lexical
sample data.
The second voting model, a maximum entropy
model (Jaynes, 1978), was built as Klein and Man-
ning (2002) found that it yielded higher accuracy
than na??ve Bayes in a subsequent comparison of
WSD performance. However, note that a different
subset of either Senseval-1 or Senseval-2 English
lexical sample data was used.
The third voting model, a boosting model (Fre-
und and Schapire, 1997), was built as boosting has
consistently turned in very competitive scores on re-
lated tasks such as named entity classification (Car-
reras et al, 2002)(Wu et al, 2002). Specifically, we
employed an AdaBoost.MH model (Schapire and
Singer, 2000), which is a multi-class generalization
of the original boosting algorithm, with boosting on
top of decision stump classifiers (decision trees of
depth one).
The fourth voting model, the KPCA-based
model, is described below.
All classifier models were selected for their abil-
ity to able to handle large numbers of sparse fea-
tures, many of which may be irrelevant. More-
over, the maximum entropy and boosting models are
known to be well suited to handling features that are
highly interdependent.
2.3 Controlled feature set
In order to facilitate a controlled comparison across
the individual voting models, the same feature set
was employed for all classifiers. The features are
as described by Yarowsky and Florian (2002) in
their ?feature-enhanced na??ve Bayes model?, with
position-sensitive, syntactic, and local collocational
features.
2.4 The KPCA-based WSD model
We briefly summarize the KPCA-based model here;
for full details including illustrative examples and
graphical interpretation, please refer to Wu et al
(2004).
Kernel PCA Kernel Principal Component Analy-
sis is a nonlinear kernel method for extracting non-
linear principal components from vector sets where,
conceptually, the n-dimensional input vectors are
nonlinearly mapped from their original space Rn
to a high-dimensional feature space F where linear
PCA is performed, yielding a transform by which
the input vectors can be mapped nonlinearly to a
new set of vectors (Scho?lkopf et al, 1998).
As with other kernel methods, a major advantage
of KPCA over other common analysis techniques is
that it can inherently take combinations of predic-
tive features into account when optimizing dimen-
sionality reduction. For WSD and indeed many nat-
ural language tasks, significant accuracy gains can
often be achieved by generalizing over relevant fea-
ture combinations (see, e.g., Kudo and Matsumoto
(2003)). A further advantage of KPCA in the con-
text of the WSD problem is that the dimensionality
of the input data is generally very large, a condition
where kernel methods excel.
Nonlinear principal components (Diamantaras
and Kung, 1996) are defined as follows. Suppose
we are given a training set of M pairs (xt, ct) where
the observed vectors xt ? Rn in an n-dimensional
input space X represent the context of the target
word being disambiguated, and the correct class ct
represents the sense of the word, for t = 1, ..,M .
Suppose ? is a nonlinear mapping from the input
space Rn to the feature space F . Without loss of
generality we assume the M vectors are centered
vectors in the feature space, i.e.,
?M
t=1 ?(xt) = 0;
uncentered vectors can easily be converted to cen-
tered vectors (Scho?lkopf et al, 1998). We wish to
diagonalize the covariance matrix in F :
C = 1M
M
?
j=1
?(xj) ?T (xj) (1)
To do this requires solving the equation ?v =
Cv for eigenvalues ? ? 0 and eigenvectors v ?
Rn\ {0}. Because
Cv = 1M
M
?
j=1
(?(xj) ? v)? (xj) (2)
we can derive the following two useful results. First,
? (?(xt) ? v) = ? (xt) ? Cv (3)
for t = 1, ..,M . Second, there exist ?i for i =
1, ...,M such that
v =
M
?
i=1
?i?(xi) (4)
Combining (1), (3), and (4), we obtain
M?
M
?
i=1
?i (?(xt) ? ?(xi ))
=
M
?
i=1
?i(? (xt) ?
M
?
j=1
?(xj)) (?(xj) ? ?(xi ))
for t = 1, ..,M . Let K? be the M ? M matrix such
that
K?ij = ?(xi) ? ?(xj) (5)
and let ??1 ? ??2 ? . . . ? ??M denote the eigenval-
ues of K? and ??1 ,..., ??M denote the corresponding
complete set of normalized eigenvectors, such that
??t(??t ? ??t) = 1 when ??t > 0. Then the lth nonlinear
principal component of any test vector xt is defined
as
ylt =
M
?
i=1
??li (?(xi) ? ?(xt )) (6)
where ??li is the lth element of ??l .
See Wu et al (2004) for a possible geometric in-
terpretation of the power of the nonlinearity.
WSD using KPCA In order to extract nonlin-
ear principal components efficiently, first note that
in both Equations (5) and (6) the explicit form of
?(xi) is required only in the form of (?(xi) ?
?(xj)), i.e., the dot product of vectors in F . This
means that we can calculate the nonlinear princi-
pal components by substituting a kernel function
k(xi, xj) for (?(xi) ? ?(xj )) in Equations (5) and
(6) without knowing the mapping ? explicitly; in-
stead, the mapping ? is implicitly defined by the
kernel function. It is always possible to construct
a mapping into a space where k acts as a dot prod-
uct so long as k is a continuous kernel of a positive
integral operator (Scho?lkopf et al, 1998).
Thus we train the KPCA model using the follow-
ing algorithm:
1. Compute an M ? M matrix K? such that
K?ij = k(xi, xj) (7)
2. Compute the eigenvalues and eigenvectors of
matrix K? and normalize the eigenvectors. Let
??1 ? ??2 ? . . . ? ??M denote the eigenvalues
and ??1,..., ??M denote the corresponding com-
plete set of normalized eigenvectors.
To obtain the sense predictions for test instances,
we need only transform the corresponding vectors
using the trained KPCA model and classify the re-
sultant vectors using nearest neighbors. For a given
test instance vector x, its lth nonlinear principal
component is
ylt =
M
?
i=1
??lik(xi, xt) (8)
where ??li is the ith element of ??l.
For our disambiguation experiments we employ a
polynomial kernel function of the form k(xi, xj) =
(xi ? xj)d, although other kernel functions such as
gaussians could be used as well. Note that the de-
generate case of d = 1 yields the dot product kernel
k(xi, xj) = (xi?xj) which covers linear PCA as a
special case, which may explain why KPCA always
outperforms PCA.
3 Results and discussion
3.1 Accuracy
Table 1 summarizes the results of the submitted sys-
tems along with the individual voting models. Since
our models attempted to disambiguate all test in-
stances, we report accuracy (precision and recall be-
ing equal). Earlier experiments on Senseval-2 data
showed that the KPCA-based model significantly
outperformed both na??ve Bayes and maximum en-
tropy models (Wu et al, 2004). On the Senseval-
3 data, the maximum entropy model fares slightly
better: it remains significantly worse on the Multi-
lingual (ts) task, but achieves statistically the same
accuracy on the English (fine) task and is slightly
Table 1: Comparison of accuracy results for various HKUST ensemble and individual models on Senseval-
3 Lexical Sample tasks, confirming the high accuracy of the KPCA-based model. All test instances were
attempted. (Bold model names were the systems entered.)
English
(fine)
English
(coarse)
Chinese Multilingual
(t)
Multilingual
(ts)
HKUST comb2 (me, boost, nb, kpca) 71.4 78.6 66.2 62.0 63.8
HKUST comb (me, boost, kpca) 70.9 78.1 66.5 61.4 63.8
HKUST me 69.3 76.4 64.4 60.6 60.8
HKUST kpca 69.2 - 63.6 60.0 63.3
HKUST boost 67.0 - 64.1 57.3 60.3
HKUST nb 64.3 - 60.4 57.3 56.8
Table 2: Confusion matrices showing that the KPCA-based model votes very differently from the other
models on the Senseval-3 Lexical Sample tasks. Percentages representing disagreement between KPCA and
other voting models are shown in bold.
kpca vs: me boost nb
task incorrect correct incorrect correct incorrect correct
English incorrect 24.14% 6.62% 21.60% 9.15% 21.04% 9.71%
(fine) correct 6.59% 62.65% 11.38% 57.86% 14.63% 54.61%
Chinese incorrect 24.01% 12.40% 22.96% 13.46% 26.65% 9.76%
correct 11.61% 51.98% 12.93% 50.66% 12.93% 50.66%
Multilingual incorrect 32.71% 7.33% 32.04% 8.01% 30.54% 9.51%
(t) correct 6.74% 53.22% 10.63% 49.33% 12.20% 47.75%
Multilingual incorrect 33.17% 3.52% 31.66% 5.03% 30.15% 6.53%
(ts) correct 6.03% 57.29% 8.04% 55.28% 13.07% 50.25%
more accurate on the Multilingual (t) task. For un-
known reasons?possibly the very small number of
training instances per Chinese target word, as men-
tioned earlier?there is an exception on the Chinese
task, where boosting outperforms the KPCA-based
model. We are investigating the possible causes.
The na??ve Bayes model remains significantly worse
under all conditions.
3.2 Differentiated voting bias
For a new voting model to raise the accuracy of an
existing classifier ensemble, it is not only important
that the new voting model achieve accuracy compa-
rable to the other voters, as shown above, but also
that it provides a significantly differentiated predic-
tion bias than the other voters. Otherwise, the accu-
racy is typically hurt rather than helped by the new
voting model.
To examine whether the KPCA-based model sat-
isfies this requirement, we compared its predictions
against each of the other classifiers (for those tasks
where we have been given the answer key). Table 2
shows nine confusion matrices revealing the per-
centage of instances where the KPCA-based model
votes differently from one of the other voters. The
disagreement between KPCA and the other voting
models ranges from 6.03% to 14.63%, as shown
by the bold entries in the confusion matrices. Note
that where there is disagreement, the KPCA-based
model predicts the correct sense with significantly
higher accuracy, in nearly all cases.
3.3 Voting effectiveness
The KPCA-based model exhibits the accuracy and
differentiation characteristics requisite for an effec-
tive additional voter, as shown in the foregoing sec-
Table 3: Comparison of the accuracies for the voting ensembles with and without the KPCA voter, confirm-
ing that adding the KPCA-based model to the voting ensemble always helps on Senseval-3 Lexical Sample
tasks.
English
(fine)
English
(coarse)
Chinese Multilingual
(t)
Multilingual
(ts)
HKUST comb3 (me, boost, nb) 71.2 - 67.5 60.6 60.8
HKUST comb2 (me, boost, nb, kpca) 71.4 78.6 66.2 62.0 63.8
tions. To verify that adding the KPCA-based model
to the voting ensemble indeed improves accuracy,
we compared our voting ensemble?s accuracies to
that obtained with KPCA removed. The results,
shown in Table 3, confirm that the KPCA-based
model generally helps on Senseval-3 Lexical Sam-
ple tasks. The only exception is on Chinese, due
to the aforementioned anomaly of boosting outper-
forming KPCA on that task. In the Multilingual (t)
and (ts) cases, the improvement in accuracy is sig-
nificant.
4 Conclusion
We have described our word sense disambiguation
system and its performance on the Senseval-3 En-
glish, Chinese, and Multilingual Lexical Sample
tasks. The system consists of an ensemble clas-
sifier utilizing combinations of maximum entropy,
boosting, na??ve Bayes, and a new Kernel PCA based
model.
We have demonstrated that our new model based
on Kernel PCA is, along with maximum entropy,
one of the most accurate stand-alone models vot-
ing in the ensemble, as evaluated under carefully
controlled to ensure the same optimized feature set
across all models being compared. Moreover, we
have shown that the KPCA model exhibits a signif-
icantly different classification bias, a characteristic
that makes it a valuable voter in an ensemble. The
results confirm that accuracy is generally improved
by the addition of the KPCA-based model.
References
Xavier Carreras, Llu??s Ma`rques, and Llu??s Padro?. Named entity
extraction using AdaBoost. In Dan Roth and Antal van den
Bosch, editors, Proceedings of CoNLL-2002, pages 167?
170, Taipei, Taiwan, 2002.
Konstantinos I. Diamantaras and Sun Yuan Kung. Principal
Component Neural Networks. Wiley, New York, 1996.
Yoram Freund and Robert E. Schapire. A decision-theoretic
generalization of on-line learning and an application to
boosting. In Journal of Computer and System Sciences,
55(1), pages 119?139, 1997.
E.T. Jaynes. Where do we Stand on Maximum Entropy? MIT
Press, Cambridge MA, 1978.
Dan Klein and Christopher D. Manning. Conditional struc-
ture versus conditional estimation in NLP models. In Pro-
ceedings of EMNLP-2002, Conference on Empirical Meth-
ods in Natural Language Processing, pages 9?16, Philadel-
phia, July 2002. SIGDAT, Association for Computational
Linguistics.
Taku Kudo and Yuji Matsumoto. Fast methods for kernel-based
text analysis. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages 24?31,
2003.
Grace Ngai, Dekai Wu, Marine Carpuat, Chi-Shing Wang,
and Chi-Yung Wang. Semantic role labeling with boost-
ing, SVMs, maximum entropy, SNOW, and decision lists.
In Proceedings of Senseval-3, Third International Work-
shop on Evaluating Word Sense Disambiguation Systems,
Barcelona, July 2004. SIGLEX, Association for Computa-
tional Linguistics.
Robert E. Schapire and Yoram Singer. Boostexter: A boosting-
based system for text categorization. In Machine Learning,
39(2/3), pages 135?168, 2000.
Bernhard Sch o?lkopf, Alexander Smola, and Klaus-Rober
M u?ller. Nonlinear component analysis as a kernel eigen-
value problem. Neural Computation, 10(5), 1998.
Richard Wicentowski, Grace Ngai, Dekai Wu, Marine Carpuat,
Emily Thomforde, and Adrian Packel. Joining forces to
resolve lexical ambiguity: East meets West in Barcelona.
In Proceedings of Senseval-3, Third International Work-
shop on Evaluating Word Sense Disambiguation Systems,
Barcelona, July 2004. SIGLEX, Association for Computa-
tional Linguistics.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and
Yongsheng Yang. Boosting for named entity recognition.
In Dan Roth and Antal van den Bosch, editors, Proceedings
of CoNLL-2002, pages 195?198. Taipei, Taiwan, 2002.
Dekai Wu, Weifeng Su, and Marine Carpuat. A Kernel PCA
method for superior word sense disambiguation. In Pro-
ceedings of the 42nd Annual Meeting of the Association for
Computational Linguistics, Barcelona, July 2004.
David Yarowsky and Radu Florian. Evaluating sense disam-
biguation across diverse parameter spaces. Natural Lan-
guage Engineering, 8(4):293?310, 2002.
