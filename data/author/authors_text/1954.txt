79
80
81
82
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 48?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic Extraction of Idioms using Graph Analysis and Asymmetric
Lexicosyntactic Patterns
Dominic Widdows
MAYA Design, Inc.
Pittsburgh, Pennsylvania
widdows@maya.com
Beate Dorow
Institute for Natural Language Processing
University of Stuttgart
dorowbe@IMS.Uni-Stuttgart.DE
Abstract
This paper describes a technique for ex-
tracting idioms from text. The tech-
nique works by finding patterns such as
?thrills and spills?, whose reversals (such
as ?spills and thrills?) are never encoun-
tered.
This method collects not only idioms, but
also many phrases that exhibit a strong
tendency to occur in one particular order,
due apparently to underlying semantic is-
sues. These include hierarchical relation-
ships, gender differences, temporal order-
ing, and prototype-variant effects.
1 Introduction
Natural language is full of idiomatic and metaphor-
ical uses. However, language resources such as dic-
tionaries and lexical knowledge bases give at best
poor coverage of such phenomena. In many cases,
knowledge bases will mistakenly ?recognize? a word
and this can lead to more harm than good: for exam-
ple, a typical mistake of blunt logic would be to as-
sume that ?somebody let the cat out of the bag? im-
plied that ?somebody let some mammal out of some
container.?
Idiomatic generation of natural language is, if
anything, an even greater challenge than idiomatic
language understanding. As pointed out decades ago
by Fillmore (1967), a complete knowledge of En-
glish requires not only an understanding of the se-
mantics of the word good, but also an awareness
that this special adjective (alone) can occur with the
word any to construct phrases like ?Is this paper
any good at all??, and traditional lexical resources
were not designed to provide this information. There
are many more general examples occur: for exam-
ple, ?the big bad wolf? sounds right and the ?the bad
big wolf? sounds wrong, even though both versions
are syntactically and semantically plausible. Such
examples are perhaps ?idiomatic?, though we would
perhaps not call them ?idioms?, since they are com-
positional and can sometimes be predicted by gen-
eral pattern of word-ordering.
In general, the goal of manually creating a com-
plete lexicon of idioms and idiomatic usage patterns
in any language is unattainable, and automatic ex-
traction and modelling techniques have been devel-
oped to fill this ever-evolving need. Firstly, auto-
matically identifying potential idioms and bringing
them to the attention of a lexicographer can be used
to improve coverage and reduce the time a lexicog-
rapher must spend in searching for such examples.
Secondly and more ambitiously, the goal of such
work is to enable computers to recognize idioms in-
dependently so that the inevitable lack of coverage
in language resources does not impede their ability
to respond intelligently to natural language input.
In attempting a first-pass at this task, the exper-
iments described in this paper proceed as follows.
We focus on a particular class of idioms that can
be extracted using lexicosyntactic patterns (Hearst,
1992), which are fixed patterns in text that suggest
that the words occurring in them have some inter-
esting relationship. The patterns we focus on are
occurrences of the form ?A and/or B?, where A and
48
B are both nouns. Examples include ?football and
cricket? and ?hue and cry.? From this list, we extract
those examples for which there is a strong prefer-
ence on the ordering of the participants. For exam-
ple, we do see the pattern ?cricket and football,? but
rarely if ever encounter the pattern ?cry and hue.?
Using this technique, 4173 potential idioms were ex-
tracted. This included a number of both true idioms,
and words that have regular semantic relationships
but do appear to have interesting orderings on these
relationships (such as earlier before later, strong be-
fore weak, prototype before variant).
The rest of this paper is organized as follows. Sec-
tion 2 elaborates on some of the previous works
that motivate the techniques we have used. Sec-
tion 3 describes the precise method used to extract
idioms through their asymmetric appearance in a
large corpus. Section 4 presents and analyses several
classes of results. Section 5 describes the methods
attempted to filter these results into pairs of words
that are more and less contextually related to one an-
other. These include a statistical method that analy-
ses the original corpus for evidence of semantic re-
latedness, and a combinatoric method that relies on
link-analysis on the resulting graph structure.
2 Previous and Related Work
This section describes previous work in extracting
information from text, and inferring semantic or id-
iomatic properties of words from the information so
derived.
The main technique used in this paper to ex-
tract groups of words that are semantically or id-
iomatically related is a form of lexicosyntactic pat-
tern recognition. Lexicosyntactic patterns were pio-
neered by Marti Hearst (Hearst, 1992; Hearst and
Schu?tze, 1993) in the early 1990?s, to enable the
addition of new information to lexical resources
such as WordNet (Fellbaum, 1998). The main in-
sight of this sort of work is that certain regular pat-
terns in word-usage can reflect underlying seman-
tic relationships. For example, the phrase ?France,
Germany, Italy, and other European countries? sug-
gests that France, Germany and Italy are part of
the class of European countries. Such hierarchi-
cal examples are quite sparse, and greater coverage
was later attained by Riloff and Shepherd (1997)
and Roark and Charniak (1998) in extracting rela-
tions not of hierarchy but of similarity, by find-
ing conjunctions or co-ordinations such as ?cloves,
cinammon, and nutmeg? and ?cars and trucks.? This
work was extended by Caraballo (1999), who built
classes of related words in this fashion and then rea-
soned that if a hierarchical relationship could be ex-
tracted for any member of this class, it could be ap-
plied to all members of the class. This technique
can often mistakenly reason across an ambiguous
middle-term, a situation that was improved upon
by Cederberg and Widdows (2003), by combining
pattern-based extraction with contextual filtering us-
ing latent semantic analysis.
Prior work in discovering non-compositional
phrases has been carried out by Lin (1999)
and Baldwin et al (2003), who also used LSA
to distinguish between compositional and non-
compositional verb-particle constructions and noun-
noun compounds.
At the same time, work in analyzing idioms and
asymmetry within linguistics has become more so-
phisticated, as discussed by Benor and Levy (2004),
and many of the semantic factors underlying our re-
sults can be understood from a sophisticated theoret-
ical perspective.
Other motivating and related themes of work for
this paper include collocation extraction and ex-
ample based machine translation. In the work of
Smadja (1993) on extracting collocations, prefer-
ence was given to constructions whose constituents
appear in a fixed order, a similar (and more generally
implemented) version of our assumption here that
asymmetric constructions are more idiomatic than
symmetric ones. Recent advances in example-based
machine translation (EBMT) have emphasized the
fact that examining patterns of language use can
significantly improve idiomatic language generation
(Carl and Way, 2003).
3 The Symmetric Graph Model as used for
Lexical Acquisition and Idiom
Extraction
This section of the paper describes the techniques
used to extract potentially idiomatic patterns from
text, as deduced from previously successful experi-
ments in lexical acquisition.
49
The main extraction technique is to use lexicosyn-
tactic patterns of the form ?A, B and/or C? to find
nouns that are linked in some way. For example,
consider the following sentence from the British Na-
tional Corpus (BNC).
Ships laden with nutmeg, cinnamon,
cloves or coriander once battled the
Seven Seas to bring home their precious
cargo.
Since the BNC is tagged for parts-of-speech, we
know that the words highlighted in bold are nouns.
Since the phrase ?nutmeg, cinnamon, cloves or co-
riander? fits the pattern ?A, B, C or D?, we create
nodes for each of these nouns and create links be-
tween them all. When applied to the whole of the
BNC, these links can be aggregated to form a graph
with 99,454 nodes (nouns) and 587,475 links, as de-
scribed by Widdows and Dorow (2002). This graph
was originally used for lexical acquisition, since
clusters of words in the graph often map to recog-
nized semantic classes with great accuracy (> 80%,
(Widdows and Dorow, 2002)).
However, for the sake of smoothing over sparse
data, these results made the assumption that the links
between nodes were symmetric, rather than directed.
In other words, when the pattern ?A and/or B? was
encountered, a link from A to B and a link from B
to A was introduced. The nature of symmetric and
antisymmetric relationships is examined in detail by
Widdows (2004). For the purposes of this paper, it
suffices to say that the assumption of symmetry (like
the assumption of transitivity) is a powerful tool for
improving recall in lexical acquisition, but also leads
to serious lapses in precision if the directed nature of
links is overlooked, especially if symmetrized links
are used to infer semantic similarity.
This problem was brought strikingly to our atten-
tion by the examples in Figure 1. In spite of appear-
ing to be a circle of related concepts, many of the
nouns in this group are not similar at all, and many
of the links in this graph are derived from very very
different contexts. In Figure 1, cat and mouse are
linked (they are re both animals and the phrase ?cat
and mouse? is used quite often): but then mouse
and keyboard are also linked because they are both
objects used in computing. A keyboard, as well
as being a typewriter or computer keyboard, is also
fiddle
cat
barrowbow
cello
flute
mouse
dog
game
kitten
violin
piano
bass
fortepiano
orchestra
keyboard
screen
monitor
memory
guitar
rat
human
Figure 1: A cluster involving several idiomatic links
used to mean (part of) a musical instrument such as
an organ or piano, and keyboard is linked to vio-
lin. A violin and a fiddle are the same instrument (as
often happens with synonyms, they don?t appear to-
gether often but have many neighbours in common).
The unlikely circle is completed (it turns out) be-
cause of the phrase from the nursery rhyme
Hey diddle diddle,
The cat and the fiddle,
The cow jumped over the moon;
It became clear from examples such as these that
idiomatic links, like ambiguous words, were a seri-
ous problem when using the graph model for lexical
acquisition. However, with ambiguous words, this
obstacle has been gradually turned into an opportu-
nity, since we have also developed ways to used the
apparent flaws in the model to detect which words
are ambiguous in the first place (Widdows, 2004, Ch
4). It is now proposed that we can take the same op-
portunity for certain idioms: that is, to use the prop-
erties of the graph model to work out which links
arise from idiomatic usage rather than semantic sim-
ilarity.
3.1 Idiom Extraction by Recognizing
Asymmetric Patterns
The link between the cat and fiddle nodes in Fig-
ure 1 arises from the phrase ?the cat and the fiddle.?
50
Table 1: Sample of asymmetric pairs extracted from
the BNC.
First word Second word
highway byway
cod haddock
composer conductor
wood charcoal
element compound
assault battery
north south
rock roll
god goddess
porgy bess
middle class
war aftermath
god hero
metal alloy
salt pepper
mustard cress
stocking suspender
bits bobs
stimulus response
committee subcommittee
continent ocean
However, no corpus examples were ever found of the
converse phrase, ?the fiddle and the cat.? In cases
like these, it may be concluded that placing a sym-
metric link between these two nodes is a mistake.
Instead, a directed link may be more appropriate.
We therefore formed the hypothesis that if the
phrase ?A and/or B? occurs frequently in a corpus,
but the phrase ?B and/or A? is absent, then the link
between A and B should be attributed to idiomatic
usage rather than semantic similarity.
The next step was to rebuild, finding those rela-
tionships that have a strong preference for occurring
in a fixed order. Sure enough, several British English
idioms were extracted in this way. However, several
other kinds of relationships were extracted as well,
as shown in the sample in Table 1.1
After extracting these pairs, groups of them were
gathered together into directed subgraphs.2 Some of
these directed subgraphs are reporduced in the anal-
ysis in the following section.
1The sample chosen here was selected by the authors to be
representative of some of the main types of results. The com-
plete list can be found at http://infomap.stanford.
edu/graphs/idioms.html.
2These can be viewed at http://infomap.
stanford.edu/graphs/directed_graphs.html
4 Analysis of Results
The experimental results include representatives of
several types of asymmetric relationships, including
the following broad categories.
?True? Idioms
There are many results that display genuinely id-
iomatic constructions. By this, we mean phrases that
have an explicitly lexicalized nature that a native
speaker may be expected to recognize as having a
special reference or significance. Examples include
the following:
thrills and spills
bread and circuses
Punch and Judy
Porgy and Bess
lies and statistics
cat and fiddle
bow and arrow
skull and crossbones
This category is quite loosely defined. It includes
1. historic quotations such as ?lies, damned lies
and statistics?3 and ?bread and circuses.?4
2. titles of well-known works.
3. colloquialisms.
4. groups of objects that have become fixed nom-
inals in their own right.
All of these types share the common property that
any NLP system that encounters such groups, in or-
der to behave correctly, should recognize, generate,
or translate them as phrases rather than words.
Hierarchical Relationships
Many of the asymmetric relationships follow
some pattern that may be described as roughly hi-
erarchical. A cluster of examples from two domains
is shown in Figure 2. In chess, a rook outranks a
bishop, and the phrase ?rook and bishop? is encoun-
tered much more often than the phrase ?bishop and
3Attributed to Benjamin Disraeli, certainly popularized by
Mark Twain.
4A translation of ?panem et circenses,? from the Roman
satirist Juvenal, 1st century AD.
51
Figure 2: Asymmetric relationships in the chess and
church hierarchies
Figure 3: Different beverages, showing their di-
rected relationships
rook.? In the church, a cardinal outranks a bishop,
a bishop outranks most of the rest of the clergy, and
the clergy (in some senses) outrank the laity.
Sometimes these relationships coincide with fig-
ure / ground and agent / patient distinctions. Ex-
amples of this kind, as well as ?clergy and laity?,
include ?landlord and tenant?, ?employer and em-
ployee?, ?teacher and pupil?, and ?driver and pas-
sengers?. An interesting exception is ?passengers
and crew?, for which we have no semantic explana-
tion.
Pedigree and potency appear to be two other di-
mensions that can be used to establish the directed-
ness of an idiomatic construction. For example, Fig-
ure 3 shows that alcoholic drinks normally appear
before their cocktail mixers, but that wine outranks
some stronger drinks.
Figure 4: Hierarchical relationships between aristo-
crats, some of which appear to be gender based
Gender Asymmetry
The relationship between corresponding concepts
of different genders also appear to be heavily biased
towards appearing in one direction. Many of these
relationships are shown in Figure 4. This shows
that, in cases where one class outranks another, the
higher class appears first, but if the classes are iden-
tical, then the male version tends to appear before
the female. This pattern is repeated in many pairs
of words such as ?host and hostess?, ?god and god-
dess?, etc. One exception appears to be in parent-
ing relationships, where female precedes male, as in
?mother and father?, ?mum and dad?, ?grandma and
grandpa?.
Temporal Ordering
If one word refers to an event that precedes an-
other temporally or logically, it almost always ap-
pears first. The examples in Table 2 were extracted
by our experiment. It has been pointed out that for
cyclical events, it is perfectly possible that the order
of these pairs may be reversed (e.g., ?late night and
early morning?), though the data we extracted from
the BNC showed strong tendencies in the directions
given.
A directed subgraph showing many events in hu-
man lives in shown in Figure 5.
Prototype precedes Variant
In cases where one participant is regarded as a
?pure? substance and the other is a variant or mix-
ture, the pure substance tends to come first. These
occur particularly in scientific writing, examples
including ?element and compound?, ?atoms and
52
Table 2: Pairs of events that have a strong tendency
to occur in asymmetric patterns.
Before After
spring autumn
morning afternoon
morning evening
evening night
morning night
beginning end
question answer
shampoo conditioner
marriage divorce
arrival departure
eggs larvae
molecules?, ?metals and alloys?. Also, we see ?ap-
ples and pears?, ?apple and plums?, and ?apples and
oranges?, suggesting that an apple is a prototypical
fruit (in agreement with some of the results of pro-
totype theory; see Rosch (1975)).
Another possible version of this tendency is that
core precedes periphery, which may also account for
asymmetric ordering of food items such as ?fish and
chips?, ?bangers and mash?, ?tea and coffee? (in the
British National Corpus, at least!) In some cases
such as ?meat and vegetables?, a hierarchical or fig-
ure / ground distinction may also be argued.
Mistaken extractions
Our preliminary inspection has shown that the ex-
traction technique finds comparatively few genuine
mistakes, and the reader is encouraged to follow the
links provided to check this claim. However, there
are some genuine errors, most of which could be
avoided with more sophisticated preprocessing.
To improve recall in our initial lexical acquisition
experiments, we chose to strip off modifiers and to
stem plural forms to singular forms, so that ?apples
and green pears? would give a link between apple
and pear.
However, in many cases this is a mistake, be-
cause the bracketing should not be of the form ?A
and (B C),? but of the form ?(A and B) C.? Us-
ing part-of-speech tags alone, we cannot recover
this information. One example is the phrase ?hard-
ware and software vendors,? from which we ob-
tain a link between hardware and vendors, in-
stead of a link between hardware and software.
A fuller degree of syntactic analysis would improve
this situation. For extracting semantic relationships,
Figure 5: Directed graph showing that life-events
are usually ordered temporally when they occur to-
gether
Cederberg and Widdows (2003) demonstrated that
nounphrase chunking does this work very satisfacto-
rily, while being much more tractable than full pars-
ing.
The mistaken pair middle and class shown in
Table 1 is another of these mistakes, arising from
phrases such as ?middle and upper class? and ?mid-
dle and working class.? These examples could be
avoided simply by more accurate part-of-speech tag-
ging (since the word ?middle? should have been
tagged as an adjective in these examples).
This concludes our preliminary analysis of re-
sults.
5 Filtering using Latent Semantic Analysis
and Combinatoric Analysis
From the results in the previous section, the follow-
ing points are clear.
1. It is possible to extract many accurate exam-
ples of asymmetric constructions, that would be
necessary knowledge for generation of natural-
sounding language.
2. Some of the pairs extracted are examples of
general semantic patterns, others are examples
of genuinely idiomatic phrases.
Even for semantically predictable phrases, the
fact that the words occur in fixed patterns can be
very useful for the purposes of disambiguation, as
demonstrated by (Yarowsky, 1995). However, it
53
would be useful to be able to tell which of the asym-
metric patterns extracted by our experiments corre-
spond to semantically regular phrases which hap-
pen to have a conventional ordering preference, and
which phrases correspond to genuine idioms. This
final section demonstrates two techniques for per-
forming this filtering task, which show promising re-
sults for improving our classification, though should
not yet be considered as reliable.
5.1 Filtering using Latent Semantic Analysis
Latent semantic analysis or LSA (Landauer and Du-
mais, 1997) is by now a tried and tested technique
for determining semantic similarity between words
by analyzing large corpus (Widdows, 2004, Ch 6).
Because of this, LSA can be used to determine
whether a pair of words is likely to participate in a
regular semantic relationship, even though LSA may
not contribute specific information regarding the na-
ture of the relationship. However, once a relation-
ship is expected, LSA can be used to predict whether
this relationship is used in contexts that are typical
uses of the words in question, or whether these uses
appear to be anomalies such as rare senses or idioms.
This technique was used successfully by (Cederberg
and Widdows, 2003) to improve the accuracy of hy-
ponymy extraction. It follows that it should be use-
ful to tell the difference between regularly related
words and idiomatically related words.
To test this hypothesis, we used an LSA model
built from the BNC using the Infomap NLP soft-
ware.5 This was used to measure the LSA similar-
ity between the words in each of the pairs extracted
by the techniques in Section 4. In cases where a
word was too infrequent to appear in the LSA model,
we used ?folding in,? which assigns a word-vector
?on the fly? by adding together the vectors of any
surrounding words of a target word that are in the
model.
The results are shown in Table 3. The hypothesis
is that words whose occurrence is purely idiomatic
would have a low LSA similarity score, because
they are otherwise not closely related. However, this
hypothesis does not seem to have been confirmed,
partly due to the effects of overall frequency. For
example, the word Porgy only occurs in the phrase
5Freely available from http://infomap-nlp.
sourceforge.net/
Table 3: Ordering of results from semantically sim-
ilar to semantically dissimilar using LSA
Word pair LSA similarity
north south 0.931
middle class 0.834
porgy bess 0.766
war aftermath 0.676
salt pepper 0.672
bits bobs 0.671
mustard cress 0.603
composer conductor 0.588
cod haddock 0.565
metal alloy 0.509
highway byway 0.480
committee subcommittee 0.479
god goddess 0.456
rock roll 0.398
continent ocean 0.300
wood charcoal 0.273
stimulus response 0.261
stocking suspender 0.177
god hero 0.115
element compound 0.044
assault battery -0.068
granite
cheese
bread
chalk
limestone   flint
marble  coal sand
 sandstone butter meat wine
sugar  margarine milk  clay
Figure 6: Nodes in the original symmetric graph in
the vicinity of chalk and cheese
?Porgy and Bess,? and the word bobs almost always
occurs in the phrase ?bits and bobs.? A more effec-
tive filtering technique would need to normalize to
account for these effects. However, there are some
good results: for example, the low score between
assault and battery reflects the fact that this usage,
though compositional, is a rare meaning of the word
battery, and the same argument can be made for el-
ement and compound. Thus LSA might be a better
guide for recognizing rarity in meaning of individual
words than it is for idiomaticity of phrases.
5.2 Link analysis
Another technique for determining whether a link is
idiomatic or not is to check whether it connects two
54
areas of meaning that are otherwise unconnected. A
hallmark example of this phenomenon is the ?chalk
and cheese? example shown in Figure 6. 6 Note that
none of the other members of the rock-types clus-
ters is linked to any of the other foodstuffs. We may
be tempted to conclude that the single link between
these clusters is an idiomatic phenomenon. This
technique shows promise, but has yet to be explored
in detail.
6 Conclusions and Further Work
It is possible to extract asymmetric constructions
from text, some of which correspond to idioms
which are indecomposable (in the sense that their
meaning cannot be decomposed into a combination
of the meanings of their constituent words).
Many other phrases were extracted which exhibit
a typical directionality that follows from underlying
semantic principles. While these are sometimes not
defined as ?idioms? (because they are still compos-
able), knowledge of their asymmetric behaviour is
necessary for a system to generate natural language
utterances that would sound ?idiomatic? to native
speakers.
While all of this information is useful for cor-
rectly interpreting and generating natural language,
further work is necessary to distinguish accurately
between these different categories. The first step in
this process will be to manually classify the results,
and evaluate the performance of different classifica-
tion techniques to see if they can reliably identify
different types of idiom, and also distinguish these
cases from false positives that were mistakenly ex-
tracted. Once some of these techniques have been
evaluated, we will be in a better position to broaden
our techniques by turning to larger corpora such as
the Web.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL-2003 Workshop on Multiword Expres-
6
?Chalk and cheese? is a widespread idiom in British En-
glish, used to contrast two very different objects, e.g. ?They
are as different as chalk and cheese.? A roughly corresponding
(though more predictable) phrase in American English might be
?They are as different as night and day.?
sions: Analysis, Acquisition and Treatment, Sapporo,
Japan.
Sarah Bunin Benor and Roger Levy. 2004. The chicken
or the egg? a probabilistic analysis of english bi-
nomials. http://www.stanford.edu/?rog/
papers/binomials.pdf.
Sharon Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In 37th
Annual Meeting of the Association for Computational
Linguistics: Proceedings of the Conference, pages
120?126.
M Carl and A Way, editors. 2003. Recent Advances in
Example-Based Machine Translation. Kluwer.
Scott Cederberg and Dominic Widdows. 2003. Using
LSA and noun coordination information to improve
the precision and recall of automatic hyponymy ex-
traction. In Conference on Natural Language Learn-
ing (CoNNL), Edmonton, Canada.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge MA.
Charles J. Fillmore. 1967. The grammar of hitting and
breaking. In R. Jacobs, editor, In Readings in English:
Transformational Grammar, pages 120?133.
Marti Hearst and Hinrich Schu?tze. 1993. Customizing
a lexicon to better suit a computational task. In ACL
SIGLEX Workshop, Columbus, Ohio.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING, Nantes,
France.
Thomas Landauer and Susan Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis the-
ory of acquisition. Psychological Review, 104(2):211?
240.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL:1999, pages 317?324.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Claire
Cardie and Ralph Weischedel, editors, Proceedings of
the Second Conference on Empirical Methods in Natu-
ral Language Processing, pages 117?124. Association
for Computational Linguistics, Somerset, New Jersey.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurence statistics for semi-automatic semantic
lexicon construction. In COLING-ACL, pages 1110?
1116.
Eleanor Rosch. 1975. Cognitive representations of se-
mantic categories. Journal of Experimental Psychol-
ogy: General, 104:192?233.
55
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In 19th In-
ternational Conference on Computational Linguistics,
pages 1093?1099, Taipei, Taiwan, August.
Dominic Widdows. 2004. Geometry and Meaning.
CSLI publications, Stanford, California.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics, pages 189?196.
56
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 91?95,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
A Graph-Theoretic Algorithm for Automatic Extension of Translation
Lexicons
Beate Dorow Florian Laws Lukas Michelbacher Christian Scheible Jason Utt
Institute for Natural Language Processing
Universita?t Stuttgart
{dorowbe,lawsfn,michells,scheibcn,uttjn}@ims.uni-stuttgart.de
Abstract
This paper presents a graph-theoretic
approach to the identification of yet-
unknown word translations. The proposed
algorithm is based on the recursive Sim-
Rank algorithm and relies on the intuition
that two words are similar if they estab-
lish similar grammatical relationships with
similar other words. We also present a for-
mulation of SimRank in matrix form and
extensions for edge weights, edge labels
and multiple graphs.
1 Introduction
This paper describes a cross-linguistic experiment
which attempts to extend a given translation dic-
tionary with translations of novel words.
In our experiment, we use an English and
a German text corpus and represent each cor-
pus as a graph whose nodes are words and
whose edges represent grammatical relationships
between words. The corpora need not be parallel.
Our intuition is that a node in the English and a
node in the German graph are similar (that is, are
likely to be translations of one another), if their
neighboring nodes are. Figure 1 shows part of the
English and the German word graph.
Many of the (first and higher order) neighbors
of food and Lebensmittel translate to one another
(marked by dotted lines), indicating that food and
Lebensmittel, too, are likely mutual translations.
Our hypothesis yields a recursive algorithm for
computing node similarities based on the simi-
larities of the nodes they are connected to. We
initialize the node similarities using an English-
German dictionary whose entries correspond to
known pairs of equivalent nodes (words). These
node equivalences constitute the ?seeds? from
which novel English-German node (word) corre-
spondences are bootstrapped.
We are not aware of any previous work using a
measure of similarity between nodes in graphs for
cross-lingual lexicon acquisition.
Our approach is appealing in that it is language
independent, easily implemented and visualized,
and readily generalized to other types of data.
Section 2 is dedicated to related research on
the automatic extension of translation lexicons. In
Section 3 we review SimRank (Jeh and Widom,
2002), an algorithm for computing similarities of
nodes in a graph, which forms the basis of our
work. We provide a formulation of SimRank in
terms of simple matrix operations which allows
an efficient implementation using optimized ma-
trix packages. We further present a generalization
of SimRank to edge-weighted and edge-labeled
graphs and to inter-graph node comparison.
Section 4 describes the process used for build-
ing the word graphs. Section 5 presents an experi-
ment for evaluating our approach to bilingual lex-
icon acquisition. Section 6 reports the results. We
present our conclusions and directions for future
research in Section 7.
2 Related Work on cross-lingual lexical
acquisition
The work by Rapp (1999) is driven by the idea
that a word and its translation to another lan-
guage are likely to co-occur with similar words.
Given a German and an English corpus, he com-
putes two word-by-word co-occurrence matrices,
one for each language, whose columns span a vec-
tor space representing the corresponding corpus.
In order to find the English translation of a Ger-
man word, he uses a base dictionary to translate
all known column labels to English. This yields
a new vector representation of the German word
in the English vector space. This mapped vector
is then compared to all English word vectors, the
most similar ones being candidate translations.
91
food Lebensmittel
receive erhalten
award Preis
provide liefern
evidence Beweis
buy kaufen
book Buch
publish verlegen
boat Haus
waste ablehnen
Figure 1: Likely translations based on neighboring nodes
Rapp reports an accuracy of 72% for a small
number of test words with well-defined meaning.
Diab and Finch (2000) first compute word sim-
ilarities within each language corpus separately
by comparing their co-occurrence vectors. Their
challenge then is to derive a mapping from one
language to the other (i.e. a translation lexicon)
which best preserves the intra-language word sim-
ilarities. The mapping is initialized with a few seed
?translations? (punctuation marks) which are as-
sumed to be common to both corpora.
They test their method on two corpora written
in the same language and report accuracy rates of
over 90% on this pseudo-translation task. The ap-
proach is attractive in that it does not require a
seed lexicon. A drawback is its high computational
cost.
Koehn and Knight (2002) use a (linear) com-
bination of clues for bootstrapping an English-
German noun translation dictionary. In addition to
similar assumptions as above, they consider words
to be likely translations of one another if they have
the same or similar spelling and/or occur with sim-
ilar frequencies. Koehn and Knight reach an accu-
racy of 39% on a test set consisting of the 1,000
most frequent English and German nouns. The
experiment excludes verbs whose semantics are
more complex than those of nouns.
Otero and Campos (2005) extract English-
Spanish pairs of lexico-syntactic patterns from a
small parallel corpus. They then construct con-
text vectors for all English and Spanish words by
recording their frequency of occurrence in each of
these patterns. English and Spanish vectors thus
reside in the same vector space and are readily
compared.
The approach reaches an accuracy of 89% on a
test set consisting of 100 randomly chosen words
from among those with a frequency of 100 or
higher. The authors do not report results for low-
frequency words.
3 The SimRank algorithm
An algorithm for computing similarities of nodes
in graphs is the SimRank algorithm (Jeh and
Widom, 2002). It was originally proposed for di-
rected unweighted graphs of web pages (nodes)
and hyperlinks (links).
The idea of SimRank is to recursively com-
pute node similarity scores based on the scores
of neighboring nodes. The similarity Sij of two
different nodes i and j in a graph is defined as
the normalized sum of the pairwise similarities of
their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl. (1)
N(i) and N(j) are the set of i?s and j?s neigh-
bors respectively, and c is a multiplicative factor
smaller than but close to 1 which demotes the con-
tribution of higher order neighbors. Sij is set to 1
if i and j are identical, which provides a basis for
the recursion.
3.1 Matrix formulation of SimRank
We derive a formulation of the SimRank similarity
updates which merely consists of matrix multipli-
cations as follows. In terms of the graph?s (binary)
adjacency matrix A, the SimRank recursion reads:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Aik Ajl Skl
(2)
noting that AikAjl = 1, iff k is a neighbor of i
and l is a neighbor of j at the same time. This is
92
equivalent to
Sij = c
?
k,l
Aik
|N(i)|
Ajl
|N(j)| Skl (3)
= c
?
k,l
Aik
?
? Ai?
Ajl
?
? Aj?
Skl.
The Sij can be assembled in a square node sim-
ilarity matrix S, and it is easy to see that the indi-
vidual similarity updates can be summarized as:
Sk = c A? Sk?1A?T (4)
where A? is the row-normalized adjacency matrix
and k denotes the current level of recursion. A? is
obtained by dividing each entry of A by the sum of
the entries in its row. The SimRank iteration is ini-
tialized with S = I , and the diagonal of S, which
contains the node self-similarities, is reset to ones
after each iteration.
This representation of SimRank in closed ma-
trix form allows the use of optimized off-the-shelf
sparse matrix packages for the implementation of
the algorithm. This rendered the pruning strate-
gies proposed in the original paper unnecessary.
We also note that the Bipartite SimRank algorithm
introduced in (Jeh and Widom, 2002) is just a spe-
cial case of Equation 4.
3.2 Extension with weights and link types
The SimRank algorithm assumes an unweighted
graph, i.e. a binary adjacency matrix A. Equa-
tion 4 can equally be used to compute similarities
in a weighted graph by letting A? be the graph?s
row-normalized weighted adjacency matrix. The
entries of A? then represent transition probabili-
ties between nodes rather than hard (binary) adja-
cency. The proof of the existence and uniqueness
of a solution to this more general recursion pro-
ceeds in analogy to the proof given in the original
paper.
Furthermore, we allow the links in the graph to
be of different types and define the following gen-
eralized SimRank recursion, where T is the set of
link types and Nt(i) denotes the set of nodes con-
nected to node i via a link of type t.
Sij =
c
|T |
?
t?T
1
|Nt(i)| |Nt(j)|
?
k?Nt(i),l?Nt(j)
Skl.
(5)
In matrix formulation:
Sk =
c
|T |
?
t?T
A?t Sk?1A?t
T (6)
where At is the adjacency matrix associated with
link type t and, again, may be weighted.
3.3 SimRank across graphs
SimRank was originally designed for the com-
parison of nodes within a single graph. However,
SimRank is readily and accordingly applied to
the comparison of nodes of two different graphs.
The original SimRank algorithm starts off with the
nodes? self-similarities which propagate to other
non-identical pairs of nodes. In the case of two dif-
ferent graphs A and B, we can instead initialize the
algorithm with a set of initially known node-node
correspondences.
The original SimRank equation (2) then be-
comes
Sij =
c
|N(i)| |N(j)|
?
k,l
Aik Bjl Skl, (7)
which is equivalent to
Sk = c A? Sk?1 B?T , (8)
or, if links are typed,
Sk =
c
|T |
?
t?T
A?t Sk?1 B?t
T . (9)
The similarity matrix S is now a rectangular
matrix containing the similarities between nodes
in A and nodes in B. Those entries of S which
correspond to known node-node correspondences
are reset to 1 after each iteration.
4 The graph model
The grammatical relationships were extracted
from the British National Corpus (BNC) (100 mil-
lion words), and the Huge German Corpus (HGC)
(180 million words of newspaper text). We com-
piled a list of English verb-object (V-O) pairs
based on the verb-argument information extracted
by (Schulte im Walde, 1998) from the BNC. The
German V-O pairs were extracted from a syntactic
analysis of the HGC carried out using the BitPar
parser (Schmid, 2004).
We used only V-O pairs because they consti-
tute far more sense-discriminative contexts than,
for example, verb-subject pairs, but we plan to ex-
amine these and other grammatical relationships
in future work.
We reduced English compound nouns to their
heads and lemmatized all data. In English phrasal
93
English German
Low Mid High Low Mid High
N V N V N V N V N V N V
0.313 0.228 0.253 0.288 0.253 0.255 0.232 0.247 0.205 0.237 0.211 0.205
Table 1: The 12 categories of test words, with mean relative ranks of test words
verbs, we attach the particles to the verbs to dis-
tinguish them from the original verb (e.g put off
vs. put). Both the English and German V-O pairs
were filtered using stop lists consisting of modal
and auxiliary verbs as well as pronouns. To reduce
noise, we decided to keep only those relationships
which occurred at least three times in the respec-
tive corpus.
The English and German data alike are then rep-
resented as a bipartite graph whose nodes divide
into two sets, verbs and nouns, and whose edges
are the V-O relationships which connect verbs to
nouns (cf. Figure 1). The edges of the graph are
weighted by frequency of occurrence.
We ?prune? both the English and German graph
by recursively removing all leaf nodes (nodes with
a single neighbor). As these correspond to words
which appear only in a single relationship, there is
only limited evidence of their meaning.
After pruning, there are 4,926 nodes (3,365
nouns, 1,561 verbs) and 43,762 links in the En-
glish, and 3,074 nodes (2,207 nouns, 867 verbs)
and 15,386 links in the German word graph.
5 Evaluation experiment
The aim of our evaluation experiment is to test
the extended SimRank algorithm for its ability to
identify novel word translations given the English
and German word graph of the previous section
and an English-German seed lexicon. We use the
dict.cc English-German dictionary 1.
Our evaluation strategy is as follows. We se-
lect a set of test words at random from among the
words listed in the dictionary, and remove their en-
tries from the dictionary. We run six iterations of
SimRank using the remaining dictionary entries
as the seed translations (the known node equiv-
alences), and record the similarities of each test
word to its known translations. As in the original
SimRank paper, c is set to 0.8.
We include both English and German test words
and let them vary in frequency: high- (> 100),
1http://www.dict.cc/ (May 5th 2008)
mid- (> 20 and ? 100), and low- (? 20) fre-
quent as well as word class (noun, verb). Thus, we
obtain 12 categories of test words (summarized in
Table 1), each of which is filled with 50 randomly
selected words, giving a total of 600 test words.
SimRank returns a matrix of English-German
node-node similarities. Given a test word, we ex-
tract its row from the similarity matrix and sort the
corresponding words by their similarities to the
test word. We then scan this sorted list of words
and their similarities for the test word?s reference
translations (those listed in the original dictionary)
and record their positions (i.e. ranks) in this list.
We then replace absolute ranks with relative ranks
by dividing by the total number of candidate trans-
lations.
6 Results
Table 1 lists the mean relative rank of the reference
translations for each of the test categories. The
values of around 0.2-0.3 clearly indicate that our
approach ranks the reference translations much
higher than a random process would.
Relative rank
Fr
eq
ue
nc
y
0.0 0.2 0.4 0.6 0.8 1.0
0
5
15
25
Figure 2: Distribution of the relative ranks of the
reference translations in the English-High-N test
set.
Exemplary of all test sets, Figure 2 shows the
distribution of the relative ranks of the reference
translations for the test words in English-High-N.
The bulk of the distribution lies below 0.3, i.e. in
the top 30% of the candidate list.
In order to give the reader an idea of the results,
we present some examples of test words and their
94
Test word Top 10 predicted translations Ranks
sanction Ausgangssperre Wirtschaftssanktion
Ausnahmezustand Embargo Moratorium
Sanktion Todesurteil Geldstrafe Bu?geld
Anmeldung
Sanktion(6)
Ma?nahme(1407)
delay anfechten revidieren zuru?ckstellen
fu?llen verku?nden quittieren vertagen
verschieben aufheben respektieren
verzo?gern(78)
aufhalten(712)
Kosten hallmark trouser blouse makup uniform
armour robe testimony witness jumper
cost(285)
o?ffnen unlock lock usher step peer shut guard
hurry slam close
open(12)
undo(481)
Table 2: Some examples of test words, their pre-
dicted translations, and the ranks of their true
translations.
predicted translations in Table 2.
Most of the 10 top-ranked candidate transla-
tions of sanction are hyponyms of the correct
translations. This is mainly due to insufficient
noun compound analysis. Both the English and
German nouns in our graph model are single
words. Whereas the English nouns consist only of
head nouns, the German nouns include many com-
pounds (as they are written without spaces), and
thus tend to be more specific.
Some of the top candidate translations of de-
lay are correct (verschieben) or at least acceptable
(vertagen), but do not count as such as they are
missing in the gold standard dictionary.
The mistranslation of the German noun Kosten
is due to semantic ambiguity. Kosten co-occurs of-
ten with the verb tragen as in to bear costs. The
verb tragen however is ambiguous and may as
well be translated as to wear which is strongly as-
sociated with clothes.
We find several antonyms of o?ffnen among its
top predicted translations. Verb-object relation-
ships alone do not suffice to distinguish synonyms
from antonyms. Similarly, it is extremely difficult
to differentiate between the members of closed
categories (e.g. the days of the week, months of
the year, mass and time units) using only syntactic
relationships.
7 Conclusions and Future Research
The matrix formulation of the SimRank algorithm
given in this paper allows an implementation using
efficient off-the-shelf software libraries for matrix
computation.
We presented an extension of the SimRank
algorithm to edge-weighted and edge-labeled
graphs. We further generalized the SimRank equa-
tions to permit the comparison of nodes from two
different graphs, and proposed an application to
bilingual lexicon induction.
Our system is not yet accurate enough to be
used for actual compilation of translation dictio-
naries. We further need to address the problem of
data sparsity. In particular, we need to remove the
bias towards low-degree words whose similarities
to other words are unduly high.
In order to solve the problem of ambiguity, we
intend to apply SimRank to the incidence repre-
sentation of the word graphs, which is constructed
by putting a node on each link. The proposed al-
gorithm will then naturally return similarities be-
tween the more sense-discriminative links (syn-
tactic relationships) in addition to similarities be-
tween the often ambiguous nodes (isolated words).
References
M. Diab and S. Finch. 2000. A statistical word-
level translation model for comparable corpora. In
In Proceedings of the Conference on Content-Based
Multimedia Information Access (RIAO).
G. Jeh and J. Widom. 2002. Simrank: A measure of
structural-context similarity. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 538?543.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of the ACL-02 Workshop on Unsupervised Lexical
Acquisition, pages 9?16.
P. Gamallo Otero and J. Ramon Pichel Campos. 2005.
An approach to acquire word translations from non-
parallel texts. In EPIA, pages 600?610.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 519?526.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING ?04: Proceedings of the 20th International
Conference on Computational Linguistics, page 162.
Sabine Schulte im Walde. 1998. Automatic Se-
mantic Classification of Verbs According to Their
Alternation Behaviour. Master?s thesis, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
95
A Graph Model for Unsupervised Lexical Acquisition
Dominic Widdows and Beate Dorow
Center for the Study of Language and Information
210 Panama Street
Stanford University
Stanford CA 94305-4115
{dwiddows,beate}@csli.stanford.edu
Abstract
This paper presents an unsupervised method for
assembling semantic knowledge from a part-of-
speech tagged corpus using graph algorithms.
The graph model is built by linking pairs of
words which participate in particular syntactic
relationships. We focus on the symmetric rela-
tionship between pairs of nouns which occur to-
gether in lists. An incremental cluster-building
algorithm using this part of the graph achieves
82% accuracy at a lexical acquisition task, eval-
uated against WordNet classes. The model nat-
urally realises domain and corpus specific am-
biguities as distinct components in the graph
surrounding an ambiguous word.
1 Introduction
Semantic knowledge for particular domains is
increasingly important in NLP. Many applica-
tions such as Word-Sense Disambiguation, In-
formation Extraction and Speech Recognition
all require lexicons. The coverage of hand-
built lexical resources such as WordNet (Fell-
baum, 1998) has increased dramatically in re-
cent years, but leaves several problems and
challenges. Coverage is poor in many criti-
cal, rapidly changing domains such as current
affairs, medicine and technology, where much
time is still spent by human experts employed
to recognise and classify new terms. Most
languages remain poorly covered in compari-
son with English. Hand-built lexical resources
which cannot be automatically updated can of-
ten be simply misleading. For example, using
WordNet to recognise that the word apple refers
to a fruit or a tree is a grave error in the many
situations where this word refers to a computer
manufacturer, a sense which WordNet does not
cover. For NLP to reach a wider class of appli-
cations in practice, the ability to assemble and
update appropriate semantic knowledge auto-
matically will be vital.
This paper describes a method for arranging
semantic information into a graph (Bolloba?s,
1998), where the nodes are words and the edges
(also called links) represent relationships be-
tween words. The paper is arranged as follows.
Section 2 reviews previous work on semantic
similarity and lexical acquisition. Section 3 de-
scribes how the graph model was built from the
PoS-tagged British National Corpus. Section 4
describes a new incremental algorithm used to
build categories of words step by step from the
graph model. Section 5 demonstrates this algo-
rithm in action and evaluates the results against
WordNet classes, obtaining state-of-the-art re-
sults. Section 6 describes how the graph model
can be used to recognise when words are poly-
semous and to obtain groups of words represen-
tative of the different senses.
2 Previous Work
Most work on automatic lexical acquisition has
been based at some point on the notion of
semantic similarity. The underlying claim is
that words which are semantically similar occur
with similar distributions and in similar con-
texts (Miller and Charles, 1991).
The main results to date in the field of au-
tomatic lexical acquisition are concerned with
extracting lists of words reckoned to belong to-
gether in a particular category, such as vehicles
or weapons (Riloff and Shepherd, 1997) (Roark
and Charniak, 1998). Roark and Charniak de-
scribe a ?generic algorithm? for extracting such
lists of similar words using the notion of seman-
tic similarity, as follows (Roark and Charniak,
1998, ?1).
1. For a given category, choose a small
set of exemplars (or ?seed words?)
2. Count co-occurrence of words and
seed words within a corpus
3. Use a figure of merit based upon
these counts to select new seed words
4. Return to step 2 and iterate n times
5. Use a figure of merit to rank words
for category membership and output a
ranked list
Algorithms of this type were used by Riloff
and Shepherd (1997) and Roark and Charniak
(1998), reporting accuracies of 17% and 35%
respectively. Like the algorithm we present in
Section 5, the similarity measure (or ?figure of
merit?) used in these cases was based on co-
occurrence in lists.
Both of these works evaluated their results
by asking humans to judge whether items gen-
erated were appropriate members of the cate-
gories sought. Riloff and Shepherd (1997) also
give some credit for ?related words? (for example
crash might be regarded as being related to the
category vehicles).
One problem with these techniques is the
danger of ?infections? ? once any incorrect or
out-of-category word has been admitted, the
neighbours of this word are also likely to be ad-
mitted. In Section 4 we present an algorithm
which goes some way towards reducing such in-
fections.
The early results have been improved upon by
Riloff and Jones (1999), where a ?mutual boot-
strapping? approach is used to extract words in
particular semantic categories and expression
patterns for recognising relationships between
these words for the purposes of information ex-
traction. The accuracy achieved in this experi-
ment is sometimes as high as 78% and is there-
fore comparable to the results reported in this
paper.
Another way to obtain word-senses directly
from corpora is to use clustering algorithms
on feature-vectors (Lin, 1998; Schu?tze, 1998).
Clustering techniques can also be used to dis-
criminate between different senses of an ambigu-
ous word. A general problem for such cluster-
ing techniques lies in the question of how many
clusters one should have, i.e. how many senses
are appropriate for a particular word in a given
domain (Manning and Schu?tze, 1999, Ch 14).
Lin?s approach to this problem (Lin, 1998) is
to build a ?similarity tree? (using what is in ef-
fect a hierarchical clustering method) of words
related to a target word (in this case the word
duty). Different senses of duty can be discerned
as different sub-trees of this similarity tree. We
present a new method for word-sense discrimi-
nation in Section 6.
3 Building a Graph from a
PoS-tagged Corpus
In this section we describe how a graph ? a
collection of nodes and links ? was built to
represent the relationships between nouns. The
model was built using the British National Cor-
pus which is automatically tagged for parts of
speech.
Initially, grammatical relations between pairs
of words were extracted. The relationships ex-
tracted were the following:
? Noun (assumed to be subject) Verb
? Verb Noun (assumed to be object)
? Adjective Noun
? Noun Noun (often the first noun is modify-
ing the second)
? Noun and/or Noun
The last of these relationships often occurs
when the pair of nouns is part of a list. Since
lists are usually comprised of objects which are
similar in some way, these relationships have
been used to extract lists of nouns with similar
properties (Riloff and Shepherd, 1997) (Roark
and Charniak, 1998). In this paper we too fo-
cus on nouns co-occurring in lists. This is be-
cause the noun and/or noun relationship is the
only symmetric relationship in our model, and
symmetric relationships are much easier to ma-
nipulate than asymmetric ones. Our full graph
contains many directed links between words of
different parts of speech. Initial experiments
with this model show considerable promise but
are at too early a stage to be reported upon yet.
Thus the graph used in most of this paper repre-
sents only nouns. Each node represents a noun
and two nodes have a link between them if they
co-occur separated by the conjunctions and or
or, and each link is weighted according to the
number of times the co-occurrence is observed.
Various cutoff functions were used to deter-
mine how many times a relationship must be
observed to be counted as a link in the graph.
A well-behaved option was to take the top n
neighbours of each word, where n could be de-
termined by the user. In this way the link-
weighting scheme was reduced to a link-ranking
scheme. One consequence of this decision was
that links to more common words were preferred
over links to rarer words. This decision may
have effectively boosted precision at the expense
of recall, because the preferred links are to fairly
common and (probably) more stable words. Re-
search is need to reveal theoretically motivated
or experimentally optimal techniques for select-
ing the importance to assign to each link ? the
choices made in this area so far are often of an
ad hoc nature.
The graph used in the experiments described
has 99,454 nodes (nouns) and 587,475 links.
There were roughly 400,000 different types
tagged as nouns in the corpus, so the graph
model represents about one quarter of these
nouns, including most of the more common
ones.
4 An Incremental Algorithm for
Extracting Categories of Similar
Words
In this section we describe a new algorithm for
adding the ?most similar node? to an existing
collection of nodes in a way which incremen-
tally builds a stable cluster. We rely entirely
upon the graph to deduce the relative impor-
tance of relationships. In particular, our algo-
rithm is designed to reduce so-called ?infections?
(Roark and Charniak, 1998, ?3) where the inclu-
sion of an out-of-category word which happens
to co-occur with one of the category words can
significantly distort the final list.
Here is the process we use to select and add
the ?most similar node? to a set of nodes:
Definition 1 Let A be a set of nodes and
let N(A), the neighbours of A, be the nodes
which are linked to any a ? A. (So N(A) =
?
a?AN(a).)
The best new node is taken to be the node
b ? N(A)\A with the highest proportion of links
to N(A). More precisely, for each u ? N(A)\A,
let the affinity between u and A be given by the
ratio
|N(u) ?N(A)|
|N(u)| .
The best new node b ? N(A) \ A is the node
which maximises this affinity score.
This algorithm has been built into an on-line
demonstration where the user inputs a given
seed word and can then see the cluster of re-
lated words being gradually assembled.
The algorithm is particularly effective at
avoiding infections arising from spurious co-
occurrences and from ambiguity. Consider, for
example, the graph built around the word ap-
ple in Figure 6. Suppose that we start with the
seed-list apple, orange, banana. However many
times the string ?Apple and Novell? occurs in
the corpus, the novell node will not be added
to this list because it doesn?t have a link to or-
ange, banana or any of their neighbours except
for apple. One way to summarise the effect of
this decision is that the algorithm adds words
to clusters depending on type frequency rather
than token frequency. This avoids spurious links
due to (for example) particular idioms rather
than geniune semantic similarity.
5 Examples and Evaluation
In this section we give examples of lexical cat-
egories extracted by our method and evaluate
them against the corresponding classes in Word-
Net.
5.1 Methodology
Our methodology is as follows. Consider an
intuitive category of objects such as musical
instruments. Define the ?WordNet class? or
?WordNet category? of musical instruments to
be the collection of synsets subsumed in Word-
Net by the musical instruments synset. Take a
?protypical example? of a musical instrument,
such as piano. The algorithm defined in (1)
gives a way of finding the n nodes deemed to be
most closely related to the piano node. These
can then be checked to see if they are mem-
bers of the WordNet class of musical instru-
ments. This method is easier to implement and
less open to variation than human judgements.
While WordNet or any other lexical resource is
not a perfect arbiter, it is hoped that this exper-
iment procedure is both reliable and repeatable.
The ten classes of words chosen were crimes,
places, tools, vehicles, musical instruments,
clothes, diseases, body parts, academic subjects
and foodstuffs. The classes were chosen before
the experiment was carried out so that the re-
sults could not be massaged to only use those
classes which gave good results. (The first 4 cat-
egories are also used by (Riloff and Shepherd,
1997) and (Roark and Charniak, 1998) and so
were included for comparison.) Having chosen
these classes, 20 words were retrieved using a
single seed-word chosen from the class in ques-
tion.
This list of words clearly depends on the seed
word chosen. While we have tried to optimise
this choice, it depends on the corpus and the
the model. The influence of semantic Proto-
type Theory (Rosch, 1988) is apparent in this
process, a link we would like to investigate in
more detail. It is possible to choose an optimal
seed word for a particular category: it should be
possible to compare these optimal seed words
with the ?prototypes? suggested by psychologi-
cal experiments (Mervis and Rosch, 1981).
5.2 Results
The results for a list of ten classes and proto-
typical words are given in Table 1. Words which
are correct members of the classes sought are
in Roman type: incorrect results are in ital-
ics. The decision between correctness and in-
correctness was made on a strict basis for the
sake of objectivity and to enable the repeata-
bility of the experiment: words which are in
WordNet were counted as correct results only if
they are actual members of the WordNet class
in question. Thus brigandage is not regarded
as a crime even though it is clearly an act of
wrongdoing, orchestra is not regarded as a mu-
sical instrument because it is a collection of in-
struments rather than a single instrument, etc.
The only exceptions we have made are the terms
wynd and planetology (marked in bold), which
are not in WordNet but are correct nonethe-
less. These conditions are at least as stringent
as those of previous experiments, particularly
those of Riloff and Shepherd (1997) who also
give credit for words associated with but not
belonging to a particular category. (It has been
pointed out that many polysemous words may
occur in several classes, making the task easier
because for many words there are several classes
which our algorithm would give credit for.)
With these conditions, our algorithm re-
trieves only 36 incorrect terms out of a total
of 200, giving an accuracy of 82%.
5.3 Analysis
Our results are an order of magnitude better
than those reported by Riloff and Shepherd
(1997) and Roark and Charniak (1998), who
report average accuracies of 17% and 35% re-
spectively. (Our results are also slightly better
than those reported by Riloff and Jones (1999)).
Since the algorithms used are in many ways
very similar, this improvement demands expla-
nation.
Some of the difference in accuracy can be at-
tributed to the corpora used. The experiments
in (Riloff and Shepherd, 1997) were performed
on the 500,000 word MUC-4 corpus, and those
of (Roark and Charniak, 1998) were performed
using MUC-4 and the Wall Street Journal cor-
pus (some 30 million words). Our model was
built using the British National Corpus (100
million words). On the other hand, our model
was built using only a part-of-speech tagged cor-
pus. The high accuracy achieved thus questions
the conclusion drawn by Roark and Charniak
(1998) that ?parsing is invaluable?. Our results
clearly indicate that a large PoS-tagged corpus
may be much better for automatic lexical ac-
quisition than a small fully-parsed corpus. This
claim could of course be tested by comparing
techniques on the same corpus.
To evaluate the advantage of using PoS infor-
mation, we compared the graph model with a
similarity thesaurus generated using Latent Se-
mantic Indexing (Manning and Schu?tze, 1999,
Ch 15), a ?bag-of-words? approach, on the same
corpus. The same number of nouns was re-
trieved for each class using the graph model
and LSI. The LSI similarity thesaurus obtained
an accuracy of 31%, much less than the graph
model?s 82%. This is because LSI retrieves
words which are related by context but are not
in the same class: for example, the neighbours
of piano found using LSI cosine-similarity on the
BNC corpus include words such as composer,
music, Bach, concerto and dance, which are re-
lated but certainly not in the same semantic
class.
The incremental clustering algorithm of Def-
inition (1) works well at preventing ?infections?
Class Seed Word Neighbours Produced by Graph Model
crimes murder crime theft arson importuning incest fraud larceny parricide
burglary vandalism indecency violence offences abuse brig-
andage manslaughter pillage rape robbery assault lewdness
places park path village lane viewfield church square road avenue garden
castle wynd garage house chapel drive crescent home place
cathedral street
tools screwdriver chisel naville nail shoulder knife drill matchstick morgenthau
gizmo hand knee elbow mallet penknife gallie leg arm sickle
bolster hammer
vehicle
conveyance
train tram car driver passengers coach lorry truck aeroplane coons
plane trailer boat taxi pedestrians vans vehicles jeep bus buses
helicopter
musical
instruments
piano fortepiano orchestra marimba clarsach violin cizek viola oboe
flute horn bassoon culbone mandolin clarinet equiluz contra-
bass saxophone guitar cello
clothes shirt chapeaubras cardigan trousers breeches skirt jeans boots pair
shoes blouse dress hat waistcoat jumper sweater coat cravat
tie leggings
diseases typhoid malaria aids polio cancer disease atelectasis illnesses cholera
hiv deaths diphtheria infections hepatitis tuberculosis cirrho-
sis diptheria bronchitis pneumonia measles dysentery
body parts stomach head hips thighs neck shoulders chest back eyes toes breasts
knees feet face belly buttocks haws ankles waist legs
academic
subjects
physics astrophysics philosophy humanities art religion science pol-
itics astronomy sociology chemistry history theology eco-
nomics literature maths anthropology culture mathematics
geography planetology
foodstuffs cake macaroons confectioneries cream rolls sandwiches croissant
buns scones cheese biscuit drinks pastries tea danish butter
lemonade bread chocolate coffee milk
Table 1: Classes of similar words given by the graph model.
and keeping clusters within one particular class.
The notable exception is the tools class, where
the word hand appears to introduce infection.
In conclusion, it is clear that the graph model
combined with the incremental clustering algo-
rithm of Definition 1 performs better than most
previous methods at the task of automatic lex-
ical acquisition.
6 Recognising Polysemy
So far we have presented a graph model built
upon noun co-occurrence which performs much
better than previously reported methods at the
task of automatic lexical acquisition. This is
an important task, because assembling and tun-
ing lexicons for specific NLP systems is increas-
ingly necessary. We now take a step further
and present a simple method for not only as-
sembling words with similar meanings, but for
empirically recognising when a word has several
meanings.
Recognising and resolving ambiguity is
an important task in semantic processing.
The traditional Word Sense Disambiguation
(WSD) problem addresses only the ambiguity-
resolution part of the problem: compiling a suit-
able list of polysemous words and their possible
senses is a task for which humans are tradition-
ally needed (Kilgarriff and Rosenzweig, 2000).
This makes traditional WSD an intensively su-
pervised and costly process. Breadth of cover-
age does not in itself solve this problem: general
lexical resources such as WordNet can provide
too many senses many of which are rarely used
in particular domains or corpora (Gale et al,
1992).
The graph model presented in this paper sug-
gests a new method for recognising relevant pol-
ysemy. We will need a small amount of termi-
nology from graph theory (Bolloba?s, 1998).
Definition 2 (Bolloba?s, 1998, Ch 1 ?1)
Let G = (V,E) be a graph, where V is the set
of vertices (nodes) of G and E ? V ? V is the
set of edges of G.
? Two nodes v1, vn are said to be connected
if there exists a path {v1, v2, . . . , vn?1, vn}
such that (vj , vj+1) ? E for 1 ? j < n.
? Connectedness is an equivalence relation.
? The equivalence classes of the graph G un-
der this relation are called the components
of G.
We are now in a position to define the senses
of a word as represented by a particular graph.
Definition 3 Let G be a graph of words closely
related to a seed-word w, and let G \ w be the
subgraph which results from the removal of the
seed-node w.
The connected components of the subgraph
G \ w are the senses of the word w with respect
to the graph G.
As an illustrative example, consider the local
graph generated for the word apple (6). The re-
moval of the apple node results in three separate
components which represent the different senses
of apple: fruit, trees, and computers. Definition
3 gives an extremely good model of the senses
of apple found in the BNC. (In this case better
than WordNet which does not contain the very
common corporate meaning.)
The intuitive notion of ambiguity being pre-
sented is as follows. An ambiguous word often
connects otherwise unrelated areas of meaning.
Definition 3 recognises the ambiguity of apple
because this word is linked to both banana and
novell, words which otherwise have nothing to
do with one another.
It is well-known that any graph can be
thought of as a collection of feature-vectors, for
example by taking the row-vectors in the adja-
cency matrix (Bolloba?s, 1998, Ch 2 ?3). There
might therefore be fundamental similarities be-
tween our approach and methods which rely on
similarities between feature-vectors.
Extra motivation for this technique is pro-
vided by Word-Sense Disambiguation. The
standard method for this task is to use hand-
labelled data to train a learning algorithm,
which will often pick out particular words as
Bayesian classifiers which indicate one sense or
the other. (So if microsoft occurs in the same
sentence as apple we might take this as evidence
that apple is being used in the corporate sense.)
Clearly, the words in the different components
in Diagram 6 can potentially be used as classi-
fiers for just this purpose, obviating the need for
time-consuming human annotation. This tech-
nique will be assessed and evaluated in future
experiments.
Demonstration
An online version of the graph model and the in-
cremental clustering algorithm described in this
paper are publicly available 1 for demonstration
purposes and to allow users to observe the gen-
erality of our techniques. A sample output is
included in Figure 6.
Acknowledgements
The authors would like to thank the anonymous
reviewers whose comments were a great help in
making this paper more focussed: any short-
comings remain entirely our own responsibility.
This research was supported in part by the
Research Collaboration between the NTT Com-
munication Science Laboratories, Nippon Tele-
graph and Telephone Corporation and CSLI,
Stanford University, and by EC/NSF grant IST-
1999-11438 for the MUCHMORE project. 2
1http://infomap.stanford.edu/graphs
2http://muchmore.dfki.de
Figure 1: Automatically generated graph show-
ing the word apple and semantically related
nouns
References
Be?la Bolloba?s. 1998. Modern Graph Theory.
Number 184 in Graduate texts in Mathemat-
ics. Springer-Verlag.
Christiane Fellbaum. 1998. WordNet: An elec-
tronic lexical database. MIT press, Cam-
bridge MA.
W. Gale, K. Church, and D. Yarowsky. 1992.
One sense per discourse. In DARPA speech
and Natural Language Workshop, Harriman,
NY.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
English senseval: report and results. In
LREC, Athens.
Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In COLING-
ACL, Montreal, August.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cam-
bridge, Massachusetts.
C. Mervis and E. Rosch. 1981. Categorization
of natural objects. Annual Review of Psychol-
ogy, 32:89?115.
George A. Miller and William G. Charles. 1991.
Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1?28.
Ellen Riloff and Rosie Jones. 1999. Learn-
ing dictionaries for infomation extraction by
multi-level bootstrapping. In Proceedings of
the Sixteenth National Conference on Artifi-
cial Intelligence, pages 472?479. AAAI.
Ellen Riloff and Jessica Shepherd. 1997. A
corpus-based approach for building seman-
tic lexicons. In Claire Cardie and Ralph
Weischedel, editors, Proceedings of the Second
Conference on Empirical Methods in Natural
Language Processing, pages 117?124. Associ-
ation for Computational Linguistics, Somer-
set, New Jersey.
Brian Roark and Eugene Charniak. 1998.
Noun-phrase co-occurence statistics for semi-
automatic semantic lexicon construction. In
COLING-ACL, pages 1110?1116.
E. Rosch. 1988. Principles of categorization. In
A. Collins and E. E. Smith, editors, Read-
ings in Cognitive Science: A Perspective from
Psychology and Artificial Intelligence, pages
312?322. Kaufmann, San Mateo, CA.
Hinrich Schu?tze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97?124.
Coling 2010: Poster Volume, pages 614?622,
Beijing, August 2010
A Linguistically Grounded Graph Model for Bilingual Lexicon
Extraction
Florian Laws, Lukas Michelbacher, Beate Dorow, Christian Scheible,
Ulrich Heid, Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn,michells,dorowbe}@ims.uni-stuttgart.de
Abstract
We present a new method, based on
graph theory, for bilingual lexicon ex-
traction without relying on resources with
limited availability like parallel corpora.
The graphs we use represent linguis-
tic relations between words such as ad-
jectival modification. We experiment
with a number of ways of combining
different linguistic relations and present
a novel method, multi-edge extraction
(MEE), that is both modular and scalable.
We evaluate MEE on adjectives, verbs
and nouns and show that it is superior
to cooccurrence-based extraction (which
does not use linguistic analysis). Finally,
we publish a reproducible baseline to es-
tablish an evaluation benchmark for bilin-
gual lexicon extraction.
1 Introduction
Machine-readable translation dictionaries are an
important resource for bilingual tasks like ma-
chine translation and cross-language information
retrieval. A common approach to obtaining bilin-
gual translation dictionaries is bilingual lexicon
extraction from corpora. Most work has used
parallel text for this task. However, parallel cor-
pora are only available for few language pairs and
for a small selection of domains (e.g., politics).
For other language pairs and domains, monolin-
gual comparable corpora and monolingual lan-
guage processing tools may be more easily avail-
able. This has prompted researchers to investigate
bilingual lexicon extraction based on monolingual
corpora (see Section 2) .
In this paper, we present a new graph-theoretic
method for bilingual lexicon extraction. Two
monolingual graphs are constructed based on syn-
tactic analysis, with words as nodes and relations
(such as adjectival modification) as edges. Each
relation acts as a similarity source for the node
types involved. All available similarity sources
interact to produce one final similarity value for
each pair of nodes. Using a seed lexicon, nodes
from the two graphs can be compared to find a
translation.
Our main contributions in this paper are: (i) we
present a new method, based on graph theory,
for bilingual lexicon extraction without relying
on resources with limited availability like paral-
lel corpora; (ii) we show that with this graph-
theoretic framework, information obtained by lin-
guistic analysis is superior to cooccurrence data
obtained without linguistic analysis; (iii) we ex-
periment with a number of ways of combining dif-
ferent linguistic relations in extraction and present
a novel method, multi-edge extraction, which is
both modular and scalable; (iv) progress in bilin-
gual lexicon extraction has been hampered by the
lack of a common benchmark; we therefore pub-
lish a benchmark and the performance of MEE as
a baseline for future research.
The paper discusses related work in Section 2.
We then describe our translation model (Sec-
tion 3) and multi-edge extraction (Section 4). The
benchmark we publish as part of this paper is de-
scribed in Section 5. Section 6 presents our ex-
perimental results and Section 7 analyzes and dis-
cusses them. Section 8 summarizes.
2 Related Work
Rapp (1999) uses word cooccurrence in a vector
space model for bilingual lexicon extraction. De-
tails are given in Section 5.
Fung and Yee (1998) also use a vector space
approach, but use TF/IDF values in the vector
components and experiment with different vec-
tor similarity measures for ranking the translation
candidates. Koehn and Knight (2002) combine
614
a vector-space approach with other clues such as
orthographic similarity and frequency. They re-
port an accuracy of .39 on the 1000 most frequent
English-German noun translation pairs.
Garera et al (2009) use a vector space model
with dependency links as dimensions instead of
cooccurring words. They report outperforming
a cooccurrence vector model by 16 percentage
points accuracy on English-Spanish.
Haghighi et al (2008) use a probabilistic model
over word feature vectors containing cooccur-
rence and orthographic features. They then use
canonical correlation analysis to find matchings
between words in a common latent space. They
evaluate on multiple languages and report high
precision even without a seed lexicon.
Most previous work has used vector spaces and
(except for Garera et al (2009)) cooccurrence
data. Our approach uses linguistic relations like
subcategorization, modification and coordination
in a graph-based model. Further, we evaluate our
approach on different parts of speech, whereas
some previous work only evaluates on nouns.
3 Translation Model
Our model has two components: (i) a graph repre-
senting words and the relationships between them
and (ii) a measure of similarity between words
based on these relationships. Translation is re-
garded as cross-lingual word similarity. We rank
words according to their similarity and choose the
top word as the translation.
We employ undirected graphs with typed nodes
and edges. Node types represent parts of speech
(POS); edge types represent different kinds of re-
lations. We use a modified version of SimRank
(Jeh and Widom, 2002) as a similarity measure
for our experiments (see Section 4 for details).
SimRank is based on the idea that two nodes
are similar if their neighbors are similar. We ap-
ply this notion of similarity across two graphs. We
think of two words as translations if they appear
in the same relations with other words that are
translations of each other. Figure 1 illustrates this
idea with verbs and nouns in the direct object rela-
tion. Double lines indicate seed translations, i.e.,
known translations from a dictionary (see Sec-
tion 5). The nodes buy and kaufen have the same
house
magazine
book
thought
buy
read
Haus
Zeitschrift
Buch
Gedanke
kaufen
lesen
Figure 1: Similarity through seed translations
objects in the two languages; one of these (maga-
zine ? Zeitschrift) is a seed translation. This re-
lationship contributes to the similarity of buy ?
kaufen. Furthermore, book and Buch are similar
(because of read ? lesen) and this similarity will
be added to buy ? kaufen in a later iteration. By
repeatedly applying the algorithm, the initial sim-
ilarity introduced by seeds spreads to all nodes.
To incorporate more detailed linguistic infor-
mation, we introduce typed edges in addition to
typed nodes. Each edge type represents a linguis-
tic relation such as verb subcategorization or ad-
jectival modification. By designing a model that
combines multiple edge types, we can compute
the similarity between two words based on mul-
tiple sources of similarity. We superimpose dif-
ferent sets of edges on a fixed set of nodes; a node
is not necessarily part of every relation.
The graph model can accommodate any kind of
nodes and relations. In this paper we use nodes
to represent content words (i.e., non-function
words): adjectives (a), nouns (n) and verbs (v).
We extracted three types of syntactic relations
from a corpus: see Table 1.
Nouns participate in two bipartite relations
(amod, dobj) and one unipartite relation (ncrd).
This means that the computation of noun similar-
ities will benefit from three different sources.
Figure 2 depicts a sample graph with all node
and edge types. For the sake of simplicity, a
monolingual example is shown. There are four
nouns in the sample graph all of which are (i)
modified by the adjectives interesting and polit-
ical and (ii) direct objects of the verbs like and
615
relation entities description example
used in this paper
amod a, n adjectival modification a fast car
dobj v, n object subcategorization drive a car
ncrd n, n noun coordination cars and busses
other possible relations
vsub v, n subject subcategorization a man sleeps
poss n, n possessive the child?s toy
acrd a, a adjective coordination red or blue car
Table 1: Relations used in this paper (top) and
possible extensions (bottom).
dobj
amod
ncrd
verb
adjective
noun
like promote
idea
article book
magazine
interesting political
Figure 2: Graph snippet with typed edges
promote. Based on amod and dobj, the four nouns
are equally similar to each other. However, the
greater similarity of article, book, and magazine
to each other can be deduced from the fact that
these three nouns also occur in the relation ncrd.
We exploit this information in the MEE method.
Data and Preprocessing. Our corpus in this
paper is the Wikipedia. We parse all German
and English articles with BitPar (Schmid, 2004)
to extract verb-argument relations. We extract
adjective-noun modification and noun coordina-
tions with part-of-speech patterns based on a
version of the corpus tagged with TreeTagger
(Schmid, 1994). We use lemmas instead of sur-
face forms. Because we perform the SimRank
matrix multiplications in memory, we need to fil-
ter out rare words and relations; otherwise, run-
ning SimRank to convergence would not be feasi-
ble. For adjective-noun pairs, we apply a filter on
pair frequency (? 3). We process noun pairs by
applying a frequency threshold on words (? 100)
and pairs (? 3). Verb-object pairs (the smallest
data set) were not frequency-filtered. Based on
the resulting frequency counts, we calculate asso-
ciation scores for all relationships using the log-
likelihood measure (Dunning, 1993). For noun
pairs, we discard all pairs with an association
score < 3.84 (significance at ? = .05). For all
three relations, we discard pairs whose observed
frequency was smaller than their expected fre-
quency (Evert, 2004, p. 76). As a last step,
we further reduce noise by removing nodes of de-
gree 1. Key statistics for the resulting graphs are
given in Table 2.
We have found that accuracy of extraction is
poor if unweighted edges are used. Using the
log-likelihood score directly as edge weight gives
too much weight to ?semantically weak? high-
frequency words like put and take. We there-
fore use the logarithms of the log-likelihood score
as edge weights in all SimRank computations re-
ported in this paper.
nodes n a v
de 34,545 10,067 2,828
en 22,257 12,878 4,866
edges ncrd amod dobj
de 65,299 417,151 143,906
en 288,889 686,073 510,351
Table 2: Node and edge statistics
4 SimRank
Our work is based on the SimRank graph similar-
ity algorithm (Jeh and Widom, 2002). In (Dorow
et al, 2009), we proposed a formulation of Sim-
Rank in terms of matrix operations, which can be
applied to (i) weighted graphs and (ii) bilingual
problems. We now briefly review SimRank and
its bilingual extension. For more details we refer
to (Dorow et al, 2009).
The basic idea of SimRank is to consider two
nodes as similar if they have similar neighbor-
hoods. Node similarity scores are recursively
computed from the scores of neighboring nodes:
the similarity Sij of two nodes i and j is computed
616
as the normalized sum of the pairwise similarities
of their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl.
where N(i) and N(j) are the sets of i?s and j?s
neighbors. As the basis of the recursion, Sij is set
to 1 if i and j are identical (self-similarity). The
constant c (0 < c < 1) dampens the contribution
of nodes further away. Following Jeh and Widom
(2002), we use c = 0.8. This calculation is re-
peated until, after a few iterations, the similarity
values converge.
For bilingual problems, we adapt SimRank for
comparison of nodes across two graphs A and B.
In this case, i is a node in A and j is a node in B,
and the recursion basis is changed to S(i, j) = 1 if
i and j are a pair in a predefined set of node-node
equivalences (seed translation pairs).
Sij =
c
|NA(i)| |NB(j)|
?
k?NA(i),l?NB(j)
Skl.
Multi-edge Extraction (MEE) Algorithm To
combine different information sources, corre-
sponding to edges of different types, in one Sim-
Rank computation, we use multi-edge extrac-
tion (MEE), a variant of SimRank (Dorow et al,
2009). It computes an aggregate similarity matrix
after each iteration by taking the average similar-
ity value over all edge types T :
Sij =
c
|T |
?
t?T
1
f(|NA,t(i)|)f(|NB,t(j)|)
?
k?NA,t(i),
l?NB,t(j)
Skl.
f is a normalization function (either f = g,
g(n) = n as before or the normalization discussed
in the next section).
While we have only reviewed the case of un-
weighted graphs, the extended SimRank can also
be applied to weighted graphs. (See (Dorow et
al., 2009) for details.) In what follows, all graph
computations are weighted.
Square Root Normalization Preliminary ex-
periments showed that SimRank gave too much
influence to words with few neighbors. We there-
fore modified the normalization function g(n) =
n. To favor words with more neighbors, we want
f to grow sublinearly with the number of neigh-
bors. On the other hand, it is important that,
even for nodes with a large number of neigh-
bors, the normalization term is not much smaller
than |N(i)|, otherwise the similarity computation
does not converge. We use the function h(n) =?n?
?
maxk(|N(k)|). h grows quickly for small
node degrees, while returning values close to the
linear term for large node degrees. This guaran-
tees that nodes with small degrees have less influ-
ence on final similarity scores. In all experiments
reported in this paper, the matrices A?, B? are nor-
malized with f = h (rather than using the stan-
dard normalization f = g). In one experiment,
accuracy of the top-ranked candidate (acc@1) was
.52 for h and .03 for g, demonstrating that the
standard normalization does not work in our ap-
plication.
Threshold Sieving For larger experiments,
there is a limit to scalability, as the similarity ma-
trix fills up with many small entries, which take up
a large amount of memory. Since these small en-
tries contribute little to the final result, Lizorkin et
al. (2008) proposed threshold sieving: an approxi-
mation of SimRank using less space by deleting
all similarity values that are below a threshold.
The quality of the approximation is set by a pa-
rameter ? that specifies maximum acceptable dif-
ference of threshold-sieved similarity and the ex-
act solution. We adapted this to the matrix formu-
lation by integrating the thresholding step into a
standard sparse matrix multiplication algorithm.
We verified that this approximation yields use-
ful results by comparing the ranks of exact and ap-
proximate solutions. We found that for the high-
ranked words that are of interest in our task, siev-
ing with a suitable threshold does not negatively
affect results.
5 Benchmark Data Set
Rapp?s (1999) original experiment was carried out
on newswire corpora and a proprietary Collins
dictionary. We use the free German (280M to-
kens) and English (850M tokens) Wikipedias as
source and target corpora. Reinhard Rapp has
generously provided us with his 100 word test set
617
n a v
training set .61 .31 .08
TS100 .65 .28 .07
TS1000 .66 .14 .20
Table 3: Percentages of POS in test and training
(TS100) and given us permission to redistribute
it. Additionally, we constructed a larger test set
(TS1000) consisting of the 1000 most frequent
words from the English Wikipedia. Unlike the
noun-only test sets used in other studies, (e.g.,
Koehn and Knight (2002), Haghighi et al (2008)),
TS1000 also contains adjectives and verbs. As
seed translations, we use a subset of the dict.cc
online dictionary. For the creation of the sub-
set we took raw word frequencies from Wikipedia
as a basis. We extracted all verb, noun and ad-
jective translation pairs from the original dictio-
nary and kept the pairs whose components were
among the 5,000 most frequent nouns, the 3,500
most frequent adjectives and the 500 most fre-
quent verbs for each language. These numbers are
based on percentages of the different node types
in the graphs. The resulting dictionary contains
12,630 pairs: 7,767 noun, 3,913 adjective and 950
verb pairs. Table 3 shows the POS composition of
the training set and the two test sets. For experi-
ments evaluated on TS100 (resp. TS1000), the set
of 100 (resp. 1000) English words it contains and
all their German translations are removed from the
seed dictionary.
Baseline. Our baseline is a reimplementation
of the vector-space method of Rapp (1999). Each
word in the source corpus is represented as a word
vector, the dimensions of which are words of seed
translation pairs. The same is done for corpus
words in the target language, using the translated
seed words as dimensions. The value of each di-
mension is determined by association statistics of
word cooccurrence. For a test word, a vector is
constructed in the same way. The labels on the
dimensions are then translated, yielding an input
vector in the target language vector space. We
then find the closest corpus word vector in the tar-
get language vector space using the city block dis-
tance measure. This word is taken as the transla-
tion of the test word.
We went to great lengths to implement Rapp?s
method, but omit the details for reasons of space.
Using the Wikipedia/dict.cc-based data set, we
achieve 50% acc@1 when translating words from
English to German. While this is somewhat lower
than the performance reported by Rapp, we be-
lieve this is due to Wikipedia being more hetero-
geneous and less comparable than news corpora
from identical time periods used by Rapp.
Publication. In conjunction with this paper we
publish the benchmark for bilingual lexicon ex-
traction described. It consists of (i) two Wikipedia
dumps from October 2008 and the linguistic re-
lations extracted from them, (ii) scripts to recre-
ate the training and test sets from the dict.cc
data base, (iii) the TS100 and TS1000 test sets,
and (iv) performance numbers of Rapp?s system
and MEE. These can serve as baselines for fu-
ture work. Note that (ii)?(iv) can be used in-
dependently of (i) ? but in that case the effect
of the corpus on performance would not be con-
trolled. The data and scripts are available at
http://ifnlp.org/wiki/extern/WordGraph
6 Results
In addition to the vector space baseline experi-
ment described above, we conducted experiments
with the SimRank model. Because TS100 only
contains one translation per word, but words can
have more than one valid translation, we manu-
ally extended the test set with other translations,
which we verified using dict.cc and leo.org. We
report the results separately for the original test set
(?strict?) and the extended test set in Table 4. We
also experimented with single-edge models con-
sisting of three separate runs on each relation.
The accuracy columns report the percentage of
test cases where the correct translation was found
among the top 1 (acc@1) or top 10 (acc@10)
candidate words found by the translation mod-
els. Some test words are not present in the data at
all; we count these as 0s when computing acc@1
and acc@10. The acc@10 measure is more use-
ful for indicating topical similarity while acc@1
measures translation accuracy.
MRR is Mean Reciprocal Rank of correct trans-
lations: 1n
?n
i
1
ranki (Voorhees and Tice, 1999).
MRR is a more fine-grained measure than acc@n,
618
TS100, strict TS100, extended TS1000
acc@1 acc@10 MRR acc@1 acc@10 MRR acc@1 acc@10 MRR
baseline .50 .67 .56 .54 .70 .60 .33 .56 .41
single .44 .67 .52 .49 .68 .56 .40? .70? .50
MEE .52 .79? .62 .58 .82? .68 .48? .76? .58
Table 4: Results compared to baseline?
e.g., it will distinguish ranks 2 and 10. All MRR
numbers reported in this paper are consistent with
acc@1/acc@10 and support our conclusions.
The results for acc@1, the measure that most
directly corresponds to utility in lexicon extrac-
tion, show that the SimRank-based models out-
perform the vector space baseline ? only slightly
on TS100, but significantly on TS1000. Using the
various relations separately (single) already yields
a significant improvement compared to the base-
line. Using all relations in the integrated MEE
model further improves accuracy. With an acc@1
score of 0.48, MEE outperforms the baseline by
.15 compared to TS1000. This shows that a com-
bination of several sources of information is very
valuable for finding the correct translation.
MEE outperforms the baseline on TS1000 for
all parts of speech, but performs especially well
compared to the baseline for adjectives and verbs
(see Table 5). It has been suggested that vector
space models perform best for nouns and poorly
for other parts of speech. Our experiments seem to
confirm this. In contrast, MEE exhibits good per-
formance for nouns and adjectives and a marked
improvement for verbs.
On acc@10, MEE is consistently better than the
baseline, on both TS100 and TS1000. All three
differences are statistically significant.
6.1 Relation Comparison
Table 5 compares baseline, single-edge and MEE
accuracy for the three parts of speech covered.
Each single-edge experiment can compute noun
similarity; for adjectives and verbs, only amod,
dobj and MEE can be used.
Performance for nouns varies greatly depend-
ing on the relation used in the model. ncrd per-
?We indicate statistical significance at the ? = 0.05 (?)
and 0.01 level (?) when compared to the baseline. We did
not calculate significance for MRR.
forms best, while dobj shows the worst perfor-
mance. We hypothesize that dobj performs badly
because (i) many verbs are semantically non-
restrictive with respect to their arguments, (e.g.,
use, contain or include) and as a result seman-
tically unrelated nouns become similar because
they share the same verb as a neighbor; (ii) light
verb constructions (e.g., take a walk or give an ac-
count) dilute the extracted relations; and (iii) dobj
is the only relation we extracted with a syntac-
tic parser. The parser was trained on newswire
text, a genre that is very different from Wikipedia.
Hence, parsing is less robust than the relatively
straightforward POS patterns used for the other
relations.
Similarly, many semantically non-restrictive
adjectives such as first and new can modify vir-
tually any noun, diluting the quality of the amod
source. We conjecture that ncrd exhibits the best
performance because there are fewer semantically
non-restrictive nouns than non-restrictive adjec-
tives and verbs.
MEE performance for nouns (.45) is signifi-
cantly better than that of the single-edge models.
The information about nouns that is contained in
the verb-object and adjective-noun data is inte-
grated in the model and helps select better trans-
lations. This, however, is only true for the noun
noun adj verb all
TS100 baseline .55 .43 .29 .50
amod .15 .71 - .30
ncrd .34 - - .22
dobj .02 - .43 .04
MEE .45 .71 .43 .52
TS1000 baseline .42 .26 .18 .33
MEE .53 .55 .27 .48
Table 5: Relation comparison, acc@1
619
source acc@1 acc@10
dobj .02 .10
amod .15 .37
amod+dobj .22 .43
ncrd+dobj .32 .65
ncrd .34 .60
ncrd+amod .49 .74
MEE .45 .77
Table 6: Accuracy of sources for nouns
node type, the ?pivot? node type that takes part in
edges of all three types. For adjectives and verbs,
the performance of MEE is the same as that of the
corresponding single-edge model.
We ran three additional experiments each of
which combines only two of the three possible
sources for noun similarity, namely ncrd+amod,
ncrd+dobj and amod+dobj and performed strict
evaluation (see Table 6). We found that in gen-
eral combination increases performance except
for ncrd+dobj vs. ncrd. We attribute this to the
lack of robustness of dobj mentioned above.
6.2 Comparison MEE vs. All-in-one
An alternative to MEE is to use untyped edges in
one large graph. In this all-in-one model (AIO),
we connect two nodes with an edge if they are
linked by any of the different linguistic relations.
While MEE consists of small adjacency matrices
for each type, the two adjacency matrices for AIO
are much larger. This leads to a much denser sim-
ilarity matrix taking up considerably more mem-
ory. One reason for this is that AIO contains simi-
larity entries between words of different parts of
speech that are 0 (and require no memory in a
sparse matrix representation) in MEE.
Since AIO requires more memory, we had to
filter the data much more strictly than before to be
able to run an experiment. We applied the follow-
ing stricter thresholds on relationships to obtain
a small graph: 5 instead of 3 for adjective-noun
MEEsmall AIOsmall
acc@1 .51 .52
acc@10 .72 .75
MRR .62 .59
Table 7: MEE vs. AIO
pairs, and 3 instead of 0 for verb-object pairs,
thereby reducing the total number of edges from
2.1M to 1.4M. We also applied threshold sieving
(see Section 4) with ? = 10?10 for AIO. The re-
sults on TS100 (strict evaluation) are reported in
Table 7. For comparison, MEE was also run on
the smaller graph. Performance of the two models
is very similar, with AIO being slightly better (not
significant). The slight improvement does not jus-
tify the increased memory requirements. MEE is
able to scale to more nodes and edge types, which
allows for better coverage and performance.
7 Analysis and Discussion
Error analysis. We examined the cases where a
reference translation was not at the top of the sug-
gested list of translation candidates. There are a
number of elements in the translation process that
can cause or contribute to this behavior.
Our method sometimes picks a cohyponym of
the correct translation. In many of these cases, the
correct translation is in the top 10 (together with
other words from the same semantic field). For
example, the correct translation of moon, Mond, is
second in a list of words belonging to the semantic
field of celestial phenomena: Komet (comet), Mond
(moon), Planet (planet), Asteroid (asteroid), Stern (star),
Galaxis (galaxy), Sonne (sun), . . . While this behavior
is undesirable for strict lexicon extraction, it can
be exploited for other tasks, e.g. cross-lingual se-
mantic relatedness (Michelbacher et al, 2010).
Similarly, the method sometimes puts the
antonym of the correct translation in first place.
For example, the translation for swift (schnell) is
in second place behind langsam (slow). Based
on the syntactic relations we use, it is difficult to
discriminate between antonyms and semantically
similar words if their syntactic distributions are
similar.
Ambiguous source words also pose a problem
for the system. The correct translation of square
(the geometric shape) is Quadrat. However, 8 out
of its top 10 translation candidates are related to
the location sense of square. The other two are ge-
ometric shapes, Quadrat being listed second. This
is only a concern for strict evaluation, since cor-
rect translations of a different sense were included
in the extended test set.
620
bed is also ambiguous (piece of furniture vs.
river bed). This introduces translation candidates
from the geographical domain. As an additional
source of errors, a number of bed?s neighbors
from the furniture sense have the German transla-
tion Bank which is ambiguous between the furni-
ture sense and the financial sense. This ambiguity
in the target language German introduces spurious
translation candidates from the financial domain.
Discussion. The error analysis demonstrates
that most of the erroneous translations are words
that are incorrect, but that are related, in some ob-
vious way, to the correct translation, e.g. by co-
hyponymy or antonymy. This suggests another
application for bilingual lexicon extraction. One
of the main challenges facing statistical machine
translation (SMT) today is that it is difficult to
distinguish between minor errors (e.g., incorrect
word order) and major errors that are completely
implausible and undermine the users? confidence
in the machine translation system. For example,
at some point Google translated ?sarkozy sarkozy
sarkozy? into ?Blair defends Bush?. Since bilin-
gual lexicon extraction, when it makes mistakes,
extracts closely related words that a human user
can understand, automatically extracted lexicons
could be used to discriminate smaller errors from
grave errors in SMT.
As we discussed earlier, parallel text is not
available in sufficient quantity or for all impor-
tant genres for many language pairs. The method
we have described here can be used in such cases,
provided that large monolingual corpora and ba-
sic linguistic processing tools (e.g. POS tagging)
are available. The availability of parsers is a more
stringent constraint, but our results suggest that
more basic NLP methods may be sufficient for
bilingual lexicon extraction.
In this work, we have used a set of seed trans-
lations (unlike e.g., Haghighi et al (2008)). We
believe that in most real-world scenarios, when
accuracy and reliability are important, seed lexica
will be available. In fact, seed translations can be
easily found for many language pairs on the web.
Although a purely unsupervised approach is per-
haps more interesting from an algorithmic point
of view, the semisupervised approach taken in this
paper may be more realistic for applications.
In this paper, we have attempted to reimplement
Rapp?s system as a baseline, but have otherwise
refrained from detailed comparison with previous
work as far as the accuracy of results is concerned.
The reason is that none of the results published so
far are easily reproducible. While previous publi-
cations have tried to infer from differences in per-
formance numbers that one system is better than
another, these comparisons have to be viewed with
caution since neither the corpora nor the gold stan-
dard translations are the same. For example, the
paper by Haghighi et al (2008) (which demon-
strates how orthography and contextual informa-
tion can be successfully used) reports 61.7% ac-
curacy on the 186 most confident predictions of
nouns. But since the evaluation data sets are not
publicly available it is difficult to compare other
work (including our own) with this baseline. We
simply do not know how methods published so far
stack up against each other.
For this reason, we believe that a benchmark
is necessary to make progress in the area of bilin-
gual lexicon extraction; and that our publication of
such a benchmark as part of the research reported
here is an important contribution, in addition to
the linguistically grounded extraction and the new
graph-theoretical method we present.
8 Summary
We have presented a new method, based on graph
theory, for bilingual lexicon extraction without re-
lying on resources with limited availability like
parallel corpora. We have shown that with this
graph-theoretic framework, information obtained
by linguistic analysis is superior to cooccurrence
data obtained without linguistic analysis. We have
presented multi-edge extraction (MEE), a scalable
graph algorithm that combines different linguis-
tic relations in a modular way. Finally, progress
in bilingual lexicon extraction has been hampered
by the lack of a common benchmark. We publish
such a benchmark with this paper and the perfor-
mance of MEE as a baseline for future research.
9 Acknowledgement
This research was funded by the German Re-
search Foundation (DFG) within the project A
graph-theoretic approach to lexicon acquisition.
621
References
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In EACL 2009 Workshop on Geo-
metrical Models of Natural Language Semantics.
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Evert, Stefan. 2004. The Statistics of Word Cooccur-
rences - Word Pairs and Collocations. Ph.D. thesis,
Institut fu?r maschinelle Sprachverarbeitung (IMS),
Universita?t Stuttgart.
Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In COLING-ACL, pages 414?
420.
Garera, Nikesh, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon
induction from monolingual corpora via depen-
dency contexts and part-of-speech equivalences. In
CoNLL ?09: Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 129?137, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In KDD
?02, pages 538?543.
Koehn, Philipp and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9?16.
Lizorkin, Dmitry, Pavel Velikhov, Maxim N. Grinev,
and Denis Turdakov. 2008. Accuracy estimate and
optimization techniques for simrank computation.
PVLDB, 1(1):422?433.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, may.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In COLING 1999.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Schmid, Helmut. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In COLING ?04, page 162.
Voorhees, Ellen M. and Dawn M. Tice. 1999. The
TREC-8 question answering track evaluation. In
Proceedings of the 8th Text Retrieval Conference.
622
