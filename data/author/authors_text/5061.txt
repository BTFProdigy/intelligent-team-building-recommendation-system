Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 779?786, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Hiero Machine Translation System:
Extensions, Evaluation, and Analysis
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin
Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742, USA
{dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu
Abstract
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has been largely absent from the best
performing machine translation systems in recent
community-wide evaluations. In this paper, we dis-
cuss a new hierarchical phrase-based statistical ma-
chine translation system (Chiang, 2005), present-
ing recent extensions to the original proposal, new
evaluation results in a community-wide evaluation,
and a novel technique for fine-grained comparative
analysis of MT systems.
1 Introduction
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has, for the last several years, been
absent from the best performing machine transla-
tion systems in community-wide evaluations. Statis-
tical phrase-based models (e.g. (Och and Ney, 2004;
Koehn et al, 2003; Marcu andWong, 2002)) charac-
terize a source sentence f as a flat partition of non-
overlapping subsequences, or ?phrases?, f?1 ? ? ? f?J ,
and the process of translation involves selecting tar-
get phrases e?i corresponding to the f? j and modify-
ing their sequential order. The need for some way
to model aspects of syntactic behavior, such as the
tendency of constituents to move together as a unit,
is widely recognized?the role of syntactic units is
well attested in recent systematic studies of trans-
lation (Fox, 2002; Hwa et al, 2002; Koehn and
Knight, 2003), and their absence in phrase-based
models is quite evident when looking at MT system
output. Nonetheless, attempts to incorporate richer
linguistic features have generally met with little suc-
cess (Och et al, 2004a).
Chiang (2005) introduces Hiero, a hierarchical
phrase-based model for statistical machine transla-
tion. Hiero extends the standard, non-hierarchical
notion of ?phrases? to include nonterminal sym-
bols, which permits it to capture both word-level and
phrase-level reorderings within the same framework.
The model has the formal structure of a synchronous
CFG, but it does not make any commitment to a
linguistically relevant analysis, and it does not re-
quire syntactically annotated training data. Chiang
(2005) reported significant performance improve-
ments in Chinese-English translation as compared
with Pharaoh, a state-of-the-art phrase-based system
(Koehn, 2004).
In Section 2, we review the essential elements
of Hiero. In Section 3 we describe extensions to
this system, including new features involving named
entities and numbers and support for a fourfold
scale-up in training set size. Section 4 presents new
evaluation results for Chinese-English as well as
Arabic-English translation, obtained in the context
of the 2005 NISTMT Eval exercise. In Section 5, we
introduce a novel technique for fine-grained com-
parative analysis of MT systems, which we em-
ploy in analyzing differences between Hiero?s and
Pharaoh?s translations.
2 Hiero
Hiero is a stochastic synchronous CFG, whose pro-
ductions are extracted automatically from unanno-
tated parallel texts, and whose rule probabilities
form a log-linear model learned by minimum-error-
rate training; together with a modified CKY beam-
search decoder (similar to that of Wu (1996)). We
describe these components in brief below.
779
S ? ?S 1 X 2 ,S 1 X 2 ?
S ? ?X 1 ,X 1 ?
X ? ?yu X 1 you X 2 , have X 2 with X 1 ?
X ? ?X 1 de X 2 , the X 2 that X 1 ?
X ? ?X 1 zhiyi, one of X 1 ?
X ? ?Aozhou,Australia?
X ? ?shi, is?
X ? ?shaoshu guojia, few countries?
X ? ?bangjiao, diplomatic relations?
X ? ?Bei Han,North Korea?
Figure 1: Example synchronous CFG
2.1 Grammar
A synchronous CFG or syntax-directed transduction
grammar (Lewis and Stearns, 1968) consists of pairs
of CFG rules with aligned nonterminal symbols. We
denote this alignment by coindexation with boxed
numbers (Figure 1). A derivation starts with a pair
of aligned start symbols, and proceeds by rewrit-
ing pairs of aligned nonterminal symbols using the
paired rules (Figure 2).
Training begins with phrase pairs, obtained as by
Och, Koehn, and others: GIZA++ (Och and Ney,
2000) is used to obtain one-to-many word align-
ments in both directions, which are combined into a
single set of refined alignments using the ?final-and?
method of Koehn et al (2003); then those pairs of
substrings that are exclusively aligned to each other
are extracted as phrase pairs.
Then, synchronous CFG rules are constructed
out of the initial phrase pairs by subtraction: ev-
ery phrase pair ? f? , e?? becomes a rule X ? ? f? , e??,
and a phrase pair ? f? , e?? can be subtracted from a
rule X ? ??1 f??2, ?1e??2? to form a new rule X ?
??1X i ?2, ?1X i ?2?, where i is an index not already
used. Various filters are also applied to reduce the
number of extracted rules. Since one of these filters
restricts the number of nonterminal symbols to two,
our extracted grammar is equivalent to an inversion
transduction grammar (Wu, 1997).
2.2 Model
The model is a log-linear model (Och and Ney,
2002) over synchronous CFG derivations. The
weight of a derivation is PLM(e)?LM , the weighted
language model probability, multiplied by the prod-
uct of the weights of the rules used in the derivation.
The weight of each rule is, in turn:
(1) w(X ? ??, ??) =
?
i
?i(X ? ??, ??)?i
where the ?i are features defined on rules. The ba-
sic model uses the following features, analogous to
Pharaoh?s default feature set:
? P(? | ?) and P(? | ?)
? the lexical weights Pw(? | ?) and Pw(? | ?)
(Koehn et al, 2003);1
? a phrase penalty exp(1);
? a word penalty exp(l), where l is the number of
terminals in ?.
The exceptions to the above are the two ?glue?
rules, which are the rules with left-hand side S in
Figure 1. The second has weight one, and the first
has weight w(S ? ?S 1 X 2 ,S 1 X 2 ?) = exp(??g),
the idea being that parameter ?g controls the model?s
preference for hierarchical phrases over serial com-
bination of phrases.
Phrase translation probabilities are estimated by
relative-frequency estimation. Since the extraction
process does not generate a unique derivation for
each training sentence pair, a distribution over pos-
sible derivations is hypothesized, which gives uni-
form weight to all initial phrases extracted from a
sentence pair and uniform weight to all rules formed
out of an initial phrase. This distribution is then used
to estimate the phrase translation probabilities.
The lexical-weighting features are estimated us-
ing a method similar to that of Koehn et al (2003).
The language model is a trigram model with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998), trained using the SRI-LM toolkit (Stolcke,
2002).
1This feature uses word alignment information, which is dis-
carded in the final grammar. If a rule occurs in training with
more than one possible word alignment, Koehn et al take the
maximum lexical weight; Hiero uses a weighted average.
780
?S 1 ,S 1 ? ? ?S 2 X 3 ,S 2 X 3 ?
? ?S 4 X 5 X 3 ,S 4 X 5 X 3 ?
? ?X 6 X 5 X 3 ,X 6 X 5 X 3 ?
? ?Aozhou X 5 X 3 ,Australia X 5 X 3 ?
? ?Aozhou shi X 3 ,Australia is X 3 ?
? ?Aozhou shi X 7 zhiyi,Australia is one of X 7 ?
? ?Aozhou shi X 8 de X 9 zhiyi,Australia is one of the X 9 that X 8 ?
? ?Aozhou shi yu X 1 you X 2 de X 9 zhiyi,Australia is one of the X 9 that have X 2 with X 1 ?
Figure 2: Example partial derivation of a synchronous CFG.
The feature weights are learned by maximizing
the BLEU score (Papineni et al, 2002) on held-out
data, using minimum-error-rate training (Och, 2003)
as implemented by Koehn. The implementation was
slightly modified to ensure that the BLEU scoring
matches NIST?s definition and that hypotheses in
the n-best lists are merged when they have the same
translation and the same feature vector.
3 Extensions
In this section we describe our extensions to the base
Hiero system that improve its performance signif-
icantly. First, we describe the addition of two new
features to the Chinese model, in a manner similar
to that of Och et al (2004b); then we describe how
we scaled the system up to a much larger training
set.
3.1 New features
The LDC Chinese-English named entity lists (900k
entries) are a potentially valuable resource, but
previous experiments have suggested that simply
adding them to the training data does not help
(Vogel et al, 2003). Instead, we placed them in
a supplementary phrase-translation table, giving
greater weight to phrases that occurred less fre-
quently in the primary training data. For each en-
try ? f , {e1, . . . , en}?, we counted the number of times
c( f ) that f appeared in the primary training data,
and assigned the entry the weight 1c( f )+1 , which
was then distributed evenly among the supplemen-
tary phrase pairs {? f , ei?}. We then created a new
model feature for named entities. When one of these
supplementary phrase pairs was used in transla-
tion, its feature value for the named-entity feature
was the weight defined above, and its value in the
other phrase-translation and lexical-weighting fea-
tures was zero. Since these scores belonged to a sep-
arate feature from the primary translation probabili-
ties, they could be reweighted independently during
minimum-error-rate training.
Similarly, to process Chinese numbers and dates,
we wrote a rule-based Chinese number/date transla-
tor, and created a new model feature for it. Again,
the weight given to this module was optimized
during minimum-error-rate training. In some cases
we wrote the rules to provide multiple uniformly-
weighted English translations for a Chinese phrase
(for example,k? (bari) could become ?the 8th? or
?on the 8th?), allowing the language model to decide
between the options.
3.2 Scaling up training
Chiang (2005) reports on experiments in Chinese-
English translation using a model trained on
7.2M+9.2M words of parallel data.2 For the NIST
MT Eval 2005 large training condition, consider-
ably more data than this is allowable. We chose
to use only newswire data, plus data from Sino-
rama, a Taiwanese news magazine.3 This amounts
to almost 30M+30M words. Scaling to this set re-
quired reducing the initial limit on phrase lengths,
previously fixed at 10, to avoid explosive growth of
2Here and below, the notation ?X + Y words? denotes X
words of foreign text and Y words of English text.
3From Sinorama, only data from 1991 and later were used,
as articles prior to that were translated quite loosely.
781
the extracted grammar. However, since longer initial
phrases can be beneficial for translation accuracy,
we adopted a variable length limit: 10 for the FBIS
corpus and other mainland newswire sources, and 7
for the HK News corpus and Sinorama. (During de-
coding, limits of up to 15 were sometimes used; in
principle these limits should all be the same, but in
practice it is preferable to tune them separately.)
For Arabic-English translation, we used the ba-
sic Hiero model, without special features for named
entities or numbers/dates. We again used only the
newswire portions of the allowable training data; we
also excluded the Ummah data, as the translations
were found to be quite loose. Since this amounted
to only about 1.5M+1.5M words, we used a higher
initial phrase limit of 15 during both training and de-
coding.
4 Evaluation
Figure 1 shows the performance of several systems
on NIST MT Eval 2003 Chinese test data: Pharaoh
(2004 version), trained only on the FBIS data; Hi-
ero, with various combinations of the new features
and the larger training data.4 This table also shows
Hiero?s performance on the NIST 2005 MT evalua-
tion task.5 The metric here is case-sensitive BLEU.6
Figure 2 shows the performance of two systems
on Arabic in the NIST 2005 MT Evaluation task:
DC, a phrase-based decoder for a model trained by
Pharaoh, and Hiero.
5 Analysis
Over the last few years, several automatic metrics
for machine translation evaluation have been intro-
duced, largely to reduce the human cost of itera-
tive system evaluation during the development cy-
cle (Lin and Och, 2004; Melamed et al, 2003; Pap-
ineni et al, 2002). All are predicated on the concept
4The third line, corresponding to the model without new fea-
tures trained on the larger data, may be slightly depressed be-
cause the feature weights from the fourth line were used instead
of doing minimum-error-rate training specially for this model.
5Full results are available at http://www.nist.gov/
speech/tests/summaries/2005/mt05.htm. For this test, a
phrase length limit of 15 was used during decoding.
6For this task, the translation output was uppercased using
the SRI-LM toolkit: essentially, it was decoded again using
an HMM whose states and transitions are a trigram language
model of cased English, and whose emission probabilities are
reversed, i.e., probability of cased word given lowercased word.
System Features Train MT03 MT05
Pharaoh standard FBIS 0.268
Hiero standard FBIS 0.288
Hiero standard full 0.329
Hiero +nums, names full 0.339 0.300
Table 1: Chinese results. (BLEU-4; MT03 case-
insensitive, MT05 case-sensitive)
System Train MT05
DC full 0.399
Hiero full 0.450
Table 2: Arabic results. (BLEU-4; MT03 case-
insensitive, MT05 scores case-sensitive.
of n-gram matching between the sentence hypothe-
sized by the translation system and one or more ref-
erence translations?that is, human translations for
the test sentence. Although the motivations and for-
mulae underlying these metrics are all different, ul-
timately they all produce a single number represent-
ing the ?goodness? of the MT system output over a
set of reference documents. This facility is valuable
in determining whether a given system modification
has a positive impact on overall translation perfor-
mance. However, the metrics are all holistic. They
provide no insight into the specific competencies or
weaknesses of one system relative to another.
Ideally, we would like to use automatic methods
to provide immediate diagnostic information about
the translation output?what the system does well,
and what it does poorly. At the most general level,
we want to know how our system performs on the
two most basic problems in translation?word trans-
lation and reordering. Unigram precision and recall
statistics tell us something about the performance of
an MT system?s internal translation dictionaries, but
nothing about reordering. It is thought that higher or-
der n-grams correlate with the reordering accuracy
of MT systems, but this is again a holistic metric.
What we would really like to know is howwell the
system is able to capture systematic reordering pat-
terns in the input, which ones it is successful with,
and which ones it has difficulty with. Word n-grams
are little help here: they are too many, too sparse, and
it is difficult to discern general patterns from them.
782
5.1 A New Analysis Method
In developing a new analysis method, we are moti-
vated in part by recent studies suggesting that word
reorderings follow general patterns with respect to
syntax, although there remains a high degree of flex-
ibility (Fox, 2002; Hwa et al, 2002). This suggests
that in a comparative analysis of two MT systems, it
may be useful to look for syntactic patterns that one
system captures well in the target language and the
other does not, using a syntax based metric.
We propose to summarize reordering patterns us-
ing part-of-speech sequences. Unfortunately, recent
work has shown that applying statistical parsers to
ungrammatical MT output is unreliable at best, with
the parser often assigning unreasonable probabili-
ties and incongruent structure (Yamada and Knight,
2002; Och et al, 2004a). Anticipating that this
would be equally problematic for part-of-speech
tagging, we make the conservative choice to apply
annotation only to the reference corpus. Word n-
gram correspondences with a reference translation
are used to infer the part-of-speech tags for words in
the system output.
First, we tagged the reference corpus with parts
of speech. We used MXPOST (Ratnaparkhi, 1996),
and in order to discover more general patterns, we
map the tag set down after tagging, e.g. NN, NNP,
NNPS and NNS all map to NN. Second, we com-
puted the frequency freq(ti . . . t j) of every possible
tag sequence ti . . . t j in the reference corpus. Third,
we computed the correspondence between each hy-
pothesis sentence and each of its corresponding ref-
erence sentences using an approximation to max-
imum matching (Melamed et al, 2003). This al-
gorithm provides a list of runs or contiguous se-
quences of words ei . . . e j in the reference that are
also present in the hypothesis. (Note that runs are
order-sensitive.) Fourth, for each recalled n-gram
ei . . . e j, we looked up the associated tag sequence
ti . . . t j and incremented a counter recalled(ti . . . t j).
Finally, we computed the recall of tag patterns,
R(ti . . . t j) = recalled(ti . . . t j)/freq(ti . . . t j), for all
patterns in the corpus.
By examining examples of these tag sequences in
the reference corpus and their hypothesized trans-
lations, we expect to gain some insight into the
comparative strengths and weaknesses of the MT
systems? reordering models. (An interactive plat-
form for this analysis is demonstrated by Lopez and
Resnik (2005).)
5.2 Chinese
We performed tag sequence analysis on the Hiero
and Pharaoh systems trained on the FBIS data only.
Table 3 shows those n-grams for which Hiero and
Pharaoh?s recall differed significantly (p < 0.01).
The numbers shown are the ratio of Hiero?s recall
to Pharaoh?s. Note that the n-grams on which Hi-
ero had better recall are dominated by fragments of
prepositional phrases (in the Penn Treebank tagset,
prepositions are tagged IN or TO).
Our hypothesis is that Hiero produces English PPs
better because many of them are translated from
Chinese phrases which have an NP modifying an NP
to its right, often connected with the particle? (de).
These are often translated into English as PPs, which
modify the NP to the left. A correct translation, then,
would have to reorder the two NPs. Notice in the ta-
ble that Hiero recalls proportionally more n-grams
as n increases, corroborating the intuition that Hiero
should be better at longer-distance reorderings.
Investigating this hypothesis qualitatively, we in-
spected the first five occurrences of the n-grams of
the first type on the list (JJ NN IN DT NN). Of
these, we omit one example because both systems
recalled the n-gram correctly, and one because they
differed only in lexical choice (Hiero matched the
5-gram with one reference sentence, Pharaoh with
zero). The other three examples are shown below (H
= Hiero, P = Pharaoh):
(2) T?
UN
?h
security
?Proceedings of the Second Workshop on Statistical Machine Translation, pages 120?127,
Prague, June 2007. c?2007 Association for Computational Linguistics
Using Paraphrases for Parameter Tuning in Statistical Machine Translation
Nitin Madnani, Necip Fazil Ayan, Philip Resnik & Bonnie J. Dorr
Institute for Advanced Computer Studies
University of Maryland
College Park, MD, 20742
{nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu
Abstract
Most state-of-the-art statistical machine
translation systems use log-linear models,
which are defined in terms of hypothesis fea-
tures and weights for those features. It is
standard to tune the feature weights in or-
der to maximize a translation quality met-
ric, using held-out test sentences and their
corresponding reference translations. How-
ever, obtaining reference translations is ex-
pensive. In this paper, we introduce a new
full-sentence paraphrase technique, based
on English-to-English decoding with an MT
system, and we demonstrate that the result-
ing paraphrases can be used to drastically re-
duce the number of human reference transla-
tions needed for parameter tuning, without a
significant decrease in translation quality.
1 Introduction
Viewed at a very high level, statistical machine
translation involves four phases: language and trans-
lation model training, parameter tuning, decoding,
and evaluation (Lopez, 2007; Koehn et al, 2003).
Since their introduction in statistical MT by Och and
Ney (2002), log-linear models have been a standard
way to combine sub-models in MT systems. Typi-
cally such a model takes the form
?
i
?i?i(f? , e?) (1)
where ?i are features of the hypothesis e and ?i are
weights associated with those features.
Selecting appropriate weights ?i is essential
in order to obtain good translation performance.
Och (2003) introduced minimum error rate train-
ing (MERT), a technique for optimizing log-linear
model parameters relative to a measure of translation
quality. This has become much more standard than
optimizing the conditional probability of the train-
ing data given the model (i.e., a maximum likelihood
criterion), as was common previously. Och showed
that system performance is best when parameters are
optimized using the same objective function that will
be used for evaluation; BLEU (Papineni et al, 2002)
remains common for both purposes and is often re-
tained for parameter optimization even when alter-
native evaluation measures are used, e.g., (Banerjee
and Lavie, 2005; Snover et al, 2006).
Minimum error rate training?and more gener-
ally, optimization of parameters relative to a trans-
lation quality measure?relies on data sets in which
source language sentences are paired with (sets of)
reference translations. It is widely agreed that, at
least for the widely used BLEU criterion, which is
based on n-gram overlap between hypotheses and
reference translations, the criterion is most accu-
rate when computed with as many distinct reference
translations as possible. Intuitively this makes sense:
if there are alternative ways to phrase the meaning
of the source sentence in the target language, then
the translation quality criterion should take as many
of those variations into account as possible. To do
otherwise is to risk the possibility that the criterion
might judge good translations to be poor when they
fail to match the exact wording within the reference
translations that have been provided.
This reliance on multiple reference translations
creates a problem, because reference translations are
labor intensive and expensive to obtain. A com-
mon source of translated data for MT research is the
Linguistic Data Consortium (LDC), where an elab-
orate process is undertaken that involves translation
agencies, detailed translation guidelines, and qual-
ity control processes (Strassel et al, 2006). Some
120
efforts have been made to develop alternative pro-
cesses for eliciting translations, e.g., from users on
the Web (Oard, 2003) or from informants in low-
density languages (Probst et al, 2002). However,
reference translations for parameter tuning and eval-
uation remain a severe data bottleneck for such ap-
proaches.
Note, however, one crucial property of reference
translations: they are paraphrases, i.e., multiple ex-
pressions of the same meaning. Automatic tech-
niques exist for generating paraphrases. Although
one would clearly like to retain human transla-
tions as the benchmark for evaluation of translation,
might it be possible to usefully increase the number
of reference translations for tuning by using auto-
matic paraphrase techniques?
In this paper, we demonstrate that it is, in fact,
possible to do so. Section 2 briefly describes our
translation framework. Section 3 lays out a novel
technique for paraphrasing, designed with the ap-
plication to parameter tuning in mind. Section 4
presents evaluation results using a state of the art sta-
tistical MT system, demonstrating that half the hu-
man reference translations in a standard 4-reference
tuning set can be replaced with automatically gener-
ated paraphrases, with no significant decrease in MT
system performance. In Section 5 we discuss related
work, and in Section 6 we summarize the results and
discuss plans for future research.
2 Translation Framework
The work described in this paper makes use
of the Hiero statistical MT framework (Chiang,
2007). Hiero is formally based on a weighted syn-
chronous context-free grammar (CFG), containing
synchronous rules of the form
X ? ?e?, f? , ?k1(f? , e?, X)? (2)
where X is a symbol from the nonterminal alpha-
bet, and e? and f? can contain both words (terminals)
and variables (nonterminals) that serve as placehold-
ers for other phrases. In the context of statistical
MT, where phrase-based models are frequently used,
these synchronous rules can be interpreted as pairs
of hierarchical phrases. The underlying strength
of a hierarchical phrase is that it allows for effec-
tive learning of not only the lexical re-orderings, but
phrasal re-orderings, as well. Each ?(e?, f? , X) de-
notes a feature function defined on the pair of hierar-
chical phrases.1 Feature functions represent condi-
tional and joint co-occurrence probabilities over the
hierarchical paraphrase pair.
The Hiero framework includes methods to learn
grammars and feature values from unannotated par-
allel corpora, without requiring syntactic annotation
of the data. Briefly, training a Hiero model proceeds
as follows:
? GIZA++ (Och and Ney, 2000) is run on the
parallel corpus in both directions, followed by
an alignment refinement heuristic that yields a
many-to-many alignment for each parallel sen-
tence.
? Initial phrase pairs are identified following the
procedure typically employed in phrase based
systems (Koehn et al, 2003; Och and Ney,
2004).
? Grammar rules in the form of equation (2)
are induced by ?subtracting? out hierarchical
phrase pairs from these initial phrase pairs.
? Fractional counts are assigned to each pro-
duced rule:
c(X ? ?e?, f??) =
m?
j=1
1
njr
(3)
where m is the number of initial phrase pairs
that give rise to this grammar rule and njr is
the number of grammar rules produced by the
jth initial phrase pair.
? Feature functions ?k1(f? , e?, X) are calculated
for each rule using the accumulated counts.
Once training has taken place, minimum error rate
training (Och, 2003) is used to tune the parameters
?i.
Finally, decoding in Hiero takes place using a
CKY synchronous parser with beam search, aug-
mented to permit efficient incorporation of language
model scores (Chiang, 2007). Given a source lan-
guage sentence f, the decoder parses the source lan-
guage sentence using the grammar it has learned
1Currently only one nonterminal symbol is used in Hiero
productions.
121
during training, with parser search guided by the
model; a target-language hypothesis is generated
simultaneously via the synchronous rules, and the
yield of that hypothesized analysis represents the hy-
pothesized string e in the target language.
3 Generating Paraphrases
As discussed in Section 1, our goal is to make it pos-
sible to accomplish the parameter-tuning phase us-
ing fewer human reference translations. We accom-
plish this by beginning with a small set of human
reference translations for each sentence in the devel-
opment set, and expanding that set by automatically
paraphrasing each member of the set rather than by
acquiring more human translations.
Most previous work on paraphrase has focused
on high quality rather than coverage (Barzilay and
Lee, 2003; Quirk et al, 2004), but generating ar-
tificial references for MT parameter tuning in our
setting has two unique properties compared to other
paraphrase applications. First, we would like to ob-
tain 100% coverage, in order to avoid modifications
to our minimum error rate training infrastructure.2
Second, we prefer that paraphrases be as distinct as
possible from the original sentences, while retaining
as much of the original meaning as possible.
In order to satisfy these two properties, we ap-
proach sentence-level paraphrase for English as
a problem of English-to-English translation, con-
structing the model using English-F translation, for
a second language F , as a pivot. Following Ban-
nard and Callison-Burch (2005), we first identify
English-to-F correspondences, then map from En-
glish to English by following translation units from
English to F and back. Then, generalizing their ap-
proach, we use those mappings to create a well de-
fined English-to-English translation model. The pa-
rameters of this model are tuned using MERT, and
then the model is used in an the (unmodified) sta-
tistical MT system, yielding sentence-level English
paraphrases by means of decoding input English
sentences. The remainder of this section presents
this process in detail.
2Strictly speaking, this was not a requirement of the ap-
proach, but rather a concession to practical considerations.
3.1 Mapping and Backmapping
We employ the following strategy for the induction
of the required monolingual grammar. First, we train
the Hiero system in standard fashion on a bilingual
English-F training corpus. Then, for each exist-
ing production in the resulting Hiero grammar, we
create multiple new English-to-English productions
by pivoting on the foreign hierarchical phrase in the
rule. For example, assume that we have the follow-
ing toy grammar for English-F , as produced by Hi-
ero:
X ? ?e?1, f?1?
X ? ?e?3, f?1?
X ? ?e?1, f?2?
X ? ?e?2, f?2?
X ? ?e?4, f?2?
If we use the foreign phrase f?1 as a pivot and
backmap, we can extract the two English-to-English
rules: X ? ?e?1, e?3? and X ? ?e?3, e?1?. Backmap-
ping using both f?1 and f?2 produces the following
new rules (ignoring duplicates and rules that map
any English phrase to itself):
X ? ?e?1, e?2?
X ? ?e?1, e?3?
X ? ?e?1, e?4?
X ? ?e?2, e?1?
X ? ?e?2, e?4?
3.2 Feature values
Each rule production in a Hiero grammar is
weighted by several feature values defined on the
rule themselves. In order to perform accurate
backmapping, we must recompute these feature
functions for the newly created English-to-English
grammar. Rather than computing approximations
based on feature values already existing in the bilin-
gual Hiero grammar, we calculate these features
in a more principled manner, by computing max-
imum likelihood estimates directly from the frac-
tional counts that Hiero accumulates in the penul-
timate training step.
We use the following features in our induced
English-to-English grammar:3
3Hiero also uses lexical weights (Koehn et al, 2003) in both
122
? The joint probability of the two English hierar-
chical paraphrases, conditioned on the nonter-
minal symbol, as defined by this formula:
p(e?1, e?2|x) =
c(X ? ?e?1, e?2?)
?
e?1?,e?2? c(X ? ?e?1
?, e?2??)
=
c(X ? ?e?1, e?2?)
c(X)
(4)
where the numerator is the fractional count of
the rule under consideration and the denomina-
tor represents the marginal count over all the
English hierarchical phrase pairs.
? The conditionals p(e?1, x|e?2) and p(e?2, x|e?1)
defined as follows:
p(e?1, x|e?2) =
c(X ? ?e?1, e?2?)
?
e?1? c(X ? ?e?1
?, e?2?)
(5)
p(e?2, x|e?1) =
c(X ? ?e?1, e?2?)
?
e?2? c(X ? ?e?1, e?2
??)
(6)
Finally, for all induced rules, we calculate a word
penalty exp(?T (e?2)), where T (e?2) just counts the
number of terminal symbols in e?2. This feature al-
lows the model to learn whether it should produce
shorter or longer paraphrases.
In addition to the features above that are estimated
from the training data, we also use a trigram lan-
guage model. Since we are decoding to produce
English sentences, we can use the same language
model employed in a standard statistical MT setting.
Calculating the proposed features is complicated
by the fact that we don?t actually have the counts
for English-to-English rules because there is no
English-to-English parallel corpus. This is where
the counts provided by Hiero come into the picture.
We estimate the counts that we need as follows:
c(X ? ?e?1, e?2?) =
?
f?
c(X ? ?e?1, f??)c(X ? ?e?2, f??) (7)
An intuitive way to think about the formula above
is by using an example at the corpus level. As-
sume that, in the given bilingual parallel corpus,
there are m sentences in which the English phrase
directions as features but we don?t use them for our grammar.
e?1 co-occurs with the foreign phrase f? and n sen-
tences in which the same foreign phrase f? co-occurs
with the English phrase e?2. The problem can then
be thought of as defining a function g(m,n) which
computes the number of sentences in a hypotheti-
cal English-to-English parallel corpus wherein the
phrases e?1 and e?1 co-occur. For this paper, we de-
fine g(m,n) to be the upper bound mn.
Tables 1 and 2 show some examples of para-
phrases generated by our system across a range of
paraphrase quality for two different pivot languages.
3.3 Tuning Model Parameters
Although the goal of the paraphrasing approach
is to make it less data-intensive to tune log-linear
model parameters for translation, our paraphrasing
approach, since it is based on an English-to-English
log-linear model, also requires its own parameter
tuning. This, however, is straightforward: regard-
less of how the paraphrasing model will be used
in statistical MT, e.g., irrespective of source lan-
guage, it is possible to use any existing set of English
paraphrases as the tuning set for English-to-English
translation. We used the 2002 NIST MT evaluation
test set reference translations. For every item in the
set, we randomly chose one sentence as the source
sentence, and the remainder as the ?reference trans-
lations? for purposes of minimum error rate training.
4 Evaluation
Having developed a paraphrasing approach based on
English-to-English translation, we evaluated its use
in improving minimum error rate training for trans-
lation from a second language into English.
Generating paraphrases via English-to-English
translation makes use of a parallel corpus, from
which a weighted synchronous grammar is automat-
ically acquired. Although nothing about our ap-
proach requires that the paraphrase system?s training
bitext be the same one used in the translation exper-
iments (see Section 6), doing so is not precluded, ei-
ther, and it is a particularly convenient choice when
the paraphrasing is being done in support of MT.4
The training bitext comprised of Chinese-English
4The choice of the foreign language used as the pivot should
not really matter but it is worth exploring this using other lan-
guage pairs as our bitext.
123
O: we must bear in mind the community as a whole .
P: we must remember the wider community .
O: thirdly , the implications of enlargement for the union ?s regional policy cannot be overlooked .
P: finally , the impact of enlargement for eu regional policy cannot be ignored .
O: how this works in practice will become clear when the authority has to act .
P: how this operate in practice will emerge when the government has to play .
O: this is an ill-advised policy .
P: this is an unwelcome in europe .
Table 1: Example paraphrases with French as the pivot language. O = Original Sentence, P = Paraphrase.
O: alcatel added that the company?s whole year earnings would be announced on february 4 .
P: alcatel said that the company?s total annual revenues would be released on february 4 .
O: he was now preparing a speech concerning the us policy for the upcoming world economic forum .
P: he was now ready to talk with regard to the us policies for the forthcoming international economic forum .
O: tibet has entered an excellent phase of political stability, ethnic unity and people living in peace .
P: tibetans have come to cordial political stability, national unity and lived in harmony .
O: its ocean and blue-sky scenery and the mediterranean climate make it world?s famous scenic spot .
P: its harbour and blue-sky appearance and the border situation decided it world?s renowned tourist attraction .
Table 2: Example paraphrases with Chinese as the pivot language. O = Original Sentence, P = Paraphrase.
Corpus # Sentences # Words
HK News 542540 11171933
FBIS 240996 9121210
Xinhua 54022 1497562
News1 9916 314121
Treebank 3963 125848
Total 851437 22230674
Table 3: Chinese-English corpora used as training
bitext both for paraphrasing and for evaluation.
parallel corpora containing 850, 000 sentence pairs ?
approx. 22 million words (details shown in Table 3).
As the source of development data for minimum
error rate training, we used the 919 source sen-
tences and human reference translations from the
2003 NIST Chinese-English MT evaluation exer-
cise. As raw material for experimentation, we gen-
erated a paraphrase for each reference sentence via
1-best decoding using the English-to-English trans-
lation approach of Section 3.
As our test data, we used the 1082 source sen-
tences and human reference translations from the
2005 NIST Chinese-English MT evaluation.
Our core experiment involved three conditions
where the only difference was the set of references
for the development set used for tuning feature
weights. For each condition, once the weights were
tuned, they were used to decode the test set. Note
that for all the conditions, the decoded test set was
always scored against the same four high-quality hu-
man reference translations included with the set.
The three experimental conditions were designed
around the constraint that our development set con-
tains a total of four human reference translations per
sentence, and therefore a maximum of four human
references with which to compute an upper bound:
? Baseline (2H): For each item in the devel-
opment set, we randomly chose two of the
four human-constructed reference translations
as references for minimum error rate training.
? Expanded (2H + 2P): For each of the two hu-
man references in the baseline tuning set, we
automatically generated a corresponding para-
phrase using (1-best) English-to-English trans-
lation, decoding using the model developed in
Section 3. This condition represents the critical
case in which you have a limited number of hu-
124
man references (two, in this case) and augment
them with artificially generated reference trans-
lations. This yields a set of four references for
minimum error rate training (two human, two
paraphrased), which permits a direct compar-
ison against the upper bound of four human-
generated reference translations.
? Upper bound: 4H: We performed minimum
error rate training using the four human refer-
ences from the development set.
In addition to these core experimental conditions,
we added a fourth condition to assess the effect on
performance when all four human reference trans-
lations are used in expanding the reference set via
paraphrase:
? Expanded (4H + 4P): This is the same as Con-
dition 2, but using all four human references.
Note that since we have only four human references
per item, this fourth condition does not permit com-
parison with an upper bound of eight human refer-
ences.
Table 4 shows BLEU and TER scores on the test
set for all four conditions.5 If only two human ref-
erences were available (simulated by using only two
of the available four), expanding to four using para-
phrases would yield a clear improvement. Using
bootstrap resampling to compute confidence inter-
vals (Koehn, 2004), we find that the improvement in
BLEU score is statistically significant at p < .01.
Equally interesting, expanding the number of ref-
erence translations from two to four using para-
phrases yields performance that approaches the up-
per bound obtained by doing MERT using all four
human reference translations. The difference in
BLEU between conditions 2 and 3 is not significant.
Finally, our fourth condition asks whether it is
possible to improve MT performance given the
typical four human reference translations used for
MERT in most statistical MT systems, by adding a
paraphrase to each one for a total eight references
per translation. There is indeed further improve-
ment, although the difference in BLEU score does
not reach significance.
5We plan to include METEOR scores in future experiments.
Condition References used BLEU TER
1 2 H 30.43 59.82
2 2 H + 2 P 31.10 58.79
3 4 H 31.26 58.66
4 4 H + 4 P 31.68 58.24
Table 4: BLEU and TER scores showing utility of
paraphrased reference translations. H = human ref-
erences, P = paraphrased references.
We also evaluated our test set using TER (Snover
et al, 2006) and observed that the TER scores follow
the same trend as the BLEU scores. Specifically, the
TER scores demonstrate that using paraphrases to
artificially expand the reference set is better than us-
ing only 2 human reference translations and as good
as using 4 human reference translations.6
5 Related Work
The approach we have taken here arises from a typ-
ical situation in NLP systems: the lack of sufficient
data to accurately estimate a model based on super-
vised training data. In a structured prediction prob-
lem such as MT, we have an example input and a
single labeled, correct output. However, this output
is chosen from a space in which the number of pos-
sible outputs is exponential in the input size, and in
which there are many good outputs in this space (al-
though they are vastly outnumbered by the bad out-
puts). Various discriminative learning methods have
attempted to deal with the first of these issues, often
by restricting the space of examples. For instance,
some max-margin methods restrict their computa-
tions to a set of examples from a ?feasible set,?
where they are expected to be maximally discrim-
inative (Tillmann and Zhang, 2006). The present
approach deals with the second issue: in a learning
problem where the use of a single positive example
is likely to be highly biased, how can we produce a
set of positive examples that is more representative
of the space of correct outcomes? Our method ex-
ploits alternative sources of information to produce
new positive examples that are, we hope, reasonably
likely to represent a consensus of good examples.
Quite a bit of work has been done on paraphrase,
6We anticipate doing significance tests for differences in
TER in future work.
125
some clearly related to our technique, although in
general previous work has been focused on human
readability rather than high coverage, noisy para-
phrases for use downstream in an automatic process.
At the sentence level, (Barzilay and Lee, 2003)
employed an unsupervised learning approach to
cluster sentences and extract lattice pairs from
comparable monolingual corpora. Their technique
produces a paraphrase only if the input sentence
matches any of the extracted lattice pairs, leading to
a bias strongly favoring quality over coverage. They
were able to generate paraphrases for 59 sentences
(12%) out of a 484-sentence test set, generating no
paraphrases at all for the remainder.
Quirk et al (2004) also generate sentential para-
phrases using a monolingual corpus. They use
IBM Model-1 scores as the only feature, and em-
ploy a monotone decoder (i.e., one that cannot pro-
duce phrase-level reordering). This approach em-
phasizes very simple ?substitutions of words and
short phrases,? and, in fact, almost a third of their
best sentential ?paraphrases? are identical to the in-
put sentence.
A number of other approaches rely on parallel
monolingual data and, additionally, require pars-
ing of the training sentences (Ibrahim et al, 2003;
Pang et al, 2003). Lin and Pantel (2001) use a
non-parallel corpus and employ a dependency parser
and computation of distributional similarity to learn
paraphrases.
There has also been recent work on using para-
phrases to improve statistical machine translation.
Callison-Burch et al (2006) extract phrase-level
paraphrases by mapping input phrases into a phrase
table and then mapping back to the source language.
However, they do not generate paraphrases of entire
sentences, but instead employ paraphrases to add en-
tries to an existing phrase table solely for the pur-
pose of increasing source-language coverage.
Other work has incorporated paraphrases into MT
evaluation: Russo-Lassner et al (2005) use a com-
bination of paraphrase-based features to evaluate
translation output; Zhou et al (2006) propose a new
metric that extends n-gram matching to include syn-
onyms and paraphrases; and Lavie?s METEOR met-
ric (Banerjee and Lavie, 2005) can be used with ad-
ditional knowledge such as WordNet in order to sup-
port inexact lexical matches.
6 Conclusions and Future Work
We introduced an automatic paraphrasing technique
based on English-to-English translation of full sen-
tences using a statistical MT system, and demon-
strated that, using this technique, it is possible to
cut in half the usual number of reference transla-
tions used for minimum error rate training with no
significant loss in translation quality. Our method
enables the generation of paraphrases for thousands
of sentences in a very short amount of time (much
shorter than creating other low-cost human refer-
ences). This might prove beneficial for various dis-
criminative training methods (Tillmann and Zhang,
2006).
This has important implications for data acquisi-
tion strategies For example, it suggests that rather
than obtaining four reference translations per sen-
tence for development sets, it may be more worth-
while to obtain fewer translations for a wider range
of sentences, e.g., expanding into new topics and
genres. In addition, this approach can significantly
increase the utility of datasets which include only a
single reference translation.
A number of future research directions are pos-
sible. First, since we have already demonstrated
that noisy paraphrases can nonetheless add value,
it would be straightforward to explore the quan-
tity/quality tradeoff by expanding the MERT refer-
ence translations with n-best paraphrases for n > 1.
We also plan to conduct an intrinsic evaluation of
the quality of paraphrases that our technique gener-
ates. It is important to note that a different tradeoff
ratio may lead to even better results, e.g, using only
the paraphrased references when they pass some
goodness threshold, as used in Ueffing?s (2006) self-
training MT approach.
We have also observed that named entities are
usually paraphrased incorrectly if there is a genre
mismatch between the training and the test data. The
Hiero decoder allows spans of source text to be an-
notated with inline translations using XML. We plan
to identify and annotate named entities in the En-
glish source so that they are left unchanged.
Also, since the languageF for English-F pivoting
is arbitrary, we plan to investigate using English-to-
English grammars created using multiple English-F
grammars based on different languages, both indi-
126
vidually and in combination, in order to improve
paraphrase quality.
We also plan to explore a wider range of
paraphrase-creation techniques, ranging from sim-
ple word substitutions (e.g., based on WordNet) to
using the pivot technique with other translations sys-
tems.
7 Acknowledgments
We are indebted to David Chiang, Adam Lopez
and Smaranda Muresan for insights and comments.
This work has been supported under the GALE pro-
gram of the Defense Advaned Research Projects
Agency, Contract No. HR0011-06-2-001. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do
not necessarily reflect the view of DARPA.
References
S. Banerjee and A. Lavie. 2005. Meteor: An auto-
matic metric for mt evaluation with improved correla-
tion with human judgments. In Proceedings of Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization at ACL.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT-NAACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting struc-
tural paraphrases from aligned monolingual corpora.
In Proceedings the Second International Workshop on
Paraphrasing (ACL 2003).
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
A. Lopez. 2007. A survey of statistical machine transla-
tion. Technical Report 2006-47, University of Mary-
land, College Park.
D. W. Oard. 2003. The surprise langauge exercises.
ACM Transactions on Asian Language Information
Processing, 2(3).
Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
putational Linguistics, 30(4).
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
K. Probst, L. Levin, E. Peterson, A. Lavie, and J. Car-
bonell. 2002. Mt for minority languages using
elicitation-based learning of syntactic transfer rules.
Machine Translation, 17(4).
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP 2004.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2005. A paraphrase-based approach to machine trans-
lation evaluation. Technical Report UMIACS-TR-
2005-57, University of Maryland, College Park.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of AMTA.
S. Strassel, C. Cieri, A. Cole, D. DiPersio, M. Liberman,
X. Ma, M. Maamouri, and K. Maeda. 2006. Inte-
grated linguistic resources for language exploitation
technologies. In Proceedings of LREC.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proceedings of ACL.
Nicola Ueffing. 2006. Using monolingual source-
language data to improve MT performance. In Pro-
ceedings of IWSLT.
L. Zhou, C.-Y. Lin, D. Muntenau, and E. Hovy. 2006.
ParaEval: Using paraphrases to evaluate summaries
automatically. In Proceedings of HLT-NAACL.
127
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 71?79,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combining Open-Source with Research to Re-engineer
a Hands-on Introductory NLP Course
Nitin Madnani Bonnie J. Dorr
Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland, College Park
{nmadnani,bonnie}@umiacs.umd.edu
Abstract
We describe our first attempts to re-engineer
the curriculum of our introductory NLP course
by using two important building blocks: (1)
Access to an easy-to-learn programming lan-
guage and framework to build hands-on pro-
gramming assignments with real-world data
and corpora and, (2) Incorporation of interest-
ing ideas from recent NLP research publica-
tions into assignment and examination prob-
lems. We believe that these are extremely im-
portant components of a curriculum aimed at a
diverse audience consisting primarily of first-
year graduate students from both linguistics
and computer science. Based on overwhelm-
ingly positive student feedback, we find that
our attempts were hugely successful.
1 Introduction
Designing an introductory level natural language
processing course for a class of first year computer
science and linguistics graduate students is a chal-
lenging task. It is important to strive for balance be-
tween breadth and depth?it is important not only
to introduce the students to a variety of language
processing techniques and applications but also to
provide sufficient detail about each. However, we
claim that there is another important requirement for
a successful implementation of such a course. Like
any other graduate-level course offered to first year
students, it should encourage them to approach so-
lutions to problems as researchers. In order to meet
such a requirement, the course should have two im-
portant dimensions:
1. Access to a programming framework that pro-
vides the tools and data used in the real world
so as to allow the students to explore each topic
hands-on and easily attempt creative solutions
to problems. The framework should be simple
enough to use so that students are not bogged
down in its intricacies and can focus on the
course concepts.
2. Exposure to novel and innovative research in
each topic. One of the most valuable contribu-
tions of a large community, such as the NLP
and CL community, is the publicly accessible
repository of research publications for a range
of topics. While the commonly used textbooks
describe established and mainstream research
methods for each topic in detail, more recent
research papers are usually omitted. By using
such papers as the bases for programming
assignments?instantiated in the framework
described earlier?and exam questions, stu-
dents can gain important insights into how new
solutions to existing problems are formulated;
insights that can only come from a hands-on
approach to problem solving.
In this paper, we describe our attempts to engineer
such a course. In section 2, we describe the specific
goals we had in mind for such a course and how it
differs from the previous version of the introductory
course we taught at our institution. Section 3 dis-
cusses how we fully integrated an open-source pro-
gramming framework into our curriculum and used
it for programming assignments as well as in-class
71
sessions. In a similar vein, section 4 describes our
preliminary efforts to combine interesting research
ideas for various topics with the framework above.
We also have definite plans to expand the course cur-
riculum to take more novel ideas from recent NLP
literature for each topic and adapt them to instructive
hands-on assignments. Furthermore, we are devel-
oping extensions and add-ons for the programming
framework that we plan to contribute to the project.
We outline these plans in section 6 and conclude in
section 7.
2 Goals
We wanted our course curriculum to fulfill some
specific goals that we discuss below, provide moti-
vation wherever appropriate.
? A Uniform Programming Framework. The
previous version of our introductory course
took a more fragmented approach and used dif-
ferent programming languages and tools for
different assignments. For example, we used
an in-house HMM library written in C for any
HMM-based assignments and Perl for some
other assignments. As expected, such an ap-
proach requires students to familiarize them-
selves with a different programming interface
for each assignment and discourages students
to explore on their own. To address this con-
cern, we chose the Python (Python, 2007) pro-
gramming language and the Natural Language
Toolkit (Loper and Bird, 2002), written entirely
in Python, for all our assignments and program-
ming tasks. We discuss our use of NLTK in
more detail in the next section.
? Real-world Data & Corpora. In our previ-
ous course, students did not have access to any
of the corpora that are used in actual NLP re-
search. We found this to be a serious short-
coming and wanted to ensure that our new cur-
riculum allowed students to use real corpora for
evaluating their programming assignments.
? Exposure to Research. While we had cer-
tainly made it a point to introduce recent re-
search work in our lectures for all topics in
the previous course, we believed that a much
richer integration was required in order to al-
low a more realistic peek into NLP research.
? Satisfying a Diverse Audience. We wanted the
curriculum to appeal to both computer science
and linguistics students since they the course
was cross-listed in both departments.
? Continuing Interest. A large number of the
students enrolled in the course were undecided
about what research area to pursue. We wanted
to present a fair picture of what NLP research
actually entails and encourage any interested
students to take the more advanced part of the
course being offered later in the year.
3 Incorporating Open Source
We use the Python programming language and
NLTK as our programming framework for the
curriculum. Python is currently one of the most
popular programming languages?it is fully object
oriented and multi-platform, natively supports
high-level dynamic data types such as lists and
hashes (termed dictionaries in Python), has very
readable syntax and, most importantly, ships with
an extensive standard library for almost every con-
ceivable task. Although Python already has most
of the functionality needed to perform very simple
NLP tasks, its still not powerful enough for most
standard ones. This is where the Natural Language
Toolkit (NLTK) comes in. NLTK1, written entirely
in Python, is a collection of modules and corpora,
released under an open-source license, that allows
students to learn and conduct research in NLP (Bird
et al, 2008). The most important advantage of
using NLTK is that it is entirely self-contained.
Not only does it provide convenient functions
and wrappers that can be used as building blocks
for common NLP tasks, it also provides raw and
pre-processed versions of standard corpora used
frequently in NLP literature. Together, Python and
NLTK constitute one of the most potent tools for
instruction of NLP (Madnani, 2007) and allow us
to develop hands-on assignments that can appeal
to a broad audience including both linguistics and
computer science students.
1http://nltk.org
72
Figure 1: An Excerpt from the output of a Python script used for an in-class exercise demonstrating the simplicity of
the Python-NLTK combination.
In order to illustrate the simplicity and utility of
this tool to the students, we went through an in-class
exercise at the beginning of the class. The exercise
asked the students to solve the following simple
language processing problem:
Find the frequency of occurrences of the following
words in the state-of-the-union addresses of the last
6 American Presidents: war, peace, economy & aid.
Also draw histograms for each word.
We then went through a step-by-step process of how
one would go about solving such a problem. The
solution hinged on two important points:
(a) NLTK ships with a corpus of the last 50 years
of state-of-the-union addresses and provides a
native conditional frequency distribution object
to easily keep track of conditional counts.
(b) Drawing a histogram in Python is as simple as
the statement print ?#?*n where n is the
count for each query word.
Given these two properties, the Python solution for
the problem was only 20 lines long. Figure 1 shows
an excerpt from the output of this script. This ex-
ercise allowed us to impress upon the students that
the programming framework for the course is sim-
ple and fun so that they may start exploring it on
their own. We describe more concrete instances of
NLTK usage in our curriculum below.
3.1 HMMs & Part-of-speech Tagging
Hidden Markov Models (Rabiner, 1989) have
proven to be a very useful formalism in NLP
and have been used in a wide range of problems,
e.g., parsing, machine translation and part-of-speech
(POS) tagging. In our previous curriculum, we
had employed an in-house C++ implementation of
HMMs for our assignments. As part of our new
curriculum, we introduced Markov models (and
HMMs) in the context of POS tagging and in a much
more hands-on fashion. To do this, we created an
assignment where students were required to imple-
ment Viterbi decoding for an HMM and output the
best POS tag sequence for any given sentence. There
were several ways in which NLTK made this ex-
tremely simple:
? Since we had the entire source code of the
HMM module from NLTK available, we fac-
tored out the part of the code that handled the
HMM training, parameterized it and provided
that to students as a separate module they they
could use to train the HMMs. Such refactor-
ing not only allows for cleaner code boundaries
but it also allows the students to use a variety
of training parameters (such as different forms
of smoothed distributions for the transition and
emission probabilities) and measure their ef-
fects with little effort. Listing 1 shows how
the refactoring was accomplished: the train-
ing code was put into a separate module called
hmmtrainer and automatically called in the
73
Listing 1: A skeleton of the refactored NLTK HMM code
used to build a hands-on HMM assignment
import hmmtrainer
import nltk.LidStoneProbDist as lidstone
class hmm:
def __init__(self):
params = hmmtrainer.train(smooth=lidstone)
self.params = params
def decode(self, word_sequence)
def tag(self, word_sequence)
main hmm class when instantiating it. The stu-
dents had to write the code for the decode and
tag methods of this class. The HMM train-
ing was setup to be able to use a variety of
smoothed distributions, e.g. Lidstone, Laplace
etc., all available from NLTK.
? NLTK ships with the tokenized and POS tagged
version of the Brown corpus?one of the most
common corpora employed for corpus linguis-
tics and, in particular, for evaluating POS tag-
gers. We used Section A of the corpus for train-
ing the HMMs and asked the students to evalu-
ate their taggers on Section B.
Another advantage of this assignment was that the if
students were interested in how the supervised train-
ing process actually worked, they could simply ex-
amine the hmmtrainermodule that was also writ-
ten entirely in Python. An assignment with such
characteristics in our previous course would have
required knowledge of C++, willingness to wade
through much more complicated code and would
certainly not have been as instructive.
3.2 Finite State Automata
Another topic where we were able to leverage the
strengths of both NLTK and Python was when
introducing the students to finite state automata.
Previously, we only discussed the fundamentals of
finite state automata in class and then asked the
students to apply this knowledge to morphological
parsing by using PC-Kimmo (Koskenniemi, 1983).
However, working with PC-Kimmo required the
students to directly fill entries in transition tables
Listing 2: An illustration of the simple finite state trans-
ducer interface in NLTK
from nltk_contrib.fst import fst
f = fst.FST(?test?) #instantiate
f.add_state(?1?) # add states
f.add_state(?2?)
f.add_state(?3?)
f.initial_state = 1 # set initial
f.set_final(?2?) # set finals
f.set_final(?3?)
f.add_arc(?1?,?2?,?a?, ?A?) # a ?> A
f.add_arc(?1?,?3?,?b?, ?B?) # b ?> B
print f.transduce([?a?, ?a?, ?b?, ?b?])
using a very rigid syntax.
In the new curriculum, we could easily rely on
the finite state module that ships with NLTK to use
such automata in a very natural way as shown in
Listing 2. With such an easy to use interface, we
could concentrate instead on the more important
concepts underlying the building and cascading of
transducers to accomplish a language processing
task.
As our example task, we asked the students
to implement the Soundex Algorithm, a phonetic
algorithm commonly used by libraries and the
Census Bureau to represent people?s names as they
are pronounced in English. We found that not only
did the students easily implement such a complex
transducer, they also took the time to perform some
analysis on their own and determine the short-
comings of the Soundex algorithm. This was only
possible because of the simple interface and short
development cycle provided by the Python-NLTK
combination. In addition, NLTK also provides a
single method2 that can render the transducer as a
postscript or image file that can prove extremely
useful for debugging.
In our new version of the course, we consciously
chose to use primarily open-source technologies in
the curriculum. We feel that it is important to say a
few words about this choice: an open-source project
2This method interfaces with an existing installation of
Graphviz, a popular open-source graph drawing software (Ell-
son et al, 2004).
74
not only allows instructors to examine the source
code and re-purpose it for their own use (as we
did in section 3.1) but it also encourages students
to delve deep into the programming framework
if they are curious about how something works.
In fact, a few of our students actually discovered
subtle idiosyncrasies and bugs in the NLTK source
while exploring on their own, filed bug reports
where necessary and shared the findings with the
entire class. This experience allowed all students
to understand the challenges of language processing.
More importantly, we believe an open-source
project fosters collaboration in the community that
it serves. For example, a lot of the functionality of
NLTK hinges on important technical contributions,
such as our SRILM interface described in section 6,
from the large academic NLP community that can be
used by any member of the community for research
and for teaching.
4 Incorporating Research
Besides employing a uniform programming frame-
work that the students could pick up easily and learn
to explore on their own, the other important goal
of the new curriculum was to incorporate ideas and
techniques from interesting NLP research publica-
tions into assignments and exams. The motivation,
of course, was to get our students to think about
and possibly even implement these ideas. Since we
cannot discuss all instances in the curriculum where
we leveraged research publications (due to space
considerations), we only discuss two such instances
in detail below.
The first topic for which we constructed a more
open-ended research-oriented assignment was lex-
ical semantics. We focused, in particular, on the
WordNet (Fellbaum, 1998) database. WordNet is
a very popular lexical database and has been used
extensively in NLP literature over the years. In the
previous course, our assignment on lexical seman-
tics asked the students to use the online interface to
WordNet to learn the basic concept of a synset and
the various relations that are defined over synsets
such as hyponymy, hypernymy etc. A very sim-
ple change would have been to ask the students to
use the WordNet interface included with NLTK to
perform the same analysis. However, we thought
that a more interesting assignment would be to ex-
plore the structure of the four WordNet taxonomies
(Noun, Verb, Adjective and Adverb). This taxon-
omy can be simplified and thought of as a directed
acyclic graph G = (V,E) where each synset u ? V
is a node and each edge (u, v) ? E represents that
v is a hypernym of u. Given such a graph, some
very interesting statistics can be computed about the
topology ofWordNet itself (Devitt and Vogel, 2004).
In our assignment, we asked the students to use the
NLTK WordNet interface to compute some of these
statistics automatically and answer some interesting
questions:
(a) What percentage of the nodes in the Noun tax-
onomy are leaf nodes?
(b) Which are the nine most general root nodes in
the Noun taxonomy and what is the node dis-
tribution across these roots?
(c) Compute the branching factor (number of de-
scendants) for each node in the Noun taxonomy
both including and excluding leaf nodes. What
percentage of nodes have a branching factor
less than 5? Less than 20? Does this tell some-
thing you about the shallowness/depth of the
taxonomy?
(d) If we plot a graph with the number of senses
of each verb in the Verb taxonomy against its
polysemy rank, what kind of graph do we get?
What conclusion can be drawn from this graph?
(e) Compare the four taxonomies on average pol-
ysemy, both including and excluding monose-
mous words. What conclusions can you draw
from this?
Of course, the assignment also contained the usual
questions pertaining to the content of the WordNet
database rather than just its structure. We believe
that this assignment was much more instructive
because not only did it afford the students a close
examination into the usage as well as structure of a
valuable NLP resource, but also required them to
apply their knowledge of graph theory.
75
The second instance where we used a research pa-
per was when writing the HMM question for the fi-
nal exam. We thought it would be illuminating to
ask the students to apply what they had learned in
class about HMMs to an instance of HMM used in
an actual NLP scenario. For this purpose, we chose
the HMM described in (Miller et al, 1999) and as
shown in Figure 2. As part of the question, we ex-
q
S
q
D
q
GE
q
E
?
s
 = 1.0
1.0
1.0a
0
a
1
b
q
D
(u
i
)
b
q
GE
(u
i
)
1.0
Figure 2: An HMM used and described in a popular re-
search publication formed the basis of a question in the
final exam.
plained the information retrieval task: generate a
ranked list of documents relevant to a user query
U = ?ui?, where the rank of the document D is
based on the probability P (D is relevant|U). We
further explained that by applying Bayes? theorem
to this quantity and assuming a uniform prior over
document selection, the only important quantity was
the probability of the query U being generated by a
relevant document D, or P (U |D is relevant). The
rest of the question demonstrated how this genera-
tive process could be modeled by the HMM in Fig-
ure 2:
? Start at the initial state qS .
? Transition with the probability a0 to state qD
which represents choosing a word directly from
document D OR transition with probability a1
to state qGE which represents choosing a word
from ?General English?, i.e., a word unrelated
to the document but that occurs commonly in
other queries.
? If in state qD, emit the current, say ith, query
word either directly from document D with
emission probability bqD(ui). Otherwise, if in
state qGE , emit the current query word from
?General English? with emission probability
bqGE (ui).
? Transition to the end state qE .
? If we have generated all the words in the query,
then stop here. If not, transition to qS and
repeat.
Given this generative process, we then asked the stu-
dents to answer the following questions:
(a) Derive a simplified closed-form expression for
the posterior probability P (U |D is relevant)
in terms of the transition probabilities
{a0, a1} and the emissions probabilities
{bqD(ui), bqGE (ui)}. You may assume that
U = ?ui?
n
i=1.
(b) What HMM algorithm will you use to com-
pute P (U |D is relevant) when implementing
this model?
(c) How will you compute the maximum like-
lihood estimate for the emission probability
bqD(ui) ?
(d) What about bqGE (ui) ? Is it practical to com-
pute the actual value of this estimate? What
reasonable approximation might be used in
place of the actual value?
This question not only required the students to apply
the concepts of probability theory and HMMs that
they learned in class but also to contemplate more
open-ended research questions where there may be
no one right answer.
For both these and other instances where we used
ideas from research publications to build assign-
ments and exam questions, we encouraged the stu-
dents to read the corresponding publications after
they had submitted their solutions. In addition, we
discussed possible answers with them in an online
forum set up especially for the course.
76
5 Indicators of Success
Since this was our first major revision of the curricu-
lum for an introductory NLP course, we were inter-
ested in getting student feedback on the changes that
we made. To elicit such feedback, we designed a
survey that asked all the students in the class (a total
of 30) to rate the new curriculum on a scale of one
to five on various criteria, particularly for the expe-
rience of using NLTK for all programming assign-
ments and on the quality of the assignments them-
selves.
 0 20 40 60 80 100 Excellent Good Satisfactory Fair Poorpercentage of students
Figure 3: Histogram of student feedback on the experi-
ence of using the Python-NLTK combination.
 0 20 40 60 80 100 Excellent Good Satisfactory Fair Poorpercentage of students
Figure 4: Histogram of student feedback on the quality
of course assignments.
Figures 3 and 4 show the histograms of the stu-
dents? survey responses for these two criteria. The
overwhelmingly positive ratings clearly indicate
that we were extremely successful in achieving the
desired goals for our revised curriculum. As part of
the survey, we had also asked the students to provide
any comments they had about the curriculum. We
received a large number of positive comments some
of which we quote below:
?Using Python and NLTK for assignments removed
any programming barriers and enabled me to focus
on the course concepts.?
?The assignments were absolutely fantastic and
supplemented the material presented in class.?
?A great experience for the students.?
The first comment?echoed by several linguistics
as well as computer science students?validates
our particular choice of programming language
and framework. In the past, we had observed
that linguistics students with little programming
background spent most of their time figuring out
how to wield the programming language or tool to
accomplish simple tasks. However, the combination
of Python and NLTK provided a way for them to
work on computational solutions without taking
too much time away from learning the core NLP
concepts.
While it is clearly apparent to us that the students
really liked the new version of the curriculum, it
would also have been worthwhile to carry out a
comparison of students? reviews of the old and new
curricula. The most frequent comments that we saw
in older versions of the course were similar to the
following:
?Although I feel you did a decent job repeating and
pointing out the interesting facts of the book, I don?t
think you really found many compelling examples of
using these techniques in practice.?
The feedback we received for the revamped curricu-
lum, such as the second comment above, clearly in-
dicated that we had addressed this shortcoming of
the older curriculum. However, due to significant
format changes in the review forms between various
offerings of this course, it is not possible to conduct
a direct, retroactive comparison. It is our intent to
offer such comparisons in the future.
77
6 Future Plans
Given the success that we had in our first attempt
to re-engineer the introductory NLP course, we plan
to continue: (1) our hands-on approach to program-
ming assignments in the NLTK framework and, (2)
our practice of adapting ideas from research publi-
cations as the bases for assignment and examination
problems. Below we describe two concrete ideas for
the next iteration of the course.
1. Hands-on Statistical Language Modeling.
For this topic, we have so far restricted our-
selves to the textbook (Jurafsky and Mar-
tin, 2000); the in-class discussion and pro-
gramming assignments have been missing a
hands-on component. We have written a
Python interface to the SRI Language Model-
ing toolkit (Stolcke, 2002) for use in our re-
search work. This interface uses the Simpli-
fied Wrapper & Interface Generator (SWIG) to
generate a Python wrapper around our C code
that does all the heavy lifting via the SRILM
libraries. We are currently working on integrat-
ing this module into NLTK which would allow
all NLTK users, including our students in the
next version of the course, to build and query
statistical language models directly inside their
Python code. This module, combined with the
large real-world corpora, would provide a great
opportunity to perform hands-on experiments
with language models and to understand the
various smoothing methods. In addition, this
would also allow a language model to be used
in an assignment for any other topic should we
need it.
2. Teaching Distributional Similarity. The
idea that a language possesses distributional
structure?first discussed at length by Har-
ris (1954)?says that one can describe a lan-
guage in terms of relationships between the oc-
currences of its elements (words, morphemes,
phonemes). The name for the phenomenon
is derived from an element?s distribution?sets
of other elements in particular positions that
occur with the element in utterances or sen-
tences. This work led to the concept of distribu-
tional similarity?words or phrases that share
the same distribution, i.e., the same set of words
or in the same context in a corpus, tend to have
similar meanings. This is an extremely popular
concept in corpus linguistics and forms the ba-
sis of a large body of work. We believe that this
is an important topic that should be included in
the curriculum. We plan to do so in the context
of lexical paraphrase acquisition or synonyms
automatically from corpora, a task that relies
heavily on this notion of distributional similar-
ity. There has been a lot of work in this area in
the past years (Pereira et al, 1993; Gasperin et
al., 2001; Glickman and Dagan, 2003; Shimo-
hata and Sumita, 2005), much of which can be
easily replicated using the Python-NLTK com-
bination. This would allow for a very hands-on
treatment and would allow the students to gain
insight into this important, but often omitted,
idea from computational linguistics.
7 Conclusion
Our primacy goal was to design an introductory level
natural language processing course for a class of first
year computer science and linguistics graduate stu-
dents. We wanted the curriculum to encourage the
students to approach solutions to problems with the
mind-set of a researcher. To accomplish this, we re-
lied on two basic ideas. First, we used a program-
ming framework which provides the tools and data
used in the real world so as to allow hands-on ex-
ploration of each topic. Second, we adapted ideas
from recent research papers into programming as-
signments and exam questions to provide students
with insight into the process of formulating a solu-
tion to common NLP problems. At the end of the
course, we asked all students to provide feedback
and the verdict from both linguistics and computer
science students was overwhelmingly in favor of the
new more hands-on curriculum.
References
Steven Bird, Ewan Klein, Edward Loper, and Jason
Baldridge. 2008. Multidisciplinary Instruction with
the Natural Language Toolkit. In Proceedings of the
Third ACL Workshop on Issues in Teaching Computa-
tional Linguistics.
Ann Devitt and Carl Vogel. 2004. The Topology of
78
WordNet: Some metrics. In Proceedings of the Sec-
ond International WordNet Conference (GWC2004).
J. Ellson, E.R. Gansner, E. Koutsofios, S.C. North, and
G. Woodhull. 2004. Graphviz and Dynagraph ? Static
and Dynamic Graph Drawing Tools. In Graph Draw-
ing Software, pages 127?148. Springer-Verlag.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Caroline Gasperin, P. Gamallo, A. Agustini, G. Lopes,
and Vera de Lima. 2001. Using syntactic contexts for
measuring word similarity. In Workshop on Knowl-
edge Acquisition Categorization, ESSLLI.
Oren Glickman and Ido Dagan. 2003. Identifying lex-
ical paraphrases from a single corpus: A case study
for verbs. In Recent Advantages in Natural Language
Processing (RANLP?03).
Zellig Harris. 1954. Distributional Structure. Word,
10(2):3.146?162.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall.
Kimmo Koskenniemi. 1983. Two-level morphology: a
general computational model for word-form recogni-
tion and production. Publication No. 11, University of
Helsinki: Department of General Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: The Nat-
ural Language Toolkit. In Proceedings of ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing NLP and CL, pages 62?69.
Nitin Madnani. 2007. Getting Started on Natural Lan-
guage Processing with Python. ACM Crossroads,
13(4).
D. R. Miller, T. Leek, and R. M. Schwartz. 1999. A
hidden Markov model information retrieval system. In
Proceedings of SIGIR, pages 214?221.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In Proceed-
ings of ACL, pages 183?190.
Python. 2007. The Python Programming Language.
http://www.python.org.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Mitsuo Shimohata and Eiichiro Sumita. 2005. Acquir-
ing synonyms from monolingual comparable texts. In
Proceedings of IJCNLP, pages 233?244.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing (ICSLP).
79
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 259?268,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Fluency, Adequacy, or HTER?
Exploring Different Human Judgments with a Tunable MT Metric
Matthew Snover?, Nitin Madnani?, Bonnie J. Dorr? ? & Richard Schwartz? ?
?Laboratory for Computational Linguistics and Information Processing
?Institute for Advanced Computer Studies
?University of Maryland, College Park
?Human Language Technology Center of Excellence
?BBN Technologies
{snover,nmadnani,bonnie}@umiacs.umd.edu schwartz@bbn.com
Abstract
Automatic Machine Translation (MT)
evaluation metrics have traditionally been
evaluated by the correlation of the scores
they assign to MT output with human
judgments of translation performance.
Different types of human judgments, such
as Fluency, Adequacy, and HTER, mea-
sure varying aspects of MT performance
that can be captured by automatic MT
metrics. We explore these differences
through the use of a new tunable MT met-
ric: TER-Plus, which extends the Transla-
tion Edit Rate evaluation metric with tun-
able parameters and the incorporation of
morphology, synonymy and paraphrases.
TER-Plus was shown to be one of the
top metrics in NIST?s Metrics MATR
2008 Challenge, having the highest aver-
age rank in terms of Pearson and Spear-
man correlation. Optimizing TER-Plus
to different types of human judgments
yields significantly improved correlations
and meaningful changes in the weight of
different types of edits, demonstrating sig-
nificant differences between the types of
human judgments.
1 Introduction
Since the introduction of the BLEU metric (Pa-
pineni et al, 2002), statistical MT systems have
moved away from human evaluation of their per-
formance and towards rapid evaluation using au-
tomatic metrics. These automatic metrics are
themselves evaluated by their ability to generate
scores for MT output that correlate well with hu-
man judgments of translation quality. Numer-
ous methods of judging MT output by humans
have been used, including Fluency, Adequacy,
and, more recently, Human-mediated Translation
Edit Rate (HTER) (Snover et al, 2006). Fluency
measures whether a translation is fluent, regard-
less of the correct meaning, while Adequacy mea-
sures whether the translation conveys the correct
meaning, even if the translation is not fully flu-
ent. Fluency and Adequacy are frequently mea-
sured together on a discrete 5 or 7 point scale,
with their average being used as a single score
of translation quality. HTER is a more complex
and semi-automatic measure in which humans do
not score translations directly, but rather generate
a new reference translation that is closer to the
MT output but retains the fluency and meaning
of the original reference. This new targeted refer-
ence is then used as the reference translation when
scoring the MT output using Translation Edit Rate
(TER) (Snover et al, 2006) or when used with
other automatic metrics such as BLEU or ME-
TEOR (Banerjee and Lavie, 2005). One of the
difficulties in the creation of targeted references
is a further requirement that the annotator attempt
to minimize the number of edits, as measured by
TER, between the MT output and the targeted ref-
erence, creating the reference that is as close as
possible to the MT output while still being ade-
quate and fluent. In this way, only true errors in
the MT output are counted. While HTER has been
shown to be more consistent and finer grained than
individual human annotators of Fluency and Ade-
quacy, it is much more time consuming and tax-
ing on human annotators than other types of hu-
man judgments, making it difficult and expensive
to use. In addition, because HTER treats all edits
equally, no distinction is made between serious er-
rors (errors in names or missing subjects) and mi-
nor edits (such as a difference in verb agreement
259
or a missing determinator).
Different types of translation errors vary in im-
portance depending on the type of human judg-
ment being used to evaluate the translation. For
example, errors in tense might barely affect the ad-
equacy of a translation but might cause the trans-
lation be scored as less fluent. On the other hand,
deletion of content words might not lower the flu-
ency of a translation but the adequacy would suf-
fer. In this paper, we examine these differences
by taking an automatic evaluation metric and tun-
ing it to these these human judgments and exam-
ining the resulting differences in the parameteri-
zation of the metric. To study this we introduce
a new evaluation metric, TER-Plus (TERp)1 that
improves over the existing Translation Edit Rate
(TER) metric (Snover et al, 2006), incorporating
morphology, synonymy and paraphrases, as well
as tunable costs for different types of errors that
allow for easy interpretation of the differences be-
tween human judgments.
Section 2 summarizes the TER metric and dis-
cusses how TERp improves on it. Correlation re-
sults with human judgments, including indepen-
dent results from the 2008 NIST Metrics MATR
evaluation, where TERp was consistently one of
the top metrics, are presented in Section 3 to show
the utility of TERp as an evaluation metric. The
generation of paraphrases, as well as the effect of
varying the source of paraphrases, is discussed in
Section 4. Section 5 discusses the results of tuning
TERp to Fluency, Adequacy and HTER, and how
this affects the weights of various edit types.
2 TER and TERp
Both TER and TERp are automatic evaluation
metrics for machine translation that score a trans-
lation, the hypothesis, of a foreign language text,
the source, against a translation of the source text
that was created by a human translator, called a
reference translation. The set of possible cor-
rect translations is very large?possibly infinite?
and any single reference translation is just a sin-
gle point in that space. Usually multiple refer-
ence translations, typically 4, are provided to give
broader sampling of the space of correct transla-
tions. Automatic MT evaluation metrics compare
the hypothesis against this set of reference trans-
lations and assign a score to the similarity; higher
1Named after the nickname??terp??of the University of
Maryland, College Park, mascot: the diamondback terrapin.
scores are given to hypotheses that are more simi-
lar to the references.
In addition to assigning a score to a hypothe-
sis, the TER metric also provides an alignment be-
tween the hypothesis and the reference, enabling it
to be useful beyond general translation evaluation.
While TER has been shown to correlate well with
human judgments of translation quality, it has sev-
eral flaws, including the use of only a single ref-
erence translation and the measuring of similarity
only by exact word matches between the hypoth-
esis and the reference. The handicap of using a
single reference can be addressed by the construc-
tion of a lattice of reference translations. Such a
technique has been used with TER to combine the
output of multiple translation systems (Rosti et al,
2007). TERp does not utilize this methodology2
and instead focuses on addressing the exact match-
ing flaw of TER. A brief description of TER is pre-
sented in Section 2.1, followed by a discussion of
how TERp differs from TER in Section 2.2.
2.1 TER
One of the first automatic metrics used to evaluate
automatic machine translation (MT) systems was
Word Error Rate (WER) (Niessen et al, 2000),
which is the standard evaluation metric for Au-
tomatic Speech Recognition. WER is computed
as the Levenshtein (Levenshtein, 1966) distance
between the words of the system output and the
words of the reference translation divided by the
length of the reference translation. Unlike speech
recognition, there are many correct translations for
any given foreign sentence. These correct transla-
tions differ not only in their word choice but also
in the order in which the words occur. WER is
generally seen as inadequate for evaluation for ma-
chine translation as it fails to combine knowledge
from multiple reference translations and also fails
to model the reordering of words and phrases in
translation.
TER addresses the latter failing of WER by al-
lowing block movement of words, called shifts.
within the hypothesis. Shifting a phrase has the
same edit cost as inserting, deleting or substitut-
ing a word, regardless of the number of words
being shifted. While a general solution to WER
with block movement is NP-Complete (Lopresti
2The technique of combining references in this fashion
has not been evaluated in terms of its benefit when correlating
with human judgments. The authors hope to examine and
incorporate such a technique in future versions of TERp.
260
and Tomkins, 1997), TER addresses this by using
a greedy search to select the words to be shifted,
as well as further constraints on the words to be
shifted. These constraints are intended to simu-
late the way in which a human editor might choose
the words to shift. For exact details on these con-
straints, see Snover et al (2006). There are other
automatic metrics that follow the general formu-
lation as TER but address the complexity of shift-
ing in different ways, such as the CDER evaluation
metric (Leusch et al, 2006).
When TER is used with multiple references, it
does not combine the references. Instead, it scores
the hypothesis against each reference individually.
The reference against which the hypothesis has the
fewest number of edits is deemed the closet refer-
ence, and that number of edits is used as the nu-
merator for calculating the TER score. For the de-
nominator, TER uses the average number of words
across all the references.
2.2 TER-Plus
TER-Plus (TERp) is an extension of TER that
aligns words in the hypothesis and reference not
only when they are exact matches but also when
the words share a stem or are synonyms. In ad-
dition, it uses probabilistic phrasal substitutions
to align phrases in the hypothesis and reference.
These phrases are generated by considering possi-
ble paraphrases of the reference words. Matching
using stems and synonyms (Banerjee and Lavie,
2005) and using paraphrases (Zhou et al, 2006;
Kauchak and Barzilay, 2006) have previously been
shown to be beneficial for automatic MT evalu-
ation. Paraphrases have also been shown to be
useful in expanding the number of references used
for parameter tuning (Madnani et al, 2007; Mad-
nani et al, 2008) although they are not used di-
rectly in this fashion within TERp. While all edit
costs in TER are constant, all edit costs in TERp
are optimized to maximize correlation with human
judgments. This is because while a set of constant
weights might prove adequate for the purpose of
measuring translation quality?as evidenced by
correlation with human judgments both for TER
and HTER?they may not be ideal for maximiz-
ing correlation.
TERp uses all the edit operations of TER?
Matches, Insertions, Deletions, Substitutions and
Shifts?as well as three new edit operations: Stem
Matches, Synonym Matches and Phrase Substitu-
tions. TERp identifies words in the hypothesis and
reference that share the same stem using the Porter
stemming algorithm (Porter, 1980). Two words
are determined to be synonyms if they share the
same synonym set according to WordNet (Fell-
baum, 1998). Sequences of words in the reference
are considered to be paraphrases of a sequence of
words in the hypothesis if that phrase pair occurs
in the TERp phrase table. The TERp phrase table
is discussed in more detail in Section 4.
With the exception of the phrase substitutions,
the cost for all other edit operations is the same re-
gardless of what the words in question are. That
is, once the edit cost of an operation is determined
via optimization, that operation costs the same no
matter what words are under consideration. The
cost of a phrase substitution, on the other hand,
is a function of the probability of the paraphrase
and the number of edits needed to align the two
phrases according to TERp. In effect, the proba-
bility of the paraphrase is used to determine how
much to discount the alignment of the two phrases.
Specifically, the cost of a phrase substitution be-
tween the reference phrase, p1 and the hypothesis
phrase p2 is:
cost(p1, p2) =w1+
edit(p1, p2)?
(w2 log(Pr(p1, p2))
+ w3 Pr(p1, p2) + w4)
where w1, w2, w3, and w4 are the 4 free param-
eters of the edit cost, edit(p1, p2) is the edit cost
according to TERp of aligning p1 to p2 (excluding
phrase substitutions) and Pr(p1, p2) is the prob-
ability of paraphrasing p1 as p2, obtained from
the TERp phrase table. The w parameters of the
phrase substitution cost may be negative while still
resulting in a positive phrase substitution cost, as
w2 is multiplied by the log probability, which is al-
ways a negative number. In practice this term will
dominate the phrase substitution edit cost.
This edit cost for phrasal substitutions is, there-
fore, specified by four parameters, w1, w2, w3
and w4. Only paraphrases specified in the TERp
phrase table are considered for phrase substitu-
tions. In addition, the cost for a phrasal substi-
tution is limited to values greater than or equal to
0, i.e., the substitution cost cannot be negative. In
addition, the shifting constraints of TERp are also
relaxed to allow shifting of paraphrases, stems,
and synonyms.
261
In total TERp uses 11 parameters out of which
four represent the cost of phrasal substitutions.
The match cost is held fixed at 0, so that only the
10 other parameters can vary during optimization.
All edit costs, except for the phrasal substitution
parameters, are also restricted to be positive. A
simple hill-climbing search is used to optimize the
edit costs by maximizing the correlation of human
judgments with the TERp score. These correla-
tions are measured at the sentence, or segment,
level. Although it was done for the experiments
described in this paper, optimization could also
be performed to maximize document level correla-
tion ? such an optimization would give decreased
weight to shorter segments as compared to the seg-
ment level optimization.
3 Correlation Results
The optimization of the TERp edit costs, and com-
parisons against several standard automatic eval-
uation metrics, using human judgments of Ade-
quacy is first described in Section 3.1. We then
summarize, in Section 3.2, results of the NIST
Metrics MATR workshop where TERp was eval-
uated as one of 39 automatic metrics using many
test conditions and types of human judgments.
3.1 Optimization of Edit Costs and
Correlation Results
As part of the 2008 NIST Metrics MATR work-
shop (Przybocki et al, 2008), a development sub-
set of translations from eight Arabic-to-English
MT systems submitted to NIST?s MTEval 2006
was released that had been annotated for Ade-
quacy. We divided this development set into an
optimization set and a test set, which we then used
to optimize the edit costs of TERp and compare it
against other evaluation metrics. TERp was op-
timized to maximize the segment level Pearson
correlation with adequacy on the optimization set.
The edit costs determined by this optimization are
shown in Table 1.
We can compare TERp with other metrics by
comparing their Pearson and Spearman corre-
lations with Adequacy, at the segment, docu-
ment and system level. Document level Ade-
quacy scores are determined by taking the length
weighted average of the segment level scores. Sys-
tem level scores are determined by taking the
weighted average of the document level scores in
the same manner.
We compare TERp with BLEU (Papineni et al,
2002), METEOR (Banerjee and Lavie, 2005), and
TER (Snover et al, 2006). The IBM version of
BLEU was used in case insensitive mode with
an ngram-size of 4 to calculate the BLEU scores.
Case insensitivity was used with BLEU as it was
found to have much higher correlation with Ade-
quacy. In addition, we also examined BLEU using
an ngram-size of 2 (labeled as BLEU-2), instead
of the default ngram-size of 4, as it often has a
higher correlation with human judgments. When
using METEOR, the exact matching, porter stem-
ming matching, and WordNet synonym matching
modules were used. TER was also used in case
insensitive mode.
We show the Pearson and Spearman correlation
numbers of TERp and the other automatic metrics
on the optimization set and the test set in Tables 2
and 3. Correlation numbers that are statistically
indistinguishable from the highest correlation, us-
ing a 95% confidence interval, are shown in bold
and numbers that are actually not statistically sig-
nificant correlations are marked with a ?. TERp
has the highest Pearson correlation in all condi-
tions, although not all differences are statistically
significant. When examining the Spearman cor-
relation, TERp has the highest correlation on the
segment and system levels, but performs worse
than METEOR on the document level Spearman
correlatons.
3.2 NIST Metrics MATR 2008 Results
TERp was one of 39 automatic metrics evaluated
in the 2008 NIST Metrics MATR Challenge. In
order to evaluate the state of automatic MT eval-
uation, NIST tested metrics across a number of
conditions across 8 test sets. These conditions in-
cluded segment, document and system level corre-
lations with human judgments of preference, flu-
ency, adequacy and HTER. The test sets included
translations from Arabic-to-English, Chinese-to-
English, Farsi-to-English, Arabic-to-French, and
English-to-French MT systems involved in NIST?s
MTEval 2008, the GALE (Olive, 2005) Phase 2
and Phrase 2.5 program, Transtac January and July
2007, and CESTA run 1 and run 2, covering mul-
tiple genres. The version of TERp submitted to
this workshop was optimized as described in Sec-
tion 3.1. The development data upon which TERp
was optimized was not part of the test sets evalu-
ated in the Challenge.
262
Phrase Substitution
Match Insert Deletion Subst. Stem Syn. Shift w1 w2 w3 w4
0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18
Table 1: Optimized TERp Edit Costs
Optimization Set Test Set Optimization+Test
Metric Seg Doc Sys Seg Doc Sys Seg Doc Sys
BLEU 0.623 0.867 0.952 0.563 0.852 0.948 0.603 0.861 0.954
BLEU-2 0.661 0.888 0.946 0.591 0.876 0.953 0.637 0.883 0.952
METEOR 0.731 0.894 0.952 0.751 0.904 0.957 0.739 0.898 0.958
TER -0.609 -0.864 -0.957 -0.607 -0.860 -0.959 -0.609 -0.863 -0.961
TERp -0.782 -0.912 -0.996 -0.787 -0.918 -0.985 -0.784 -0.914 -0.994
Table 2: Optimization & Test Set Pearson Correlation Results
Due to the wealth of testing conditions, a sim-
ple overall view of the official MATR08 results re-
leased by NIST is difficult. To facilitate this anal-
ysis, we examined the average rank of each metric
across all conditions, where the rank was deter-
mined by their Pearson and Spearman correlation
with human judgments. To incorporate statistical
significance, we calculated the 95% confidence in-
terval for each correlation coefficient and found
the highest and lowest rank from which the cor-
relation coefficient was statistically indistinguish-
able, resulting in lower and upper bounds of the
rank for each metric in each condition. The aver-
age lower bound, actual, and upper bound ranks
(where a rank of 1 indicates the highest correla-
tion) of the top metrics, as well as BLEU and TER,
are shown in Table 4, sorted by the average upper
bound Pearson correlation. Full descriptions of the
other metrics3, the evaluation results, and the test
set composition are available from NIST (Przy-
bocki et al, 2008).
This analysis shows that TERp was consistently
one of the top metrics across test conditions and
had the highest average rank both in terms of Pear-
son and Spearman correlations. While this anal-
ysis is not comprehensive, it does give a general
idea of the performance of all metrics by syn-
thesizing the results into a single table. There
are striking differences between the Spearman and
Pearson correlations for other metrics, in particu-
lar the CDER metric (Leusch et al, 2006) had the
second highest rank in Spearman correlations (af-
3System description of metrics are also distributed
by AMTA: http://www.amtaweb.org/AMTA2008.
html
ter TERp), but was the sixth ranked metric accord-
ing to the Pearson correlation. In several cases,
TERp was not the best metric (if a metric was the
best in all conditions, its average rank would be 1),
although it performed well on average. In partic-
ular, TERp did significantly better than the TER
metric, indicating the benefit of the enhancements
made to TER.
4 Paraphrases
TERp uses probabilistic phrasal substitutions to
align phrases in the hypothesis with phrases in the
reference. It does so by looking up?in a pre-
computed phrase table?paraphrases of phrases in
the reference and using its associated edit cost as
the cost of performing a match against the hy-
pothesis. The paraphrases used in TERp were ex-
tracted using the pivot-based method as described
in (Bannard and Callison-Burch, 2005) with sev-
eral additional filtering mechanisms to increase
the precision. The pivot-based method utilizes the
inherent monolingual semantic knowledge from
bilingual corpora: we first identify English-to-F
phrasal correspondences, then map from English
to English by following translation units from En-
glish to F and back. For example, if the two En-
glish phrases e1 and e2 both correspond to the
same foreign phrase f, then they may be consid-
ered to be paraphrases of each other with the fol-
lowing probability:
p(e1|e2) ? p(e1|f) ? p(f |e2)
If there are several pivot phrases that link the two
English phrases, then they are all used in comput-
263
Optimization Set Test Set Optimization+Test
Metric Seg Doc Sys Seg Doc Sys Seg Doc Sys
BLEU 0.635 0.816 0.714? 0.550 0.740 0.690? 0.606 0.794 0.738?
BLEU-2 0.643 0.823 0.786? 0.558 0.747 0.690? 0.614 0.799 0.738?
METEOR 0.729 0.886 0.881 0.727 0.853 0.738? 0.730 0.876 0.922
TER -0.630 -0.794 -0.810? -0.630 -0.797 -0.667? -0.631 -0.801 -0.786?
TERp -0.760 -0.834 -0.976 -0.737 -0.818 -0.881 -0.754 -0.834 -0.929
Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation Results
Metric Average Rank by Pearson Average Rank by Spearman
TERp 1.49 6.07 17.31 1.60 6.44 17.76
METEOR v0.7 1.82 7.64 18.70 1.73 8.21 19.33
METEOR ranking 2.39 9.45 19.91 2.18 10.18 19.67
METEOR v0.6 2.42 10.67 19.11 2.47 11.27 19.60
EDPM 2.45 8.21 20.97 2.79 7.61 20.52
CDER 2.93 8.53 19.67 1.69 8.00 18.80
BleuSP 3.67 9.93 21.40 3.16 8.29 20.80
NIST-v11b 3.82 11.13 21.96 4.64 12.29 23.38
BLEU-1 (IBM) 4.42 12.47 22.18 4.98 14.87 24.00
BLEU-4 (IBM) 6.93 15.40 24.69 6.98 14.38 25.11
TER v0.7.25 8.87 16.27 25.29 6.93 17.33 24.80
BLEU-4 v12 (NIST) 10.16 18.02 27.64 10.96 17.82 28.16
Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Results
ing the probability:
p(e1|e2) ?
?
f ?
p(e1|f ?) ? p(f ?|e2)
The corpus used for extraction was an Arabic-
English newswire bitext containing a million sen-
tences. A few examples of the extracted para-
phrase pairs that were actually used in a run of
TERp on the Metrics MATR 2008 development
set are shown below:
(brief ? short)
(controversy over? polemic about)
(by using power? by force)
(response? reaction)
A discussion of paraphrase quality is presented
in Section 4.1, followed by a brief analysis of the
effect of varying the pivot corpus used by the auto-
matic paraphrase generation upon the correlation
performance of the TERp metric in Section 4.2.
4.1 Analysis of Paraphrase Quality
We analyzed the utility of the paraphrase probabil-
ity and found that it was not always a very reliable
estimate of the degree to which the pair was se-
mantically related. For example, we looked at all
paraphrase pairs that had probabilities greater than
0.9, a set that should ideally contain pairs that are
paraphrastic to a large degree. In our analysis, we
found the following five kinds of paraphrases in
this set:
(a) Lexical Paraphrases. These paraphrase
pairs are not phrasal paraphrases but instead
differ in at most one word and may be con-
sidered as lexical paraphrases for all practical
purposes. While these pairs may not be very
valuable for TERp due to the obvious overlap
with WordNet, they may help in increasing
the coverage of the paraphrastic phenomena
that TERp can handle. Here are some exam-
ples:
(2500 polish troops? 2500 polish soldiers)
(accounting firms? auditing firms)
(armed source? military source)
(b) Morphological Variants. These phrasal
pairs only differ in the morphological form
264
for one of the words. As the examples show,
any knowledge that these pairs may provide
is already available to TERp via stemming.
(50 ton? 50 tons)
(caused clouds? causing clouds)
(syria deny? syria denies)
(c) Approximate Phrasal Paraphrases. This
set included pairs that only shared partial se-
mantic content. Most paraphrases extracted
by the pivot method are expected to be of this
nature. These pairs are not directly beneficial
to TERp since they cannot be substituted for
each other in all contexts. However, the fact
that they share at least some semantic content
does suggest that they may not be entirely
useless either. Examples include:
(mutual proposal? suggest)
(them were exiled? them abroad)
(my parents? my father)
(d) Phrasal Paraphrases. We did indeed find
a large number of pairs in this set that were
truly paraphrastic and proved the most useful
for TERp. For example:
(agence presse? news agency)
(army roadblock? military barrier)
(staff walked out? team withdrew)
(e) Noisy Co-occurrences. There are also pairs
that are completely unrelated and happen
to be extracted as paraphrases based on the
noise inherent in the pivoting process. These
pairs are much smaller in number than the
four sets described above and are not signif-
icantly detrimental to TERp since they are
rarely chosen for phrasal substitution. Exam-
ples:
(counterpart salam? peace)
(regulation dealing? list)
(recall one? deported)
Given this distribution of the pivot-based para-
phrases, we experimented with a variant of TERp
that did not use the paraphrase probability at all
but instead only used the actual edit distance be-
tween the two phrases to determine the final cost
of a phrase substitution. The results for this exper-
iment are shown in the second row of Table 5. We
can see that this variant works as well as the full
version of TERp that utilizes paraphrase probabil-
ities. This confirms our intuition that the proba-
bility computed via the pivot-method is not a very
useful predictor of semantic equivalence for use in
TERp.
4.2 Varying Paraphrase Pivot Corpora
To determine the effect that the pivot language
might have on the quality and utility of the ex-
tracted paraphrases in TERp, we used paraphrase
pairsmade available by Callison-Burch (2008).
These paraphrase pairs were extracted from Eu-
roparl data using each of 10 European languages
(German, Italian, French etc.) as a pivot language
separately and then combining the extracted para-
phrase pairs. Callison-Burch (2008) also extracted
and made available syntactically constrained para-
phrase pairs from the same data that are more
likely to be semantically related.
We used both sets of paraphrases in TERp as al-
ternatives to the paraphrase pairs that we extracted
from the Arabic newswire bitext. The results are
shown in the last four rows of Table 5 and show
that using a pivot language other than the one that
the MT system is actually translating yields results
that are almost as good. It also shows that the
syntactic constraints imposed by Callison-Burch
(2008) on the pivot-based paraphrase extraction
process are useful and yield improved results over
the baseline pivot-method. The results further sup-
port our claim that the pivot paraphrase probability
is not a very useful indicator of semantic related-
ness.
5 Varying Human Judgments
To evaluate the differences between human judg-
ment types we first align the hypothesis to the ref-
erences using a fixed set of edit costs, identical to
the weights in Table 1, and then optimize the edit
costs to maximize the correlation, without realign-
ing. The separation of the edit costs used for align-
ment from those used for scoring allows us to re-
move the confusion of edit costs selected for align-
ment purposes from those selected to increase cor-
relation.
For Adequacy and Fluency judgments, the
MTEval 2002 human judgement set4 was used.
This set consists of the output of ten MT sys-
tems, 3 Arabic-to-English systems and 7 Chinese-
4Distributed to the authors by request from NIST.
265
Pearson Spearman
Paraphrase Setup Seg Doc Sys Seg Doc Sys
Arabic pivot -0.787 -0.918 -0.985 -0.737 -0.818 -0.881
Arabic pivot and no prob -0.787 -0.933 -0.986 -0.737 -0.841 -0.881
Europarl pivot -0.775 -0.940 -0.983 -0.738 -0.865 -0.905
Europarl pivot and no prob -0.775 -0.940 -0.983 -0.737 -0.860 -0.905
Europarl pivot and syntactic constraints -0.781 -0.941 -0.985 -0.739 -0.859 -0.881
Europarl pivot, syntactic constraints and no prob -0.779 -0.946 -0.985 -0.737 -0.866 -0.976
Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.
Human Phrase Substitution
Judgment Match Insert Deletion Subst. Stem Syn. Shift w1 w2 w3 w4
Alignment 0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18
Adequacy 0.0 0.18 1.42 1.71 0.0 0.0 0.19 -0.38 -0.03 0.22 0.47
Fluency 0.0 0.12 1.37 1.81 0.0 0.0 0.43 -0.63 -0.07 0.12 0.46
HTER 0.0 0.84 0.76 1.55 0.90 0.75 1.07 -0.03 -0.17 -0.08 -0.09
Table 6: Optimized Edit Costs
to-English systems, consisting of a total, across
all systems and both language pairs, of 7,452 seg-
ments across 900 documents. To evaluate HTER,
the GALE (Olive, 2005) 2007 (Phase 2.0) HTER
scores were used. This set consists of the out-
put of 6 MT systems, 3 Arabic-to-English systems
and 3 Chinese-to-English systems, although each
of the systems in question is the product of system
combination. The HTER data consisted of a total,
across all systems and language pairs, of 16,267
segments across a total of 1,568 documents. Be-
cause HTER annotation is especially expensive
and difficult, it is rarely performed, and the only
source, to the authors? knowledge, of available
HTER annotations is on GALE evaluation data for
which no Fluency and Adequacy judgments have
been made publicly available.
The edit costs learned for each of these human
judgments, along with the alignment edit costs are
shown in Table 6. While all three types of human
judgements differ from the alignment costs used
in alignment, the HTER edit costs differ most sig-
nificantly. Unlike Adequacy and Fluency which
have a low edit cost for insertions and a very high
cost for deletions, HTER has a balanced cost for
the two edit types. Inserted words are strongly pe-
nalized against in HTER, as opposed to in Ade-
quacy and Fluency, where such errors are largely
forgiven. Stem and synonym edits are also penal-
ized against while these are considered equivalent
to a match for both Adequacy and Fluency. This
penalty against stem matches can be attributed to
Fluency requirements in HTER that specifically
penalize against incorrect morphology. The cost
of shifts is also increased in HTER, strongly penal-
izing the movement of phrases within the hypoth-
esis, while Adequacy and Fluency give a much
lower cost to such errors. Some of the differences
between HTER and both fluency and adequacy
can be attributed to the different systems used. The
MT systems evaluated with HTER are all highly
performing state of the art systems, while the sys-
tems used for adequacy and fluency are older MT
systems.
The differences between Adequacy and Fluency
are smaller, but there are still significant differ-
ences. In particular, the cost of shifts is over twice
as high for the fluency optimized system than the
adequacy optimized system, indicating that the
movement of phrases, as expected, is only slightly
penalized when judging meaning, but can be much
more harmful to the fluency of a translation. Flu-
ency however favors paraphrases more strongly
than the edit costs optimized for adequacy. This
might indicate that paraphrases are used to gener-
ate a more fluent translation although at the poten-
tial loss of meaning.
266
6 Discussion
We introduced a new evaluation metric, TER-Plus,
and showed that it is competitive with state-of-the-
art evaluation metrics when its predictions are cor-
related with human judgments. The inclusion of
stem, synonym and paraphrase edits allows TERp
to overcome some of the weaknesses of the TER
metric and better align hypothesized translations
with reference translations. These new edit costs
can then be optimized to allow better correlation
with human judgments. In addition, we have ex-
amined the use of other paraphrasing techniques,
and shown that the paraphrase probabilities esti-
mated by the pivot-method may not be fully ad-
equate for judgments of whether a paraphrase in
a translation indicates a correct translation. This
line of research holds promise as an external eval-
uation method of various paraphrasing methods.
However promising correlation results for an
evaluation metric may be, the evaluation of the
final output of an MT system is only a portion
of the utility of an automatic translation metric.
Optimization of the parameters of an MT system
is now done using automatic metrics, primarily
BLEU. It is likely that some features that make an
evaluation metric good for evaluating the final out-
put of a system would make it a poor metric for use
in system tuning. In particular, a metric may have
difficulty distinguishing between outputs of an MT
system that been optimized for that same metric.
BLEU, the metric most frequently used to opti-
mize systems, might therefore perform poorly in
evaluation tasks compared to recall oriented met-
rics such as METEOR and TERp (whose tuning
in Table 1 indicates a preference towards recall).
Future research into the use of TERp and other
metrics as optimization metrics is needed to better
understand these metrics and the interaction with
parameter optimization.
Finally, we explored the difference between
three types of human judgments that are often
used to evaluate both MT systems and automatic
metrics, by optimizing TERp to these human
judgments and examining the resulting edit costs.
While this can make no judgement as to the pref-
erence of one type of human judgment over an-
other, it indicates differences between these hu-
man judgment types, and in particular the differ-
ence between HTER and Adequacy and Fluency.
This exploration is limited by the the lack of a
large amount of diverse data annotated for all hu-
man judgment types, as well as the small num-
ber of edit types used by TERp. The inclusion
of additional more specific edit types could lead
to a more detailed understanding of which trans-
lation phenomenon and translation errors are most
emphasized or ignored by which types of human
judgments.
Acknowledgments
This work was supported, in part, by BBN Tech-
nologies under the GALE Program, DARPA/IPTO
Contract No. HR0011-06-C-0022 and in part by
the Human Language Technology Center of Ex-
cellence.. TERp is available on the web for down-
load at: http://www.umiacs.umd.edu/?snover/terp/.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL 2005 Workshop on Intrinsic and
Extrinsic Evaulation Measures for MT and/or Sum-
marization.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005),
pages 597?604, Ann Arbor, Michigan, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 196?
205, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
http://www.cogsci.princeton.edu/?wn [2000,
September 7].
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the ACL, pages 455?
462.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using Block
Movements. In Proceedings of the 11th Confer-
enceof the European Chapter of the Association for
Computational Linguistics (EACL 2006).
V. I. Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Soviet
Physics Doklady, 10:707?710.
267
Daniel Lopresti and Andrew Tomkins. 1997. Block
edit models for approximate string matching. Theo-
retical Computer Science, 181(1):159?179, July.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Workshop on Statistical Machine
Translation, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are Multiple Reference
Translations Necessary? Investigating the Value
of Paraphrased Reference Translations in Parameter
Optimization. In Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, October.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evalua-
tion for MT research. In Proceedings of the 2nd In-
ternational Conference on Language Resources and
Evaluation (LREC-2000), pages 39?45.
Joseph Olive. 2005. Global Autonomous Language
Exploitation (GALE). DARPA/IPTO Proposer In-
formation Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Traslation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Martin F. Porter. 1980. An algorithm for suffic strip-
ping. Program, 14(3):130?137.
Mark Przybocki, Kay Peterson, and Sebas-
tian Bronsart. 2008. Official results
of the NIST 2008 ?Metrics for MAchine
TRanslation? Challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/,
October.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 312?319, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
Liang Zhou, Chon-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating Machine Translation Results with
Paraphrase Support. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2006), pages 77?84.
268
Generating Phrasal and Sentential
Paraphrases: A Survey of
Data-Driven Methods
Nitin Madnani?
University of Maryland, College Park
Bonnie J. Dorr??
University of Maryland, College Park
The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of
automatically generating or extracting semantic equivalences for the various units of language?
words, phrases, and sentences?is an important part of natural language processing (NLP)
and is being increasingly employed to improve the performance of several NLP applications.
In this article, we attempt to conduct a comprehensive and application-independent survey of
data-driven phrasal and sentential paraphrase generation methods, while also conveying an
appreciation for the importance and potential use of paraphrases in the field of NLP research.
Recent work done in manual and automatic construction of paraphrase corpora is also examined.
We also discuss the strategies used for evaluating paraphrase generation techniques and briefly
explore some future trends in paraphrase generation.
1. Introduction
Although everyone may be familiar with the notion of paraphrase in its most funda-
mental sense, there is still room for elaboration on how paraphrases may be automat-
ically generated or elicited for use in language processing applications. In this survey,
we make an attempt at such an elaboration. An important outcome of this survey is
the discovery that there are a large variety of paraphrase generation methods, each
with widely differing sets of characteristics, in terms of performance as well as ease
of deployment. We also find that although many paraphrase methods are developed
with a particular application in mind, all methods share the potential for more general
applicability. Finally, we observe that the choice of the most appropriate method for
an application depends on proper matching of the characteristics of the produced
paraphrases with an appropriate method.
It could be argued that it is premature to survey an area of research that has shown
promise but has not yet been tested for a long enough period (and in enough systems).
However, we believe this argument actually strengthens the motivation for a survey
? Department of Computer Science and Institute for Advanced Computer Studies, A.V. Williams Bldg,
University of Maryland, College Park, MD 20742, USA. E-mail: nmadnani@umiacs.umd.edu.
?? Department of Computer Science and Institute for Advanced Computer Studies, A.V. Williams Bldg,
University of Maryland, College Park, MD 20742, USA. E-mail: bonnie@umiacs.umd.edu.
Submission received: 16 December 2008; revised submission received: 30 November 2009; accepted for
publication: 7 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
that can encourage the community to use paraphrases by providing an application-
independent, cohesive, and condensed discussion of data-driven paraphrase generation
techniques. We should also acknowledge related work that has been done on furthering
the community?s understanding of paraphrases. Hirst (2003) presents a comprehensive
survey of paraphrasing focused on a deep analysis of the nature of a paraphrase. The
current survey focuses instead on delineating the salient characteristics of the various
paraphrase generation methods with an emphasis on describing how they could be
used in several different NLP applications. Both these treatments provide different but
valuable perspectives on paraphrasing.
The remainder of this section formalizes the concept of a paraphrase, scopes out
the coverage of this survey?s discussion, and provides broader context and motivation
by discussing applications in which paraphrase generation has proven useful, along
with examples. Section 2 briefly describes the tasks of paraphrase recognition and
textual entailment and their relationship to paraphrase generation and extraction. Sec-
tion 3 forms the major contribution of this survey by examining various corpora-based
techniques for paraphrase generation, organized by corpus type. Section 4 examines
recent work done to construct various types of paraphrase corpora and to elicit human
judgments for such corpora. Section 5 considers the task of evaluating the performance
of paraphrase generation and extraction techniques. Finally, Section 6 provides a brief
glimpse of the future trends in paraphrase generation and Section 7 concludes the
survey with a summary.
1.1 What is a Paraphrase?
The concept of paraphrasing is most generally defined on the basis of the principle of
semantic equivalence: A paraphrase is an alternative surface form in the same language
expressing the same semantic content as the original form. Paraphrases may occur at
several levels.
Individual lexical items having the same meaning are usually referred to as lexical
paraphrases or, more commonly, synonyms, for example, ?hot, warm? and ?eat, consume?.
However, lexical paraphrasing cannot be restricted strictly to the concept of synonymy.
There are several other forms such as hyperonymy, where one of the words in the
paraphrastic relationship is either more general or more specific than the other, for
example, ?reply, say? and ?landlady, hostess?.
The term phrasal paraphrase refers to phrasal fragments sharing the same semantic
content. Although these fragments most commonly take the form of syntactic phrases
(?work on, soften up? and ?take over, assume control of ?) they may also be patterns with
linked variables, for example, ?Y was built by X, X is the creator of Y?.
Two sentences that represent the same semantic content are termed sentential
paraphrases, for example, ?I finished my work, I completed my assignment?. Although it is
possible to generate very simple sentential paraphrases by simply substituting words
and phrases in the original sentence with their respective semantic equivalents, it is
significantly more difficult to generate more interesting ones such as ?He needed to make
a quick decision in that situation, The scenario required him to make a split-second judgment?.
Culicover (1968) describes some common forms of sentential paraphrases.
1.2 Scope of Discussion
The idea of paraphrasing has been explored in conjunction with, and employed in, a
large number of natural language processing applications. Given the difficulty inherent
342
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
in surveying such a diverse task, an unfortunate but necessary remedy is to impose
certain limits on the scope of our discussion. In this survey, we will be restricting our
discussion to only automatic acquisition of phrasal paraphrases (including paraphrastic
patterns) and on generation of sentential paraphrases. More specifically, this entails the
exclusion of certain categories of paraphrasing work. However, as a compromise for
the interested reader, we do include a relatively comprehensive list of references in this
section for the work that is excluded from the survey.
For one, we do not discuss paraphrasing techniques that rely primarily on
knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al 2004), hand-
written rules (Fujita et al 2007), and formal grammars (McKeown 1979; Dras 1999;
Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from dis-
cussing work on purely lexical paraphrasing which usually comprises various ways to
cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira,
Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al 2001; Glickman and
Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing
methods obviously implies that other lexical methods developed just for specific
applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and
Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at
the other end of the spectrum that paraphrase supra-sentential units such as paragraphs
and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami
2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the
notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002).
1.3 Applications of Paraphrase Generation
Before describing the techniques used for paraphrasing, it is essential to examine the
broader context of the application of paraphrases. For some of the applications we
discuss subsequently, the use of paraphrases in the manner described may not yet be
the norm. However, wherever applicable, we cite recent research that promises gains
in performance by using paraphrases for these applications. Also note that we only
discuss those paraphrasing techniques that can generate the types of paraphrases under
examination in this survey: phrasal and sentential.
1.3.1 Query and Pattern Expansion. One of the most common applications of paraphrasing
is the automatic generation of query variants for submission to information retrieval
systems or of patterns for submission to information extraction systems. Culicover
(1968) describes one of the earliest theoretical frameworks for query keyword expansion
using paraphrases. One of the earliest works to implement this approach (Spa?rck-
Jones and Tait 1984) generates several simple variants for compound nouns in queries
submitted to a technical information retrieval system. For example:
Original : circuit details
Variant 1 : details about the circuit
Variant 2 : the details of circuits
1 Inferring words to be similar based on similar contexts can be thought of as the most common instance
of employing distributional similarity. The concept of distributional similarity also turns out to be quite
important for phrasal paraphrase generation and is discussed in more detail in Section 3.1.
343
Computational Linguistics Volume 36, Number 3
In fact, in recent years, the information retrieval community has extensively explored
the task of query expansion by applying paraphrasing techniques to generate similar or
related queries (Beeferman and Berger 2000; Jones et al 2006; Sahami and Hellman 2006;
Metzler, Dumais, and Meek 2007; Shi and Yang 2007). The generation of paraphrases in
these techniques is usually effected by utilizing the query log (a log containing the record
of all queries submitted to the system) to determine semantic similarity. Jacquemin
(1999) generates morphological, syntactic, and semantic variants for phrases in the
agricultural domain. For example:
Original : simultaneous measurements
Variant : concurrent measures
Original : development area
Variant : area of growth
Ravichandran and Hovy (2002) use semi-supervised learning to induce several para-
phrastic patterns for each question type and use them in an open-domain question
answering system. For example, for the INVENTOR question type, they generate:
Original : X was invented by Y
Variant 1 : Y?s invention of X
Variant 2 : Y, inventor of X
Riezler et al (2007) expand a query by generating n-best paraphrases for the query
(via a pivot-based sentential paraphrasing model employing bilingual parallel corpora,
detailed in Section 3) and then using any new words introduced therein as additional
query terms. For example, for the query how to live with cat allergies, they may generate
the following two paraphrases. The novel words in the two paraphrases are highlighted
in bold and are used to expand the original query:
P1 : ways to live with feline allergy
P2 : how to deal with cat allergens
Finally, paraphrases have also been used to improve the task of relation extraction
(Romano et al 2006). Most recently, Bhagat and Ravichandran (2008) collect paraphras-
tic patterns for relation extraction by applying semi-supervised paraphrase induction
to a very large monolingual corpus. For example, for the relation of ?acquisition,? they
collect:
Original : X agreed to buy Y
Variant 1 : X completed its acquisition of Y
Variant 2 : X purchased Y
1.3.2 Expanding Sparse Human Reference Data for Evaluation. A large percentage of NLP
applications are evaluated by having human annotators or subjects carry out the same
344
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
task for a given set of data and using the output so created as a reference against which
to measure the performance of the system. The two applications where comparison
against human-authored reference output has become the norm are machine translation
and document summarization.
In machine translation evaluation, the translation hypotheses output by a ma-
chine translation system are evaluated against reference translations created by human
translators by measuring the n-gram overlap between the two (Papineni et al 2002).
However, it is impossible for a single reference translation to capture all possible
verbalizations that can convey the same semantic content. This may unfairly penalize
translation hypotheses that have the same meaning but use n-grams that are not present
in the reference. For example, the given system output S will not have a high score
against the reference R even though it conveys precisely the same semantic content:
S: We must consider the entire community.
R: We must bear in mind the community as a whole.
One solution is to use multiple reference translations, which is expensive. An alternative
solution, tried in a number of recent approaches, is to address this issue by allowing the
evaluation process to take into account paraphrases of phrases in the reference trans-
lation so as to award credit to parts of the translation hypothesis that are semantically,
even if not lexically, correct (Owczarzak et al 2006; Zhou, Lin, and Hovy 2006).
In evaluation of document summarization, automatically generated summaries
(peers) are also evaluated against reference summaries created by human authors
(models). Zhou et al (2006) propose a new metric called ParaEval that leverages an
automatically extracted database of phrasal paraphrases to inform the computation of
n-gram overlap between peer summaries and multiple model summaries.
1.3.3 Machine Translation. Besides being used in evaluation of machine translation sys-
tems, paraphrasing has also been applied to directly improve the translation process.
Callison-Burch, Koehn, and Osborne (2006) use automatically induced paraphrases to
improve a statistical phrase-based machine translation system. Such a system works by
dividing the given sentence into phrases and translating each phrase individually by
looking up its translation in a table. The coverage of the translation system is improved
by allowing any source phrase that does not have a translation in the table to use
the translation of one of its paraphrases. For example, if a given Spanish sentence
contains the phrase presidente de Brazil but the system does not have a translation for
it, another Spanish phrase such as presidente brasilen?o may be automatically detected as
a paraphrase of presidente de Brazil; then if the translation table contains a translation for
the paraphrase, the system can use the same translation for the given phrase. Therefore,
paraphrasing allows the translation system to properly handle phrases that it does not
otherwise know how to translate.
Another important issue for statistical machine translation systems is that of
reference sparsity. The fundamental problem that translation systems have to face is
that there is no such thing as the correct translation for any sentence. In fact, any given
source sentence can often be translated into the target language in many valid ways.
Because there can be many ?correct answers,? almost all models employed by SMT
systems require, in addition to a large bitext, a held-out development set comprising
multiple high-quality, human-authored reference translations in the target language in
order to tune their parameters relative to a translation quality metric. However, given
345
Computational Linguistics Volume 36, Number 3
the time and cost implications of such a process, most such data sets usually have
only a single reference translation. Madnani et al (2007, 2008b) generate sentential
paraphrases and use them to expand the available reference translations for such sets
so that the machine translation system can learn a better set of system parameters.
2. Paraphrase Recognition and Textual Entailment
A problem closely related to, and as important as, generating paraphrases is that of
assigning a quantitative measurement to the semantic similarity of two phrases (Fujita
and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and
Katz 2005). A more complex formulation of the task would be to detect or recognize
which sentences in the two texts are paraphrases of each other (Brockett and Dolan
2005; Marsi and Krahmer 2005a; Wu 2005; Joa`o, Das, and Pavel 2007a, 2007b; Das and
Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category
of paraphrase detection or recognition. The latter formulation of the task has become
popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques
that require monolingual parallel or comparable corpora (discussed in Section 3) can
benefit immensely from this task. In general, paraphrase recognition can be very helpful
for several NLP applications. Two examples of such applications are text-to-text gener-
ation and information extraction.
Text-to-text generation applications rely heavily on paraphrase recognition. For a
multi-document summarization system, detecting redundancy is a very important con-
cern because two sentences from different documents may convey the same semantic
content and it is important not to repeat the same information in the summary. On
this note, Barzilay and McKeown (2005) exploit the redundancy present in a given set
of sentences by detecting paraphrastic parts and fusing them into a single coherent
sentence. Recognizing similar semantic content is also critical for text simplification
systems (Marsi and Krahmer 2005b).
Information extraction enables the detection of regularities of information
structure?events which are reported many times, about different individuals and in
different forms?and making them explicit so that they can be processed and used in
other ways. Sekine (2006) shows how to use paraphrase recognition to cluster together
extraction patterns to improve the cohesion of the extracted information.
Another recently proposed natural language processing task is that of recognizing
textual entailment: A piece of text T is said to entail a hypothesis H if humans reading
T will infer that H is most likely true. The observant reader will notice that, in this sense,
the task of paraphrase recognition can simply be formulated as bidirectional entailment
recognition. The task of recognizing entailment is an application-independent task and
has important ramifications for almost all other language processing tasks that can
derive benefit from some form of applied semantic inference. For this reason, the task
has received noticeable attention in the research community and annual community-
wide evaluations of entailment systems have been held in the form of the Recognizing
Textual Entailment (RTE) Challenges (Dagan, Glickman, and Magnini 2006; Bar-Haim
et al 2007; Sekine et al 2007; Giampiccolo et al 2008).
Looking towards the future, Dagan (2008) suggests that the textual entailment task
provides a comprehensive framework for semantic inference and argues for building
a concrete inference engine that not only recognizes entailment but also searches for
all entailing texts given an entailment hypothesis H and, conversely, generates all
entailed statements given a text T. Given such an engine, Dagan claims that paraphrase
346
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
generation is simply a matter of generating all entailed statements given any sentence.
Although this is a very attractive proposition that defines both paraphrase generation
and recognition in terms of textual entailment, there are some important caveats. For
example, textual entailment cannot guarantee that the entailed hypothesis H captures
all of the same meaning as the given text T. Consider the following example:
T: Yahoo?s buyout of Overture was finalized.
H1: Yahoo bought Overture.
H2: Overture is now owned by Yahoo.
Although both H1 and H2 are entailed by T, they are not strictly paraphrases of T
because some of the semantic content has not been carried over. This must be an
important consideration when building the proposed entailment engine. Of course,
even these approximately semantically equivalent constructions may prove useful in
an appropriate downstream application.
The relationship between paraphrasing and entailment is more tightly entwined
than it might appear. Entailment recognition systems sometimes rely on the use of
paraphrastic templates or patterns as inputs (Iftene 2009) and might even use para-
phrase recognition to improve their performance (Bosma and Callison-Burch 2007).
In fact, examination of some RTE data sets in an attempt to quantitatively determine
the presence of paraphrases has shown that a large percentage of the set consists of
paraphrases rather than typical entailments (Bayer et al 2005; Garoufi 2007). It has
also been observed that, in the entailment challenges, it is relatively easy for submitted
systems to recognize constructions that partially overlap in meaning (approximately
paraphrastic) from those that are actually bound by an entailment relation. On the
flip side, work has also been done to extend entailment recognition techniques for the
purpose of paraphrase recognition (Rus, McCarthy, and Lintean 2008).
Detection of semantic similarity and, to some extent, that of bidirectional entailment
is usually an implicit part of paraphrase generation. However, given the interesting
and diverse work that has been done in both these areas, we feel that any significant
discussion beyond the treatment above merits a separate, detailed survey.
3. Paraphrasing with Corpora
In this section, we explore in detail the data-driven paraphrase generation approaches
that have emerged and have become extremely popular in the last decade or so. These
corpus-based methods have the potential of covering a much wider range of paraphras-
ing phenomena and the advantage of widespread availability of corpora.
We organize this section by the type of corpora used to generate the paraphrases:
a single monolingual corpus, monolingual comparable corpora, monolingual parallel
corpora, and bilingual parallel corpora. This form of organization, in our opinion, is
the most instructive because most of the algorithmic decisions made for paraphrase
generation will depend heavily on the type of corpus used. For instance, it is reasonable
to assume that a different set of considerations will be paramount when using a large
single monolingual corpus than when using bilingual parallel corpora.
However, before delving into the actual paraphrasing methods, we believe that
it would be very useful to explain the motivation behind distributional similarity, an
extremely popular technique that can be used for paraphrase generation with several
different types of corpora. We devote the following section to such an explanation.
347
Computational Linguistics Volume 36, Number 3
3.1 Distributional Similarity
The idea that a language possesses distributional structure was first discussed at length
by Harris (1954). The term represents the notion that one can describe a language in
terms of relationships between the occurrences of its elements (words, morphemes,
phonemes) relative to the occurrence of other elements. The name for the phenomenon
is derived from an element?s distribution?sets of elements in particular positions that
the element occurs with to produce an utterance or a sentence.
More specifically, Harris presents several empirical observations to support the
hypothesis that such a structure exists naturally for language. Here, we closely quote
these observations:
 Utterances and sentences are not produced by arbitrarily putting together
the elements of the language. In fact, these elements usually occur only in
certain positions relative to certain other elements.
 The empirical restrictions on the co-occurrents of a class are respected for
each and every one of its members and are not disregarded for arbitrary
reasons.
 The occurrence of a member of a class relative to another member of a
different class can be computed as a probabilistic measure, defined in
terms of the frequency of that occurrence in some sample or corpus.
 Not every member of every class can occur with every member of another
class (think nouns and adjectives). This observation can be used as a
measure of difference in meaning. For example, if the pair of words teacher
and instructor is considered to be more semantically equivalent than, say,
the pair teacher and musician, then the distributions of the first pair will
also be more alike than that of the latter pair.
Given these observations, it is relatively easy to characterize the concept of distrib-
utional similarity: words or phrases that share the same distribution?the same set of
words in the same context in a corpus?tend to have similar meanings.
Figure 1 shows the basic idea behind phrasal paraphrase generation techniques that
leverage distributional similarity. The input corpus is usually a single or set of mono-
lingual corpora (parallel or non-parallel). After preprocessing?which may include
tagging the parts of speech, generating parse trees, and other transformations?the next
step is to extract pairs of words or phrases (or patterns) that occur in the same context in
the corpora and hence may be considered (approximately) semantically equivalent. This
extraction may be accomplished by several means (e.g., by using a classifier employing
contextual features or by finding similar paths in dependency trees). Although it is
possible to stop at this point and consider this list as the final output, the list usually
contains a lot of noise and may require additional filtering based on other criteria,
such as collocations counts from another corpus (or the Web). Finally, some techniques
may go even further and attempt to generalize the filtered list of paraphrase pairs
into templates or rules which may then be applied to other sentences to generate their
paraphrases. Note that generalization as a post-processing step may not be necessary if
the induction process can extract distributionally similar patterns directly.
One potential disadvantage of relying on distributional similarity is that items
that are distributionally similar may not necessarily end up being paraphrastic: Both
348
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Figure 1
A general architecture for paraphrasing approaches leveraging the distributional similarity
hypothesis.
elements of the pairs ?boys, girls?, ?cats, dogs?, ?high, low? can occur in similar contexts
but are not semantically equivalent.
3.2 Paraphrasing Using a Single Monolingual Corpus
In this section, we concentrate on paraphrase generation methods that operate on a
single monolingual corpus. Most, if not all, such methods usually perform paraphrase
induction by employing the idea of distributional similarity as outlined in the previous
section. Besides the obvious caveat discussed previously regarding distributional sim-
ilarity, we find that the other most important factor affecting the performance of these
methods is the choice of distributional ingredients?that is, the features used to formu-
late the distribution of the extracted units. We consider three commonly used techniques
that generate phrasal paraphrases (or paraphrastic patterns) from a single monolingual
corpus but use very different distributional features in terms of complexity. The first
uses only surface-level features and the other two use features derived from additional
semantic knowledge. Although the latter two methods are able to generate more so-
phisticated paraphrases by virtue of more specific and more informative ingredients,
we find that doing so usually has an adverse effect on their coverage.
Pas?ca and Dienes (2005) use as their input corpus a very large collection of Web
documents taken from the repository of documents crawled by Google. Although using
Web documents as input data does require a non-trivial pre-processing phase since such
documents tend to be noisier, there are certainly advantages to the use of Web docu-
ments as the input corpus: It does not require parallel (or even comparable) documents
349
Computational Linguistics Volume 36, Number 3
and can allow leveraging of even larger document collections. In addition, the extracted
paraphrases are not tied to any specific domain and are suitable for general application.
Algorithm 1 shows the details of the induction process. Steps 3?6 extract all n-grams
of a specific kind from each sentence: Each n-gram has Lc words at the beginning,
between M1 to M2 words in the middle, and another Lc words at the end. Steps 7?13
can intuitively be interpreted as constructing a textual anchor A?by concatenating a
fixed number of words from the left and the right?for each candidate paraphrase C
and storing the ?anchor, candidate? tuple in H. These anchors are taken to constitute
the distribution of the words and phrases under inspection. Finally, each occurrence of
a pair of potential paraphrases, that is, a pair sharing one or more anchors, is counted.
The final set of phrasal paraphrastic pairs is returned.
This algorithm embodies the spirit of the hypothesis of distributional similarity: It
considers all words and phrases that are distributionally similar?those that occur with
the same sets of anchors (or distributions)?to be paraphrases of each other. Addition-
ally, the larger the set of shared anchors for two candidate phrases, the stronger the like-
lihood that they are paraphrases of each other. After extracting the list of paraphrases,
less likely phrasal paraphrases are filtered out by using an appropriate count threshold.
Pas?ca and Dienes (2005) attempt to make their anchors even more informative by
attempting variants where they extract the n-grams only from sentences that include
specific additional information to be added to the anchor. For example, in one variant,
they only use sentences where the candidate phrase is surrounded by named entities
Algorithm 1 (Pas?ca and Dienes 2005). Induce a set of phrasal paraphrase pairs H with
associated counts from a corpus of pre-processed Web documents.
Summary. Extract all n-grams from the corpus longer than a pre-stipulated length.
Compute a lexical anchor for each extracted n-gram. Pairs of n-grams that share lexical
anchors are then construed to be paraphrases.
1: Let N represent a set of n-grams extracted from the corpus
2: N ? {?}, H ? {?}
3: for each sentence E in the corpus do
4: Extract the set of n-grams NE = {e?i s.t (2Lc + M1) ? |e?i| ? (2Lc + M2)}}, where
M1, M2, and Lc are all preset constants and M1 ? M2
5: N ? N ? NE
6: end for
7: for each n-gram e? in N do
8: Extract the subsequence C, such that Lc ? |C| ? (|e?| ? Lc ? 1)
9: Extract the subsequence AL, such that 0 ? |AL| ? (Lc ? 1)
10: Extract the subsequence AR, such that (|e?| ? Lc) ? |AR| ? (|e?| ? 1)
11: A ? AL + AR
12: Add the pair (A, C) to H
13: end for
14: for each subset of H with the same anchor A do
15: Exhaustively compare each pair of tuples (A, Ci) and (A, Cj) in this subset
16: Update the count of the candidate paraphrase pair (Ci, Cj) by 1
17: Update the count of the candidate paraphrase pair (Cj, Ci) by 1
18: end for
19: Output H containing paraphrastic pairs and their respective counts
350
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
on both sides and they attach the nearest pair of entities to the anchor. As expected, the
paraphrases do improve in quality as the anchors become more specific. However, they
also report that as anchors are made more specific by attaching additional information,
the likelihood of finding a candidate pair with the same anchor is reduced.
The ingredients for measuring distributional similarity in a single corpus can cer-
tainly be more complex than simple phrases used by Pas?ca and Dienes. Lin and Pantel
(2001) discuss how to measure distributional similarity over dependency tree paths in
order to induce generalized paraphrase templates such as:2
X found answer to Y ? X solved Y
X caused Y ? Y is blamed on X
Whereas single links between nodes in a dependency tree represent direct semantic
relationships, a sequence of links, or a path, can be understood to represent an indirect
relationship. Here, a path is named by concatenating the dependency relationships and
lexical items along the way but excluding the lexical items at the end. In this way, a
path can actually be thought of as a pattern with variables at either end. Consider the
first dependency tree in Figure 2. One dependency path that we could extract would be
between the node John and the node problem. We start at John and see that the first item
in the tree is the dependency relation subject that connects a noun to a verb and so we
append that information to the path.3 The next item in the tree is the word found and
we append its lemma (find) to the path. Next is the semantic relation object connecting a
verb to a noun and we append that. The process continues until we reach the other slot
(the word problem) at which point we stop.4 The extracted path is shown below the tree.
Similarly, we can extract a path for the second dependency tree. Let?s briefly mention
the terminology associated with such paths:
 The relations on either end of a path are referred to as SlotX and SlotY.
 The tuples (SlotX, John) and (SlotY, problem) are known as the two features
of the path.
 The dependency relations inside the path that are not slots are termed
internal relations.
Intuitively, one can imagine a path to be a complex representation of the pattern X finds
answer to Y, where X and Y are variables. This representation for a path is a perfect fit for
an extended version of the distributional similarity hypothesis: If similar sets of words
fill the same variables for two different patterns, then the patterns may be considered to
have similar meaning, which is indeed the case for the paths in Figure 2.
Lin and Pantel (2001) use newspaper text as their input corpus and create depen-
dency parses for all the sentences in the corpus in the pre-processing step. Algorithm 2
provides the details of the rest of the process: Steps 1 and 2 extract the paths and
compute their distributional properties, and Steps 3?14 extract pairs of paths which are
2 Technically, these templates represent inference rules, such that the right-hand side can be inferred from
the left-hand side but is not semantically equivalent to it. This form of inference is closely related to that
exhibited in textual entailment. This work is primarily concerned with inducing such rules rather than
strict paraphrases.
3 Although the first item is the word John, the words at either end are, by definition, considered slots and
not included in the path.
4 Any relations not connecting two content words, such as determiners and auxiliaries, are ignored.
351
Computational Linguistics Volume 36, Number 3
Figure 2
Two different dependency tree paths (a and b) that are considered paraphrastic because the same
words ( John and problem) are used to fill the corresponding slots (shown co-indexed) in both the
paths. The implied meaning of each dependency path is also shown.
similar, insofar as such properties are concerned.5 At the end, we have sets of paths (or
inference rules) that are considered to have similar meanings by the algorithm.
The performance of their dependency-path based algorithm depends heavily on the
root of the extracted path. For example, whereas verbs frequently tend to have several
modifiers, nouns tend to have no more than one. However, if a word has any fewer than
two modifiers, no path can go through it as the root. Therefore, the algorithm tends to
perform better for paths with verbal roots. Another issue is that this algorithm, despite
the use of more informative distributional features, can generate several incorrect or im-
plausible paraphrase patterns (inference rules). Recent work has shown how to filter out
incorrect inferences when using them in a downstream application (Pantel et al 2007).
Finally, there is no reason for the distributional features to be in the same language
as the one in which the paraphrases are desired. Wu and Zhou (2003) describe a
5 A demo of the algorithm is available online at http://demo.patrickpantel.com/Content/LexSem/
paraphrase.htm.
352
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Algorithm 2 (Lin and Pantel 2001). Produce inference rules from a parsed corpus.
Summary. Adapt Harris?s (1954) hypothesis of distributional similarity for paths in
dependency trees: If two tree paths have similar distributions such that they tend to
link the same set of words, then they likely mean the same thing and together generate
an inference rule.
1: Extract paths of the form described above from the parsed corpus
2: Initialize a hash H that stores, for each tuple of the form (p, s, w)?where p is a path,
s is one of the two slots in p, and w is a word that appears in that slot?the following
two quantities:
(a) A count C(p, s, w) indicating how many times word w appeared in slot
s in path p
(b) The mutual information I(p, s, w) indicating the strength of association
between slot s and word w in path p:
I(p, s, w) = log
(
C(p, s, w)
?
p?,w? C(p
?, s, w?)
?
w? C(p, s, w
?)
?
p? C(p
?, s, w)
)
3: for each extracted path p do
4: Find all instances (p, w1, w2) such that p connects the words w1 and w2
5: for each such instance do
6: Update C(p, SlotX, w1) and I(p, SlotX, w1) in H
7: Update C(p, SlotY, w2) and I(p, SlotY, w2) in H
8: end for
9: end for
10: for each extracted path p do
11: Create a candidate set C of similar paths by extracting all paths from H that share
at least one feature with p
12: Prune candidates from C based on feature overlap with p
13: Compute the similarity between p and the remaining candidates in C. The simi-
larity is defined in terms of the various values of mutual information I between
the paths? two slots and all the words that appear in those slots
14: Output all paths in C sorted by their similarity to p
15: end for
bilingual approach to extract English relation-based paraphrastic patterns of the form
?w1, R, w2?, where w1 and w2 are English words connected by a dependency link with
the semantic relation R. Figure 3 shows a simple example based on their approach. First,
instances of one type of pattern are extracted from a parsed monolingual corpus. In the
figure, for example, a single instance of the pattern ?verb, IN, pobj? has been extracted.
Several new, potentially paraphrastic, English candidate patterns are then induced by
replacing each of the English words with its synonyms in WordNet, one at a time. The
figure shows the list of induced patterns for the given example. Next, each of the English
words in each candidate pattern is translated to Chinese, via a bilingual dictionary.6
6 The semantic relation R is deemed to be invariant under translation.
353
Computational Linguistics Volume 36, Number 3
Figure 3
Using Chinese translations as the distributional elements to extract a set of English paraphrastic
patterns from a large English corpus.
Given that the bilingual dictionary may contain multiple Chinese translations for a
given English word, several Chinese patterns may be created for each English candidate
pattern. Each Chinese pattern is assigned a probability value via a simple bag-of-words
translation model (built from a small bilingual corpus) and a language model (trained
on a Chinese collocation database); all translated patterns, along with their probability
values, are then considered to be features of the particular English candidate pattern.
Any English pattern can subsequently be compared to another by computing cosine
similarity over their shared features. English collocation pairs whose similarity value
exceeds some threshold are construed to be paraphrastic.
The theme of a trade-off between the precision of the generated paraphrase set?by
virtue of the increased informativeness of the distributional features?and its coverage
is seen in this work as well. When using translations from the bilingual dictionary, a
knowledge-rich resource, the authors report significantly higher precision than compa-
rable methods that rely only on monolingual information to compute the distributional
similarity. Predictably, they also find that recall values obtained with their dictionary-
based method are lower than those obtained by other methods.
Paraphrase generation techniques using a single monolingual corpus have to rely
on some form of distributional similarity because there are no explicit clues available
that indicate semantic equivalence. In the next section, we look at paraphrasing methods
operating over data that does contain such explicit clues.
3.3 Paraphrasing Using Monolingual Parallel Corpora
It is also possible to generate paraphrastic phrase pairs from a parallel corpus where
each component of the corpus is in the same language. Obviously, the biggest advantage
of parallel corpora is that the sentence pairs are paraphrases almost by definition; they
represent different renderings of the same meaning created by different translators
making different lexical choices. In effect, they contain pairs (or sets) of sentences
354
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
that are either semantically equivalent (sentential paraphrases) or have significant se-
mantic overlap. Extraction of phrasal paraphrases can then be effected by extracting
phrasal correspondences from a set of sentences that represent the same (or similar)
semantic content. We present four techniques in this section that generate paraphrases
by finding such correspondences. The first two techniques attempt to do so by relying,
again, on the paradigm of distributional similarity: one by positing a bootstrapping
distributional similarity algorithm and the other by simply adapting the previously
described dependency path similarity algorithm to work with a parallel corpus. The
next two techniques rely on more direct, non-distributional methods to compute the
required correspondences.
Barzilay and McKeown (2001) align phrasal correspondences by attempting to
move beyond a single-pass distributional similarity method. They propose a bootstrap-
ping algorithm that allows for the gradual refinement of the features used to determine
similarity and yields improved paraphrase pairs. As their input corpus, they use mul-
tiple human-written English translations of literary texts such as Madame Bovary and
Twenty Thousand Leagues Under the Sea that are expected to be rich in paraphrastic ex-
pressions because different translators would use their own words while still preserving
the meaning of the original text. The parallel components are obtained by performing
sentence alignment (Gale and Church 1991) on the corpora to obtain sets of parallel
sentences that are then lemmatized, part-of-speech tagged and chunked in order to
identify all the verb and noun phrases. The bootstrapping algorithm is then employed
to incrementally learn better and better contextual features that are then leveraged to
generate semantically similar phrasal correspondences.
Figure 4 shows the basic steps of the algorithm. To seed the algorithm, some fake
paraphrase examples are extracted by using identical words from either side of the
aligned sentence pair. For example, given the following sentence pair:
S1: Emma burst into tears and he tried to comfort her.
S2: Emma cried and he tried to console her.
Figure 4
A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora.
355
Computational Linguistics Volume 36, Number 3
?tried, tried?, ?her, her? may be extracted as positive examples and ?tried, Emma?, ?tried,
console? may be extracted as negative examples. Once the seeding examples are ex-
tracted, the next step is to extract contextual features for both the positive and the
negative examples. These features take the form of aligned part-of-speech sequences of
a given length from the left and the right of the example. For instance, we can extract the
contextual feature [?L1 : PRP1, R1 : TO1?, ?L2 : PRP1, R2 : TO1?] of length 1 for the positive
example ?tried, tried?. This particular contextual feature contains two tuples, one for
each sentence. The first tuple ?L1 : PRP1, R1 : TO1? indicates that, in the first sentence, the
POS tag sequence to the left of the word tried is a personal pronoun (he) and the POS
tag sequence to the right of tired is the preposition to. The second tuple is identical for
this case. Note that the tags of identical tokens are indicated as such by subscripts on the
POS tags. All such features are extracted for both the positive and the negative examples
for all lengths less than or equal to some specified length. In addition, a strength value
is calculated for each positive (negative) contextual feature f using maximum likelihood
estimation as follows:
strength( f ) =
Number of positive (negative) examples surrounded by f
Total occurrences of f
The extracted list of contextual features is thresholded on the basis of this strength
value. The remaining contextual rules are then applied to the corpora to obtain addi-
tional positive and negative paraphrase examples that, in turn, lead to more refined
contextual rules, and so on. The process is repeated for a fixed number of iterations or
until no new paraphrase examples are produced. The list of extracted paraphrases at
the end of the final iteration represents the final output of the algorithm. In total, about
9, 000 phrasal (including lexical) paraphrases are extracted from 11 translations of five
works of classic literature. Furthermore, the extracted paraphrase pairs are also gener-
alized into about 25 patterns by extracting part-of-speech tag sequences corresponding
to the tokens of the paraphrase pairs.
Barzilay and McKeown also perform an interesting comparison with another tech-
nique that was originally developed for compiling translation lexicons from bilingual
parallel corpora (Melamed 2001). This technique first compiles an initial lexicon using
simple co-occurrence statistics and then uses a competitive linking algorithm (Melamed
1997) to improve the quality of the lexicon. The authors apply this technique to their
monolingual parallel data and observe that the extracted paraphrase pairs are of much
lower quality than the pairs extracted by their own method. We present similar obser-
vations in Section 3.5 and highlight that although more recent translation techniques?
specifically ones that use phrases as units of translation?are better suited to the task
of generating paraphrases than the competitive linking approach, they continue to
suffer from the same problem of low precision. On the other hand, such techniques
can take good advantage of large bilingual corpora and capture a much larger variety
of paraphrastic phenomena.
Ibrahim, Katz, and Lin (2003) propose an approach that applies a modified version
of the dependency path distributional similarity algorithm proposed by Lin and Pantel
(2001) to the same monolingual parallel corpus (multiple translations of literary works)
used by Barzilay and McKeown (2001). The authors claim that their technique is more
tractable than Lin and Pantel?s approach since the sentence-aligned nature of the input
parallel corpus obviates the need to compute similarity over tree paths drawn from
sentences that have zero semantic overlap. Furthermore, they also claim that their
technique exploits the parallel nature of a corpus more effectively than Barzilay and
356
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
McKeown?s approach simply because their technique uses tree paths and not just lexical
information. Specifically, they propose the following modifications to Lin and Pantel?s
algorithm:
1. Extracting tree paths with aligned anchors. Rather than using a single
corpus and comparing paths extracted from possibly unrelated sentences,
the authors leverage sentence-aligned monolingual parallel corpora; the
same as used in Barzilay and McKeown (2001). For each sentence in an
aligned pair, anchors are identified. The anchors from both sentences are
brought into alignment. Once anchor pairs on either side have been
identified and aligned, a breadth-first search algorithm is used to find the
shortest path between the anchor nodes in the dependency trees. All paths
found between anchor pairs for a sentence pair are taken to be
distributionally?and, hence, semantically?similar.
2. Using a sliding frequency measure. The original dependency-based
algorithm (Lin and Pantel 2001) weights all subsequent occurrences of the
same paraphrastic pair of tree paths as much as the first one. In this
version, every successive induction of a paraphrastic pair using the same
anchor pair is weighted less than the previous one. Specifically, inducing
the same paraphrase pair using an anchor pair that has already been seen
only counts for 12n , where n is the number of times the specific anchor pair
has been seen so far. Therefore, induction of a path pair using new anchors
is better evidence that the pair is paraphrastic, as opposed to the repeated
induction of the path pair from the same anchor over and over again.
Despite the authors? claims, they offer no quantitative evaluation comparing their
paraphrases with those from Lin and Pantel (2001) or from Barzilay and McKeown
(2001).
It is also possible to find correspondences between the parallel sentences using a
more direct approach instead of relying on distributional similarity. Pang, Knight, and
Marcu (2003) propose an algorithm to align sets of parallel sentences driven entirely
by the syntactic representations of the sentences. The alignment algorithm outputs a
merged lattice from which lexical, phrasal, and sentential paraphrases can simply be
read off. More specifically, they use the Multiple-Translation Chinese corpus that was
originally developed for machine translation evaluation and contains 11 human-written
English translations for each sentence in a news document. Using several sentences
explicitly equivalent in semantic content has the advantage of being a richer source
for paraphrase induction.
As a pre-processing step, any group (of 11 sentences) that contains sentences longer
than 45 words is discarded. Next, each sentence in each of the groups is parsed. All
the parse trees are then iteratively merged into a shared forest. The merging algo-
rithm proceeds top-down and continues to recursively merge constituent nodes that
are expanded identically. It stops upon reaching the leaves or upon encountering the
same constituent node expanded using different grammar rules. Figure 5(a) shows
how the merging algorithm would work on two simple parse trees. In the figure, it
is apparent that the leaves of the forest encode paraphrasing information. However,
the merging only allows identical constituents to be considered as paraphrases. In
addition, keyword-based heuristics need to be employed to prevent inaccurate merging
of constituent nodes due to, say, alternations of active and passive voices among the
357
Computational Linguistics Volume 36, Number 3
Figure 5
The merging algorithm. (a) How the merging algorithm works for two simple parse trees to
produce a shared forest. Note that for clarity, not all constituents are expanded fully. Leaf nodes
with two entries represent paraphrases. (b) The word lattice generated by linearizing the forest
in (a).
sentences in the group. Once the forest is created, it is linearized to create the word
lattice by traversing the nodes in the forest top-down and producing an alternative
path in the lattice for each merged node. Figure 5(b) shows the word lattice generated
for the simple two-tree forest. The lattices also require some post-processing to remove
redundant edges and nodes that may have arisen due to parsing errors or limitations in
the merging algorithm. The final output of the paraphrasing algorithm is a set of word
lattices, one for each sentence group.
These lattices can be used as sources of lexical as well as phrasal paraphrases. All
alternative paths between any pair of nodes can be considered to be paraphrases of
each other. For example, besides the obvious lexical paraphrases, the paraphrase pair
?ate at cafe, chowed down at bistro? can also be extracted from the lattice in Figure 5(b).
In addition, each path between the START and the END nodes in the lattice represents a
sentential paraphrase of the original 11 sentences used to create the lattice.
The direct alignment approach is able to leverage the sheer width (number of
parallel alternatives per sentence position; 11 in this case) of the input corpus to do
away entirely with any need for measuring distributional similarity. In general, it has
several advantages. It can capture a very large number of paraphrases: Each lattice has
on the order of hundreds or thousands of paths depending on the average length of
the sentence group that it was generated from. In addition, the paraphrases produced
are of better quality than other approaches employing parallel corpora for paraphrase
induction discussed so far. However, the approach does have a couple of drawbacks:
 No paraphrases for unseen data. The lattices cannot be applied to new
sentences for generating paraphrases because no form of generalization
is performed to convert lattices into patterns.
358
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
 Requirement of a large number of human-written translations. Each of
the lattices described is built using 11 manually written translations of the
same sentence, each by a different translator. There are very few corpora
that provide such a large number of human translations. In recent years,
most MT corpora have had no more than four references, which would
certainly lead to much sparser word lattices and smaller numbers of
paraphrases that can be extracted. In fact, given the cost and amount of
effort required for humans to translate a relatively large corpus, it is
common to encounter corpora with only a single human translation.
With such a corpus, of course, this technique would be unable to produce
any paraphrases. One solution might be to augment the relatively few
human translations with translations obtained from automatic machine
translation systems. In fact, the corpus used (Huang, Graff, and
Doddington 2002) also contains, besides the 11 human translations,
6 translations of the same sentence by machine translation systems
available on the Internet at the time. However, no experiments are
performed with the automatic translations.
Finally, an even more direct method to align equivalences in parallel sentence pairs
can be effected by building on word alignment techniques from the field of statistical
machine translation (Brown et al 1990). Current state-of-the-art SMT methods rely on
unsupervised induction of word alignment between two bilingual parallel sentences to
extract translation equivalences that can then be used to translate a given sentence in
one language into another language. The same methods can be applied to monolingual
parallel sentences without any loss of generality. Quirk, Brockett, and Dolan (2004)
use one such method to extract phrasal paraphrase pairs. Furthermore, they use these
extracted phrasal pairs to construct sentential paraphrases for new sentences.
Mathematically, Quirk, Brockett, and Dolan?s approach to sentential paraphrase
generation may be expressed in terms of the typical channel model equation for
statistical machine translation:
E?p = arg max
Ep
P(Ep|E) (1)
The equation denotes the search for the optimal paraphrase Ep for a given sentence E.
We may use Bayes? Theorem to rewrite this as:
E?p = arg max
Ep
P(Ep) P(E|Ep)
where P(Ep) is an n-gram language model providing a probabilistic estimate of the
fluency of a hypothesis Ep and P(E|Ep) is the translation model, or more appropriately
for paraphrasing, the replacement model, providing a probabilistic estimate of what is
essentially the semantic adequacy of the hypothesis paraphrase. Therefore, the optimal
sentential paraphrase may loosely be described as one that fluently captures most, if
not all, of the meaning contained in the input sentence.
It is important to provide a brief description of the parallel corpus used here because
unsupervised induction of word alignments typically requires a relatively large number
of parallel sentence pairs. The monolingual parallel corpus (or more accurately, quasi-
parallel, since not all sentence pairs are fully semantically equivalent) is constructed
by scraping on-line news sites for clusters of articles on the same topic. Such clusters
359
Computational Linguistics Volume 36, Number 3
contain the full text of each article and the dates and times of publication. After re-
moving the mark-up, the authors discard any pair of sentences in a cluster where the
difference in the lengths or the edit distance is larger than some stipulated value. This
method yields a corpus containing approximately 140, 000 quasi-parallel sentence pairs
{(E1, E2)}, where E1 = e11e21 . . . em1 , E2 = e12e22 . . . en2. The following examples show that the
proposed method can work well:
S1: In only 14 days, U.S. researchers have created an artificial bacteria-eating virus
from synthetic genes.
S2: An artificial bacteria-eating virus has been made from synthetic genes in the
record time of just two weeks.
S1: The largest gains were seen in prices, new orders, inventories, and exports.
S2: Sub-indexes measuring prices, new orders, inventories, and exports increased.
For more details on the creation of this corpus, we refer the reader to Dolan, Quirk,
and Brockett (2004) and, more specifically, to Section 4. Algorithm 3 shows how to
Algorithm 3 (Quirk, Dolan, and Brockett 2004). Generate a set M of phrasal para-
phrases with associated likelihood values from a monolingual parallel corpus C.
Summary. Estimate a simple English to English phrase translation model from C using
word alignments. Use this model to create sentential paraphrases as explained later.
1: M ? {?}
2: Compute lexical replacement probabilities P(e1|e2) from all sentence pairs in C via
IBM Model 1 estimation
3: Compute a set of word alignments {a} such that for each sentence pair (E1, E2)
a = a1a2 . . . am
where ai ? {0 . . . n}, m = |E1|, n = |E2|
4: for each word-aligned sentence pair (E1, E2)a in C do
5: Extract pairs of contiguous subsequences (e?1, e?2) such that:
(a) |e?1| ? 5, |e?2| ? 5
(b) ?i ? {1, . . . , |e?1|} ?j ? {1, . . . , |e?2|}, e1,i
a? e2,j
(c) ?i ? {1, . . . , |e?2|} ?j ? {1, . . . , |e?1|}, e2,i
a? e1,j
6: Add all extracted pairs to M
7: end for
8: for each paraphrase pair (e?1, e?2) in M do
9: Compute P(e?1|e?2) =
?
e j1?e?1
?
ek2?e?2
P(e j1|ek2)
10: end for
11: Output M containing paraphrastic pairs and associated probabilities
360
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
generate a set of phrasal paraphrase pairs and compute a probability value for each
such pair. In Step 2, a simple parameter estimation technique (Brown et al 1993) is used
to compute, for later use, the probability of replacing any given word with another.
Step 3 computes a word alignment (indicated by a) between each pair of sentences. This
alignment indicates for each word ei in one string that word ej in the other string from
which it was most likely produced (denoted here by ei
a? ej). Steps 4?7 extract, from each
pair of sentences, pairs of short contiguous phrases that are aligned with each other
according to this alignment. Note that each such extracted pair is essentially a phrasal
paraphrase. Finally, a probability value is computed for each such pair by assuming that
each word of the first phrase can be replaced with each word of the second phrase. This
computation uses the lexical replacement probabilities computed in Step 2.
Now that a set of scored phrasal pairs has been extracted, these pairs can be used to
generate paraphrases for any unseen sentence. Generation proceeds by creating a lattice
for the given sentence. Given a sentence E, the lattice is populated as follows:
1. Create |E| + 1 vertices q0, q1 . . . q|E|.
2. Create N edges between each pair of vertices qj and qk ( j < k) such that N =
the number of phrasal paraphrases for the input phrase e( j+1)e( j+2) . . . ek.
Label each edge with the phrasal paraphrase string itself and its
probability value. Each such edge denotes a possible paraphrasing of the
above input phrase by the replacement model.
3. Add the edges {(qj?1, qj)} and label each edge with the token sj and a
constant u. This is necessary to handle words from the sentence that do
not occur anywhere in the set of paraphrases.
Figure 6 shows an example lattice. Once the lattice has been constructed, it is straight-
forward to extract the 1-best paraphrase by using a dynamic programming algorithm
such as Viterbi decoding and extracting the optimal path from the lattice as scored by the
product of an n-gram language model and the replacement model. In addition, as with
SMT decoding, it is also possible to extract a list of n-best paraphrases from the lattice
by using the appropriate algorithms (Soong and Huang 1990; Mohri and Riley 2002).
Quirk, Brockett, and Dolan (2004) borrow from the statistical machine translation
literature so as to align phrasal equivalences as well as to utilize the aligned phrasal
equivalences to rewrite new sentences. The biggest advantage of this method is its
SMT inheritance: It is possible to produce multiple sentential paraphrases for any new
Figure 6
A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris. Alternate paths
between various nodes represent phrasal replacements. The probability values associated with
each edge are not shown for the sake of clarity.
361
Computational Linguistics Volume 36, Number 3
sentence, and there is no limit on the number of sentences that can be paraphrased.7
However, there are certain limitations:
 Monotonic Translation. It is assumed that a phrasal replacement will
occur in the exact same position in the output sentence as that of the
original phrase in the input sentence. In other words, reorderings of
phrasal units are disallowed.
 Naive Parameter Estimation. Using a bag-of-words approach to
parameter estimation results in a relatively uninformative probability
distribution over the phrasal paraphrases.
 Reliance on edit distance. Relying on edit distance to build the training
corpus of quasi-parallel sentences may exclude sentences that do exhibit a
paraphrastic relationship but differ significantly in constituent orderings.
All of these limitations combined lead to paraphrases that, although grammatically
sound, contain very little variety. Most sentential paraphrases that are generated involve
little more than simple substitutions of words and short phrases. In Section 3.5, we will
discuss other approaches that also find inspiration from statistical machine translation
and attempt to circumvent the above limitations by using a bilingual parallel corpus
instead of a monolingual parallel corpus.
3.4 Paraphrasing Using Monolingual Comparable Corpora
Whereas it is clearly to our advantage to have monolingual parallel corpora, such
corpora are usually not very readily available. The corpora usually found in the real
world are comparable instead of being truly parallel: Parallelism between sentences is
replaced by just partial semantic and topical overlap at the level of documents. There-
fore, for monolingual comparable corpora, the task of finding phrasal correspondences
becomes harder because the two corpora may only be related by way of describing
events under the same topic. In such a scenario, possible paraphrasing methods either
(a) forgo any attempts at directly finding such correspondences and fall back to the
distributional similarity workhorse or, (b) attempt to directly induce a form of coarse-
grained alignment between the two corpora and leverage this alignment.
In this section, we describe three methods that generate paraphrases from such
comparable corpora. The first method falls under category (a): Here the elements whose
distributional similarity is being measured are paraphrastic patterns and the distri-
butions themselves are the named entities with which the elements occur in various
sentences. In contrast, the next two methods fall under category (b) and attempt to
directly discover correspondences between two comparable corpora by leveraging a
novel alignment algorithm combined with some similarity heuristics. The difference
between the two latter methods lies only in the efficacy of the alignment algorithm.
Shinyama et al (2002) use two sets of 300 news articles from two different Japanese
newspapers from the same day as their source of paraphrases. The comparable nature of
the articles is ensured because both sets are from the same day. During pre-processing,
7 However, if no word in the input sentence has been observed in the parallel corpus, the paraphraser
simply reproduces the original sentence as the paraphrase.
362
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
all named entities in each article are tagged and dependency parses are created for each
sentence in each article. The distributional similarity driven algorithm then proceeds as
follows:
1. For each article in the first set, find the most ?similar? article from the
other set, based on a similarity measure computed over the named
entities appearing in the two articles.
2. From each sentence in each such pair of articles, extract all dependency
tree paths that contain at least one named entity and generalize them
into patterns wherein the named entities have been replaced with
variables. Each class of named-entity (e.g., Organization, Person,
Location) gets its own variable. For example, the following sentence:8
Vice President Kuroda of Nihon Yamamura Glass Corp. was promoted to
President.
may give us the following two patterns, among others:
?PERSON? of ?ORGANIZATION? was promoted
?PERSON? was promoted to ?POST?
3. Find all sentences in the two newswire corpora that match these
patterns. When a match is found, attach the pattern to the sentence
and link all variables to the corresponding named entities in the
sentences.
4. Find all sentences that are most similar to each other (above some preset
threshold), again based on the named entities they share.
5. For each pair of similar sentences, compare their respective attached
patterns. If the variables in the patterns link to the same or comparable
named entities (based on the entity text and type), then consider the
patterns to be paraphrases of each other.
At the end, the output is a list of generalized paraphrase patterns with named entity
types as variables. For example, the algorithm may generate the following two patterns
as paraphrases:
?PERSON? is promoted to ?POST?
the promotion of ?PERSON? to ?POST? is decided
As a later refinement, Sekine (2005) makes a similar attempt at using distributional
similarity over named entity pairs in order to produce a list of fully lexicalized phrasal
paraphrases for specific concepts represented by keywords.
The idea of enlisting named entities as proxies for detecting semantic equivalence is
interesting and has certainly been explored before (see the discussion regarding Pas?ca
and Dienes [2005] in Section 3.2). However, it has some obvious disadvantages. The
authors manually evaluate the technique by generating paraphrases for two specific
8 Although the authors provide motivating examples in Japanese (transliterated into romaji) in their paper,
we choose to use English here for the sake of clarity.
363
Computational Linguistics Volume 36, Number 3
domains (arrest events and personnel hirings) and find that while the precision is
reasonably good, the coverage is very low primarily due to restrictions on the patterns
that may be extracted in Step 2. In addition, if the average number of entities in
sentences is low, the likelihood of creating incorrect paraphrases is confirmed to be
higher.
Let us now consider the altogether separate idea of deriving coarse-grained corre-
spondences by leveraging the comparable nature of the corpora. Barzilay and Lee (2003)
attempt to do so by generating compact sentence clusters in template form (stored as
word lattices with slots) separately from each corpora and then pairing up templates
from one corpus with those from the other. Once the templates are paired up, a new
incoming sentence that matches one member of a template pair gets rendered as the
other member, thereby generating a paraphrase. They use as input a pair of corpora:
the first (C1) consisting of clusters of news articles published by Agence France Presse
(AFP) and the second (C2) consisting of those published by Reuters. The two corpora
may be considered comparable since the articles in each are related to the same topic
and were published during the same time frame.
Algorithm 4 shows some details of how their technique works. Steps 3?18 show
how to cluster topically related sentences, construct a word lattice from such a cluster,
and convert that into a slotted lattice?basically a word lattice with certain nodes recast
as variables or empty slots. The clustering is done so as to bring together sentences
pertaining to the same topics and having similar structure. The word lattice is the prod-
uct of an algorithm that computes a multiple-sequence alignment (MSA) for a cluster
of sentences (Step 6). A very brief outline of such an algorithm, originally developed
to compute an alignment for a set of three or more protein or DNA sequences, is as
follows:9
1. Find the most similar pair of sentences in the cluster according to a
similarity scoring function. For this approach, a simplified version of
the edit-distance measure (Barzilay and Lee 2002) is used.
2. Align this sentence pair and replace the pair with this single alignment.
3. Repeat until all sentences have been aligned together.
The word lattice so generated now needs to be converted into a slotted lattice to allow
its use as a paraphrase template. Slotting is performed based on the following intuition:
Areas of high variability between backbone nodes, that is, several distinct parallel paths,
may correspond to template arguments and can be collapsed into one slot that can be
filled by these arguments. However, multiple parallel paths may also appear in the
lattice because of simple synonymy and those paths must be retained for paraphrase
generation to be useful. To differentiate between the two cases, a synonymy threshold s
of 30% is used, as shown in Steps 8?14. The basic idea behind the threshold is that as the
number of sentences increases, the number of different arguments will increase faster
than the number of synonyms. Figure 7 shows how a very simple word lattice may be
generalized into a slotted lattice.
Once all the slotted lattices have been constructed for each corpus, Steps 19?24
try to match the slotted lattices extracted from one corpus to those extracted from the
other by referring back to the sentence clusters from which the original lattices were
9 For more details on MSA algorithms, refer to Gusfield (1997) and Durbin et al (1998).
364
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Algorithm 4 (Barzilay and Lee 2003). Generate set M of matching lattice pairs given a
pair of comparable corpora C1 and C2.
Summary. Gather topically related sentences from C1 into clusters. Do the same for C2.
Convert each sentence cluster into a slotted lattice using a multiple-sequence alignment
(MSA) algorithm. Compare all lattice pairs and output those likely to be paraphrastic.
1: Let WC1 and WC2 represent word lattices obtained from C1 and C2, respectively
2: M ? {?}, WC1 ? {?}, WC2 ? {?}
3: for each input corpus Ci ? {C1, C2} do
4: Create a set of clusters GCi = {GCi,k} of sentences based on n-gram overlap such
that all sentences in a cluster describe the same kinds of events and share similar
structure
5: for each cluster GCi,k do
6: Compute an MSA for all sentences in GCi,k by using a pre-stipulated scoring
function and represent the output as a word lattice WCi,k
7: Compute the set of backbone nodes Bk for WCi,k, that is, the nodes that are
shared by a majority (?50%) of the sentences in GCi,k
8: for each backbone node b ? Bk do
9: if no more than 30% of all the edges from b lead to the same node then
10: Replace all nodes adjacent to b with a single slot
11: else
12: Delete any node with < 30% of the edges from b leading to it and preserve
the rest
13: end if
14: end for
15: Merge any consecutive slot nodes into a single slot
16: WCi ? WCi ? {WCi,k}
17: end for
18: end for
19: for each lattice pair (WC1,j, WC2,k) ? WC1 ? WC2 do
20: Inspect clusters GC1,j and GC2,k and compare slot fillers in the cross-corpus
sentence pairs written on the same day
21: if comparison score > a pre-stipulated threshold ? then
22: M ? M ? {(WC1,j, WC2,k)}
23: end if
24: end for
25: Output M containing paraphrastic lattice pairs with linked slots
generated, comparing the sentences that were written on the same day and computing
a comparison score based on overlap between the sets of arguments that fill the slots. If
this computed score is greater than some fixed threshold value ?, then the two lattices
(or patterns) are considered to be paraphrases of each other.
Besides generating pairs of paraphrastic patterns, the authors go one step further
and actually use the patterns to generate paraphrases for new sentences. Given such
a sentence S, the first step is to find an existing slotted lattice from either corpus that
aligns best with S, in terms of the previously mentioned alignment scoring function.
If some lattice is found as a match, then all that remains is to take all corresponding
lattices from the other corpus that are paired with this lattice and use them to create
365
Computational Linguistics Volume 36, Number 3
Figure 7
An example showing the generalization of the word lattice (a) into a slotted lattice (b). The word
lattice is produced by aligning seven sentences. Nodes having in-degrees > 1 occur in more than
one sentence. Nodes with thick incoming edges occur in all sentences.
multiple rewritings (paraphrases) for S. Rewriting in this context is a simple matter of
substitution: For each slot in the matching lattice, we know not only the argument from
the sentence that fills it but also the slot in the corresponding rewriting lattice.
As far as the quality of acquired paraphrases is concerned, this approach easily out-
performs almost all other sentential paraphrasing approaches surveyed in this article.
However, a paraphrase is produced only if the incoming sentence matches some existing
template, which leads to a strong bias favoring quality over coverage. In addition,
the construction and generalization of lattices may become computationally expensive
when dealing with much larger corpora.
We can also compare and contrast Barzilay and Lee?s work and the work from
Section 3.3 that seems most closely related: that of Pang, Knight, and Marcu (2003).
Both take sentences grouped together in a cluster and align them into a lattice using a
particular algorithm. Pang, Knight, and Marcu have a pre-defined size for all clusters
since the input corpus is an 11-way parallel corpus. However, Barzilay and Lee have to
construct the clusters from scratch because their input corpus has no pre-defined notion
of parallelism at the sentence level. Both approaches use word lattices to represent and
induce paraphrases since a lattice can efficiently and compactly encode n-gram similar-
ities (sets of shared overlapping word sequences) between a large number of sentences.
However, the two approaches are also different in that Pang, Knight, and Marcu use the
parse trees of all sentences in a cluster to compute the alignment (and build the lattice),
whereas Barzilay and Lee use only surface level information. Furthermore, Barzilay
and Lee can use their slotted lattice pairs to generate paraphrases for novel and unseen
sentences, whereas Pang, Knight, and Marcu cannot paraphrase new sentences at all.
366
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Shen et al (2006) attempt to improve Barzilay and Lee?s technique by trying to
include syntactic constraints in the cluster alignment algorithm. In that way, it is doing
something similar to what Pang, Knight, and Marcu do but with a comparable corpus
instead of a parallel one. More precisely, whereas Barzilay and Lee use a relatively
simple alignment scoring function based on purely lexical features, Shen et al try to
bring syntactic features into the mix. The motivation is to constrain the relatively free
nature of the alignment generated by the MSA algorithm?which may lead to the gen-
eration of grammatically incorrect sentences?by using informative syntactic features.
In their approach, even if two words are a lexical match?as defined by Barzilay and
Lee (2003)?they are further inspected in terms of certain pre-defined syntactic features.
Therefore, when computing the alignment similarity score, two lexically matched words
across a sentence pair are not considered to fully match unless their score on syntactic
features also exceeds a preset threshold.
The syntactic features constituting the additional constraints are defined in terms
of the output of a chunk parser. Such a parser takes as input the syntactic trees
of the sentences in a topic cluster and provides the following information for each
word:
 Part-of-speech tag
 IOB tag. This is a notation denoting the constituent covering a word
and its relative position in that constituent (Ramshaw and Marcus 1995).
If a word has the tag I-NP, we can infer that the word is covered by an
NP and located inside that NP. Similarly, B denotes that the word is at
the beginning and O denotes that the word is not covered by any
constituent.
 IOB chain. A concatenation of all IOB tags going from the root of the tree
to the word under consideration.
With this information and a heuristic to compute the similarity between two words
in terms of their POS and IOB tags, the alignment similarity score can be calculated
as the sum of the heuristic similarity value for the given two words and the heuristic
similarity values for each corresponding node in the two IOB chains. If this score is
higher than some threshold and the two words have similar positions in their respective
sentences, then the words are considered to be a match and can be aligned. Given this
alignment algorithm, the word lattice representing the global alignment is constructed
in an iterative manner similar to the MSA approach.
Shen et al (2006) present evidence from a manual evaluation that sentences sam-
pled from lattices constructed via the syntactically informed alignment method receive
higher grammaticality scores as compared to sentences from the lattices constructed via
the purely lexical method. However, they present no analysis of the actual paraphrasing
capacity of their, presumably better aligned, lattices. Indeed, they explicitly mention that
their primary goal is to measure the correlation between the syntax-augmented scoring
function and the correctness of the sentences being generated from such lattices, even
if the sentences do not bear a paraphrastic relationship to the input. Even if one were
to assume that the syntax-based alignment method would result in better paraphrases,
it still would not address the primary weakness of Barzilay and Lee?s method: Para-
phrases are only generated for new sentences that match an already existing lattice,
leading to lower coverage.
367
Computational Linguistics Volume 36, Number 3
3.5 Paraphrasing Using Bilingual Parallel Corpora
In the last decade, there has been a resurgence in research on statistical machine transla-
tion. There has also been an accompanying dramatic increase in the number of available
bilingual parallel corpora due to the strong interest in SMT from both the public and
private sectors. Recent research in paraphrase generation has attempted to leverage
these very large bilingual corpora. In this section, we look at such approaches that rely
on the preservation of meaning across languages and try to recover said meaning by
using cues from the second language.
Using bilingual parallel corpora for paraphrasing has the inherent advantage that
sentences in the other language are exactly semantically equivalent to sentences in
the intended paraphrasing language. Therefore, the most common way to generate
paraphrases with such a corpus exploits both its parallel and bilingual natures: Align
phrases across the two languages and consider all co-aligned phrases in the intended
language to be paraphrases. The bilingual phrasal alignments can simply be generated
by using the automatic techniques developed for the same task in the SMT literature.
Therefore, arguably the most important factor affecting the performance of these
techniques is usually the quality of the automatic bilingual phrasal (or word) alignment
techniques.
One of the most popular methods leveraging bilingual parallel corpora is that
proposed by Bannard and Callison-Burch (2005). This technique operates exactly as
described above by attempting to infer semantic equivalence between phrases in the
same language indirectly with the second language as a bridge. Their approach builds
on one of the initial steps used to train a phrase-based statistical machine translation
system (Koehn, Och, and Marcu 2003). Such systems rely on phrase tables?a tabulation
of correspondences between phrases in the source language and phrases in the target
language. These tables are usually extracted by inducing word alignments between
sentence pairs in a training corpus and then incrementally building longer phrasal
correspondences from individual words and shorter phrases. Once such a tabulation of
bilingual phrasal correspondences is available, correspondences between phrases in one
language may be inferred simply by using the phrases in the other language as pivots.
Algorithm 5 shows how monolingual phrasal correspondences are extracted from
a bilingual corpus C by using word alignments. Steps 3?7 extract bilingual phrasal
correspondences from each sentence pair in the corpus by using heuristically induced
bidirectional word alignments. Figure 8 illustrates this extraction process for two exam-
ple sentence pairs. For each pair, a matrix shows the alignment between the Chinese
and the English words. Element (i, j) of the matrix is filled if there is an alignment link
between the ith Chinese word and the jth English word ej. All phrase pairs consistent
with the word alignment are then extracted. A consistent phrase pair can intuitively
be thought of as a sub-matrix where all alignment points for its rows and columns are
inside it and never outside. Next, Steps 8?11 take all English phrases that correspond
to the same foreign phrase and infer them all to be paraphrases of each other.10 For
example, the English paraphrase pair ?effectively contained, under control? is obtained
from Figure 8 by pivoting on the Chinese phrase , shown underlined for both
matrices.
10 Note that it would have been equally easy to pivot on the English side and generate paraphrases in the
other language instead.
368
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Algorithm 5 (Bannard and Callison-Burch 2005). Generate set M of monolingual para-
phrase pairs given a bilingual parallel corpus C.
Summary. Extract bilingual phrase pairs from C using word alignments and standard
SMT heuristics. Pivot all pairs of English phrases on any shared foreign phrases and
consider them paraphrases. The alignment notation from Algorithm 3 is employed.
1: Let B represent the bilingual phrases extracted from C
2: B ? {?}, M ? {?}
3: Compute a word alignment a for each sentence pair (E, F) ? C
4: for each aligned sentence pair (E, F)a do
5: Extract the set of bilingual phrasal correspondences {(e?, f? )} such that:
(a) ?ei ? e? : ei
a? fj ? fj ? f? , and
(a) ?fj ? f? : fj
a? ei ? ei ? e?
6: B ? B ? {(e?, f? )}
7: end for
8: for each member of the set {?(e?j, f?k), (e?l, f?m)? s.t. (e?j, f?k) ? B
? (e?l, ?fm) ? B
? f?k = ?fm} do
9: M ? M ? {(e?j, e?l)}
10: Compute p(e?j|e?l) =
?
f? p(e?j| f? )p( f? |e?l)
11: end for
12: Output M containing paraphrastic pairs and associated probabilities
Using the components of a phrase-based SMT system also makes it easy to assign a
probability value to any of the inferred paraphrase pairs as follows:
p(e?j|e?k) =
?
f?
p(e?j, f? |e?k) ?
?
f?
p(e?j| f? )p( f? |e?k)
where both p(e?j| f? ) and p( f? |e?k) can be computed using maximum likelihood estimation
as part of the bilingual phrasal extraction process:
p(e?j| f? ) =
number of times f? is extracted with e?j
number of times f? is extracted with any e?
Once the probability values are obtained, the most likely paraphrase can be chosen for
any phrase.
Bannard and Callison-Burch (2005) are able to extract millions of phrasal para-
phrases from a bilingual parallel corpus. Such an approach is able to capture a large
variety of paraphrastic phenomena in the inferred paraphrase pairs but is seriously
limited by the bilingual word alignment technique. Even state-of-the-art alignment
methods from SMT are known to be notoriously unreliable when used for aligning
phrase pairs. The authors find via manual evaluation that the quality of the phrasal
369
Computational Linguistics Volume 36, Number 3
Figure 8
Extracting consistent bilingual phrasal correspondences from the shown sentence pairs.
(i1, j1) ? (i2, j2) denotes the correspondence ? fi1 . . . fj1 , ei2 . . . ej2?. Not all extracted
correspondences are shown.
paraphrases obtained via manually constructed word alignments is significantly better
than that of the paraphrases obtained from automatic alignments.
It has been widely reported that the existing bilingual word alignment techniques
are not ideal for use in translation and, furthermore, improving these techniques does
not always lead to an improvement in translation performance. (Callison-Burch, Talbot,
and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu
2007). More details on the relationship between word alignment and SMT can be found
in the comprehensive SMT survey recently published by Lopez (2008) (particularly
Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments
in the same way as a translation system would and, therefore, would be equally
susceptible to the shortcomings of the word alignment techniques. To determine how
noisy automatic word alignments affect paraphrasing done via bilingual corpora, we
inspected a sample of paraphrase pairs that were extracted when using Arabic?a
language significantly different from English?as the pivot language.11 In this study, we
found that the paraphrase pairs in the sample set could be grouped into the following
three broad categories:
(a) Morphological variants. These pairs only differ in the morphological
form of one of the words in the phrases and cannot really be considered
paraphrases. Examples: ?ten ton, ten tons?, ?caused clouds, causing clouds?.
11 The bilingual Arabic?English phrases were extracted from a million sentences of Arabic newswire data
using the freely available and open source Moses SMT toolkit (http://www.statmt.org/moses/). The
default Moses parameters were used. The English paraphrases were generated by simply applying the
pivoting process described herein to the bilingual phrase pairs.
370
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
(b) Approximate Phrasal Paraphrases. These are pairs that only share partial
semantic content. Most paraphrases extracted by the pivot method using
automatic alignments fall into this category. Examples: ?were exiled, went
abroad?, ?accounting firms, auditing firms?.
(c) Phrasal Paraphrases. Despite unreliable alignments, there were indeed a
large number of truly paraphrastic pairs in the set that were semantically
equivalent. Examples: ?army roadblock, military barrier? ?staff walked out,
team withdrew?.
Besides there being obvious linguistic differences between Arabic and English, the
primary reason for the generation of phrase pairs that lie in categories (a) and (b)
is incorrectly induced alignments between the English and Arabic words, and hence,
phrases. Therefore, a good portion of subsequent work on paraphrasing using bilingual
corpora, as discussed below focuses on using additional machinery or evidence to cope
with the noisy alignment process. Before we continue, we believe it would be useful
to draw a connection between Bannard and Callison-Burch?s (2005) work and that of
Wu and Zhou (2003) as discussed in Section 3.2. Note that both of these techniques
rely on a secondary language to provide the cues for generating paraphrases in the
primary language. However, Wu and Zhou rely on a pre-compiled bilingual dictionary
to discover these cues whereas Bannard and Callison-Burch have an entirely data-
driven discovery process.
In an attempt to address some of the noisy alignment issues, Callison-Burch (2008)
recently proposed an improvement that places an additional syntactic constraint on the
phrasal paraphrases extracted via the pivot-based method from bilingual corpora and
showed that using such a constraint leads to a significant improvement in the qual-
ity of the extracted paraphrases.12 The syntactic constraint requires that the extracted
paraphrase be of the same syntactic type as the original phrase. With this constraint,
estimating the paraphrase probability now requires the incorporation of syntactic type
into the equation:
p(e?j|e?k, s(ek)) ?
?
f?
p(e?j| f? , s(ek))p( f? |e?k, s(ek))
where s(e) denotes the syntactic type of the English phrase e. As before, maximum
likelihood estimation is employed to compute the two component probabilities:
p(e?j| f? , s(ek)) =
number of times f? is extracted with e?j and type s(ek)
number of times f? is extracted with any e? and type s(ek)
If the syntactic types are restricted to be simple constituents (NP, VP, etc.), then
using this constraint will actually exclude some of the paraphrase pairs that could
have been extracted in the unconstrained approach. This leads to the familiar precision-
recall tradeoff: It only extracts paraphrases that are of higher quality, but the approach
has a significantly lower coverage of paraphrastic phenomena that are not necessarily
syntactically motivated. To increase the coverage, complex syntactic types such as those
12 The software for generating these phrasal paraphrases along with a large collection of already extracted
paraphrases is available at http://www.cs.jhu.edu/?ccb/howto-extract-paraphrases.html.
371
Computational Linguistics Volume 36, Number 3
used in Combinatory Categorial Grammars (Steedman 1996) are employed, which can
help denote a syntactic constituent with children missing on the left and/or right hand
sides. An example would be the complex type VP/(NP/NNS) which denotes a verb
phrase missing a noun phrase to its right which, in turn, is missing a plural noun to its
right. The primary benefit of using complex types is that less useful paraphrastic phrase
pairs from different syntactic categories such as ?accurately, precise?, that would have
been allowed in the unconstrained pivot-based approach, are now disallowed.
The biggest advantage of this approach is the use of syntactic knowledge as one
form of additional evidence in order to filter out phrase pairs from categories (a) and
(b) as defined in the context of our manual inspection of pivot-based paraphrases
above. Indeed, the authors conduct a manual evaluation to show that the syntactically
constrained paraphrase pairs are better than those produced without such constraints.
However, there are two additional benefits of this technique:
1. The constrained approach might allow induction of some new phrasal
paraphrases in category (c) since now an English phrase only has to
compete with other pivoted phrases of similar syntactic type and not all of
them.
2. The effective partitioning of the probability space for a given paraphrase
pair by syntactic types can be exploited: Overly specific syntactic types
that occur very rarely can be ignored and a less noisy paraphrase
probability estimate can be computed, which may prove more useful
in a downstream application than its counterpart computed via the
unconstrained approach.
We must also note that requiring syntactic constraints for pivot-based paraphrase ex-
traction restricts the approach to those languages where a reasonably good parser is
available.
An obvious extension of the Callison-Burch style approach is to use the collection
of pivoted English-to-English phrase pairs to generate sentential paraphrases for new
sentences. Madnani et al (2008a) combine the pivot-based approach to paraphrase
acquisition with a well-defined English-to-English translation model that is then used in
an (unmodified) SMT system, yielding sentential paraphrases by means of ?translating?
input English sentences. However, instead of fully lexicalized phrasal correspondences
as in (Bannard and Callison-Burch 2005), the fundamental units of translation (and
paraphrasing) are hierarchical phrase pairs. The latter can be extracted from the former
by replacing aligned sub-phrases with non-terminal symbols. For example, given the
initial phrase pair , growth rate has been effectively contained?, the
hierarchical phrase pair ?X1 X2, X1 has been X2? can be formed.13 Each hierarchical
phrase pair can also have certain features associated with it that are estimated via
maximum likelihood estimation during the extraction process. Such phrase pairs can
formally be considered the rules of a bilingual synchronous context-free grammar
(SCFG). Translation with SCFGs is equivalent to parsing the string in the source lan-
guage using these rules to generate the highest-scoring tree and then reading off the
tree in target order. For the purposes of this survey, it is sufficient to state that efficient
13 The process of converting an initial phrase into a hierarchical one is subject to several additional
constraints on the lengths of the initial and hierarchical phrases and the number and position of
non-terminals in the hierarchical phrase.
372
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
methods to extract such rules, to estimate their features, and to translate with them are
now well established. For more details on building SCFG-based models and translating
with them, we refer the reader to (Chiang 2006, 2007).
Once a set of bilingual hierarchical rules has been extracted along with associated
features, the pivoting trick can be applied to infer monolingual hierarchical paraphrase
pairs (or paraphrastic patterns). However, the patterns are not the final output and
are actually used as rules from a monolingual SCFG grammar in order to define an
English-to-English translation model. Features for each monolingual rule are estimated
in terms of the features of the bilingual pairs that the rule was inferred from. A sentential
paraphrase can then be generated for any given sentence by using this model along with
an n-gram language model and a regular SMT decoder to paraphrase (or monolingually
translate) any sentence just as one would translate bilingually.
The primary advantage of this approach is the ability to produce good quality
sentential paraphrases by leveraging the SMT machinery to address the noise issue.
However, although the decoder and the language model do serve to counter the noisy
word alignment process, they do so only to a degree and not entirely.
Again, we must draw a connection between this work and that of Quirk, Brockett,
and Dolan (2004) (discussed in Section 3.3) because both treat paraphrasing as
monolingual translation. However, as outlined in the discussion of that work, Quirk,
Brockett, and Dolan use a relatively simplistic translation model and decoder which
leads to paraphrases with little or no lexical variety. In contrast, Madnani et al use a
more complex translation model and an unmodified state-of-the-art SMT decoder to
produce paraphrases that are much more diverse. Of course, the reliance of the latter
approach on automatic word alignments does inevitably lead to much noisier sentential
paraphrases than those produced by Quirk, Brockett, and Dolan.
Kok and Brockett (2010) present a novel take on generating phrasal paraphrases
with bilingual corpora. As with most approaches based on parallel corpora, they also
start with phrase tables extracted from such corpora along with the corresponding
phrasal translation probabilities. However, instead of performing the usual pivoting
step with the bilingual phrases in the table, they take a graphical approach and represent
each phrase in the table as a node, leading to a bipartite graph. Two nodes in the
graph are connected to each other if they are aligned to each other. In order to extract
paraphrases, they sample random paths in the graph from any English node to another.
Note that the traditional pivot step is equivalent to a path of length two: one English
phrase to the foreign pivot phrase and then to the potentially paraphrastic English
phrase. By allowing paths of lengths longer than two, this graphical approach can find
more paraphrases for any given English phrase.
Furthermore, instead of restricting themselves to a single bilingual phrase table,
they take as input a number of phrase tables, each corresponding to a different pair of
six languages. Similar to the single-table case, each phrase in each table is represented
as a node in a graph that is no longer bipartite in nature. By allowing edges to exist
between nodes of all the languages if they are aligned, the pivot can now even be a set
of nodes rather than a single node in another language. For example, one could easily
find the following path in such a graph:
ate lunch ? a?en zu ittag (German) ? aten een hapje (Dutch) ? had a bite
In general, each edge is associated with a weight corresponding to the bilingual phrase
translation probability. Random walks are then sampled from the graph in such a way
that only paths of high probability end up contributing to the extracted paraphrases.
373
Computational Linguistics Volume 36, Number 3
Obviously, the alignment errors discussed in the context of simple pivoting will also
have an adverse effect on this approach. In order to prevent this, the authors add special
feature nodes to the graph in addition to regular nodes. These feature nodes represent
domain-specific knowledge of what would make good paraphrases. For example, nodes
representing syntactic equivalence classes of the start and end words of the English
phrases are added. This indicates that phrases that start and end with the same kind of
words (interrogatives or articles) are likely to be paraphrases. Astute readers will make
the following observations about the syntactic feature nodes used by the authors:
 Such nodes can be seen as an indirect way of incorporating a limited form
of distributional similarity.
 By including such nodes?essentially based on lexical equivalence
classes?the authors are, in a way, imposing weaker forms of syntactic
constraints described in Callison-Burch (2008) without requiring a parser.
The authors extract paraphrases for a small set of input English paraphrases and
show that they are able to generate a larger percentage of correct paraphrases compared
to the syntactically constrained approach proposed by Callison-Burch (2008). They con-
duct no formal evaluation of the coverage of their approach but show that, in a limited
setting, it is higher than that for the syntactically constrained pivot-based approach.
However, they perform no comparisons of their coverage with the original pivot-based
approach (Bannard and Callison-Burch 2005).
4. Building Paraphrase Corpora
Before we present some specific techniques from the literature that have been employed
to evaluate paraphrase generation methods, it is important to examine some recent
work that has been done on constructing paraphrase corpora. As part of this work, hu-
man subjects are generally asked to judge whether two given sentences are paraphrases
of each other. We believe that a detailed examination of this manual evaluation task
provides an illuminating insight into the nature of a paraphrase in a practical, rather
than a theoretical, context. In addition, it has obvious implications for any method,
whether manual or automatic, that is used to evaluate the performance of a paraphrase
generator.
Dolan and Brockett (2005) were the first to attempt to build a paraphrase corpus
on a large scale. The Microsoft Research Paraphrase (MSRP) Corpus is a collection of
5, 801 sentence pairs, each manually labeled with a binary judgment as to whether it
constitutes a paraphrase or not. As a first step, the corpus was created using a heuristic
extraction method in conjunction with an SVM-based classifier that was trained to
select likely sentential paraphrases from a large monolingual corpus containing news
article clusters. However, the more interesting aspects of the task were the subsequent
evaluation of these extracted sentence pairs by human annotators and the set of issues
encountered when defining the evaluation guidelines for these annotators.
It was observed that if the human annotators were instructed to mark only the
sentence pairs that were strictly semantically equivalent or that exhibited bidirectional
entailment as paraphrases, then the results were limited to uninteresting sentence pairs
such as the following:
S1: The euro rose above US$1.18, the highest price since its January 1999 launch.
S2: The euro rose above $1.18, the highest level since its launch in January 1999.
374
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
S1: However, without a carefully controlled study, there was little clear proof that
the operation actually improves people?s lives.
S2: But without a carefully controlled study, there was little clear proof that the
operation improves people?s lives.
Instead, they discovered that most of the complex paraphrases?ones with alter-
nations more interesting than simple lexical synonymy and local syntactic changes?
exhibited varying degrees of semantic divergence. For example:
S1: Charles O. Prince, 53, was named as Mr. Weill?s successor.
S2: Mr. Weill?s longtime confidant, Charles O. Prince, 53, was named as his successor.
S1: David Gest has sued his estranged wife Liza Minelli for beating him when she was
drunk.
S2: Liza Minelli?s estranged husband is taking her to court after saying she threw a
lamp at him and beat him in drunken rages.
Therefore, in order to be able to create a richer paraphrase corpus, one with many
complex alternations, the instructions to the annotators had to be relaxed; the degree of
mismatch accepted before a sentence pair was judged to be fully semantically divergent
(or ?non-equivalent?) was left to the human subjects. It is also reported that, given the
idiosyncratic nature of each sentence pair, only a few formal guidelines were generaliz-
able enough to take precedence over the subjective judgments of the human annotators.
Despite the somewhat loosely defined guidelines, the inter-annotator agreement for the
task was 84%. However, a kappa score of 62 indicated that the task was overall a difficult
one (Cohen 1960). At the end, 67% of the sentence pairs were judged to be paraphrases
of each other and the rest were judged to be non-equivalent.14
Although the MSRP Corpus is a valuable resource and its creation provided valu-
able insight into what constitutes a paraphrase in the practical sense, it does have some
shortcomings. For example, one of the heuristics used in the extraction process was
that the two sentences in a pair must share at least three words. Using this constraint
rules out any paraphrase pairs that are fully lexically divergent but still semantically
equivalent. The small size of the corpus, when combined with this and other such
constraints, precludes the use of the corpus as training data for a paraphrase generation
or extraction system. However, it is fairly useful as a freely available test set to evaluate
paraphrase recognition methods.
On a related note, Fujita and Inui (2005) take a more knowledge-intensive ap-
proach to building a Japanese corpus containing sentence pairs with binary paraphrase
judgments and attempt to focus on variety and on minimizing the human annotation
cost. The corpus contains 2, 031 sentence pairs each with a human judgment indicating
whether the paraphrase is correct or not. To build the corpus, they first stipulate a
typology of paraphrastic phenomena (rewriting light-verb constructions, for example)
and then manually create a set of morpho-syntactic paraphrasing rules and patterns
describing each type of paraphrasing phenomenon. A paraphrase generation system
14 The MSR paraphrase corpus is available at http://research.microsoft.com/en-us/downloads/
607d14d9-20cd-47e3-85bc-a2f65cd28042.
375
Computational Linguistics Volume 36, Number 3
using these rules (Fujita et al 2004) is then applied to a corpus containing Japanese
news articles, and example paraphrases are generated for the sentences in the corpus.
These paraphrase pairs are then handed to two human annotators who create binary
judgments for each pair indicating whether or not the paraphrase is correct. Using a
class-oriented approach is claimed to have a two-fold advantage:
1. Exhaustive Collection of Paraphrases. Creating specific paraphrasing
rules for each class manually is likely to increase the chance of the
collected examples accurately reflecting the distribution of occurrences
in the real world.
2. Low Annotation Cost. Partitioning the annotation task into classes is
expected to make it easier (and faster) to arrive at a binary judgment given
that an annotator is only concerned with a specific type of paraphrasing
when creating said judgment.
The biggest disadvantage of this approach is that only two types of paraphrastic phe-
nomena are used: light-verb constructions and transitivity alternations (using intransi-
tive verbs in place of transitive verbs). The corpus indeed captures almost all examples
of both types of paraphrastic phenomena and any that are absent can be easily covered
by adding one or two more patterns to the class. The claim of reduced annotation cost is
not necessarily borne out by the observations. Despite partitioning the annotation task
by types, it was still difficult to provide accurate annotation guidelines. This led to a
significant difference in annotation time?with some annotations taking almost twice
as long as others. Given the small size of the corpus, it is unlikely that it may be used
as training data for corpus-based paraphrase generation methods and, like the MSRP
corpus, would be best suited to the evaluation of paraphrase recognition techniques.
Most recently, Cohn, Callison-Burch, and Lapata (2008) describe a different take
on the creation of a monolingual parallel corpus containing 900 sentence pairs with
paraphrase annotations that can be used for both development and evaluation of para-
phrase systems. These paraphrase annotations take the form of alignments between the
words and sequences of words in each sentence pair; these alignments are analogous
to the word- and phrasal-alignments induced in SMT systems that were illustrated in
Section 3.5. As is the case with SMT alignments, the paraphrase annotations can be
of different forms: one-word-to-one-word, one-word-to-many-words, as well as fully
phrasal alignments.15
The authors start from a sentence-aligned paraphrase corpus compiled from three
corpora that we have already described elsewhere in this survey: (1) the sentence
pairs judged equivalent from the MSRP Corpus: (2) the Multiple Translation Chinese
(MTC) corpus of multiple human-written translations of Chinese news stories used
by Pang, Knight, and Marcu (2003); and (3) two English translations of the French novel
Twenty Thousand Leagues Under the Sea, a subset of the monolingual parallel corpus used
by Barzilay and McKeown (2001). The words in each sentence pair from this corpus
are then aligned automatically to produce the initial paraphrase annotations that are
then refined by two human annotators. The annotation guidelines required that the
annotators judge which parts of a given sentence pair were in correspondence and to in-
dicate this by creating an alignment between those parts (or correcting already existing
15 The paraphrase-annotated corpus can be found at http://www.dcs.shef.ac.uk/?tcohn/
paraphrase corpus.html.
376
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
alignments, if present). Two parts were said to correspond if they could be substituted
for each other within the specific context provided by the respective sentence pair. In
addition, the annotators were instructed to classify the created alignments as either sure
(the two parts are clearly substitutable) or possible (the two parts are slightly divergent
either in terms of syntax or semantics). For example, given the following paraphrastic
sentence pair:
S1: He stated the convention was of profound significance.
S2: He said that the meeting could have very long-term effects.
the phrase pair ?the convention, the meeting? will be aligned as a sure correspondence
whereas the phrase pair ?was of profound significance, could have very long-term effects? will
be aligned as a possible correspondence. Other examples of possible correspondences
could include the same stem expressed as different parts-of-speech (such as ?significance,
significantly?) or two non-synonymous verbs (such as ?this is also, this also marks?). For
more details on the alignment guidelines that were provided to the annotators, we refer
the reader to (Callison-Burch, Cohn, and Lapata 2006).
Extensive experiments are conducted to measure inter-annotator agreements and
obtain good agreement values but they are still low enough to confirm that it is difficult
for humans to recognize paraphrases even when the task is formulated differently.
Overall, such a paraphrase corpus with detailed paraphrase annotations is much more
informative than a corpus containing binary judgments at the sentence level such as
the MSRP corpus. As an example, because the corpus contains paraphrase annotations
at the word as well as phrasal levels, it can be used to build systems that can learn
from these annotations and generate not only fully lexicalized phrasal paraphrases but
also syntactically motivated paraphrastic patterns. To demonstrate the viability of the
corpus for this purpose, a grammar induction algorithm (Cohn and Lapata 2007) is
applied?originally developed for sentence compression?to the parsed version of their
paraphrase corpus and the authors show that they can learn paraphrastic patterns such
as those shown in Figure 9.
In general, building paraphrase corpora, whether it is done at the sentence level or
at the sub-sentential level, is extremely useful for the fostering of further research and
development in the area of paraphrase generation.
5. Evaluation of Paraphrase Generation
Whereas other language processing tasks such as machine translation and docu-
ment summarization usually have multiple annual community-wide evaluations using
Figure 9
An example of syntactically motivated paraphrastic patterns that can be extracted from the
paraphrase corpus constructed by Cohn, Callison-Burch, and Lapata (2008).
377
Computational Linguistics Volume 36, Number 3
standard test sets and manual as well as automated metrics, the task of automated
paraphrasing does not. An obvious reason for this disparity could be that paraphrasing
is not an application in and of itself. However, the existence of similar evaluations for
other tasks that are not applications, such as dependency parsing (Buchholz and Marsi
2006; Nivre et al 2007) and word sense disambiguation (Senseval), suggests otherwise.
We believe that the primary reason is that, over the years, paraphrasing has been em-
ployed in an extremely fragmented fashion. Paraphrase extraction and generation are
used in different forms and with different names in the context of different applications
(for example: synonymous collocation extraction, query expansion). This usage pattern
does not allow researchers in one community to share the lessons learned with those
from other communities. In fact, it may even lead to research being duplicated across
communities.
However, more recent work?some of it discussed in this survey?on extracting
phrasal paraphrases (or patterns) does include direct evaluation of the paraphrasing
itself: The original phrase and its paraphrase are presented to multiple human judges,
along with the contexts in which the phrase occurs in the original sentence, who
are asked to determine whether the relationship between the two phrases is indeed
paraphrastic (Barzilay and McKeown 2001; Barzilay and Lee 2003; Ibrahim, Katz, and
Lin 2003; Pang, Knight, and Marcu 2003). A more direct approach is to substitute the
paraphrase in place of the original phrase in its sentence and present both sentences
to judges who are then asked to judge not only their semantic equivalence but also
the grammaticality of the new sentence (Bannard and Callison-Burch 2005; Callison-
Burch 2008). Motivation for such substitution-based evaluation is discussed in Callison-
Burch (2007): the basic idea being that items deemed to be paraphrases may behave as
such only in some contexts and not others. Szpektor, Shnarch, and Dagan (2007) posit
a similar form of evaluation for textual entailment wherein the human judges are not
only presented with the entailment rule but also with a sample of sentences that match
its left-hand side (called instances), and then asked to assess whether the rule holds
under each specific instance.
Sentential paraphrases may be evaluated in a similar fashion without the need for
any surrounding context (Quirk, Brockett, and Dolan 2004). An intrinsic evaluation of
this form must employ the usual methods for avoiding any bias and for maximizing
inter-judge agreement. In addition, we believe that, given the difficulty of this task even
for human annotators, adherence to strict semantic equivalence may not always be a
suitable guideline and intrinsic evaluations must be very carefully designed. A number
of these approaches also perform extrinsic evaluations, in addition to the intrinsic
one, by utilizing the extracted or generated paraphrases to improve other applications
such as machine translation (Callison-Burch, Koehn, and Osborne 2006) and others as
described in Section 1.
Another option when evaluating the quality of a paraphrase generation method is
that of using automatic measures. The traditional automatic evaluation measures of pre-
cision and recall are not particularly suited to this task because, in order to use them, a
list of reference paraphrases has to be constructed against which these measures may be
computed. Given that it is extremely unlikely that any such list will be exhaustive, any
precision and recall measurements will not be accurate. Therefore, other alternatives
are needed. Since the evaluation of paraphrases is essentially the task of measuring
semantic similarity or of paraphrase recognition, all of those metrics, including the ones
discussed in Section 2, can be employed here.
Most recently, Callison-Burch, Cohn, and Lapata (2008) discuss ParaMetric, another
automatic measure that may be used to evaluate paraphrase extraction methods. This
378
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
work follows directly from the work done by the authors to create the paraphrase-
annotated corpus described in the previous section. Recall that this corpus contains
paraphrastic sentence pairs with annotations in the form of alignments between their
respective words and phrases. It is posited that to evaluate any paraphrase generation
method, one could simply have it produce its own set of alignments for the sentence
pairs in the corpus and precision and recall could then be computed over alignments
instead of phrase pairs. These alignment-oriented precision (Palign) and recall (Ralign)
measures are computed as follows:
Palign =
?
?s1,s2? |NP(s1, s2) ? NM(s1, s2)|
?
?s1,s2? |NP(s1, s2)|
Ralign =
?
?s1,s2? |NP(s1, s2) ? NM(s1, s2)|
?
?s1,s2? |NM(s1, s2)|
where ?s1, s2? denotes a sentence pair, NM(s1, s2) denotes the phrases extracted via the
manual alignments for the pair ?s1, s2?, and NP(s1, s2) denotes the phrases extracted via
the automatic alignments induced using the paraphrase method P that is to be evalu-
ated. The phrase extraction heuristic used to compute NP and NM from the respective
alignments is the same as that employed by Bannard and Callison-Burch (2005) and
illustrated in Figure 8.
Although using alignments as the basis for computing precision and recall is a
clever trick, it does require that the paraphrase generation method be capable of produc-
ing alignments between sentence pairs. For example, the methods proposed by Pang,
Knight, and Marcu (2003) and Quirk, Brockett, and Dolan (2004) for generating sen-
tential paraphrases from monolingual parallel corpora and described in Section 3.3 do
produce alignments as part of their respective algorithms. Indeed, Callison-Burch et al
provide a comparison of their pivot-based approach?operating on bilingual parallel
corpora?with the two monolingual approaches just mentioned in terms of ParaMetric,
since all three methods are capable of producing alignments.
However, for other approaches that do not necessarily operate at the level of
sentences and cannot produce any alignments, falling back on estimates of traditional
formulations of precision and recall is suggested.
There has also been some preliminary progress toward using standardized test sets
for intrinsic evaluations. A test set containing 20 AFP articles (484 sentences) about
violence in the Middle East that was used for evaluating the lattice-based paraphrase
technique in (Barzilay and Lee 2003) has been made freely available.16 In addition to
the original sentences for which the paraphrases were generated, the set alo contains
the paraphrases themselves and the judgments assigned by human judges to these
paraphrases. The paraphrase-annotated corpus discussed in the previous section would
also fall under this category of resources.
As with many other fields in NLP, paraphrase generation also lacks serious extrinsic
evaluation (Belz 2009). As described herein, many paraphrase generation techniques
are developed in the context of a host NLP application and this application usually
serves as one form of extrinsic evaluation for the quality of the paraphrases generated
16 The corpus is available at http://www.cs.cornell.edu/Info/Projects/NLP/statpar.html.
379
Computational Linguistics Volume 36, Number 3
by that technique. However, as yet there is no widely agreed-upon method of extrinsi-
cally evaluating paraphrase generation. Addressing this deficiency should be a crucial
consideration for any future community-wide evaluation effort.
An important dimension for any area of research is the availability of fora where
members of the community may share their ideas with their colleagues and receive
valuable feedback. In recent years, a number of such fora have been made available to
the automatic paraphrasing community (Inui and Hermjakob 2003; Tanaka et al 2004;
Dras and Yamamoto 2005; Sekine et al 2007), which represents an extremely important
step toward countering the fragmented usage pattern described previously.
6. Future Trends
It is important for any survey to provide a look to the future of the surveyed task and
general trends for the corresponding research methods. We identify several such trends
in the area of paraphrase generation that are gathering momentum.
The Influence of the Web. The Web is rapidly becoming one of the most important
sources of data for natural language processing applications, which should not be sur-
prising given its phenomenal rate of growth. The (relatively) freely available Web data,
massive in scale, has already had a definite influence over data-intensive techniques
such as those employed for paraphrase generation (Pas?ca and Dienes 2005). However,
the availability of such massive amounts of Web data comes with serious concerns for
efficiency and has led to the development of efficient methods that can cope with such
large amounts of data. Bhagat and Ravichandran (2008) extract phrasal paraphrases by
measuring distributional similarity over a 150GB monolingual corpus (25 billion words)
via locality sensitive hashing, a randomized algorithm that involves the creation of
fingerprints for vectors in space (Broder 1997). Because vectors that are more similar
are more likely to have similar fingerprints, vectors (or distributions) can simply be
compared by comparing their fingerprints, leading to a more efficient distributional
similarity algorithm (Charikar 2002; Ravichandran, Pantel, and Hovy 2005). We also
believe that the influence of the Web will extend to other avenues of paraphrase genera-
tion such as the aforementioned extrinsic evaluation or lack thereof. For example, Fujita
and Sato (2008b) propose evaluating phrasal paraphrase pairs, automatically generated
from a monolingual corpus, by querying the Web for snippets related to the pairs and
using them as features to compute the pair?s paraphrasability.
Combining Multiple Sources of Information. Another important trend in para-
phrase generation is that of leveraging multiple sources of information to determine
whether two units are paraphrastic. For example, Zhao et al (2008) improve the sen-
tential paraphrases that can be generated via the pivot method by leveraging five other
sources in addition to the bilingual parallel corpus itself: (1) a corpus of Web queries
similar to the phrase, (2) definitions from the Encarta dictionary, (3) a monolingual par-
allel corpus, (4) a monolingual comparable corpus, and (5) an automatically constructed
thesaurus. Phrasal paraphrase pairs are extracted separately from all six models and
then combined in a log-linear paraphrasing-as-translation model proposed by Madnani
et al (2007). A manual inspection reveals that using multiple sources of information
yields paraphrases with much higher accuracy. We believe that such exploitation of
multiple types of resources and their combinations is an important development. Zhao
et al (2009) further increase the utility of this combination approach by incorporating
application specific constraints on the pivoted paraphrases. For example, if the output
paraphrases need to be simplified versions of the input sentences, then only those
phrasal paraphrase pairs are used where the output is shorter than the input.
380
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Use of SMT Machinery. In theory, statistical machine translation is very closely
related to paraphrase generation since the former also relies on finding semantic equiv-
alence, albeit in a second language. Hence, there have been numerous paraphrasing ap-
proaches that have relied on different components of an SMT pipeline (word alignment,
phrase extraction, decoding/search) as we saw in the preceding pages of this survey.
Despite the obvious convenience of using SMT components for the purpose of mono-
lingual translation, we must consider that doing so usually requires additional work to
deal with the added noise due to the nature of such components. We believe that SMT
research will continue to influence research in paraphrasing; both by providing ready-
to-use building blocks and by necessitating development of methods to effectively use
such components for the unintended task of paraphrase generation.
Domain-Specific Paraphrasing. Recently, work has been done to generate phrasal
paraphrases in specialized domains. For example, in the field of health literacy, it
is well known that documents for health consumers are not very well-targeted to
their purported audience. Recent research has shown how to generate a lexicon of
semantically equivalent phrasal (and lexical) pairs of technical and lay medical terms
from monolingual parallel corpora (Elhadad and Sutaria 2007) as well as monolingual
comparable corpora (Dele?ger and Zweigenbaum 2009). Examples include pairs such as
?myocardial infarction, heart attack? and ?leucospermia, increased white cells in the sperm?.
In another domain, Max (2008) proposes an adaptation of the pivot-based method to
generate rephrasings of short text spans that could help a writer revise a text. Because
the goal is to assist a writer in making revisions, the rephrasings do not always need
to bear a perfect paraphrastic relationship to the original, a scenario suited for the
pivot-based method. Several variants of such adaptations are developed that generate
candidate rephrasings driven by fluency, semantic equivalence, and authoring value,
respectively.
We also believe that a large-scale annual community-wide evaluation should be-
come a trend since it is required to foster further research in, and use of, paraphrase
extraction and generation. Although there have been recent workshops and tasks on
paraphrasing and entailment as discussed in Section 5, this evaluation would be much
more focused, providing sets of shared guidelines and resources, in the spirit of the
recent NIST MT Evaluation Workshops (NIST 2009).
7. Summary
Over the last two decades, there has been much research on paraphrase extraction and
generation within a number of research communities in natural language processing,
in order to improve the specific application with which that community is concerned.
However, a large portion of this research can be easily adapted for more widespread use
outside its particular host and can provide significant benefits to the whole field. Only
recently have there been serious efforts to conduct research on the topic of paraphrasing
by treating it as an important natural language processing task independent of a host
application.
In this article, we have presented a comprehensive survey of the task of paraphrase
extraction and generation motivated by the fact that paraphrases can help in a multi-
tude of applications such as machine translation, text summarization, and information
extraction. The aim was to provide an application-independent overview of paraphrase
generation, while also conveying an appreciation for the importance and potential use
of paraphrasing in the field of NLP research. We show that there are a large variety
381
Computational Linguistics Volume 36, Number 3
of paraphrase generation methods and each such method has a very different set of
characteristics, in terms of both its performance and its ease of deployment. We also
observe that whereas most of the methods in this survey can be used in multiple
applications, the choice of the most appropriate method depends on how well the
characteristics of the produced paraphrases match the requirements of the downstream
application in which the paraphrases are being utilized.
References
Ayan, Necip Fazil and Bonnie Dorr. 2006.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of ACL/COLING,
pages 9?16, Sydney.
Bangalore, Srinivas and Owen Rambow.
2000. Corpus-based lexical choice in
natural language generation. In
Proceedings of ACL, pages 464?471,
Hong Kong.
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual
parallel corpora. In Proceedings of ACL,
pages 597?604, Ann Arbor, MI.
Bar-Haim, Roy, Ido Dagan, Bill Dolan, Lisa
Ferro, Danilo Giampiccolo, Bernardo
Magnini, and Idan Szpektor, editors. 2007.
Proceedings of the Second PASCAL Challenges
Workshop on Recognizing Textual Entailment,
Venice.
Barzilay, Regina and Lillian Lee. 2002.
Bootstrapping lexical choice via
multiple-sequence alignment. In
Proceedings of EMNLP, pages 164?171,
Philadelphia, PA.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: An unsupervised
approach using multiple-sequence
alignment. In Proceedings of HLT-NAACL
2003, pages 16?23, Edmonton.
Barzilay, Regina and Kathleen McKeown.
2001. Extracting paraphrases from a
parallel corpus. In Proceedings of ACL,
pages 50?57, Toulouse.
Barzilay, Regina and Kathleen R. McKeown.
2005. Sentence fusion for multidocument
news summarization. Computational
Linguistics, 31(3):297?328.
Bayer, Samuel, John Burger, Lisa Ferro, John
Henderson, and Alexander Yeh. 2005.
MITREs submissions to the EU Pascal
RTE Challenge. In Proceedings of the
PASCAL Challenges Workshop on
Recognizing Textual Entailment,
pages 41?44, Southampton, U.K.
Beeferman, Doug and Adam Berger. 2000.
Agglomerative clustering of a search
engine query log. In Proceedings of the ACM
SIGKDD International Conference on
Knowledge Discovery and Data mining,
pages 407?416, Boston, MA.
Belz, Anja. 2009. That?s nice...what can you
do with it? Computational Linguistics,
35(1):111?118.
Bhagat, Rahul and Deepak Ravichandran.
2008. Large scale acquisition of
paraphrases for learning surface
patterns. In Proceedings of ACL,
pages 674?682, Columbus, OH.
Bosma, Wauter and Chris Callison-Burch.
2007. Paraphrase substitution for
recognizing textual entailment.
In Evaluation of Multilingual and
Multimodal Information Retrieval,
Lecture Notes in Computer Science,
Volume 4730, Springer-Verlag,
pages 502?509.
Brockett, Chris and William B. Dolan. 2005.
Support vector machines for paraphrase
identification and corpus construction. In
Proceedings of the Third International
Workshop on Paraphrasing, pages 1?8,
Jeju Island.
Broder, Andrei. 1997. On the resemblance
and containment of documents. In
Proceedings of the Compression and
Complexity of Sequences, pages 21?29,
Salemo.
Brown, Peter F., John Cocke, Stephen Della
Pietra, Vincent J. Della Pietra, Frederick
Jelinek, John D. Lafferty, Robert L. Mercer,
and Paul S. Roossin. 1990. A Statistical
Approach to Machine Translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
Conference on Computational Natural
Language Learning (CoNLL-X),
pages 149?164, New York, NY.
Callison-Burch, Chris. 2007. Paraphrasing and
Translation. Ph.D. thesis, School of
Informatics, University of Edinburgh.
382
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Callison-Burch, Chris. 2008. Syntactic
constraints on paraphrases extracted from
parallel corpora. In Proceedings of EMNLP,
pages 196?205, Waikiki, HI.
Callison-Burch, Chris, Trevor Cohn, and
Mirella Lapata. 2006. Annotation
guidelines for paraphrase alignment.
Technical report, University of Edinburgh.
http://www.dcs.shef.ac.uk/?tcohn/
paraphrase guidelines.pdf.
Callison-Burch, Chris, Trevor Cohn, and
Mirella Lapata. 2008. ParaMetric: An
automatic evaluation metric for
paraphrasing. In Proceedings of COLING,
pages 97?104, Manchester.
Callison-Burch, Chris, Philipp Koehn, and
Miles Osborne. 2006. Improved statistical
machine translation using paraphrases. In
Proceedings of NAACL, pages 17?24,
New York, NY.
Callison-Burch, Chris, David Talbot,
and Miles Osborne. 2004. Statistical
machine translation with word- and
sentence-aligned parallel corpora. In
Proceedings of ACL, pages 176?183,
Barcelona.
Charikar, Moses. 2002. Similarity estimation
techniques from rounding algorithms. In
Proceedings of the 34th Annual ACM
Symposium on Theory of Computing,
pages 380?388, Montre?al.
Chiang, David. 2006. An Introduction to
Synchronous Grammars. Part of a tutorial
given at ACL. Sydney, Australia.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educational and
Psychological Measurement, 20:3746.
Cohn, Trevor, Chris Callison-Burch, and
Mirella Lapata. 2008. Constructing corpora
for the development and evaluation of
paraphrase systems. Computational
Linguistics, 34:597?614.
Cohn, Trevor and Mirella Lapata. 2007. Large
margin synchronous generation and its
application to sentence compression. In
Proceedings of EMNLP-CoNLL, pages 73?82,
Prague.
Corley, Courtney and Rada Mihalcea.
2005. Measuring the semantic similarity
of texts. In Proceedings of the ACL Workshop
on Empirical Modeling of Semantic
Equivalence and Entailment, pages 13?18,
Ann Arbor, MI.
Crouch, Carolyn J. and Bokyung Yang.
1992. Experiments in automatic statistical
thesaurus construction. In Proceedings
of the ACM SIGIR conference on Research
and Development in Information
Retrieval, pages 77?88, Copenhagen,
Denmark.
Culicover, P. W. 1968. Paraphrase generation
and information retrieval from stored text.
Mechanical Translation and Computational
Linguistics, 11(1?2):78?88.
Dagan, Ido. 2008. Invited Talk: It?s time for
a semantic engine! In Proceedings of the
NSF Symposium on Semantic Knowledge
Discovery, Organization and Use,
pages 20?28, New York, NY.
Dagan, Ido, Oren Glickman, and Bernardo
Magnini. 2006. The PASCAL Recognising
Textual Entailment Challenge. In Machine
Learning Challenges, Lecture Notes in
Computer Science, Volume 3944,
Springer-Verlag, pages 177?190.
Das, Dipanjan and Noah A. Smith. 2009.
Paraphrase identification as probabilistic
quasi-synchronous recognition. In
Proceedings of ACL/IJCNLP, pages 468?476,
Singapore.
Dele?ger, Louise and Pierre Zweigenbaum.
2009. Extracting lay paraphrases of
specialized expressions from monolingual
comparable medical corpora. In
Proceedings of the ACL Workshop on Building
and Using Comparable Corpora, pages 2?10,
Singapore.
Dolan, Bill and Ido Dagan, editors. 2005.
Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence
and Entailment, Ann Arbor, MI.
Dolan, William, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction
of large paraphrase corpora: Exploiting
massively parallel news sources. In
Proceedings of COLING, pages 350?356,
Geneva.
Dolan, William B. and Chris Brockett. 2005.
Automatically constructing a corpus of
sentential paraphrases. In Proceedings
of the Third International Workshop on
Paraphrasing, pages 9?16, Jeju Island.
Dras, Mark. 1999. A Meta-level grammar:
Redefining synchronous TAG for
translation and paraphrase. In Proceedings
of ACL, pages 80?88, College Park, MD.
Dras, Mark and Kazuhide Yamamoto,
editors. 2005. Proceedings of the Third
International Workshop on Paraphrasing,
Jeju Island.
Duclaye, Florence, Franc?ois Yvon, and
Olivier Collin. 2003. Learning paraphrases
to improve a question-answering system.
In Proceedings of the EACL Workshop on
Natural Language Processing for
383
Computational Linguistics Volume 36, Number 3
Question-Answering, pages 35?41,
Budapest.
Durbin, Richard, Sean R. Eddy, Anders
Krogh, and Graeme Mitchison. 1998.
Biological Sequence Analysis: Probabilistic
Models of Proteins and Nucleic Acids.
Cambridge University Press, Cambridge.
Edmonds, Philip and Graeme Hirst. 2002.
Near-synonymy and lexical choice.
Computational Linguistics, 28(2):105?144.
Elhadad, Noemie and Komal Sutaria. 2007.
Mining a lexicon of technical terms and lay
equivalents. In Proceedings of the ACL
BioNLP Workshop, pages 49?56, Prague.
Fraser, Alexander and Daniel Marcu. 2007.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293?303.
Fujita, Atsushi, Kentaro Furihata, Kentaro
Inui, Yuji Matsumoto, and Koichi
Takeuchi. 2004. Paraphrasing of Japanese
light-verb constructions based on lexical
conceptual structure. In Proceedings of the
ACL Workshop on Multiword Expressions:
Integrating Processing, pages 9?16,
Barcelona.
Fujita, Atsushi and Kentaro Inui. 2005. A
Class-oriented approach to building
a paraphrase corpus. In Proceedings
of the Third International Workshop on
Paraphrasing, pages 25?32, Jeju Island.
Fujita, Atsushi, Shuhei Kato, Naoki Kato,
and Satoshi Sato. 2007. A compositional
approach toward dynamic phrasal
thesaurus. In Proceedings of the
ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing,
pages 151?158, Prague.
Fujita, Atsushi and Satoshi Sato. 2008a. A
probabilistic model for measuring
grammaticality and similarity of
automatically generated paraphrases of
predicate phrases. In Proceedings of
COLING, pages 225?232, Manchester.
Fujita, Atsushi and Satoshi Sato. 2008b.
Computing paraphrasability of syntactic
variants using Web snippets. In Proceedings
of IJCNLP, pages 537?544, Hyderabad.
Gale, William A. and Kenneth W. Church.
1991. A program for aligning sentences in
bilingual corpora. In Proceedings of ACL,
pages 177?184, Berkeley, CA.
Gardent, Claire, Marilisa Amoia, and
Evelyne Jacquey. 2004. Paraphrastic
grammars. In Proceedings of the Second
Workshop on Text Meaning and
Interpretation, pages 73?80, Barcelona.
Gardent, Claire and Eric Kow. 2005.
Generating and selecting grammatical
paraphrases. In Proceedings of the European
Workshop on Natural Language Generation,
pages 49?57, Abderdeen.
Garoufi, Konstantina. 2007. Towards a Better
Understanding of Applied Textual Entailment:
Annotation and Evaluation of the RTE-2
Dataset. Master?s thesis, Language Science
and Technology, Saarland University.
Gasperin, Caroline, P. Gamallo, A. Agustini,
G. Lopes, and Vera de Lima. 2001.
Using syntactic contexts for measuring
word similarity. In Proceedings of the
Workshop on Knowledge Acquisition and
Categorization, ESSLLI, pages 18?23,
Helsinki.
Giampiccolo, Danilo, Hoa Dang, Ido Dagan,
Bill Dolan, and Bernardo Magnini, editors.
2008. Proceedings of the Text Analysis
Conference (TAC): Recognizing Textual
Entailment Track, Gaithersburg, MD.
Glickman, Oren and Ido Dagan. 2003.
Identifying lexical paraphrases from a
single corpus: A case study for verbs. In
Recent Advantages in Natural Language
Processing (RANLP?03), pages 81?90,
Borovets.
Grefenstette, G. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Press, Dordrecht.
Gusfield, D. 1997. Algorithms on Strings, Trees,
and Sequences: Computational Science and
Computational Biology. Cambridge
University Press, Cambridge.
Hallett, Catalina and Donia Scott. 2005.
Structural variation in generated health
reports. In Proceedings of the Third
International Workshop on Paraphrasing,
pages 33?40, Jeju Island.
Harris, Zellig. 1954. Distributional structure.
Word, 10(2):3.146?162.
Hearst, Graeme. 1995. Near-synonymy and
the structure of lexical knowledge. In
Working notes of the AAAI Spring
Symposium on Representation and Acquisition
of Lexical Knowledge, Stanford, CA.
Hirst, Graeme. 2003. Paraphrasing
paraphrased. Unpublished invited talk at
the ACL International Workshop on
Paraphrasing, Sapporo, Japan.
Hovy, Eduard H. 1988. Generating Natural
Language under Pragmatic Constraints.
Lawrence Erlbaum Associates, Inc.,
Mahwah, NJ.
Huang, Shudong, David Graff, and George
Doddington. 2002. Multiple-translation
chinese corpus. Linguistic Data
Consortium. http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC2002T01
384
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Ibrahim, Ali, Boris Katz, and Jimmy Lin.
2003. Extracting structural paraphrases
from aligned monolingual corpora. In
Proceedings of the International Workshop on
Paraphrasing, pages 57?64, Sapporo.
Iftene, Adrian. 2009. Textual Entailment. Ph.D.
thesis, Faculty of Computer Science,
University of Ias?i.
Inoue, Naomi. 1991. Automatic noun
classification by using Japanese-English
word pairs. In Proceedings of ACL,
pages 201?208, Berkeley, CA.
Inui, Kentaro and Ulf Hermjakob, editors.
2003. Proceedings of the Second International
Workshop on Paraphrasing. Association for
Computational Linguistics, Sapporo.
Inui, Kentaro and Masaru Nogami. 2001. A
paraphrase-based exploration of
cohesiveness criteria. In Proceedings of the
European Workshop on Natural Language
Generation (ENLG?01), pages 1?10,
Toulouse.
Jacquemin, Christian. 1999. Syntagmatic and
paradigmatic representations of term
variation. In Proceedings of ACL,
pages 341?348, College Park, MD.
Joa?o, Cordeiro, Gae?l Dias, and Brazdil Pavel.
2007a. A metric for paraphrase detection.
In Proceedings of the Second International
Multi-Conference on Computing in the
Global Information Technology, page 7,
Guadeloupe.
Joa?o, Cordeiro, Gae?l Dias, and Brazdil Pavel.
2007b. New functions for unsupervised
asymmetrical paraphrase detection.
Journal of Software, 2(4):12?23.
Jones, Rosie, Benjamin Rey, Omid Madani,
and Wile Greiner. 2006. Generating query
substitutions. In Proceedings of the World
Wide Web Conference, pages 387?396,
Edinburgh.
Kauchak, David and Regina Barzilay. 2006.
Paraphrasing for automatic evaluation. In
Proceedings of HLT-NAACL, pages 455?462,
New York, NY.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 48?54, Edmonton.
Kok, Stanley and Chris Brockett. 2010.
Hitting the right paraphrases in good
time. In Proceedings of NAACL, Los
Angeles, CA.
Lin, Dekang. 1998. Automatic Retrieval and
Clustering of Similar Words. In Proceedings
of ACL-COLING, pages 768?774, Montre?al.
Lin, Dekang and Lin Pantel. 2001. DIRT -
Discovery of Inference Rules from Text. In
Proceedings of ACM SIGKDD Conference on
Knowledge Discovery and Data Mining,
pages 323?328, San Francisco, CA.
Lopez, Adam. 2008. Statistical machine
translation. ACM Computing Surveys,
40(3):1?49.
Lopez, Adam and Philip Resnik. 2006.
Word-based alignment, phrase-based
translation: What?s the link? In Proceedings
of AMTA, pages 90?99, Boston, MA.
Madnani, Nitin, Necip Fazil Ayan, Philip
Resnik, and Bonnie J. Dorr. 2007. Using
paraphrases for parameter tuning in
statistical machine translation. In
Proceedings of the Workshop on Statistical
Machine Translation, pages 120?127, Prague.
Madnani, Nitin, Philip Resnik, Bonnie J.
Dorr, and Richard Schwartz. 2008a.
Applying automatically generated
semantic knowledge: A case study in
machine translation. In Proceedings
of the NSF Symposium on Semantic
Knowledge Discovery, Organization and Use,
pages 60?61, New York, NY.
Madnani, Nitin, Philip Resnik, Bonnie J.
Dorr, and Richard Schwartz. 2008b. Are
multiple reference translations necessary?
Investigating the value of paraphrased
reference translations in parameter
optimization. In Proceedings of the Eighth
Conference of the Association for Machine
Translation in the Americas (AMTA),
pages 143?152, Waikiki, HI.
Malakasiotis, Prodromos. 2009. Paraphrase
recognition using machine learning to
combine similarity measures. In
Proceedings of the ACL-IJCNLP 2009 Student
Research Workshop, pages 27?35, Singapore.
Marsi, Erwin and Emiel Krahmer. 2005a.
Classification of semantic relations by
humans and machines. In Proceedings of the
ACL Workshop on Empirical Modeling of
Semantic Equivalence and Entailment,
pages 1?6, Ann Arbor, MI.
Marsi, Erwin and Emiel Krahmer. 2005b.
Explorations in sentence fusion.
In Proceedings of the European Workshop
on Natural Language Generation,
pages 109?117, Aberdeen.
Max, Aure?lien. 2008. Local rephrasing
suggestions for supporting the work of
writers. In Proceedings of GoTAL,
pages 324?335, Gothenburg.
McKeown, Kathleen R. 1979. Paraphrasing
using given and new information in a
question-answer system. In Proceedings of
ACL, pages 67?72, San Diego, CA.
Melamed, Dan. 2001. Empirical Methods for
Exploiting Parallel Texts. MIT Press,
Cambridge, MA.
385
Computational Linguistics Volume 36, Number 3
Melamed, I. Dan. 1997. A word-to-word
model of translational equivalence. In
Proceedings of ACL, pages 490?497, Madrid.
Metzler, Donald, Susan Dumais, and
Christopher Meek. 2007. Similarity
measures for short segments of text. In
Proceedings of the European Conference on
Information Retrieval (ECIR), pages 16?27,
Rome.
Mohri, Mehryar and Michael Riley. 2002. An
efficient algorithm for the n-best-strings
problem. In Proceedings of the 7th
International Conference on Spoken Language
Processing (ICSLP?02), pages 1313?1316,
Denver, CO.
Murakami, Akiko and Tetsuya Nasukawa.
2004. Term aggregation: Mining
synonymous expressions using personal
stylistic variations. In Proceedings of
COLING, pages 806?812, Geneva.
NIST. 2009. NIST Open Machine Translation
(MT) Evaluation. Information Access
Division. http://www.nist.gov/
speech/tests/mt/.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The
CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007,
pages 915?932, Prague.
Owczarzak, Karolina, Declan Groves, Josef
Van Genabith, and Andy Way. 2006.
Contextual bitext-derived paraphrases in
automatic MT evaluation. In Proceedings on
the Workshop on Statistical Machine
Translation, pages 86?93, New York, NY.
Pang, Bo, Kevin Knight, and Daniel Marcu.
2003. Syntax-based alignment of multiple
translations: Extracting paraphrases
and generating new sentences. In
Proceedings of HLT-NAACL, pages 102?109,
Edmonton.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and Eduard
Hovy. 2007. ISP: Learning inferential
selectional preferences. In Proceedings of
NAACL, pages 564?571, Rochester, NY.
Papineni, K., S. Roukos, T. Ward, and W.-J.
Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In
Proceedings of ACL, pages 311?318,
Philadelphia, PA.
Pas?ca, Marius and Pe?ter Dienes. 2005.
Aligning needles in a haystack: Paraphrase
acquisition across the Web. In Proceedings
of IJCNLP, pages 119?130, Jeju Island.
Pereira, Fernando, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of
English words. In Proceedings of ACL,
pages 183?190, Columbus, OH.
Power, Richard and Donia Scott. 2005.
Automatic generation of large-scale
paraphrases. In Proceedings of the Third
International Workshop on Paraphrasing,
pages 57?64, Jeju Island.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of EMNLP, pages 142?149,
Barcelona.
Ramshaw, Lance and Mitch Marcus. 1995.
Text chunking using transformation-based
learning. In Proceedings of the Third
Workshop on Very Large Corpora,
pages 82?94, Cambridge, MA.
Ravichandran, Deepak and Eduard Hovy.
2002. Learning surface text patterns for a
question answering system. In Proceedings
of ACL, pages 41?47, Philadelphia, PA.
Ravichandran, Deepak, Patrick Pantel, and
Eduard H. Hovy. 2005. Randomized
algorithms and NLP: Using locality
sensitive hash function for high speed
noun clustering. In Proceedings of ACL,
pages 622?629, Ann Arbor, MI.
Riezler, Stefan, Alexander Vasserman,
Ioannis Tsochantaridis, Vibhu O. Mittal,
and Yi Liu. 2007. Statistical machine
translation for query expansion in
answer retrieval. In Proceedings of ACL,
pages 464?471, Prague.
Romano, Lorenza, Milen Kouylekov,
Idan Szpektor, Ido Dagan, and Alberto
Lavelli. 2006. Investigating a generic
paraphrase-based approach for relation
extraction. In Proceedings of EACL,
pages 409?416, Trento.
Rus, Vasile, Philip M. McCarthy, and
Mihai C. Lintean. 2008. Paraphrase
identification with lexico-syntactic graph
subsumption. In Proceedings of the 21st
International FLAIRS Conference,
pages 201?206, Coconut Grove, FL.
Sahami, Mehran and Timothy D. Heilman.
2006. A Web-based kernel function for
measuring the similarity of short text
snippets. In Proceedings of the World
Wide Web Conference, pages 377?386,
Edinburgh.
Sekine, Satoshi. 2005. Automatic paraphrase
discovery based on context and keywords
between NE pairs. In Proceedings of the
International Workshop on Paraphrasing,
pages 80?87, Jeju Island, South Korea.
Sekine, Satoshi. 2006. On-demand
information extraction. In Proceedings of
COLING-ACL, pages 731?738, Sydney.
386
Madnani and Dorr Generating Phrasal and Sentential Paraphrases
Sekine, Satoshi, Kentaro Inui, Ido Dagan,
Bill Dolan, Danilo Giampiccolo, and
Bernardo Magnini, editors. 2007.
Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
Association for Computational
Linguistics, Prague.
Shen, Siwei, Dragomir R. Radev, Agam
Patel, and Gu?nes? Erkan. 2006. Adding
syntax to dynamic programming for
aligning comparable texts for the
generation of paraphrases. In Proceedings
of ACL-COLING, pages 747?754,
Sydney.
Shi, Xiaodong and Christopher C. Yang.
2007. Mining related queries from Web
search engine query logs using an
improved association rule mining model.
JASIST, 58(12):1871?1883.
Shimohata, Mitsuo and Eiichiro Sumita.
2005. Acquiring synonyms from
monolingual comparable texts. In
Proceedings of IJCNLP, pages 233?244,
Jeju Island.
Shinyama, Y., S. Sekine, K. Sudo, and
R. Grishman. 2002. Automatic paraphrase
acquisition from news articles. In
Proceedings of HLT, pages 313?318,
San Diego, CA.
Soong, Frank K. and Eng-Fong Huang.
1990. A tree-trellis based fast search for
finding the n-best sentence hypotheses in
continuous speech recognition. In
Proceedings of the HLT workshop on Speech
and Natural Language, pages 12?19,
Hidden Valley, PA.
Spa?rck-Jones, Karen and J. I. Tait. 1984.
Automatic search term variant
generation. Journal of Documentation,
40(1):50?66.
Steedman, Mark, editor. 1996. Surface
Structure and Interpretation (Linguistic
Inquiry Monograph No. 30). MIT Press,
Cambridge, MA.
Szpektor, Idan, Eyal Shnarch, and Ido Dagan.
2007. Instance-based evaluation of
entailment rule acquisition. In Proceedings
of ACL, pages 456?463, Prague.
Tanaka, Takaaki, Aline Villavicencio, Francis
Bond, and Anna Korhonen, editors. 2004.
Proceedings of the Workshop on Multiword
Expressions: Integrating Processing.
Association for Computational Linguistics,
Barcelona.
Uzuner, O?zlem and Boris Katz. 2005.
Capturing expression using linguistic
information. In Proceedings of AAAI,
pages 1124?1129, Pittsburgh, PA.
Wallis, Peter. 1993. Information retrieval
based on paraphrase. In Proceedings of the
3rd Conference of the Pacific Association for
Computational Linguistics (PACLING),
pages 118?126, Vancouver.
Wu, Dekai. 2005. Recognizing paraphrases
and textual entailment using inversion
transduction grammars. In Proceedings of
the ACL Workshop on Empirical Modeling of
Semantic Equivalence and Entailment,
pages 25?30, Ann Arbor, MI.
Wu, Hua and Ming Zhou. 2003. Synonymous
collocation extraction using translation
information. In Proceedings of the ACL
Workshop on Multiword Expressions:
Integrating Processing, pages 120?127,
Sapporo.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng
Li. 2009. Application-driven statistical
paraphrase generation. In Proceedings of
ACL/AFNLP, pages 834?842, Singapore.
Zhao, Shiqi, Cheng Niu, Ming Zhou, Ting
Liu, and Sheng Li. 2008. Combining
multiple resources to improve SMT-based
paraphrasing model. In Proceedings of
ACL-08: HLT, pages 1021?1029,
Columbus, OH.
Zhou, Liang, Chin-Yew Lin, and Eduard
Hovy. 2006. Re-evaluating machine
translation results with paraphrase
support. In Proceedings of EMNLP,
pages 77?84, Sydney.
Zhou, Liang, Chin-Yew Lin, Dragos Stefan
Muntenau, and Eduard Hovy. 2006.
ParaEval: Using paraphrases to
evaluate summaries automatically. In
Proceedings of HLT-NAACL, pages 447?454,
New York, NY.
387

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 305?308,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Putting the User in the Loop: Interactive Maximal Marginal Relevance for
Query-Focused Summarization
Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu, {nmadnani,bonnie}@umiacs.umd.edu
Abstract
This work represents an initial attempt to
move beyond ?single-shot? summarization to
interactive summarization. We present an ex-
tension to the classic Maximal Marginal Rel-
evance (MMR) algorithm that places a user
?in the loop? to assist in candidate selec-
tion. Experiments in the complex interac-
tive Question Answering (ciQA) task at TREC
2007 show that interactively-constructed re-
sponses are significantly higher in quality than
automatically-generated ones. This novel al-
gorithm provides a starting point for future
work on interactive summarization.
1 Introduction
Document summarization, as captured in modern
comparative evaluations such as TAC and DUC, is
mostly conceived as a ?one-shot? task. However, re-
searchers have long known that information seeking
is an iterative activity, which suggests that an inter-
active approach might be worth exploring.
This paper present a simple extension of a well-
known algorithm, Maximal Marginal Relevance
(MMR) (Goldstein et al, 2000), that places the user
in the loop. MMR is an iterative algorithm, where
at each step a candidate extract c (e.g., a sentence) is
assigned the following score:
?Rel(q, c)? (1? ?)max
s?S
Sim(s, c)
The score consists of two components: the rele-
vance of the candidate c with respect to the query
q (Rel) and the similarity of the candidate c to each
extract s in the current summary S (Sim). The maxi-
mum score from these similarity comparisons is sub-
tracted from the relevance score, subjected to a tun-
ing parameter that controls the emphasis on rele-
vance and anti-redundancy. Scores are recomputed
after each step and the algorithm iterates until a stop-
ping criterion has been met (e.g., length quota).
We propose a simple extension to MMR: at each
step, we interactively ask the user to select the best
sentence for inclusion in the summary. That is, in-
stead of the system automatically selecting the can-
didate with the highest score, it presents the user
with a ranked list of candidates for selection.
2 Complex, Interactive QA
One obstacle to assessing the effectiveness of in-
teractive summarization algorithms is the lack of a
suitable evaluation vehicle. Given the convergence
of complex QA and summarization (particularly the
query-focused variant) in recent years, we found an
appropriate evaluation vehicle in the ciQA (com-
plex, interactive Question Answering) task at TREC
2007 (Dang et al, 2007).
Information needs in the ciQA task, called top-
ics, consist of two parts: the question template and
the narrative. The question template is a stylized in-
formation need that has a fixed structure and free
slots whose instantiation varies across different top-
ics. The narrative is unstructured prose that elabo-
rates on the information need. For the evaluation,
NIST assessors developed 30 topics, grouped into
five templates. See Figure 1 for an example.
Participants in the task were able to deploy fully-
functional web-based QA systems, with which the
305
Template: What evidence is there for transport of
[drugs] from [Mexico] to [the U.S.]?
Narrative: The analyst would like to know of efforts
to curtail the transport of drugs from Mexico to the
U.S. Specifically, the analyst would like to know of
the success of the efforts by local or international au-
thorities.
Figure 1: Example topic from the TREC 2007 ciQA task.
NIST assessors interacted (serving as surrogates for
users). Upon receiving the topics, participants first
submitted an initial run. During a pre-arranged pe-
riod of time shortly thereafter, each assessor was
given five minutes to interact with the participant?s
system, live over the web. After this interaction pe-
riod, participants submitted a final run, which had
presumably gained the benefit of user interaction.
By comparing initial and final runs, it was possible
to quantify the effect of the interaction.
The target corpus was AQUAINT-2, which con-
sists of around 970k documents totaling 2.5 GB.
System responses consisted of multi-line answers
and were evaluated using the ?nugget? methodol-
ogy with the ?nugget pyramid? extension (Lin and
Demner-Fushman, 2006).
3 Experiment Design
This section describes our experiments for the
TREC 2007 ciQA task. In summary: the initial run
was generated automatically using standard MMR.
The web-based interactions consisted of iterations of
interactive MMR, where the user selected the best
candidate extract at each step. The final run con-
sisted of the output of interactive MMR padded with
automatically-generated output.
Sentence extracts were used as the basic re-
sponse unit. For each topic, the top 100 documents
were retrieved from the AQUAINT-2 collection with
Lucene, using the topic template verbatim as the
query. Neither the template structure nor the narra-
tive text were exploited. All documents were then
broken into individual sentences, which served as
the pool of candidates. The relevance of each sen-
tence was computed as the sum of the inverse doc-
ument frequencies of matching terms from the topic
template. Redundancy was computed as the cosine
similarity between the current answer (consisting of
Figure 2: Screenshot of the interface for interactive
MMR, which shows the current topic (A), the current an-
swer (B), and a ranked list of document extracts (C).
all previously-selected sentences) and the current
candidate. The relevance and redundancy scores
were then normalized and combined (? = 0.8). For
the initial run, the MMR algorithm iterated until 25
candidates had been selected.
For interactive MMR, a screenshot of the web-
based system is shown in Figure 2. The interface
consists of three elements: at the top (label A) is the
current topic; in the middle (label B) is the current
answer, containing user selections from previous it-
erations; the bottom area (label C) shows a ranked
list of candidate sentences ordered by MMR score.
At each iteration, the user is asked to select one can-
didate by clicking the ?Add to answer? button next
to that candidate. The selected candidate is then
added to the current answer. Ten answer candidates
are shown per page. Clicking on a button labeled
?Show more candidates? at the bottom of the page
(not shown in the screenshot) displays the next ten
candidates. In the ciQA 2007 evaluation, NIST as-
sessors engaged with this interface for the entire al-
lotted five minute interaction period. Note that this
simple interface was designed only to assess the ef-
fectiveness of interactive MMR, and not intended to
represent an actual interactive system.
To prevent users from seeing the same sentences
repeatedly once a candidate selection has been
recorded, we divide the scores of all candidates
ranked higher than the selected candidate by two (an
306
 0 5 10 15 20 25 30
 35 40
 56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85Number of Extracts Topic
Length of ciQA 2007 Final Answers: Number of Extracts complete answerinteractive MMRmean, interactive MMR
Figure 3: Per-topic lengths of final run in terms of num-
ber of extracts. Bars show contribution from interactive
MMR (darker) and ?padding? (lighter).
arbitrary constant). For example, if the user clicked
on candidate five, scores for candidates one through
four are cut in half. Previous studies have shown
that users generally examine ranked lists in order, so
the lack of a selection can be interpreted as negative
feedback (Joachims et al, 2007).
The answers constructed interactively were sub-
mitted to NIST as the final (post-interaction) run.
However, since these answers were significantly
shorter than the initial run (given the short interac-
tion period), the responses were ?padded? by run-
ning additional iterations of automatic MMR until a
length quota of 4000 characters had been achieved.
4 Results and Discussion
First, we present descriptive statistics of the final
run submitted to NIST. Lengths of the answers on
a per-topic basis are shown in Figure 3 in terms of
number of extracts: darker bars show the number of
manually-selected extracts for each topic during the
five-minute interaction period (i.e., the number of in-
teractive MMR iterations). The average across all
topics was 6.5 iterations, shown by the horizontal
line; the average length of answers (all user selec-
tions) was 1186 characters. The average rank of the
user selection was 4.9, and the user selected the top
ranking sentence 28% of the time. Note that the in-
teraction period included system processing as well
as delays caused by network traffic. The number of
extracts contained in the padding is shown by the
lighter gray portions of the bars. For topic 68, the
system did not record any user interactions (possi-
bly resulting from a network glitch).
 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
 0.4 0.45
 0  500  1000  1500  2000  2500  3000  3500  4000Weighted Recall Length of Answer (non-whitespace characters)
TREC 2007 ciQA: interactive vs. non-interactive MMR
non-interactive MMRinteractive MMRsig., p<0.05
Figure 4: Weighted recall at different length increments,
comparing interactive and non-interactive MMR.
The official metric for the ciQA task was F-
measure, but a disadvantage of this single-point met-
ric is that it doesn?t account for answers of vary-
ing lengths. An alternative proposed by Lin (2007)
and used as the secondary metric in the evalua-
tion is recall-by-length plots, which characterize
weighted nugget recall at varying length incre-
ments. Weighted recall captures how much rele-
vant information is contained in the system response
(weighted by each nugget?s importance, with an up-
per bound of one). Responses that achieve higher
nugget recall at shorter length increments are desir-
able in providing concise, informative answers.
Recall-by-length plots for both the initial run
(non-interactive MMR) and final run (interactive
MMR with padding) are shown in Figure 4, in length
increments of 1000 characters. The vertical dotted
line denotes the average length of interactive MMR
answers (without padding). Taking length as a proxy
for time, one natural interpretation of this plot is how
quickly users are able to ?learn? about the topic of
interest under the two conditions.
We see that interactive MMR yields higher
weighted recall at all length increments. The
Wilcoxon signed-rank test was applied to assess the
statistical significance of the differences in weighted
recall at each length increment. Solid circles in the
graph represent improvements that are statistically
significant (p < 0.05). Furthermore, in the 700?
1000 character range, weighted recall is significantly
higher for interactive MMR at the 99% level.
Viewing weighted recall as a proxy for answer
quality, interactive MMR yields responses that are
significantly better than non-interactive MMR at
307
a range of length increments. This is an impor-
tant finding, since effective interaction techniques
that require little training and work well in limited-
duration settings are quite elusive. Often, user in-
put actually makes answers worse. Results from
both ciQA 2006 and ciQA 2007 show that, overall,
F-measure improved little between initial and final
runs. Although it is widely accepted that user feed-
back can enhance interactive IR, effective interac-
tion techniques to exploit this feedback are by no
means obvious.
To better understand the characteristics of interac-
tive MMR, it is helpful to compare our experiments
with the ciQA task-wide baseline. As a reference
for all participants, the organizers of the task sub-
mitted a pair of runs to help calibrate effectiveness.
According to Dang et al (2007), the first run was
prepared by submitting the question template ver-
batim as a query to Lucene to retrieve the top 20
documents. These documents were then tokenized
into individual sentences. Sentences that contained
at least one non-stopword from the question were re-
tained and returned as the initial run (up to a quota
of 5,000 characters). Sentence order within each
document and across the ranked list was preserved.
The interaction associated with this run asked the as-
sessor for relevance judgments on each of the sen-
tences. Three options were given: ?relevant?, ?not
relevant?, and ?no opinion?. The final run was pre-
pared by removing sentences judged not relevant.
Other evidence suggests that the task-wide sen-
tence retrieval algorithm represents a strong base-
line. Similar algorithms performed well in other
complex QA tasks?in TREC 2003, a sentence re-
trieval variant beat all but one run on definition ques-
tions (Voorhees, 2003). The sentence retrieval base-
line also performed well in ciQA 2006.
The MMR runs are compared to the task-wide
reference runs in Figure 5: diamonds denote the
sentence retrieval baseline and triangles mark the
manual sentence selection final run. The manual
sentence selection run outperforms the sentence re-
trieval baseline (as expected), but its weighted recall
is still below that of interactive MMR across almost
all length increments. The weighted recall of inter-
active MMR is significantly better at 1000 characters
(at the 95% level), but nowhere else. So, the bottom
line is: for limited-duration interactions, interactive
 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
 0.4 0.45
 0  500  1000  1500  2000  2500  3000  3500  4000Weighted Recall Length of Answer (non-whitespace characters)
TREC 2007 ciQA: MMR vs. task-wide baseline
non-interactive MMRinteractive MMRsentence retrieval baselinemanual sentence selection
Figure 5: Weighted recall at different length increments,
comparing MMR with the task-wide baseline.
MMR is more effective than simply asking for rele-
vance judgments, but not significantly so.
5 Conclusion
We present an interactive extension of the Maximal
Marginal Relevance algorithm for query-focused
summarization. Results from the TREC 2007 ciQA
task demonstrate it is a simple yet effective tech-
nique for involving users in interactively construct-
ing responses to complex information needs. These
results provide a starting point for future work in in-
teractive summarization.
Acknowledgments
This work was supported in part by NLM/NIH. The
first author would like to thank Esther and Kiri for
their loving support.
References
H. Dang, J. Lin, and D. Kelly. 2007. Overview of the
TREC 2007 question answering track. TREC 2007.
J. Goldstein, V. Mittal, J. Carbonell, and J. Callan. 2000.
Creating and evaluating multi-document sentence ex-
tract summaries. CIKM 2000.
T. Joachims, L. Granka, B. Pan, H. Hembrooke,
F. Radlinski, and G. Gay. 2007. Evaluating the ac-
curacy of implicit feedback from clicks and query re-
formulations in Web search. TOIS, 25(2):1?27.
J. Lin and D. Demner-Fushman. 2006. Will pyramids
built of nuggets topple over? HLT/NAACL 2006.
J. Lin. 2007. Is question answering better than informa-
tion retrieval? Towards a task-based evaluation frame-
work for question series. HLT/NAACL 2007.
E. Voorhees. 2003. Overview of the TREC 2003 ques-
tion answering track. TREC 2003.
308
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20?28,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Identifying High-Level Organizational Elements
in Argumentative Discourse
Nitin Madnani Michael Heilman Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,mheilman,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
Argumentative discourse contains not only
language expressing claims and evidence, but
also language used to organize these claims
and pieces of evidence. Differentiating be-
tween the two may be useful for many appli-
cations, such as those that focus on the content
(e.g., relation extraction) of arguments and
those that focus on the structure of arguments
(e.g., automated essay scoring). We propose
an automated approach to detecting high-level
organizational elements in argumentative dis-
course that combines a rule-based system and
a probabilistic sequence model in a principled
manner. We present quantitative results on a
dataset of human-annotated persuasive essays,
and qualitative analyses of performance on es-
says and on political debates.
1 Introduction
When presenting an argument, a writer or speaker
usually cannot simply state a list of claims and
pieces of evidence. Instead, the arguer must explic-
itly structure those claims and pieces of evidence, as
well as explain how they relate to an opponent?s ar-
gument. Consider example 1 below, adapted from
an essay rebutting an opponent?s argument that griz-
zly bears lived in a specific region of Canada.
The argument states that based on the
result of the recent research, there proba-
bly were grizzly bears in Labrador. It may
seem reasonable at first glance, but ac-
tually, there are some logical mistakes
in it. . . . There is a possibility that they
were a third kind of bear apart from black
and grizzly bears. Also, the explorer ac-
counts were recorded in the nineteenth
century, which was more than 100 years
ago. . . . In sum, the conclusion of this
argument is not reasonable since the ac-
count and the research are not convinc-
ing enough. . . .
The argument begins by explicitly restating the
opponent?s claim, prefacing the claim with the
phrase ?The argument states that.? Then, the sec-
ond sentence explicitly marks the opponent?s argu-
ment as flawed. Later on, the phrase ?There is a
possibility that? indicates the subsequent clause in-
troduces evidence contrary to the opponent?s claim.
Finally, the sentence ?In sum, . . .? sums up the ar-
guer?s stance in relation to the opponent?s claim.1
As illustrated in the above example, argumenta-
tive discourse can be viewed as consisting of lan-
guage used to express claims and evidence, and
language used to organize them. We believe that
differentiating organizational elements from content
would be useful for analyzing persuasive discourse.
1The word Also signals that additional evidence is about to
be presented and should also be marked as shell. However, it
was not marked in this specific case by our human annotator
(?3.2).
20
We refer to such organizational elements as shell, in-
dicating that they differ from the specific claims and
evidence, or ?meat,? of an argument. In this work,
we develop techniques for detecting shell in texts.
We envision potential applications in political sci-
ence (e.g., to better understand political debates), in-
formation extraction or retrieval (e.g., to help a sys-
tem focus on content rather than organization), and
automated essay scoring (e.g., to analyze the quality
of a test-taker?s argument), though additional work
is needed to determine exactly how to integrate our
approach into such applications.
Detecting organizational elements could also be a
first step in parsing an argument to infer its structure.
We focus on this initial step, leaving the other steps
of categorization of spans (as to whether they evalu-
ate the opponent?s claims, connect one?s own claims,
etc.), and the inference of argumentation structure to
future work.
Before describing our approach to identifying
shell, we begin by defining it. Shell refers to se-
quences of words used to refer to claims and evi-
dence in persuasive writing or speaking, providing
an organizational framework for an argument. It
may be used by the writer or the speaker in the fol-
lowing ways:
? to declare one?s own claims (e.g., ?There is the
possibility that?)
? to restate an opponent?s claims (e.g., ?The argu-
ment states that?)
? to evaluate an opponent?s claims (e.g., ?It may
seem reasonable at first glance, but actually, there
are some logical mistakes in it?)
? to present evidence and relate it to specific claims
(e.g., ?To illustrate my point, I will now give the
example of?)
There are many ways of analyzing discourse. The
most relevant is perhaps rhetorical structure theory
(RST) (Mann and Thompson, 1988). To our knowl-
edge, the RST parser from Marcu (2000) is the only
RST parser readily available for experimentation.
The parser is trained to model the RST corpus (Carl-
son et al, 2001), which treats complete clauses (i.e.,
clauses with their obligatory complements) as the el-
ementary units of analysis. Thus, the parser treats
the first sentence in example 1 as a single unit and
does not differentiate between the main and subordi-
nate clauses. In contrast, our approach distinguishes
the sequence ?The argument states that . . . ? as shell
(which is used here to restate the external claim).
Furthermore, we identify the entire second sentence
as shell (here, used to evaluate the external claim),
whereas the RST parser splits the sentence into two
clauses, ?It may seem . . .? and ?but actually . . .?,
linked by a ?contrast? relationship.2 Finally, our
approach focuses on explicit markers of organiza-
tional structure in arguments, whereas RST covers a
broader range of discourse connections (e.g., elabo-
ration, background information, etc.), including im-
plicit ones. (Note that additional related work is de-
scribed in ?6.)
This work makes the following contributions:
? We describe a principled approach to the task
of detecting high-level organizational elements in
argumentative discourse, combining rules and a
probabilistic sequence model (?2).
? We conduct experiments to validate the approach
on an annotated sample of essays (?3, ?4).
? We qualitatively explore how the approach per-
forms in a new domain: political debate (?5).
2 Detection Methods
In this section, we describe three approaches to the
problem of shell detection: a rule-based system
(?2.1), a supervised probabilistic sequence model
(?2.2), and a simple lexical baseline (?2.3).
2.1 Rule-based system
We begin by describing a knowledge-based ap-
proach to detecting organizational elements in argu-
mentative discourse. This approach uses a set of 25
hand-written regular expression patterns.3
In order to develop these patterns, we created a
sample of 170 annotated essays across 57 distinct
prompts.4 The essays were written by test-takers of
a standardized test for graduate admissions. This
sample of essays was similar in nature to but did
not overlap with those discussed in other sections
2We used the RST parser of Marcu (2000) to analyze the
original essay from which the example was adapted.
3We use the PyParsing toolkit to parse sentences with the
grammar for the rule system.
4Prompts are short texts that present an argument or issue
and ask test takers to respond to it, either by analyzing the given
argument or taking a stance on the given issue.
21
MODAL? do | don?t | can | cannot | will | would | . . .
ADVERB? strongly | totally | fundamentally | vehemently | . . .
AGREEVERB? disagree | agree | concur | . . .
AUTHORNOUN? writer | author | speaker | . . .
SHELL? I [MODAL] [ADVERB] AGREEVERB with the AUTHORNOUN
Figure 1: An example pattern that recognizes shell language describing the author?s position with respect to an oppo-
nent?s, e.g., I totally agree with the author or I will strongly disagree with the speaker.
of the paper (?2.2, ?3.2). The annotations were car-
ried out by individuals experienced in scoring per-
suasive writing. No formal annotation guidelines
were provided. Besides shell language, there were
other annotations relevant to essay scoring. How-
ever, we ignored them for this study because they
are not directly relevant to the task of shell language
detection.
From this sample, we computed lists of n-grams
(n = 1, 2, . . . , 9) that occurred more than once in
essays from at least half of the 57 distinct essay
prompts. We then wrote rules to recognize the shell
language present in the n-gram lists. Additional
rules were added to cover instances of shell that we
observed in the annotated essays but that were not
frequent enough to appear in the n-gram analysis.
We use ?Rules? to refer to this method.
2.2 Supervised Sequence Model
The next approach we describe is a supervised, prob-
abilistic sequence model based on conditional ran-
dom fields (CRFs) (Lafferty et al, 2001), using a
small number of general features based on lexical
frequencies. We assume access to a labeled dataset
of N examples (w,y) indexed by i, containing se-
quences of words w(i) and sequences of labels y(i),
with individual words and labels indexed by j (?3
describes our development and testing sets). y(i) is a
sequence of binary values, indicating whether each
word w(i)j in the sequence is shell (y
(i)
j = 1) or not
(y(i)j = 0). Following Lafferty et al (2001), we find
a parameter vector ? that maximizes the following
log-likelihood objective function:
L(?|w,y) =
N?
i=1
log p
(
y(i) | w(i), ?
)
(1)
=
N?
i=1
(
?>f(w(i), y(i))? logZ(i)
)
The normalization constant Zi is a sum over all
possible label sequences for the ith example, and f
is a feature function that takes pairs of word and la-
bel sequences and returns a vector of feature values,
equal in dimensions to the number of parameters in
?.5
The feature values for the jth word and label pair
are as follows (these are summed over all elements
to compute the values of f for the entire sequence):
? The relative frequency of w(i)j in the British Na-
tional Corpus.
? The relative frequency of w(i)j in a set of 100,000
essays (see below).
? Eight binary features for whether the above fre-
quencies meet or exceed the following thresholds:
10{?6,?5,?4,?3}.
? The proportion of prompts for which w(i)j ap-
peared in at least one essay about that prompt in
the set of 100,000.
? Three binary features for whether the above pro-
portion of prompts meets or exceeds the following
thresholds: {0.25, 0.50, 0.75}.
? A binary feature with value 1 if w(i)j consists only
of letters a-z, and 0 otherwise. This feature dis-
tinguishes punctuation and numbers from other to-
kens.
5We used CRFsuite 0.12 (Okazaki, 2007) to implement the
CRF model.
22
? A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)j is shell, and 0 otherwise.
? A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)j?1 is shell, and 0 otherwise.
? Two binary features for whether or not the current
token was the first or last in the sentence, respec-
tively.
? Four binary features for the possible transitions
between previous and current labels (y(i)j and y
(i)
j?1,
respectively).
To define the features related to essay prompts
and lexical frequencies in essays, we created a set
of 100,000 essays from a larger set of essays written
by test-takers of a standardized test for graduate ad-
missions (the same domain as in ?2.1). The essays
were written in response to 228 different prompts
that asked students to analyze various issues or ar-
guments. We use additional essays sampled from
this source later to acquire annotated training and
test data (?3.2).
We developed the above feature set using cross-
validation on our development set (?3). The intu-
ition behind developing the word frequency features
is that shell language generally consists of chunks of
words that occur frequently in persuasive language
(e.g., ?claims,? ?conclude?) but not necessarily as
frequently in general text (e.g., the BNC). The se-
quence model can also learn to disprefer changes of
state, such that multi-word subsequences are labeled
as shell even though some of the individual words in
the subsequence are stop words, punctuation, etc.
Note there are a relatively small number of pa-
rameters in the model,6 which allows us to estimate
parameters on a relatively small set of labeled data.
We briefly experimented with adding an `2 penalty
on the magnitude of ? in Equation 2, but this did not
seem to improve performance.
When making predictions y?(i) about the label se-
quence for a new sentence, the most common ap-
proach is to find the most likely sequence of labels y
given the words w(i), found with Viterbi decoding:
6There were 42 parameters in our implementation of the full
CRF model. Excluding the four transition features, each of the
19 features had two parameters, one for the positive class and
one for the negative class. Having two parameters for each is
unnecessary, but we are not aware of how to have the crfsuite
toolkit avoid these extra features.
y?(i) = argmax
y
p?(y | w
(i)) (2)
We use ?CRFv? to refer to this approach. We use
the suffix ?+R? to denote models that include the
two rule-based system prediction features, and we
use ?-R? to denote models that exclude these two
features.
In development, we observed that this decoding
approach seemed to very strongly prefer labeling an
entire sentence as shell or not, which is often not
desirable since shell often appears at just the begin-
nings of sentences (e.g., ?The argument states that?).
We therefore test an alternative prediction rule
that works at the word-level, rather than sequence-
level. This approach labels each word as shell if
the sum of the probabilities of all paths in which
the word was labeled as shell?that is, the marginal
probability?exceeds some threshold ?. Words are
labeled as non-shell otherwise. Specifically, an indi-
vidual word w(i)j is labeled as shell (i.e., y?
(i)
j = 1)
according to the following equation, where 1(q) is
an indicator function that returns 1 if its argument q
is true, and 0 otherwise.
y?(i)j = 1
((
?
y
p?(y | w
(i)) yj
)
? ?
)
(3)
We tune ? using the development set, as discussed
in ?3.
We use ?CRFm? to refer to this approach.
2.3 Lexical Baseline
As a simple baseline, we also evaluated a method
that labels words as shell if they appear frequently
in persuasive writing?specifically, in the set of
100,000 unannotated essays described in ?2.2. In
this approach, word tokens are marked as shell
if they belonged to the set of k most frequent
words from the essays. Using the development
set discussed in ?3.2, we tested values of k in
{100, 200, . . . , 1000}. Setting k = 700 led to the
highest F1.
We use ?TopWords? to refer to this method.
23
3 Experiments
In this section, we discuss the design of our exper-
imental evaluation and present results on our devel-
opment set, which we used to select the final meth-
ods to evaluate on the held-out test set.
3.1 Metrics
In our experiments, we evaluated the performance
of the shell detection methods by comparing token-
level system predictions to human labels. Shell lan-
guage typically occurs as fairly long sequences of
words, but identifying the exact span of a sequence
of shell seems less important than in related tag-
ging tasks, such as named entity recognition. There-
fore, rather than evaluating based on spans (either
with exact or a partial credit system), we measured
performance at the word token-level using standard
metrics: precision, recall, and the F1 measure. For
example, for precision, we computed the propor-
tion of tokens predicted as shell by a system that
were also labeled as shell in our human-annotated
datasets.
3.2 Annotated Data
To evaluate the methods described in ?2, we gath-
ered annotations for 200 essays that were not in the
larger, unannotated set discussed in ?2.2. We split
this set of essays into a development set of 150 es-
says (68,601 word tokens) and a held-out test set of
50 essays (21,277 word tokens). An individual with
extensive experience at scoring persuasive writing
and familiarity with shell language annotated all to-
kens in the essays with judgments of whether they
were shell or not (in contrast to ?2.1, this annotation
only involved labeling shell language).
From the first annotator?s judgments on the devel-
opment set, we created a set of annotation guidelines
and trained a second annotator. The second anno-
tator marked the held-out test set so that we could
measure human agreement. Comparing the two an-
notators? test set annotations, we observed agree-
ment of F1 = 0.736 and Cohen?s ? = 0.699 (we
do not use ? in our experiments but report it here
since it is a common measure of human agreement).
Except for measuring agreement, we did not use the
second annotator?s judgments in our experiments.7
7In the version of this paper submitted for review, we mea-
recall
pre
cisi
on
0.0
0.2
0.4
0.6
0.8
1.0
l
0.0 0.2 0.4 0.6 0.8 1.0
linesCRFm?RCRFm+R
points
l CRFm?RCRFm+RCRFv?RCRFv+RRulesTopWords
Figure 2: Precision and recall of the detection methods at
various thresholds, computed through cross-validation on
the development set. Points indicate performance for the
rule-based and baseline system as well as points where
F1 is highest.
3.3 Cross-validation Results
To develop the CRF?s feature set, to tune hyperpa-
rameters, and to select the most promising systems
to evaluate on the test set, we randomly split the sen-
tences from the development set into two halves and
conducted tests with two-fold cross-validation.
We tested thresholds for the CRF at ? =
{0.01, 0.02, . . . , 1.00}.
Figure 2 shows the results on the development set.
For the rule-based system, which did not require la-
beled data, performance is computed on the entire
development set. For the CRF approaches, the pre-
cision and recall were computed after concatenating
predictions on each of the cross-validation folds.
The TopWords baseline performed quite poorly,
with F1 = 0.205. The rule-based system performed
much better, with F1 = 0.382, but still not as well
as the CRF systems. The CRF systems that pre-
dict maximum sequences had F1 = 0.382 without
the rule-based system features (CRFv?R), and F1 =
0.467 with the rule-based features (CRFv+R). The
CRF systems that made predictions from marginal
scores performed best, with F1 = 0.516 without
the rule-based features, and F1 = 0.551 with the
rule-based features. Thus, both the rule-based sys-
sured test set agreement with judgments from a third individ-
ual, who was informally trained by the first, without the formal
guidelines. Agreement was somewhat lower: F1 = 0.668 and
? = 0.613.
24
Method P R F1 Len
TopWords 0.125 0.759 0.214 ? 2.80
Rules 0.561 0.360 0.439 ? 4.99
CRFv?R 0.729 0.268 0.392 ? 15.67
CRFv+R 0.763 0.369 0.498 ? 13.30
CRFm?R 0.586 0.574 0.580 9.00
CRFm+R 0.556 0.670 0.607 9.96
Human 0.685 0.796 0.736 ? 7.91
Table 1: Performance on the held-out test set, in terms of
precision (P), recall (R), F1 measure, and average length
in tokens of sequences of one or more words labeled as
shell (Len). ? indicates F1 scores that are statistically
reliably different from CRFm+R at the p < 0.01 level.
tem features and the marginal prediction approach
led to gains in performance.
From an examination of the predictions from the
CRFm+R and CRFm?R systems, it appears that a
major contribution of the features derived from the
rule-based system is to help the hybrid CRFm+R
system avoid tagging entire sentences as shell when
only parts of them are actually shell. For exam-
ple, consider the sentence ?According to this state-
ment, the speaker asserts that technology can not
only influence but also determine social customs and
ethics? (typographical errors included). CRFm?R
tags everything up to ?determine? as shell, whereas
the rule-based system and CRFm+R correctly stop
after ?asserts that.?
4 Test Set Results
Next, we present results on the held-out test set.
For the CRFm systems, we used the thresholds that
led to the highest F1 scores on the development
set (? = 0.26 for CRFm+R and ? = 0.32 for
CRFm?R). Table 1 presents the results for all sys-
tems, along with results comparing the second anno-
tator?s labels (?Human?) to the gold standard labels
from the first annotator.
The same pattern emerged as on the development
set, with CRFm+R performing the best. The F1
score of 0.607 for the CRFm+R system was rel-
atively close to the F1 score of 0.736 for agree-
ment between human annotators. To test whether
CRFm+R?s relatively high performance was due to
chance, we computed 99% confidence intervals for
the differences in F1 score between CRFm+R and
each of the other methods. We used the bias-
corrected and accelerated (BCa) Bootstrap (Efron
and Tibshirani, 1993) with 10,000 rounds of resam-
pling at the sentence level for each comparison. A
difference is statistically reliable at the ? level (i.e.,
p < ?) if the (1 ? ?)% confidence interval for the
difference does not contain zero, which corresponds
to the null hypothesis. Statistically reliable differ-
ences are indicated in Table 1. The only system that
did not have a reliably lower F1 score than CRFm+R
was CRFm?R, though due to the relatively small
size of our test set, we do not take this as strong ev-
idence against using the rule-based system features
in the CRF.
We note that while the CRFm+R system had lower
precision (0.556) than the CRFv+R system (0.763),
its threshold ? could be tuned to prefer high preci-
sion rather than the best development set F1. Such
tuning could be very important depending on the rel-
ative costs of false positives and false negatives for
a particular application.
We also computed the mean length of sequences
of one or more contiguous words labeled as shell.
Here also, we observed that the CRFm+R approach
provided a close match to human performance. The
mean lengths of shell for the first and second anno-
tators were 8.49 and 7.91 tokens, respectively. For
the CRFm+R approach, the mean length was slightly
higher at 9.96 tokens, but this was much closer to the
means of the human annotators than the mean for
the CRFv+R system, which was 13.30 tokens. For
the rule-based system, the mean length was 4.99 to-
kens, indicating that it captures short sequences such
as ?In addition,? more often than the other systems.
5 Observations about a New Domain
In this section, we apply our system to a corpus of
transcripts of political debates8 in order to under-
stand whether the system can generalize to a new
domain with a somewhat different style of argu-
mentation. Our analyses are primarily qualitative
in nature due to the lack of gold-standard annota-
tions. We chose two historically well-known debates
8The Lincoln?Douglas debates were downloaded from
http://www.bartleby.com/251/. The other debates
were downloaded from http://debates.org/.
25
(Lincoln?Douglas from 1858 and Kennedy?Nixon
from 1960) and two debates that occurred more re-
cently (Gore?Bush from 2000 and Obama?McCain
from 2008). These debates range in length from
38,000 word tokens to 65,000 word tokens.
Political debates are similar to the persuasive es-
says we used above in that debate participants state
their own claims and evidence as well as evaluate
their opponents? claims. They are different from es-
says in that they are spoken rather than written?
meaning that they contain more disfluencies, collo-
quial language, etc.?and that they cover different
social and economic issues. Also, the debates are in
some sense a dialogue between two people.
We tagged all the debates using the CRFm+R sys-
tem, using the same parameters as for the test set
experiments (?4).
First, we observed that a smaller percentage of
tokens were tagged as shell in the debates than in
the essays. For the annotated essay test set (?3.2),
the percentage of tokens tagged as shell was 14.0%
(11.6% were labeled as shell by the first annota-
tor). In contrast, the percentage of tokens tagged
as shell was 4.2% for Lincoln?Douglas, 5.4% for
Kennedy?Nixon, 4.6% for Gore?Bush, and 4.8% for
Obama?McCain. It is not completely clear whether
the smaller percentages tagged as shell are due to a
lack of coverage by the shell detector or more sub-
stantial differences in the domain.
However, it seems that these debates genuinely in-
clude less shell. One potential reason is that many of
the essay prompts asked test-takers to respond to a
particular argument, leading to responses containing
many phrases such as ?The speaker claims that? and
?However, the argument lacks specificity . . . ?.
We analyzed the system?s predictions and ex-
tracted a set of examples, some of which appear in
Table 2, showing true positives, where most of the
tokens appear to be labeled correctly as shell; false
positives, where tokens were incorrectly labeled as
shell; and false negatives, where the system missed
tokens that should have been marked.
Table 2 also provides some examples from our de-
velopment set, for comparison.
We observed many instances of correctly marked
shell, including many that appeared very different
in style than the language used in essays. For ex-
ample, Lincoln demonstrates an aggressive style in
the following: ?Now, I say that there is no charitable
way to look at that statement, except to conclude that
he is actually crazy.? Also, Bush employs a some-
what atypical sentence structure here: ?It?s not what
I think and its not my intentions and not my plan.?
However, the system also incorrectly tagged se-
quences as shell, particularly in short sentences (e.g.,
?Are we as strong as we should be??). It also missed
shell, partially or entirely, such as in the following
example: ?But let?s get back to the core issue here.?
These results suggest that although there is poten-
tial for improvement in adapting to new domains,
our approach to shell detection at least partially gen-
eralizes beyond our initial domain of persuasive es-
say writing.
6 Related Work
There has been much previous work on analyzing
discourse. In this section, we describe similarities
and differences between that work and ours.
Rhetorical structure theory (Mann and Thomp-
son, 1988) is perhaps the most relevant area of work.
See ?1 for a discussion.
In research on intentional structure, Grosz and
Sidner (1986) propose that any discourse is com-
posed of three interacting components: the linguistic
structure defined by the actual utterances, the inten-
tional structure defined by the purposes underlying
the discourse, and an attentional structure defined by
the discourse participants? focus of attention. De-
tecting shell may also be seen as trying to identify
explicit cues of intentional structure in a discourse.
Additionally, the categorization of shell spans as to
whether they evaluate the opponents claims, connect
ones own claims, etc., may be seen as determining
what Grosz and Sidener call ?discourse segment pur-
poses? (i.e., the intentions underlying the segments
containing the shell spans).
We can also view shell detection as the task of
identifying phrases that indicate certain types of
speech acts (Searle, 1975). In particular, we aim to
identify markers of assertive speech acts, which de-
clare that the speaker believes a certain proposition,
and expressive speech acts, which express attitudes
toward propositions.
Shell also overlaps with the concept of discourse
markers (Hutchinson, 2004), such as ?however? or
26
LINCOLN (L) ? DOUGLAS (D) DEBATES
TP L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is
actually crazy.
L: The first thing I see fit to notice is the fact that . . .
FP D: He became noted as the author of the scheme to . . .
D: . . . such amendments were to be made to it as would render it useless and inefficient . . .
FN D: I wish to impress it upon you, that every man who voted for those resolutions . . .
L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to
come down here . . .
KENNEDY (K) ? NIXON (N) DEBATES
TP N: I favor that because I believe that?s the best way to aid our schools . . .
N: And in our case, I do believe that our programs will stimulate the creative energies of . . .
FP N: We are for programs, in addition, which will see that our medical care for the aged . . .
K: Are we as strong as we should be?
FN K: I should make it clear that I do not think we?re doing enough . . .
N: Why did Senator Kennedy take that position then? Why do I take it now?
BUSH (B) ? GORE (G) DEBATES
TP B: It?s not what I think and its not my intentions and not my plan.
G: And FEMA has been a major flagship project of our reinventing government efforts. And I agree, it
works extremely well now.
FP B: First of all, most of this is at the state level.
G: And it focuses not only on increasing the supply, which I agree we have to do, but also on . . .
FN B: My opponent thinks the government?the surplus is the government?s money. That?s not what I
think
G: I strongly support local control, so does Governor Bush.
OBAMA (O) ? MCCAIN (M) DEBATES
TP M: But the point is?the point is, we have finally seen Republicans and Democrats sitting down and
negotiating together . . .
O: And one of the things I think we have to do is make sure that college is affordable . . .
FP O: . . . but in the short term there?s an outlay and we may not see that money for a while.
O: We have to do that now, because it will actually make our businesses and our families better off.
FN O: So I think the lesson to be drawn is that we should never hesitate to use military force . . . to keep the
American people safe.
O: But let?s get back to the core issue here.
PERSUASIVE ESSAYS (DEVELOPMENT SET, SPELLING ERRORS INCLUDED)
TP However, the argument lacks specificity and relies on too many questionable assumptions to make a
strong case for adopting an expensive and logistically complicated program.
I believe that both of these claims have been made in hase and other factors need to be considered.
FP Since they are all far from now, the prove is not strong enough to support the conclusion.
As we know that one mind can not think as the other does.
FN History has proven that . . .
The given issue which states that in any field of inquiry . . . is a controversional one.
Table 2: Examples of CRFm+R performance. Underlining marks tokens predicted to be shell, and bold font indicates
shell according to human judgments (our judgments for the debate transcripts, and the annotator?s judgments for the
development set). Examples include true positives (TP), false positives (FP), and false negatives (FN). Note that some
FP and FN examples include partially accurate predictions.
27
?therefore.? Discourse markers, however, are typ-
ically only single words or short phrases that ex-
press a limited number of relationships. On the other
hand, shell can capture longer sequences that ex-
press more complex relationships between the com-
ponents of an argumentative discourse (e.g., ?But
let?s get back to the core issue here? signals that the
following point is more important than the previous
one).
There are also various other approaches to ana-
lyzing arguments. Notably, much recent theoreti-
cal research on argumentation has focused on ar-
gumentation schemes (Walton et al, 2008), which
are high-level strategies for constructing arguments
(e.g., argument from consequences). Recently, Feng
and Hirst (2011) developed automated methods for
classifying texts by argumentation scheme. In sim-
ilar work, Anand et al (2011) use argumentation
schemes to identify tactics in blog posts (e.g., moral
appeal, social generalization, appeals to external au-
thorities etc.). Although shell language can certainly
be found in persuasive writing, it is used to orga-
nize the persuader?s tactics and claims rather than
to express them. For example, consider the follow-
ing sentence: ?It must be the case that this diet
works since it was recommended by someone who
lost 20 pounds on it.? In shell detection, we focus
on the lexico-syntactic level, aiming to identify the
bold words as shell. In contrast, work on argumenta-
tion schemes focuses at a higher level of abstraction,
aiming to classify the sentence as an attempt to per-
suade by appealing to an external authority.
7 Conclusions
In this paper, we described our approach to detect-
ing language used to explicitly structure an arguer?s
claims and pieces of evidence as well as explain
how they relate to an opponent?s argument. We im-
plemented a rule-based system, a supervised proba-
bilistic sequence model, and a principled hybrid ver-
sion of the two. We presented evaluations of these
systems using human-annotated essays, and we ob-
served that the hybrid sequence model system per-
formed the best. We also applied our system to po-
litical debates and found evidence of the potential to
generalize to new domains.
Acknowledgments
We would like to thank the annotators for helping
us create the essay data sets. We would also like
to thank James Carlson, Paul Deane, Yoko Futagi,
Beata Beigman Klebanov, Melissa Lopez, and the
anonymous reviewers for their useful comments on
the paper and annotation scheme.
References
P. Anand, J. King, J. Boyd-Graber, E. Wagner, C. Martell,
D. Oard, and P. Resnik. 2011. Believe me?we can
do this! annotating persuasive acts in blog text. In
Proc. of AAAI Workshop on Computational Models of
Natural Argument.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proc. of the Second
SIGdial Workshop on Discourse and Dialogue.
B. Efron and R. Tibshirani. 1993. An Introduction to the
Bootstrap. Chapman and Hall/CRC.
V. W. Feng and G. Hirst. 2011. Classifying arguments
by scheme. In Proc. of ACL.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
put. Linguist., 12(3):175?204.
B. Hutchinson. 2004. Acquiring the meaning of dis-
course markers. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3).
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
N. Okazaki. 2007. CRFsuite: a fast implementation of
conditional random fields (CRFs).
J. R. Searle. 1975. A classification of illocutionary acts.
Language in Society, 5(1).
D. Walton, C. Reed, and F. Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
28
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182?190,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Re-examining Machine Translation Metrics for Paraphrase Identification
Nitin Madnani Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
We propose to re-examine the hypothesis that
automated metrics developed for MT evalu-
ation can prove useful for paraphrase iden-
tification in light of the significant work on
the development of new MT metrics over the
last 4 years. We show that a meta-classifier
trained using nothing but recent MT metrics
outperforms all previous paraphrase identifi-
cation approaches on the Microsoft Research
Paraphrase corpus. In addition, we apply our
system to a second corpus developed for the
task of plagiarism detection and obtain ex-
tremely positive results. Finally, we conduct
extensive error analysis and uncover the top
systematic sources of error for a paraphrase
identification approach relying solely on MT
metrics. We release both the new dataset and
the error analysis annotations for use by the
community.
1 Introduction
One of the most important reasons for the recent
advances made in Statistical Machine Translation
(SMT) has been the development of automated met-
rics for evaluation of translation quality. The goal
of any such metric is to assess whether the trans-
lation hypothesis produced by a system is seman-
tically equivalent to the source sentence that was
translated. However, cross-lingual semantic equiv-
alence is even harder to assess than monolingual,
therefore, most MT metrics instead try to measure
whether the hypothesis is semantically equivalent to
a human-authored reference translation of the same
source sentence. Using such automated metrics as
proxies for human judgments can provide a quick as-
sessment of system performance and allow for short
feature and system development cycles, which are
important for evaluating research ideas.
In the last 5 years, several shared tasks and com-
petitions have led to the development of increasingly
sophisticated metrics that go beyond the computa-
tion of n-gram overlaps (BLEU, NIST) or edit dis-
tances (TER, WER, PER etc.). Note that the task
of an MT metric is essentially one of identifying
whether the translation produced by a system is a
paraphrase of the reference translation. Although
the notion of using MT metrics for the task of para-
phrase identification is not novel (Finch et al, 2005;
Wan et al, 2006), it merits a re-examination in the
light of the development of these novel MT metrics
for which we can ask ?How much better, if at all,
do these newer metrics perform for the task of para-
phrase identification??
This paper describes such a re-examination. We
employ 8 different MT metrics for identifying
paraphrases across two different datasets - the
well-known Microsoft Research paraphrase corpus
(MSRP) (Dolan et al, 2004) and the plagiarism
detection corpus (PAN) from the 2010 Uncovering
Plagiarism, Authorship and Social Software Misuse
shared task (Potthast et al, 2010). We include both
MSRP and PAN in our study because they represent
two very different sources of paraphrased text. The
creation of MSRP relied on the massive redundancy
of news articles on the web and extracted senten-
tial paraphrases from different stories written about
the same topic. In the case of PAN, humans con-
sciously paraphrased existing text to generate new,
182
plagiarized text.
In the next section, we discuss previous work on
paraphrase identification. In ?3, we describe our ap-
proach to paraphrase identification using MT met-
rics as features. Our approach yields impressive re-
sults ? the current state of the art for MSRP and ex-
tremely positive for PAN. In the same section, we
examine whether each metric?s purported strength is
demonstrated in our datasets. Next, in ?4 we con-
duct an analysis of our system?s misclassifications
for both datasets and outline a taxonomy of errors
that our system makes. We also look at annotation
errors in the datasets themselves. We discuss the
findings of the error analysis in ?5 and conclude in
?6.
2 Related Work & Our Contributions
Our goal in this paper is to examine the utility of a
paraphrase identification approach that relies solely
on MT evaluation metrics and no other evidence of
semantic equivalence. Given this setup, the most rel-
evant previous work is by Finch et al (2005) which
uses BLEU, NIST, WER and PER as features for
a supervised classification approach using SVMs.
In addition, they also incorporate part-of-speech in-
formation as well as the Jiang-Conrath WordNet-
based lexical relatedness measure (Jiang and Con-
rath, 1997) into their edit distance calculations. In
the first part of our paper, we present classification
experiments with newer MT metrics not available in
2005, a worthwhile exercise in itself. However, we
go much further in our study:
? We apply our approach to two different para-
phrase datasets (MSRP and PAN) that were cre-
ated via different processes.
? We attempt to find evidence of each metric?s
purported strength in both datasets.
? We conduct an extensive error analysis to find
types of errors that a system based solely on
MT metrics is likely to make. In addition, we
also discover interesting paraphrase pairs in the
datasets.
? We release our sentence-level PAN dataset (see
?3.3.2) which contains more realistic exam-
ples of paraphrase and can prove useful to the
community for future evaluations of paraphrase
identification.
BLEU-based features were also employed by
Wan et al (2006) who use them in combination with
several other features based on dependency relations
and tree edit-distance inside an SVM.
There are several other supervised approaches to
paraphrase identification that do not use any features
based on MT metrics. Mihalcea et al (2006) com-
bine pointwise mutual information, latent semantic
analysis and WordNet-based measures of word se-
mantic similarity into an arbitrary text-to-text sim-
ilarity metric. Qiu et al (2006) build a frame-
work that detects dissimilarities between sentences
and makes its paraphrase judgment based on the
significance of such dissimilarities. Kozareva and
Montoyo (2006) use features based on LCS, skip
n-grams and WordNet with a meta-classifier com-
posed of SVM, k-nearest neighbor and maximum
entropy classifiers. Islam and Inkpen (2007) mea-
sure semantic similarity using a corpus-based mea-
sure and a modified version of the Longest Common
Subsequence (LCS) algorithm. Rus et al (2008)
take a graph-based approach originally developed
for recognizing textual entailment and adapt it for
paraphrase identification. Fernando and Stevenson
(2008) construct a matrix of word similarities be-
tween all pairs of words in both sentences instead
of relying only on the maximal similarities. Das and
Smith (2009) use an explicit model of alignment be-
tween the corresponding parts of two paraphrastic
sentences and combine it with a logistic regression
classifier built from n-gram overlap features. Most
recently, Socher et al (2011) employ a joint model
that incorporates the similarities between both sin-
gle word features as well as multi-word phrases ex-
tracted from the parse trees of the two sentences.
We compare our results to those from all the ap-
proaches described in this section later in ?3.4.
3 Classifying with MT Metrics
In this section, we first describe our overall approach
to paraphrase identification that utilizes only MT
metrics. We then discuss the actual MT metrics we
used. Finally, we describe the datasets on which we
evaluated our approach and present our results.
183
MSRP
They had published an advertisement on the Internet on June 10,
offering the cargo for sale, he added.
On June 10, the ship?s owners had published an advertisement on the
Internet, offering the explosives for sale.
Security lights have also been installed and police have swept
the grounds for booby traps.
Security lights have also been installed on a barn near the front gate.
PAN
Dense fogs wrapped the mountains that shut in the little hamlet,
but overhead the stars were shining in the near heaven.
The hamlet is surrounded by mountains which is wrapped with dense
fogs, though above it, near heaven, the stars were shining.
In still other places, the strong winds carry soil over long
distances to be mixed with other soils.
In other places, where strong winds blow with frequent regularity,
sharp soil grains are picked up by the air and hurled against the
rocks, which, under this action, are carved into fantastic forms.
Table 1: Examples of paraphrases and non-paraphrases (in italics) from the MSRP and PAN corpora.
3.1 Classifier
Our best system utilized a classifier combination ap-
proach. We used a simple meta-classifier that uses
the average of the unweighted probability estimates
from the constituent classifiers to make its final de-
cision. We used three constituent classifiers: Logis-
tic regression, the SMO implementation of a support
vector machine (Platt, 1999; Keerthi et al, 2001)
and a lazy, instance-based classifier that extends the
nearest neighbor algorithm (Aha et al, 1991). We
used the WEKA machine learning toolkit to perform
our experiments (Hall et al, 2009). 1
3.2 MT metrics used
1. BLEU (Papineni et al, 2002) is the most com-
monly used metric for MT evaluation. It is
computed as the amount of n-gram overlap?
for different values of n?between the system
output and the reference translation, tempered
by a penalty for translations that might be too
short. BLEU relies on exact matching and has
no concept of synonymy or paraphrasing. We
use BLEU1 through BLEU4 as 4 different fea-
1These constituent classifiers were chosen since they were
the top 3 performers in 5-fold cross-validation experiments
conducted on both MSRP and PAN training sets. The meta-
classifier was chosen similarly once the constituent classifiers
had been chosen.
tures for our classifier (hereafter BLEU(1-4)).
2. NIST (Doddington, 2002) is a variant of BLEU
that uses the arithmetic mean of n-gram over-
laps, rather than the geometric mean. It also
weights each n-gram according to its informa-
tiveness as indicated by its frequency. We use
NIST1 through NIST5 as 5 different features
for our classifier (hereafter NIST(1-5)).
3. TER (Snover et al, 2006) is defined as the
number of edits needed to ?fix? the translation
output so that it matches the reference. TER
differs from WER in that it includes a heuris-
tic algorithm to deal with shifts in addition to
insertions, deletions and substitutions.
4. TERp (TER-Plus) (Snover et al, 2009) builds
upon the core TER algorithm by providing ad-
ditional edit operations based on stemming,
synonymy and paraphrase.
5. METEOR (Denkowski and Lavie, 2010) uses
a combination of both precision and recall un-
like BLEU which focuses on precision. Fur-
thermore, it incorporates stemming, synonymy
(via WordNet) and paraphrase (via a lookup ta-
ble).
6. SEPIA (Habash and El Kholy, 2008) is a
syntactically-aware metric designed to focus on
184
structural n-grams with long surface spans that
cannot be captured efficiently with surface n-
gram metrics. Like BLEU, it is a precision-
based metric and requires a length penalty to
minimize the effects of length.
7. BADGER (Parker, 2008) is a language inde-
pendent metric based on compression and in-
formation theory. It computes a compression
distance between the two sentences that utilizes
the Burrows Wheeler Transformation (BWT).
The BWT enables taking into account common
sentence contexts with no limit on the size of
these contexts.
8. MAXSIM (Chan and Ng, 2008) treats the
problem as one of bipartite graph matching and
maps each word in one sentence to at most one
word in the other sentence. It allows the use of
arbitrary similarity functions between words.2
Our choice of metrics was based on their popular-
ity in the MT community, their performance in open
competitions such as the NIST MetricsMATR chal-
lenge (NIST, 2008) and the WMT shared evaluation
task (Callison-Burch et al, 2010), their availability,
and their relative complementarity.
3.3 Datasets
In this section, we describe the two datasets that we
used to evaluate our approach.
3.3.1 Microsoft Research Paraphrase Corpus
The MSRP corpus was created by mining news
articles on the web for topically similar articles and
then extracting potential sentential paraphrases us-
ing a set of heuristics. Extracted pairs were then
shown to two human judges with disagreements
handled by a third adjudicator. The kappa was re-
ported as 0.62, which indicates moderate to high
agreement. We used the pre-stipulated train-test
splits (4,076 sentence pairs in training and 1,725 in
test) to train and test our classifier.
2We also experimented with TESLA?a variant of
MAXSIM that performs better for MT evaluation?in our pre-
liminary experiments However, both MAXSIM and TESLA
performed almost identically in our cross-validation experi-
ments. Therefore, we only retained MAXSIM in our final ex-
periment since it was significantly faster to run than the version
of TESLA we had.
3.3.2 Plagiarism Detection Corpus (PAN)
We wanted to evaluate our approach on a set of
paraphrases where the semantic similarity was not
simply an accidental by-product of topical similarity
but rather consciously generated. We used the test
collection from the PAN 2010 plagiarism detection
competition. This dataset consists of 41,233 text
documents from Project Gutenberg in which 94,202
cases of plagiarism have been inserted. The pla-
giarism was created either by using an algorithm or
by explicitly asking Turkers to paraphrase passages
from the original text. We focus only on the human-
created plagiarism instances.
Note also that although the original PAN dataset
has been used in plagiarism detection shared tasks,
those tasks are generally formulated differently in
that the goal is to find all potentially plagiarized pas-
sages in a given set of documents along with the cor-
responding source passages from other documents.
In this paper, we wanted to focus on the task of iden-
tifying whether two given sentences can be consid-
ered paraphrases.
To generate a sentence-level PAN dataset, we
wrote a heuristic alignment algorithm to find cor-
responding pairs of sentences within a passage pair
linked by the plagiarism relationship. The align-
ment algorithm utilized only bag-of-words overlap
and length ratios and no MT metrics. For our nega-
tive evidence, we sampled sentences from the same
document and extracted sentence pairs that have at
least 4 content words in common. We then sampled
randomly from both the positive and negative evi-
dence files to create a training set of 10,000 sentence
pairs and a test set of 3,000 sentence pairs.
Table 1 shows examples of paraphrastic and non-
paraphrastic sentence pairs from both the MSRP and
PAN datasets.
3.4 Results
Before presenting the results of experiments that
used multiple metrics as features, we wanted to de-
termine how well each metric performs on its own
when used for paraphrase identification. Table 2
shows the classification results on both the MSRP
and PAN datasets using each metric as the only fea-
ture. Although previously explored metrics such as
BLEU and NIST perform reasonably well, they are
185
MSRP PAN
Metric Acc. F1 Acc. F1
MAXSIM 67.2 79.4 84.7 83.4
BADGER 67.6 79.9 88.5 87.9
SEPIA 68.1 79.8 87.7 86.8
TER 69.9 80.9 85.7 83.8
BLEU(1-4) 72.3 80.9 87.9 87.1
NIST(1-5) 72.8 81.2 88.2 87.3
METEOR 73.1 81.0 89.5 88.9
TERp 74.3 81.8 91.2 90.9
Table 2: Classification results for MSRP and PAN with
individual metrics as features. Entries are sorted by accu-
racies on MSRP.
clearly outperformed by some of the more robust
metrics such as TERp and METEOR.
Table 3 shows the results of our experiments em-
ploying multiple metrics as features, for both MSRP
and PAN. The final row in the table shows the results
of our best system. The remaining rows of this table
show the top performing metrics for both datasets;
we treat BLEU, NIST and TER as our baseline met-
rics since they are not new and are not the primary
focus of our investigation. In terms of novel met-
rics, we find that the top 3 metrics for both datasets
were TERp, METEOR and BADGER respectively
as shown. Combining all 8 metrics led to the best
performance for MSRP but showed no performance
increase for PAN.
MSRP PAN
Features Acc. F1 Acc. F1
Base Metrics 74.1 81.5 88.6 87.8
+ TERp 75.6 82.5 91.5 91.2
+ METEOR 76.6 83.2 92.0 91.8
+ BADGER 77.0 83.7 92.3 92.1
+ Others 77.4 84.1 92.3 92.1
Table 3: The top 3 performing MT metrics for both
MSRP and PAN datasets as identified by ablation stud-
ies. BLEU(1-4), NIST(1-5) and TER were used as the 10
base features in the classifiers.
Our results for the PAN dataset are much better than
those for MSRP since:
(a) It is likely that our negative evidence is too easy
for most MT metrics.
(b) Many plagiarized pairs are linked simply via
lexical synonymy which can be easily captured
by metrics like METEOR and TERp, e.g., the
sentence ?Young?s main contention is that in lit-
erature genius must make rules for itself, and
that imitation is suicidal? is simply plagiarized
as ?Young?s major argument is that in litera-
ture intellect must make rules for itself, and
that replication is dangerous.? However, the
PAN corpus does contains some very challeng-
ing and interesting examples of paraphrases?
even more so than MSRP?which we describe
in ?4.
Finally, Table 4 shows that the results from our
best system are the best ever reported on the MSRP
test set when compared to all previously published
work. Furthermore, the single best performing met-
ric (TERp)?also shown in the table?outperforms,
by itself, many previous approaches utilizing multi-
ple, complex features.
Model Acc. F1
All Paraphrase Baseline 66.5 79.9
(Mihalcea et al, 2006) 70.3 81.3
(Rus et al, 2008) 70.6 80.5
(Qiu et al, 2006) 72.0 81.6
(Islam and Inkpen, 2007) 72.6 81.3
(Fernando and Stevenson, 2008) 74.1 82.4
TERp 74.3 81.8
(Finch et al, 2005) 75.0 82.7
(Wan et al, 2006) 75.6 83.0
(Das and Smith, 2009) 76.1 82.7
(Kozareva and Montoyo, 2006) 76.6 79.6
(Socher et al, 2011) 76.8 83.6
Best MT Metrics 77.4 84.1
Table 4: Comparing the accuracy and F -score for the sin-
gle best performing MT metric TERp (in gray) as well as
the best metric combination system (in gray and bold)
with previously reported results on the MSRP test set
(N = 1, 752). Entries are sorted by accuracy.
3.5 Metric Contributions
In addition to quantitative results, we also wanted to
highlight specific examples from our datasets that
can demonstrate the strength of the new metrics
over simple n-gram overlap and edit-distance based
metrics. Below we present examples for the 4 best
186
metrics across both datasets:
? TERp uses stemming and phrasal paraphrase
recognition to accurately classify the sentence
pair ?For the weekend, the top 12 movies
grossed $157.1 million, up 52 percent from
the same weekend a year earlier.? and ?The
overall box office soared, with the top 12
movies grossing $157.1 million, up 52 percent
from a year ago.? from MSRP as paraphrases.
? METEOR uses synonymy and stemming
to accurately classify the sentence pair ?Her
letters at this time exhibited the two extremes of
feeling in a marked degree.? and ?Her letters
at this time showed two extremes of feelings.?
from PAN as plagiarized.
? BADGER uses unsupervised contextual
similarity detection to accurately classify the
sentence pair ?Otherwise they were false or
mistaken reactions? and ?Otherwise, were false
or wrong responses? from PAN as plagiarized.
? SEPIA uses structural n-grams via dependency
trees to accurately classify the sentence pair
?At his sentencing, Avants had tubes in his
nose and a portable oxygen tank beside him.?
and ?Avants, wearing a light brown jumpsuit,
had tubes in his nose and a portable oxygen
tank beside him.? from MSRP as paraphrases.
4 Error Analysis
In this section, we conduct an analysis of the
misclassifications that our system makes on both
datasets. Our analyses consisted of finding the sen-
tences pairs from the test set for each dataset which
none of our systems (not just the best one) ever clas-
sified correctly and inspecting a random sample of
100 of these. This inspection yields not only the top
sources of error for an approach that relies solely on
MT metrics but also uncovers sources of annotation
errors in both datasets themselves.
4.1 MSRP
In their paper describing the creation of the MSRP
corpus, Dolan et al (2004) clearly state that ?the de-
gree of mismatch allowed before the pair was judged
non-equivalent was left to the discretion of the indi-
vidual rater? and that ?many of the 33% of sentence
pairs judged to be not equivalent still overlap signif-
icantly in information content and even wording?.
We found evidence that the raters were not always
consistent in applying the annotation guidelines. For
example, in some cases the lack of attribution for a
quotation led the raters to label a pair as paraphrastic
whereas in other cases it did not. For example, the
pair ?These are real crimes that hurt a lot of people.?
and ??These are real crimes that disrupt the lives of
real people,? Smith said.? was not marked as para-
phrastic. Furthermore, even though the guidelines
instruct the raters to ?treat anaphors and their full
forms as equivalent, regardless of how great the dis-
parity in length or lexical content between the two
sentences?, we found pairs of sentences marked as
non-paraphrastic which only differed in anaphora.
However, the primary goal of this analysis is to find
sources of errors in an MT-metric driven approach
and below we present the top 5 such sources:
1. Misleading Lexical Overlap. Non-
paraphrastic pairs where there is large
lexical overlap of secondary material between
the two sentences but the primary semantic
content is different. For example, ?Gyorgy
Heizler, head of the local disaster unit, said the
coach had been carrying 38 passengers.?
and ?The head of the local disaster
unit, Gyorgy Heizler, said the coach
driver had failed to heed red stop lights.?.
2. Lack of World Knowledge. Paraphrastic
pairs that require world knowledge. For ex-
ample, ?Security experts are warning that a
new mass-mailing worm is spreading widely
across the Internet, sometimes posing as e-
mail from the Microsoft founder.? and ?A
new worm has been spreading rapidly across
the Internet, sometimes pretending to be
an e-mail from Microsoft Chairman Bill Gates,
antivirus vendors said Monday.?.
3. Tricky Phrasal Paraphrases. Paraphras-
187
tic pairs that contain domain-dependent se-
mantic alternations. For example, ?The
leading actress nod went to energetic new-
comer Marissa Jaret Winokur as Edna?s
daughter Tracy.? and ?Marissa Jaret Winokur,
as Tracy, won for best actress in a musical.?.
4. Date, Time and Currency Differences. Para-
phrastic pairs that contain different temporal
or currency references. These references were
normalized to generic tokens (e.g., $NUMBER)
before being shown to MSRP raters but are re-
tained in the released dataset. For example,
?Expenses are expected to be approximately
$2.3 billion, at the high end of the previous ex-
pectation of $2.2-to-$2.3 billion.? and ?Spend-
ing on research and development is expected to
be $4.4 billion for the year, compared with the
previous expectation of $4.3 billion.?.
5. Anaphoric References. Paraphrastic pairs
wherein one member of the pair contains
anaphora and the other doesn?t (these are con-
sidered paraphrases according to MSRP guide-
lines). For example, ?They certainly reveal a
very close relationship between Boeing and se-
nior Washington officials.? and ?The e-mails
reveal the close relationship between Boeing
and the Air Force.?.
Note that most misclassified sentence pairs can be
categorized into more than one of the above cate-
gories.
4.2 PAN
For the PAN corpus, the only real source of error in
the dataset itself was the sentence alignment algo-
rithm. There were many sentence pairs that were
erroneously linked as paraphrases. Leaving aside
such pairs, the 3 largest sources of error for our MT-
metric based approach were:
1. Complex Sentential Paraphrases. By far,
most of the misclassified pairs were paraphras-
tic pairs that could be categorized as real world
plagiarism, i.e., where the plagiarizer copies
the idea from the source but makes several
complex transformations, e.g., sentence split-
ting, structural paraphrasing etc. so as to ren-
der an MT-metric based approach powerless.
For example, consider the pair ?The school
bears the honored name of one who, in the long
years of the anti-slavery agitation, was known
as an uncompromising friend of human free-
dom.? and ?The school is named after a man
who defended the right of all men and women
to be free, all through the years when people
campaigned against slavery.? Another inter-
esting example is the pair ?The most unpromis-
ing weakly-looking creatures sometimes live to
ninety while strong robust men are carried off
in their prime.? and ?Sometimes the strong per-
sonalities live shorter than those who are unex-
pected.?.
2. Misleading Lexical Overlap. Similar to
MSRP. For example, ?Here was the second pe-
riod of Hebraic influence, an influence wholly
moral and religious.? and ?This was the sec-
ond period of Hellenic influence, an influence
wholly intellectual and artistic.?.
3. Typographical and Spelling Errors. Para-
phrastic pairs where the Turkers creating the
plagiarism also introduced other typos and
spelling errors. For example, ?The boat then
had on board over 1,000 souls in all? and
?1000 people where on board at that tim?.
5 Discussion
The misses due to ?Date, Time, and Currency Dif-
ferences? are really just the result of an artifact in
the testing. It is possible that an MT metrics based
approach could accurately predict these cases if the
references to dates etc. were replaced with generic
tokens as was done for the human raters. In a
similar vein, some of the misses that are due to a
lack of world knowledge might become hits if a
named entity recognizer could discover that ?Mi-
crosoft founder? is the same as ?Microsoft Chair-
man?. Similarly, some of the cases of anaphoric ref-
erence might be recognized with an anaphora res-
olution system. And the problem of misspelling in
PAN could be remedied with automatic spelling cor-
rection. Therefore, it is possible to improve the MT
metrics based approach further by utilizing certain
NLP systems as pre-processing modules for the text.
The only error category in MSRP and PAN
188
that caused false positives was ?Misleading Lexical
Overlap?. Here, the take-away message is that not
every part of a sentence is equally important for rec-
ognizing semantic equivalence or non-equivalence.
In a sentence that describes what someone commu-
nicated, the content of what was said is crucial. For
example, despite lexical matches everywhere else,
the mismatch of ?the coach had been carrying 38
passengers? and ?the driver had failed to heed the
red stop lights? disqualifies the respective sentences
from being paraphrases. Along the same line, dif-
ferences in proper names and their variants should
receive more weight than other words. A sentence
about ?Hebraic influence? on a period in history is
not the same as a sentence which matches in ev-
ery other way but is instead about ?Hellenic influ-
ence?. These sentences represent a bigger chal-
lenge for an approach based solely on MT metrics.
Given enough pairs of ?near-miss? non-paraphrases,
our system might be able to figure this out, but this
would require a large amount of annotated data.
6 Conclusions
In this paper, we re-examined the idea that automatic
metrics used for evaluating translation quality can
perform well explicitly for the task of paraphrase
recognition. The goal of our paper was to deter-
mine whether approaches developed for the related
but different task of MT evaluation can be as com-
petitive as approaches developed specifically for the
task of paraphrase identification. While we do treat
the metrics as black boxes to an extent, we explic-
itly chose metrics that were high performing but also
complementary in nature.
Specifically, our re-examination focused on the
more sophisticated MT metrics of the last few years
that claim to go beyond simple n-gram overlap and
edit distance. We found that a meta-classifier trained
using only MT metrics outperforms all previous ap-
proaches for the MSRP corpus. Unlike previous
studies, we also applied our approach to a new pla-
giarism dataset and obtained extremely positive re-
sults. We examined both datasets not only to find
pairs that demonstrated the strength of each met-
ric but also to conduct an error analysis to discover
the top sources of errors that an MT metric based
approach is susceptible to. Finally, we discovered
that using the TERp metric by itself provides fairly
good performance and can outperform many other
supervised classification approaches utilizing multi-
ple, complex features.
We also have two specific suggestions that we be-
lieve can benefit the community. First, we believe
that binary indicators of semantic equivalence are
not ideal and a continuous value between 0 and 1
indicating the degree to which two pairs are para-
phrastic is more suitable for most approaches. How-
ever, rather than asking annotators to rate pairs on
a scale, a better idea might be to show the sentence
pairs to a large number of Turkers (? 20) on Ama-
zon Mechanical Turk and ask them to classify it as
either a paraphrase or a non-paraphrase. A simple
estimate of the degree of semantic equivalence of
the pair is simply the proportion of the Turkers who
classified the pair as paraphrastic. An example of
such an approach, as applied to the task of grammat-
ical error detection, can be found in (Madnani et al,
2011).3 Second, we believe that the PAN corpus?
with Turker simulated plagiarism?contains much
more realistic examples of paraphrase and should
be incorporated into future evaluations of paraphrase
identification. In order to encourage this, we are re-
leasing our PAN dataset containing 13,000 sentence
pairs.
We are also releasing our error analysis data (100
pairs for MSRP and 100 pairs for PAN) since they
might prove useful to other researchers as well. Note
that the annotations for this analysis were produced
by the authors themselves and, although, they at-
tempted to accurately identify all error categories for
most sentence pairs, it is possible that the errors in
some sentence pairs were not comprehensively iden-
tified.4
Acknowledgments
We would like to thank Aoife Cahill, Michael Heil-
man and the three anonymous reviewers for their
useful comments and suggestions.
3A good approximation is to use an ordinal scale for the
human judgments as in the Semantic Textual Similarity task
of SemEval 2012. See http://www.cs.york.ac.uk/
semeval-2012/task6/ for more details.
4The data is available at http://bit.ly/mt-para.
189
References
D. W. Aha, D. Kibler, and M. K. Albert. 1991. Instance-
based learning algorithms. Mach. Learn., 6:37?66.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, and
O. Zaidan, editors. 2010. Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR.
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maxi-
mum similarity metric for machine translation evalua-
tion. In Proceedings of ACL-HLT, pages 55?62.
D. Das and N.A. Smith. 2009. Paraphrase Identifica-
tion as Probabilistic Quasi-synchronous Recognition.
In Proceedings of ACL-IJCNLP, pages 468?476.
M. Denkowski and M. Lavie. 2010. Extending the
METEOR Machine Translation Metric to the Phrase
Level. In Proceedings of NAACL.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality using N-gram Co-occurrence
Statistics. In Proceedings of HLT, pages 138?145.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised Construction of Large Paraphrase Corpora: Ex-
ploiting Massively Parallel News Sources. In Proceed-
ings of COLING, pages 350?356, Geneva, Switzer-
land.
S. Fernando and M. Stevenson. 2008. A Semantic Simi-
larity Approach to Paraphrase Detection. In Proceed-
ings of the Computational Linguistics UK (CLUK)
11th Annual Research Colloquium.
A. Finch, Y.S. Hwang, and E. Sumita. 2005. Using Ma-
chine Translation Evaluation Techniques to Determine
Sentence-level Semantic Equivalence. In Proceedings
of the Third International Workshop on Paraphrasing,
pages 17?24.
N. Habash and A. El Kholy. 2008. SEPIA: Surface
Span Extension to Syntactic Dependency Precision-
based MT Evaluation. In Proceedings of the Workshop
on Metrics for Machine Translation at AMTA.
M. Hall, E. Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11.
A. Islam and D. Inkpen. 2007. Semantic Similarity of
Short Texts. In Proceedings of RANLP, pages 291?
297.
J. J. Jiang and D. W. Conrath. 1997. Semantic Similar-
ity Based on Corpus Statistics and Lexical Taxonomy.
CoRR, cmp-lg/9709008.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 2001. Improvements to Platt?s SMO
Algorithm for SVM Classifier Design. Neural Com-
put., 13(3):637?649.
Z. Kozareva and A. Montoyo. 2006. Paraphrase Identi-
fication on the Basis of Supervised Machine Learning
Techniques. In Proceedings of FinTAL, pages 524?
233.
N. Madnani, J. Tetreault, M. Chodorow, and A. Ro-
zovskaya. 2011. They Can Help: Using Crowdsourc-
ing to Improve the Evaluation of Grammatical Error
Detection Systems. In Proceedings of ACL (Short Pa-
pers).
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and Knowledge-based Measures Of
Text Semantic Similarity. In Proceedings of AAAI,
pages 775?780.
NIST. 2008. NIST MetricsMATR Challenge. Informa-
tion Access Division. http://www.itl.nist.
gov/iad/mig/tests/metricsmatr/.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of ACL.
S. Parker. 2008. BADGER: A New Machine Translation
Metric. In Proceedings of the Workshop on Metrics
for Machine Translation at AMTA.
John C. Platt. 1999. Advances in kernel methods. chap-
ter Fast Training of Support Vector Machines using Se-
quential Minimal Optimization, pages 185?208. MIT
Press.
M. Potthast, B. Stein, A. Barro?n-Ceden?o, and P. Rosso.
2010. An Evaluation Framework for Plagiarism De-
tection. In Proceedings of COLING, pages 997?1005.
L. Qiu, M. Y. Kan, and T. S. Chua. 2006. Paraphrase
Recognition via Dissimilarity Significance Classifica-
tion. In Proceedings of the EMNLP, pages 18?26.
V. Rus, P.M. McCarthy, M.C. Lintean, D.S. McNamara,
and A.C. Graesser. 2008. Paraphrase Identification
with Lexico-Syntactic Graph Subsumption. In Pro-
ceedings of FLAIRS, pages 201?206.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings of
AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
TER-Plus: Paraphrase, Semantic, and Alignment En-
hancements to Translation Edit Rate. Machine Trans-
lation, 23(2?3):117?127.
R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D.
Manning. 2011. Dynamic Pooling and Unfolding
Recursive Autoencoders for Paraphrase Detection. In
Advances in Neural Information Processing Systems
24 (NIPS).
S. Wan, R. Dras, M. Dale, and C. Paris. 2006. Using
Dependency-based Features to Take the ?para-farce?
Out of Paraphrase. In Proceedings of the Australasian
Language Technology Workshop (ALTW), pages 131?
138.
190
Proceedings of NAACL-HLT 2013, pages 507?517,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Robust Systems for Preposition Error Correction Using Wikipedia Revisions
Aoife Cahill?, Nitin Madnani?, Joel Tetreault? and Diane Napolitano?
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{acahill, nmadnani, dnapolitano}@ets.org
? Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA
Joel.Tetreault@nuance.com
Abstract
We show that existing methods for training
preposition error correction systems, whether
using well-edited text or error-annotated cor-
pora, do not generalize across very differ-
ent test sets. We present a new, large error-
annotated corpus and use it to train systems
that generalize across three different test sets,
each from a different domain and with differ-
ent error characteristics. This new corpus is
automatically extracted from Wikipedia revi-
sions and contains over one million instances
of preposition corrections.
1 Introduction
One of the main themes that has defined the field of
automatic grammatical error correction has been the
availability of error-annotated learner data to train
and test a system. Some errors, such as determiner-
noun number agreement, are easily corrected us-
ing rules and regular expressions (Leacock et al,
2010). On the other hand, errors involving the usage
of prepositions and articles are influenced by sev-
eral factors including the local context, the prior dis-
course and semantics. These errors are better han-
dled by statistical models which potentially require
millions of training examples.
Most statistical approaches to grammatical error
correction have used one of the following training
paradigms: 1) training solely on examples of cor-
rect usage (Han et al, 2006); 2) training on exam-
ples of correct usage and artificially generated er-
rors (Rozovskaya and Roth, 2010); and 3) training
on examples of correct usage and real learner er-
rors (Dahlmeier and Ng, 2011; Dale et al, 2012).
The latter two methods require annotated corpora of
errors, and while they have shown great promise,
manually annotating grammatical errors in a large
enough corpus of learner writing is often a costly
and time-consuming endeavor.
In order to efficiently and automatically acquire a
very large corpus of annotated learner errors, we in-
vestigate the use of error corrections extracted from
Wikipedia revision history. While Wikipedia re-
vision history has shown promise for other NLP
tasks including paraphrase generation (Max and
Wisniewski, 2010; Nelken and Yamangil, 2008) and
spelling correction (Zesch, 2012), this resource has
not been used for the task of grammatical error cor-
rection.
To evaluate the usefulness of Wikipedia revision
history for grammatical error correction, we address
the task of correcting errors in preposition selection
(i.e., where the context licenses the use of a prepo-
sition, but the writer selects the wrong one). We
first train a model directly on instances of correct
and incorrect preposition usage extracted from the
Wikipedia revision data. We also generate artificial
errors using the confusion distributions derived from
this data. We compare both of these approaches to
models trained on well-edited text and evaluate each
on three test sets with a range of different character-
istics. Each training paradigm is applied to multiple
data sources for comparison. With these multiple
evaluations, we address the following research ques-
tions:
1. Across multiple test sets, which data source
507
is more useful for correcting preposition er-
rors: a large amount of well-edited text, a large
amount of potentially noisy error-annotated
data (either artificially generated or automati-
cally extracted) or a smaller amount of higher
quality error-annotated data?
2. Given error-annotated data, is it better to train
on the corrections directly or to use the con-
fusion distributions derived from these correc-
tions for generating artificial errors in well-
edited text?
3. What is the impact of having a mismatch in the
error distributions of the training and test sets?
2 Related Work
In this section, we only review work in preposi-
tion error correction in terms of the three training
paradigms and refer the reader to Leacock et al
(2010) for a more comprehensive review of the field.
2.1 Training on Well-Edited Text
Early approaches to error detection and correction
did not have access to large amounts of error-
annotated data to train statistical models and thus,
systems were trained on millions of well-edited ex-
amples from news text instead (Gamon et al, 2008;
Tetreault and Chodorow, 2008; De Felice and Pul-
man, 2009). Feature sets usually consisted of n-
grams around the preposition, POS sequences, syn-
tactic features and semantic information. Since the
model only had knowledge of correct usage, an error
was flagged if the system?s prediction for a particu-
lar preposition context differed from the preposition
the writer used.
2.2 Artificial Errors
The issue with training solely on correct usage was
that the systems had no knowledge of typical learner
errors. Ideally, a system would be trained on ex-
amples of correct and incorrect usage, however, for
many years, such error-annotated corpora were not
available. Instead, several researchers generated ar-
tificial errors based on the error distributions derived
from the error-annotated learner corpora available at
the time. Izumi et al (2003) was the first to evaluate
a model trained on incorrect usage as well as artifi-
cial errors for the task of correcting several different
error types, including prepositions. However, with
limited training data, system performance was quite
poor. Rozovskaya and Roth (2010) evaluated dif-
ferent ways of generating artificial errors and found
that a system trained on artificial errors could outper-
form the more traditional training paradigm of using
only well-edited texts. Most recently, Imamura et al
(2012) showed that performance could be improved
by training a model on artificial errors and address-
ing domain adaptation for the task of Japanese par-
ticle correction.
2.3 Error-Annotated Learner Corpora
Recently, error-annotated learner data has become
more readily and publicly available allowing models
to be trained on both examples of correct usage as
well typical learner errors. Han et al (2010) showed
that a preposition error detection and correction sys-
tem trained on 100,000 annotated preposition errors
from the Chungdahm Corpus of Korean Learner En-
glish (in addition to 1 million examples of correct
usage) outperformed a model trained only on 5 mil-
lion examples of correct usage. Gamon (2010) and
Dahlmeier and Ng (2011) showed that combining
models trained separately on examples of correct
and incorrect usage could also improve the perfor-
mance of a preposition error correction system.
3 Mining Wikipedia Revisions for
Grammatical Error Corrections
3.1 Related Work
Many NLP researchers have taken advantage of the
wealth of information available in Wikipedia revi-
sions. Dutrey et al (2011) define a typology of mod-
ifications found in the French Wikipedia (WiCo-
PaCo). They show that the kinds of edits made range
from specific lexical changes to more general rewrite
edits. Similar types of edits are found in the En-
glish Wikipedia. The data extracted from Wikipedia
revisions has been used for a wide variety of tasks
including spelling correction (Max and Wisniewski,
2010; Zesch, 2012), lexical error detection (Nelken
and Yamangil, 2008), sentence compression (Ya-
mangil and Nelken, 2008), paraphrase generation
(Max and Wisniewski, 2010; Nelken and Yamangil,
2008), lexical simplification (Yatskar et al, 2010)
and entailment (Zanzotto and Pennacchiotti, 2010;
508
(1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at? in) the refrigerator.
(2) [Wiki clean] Also none of the witnesses present (of? on) those dates supports Ranneft?s claims.
(3) [Wiki dirty] . . . cirque has a permanent production (to? at) the Mirage, love.
(4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his
performances for French tourists (in? to) Petersburg.
Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second preposition is
assumed to be the correction.
Cabrio et al, 2012). To our knowledge, no one has
previously extracted data for training a grammatical
error detection system from Wikipedia revisions.
3.2 Extracting Preposition Correction Data
from Wikipedia Revisions
As the source of our Wikipedia revisions, we used an
XML snapshot of Wikipedia generated in July 2011
containing 8,735,890 articles and 288,583,063 revi-
sions.1 We then used the following process to ex-
tract preposition errors and their corresponding cor-
rections from this snapshot:
Step 1: Extract the plain text versions of all revi-
sions of all articles using the Java Wikipedia
Library (Ferschke et al, 2011).
Step 2: For each Wikipedia article, compare each
revision with the revision immediately preced-
ing it using an efficient diff algorithm.2
Step 3: Compute all 1-word edit chains for the arti-
cle, i.e., sequences of related edits derived from
all revisions of the same article. For example,
say revision 10 of an article inserts the preposi-
tion of into a sentence and revision 12 changes
that preposition to on. Assuming that no other
revisions change this sentence, the correspond-
ing edit chain would contain the following 3 el-
ements: ?of?on. The extracted chains con-
tain the full context on either side of the 1-word
edit, up to the automatically detected sentence
boundaries.
Step 4: (a) Ignore any circular chains, i.e., where
the first element in the edit chain is the same as
the last element. (b) Collapse all non-circular
1http://dumps.wikimedia.org/enwiki/
2http://code.google.com/p/google-diff-match-patch/
chains, i.e., only retain the first and the last ele-
ments in a chain. Both these decisions are mo-
tivated by the assumption that the intermediate
links in the chain are unreliable for training an
error correction system since a Wikipedia con-
tributor modified them.
Step 5 : From all remaining 2-element chains, find
those where a preposition is replaced with an-
other preposition. If the preposition edit is the
only edit in the sentence, we convert the chain
into a sentence pair and label it clean. If there
are other 1-word edits but not within 5 words of
the preposition edit on either side, we label the
sentence somewhat clean. Otherwise, we label
it dirty. The motivation is that the presence of
other nearby edits make the preposition correc-
tion less reliable when used in isolation, due to
the possible dependencies between corrections.
All extracted sentences were part-of-speech tagged
using the Stanford Tagger (Toutanova et al, 2003).
Using the above process, we are able to extract ap-
proximately 2 million sentences containing preposi-
tions errors and their corrections. Some examples
of the sentences we extracted are given in Figure 1.
Example (4) shows an example of a bad correction.
4 Corpora
We use several corpora for training and testing our
preposition error correction system. The proper-
ties of each are outlined in Table 1, organized by
paradigm. For each corpus we report the total num-
ber of prepositions used for training, as well as the
number and percentage of preposition corrections.
4.1 Well-edited Text
We train our system on two well-edited corpora.
The first is the same corpus used by Tetreault and
509
Corpus Total # Preps # Corrected Preps
Well-edited Text
Wikipedia Snapshot (10m sents) 26,069,860 0 (0%)
Lexile/SJM 6,719,077 0 (0%)
Artificially Generated
Errors
Wikipedia Snapshot 26,127,464 2,844,227 (10.9%)
Lexile/SJM 6,723,206 792,195 (11.8%)
Naturally Occurring
Errors
Wikipedia Revisions All 7,125,317 1,027,643 (20.6%)
Wikipedia Revisions ?Clean 3,001,900 381,644 (12.7%)
Wikipedia Revisions Clean 1,978,802 266,275 (14.4%)
Lang-8 129,987 53,493 (41.2%)
NUCLE Train 72,741 922 (1.3%)
Test Corpora
NUCLE Test 9,366 125 (1.3%)
FCE 33,243 2,900 (8.7%)
HOO 2011 Test 1,703 81 (4.8%)
Table 1: Corpora characteristics
Chodorow (2008), comprising roughly 1.8 million
sentences from the San Jose Mercury News Corpus3
and roughly 1.8 million sentences from grades 11
and 12 of the MetaMetrics Lexile Corpus. Our sec-
ond corpus is a random sample of 10 million sen-
tences containing at least one preposition from the
June 2012 snapshot of English Wikipedia Articles.4
4.2 Artificially Generated Errors
Similar to Foster and Andersen (2009) and Ro-
zovskaya and Roth (2010), we artificially introduce
preposition errors into well-edited corpora (the two
described above). We do this based on a distribu-
tion of possible confusions and train a model that
is aware of the corrections. The two sets of con-
fusion distributions we used were derived based on
the errors extracted from Wikipedia revisions and
Lang-8 respectively (discussed in Section 4.3). For
each corrected preposition pi in the revision data,
we calculated P (pi|pj), where pj is each of the pos-
sible original prepositions that were confused with
pi. Then, for each sentence in the well-edited text,
all prepositions are extracted. A preposition is ran-
domly selected (without replacement) and changed
based on the distribution of possible confusions
(note that the original preposition is also included
in the distribution, usually with a high probabil-
3The San Jose Mercury News is available from the Linguis-
tic Data Consortium (catalog number LDC93T3A).
4We used a newer version of the Wikipedia text for the well-
edited text, since we assume that more recent versions of the
text will be most grammatical, and therefore closer to well-
edited.
ity, meaning that there is a strong preference not to
change the preposition). If a preposition is changed
to something other than the original preposition, all
remaining prepositions in the sentence are left un-
changed.
4.3 Naturally Occurring Errors
We have a number of corpora that contain annotated
preposition errors. Note that we are only considering
incorrectly selected prepositions, we do not consider
missing or extraneous.
NUCLE The NUS Corpus of Learner English (NU-
CLE)5 contains one million words of learner
essay text, manually annotated with error tags
and corrections. We use the same training, dev
and test splits as Dahlmeier and Ng (2011).
FCE The CLC FCE Dataset6 is a collection of
1,244 exam scripts written by learners of En-
glish as part of the Cambridge ESOL First Cer-
tificate in English (Yannakoudakis et al, 2011).
It includes demographic metadata about the
candidate, a grade for each essay and manually-
annotated error corrections.
Wikipedia We use three versions of the preposi-
tion errors extracted from the Wikipedia revi-
sions as described in Section 3.2. The first in-
cludes corrections where the preposition was
the only word corrected in the entire sentence
5http://bit.ly/nuclecorpus
6http://ilexir.co.uk/applications/clc-fce-dataset/
510
(clean). The second contains all clean cor-
rections, as well as all corrections where there
were no other edits within a five-word span on
either side of the preposition (?clean). The
third contains all corrections regardless of any
other changes in the surrounding context (all).
Lang-8 The Lang-8 website contains journals writ-
ten by language learners, where native speakers
highlight and correct errors on a sentence-by-
sentence basis. As a result, it contains typical
grammatical mistakes made by language learn-
ers, which can be easily downloaded. We auto-
matically extract 75,622 sentences with prepo-
sition errors and corrections from the first mil-
lion journal entries.7
HOO 2011 We take the test set from the HOO 2011
shared task (Dale and Kilgarriff, 2011) and ex-
tract all examples of preposition selection er-
rors. The texts are fragments of ACL papers
that have been manually annotated for gram-
matical errors.8
It is important to note that the three test sets we use
are from entirely different domains: exam scripts
from non-native English speakers (FCE), essays by
highly proficient college students in Singapore (NU-
CLE) and ACL papers (HOO). In addition, they have
a different number of total prepositions as well as er-
roneous prepositions.
5 Preposition Error Correction
Experiments
We use the preposition error correction model de-
scribed in Tetreault and Chodorow (2008)9 to eval-
uate the many ways of using Wikipedia error cor-
rections as described in the Section 4. We use this
system since it has been recreated for other work
(Dahlmeier and Ng, 2011; Tetreault et al, 2010) and
is similar in methodology to Gamon et al (2008)
7Tajiri et al (2012) extract a corpus of English verb phrases
corrected for tense/aspect errors from Lang-8. They kindly pro-
vided us with their scripts to carry out the scraping of Lang-8.
8The results of the HOO 2011 shared task were not reported
at level of preposition selection error, therefore it is not possible
to compare the results presented in this paper with those results.
9Note that in that work, the model was evaluated in terms of
preposition error detection rather than correction, however the
model itself does not change.
and De Felice and Pulman (2009). In short, the
method models the problem of preposition error cor-
rection (for replacement errors) as a 36-way classifi-
cation problem using a multinomial logistic regres-
sion model.10 The system uses 25 lexical, syntac-
tic and n-gram features derived from the contexts of
each preposition training instance.
We modified the training paradigm of Tetreault
and Chodorow (2008) so that a model could be
trained on examples of correct usage as well as ac-
tual errors. We did this by adding a new feature
specifying the writer?s original preposition (as in
Han et al (2010) and Dahlmeier and Ng (2011)).
5.1 Results
We train a preposition correction system using each
of the three data paradigms and test on the FCE,
NUCLE and HOO 2011 test corpora. For each
preposition in the test corpus, we record whether
the system predicted that it should be changed,
and if so, what it should be changed to. We then
compare the prediction to the annotation in the test
corpus. We report results in terms of f-score, where
precision and recall are calculated as follows:11
Precision = Number of correct preposition correctionsTotal number of corrections suggested
Recall = Number of correct preposition correctionsTotal number of corrections in test set
Note that due to the high volume of unchanged
prepositions in the test corpus, we obtain very high
accuracies, which are not indicative of true perfor-
mance, and are not included in our results.
The results of our experiments are presented in
Table 2.12 The first part of the table shows the f-
scores of preposition error correction systems that
10We use liblinear (Fan et al, 2008) with the L1-regularized
logistic regression solver and default parameters.
11As Chodorow et al (2012) note, it is not clear how to han-
dle cases where the system predicts a preposition that is neither
the same as the writer preposition nor the correct preposition.
We count these cases as false positives.
12No thresholds were used in the systems that were trained
on well-edited text. Traditionally, thresholds are applied so as
to only predict a correction when the system is highly confident.
This has the effect of increasing precision at the cost of recall,
and sometimes leads to an overall improved f-score. Here we
take the prediction of the system, regardless of the confidence,
reflecting a lower-bound of this method.
511
Data Source Paradigm CLC-FCE NUCLE HOO2011
N=33,243 N=9,366 N=1,703
Without
Wikipedia
Revisions
(nonWikiRev)
Wikipedia Snapshot Well-edited Text 24.43? 5.02? 12.36?
Lexile/SJM Well-edited Text 24.73? 4.29? 9.73?
Wikipedia Snapshot Artificial Errors (Lang-8) 42.15? 19.91? 28.75
Lexile/SJM Artificial Errors (Lang-8) 45.36 18.00? 25.15
Lang-8 Error-annotated Text 38.22? 8.18? 24.00
NUCLE train Error-annotated Text 5.38? 20.14 4.82?
With
Wikipedia
Revisions
(WikiRev)
Wikipedia Snapshot Artificial Errors (Wiki) 31.17? 24.52 28.30
Lexile/SJM Artificial Errors (Wiki) 34.35? 23.38 32.76
Wikipedia Revisions All Error-annotated Text 33.59? 26.39 36.84
Wikipedia Revisions ?Clean Error-annotated Text 29.68? 22.13 36.04
Wikipedia Revisions Clean Error-annotated Text 28.09? 21.74 28.30
Table 2: Preposition selection error correction results (f-score). The systems with scores in bold are statistically
significantly better than all systems marked with an asterisk (p < 0.01). Confidence intervals were obtained using
bootstrap resampling with 50,000 replicates.
one might be able to train with publicly available
data excluding the Wikipedia revisions that we have
extracted. We refer to these systems as nonWikiRev
systems. The second part of the table shows the f-
scores of systems trained on the Wikipedia revisions
data ? either directly on the annotated errors or on
the artificial errors produced using the confusion dis-
tributions derived from these annotated errors. We
refer to this second set of systems as WikiRev sys-
tems. The nonWikiRev systems perform inconsis-
tently, heavily dependent on the characteristics of
the test set in question. On the other hand, it is
obvious that the WikiRev systems ? while not al-
ways outperforming the best nonWikiRev systems
? generalize much better across the three test sets.
In fact, for the NUCLE test set, the best WikiRev
system performs as well as the nonWikiRev system
trained on data from the same domain and with iden-
tical error characteristics as the test set. The distri-
butions of errors in the three test sets are not sim-
ilar, and therefore, the stability in performance of
the WikiRev systems cannot be attributed to the hy-
pothesis that the WikiRev training data error distri-
butions are more similar to the test data than any of
the other training corpora. Therefore, we claim that
if a preposition error correction system is to be de-
ployed on data for which the error characteristics are
not known in advance, i.e. most real-world scenar-
ios, training the system using Wikipedia revisions is
likely to be the most robust option.
6 Discussion
We examine the results of our experiments in light
of the research questions we posed in Section 1.
6.1 Which Data Source is More Useful?
We wanted to know whether it was better to have
a smaller corpus of carefully annotated corrections,
or a much larger (but automatically generated, and
therefore noisier) error-annotated corpus. We also
wanted to compare this scenario to training on large
amounts of well-edited text. From our experiments,
it is clear that the composition of the test set plays
a major role in answering this question. On a test
set with few corrections (NUCLE), training on well-
edited text (and without using thresholds) performs
particularly poorly. On the other hand, when eval-
uating on the FCE test set which contains far more
errors, training on well-edited text performs reason-
ably well (though statistically significantly worse
than training on all of the Wikipedia errors). Sim-
ilarly, training on the smaller, high-quality NU-
CLE corpus and evaluating on the NUCLE test set
achieves good results, however training on NUCLE
and testing on FCE achieves the lowest f-score of all
our systems on that test set.
Figure 2 shows the learning curves obtained by
increasing the size of the training data for two
of the test sets.13 Although one might assume
13For space reasons, the graph for HOO2011 is omitted. Also
note that the results in Table 2 may not appear in the graph,
512
Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
10
20
30
40
50
log(training data size in thousands of instances)1 2 3 4
Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
5
10
15
20
25
1 2 3 4
(a) NUCLE
(b) FCE
Figure 2: The effect of varying the size of the training corpus
that Wikipedia-clean would be more reliable than
Wikipedia-all, the cleanness of the Wikipedia data
seems to make very little difference, probably be-
cause the data extracted in the dirty contexts is not
as noisy as we expected. Interestingly, it also seems
that additional data would lead to further improve-
ments for models trained on artificial errors in Lexile
data and for those trained on all of the automatically
extracted Wikipedia errors.
Another interesting aspect of Figure 2 is that
since we were sampling at specific data points which did not
correspond exactly to the total sizes of the training corpora.
training on the Lang-8 data shows a very steep rising
trend. This suggests that automatically-scraped data
that is highly targeted towards language learners is
very useful in correcting preposition errors in texts
where they are reasonably frequent.
6.2 Natural or Artificially Generated Errors?
Table 2 shows that training on artificially generated
errors via Wikipedia revisions performs fairly con-
sistently across test corpora. While using Lang-8
for artificial error generation is also quite promis-
ing for FCE, it does not generalize across test sets.
513
Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
10
20
30
40
50
Percentage of Errors in Training Data0 5 10 15 20 25 30 35 40 45 50 55
Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
5
10
15
20
25
30
0 5 10 15 20 25 30 35 40 45 50 55
(a) NUCLE
(b) FCE
Figure 3: The effect of varying the percentage of errors in the training corpus
On FCE it achieves the highest results, on NUCLE
it performs statistically significantly worse than the
best system, and on HOO 2011 it achieves a lower
(though not statistically significant) result than the
best system. This highlights that extracting errors
from Wikipedia is useful in two ways: (1) training a
system on the errors alone works well and (2) gener-
ating artificial errors in well-edited corpora of differ-
ent domains and training a system on that also works
well. It also indicates that if the system were to be
applied to a specific domain, applying the confusion
distributions to a domain specific corpus ? if avail-
able ? would likely yield the best results.
6.3 Mismatching Distributions
The proportion of errors in the training and test data
plays an important role in the performance of any
preposition error correction system. This is clearly
evident by comparing system performances across
the three test sets which have fairly different compo-
sitions. FCE contains a much higher proportion of
errors than NUCLE, and HOO falls somewhere in
between. Interestingly, the system trained on Lang-
8 data (which contains the highest proportion of er-
514
rors among all training corpora) performs best on
the FCE data. On the other hand, the same sys-
tem performs poorly on NUCLE test which contains
far fewer errors. In this instance, the system learns
to predict an incorrect preposition too often. We
see a similar pattern with the system trained on the
NUCLE training data. It performs poorly on FCE
which contains many errors, but well on NUCLE
test which contains a similar proportion of errors.
In order to better understand the relationship be-
tween the percentage of errors in the training data
and system performance, we vary the percentage of
errors in each training corpus from 1-50% and test
on the unchanged FCE and NUCLE test corpora.
For each training corpus, we reduce the size to be
twice the size of the total number of errors.14 Keep-
ing this size constant, we then artificially change the
percentage of errors. Note that because the total size
of the corpus has changed, the results in Table 2 may
not appear in the graph. Figure 3 shows the effect on
f-score when the data composition is changed. For
both test sets, there is a peak after which increas-
ing the proportion of errors in the training corpus is
detrimental. For NUCLE test with its low number
of preposition errors, this peak is very pronounced.
For FCE, it is more of a gentle degradation in per-
formance, but the pattern is clear. Also noteworthy
is the fact that the degradation for models trained on
artificial errors is less steep suggesting that they may
be more stable across test sets.
In general, these results indicate that when
building a preposition error detection using error-
annotated data, the characteristics of the data to
which the system will be applied should play a vital
role in how the system is to be trained. Our results
show that the WikiRev systems are robust across
test sets, however if the exact distribution of errors
in the data is known in advance, other models may
perform better.
7 Conclusion
Although previous approaches to preposition er-
ror correction using either well-edited text or small
hand-annotated corrections performed well on some
specific test set, they did not generalize well across
14We omit the NUCLE train corpus from this comparison,
because it contains too few errors to obtain a meaningful result.
very different test sets. In this paper, we present
work that automatically extracts preposition error
corrections from Wikipedia Revisions and uses it
to build robust error correction systems. We show
that this data is useful for two purposes. Firstly, a
model trained directly on the corrections performs
well across test sets. Secondly, models trained on ar-
tificial errors generated from the distribution of con-
fusions in the Wikipedia data perform equally well.
The distribution of confusions can also be applied to
other well-edited corpora in different domains, pro-
viding a very powerful method of automatically gen-
erating error corpora. The results of our experiments
also highlight the importance of the distribution of
expected errors in the test set. Models that perform
well on one kind of distribution may not necessar-
ily work on a completely different one, as evident
in the performances of the systems trained on either
Lang-8 or NUCLE. In general, the WikiRev mod-
els perform well across distributions. We also con-
ducted some preliminary system combination exper-
iments and found that while they yielded promising
results, further investigation is necessary. We have
also made the Wikipedia preposition correction cor-
pus available for download.15
In future work, we will examine whether the
results we obtain for English generalize to other
Wikipedia languages. We also plan to extract multi-
word corrections for other types of errors and to ex-
amine the usefulness of including error contexts in
our confusion distributions (e.g., preposition confu-
sions following verbs versus those following nouns).
Acknowledgments
The authors would like to thank Daniel Dahlmeier,
Torsten Zesch, Mamoru Komachi, Tajiri Toshikazu,
Tomoya Mizumoto and Yuji Matsumoto for provid-
ing scripts and data that enabled us to carry out
this research. We would also like to thank Martin
Chodorow and the anonymous reviewers for their
helpful suggestions and comments.
References
Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.
2012. Extracting Context-Rich Entailment Rules from
15http://bit.ly/etsprepdata
515
Wikipedia Revision History. In Proceedings of the 3rd
Workshop on the People?s Web Meets NLP: Collabora-
tively Constructed Semantic Resources and their Ap-
plications to NLP, pages 34?43, Jeju, Republic of Ko-
rea, July. Association for Computational Linguistics.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel Tetreault. 2012. Problems in Evaluating Gram-
matical Error Detection Systems. In Proceedings of
COLING 2012, pages 611?628, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical Error Correction with Alternating Structure Op-
timization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 915?923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 242?249, Nancy, France, September. As-
sociation for Computational Linguistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3):512?528.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in Wikipedias revision history. SEPLN
journal(Revista de Procesamiento del Lenguaje Nat-
ural), 46:51?58.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Access-
ing Wikipedia?s Edit History. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
System Demonstrations.
Jennifer Foster and Oistein Andersen. 2009. Gen-
ERRate: Generating Errors for Use in Grammatical
Error Detection. In Proceedings of the Fourth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 82?90, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 449?456, Hyderabad, India.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California, June. Association for Computational
Linguistics.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using Error-Annotated ESL Data
to Develop an ESL Error Correction System. In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC), Malta.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar Error Correc-
tion Using Pseudo-Error Sentences and Domain Adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 388?392, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic Error
Detection in the Japanese Learners? English Spoken
Data. In The Companion Volume to the Proceedings
of 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 145?148, Sapporo, Japan,
July. Association for Computational Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Aure?lien Max and Guillaume Wisniewski. 2010. Mining
Naturally-occurring Corrections and Paraphrases from
Wikipedia?s Revision History. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Rami Nelken and Elif Yamangil. 2008. Mining
Wikipedias Article Revision History for Training
516
Computational Linguistics Algorithms. In Proceed-
ings of the 1st AAAI Workshop on Wikipedia and Arti-
ficial Intelligence, pages 31?36, Chicago, IL.
Alla Rozovskaya and Dan Roth. 2010. Generating Con-
fusion Sets for Context-Sensitive Error Correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 961?
970, Cambridge, MA, October. Association for Com-
putational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and Aspect Error Correction
for ESL Learners Using Global Context. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), Short Papers, pages
198?202, Jeju Island, Korea.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 865?872, Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selection
and Error Detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich Part-of-speech
Tagging with a Cyclic Dependency Network. In Pro-
ceedings of NAACL, pages 173?180.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sentence
Compression. In Proceedings of ACL-08: HLT, Short
Papers, pages 137?140, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365?368, Los Angeles, California, June.
Association for Computational Linguistics.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of the
2nd Workshop on The People?s Web Meets NLP: Col-
laboratively Constructed Semantic Resources, pages
28?36, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Torsten Zesch. 2012. Measuring Contextual Fitness Us-
ing Error Contexts Extracted from the Wikipedia Revi-
sion History. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 529?538, Avignon, France,
April. Association for Computational Linguistics.
517
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508?513,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
They Can Help: Using Crowdsourcing to Improve the Evaluation of
Grammatical Error Detection Systems
Nitin Madnania Joel Tetreaulta Martin Chodorowb Alla Rozovskayac
aEducational Testing Service
Princeton, NJ
{nmadnani,jtetreault}@ets.org
bHunter College of CUNY
martin.chodorow@hunter.cuny.edu
cUniversity of Illinois at Urbana-Champaign
rozovska@illinois.edu
Abstract
Despite the rising interest in developing gram-
matical error detection systems for non-native
speakers of English, progress in the field has
been hampered by a lack of informative met-
rics and an inability to directly compare the
performance of systems developed by differ-
ent researchers. In this paper we address
these problems by presenting two evaluation
methodologies, both based on a novel use of
crowdsourcing.
1 Motivation and Contributions
One of the fastest growing areas in need of NLP
tools is the field of grammatical error detection for
learners of English as a Second Language (ESL).
According to Guo and Beckett (2007), ?over a bil-
lion people speak English as their second or for-
eign language.? This high demand has resulted in
many NLP research papers on the topic, a Synthesis
Series book (Leacock et al, 2010) and a recurring
workshop (Tetreault et al, 2010a), all in the last five
years. In this year?s ACL conference, there are four
long papers devoted to this topic.
Despite the growing interest, two major factors
encumber the growth of this subfield. First, the lack
of consistent and appropriate score reporting is an
issue. Most work reports results in the form of pre-
cision and recall as measured against the judgment
of a single human rater. This is problematic because
most usage errors (such as those in article and prepo-
sition usage) are a matter of degree rather than sim-
ple rule violations such as number agreement. As a
consequence, it is common for two native speakers
to have different judgments of usage. Therefore, an
appropriate evaluation should take this into account
by not only enlisting multiple human judges but also
aggregating these judgments in a graded manner.
Second, systems are hardly ever compared to each
other. In fact, to our knowledge, no two systems
developed by different groups have been compared
directly within the field primarily because there is
no common corpus or shared task?both commonly
found in other NLP areas such as machine transla-
tion.1 For example, Tetreault and Chodorow (2008),
Gamon et al (2008) and Felice and Pulman (2008)
developed preposition error detection systems, but
evaluated on three different corpora using different
evaluation measures.
The goal of this paper is to address the above
issues by using crowdsourcing, which has been
proven effective for collecting multiple, reliable
judgments in other NLP tasks: machine transla-
tion (Callison-Burch, 2009; Zaidan and Callison-
Burch, 2010), speech recognition (Evanini et al,
2010; Novotney and Callison-Burch, 2010), au-
tomated paraphrase generation (Madnani, 2010),
anaphora resolution (Chamberlain et al, 2009),
word sense disambiguation (Akkaya et al, 2010),
lexicon construction for less commonly taught lan-
guages (Irvine and Klementiev, 2010), fact min-
ing (Wang and Callison-Burch, 2010) and named
entity recognition (Finin et al, 2010) among several
others.
In particular, we make a significant contribution
to the field by showing how to leverage crowdsourc-
1There has been a recent proposal for a related shared
task (Dale and Kilgarriff, 2010) that shows promise.
508
ing to both address the lack of appropriate evaluation
metrics and to make system comparison easier. Our
solution is general enough for, in the simplest case,
intrinsically evaluating a single system on a single
dataset and, more realistically, comparing two dif-
ferent systems (from same or different groups).
2 A Case Study: Extraneous Prepositions
We consider the problem of detecting an extraneous
preposition error, i.e., incorrectly using a preposi-
tion where none is licensed. In the sentence ?They
came to outside?, the preposition to is an extrane-
ous error whereas in the sentence ?They arrived
to the town? the preposition to is a confusion er-
ror (cf. arrived in the town). Most work on au-
tomated correction of preposition errors, with the
exception of Gamon (2010), addresses preposition
confusion errors e.g., (Felice and Pulman, 2008;
Tetreault and Chodorow, 2008; Rozovskaya and
Roth, 2010b). One reason is that in addition to the
standard context-based features used to detect con-
fusion errors, identifying extraneous prepositions
also requires actual knowledge of when a preposi-
tion can and cannot be used. Despite this lack of
attention, extraneous prepositions account for a sig-
nificant proportion?as much as 18% in essays by
advanced English learners (Rozovskaya and Roth,
2010a)?of all preposition usage errors.
2.1 Data and Systems
For the experiments in this paper, we chose a propri-
etary corpus of about 500,000 essays written by ESL
students for Test of English as a Foreign Language
(TOEFL R?). Despite being common ESL errors,
preposition errors are still infrequent overall, with
over 90% of prepositions being used correctly (Lea-
cock et al, 2010; Rozovskaya and Roth, 2010a).
Given this fact about error sparsity, we needed an ef-
ficient method to extract a good number of error in-
stances (for statistical reliability) from the large es-
say corpus. We found all trigrams in our essays con-
taining prepositions as the middle word (e.g., marry
with her) and then looked up the counts of each tri-
gram and the corresponding bigram with the prepo-
sition removed (marry her) in the Google Web1T
5-gram Corpus. If the trigram was unattested or had
a count much lower than expected based on the bi-
gram count, then we manually inspected the trigram
to see whether it was actually an error. If it was,
we extracted a sentence from the large essay corpus
containing this erroneous trigram. Once we had ex-
tracted 500 sentences containing extraneous prepo-
sition error instances, we added 500 sentences con-
taining correct instances of preposition usage. This
yielded a corpus of 1000 sentences with a 50% error
rate.
These sentences, with the target preposition high-
lighted, were presented to 3 expert annotators who
are native English speakers. They were asked to
annotate the preposition usage instance as one of
the following: extraneous (Error), not extraneous
(OK) or too hard to decide (Unknown); the last cat-
egory was needed for cases where the context was
too messy to make a decision about the highlighted
preposition. On average, the three experts had an
agreement of 0.87 and a kappa of 0.75. For subse-
quent analysis, we only use the classes Error and
OK since Unknown was used extremely rarely and
never by all 3 experts for the same sentence.
We used two different error detection systems to
illustrate our evaluation methodology:2
? LM: A 4-gram language model trained on
the Google Web1T 5-gram Corpus with
SRILM (Stolcke, 2002).
? PERC: An averaged Perceptron (Freund and
Schapire, 1999) classifier? as implemented in
the Learning by Java toolkit (Rizzolo and Roth,
2007)?trained on 7 million examples and us-
ing the same features employed by Tetreault
and Chodorow (2008).
3 Crowdsourcing
Recently,we showed that Amazon Mechanical Turk
(AMT) is a cheap and effective alternative to expert
raters for annotating preposition errors (Tetreault et
al., 2010b). In other current work, we have extended
this pilot study to show that CrowdFlower, a crowd-
sourcing service that allows for stronger quality con-
trol on untrained human raters (henceforth, Turkers),
is more reliable than AMT on three different error
detection tasks (article errors, confused prepositions
2Any conclusions drawn in this paper pertain only to these
specific instantiations of the two systems.
509
& extraneous prepositions). To impose such quality
control, one has to provide ?gold? instances, i.e., ex-
amples with known correct judgments that are then
used to root out any Turkers with low performance
on these instances. For all three tasks, we obtained
20 Turkers? judgments via CrowdFlower for each in-
stance and found that, on average, only 3 Turkers
were required to match the experts.
More specifically, for the extraneous preposition
error task, we used 75 sentences as gold and ob-
tained judgments for the remaining 923 non-gold
sentences.3 We found that if we used 3 Turker judg-
ments in a majority vote, the agreement with any one
of the three expert raters is, on average, 0.87 with a
kappa of 0.76. This is on par with the inter-expert
agreement and kappa found earlier (0.87 and 0.75
respectively).
The extraneous preposition annotation cost only
$325 (923 judgments ? 20 Turkers) and was com-
pleted in a single day. The only restriction on the
Turkers was that they be physically located in the
USA. For the analysis in subsequent sections, we
use these 923 sentences and the respective 20 judg-
ments obtained via CrowdFlower. The 3 expert
judgments are not used any further in this analysis.
4 Revamping System Evaluation
In this section, we provide details on how crowd-
sourcing can help revamp the evaluation of error de-
tection systems: (a) by providing more informative
measures for the intrinsic evaluation of a single sys-
tem (? 4.1), and (b) by easily enabling system com-
parison (? 4.2).
4.1 Crowd-informed Evaluation Measures
When evaluating the performance of grammatical
error detection systems against human judgments,
the judgments for each instance are generally re-
duced to the single most frequent category: Error
or OK. This reduction is not an accurate reflection
of a complex phenomenon. It discards valuable in-
formation about the acceptability of usage because
it treats all ?bad? uses as equal (and all good ones
as equal), when they are not. Arguably, it would
be fairer to use a continuous scale, such as the pro-
portion of raters who judge an instance as correct or
3We found 2 duplicate sentences and removed them.
incorrect. For example, if 90% of raters agree on a
rating of Error for an instance of preposition usage,
then that is stronger evidence that the usage is an er-
ror than if 56% of Turkers classified it as Error and
44% classified it as OK (the sentence ?In addition
classmates play with some game and enjoy? is an ex-
ample). The regular measures of precision and recall
would be fairer if they reflected this reality. Besides
fairness, another reason to use a continuous scale is
that of stability, particularly with a small number of
instances in the evaluation set (quite common in the
field). By relying on majority judgments, precision
and recall measures tend to be unstable (see below).
We modify the measures of precision and re-
call to incorporate distributions of correctness, ob-
tained via crowdsourcing, in order to make them
fairer and more stable indicators of system perfor-
mance. Given an error detection system that classi-
fies a sentence containing a specific preposition as
Error (class 1) if the preposition is extraneous and
OK (class 0) otherwise, we propose the following
weighted versions of hits (Hw), misses (Mw) and
false positives (FPw):
Hw =
N?
i
(cisys ? p
i
crowd) (1)
Mw =
N?
i
((1? cisys) ? p
i
crowd) (2)
FPw =
N?
i
(cisys ? (1? p
i
crowd)) (3)
In the above equations, N is the total number of
instances, cisys is the class (1 or 0) , and p
i
crowd
indicates the proportion of the crowd that classi-
fied instance i as Error. Note that if we were to
revert to the majority crowd judgment as the sole
judgment for each instance, instead of proportions,
picrowd would always be either 1 or 0 and the above
formulae would simply compute the normal hits,
misses and false positives. Given these definitions,
weighted precision can be defined as Precisionw =
Hw/(Hw + FPw) and weighted recall as Recallw =
Hw/(Hw + Mw).
510
agreement
co
un
t
0
100
200
300
400
500
50 60 70 80 90 100
Figure 1: Histogram of Turker agreements for all 923 in-
stances on whether a preposition is extraneous.
Precision Recall
Unweighted 0.957 0.384
Weighted 0.900 0.371
Table 1: Comparing commonly used (unweighted) and
proposed (weighted) precision/recall measures for LM.
To illustrate the utility of these weighted mea-
sures, we evaluated the LM and PERC systems
on the dataset containing 923 preposition instances,
against all 20 Turker judgments. Figure 1 shows a
histogram of the Turker agreement for the major-
ity rating over the set. Table 1 shows both the un-
weighted (discrete majority judgment) and weighted
(continuous Turker proportion) versions of precision
and recall for this system.
The numbers clearly show that in the unweighted
case, the performance of the system is overesti-
mated simply because the system is getting as much
credit for each contentious case (low agreement)
as for each clear one (high agreement). In the
weighted measure we propose, the contentious cases
are weighted lower and therefore their contribution
to the overall performance is reduced. This is a
fairer representation since the system should not be
expected to perform as well on the less reliable in-
stances as it does on the clear-cut instances. Essen-
tially, if humans cannot consistently decide whether
0.0
0.2
0.4
0.6
0.8
1.0
Pre
cisio
n/Re
call
50?75%[n=93] 75?90%[n=114] 90?100%[n=716]Agreement Bin
LM PrecisionPERC PrecisionLM RecallPERC Recall
Figure 2: Unweighted precision/recall by agreement bins
for LM & PERC.
a case is an error then a system?s output cannot be
considered entirely right or entirely wrong.4
As an added advantage, the weighted measures
are more stable. Consider a contentious instance in
a small dataset where 7 out of 15 Turkers (a minor-
ity) classified it as Error. However, it might easily
have happened that 8 Turkers (a majority) classified
it as Error instead of 7. In that case, the change in
unweighted precision would have been much larger
than is warranted by such a small change in the
data. However, weighted precision is guaranteed to
be more stable. Note that the instability decreases
as the size of the dataset increases but still remains a
problem.
4.2 Enabling System Comparison
In this section, we show how to easily compare dif-
ferent systems both on the same data (in the ideal
case of a shared dataset being available) and, more
realistically, on different datasets. Figure 2 shows
(unweighted) precision and recall of LM and PERC
(computed against the majority Turker judgment)
for three agreement bins, where each bin is defined
as containing only the instances with Turker agree-
ment in a specific range. We chose the bins shown
4The difference between unweighted and weighted mea-
sures can vary depending on the distribution of agreement.
511
since they are sufficiently large and represent a rea-
sonable stratification of the agreement space. Note
that we are not weighting the precision and recall in
this case since we have already used the agreement
proportions to create the bins.
This curve enables us to compare the two sys-
tems easily on different levels of item contentious-
ness and, therefore, conveys much more information
than what is usually reported (a single number for
unweighted precision/recall over the whole corpus).
For example, from this graph, PERC is seen to have
similar performance as LM for the 75-90% agree-
ment bin. In addition, even though LM precision is
perfect (1.0) for the most contentious instances (the
50-75% bin), this turns out to be an artifact of the
LM classifier?s decision process. When it must de-
cide between what it views as two equally likely pos-
sibilities, it defaults to OK. Therefore, even though
LM has higher unweighted precision (0.957) than
PERC (0.813), it is only really better on the most
clear-cut cases (the 90-100% bin). If one were to re-
port unweighted precision and recall without using
any bins?as is the norm?this important qualifica-
tion would have been harder to discover.
While this example uses the same dataset for eval-
uating two systems, the procedure is general enough
to allow two systems to be compared on two dif-
ferent datasets by simply examining the two plots.
However, two potential issues arise in that case. The
first is that the bin sizes will likely vary across the
two plots. However, this should not be a significant
problem as long as the bins are sufficiently large. A
second, more serious, issue is that the error rates (the
proportion of instances that are actually erroneous)
in each bin may be different across the two plots. To
handle this, we recommend that a kappa-agreement
plot be used instead of the precision-agreement plot
shown here.
5 Conclusions
Our goal is to propose best practices to address the
two primary problems in evaluating grammatical er-
ror detection systems and we do so by leveraging
crowdsourcing. For system development, we rec-
ommend that rather than compressing multiple judg-
ments down to the majority, it is better to use agree-
ment proportions to weight precision and recall to
yield fairer and more stable indicators of perfor-
mance.
For system comparison, we argue that the best
solution is to use a shared dataset and present the
precision-agreement plot using a set of agreed-upon
bins (possibly in conjunction with the weighted pre-
cision and recall measures) for a more informative
comparison. However, we recognize that shared
datasets are harder to create in this field (as most of
the data is proprietary). Therefore, we also provide
a way to compare multiple systems across differ-
ent datasets by using kappa-agreement plots. As for
agreement bins, we posit that the agreement values
used to define them depend on the task and, there-
fore, should be determined by the community.
Note that both of these practices can also be im-
plemented by using 20 experts instead of 20 Turkers.
However, we show that crowdsourcing yields judg-
ments that are as good but without the cost. To fa-
cilitate the adoption of these practices, we make all
our evaluation code and data available to the com-
munity.5
Acknowledgments
We would first like to thank our expert annotators
Sarah Ohls and Waverely VanWinkle for their hours
of hard work. We would also like to acknowledge
Lei Chen, Keelan Evanini, Jennifer Foster, Derrick
Higgins and the three anonymous reviewers for their
helpful comments and feedback.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In Pro-
ceedings of the NAACL Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk,
pages 195?203.
Chris Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP, pages 286?
295.
Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz.
2009. A Demonstration of Human Computation Us-
ing the Phrase Detectives Annotation Game. In ACM
SIGKDD Workshop on Human Computation, pages
23?24.
5http://bit.ly/crowdgrammar
512
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text Massaging for Computational Linguistics
as a New Shared Task. In Proceedings of INLG.
Keelan Evanini, Derrick Higgins, and Klaus Zechner.
2010. Using Amazon Mechanical Turk for Transcrip-
tion of Non-Native Speech. In Proceedings of the
NAACL Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 53?56.
Rachele De Felice and Stephen Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceedings
of COLING, pages 169?176.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating Named Entities in Twitter Data with
Crowdsourcing. In Proceedings of the NAACL Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 80?88.
Yoav Freund and Robert E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277?296.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of IJCNLP.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Proceedings
of NAACL, pages 163?171.
Y. Guo and Gulbahar Beckett. 2007. The Hegemony
of English as a Global Language: Reclaiming Local
Knowledge and Culture in China. Convergence: In-
ternational Journal of Adult Education, 1.
Ann Irvine and Alexandre Klementiev. 2010. Using
Mechanical Turk to Annotate Lexicons for Less Com-
monly Used Languages. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 108?113.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. thesis,
Department of Computer Science, University of Mary-
land College Park.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
Fast and Good Enough: Automatic Speech Recogni-
tion with Non-Expert Transcription. In Proceedings
of NAACL, pages 207?215.
Nicholas Rizzolo and Dan Roth. 2007. Modeling
Discriminative Global Inference. In Proceedings of
the First IEEE International Conference on Semantic
Computing (ICSC), pages 597?604, Irvine, California,
September.
Alla Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACLWorkshop on Innovative Use of NLP for Build-
ing Educational Applications.
Alla Rozovskaya and D. Roth. 2010b. Generating Con-
fusion Sets for Context-Sensitive Error Correction. In
Proceedings of EMNLP.
Andreas Stolcke. 2002. SRILM: An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 257?286.
Joel Tetreault and Martin Chodorow. 2008. The Ups and
Downs of Preposition Error Detection in ESL Writing.
In Proceedings of COLING, pages 865?872.
Joel Tetreault, Jill Burstein, and Claudia Leacock, edi-
tors. 2010a. Proceedings of the NAACL Workshop on
Innovative Use of NLP for Building Educational Ap-
plications.
Joel Tetreault, Elena Filatova, and Martin Chodorow.
2010b. Rethinking Grammatical Error Annotation and
Evaluation with the Amazon Mechanical Turk. In Pro-
ceedings of the NAACL Workshop on Innovative Use
of NLP for Building Educational Applications, pages
45?48.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 163?167.
Omar F. Zaidan and Chris Callison-Burch. 2010. Pre-
dicting Human-Targeted Translation Edit Rate via Un-
trained Human Annotators. In Proceedings of NAACL,
pages 369?372.
513
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 145?150,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ParaQuery: Making Sense of Paraphrase Collections
Lili Kotlerman
Bar-Ilan University
Israel
lili.dav@gmail.com
Nitin Madnani and Aoife Cahill
Educational Testing Service
Princeton, NJ, USA
{nmadnani,acahill}@ets.org
Abstract
Pivoting on bilingual parallel corpora is a
popular approach for paraphrase acquisi-
tion. Although such pivoted paraphrase
collections have been successfully used to
improve the performance of several dif-
ferent NLP applications, it is still difficult
to get an intrinsic estimate of the qual-
ity and coverage of the paraphrases con-
tained in these collections. We present
ParaQuery, a tool that helps a user inter-
actively explore and characterize a given
pivoted paraphrase collection, analyze its
utility for a particular domain, and com-
pare it to other popular lexical similarity
resources ? all within a single interface.
1 Introduction
Paraphrases are widely used in many Natural Lan-
guage Processing (NLP) tasks, such as informa-
tion retrieval, question answering, recognizing
textual entailment, text simplification etc. For ex-
ample, a question answering system facing a ques-
tion ?Who invented bifocals and lightning rods??
could retrieve the correct answer from the text
?Benjamin Franklin invented strike termination
devices and bifocal reading glasses? given the in-
formation that ?bifocal reading glasses? is a para-
phrase of ?bifocals? and ?strike termination de-
vices? is a paraphrase of ?lightning rods?.
There are numerous approaches for automati-
cally extracting paraphrases from text (Madnani
and Dorr, 2010). We focus on generating para-
phrases by pivoting on bilingual parallel corpora
as originally suggested by Bannard and Callison-
Burch (2005). This technique operates by attempt-
ing to infer semantic equivalence between phrases
in the same language by using a second language
as a bridge. It builds on one of the initial steps used
to train a phrase-based statistical machine transla-
tion system. Such systems rely on phrase tables ?
a tabulation of correspondences between phrases
in the source language and phrases in the target
language. These tables are usually extracted by in-
ducing word alignments between sentence pairs in
a parallel training corpus and then incrementally
building longer phrasal correspondences from in-
dividual words and shorter phrases. Once such a
tabulation of bilingual correspondences is avail-
able, correspondences between phrases in one lan-
guage may be inferred simply by using the phrases
in the other language as pivots, e.g., if both ?man?
and ?person? correspond to ?personne? in French,
then they can be considered paraphrases. Each
paraphrase pair (rule) in a pivoted paraphrase col-
lection is defined by a source phrase e1, the target
phrase e2 that has been inferred as its paraphrase,
and a probability score p(e2|e1) obtained from the
probability values in the bilingual phrase table.1
Pivoted paraphrase collections have been suc-
cessfully used in different NLP tasks including
automated document summarization (Zhou et al,
2006), question answering (Riezler et al, 2007),
and machine translation (Madnani, 2010). Yet, it
is still difficult to get an estimate of the intrinsic
quality and coverage of the paraphrases contained
in these collections. To remedy this, we propose
ParaQuery ? a tool that can help explore and ana-
lyze pivoted paraphrase collections.
2 ParaQuery
In this section we first briefly describe how to set
up ParaQuery (?2.1) and then demonstrate its use
in detail for interactively exploring and character-
izing a paraphrase collection, analyzing its util-
ity for a particular domain, and comparing it with
other word-similarity resources (?2.2). Detailed
documentation will be included in the tool.
1There may be other values associated with each pair, but
we ignore them for the purposes of this paper.
145
2.1 Setting up
ParaQuery operates on pivoted paraphrase collec-
tions and can accept collections generated using
any set of tools that are preferred by the user, as
long as the collection is stored in a pre-defined
plain-text format containing the source and target
phrases, the probability values, as well as informa-
tion on pivots (optional but useful for pivot-driven
analysis, as shown later). This format is com-
monly used in the machine translation and para-
phrase generation community. In this paper, we
adapt the Thrax and Joshua (Ganitkevitch et al,
2012) toolkits to generate a pivoted paraphrase
collection using the English-French EuroParl par-
allel corpus, which we use as our example col-
lection for demonstrating ParaQuery. Once a piv-
oted collection is generated, ParaQuery needs to
convert it into an SQLite database against which
queries can be run. This is done by issuing the
index command at the ParaQuery command-line
interface (described in ?2.2.1).
2.2 Exploration and Analysis
In order to provide meaningful exploration and
analysis, we studied various scenarios in which
paraphrase collections are used, and found that the
following issues typically interest the developers
and users of such collections:
1. Semantic relations between the paraphrases
in the collection (e.g. synonymy, hyponymy)
and their frequency.
2. The frequency of inaccurate paraphrases,
possible ways of de-noising the collection,
and the meaningfulness of scores (better
paraphrases should be scored higher).
3. The utility of the collection for a specific do-
main, i.e. whether domain terms of interest
are present in the collection.
4. Comparison of different collections based on
the above dimensions.
We note that paraphrase collections are used in
many tasks with different acceptability thresholds
for semantic relations, noisy paraphrases etc. We
do not intend to provide an exhaustive judgment
of paraphrase quality, but instead allow users to
characterize a collection, enabling an analysis of
the aforesaid issues and providing information for
them to decide whether a given collection is suit-
able for their specific task and/or domain.
2.2.1 Command line interface
ParaQuery allows interactive exploration and
analysis via a simple command line interface, by
processing user issued queries such as:
show <query>: display the rules which satisfy
the conditions of the given query.
show count <query>: display the number of
such rules.
explain <query>: display information about the
pivots which yielded each of these rules.
analyze <query>: display statistics about these
rules and save a report to an output file.
The following information is stored in the
SQLite database for each paraphrase rule:2
? The source and the target phrases, and the
probability score of the rule.
? Are the source and the target identical?
? Do the source and the target have the same
part of speech?3
? Length of the source and the target, and the
difference in their lengths.
? Number of pivots and the list of pivots.
? Are both the source and the target found in
WordNet (WN)? If yes, the WN relation be-
tween them (synonym, derivation, hypernym,
hyponym, co-hyponym, antonym, meronym,
holonym, pertainym) or the minimal dis-
tance, if they are not connected directly.
Therefore, all of the above can be used, alone or
in combination, to constrain the queries and de-
fine the rule(s) of interest. Figure 1 presents sim-
ple queries processed by the show command: the
first query displays top-scoring rules with ?man?
as their source phrase, while the second adds re-
striction on the rules? score. By default, the tool
displays the 10 best-scoring rules per query, but
this limit can be changed as shown. For each
rule, the corresponding score and semantic rela-
tion/distance is displayed.
2Although some of this information is available in the
paraphrase collection that was indexed, the remaining is auto-
matically computed and injected into the database during the
indexing process. Indexing the French-pivoted paraphrase
collection (containing 3,633,015 paraphrase rules) used in
this paper took about 6 hours.
3We use the simple parts of speech provided by WordNet
(nouns, verbs, adjectives and adverbs).
146
The queries provide a flexible way to define and
work with the rule set of interest, starting from fil-
tering low-scoring rules till extracting specific se-
mantic relations or constraining on the number of
pivots. Figure 2 presents additional examples of
queries. The tool also enables filtering out target
terms with a recurrent lemma, as illustrated in the
same figure. Note that ParaQuery also contains a
batch mode (in addition to the interactive mode il-
lustrated so far) to automatically extract the output
for a set of queries contained in a batch script.
Figure 1: Examples of the show command and the
probability constraint.
2.2.2 Analyzing pivot information
It is well known that pivoted paraphrase collec-
tions contain a lot of noisy rules. To understand
the origins of such rules, an explain query can be
used, which displays the pivots that yielded each
paraphrase rule, and the probability share of each
pivot in the final probability score. Figure 3 shows
an example of this command.
We see that noisy rules can originate from stop-
word pivots, e.g. ?l?. It is common to filter rules
containing stop-words, yet perhaps it is also im-
portant to exclude stop-word pivots, which was
never considered in the past. We can use Para-
Query to further explore whether discarding stop-
word pivots is a good idea. Figure 4 presents
a more complex query showing paraphrase rules
that were extracted via a single pivot ?l?. We see
that the top 5 such rules are indeed noisy, indicat-
ing that perhaps all of the 5,360 rules satisfying
the query can be filtered out.
2.2.3 Analysis of rule sets
In order to provide an overall analysis of a rule set
or a complete collection, ParaQuery includes the
Figure 2: Restricting the output of the show com-
mand using WordNet relations and distance, and
the unique lemma constraint.
Figure 3: An example of the explain command.
analyze command. Figure 5 shows the typical in-
formation provided by this command. In addition,
a report is generated to a file, including the anal-
ysis information for the whole rule set and for its
three parts: top, middle and bottom, as defined by
the scores of the rules in the set. The output to the
file is more detailed and expands on the informa-
tion presented in Figure 5. For example, it also
includes, for each part, rule samples and score dis-
tributions for each semantic relation and different
WordNet distances.
The information contained in the report can be
147
Figure 4: Exploring French stop-word pivots using the pivots condition of the show command.
Figure 5: An example of the analyze command (full output not shown for space reasons).
148
TOP BOTTOM
finest? better approach? el
outdoors? external effect? parliament
unsettled? unstable comment? speak up
intelligentsia? intelligence propose? allotted
caretaker? provisional prevent? aimed
luckily? happily energy? subject matter
Table 1: A random sample of undefined relation
rules from our collection?s top and bottom parts.
easily used for generating graphs and tables. For
example, Figure 6 shows the distribution of se-
mantic relations in the three parts of our exam-
ple paraphrase collection. The figure character-
izes the collection in terms of semantic relations
it contains and illustrates the fact that the scores
agree with their desired behavior: (1) the collec-
tion?s top-scoring part contains significantly more
synonyms than its middle and bottom parts, (2)
similar trends hold for derivations and hypernyms,
which are more suitable for paraphrasing than co-
hyponyms and other relations not defined in Word-
Net (we refer to these relations as undefined rela-
tions), (3) such undefined relations have the high-
est frequency in the collection?s bottom part, and
are least frequent in its top part. Among other
conclusions, the figure shows, that discarding the
lower-scoring middle and bottom parts of the col-
lection would allow retaining almost all the syn-
onyms and derivations, while filtering out most of
the co-hyponyms and a considerable number of
undefined relations.
Yet from Figure 6 we see that undefined rela-
tions constitute the majority of the rules in the col-
lection. To better understand this, random rule
samples provided in the analysis output can be
used, as shown in Table 1. From this table, we see
that the top-part rules are indeed mostly valid for
paraphrasing, unlike the noisy bottom-part rules.
The score distributions reported as part of the anal-
ysis can be used to further explore the collec-
tion and set sound thresholds suitable for different
tasks and needs.
2.2.4 Analysis of domain utility
One of the frequent questions of interest is
whether a given collection is suitable for a specific
domain. To answer this question, ParaQuery al-
lows the user to run the analysis from ?2.2.3 over
rules whose source phrases belong to a specific
domain, by means of the analyze <query> us-
ing <file> command. The file can hold either a
list of domain terms or a representative domain
text, from which frequent terms and term collo-
cations will be automatically extracted, presented
to the user, and utilized for analysis. The analysis
includes the coverage of the domain terms in the
paraphrase collection, and can also be restricted to
top-K rules per source term, a common practice in
many NLP applications. We do not show an exam-
ple of this command due to space considerations.
2.2.5 Comparison with other collections
The output of the analyze command can also be
used to compare different collections, either in
general or for a given domain. Although Para-
Query is designed for pivoted paraphrase collec-
tions, it allows comparing them to non-pivoted
paraphrase collections as well. Next we present an
example of such a comparative study, performed
using ParaQuery via several analyze commands.
Table 2 compares three different collections:
the French pivoted paraphrase collection, a dis-
tributional similarity resource (Kotlerman et al,
2010) and a Wikipedia-based resource (Shnarch et
al., 2009). The table shows the collection sizes,
as well as the number of different (unique) source
phrases in them and, correspondingly, the average
number of target phrases per source. From the
table we can see that the distributional similarity
resource contains a lot of general language terms
found in WordNet, while the Wikipedia resource
includes only a small amount of such terms. A
sample of rules from the Wikipedia collection ex-
plains this behavior, e.g. ?Yamaha SR500 ? mo-
torcycle?. The table provides helpful information
to decide which collection is (more) suitable for
specific tasks, such as paraphrase recognition and
generation, query expansion, automatic generation
of training data for different supervised tasks, etc.
3 Conclusions and Future Work
We presented ParaQuery?a tool for interactive
exploration and analysis of pivoted paraphrase
collections?and showed that it can be used to
estimate the intrinsic quality and coverage of the
paraphrases contained in these collections, a task
that is still somewhat difficult. ParaQuery can also
be used to answer the questions that users of such
collections are most interested in. We plan to re-
lease ParaQuery under an open-source license, in-
cluding our code for generating paraphrase col-
lections that can then be indexed and analyzed by
149
0%
20%
40%
60%
80% Top Middle Bottom
22% 11% 8% 4% 8% 0%
45%
6% 1% 4% 4%
17%
0%
68%
1% 0% 1% 1% 11% 0%
86%
Synonym Derivation Hypernym Hyponym Co-hyponym Antonym Undefined
Figure 6: Distribution of semantic relations in the top, middle and bottom parts of the example collection.
The parts are defined by binning the scores of the rules in the collection.
Collection Size (rules) In WordNet Unique Src Avg. Tgts per Src davg for UR
Pivoted (FR) 3,633,015 757,994 (21%) 188,898 16.064 2.567
Dist.Sim. 7,298,321 3,252,967 (45%) 113,444 64.334 6.043
Wikipedia 7,880,962 295,161 (4%) 2,727,362 2.890 8.556
Table 2: Comparing the French-pivoted paraphrase collection to distributional-similarity based and
Wikipedia-based similarity collections, in terms of total size, percentage of rules in WordNet, number
of unique source phrases, average number of target phrases per source phrase, and the average WordNet
distance between the two sides of the undefined relation (UR) rules.
ParaQuery. We also plan to include pre-generated
paraphrase collections in the release so that users
of ParaQuery can use it immediately.
In the future, we plan to use this tool for analyz-
ing the nature of pivoted paraphrases. The quality
and coverage of these paraphrases is known to de-
pend on several factors, including (a) the genre of
the bilingual corpus, (b) the word-alignment algo-
rithm used during bilingual training, and (c) the
pivot language itself. However, there have been
no explicit studies designed to measure such vari-
ations. We believe that ParaQuery is perfectly
suited to conducting such studies and moving the
field of automated paraphrase generation forward.
Acknowledgments
This work was partially supported by the European Commu-
nity?s Seventh Framework Programme (FP7/2007-2013) un-
der grant agreement no. 287923 (EXCITEMENT).
References
Colin Bannard and Chris Callison-Burch. 2005. Paraphras-
ing with Bilingual Parallel Corpora. In Proceedings of
ACL, pages 597?604.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and
Chris Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and Paraphrases. In Proceedings of WMT, pages 283?291.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distributional
Similarity for Lexical Inference. Natural Language En-
gineering, 16(4):359?389.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
driven Methods. Computational Linguistics, 36(3):341?
387.
Nitin Madnani. 2010. The Circle of Meaning: From Trans-
lation to Paraphrasing and Back. Ph.D. thesis, Depart-
ment of Computer Science, University of Maryland Col-
lege Park.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu O. Mittal, and Yi Liu. 2007. Statistical
Machine Translation for Query Expansion in Answer Re-
trieval. In Proceedings of ACL, pages 464?471.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Extracting
lexical reference rules from Wikipedia. In Proceedings of
ACL-IJCNLP, pages 450?458.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Muntenau, and
Eduard Hovy. 2006. ParaEval: Using Paraphrases to
Evaluate Summaries Automatically. In Proceedings of
HLT-NAACL, pages 447?454.
150
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 174?180,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Predicting Grammaticality on an Ordinal Scale
Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland
Educational Testing Service
Princeton, NJ, USA
{mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org
Joel Tetreault
Yahoo! Research
New York, NY, USA
tetreaul@yahoo-inc.com
Abstract
Automated methods for identifying
whether sentences are grammatical
have various potential applications (e.g.,
machine translation, automated essay
scoring, computer-assisted language
learning). In this work, we construct a
statistical model of grammaticality using
various linguistic features (e.g., mis-
spelling counts, parser outputs, n-gram
language model scores). We also present
a new publicly available dataset of learner
sentences judged for grammaticality on
an ordinal scale. In evaluations, we
compare our system to the one from Post
(2011) and find that our approach yields
state-of-the-art performance.
1 Introduction
In this paper, we develop a system for the task
of predicting the grammaticality of sentences, and
present a dataset of learner sentences rated for
grammaticality. Such a system could be used, for
example, to check or to rank outputs from systems
for text summarization, natural language genera-
tion, or machine translation. It could also be used
in educational applications such as essay scoring.
Much of the previous research on predicting
grammaticality has focused on identifying (and
possibly correcting) specific types of grammati-
cal errors that are typically made by English lan-
guage learners, such as prepositions (Tetreault and
Chodorow, 2008), articles (Han et al, 2006), and
collocations (Dahlmeier and Ng, 2011). While
some applications (e.g., grammar checking) rely
on such fine-grained predictions, others might be
better addressed by sentence-level grammaticality
judgments (e.g., machine translation evaluation).
Regarding sentence-level grammaticality, there
has been much work on rating the grammatical-
ity of machine translation outputs (Gamon et al,
2005; Parton et al, 2011), such as the MT Quality
Estimation Shared Tasks (Bojar et al, 2013, ?6),
but relatively little on evaluating the grammatical-
ity of naturally occurring text. Also, most other re-
search on evaluating grammaticality involves arti-
ficial tasks or datasets (Sun et al, 2007; Lee et al,
2007; Wong and Dras, 2010; Post, 2011).
Here, we make the following contributions.
? We develop a state-of-the-art approach for
predicting the grammaticality of sentences on
an ordinal scale, adapting various techniques
from the previous work described above.
? We create a dataset of grammatical and un-
grammatical sentences written by English
language learners, labeled on an ordinal scale
for grammaticality. With this unique data set,
which we will release to the research com-
munity, it is now possible to conduct realis-
tic evaluations for predicting sentence-level
grammaticality.
2 Dataset Description
We created a dataset consisting of 3,129 sentences
randomly selected from essays written by non-
native speakers of English as part of a test of
English language proficiency. We oversampled
lower-scoring essays to increase the chances of
finding ungrammatical sentences. Two of the au-
thors of this paper, both native speakers of English
with linguistic training, annotated the data. We
refer to these annotators as expert judges. When
making judgments of the sentences, they saw the
previous sentence from the same essay as context.
These two authors were not directly involved in
development of the system in ?3.
Each sentence was annotated on a scale from
1 to 4 as described below, with 4 being the most
174
grammatical. We use an ordinal rather than bi-
nary scale, following previous work such as that of
Clark et al (2013) and Crocker and Keller (2005)
who argue that the distinction between grammati-
cal and ungrammatical is not simply binary. Also,
for practical applications, we believe that it is use-
ful to distinguish sentences with minor errors from
those with major errors that may disrupt communi-
cation. Our annotation scheme was influenced by
a translation rating scheme by Coughlin (2003).
Every sentence judged on the 1?4 scale must be
a clause. There is an extra category (?Other?) for
sentences that do not fit this criterion. We exclude
instances of ?Other? in our experiments (see ?4).
4. Perfect The sentence is native-sounding. It has
no grammatical errors, but may contain very mi-
nor typographical and/or collocation errors, as in
Example (1).
(1) For instance, i stayed in a dorm when i
went to collge.
3. Comprehensible The sentence may contain
one or more minor grammatical errors, includ-
ing subject-verb agreement, determiner, and mi-
nor preposition errors that do not make the mean-
ing unclear, as in Example (2).
(2) We know during Spring Festival, Chinese
family will have a abundand family banquet
with family memebers.
?Chinese family?, which could be corrected to
?Chinese families?, ?each Chinese family?, etc.,
would be an example of a minor grammatical er-
ror involving determiners.
2. Somewhat Comprehensible The sentence
may contain one or more serious grammatical
errors, including missing subject, verb, object,
etc., verb tense errors, and serious preposition
errors. Due to these errors, the sentence may
have multiple plausible interpretations, as in
Example (3).
(3) I can gain the transportations such as buses
and trains.
1. Incomprehensible The sentence contains so
many errors that it would be difficult to correct,
as in Example (4).
(4) Or you want to say he is only a little boy do
not everything clearly?
The phrase ?do not everything? makes the sen-
tence practically incomprehensible since the sub-
ject of ?do? is not clear.
O. Other/Incomplete This sentence is incom-
plete. These sentences, such as Example (5), ap-
pear in our corpus due to the nature of timed tests.
(5) The police officer handed the
This sentence is cut off and does not at least in-
clude one clause.
We measured interannotator agreement on a
subset of 442 sentences that were independently
annotated by both expert annotators. Exact agree-
ment was 71.3%, unweighted ? = 0.574, and
Pearson?s r = 0.759.
1
For our experiments, one
expert annotator was arbitrarily selected, and for
the doubly-annotated sentences, only the judg-
ments from that annotator were retained.
The labels from the expert annotators are dis-
tributed as follows: 72 sentences are labeled 1;
538 are 2; 1,431 are 3; 978 are 4; and 110 are ?O?.
We also gathered 5 additional judgments using
Crowdflower.
2
For this, we excluded the ?Other?
category and any sentences that had been marked
as such by the expert annotators. We used 100
(3.2%) of the judged sentences as ?gold? data in
Crowdflower to block contributors who were not
following the annotation guidelines. For those
sentences, only disagreements within 1 point of
the expert annotator judgment were accepted. In
preliminary experiments, averaging the six judg-
ments (1 expert, 5 crowdsourced) for each item
led to higher human-machine agreement. For all
experiments reported later, we used this average
of six judgments as our gold standard.
For our experiments (?4), we randomly split the
data into training (50%), development (25%), and
testing (25%) sets. We also excluded all instances
labeled ?Other?. These are relatively uncommon
and less interesting to this study. Also, we believe
that simpler, heuristic approaches could be used to
identify such sentences.
We use ?GUG? (?Grammatical? versus ?Un-
Grammatical?) to refer to this dataset. The dataset
is available for research at https://github.
com/EducationalTestingService/
gug-data.
1
The reported agreement values assume that ?Other?
maps to 0. For the sentences where both labels were in the
1?4 range (n = 424), Pearson?s r = 0.767.
2
http://www.crowdflower.com
175
3 System Description
This section describes the statistical model (?3.1)
and features (?3.2) used by our system.
3.1 Statistical Model
We use `
2
-regularized linear regression (i.e., ridge
regression) to learn a model of sentence grammat-
icality from a variety of linguistic features.
34
To tune the `
2
-regularization hyperparameter ?,
the system performs 5-fold cross-validation on the
data used for training. The system evaluates ? ?
10
{?4,...,4}
and selects the one that achieves the
highest cross-validation correlation r.
3.2 Features
Next, we describe the four types of features.
3.2.1 Spelling Features
Given a sentence with with n word tokens, the
model filters out tokens containing nonalpha-
betic characters and then computes the num-
ber of misspelled words n
miss
(later referred to
as num misspelled), the proportion of mis-
spelled words
n
miss
n
, and log(n
miss
+ 1) as fea-
tures. To identify misspellings, we use a freely
available spelling dictionary for U.S. English.
5
3.2.2 n-gram Count and Language Model
Features
Given each sentence, the model obtains the counts
of n-grams (n = 1 . . . 3) from English Gigaword
and computes the following features:
6
?
?
s?S
n
log(count(s) + 1)
?S
n
?
3
We use ridge regression from the scikit-learn
toolkit (Pedregosa et al, 2011) v0.23.1 and the
SciKit-Learn Laboratory (http://github.com/
EducationalTestingService/skll).
4
Regression models typically produce conservative pre-
dictions with lower variance than the original training data.
So that predictions better match the distribution of labels in
the training data, the system rescales its predictions. It saves
the mean and standard deviation of the training data gold
standard (M
gold
and SD
gold
, respectively) and of its own
predictions on the training data (M
pred
and SD
pred
, respec-
tively). During cross-validation, this is done for each fold.
From an initial prediction y?, it produces the final prediction:
y?
?
=
y??M
pred
SD
pred
? SD
gold
+M
gold
. This transformation does
not affect Pearson?s r correlations or rankings, but it would
affect binarized predictions.
5
http://pythonhosted.org/pyenchant/
6
We use the New York Times (nyt), the Los Ange-
les Times-Washington Post (ltw), and the Washington Post-
Bloomberg News (wpb) sections from the fifth edition of En-
glish Gigaword (LDC2011T07).
? max
s?S
n
log(count(s) + 1)
? min
s?S
n
log(count(s) + 1)
where S
n
represents the n-grams of order n from
the given sentence. The model computes the fol-
lowing features from a 5-gram language model
trained on the same three sections of English Gi-
gaword using the SRILM toolkit (Stolcke, 2002):
? the average log-probability of the
given sentence (referred to as
gigaword avglogprob later)
? the number of out-of-vocabulary words in the
sentence
Finally, the system computes the average
log-probability and number of out-of-vocabulary
words from a language model trained on a col-
lection of essays written by non-native English
speakers
7
(?non-native LM?).
3.2.3 Precision Grammar Features
Following Wagner et al (2007) and Wagner et
al. (2009), we use features extracted from preci-
sion grammar parsers. These grammars have been
hand-crafted and designed to only provide com-
plete syntactic analyses for grammatically cor-
rect sentences. This is in contrast to treebank-
trained grammars, which will generally provide
some analysis regardless of grammaticality. Here,
we use (1) the Link Grammar Parser
8
and (2)
the HPSG English Resource Grammar (Copestake
and Flickinger, 2000) and PET parser.
9
We use a binary feature, complete link,
from the Link grammar that indicates whether at
least one complete linkage can be found for a sen-
tence. We also extract several features from the
HPSG analyses.
10
They mostly reflect information
about unification success or failure and the associ-
ated costs. In each instance, we use the logarithm
of one plus the frequency.
7
This did not overlap with the data described in ?2 and
was a subset of the data released by Blanchard et al (2013).
8
http://www.link.cs.cmu.edu/link/
9
http://moin.delph-in.net/PetTop
10
The complete list of relevant statistics used as features
is: trees, unify cost succ, unify cost fail,
unifications succ, unifications fail,
subsumptions succ, subsumptions fail,
words, words pruned, aedges, pedges,
upedges, raedges, rpedges, medges. During
development, we observed that some of these features vary
for some inputs, probably due to parsing search timeouts. On
10 preliminary runs with the development set, this variance
had minimal effects on correlations with human judgments
(less than 0.00001 in terms of r).
176
rour system 0.668
? non-native LM (?3.2.2) 0.665
? HPSG parse (?3.2.3) 0.664
? PCFG parse (?3.2.4) 0.662
? spelling (?3.2.1) 0.643
? gigaword LM (?3.2.2) 0.638
? link parse (?3.2.3) 0.632
? gigaword count (?3.2.2) 0.630
Table 1: Pearson?s r on the development set, for
our full system and variations excluding each fea-
ture type. ?? X? indicates the full model without
the ?X? features.
3.2.4 PCFG Parsing Features
We find phrase structure trees and basic depen-
dencies with the Stanford Parser?s English PCFG
model (Klein and Manning, 2003; de Marneffe et
al., 2006).
11
We then compute the following:
? the parse score as provided by the Stan-
ford PCFG Parser, normalized for sentence
length, later referred to as parse prob
? a binary feature that captures whether the top
node of the tree is sentential or not (i.e. the
assumption is that if the top node is non-
sentential, then the sentence is a fragment)
? features binning the number of dep rela-
tions returned by the dependency conversion.
These dep relations are underspecified for
function and indicate that the parser was un-
able to find a standard relation such as subj,
possibly indicating a grammatical error.
4 Experiments
Next, we present evaluations on the GUG dataset.
4.1 Feature Ablation
We conducted a feature ablation study to iden-
tify the contributions of the different types of fea-
tures described in ?3.2. We compared the perfor-
mance of the full model with all of the features
to models with all but one type of feature. For
this experiment, all models were estimated from
the training set and evaluated on the development
set. We report performance in terms of Pearson?s
r between the averaged 1?4 human labels and un-
rounded system predictions.
The results are shown in Table 1. From these
results, the most useful features appear to be the
n-gram frequencies from Gigaword and whether
the link parser can fully parse the sentence.
4.2 Test Set Results
In this section, we present results on the held-out
test set for the full model and various baselines,
summarized in Table 2. For test set evaluations,
we trained on the combination of the training and
development sets (?2), to maximize the amount of
training data for the final experiments.
We also trained and evaluated on binarized ver-
sions of the ordinal GUG labels: a sentence was
labeled 1 if the average judgment was at least 3.5
(i.e., would round to 4), and 0 otherwise. Evaluat-
ing on a binary scale allows us to measure how
well the system distinguishes grammatical sen-
tences from ungrammatical ones. For some ap-
plications, this two-way distinction may be more
relevant than the more fine-grained 1?4 scale. To
train our system on binarized data, we replaced the
`
2
-regularized linear regression model with an `
2
-
regularized logistic regression and used Kendall?s
? rank correlation between the predicted probabil-
ities of the positive class and the binary gold stan-
dard labels as the grid search metric (?3.1) instead
of Pearson?s r.
For the ordinal task, we report Pearson?s r be-
tween the averaged human judgments and each
system. For the binary task, we report percentage
accuracy. Since the predictions from the binary
and ordinal systems are on different scales, we in-
clude the nonparametric statistic Kendall?s ? as a
secondary evaluation metric for both tasks.
We also evaluated the binary system for the or-
dinal task by computing correlations between its
estimated probabilities and the averaged human
scores, and we evaluated the ordinal system for the
binary task by binarizing its predictions.
12
We compare our work to a modified version of
the publicly available
13
system from Post (2011),
which performed very well on an artificial dataset.
To our knowledge, it is the only publicly available
system for grammaticality prediction. It is very
11
We use the Nov. 12, 2013 version of the Stanford Parser.
12
We selected a threshold for binarization from a grid of
1001 points from 1 to 4 that maximized the accuracy of bina-
rized predictions from a model trained on the training set and
evaluated on the binarized development set. For evaluating
the three single-feature baselines discussed below, we used
the same approach except with grid ranging from the min-
imum development set feature value to the maximum plus
0.1% of the range.
13
The Post (2011) system is available at https://
github.com/mjpost/post2011judging.
177
Ordinal Task Binary Task
r Sig.
r
? % Acc. Sig.
%Acc.
?
our system 0.644 0.479 79.3 0.419
our system
logistic
0.616 * 0.484 80.7 0.428
Post 0.321 * 0.225 75.5 * 0.195
Post
logistic
0.259 * 0.181 74.4 * 0.181
complete link 0.386 * 0.335 74.8 * 0.302
gigaword avglogprob 0.414 * 0.290 76.7 * 0.280
num misspelled -0.462 * -0.370 74.8 * -0.335
Table 2: Human-machine agreement statistics for our system, the system from Post (2011), and simple
baselines, computed from the averages of human ratings in the testing set (?2). ?*? in a Sig. column
indicates a statistically significant difference from ?our system? (p < .05, see text for details). A majority
baseline for the binary task achieves 74.8% accuracy. The best results for each metric are in bold.
different from our system since it relies on par-
tial tree-substitution grammar derivations as fea-
tures. We use the feature computation components
of that system but replace its statistical model. The
system was designed for use with a dataset consist-
ing of 50% grammatical and 50% ungrammatical
sentences, rather than data with ordinal or continu-
ous labels. Additionally, its classifier implementa-
tion does not output scores or probabilities. There-
fore, we used the same learning algorithms as for
our system (i.e., ridge regression for the ordinal
task and logistic regression for the binary task).
14
To create further baselines for comparison,
we selected the following features that represent
ways one might approximate grammaticality if a
comprehensive model was unavailable: whether
the link parser can fully parse the sentence
(complete link), the Gigaword language
model score (gigaword avglogprob),
and the number of misspelled tokens
(num misspelled). Note that we expect
the number of misspelled tokens to be negatively
correlated with grammaticality. We flipped the
sign of the misspelling feature when computing
accuracy for the binary task.
To identify whether the differences in perfor-
mance for the ordinal task between our system and
each of the baselines are statistically significant,
we used the BC
a
Bootstrap (Efron and Tibshirani,
1993) with 10,000 replications to compute 95%
confidence intervals for the absolute value of r for
our system minus the absolute value of r for each
of the alternative methods. For the binary task, we
14
In preliminary experiments, we observed little difference
in performance between logistic regression and the original
support vector classifier used by the system from Post (2011).
used the sign test to test for significant differences
in accuracy. The results are in Table 2.
5 Discussion and Conclusions
In this paper, we developed a system for predict-
ing grammaticality on an ordinal scale and cre-
ated a labeled dataset that we have released pub-
licly (?2) to enable more realistic evaluations in
future research. Our system outperformed an ex-
isting state-of-the-art system (Post, 2011) in eval-
uations on binary and ordinal scales. This is the
most realistic evaluation of methods for predicting
sentence-level grammaticality to date.
Surprisingly, the system from Post (2011) per-
formed quite poorly on the GUG dataset. We spec-
ulate that this is due to the fact that the Post sys-
tem relies heavily on features extracted from au-
tomatic syntactic parses. While Post found that
such a system can effectively distinguish gram-
matical news text sentences from sentences gen-
erated by a language model, measuring the gram-
maticality of real sentences from language learn-
ers seems to require a wider variety of features,
including n-gram counts, language model scores,
etc. Of course, our findings do not indicate that
syntactic features such as those from Post (2011)
are without value. In future work, it may be pos-
sible to improve grammaticality measurement by
integrating such features into a larger system.
Acknowledgements
We thank Beata Beigman Klebanov, Yoko Futagi,
Su-Youn Yoon, and the anonymous reviewers for
their helpful comments. We also thank Jennifer
Foster for discussions about this work and Matt
Post for making his system publicly available.
178
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Alexander Clark, Gianluca Giorgolo, and Shalom Lap-
pin. 2013. Towards a statistical model of grammat-
icality. In Proceedings of the 35th Annual Confer-
ence of the Cognitive Science Society, pages 2064?
2069.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation (LREC
2000), Athens, Greece.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX, pages 63?70.
Matthew W. Crocker and Frank Keller. 2005. Prob-
abilistic grammars as models of gradience in lan-
guage processing. In Gradience in Grammar: Gen-
erative Perspectives. University Press.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Correcting
Semantic Collocation Errors with L1-induced Para-
phrases. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 107?117, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC 2006, pages 449?454.
B. Efron and R. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman and Hall/CRC, Boca Ra-
ton, FL.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. In
Proceedings of EAMT, pages 103?111. Springer-
Verlag.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
John Lee, Ming Zhou, and Xiaohua Liu. 2007. De-
tection of Non-Native Sentences Using Machine-
Translated Training Data. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 93?96, Rochester, New York, April. As-
sociation for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 108?115, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Matt Post. 2011. Judging Grammaticality with Tree
Substitution Grammar Derivations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 217?222, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In 7th International Con-
ference on Spoken Language Processing.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting Erroneous Sentences using Auto-
matically Mined Sequential Patterns. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 81?88, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 865?872, Manchester, UK,
August. Coling 2008 Organizing Committee.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2007. A Comparative Evaluation of Deep
and Shallow Approaches to the Automatic Detec-
tion of Common Grammatical Errors. In Proceed-
ings of the 2007 Joint Conference on Empirical
179
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 112?121, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experi-
ments in sentence classification. CALICO Journal,
26(3):474?490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
Features for Sentence Grammaticality Classifica-
tion. In Proceedings of the Australasian Language
Technology Association Workshop 2010, pages 67?
75, Melbourne, Australia, December.
180
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 247?252,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Content Importance Models for Scoring Writing From Sources
Beata Beigman Klebanov Nitin Madnani Jill Burstein Swapna Somasundaran
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,nmadnani,jburstein,ssomasundaran}@ets.org
Abstract
Selection of information from external
sources is an important skill assessed in
educational measurement. We address an
integrative summarization task used in an
assessment of English proficiency for non-
native speakers applying to higher educa-
tion institutions in the USA. We evaluate a
variety of content importance models that
help predict which parts of the source ma-
terial should be selected by the test-taker
in order to succeed on this task.
1 Introduction
Selection and integration of information from ex-
ternal sources is an important academic and life
skill, mentioned as a critical competency in the
Common Core State Standards for English Lan-
guage Arts/Literacy: College-ready students will
be able to ?gather relevant information from mul-
tiple print and digital sources, assess the credibi-
lity and accuracy of each source, and integrate the
information while avoiding plagiarism.?
1
Accordingly, large-scale assessments of writing
incorporate tasks that test this skill. One such test
requires test-takers to read a passage, then to lis-
ten to a lecture discussing the same topic from
a different point of view, and to summarize the
points made in the lecture, explaining how they
cast doubt on points made in the reading. The qua-
lity of the information selected from the lecture is
emphasized in excerpts from the scoring rubric for
this test (below); essays are scored on a 1-5 scale:
Score 5 successfully selects the important infor-
mation from the lecture and coherently and
accurately presents this information in rela-
tion to the relevant information presented in
the reading.
1
http://www.corestandards.org/
ELA-Literacy/CCRA/W.
Score 4 is generally good in selecting the impor-
tant information from the lecture ..., but it
may have a minor omission.
Score 3 contains some important information
from the lecture ..., but it may omit one major
key point.
Score 2 contains some relevant information from
the lecture ... The response significantly
omits or misrepresents important points.
Score 1 provides little or no meaningful or rele-
vant coherent content from the lecture.
The ultimate goal of our project is to improve
automated scoring of such essays by taking into
account the extent to which a response integrates
important information from the lecture. This pa-
per reports on the first step aimed at automatically
assigning importance scores to parts of the lecture.
The next step ? developing an essay scoring sys-
tem using content importance models along with
other features of writing quality, will be addressed
in future work. A simple essay scoring mechanism
will be used for evaluation purposes in this paper,
as described in the next section.
2 Design of Experiment
In evaluations of summarization algorithms, it is
common practice to derive the gold standard con-
tent importance scores from human summaries, as
done, for example, in the pyramid method, where
the importance of a content element corresponds
to the number of reference human summaries that
make use of it (Nenkova and Passonneau, 2004).
Selection of the appropriate content plays a cru-
cial role in attaining a high score for the essays
we consider here, as suggested by the quotes from
the scoring rubric in ?1, as well as by a corpus
study by Plakans and Gebril (2013). We therefore
observe that high-scoring essays can be thought
247
of as high-quality human summaries of the lec-
ture, albeit containing, in addition, references to
the reading material and language that contrasts
the different viewpoints, making them a somewhat
noisy gold standard. On the other hand, since low-
scoring essays contain deficient summaries of the
lecture, our setup allows for a richer evaluation
than typical in studies using gold standard human
data only, in that a good model should not only
agree with the gold standard human summaries
but should also disagree with sub-standard human
summaries. We therefore use correlation with es-
say score to evaluate content importance models.
The evaluation will proceed as follows. Every
essay E is responding to a test prompt that con-
tains a lecture L and a reading R. We identify the
essay?s overlap with the lecture:
O(E,L) = {x|x ? L, x ? E} (1)
where the exact definition of x, that is, what is
taken to be a single unit of information, will be
one of the parameters to be studied. The essay is
then assigned the following score by the content
importance model M :
S
M
(E) =
?
x?O(E,L)
w
M
(x)? C(x,E)
n
E
(2)
where w
M
(x) is the importance weight as-
signed by model M to item x in the lecture,
C(x,E) is the count of tokens in E that realize
the information unit x, and n
E
is the number of
tokens in the essay. In this paper, the distinction
between x and C is that between type and token
count of instances of that type.
2
This simple sco-
ring mechanism quantifies the rate of usage of im-
portant information per token in the essay. Finally,
we calculate the correlation of scores assigned to
essays by model M with scores assigned to the
same essays by human graders.
This design ensures that once x is fixed, all the
content importance models are evaluated within
the same scoring scheme, so any differences in the
correlations can be attributed to the differences in
the weights assigned by the importance models.
2
In the future, we intend to explore more complex rea-
lization functions, allowing paraphrase, skip n-grams (as in
ROUGE (Lin, 2004)), and other approximate matches, such
as misspellings and inflectional variants.
3 Content Importance Models
Our setting can be thought of as a special kind
of summarization task. Test-takers are required
to summarize the lecture while referencing the
reading, making this a hybrid of single- and multi-
document summarization, where one source is
treated as primary and the other as secondary.
We therefore consider models of content impor-
tance that had been found useful in the summariza-
tion literature, as well as additional models that
utilize a special feature of our scenario: We have
hundreds of essays of varying quality responding
to any given prompt, as opposed to a typical news
summarization scenario where a small number of
high quality human summaries are available for a
given article. A sample of these essays can be used
when developing a content importance model.
We define the following importance models.
For all definitions, x is a unit of information
in the lecture; C(x, t) is the number of tokens in
text t that realize x; n
L
and n
R
are the number of
tokens in the lecture and the reading, respectively.
3
Na??ve: w(x) = 1. This is a simple overlap model.
Prob: w(x) =
C(x,L)
n
L
, an MLE estimate of
the probability that x appears in the lecture.
Those x that appear more are more important.
Position: w(x) =
FP (x)
n
L
, where FP (x) is the
offset of the first occurrence of x in the lec-
ture. The offset corresponds to the token?s
serial number in the text, 1 through n
L
.
LectVsRead: w(x) =
C(x,L)
n
L
?
C(x,R)
n
R
, that is, the
difference in the probabilities of occurrence
of x in the lecture and in the reading passage
that accompanies the lecture. This model at-
tempts to capture the contrastive aspect of
importance ? the content that is unique to
the lecture is more important than the content
that is shared by the lecture and the reading.
The following two models capitalize on evi-
dence of use of information in better and worse es-
says. For estimating these models, we sample, for
each prompt, a development set of 750 essays re-
sponding to the prompt (that is, addressing a given
pair of lecture and reading stimuli). Out of these,
we take, for each prompt, all essays at score points
3
Prob, Position, and LectVsRead models normalize by
n
R
and n
L
to enable comparison of essays responding to dif-
ferent lecture + reading stimuli (prompts).
248
4 and 5 (EGood) and all essays at score points 1
and 2 (EBad). These data do not overlap with the
experimental data described in section 4. In both
definitions below, e is an essay.
Good: w(x) =
|{e?EGood|x?e}|
|EGood|
. An x is more im-
portant if more good essays use it. Hong and
Nenkova (2014) showed that a variant of this
measure used on pairs of articles and their ab-
stracts from the New York Times effectively
identified words that typically go into sum-
maries, across topics. In contrast, our mea-
surements are prompt-specific.
GoodVsBad: w(x) =
|{e?EGood|x?e}|
|EGood|
?
|{e?EBad|x?e}|
|EBad|
. An x is more important if
good essays use it more than bad essays.
To our knowledge, this measure has not
been used in the summarization literature,
probably because a large sample of human
summaries of varying quality is typically not
available.
4 Data
We use 116 prompts drawn from an assessment of
English proficiency for non-native speakers. Each
prompt contains a lecture and a reading passage.
For each prompt, we sample about 750 essays.
Each essay has an operational score provided by
a human grader. Table 1 shows the distribution of
essay scores; mean score is 3. Text transcripts of
the lectures were used.
Score 1 2 3 4 5
Proportion 0.13 0.18 0.35 0.25 0.09
Table 1: Distribution of essay scores.
5 Results
Independently from the content importance
models, we address the effect of the granularity of
the unit of information. Intuitively, since all the
materials for a given prompt deal with the same
topic, we expect large unigram overlaps between
lecture and reading, and between good and bad
essays, whereas n-grams with larger n can be
more distinctive. On the other hand, larger n lead
to misses, where an information unit would fail
to be identified in an essay due to a paraphrase,
thus impairing the ability of the scoring function
to use the content importance model effectively.
We therefore evaluate each content importance
model for different granularities of the content
unit x: n-grams for n = 1, 2, 3, 4. Table 2 shows
the correlations with essay scores.
Content Pearson?s r
Importance
Model n=1 n=2 n=3 n=4
Na??ve 0.24 0.27* 0.24 0.20
Prob 0.04 0.14 0.17 0.14
Position 0.22 0.30* 0.26* 0.20
LectVsRead 0.09 0.25* 0.31* 0.26*
Good 0.07 0.15 0.10 0.07
GoodVsBad 0.54* 0.42* 0.32* 0.21
Table 2: Correlations with essay scores attained by
content models, for various definitions of informa-
tion unit (n-grams with n = 1, 2, 3, 4). Five top
scores are boldfaced. The baseline performance
is shown in underlined italics. Correlations that
are significantly better (p < 0.05) than the na??ve
n = 1 model are marked with an asterisk. We
use McNemar (1955, p. 148) test for significance
of difference between same-sample correlations.
N = 85, 252 for all correlations.
6 Discussion
The Na??ve model with n = 1 can be considered a
baseline, corresponding to unweighted word over-
lap between the lecture and the essay. This model
attains a significant positive correlation with essay
score (r = 0.24), suggesting that, in general, bet-
ter writers use more material from the lecture.
Our next observation is that the Prob and Good
models do not improve over the baseline, that is,
their weighting schemes generally assign higher
weights to the wrong units. We believe the rea-
son for this is that the most highly used n-grams,
in the lecture and in the essays, correspond to ge-
neral topical and functional elements. The impor-
tance of these elements is discounted in the more
effective Position, LectVsRead, and GoodVsBad
models, highlighting subtler aspects of the lecture.
Next, let us consider the granularity of the units
of information. We observe that 4-grams are in-
ferior to trigrams for all models, suggesting that
data sparsity is becoming a problem for matching
4-word sequences. For models that assign weight
based on one or two sources (lecture, or lecture
and reading) ? Na??ve, Position, LectVsRead ? un-
igram models are generally ineffective, while bi-
249
gram and trigram models significantly outperform
the baseline. We interpret this as suggesting that
it is certain particular, detailed aspects of the top-
ical concepts that constitute the important nuggets
in the lecture; these are usually realized by multi-
word sequences.
The GoodVsBad models show a different pat-
tern, obtaining the best performance with a uni-
gram version. These models are sensitive to data
sparsity not only when matching essays to the
lecture (this problem is common to all models)
but also during model building. Recall that the
weights in a GoodVsBad model are estimated
based on differential use in samples of good and
bad essays. The estimation of use-in-a-corpus is
more accurate for smaller n, because longer n-
grams are more susceptible to paraphrasing, which
leads to under-estimation of use. Assuming that
paraphrasing behavior of good and bad writers is
not the same ? in fact, there is corpus evidence
that better writers paraphrase more (Burstein et
al., 2012) ? the resulting inaccuracies might im-
pact the estimation of differential use in a sys-
tematic manner, making the n > 1 models less
effective than the unigrams. Given that (a) the
GoodVsBad bigram model is the second best over-
all in spite of the shortcomings of the estimation
process, and (b) that the bigram models worked
better than unigram models for all the other con-
tent importance models, the GoodVsBad bigram
model could probably be improved significantly
by using a more flexible information realization
mechanism.
To illustrate the information assigned high im-
portance by different models, consider a lec-
ture discussing advantages of fish farming. The
top-scoring Good bigrams are topical expressions
(fish farming), functional bigrams around fish and
farming,
4
aspects of content dealt with at length
in the lecture (wild fish, commercial fishing), bi-
grams referencing some of the claims ? fish con-
taining less fat and being used for fish meal. In
addition, this model picks out some sequences of
function words and punctuation (of the, are not,
?, and?, ?, the?) that suggest that better essays
tend to give more detail (hence have more com-
plex noun phrases and coordinated constructions)
and to draw contrast.
For the bigram GoodVsBad model, the topi-
cal bigram fish farming is not in the top 20 bi-
4
such as that fish, of fish, farming is, ?, fish?
grams. Although some bigrams are shared with
the Good model, the GoodVsBad model selects
additional details about the claims, such as the
contrast between inedible fish and edible fish that
is eaten by humans, as well as reference to chemi-
cals used in farming and to the claim that wild fish
are already endangered by other practices.
The most important bigrams according to the
LectVsRead model include functional bigrams
around fish and farming, functional sequences
(that the, is a), as well as commercial fishing and
edible fish. Also selected are functional bigrams
around consumption and species, hinting, indi-
rectly, at the edibility differences between species.
Finally, this model selects almost all bigrams in
the reading passage makes, the reading makes
claims that and the reading says. While distin-
guishing the lecture from the reading, these do not
capture topic-relevant content of the lecture.
The GoodVsBad unigram model selects poul-
try, endangered, edible, chemicals among its top 6
unigrams,
5
effectively touching upon the connec-
tion with other farm-raised foods (poultry, chemi-
cals), with wild fish (endangered) and with human
benefit (edible) that are made in the lecture.
7 Related work
Modern essay scoring systems are complex and
cover various aspects of the writing construct,
such as grammar, organization, vocabulary (Sher-
mis and Burstein, 2013). The quality of content
is often addressed by features that quantify the
similarity between the vocabulary used in an es-
say and reference essays from given score points
(Attali and Burstein, 2006; Foltz et al, 2013; At-
tali, 2011). For example, Attali (2011) proposed a
measure of differential use of words in higher and
lower scoring essays defined similarly to Good-
VsBad, without, however, considering the source
text at all. Such features can be thought of as con-
tent quality features, as they implicitly assume that
writers of better essays use better content. How-
ever, there are various kinds of better content, only
one of them being selection of important informa-
tion from the source; other elements of content
originate with the writer, such as examples, dis-
course markers, evaluations, introduction and con-
clusion, etc. Our approach allows focusing on a
particular aspect of content quality, namely, selec-
tion of appropriate materials from the source.
5
the other two being fishing and used.
250
Our results are related to the findings of Gure-
vich and Deane (2007) who studied the difference
between the reading and the lecture in their im-
pact on essay scores for this test. Using data from
a single prompt, they showed that the difference
between the essay?s average cosine similarity to
the reading and its average cosine similarity to the
lecture is predictive of the score for non-native
speakers of English, thus using a model similar
to LectVsRead, although they took all lecture,
reading, and essay words into account, in contrast
to our model that looks only at n-grams that ap-
pear in the lecture. Our study shows that the ef-
fectiveness of lecture-reading contrast models for
essay scoring generalizes to a large set of prompts.
Similarly, Evanini et al (2013) found that over-
lap with material that is unique to the lecture (not
shared with the reading) was predictive of scores
in a spoken source-based question answering task.
In the vast literature on summarization, our
work is closest to Hong and Nenkova (2014) who
studied models of word importance for multi-
document summarization of news. The Prob, Po-
sition, and Good models are inspired by their
findings of the effectiveness of similar models in
their setting. We found that, in our setting, Prob
and Good models performed worse than assigning
a uniform weight to all words. We note, however,
that models from Hong and Nenkova (2014) are
not strictly comparable, since their word proba-
bility models were calculated after stopword ex-
clusion, and their model that inspired our Good
model was defined somewhat differently and val-
idated using content words only. The defini-
tion of our Position model and its use in the es-
say scoring function S (equation 2) correspond to
Hong and Nenkova (2014) average first location
model for scoring summaries. Differently from
their findings, this model is not effective for sin-
gle words in our setting. Position models over n-
grams with n > 1 are effective, but their predic-
tion is in the opposite direction of that found for
the news data ? the more important materials tend
to appear later in the lecture, as indicated by the
positive r between average first position and essay
score. These findings underscore the importance
of paying attention to the genre of the source ma-
terial when developing summarization systems.
Our summarization task incorporates elements
of contrastive opinion summarization (Paul et al,
2010; Kim and Zhai, 2009), since the lecture and
the reading sometimes interpret the same facts in
a positive or negative light (for example, the fact
that chemicals are used in fish farms is negative
if compared to wild fish, but not so if compared
to other farm-raised foods like poultry). Relation-
ships between aspect and sentiment (Brody and
Elhadad, 2010; Lazaridou et al, 2013) are also
relevant, since aspects of the same fact are em-
phasized with different evaluations (the quantity
vs the variety of species that go into fish meal for
farmed fish). We hypothesize that units participat-
ing in sentiment and aspect contrasts are of higher
importance; this is a direction for future work.
8 Conclusion
In this paper, we addressed the task of automati-
cally assigning importance scores to parts of a lec-
ture that is to be summarized as part of an English
language proficiency test. We investigated the op-
timal units of information to which importance
should be assigned, as well as a variety of impor-
tance scoring models, drawing on the news sum-
marization and essay scoring literature.
We found that bigrams and trigrams were ge-
nerally more effective than unigrams and 4-grams
across importance models, with some exceptions.
We also found that the most effective impor-
tance models are those that equate importance
of an n-gram with its preferential use in higher-
scoring essays than in lower-scoring ones, above
and beyond merely looking at the n-grams used in
good essays. This demonstrates the utility of using
not only gold, high-quality human summaries, but
also sub-standard ones when developing content
importance models.
Additional importance criteria that are intrinsic
to the lecture, as well as those that capture contrast
with a different source discussing the same topic,
were also found to be reasonably effective. Since
different importance models often select different
items as most important, we intend to investigate
complementarity of the different models.
Finally, our results highlight that the effective-
ness of an importance model depends on the genre
of the source text. Thus, while a first sentence
baseline is very competitive in news summariza-
tion, we found that important information tends
not to be located in the opening sentences in our
data (these tend to provide general, introductory
information), but appears later on, when more de-
tailed, specific claims are put forward.
251
References
Yigal Attali and Jill Burstein. 2006. Automated Essay
Scoring With e-rater
R
?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Yigal Attali. 2011. A Differential Word Use Measure
for Content Analysis in Automated Essay Scoring.
ETS Research Report, RR-11-36.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 804?812, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jill Burstein, Michael Flor, Joel Tetreault, Nitin Mad-
nani, and Steven Holtzman. 2012. Examining Lin-
guistic Characteristics of Paraphrase in Test-Taker
Summaries. ETS Research Report, RR-12-18.
Keelan Evanini, Shasha Xie, and Klaus Zechner. 2013.
Prompt-based content scoring for automated spoken
language assessment. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 157?162, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Peter Foltz, Lynn Streeter, Karen Lochbaum, and
Thomas Landauer. 2013. Implementation and Ap-
plication of the Intelligent Essay Assessor. In Mark
Shermis and Jill Burstein, editors, Handbook of au-
tomated essay evaluation: Current applications and
new directions, pages 68?88. New York: Routh-
ledge.
Olga Gurevich and Paul Deane. 2007. Document
similarity measures to distinguish native vs. non-
native essay writers. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages
49?52, Rochester, New York, April. Association for
Computational Linguistics.
Kai Hong and Ani Nenkova. 2014. Improving
the estimation of word importance for news multi-
document summarization. In The Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Gottenberg, Sweden, April. As-
sociation for Computational Linguistics.
Hyun Duk Kim and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In Proceedings of the 18th ACM Confer-
ence on Information and Knowledge Management,
CIKM ?09, pages 385?394, New York, NY, USA.
ACM.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1630?1639, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings of
ACL workshop: Text summarization branches out,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
Quinn McNemar. 1955. Psychological Statistics. New
York: J. Wiley and Sons, 2nd edition.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Human Language Technologies
2004: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 145?152, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguis-
tics.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 66?76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lia Plakans and Atta Gebril. 2013. Using multiple
texts in an integrated writing assessment: Source
text use as a predictor of score. Journal of Second
Language Writing, 22:217?230.
Mark Shermis and Jill Burstein, editors. 2013. Hand-
book of Automated Essay Evaluation: Current Ap-
plications and Future Directions. New York: Rout-
ledge.
252
Transactions of the Association for Computational Linguistics, 1 (2013) 99?110. Action Editor: Chris Callison-Burch.
Submitted 12/2012; Published 5/2013. c?2013 Association for Computational Linguistics.
Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a
Subjectivity Lexicon for Essay Data
Beata Beigman Klebanov, Nitin Madnani, Jill Burstein
Educational Testing Service
660 Rosedale Road, Princeton, NJ 08541, USA
{bbeigmanklebanov,nmadnani,jburstein@ets.org}
Abstract
We demonstrate a method of improving a seed
sentiment lexicon developed on essay data by
using a pivot-based paraphrasing system for
lexical expansion coupled with sentiment pro-
file enrichment using crowdsourcing. Profile
enrichment alone yields up to 15% improve-
ment in the accuracy of the seed lexicon on 3-
way sentence-level sentiment polarity classifi-
cation of essay data. Using lexical expansion
in addition to sentiment profiles provides a
further 7% improvement in performance. Ad-
ditional experiments show that the proposed
method is also effective with other subjectivity
lexicons and in a different domain of applica-
tion (product reviews).
1 Introduction
In almost any sub-field of computational linguistics,
creation of working systems starts with an invest-
ment in manually-generated or manually-annotated
data for computational exploration. In subjectivity
and sentiment analysis, annotation of training and
testing data and construction of subjectivity lexicons
have been the loci of costly labor investment.
Many subjectivity lexicons are mentioned in the
literature. The two large manually-built lexicons
for English ? the General Inquirer (Stone et al,
1966) and the lexicon provided with the Opinion-
Finder distribution (Wiebe and Riloff, 2005) ? are
available for research and education only1 and un-
der GNU GPL license that disallows their incor-
poration into proprietary materials,2 respectively.
1http://www.wjh.harvard.edu/ inquirer/j1 1/manual/
2http://www.gnu.org/copyleft/gpl.html
Those wishing to integrate sentiment analysis into
products, along with those studying subjectivity in
languages other than English, or for specific do-
mains such as finance, or for particular genres
such as MySpace comments, reported construction
of lexicons (Taboada et al, 2011; Loughran and
McDonald, 2011; Thelwall et al, 2010; Rao and
Ravichandran, 2009; Jijkoun and Hofmann, 2009;
Pitel and Grefenstette, 2008; Mihalcea et al, 2007).
In this paper, we address the step of expanding
a small-scale, manually-built subjectivity lexicon (a
seed lexicon, typically for a domain or language
in question) into a much larger but noisier lexi-
con using an automatic procedure. We present
a novel expansion method using a state-of-the-art
paraphrasing system. The expansion yields a 4-fold
increase in lexicon size; yet, the expansion alone
is insufficient in order to improve performance on
sentence-level sentiment polarity classification.
In this paper we test the following hypothesis.
We suggest that the effectiveness of the expansion
is hampered by (1) introduction of opposite-polarity
items, such as introducing resolute as an expansion
of forceful, or remarkable as an expansion of pecu-
liar; (2) introduction of weakly polar, neutral, or am-
biguous words as expansions of polar seed words,
such as generating concern as an expansion of anx-
iety or future as an expansion of aftermath;3 (3) in-
ability to distinguish between stronger or clear-cut
versus weaker or ambiguous sentiment and to make
a differential use of those.
We address items (1) and (2) by enriching the lexi-
con with sentiment profiles (section 3), and propose
3Table 2 and Figure 1 provide support to these assessments.
99
a way of effectively utilizing this information for
the sentence-level sentiment polarity classification
task (sections 5 and 6). Profile-enrichment alone
yields up to 15% increase in performance for the
seed lexicon when using different machine learning
algorithms; paraphraser-based expansion with sen-
timent profiles improves performance by an addi-
tional 7%. Overall, we observe an improvement of
up to 25% in classification accuracy over the seed
lexicon without profiles.
In section 7, we present comparative evaluations,
demonstrating the competitiveness of the expanded
and profile-enriched lexicon, as well as the effective-
ness of the expansion and enrichment paradigm pre-
sented here for different subjectivity lexicons, dif-
ferent lexical expansion methods, and in a different
domain of application (product reviews).
2 Building Subjectivity Lexicons
The goal of our sentiment analysis project is to allow
for the identification of sentiment in sentences that
appear in essay responses to a variety of tasks de-
signed to test English proficiency in both native- and
non-native-speaker populations in a standardized as-
sessment as well as in an instructional settings. In
order to allow for the future use of the sentiment
analyzer in a proprietory product and to ensure its fit
to the test-taker essay domain, we began our work
with the construction of a seed lexicon relying on
our materials (section 2.1). We then used a statisti-
cal paraphrasing system to expand the seed lexicon
(section 2.2).
2.1 Seed Lexicon
In order to inform the process of lexicon construc-
tion, we randomly sampled 5,000 essays from a cor-
pus of about 100,000 essays containing writing sam-
ples across many topics. Essays were responses
to several different writing assignments, including
graduate school entrance exams, non-native English
speaker proficiency exams, and professional licen-
sure exams. Our seed lexicon is a combination of
(1) positive and negative sentiment words manually
selected from a full list of word types in these data,
and (2) words marked in a small-scale annotation of
a sample of sentences from these data for all posi-
tive and negative words. A more detailed descrip-
tion of the construction of seed lexicon can be found
in Beigman Klebanov et al(2012). The seed lexi-
con contains 749 single words, 406 positive and 343
negative.
2.2 Expanded Lexicon
We used a pivot-based lexical and phrasal para-
phrase generation system (Madnani and Dorr, 2013).
The paraphraser implements the pivot-based method
as described by Bannard and Callison-Burch (2005)
with several additional filtering mechanisms to in-
crease the precision of the extracted pairs. The
pivot-based method utilizes the inherent monolin-
gual semantic knowledge from bilingual corpora:
We first identify phrasal correspondences between
English and a given foreign language F , then map
from English to English by following translation
units from English to the other language and back.
For example, if the two English phrases e1 and e2
both correspond to the same foreign phrase f , then
they may be considered to be paraphrases of each
other with the following probability:
p(e1|e2) ? p(e1|f)p(f |e2)
If there are several pivot phrases that link the two
English phrases, then they are all used in computing
the probability:
p(e1|e2) ?
?
f ?
p(e1|f ?)p(f ?|e2)
Seed Expansion Seed Expansion
abuse exploitation costly onerous
accuse reproach dangerous unsafe
anxiety disquiet improve reinforce
conflict crisis invaluable precious
Table 1: Examples of paraphraser expansions.
Some examples of expansions generated by the
paraphraser are shown in Table 1. More details
about this kind of approach can be found in Ban-
nard and Callison-Burch (2005). We use the French-
English parallel corpus (approximately 1.2 million
sentences) from the corpus of European parliamen-
tary proceedings (Koehn, 2005) as the data on which
pivoting is performed to extract the paraphrases.
However, the base paraphrase system is susceptible
100
to large amounts of noise due to the imperfect bilin-
gual word alignments. Therefore, we implement ad-
ditional heuristics in order to minimize the num-
ber of noisy paraphrase pairs (Madnani and Dorr,
2013). For example, one such heuristic filters out
pairs where a function word may have been inferred
as a paraphrase of a content word. For the lexicon
expansion experiment reported here, we use the top
15 single-word paraphrases for every word from the
seed lexicon, excluding morphological variants of
the seed word. This process results in an expanded
lexicon of 2,994 different words, 1,666 positive and
1,761 negative (433 words are in both the positive
and the negative lists). The expanded lexicon in-
cludes the seed lexicon.
3 Inducing sentiment profiles
Let ?w be the sentiment profile of the word w.
?w = (pposw , pnegw , pneuw ) (1)
where ?i?{pos,neg,neu} piw = 1. Thus, a sentiment
profile of a word is essentially a 3-sided coin, cor-
responding to its probability of coming out positive,
negative, and neutral, respectively.
3.1 Estimating sentiment profiles
Our goal is to estimate the profile using outcomes of
multiple trials as follows. For every word, a person
is shown the word and asked whether it is positive,
negative, or neutral. A person?s decision is modeled
as flipping the coin corresponding to the word, and
recording the outcome ? positive, negative, or neu-
tral. We run N=20 such trials for every word in the
expanded lexicon using the CrowdFlower crowd-
sourcing site,4 for a total cost of $800. We use maxi-
mum likelihood estimate of sentiment profile:
p?iw = niw (2)
where niw is the proportion ofN trials on the wordw
that fell in cell i ? {pos, neg, neu}. Table 2 shows
some estimated profiles.
Following Goodman (1965) and Quesenberry and
Hurst (1964), we calculate confidence intervals for
the parameters piw:
(p?iw)? = (B + 2niw ? T )/(2(N +B)) (3)
4www.crowdflower.com
Word p?posw p?neuw p?negw
forceful 0 0.15 0.85
resolute 0.8 0.15 0.05
peculiar 0.05 0.15 0.8
remarkable 1 0 0
anxiety 0 0 1
concern 0.25 0.4 0.35
absurd 0 0 1
laughable 0.5 0.05 0.45
deadly 0 0 1
fateful 0.25 0.45 0.3
consequence 0.05 0.15 0.8
outcome 0.15 0.85 0
Table 2: Examples of estimated sentiment profiles.
Words in gray are expansions generated from words in
the preceding row; note the difference in the profiles.
(p?iw)+ = (B + 2niw + T )/(2(N +B)) (4)
where
T =
?
B[B + 4niw(N ? niw)/N ]) (5)
For confidence ? that all piw, i ? {pos, neg, neu}
are simultaneously within their respective intervals,
the value of B is determined as the upper?/3?100th
percentile of the ?2 distribution with one degree of
freedom. We use ?=0.1, resulting in B=4.55. The
resulting interval is about 0.2 around the estimated
value when p?iw is close to 0.5, and somewhat nar-
rower for p?iw closer to 0 or 1. We will use this infor-
mation when inducing features from the profiles.
3.2 Sentiment distributions of the lexicons
The estimated sentiment profiles per word allow us
to visualize the distributions of the two lexicons. In
Figure 1, we plot the number of entries in the lexi-
con as a function of the difference in positive and
negative parts of the profile, in 0.2-wide bins. Thus,
a word w would be in the second-leftmost bin if
?0.8 < (p?posw ? p?negw ) < ?0.6.
While the expansion process more than doubles
the number of words in the highest bins for both
the positive and the negative polarity, it clearly
introduces a large number of words in the low-
and medium bins into the lexicon. It is in this
sense that the expansion process is noisy; appa-
rently, seed words with clear and strong polarity
101
10
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
Figure 1: Sentiment distributions for the seed (left) and
the expanded (right) lexicons.
are often expanded into low intensity, neutral, or
ambiguous ones, as in pairs like absurd/laughable,
deadly/fateful, anxiety/concern shown in Table 2.
4 Related Work
The most popular seed expansion methods discussed
in the literature are based on WordNet (Miller,
1995) or another lexicographic resource, on dis-
tributional similarity with the seeds, or on a mix-
ture thereof (Cruz et al, 2011; Baccianella et al,
2010; Velikovich et al, 2010; Qiu et al, 2009; Mo-
hammad et al, 2009; Esuli and Sebastiani, 2006;
Kim and Hovy, 2004; Andreevskaia and Bergler,
2006; Hu and Liu, 2004; Kanayama and Nasukawa,
2006; Strapparava and Valitutti, 2004; Kamps et al,
2004; Takamura et al, 2005; Turney and Littman,
2003; Hatzivassiloglou and McKeown, 1997). The
paraphrase-based expansion method is in the dis-
tributional similarity camp; we also experimented
with WordNet-based expansion as descibed in sec-
tion 7.2.
The task of assigning sentiment profiles to words
in a sentiment lexicon has been addressed in the lite-
rature. SentiWordNet assigns profiles to all words in
WordNet based on a propagation algorithm from a
small seed set manually annotated by a small num-
ber of judges (Baccianella et al, 2010; Cerini et al,
2007). Andreevskaia and Bergler (2006) use graph
propagation algorithms on WordNet to assign cen-
trality scores in positive and negative categories; a
similar approach based on web-scale co-occurrence
graphs is discussed in Velikovich et al(2010). Thel-
wall et al(2010) manually annotated a set of words
for strength of sentiment and used machine learning
to fine-tune it. Taboada et al(2011) produced an
expert annotation of their lexicon with strength of
sentiment. Subasic and Huettner (2001) manually
built an affect lexicon with intensities. Wiebe and
Riloff (2005) classifed lexicon entries into weakly
and strongly subjective, based on their relative fre-
quency of appearance in subjective versus objective
contexts in a large annotated dataset.
Our sentiment profiles are best thought of as
relatively fine-grained priors for the sentiment ex-
pressed by a given word out-of-context. These re-
flect a mixture of strength of sentiment (p?posgood >
p?posdecent), contextual ambiguity (concern can be in-terpreted as similar to worry or to care, as in ?Her
condition was causing concern? versus ?He showed
genuine concern for her?), and dominance of a po-
lar connotation (abandon is p?neg=1; it has a negative
overtone even if the actual sense is not that of desert
but of vacate, as in ?You must abandon your office?).
To the best of our knowledge, this paper presents
the first attempt to integrate judgements obtained
through crowdsourcing on a large scale into a sen-
timent lexicon, showing the effectiveness of this
lexicon-enrichment procedure for a sentiment clas-
sification task.
5 Using profiles for sentence-level
sentiment polarity classification
To evaluate the usefulness of the lexicons, we use
them to generate features for machine learning sys-
tems, and compare performance on 3-way sentence-
level sentiment polarity classification. To ensure ro-
bustness of the observed trends, we experiment with
a number of machine learning algorithms: SVM
Linear and RBF, Na??ve Bayes, Logistic Regression
(using WEKA (Hall et al, 2009)), and c5.0 Decision
Trees (Quinlan, 1993).5
5.1 Data
We generated the data for training and testing the
machine learning systems as follows. We used our
5available from http://rulequest.com/
102
pool of 100,000 essays to sample a second, non-
overlapping set of 5,000 essays, so that no essay
used for lexicon development appears in this set.
From these essays, we randomly sampled 550 sen-
tences, and submitted them to sentiment polarity an-
notation by two experienced research assistants; 50
double-annotated sentenced showed ?=0.8. TEST
set contains the 43 agreed double-annotated sen-
tences, and additional 238 sampled from the 500
single-annotated sentences, 281 sentence in total.
The category distribution in the TEST set is 46.6%
neutral, 32.4% positive, and 21% negative.
The TRAIN set contains the remaining sentences,
plus positive, negative, and neutral sentences anno-
tated during lexicon development, for the total of
1,631 sentences. The category distribution in TRAIN
is 39% neutral, 35% positive, 26% negative.
5.2 From lexicons to features
Our goal is to evaluate the impact of sentiment pro-
files on sentence-level sentiment polarity classifica-
tion for the seed and the expanded lexicons, while
also looking for the most effective ways to represent
this information for machine learners.
We implement two baseline systems. One pro-
vides the machine learner with the most detailed in-
formation contained in a lexicon: BL-full has 2 fea-
tures for every lexicon word, taking the values (1,0)
for positive match in a sentence, (0,1) ? for negative,
(1,1) for a word in both positive and negative parts
of the lexicon, and (0,0) otherwise.
The second baseline provides the machine learner
with only summary information about the overall
sentiment of the sentence. BL-sum uses only 2 fea-
tures: (1) the total count of positive words in the
sentence; (2) the total count of negative words in the
sentence, according to the given lexicon.
For the sentiment-enriched runs, we construct a
number of representations: Int-full, Int-sum, Int-
bin, and Int-c. Int-full and Int-sum are parallel to
the respective baseline systems. Int-full represents
each lexicon word as 2 features corresponding to the
word?s estimated p?posw and p?negw , providing the most
detailed information to the machine learner. In the
Int-sum condition, we use p?posw and p?negw for every
word to induce 2 features: (1) the sum of positive
probabilities of all words in the sentence; (2) the
sum of negative probabilities for all words in the
sentence, according to the given lexicon.
For Int-bin runs, we use bins of the size of 0.2 ?
half of the maximal confidence interval ? to group
together words with close estimates. We produce
10 features. For positive bins, the 5 features count
the number of words in the sentence that fall in
bini, 1 ? i ? 5, respectively, that is, words with
0.2(i? 1) < p?posw ? 0.2i. Bin 1 also includes words
with p?posw = 0, since these cannot be distinguished
with high confidence from p?posw =0.1. Note that we
do not provide a scale, we merely represent different
ranges with different features. This should allow the
machine learners the flexibility to weight the diffe-
rent bins differently when inducing classifiers.
The Int-c condition represents a coarse-grained
setting. We produce 4 features, two for each pola-
rity: (1) the number of words such that 0 ? p?posw <
0.4; (2) the number of words such that 0.4 ? p?posw ?
1; similarity for the negative polarity.
Table 3 summarizes conditions and features.
Cond. #F Feature Description
BL-full 2|L| (1Lpos?S(w),1Lneg?S(w))
BL-sum 2 f1=|{w : w ? Lpos ? S}|
f2=|{w : w ? Lneg ? S}|
Int-full 2|L| (p?posw , p?negw ) ?w ? A
Int-sum 2 (?w?A p?posw , ?w?A p?negw )
Int-bin 10 f1=|{w ? A : 0 ? p?posw ? 0.2}|
...
f10=|{w ? A : 0.8 < p?negw ? 1}|
Int-c 4 f1=|{w ? A : 0 ? p?posw < 0.4}|
...
f4=|{w ? A : 0.4 ? p?negw ? 1}|
Table 3: Description of conditions. Column 2 shows the
number of features. In column 3: 1 is an indicator func-
tion; L is a lexicon; Lpos is the part of the lexicon con-
taining positive words (same with negatives); S is a sen-
tence for which a feature vector is built; A = L ? S. For
all w ? L ? S in the -full conditions, w is represented
with (0,0).
6 Results
Table 4 shows classification accuracies for 5 ma-
chine learning systems across 6 conditions, for the
seed and the expanded lexicons.
Let BL denote the best-performing baseline (BL-
103
Machine Condition Seed Expanded
Learner
? Majority 0.466 0.466
c5.0 BL-full 0.441 0.498
BL-sum 0.512 0.480
Int-full 0.441 0.498
Int-sum 0.566 0.616
Int-bin 0.587 0.641
Int-c 0.530 0.577
SVM BL-full 0.466 0.466
RBF BL-sum 0.527 0.495
Int-full 0.466 0.466
Int-sum 0.548 0.601
Int-bin 0.573 0.644
Int-c 0.530 0.562
SVM BL-full 0.584 0.566
Linear BL-sum 0.509 0.502
Int-full 0.580 0.609
Int-sum 0.601 0.580
Int-bin 0.573 0.630
Int-c 0.569 0.569
Logistic BL-full 0.545 0.509
Regression BL-sum 0.545 0.509
Int-full 0.534 0.502
Int-sum 0.555 0.584
Int-bin 0.584 0.616
Int-c 0.545 0.577
Na??ve BL-full 0.598 0.584
Bayes BL-sum 0.509 0.473
Int-full 0.598 0.580
Int-sum 0.545 0.605
Int-bin 0.559 0.626
Int-c 0.537 0.601
Table 4: Classification accuracies on TEST set. Majo-
rity baseline corresponds to classifying all sentences as
neutral. The best performance is boldfaced. Let BL
stand for the best-performing baseline (BL-full or BL-
sum) for a combination of machine learner and lexicon.
We use Wilcoxon Signed-Rank test, reporting the num-
ber of signed ranks (N) and the sum of signed ranks (W).
Statistically significant results at p=0.05 are: Int-sum >
BL (N=10, W=43); Int-bin > BL (N=10, W=48); Int-
bin > Int-sum (N=10, W=43); Int-bin > Int-full (N=10,
W=47); Int-sum > Int-full (N=10, W=37); Int-bin > Int-
c (N=10, W=55); Int-sum > Int-c (N=10, W=55); Ex-
panded> Seed under Int condition (includes Int-full, Int-
sum, Int-bin, Int-c) (N=18, W=152, z=3.3). Differences
between Int-full, Int-c, and BL are not significant.
full or BL-sum) for a combination of machine
learner and lexicon. The results show that (1) Int-
bin > Int-sum > BL = Int-c = Int-full; (2) Ex-
panded > Seed under Int condition. All inequalities
are statistically significant at p=0.05 (see caption of
Table 4 for details).
First, both the seed and the expanded lexicons
benefit from profile enrichment, although, as pre-
dicted, the expanded lexicon yields larger gains due
to its more varied profiles: The seed lexicon gains up
to 15% in accuracy (c5.0 BL-sum vs Int-bin), while
the expanded lexicon gains up to 30%, as SVM RBF
scores go up from 0.495 to 0.644.
Second, observe that profiling allows the ex-
panded lexicon to leverage its improved coverage:
While it is inferior to the best baseline run with the
seed lexicon for all systems, it succeeds in impro-
ving the seed lexicon accuracies by 5%-12% across
the different systems for the Int-bin runs. The best
run of the expanded lexicon (Int-bin for SVM RBF)
improves upon the best run of the seed lexicon (Int-
sum for SVM-linear) by 7%, demonstrating the suc-
cess of the paraphraser-based expansion once pro-
files are taken into account. Overall, comparing the
best baseline for the seed lexicon with Int-bin con-
dition of the expanded lexicon, we observe an im-
provement between 5% (0.598 to 0.626 for Na??ve
Bayes) and 25% (0.512 to 0.641 for c5.0), proving
the effectiviness of the paraphrase-based expansion
with profile enrichment paradigm.
Third, representing profiles using 10 bins (Int-bin)
provides a small but consistent improvement over
the summary representation (Int-sum) that sums
positivity and negativity of the sentiment-bearing
words in a sentence, over a coarse-grained represen-
tation (Int-c), as well as over the full-information
representation (Int-full). Even Na??ve Bayes and
SVM linear, known to work well with large feature
sets, show better performance in the Int-bin con-
dition for the expanded lexicon. The results indi-
cate that an intermediate degree of detail ? between
summary-only and coarse-grained representation on
the one hand and full-information representation on
the other ? is the best choice in our setting.
104
7 Comparative Evaluations
In this section, we present comparative evaluations
of the work presented in this paper with respect to
related work. This section shows that the paraphrase
expansion+profile enrichment solution proposed in
this paper is effective for our task beyond off-the-
shelf solutions, and that its effectiveness generalizes
to sentiment analysis in a different domain. We also
show that profile enrichment can be effectively cou-
pled with other methods of lexical expansion, al-
though the paraphraser-based expansion receives a
larger boost in performance from profile enrichment
than the alternative expansion methods we consider.
In section 7.1, we demonstrate that the
paraphrase-based expansion and profile enrich-
ment yield superior performance on our data
relative to state-of-art subjectivity lexicons ? Opin-
ionFinder, General Inquirer, and SentiWordNet.
In section 7.2, we show that profile enrichment
can be effectively coupled with other methods
of lexical expansion, such as a WordNet-based
expansion and an expansion that utilizes Lin?s
distributional thesaurus. However, we find that the
paraphraser-based expansion benefits the most from
profile enrichment, and attains better performance
on our data than the alterantive expansion methods.
In section 7.3, we show that the paraphrase-based
expansion and profile enrichment paradigm is
effective for other subjecitivy lexicons on other
data. We use a dataset of product reviews annotated
for sentence-level positivity and negativity as
new data for evaluation (Hu and Liu, 2004). We
use subsets of OpinionFinder, General Inquirer,
and sentiment lexicon from Hu and Liu (2004).
We demonstrate that paraphrase-based expansion
and profile enrichment improve the accuracy of
sentiment classification of product reviews for
every lexicon and machine learner combination; the
magnitude of improvement is 5% on average.
7.1 Competitiveness of the Expanded Lexicon
Had we been able to use the OpinionFinder or
the General Inquirer lexicons (OFL and GIL) as-
is, how would the results have compared to those
attained using our lexicons? We performed the
baseline runs with both lexicons; OFL accuracies
were 0.544-0.594 across machine learning systems,
GIL?s ? 0.491-0.584 (see GIL column in Table 5).
We also experimented with using the weaksubj
and strongsubj labels in OFL as somewhat parallel
distinctions to the ones presented here (see sec-
tion 4 ? Related Work ? for a more detailed discus-
sion). We used (1,0,0) profile for strong positives,
(0.3,0,0.7) for weak positives, (0,1,0) for strong neg-
atives, and (0,0.3,0.7) for weak negatives, and ran all
the feature representations discussed in section 5.2.
Table 5 column OFL shows the best run for every
machine learning system, across the different feature
representations, and choosing the better performing
run between vanilla OFL and the version enriched
with weak/strong distinctions.
Machine Seed OFL GIL SWN Exp.
Learner BL
c5.0 0.512 0.598 0.491 0.516 0.641
SVM-RBF 0.527 0.594 0.495 0.520 0.644
SVM-lin. 0.584 0.594 0.580 0.569 0.630
Log. Reg. 0.545 0.598 0.541 0.537 0.616
Na??ve B. 0.598 0.573 0.584 0.587 0.626
Table 5: Performance of different lexicons on essay data
using various machine learning systems. For each sys-
tem and lexicon, the best performance across the applica-
ble feature representations from section 5.2 and the vari-
ants (see text) is shown. Seed BL column shows the best
baseline performance of our seed lexicon ? before para-
phraser expansion and profile enrichment were applied.
Exp. column shows the performance of Int-bin feature
representation for the expanded lexicon after profile en-
richment.
Additionally, we experimented with SentiWord-
Net (Baccianella et al, 2010). SentiWordNet is a
resource for opinion mining built on top of Word-
Net, which assigns each synset in WordNet a score
triplet (positive, negative, and objective), indicating
the strength of each of these three properties for the
words in the synset. The SentiWordNet annotations
were automatically generated, starting with a set of
manually labeled synsets. Currently, SentiWordNet
includes an automatic annotation for all the synsets
in WordNet, totaling more than 100,000 words. It
is therefore the largest-scale lexicon with intensity
information that is currently available.
Since SentiWordNet assigns scores to synsets and
since our data is not sense-tagged, we induced Sen-
105
tiWordNet scores in the following ways. We part-
of-speech tagged our train and test data using Stan-
ford tagger (Toutanova et al, 2003). Then, we took
the SentiWordNet scores for the top sense for the
given part-of-speech (SWN-1). In a different vari-
ant, we took a weighted average of the scores for the
different senses, using the weighting algorithm pro-
vided on SentiWordNet website6 (SWN-2). Table 5
column SWN shows the best performance figures
between SWN-1 and SWN-2, across the feature rep-
resentations in section 5.2.
The comparative results in Table 5 clearly show
that while our vanilla seed lexicon performs com-
parably to off-the-shelf lexicons on our data, the
paraphraser-expanded lexicon with sentitment pro-
files outperforms OpinionFinder, General Inquirer,
and SentiWordNet.
7.2 Sentiment Profile Enrichment with Other
Lexical Expansion Methods
We presented a novel lexicon expansion method
using a paraphrasing system. We also experimented
with more standard methods, using WordNet and
distributional similarity (Beigman Klebanov et al,
2012; Esuli and Sebastiani, 2006; Kim and Hovy,
2004; Andreevskaia and Bergler, 2006; Hu and Liu,
2004; Kanayama and Nasukawa, 2006; Strapparava
and Valitutti, 2004; Kamps et al, 2004; Takamura
et al, 2005; Turney and Littman, 2003; Hatzivas-
siloglou and McKeown, 1997). Specifically, we im-
plemented a WordNet (Miller, 1995) based expan-
sion that uses the 3 most frequent synonyms of the
top sense of the seed word (WN-e). We also imple-
mented a method based on distributional similarity:
Using Lin?s proximity-based thesaurus (Lin, 1998)
trained on our in-house essay data as well as on well-
formed newswire texts, we took all words with the
proximity score > 1.80 to any of the seed lexicon
words (Lin-e). Just like the paraphraser lexicon,
both perform worse than the seed lexicon in 9 out
of 10 baseline runs (BL-sum and Bl-full conditions
for the 5 machine learners).
To test the effect of profile enrichment, all words
in WN-e and Lin-e underwent profile estimation as
described in section 3.1, yielding lexicons WN-e-p
and Lin-e-p, respectively. Figure 2 shows the distri-
6http://sentiwordnet.isti.cnr.it/, under ?Sample code.?
1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
Figure 2: Sentiment profile distributions for Lin-e-p (left)
and WN-e-p (right) lexicons.
butions. WN-e-p and Lin-e-p exhibit similar trends
to those of the paraphraser. Substituting WN-e-p
for Expanded data in Table 4, we find the same re-
lationships between the different feature sets: Int-
bin>Int-sum>Int-full=BL. For Lin-e-p, Int-sum de-
teriorates: Int-bin>Int-sum=Int-full=BL. For the
20 runs in the Int condition, Paraphraser>WN-e-
p>Lin-e-p.7 Note that this is also the order of lexi-
con sizes: Lin-e is the most conservative expan-
sion (1,907 words), WN-e is the second with 2,527
words, and the lexicon expanded using paraphrasing
is the largest with 2,994 words. Table 6 shows the
performance of Lin-e-p, WN-e-p, and of the Ex-
panded lexicon from Table 4 using the Int-bin fea-
ture representation. The average relative improve-
ments over the best baseline range between 6.6% to
14.6% for the different expansion methods.
Profile induction appears to be a powerful lexicon
clean-up procedure that works especially well with
more aggressive and thus potentially noisier expan-
sions: The machine learners depress low-intensity
and ambiguous expansions, thereby allowing the
effective utilization of the improved coverage of
sentiment-bearing vocabulary.
7.3 Effectiveness of the Paraphrase Expansion
with Profile Enrichment Paradigm in a
Different Domain
In order to check whether the paraphrase-based ex-
pasion and profile enrichment paradigm discussed in
this paper generalizes to other subjectivity lexicons
7All > are signficant at p=0.05 using Wilcoxon test.
106
Machine Seed Lin-e-p WN-e-p Exp.
Learner BL
c5.0 0.512 0.584 0.616 0.641
SVM-RBF 0.527 0.598 0.601 0.644
SVM-lin. 0.584 0.577 0.569 0.630
Log. Reg. 0.545 0.587 0.580 0.616
Na??ve B. 0.598 0.591 0.623 0.626
Av. Gain 0.066 0.085 0.146
Table 6: Performance of WordNet-based, Lin-based, and
Paraphraser-based expansions with profile enrichment in
the Int-bin condition. Seed BL column shows the best
baseline performance of the seed lexicon ? before expan-
sion and profile enrichment were applied. The last line
shows the average relative gain over the best baseline
calculated as AGlex = ?m?M Lexm?SeedBLmSeedBLm , where
M = {c5.0, SVM-RBF, SVM-linear, Logistic Regres-
sion, Na??ve Bayes}, and lex ? {Lin-e-p, Wn-e-p, Exp}.
and domains of application, we experimented with
a product reviews dataset (Hu and Liu, 2004) and
additional lexicons as follows.
7.3.1 Lexicons
We use the OpinionFinder and General Inquirer
lexicons (OFL and GIL) as before, as well as
the lexicon of positive and negative sentiment and
opinion words available along with (Hu and Liu,
2004) product reviews dataset ? HL.8
Since each of these lexicons contains more than
3,000 words, enrichment of the full lexicons with
profiles is beyond the financial scope of our project.
We therefore restrict each of the lexicons to the size
of their overlap with our seed lexicon (see 2.1); the
overlaps have between 415 and 467 words. These re-
stricted lexicons are our initial lexicons for the new
experiment that parallel the role of the seed lexicon
in the experiments on essay data.
For each of the 3 initial lexicons L, L?{OFL,
GIL, HL}, we follow the paraphrase-based expan-
sion as described in section 2.2. This results in about
4.5-fold expansion of each lexicon, the new lexi-
cons L-e, L?{OFL, GIL, HL}, numbering between
2,015 and 2,167 words. Both the initial and the ex-
panded lexicons now undergo profile enrichment as
described in section 3.1, producing lexicons L-p and
8http://www.cs.uic.edu/?liub/FBS/sentiment-
analysis.html#lexicon
L-e-p, L?{OFL, GIL, HL}.
7.3.2 Data
We use the dataset from Hu and Liu (2004)9 that
contains reviews of 5 products from amazon.com:
two digital cameras, a DVD player, an MP3 player,
and a cellular phone. The reviews are annotated at
sentence level with a label that desrcibes the par-
ticular feature that is the subject of the positive or
negative evaluation and the polarity and extent of
the evaluation. For example, the sentence ?The
phone book is very user-friendly and the speaker-
phone is excellent? is labeled as PHONE BOOK[+2],
SPEAKERPHONE[+2], while the sentence ?I am
bored with the silver look? is labeled LOOK[?1]. We
used all sentences that were labeled with a numeri-
cal score for at least one feature, removing a small
number of sentences labeled with both positive and
negative scores for different features.10 We used the
sign of the numerical score to label the sentences as
positive or negative. The resulting dataset consists
of 1,695 sentences, 1,061 positive and 634 nega-
tive; accuracy for a majority baseline on this dataset
is 0.626. Our experiments on this dataset are done
using 5-fold cross-validation.
7.3.3 Results
Table 7 shows classification accuracies for the
product review data using different lexicons and ma-
chine learners. We observe that the combination of
paraphrase-based expansion and profile enrichment
(L-e-p column in the table) resulted in an improved
performance over the initial lexicon (L column in
the table) in all cases, with the average gain of 5%
in accuracy.
Furthermore, the contributions of the expansion
and the profile enrichment are complementary, since
their combination performs better than each in iso-
lation. We note that profile enrichment alone for the
initial lexicon did not yield an improvement. This
can be explained by the fact that the initial lexicons
are highly polar, so profiles provide little additional
information: The percentage of words with p?pos ?
0.8 or p?neg ? 0.8 is 84%, 86% and 91% for GIL,
9http://www.cs.uic.edu/?liub/FBS/sentiment-
analysis.html#datasets, the link under ?Customer Review
Datasets (5 products)?
10such as ?The headset that comes with the phone has good
sound volume but it hurts the ears like you cannot imagine!?
107
Machine Lexicon Variant
Learner L L-p L-e L-e-p
L = OFL?Seed, |L|=467, |L-e|=2,167
c5.0 0.663 0.670 0.691 0.704
SVM-RBF 0.668 0.676 0.693 0.714
SVM-lin. 0.675 0.670 0.688 0.696
Log. Reg. 0.666 0.658 0.693 0.698
Na??ve B. 0.668 0.668 0.686 0.695
L = GIL?Seed, |L|=415,|L-e|=2,015
c5.0 0.644 0.658 0.663 0.686
SVM-RBF 0.650 0.665 0.653 0.683
SVM-lin. 0.665 0.665 0.677 0.681
Log. Reg. 0.664 0.658 0.678 0.694
Na??ve B. 0.669 0.666 0.678 0.703
L = HL?Seed, |L|=434, |L-e|=2,054
c5.0 0.676 0.675 0.689 0.706
SVM-RBF 0.673 0.674 0.700 0.713
SVM-lin. 0.676 0.664 0.703 0.710
Log. Reg. 0.668 0.661 0.703 0.699
Na??ve B. 0.668 0.672 0.697 0.697
Table 7: Accuracies on product review data. For each ma-
chine learner and lexicon, the best baseline performance
is shown as L for the initial lexicon and as L-e for the
paraphrase-expanded lexicon. L-p and L-e-p show the
performance of Int-bin feature set on the profile-enriched
initial and paraphrase-expanded lexicons, respectively.
The three initial lexicons L are OpinionFinder (OFL),
General Inquirer (GIL), and (Hu and Liu, 2004) (HL),
each intersected with our seed lexicon. Sizes of the intial
and expanded lexicons are provided.
OFL, and HL-derived lexicons, respectively. In con-
trast, for the expanded lexicons, these percentages
are 51%, 53%, and 56%; these lexicons benefit from
profile enrichment.
8 Conclusions
We demonstrated a method of improving a seed sen-
timent lexicon by using a pivot-based paraphrasing
system for lexical expansion and sentiment profile
enrichment using crowdsourcing. Profile enrich-
ment alone yielded up to 15% improvement in the
performance of the seed lexicon on the task of 3-
way sentence-level sentiment polarity classification
of test-taker essay data. While the lexical expansion
on its own failed to improve upon the performance
of the seed lexicon, it became much more effective
on top of sentiment profiles, generating a 7% perfor-
mance boost over the best profile-enriched run with
the seed lexicon. Overall, paraphrase-based expan-
sion coupled with profile enrichment yields an up to
25% improvement in accuracy.
Additionally, we showed that our paraphrase-
expanded and profile-enriched lexicon performs
significantly better on our data than off-the-shelf
subjectivity lexicons, namely, Opinion Finder, Gen-
eral Inquirer, and SentiWordNet. Furthermore, our
results suggest that paraphrase-based expansion de-
rives more benefit from profiles than two competing
expansion mechanisms based on WordNet and on
Lin?s distributional thesaurus.
Finally, we demonstrated the effectiveness of the
paraphraser-based expansion with profile enrich-
ment paradigm on a different dataset. We used Hu
and Liu (2004) product review data with sentence-
level sentiment polarity labels. Paraphrase-based
expansion with profile enrichment yielded an im-
proved performance across all lexicons and machine
learning algorithms we tried, with an average im-
provement rate of 5% in classification accuracy.
Recent literature argues that sentiment polarity
is a property of word senses, rather than of words
(Gyamfi et al, 2009; Su and Markert, 2008; Wiebe
and Mihalcea, 2006), although Dragut et al(2012)
successfully operate with ?mostly negative? and
?mostly positive? words based on the polarity distri-
butions of word senses. We plan to address in future
work sense disambiguation for words that have mul-
tiple senses with very different sentiment, such as
stress, as either anxiety (negative) or emphasis (neu-
tral).
References
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for a fuzzy sentiment: Sentiment tag extrac-
tion of WordNet glosses. In Proceedings of EACL,
pages 209?216, Trento, Italy.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SENTIWORDNET 3.0: An Enhanced
Lexical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of LREC, pages 2200?2204,
Malta.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, pages 597?604, Ann Arbor, MI.
108
Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,
Adam Faulkner, and Joel Tetreault. 2012. Build-
ing sentiment lexicon(s) from scratch for essay data.
In Proceedings of the 13th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing), New Delhi, India, March.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lexi-
cal resources for opinion mining. In Andrea Sanso,
editor, Language resources and linguistic theory: Ty-
pology, second language acquisition, pages 200?210.
Franco Angeli Editore, Milano, IT.
Ferm??n L. Cruz, Jose? A. Troyano, F. Javier Ortega, and
Fernando Enr??quez. 2011. Automatic expansion
of feature-level opinion lexicons. In Proceedings of
the 2nd Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis, pages 125?131,
Portland, Oregon, June.
Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla,
and Weiyi Meng. 2012. Polarity consistency check-
ing for sentiment dictionaries. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
997?1005, Jeju Island, Korea, July. Association for
Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Determin-
ing term subjectivity and term orientation for opinion
mining. In Proceedings of EACL, pages 193?200,
Trento, Italy.
Leo A. Goodman. 1965. On Simultaneous Confidence
Intervals for Multinomial Proportions. Technometrics,
7(2):247?254.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectivity
sense labeling. In Proceedings of NAACL, pages 10?
18, Boulder, CO.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174?181, Madrid,
Spain.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177,
Seattle, WA.
Valentin Jijkoun and Katja Hofmann. 2009. Gener-
ating a Non-English Subjectivity Lexicon: Relations
That Matter. In Proceedings of EACL, pages 398?405,
Athens, Greece.
Jaap Kamps, Maarten Marx, Robert Mokken, and
Maarten de Rijke. 2004. Using WordNet to measure
semantic orientation of adjectives. In Proceedings of
LREC, pages 1115?1118, Lisbon, Portugal.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic Lexicon Expansion for Domain-oriented
Sentiment Analysis. In Proceedings of EMNLP, pages
355?363, Syndey, Australia.
Soo-Min Kim and Edward Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING,
pages 1367?1373, Geneva, Switzerland.
Philip Koehn. 2005. EUROPARL: A Parallel corpus for
Statistical Machine Translation. In Proceedings of the
Machine Translation Summit.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of ACL, pages 768?774,
Montreal, Canada.
Tim Loughran and Bill McDonald. 2011. When is a Li-
ability not a Liability? Textual Analysis, Dictionaries,
and 10-Ks. Journal of Finance, 66:35?65.
Nitin Madnani and Bonnie Dorr. 2013. Generating Tar-
geted Paraphrases for Improved Translation. ACM
Transactions on Intelligent Systems and Technology, to
appear.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of ACL, pages
976?983, Prague, Czech Republic.
George Miller. 1995. WordNet: A lexical database.
Communications of the ACM, 38:39?41.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus. In
Proceedings of EMNLP, pages 599?608, Singapore,
August.
Guillaume Pitel and Gregory Grefenstette. 2008. Semi-
automatic building method for a multidimensional af-
fect dictionary for a new language. In Proceedings of
LREC, Marrakech, Morocco.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI?09,
pages 1199?1204.
C. Quesenberry and D. Hurst. 1964. Large sample si-
multaneous confidence intervals for multinomial pro-
portions. Technometrics, 6:191?195.
J. R. Quinlan. 1993. C4.5: Programs for machine lear-
ning. Morgan Kaufmann Publishers.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of EACL, pages 675?682, Athens.
109
Philip Stone, Dexter Dunphy, Marshall Smith, and Daniel
Ogilvie. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-affect: an affective extension of WordNet.
In Proceedings of LREC, pages 1083?1086, Lisbon,
Portugal.
Fangzhong Su and Katja Markert. 2008. Eliciting
Subjectivity and Polarity Judgements on Word Senses.
In Proceedings of COLING, pages 825?832, Manch-
ester, UK.
P. Subasic and A. Huettner. 2001. Affect analysis of text
using fuzzy semantic typing. IEEE Transactions on
Fuzzy Systems, 9(4).
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-Based
Method for Sentiment Analysis. Computational Lin-
guistics, 37(2):267?307.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words using
spin model. In Proceedings of ACL, pages 133?140,
Ann Arbor, MI.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
detection in short informal text. Journal of the Amer-
ican Society for Information Science and Technology,
61(12):2544?2558.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL, pages 252?259.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21(4):315346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of
Web-derived polarity lexicons. In Proceedings of
NAACL, pages 777?785, Los Angeles, CA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of ACL, pages 1065?
1072, Sydney, Australia.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLING (invited pa-
per), pages 486?497, Mexico City.
110
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 529?535,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ETS: Discriminative Edit Models for Paraphrase Scoring
Michael Heilman and Nitin Madnani
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,nmadnani}@ets.org
Abstract
Many problems in natural language process-
ing can be viewed as variations of the task of
measuring the semantic textual similarity be-
tween short texts. However, many systems
that address these tasks focus on a single task
and may or may not generalize well. In this
work, we extend an existing machine transla-
tion metric, TERp (Snover et al, 2009a), by
adding support for more detailed feature types
and by implementing a discriminative learning
algorithm. These additions facilitate applica-
tions of our system, called PERP, to similar-
ity tasks other than machine translation eval-
uation, such as paraphrase recognition. In
the SemEval 2012 Semantic Textual Similar-
ity task, PERP performed competitively, par-
ticularly at the two surprise subtasks revealed
shortly before the submission deadline.
1 Introduction
Techniques for measuring the similarity of two sen-
tences have various potential applications: auto-
mated short answer scoring (Nielsen et al, 2008;
Leacock and Chodorow, 2003), question answering
(Wang et al, 2007), machine translation evaluation
(Przybocki et al, 2009; Snover et al, 2009a), etc.
An important aspect of this problem is that sim-
ilarity is not binary. Sentences can be very seman-
tically similar, such that they might be called para-
phrases of each other. They might be completely
different. Or, they might be somewhere in between.
Indeed, it is arguable that all sentence pairs (except
exact duplicates) lie somewhere on a continuum of
similarity. Therefore, it is desirable to develop meth-
ods that model sentence pair similarity on a contin-
uous, or at least ordinal, scale.
In this paper, we describe a system for measuring
the semantic similarity of pairs of short texts. As a
starting point, we use the Translation Error Rate Plus
(Snover et al, 2009a), or TERp, system, which was
specifically developed for machine translation eval-
uation. TERp takes two sentences as input, finds a
set of weighted edits that convert one into the other
with low overall weight, and then produces a length-
normalized score. TERp also has a greedy, heuris-
tic learning algorithm for inducing weights from la-
beled sentence pairs in order to increase correlations
with human similarity scores.
Some features of the original TERp make adap-
tation to other semantic similarity tasks difficult, in-
cluding its largely one-to-one mapping of features
to edits and its heuristic, greedy learning algorithm.
For example, there is a single feature for lexical sub-
stitution, even though it is clear that different types
of substitutions have different effects on similarity
(e.g., substituting ?43.6? with ?17? versus substitut-
ing ?a? for ?an?). In addition, the heuristic learn-
ing algorithm, which involves perturbing the weight
vector by small amounts as in grid search, seems un-
scalable to larger sets of overlapping features.
Therefore, here, we use TERp?s inference algo-
rithms that find low cost edit sequences but use a dis-
criminative learning algorithm based on the Percep-
tron (Rosenblatt, 1958; Collins, 2002) to estimate
edit cost parameters, along with an expanded fea-
ture set for broader coverage of the phenomena that
are relevant to sentence-to-sentence similarity. We
529
refer to this new approach as Paraphrase Edit Rate
with the Perceptron (PERP).
In addition to describing PERP, we discuss how it
was applied for the SemEval 2012 Semantic Textual
Similarity (STS) task.
2 Problem Definition
In this work, our goal is to create a system that can
take as input two sentences (or short texts) x1 and x2
and produce as output a prediction y? for how simi-
lar they are. Here, we use the 0 to 5 ordinal scale
from the STS task, where increasing values indicate
greater semantic similarity.
The STS task data includes five subtasks with text
pairs from different sources: the Microsoft Research
Paraphrase Corpus (Dolan et al, 2004) (MSRpar),
The Microsoft Research Video corpus (Chen and
Dolan, 2011) (MSRvid), statistical machine transla-
tion output of parliament proceedings (Koehn, 2005)
(SMT-eur). For each of these sources, approxi-
mately 750 sentence pairs x1 and x2 and gold stan-
dard similarity values y were provided for training
and development.
In addition, there were two surprise data sources
revealed shortly before the submission deadline:
pairs of sentences from Ontonotes (Pradhan and
Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN),
and machine translations of sentences from news
conversations (SMT-news). For all five sources,
the held-out test set contained several hundred text
pairs. See the task description (Agirre et al, 2012)
for additional details.
3 TER, TERp, and PERP
In this section, we briefly describe the TER and
TERp machine translation metrics, and how the
PERP system extends them in order to better model
semantic textual similarity.
TER (Snover et al, 2006) uses a greedy search al-
gorithm to find a set of edits to convert one of the
paired input sentences into the other. We can view
this set of edits as an alignment a between the two
input sentences x1 and x2, and when two words in
x1 and x2, respectively, are part of an edit operation,
we say that those words are aligned.1 Unlike tradi-
1For machine translation evaluation with TERp and PERP,
x1 is a system?s hypothesis and x2 is a reference translation. For
tional edit distance measures, TER allow for shifts?
that is, edits that change the positions of words or
phrases in the input sentence x1. Essentially, TER
searches among a set of possible shifts of the phrases
in x1 to find a set of shifts that result in the least
cost alignment, using edits of other types, between
x2 and the shifted version of x1. TER allows one to
specify costs for different edit types, but it does not
include a method for learning those costs from data.
TERp (Snover et al, 2009b; Snover et al, 2009a)
extends TER in two key ways. First, TERp in-
cludes new types of edits, including edits for substi-
tution of synonyms, word stems, and phrasal para-
phrases extracted from a pivot-based paraphrase ta-
ble (?3.1). Second, it includes a heuristic learning
algorithm for inferring cost parameters from labeled
data. TERp includes 8 types of edits: match (M), in-
sertion (I), deletion (D), substitution (S), stemming
(T), synonymy (Y), shift (Sh), and phrase substitu-
tion (P). The edits are mutually exclusive, such that
synonymy edits do not count as substitutions, for ex-
ample. TERp has 11 total parameters, with a single
parameter for each edit except for phrase substition,
which has four.
PERP has a general framework similar to that
of TERp. It extends TERp, however, by includ-
ing additional edit parameters, and by using a dis-
criminative learning algorithm (see ?5) to learn pa-
rameters rather than the heuristic technique used by
TERp. Thus, PERP uses the same greedy algorithm
as TERp for finding the optimal sets of edits given
the cost parameters, but it allows the cost for an indi-
vidual edit to depend on multiple, overlapping fea-
tures of that edit. For example, costs for substitu-
tion edits depend on whether the aligned words are
pronouns, whether the aligned words represent num-
bers, the lengths of the aligned words, etc. See ?4 for
the full list of features in PERP.
An alignment from the MSRpar portion of the
STS training data is illustrated in Figure 1.
3.1 Phrasal Paraphrases
PERP uses probabilistic phrasal substitutions to
align phrases in the hypothesis with phrases in the
all STS subtasks, we assigned sentences in the first and second
columns of the input files to x2 and x1, respectively, so that
the hypotheses and references in the SMT-eur subtask would be
assigned appropriately.
530
the research firm earlier had forecast an increase of 4.9 percent .
the firm earlier had predicted increase this year a 4.9 percent .
the firm had predicted earlier this year a 4.9 percent increase .
synonymy
shift shift insert
delete delete delete
insertinsert
x1
x2
Figure 1: An example of a PERP alignment for a sentence pair from the Microsoft Research Paraphrase Corpus.
The search algorithm first performs shifts on x1 and then performs other edits on x2. The zero cost edits that match
individual words are not shown.
reference. It does so by looking up?in a pre-
computed phrase table?paraphrases of phrases in
the reference and using its associated edit cost as
the cost of performing a match against the hypoth-
esis. The paraphrase table used in PERP was iden-
tical to the one used by Snover et al (2009a). It
was extracted using the pivot-based method as de-
scribed by Bannard and Callison-Burch (2005) with
several additional filtering mechanisms to increase
the precision of the extracted pairs. The pivot-based
method utilizes the inherent monolingual semantic
knowledge from bilingual corpora: we first iden-
tify phrasal correspondences between English and a
given foreign language F , then map from English to
English by following translation units from English
to the other language and back. For example, if the
two English phrases e1 and e2 both correspond to
the same foreign phrase f , then they may be consid-
ered to be paraphrases of each other with the follow-
ing probability:
p(e1|e2) ? p(e1|f)p(f |e2)
If there are several pivot phrases that link the two
English phrases, then they are all used in computing
the probability:
p(e1|e2) ?
?
f ?
p(e1|f ?)p(f ?|e2)
We used the same phrasal paraphrase database as
in TERp (Snover et al, 2009a), which was extracted
from an Arabic-English newswire bitext containing
a million sentences. A few examples of the para-
phrase pairs used in the MSRpar portion of the STS
training data are shown below:
(commission? panel)
(the spying? espionage)
(suffered? underwent)
(room to? space for)
(per cent? percent)
4 Features
As discussed in ?3, PERP expands on TERp?s origi-
nal features in order to better model semantic textual
similarity.
PERP models a pair of sentences x1 and x2 us-
ing a feature function f(a) that extracts a vector of
real-valued features from an alignment a between
x1 and x2. This alignment is found with TERp?s
inference algorithm and consists of a set of edits
of various types along with information about the
words on which those edits operate. For example,
the alignment might contain an edit with the infor-
mation, ?The token ?the? in x1 was substituted for
the token ?an? in x2.? This edit would increment the
features in f(a) for the number of substitutions and
the number of substitutions of stopwords, along with
other relevant substitution features.
The set of features encoded in f(a) are described
in Table 1.2 It includes general features that always
fire for edits of a particular type (e.g., the ?Substi-
tution? feature) as well as specific features that fire
only in specific situations (e.g., the ?Sub-Pronoun-
Both? edit, which fires only when one pronoun is
substituted for another).
The function f(a) is normalized for sentence
2All words were converted to lower-case. Word frequen-
cies were calculated from the NYT stories in the fifth edition
of the English Gigaword corpus. The stories were tokenized
using NLTK and words occurring fewer than 100 times were
excluded. Words occurring at least 100 times constituted the vo-
cabulary used for computing the OOV features. The OOV and
frequency features only fired for words that consisted only of
letters, and the frequency features did not fire for OOV words.
The set of negation words including the following: ?no?, ?not?,
?never?, and ?n?t?. The stopword list contained 158 common
words and punctuation symbols.
531
Edits Feature Name Description
- Intercept Always 1 (and not normalized by text lengths)
T Stemming The number of times that two words with the same stem, according to the Porter
(1980) stemmer, were aligned.
Y Synonymy The number of times that a pair of synonyms, according to WordNet (Fellbaum,
1998), were aligned.
Sh Shift The number of shifts.
P Paraphrase1 The number of phrasal paraphrasing operations.
P Paraphrase2 The sum of q log10(p), where p is the probability in the pivot-based paraphrase table
for a paraphrase edit and q is the number of edits for that paraphrase edit. See Snover
et al (2009a) for further explanation.
P Paraphrase3 The sum of pq, where p and q are as above.
P Paraphrase4 The sum of q, where q is as above.
I Insertion The number of insertions.
D Deletion The number of deletions.
I, D Insert-Delete-
LogFreq
The sum of log10 freq(w) over all insertions and deletions, where w is the word
being inserted or deleted and freq(w) is the relative frequency of w.
I, D Insert-Delete-
LogWordLen
The sum of log10 length(w) over all insertions and deletions, where w is the word
being inserted or deleted.
I, D Insert-Delete-
X
The number of insertions and deletions of X in alignment, where X is: (a) punctu-
ation, (b) numbers, (c) personal pronouns, (d) negation words, (e) stop words, or (f)
out-of-vocabulary (OOV) words (6 features in all).
S Substitution The number of substitutions.
S Sub-X-Both The number of substitutions where both words are: (a) punctuation, (b) numbers, (c)
personal pronouns, (d) negation words, (e) stop words, or (f) OOV words (6 features
in all).
S Sub-X-1only The number of substitutions where only one word is: (a) punctuation, (b) a number,
(c) a personal pronoun, (d) a negation word, (e) a stop word, or (f) an OOV word (6
features in all).
S Sub-LogFreq-
Diff
The sum of | log10 freq(w1)? log10 freq(w2)| over all substitutions.
S Sub-Contain The number of substitutions where both words have more than 5 characters and one
is a proper substring of the other.
S Sub-Diff-By-
NonWord
The number of substitutions where the words differ only by non-alphanumeric char-
acters.
S Sub-Small-
LevDist
The number of substitutions where both words have more than 5 characters and the
Levenshtein distance between them is 1.
S Sub-Norm-
LevDist
The sum of the following over all substitutions: the Levenshtein distance between
the words normalized by the length of the longer word.
Table 1: The set of features in PERP. The first column lists which edits for which each feature is relevant.
lengths by dividing all the values in Table 1 by the
sum of the number of words in x1 and x2, except for
the intercept feature that models the base similarity
value in the training data and always has value 1.
There are 36 features and corresponding parame-
ters in all, compared to 11 for TERp.
It is worth pointing out that while the mutual ex-
clusivity between most of the original TERp edits
is preserved, PERP does have shared features be-
tween insert and delete edits (e.g., ?Insert-Delete-
Number?), and could in principle share features be-
tween substitution, stemming, and synonymy edits.
5 Learning
Given a training set consisting of paired sentences
x1 and x2 and gold standard semantic similarity rat-
ings y, PERP uses Algorithm 1 to induce a good set
532
Algorithm 1 learn(w, T , ?, x1,x2,y):
An Averaged Perceptron algorithm for learning edit
cost parameters. T is the number of iterations
through the dataset. ? is a learning rate. x1 and
x2 are paired lists of sentences, and y is a list of
similarities that correspond to those sentence pairs.
wsum = 0
for t = 1, 2, . . . , T do
x1,x2,y = shuffle(x1,x2,y)
for i = 1, 2, . . . , |y| do
a = TERpAlign(w, x1i, x2i)
y? = w ? f(a)
w = w + ?(yi ? y?)f(a)
w = applyShiftConstraint(w)
wsum = wsum + w
end for
end for
return wsumT |y|
of cost parameters for its various features.3 The al-
gorithm is a fairly straightforward application of the
Perceptron algorithm described by Collins (2002).4
The only notable difference is that the algorithm
constrains PERP?s shift parameter to be at least 0.01
in the step labeled ?applyShiftConstraint.? We found
that TERp?s inference algorithm would fail if the
shift cost reached zero.5 In our experiments, we ini-
tialized all weights to 0, except for the following: the
?Substitution,? ?Insertion,? and ?Deletion? weights
were initialized to 1.0, and the ?Shift? weight was
initialized to 0.1. Following Collins (2002), the al-
gorithm returns an averaged version of the weights,
though this did not appear to substantially impact
performance.
3The ?shuffle? step shuffles the lists of sentence pairs and
scores together such that their orderings are randomized but that
they stay aligned with each other.
4There are a few hyperparameters in the learning algorithms.
For our experiments, we set the number of iterations through
the training data T to 200. We set the learning rate ? to 0.01 to
avoid large oscillations in the parameters. We did not system-
atically tune the hyperparameters. Other values might lead to
better performance.
5With zero cost shifts, TERp would enter a loop and even-
tually exceed the amount of available memory. We also set the
same minimum cost of 0.01 for shifts in our experiments with
the original TERp.
6 Experiments
In this section, we report results for the STS shared
task. For a full description of the task, see Agirre et
al. (2012).
The task consisted of three known subtasks
(MSRpar, MSRvid, and SMT-eur) and two surprise
subtasks (On-WN, SMT-news). For the known sub-
tasks, we trained models with task-specific data
only. For the On-WN subtask, we used the model
trained for MSRpar. For SMT-news, we used the
model trained for SMT-eur.
Our submissions to the task included results from
two variations, one using the full system (PERP-
phrases) and one with the paraphrase substitution
edits disabled (PERP), in order to isolate the effect
of including phrasal paraphrases. In our original
submission, the PERPphrases system included a mi-
nor bug that affected the calculation of the phrasal
paraphrasing features. Here, we report both the orig-
inal results and a corrected version (?PERPphrases
(fix)?), though the correction only minimally af-
fected performance. We also tested two variations
of the original TERp system: one with the weights
set as reported by Snover et al (2009a) (?TERp
(default)?), and one tuned in the same task-specific
manner as PERP (?TERp (tuned)?). We multiplied
TERp?s predictions by ?1 since it produces costs
rather than similarities.
The results, in terms of Pearson correlations with
test set gold standard scores, are shown in Table 2.
In addition to correlations for each subtask, we in-
clude the three aggregated measures used for the
task. The ?ALL? measure is the Pearson correlations
on the concatenation of all the data for all five sub-
tasks. It was the original measured used to aggregate
the results for the different subtasks. The second ag-
gregated measure is the ?Allnrm? measure, which
we view as an oracle because it uses the gold stan-
dard similarity values from the test set to adjust sys-
tem predictions. The final aggregate measure is the
mean of the correlations for the subtasks, weighted
by the number of examples in each subtask?s test set
(?Mean?). See Agirre et al (2012) for a full descrip-
tion of the metrics.
For comparison, the table also includes the re-
sults from the top-ranked submission according to
the ?ALL? measure, the results for the word-overlap
533
Aggregated Measures Subtask Measures
ALL ALLnrm Mean MSRpar MSRvid SMT-eur On-WN SMT-news
UKP (top-ranked) .8239 .8579 .6773 .6830 .8739 .5280 .6641 .4937
PERPphrases (fix) ? .7837 ? .6405 .6410 .7209 .4852 .7127 .5312
PERPphrases .7834 .8089 .6399 .6397 .7200 .4850 .7124 .5312
PERP .7808 .8064 .6305 .6211 .7210 .4722 .7080 .5149
TERp (tuned) ? .5558 ? .5582 .5400 .6099 .4967 .5862 .5135
TERp (default) .4477 .7291 .5253 .5049 .5217 .4748 .6169 .4566
baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .3908
mean of submissions .5864 .7773 .5286 .4894 .7049 .3958 .5557 .3731
Table 2: Pearson correlations between predictions about the test data and gold standard scores. ??? marks experiments
that were not parts of the official SemEval task 6 evaluation. The highest correlation in each column is given in bold.
ALLnrm results are not included for all runs because we did not have an implementation of that measure.
baseline from the organizers (Agirre et al, 2012),
and the means across all 88 submissions (not includ-
ing the baseline).
Table 3 shows the rankings in the official results
of the PERPphrases submission, for each subtask
and overall, along with Pearson correlations from
PERP and the best submission for each subtask.
Aggregated Measure Rank ? ?best
ALL 6 .7834 .8239
ALLnrm 27 .8089 .8635
Mean 7 .6399 .6773
Subtask Measure Rank ? ?best
MSRpar 8 .6397 .7343
MSRvid 52 .7200 .8803
SMT-eur 21 .4850 .5666
On-WN 2 .7124 .7273
SMT-news 4 .5312 .6085
Table 3: The ranking and correlation (?) obtained by
PERPphrases for each of the five datasets as well for all
datasets combined. The STS task had a total of 88 sub-
missions. ?best shows the correlation for the best submis-
sion, across all submissions, for each dataset.
7 Conclusion
From the results in ?6, PERP appears to be com-
petitive at measuring semantic textual similarity. It
performed particularly well on the surprise subtasks,
indicating that it generalizes well to new data. Fi-
nally, with the exception of the SMT-eur machine
translation evaluation subtask, PERP outperformed
the TERp system for all of the STS subtasks.
Acknowledgments
We would like to thank the organizers of SemEval
and the Semantic Textual Similarity task. We would
also like to thank Matt Snover for making the origi-
nal TERp code available.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. SemEval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL, pages
597?604.
D. Chen and W. B. Dolan. 2011. Collecting highly par-
allel data for paraphrase evaluation. In Proc. of ACL,
pages 190?200.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
the perceptron algorithm. In Proc. of EMNLP.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Proc. of
COLING, pages 350?356, Geneva, Switzerland.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In Proc. of Machine Transla-
tion Summit.
C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.
534
R. D. Nielsen, W. Ward, and J. H. Martin. 2008. Clas-
sification errors in a domain-independent assessment
system. In Proc. of the Third Workshop on Innova-
tive Use of Natural Language Processing for Building
Educational Applications.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137.
S. S. Pradhan and N. Xue. 2009. OntoNotes: The 90%
solution. In Proc. of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Companion Volume: Tutorial Abstracts, pages
11?12.
M. A. Przybocki, K. Peterson, S. Bronsart, and G. A.
Sanders. 2009. The NIST 2008 metrics for machine
translation challenge - overview, methodology, met-
rics, and results. Machine Translation, 23(2-3):71?
103.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of Translation Edit Rate
with targeted human annotation. In Proc. of the Con-
ference of the Association for Machine Translation in
the Americas (AMTA).
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009a. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric. In
Proc. of the Fourth Workshop on Statistical Machine
Translation at the 12th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2009), March.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009b. TER-Plus: Paraphrase, semantic, and align-
ment enhancements to Translation Edit Rate. Machine
Translation, 23(2?3):117?127.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proc. of of EMNLP.
535
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 96?102, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
HENRY-CORE: Domain Adaptation and Stacking for Text Similarity?
Michael Heilman and Nitin Madnani
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,nmadnani}@ets.org
Abstract
This paper describes a system for automat-
ically measuring the semantic similarity be-
tween two texts, which was the aim of the
2013 Semantic Textual Similarity (STS) task
(Agirre et al, 2013). For the 2012 STS task,
Heilman and Madnani (2012) submitted the
PERP system, which performed competitively
in relation to other submissions. However,
approaches including word and n-gram fea-
tures also performed well (Ba?r et al, 2012;
S?aric? et al, 2012), and the 2013 STS task fo-
cused more on predicting similarity for text
pairs from new domains. Therefore, for the
three variations of our system that we were al-
lowed to submit, we used stacking (Wolpert,
1992) to combine PERP with word and n-
gram features and applied the domain adapta-
tion approach outlined by Daume III (2007)
to facilitate generalization to new domains.
Our submissions performed well at most sub-
tasks, particularly at measuring the similarity
of news headlines, where one of our submis-
sions ranked 2nd among 89 from 34 teams, but
there is still room for improvement.
1 Introduction
We aim to develop an automatic measure of the se-
mantic similarity between two short texts (e.g., sen-
tences). Such a measure could be useful for vari-
ous applications, including automated short answer
scoring (Leacock and Chodorow, 2003; Nielsen et
al., 2008), question answering (Wang et al, 2007),
? System description papers for this task were required to
have a team ID and task ID (e.g., ?HENRY-CORE?) as a prefix.
and machine translation evaluation (Przybocki et al,
2009).
In this paper, we describe our submissions to the
2013 Semantic Textual Similarity (STS) task (Agirre
et al, 2013), which evaluated implementations of
text-to-text similarity measures. Submissions were
evaluated according to Pearson correlations between
gold standard similarity values acquired from hu-
man raters and machine-produced similarity val-
ues. Teams were allowed to submit up to three
submissions. For each submission, correlations
were calculated separately for four subtasks: mea-
suring similarity between news headlines (?head-
lines?), between machine translation outputs and hu-
man reference translations (?SMT?), between word
glosses from OntoNotes (Pradhan and Xue, 2009)
and WordNet (Fellbaum, 1998) (?OnWN?), and be-
tween frame descriptions from FrameNet (Fillmore
et al, 2003) and glosses from WordNet (?FNWN?).
A weighted mean of the correlations was also com-
puted as an overall evaluation metric (the OnWn and
FNWN datasets were smaller than the headlines and
SMT datasets).
The suggested training data for the 2013 STS
task was the data from the 2012 STS task (Agirre
et al, 2012), including both the training and test
sets for that year. The 2012 task was similar ex-
cept that the data were from a different set of sub-
tasks: measuring similarity between sentences from
the Microsoft Research Paraphrase corpus (Dolan
et al, 2004) (?MSRpar?), between sentences from
the Microsoft Research Video Description corpus
(Chen and Dolan, 2011) (?MSRvid?), and between
human and machine translations of parliamentary
96
proceedings (?SMTeuroparl?). The 2012 task pro-
vided training and test sets for those three subtasks
and also included two additional tasks with just test
sets: a similar OnWN task, and measuring similar-
ity between human and machine translations of news
broadcasts (?SMTnews?).
Heilman and Madnani (2012) described the PERP
system and submitted it to the 2012 STS task. PERP
measures the similarity of a sentence pair by find-
ing a sequence of edit operations (e.g., insertions,
deletions, substitutions, and shifts) that converts one
sentence to the other. It then uses various features
of the edits, with weights learned from labeled sen-
tence pairs, to assign a similarity score. PERP per-
formed well, ranking 7th out of 88 submissions from
35 teams according to the weighted mean correla-
tion. However, PERP lacked some of the useful
word and n-gram overlap features included in some
of the other top-performing submissions. In addi-
tion, domain adaptation seemed more relevant for
the STS 2013 task since in-domain data was avail-
able only for one (OnWN) of the four subtasks.
Therefore, in this work, we combine the PERP
system with various word and n-gram features.
We also apply the domain adaptation technique of
Daume III (2007) to support generalization beyond
the domains in the training data.
2 System Details
In this section, we describe the system we devel-
oped, and the variations of it that comprise our sub-
missions to the 2013 STS task.
Our system is a linear model estimated using
ridge regression, as implemented in the scikit-learn
toolkit (Pedregosa et al, 2011). The system uses
a 5-fold cross-validation grid search to tune the ?
penalty for ridge regression (with ? ? 2{?5,?4,...,4}).
During development, we evaluated its performance
on the full STS 2012 data (training and test) us-
ing 10-fold cross-validation, with the 5-fold cross-
validation being used to tune within each training
partition.
2.1 Features
Our full system uses the following features com-
puted from an input sentence pair (s1, s2).
The system standardizes feature values to zero
mean and unit variance by subtracting the feature?s
mean and dividing by its standard deviation. The
means and standard deviations are estimated from
the training set, or from each training partition dur-
ing cross-validation.
2.1.1 n-gram Overlap Features
The system computes Jaccard similarity (i.e., the
ratio of the sizes of the set intersection to the set
union) for the following overlap features:
? character n-gram overlap (n = 1 . . . 12). Note
that this is computed from the entire original
texts for a pair, including punctuation, whites-
pace, etc.
? word n-gram overlap (n = 2 . . . 8). We do not
include n = 1 here because it would be identi-
cal to the n = 1 version for the unordered word
n-gram feature described next.
? unordered word n-gram overlap features (n =
1 . . . 3). By unordered, we mean combina-
tions (in the mathematical sense of ?combi-
nations?) of word tokens, regardless of order.
Note that these features are similar to the word
n-gram overlap features except that the words
need not be contiguous to match. For example,
the text ?John saw Mary? would result in the
following unordered word n-grams: {john},
{mary}, {saw}, {john, saw}, {mary, saw},
{john, mary}, and {john, mary, saw}.
For the word and unordered n-gram overlap fea-
tures, we computed two variants: one based on all
tokens and one based on just content words, which
we define as words that are not punctuation and do
not appear in the NLTK (Bird et al, 2009) English
stopword list. We lowercase everything for the word
overlap measures but not for character overlap.
2.1.2 Length Features
The system includes various length-related fea-
tures, where Lmax = max(length(s1), length(s2)),
Lmin = min(length(s1), length(s2)), and length(x)
denotes the number of tokens in x. log denotes the
natural logarithm.
? log(LmaxLmin )
? Lmax?LminLmax
97
? log(Lmin)
? log(Lmax)
? log(|Lmax ? Lmin|+ 1)
2.1.3 Sentiment Features
The system includes various features based on the
proprietary sentiment lexicon described by Beigman
Klebanov et al (2012). Each word in this lexicon
is associated with a 3-tuple specifying a distribution
over three classes: positive, negative, and neutral.
These distributions were estimated via crowdsourc-
ing. If a word is not in the lexicon, we assume its
positivity and negativity are zero.
We define the set of sentiment words in a sen-
tence s as ?(s) = {w : positivity(w) > 0.5 ?
negativity(w) > 0.5}. We also define the pos-
itivity, negativity, and neutrality of a sentence as
the sum over the corresponding values of indi-
vidual words w. For example, positivity(s) =
?
w?s positivity(w).
The system includes the following features:
? ?(s1)??(s2)?(s1)??(s2) (i.e., the Jaccard similarity of the
sentiment words)
? The cosine distance between
(positivity(s1), negativity(s1)) and
(positivity(s2), negativity(s2))
? |positivity(s1)? positivity(s2)|
? |negativity(s1)? negativity(s2)|
? |neutrality(s1)? neutrality(s2)|
2.1.4 PERP with Stacking
The system also incorporates the PERP system
(Heilman and Madnani, 2012) (as briefly described
in ?1) as a feature in its model by using 10-fold
stacking (Wolpert, 1992). Stacking is a procedure
similar to k-fold cross-validation that allows one to
use the output of one model as the input to another
model, without requiring multiple training sets. A
PERP model is iteratively trained on nine folds and
then the PERP feature is computed for the tenth,
producing PERP features for the whole training set,
which are then used in the final regression model.
We trained PERP in a general manner using data
from all the STS 2012 subtasks rather than training
subtask-specific models. PERP was trained for 100
iterations.
We refer readers to Heilman and Madnani (2012)
for a full description of PERP. Next, we provide de-
tails about modifications made to PERP since STS
2012. Although these details are not necessary to
understand how the system works in general, we in-
clude them here for completeness.
? We extended PERP to model abbreviations as
zero cost edits, using a list of common abbrevi-
ations extracted from Wikipedia.1
? In a similar vein, we also extended PERP
to model multiword sequences with differing
punctuation (e.g., ?Built-In Test? ? ?Built In
Test?) as zero cost edits.
? We changed the stemming and synonymy edits
in the original PERP (Heilman and Madnani,
2012) to be substitution edits that activate addi-
tional stemming and synonymy indicator fea-
tures.
? We added an incentive to TERp?s (Snover et
al., 2009) original inference algorithm to pre-
fer matching words when searching for a good
edit sequence. We added this to avoid rare
cases where other edits would have a negative
costs, and then the same word in a sentence
pair would be, for example inserted and deleted
rather than matched.
? We fixed a minor bug in the inference algo-
rithm, which appeared to only affect results on
the MSRvid subtask in the STS 2012 task.
? We tweaked the learning algorithm by increas-
ing the learning rate and not performing weight
averaging.
2.2 Domain Adaptation
The system also uses the domain adaptation tech-
nique described by Daume III (2007) to facilitate
generalization to new domains. Instead of having
a single weight for each of the features described
above, the system maintains a generic and a subtask-
specific copy. For example, the content bigram over-
lap feature had six copies: a generic copy and one
for each of the five subtasks in the training data from
1http://en.wikipedia.org/wiki/List_of_
acronyms_and_initialisms, downloaded April 27,
2012
98
STS 2012 (i.e., OnWN, MSRpar, MSRvid, SMTeu-
roparl, SMTnews). And then for an instance from
MSRpar, only the generic and MSRpar-specific ver-
sions of the feature will be active. For an instance
from a new subtask (e.g., a test set instance), only
the generic feature will be active.
We also included a generic intercept feature and
intercept features for each subtask (these always had
a value of 1). These help the model capture, for
example, whether high or low similarities are more
frequent in general, without having to use the other
feature weights to do so.
2.3 Submissions
We submitted three variations of the system.
? Run 1: This run used all the features described
above. In addition, we mapped the test subtasks
to the training subtasks as follows so that the
specific features would be active for test data
from previously unseen but related subtasks:
headlines to MSRpar, SMT to SMTnews, and
FNWN to OnWN.
? Run 2: As in Run 1, this run used all the fea-
tures described above. However, we did not
map the STS 2013 subtasks to STS 2012 sub-
tasks. Thus, the specific copies of features were
only active for OnWN test set examples.
? Run 3: This run used all the features except for
the PERP and sentiment features. Like Run 2,
this run did not map subtasks.
3 Results
This section presents results on the STS 2012 data
(our development set) and results for our submis-
sions to STS 2013.
3.1 STS 2012 (development set)
Although we used cross-validation on the entire STS
2012 dataset during preliminary experiments (?2),
in this section, we train the system on the original
STS 2012 training set and report performance on the
original STS 2012 test set, in order to facilitate com-
parison to submissions to that task. It is important to
note that our system?s results here may be somewhat
optimistic since we had access to the STS 2012 test
data and were using it for development, whereas the
participants in the 2012 task only had access to the
training data.
Table 1 presents the results. We include the results
for our three submissions, the results for the top-
ranked submission according to the weighted mean
(?UKP?), the results for the best submission from
Heilman and Madnani (2012) (?PERPphrases?), and
the mean across all submissions. Note that while we
compare to the PERP submission from Heilman and
Madnani (2012), the results are not directly compa-
rable since the version of PERP is not the same and
since PERP was trained differently.
For Run 1 on the STS 2012 data, we mapped
OnWN to MSRpar, and SMTnews to SMTeuroparl,
similar to Heilman and Madnani (2012).
3.2 STS 2013 (unseen test set)
Table 2 presents results for our submissions to the
2013 STS task. We include results for our three sub-
missions, results for the top-ranked submission ac-
cording to the weighted mean, results for the base-
line provided by the task organizers, and the mean
across all submissions and the baseline from the or-
ganizers.2
Note that while our Run 2 submission outper-
formed the top-ranked UMBC submission on the
headlines subtask, as shown in 2, there was another
UMBC submission that performed better than Run 2
for the headlines subtask.
4 Discussion
The weighted mean correlation across tasks for our
submissions was relatively poor compared to the
top-ranked systems for STS 2013: our Run 1, Run 2,
and Run 3 submissions beat the baseline and ranked
41st, 26th, and 48th, respectively, out of 89 submis-
sions.
The primary reason for this result is that perfor-
mance of our submissions was poor for the OnWN
subtask, where, e.g., our Run 2 submission?s corre-
lation was r = .4631, compared to r = .8431 for
the top-ranked submission for that subtask (?deft-
baseline?). Upon investigation, we found that
OnWN training and test data were very different in
terms of their score distributions. The mean gold
2The STS 2013 results are from http://ixa2.si.
ehu.es/sts/.
99
Submission MSRpar MSRvid SMTeuroparl OnWN SMTnews W. Mean
Run 1 .6461 .8060 .5014 .7073 .4876 .6577
Run 2 .6461 .8060 .5014 .7274 .4744 .6609
Run 3 .6369 .7904 .5101 .7010 .4985 .6529
UKP (top-ranked) .6830 .8739 .5280 .6641 .4937 .6773
PERPphrases .6397 .7200 .4850 .7124 .5312 .6399
mean-2012 .4894 .7049 .3958 .5557 .3731 .5286
Table 1: Pearson correlations for STS 2012 data for each subtask and then the weighted mean across subtasks. ?UKP?
was submitted by Ba?r et al (2012), ?PERPphrases? was submitted by Heilman and Madnani (2012), and ?mean-2012?
is the mean of all submissions to STS 2012.
Submission headlines OnWN FNWN SMT W. Mean
Run 1 .7601 .4631 .3516 .2801 .4917
Run 2 .7645 .4631 .3905 .3593 .5229
Run 3 .7103 .3934 .3364 .3308 .4734
UMBC (top-ranked) .7642 .7529 .5818 .3804 .6181
baseline .5399 .2828 .2146 .2861 .3639
mean-2013 .6022 .5042 .2887 .2989 .4503
Table 2: Pearson correlations for STS 2013 data for each subtask and then the weighted mean across subtasks.
?UMBC? = ?UMBC EBIQUITY-ParingWords?, and ?mean-2013? is the mean of the submissions to STS 2013 and
the baseline.
standard similarity value for the STS 2012 OnWN
data was 3.87 (with a standard deviation of 1.02),
while the mean for the 2013 OnWN data was 2.31
(with a standard deviation of 1.76). We speculate
that our system performed relatively poorly because
it was expecting the OnWN data to include many
highly similar sentences (as in the 2012 data). We
hypothesize that incorporating more detailed Word-
Net information (only the PERP feature used Word-
Net, and only in a limited fashion, to check syn-
onymy) and task-specific features for comparing
definitions might have helped performance for the
OnWN subtask.
If we ignore the definition comparison subtasks,
and consider performance on just the headlines and
SMT subtasks, the system performed quite well.
Our Run 2 submission had a mean correlation of
r = .5619 for those two subtasks, which would rank
5th among all submissions.
We have not fully explored the effects on perfor-
mance of the domain adaptation approach used in
the system, but our approach of mapping tasks used
for our Run 1 submission did not seem to help. It
seems better to keep a general model, as in Runs 2
and 3.
Additionally, we observe that the performance of
Run 3, which did not use the PERP and sentiment
features, was relatively good compared to Runs 1
and 2, which used all the features. This indicates
that if speed and implementation simplicity are im-
portant concerns for an application, it may suffice to
use relatively simple overlap and length features to
measure semantic similarity.
The contribution of domain adaptation is not
clear. Mapping novel subtasks to tasks for which
training data is available (?2.3), in combination with
the domain adaptation technique we used, did not
generally improve performance. However, we leave
to future work a detailed analysis of whether the
domain adaptation approach (without mapping) is
better than simply training a separate system for
each subtask and using out-of-domain data when in-
domain data is unavailable.
5 Conclusion
In this paper, we described a system for predicting
the semantic similarity of two short texts. The sys-
tem uses stacking to combine a trained edit-based
similarity model (Heilman and Madnani, 2012) with
100
simple features such as word and n-gram overlap,
and it uses the technique described by Daume III
(2007) to support generalization to domains not rep-
resented in the training data. We also presented eval-
uation results, using data from the STS 2012 and
STS 2013 shared tasks, that indicate that the system
performs competitively relative to other approaches
for many tasks. In particular, we observed very
good performance on the news headline similarity
and MT evaluation subtasks of the STS 2013 shared
task.
Acknowledgments
We would like to thank the STS 2013 task organizers
for facilitating this research and Dan Blanchard for
helping with scikit-learn.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,
Adam Faulkner, and Joel Tetreault. 2012. Build-
ing sentiment lexicon(s) from scratch for essay data.
In Proceedings of the 13th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing), New Delhi, India, March.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly Media.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 190?200, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of Coling 2004, pages 350?356, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to Framenet.
International Journal of Lexicography, 16(3):235?
250.
Michael Heilman and Nitin Madnani. 2012. ETS: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Classification errors in a domain-independent
assessment system. In Proceedings of the Third Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 10?18, Columbus, Ohio,
June. Association for Computational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
S. S. Pradhan and N. Xue. 2009. OntoNotes: The 90%
solution. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
101
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Tutorial Ab-
stracts, pages 11?12.
M. A. Przybocki, K. Peterson, S. Bronsart, and G. A.
Sanders. 2009. The NIST 2008 metrics for machine
translation challenge - overview, methodology, met-
rics, and results. Machine Translation, 23(2-3):71?
103.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: Paraphrase,
semantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127,
September.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. TakeLab: Systems
for measuring semantic text similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 22?32,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
102
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 275?279, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ETS: Domain Adaptation and Stacking for Short Answer Scoring?
Michael Heilman and Nitin Madnani
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,nmadnani}@ets.org
Abstract
Automatic scoring of short text responses to
educational assessment items is a challeng-
ing task, particularly because large amounts
of labeled data (i.e., human-scored responses)
may or may not be available due to the va-
riety of possible questions and topics. As
such, it seems desirable to integrate various
approaches, making use of model answers
from experts (e.g., to give higher scores to
responses that are similar), prescored student
responses (e.g., to learn direct associations
between particular phrases and scores), etc.
Here, we describe a system that uses stack-
ing (Wolpert, 1992) and domain adaptation
(Daume III, 2007) to achieve this aim, allow-
ing us to integrate item-specific n-gram fea-
tures and more general text similarity mea-
sures (Heilman and Madnani, 2012). We re-
port encouraging results from the Joint Stu-
dent Response Analysis and 8th Recognizing
Textual Entailment Challenge.
1 Introduction
In this paper, we address the problem of automati-
cally scoring short text responses to educational as-
sessment items for measuring content knowledge.
Many approaches can be and have been taken to
this problem?e.g., Leacock and Chodorow (2003),
Nielsen et al (2008), inter alia. The effectiveness
of any particular approach likely depends on the the
availability of data (among other factors). For exam-
ple, if thousands of prescored responses are avail-
?System description papers for SemEval 2013 are required
to have a team ID (e.g., ?ETS?) as a prefix.
able, then a simple classifier using n-gram features
may suffice. However, if only model answers (i.e.,
reference answers) or rubrics are available, more
general semantic similarity measures (or even rule-
based approaches) would be more effective.
It seems likely that, in many cases, there will
be model answers as well as a modest number of
prescored responses available, as was the case for
the Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment Challenge (?2). There-
fore, we desire to incorporate both task-specific fea-
tures, such as n-grams, as well as more general fea-
tures such as the semantic similarity of the response
to model answers.
We also observe that some features may them-
selves require machine learning or tuning on data
from the domain, in addition to any machine learn-
ing required for the overall system.
In this paper, we describe a machine learning ap-
proach to short answer scoring that allows us to in-
corporate both item-specific and general features by
using the domain adaptation technique of Daume III
(2007). In addition, the approach employs stacking
(Wolpert, 1992) to support the integration of com-
ponents that require tuning or machine learning.
2 Task Overview
In this section, we describe the task to which we ap-
plied our system: the Joint Student Response Anal-
ysis and 8th Recognizing Textual Entailment Chal-
lenge (Dzikovska et al, 2013), which was task 7 at
SemEval 2013.
The aim of the task is to classify student responses
to assessment items from two datasets represent-
275
ing different science domains: the Beetle dataset,
which pertains to basic electricity and electronics
(Dzikovska et al, 2010), and the Science Entail-
ments corpus (SciEntsBank) (Nielsen et al, 2008),
which covers a wider range of scientific topics.
Responses were organized into five categories:
correct, partially correct, contradictory, irrelevant,
and non-domain. The SciEntsBank responses were
converted to this format as described by Dzikovska
et al (2012).
The Beetle training data had about 4,000 student
answers to 47 questions. The SciEntsBank training
data had about 5,000 prescored student answers to
135 questions from 12 domains (different learning
modules). For each item, one or more model re-
sponses were provided by the task organizers.
There were three different evaluation scenarios:
?unseen answers?, for scoring new answers to items
represented in the training data; ?unseen questions?,
for scoring answers to new items from domains rep-
resented in the training data; and ?unseen domains?,
for scoring answers to items from new domains
(only for SciEntsBank since Beetle focused on a sin-
gle domain).
Performance was evaluated using accuracy,
macro-average F1 scores, and weighted average F1
scores.
For additional details, see the task description pa-
per (Dzikovska et al, 2013).
3 System Details
In this section, we describe the short answer scoring
system we developed, and the variations of it that
comprise our submissions to task 7. We begin by
describing our statistical modeling approach. There-
after, we describe the features used by the model
(?3.1), including the PERP feature that relies on
stacking (Wolpert, 1992), and then the domain adap-
tation technique we used (?3.2).
Our system is a logistic regression model with
`2 regularization. It uses the implementation of lo-
gistic regression from the scikit-learn toolkit (Pe-
dregosa et al, 2011).1 To tune the C hyperparame-
ter, it uses a 5-fold cross-validation grid search (with
1The scikit-learn toolkit uses a one-versus-all scheme, us-
ing multiple binary logistic regression classifiers, rather than a
single multiclass logistic regression classifier.
C ? 10{?3,?2,...,3}).
During development, we evaluated performance
using 10-fold cross-validation, with the 5-fold cross-
validation grid search still used for tuning within
each training partition (i.e., each set of 9 folds used
for training during cross-validation).
3.1 Features
Our full system includes the following features.
3.1.1 Baseline Features
It includes all of the baseline features generated
with the code provided by the task organizers.2
There are four types of lexically-driven text similar-
ity measures, and each is computed by comparing
the learner response to both the expected answer(s)
and the question, resulting in eight features in total.
They are described more fully by Dzikovska et al
(2012).
3.1.2 Intercept Feature
The system includes an intercept feature that is al-
ways equal to one, which, in combination with the
domain adaptation technique described in ?3.2, al-
lows the system to model the a priori distribution
over classes for each domain and item. Having these
explicit intercept features effectively saves the learn-
ing algorithm from having to use other features to
encode the distribution over classes.
3.1.3 Word and Character n-gram Features
The system includes binary indicator features for
the following types of n-grams:
? lowercased word n-grams in the response text
for n ? {1, 2, 3}.
? lowercased word n-grams in the response text
for n ? {4, 5, . . . , 11}, grouped into 10,000
bins by hashing and using a modulo operation
(i.e., the ?hashing trick?) (Weinberger et al,
2009).
? lowercased character n-grams in the response
text for n ? {5, 6, 7, 8}
2At the time of writing, the baseline code could
be downloaded at http://www.cs.york.ac.uk/
semeval-2013/task7/.
276
3.1.4 Text Similarity Features
The system includes the following text similarity
features that compare the student response either to
a) the reference answers for the appropriate item, or
b) the student answers in the training set that are la-
beled ?correct?.
? the maximum of the smoothed, uncased BLEU
(Papineni et al, 2002) scores obtained by com-
paring the student response to each correct
reference answer. We also include the word
n-gram precision and recall values for n ?
{1, 2, 3, 4} for the maximally similar reference
answer.
? the maximum of the smoothed, uncased BLEU
scores obtained by comparing the student re-
sponse to each correct training set student an-
swer. We also include the word n-gram preci-
sion and recall values for n ? {1, 2, 3, 4} for
the maximally similar student answer.
? the maximum PERP (Heilman and Madnani,
2012) score obtained by comparing the student
response to the correct reference answers.
? the maximum PERP score obtained by compar-
ing the student response to the correct student
answers.
PERP is an edit-based approach to text similar-
ity. It computes the similarity of sentence pairs by
finding sequences of edit operations (e.g., insertions,
deletions, substitutions, and shifts) that convert one
sentence in a pair to the other. Then, using various
features of the edits and weights for those features
learned from labeled sentence pairs, it assigns a sim-
ilarity score. Heilman and Madnani (2012) provide
a detailed description of the original PERP system.
In addition, Heilman and Madnani (To Appear) de-
scribe some minor modifications to PERP used in
this work.
To estimate weights for PERP?s edit features, we
need labeled sentence pairs. First, we describe how
these labeled sentence pairs are generated from the
task data, and then we describe the stacking ap-
proach used to avoid training PERP on the same data
it will compute features for.
For the reference answer PERP feature, we use
the Cartesian product of the set of correct reference
answers (?good? or ?best? for Beetle) and the set
of student answers, using 1 as the similarity score
(i.e., the label for training PERP) for pairs where the
student answer is labeled ?correct? and 0 for all oth-
ers. For the student answer PERP feature, we use
the Cartesian product of the set of correct student
answers and the set of all student answers, using 1
as the similarity score for pairs where both student
answers are labeled ?correct? and 0 for all others.3
We use 10 iterations for training PERP.
In order to avoid training PERP on the same re-
sponses it will compute features for, we use 10-fold
stacking (Wolpert, 1992). In this process, the train-
ing data are split up into ten folds. To compute the
PERP features for the instances in each fold, PERP
is trained on the other nine folds. After all 10 itera-
tions, there are PERP features for every example in
the training set. This process is similar to 10-fold
cross-validation.
3.2 Domain Adaptation
The system uses the domain adaptation technique
from Daume III (2007) to support generalization
across items and domains.
Instead of having a single weight for each feature,
following Daume III (2007), the system has multiple
copies with potentially different weights: a generic
copy, a domain-specific copy, and an item-specific
copy. For an answer to an unseen item (i.e., ques-
tion) from a new domain in the test set, only the
generic feature will be active. In contrast, for an an-
swer to an item represented in the training data, the
generic, domain-specific, and item-specific copies
of the feature would be active and contribute to the
score.
For our submissions, this feature copying ap-
proach was not used for the baseline features
(?3.1.1) or the BLEU and PERP text similarity fea-
tures (?3.1.4), which are less item-specific. Those
features had only general copies. We did not test
whether doing so would affect performance.
3The Cartesian product of the sets of correct student answers
and of all student answers will contain some pairs of identi-
cal correct answers. We decided to simply include these when
training PERP, since we felt it would be desirable for PERP to
learn that identical sentences should be considered similar.
277
Beetle SciEntsBank
Submission A Q A Q D
Run 1 .5520 .5470 .5350 .4870 .4470
Run 2 .7050 .6140 .6250 .3560 .4340
Run 3 .7000 .5860 .6400 .4110 .4140
maximum .7050 .6140 .6400 .4920 .4710
mean .5143 .3978 .4568 .3769 .3736
Table 1: Weighted average F1 scores for 5-way classification for our SemEval 2013 task 7 submissions, along with
the maximum and mean performance, for comparison. ?A? = unseen answers, ?Q? = unseen questions, ?D? = unseen
domains (see ?2 for details). Results that were the maximum score among submissions for part of the task are in bold.
3.3 Submissions
We submitted three variations of the system. For
each variation, a separate model was trained for Bee-
tle and for SciEntsBank.
? Run 1: This run included the baseline (?3.1.1),
intercept (?3.1.2), and the text-similarity fea-
tures (?3.1.4) that compare student responses to
reference answers (but not those that compare
to scored student responses in the training set).
? Run 2: This run included the baseline (?3.1.1),
intercept (?3.1.2), and n-gram features (?3.1.3).
? Run 3: This run included all features.
4 Results
Table 1 presents the weighted averages of F1 scores
across the five categories for the 5-way subtask, for
each dataset and scenario. The maximum and mean
scores of all the submissions are included for com-
parison. These results were provided to us by the
task organizers.
For conciseness, we do not include accuracy or
macro-average F1 scores here. We observed that, in
general, the results from different evaluation metrics
were very similar to each other. We refer the reader
to the task description paper (Dzikovska et al, 2013)
for a full report of the task results.
Interestingly, the differences in performance be-
tween the unseen answers task and the other tasks
was somewhat larger for the SciEntsBank dataset
than for the Beetle dataset. We speculate that this re-
sult is because the SciEntsBank data covered a more
diverse set of topics.
Note that Runs 1 and 2 use subsets of the features
from the full system (Run 3). While Runs 1 and 2
are not directly comparable to each other, Runs 1
and 3 can be compared to measure the effect of the
features based on other previously scored student re-
sponses (i.e., n-grams, and the PERP and BLEU fea-
tures based on student responses). Similarly, Runs 2
and 3 can be compared to measure the combined ef-
fect of all BLEU and PERP features.
It appears that features of the other student re-
sponses improve performance for the unseen an-
swers task. For example, the full system (Run 3)
performed better than Run 1, which did not include
features of other student responses, on the unseen
answers task for both Beetle and SciEntsBank.
However, it is not clear whether the PERP and
BLEU features improve performance. The full sys-
tem (Run 3) did not always outperform Run 2, which
did not include these features.
We leave to future work various additional ques-
tions, such as whether student response features or
reference answer similarity features are more use-
ful in general, and whether there are any systematic
differences between human-machine and human-
human disagreements.
5 Conclusion
We have presented an approach for short answer
scoring that uses stacking (Wolpert, 1992) and do-
main adaptation (Daume III, 2007) to support the
integration of various types of task-specific and gen-
eral features. Evaluation results from task 7 at Se-
mEval 2013 indicate that the system achieves rela-
tively high levels of agreement with human scores,
as compared to other systems submitted to the
shared task.
278
Acknowledgments
We would like to thank the task organizers for facil-
itating this research and Dan Blanchard for helping
with scikit-learn.
References
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Diana Bental, Johanna D.
Moore, Natalie Steinhauser, Gwendolyn Campbell,
Elaine Farrow, and Charles B. Callaway. 2010. In-
telligent tutoring with natural language support in the
BEETLE II system. In Proceedings of Fifth European
Conference on Technology Enhanced Learning (EC-
TEL 2010).
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210, Montre?al, Canada, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Michael Heilman and Nitin Madnani. 2012. ETS: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Michael Heilman and Nitin Madnani. To Appear. Henry:
Domain adapation and stacking for text similarity. In
*SEM 2013: The Second Joint Conference on Lexical
and Computational Semantics. Association for Com-
putational Linguistics.
C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Classification errors in a domain-independent
assessment system. In Proceedings of the Third Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 10?18, Columbus, Ohio,
June. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature hash-
ing for large scale multitask learning. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ?09, pages 1113?1120, New
York, NY, USA. ACM.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
279
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 188?194,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Measuring Transitivity Using Untrained Annotators
Nitin Madnania,b Jordan Boyd-Grabera Philip Resnika,c
aInstitute for Advanced Computer Studies
bDepartment of Computer Science
cDepartment of Linguistics
University of Maryland, College Park
{nmadnani,jbg,resnik}@umiacs.umd.edu
Abstract
Hopper and Thompson (1980) defined a multi-axis
theory of transitivity that goes beyond simple syn-
tactic transitivity and captures how much ?action?
takes place in a sentence. Detecting these features
requires a deep understanding of lexical semantics
and real-world pragmatics. We propose two gen-
eral approaches for creating a corpus of sentences
labeled with respect to the Hopper-Thompson transi-
tivity schema using Amazon Mechanical Turk. Both
approaches assume no existing resources and incor-
porate all necessary annotation into a single system;
this is done to allow for future generalization to other
languages. The first task attempts to use language-
neutral videos to elicit human-composed sentences
with specified transitivity attributes. The second task
uses an iterative process to first label the actors and
objects in sentences and then annotate the sentences?
transitivity. We examine the success of these tech-
niques and perform a preliminary classification of
the transitivity of held-out data.
Hopper and Thompson (1980) created a multi-axis the-
ory of Transitivity1 that describes the volition of the sub-
ject, the affectedness of the object, and the duration of the
action. In short, this theory goes beyond the simple gram-
matical notion of transitivity (whether verbs take objects
? transitive ? or not ? intransitive) and captures how
much ?action? takes place in a sentence. Such notions of
Transitivity are not apparent from surface features alone;
identical syntactic constructions can have vastly different
Transitivity. This well-established linguistic theory, how-
ever, is not useful for real-world applications without a
Transitivity-annotated corpus.
Given such a substantive corpus, conventional machine
learning techniques could help determine the Transitivity
of verbs within sentences. Transitivity has been found to
play a role in what is called ?syntactic framing,? which
expresses implicit sentiment (Greene and Resnik, 2009).
1We use capital ?T? to differentiate from conventional syntactic tran-
sitivity throughout the paper.
In these contexts, the perspective or sentiment of the
writer is reflected in the constructions used to express
ideas. For example, a less Transitive construction might
be used to deflect responsibility (e.g. ?John was killed?
vs. ?Benjamin killed John?).
In the rest of this paper, we review the Hopper-
Thompson transitivity schema and propose two relatively
language-neutral methods to collect Transitivity ratings.
The first asks humans to generate sentences with de-
sired Transitivity characteristics. The second asks hu-
mans to rate sentences on dimensions from the Hopper-
Thompson schema. We then discuss the difficulties of
collecting such linguistically deep data and analyze the
available results. We then pilot an initial classifier on the
Hopper-Thompson dimensions.
1 Transitivity
Table 1 shows the subset of the Hopper-Thompson di-
mensions of Transitivity used in this study. We excluded
noun-specific aspects as we felt that these were well cov-
ered by existing natural language processing (NLP) ap-
proaches (e.g. whether the object / subject is person, ab-
stract entity, or abstract concept is handled well by exist-
ing named entity recognition systems) and also excluded
aspects which we felt had significant overlap with the
dimensions we were investigating (e.g. affirmation and
mode).
We also distinguished the original Hopper-Thompson
?Affectedness? aspect into separate ?Benefit? and
?Harm? components, as we suspect that these data will
be useful to other applications such as sentiment analy-
sis.
We believe that these dimensions of transitivity are
simple and intuitive enough that they can be understood
and labeled by the people on Amazon Mechanical Turk,
a web service. Amazon Mechanical Turk (MTurk) allows
individuals to post jobs on MTurk with a set fee that are
then performed by workers on the Internet. MTurk con-
nects workers to people with tasks and handles the coor-
dination problems of payment and transferring data.
188
Kinesis Sentences where movement happens are perceived to be more Transitive.?Sue jumped out of an airplane? vs. ?The corporation jumped to a silly conclusion.?
Punctuality Sentences where the action happens quickly are perceived to be more Transitive.?She touched her ID to the scanner to enter? vs. ?I was touched by how much she helped me.?
Mode Sentences with no doubt about whether the action happened are perceived to be more Transitive.?Bob was too busy to fix the drain? vs. ?Bob fixed the drain.?
Affectedness Sentences where the object is more affected by the action are perceived to be more Transitive.?The St. Bernard saved the climber? vs. ?Melanie looked at the model.?
Volition Sentences where the actor chose to perform the action are perceived to be more Transitive.?Paul jumped out of the bushes and startled his poor sister? vs. ?The picture startled George.?
Aspect Sentences where the action is done to completion are perceived to be more Transitive.?Walter is eating the hamburger? vs. ?Walter ate the pudding up.?
Table 1: The Hopper-Thompson dimensions of transitivity addressed in this paper. In experiments, ?Affectedness? was divided into
?Harm? and ?Benefit.?
2 Experiments
Our goal is to create experiments for MTurk that will pro-
duce a large set of sentences with known values of Tran-
sitivity. With both experiments, we design the tasks to
be as language independent as possible, thus not depend-
ing on language-specific preprocessing tools. This allows
the data collection approach to be replicated in other lan-
guages.
2.1 Elicitation
The first task is not corpus specific, and requires no
language-specific resources. We represent verbs using
videos (Ma and Cook, 2009). This also provides a form
of language independent sense disambiguation. We dis-
play videos illustrating verbs (Figure 1) and ask users on
MTurk to identify the action and give nouns that can do
the action and ? in a separate task ? the nouns that the
action can be done to. For quality control, Turkers must
match a previous Turker?s response for one of their an-
swers (a la the game show ?Family Feud?).
Figure 1: Stills from three videos depicting the verbs ?receive,?
?hear,? and ?help.?
We initially found that subjects had difficulty distin-
guishing what things could do the action (subjects) vs.
what things the action could be done to (objects). In or-
der to suggest the appropriate syntactic frame, we use
javascript to form their inputs into protosentences as they
typed. For example, if they identified an action as ?pick-
ing? and suggested ?fruit? as a possible object, the pro-
tosentence ?it is picking fruit? is displayed below their
input (Figure 2). This helped ensure consistent answers.
The subject and object tasks were done separately, and
for the object task, users were allowed to say that there
is nothing the action can be done to (for example, for an
intransitive verb).
Figure 2: A screenshot of a user completing a task to find ob-
jects of a particular verb, where the verb is represented by a
film. After the user has written a verb and a noun, a protosen-
tence is formed and shown to ensure that the user is using the
words in the appropriate roles.
These subjects and objects we collected were then used
as inputs for a second task. We showed workers videos
with potential subjects and objects and asked them to
create pairs of sentences with opposite Transitivity at-
tributes. For example, Write a sentence where the thing
to which the action is done benefits and Write a sentence
where the thing to which the action is done is not affected
by the action. For both sides of the Transitivity dimen-
sion, we allowed users to say that writing such a sentence
is impossible. We discuss the initial results of this task in
Section 3.
2.2 Annotation
Our second task?one of annotation?depends on having
a corpus available in the language of interest. For con-
189
creteness and availability, we use Wikipedia, a free mul-
tilingual encyclopedia. We extract a large pool of sen-
tences from Wikipedia containing verbs of interest. We
apply light preprocessing to remove long, unclear (e.g.
starting with a pronoun), or uniquely Wikipedian sen-
tences (e.g. very short sentences of the form ?See List
of Star Trek Characters?). We construct tasks, each for a
single verb, that ask users to identify the subject and ob-
ject for the verb in randomly selected sentences.2 Users
were prompted by an interactive javascript guide (Fig-
ure 3) that instructed them to click on the first word of the
subject (or object) and then to click on the last word that
made up the subject (or object). After they clicked, a text
box was automatically populated with their answer; this
decreased errors and made the tasks easier to finish. For
quality control, each HIT has a simple sentence where
subject and object were already determined by the au-
thors; the user must match the annotation on that sentence
for credit. We ended up rejecting less than one percent of
submitted hits.
Figure 3: A screenshot of the subject identification task. The
user has to click on the phrase that they believe is the subject.
Once objects and subjects have been identified, other
users rate the sentence?s Transitivity by answering the
following questions like, where $VERB represents the
verb of interest, $SUBJ is its subject and $OBJ is its ob-
ject3:
? Aspect. After reading this sentence, do you know
that $SUBJ is done $VERBing?
? Affirmation. From reading the sentence, how cer-
tain are you that $VERBing happened?
? Benefit. How much did $OBJ benefit?
? Harm. How much was $OBJ harmed?
? Kinesis. Did $SUBJ move?
? Punctuality. If you were to film $SUBJ?s act of
$VERBing in its entirety, how long would the movie
be?
? Volition. Did the $SUBJ make a conscious choice
to $VERB?
The answers were on a scale of 0 to 4 (higher num-
bers meant the sentence evinced more of the property in
2Our goal of language independence and the unreliable correspon-
dence between syntax and semantic roles precludes automatic labeling
of the subjects and objects.
3These questions were developed using Greene and Resnik?s (2009)
surveys as a foundation.
question), and each point in the scale had a description to
anchor raters and to ensure consistent results.
2.3 Rewards
Table 2 summarizes the rewards for the tasks used in
these experiments. Rewards were set at the minimal rate
that could attract sufficient interest from users. For the
?Video Elicitation? task, where users wrote sentences
with specified Transitivity properties, we also offered
bonuses for clever, clear sentences. However, this was
our least popular task, and we struggled to attract users.
3 Results and Discussion
3.1 Creative but Unusable Elicitation Results
We initially thought that we would have difficulty coax-
ing users to provide full sentences. This turned out not
to be the case. We had no difficulty getting (very imag-
inative) sentences, but the sentences were often incon-
sistent with the Transitivity aspects we are interested in.
This shows both the difficulty of writing concise instruc-
tions for non-experts and the differences between every-
day meanings of words and their meaning in linguistic
contexts.
For example, the ?volitional? elicitation task asked
people to create sentences where the subject made a con-
scious decision to perform the action. In the cases where
we asked users to create sentences where the subject did
not make a conscious decision to perform an action, al-
most all of the sentences created by users focused on sen-
tences where a person (rather than employ other tactics
such as using a less individuated subject, e.g. replacing
?Bob? with ?freedom?) was performing the action and
was coerced into doing the action. For example:
? Sellers often give gifts to their clients when they are
trying to make up for a wrongdoing.
? A man is forced to search for his money.
? The man, after protesting profusely, picked an exer-
cise class to attend
? The vegetarian Sherpa had to eat the pepperoni pizza
or he would surely have died.
While these data are likely still interesting for other pur-
poses, their biased distribution is unlikely to be useful for
helping identify whether an arbitrary sentence in a text
expresses the volitional Transitivity attribute. The users
prefer to have an animate agent that is compelled to take
the action rather than create sentences where the action
happens accidentally or is undertaken by an abstract or
inanimate actor.
Similarly, for the aspect dimension, many users simply
chose to represent actions that had not been completed
190
Task Questions / Hit Pay Repetition Tasks Total
Video Object 5 0.04 5 10 $2.00
Video Subject 5 0.04 5 10 $2.00
Corpus Object 10 0.03 5 50 $7.50
Corpus Subject 10 0.03 5 50 $7.50
Video Elicitation 5 0.10 2 70 $14.00
Corpus Annotation 7 0.03 3 400 $36.00
Total $69.00
Table 2: The reward structure for the tasks presented in this paper (not including bonuses or MTurk overhead). ?Video Subject? and
?Video Object? are where users were presented with a video and supplied the subjects and objects of the depicted actions. ?Corpus
Subject? and ?Corpus Object? are the tasks where users identified the subject and objects of sentences from Wikipedia. ?Video
Elicitation? refers to the task where users were asked to write sentences with specified Transitivity properties. ?Corpus Annotation?
is where users are presented with sentences with previously identified subjects and objects and must rate various dimensions of
Transitivity.
using the future tense. For the kinesis task, users dis-
played amazing creativity in inventing situations where
movement was correlated with the action. Unfortunately,
as before, these data are not useful in generating predic-
tive features for capturing the properties of Transitivity.
We hope to improve experiments and instructions to
better align everyday intuitions with the linguistic proper-
ties of interest. While we have found that extensive direc-
tions tend to discourage users, perhaps there are ways in-
crementally building or modifying sentences that would
allow us to elicit sentences with the desired Transitivity
properties. This is discussed further in the conclusion,
Section 4.
3.2 Annotation Task
For the annotation task, we observed that users often had
a hard time keeping their focus on the words in question
and not incorporating additional knowledge. For exam-
ple, for each of the following sentences:
? Bonosus dealt with the eastern cities so harshly that
his severity was remembered centuries later .
? On the way there, however, Joe and Jake pick an-
other fight .
? The Black Sea was a significant naval theatre of
World War I and saw both naval and land battles
during World War II .
? Bush claimed that Zubaydah gave information that
lead to al Shibh ?s capture .
some users said that the objects in bold were greatly
harmed, suggesting that users felt even abstract concepts
could be harmed in these sentences. A rigorous inter-
pretation of the affectedness dimension would argue that
these abstract concepts were incapable of being harmed.
We suspect that the negative associations (severity, fight,
battles, capture) present in this sentence are causing users
to make connections to harm, thus creating these ratings.
Similarly, world knowledge flavored other questions,
such as kinesis, where users were able to understand from
context that the person doing the action probably moved
at some point near the time of the event, even if move-
ment wasn?t a part of the act of, for example, ?calling? or
?loving.?
3.3 Quantitative Results
For the annotation task, we were able to get consistent
ratings of transitivity. Table 3 shows the proportion of
sentences where two or more annotators agreed on the
a Transitivity label of the sentences for that dimension.
All of the dimensions were significantly better than ran-
dom chance agreement (0.52); the best was harm, which
has an accessible, clear, and intuitive definition, and the
worst was kinesis, which was more ambiguous and prone
to disagreement among raters.
Dimension Sentences
with Agreement
HARM 0.87
AFFIRMATION 0.86
VOLITION 0.86
PUNCTUALITY 0.81
BENEFIT 0.81
ASPECT 0.80
KINESIS 0.70
Table 3: For each of the dimensions of transitivity, the propor-
tion of sentences where at least two of three raters agreed on the
label. Random chance agreement is 0.52.
Figure 4 shows a distribution for each of the Transitiv-
ity data on the Wikipedia corpus. These data are consis-
tent with what one would expect from random sentences
from an encyclopedic dataset; most of the sentences en-
191
Median Score
Co
un
t
0
50
100
150
200
250
0
50
100
150
200
250
AFFIRMATION
KINESIS
0 1 2 3 4
ASPECT
PUNCTUALITY
0 1 2 3 4
BENEFIT
VOLITIONALITY
0 1 2 3 4
HARM
0 1 2 3 4
Figure 4: Histograms of median scores from raters by Transitivity dimension. Higher values represent greater levels of Transitivity.
code truthful statements, most actions have been com-
pleted, most objects are not affected, most events are over
a long time span, and there is a bimodal distribution over
volition. One surprising result is that for kinesis there
is a fairly flat distribution. One would expect a larger
skew toward non-kinetic words. Qualitative analysis of
the data suggest that raters used real-world knowledge to
associate motion with the context of actions (even if mo-
tion is not a part of the action), and that raters were less
confident about their answers, prompting more hedging
and a flat distribution.
3.4 Predicting Transitivity
We also performed an set of initial experiments to investi-
gate our ability to predict Transitivity values for held out
data. We extracted three sets of features from the sen-
tences: lexical features, syntactic features, and features
derived from WordNet (Miller, 1990).
Lexical Features A feature was created for each word
in a sentence after being stemmed using the Porter stem-
mer (Porter, 1980).
Syntactic Features We parsed each sentence using the
Stanford Parser (Klein and Manning, 2003) and used
heuristics to identify cases where the main verb is tran-
sitive, where the subject is a nominalization (e.g. ?run-
ning?), or whether the sentence is passive. If any of these
constructions appear in the sentence, we generate a corre-
sponding feature. These represent features identified by
Greene and Resnik (2009).
WordNet Features For each word in the sentence, we
extracted all the possible senses for each word. If any
possible sense was a hyponym (i.e. an instance of) one
of: artifact, living thing, abstract entity, location, or food,
we added a feature corresponding to that top level synset.
For example, the string ?Lincoln? could be an instance
of both a location (Lincoln, Nebraska) and a living thing
(Abe Lincoln), so a feature was added for both the loca-
tion and living thing senses. In addition to these noun-
based features, features were added for each of the pos-
sible verb frames allowed by each of a word?s possible
senses (Fellbaum, 1998).
At first, we performed simple 5-way classification and
found that we could not beat the most frequent class base-
line for any dimension. We then decided to simplify the
classification task to make binary predictions of low-vs-
high instead of fine gradations along the particular di-
mension. To do this, we took all the rated sentences for
each of the seven dimensions and divided the ratings into
low (ratings of 0-1) and high (ratings of 2-4) values for
that dimension. Table 4 shows the results for these bi-
nary classification experiments using different classifiers.
All of the classification experiments were conducted us-
ing the Weka machine learning toolkit (Hall et al, 2009)
and used 10-fold stratified cross validation.
Successfully rating Transitivity requires knowledge
beyond individual tokens. For example, consider kine-
sis. Judging kinesis requires lexical semantics to realize
whether a certain actor is capable of movement, pragmat-
ics to determine if the described situation permits move-
ment, and differentiating literal and figurative movement.
One source of real-world knowledge is WordNet;
adding some initial features from WordNet appears to
help aid some of these classifications. For example, clas-
sifiers trained on the volitionality data were not able to
do better than the most frequent class baseline before the
addition of WordNet-based features. This is a reasonable
result, as WordNet features help the algorithm generalize
which actors are capable of making decisions.
192
Dimension Makeup
Classifier Accuracy
Baseline NB VP SVM-WN +WN -WN +WN -WN +WN
HARM 269/35 88.5 83.9 84.9 87.2 87.8 88.5 88.5
AFFIRMATION 380/20 95.0 92.5 92.0 94.3 95.0 95.0 95.0
VOLITION 209/98 68.1 66.4 69.4 67.1 73.3 68.1 68.1
PUNCTUALITY 158/149 51.5 59.6 61.2 57.0 59.6 51.5 51.5
BENEFIT 220/84 72.4 69.1 65.1 73.4 71.4 72.4 72.4
ASPECT 261/46 85.0 76.5 74.3 81.1 84.7 85.0 85.0
KINESIS 160/147 52.1 61.2 61.2 56.4 60.9 52.1 52.1
Table 4: The results of preliminary binary classification experiments for predicting various transitivity dimensions using different
classifiers such as Naive Bayes (NB), Voted Perceptron (VP) and Support Vector Machines (SVM). Classifier accuracies for two
sets of experiments are shown: without WordNet features (-WN) and with WordNet features (+WN). The baseline simply predicts
the most frequent class. For each dimension, the split between low Transitivity (rated 0-1) and high Transitivity (rated 2-4) is shown
under the ?Makeup? column. All reported accuracies are using 10-fold stratified cross validation.
4 Conclusion
We began with the goal of capturing a subtle linguistic
property for which annotated datasets were not available.
We created a annotated dataset of 400 sentences taken
from the real-word dataset Wikipedia annotated for seven
different Transitivity properties. Users were able to give
consistent answers, and we collected results in a man-
ner that is relatively language independent. Once we ex-
pand and improve this data collection scheme for English,
we hope to perform similar data collection in other lan-
guages. We have available the translated versions of the
questions used in this study for Arabic and German.
Our elicitation task was not as successful as we had
hoped. We learned that while we could form tasks using
everyday language that we thought captured these sub-
tle linguistic properties, we also had many unspoken as-
sumptions that the creative workers on MTurk did not
necessarily share. As we articulated these assumptions
in increasingly long instruction sets to workers, the sheer
size of the instructions began to intimidate and scare off
workers.
While it seems unlikely we can strike a balance that
will give us the answers we want with the elegant instruc-
tions that workers need to feel comfortable for the tasks
as we currently defined them, we hope to modify the task
to embed further linguistic assumptions. For example, we
hope to pilot another version of the elicitation task where
workers modify an existing sentence to change one Tran-
sitivity dimension. Instead of reading and understanding
a plodding discussion of potentially irrelevant details, the
user can simply see a list of sentence versions that are not
allowed.
Our initial classification results suggest that we do not
yet have enough data to always detect these Transitiv-
ity dimensions from unlabeled text or that our algorithms
are using features that do not impart enough information.
It is also possible that using another corpus might yield
greater variation in Transitivity that would aid classifica-
tion; Wikipedia by design attempts to keep a neutral tone
and eschews the highly charged prose that would contain
a great deal of Transitivity.
Another possibility is that, instead of just the Transi-
tivity ratings alone, tweaks to the data collection process
could also help guide classification algorithms (Zaidan et
al., 2008). Thus, instead of clicking on a single annota-
tion label in our current data collection process, Turkers
would click on a data label and the word that most helped
them make a decision.
Our attempts to predict Transitivity are not exhaus-
tive, and there are a number of reasonable algorithms
and resources which could also be applied to the prob-
lem; for example, one might expect semantic role label-
ing or sense disambiguation to possibly aid the prediction
of Transitivity. Determining which techniques are effec-
tive and the reasons why they are effective would aid not
just in predicting Transitivity, which we believe to be an
interesting problem, but also in understanding Transitiv-
ity.
Using services like MTurk allows us to tighten the loop
between data collection, data annotation, and machine
learning and better understand difficult problems. We
hope to refine the data collection process to provide more
consistent results on useful sentences, build classifiers,
and extract features that are able to discover the Transi-
tivity of unlabeled text. We believe that our efforts will
help cast an interesting aspect of theoretical linguistics
into a more pragmatic setting and make it accessible for
use in more practical problems like sentiment analysis.
References
C. Fellbaum, 1998. WordNet : An Electronic Lexi-
cal Database, chapter A semantic network of English
193
verbs. MIT Press, Cambridge, MA.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 503?
511.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Paul J. Hopper and Sandra A. Thompson. 1980.
Transitivity in grammar and discourse. Language,
(56):251?299.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 423?430.
Xiaojuan Ma and Perry R. Cook. 2009. How well do
visual verbs work in daily communication for young
and old adults? In international conference on Human
factors in computing systems, pages 361?364.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2008. Machine learning with annotator rationales
to reduce annotation cost. In Proceedings of the
NIPS*2008 Workshop on Cost Sensitive Learning,
Whistler, BC, December. 10 pages.
194
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 57?64,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
The Web is not a PERSON, Berners-Lee is not an ORGANIZATION, and
African-Americans are not LOCATIONS:
An Analysis of the Performance of Named-Entity Recognition
Robert Krovetz
Lexical Research
Hillsborough, NJ 08844
rkrovetz@lexicalresearch.com
Paul Deane Nitin Madnani
Educational Testing Service
Princeton, NJ 08541
{pdeane,nmadnani}@ets.org
Abstract
Most work on evaluation of named-entity
recognition has been done in the context of
competitions, as a part of Information Extrac-
tion. There has been little work on any form of
extrinsic evaluation, and how one tagger com-
pares with another on the major classes: PER-
SON, ORGANIZATION, and LOCATION.
We report on a comparison of three state-of-
the-art named entity taggers: Stanford, LBJ,
and IdentiFinder. The taggers were compared
with respect to: 1) Agreement rate on the clas-
sification of entities by class, and 2) Percent-
age of ambiguous entities (belonging to more
than one class) co-occurring in a document.
We found that the agreement between the tag-
gers ranged from 34% to 58%, depending on
the class and that more than 40% of the glob-
ally ambiguous entities co-occur within the
same document. We also propose a unit test
based on the problems we encountered.
1 Introduction
Named-Entity Recognition (NER) has been an im-
portant task in Computational Linguistics for more
than 15 years. The aim is to recognize and clas-
sify different types of entities in text. These might
be people?s names, or organizations, or locations, as
well as dates, times, and currencies. Performance
assessment is usually made in the context of In-
formation Extraction, of which NER is generally a
component. Competitions have been held from the
earliest days of MUC (Message Understanding Con-
ference), to the more recent shared tasks in CoNLL.
Recent research has focused on non-English lan-
guages such as Spanish, Dutch, and German (Meul-
der et al, 2002; Carreras et al, 2003; Rossler, 2004),
and on improving the performance of unsupervised
learning methods (Nadeau et al, 2006; Elsner et al,
2009).
There are no well-established standards for eval-
uation of NER. Since criteria for membership in the
classes can change from one competition to another,
it is often not possible to compare performance di-
rectly. Moreover, since some of the systems in the
competition may use proprietary software, the re-
sults in a competition might not be replicable by
others in the community; however, this applies to
the state of the art for most NLP applications rather
than just NER.
Our work is motivated by a vocabulary as-
sessment project in which we needed to identify
multi-word expressions and determine their asso-
ciation with other words and phrases. However,
we found that state-of-the-art software for named-
entity recognition was not reliable; false positives
and tagging inconsistencies significantly hindered
our work. These results led us to examine the state-
of-the-art in more detail.
The field of Information Extraction (IE) has been
heavily influenced by the Information Retrieval (IR)
community when it comes to evaluation of system
performance. The use of Recall and Precision met-
rics for evaluating IE comes from the IR commu-
nity. However, while the IR community regularly
conducts a set of competitions and shared tasks us-
ing standardized test collections, the IE community
does not. Furthermore, NER is just one component
57
of an IE pipeline and any proposed improvements
to this component must be evaluated by determining
whether the performance of the overall IE pipeline
has improved. However, most, if not all, NER eval-
uations and shared tasks only focus on intrinsic NER
performance and ignore any form of extrinsic eval-
uation. One of the contributions of this paper is
a freely available unit test based on the systematic
problems we found with existing taggers.
2 Evaluation Methodology
We compared three state-of-the-art NER taggers:
one from Stanford University (henceforth, Stanford
tagger), one from the University of Illinois (hence-
forth, the LBJ tagger) and BBN IdentiFinder (hence-
forth, IdentiFinder).
The Stanford Tagger is based on Conditional Ran-
dom Fields (Finkel et al, 2005). It was trained on
100 million words from the English Gigawords cor-
pus. The LBJ Tagger is based on a regularized av-
erage perceptron (Ratinov and Roth, 2009). It was
trained on a subset of the Reuters 1996 news cor-
pus, a subset of the North American News Corpus,
and a set of 20 web pages. The features for both
these taggers are based on local context for a target
word, orthographic features, label sequences, and
distributional similarity. Both taggers include non-
local features to ensure consistency in the tagging of
identical tokens that are in close proximity. Identi-
Finder is a state-of-the-art commercial NER tagger
that uses Hidden Markov Models (HMMs) (Bikel et
al., 1999).
Since we did not have gold standard annotations
for any of the real-world data we evaluated on, we
instead compared the three taggers along two dimen-
sions:
? Agreement on classification. How well do
the taggers work on the three most diffi-
cult classes: PERSON, ORGANIZATION, and
LOCATION and, more importantly, to what
extent does one tagger agree with another?
What types of mistakes do they make system-
atically?1
1Although one could draw a distinction between named en-
tity identification and classification, we focus on the final output
of the taggers, i.e., classified named entities.
? Ambiguity in discourse. Although entities
can potentially have more than one entity clas-
sification, such as Clinton (PERSON or LO-
CATION), it would be surprising if they co-
occurred in a single discourse unit such as a
document. How frequently does each tagger
produce multiple classifications for the same
entity in a single document?
We first compared the two freely available, aca-
demic taggers (Stanford and LBJ) on a corpus of
425 million words that is used internally at the Ed-
ucational Testing Service. Note that we could not
compare these two taggers to IdentiFinder on this
corpus since IdentiFinder is not available for public
use without a license.
Next, we compared all three taggers on the Amer-
ican National Corpus. The American National Cor-
pus (ANC) has recently released a copy which is
tagged by IdentiFinder.2 Since the ANC is a pub-
licly available corpus, we tagged it using both the
Stanford and LBJ taggers and could then compare
all three taggers along the two intended dimensions.
We found that the public corpus had many of the
same problems as the ones we found with our in-
ternally used corpus. Some of these problems have
been discussed before (Marrero et al, 2009) but not
in sufficient detail.
The following section describes our evaluation of
the Stanford and LBJ taggers on the internal ETS
corpus. Section 4 describes a comparison of all three
taggers on the American National Corpus. Section 5
describes the unit test we propose. In Section 6, we
propose and discuss the viability of the ?one named-
entity tag per discourse? hypothesis. In Section 7,
we highlight the problems we find during our com-
parisons and propose a methodology for improved
intrinsic evaluation for NER. Finally, we conclude
in Section 8.
3 Comparing Stanford and LBJ
In this section, we compare the two academic tag-
gers in terms of classification agreement by class
and discourse ambiguity on the ETS SourceFinder
corpus, a heterogeneous corpus containing approx-
imately 425 million words, and more than 270, 000
2http://www.anc.org/annotations.html
58
Person Organization Location
Stanford LBJ Stanford LBJ Stanford LBJ
Shiloh A.sub.1 RNA Santa Barbara Hebrew The New Republic
Yale What Arnold FIGURE ASCII DNA
Motown Jurassic Park NaCl Number: Tina Mom
Le Monde Auschwitz AARGH OMITTED Jr. Ph.D
Drosophila T. Rex Drosophila Middle Ages Drosophila Drosophila
Table 1: A sampling of false positives for each class as tagged by the Stanford and LBJ taggers
Common Entities Percentage
Person 548,864 58%
Organization 249,888 34%
Location 102,332 37%
Table 2: Agreement rate by class between the Stanford and LBJ taggers
articles. The articles were extracted from a set of
60 different journals, newspapers and magazines fo-
cused on both literary and scientific topics.
Although Named Entity Recognition is reported
in the literature to have an accuracy rate of 85-95%
(Finkel et al, 2005; Ratinov and Roth, 2009), it was
clear by inspection that both the Stanford and the
LBJ tagger made a number of mistakes. The ETS
corpus begins with an article about Tim Berners-
Lee, the man who created the World Wide Web.
At the beginning of the article, ?Tim? as well as
?Berners-Lee? are correctly tagged by the Stanford
tagger as belonging to the PERSON class. But
later in the same article, ?Berners-Lee? is incorrectly
tagged as ORGANIZATION. The LBJ tagger makes
many mistakes as well, but they are not necessarily
the same mistakes as the mistakes made by the Stan-
ford tagger. For example, the LBJ tagger sometimes
classifies ?The Web? as a PERSON, and the Stan-
ford tagger classifies ?Italian? as a LOCATION.3
Table 1 provides an anecdotal list of the ?entities?
that were misclassified by the two taggers.4
Both taggers produced about the same number
of entities overall: 1.95 million for Stanford, and
3?Italian? is classified primarily as MISC by the LBJ tagger.
These terms are sometimes called Gentilics or Demonyms.
4Both taggers can use a fourth class MISC in addition to
the standard entity classes PERSON, ORGANIZATION, and
LOCATION. We ran Stanford without the MISC class and LBJ
with MISC. However, the problems highlighted in this paper
remain equally prevalent even without this discrepancy.
1.8 million for LBJ. The agreement rate between
the taggers is shown in Table 2. We find that the
highest rate of agreement is for PERSONS, with
an agreement rate of 58%. The agreement rate on
LOCATIONS is 37%, and the agreement rate on
ORGANIZATIONS is 34%. Even on cases where
the taggers agree, the classification can be incorrect.
Both taggers classify ?African Americans? as LO-
CATIONS.5 Both treat ?Jr.? as being part of a per-
son?s name, as well as being a LOCATION (in fact,
the tagging of ?Jr.? as a LOCATION is more fre-
quent in both).
For our second evaluation criterion, i.e., within-
discourse ambiguity, we determined the percent-
age of globally ambiguous entities (entities that had
more than one classification across the entire corpus)
that occurred with multiple taggings within a single
document. This analysis showed that the problems
described above are not anecdotal. Table 3 shows
that at least 40% of the entities that have more than
one classification co-occur within a document. This
is true for both taggers and all of the named entity
classes.6
5The LBJ tagger classifies the majority of instances of
?African American? as MISC.
6The LBJ tagger also includes the class MISC. We looked at
the co-occurrence rate between the different classes and MISC,
and we found that the majority of each group co-occurred within
a document there as well.
59
Stanford LBJ
Overlap Co-occurrence Overlap Co-occurrence
Person-Organization 98,776 40% 58,574 68%
Person-Location 72,296 62% 55,376 69%
Organization-Location 80,337 45% 64,399 63%
Table 3: Co-occurrence rates between entities with more than one tag for Stanford and LBJ taggers
Stanford-BBN LBJ-BBN
Common Entities Percentage Common Entities Percentage
Person 8034 28% 27,687 53%
Organization 12533 50% 21,777 51%
Location(GPE) 3289 28% 5475 47%
Table 4: Agreement rate by class between the Stanford (and LBJ) and BBN IdentiFinder taggers on the ANC Corpus
4 Comparing All 3 Taggers
A copy of the American National Corpus was re-
cently released with a tagging by IdentiFinder. We
tagged the corpus with the Stanford and LBJ tagger
to see how the results compared.
We found many of the same problems with the
American National Corpus as we found with the
SourceFinder corpus used in the previous section.
The taggers performed very well for entities that
were common in each class, but we found misclas-
sifications even for terms at the head of the Zipfian
curve. Terms such as ?Drosophila? and ?RNA? were
classified as a LOCATION. ?Affymetrix? was clas-
sified as a PERSON, LOCATION, and ORGANI-
ZATION.
Table 4 shows the agreement rate between the
Stanford and IdentiFinder taggers as well as that be-
tween the LBJ and IdentiFinder taggers. A sample
of terms that were classified as belonging to more
than one class, across all 3 taggers, is given in Table
5.
All taggers differ in how the entities are tok-
enized. The Stanford tagger tags each component
word of the multi-word expressions separately. For
example, ?John Smith? is tagged as John/PERSON
and Smith/PERSON. But it would be tagged as
[PER John Smith] by the LBJ tagger, and similarly
by IdentiFinder. This results in a higher overlap be-
tween classes in general, and there is a greater agree-
ment rate between LBJ and IdentiFinder than be-
tween Stanford and either one.
The taggers also differ in the number of entities
that are recognized overall, and the percentage that
are classified in each category. IdentiFinder recog-
nizes significantly more ORGANIZATION entities
than Stanford and LBJ. IdentiFinder also uses a GPE
(Geo-Political Entity) category that is not found in
the other two. This splits the LOCATION class. We
found that many of the entities that were classified as
LOCATION by the other two taggers were classified
as GPE by IdentiFinder.
Although the taggers differ in tokenization as well
as categories, the results on ambiguity in a discourse
support our findings on the larger corpus. The re-
sults are shown in Table 6. For both the Stanford and
LBJ tagger, between 42% and 58% of the entities
with more than one classification co-occur within a
document. For IdentiFinder, the co-occurrence rate
was high for two of the groupings, but significantly
less for PERSON and GPE.
5 Unit Test for NER
We created a unit test based on our experiences in
comparing the different taggers. We were particular
about choosing examples that test the following:
1. Capitalized, upper case, and lower case ver-
sions of entities that are true positives for PER-
SON, ORGANIZATION, and LOCATION (for
a variety of frequency ranges).
2. Terms that are entirely in upper case that are not
named entities (such as RNA and AAARGH).
60
Person/Organization Person/Location Organization/Location
Bacillus Bacillus Affymetrix
Michelob Aristotle Arp2/3
Phenylsepharose ArrayOligoSelector ANOVA
Synagogue Auschwitz Godzilla
Transactionalism Btk:ER Macbeth
Table 5: A sampling of terms that were tagged as belonging to more than one class in the American National Corpus
Stanford LBJ IdentiFinder
Overlap Co-occurrence Overlap Co-occurrence Overlap Co-occurrence
Person-Org 5738 53% 2311 58% 8379 57%
Person-Loc(GPE) 4126 58% 3283 43% 2412 22%
Org-Loc(GPE) 5109 57% 4592 50% 4093 60%
Table 6: Co-occurrence rates between entities with more than one tag for the American National Corpus
3. Terms that contain punctuation marks such as
hyphens, and expressions (such as ?A.sub.1?)
that are clearly not named entities.
4. Terms that contain an initial, such as ?T. Rex?,
?M.I.T?, and ?L.B.J.?
5. Acronym forms such as ETS and MIT, some
with an expanded form and some without.
6. Last names that appear in close proximity to the
full name (first and last). This is to check on the
impact of discourse and consistency of tagging.
7. Terms that contain a preposition, such as ?Mas-
sachusetts Institute of Technology?. This is in-
tended to test for correct extent in identifying
the entity.
8. Terms that are a part of a location as well as an
organization. For example, ?Amherst, MA? vs.
?Amherst College?.
An excerpt from this unit test is shown in Table 7.
We provide more information about the full unit test
at the end of the paper.
6 One Named-Entity Tag per Discourse
Previous papers have noted that it would be unusual
for multiple occurrences of a token in a document to
be classified as a different type of entity (Mikheev
et al, 1999; Curran and Clark, 2003). The Stan-
ford and LBJ taggers have features for non-local de-
pendencies for this reason. The observation is sim-
ilar to a hypothesis proposed by Gale, Church, and
Yarowsky with respect to word-sense disambigua-
tion and discourse (Gale et al, 1992). They hypoth-
esized that when an ambiguous word appears in a
document, all subsequent instances of that word in
the document will have the same sense. This hy-
pothesis is incorrect for word senses that we find in
a dictionary (Krovetz, 1998) but is likely to be cor-
rect for the subset of the senses that are homony-
mous (unrelated in meaning). Ambiguity between
named entities is similar to homonymy, and for most
entities it is unlikely that they would co-occur in a
document.7 However, there are cases that are excep-
tions. For example, Finkel et al (2005) note that in
the CoNLL dataset, the same term can be used for a
location and for the name of a sports team. Ratinov
and Roth (2009) note that ?Australia? (LOCATION)
can occur in the same document as ?Bank of Aus-
tralia? (ORGANIZATION).
Existing taggers treat the non-local dependencies
as a way of dealing with the sparse data problem,
and as a way to resolve tagging differences by look-
ing at how often one token is classified as one type
7Krovetz (1998) provides some examples where different
named entities co-occur in a discourse, such as ?New York?
(city) and ?New York? (state). However, these are both in the
same class (LOCATION) and are related to each other.
61
This is not a Unit Test
(a tribute to Rene Magritte and RMS)
Although we created this test with humor, we intend it as a serious
test of the phenomena we encountered. These problems include
ambiguity between entities (such as Bill Clinton and Clinton,
Michigan), uneven treatment of variant forms (MIT, M.I.T., and
Massachusetts Institute of Technology - these should all be
labeled the same in this text - are they?), and frequent false
positives such as RNA and T. Rex.
...
Table 7: Excerpt from a Unit test for Named-Entity Recognition
versus another. We propose that these dependencies
can be used in two other aspects: (a) as a source
of error in evaluation and, (b) as a way to identify
semantically related entities that are systematic ex-
ceptions. There is a grammar to named entity types.
?Bank of Australia? is a special case of Bank of
[LOCATION]. The same thing is true for ?China
Daily? as a name for a newspaper. We propose that
co-occurrences of different labels for particular in-
stances can be used to create such a grammar; at the
very least, particular types of co-occurrences should
be treated as an exception to what is otherwise an
indication of a tagging mistake.
7 Discussion
The Message Understanding Conference (MUC) has
guidelines for named-entity recognition. But the
guidelines are just that. We believe that there should
be standards. Without such standards it is difficult
to determine which tagger is correct, and how the
accuracy varies between the classes.
We propose that the community focus on four
classes: PERSON, ORGANIZATION, LOCA-
TION, and MISC. This does not mean that the other
classes are not important. Rather it is recognition of
the following facts:
? These classes are more difficult than dates,
times, and currencies.
? There is widespread disagreement between tag-
gers on these classes, and evidence that they are
misclassifying unique entities a significant per-
centage of the time.
? We need at least one class for handling terms
that do not fit into the first three classes.
? The first three classes have important value in
other areas of NLP.
Although we recognize that an extrinsic evalu-
ation of named entity recognition would be ideal,
we also realize that intrinsic evaluations are valu-
able in their own right. We propose that the exist-
ing methodology for intrinsically evaluating named
entity taggers can be improved in the following man-
ner:
1. Create test sets that are organized across a va-
riety of domains. It is not enough to work with
newswire and biomedical text.
2. Use standardized sets that are designed to test
different types of linguistic phenomena, and
make it a de facto norm to use more than one
set as part of an evaluation.
3. Report accuracy rates separately for the three
major classes. Accuracy rates should be further
broken down according to the items in the unit
test that are designed to assess mistakes: or-
thography, acronym processing, frequent false
positives, and knowledge-based classification.
4. Establish a way for a tagging system to express
uncertainty about a classification.
62
The approach taken by the American National
Corpus is a good step in the right direction. Like
the original Brown Corpus and the British National
Corpus, it breaks text down according to informa-
tional/literary text types, and spoken versus written
text. The corpus also includes text that is drawn from
the literature of science and medicine. However, the
relatively small number of files in the corpus makes
it difficult to assess accuracy rates on the basis of re-
peated occurrences within a document, but with dif-
ferent tags. Because there are hundreds of thousands
of files in the internal ETS corpus, there are many
opportunities for observations. The tagged version
of the American National Corpus has about 8800
files. This is one of the biggest differences between
the evaluation on the corpus we used internally at
ETS and the American National Corpus.
The use of a MISC class is needed for reasons
that are independent of certainty. This is why we
propose a goal of allowing systems to express this
aspect of the classification. We suggest a meta-tag of
a question-mark. The meta-tag can be applied to any
class. Entities for which the system is uncertain can
then be routed for active learning. This also allows a
basic separation of entities into those for which the
system is confident of its classification, and those for
which it is not.
8 Conclusion
Although Named Entity Recognition has a reported
accuracy rate of more than 90%, the results show
they make a significant number of mistakes. The
high accuracy rates are based on inadequate meth-
ods for testing performance. By considering only
the entities where both taggers agree on the classifi-
cation, it is likely that we can obtain improved accu-
racy. But even so, there are cases where both taggers
agree yet the agreement is on an incorrect tagging.
The unit test for assessing NER performance is
freely available to download.8
As with Information Retrieval test collections, we
hope that this becomes one of many, and that they be
adopted as a standard for evaluating performance.
8http://bit.ly/nertest
Acknowledgments
This work has been supported by the Institute
for Education Sciences under grant IES PR/Award
Number R305A080647. We are grateful to Michael
Flor, Jill Burstein, and anonymous reviewers for
their comments.
References
Daniel M. Bikel, Richard M. Schwartz, and Ralph M.
Weischedel. 1999. An Algorithm that Learns What?s
in a Name. Machine Learning, 34:211?231.
Xavier Carreras, Llus Mrquez, and Llus Padr. 2003.
Named entity recognition for Catalan using Spanish
resources. In Proceedings of EACL.
James R. Curran and Stephen Clark. 2003. Language
Independent NER using a Maximum Entropy Tagger.
In Proceeding of the 7th Conference on Computational
Natural Language Learning (CoNLL), pages 164?167.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured Generative Models for Unsupervised
Named-Entity Clustering. In Proceedings of NAACL,
pages 164?172.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of ACL, pages 363?370.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One Sense per Discourse. In Pro-
ceedings of the ARPA Workshop on Speech and Natu-
ral Language Processing, pages 233?237.
Robert Krovetz. 1998. More than One Sense Per Dis-
course. In Proceedings of the ACL-SIGLEX Work-
shop: SENSEVAL-1.
Monica Marrero, Sonia Sanchez-Cuadrado, Jorge Morato
Lara, and George Andreadakis. 2009. Evaluation of
Named Entity Extraction Systems. Advances in Com-
putational Linguistics, Research in Computing Sci-
ence, 41:47?58.
Fien De Meulder, V Eronique Hoste, and Walter Daele-
mans. 2002. A Named Entity Recognition System for
Dutch. In Computational Linguistics in the Nether-
lands, pages 77?88.
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named Entity Recognition Without Gazetteers. In
Proceedings of EACL, pages 1?8.
David Nadeau, Peter D. Turney, and Stan Matwin. 2006.
Unsupervised Named-Entity Recognition: Generating
Gazetteers and Resolving Ambiguity. In Proceedings
of the Canadian Conference on Artificial Intelligence,
pages 266?277.
63
L. Ratinov and D. Roth. 2009. Design Challenges
and Misconceptions in Named Entity Recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
147?155.
Marc Rossler. 2004. Adapting an NER-System for Ger-
man to the Biomedical Domain. In Proceedings of
the International Joint Workshop on Natural Language
Processing in Biomedicine and its Applications, pages
92?95.
64
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108?115,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
E-rating Machine Translation
Kristen Parton1 Joel Tetreault2 Nitin Madnani2 Martin Chodorow3
1Columbia University, NY, USA
kristen@cs.columbia.edu
2Educational Testing Service, Princeton, NJ, USA
{jtetreault, nmadnani}@ets.org
3Hunter College of CUNY, New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
We describe our submissions to the WMT11
shared MT evaluation task: MTeRater and
MTeRater-Plus. Both are machine-learned
metrics that use features from e-rater R?, an au-
tomated essay scoring engine designed to as-
sess writing proficiency. Despite using only
features from e-rater and without comparing
to translations, MTeRater achieves a sentence-
level correlation with human rankings equiva-
lent to BLEU. Since MTeRater only assesses
fluency, we build a meta-metric, MTeRater-
Plus, that incorporates adequacy by combin-
ing MTeRater with other MT evaluation met-
rics and heuristics. This meta-metric has a
higher correlation with human rankings than
either MTeRater or individual MT metrics
alone. However, we also find that e-rater fea-
tures may not have significant impact on cor-
relation in every case.
1 Introduction
The evaluation of machine translation (MT) systems
has received significant interest over the last decade
primarily because of the concurrent rising interest in
statistical machine translation. The majority of re-
search on evaluating translation quality has focused
on metrics that compare translation hypotheses to a
set of human-authored reference translations. How-
ever, there has also been some work on methods that
are not dependent on human-authored translations.
One subset of such methods is task-based in that
the methods determine the quality of a translation in
terms of how well it serves the need of an extrin-
sic task. These tasks can either be downstream NLP
tasks such as information extraction (Parton et al,
2009) and information retrieval (Fujii et al, 2009) or
human tasks such as answering questions on a read-
ing comprehension test (Jones et al, 2007).
Besides extrinsic evaluation, there is another set
of methods that attempt to ?learn? what makes a
good translation and then predict the quality of new
translations without comparing to reference trans-
lations. Corston-Oliver et al (2001) proposed the
idea of building a decision tree classifier to sim-
ply distinguish between machine and human transla-
tions using language model (LM) and syntactic fea-
tures. Kulesza and Shieber (2004) attempt the same
task using an support vector machine (SVM) classi-
fier and features derived from reference-based MT
metrics such as WER, PER, BLEU and NIST. They
also claim that the confidence score for the classi-
fier being used, if available, may be taken as an es-
timate of translation quality. Quirk (2004) took a
different approach and examined whether it is pos-
sible to explicitly compute a confidence measure for
each translated sentence by using features derived
from both the source and target language sides. Al-
brecht and Hwa (2007a) expanded on this idea and
conducted a larger scale study to show the viabil-
ity of regression as a sentence-level metric of MT
quality. They used features derived from several
other reference-driven MT metrics. In other work
(Albrecht and Hwa, 2007b), they showed that one
could substitute translations from other MT systems
for human-authored reference translations and de-
rive the regression features from them.
Gamon et al (2005) build a classifier to distin-
guish machine-generated translations from human
108
ones using fluency-based features and show that by
combining the scores of this classifier with LM per-
plexities, they obtain an MT metric that has good
correlation with human judgments but not better
than the baseline BLEU metric.
The fundamental questions that inspired our pro-
posed metrics are as follows:
? Can an operational English-proficiency mea-
surement system, built with absolutely no fore-
thought of using it for evaluation of translation
quality, actually be used for this purpose?
? Obviously, such a system can only assess the
fluency of a translation hypothesis and not the
adequacy. Can the features derived from this
system then be combined with metrics such
as BLEU, METEOR or TERp?measures of
adequacy?to yield a metric that performs bet-
ter?
The first metric we propose (MTeRater) is an
SVM ranking model that uses features derived from
the ETS e-rater R? system to assess fluency of trans-
lation hypotheses. Our second metric (MTeRater-
Plus) is a meta-metric that combines MTeRater fea-
tures with metrics such as BLEU, METEOR and
TERp as well as features inspired by other MT met-
rics.
Although our work is intimately related to some
of the work cited above in that it is a trained regres-
sion model predicting translation quality at the sen-
tence level, there are two important differences:
1. We do not use any human translations ? ref-
erence or otherwise ? for MTeRater, not even
when training the metric. The classifier is
trained using human judgments of translation
quality provided as part of the shared evalua-
tion task.
2. Most of the previous approaches use feature
sets that are designed to capture both transla-
tion adequacy and fluency. However, MTeRater
uses only fluency-based features.
The next section provides some background on
the e-rater system. Section 3 presents a discussion
of the differences between MT errors and learner er-
rors. Section 4 describes how we use e-rater to build
our metrics. Section 5 outlines our experiments and
Section 5 discusses the results of these experiments.
Finally, we conclude in Section 6.
2 E-rater
E-rater is a proprietary automated essay scoring
system developed by Educational Testing Service
(ETS) to assess writing quality.1 The system has
been used operationally for over 10 years in high-
stakes exams such as the GRE and TOEFL given
its speed, reliability and high agreement with human
raters.
E-rater combines 8 main features using linear re-
gression to produce a numerical score for an es-
say. These features are grammar, usage, mechan-
ics, style, organization, development, lexical com-
plexity and vocabulary usage. The grammar feature
covers errors such as sentence fragments, verb form
errors and pronoun errors (Chodorow and Leacock,
2000). The usage feature detects errors related to
articles (Han et al, 2006), prepositions (Tetreault
and Chodorow, 2008) and collocations (Futagi et al,
2008). The mechanics feature checks for spelling,
punctuation and capitalization errors. The style fea-
ture checks for passive constructions and word rep-
etition, among others. Organization and develop-
ment tabulate the presence or absence of discourse
elements and the length of each element. Finally,
the lexical complexity feature details how complex
the writer?s words are based on frequency indices
and writing scales, and the vocabulary feature eval-
uates how appropriate the words are for the given
topic). Since many of the features are essay-specific,
there is certainly some mismatch between what e-
rater was intended for and the genres we are using it
for in this experiment (translated news articles).
In our work, we separate e-rater features into two
classes: sentence level and document level. The
sentence level features consist of all errors marked
by the various features for each sentence alone. In
contrast, the document level features are an aggre-
gation of the sentence level features for the entire
document.
1A detailed description of e-rater is outside the scope of this
paper and the reader is referred to (Attali and Burstein, 2006).
109
3 Learner Errors vs. MT Errors
Since e-rater is trained on human-written text and
designed to look for errors in usage that are com-
mon to humans, one research question is whether it
is even useful for assessing the fluency of machine
translated text. E-rater is unaware of the transla-
tion context, so it does not look for common MT
errors, such as untranslated words, mistranslations
and deleted content words. However, these may get
flagged as other types of learner errors: spelling mis-
takes, confused words, and sentence fragments.
Machine translations do contain learner-like mis-
takes in verb conjugations and word order. In an
error analysis of SMT output, Vilar et al (2006) re-
port that 9.9% - 11.7% of errors made by a Spanish-
English SMT system were incorrect word forms, in-
cluding incorrect tense, person or number. These
error types are also account for roughly 14% of er-
rors made by ESL (English as a Second Language)
writers in the Cambridge Learner Corpus (Leacock
et al, 2010).
On the other hand, some learner mistakes are un-
likely to be made by MT systems. The Spanish-
English SMT system made almost no mistakes in
idioms (Vilar et al, 2006). Idiomatic expressions
are strongly preferred by language models, but may
be difficult for learners to memorize (?kicked a
bucket?). Preposition usage is a common problem
in non-native English text, accounting for 29% of
errors made by intermediate to advanced ESL stu-
dents (Bitchener et al, 2005) but language models
are less likely to prefer local preposition errors e.g.,
?he went to outside?. On the other hand, a language
model will likely not prevent errors in prepositions
(or in other error types) that rely on long-distance
dependencies.
4 E-rating Machine Translation
The MTeRater metric uses only features from e-rater
to score translations. The features are produced di-
rectly from the MT output, with no comparison to
reference translations, unlike most MT evaluation
metrics (such as BLEU, TERp and METEOR).
An obvious deficit of MTeRater is a measure of
adequacy, or how much meaning in the source sen-
tence is expressed in the translation. E-rater was
not developed for assessing translations, and the
MTeRater metric never compares the translation to
the source sentence. To remedy this, we propose
the MTeRater-Plus meta-metric that uses e-rater fea-
tures plus all of the hybrid features described below.
Both metrics were trained on the same data using
the same machine learning model, and differ only in
their feature sets.
4.1 E-rater Features
Each sentence is associated with an e-rater sentence-
level vector and a document-level vector as previ-
ously described and each column in these vectors
was used a feature.
4.2 Features for Hybrid Models
We used existing automatic MT metrics as baselines
in our evaluation, and also as features in our hybrid
metric. The metrics we used were:
1. BLEU (Papineni et al, 2002): Case-insensitive
and case-sensitive BLEU scores were pro-
duced using mteval-v13a.pl, which calculates
smoothed sentence-level scores.
2. TERp (Snover et al, 2009): Translation Edit
Rate plus (TERp) scores were produced using
terp v1. The scores were case-insensitive and
edit costs from Snover et al (2009) were used
to produce scores tuned for fluency and ade-
quacy.
3. METEOR (Lavie and Denkowski, 2009): Me-
teor scores were produced using Meteor-next
v1.2. All types of matches were allowed (ex-
act, stem, synonym and paraphrase) and scores
tuned specifically to rank, HTER and adequacy
were produced using the ?-t? flag in the tool.
We also implemented features closely related to
or inspired by other MT metrics. The set of these
auxiliary features is referred to as ?Aux?.
1. Character-level statistics: Based on the suc-
cess of the i-letter-BLEU and i-letter-recall
metrics from WMT10 (Callison-Burch et al,
2010), we added the harmonic mean of preci-
sion (or recall) for character n-grams (from 1
to 10) as features.
110
2. Raw n-gram matches: We calculated the pre-
cision and precision for word n-grams (up to
n=6) and added each as a separate feature (for
a total of 12). Although these statistics are also
calculated as part of the MT metrics above,
breaking them into separate features gives the
model more information.
3. Length ratios: The ratio between the lengths
of the MT output and the reference translation
was calculated on a character level and a word
level. These ratios were also calculated be-
tween the MT output and the source sentence.
4. OOV heuristic: The percentage of tokens in
the MT that match the source sentence. This
is a low-precision heuristic for counting out of
vocabulary (OOV) words, since it also counts
named entities and words that happen to be the
same in different languages.
4.3 Ranking Model
Following (Duh, 2008), we represent sentence-level
MT evaluation as a ranking problem. For a partic-
ular source sentence, there are N machine transla-
tions and one reference translation. A feature vector
is extracted from each {source, reference, MT} tu-
ple. The training data consists of sets of translations
that have been annotated with relative ranks. Dur-
ing training, all ranked sets are converted to sets of
feature vectors, where the label for each feature vec-
tor is the rank. The ranking model is a linear SVM
that predicts a relative score for each feature vector,
and is implemented by SVM-rank (Joachims, 2006).
When the trained classifier is applied to a set of N
translations for a new source sentence, the transla-
tions can then be ranked by sorting the SVM scores.
5 Experiments
All experiments were run using data from three
years of previous WMT shared tasks (WMT08,
WMT09 and WMT10). In these evaluations, anno-
tators were asked to rank 3-5 translation hypothe-
ses (with ties allowed), given a source sentence and
a reference translation, although they were only re-
quired to be fluent in the target language.
Since e-rater was developed to rate English sen-
tences only, we only evaluated tasks with English
as the target language. All years included source
languages French, Spanish, German and Czech.
WMT08 and WMT09 also included Hungarian and
multisource English. The number of MT systems
was different for each language pair and year, from
as few as 2 systems (WMT08 Hungarian-English) to
as many as 25 systems (WMT10 German-English).
All years had a newswire testset, which was divided
into stories. WMT08 had testsets in two additional
genres, which were not split into documents.
All translations were pre-processed and run
through e-rater. Each document was treated as an es-
say, although news articles are generally longer than
essays. Testsets that were not already divided into
documents were split into pseudo-documents of 20
contiguous sentences or less. Missing end of sen-
tence markers were added so that e-rater would not
merge neighboring sentences.
6 Results
For assessing our metrics prior to WMT11, we
trained on WMT08 and WMT09 and tested on
WMT10. The metrics we submitted to WMT11
were trained on all three years. One criticism of
machine-learned evaluation metrics is that they may
be too closely tuned to a few MT systems, and thus
not generalize well as MT systems evolve or when
judging new sets of systems. In this experiment,
WMT08 has 59 MT systems, WMT09 has 70 dif-
ferent MT systems, and WMT10 has 75 different
systems. Different systems participate each year,
and those that participate for multiple years often
improve from year to year. By training and test-
ing across years rather than within years, we hope
to avoid overfitting.
To evaluate, we measure correlation between each
metric and the human annotated rankings according
to (Callison-Burch et al, 2010): Kendall?s tau is cal-
culated for each language pair and the results are
averaged across language pairs. This is preferable
to averaging across all judgments because the num-
ber of systems and the number of judgments vary
based on the language pair (e.g., there were 7,911
ranked pairs for 14 Spanish-English systems, and
3,575 ranked pairs for 12 Czech-English systems).
It is difficult to calculate the statistical signifi-
cance of Kendall?s tau on these data. Unlike the
111
Source language cz de es fr avg
Individual Metrics & Baselines
MTeRater .32 .31 .19 .23 .26
bleu-case .26 .27 .28 .22 .26
meteor-rank .33 .36 .33 .27 .32
TERp-fluency .30 .36 .28 .28 .30
Meta-Metric & Baseline
BMT+Aux+MTeRater .38 .42 .37 .38 .39
BMT .35 .40 .35 .34 .36
Additional Meta-Metrics
BMT+LM .36 .41 .36 .36 .37
BMT+MTeRater .38 .42 .36 .38 .38
BMT+Aux .38 .41 .38 .37 .39
BMT+Aux+LM .39 .42 .38 .36 .39
Table 1: Kendall?s tau correlation with human rankings.
BMT includes bleu, meteor and TERp; Aux includes aux-
iliary features. BMT+Aux+MTeRater is MTeRater-Plus.
Metrics MATR annotations (Przybocki et al, 2009),
(Peterson and Przybocki, 2010), the WMT judg-
ments do not give a full ranking over all systems for
all judged sentences. Furthermore, the 95% confi-
dence intervals of Kendall?s tau are known to be very
large (Carterette, 2009) ? in Metrics MATR 2010,
the top 7 metrics in the paired-preference single-
reference into-English track were within the same
confidence interval.
To compare metrics, we use McNemar?s test
of paired proportions (Siegel and Castellan, 1988)
which is more powerful than tests of independent
proportions, such as the chi-square test for indepen-
dent samples.2 As in Kendall?s tau, each metric?s
relative ranking of a translation pair is compared to
that of a human. Two metrics, A and B, are com-
pared by counting the number of times both A and B
agree with the human ranking, the number of times
A disagrees but B agrees, the number of times A
agrees but B disagrees, and the number of times both
A and B disagree. These counts can be arranged in
a 2 x 2 contingency table as shown below.
A agrees A disagrees
B agrees a b
B disagrees c d
McNemar?s test determines if the cases of mis-
match in agreement between the metrics (cells b and
c) are symmetric or if there is a significant difference
2See http://faculty.vassar.edu/lowry/propcorr.html for an ex-
cellent description.
in favor of one of the metrics showing more agree-
ment with the human than the other. The two-tailed
probability for McNemar?s test can be calculated us-
ing the binomial distribution over cells b and c.
6.1 Reference-Free Evaluation with MTeRater
The first group of rows in Table 1 shows the
Kendall?s tau correlation with human rankings of
MTeRater and the best-performing version of the
three standard MT metrics. Even though MTeR-
ater is blind to the MT context and does not use the
source or references at all, MTeRater?s correlation
with human judgments is the same as case-sensitive
bleu (bleu-case). This indicates that a metric trained
to assess English proficiency in non-native speakers
is applicable to machine translated text.
6.2 Meta-Metrics
The second group in Table 1 shows the cor-
relations of our second metric, MTeRater-Plus
(BMT+Aux+MTeRater), and a baseline meta-metric
(BMT) that combined BLEU, METEOR and TERp.
MTeRater-Plus performs significantly better than
BMT, according to McNemar?s test.
We also wanted to determine whether the e-
rater features have any significant impact when used
as part of meta-metrics. To this end, we first
created two variants of MTeRater-Plus: one that
removed the MTeRater features (BMT+Aux) and
another that replaced the MTeRater features with
the LM likelihood and perplexity of the sentence
(BMT+Aux+LM).3 Both models perform as well
as MTeRater-Plus, i.e., adding additional fluency
features (either LM scores or MTeRater) to the
BMT+Aux meta-metric has no significant impact.
To determine whether this was generally the case,
we also created two variants of the BMT baseline
meta-metric that added fluency features to it: one in
the form of LM scores (BMT+LM) and another in
the form of the MTeRater score (BMT+MTeRater).
Based on McNemar?s test, both models are sig-
nificantly better than BMT, indicating that these
reference-free fluency features indeed capture an as-
pect of translation quality that is absent from the
standard MT metrics. However, there is no signfi-
cant difference between the two variants of BMT.
3The LM was trained on English Gigaword 3.0, and was
provided by WMT10 organizers.
112
1) Ref: Gordon Brown has discovered yet another hole to fall into; his way out of it remains the same
MT+: Gordon Brown discovered a new hole in which to sink; even if it resigned, the position would not change.
Errors: None marked
MT-: Gordon Brown has discovered a new hole in which could, Even if it demissionnait, the situation does not change not.
Errors: Double negative, spelling, preposition
2) Ref: Jancura announced this in the Twenty Minutes programme on Radiozurnal.
MT+: Jancura said in twenty minutes Radiozurnal. Errors: Spelling
MT-: He said that in twenty minutes. Errors: none marked
Table 2: Translation pairs ranked correctly by MTeRater but not bleu-case (1) and vice versa (2).
6.3 Discussion
Table 2 shows two pairs of ranked translations (MT+
is better than MT-), along with some of the errors de-
tected by e-rater. In pair 1, the lower-ranked trans-
lation has major problems in fluency as detected by
e-rater, but due to n-gram overlap with the reference,
bleu-case ranks it higher. In pair 2, MT- is more
fluent but missing two named entities and bleu-case
correctly ranks it lower.
One disadvantage of machine-learned metrics is
that it is not always clear which features caused one
translation to be ranked higher than another. We
did a feature ablation study for MTeRater which
showed that document-level collocation features sig-
nificantly improve the metric, as do features for
sentence-level preposition errors. Discourse-level
features were harmful to MT evaluation. This is un-
surprising, since MT sentences are judged one at a
time, so any discourse context is lost.
Overall, a metric with only document-level fea-
tures does better than one with only sentence-level
features due to data sparsity ? many sentences have
no errors, and we conjecture that the document-level
features are a proxy for the quality of the MT sys-
tem. Combining both document-level and sentence-
level e-rater features does significantly better than
either alone. Incorporating document-level features
into sentence-level evaluation had one unforeseen
effect: two identical translations can get different
scores depending on how the rest of the document
is translated. While using features that indicate the
relative quality of MT systems can improve overall
correlation, it fails when the sentence-level signal is
not strong enough to overcome the prior belief.
7 Conclusion
We described our submissions to the WMT11 shared
evaluation task: MTeRater and MTeRater-Plus.
MTeRater is a fluency-based metric that uses fea-
tures from ETS?s operational English-proficiency
measurement system (e-rater) to predict the qual-
ity of any translated sentence. MTeRater-Plus is a
meta-metric that combines MTeRater?s fluency-only
features with standard MT evaluation metrics and
heuristics. Both metrics are machine-learned mod-
els trained to rank new translations based on existing
human judgments of translation.
Our experiments showed that MTeRater, by it-
self, achieves a sentence-level correlation as high as
BLEU, despite not using reference translations. In
addition, the meta-metric MTeRater-Plus achieves
higher correlations than MTeRater, BLEU, ME-
TEOR, TERp as well as a baseline meta-metric com-
bining BLEU, METEOR and TERp (BMT). How-
ever, further analysis showed that the MTeRater
component of MTeRater-Plus does not contribute
significantly to this improved correlation. How-
ever, when added to the BMT baseline meta-metric,
MTeRater does make a significant contribution.
Our results, despite being a mixed bag, clearly
show that a system trained to assess English-
language proficiency can be useful in providing an
indication of translation fluency even outside of the
specific WMT11 evaluation task. We hope that this
work will spur further cross-pollination between the
fields of MT evaluation and grammatical error de-
tection. For example, we would like to explore using
MTeRater for confidence estimation in cases where
reference translations are unavailable, such as task-
oriented MT.
Acknowledgments
The authors wish to thank Slava Andreyev at ETS
for his help in running e-rater. This research was
supported by an NSF Graduate Research Fellowship
for the first author.
113
References
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of
ACL.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of ACL.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
John Bitchener, Stuart Young, and Denise Cameron.
2005. The effect of different types of corrective feed-
back on esl student writing. Journal of Second Lan-
guage Writing.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
WMT ?10, pages 17?53, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Ben Carterette. 2009. On rank correlation and the
distance between rankings. In Proceedings of the
32nd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?09, pages 436?443, New York, NY, USA. ACM.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of the Conference of the North American
Chapter of the Association of Computational Linguis-
tics (NAACL), pages 140?147.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to
the Automatic Evaluation of Machine Translation. In
Proceedings of the 39th Annual Meeting on Associa-
tion for Computational Linguistics, pages 148?155.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, StatMT
?08, pages 191?194, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2009. Evaluating Effects of Ma-
chine Translation Accuracy on Cross-lingual Patent
Retrieval. In Proceedings of SIGIR, pages 674?675.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning, 21:353?367.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT Evaluation Without Refer-
ence Translations: Beyond Language Modeling. In
Proceedings of the European Association for Machine
Translation (EAMT).
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Thorsten Joachims. 2006. Training linear SVMs in linear
time. In ACM SIGKDD International Conference On
Knowledge Discovery and Data Mining (KDD), pages
217?226.
Douglas Jones, Martha Herzog, Hussny Ibrahim, Arvind
Jairam, Wade Shen, Edward Gibson, and Michael
Emonts. 2007. ILR-Based MT Comprehension Test
with Multi-Level Questions. In HLT-NAACL (Short
Papers), pages 77?80.
Alex Kulesza and Stuart M. Shieber. 2004. A Learn-
ing Approach to Improving Sentence-level MT Evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI).
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23:105?115, September.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kristen Parton, Kathleen R. McKeown, Bob Coyne,
Mona T. Diab, Ralph Grishman, Dilek Hakkani-Tu?r,
Mary Harper, Heng Ji, Wei Yun Ma, Adam Meyers,
Sara Stolbach, Ang Sun, Gokhan Tur, Wei Xu, and
Sibel Yaman. 2009. Who, What, When, Where, Why?
Comparing Multiple Approaches to the Cross-Lingual
5W Task. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 423?431.
Kay Peterson and Mark Przybocki. 2010. Nist
2010 metrics for machine translation evalua-
tion (metricsmatr10) official release of results.
http://www.itl.nist.gov/iad/mig/
tests/metricsmatr/2010/results.
114
Mark Przybocki, Kay Peterson, Se?bastien Bronsart, and
Gregory Sanders. 2009. The nist 2008 metrics for ma-
chine translation challenge?overview, methodology,
metrics, and results. Machine Translation, 23:71?103,
September.
Christopher Quirk. 2004. Training a Sentence-level Ma-
chine Translation Confidence Measure. In Proceed-
ings of LREC.
Sidney Siegel and N. John Castellan. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill, 2 edition.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
hter?: exploring different human judgments with a tun-
able mt metric. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, StatMT ?09,
pages 259?268, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING), pages 865?
872.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of machine transla-
tion output. In International Conference on Language
Resources and Evaluation, pages 697?702, Genoa,
Italy, May.
115
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 44?53,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploring Grammatical Error Correction with
Not-So-Crummy Machine Translation?
Nitin Madnani Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
To date, most work in grammatical error cor-
rection has focused on targeting specific er-
ror types. We present a probe study into
whether we can use round-trip translations ob-
tained from Google Translate via 8 different
pivot languages for whole-sentence grammat-
ical error correction. We develop a novel
alignment algorithm for combining multiple
round-trip translations into a lattice using the
TERp machine translation metric. We further
implement six different methods for extract-
ing whole-sentence corrections from the lat-
tice. Our preliminary experiments yield fairly
satisfactory results but leave significant room
for improvement. Most importantly, though,
they make it clear the methods we propose
have strong potential and require further study.
1 Introduction
Given the large and growing number of non-native
English speakers around the world, detecting and
correcting grammatical errors in learner text cur-
rently ranks as one of the most popular educational
NLP applications. Previously published work has
explored the effectiveness of using round-trip ma-
chine translation (translating an English sentence
to some foreign language F, called the pivot, and
then translating the F language sentence back to En-
glish) for correcting preposition errors (Hermet and
De?silets, 2009). In this paper, we present a pilot
study that explores the effectiveness of extending
?cf. Good Applications for Crummy Machine Translation.
Ken Church & Ed Hovy. Machine Translation, 8(4). 1993
this approach to whole-sentence grammatical error
correction.
Specifically, we explore whether using the con-
cept of round-trip machine translation via multi-
ple?rather than single?pivot languages has the po-
tential of correcting most, if not all, grammatical
errors present in a sentence. To do so, we de-
velop a round-trip translation framework using the
Google Translate API. Furthermore, we propose a
novel combination algorithm that can combine the
evidence present in multiple round-trip translations
and increase the likelihood of producing a whole-
sentence correction. Details of our methodology are
presented in ?3 and of the dataset we use in ?4. Since
this work is of an exploratory nature, we conduct a
detailed error analysis and present the results in ?5.
Finally, ?6 summarizes the contributions of this pi-
lot study and provides a discussion of possible future
work.
2 Related Work
To date, most work in grammatical error detection
has focused on targeting specific error types (usu-
ally prepositions or article errors) by using rule-
based methods or statistical machine-learning clas-
sification algorithms, or a combination of the two.
Leacock et al (2010) present a survey of the com-
mon approaches. However, targeted errors such as
preposition and determiner errors are just two of the
many types of grammatical errors present in non-
native writing. One of the anonymous reviewers for
this paper makes the point eloquently: ?Given the
frequent complexity of learner errors, less holistic,
error-type specific approaches are often unable to
44
disentangle compounded errors of style and gram-
mar.? Below we discuss related work that uses ma-
chine translation to address targeted errors and some
recent work that also focused on whole-sentence er-
ror correction.
Brockett et al (2006) use information about mass
noun errors from a Chinese learner corpus to engi-
neer a ?parallel? corpus with sentences containing
mass noun errors on one side and their corrected
counterparts on the other. With this parallel corpus,
the authors use standard statistical machine transla-
tion (SMT) framework to learn a translation (correc-
tion) model which can then be applied to unseen sen-
tences containing mass noun errors. This approach
was able to correct almost 62% of the errors found
in a test set of 150 errors. In our approach, we do not
treat correction directly as a translation problem but
instead rely on an MT system to round-trip translate
an English sentence back to English.
Park and Levy (2011) use a noisy channel model
to achieve whole-sentence grammar correction; they
learn a noise model from a dataset of errorful sen-
tences but do not rely on SMT. They show that the
corrections produced by their model generally have
higher n-gram overlap with human-authored refer-
ence corrections than the original errorful sentences.
The previous work that is most directly rele-
vant to our approach is that of Hermet and De?silets
(2009) who focused only on sentences containing
pre-marked preposition errors and generated a sin-
gle round-trip translation for such sentences via a
single pivot language (French). They then simply
posited this round-trip translation as the ?correc-
tion? for the original sentence. In their evaluation
on sentences containing 133 unique preposition er-
rors, their round-trip translation system was able to
correct 66.4% of the cases. However, this was out-
performed by a simple method based on web counts
(68.7%). They also found that combining the round-
trip method with the web counts method into a hy-
brid system yielded higher performance (82.1%).
In contrast, we use multiple pivot languages to
generate several round-trip translations. In addition,
we use a novel alignment algorithm that allows us to
combine different parts of different round-trip trans-
lations and explore a whole new set of corrections
that go beyond the translations themselves. Finally,
we do not restrict our analysis to any single type of
error. In fact, our test sentences contain several dif-
ferent types of grammatical errors.
Outside of the literature on grammatical error de-
tection, our combination approach is directly related
to the research on machine translation system com-
bination wherein translation hypotheses produced
by different SMT systems are combined to allow the
extraction of a better, combined hypothesis (Ban-
galore et al, 2001; Rosti et al, 2007; Feng et al,
2009). However, our combination approach is dif-
ferent in that all the round-trip translations are pro-
duced by a single system but via different pivot lan-
guages.
Finally, the idea of combining multiple surface
renderings with the same meaning has also been ex-
plored in paraphrase generation. Pang et al (2003)
propose an algorithm to align sets of parallel sen-
tences driven entirely by the syntactic representa-
tions of the sentences. The alignment algorithm out-
puts a merged lattice from which lexical, phrasal,
and sentential paraphrases could simply be read off.
Barzilay and Lee (2003) cluster topically related
sentences into slotted word lattices by using mul-
tiple sequence alignment for the purpose of down-
stream paraphrase generation from comparable cor-
pora. More recently, Zhao et al (2010) perform
round-trip translation of English sentences via dif-
ferent pivot languages and different off-the-shelf
SMT systems to generate candidate paraphrases.
However, they do not combine the candidate para-
phrases in any way. A detailed survey of paraphrase
generation techniques can be found in (Androut-
sopoulos and Malakasiotis, 2010) and (Madnani and
Dorr, 2010).
3 Methodology
The basic idea underlying our error correction tech-
nique is quite simple: if we can automatically gen-
erate alternative surface renderings of the meaning
expressed in the original sentence and then pick the
one that is most fluent, we are likely to have picked
a version of the sentence in which the original gram-
matical errors have been fixed.
In this paper, we propose generating such alter-
native formulations using statistical machine trans-
lation. For example, we take the original sentence E
and translate it to Chinese using the Google Trans-
45
Original Both experience and books are very important about living.
Swedish Both experience and books are very important in live.
Italian Both books are very important experience and life.
Russian And the experience, and a very important book about life.
French Both experience and the books are very important in life.
German Both experience and books are very important about life.
Chinese Related to the life experiences and the books are very important.
Spanish Both experience and the books are very important about life.
Arabic Both experience and books are very important for life.
Figure 1: Illustrating the deficiency in using an n-gram language model to select one of the 8 round-trip translations
as the correction for the Original sentence. The grammatical errors in the Original sentence are shown in italics. The
round-trip translation via Russian is chosen by a 5-gram language model trained on the English gigaword corpus even
though it changes the meaning of the original sentence entirely.
late API. We then take the resulting Chinese sen-
tence C and translate it back to English. Since
the translation process is designed to be meaning-
preserving, the resulting round-trip translation E?
can be seen as an alternative formulation of the orig-
inal sentence E. Furthermore, if additional pivot lan-
guages besides Chinese are used, several alterna-
tive formulations of E can be generated. We use 8
different pivot languages: Arabic, Chinese, Span-
ish, French, Italian, German, Swedish, Russian. We
chose these eight languages since they are frequently
used in SMT research and shared translation tasks.
To obtain the eight round-trip translations via each
of these pivot languages, we use the Google Trans-
late research API.1
3.1 Round-Trip Translation Combination
Once the translations are generated, an obvious so-
lution is to pick the most fluent alternative, e.g.,
using an n-gram language model. However, since
the language model has no incentive to preserve the
meaning of the sentence, it is possible that it might
pick a translation that changes the meaning of the
original sentence entirely. For example, consider
the sentence and its round-trip translations shown
in Figure 1. For this sentence, a 5-gram language
model trained on gigaword picks the Russian round-
trip translation simply because it has n-grams that
were seen more frequently in the English gigaword
corpus.
Given the deficiencies in statistical phrase-based
translation, it is also possible that no single round-
1http://research.google.com/university/
translate/
trip translation fixes all of the errors. Again, con-
sider Figure 1. None of the 8 round-trip transla-
tions is error-free itself. Therefore, the task is more
complex than simply selecting the right round-trip
translation. We posit that a better approach will be
to combine the evidence of correction produced by
each independent translation model and increase the
likelihood of producing a final whole-sentence cor-
rection. Additionally, by engineering such a combi-
nation, we increase the likelihood that the final cor-
rection will preserve the meaning of the original sen-
tence.
In order to combine the round-trip translations,
we developed a heuristic alignment algorithm that
uses the TERp machine translation metric (Snover
et al, 2009). The TERp metric takes a pair of sen-
tences and computes the least number of edit opera-
tions that can be employed to turn one sentence into
the other.2 As a by-product of computing the edit
sequence, TERp produces an alignment between the
two sentences where each alignment link is defined
by an edit operation. Figure 2 shows an example of
the alignment produced by TERp between the orig-
inal sentence from Figure 1 and its Russian round-
trip translation. Note that TERp also allows shifting
words and phrases in the second sentence in order
to obtain a smaller edit cost (as indicated by the as-
terisk next to the word book which has shifted from
its original position in the Russian round-trip trans-
lation).
Our algorithm starts by treating the original sen-
tence as the backbone of a lattice. First, it cre-
2Edit operations in TERp include matches, substitutions, in-
sertion, deletions, paraphrase, synonymy and stemming.
46
ates a node for each word in the original sentence
and creates edges between them with a weight of
1. Then, for each of the round-trip translations, it
computes its TERp alignment with the original sen-
tence and aligns it to the backbone based on the edit
operations in the alignment. Specifically, each in-
sertion, substitution, stemming, synonymy and para-
phrase operation lead to creation of new nodes that
essentially provide an alternative formulation for the
aligned substring from the backbone. Any duplicate
nodes are merged. Finally, edges produced by dif-
ferent translations between the same pairs of nodes
are merged and their weights added. Figure 3(a)
shows how our algorithm aligns the Russian round-
trip translation from Figure 1 to the original sentence
using the TERp alignment from Figure 2. Figure
3(b) shows the final lattice produced by our algo-
rithm for the sentence and all the round-trip transla-
tions from Figure 1.
-- and [I]
both -- the [S]
experience -- experience [M]
-- , [I]
and -- and [M]
books -- book [T] [*]
are -- a [S]
very -- very [M]
important -- important [M]
about -- about [M]
living -- life [Y]
. -- . [M]
Figure 2: The alignment produced by TERp between the
original sentence from Figure 1 and its Russian round-
trip translation. The alignment operations are indicated
in square brackets after each alignment link: I=insertion,
M=match, S=substitution, T=stemming and Y=WordNet
synonymy. The asterisk next to the work book denotes
that TERp chose to shift its position before computing an
edit operation for it.
3.2 Correction Generation
For each original sentence, we computed six possi-
ble corrections from the round-trip translations and
the combined lattice:
1. Baseline LM (B). The most fluent round-trip
translation out of the eight as measured by a
5-gram language model trained on the English
gigaword corpus.
2. Greedy (G). A path is extracted from the TERp
lattice using a greedy best-first strategy at each
node, i.e., at each node, the outgoing edge with
the largest weight is followed.
3. 1-Best (1): The shortest path is extracted
from the TERp lattice by using the OpenFST
toolkit.3. This method assumes that, like G, the
combined evidence from the round-trip trans-
lations itself is enough to produce a good final
correction and no external method for measur-
ing fluency is required.4
4. LM Re-ranked (L). An n-best (n=20) list is
extracted from the lattice using the OpenFST
toolkit and re-ranked using the 5-gram giga-
word language model. The 1-best reranked
item is then extracted as the correction. This
method assumes that an external method
of measuring fluency?the 5-gram language
model?can help to bring the most grammati-
cal correction to the top of the n-best list.
5. Product Re-ranked (P). Same as L except the
re-ranking is done based on the product of the
cost of each hypothesis in the n-best list and
the language model score, i.e., both the evi-
dence from the round-trip translations and the
language model is weighted equally.
6. Full LM Composition (C). The edge weights
in the TERp lattice are converted to probabil-
ities. The lattice is then composed with a tri-
gram finite state language model (trained on
a corpus of 100, 000 high-scoring student es-
says).5 The shortest path through the composed
lattice is then extracted as the correction. This
method assumes that using an n-gram language
model during the actual search process is better
than using it as a post-processing tool on an al-
ready extracted n-best list, such as for L and
P.
3http://www.openfst.org/
4Note that the edge weights in the lattice must be converted
into costs for this method (we do so by multiplying the weights
by ?1).
5We adapted the code available at http://www.
ling.ohio-state.edu/?bromberg/ngramcount/
ngramcount.html to perform the LM composition.
47
bo
th
exp
eri
enc
e
1
and
1 ,
1
bo
ok
s
1
bo
ok
1
are
1
ver
y
1
im
po
rta
nt
2
abo
ut
2
liv
ing
1
life
1
.
1
and
the
1
1
1
a
1
1
1
(a
)
bo
th
exp
eri
enc
e
1
and
1,
k
bo
osv
y
bo
os
k
the
m
are
2
uer
l
2
igp
ort
ant
f
abo
.t

in

ie
 or
k
iui
n
k m

k
iue
kk
k1
and
the
k
k
k
a
k
k
m
rea
ted
to
k
the
k
exp
eri
enc
ev
k
k
k
(b
)
O
ri
gi
na
l(
O
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
vi
ng
.
B
as
el
in
e
L
M
(B
)
A
nd
th
e
ex
pe
ri
en
ce
,a
nd
a
ve
ry
im
po
rt
an
tb
oo
k
ab
ou
tl
if
e.
G
re
ed
y
(G
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
1-
be
st
(1
)
B
ot
h
ex
pe
ri
en
ce
an
d
th
e
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
L
M
R
e-
ra
nk
ed
(L
)
A
nd
th
e
ex
pe
ri
en
ce
an
d
th
e
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ti
n
li
fe
.
P
ro
du
ct
R
e-
ra
nk
ed
(P
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
L
M
C
om
po
si
ti
on
(C
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
t
in
lif
e.
(c
)
F
ig
ur
e
3:
(a
)
sh
ow
s
th
e
ou
tp
ut
of
ou
r
al
ig
nm
en
t
al
go
ri
th
m
fo
r
th
e
R
us
si
an
ro
un
d-
tr
ip
tr
an
sl
at
io
n
fr
om
F
ig
ur
e
1.
(b
)
sh
ow
s
th
e
fi
na
l
T
E
R
p
la
tt
ic
e
af
te
r
al
ig
ni
ng
al
l
ei
gh
tr
ou
nd
-t
ri
p
tr
an
sl
at
io
ns
fr
om
F
ig
ur
e
1.
(c
)
sh
ow
s
th
e
co
rr
ec
ti
on
s
fo
r
th
e
or
ig
in
al
se
nt
en
ce
(O
)
pr
od
uc
ed
by
th
e
si
x
te
ch
ni
qu
es
di
sc
us
se
d
in
3.
2.
T
he
co
rr
ec
ti
on
pr
od
uc
ed
by
th
e
F
ul
lL
M
C
om
po
si
ti
on
te
ch
ni
qu
e
(C
)
fi
xe
s
bo
th
th
e
er
ro
rs
in
th
e
or
ig
in
al
se
nt
en
ce
.
48
No. of Errors Sentences Avg. Length
1 61 14.4
2 45 19.9
3 29 24.2
4 14 29.4
> 4 13 38.0
Table 1: The distribution of grammatical errors for the
162 errorful sentences.
Figure 3(c) shows these six corrections as computed
for the sentence from Figure 1.
4 Corpus
To assess our system, we manually selected 200
sentences from a corpus of essays written by non-
native English speakers for a college-level English
proficiency exam. In addition to sentences contain-
ing grammatical errors, we also deliberately sam-
pled sentences that contained no grammatical errors
in order to determine how our techniques perform
in those cases. In total, 162 of the sentences con-
tained at least one error, and the remaining 38 were
perfectly grammatical. For both errorful as well
as grammatical sentences, we sampled sentences of
different lengths (under 10 words, 10-20 words, 20-
30 words, 30-40 words, and over 40 words). The
162 errorful sentences varied in the number and type
of errors present. Table 1 shows the distribution of
the number of errors across these 162 sentences.
Specifically, the error types found in these sen-
tences included prepositions, articles, punctuation,
agreement, collocations, confused words, etc. Some
only contained a handful of straightforward errors,
such as ?In recent day, transportation is one of the
most important thing to support human activity?,
where day and thing should be pluralized. On the
other hand, others were quite garbled to the point
where it was difficult to understand the meaning,
such as ?Sometimes reading a book is give me in-
formation about the knowledge of life so that I can
prevent future happened but who knows that when it
will happen and how fastly can react to that hap-
pen.? During development, we noticed that the
round-trip translation process could not handle mis-
spelled words, so we manually corrected all spelling
mistakes which did not result in a real word.6
6A total of 82 spelling errors were manually corrected.
5 Evaluation
In order to evaluate the six techniques for generating
corrections, we designed an evaluation task where
the annotators would be shown a correction along
with the original sentence for which it was gener-
ated. Since there are 6 corrections for each of the
200 sentences, this yields a total of 1, 200 units for
pairwise preference judgments. We chose two anno-
tators, both native English speakers, each of whom
annotated half of the judgment units.
Given the idiosyncrasies of the statistical machine
translation process underlying our correction tech-
niques, it is quite possible that:
? A correction may fix some, but not all, of the
grammatical errors present in the original sen-
tence, and
? A correction may be more fluent but might
change the meaning of the original sentence.
? A correction may introduce a new disfluency,
even though other errors in the sentence have
been largely corrected. This is especially likely
to be the case for longer sentences.
Therefore, the pairwise preference judgment task
is non-trivial in that it expects the annotators to con-
sider two dimensions: that of grammaticality and of
meaning. To accommodate these considerations, we
designed the evaluation task such that it asked the
annotators to answer the following two questions:
1. Grammaticality. The annotators were asked
to choose between three options: ?Original
sentence sounds better?, ?Correction sounds
better? and ?Both sound about the same?.
2. Meaning. The annotators were asked to choose
between two options: ?Correction preserves
the original meaning? and ?Correction changes
the original meaning?. It should be noted that
determining change in or preservation of mean-
ing was treated as a very strict judgment. Subtle
changes such as the omission of a determiner
were deemed to change the meaning. In some
cases, the original sentences were too garbled
to determine the original meaning itself.
49
C > O C = O C < O
Meaning = 1 S D F
Meaning = 0 F F F
Table 2: A matrix illustrating the Success-Failure-Draw
evaluation criterion for the 162 errorful sentences. The
rows represent the meaning dimension (1 = meaning pre-
served, 0 = meaning changed) and the columns represent
the grammaticality dimension (C > O denotes correc-
tion being more grammatical than the original, C = O
denotes they are about the same and C < O denotes that
the correction is worse). Such a matrix is computed for
each of the six techniques.
5.1 Effectiveness
First, we concentrate our analysis on the original
sentences which contain at least one grammatical er-
ror. We aggregated the results of the pairwise pref-
erence judgments for each of the six specific correc-
tion generation techniques and applied the strictest
evaluation criterion by computing the following, for
each technique:
? Successes. Only those sentences for which
the correction generated by method is not only
more grammatical but also preserves the mean-
ing.
? Failures. All those sentences for which the cor-
rection is either less grammatical or changes
the original meaning.
? Draws. Those sentences for which the correc-
tion preserves the meaning but sounds about
the same as the original.
Table 2 shows a matrix of the six possible com-
binations of grammaticality and meaning for each
method and demonstrates which cells of the matrix
contribute to which of the above three measures:
Successes (S), Failures (F) and Draws (D).
In addition to the six techniques, we also posit an
oracle in order to determine the upper bound on the
performance of our round-trip translation approach.
The oracle picks the most accurate correction gen-
eration method for each individual sentence out of
the 6 that are available. For sentences where none of
the six techniques produce an adequate correction,
the oracle just picks the original sentence. Table 3
shows how the various techniques (including the or-
acle) perform on the 162 errorful sentences as mea-
sured by this criterion. Based on this criterion, the
greedy technique performs the best compared to the
others since it has a higher success rate (36%) and
a lower failure rate (31%). The oracle shows that
60% of the errorful sentences are fixed by at least
one of the six correction generation techniques. We
show examples of success and failure for the greedy
technique in Figure 4.
5.2 Effect of sentence length
From our observations on development data (not
part of the test set), we noticed that Google Trans-
late, like most statistical machine translation sys-
tems, performs significantly better on shorter sen-
tences. Therefore, we wanted to measure whether
the successes for the best method were biased to-
wards shorter sentences and the failures towards
longer ones. To do so, we measured the mean and
standard deviation of lengths of sentences compris-
ing the successes and failures of the greedy tech-
nique. Successful sentences had an average length
of approximately 18 words with a standard devia-
tion of 9.5. Failed sentences had an average length
of 23 words with a standard deviation of 12.31.
These numbers indicate that although the failures
are somewhat correlated with larger sentence length,
there is no evidence of a significant length bias.
5.3 Effect on grammatical sentences
Finally, we also carried out the same Success-
Failure-Draw analysis for the 38 sentences in our
test set that were perfectly grammatical to begin
with. The analysis differs from that of errorful sen-
tences in one key aspect: since the sentences are al-
ready free of any grammatical errors, no correction
can be grammatically better. Therefore, sentences
for which the correction preserves the meaning and
is not grammaticality worse will count as successes
and all other cases will count as failures. There are
no draws. Table 4 illustrates this difference and Ta-
ble 5 presents the success and failure rates for all six
methods. The greedy technique again performs the
best out of all six methods and successfully retains
the meaning and grammaticality for almost 80% of
50
Method Success Draw Failure
Baseline LM (B) 21% (34) 9% (15) 70% (113)
Greedy (G) 36% (59) 33% (52) 31% (51)
1-best (1) 32% (52) 30% (48) 38% (62)
LM Re-ranked (L) 30% (48) 17% (27) 54% (87)
Product Re-ranked (P) 23% (37) 38% (61) 40% (64)
LM Composition (C) 19% (31) 12% (20) 69% (111)
Oracle 60% (97) 40% (65) -
Table 3: The success, draw and failure rates for the six correction generation techniques and the oracle as computed for
the 162 errorful sentences from the test set. The oracle picks the method that produces the most meaning-preserving
and grammatical correction for each sentence. For sentences that have no adequate correction, it picks the original
sentence. Numbers in parentheses represent counts.
Success
That?s why I like to make travel by using my own car.
That?s why I like to travel using my own car.
Having discuss all this I must say that I must rather prefer to be a leader than just a member.
After discussing all this, I must say that I would prefer to be a leader than a member.
Failure
And simply there is fantastic for everyone
All magical and simply there is fantastic for all
I hope that share a room with she can be certainly kindle, because she is likely me
and so will not be problems with she.
I hope that sharing a room with her can be certainly kindle, because it is likely that
I and so there will be no problems with it.
Figure 4: Two examples of success and failure for the Greedy (G) technique. Original sentences are shown first
followed by the corrections in bold. Grammatical errors in the original sentences are in italics.
the grammatical sentences.7
C > O C = O C < O
Meaning = 1 - S F
Meaning = 0 - F F
Table 4: A matrix illustrating the Success-Draw-Failure
evaluation criterion for the 38 grammatical sentences.
There are no draws and sentences for which corrections
preserve meaning and are not grammatically worse count
as successes. The rest are failures.
6 Discussion & Future Work
In this paper, we explored the potential of a novel
technique based on round-trip machine translation
for the more ambitious and realistic task of whole-
sentence grammatical error correction. Although the
idea of round-trip machine translation (via a single
pivot language) has been explored before in the con-
text of just preposition errors, we expanded on it sig-
nificantly by combining multiple round-trip transla-
7An oracle for this setup is uninteresting since it will simply
return the original sentence for every sentence.
Method Success Failure
Baseline LM (B) 26% (10) 74% (28)
Greedy (G) 79% (30) 21% (8)
1-best (1) 61% (23) 39% (15)
LM Re-ranked (L) 34% (13) 66% (25)
Product Re-ranked (P) 42% (16) 58% (22)
LM Composition (C) 29% (11) 71% (25)
Table 5: The success and failure rates for the six correc-
tion generation techniques as computed for the 38 gram-
matical sentences from the test set.
tions and developed several new methods for pro-
ducing whole-sentence error corrections. Our oracle
experiments show that the ideas we explore have the
potential to produce whole-sentence corrections for
a variety of sentences though there is clearly room
for improvement.
An important point needs to be made regard-
ing the motivation for the round-trip translation ap-
proach. We claim that this approach is useful not
just because it can produce alternative renderings of
a given sentence but primarily because each of those
51
renderings is likely to retain at least some of mean-
ing of the original sentence.
Most of the problems with our techniques arise
due to the introduction of new errors by Google
Translate. One could use an error detection sys-
tem (or a human) to explicitly identify spans con-
taining grammatical errors and constrain the SMT
system to translate only these errorful spans while
still retaining the rest of the words in the sentence.
This approach should minimize the introduction of
new errors. Note that Google Translate does not
currently provide a way to perform such selective
translation. However, other open-source SMT sys-
tems such as Moses8 and Joshua9 do. Furthermore,
it might also be useful to exploit n-best translation
outputs instead of just relying on the 1-best as we
currently do.
As an alternative to selective translation, one
could simply extract the identified errorful spans and
round-trip translate each of them individually. For
example, consider the sentence: ?Most of all, luck
is null prep no use without a hard work.? where the
preposition of is omitted and there is an extraneous
article a before ?hard work?. With this approach,
one would simply provide Google Translate with the
two phrasal spans containing the errors, instead of
the entire sentence.
More generally, although we use Google Trans-
late for this pilot study due to its easy availability, it
might be more practical and useful to rely on an in-
house SMT system that trades-off translation quality
for additional features.
We also found that the language-model based
techniques performed quite poorly compared to the
other techniques. We suspect that this is due to the
fact that Google Translate already employs large-
order language models trained on trillions of words.
Using lower-order models trained on much smaller
corpora might simply introduce noise. However, a
detailed analysis is certainly warranted.
In conclusion, we claim that our preliminary ex-
ploration of large-scale round-trip translation based
techniques yielded fairly reasonable results. How-
ever, more importantly, it makes it clear that, with
additional research, these techniques have the poten-
8http://www.statmt.org/moses
9https://github.com/joshua-decoder
tial to be very effective at whole-sentence grammat-
ical error correction.
Acknowledgments
We would like to thank Aoife Cahill, Michael Heil-
man and the three anonymous reviewers for their
useful comments and suggestions. We would also
like to thank Melissa Lopez and Matthew Mulhol-
land for helping with the annotation.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual Entail-
ment Methods. J. Artif. Int. Res., 38(1):135?187.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing Consensus Translation from
Multiple Machine Translation Systems. In Proceed-
ings of ASRU, pages 351?354.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL Errors Using Phrasal SMT
Techniques. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 249?256.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Ya-
juan Lu?. 2009. Lattice-based System Combination
for Statistical Machine Translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3 - Volume 3, pages
1105?1113.
Matthieu Hermet and Alain De?silets. 2009. Using First
and Second Language Models to Correct Preposition
Errors in Second Language Authoring. In Proceedings
of the Fourth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 64?72.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
driven Methods. Computational Linguistics, 36(3).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102?109.
52
Y. Albert Park and Roger Levy. 2011. Automated Whole
Sentence Grammar Correction using a Noisy Channel
Model. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, pages 934?
944.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining Outputs from Multiple Machine
Translation Systems. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 228?
235.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation at the
12th Meeting of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2009).
Shiqi Zhao, Haifeng Wang, Xiang Lan, and Ting Liu.
2010. Leveraging Multiple MT Engines for Para-
phrase Generation. In COLING, pages 1326?1334.
53
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 163?168,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Automated Scoring of a Summary Writing Task
Designed to Measure Reading Comprehension
Nitin Madnani, Jill Burstein, John Sabatini and Tenaha O?Reilly
Educational Testing Service
660 Rosedale Road, Princeton, NJ 08541, USA
{nmadnani,jburstein,jsabatini,toreilly}@ets.org
Abstract
We introduce a cognitive framework for mea-
suring reading comprehension that includes
the use of novel summary writing tasks. We
derive NLP features from the holistic rubric
used to score the summaries written by stu-
dents for such tasks and use them to design a
preliminary, automated scoring system. Our
results show that the automated approach per-
forms well on summaries written by students
for two different passages.
1 Introduction
In this paper, we present our preliminary work on
automatic scoring of a summarization task that is de-
signed to measure the reading comprehension skills
of students from grades 6 through 9. We first intro-
duce our underlying reading comprehension assess-
ment framework (Sabatini and O?Reilly, In Press;
Sabatini et al, In Press) that motivates the task of
writing summaries as a key component of such as-
sessments in ?2. We then describe the summariza-
tion task in more detail in ?3. In ?4, we describe our
approach to automatically scoring summaries writ-
ten by students for this task and compare the results
we obtain using our system to those obtained by hu-
man scoring. Finally, we conclude in ?6 with a brief
discussion and possible future work.
2 Reading for Understanding (RfU)
Framework
We claim that to read for understanding, readers
should acquire the knowledge, skills, strategies, and
dispositions that will enable them to:
? learn and process the visual and typographical
elements and conventions of printed texts and
print world of literacy;
? learn and process the verbal elements of lan-
guage including grammatical structures and
word meanings;
? form coherent mental representations of texts,
consistent with discourse, text structures, and
genres of print;
? model and reason about conceptual content;
? model and reason about social content.
We also claim that the ability to form a coher-
ent mental model of the text that is consistent with
text discourse is a key element of skilled reading.
This mental model should be concise but also reflect
the most likely intended meaning of the source. We
make this claim since acquiring this ability:
1. requires the reader to have knowledge of
rhetorical text structures and genres;
2. requires the reader to model the propositional
content of a text within that rhetorical frame,
both from an author?s or reader?s perspective;
and
3. is dependent on a skilled reader having ac-
quired mental models for a wide variety of
genres, each embodying specific strategies for
modeling the meaning of the text sources to
achieve reading goals.
In support of the framework, research has shown
that the ability to form a coherent mental model
163
is important for reading comprehension. Kintsch
(1998) showed that it is a key aspect in the process of
construction integration and essential to understand-
ing the structure and organization of the text. Sim-
ilarly, Gernsbacher (1997) considers mental models
essential to structure mapping and in bridging and
making knowledge-based inferences.
2.1 Assessing Mental Models
Given the importance of mental models for reading
comprehension, the natural question is how does one
assess whether a student has been able to build such
models after reading a text. We believe that such
an assessment must encompass asking a reader to
(a) sample big ideas by asking them to describe the
main idea or theme of a text, (b) find specific details
in the text using locate/retrieve types of questions,
and (c) bridging gaps between different points in the
text using inference questions. Although these ques-
tions can be multiple-choice, existing research indi-
cates that it is better to ask the reader to write a brief
summary of the text instead. Yu (2003) states that
a good summary can prove useful for assessment of
reading comprehension since it contains the relevant
important ideas, distinguishes accurate information
from opinions, and reflects the structure of the text
itself. More specifically, having readers write sum-
maries is a promising solution since:
? there is considerable empirical support that it
both measures and encourages reading compre-
hension and is an effective instructional strat-
egy to help students improve reading skills
(Armbruster et al, 1989; Bean and Steenwyk,
1984; Duke and Pearson, 2002; Friend, 2001;
Hill, 1991; Theide and Anderson, 2003);
? it is a promising technique for engaging stu-
dents in building mental models of text; and
? it aligns with our framework and cognitive the-
ory described earlier in this section.
However, asking students to write summaries in-
stead of answering multiple choice questions entails
that the summaries must be scored. Asking human
raters to score these summaries, however, can be
time consuming as well as costly. A more cost-
effective and efficient solution would be to use an
automated scoring technique using machine learn-
ing and natural language processing. We describe
such a technique in the subsequent sections.
During the Neolithic Age, humans developed agriculture-what we 
think of as farming.  Agriculture meant that people stayed in one 
place to grow their crops.  They stopped moving from place to 
place to follow herds of animals or to find new wild plants to eat. 
And because they were settling down, people built permanent 
shelters.  The caves they had found and lived in before could be 
replaced by houses they built themselves.
To build their houses, the people of this Age often stacked mud 
bricks together to make rectangular or round buildings.  At first, 
these houses had one big room.  Gradually, they changed to 
include several rooms that could be used for different purposes. 
People dug pits for cooking inside the houses.  They may have 
filled the pits with water and dropped in hot stones to boil it.  You 
can think of these as the first kitchens.
The emergence of permanent shelters had a dramatic effect on 
humans.  They gave people more protection from the weather and 
from wild animals.  Along with the crops that provided more food 
than hunting and gathering, permanent housing allowed people to 
live together in larger communities.
Please write a summary. The first sentence of your summary 
should be about the whole passage.  Then write 3 more 
sentences. Each sentence should be about one of the 
paragraphs.
Passage
Directions
Figure 1: An example passage for which students are
asked to write a summary, and the summary-writing di-
rections shown to the students.
3 Summary Writing Task
Before describing the automated scoring approach,
we describe the details of the summary writing task
itself. The summarization task is embedded within
a larger reading comprehension assessment. As part
of the assessment, students read each passage and
answer a set of multiple choice questions and, in ad-
dition, write a summary for one of the passages. An
example passage and the instructions can be seen in
Figure 1. Note the structured format of summary
that is asked for in the directions: the first sentence
of the summary must be about the whole passage
and the next three should correspond to each of the
paragraphs in the passage. All summary tasks are
structured similarly in that the first sentence should
identify the ?global concept? of the passage and the
164
next three sentences should identify ?local concepts?
corresponding to main points of each subsequent
paragraph.
Each summary written by a student is scored ac-
cording to a holistic rubric, i.e., based on holistic
criteria rather than criteria based on specific dimen-
sions of summary writing. The scores are assigned
on a 5-point scale which are defined as:
Grade 4: summary demonstrates excellent global
understanding and understanding of all 3 lo-
cal concepts from the passage; does not include
verbatim text (3+ words) copied from the pas-
sage; contains no inaccuracies.
Grade 3: summary demonstrates good global un-
derstanding and demonstrates understanding of
at least 2 local concepts; may or may not in-
clude some verbatim text, contains no more
than 1 inaccuracy.
Grade 2: summary demonstrates moderate local
understanding only (2-3 local concepts but no
global); with or without verbatim text, contains
no more than 1 inaccuracy; OR good global un-
derstanding only with no local concepts
Grade 1: summary demonstrates minimal local
understanding (1 local concept only), with or
without verbatim text; OR contains only verba-
tim text
Grade 0: summary is off topic, garbage, or demon-
strates no understanding of the text; OR re-
sponse is ?I don?t know? or ?IDK?.
Note that students had the passage in front of them
when writing the summaries and were not penalized
for poor spelling or grammar in their summaries. In
the next section, we describe a system to automati-
cally score these summaries.
4 Automated Scoring of Student
Summaries
We used a machine learning approach to build an
automated system for scoring summaries of the type
described in ?3. To train and test our system, we
used summaries written by more than 2600 students
from the 6th, 7th and 9th grades about two differ-
ent passages. Specifically, there were a total of 2695
summaries ? 1016 written about a passage describ-
ing the evolution of permanent housing through his-
tory (the passage shown in Figure 1) and 1679 writ-
ten about a passage describing living conditions at
the South Pole. The distribution of the grades for
the students who wrote the summaries for each pas-
sage is shown in Table 1.
Passage Grade Count
South Pole
6 574
7 521
9 584
Perm. Housing
6 387
7 305
9 324
Table 1: The grade distribution of the students who wrote
summaries for each of the two passages.
All summaries were also scored by an experi-
enced human rater in accordance with the 5-point
holistic rubric described previously. Figure 2 shows
the distribution of the human scores for both sets of
summaries.
South Pole (N=1679)
Permanent Housing (N=1016)
0100
200300
400500
600700
800900
Score0 1 2 3 4 Score0 1 2 3 4
Figure 2: A histogram illustrating the human score distri-
bution of the summaries written for the two passages.
Our approach to automatically scoring these sum-
maries is driven by features based on the rubric.
Specifically, we use the following features:
1. BLEU: BLEU (BiLingual Evaluation Under-
study) (Papineni et al, 2002) is an automated
metric used extensively in automatically scor-
ing the output of machine translation systems.
165
It is a precision-based metric that computes n-
gram overlap (n=1 . . . 4) between the summary
(treated as a single sentence) against the pas-
sage (treated as a single sentence). We chose to
use BLEU since it measures how many of the
words and phrases are borrowed directly from
the passage. Note that some amount of borrow-
ing from the passage is essential for writing a
good summary.
2. ROUGE: ROUGE (Recall-Oriented Under-
study for Gisting Evaluation) (Lin and Hovy,
2003) is an automated metric used for scoring
summaries produced by automated document
summarization systems. It is a recall-based
metric that measures the lexical and phrasal
overlap between the summary under consider-
ation and a set of ?model? (or reference) sum-
maries. We used a single model summary for
the two passages by randomly selecting each
from the set of student summaries assigned a
score of 4 by the human rater.
3. CopiedSumm: Ratio of the sum of lengths of
all 3-word (or longer) sequences that are copied
from the passage to the length of the summary.
4. CopiedPassage: Same as CopiedSumm but
with the denominator being the length of the
passage.
5. MaxCopy: Length of the longest word se-
quence in the summary copied from the pas-
sage.
6. FirstSent: Number of passage sentences that
the first sentence of the summary borrows 2-
word (or longer) sequences from.
7. Length: Number of sentences in the summary.
8. Coherence: Token counts of commonly used
discourse connector words in the summary.
ROUGE computes the similarity between the
summary S under consideration and a high-scoring
summary - a high value of this similarity indicates
that S should also receive a high score. Copied-
Summ, CopiedPassage, BLEU, and MaxCopy
capture verbatim copying from the passage. First-
Sent directly captures the ?global understanding?
concept for the first sentence, i.e., a large value for
this feature means that the first sentence captures
more of the passage as expected. Length captures
the correspondence between the number of para-
graphs in the passage and the number of sentences
in the summary. Finally, Coherence captures how
well the student is able to connect the different ?lo-
cal concepts? present in the passage. Note that:
? Although the rubric states that students not be
penalized for spelling errors, we did not spell-
correct the summaries before scoring them. We
plan to do this for future experiments.
? The students were not explicitly told to refrain
from verbatim copying since the summary-
writing instructions indicated this implicitly
(?. . . about the whole passage? and ?. . . about
one of the paragraphs?). However, for future
experiments, we plan to include explicit in-
structions regarding copying.
All features were combined in a logistic regres-
sion classifier that output a prediction on the same
5-point scale as the holistic rubric. We trained a sep-
arate classifier for each of the two passage types.1
The 5-fold cross-validation performance of this clas-
sifier on our data is shown in Table 2. We compute
exact as well as adjacent agreement of our predic-
tions against the human scores using the confusion
matrices from the two classifiers. The exact agree-
ment shows the rate at which the system and the
human rater awarded the same score to a summary.
Adjacent agreement shows the rate at which scores
given by the system and the human rater were no
more than one score point apart (e.g., the system as-
signed a score of 4 and the human rater assigned a
score of 5 or 3). For holistic scoring using 5-point
rubrics, typical exact agreement rates are in the same
range as our scores (Burstein, 2012; Burstein et al,
2013). Therefore, our system performed reasonably
well on the summary scoring task. For comparison,
we also show the exact and adjacent agreement of
the most-frequent-score baseline.
It is important to investigate whether the various
features correlated in an expected manner with the
score in order to ensure that the summary-writing
construct is covered accurately. We examined the
weights assigned to the various features in the clas-
sifier and found that this was indeed the case. As ex-
pected, the CopiedSumm, CopiedPassage, BLEU,
1We used the Weka Toolkit (Hall et al, 2009).
166
Method Passage Exact Adjacent
Baseline
South Pole .51 .90
Perm. Housing .32 .77
Logistic
South Pole .65 .97
Perm. Housing .52 .93
Table 2: Exact and adjacent agreements of the most-
frequent-score baseline and of the 5-fold cross-validation
predictions from the logistic regression classifier, for both
passages.
and MaxCopy features all correlate negatively with
score, and ROUGE, FirstSent and Coherence cor-
relate positively.
In addition to overall performance, we also exam-
ined which features were most useful to the classi-
fier in predicting summary scores. Table 3 shows the
various features ranked using the information-gain
metric for both logistic regression models. These
rankings show that the features performed consis-
tently for both models.
South Pole Perm. Housing
BLEU (.375) BLEU (.450)
CopiedSumm (.290) ROUGE (.400)
ROUGE (.264) CopiedSumm (.347)
Length (.257) Length (.340)
CopiedPassage (.246) MaxCopy(.253)
MaxCopy (.231) CopiedPassage (.206)
FirstSent (.120) Coherence (.155)
Coherence (.103) FirstSent (.058)
Table 3: Classifier features for both passages ranked by
average merit values obtained using information-gain.
5 Related Work
There has been previous work on scoring summaries
as part of the automated document summarization
task (Nenkova and McKeown, 2011). In that task,
automated systems produce summaries of multiple
documents on the same topic and those machine-
generated summaries are then scored by either hu-
man raters or by using automated metrics such as
ROUGE. In our scenario, however, the summaries
are produced by students?not automated systems?
and the goal is to develop an automated system to
assign scores to these human-generated summaries.
Although work on automatically scoring student
essays (Burstein, 2012) and short answers (Lea-
cock and Chodorow, 2003; Mohler et al, 2011) is
marginally relevant to the work done here, we be-
lieve it is different in significant aspects based on
the scoring rubric and on the basis of the underlying
RfU framework. We believe that the work most di-
rectly related to ours is the Summary Street system
(Franzke et al, 2005; Kintsch et al, 2007) which
attempts to score summaries written for tasks not
based on the RfU framework and uses latent seman-
tic analysis (LSA) rather than a feature-based classi-
fication approach.
6 Conclusion & Future Work
We briefly introduced the Reading for Understand-
ing cognitive framework and how it motivates the
use of a summary writing task in a reading compre-
hension assessment. Our motivation is that such a
task is theoretically suitable for capturing the abil-
ity of a reader to form coherent mental representa-
tions of the text being read. We then described a
preliminary, feature-driven approach to scoring such
summaries and showed that it performed quite well
for scoring the summaries about two different pas-
sages. Obvious directions for future work include:
(a) getting summaries double-scored to be able to
compare system-human agreement against human-
human agreement (b) examining whether a single
model trained on all the data can perform as well as
passage-specific models, and (c) using more sophis-
ticated features such as TERp (Snover et al, 2010)
which can capture and reward paraphrasing in ad-
dition to exact matches, and features that can better
model the ?local concepts? part of the scoring rubric.
Acknowledgments
The research reported here was supported by the Institute
of Education Sciences, U.S. Department of Education,
through Grant R305F100005 to the Educational Testing
Service as part of the Reading for Understanding Re-
search Initiative. The opinions expressed are those of the
authors and do not represent views of the Institute or the
U.S. Department of Education. We would also like to
thank Kelly Bruce, Kietha Biggers and the Strategic Ed-
ucational Research Partnership.
167
References
B. B. Armbruster, T. H. Anderson, and J. Ostertag. 1989.
Teaching Text Structure to Improve Reading and Writ-
ing. Educational Leadership, 46:26?28.
T. W. Bean and F. L. Steenwyk. 1984. The Effect of
Three Forms of Summarization Instruction on Sixth-
graders? Summary Writing and Comprehension. Jour-
nal of Reading Behavior, 16(4):297?306.
J. Burstein, J. Tetreault, and N. Madnani. 2013. The E-
rater Automated Essay Scoring System. In M.D. Sher-
mis and J. Burstein, editors, Handbook for Automated
Essay Scoring. Routledge.
J. Burstein. 2012. Automated Essay Scoring and Evalu-
ation. In Carol Chapelle, editor, The Encyclopedia of
Applied Linguistics. Wiley-Blackwell.
N. K. Duke and P. D. Pearson. 2002. Effective Practices
for Developing Reading Comprehension. In A. E.
Farstrup and S. J. Samuels, editors, What Research has
to Say about Reading Instruction, pages 205?242. In-
ternational Reading Association.
M. Franzke, E. Kintsch, D. Caccamise, N. Johnson, and
S. Dooley. 2005. Summary Street: Computer sup-
port for comprehension and writing. Journal of Edu-
cational Computing Research, 33:53?80.
R. Friend. 2001. Effects of Strategy Instruction on Sum-
mary Writing of College Students. Contemporary Ed-
ucational Psychology, 26(1):3?24.
M. A. Gernsbacher. 1997. Two Decades of Structure
Building. Discourse Processes, 23:265?304.
P. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explorations,
11(1).
M. Hill. 1991. Writing Summaries Promotes Think-
ing and Learning Across the Curriculum ? But Why
are They So Difficult to Write? Journal of Reading,
34(7):536?639.
E. Kintsch, D. Caccamise, M. Franzke, N. Johnson, and
S. Dooley. 2007. Summary Street: Computer-guided
summary writing. In T. K. Landauer, D. S. McNa-
mara, S. Dennis, and W. Kintsch, editors, Handbook
of latent semantic analysis. Lawrence Erlbaum Asso-
ciates Publishers.
W. Kintsch. 1998. Comprehension: A Paradigm for
Cognition. Cambridge University Press.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated Scoring of Short-Answer Questions. Comput-
ers and the Humanities, 37(4):389?405.
C.-Y. Lin and E. H. Hovy. 2003. Automatic Evaluation
of Summaries Using N-gram Co-occurrence Statistics.
In Proceedings of HLT-NAACL, pages 71?78.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to Grade Short Answer Questions using Seman-
tic Similarity Measures and Dependency Graph Align-
ments. In Proceedings of ACL, pages 752?762.
A. Nenkova and K. McKeown. 2011. Automatic Sum-
marization. Foundations and Trends in Information
Retrieval, 5(2?3):103?233.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of ACL, pages 311?
318.
J. Sabatini and T. O?Reilly. In Press. Rationale For a
New Generation of Reading Comprehension Assess-
ments. In B. Miller, L. Cutting, and P. McCardle,
editors, Unraveling the Behavioral, Neurobiological,
and Genetic Components of Reading Comprehension.
Brookes Publishing, Inc.
J. Sabatini, T. O?Reilly, and P. Deane. In Press. Prelimi-
nary Reading Literacy Assessment Framework: Foun-
dation and Rationale for Assessment and System De-
sign.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2010.
TER-Plus: Paraphrase, Semantic, and Alignment En-
hancements to Translation Edit Rate. Machine Trans-
lation, 23:117?127.
K. W. Theide and M. C. M. Anderson. 2003. Summariz-
ing Can Improve Metacomprehension Accuracy. Edu-
cational Psychology, 28(2):129?160.
G. Yu. 2003. Reading for Summarization as Reading
Comprehension Test Method: Promises and Problems.
Language Testing Update, 32:44?47.
168
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300?305,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Detecting Missing Hyphens in Learner Text
Aoife Cahill?, Martin Chodorow?, Susanne Wolff? and Nitin Madnani?
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{acahill, swolff, nmadnani}@ets.org
? Hunter College and the Graduate Center, City University of New York, NY 10065, USA
martin.chodorow@hunter.cuny.edu
Abstract
We present a method for automatically de-
tecting missing hyphens in English text. Our
method goes beyond a purely dictionary-based
approach and also takes context into account.
We evaluate our model on artificially gener-
ated data as well as naturally occurring learner
text. Our best-performing model achieves
high precision and reasonable recall, making
it suitable for inclusion in a system that gives
feedback to language learners.
1 Introduction
While errors of punctuation are not as frequent, nor
often as serious, as some of the other typical mis-
takes that learners make, they are nevertheless an
important consideration for students aiming to im-
prove the overall quality of their writing. In this pa-
per we focus on the error of missing hyphens. The
following example is a typical mistake made by a
student writer:
(1) Schools may have more after school sports.
In this case the tokens after and school should be hy-
phenated as they modify the noun sports. However,
in Example (2) a hyphen between after and school
would be incorrect, since in this instance after func-
tions as as the head of a prepositional phrase modi-
fying went.
(2) I went to the dentist after school today.
These examples illustrate that purely dictionary-
based approaches to detecting missing hyphens are
not likely to be sophisticated enough to differentiate
the contexts in which a hyphen is required. In addi-
tion, learner text frequently contains other grammat-
ical and spelling errors, further complicating auto-
matic error detection. Example (3) contains an error
father like instead of father likes to. This causes dif-
ficulty for automated hyphenation systems because
like is a frequent suffix of hyphenated words and
play can function as a noun.
(3) My father like play basketball with me.
In this paper, we propose a classifier-based approach
to automatically detecting missing hyphen errors.
The goal of our system is to detect missing hyphen
errors and provide feedback to language learners.
Therefore, we place more importance on the preci-
sion of the system than recall. We train our model on
features that take the context of a pair of words into
account, as well as other discriminative features. We
present a number of evaluations on both artificially
generated errors and naturally occurring learner er-
rors and show that our classifiers achieve high preci-
sion and reasonable recall.
2 Related Work
The task of detecting missing hyphens is related to
previous work on detecting punctuation errors. One
of the classes of errors in the Helping Our Own
(HOO) 2011 shared task (Dale and Kilgarriff, 2011)
was punctuation. Comma errors are the most fre-
quent kind of punctuation error made by learners. Is-
rael et al (2012) present a model for detecting these
kinds of errors in learner texts. They train CRF mod-
els on sentences from unedited essays written by
high-level college students and show that they per-
forms well on detecting errors in learner text. As
300
far as we are aware, the HOO 2011 system descrip-
tion of Rozovskaya et al (2011) is the only work to
specifically reference hyphen errors. They use rules
derived from frequencies in the training corpus to
determine whether a hyphen was required between
two words separated by white space.
The task of detecting missing hyphens is related
to the task of inserting punctuation into the output of
unpunctuated text (for example, the output of speech
recognition, automatic generation, machine transla-
tion, etc.). Systems that are built on the output of
speech recognition can obviously take features like
prosody into account. In our case, we are deal-
ing only with written text. Gravano et al (2009)
present an n-gram-based model for automatically
adding punctuation and capitalization to the output
of an ASR system, without taking any of the speech
signal information into account. They conclude that
more training data, rather than wider n-gram con-
texts leads to a greater improvement in accuracy.
3 Baselines
We implement three baseline systems which we will
later compare to our classification approach. The
first baseline is a na??ve heuristic that predicts a miss-
ing hyphen between bigrams that appear hyphenated
in the Collins Dictionary.1 As a somewhat less-
na??ve baseline, we implement a heuristic that pre-
dicts a missing hyphen between bigrams that occur
hyphenated more than 1,000 times in Wikipedia. A
third baseline is a heuristic that predicts a missing
hyphen between bigrams where the probability of
the hyphenated form as estimated from Wikipedia
is greater than 0.66, meaning that the hyphenated
bigram is twice as likely as the non-hyphenated bi-
gram. This baseline is similar to the approach taken
by Rozovskaya et al (2011), except that the proba-
bilities are estimated from a much larger corpus.
4 System Description
Using the features in Table 1, we build a logis-
tic regression model which assigns a probability to
the likelihood of a hyphen occurring between two
words, wi and wi+1. As we are primarily interested
in using this system for giving feedback to language
learners, we require very high precision. Therefore,
1LDC catalog number LDC93T1
Tokens wi?1, wi, wi+1, wi+2
Stems si?1, si, si+1, si+2
Tags ti?1, ti, ti+1, ti+2
Bigrams wi?wi+1, si?si+1, ti?ti+1
Dict Does the hyphenated form appear in
the Collins dictionary?
Prob What is the probability of the word
bigram appearing hyphenated in
Wikipedia?
Distance Distance to following and preced-
ing verb, noun
Verb/Noun Is there a verb/noun preced-
ing/following this bigram
Table 1: Features used in all models. Positive in-
stances are those where there was a hyphen between
wi and wi+1 in the data. Stems are generated using
NLTK?s implementation of the Lancaster Stemmer,
and tags are obtained from the Stanford Parser.
we only predict a missing hyphen error when the
probability of the prediction is >0.99.
We experiment with two different sources of
training data, in addition to their combination. We
first train on well-edited text, using almost 1.8 mil-
lion sentences from the San Jose Mercury News cor-
pus.2 For training, hyphenated words are automati-
cally split (i.e. well-known becomes well known).
The positive examples for the classifier are all bi-
grams where a hyphen was removed. Negative ex-
amples consist of bigrams where there was no hy-
phen in the training data. Since this is over 99% of
the data, we randomly sample 3% of the negative
examples for training. We also restrict the negative
examples to only the most likely contexts, where a
context is defined as a part-of-speech bigram. A list
of possible contexts in which hyphens occur is ex-
tracted from the entire training set. Only contexts
that occur more than 20 times are selected during
training. All contexts are evaluated during testing.
Table 2 lists some of the most frequent contexts with
examples of when they should be hyphenated and
when they should remain unhyphenated.
The second data source for training the model
comes from pairs of revisions from Wikipedia ar-
ticles. Following Cahill et al (2013), we automati-
cally extract a corpus of error annotations for miss-
2LDC catalog number LDC93T3A.
301
Context Hyphenated Unhyphenated
NN NN terrific truck-stop
waitress
a quake insurance
surcharge
CD CD Twenty-two thou-
sand
the 126 million
Americans
JJ NN an early-morning
blaze
an entire practice
session
CD NN a two-year contract about 600 tank cars
NN VBN a court-ordered
program
a letter delivered to-
day
Table 2: Some frequent likely POS contexts for hy-
phenation, with examples from the Brown corpus.
ing hyphens. This is done by extracting the plain
text from every revision to every article and com-
paring adjacent pairs of revisions. For each article,
chains of errors are detected, using the surrounding
text to identify them. When a chain begins and ends
with the same form, it is ignored. Only the first and
last points in an error chain are retained for train-
ing. An example chain is the following: It has been
an ancient {focal point ? location ? focal point
? focal-point} of trade and migration., where we
would extract the correction focal point ? focal-
point. In total, we extract a corpus of 390,298 sen-
tences containing missing hyphen error annotations.
Finally, we combine both data sources.
5 Evaluating on Artificial Data
Since there are large corpora of well-edited text
readily available, it is easy to evaluate on artifi-
cial data. For testing, we take 24,243 sentences
from the Brown corpus and automatically remove
hyphens from the 2,072 hyphenated words (but not
free-standing dashes). Each system makes a predic-
tion for all bigrams about whether a hyphen should
appear between the pair of words. We measure the
performance of each system in terms of precision, P,
(how many of the missing hyphen errors predicted
by the system were true errors), recall, R, (how many
of the artificially removed hyphens the system de-
tected as errors) and f-score, F, (the harmonic mean
of precision and recall). The results are given in
Table 3, and also include the raw number of true
positives, TP, detected by each system. The results
show that the baseline using Wikipedia probabilities
obtains the highest precision, however with low re-
call. The classifiers trained on newswire text and the
TP P R F
Baseline
Collins dict 397 40.5 19.2 26.0
Wiki Counts-1000 359 39.1 17.3 24.0
Wiki Probs-0.66 811 85.5 39.1 53.7
Classifier
SJM-trained 1097 82.0 52.9 64.3
Wiki-revision-trained 1061 72.8 51.2 60.1
Combined 1106 80.9 53.4 64.3
Table 3: Results of evaluating on the Brown Corpus
with hyphens removed
combined news and Wikipedia revision text achieve
the highest overall f-score. Figure (1a) shows the
Precision Recall curves for the Wikipedia baselines
and the three classifiers. The curves mirror the re-
sults in the table, showing that the classifier trained
on the newswire text, and the classifier trained on the
combined data perform best. The Wikipedia counts
baseline performs worst.
6 Evaluating on Learner Text
We carry out two evaluations of our system on
learner text. We first evaluate on the missing hyphen
errors contained in the CLC-FCE (Yannakoudakis et
al., 2011). This corpus contains 1,244 exam scripts
written by learners of English as part of the Cam-
bridge ESOL First Certificate in English. In total,
there are 173 instances of missing hyphen errors.
The results are given in Table 4, and the precision
recall curves are displayed in Figure (1b).
The results show that the classifiers consistently
achieve high precision on this data set. This is as
expected, given the high threshold set. Looking at
the curves, it seems that a slightly lower threshold in
this case may lead to better results. The curves show
that the combined classifier is performing slightly
better than the other two classifiers. The baselines
are clearly not performing as well on this dataset.
While the overall size of the CLC-FCE data set
is quite large, the low frequency of this kind of er-
ror means that the evaluation was carried out on a
relatively small number of examples. For this rea-
son, the reliability of the results may be called into
question. There is, for instance, a striking difference
between the f-scores for the Collins Dictionary base-
302
0.0 0.2 0.4 0.6 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
combinedsjmwikibaseline?wiki?countsbaseline?wiki?probs
(a) Brown Corpus
0.2 0.4 0.6 0.8 1.00
.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
combinedsjmwikibaseline?wiki?countsbaseline?wiki?probs
(b) CLC-FCE Corpus
Figure 1: Precision Recall curves for the Wikipedia baselines and the three classifiers.
TP P R F
Baseline
Collins dict 131 64.5 75.7 69.7
Wiki Counts-1000 141 73.1 81.5 77.0
Wiki Probs-0.66 36 92.3 20.8 34.0
Classifier
SJM-trained 60 84.5 34.7 49.2
Wiki-revision-trained 71 98.6 41.0 58.0
Combined 66 98.5 38.2 55.0
Table 4: Results of evaluating on the CLC-FCE
dataset
line on the Brown corpus (26.0) and on the learner
data (69.7). Inspection of the 131 true positives for
the learner data reveal that 87 of these are cases of a
single type, the word ?make-up?, which students of-
ten wrote without a hyphen in response to a prompt
about a fashion and leisure show. Since the hyphen-
ated form was in the Collins Dictionary, the base-
line system was credited with detection of this error.
However, when the 87 occurrences of ?make up? are
removed from the data set, the values of precision,
recall and f-score for the Collins Dictionary baseline
fall to 37.9, 51.2, and 42.9, respectively. This points
to a problem for system evaluation that is more gen-
303
eral than the low frequency of an error type, such
as missing hyphens. The more general problem is
that of non-independence among errors, which oc-
curs when an individual writer contributes multiple
times to an error count or when a particular prompt
gives rise to many occurrences of the same error, as
in the current case of ?make-up?.
Despite the problem of non-independent errors, a
more accurate picture of system performance may
nonetheless emerge with more evidence. Therefore,
we evaluate system precision on a data set of 1,000
student GRE and TOEFL essays written by both na-
tive and nonnative speakers, across a wide range of
proficiency levels and prompts. The essays, drawn
from 295 prompts, ranged in length from 1 to 50
sentences, with an average of 378 words per essay.
We manually inspect a random sample of 100 in-
stances where each system detected a missing hy-
phen. Two native-English speakers judged the cor-
rectness of the predictions using the Chicago Man-
ual of Style as a guide.3 Inter-annotator agreement
on the binary classification task for 600 items was
0.79?, showing high agreement. The results are
given in Table 5.
Total Judge-1 Judge 2
Predictions Precision Precision
Baseline
Collins dict 416 11 8
Wiki Counts 2185 20 21
Wiki Probs 224 54 52
Classifier
SJM-trained 421 62 69
Wiki-revision 577 43 41
Combined 450 60 62
Table 5: Precision results on 1000 student responses,
estimated by randomly sampling 100 hyphen predic-
tions of each system and manually evaluating them.
The results show that the first two baseline sys-
tems do not perform well on this essay data. This
is mainly because they do not take context into ac-
count. Many of the errors made by these systems in-
volved verb + preposition bigrams, as in Examples
(4) and (5). Restricting the detection by probability
clearly improves precision, but at the cost of recall
3http://www.chicagomanualofstyle.org
(only 224 total instances of missing hyphen errors
detected, the lowest of all 6 systems). In the man-
ual evaluation, the system trained on the SJM corpus
achieves the highest precision, though all precision
figures are lower than the previous evaluations. Ex-
ample (6) is a typical example of the kinds of false
positives made by the classifier models.
(4) If these men were required to step-down after a
limited number of years, the damage would be
contained.
(5) These families may even choose to eat at-home
than outside.
(6) The wellness program will save money in the
long-term.
Future work will explore additional features that
may help improve performance. A more thorough
study will also be carried out to fully understand the
differences in performance of the classifiers across
corpora. Another direction to explore in future work
is the related task of identifying extraneous hyphens
in learner text. These are even less frequent than
missing hyphens (87 annotated cases in the CLC-
FCE corpus), but we believe a similar classification
approach could be successful.
7 Conclusion
In this paper we presented a model for automatically
detecting missing hyphen errors in learner text. We
experimented with two kinds of training data, one
well-edited text, and the other an automatically ex-
tracted corpus of error annotations. When evaluat-
ing on artificially generated errors in otherwise well-
edited text, the classifiers generally performed bet-
ter than the baseline systems. When evaluating on
the small number of missing hyphen errors in the
CLC-FCE corpus, the word-based models did well,
though the classifiers also achieved consistently high
precision. A precision-only evaluation on a sample
of learner essays resulted in overall lower scores, but
the classifier trained on well-edited text performed
best. In general, the classifiers outperform the base-
line, especially in terms of precision, showing that
taking context into account when detecting these
kinds of errors is important.
304
References
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane
Napolitano. 2013. Robust Systems for Preposition
Error Correction Using Wikipedia Revisions. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Atlanta, GA.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the Generation Challenges Session at the
13th EuropeanWorkshop on Natural Language Gener-
ation, pages 242?249, Nancy, France, September. As-
sociation for Computational Linguistics.
Agustin Gravano, Martin Jansche, and Michiel Bacchi-
ani. 2009. Restoring punctuation and capitalization in
transcribed speech. In Acoustics, Speech and Signal
Processing, 2009. ICASSP 2009. IEEE International
Conference on, pages 4741?4744. IEEE.
Ross Israel, Joel Tetreault, and Martin Chodorow. 2012.
Correcting Comma Errors in Learner Essays, and
Restoring Commas in Newswire Text. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 284?
294, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 263?266, Nancy, France, September. Associa-
tion for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
305
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 79?88,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
An Explicit Feedback System for Preposition Errors
based on Wikipedia Revisions
Nitin Madnani and Aoife Cahill
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{nmadnani,acahill}@ets.org
Abstract
This paper presents a proof-of-concept
tool for providing automated explicit feed-
back to language learners based on data
mined from Wikipedia revisions. The tool
takes a sentence with a grammatical er-
ror as input and displays a ranked list of
corrections for that error along with evi-
dence to support each correction choice.
We use lexical and part-of-speech con-
texts, as well as query expansion with a
thesaurus to automatically match the er-
ror with evidence from the Wikipedia revi-
sions. We demonstrate that the tool works
well for the task of preposition selection
errors, evaluating against a publicly avail-
able corpus.
1 Introduction
A core feature of learning to write is receiving
feedback and making revisions based on that feed-
back (Biber et al., 2011; Lipnevich and Smith,
2008; Truscott, 2007; Rock, 2007). In the field of
second language acquisition, the main focus has
been on explicit or direct feedback vs. implicit
or indirect feedback. In writing, explicit or direct
feedback involves a clear indication of the location
of an error as well as the correction itself, or, more
recently, a meta-linguistic explanation (of the un-
derlying grammatical rule). Implicit or indirect
written feedback indicates that an error has been
made at a location, but it does not provide a cor-
rection.
The work in this paper describes a novel tool
for presenting language learners with explicit
feedback based on human-authored revisions in
Wikipedia. Here we describe the proof-of-concept
tool that provides explicit feedback on one specific
category of grammatical errors, preposition selec-
tion. We restrict the scope of the tool in order to
be able to carry out a focused study, but expect
that our findings presented here will also general-
ize to other error types. The task of preposition se-
lection errors has been well studied (Tetreault and
Chodorow, 2008; De Felice and Pulman, 2009;
Tetreault et al., 2010; Rozovskaya and Roth, 2010;
Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill
et al., 2013), and the availability of public, anno-
tated corpora containing such errors provides easy
access to evaluation data.
Our tool takes a sentence with a grammatical
error as input, and returns a ranked list of possi-
ble corrections. The tool makes use of frequency
of correction in edits to Wikipedia articles (as
recorded in the Wikipedia revision history) to cal-
culate the rank order. In addition to the ranked list
of suggestions, the tool also provides evidence for
each correction based on the actual changes made
between different versions of Wikipedia articles.
The tool uses the notion of ?context similarity? to
determine whether a particular edit to a Wikipedia
article can provide evidence of a correction in a
given context.
Specifically, this paper makes the following
contributions:
1. We build a tool to provide explicit feedback
for preposition selection errors in the form of
ranked lists of suggested corrections.
2. We use evidence from human-authored cor-
rections for each suggested correction on a
list.
3. We conduct a detailed examination of how
the performance of the tool is affected by
varying the type and size of contextual infor-
mation and by the use of query expansion.
The remainder of this paper is organized as fol-
lows: ?2 describes related work and ?3 outlines
potential approaches for using Wikipedia revision
data in a feedback tool. ?4 outlines the core system
79
for generating feedback and ?5 presents an empir-
ical evaluation of this system. In ?6 we describe a
method for enhancing the system using query ex-
pansions. We discuss our findings and some future
work in ?7 and, finally, conclude in ?8.
2 Related Work
Attali (2004) examines the general effect of feed-
back in the Criterion system (Burstein et al., 2003)
and finds that students presented with feedback are
able to improve the overall quality of their writ-
ing, as measured by an automated scoring system.
This study does not investigate different kinds of
feedback, but rather looks at the issue of whether
feedback in general is useful for students. Shermis
et al. (2004) look at groups of students who used
Criterion and students who did not and compare
their writing performance as measured by high-
stakes state assessment. They found that, in gen-
eral, the students who made use of Criterion and
its feedback improved their writing skills. They
analyze the distributions of the individual gram-
mar and style error types and found that Criterion
helped reduce the number of repeated errors, par-
ticularly for mechanics (e.g. spelling and punctu-
ation errors). Chodorow et al. (2010) describe a
small study in which Criterion provided feedback
about article errors to students writing an essay for
a college-level course. They find, similarly to At-
tali (2004), that the number of article errors was
reduced in the final revised version of the essay.
Gamon et al. (2009) describe ESL Assistant ?
a web-based proofreading tool designed for lan-
guage learners who are native speakers of East-
Asian languages. They used a decision-tree ap-
proach to detect and offer suggestions for poten-
tial article and preposition errors. They also al-
lowed the user to compare the various suggestions
by showing results of corresponding web searches.
Chodorow et al. (2010) also describe a small study
where ESL Assistant was used to offer sugges-
tions for potential grammatical errors to web users
while they were composing email messages. They
reported that users were able to make effective use
of the explicit feedback for that task. The tool had
been offered as a web service but has since been
discontinued.
Our tool is similar to ESL Assistant in that both
produce a list of possible corrections. The main
difference between the tools is that ours automat-
ically derives the ranked list of correction sugges-
tions from a very large corpus of annotated errors,
rather than performing a web search on all pos-
sible alternatives in the context. The advantage
of using an error-annotated corpus is that it con-
tains implicit information about frequent confu-
sion pairs (e.g. ?at? instead of ?in?) that are in-
dependent of the frequency of the preposition and
the current context.
Milton and Cheng (2010) describe a toolkit for
helping Chinese learners of English become more
independent writers. The toolkit gives the learners
access to online resources including web searches,
online concordance tools, and dictionaries. Users
are provided with snapshots of the word or struc-
ture in context. In Milton (2006), 500 revisions
to 323 journal entries were made using an earlier
version of this tool. Around 70 of these revisions
had misinterpreted the evidence presented or were
careless mistakes; the remaining revisions resulted
in more natural sounding sentences.
3 Wikipedia Revisions
Our goal is to build a tool that can provide explicit
feedback about errors to writers. We take advan-
tage of the recently released Wikipedia preposi-
tion error corpus (Cahill et al., 2013) and design
our tool based on this large corpus containing sen-
tences annotated for preposition errors and their
corrections. The corpus was produced automati-
cally by mining a total of 288 million revisions for
8.8 million articles present in a Wikipedia XML
snapshot from 2011. The Wikipedia error corpus,
as we refer to in the rest of the paper, contains
2 million sentences annotated with preposition er-
rors and their respective corrections.
There are two possible approaches to building
an explicit feedback tool for preposition errors
based on this corpus:
1. Classifier-based. We could train a classi-
fier on the Wikipedia error corpus to predict
the correct preposition in a given context, as
Cahill et al. (2013) did. Although this would
allow us to suggest corrections for contexts
that are unseen in the Wikipedia data, the
suggestions would likely be quite noisy given
the inherent difficulty of a classification prob-
lem with a large number of classes.
1
In addi-
tion, this approach would not facilitate pro-
1
Cahill et al. (2013) used a list of 36 prepositions as
classes.
80
viding evidence for each correction to the
user.
2. Corpus-based. We could use the Wikipedia
error corpus directly for feedback. Al-
though this means that suggestions can only
be generated for contexts occurring in the
Wikipedia data, it also means that all sug-
gestion would be grounded in actual revisions
made by other humans on Wikipedia.
We believe that anchoring suggestions to
human-authored corrections affords greater util-
ity to a language learner, in line with the current
practice in lexicography that emphasizes authen-
tic usage examples (Collins COBUILD learner?s
dictionary, Sketch Engine (Kilgarriff et al., 2004)).
Therefore, in this paper, we choose the second ap-
proach to build our tool.
4 Methodology
In order to use the Wikipedia error corpus directly
for feedback, we first index the sentences in the
corpus using the following fields:
? The incorrect preposition.
? The correct preposition.
? The words, bigrams, and trigrams before (and
after) the preposition error (indexed sepa-
rately).
? The part-of-speech tags, tag bigrams, and tag
trigrams before (and after) the error (indexed
separately).
? The title and URL of the Wikipedia article in
which the sentence occurred.
? The ID of the article revision containing the
preposition error.
? The ID of the article revision in which the
correction was made.
Once the index is constructed, eliciting explicit
feedback is straightforward. The input to the sys-
tem is a tokenized sentence with a marked up
preposition error (e.g. from an automated prepo-
sition error detection system). For each input sen-
tence, the Wikipedia index is then searched with
the identified preposition error and the words (or
n-grams) present in its context. The index returns
a list of the possible corrections occurring in the
given context. The tool then counts how often
each possible preposition is returned as a possible
correction and orders its suggestions from most
frequent to least frequent. In addition, the tool also
displays five randomly chosen sentences from the
index as evidence for each correction in order to
help the learner make a better choice. The tool
can use either the lexical n-grams (n=1,2,3) or the
part-of-speech n-grams (n=1,2,3) around the error
for the contextualized search of the Wikipedia in-
dex.
Figure 1 shows a screenshot of the tool in oper-
ation. The input sentence is entered into the text
box at the top, with the preposition error enclosed
in asterisks. In this case, the tool is using parts-of-
speech on either side of the error for context. By
default, the tool shows the top five possible correc-
tions as a bar chart, sorted according to how many
times the erroneous preposition was changed to
the correction in the Wikipedia revision index. In
this example, the preposition of with the left con-
text of <DT, NNS> and the right context of <DT,
NN> was changed to the preposition in 242 times
in the Wikipedia revisions. When the user clicks
on a bar, the box on the top shows the sentence
with the change and the gray box on the right
shows 5 (randomly chosen) actual sentences from
Wikipedia where the change represented by the
bar was made.
If parts-of-speech are chosen as context, the tool
uses WebSockets to send the sentence to the Stan-
ford Tagger (Toutanova et al., 2003) in the back-
ground and compute its part-of-speech tags before
searching the index.
5 Evaluation
In order to determine how well the tool performs
at suggesting corrections, we used sentences con-
taining preposition errors from the CLC FCE
dataset. The CLC FCE Dataset is a collection of
1,244 exam scripts written by learners of English
as part of the Cambridge ESOL First Certificate in
English (Yannakoudakis et al., 2011). Our evalua-
tion set consists of 3,134 sentences, each contain-
ing a single preposition error.
We evaluate the tool on two criteria:
? Coverage. We define coverage as the pro-
portion of errors for which the tool is able to
suggest any corrections.
? Accuracy. The obvious definition of accu-
81
F
i
g
u
r
e
1
:
A
s
c
r
e
e
n
s
h
o
t
o
f
t
h
e
t
o
o
l
s
u
g
g
e
s
t
i
n
g
t
h
e
t
o
p
5
c
o
r
r
e
c
t
i
o
n
s
f
o
r
a
s
e
n
t
e
n
c
e
u
s
i
n
g
t
w
o
p
a
r
t
s
-
o
f
-
s
p
e
e
c
h
o
n
e
i
t
h
e
r
s
i
d
e
o
f
t
h
e
m
a
r
k
e
d
e
r
r
o
r
a
s
c
o
n
t
e
x
t
.
T
h
e
c
o
r
r
e
c
t
i
o
n
s
a
r
e
d
i
s
p
l
a
y
e
d
i
n
r
a
n
k
e
d
f
a
s
h
i
o
n
a
s
a
h
i
s
t
o
g
r
a
m
a
n
d
c
l
i
c
k
i
n
g
o
n
o
n
e
d
i
s
p
l
a
y
s
t
h
e
?
c
o
r
r
e
c
t
e
d
?
s
e
n
t
e
n
c
e
a
b
o
v
e
a
n
d
t
h
e
c
o
r
r
e
s
p
o
n
d
i
n
g
e
v
i
d
e
n
c
e
f
r
o
m
W
i
k
i
p
e
d
i
a
r
e
v
i
s
i
o
n
s
o
n
t
h
e
l
e
f
t
.
82
Context Found Missed Blank MRR
words1 889 (28.4%) 356 (11.4%) 1889 (60.3%) .522
words2 55 ( 1.8%) 22 ( 0.7%) 3057 (97.5%) .619
words3 16 ( 0.5%) 5 ( 0.2%) 3113 (99.3%) .762
tags1 2821 (90.0%) 241 ( 7.7%) 72 ( 2.3%) .419
tags2 1896 (60.5%) 718 (22.9%) 520 (16.6%) .390
tags3 661 (21.1%) 633 (20.2%) 1840 (58.7%) .325
Table 1: A detailed breakdown of the Found, Missing and Blank classes along with the Mean Reciprocal
Rank (MRR) values, for different types (words, tags) and sizes (1, 2, or 3 around the error) of
contextual information used in the search.
racy would be the proportion of errors for
which the tool?s best suggestion is the cor-
rect one. However, since the tool returns
a ranked list of suggestions, it is important
to award partial credit for errors where the
tool made a correct suggestion but it was not
ranked at the top. Therefore, we use the Mean
Reciprocal Rank (MRR), a standard metric
used for evaluating ranked retrieval systems
(Voorhees, 1999). MRR is computed as fol-
lows:
MRR =
1
|S|
|S|
?
i=1
1
R
i
where S denotes the set of sentences for
which ranked lists of suggestions are gener-
ated and R
i
denotes the rank of the true cor-
rection in the list of suggestions the tool re-
turns for sentence i. A higher MRR is better
since that means that the tool ranked the true
correction closer to the top of the list.
To conduct the evaluation on the FCE dataset,
we run each of the sentences through the tool and
extract the top 5 suggestions for each error anno-
tated in the sentence.
2
At this point, each error
instance input to the tool can be classified as one
of three classes:
1. Found. The true correction for the error was
found in the ranked list of suggestions made
by the tool.
2. Missing. The true correction for the error
was not found in the ranked list of sugges-
tions.
3. Blank. The tool did not return any sugges-
tions for the error.
2
In this paper, we separate the tasks of error detection and
correction and use the gold standard as an oracle to detect er-
rors and then use our system to propose and rank corrections.
First, we examine the distribution of the three
classes across the types and sizes of the contextual
information used to conduct the search. Table 1
shows, for each context type and size, a detailed
breakdown of the distribution of the three classes
along with the mean reciprocal rank (MRR) val-
ues.
3
We observe that, with words as contexts, us-
ing larger contexts certainly produces more accu-
rate results (as indicated by the larger MRR val-
ues). However, we also observe that employing
larger contexts reduces coverage (as indicated by
the decreasing percentage of Found sentences and
by the the increasing percentage of the Blank sen-
tences).
With part-of-speech tags, we observe that al-
though using larger tag contexts can find correc-
tions for a significantly larger number of sentences
as compared to similar-sized word contexts (as in-
dicated by the larger percentages of Found sen-
tences), doing so yields overall worse MRR val-
ues. This is primarily due to the fact that with
larger part-of-speech contexts the system produces
more suggestions that never contain the true cor-
rection, i.e., an increasing percentage of Missed
sentences. The most likely reason is that signifi-
cantly reducing the vocabulary size by using part-
of-speech tags introduces a lot of noise.
Figure 2 shows the distribution of the rank R
of the true correction in the list of suggestions.
4
The figure uses a rank of 10+ to denote all ranks
greater than 10 to conserve space. We observe
similar trends in the figure as in Table 1 ? us-
ing larger word contexts yield higher accuracies
but significantly lower coverage and using larger
3
We do not include Blank sentences when computing the
MRR values.
4
Note that in this figure, the bar for R = 0 includes both
sentences where no ranked list was produced (Blank) and
those where the true correction was not produced as a sug-
gestion at all (Missing).
83
wo
rds
1
wo
rds
2
wo
rds
3
tag
s1
tag
s2
tag
s3
0
100
0
200
0
300
0 0
100
0
200
0
300
0
0
1
2
3
4
5
6
7
8
9
10
10+
0
1
2
3
4
5
6
7
8
9
10
10+
0
1
2
3
4
5
6
7
8
9
10
10+
Ra
nk 
of t
rue
 co
rrec
tion
 (R)
Number of FCE sentences
Cla
ss
Bla
nk
Mi
ssin
g
Fou
nd
F
i
g
u
r
e
2
:
T
h
e
d
i
s
t
r
i
b
u
t
i
o
n
o
f
t
h
e
r
a
n
k
t
h
a
t
t
h
e
t
r
u
e
c
o
r
r
e
c
t
i
o
n
h
a
s
i
n
t
h
e
l
i
s
t
o
f
s
u
g
g
e
s
t
i
o
n
s
f
o
r
t
h
e
F
C
E
s
e
n
t
e
n
c
e
s
,
a
c
r
o
s
s
e
a
c
h
c
o
n
t
e
x
t
t
y
p
e
a
n
d
s
i
z
e
u
s
e
d
.
84
tag contexts yield lower accuracies and lower cov-
erage, even though the coverage is significantly
larger than that of the correspondingly sized word
context.
6 Query Expansion
The results in the previous section indicate that al-
though we could use part-of-speech tags as con-
texts to improve the coverage of the tool (as indi-
cated by the number of Found sentences), doing
so leads to a significant reduction in accuracy, as
indicated by the lower MRR values.
In the field of information retrieval, a common
practice is to expand the query with words similar
to words in the query in order to increase the like-
lihood of finding documents relevant to the query
(Sp?arck-Jones and Tait, 1984). In this section, we
examine whether we can use a similar technique
to improve the coverage of the tool.
We employ a simple query expansion technique
for the cases where no results would otherwise be
returned by the tool. For these cases, we first ob-
tain a list of K words similar to the two words
around the error from a distributional thesaurus
(Lin, 1998), ranked by similarity. We then gener-
ate a list of additional queries by combining these
two ranked lists of similar words. We then run
each query in the list against the Wikipedia index
until one of them yields results. Note that since
we are using a word-based thesaurus, this expan-
sion technique can only increase coverage when
applied to the words1 condition, i.e., single word
contexts. We investigate K = 1, 2, 5, or 10 expan-
sions for each of the context words.
Table 2 shows the a detailed breakdown of the
distribution of the three classes and the MRR val-
ues with query expansion integrated into the tool
for sentences where it would generally produce no
output. Each row corresponds to a different value
of K ? the number of expansions used per context
word ? is varied. Note that K = 0 corresponds to
the condition where query expansion is not used.
From the table, we observe that using query ex-
pansion indeed seems to increase the coverage of
the tool as indicated by the increasing percentage
of Found sentences and decreasing percentage of
Blank sentences. However, we also find that using
query expansion yields worse MRR values, again
because of the increasing percentage of Missed
sentences. This represents a traditional trade-off
scenario where accuracy can be traded off for an
increase in coverage, depending on the desired op-
erating characteristics.
7 Discussion and Future Work
There are several issues that merit further discus-
sion and possibly provide future extensions to the
work described in this paper.
? Need for an extrinsic evaluation. Although
our intrinsic evaluation clearly shows that the
tool has reasonably good coverage as well
as accuracy on publicly available data con-
taining preposition errors, it does not provide
any evidence that the explicit feedback pro-
vided by the tool is useful to English lan-
guage learners in a classroom setting. In the
future, we plan to conduct a controlled study
in a classroom setting that measures, for ex-
ample, whether the students that see the im-
proved feedback from the tool learn more
or better than those who either see no feed-
back at all or those who see only implicit
feedback. Biber et al. (2011) review sev-
eral previously published studies on the ef-
fects of feedback on writing development in
classrooms. Although the number of studies
that were included in the analysis is small,
some patterns did emerge. In general, stu-
dents improve their writing when they re-
ceive feedback, however greater gains are
made when they are presented with com-
ments rather than direct location and correc-
tion of errors. It is unclear how students
would react to a ranked list of suggestions
for a particular error at a given location. An
interesting finding was that L2-English stu-
dents showed greater improvements in writ-
ing when they received either feedback from
peers or computer-generated feedback than
when they received feedback from teachers.
? Assuming a single true correction. Our
evaluation setup assumes that the single cor-
rection provided as part of the FCE data set is
the only correct preposition for a given sen-
tence. However, it is well known in the gram-
matical error detection community that this is
not always the case. Most usage errors such
as preposition selection errors are a matter of
degree rather than simple rule violations such
as number agreement. As a consequence, it
is common for two native English speakers
85
Context K Found Missed Blank MRR
words1 0 889 (28.4%) 356 (11.4%) 1889 (60.3%) .522
words1 1 932 (29.7%) 417 (13.3%) 1785 (57.0%) .513
words1 2 1033 (33.0%) 550 (17.6%) 1551 (49.5%) .493
words1 5 1118 (35.7%) 691 (22.1%) 1325 (42.3%) .476
words1 10 1160 (37.0%) 780 (24.9%) 1194 (38.1%) .465
Table 2: A detailed breakdown of the Found, Missing and Blank classes along with the Mean Reciprocal
Rank (MRR) values, for different number of query expansions (K).
to have different judgments of usage. In fact,
this is exactly why the tool is designed to re-
turn a ranked list of suggestions rather than
a single suggestion. Therefore, it is possible
that our intrinsic evaluation is underestimat-
ing the performance of the tool.
? Practical considerations for deployment.
In this study, we used the gold standard er-
ror annotations for detecting preposition er-
rors before querying the tool for suggestions.
Such a setup allowed us to separate the prob-
lems of error detection and the generation
of feedback and likely gives an upper bound
on performance. Using a fully automatic
error detection system will likely introduce
additional noise into the pipeline, however,
we believe that tuning the detection system
for higher precision could mitigate that ef-
fect. Another useful idea would be to use the
classifier-based approach (see ?3) as a backup
for the corpus-based approach for providing
suggestions, i.e., using the classifier to pre-
dict the suggested corrections when no cor-
rections can be found in the Wikipedia revi-
sions.
? Using other types of expansions. In this pa-
per, we used a very simple method of gener-
ating query expansions ? a distributional the-
saurus. However, in the future, it may be
worth exploring other distributional similar-
ity methods such as Brown clusters (Brown
et al., 1992; Miller et al., 2004; Liang, 2005)
or word2vec (Mikolov et al., 2013).
8 Conclusions
In this paper, we presented our work on build-
ing a proof-of-concept tool that can provide au-
tomated explicit feedback for preposition errors.
We used an existing, error-annotated preposition
corpus produced by mining Wikipedia revisions
(Cahill et al., 2013) to not only provide a ranked
list of suggestions for any given preposition error
but also to produce human-authored evidence for
each suggested correction. The tool can use either
words or part-of-speech tags around the error as
context. We evaluated the tool in terms of both
accuracy and coverage and found that: (1) using
larger context window sizes for words increases
accuracy but reduces coverage due to sparsity (2)
using part-of-speech tags leads to increased cov-
erage compared to using words as contexts but
decreases accuracy. We also experimented with
query expansion for single words around the er-
ror and found that it led to an increase in cover-
age with only a slight decrease in accuracy; using
a larger set of expansions added more noise. In
general, we find that the approach of using a large
error-annotated corpus to provide explicit feed-
back to writers performs reasonably well in terms
of providing ranked lists of alternatives. It remains
to be seen how useful this tool is in a practical sit-
uation.
Acknowledgments
We would like to thank Beata Beigman Klebanov,
Michael Heilman, Jill Burstein, and the anony-
mous reviewers for their helpful comments about
the paper. We also thank Ani Nenkova, Chris
Callison-Burch, Lyle Ungar and their students at
the University of Pennsylvania for their feedback
on this work.
References
Yigal Attali. 2004. Exploring the Feedback and Re-
vision Features of Criterion. Paper presented at
the National Council on Measurement in Education
(NCME), Educational Testing Service, Princeton,
NJ.
Douglas Biber, Tatiana Nekrasova, and Brad Horn.
2011. The Effectiveness of Feedback for L1-English
and L2-Writing Development: A Meta-Analysis.
86
Research Report RR-11-05, Educational Testing
Service, Princeton, NJ.
Peter F. Brown, Vincent J. Della Pietra, Peter V.
de Souza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-Based n-gram Models of Natural Lan-
guage. Computational Linguistics, 18(4):467?479.
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003. Criterion online essay evaluation: An applica-
tion for automated evaluation of student essays. In
Proceedings of IAAI, pages 3?10, Acapulco, Mex-
ico.
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Di-
ane Napolitano. 2013. Robust Systems for Prepo-
sition Error Correction Using Wikipedia Revisions.
In Proceedings of NAACL, pages 507?517, Atlanta,
GA, USA.
Martin Chodorow, Michael Gamon, and Joel Tetreault.
2010. The Utility of Article and Preposition Er-
ror Correction Systems for English Language Learn-
ers: Feedback and Assessment. Language Testing,
27(3):419?436.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Gram-
matical Error Correction with Alternating Structure
Optimization. In Proceedings of ACL-HLT, pages
915?923, Portland, Oregon, USA.
Rachele De Felice and Stephen G. Pulman. 2009.
Automatic detection of preposition errors in learner
writing. CALICO Journal, 26(3):512?528.
Michael Gamon, Claudia Leacock, Chris Brockett,
William B Dolan, Jianfeng Gao, Dmitriy Belenko,
and Alexandre Klementiev. 2009. Using Statistical
Techniques and Web Search to Correct ESL Errors.
CALICO Journal, 26(3):491?511.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of EURALEX, pages 105?116.
Percy Liang. 2005. Semi-supervised Learning for Nat-
ural Language. Master?s thesis, Massachusetts Insti-
tute of Technology.
Dekang Lin. 1998. Automatic Retrieval and
Clustering of Similar Words. In Proceedings of
ACL-COLING, pages 768?774, Montreal, Quebec,
Canada.
Anastasiya A. Lipnevich and Jeffrey K. Smith. 2008.
Response to Assessment Feedback: The Effects of
Grades, Praise, and Source of Information. Re-
search Report RR-08-30, Educational Testing Ser-
vice, Princeton, NJ.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed Rep-
resentations of Words and Phrases and their Com-
positionality. In Proceedings of NIPS, pages 3111?
3119.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name Tagging with Word Clusters and
Discriminative Training. In Proceedings of HLT-
NAACL, pages 337?342, Boston, MA, USA.
John Milton and Vivying SY Cheng. 2010. A Toolkit
to Assist L2 Learners Become Independent Writers.
In Proceedings of the NAACL Workshop on Compu-
tational Linguistics and Writing: Writing Processes
and Authoring Aids, pages 33?41, Los Angeles, CA,
USA.
John Milton. 2006. Resource-rich Web-based Feed-
back: Helping learners become Independent Writ-
ers. Feedback in second language writing: Contexts
and issues, pages 123?139.
JoAnn Leah Rock. 2007. The Impact of Short-
Term Use of Criterion on Writing Skills in Ninth
Grade. Research Report RR-07-07, Educational
Testing Service, Princeton, NJ.
Alla Rozovskaya and Dan Roth. 2010. Training
Paradigms for Correcting Errors in Grammar and
Usage. In Proceedings of NAACL-HLT, pages 154?
162, Los Angeles, California.
Hongsuck Seo, Jonghoon Lee, Seokhwan Kim, Kyu-
song Lee, Sechun Kang, and Gary Geunbae Lee.
2012. A Meta Learning Approach to Grammatical
Error Correction. In Proceedings of ACL (short pa-
pers), pages 328?332, Jeju Island, Korea.
Mark D. Shermis, Jill C. Burstein, and Leonard Bliss.
2004. The Impact of Automated Essay Scoring on
High Stakes Writing Assessments. In Annual Meet-
ing of the National Council on Measurement in Ed-
ucation.
Karen Sp?arck-Jones and J. I. Tait. 1984. Automatic
Search Term Variant Generation. Journal of Docu-
mentation, 40(1):50?66.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of COLING, pages
865?872, Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selec-
tion and Error Detection. In Proceedings of ACL
(short papers), pages 353?358, Uppsala, Sweden.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of NAACL, pages 173?180, Edmon-
ton, Canada.
John Truscott. 2007. The Effect of Error Correction
on Learners? Ability to Write Accurately. Journal
of Second Language Writing, 16(4):255?272.
Ellen M. Voorhees. 1999. The TREC-8 Question An-
swering Track Report. In Proceedings of the Text
REtrieval Conference (TREC), volume 99, pages
77?82.
87
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In Proceedings of the ACL:
HLT, pages 180?189, Portland, OR, USA.
88
